[
  {
    "text": "hi everyone this is really happened from AWS today we have a webinar combining",
    "start": "4650",
    "end": "10469"
  },
  {
    "text": "AWS and ops over talking about frictionless data Lake ETL for petabyte scale streaming data next slightly for",
    "start": "10469",
    "end": "21780"
  },
  {
    "text": "me a general perspective there'll be a few topics that we talked about a quick introduction and then we're going to get",
    "start": "21780",
    "end": "27690"
  },
  {
    "text": "into some use cases around machine learning data leaks on AWS how I earn",
    "start": "27690",
    "end": "33360"
  },
  {
    "text": "source one of our customers use an observer to provide to to do analytics on for petabytes of data per month and",
    "start": "33360",
    "end": "41430"
  },
  {
    "text": "then also talk about how observer is a solution solve these problems for us",
    "start": "41430",
    "end": "46590"
  },
  {
    "text": "next steps please my name is Roy castle",
    "start": "46590",
    "end": "51780"
  },
  {
    "text": "I'm a senior manager for business development at AWS also on the panel for",
    "start": "51780",
    "end": "56910"
  },
  {
    "text": "us today is Ori I eat the CEO and co-founder box over and also c-value the VP of R&D for ion source don't use",
    "start": "56910",
    "end": "64350"
  },
  {
    "text": "themselves as the session begins next slightly so it's not about what is a",
    "start": "64350",
    "end": "70800"
  },
  {
    "text": "data leak I don't like if we think about it the centralized curated and secure repository that's a really song of sorry",
    "start": "70800",
    "end": "78780"
  },
  {
    "text": "store all of our data in a single location without really having to think too much about how do we analyze the",
    "start": "78780",
    "end": "85080"
  },
  {
    "text": "data what do you want to do with it what questions do we want to ask of that data so think of it as a place to store all",
    "start": "85080",
    "end": "90360"
  },
  {
    "text": "of your data without a lot of pre-planning or processing next slide",
    "start": "90360",
    "end": "96560"
  },
  {
    "text": "why do we want to build early that's a great question right when we have a conversation around building data leaks it's really",
    "start": "96830",
    "end": "102569"
  },
  {
    "text": "important to understand so what is the value proposition and why do we want to do it so the real reason here is we want",
    "start": "102569",
    "end": "108599"
  },
  {
    "text": "to be able to break down data files those are the biggest challenges that customers have today if they want",
    "start": "108599",
    "end": "114569"
  },
  {
    "text": "analyze the data and be able to get a holistic view of what their customers are doing and how the business is",
    "start": "114569",
    "end": "120479"
  },
  {
    "text": "performing it's important to bring all the data from all the different corners of the organization into a single",
    "start": "120479",
    "end": "125940"
  },
  {
    "text": "location so it's easy for us to analyze our data very quickly and cost-effectively but also furthermore we",
    "start": "125940",
    "end": "133079"
  },
  {
    "text": "want to be able to use different tools and different analytic capabilities whether it's machine learning",
    "start": "133079",
    "end": "138390"
  },
  {
    "text": "or simple data analytics to be able to process that data doing it all in one",
    "start": "138390",
    "end": "143520"
  },
  {
    "text": "place just makes it a lot easier for us to do that excites so when we talk about",
    "start": "143520",
    "end": "150780"
  },
  {
    "text": "data leaks there's a few components that I wanted to highlight the first kind of part about this is thinking about it",
    "start": "150780",
    "end": "157230"
  },
  {
    "text": "from a business perspective right not so much a technical standpoint or from a business standpoint so the first thing",
    "start": "157230",
    "end": "162540"
  },
  {
    "text": "that we typically have to do is we have to ingest the data there's a number of ways to do that traditionally this batch",
    "start": "162540",
    "end": "168620"
  },
  {
    "text": "workloads that we have to move let's say take a large table from a database on-premise and move that to to the cloud",
    "start": "168620",
    "end": "176100"
  },
  {
    "text": "maybe they're streaming use cases right be able to take mobile device data IOT",
    "start": "176100",
    "end": "182340"
  },
  {
    "text": "data maybe clickstream data so there's different mechanisms and different tools to move the data so that's one area that",
    "start": "182340",
    "end": "187980"
  },
  {
    "text": "we have to focus on another area that we have to focus on is once the data has been moved to the cloud we need to be",
    "start": "187980",
    "end": "194790"
  },
  {
    "text": "able to catalog it so we actually know what exists otherwise we just have a lot of data scattered all over our data Lake",
    "start": "194790",
    "end": "200160"
  },
  {
    "text": "but we don't have a real understanding of what it is so we catalog that and we",
    "start": "200160",
    "end": "205560"
  },
  {
    "text": "expose that to a metadata store so users can explore and really understand what exists the next type that we want to do",
    "start": "205560",
    "end": "213000"
  },
  {
    "text": "after we catalog that we also want to process it so typically where we see in large organizations that have a lot of",
    "start": "213000",
    "end": "218579"
  },
  {
    "text": "data in silos is that the end user the data analyst a data scientist is really",
    "start": "218579",
    "end": "223829"
  },
  {
    "text": "forced to take the data and process it and do the the data wrangling sort of on",
    "start": "223829",
    "end": "229650"
  },
  {
    "text": "their own every single time because there's really no centralized way to process that data so we're building the",
    "start": "229650",
    "end": "235440"
  },
  {
    "text": "data like you want to centralize that process in a way that is easy for us to",
    "start": "235440",
    "end": "240750"
  },
  {
    "text": "automate secure and also govern the next aspect that we have to consider is is",
    "start": "240750",
    "end": "245910"
  },
  {
    "text": "storage there's a number of different ways and technologies to store the data but for data like remember we're talking",
    "start": "245910",
    "end": "252090"
  },
  {
    "text": "about a lot of these maybe some of the data we're not ready to use yet some of the data we're using already so we need",
    "start": "252090",
    "end": "258359"
  },
  {
    "text": "a way to store the data in a durable fashion but also make sure that its cost effective so we're not paying a lot of",
    "start": "258359",
    "end": "264120"
  },
  {
    "text": "money to store all that data before we get inside that adapts and then lastly once the data has been stored process",
    "start": "264120",
    "end": "270780"
  },
  {
    "text": "and catalogs now we want to be able to bring different compute engines to process that data whether it's for reporting bi use cases",
    "start": "270780",
    "end": "277960"
  },
  {
    "text": "and even machine learn and all of that has to be contained in a secure govern",
    "start": "277960",
    "end": "284950"
  },
  {
    "text": "and auditable fashion so we can actually control that data and make sure there's no leakage or anything like that even if",
    "start": "284950",
    "end": "291760"
  },
  {
    "text": "the order does come around and say you know what is it you have how are you using it we have a track and we have a",
    "start": "291760",
    "end": "298780"
  },
  {
    "text": "wage it's kind of audits that that system looks like okay so from our",
    "start": "298780",
    "end": "305860"
  },
  {
    "text": "reference architecture perspective again if we look at the previous slide that talked about the ingestion did the",
    "start": "305860",
    "end": "312100"
  },
  {
    "text": "processing and cataloging the storage in the compute we sort of break it down and we take it a step deeper we said on the",
    "start": "312100",
    "end": "318490"
  },
  {
    "text": "left side we have a bunch of sources right there's some streaming sources there's some data database sources data",
    "start": "318490",
    "end": "324580"
  },
  {
    "text": "warehouses some of it may be on-premise some of them may be on AWS already then",
    "start": "324580",
    "end": "330700"
  },
  {
    "text": "we want to bring that into the central location right the lake and from AWS perspective the storage the durable",
    "start": "330700",
    "end": "336910"
  },
  {
    "text": "storage is based on Amazon s3 it's an object store think of it as a place to put all of your data we're not really",
    "start": "336910",
    "end": "342790"
  },
  {
    "text": "thinking about what it is and how to use it but also gives you lifecycle policies that allow you to move the hard data to",
    "start": "342790",
    "end": "349690"
  },
  {
    "text": "warm and cold as it transitions as time evolves so you're saving on storage by",
    "start": "349690",
    "end": "355090"
  },
  {
    "text": "moving the data to colder more cost-effective storage and then from AWS CloudFormation that's a service that we",
    "start": "355090",
    "end": "361930"
  },
  {
    "text": "introduced at wins and last year and also made GA a couple months ago and it",
    "start": "361930",
    "end": "367390"
  },
  {
    "text": "gives you a way to build governance around your Dilek securing authorization who can access to what tables when they",
    "start": "367390",
    "end": "374290"
  },
  {
    "text": "access it how often the access and what tools they're accessing it from and I don't know right that we have the data",
    "start": "374290",
    "end": "379540"
  },
  {
    "text": "consumption layer because we built this data like it's a common lake for all of our users but now we don't want to force",
    "start": "379540",
    "end": "386080"
  },
  {
    "text": "them to to use a particular engine to create a data we don't want to tell them hey data scientists you can only use BI",
    "start": "386080",
    "end": "393490"
  },
  {
    "text": "tools right it's not efficient for that so we want to be able to open it up and expose it it's for the link to any tools",
    "start": "393490",
    "end": "399700"
  },
  {
    "text": "where there is Amazon redshift for data warehousing Amazon Athena for analytics",
    "start": "399700",
    "end": "405190"
  },
  {
    "text": "and interactive queries whether it's Dupin or notebooks using shade maker to be able to process and analyze that data",
    "start": "405190",
    "end": "411780"
  },
  {
    "text": "next step so what is amazon athena so we",
    "start": "411780",
    "end": "417940"
  },
  {
    "text": "talked about a bunch of different things and one of the the compute engines that line the agendas amazon athena so what",
    "start": "417940",
    "end": "423490"
  },
  {
    "text": "really is so think of athena is a silver less interactive query engine for data",
    "start": "423490",
    "end": "428680"
  },
  {
    "text": "on history there's no need to load the data into a database there's no need to manage any infrastructure or anything",
    "start": "428680",
    "end": "435340"
  },
  {
    "text": "like that you simply submit a sicko query to athena and we execute the query behind the things on top of data in s3",
    "start": "435340",
    "end": "442500"
  },
  {
    "text": "optimization for queries tend to happen in two places in the query itself how do",
    "start": "442500",
    "end": "447610"
  },
  {
    "text": "you write the query in a way that's fully optimized but also on the data side what is the best way to store the",
    "start": "447610",
    "end": "452650"
  },
  {
    "text": "data whether it's CSV or comma format like park' do we add any kind of",
    "start": "452650",
    "end": "458260"
  },
  {
    "text": "optimization sorting or things like that to the data to improve the performance and some of the things that Athena is",
    "start": "458260",
    "end": "464650"
  },
  {
    "text": "really great at is be able to analyze logs we see this all the time with our users they collect a lot of AWS",
    "start": "464650",
    "end": "471940"
  },
  {
    "text": "infrastructure logs maybe server logs maybe application locks all the data gets stored in s3 in",
    "start": "471940",
    "end": "478240"
  },
  {
    "text": "some structured format maybe JSON maybe CSV format maybe even column repartee format I think that gives you a quick",
    "start": "478240",
    "end": "485050"
  },
  {
    "text": "and easy way to dive into those logs without building a lot of infrastructure any kind of systems around it so very",
    "start": "485050",
    "end": "491410"
  },
  {
    "text": "quickly just point create a table on that data and start running query also interactive analytics is that data grows",
    "start": "491410",
    "end": "498490"
  },
  {
    "text": "is more use cases come in I feel it can be exposed to a JDBC an ODBC driver to your favorite bi tool to be able to run",
    "start": "498490",
    "end": "505840"
  },
  {
    "text": "a little queries and build dashboards and reports on top of that you know ask me also be able to take that because",
    "start": "505840",
    "end": "511780"
  },
  {
    "text": "Athena has a REST API you can take those those those all out those analytic",
    "start": "511780",
    "end": "517419"
  },
  {
    "text": "queries and embed them into your application and it doesn't mean embedding bi widgets at your application",
    "start": "517420",
    "end": "523060"
  },
  {
    "text": "actually means embedding actual analytical type of queries and capabilities inside of your application",
    "start": "523060",
    "end": "528550"
  },
  {
    "text": "so if a user wants to come to your website and slice and dice the data the expose of them",
    "start": "528550",
    "end": "534010"
  },
  {
    "text": "Athena is a great tool to integrate into that next slide so when we talk about how do",
    "start": "534010",
    "end": "544640"
  },
  {
    "text": "we optimize performance because the fina is not a database all right let's ena is a distributed query engine and a data",
    "start": "544640",
    "end": "551960"
  },
  {
    "text": "lives domestically data whenever we scan it gets loaded into memory in the Athena service and then we execute the query",
    "start": "551960",
    "end": "558410"
  },
  {
    "text": "plan on top of that so the best way to serve optimize it is like I said before at the query side right can we have some",
    "start": "558410",
    "end": "564709"
  },
  {
    "text": "eyes the query can we improve the joint operations and things like that but also on the data side so comma format is a",
    "start": "564709",
    "end": "570380"
  },
  {
    "text": "typical one right be able to take a CSV file format even though it's compressed",
    "start": "570380",
    "end": "575450"
  },
  {
    "text": "but still converted to a columnar format that gives us better structure a binary format better compression better access",
    "start": "575450",
    "end": "582649"
  },
  {
    "text": "mechanisms also exposes some statistics about the data inside so we're interested in only a specific user Oh",
    "start": "582649",
    "end": "590240"
  },
  {
    "text": "use your idea inside that file we can only scan we can only return the data relevant to that user instead of",
    "start": "590240",
    "end": "596240"
  },
  {
    "text": "returning the entire data set so it reduces how much data we ultimately scan partitioning is another way to sort of",
    "start": "596240",
    "end": "602089"
  },
  {
    "text": "group similar data together so if you have a big data set and let's say the",
    "start": "602089",
    "end": "608089"
  },
  {
    "text": "data set is product reviews and some data belongs to book reviews and some data belongs to video readings if you",
    "start": "608089",
    "end": "615650"
  },
  {
    "text": "group those together into two separate groups when you execute a query that's only interested in the books categories",
    "start": "615650",
    "end": "622279"
  },
  {
    "text": "we don't have to scan the video category or other categories right so we reduce the amount of data that we scan",
    "start": "622279",
    "end": "627800"
  },
  {
    "text": "therefore improving performance and also reducing cost compression comes along with the condor format I think that",
    "start": "627800",
    "end": "635720"
  },
  {
    "text": "charges you per data scan but uncompressed data so if you compress the data from 100 megabytes to one megabyte",
    "start": "635720",
    "end": "643400"
  },
  {
    "text": "you only pay for the one megabyte of data so that's really really important compression definitely comes with",
    "start": "643400",
    "end": "648410"
  },
  {
    "text": "calendar format format like like Park AOR see automatically normalization is",
    "start": "648410",
    "end": "654920"
  },
  {
    "text": "another aspect right when we take traditional data warehouse query and we bring them in to a distributed query",
    "start": "654920",
    "end": "660230"
  },
  {
    "text": "engine like Athena it's important to sort of think about it from the perspective of how can we sort of normalize that data set instead of",
    "start": "660230",
    "end": "667220"
  },
  {
    "text": "actually it's more like denormalizing the data set instead of having complex join multiple tables together can we pre",
    "start": "667220",
    "end": "674149"
  },
  {
    "text": "process that data set and flatten it out into one single wider table that makes",
    "start": "674149",
    "end": "680209"
  },
  {
    "text": "clearing it and analyzing it much easier it's also helpful for the analyst and the data scientist because they don't",
    "start": "680209",
    "end": "686059"
  },
  {
    "text": "really have to understand the relationships is in the tables and how to join it together now it's all in one",
    "start": "686059",
    "end": "691910"
  },
  {
    "text": "place so it makes it a lot easier but also improve the the performance because the engine doesn't have to do extra work",
    "start": "691910",
    "end": "697100"
  },
  {
    "text": "to join our data together stream processing is another great thing right",
    "start": "697100",
    "end": "702230"
  },
  {
    "text": "so student processing is something that is becoming more and more of a reality for a lot of our customers to be able to",
    "start": "702230",
    "end": "708800"
  },
  {
    "text": "take continuous data capture events from databases and process them in real time",
    "start": "708800",
    "end": "714230"
  },
  {
    "text": "ok click stream data and process it in real time get real-time analytics so if",
    "start": "714230",
    "end": "719660"
  },
  {
    "text": "you're looking at that data in real time you can start taking action on the data without having to wait for somewhere",
    "start": "719660",
    "end": "725360"
  },
  {
    "text": "batch processes to take place and an absurd that's something that is becoming more and more interesting for customers",
    "start": "725360",
    "end": "733009"
  },
  {
    "text": "today as you build data lakes on AWS on Amazon s3 Amazon s3 is an object store",
    "start": "733009",
    "end": "738499"
  },
  {
    "text": "so to update individual rows or records inside of a file becomes a bit more difficult right you have to get the file",
    "start": "738499",
    "end": "744740"
  },
  {
    "text": "you have to update it in some tool and then you have to replace the file on next week so how do we improve that",
    "start": "744740",
    "end": "751629"
  },
  {
    "text": "operation all together how do we insert and update existing records in files that already exist in histories",
    "start": "751629",
    "end": "757809"
  },
  {
    "text": "next slide so ox over you'll hear a lot",
    "start": "757809",
    "end": "763279"
  },
  {
    "text": "more about it and show you how we can help you optimize a lot of these things that we just talked about so the best",
    "start": "763279",
    "end": "768769"
  },
  {
    "text": "practice is that I explain a great write those things that you can implement on your own the things that we provide you",
    "start": "768769",
    "end": "776240"
  },
  {
    "text": "with with instructions and guidance and things like that but I'm so much sort of take these things and makes them super",
    "start": "776240",
    "end": "782449"
  },
  {
    "text": "easy for you it applies them automatically and if izing in a continuous manner so it's not something that you have to keep messing with so",
    "start": "782449",
    "end": "789259"
  },
  {
    "text": "keep adjusting they sort of apply this on the fly automatically so data can just flow in all best practices applied",
    "start": "789259",
    "end": "796189"
  },
  {
    "text": "in the user can just query and analyze that data without really having to worry about is it optimized for best",
    "start": "796189",
    "end": "802699"
  },
  {
    "text": "performance is it optimize for the best cost all right so that's something that's that's really important you'll see how",
    "start": "802699",
    "end": "808190"
  },
  {
    "text": "it helps you over time manage your data lake as a whole and make adding more data sets",
    "start": "808190",
    "end": "814700"
  },
  {
    "text": "into your they're like much much easier so this is it I'm gonna hand it over right now to save up from iron so he's",
    "start": "814700",
    "end": "821690"
  },
  {
    "text": "gonna take you through what they're doing how they're using absorber and Amazon Athena and some of their use",
    "start": "821690",
    "end": "827300"
  },
  {
    "text": "cases thank you very much right so hi my name",
    "start": "827300",
    "end": "839660"
  },
  {
    "text": "is Sam Feldman I am working for Aaron source mobile source mobile is a P&L and",
    "start": "839660",
    "end": "845090"
  },
  {
    "text": "Alice's family basically we are developing an operating large in a video",
    "start": "845090",
    "end": "852380"
  },
  {
    "text": "network and the mediation and user acquisition platform we are weighed 270",
    "start": "852380",
    "end": "859670"
  },
  {
    "text": "employees in total about 110 people in aren t all our infrastructure size in",
    "start": "859670",
    "end": "866960"
  },
  {
    "text": "Amazon Web Services we are receiving about 1.7 million events a second and",
    "start": "866960",
    "end": "873110"
  },
  {
    "text": "holding right now 5.3 petabytes of data warehouse our major developing languages",
    "start": "873110",
    "end": "879620"
  },
  {
    "text": "are Scala no GS and Python for data science and there is one very important",
    "start": "879620",
    "end": "885800"
  },
  {
    "text": "thing you need to know about our area advertisement area is that we are living",
    "start": "885800",
    "end": "893330"
  },
  {
    "text": "in very competitive world and we need a",
    "start": "893330",
    "end": "900080"
  },
  {
    "text": "taste weeks that a product cycle meaning that we are receiving business requirements and then we need to provide",
    "start": "900080",
    "end": "908740"
  },
  {
    "text": "the value very fast including the data so before observer we have a pretty",
    "start": "908740",
    "end": "917720"
  },
  {
    "text": "known infrastructure architecture while we have in a team in our sauce called",
    "start": "917720",
    "end": "926900"
  },
  {
    "text": "iron beast which was providing the collection and ingestion services for all irons of PLL's",
    "start": "926900",
    "end": "933190"
  },
  {
    "text": "it was no cell service it was data engineering silo",
    "start": "933190",
    "end": "938420"
  },
  {
    "text": "the cost was pretty high and the scale was very problematic so if I would ask",
    "start": "938420",
    "end": "945050"
  },
  {
    "text": "for example for some feature it would take me weeks to months to get this feature because I'm competing and",
    "start": "945050",
    "end": "951589"
  },
  {
    "text": "competiting with other islands of spinels so while we started to",
    "start": "951589",
    "end": "963620"
  },
  {
    "text": "evaluating other alternatives we keep kept in mind a few very important",
    "start": "963620",
    "end": "969850"
  },
  {
    "text": "criterias for us ok it was obviously cost it was scalability because amount of",
    "start": "969850",
    "end": "976940"
  },
  {
    "text": "data be receiving and due by the way during the evaluation we we kind of was",
    "start": "976940",
    "end": "982399"
  },
  {
    "text": "in hyper growth in our company so we were multiplying users each month and it",
    "start": "982399",
    "end": "992779"
  },
  {
    "text": "was very important for us to break silos and being free agile in our development",
    "start": "992779",
    "end": "999320"
  },
  {
    "text": "cycles however we very like open source so the",
    "start": "999320",
    "end": "1004839"
  },
  {
    "text": "first alternative was going to Apache spark developing and creating our own",
    "start": "1004839",
    "end": "1010570"
  },
  {
    "text": "data pipelines while evaluating that we understood that it's nice we like open",
    "start": "1010570",
    "end": "1015760"
  },
  {
    "text": "source of data science team had great experience with Apache spark we are",
    "start": "1015760",
    "end": "1022480"
  },
  {
    "text": "still using Apache spark there and however it it would take us about half a",
    "start": "1022480",
    "end": "1028178"
  },
  {
    "text": "year to create stable robust environments for Apache spark in our",
    "start": "1028179",
    "end": "1034900"
  },
  {
    "text": "infrastructure additionally to Apache spark we evaluated few competitive that",
    "start": "1034900",
    "end": "1040600"
  },
  {
    "text": "overall solutions and also we evaluated the other ETL basically is really",
    "start": "1040600",
    "end": "1049049"
  },
  {
    "text": "located company which has a great deal problem with our scale and then we met",
    "start": "1049049",
    "end": "1056559"
  },
  {
    "text": "up solder and choose them for following reasons we looked for a solution ok and",
    "start": "1056559",
    "end": "1066400"
  },
  {
    "text": "which were basically implemented while",
    "start": "1066400",
    "end": "1071620"
  },
  {
    "text": "all our data reciting Kafka we have many Kafka clusters many topics all our",
    "start": "1071620",
    "end": "1079590"
  },
  {
    "text": "development they both through the Kafka on our messages there so the solution",
    "start": "1079590",
    "end": "1086950"
  },
  {
    "text": "basically right now is connecting up solar to our Kafka starting digesting",
    "start": "1086950",
    "end": "1092020"
  },
  {
    "text": "the data transforming it delete some field set some fields calculate some",
    "start": "1092020",
    "end": "1098529"
  },
  {
    "text": "fields and then push it into the Amazon s3 4k files Amazon redshift an elastic",
    "start": "1098529",
    "end": "1107770"
  },
  {
    "text": "show search and then we can consume this data using gamma so matina and the",
    "start": "1107770",
    "end": "1115210"
  },
  {
    "text": "apache spark for further transitions we",
    "start": "1115210",
    "end": "1123340"
  },
  {
    "text": "using the data for new real-time analytics of raw data we are creating data in our outputs we also using it",
    "start": "1123340",
    "end": "1131740"
  },
  {
    "text": "vastly for gdpr and copper compliances it was a great solution for us it was",
    "start": "1131740",
    "end": "1139270"
  },
  {
    "text": "the GDP re-introduced was just introduced and we were able to complete",
    "start": "1139270",
    "end": "1147760"
  },
  {
    "text": "the project in WoW I think in six weeks tends to absorb abilities also we are",
    "start": "1147760",
    "end": "1155500"
  },
  {
    "text": "using absolute to push a business and the infrastructure look for analysis and",
    "start": "1155500",
    "end": "1163500"
  },
  {
    "text": "we are duplicating the data which enters are almost in elastic search service by",
    "start": "1163500",
    "end": "1169720"
  },
  {
    "text": "the way we are more elastic search class there is about 55 to 60 terabytes right now also just to understand at the scale",
    "start": "1169720",
    "end": "1179080"
  },
  {
    "text": "it's 1.7 million events per second in Greece we have 15 table outputs what",
    "start": "1179080",
    "end": "1193450"
  },
  {
    "text": "benefit to achieved first time to market date weeks eliminating the data engineer silo",
    "start": "1193450",
    "end": "1199240"
  },
  {
    "text": "as I said before we had one team for 800 people in iron source providing the",
    "start": "1199240",
    "end": "1205600"
  },
  {
    "text": "service right now every niche developer in all my development groups capable to",
    "start": "1205600",
    "end": "1213250"
  },
  {
    "text": "enter apps over create the feature add change or delete any field in the data",
    "start": "1213250",
    "end": "1221610"
  },
  {
    "text": "go to observer and change it immediately so basically on my product my bi my",
    "start": "1221610",
    "end": "1230290"
  },
  {
    "text": "operation folks capable to receive the feature very fast as there is no silo we",
    "start": "1230290",
    "end": "1239380"
  },
  {
    "text": "saved thousands of big data engineering hours we increased our scale almost 20",
    "start": "1239380",
    "end": "1247060"
  },
  {
    "text": "times in here in half because it's so easy to create the data pipelines and",
    "start": "1247060",
    "end": "1254410"
  },
  {
    "text": "play with it an exercise with it and assimilate similar changes and so on",
    "start": "1254410",
    "end": "1261090"
  },
  {
    "text": "through that we received also significant cost reduction as previous data warehouse solution was pretty",
    "start": "1261090",
    "end": "1268120"
  },
  {
    "text": "costly which",
    "start": "1268120",
    "end": "1276080"
  },
  {
    "text": "top pay absolute feature we got it's very important to have a scale inside",
    "start": "1276080",
    "end": "1282830"
  },
  {
    "text": "Chicago's environment we capable to scale I believe in two years we never",
    "start": "1282830",
    "end": "1290110"
  },
  {
    "text": "got any scale problems absolute whatsoever absolutely in our case uses the same",
    "start": "1290110",
    "end": "1297580"
  },
  {
    "text": "technologies we are using so it it was very easy to integrate visual servicer",
    "start": "1297580",
    "end": "1306649"
  },
  {
    "text": "transformational gradiation deduplication you know think about it that you don't need you can you can code",
    "start": "1306649",
    "end": "1316039"
  },
  {
    "text": "through the yuan okay so you can create a transformation you can aggregate and",
    "start": "1316039",
    "end": "1321740"
  },
  {
    "text": "the duplicate data and you can see it in real time reflecting in your in your",
    "start": "1321740",
    "end": "1327860"
  },
  {
    "text": "data like you can create testing testing table you can test data from the state",
    "start": "1327860",
    "end": "1335360"
  },
  {
    "text": "the same CAFTA topic your production is working and you'd see that immediate a",
    "start": "1335360",
    "end": "1340600"
  },
  {
    "text": "tina connector so we started using the absorber while we used only redshift and",
    "start": "1340600",
    "end": "1347690"
  },
  {
    "text": "then we introduced the tina using the app soldier so for us compaction custom",
    "start": "1347690",
    "end": "1356510"
  },
  {
    "text": "partitioning was a huge huge deal sometimes we are pushing data lately",
    "start": "1356510",
    "end": "1363429"
  },
  {
    "text": "sometimes it's just minutes or hours sometimes days so compacting data was a",
    "start": "1363429",
    "end": "1369740"
  },
  {
    "text": "must feature for us and we got it using absol and as VIP energy and managing of",
    "start": "1369740",
    "end": "1377899"
  },
  {
    "text": "DevOps and you know I know that some of you understand the headed pair of",
    "start": "1377899",
    "end": "1383090"
  },
  {
    "text": "compliance and security introducing you that data pattern technology into your environment can be pretty scary and",
    "start": "1383090",
    "end": "1390889"
  },
  {
    "text": "frustrating because you need to work with your legal and with your IT and so on and and you it may take months until",
    "start": "1390889",
    "end": "1399830"
  },
  {
    "text": "you will capable to start pocs in our case UPS or deployed an RVP see it so",
    "start": "1399830",
    "end": "1406370"
  },
  {
    "text": "there is no data out absorber is not and all that or any other company touching our data and and it it made our",
    "start": "1406370",
    "end": "1414630"
  },
  {
    "text": "process super easy and we got basically approved for my tea and legal I believe",
    "start": "1414630",
    "end": "1422190"
  },
  {
    "text": "after one week of of testing Ori from",
    "start": "1422190",
    "end": "1434370"
  },
  {
    "text": "Absalom will take it from here and explaining about the features of the",
    "start": "1434370",
    "end": "1440100"
  },
  {
    "text": "product thank you very much Sarah and so my name is Ollie I'm the",
    "start": "1440100",
    "end": "1446130"
  },
  {
    "text": "co-founder and CEO of observer I'm an ex DBA and head of data integration",
    "start": "1446130",
    "end": "1451710"
  },
  {
    "text": "platform and today I want to tell you why we decided to build up solver also",
    "start": "1451710",
    "end": "1457049"
  },
  {
    "text": "all those ever gave a very good into it why we built up solver and I want to",
    "start": "1457049",
    "end": "1462720"
  },
  {
    "text": "show you them of how the product actually works so first observer is an advanced technology partner for AWS with",
    "start": "1462720",
    "end": "1469950"
  },
  {
    "text": "the data analytics competency you can use the product by just going to the AWS",
    "start": "1469950",
    "end": "1475140"
  },
  {
    "text": "marketplace and opening a free trial and all the building that comes after that it would be as part of your AWS bill and",
    "start": "1475140",
    "end": "1481590"
  },
  {
    "text": "the app server solution will also validate it technically by AWS as part",
    "start": "1481590",
    "end": "1486929"
  },
  {
    "text": "of the well architected program and also as part of the competency program so as",
    "start": "1486929",
    "end": "1494760"
  },
  {
    "text": "we Hassan said in his in his slides everyone wants to use the database",
    "start": "1494760",
    "end": "1499919"
  },
  {
    "text": "because of the the advantages for storing data it's very low cost it's very reliable and you can store data",
    "start": "1499919",
    "end": "1506970"
  },
  {
    "text": "which is also unstructured or semi structured which you can't really do",
    "start": "1506970",
    "end": "1512039"
  },
  {
    "text": "with databases and that's really necessary for an initiative like machine learning the challenge usually start",
    "start": "1512039",
    "end": "1518970"
  },
  {
    "text": "when you actually want to use the data and then you don't only have all the comfort that you could used to have when",
    "start": "1518970",
    "end": "1524640"
  },
  {
    "text": "you worked with the database so there isn't there are no schemas there is no sequel no out-of-the-box performance",
    "start": "1524640",
    "end": "1531600"
  },
  {
    "text": "optimization and you need to know all of the best practices just like we Hassan described and you need to spend a lot of",
    "start": "1531600",
    "end": "1538080"
  },
  {
    "text": "time on doing data ops so the process of data et al which is the topic of our session today",
    "start": "1538080",
    "end": "1544980"
  },
  {
    "text": "is moving data from the left side of the diagram to the right side of the diagram and doing it today requires a variety of",
    "start": "1544980",
    "end": "1554640"
  },
  {
    "text": "tools and it doesn't really matter if you're using open source or you're using AWS you would need to use several tools",
    "start": "1554640",
    "end": "1562050"
  },
  {
    "text": "to implement data like it yet for example let's say you would write an ETL",
    "start": "1562050",
    "end": "1567930"
  },
  {
    "text": "job using spark then you would orchestrate those those ETL jobs with a",
    "start": "1567930",
    "end": "1573300"
  },
  {
    "text": "platform like Apache airflow or a putty knife I'll and if you want to do operations like joins and aggregation",
    "start": "1573300",
    "end": "1580140"
  },
  {
    "text": "you would also also need to manage the state of your ETL so you would actually",
    "start": "1580140",
    "end": "1585720"
  },
  {
    "text": "need to spin up an additional database like Cassandra like Redis and use them",
    "start": "1585720",
    "end": "1591390"
  },
  {
    "text": "use them to implement those choice the result of what you're seeing here is that you kind of need the you've got any",
    "start": "1591390",
    "end": "1599040"
  },
  {
    "text": "data engineered for everything that's how you're creating a data engineering silo and that's the reasoning data legs are",
    "start": "1599040",
    "end": "1605130"
  },
  {
    "text": "often notorious for being very very long so the project's take quite a lot of time to do and there's no soft service",
    "start": "1605130",
    "end": "1612270"
  },
  {
    "text": "so the data consumers who understand how to manipulate the data don't get that direct access like they had in the",
    "start": "1612270",
    "end": "1619520"
  },
  {
    "text": "databases this this challenge this pain is the reason that we created up solver",
    "start": "1619520",
    "end": "1625200"
  },
  {
    "text": "and the mission is one platform that takes you from the left side of this",
    "start": "1625200",
    "end": "1630540"
  },
  {
    "text": "diagram from streaming data from data legs into the right side we're actually consuming the data and getting the",
    "start": "1630540",
    "end": "1636870"
  },
  {
    "text": "business value that you that you want absorbers approach to data Lake ETL is",
    "start": "1636870",
    "end": "1642470"
  },
  {
    "text": "streaming so the data is going to be delivered using streaming every event is",
    "start": "1642470",
    "end": "1647700"
  },
  {
    "text": "going to be cost separately but you will also be able to use all your historical",
    "start": "1647700",
    "end": "1653220"
  },
  {
    "text": "data so absorber transformations are fully stateful and you can use absorber and indexing engine which is baked into",
    "start": "1653220",
    "end": "1660300"
  },
  {
    "text": "the platform to connect your historical your historical data it was also very",
    "start": "1660300",
    "end": "1665700"
  },
  {
    "text": "important for both remote customers in AWS the data so we will not create a",
    "start": "1665700",
    "end": "1670950"
  },
  {
    "text": "login for the customer so they will be able to use the same kind of tools that we face on the",
    "start": "1670950",
    "end": "1676940"
  },
  {
    "text": "driving his part of the presentation that's why I'm so very storing all it's low data index data",
    "start": "1676940",
    "end": "1683900"
  },
  {
    "text": "Palkia data everything is stored on a street all the metadata is stored in in",
    "start": "1683900",
    "end": "1689450"
  },
  {
    "text": "an open metadata store like glue data catalog or Apache hive matter store and",
    "start": "1689450",
    "end": "1694940"
  },
  {
    "text": "the code that you are generating with apps offer is actually sequel so you can take that sequel push it to a git",
    "start": "1694940",
    "end": "1701510"
  },
  {
    "text": "repository and continue to do the ICD just that you are doing today there are",
    "start": "1701510",
    "end": "1708650"
  },
  {
    "text": "a few benefits that say that our that are common to absorber customers in many",
    "start": "1708650",
    "end": "1714500"
  },
  {
    "text": "cases we see what's ever described in ayran source recovery is want to get self-service access to the data and App",
    "start": "1714500",
    "end": "1721370"
  },
  {
    "text": "Store it takes away about 95% of the effort comparing to Apache spark",
    "start": "1721370",
    "end": "1726680"
  },
  {
    "text": "pipelines just because you're using this the SQL you already know or use a visual",
    "start": "1726680",
    "end": "1732140"
  },
  {
    "text": "interface instead of integrating multiple infrastructures together the",
    "start": "1732140",
    "end": "1737570"
  },
  {
    "text": "second thing is that you can connect the different silos in your lake the streaming data the lake so all sources",
    "start": "1737570",
    "end": "1744260"
  },
  {
    "text": "of data can be joined in absorber as part of the ETL and delivered to the",
    "start": "1744260",
    "end": "1749480"
  },
  {
    "text": "consumption layer in real time the last part and in one of observers best",
    "start": "1749480",
    "end": "1755030"
  },
  {
    "text": "partnerships is the partnership with Athena app server has the deep integration into Athena and by running a",
    "start": "1755030",
    "end": "1761510"
  },
  {
    "text": "teener and an absorber generated file system on a stream your queries Timlin a",
    "start": "1761510",
    "end": "1766970"
  },
  {
    "text": "hundred times faster comparing to just storing the raw data as is honest tree",
    "start": "1766970",
    "end": "1773320"
  },
  {
    "text": "observer also adds capabilities to Athena and that would be part of the demo and for example the ability to",
    "start": "1773320",
    "end": "1780440"
  },
  {
    "text": "update or delete tables by by key all of those benefits as I just described",
    "start": "1780440",
    "end": "1787280"
  },
  {
    "text": "they're made possible with the indexing technology that observability to the ability to create an index on top of the",
    "start": "1787280",
    "end": "1795950"
  },
  {
    "text": "data neck kind of closes the gap between the functionality you would get from a database and the functionality you would",
    "start": "1795950",
    "end": "1801980"
  },
  {
    "text": "get from a data Lake at at this point we",
    "start": "1801980",
    "end": "1808420"
  },
  {
    "text": "the first slides I think now we are ready for the demo and actually see how the product works and the scenario we",
    "start": "1808420",
    "end": "1816220"
  },
  {
    "text": "chose for this for this webinar is click prediction so in this case we have three",
    "start": "1816220",
    "end": "1821590"
  },
  {
    "text": "sources of data one of them is the impressions they add impression peeping people clicking on ads that's coming",
    "start": "1821590",
    "end": "1828490"
  },
  {
    "text": "from Apache Kafka there are some of those people actually click on the ads that they see so that stream is coming",
    "start": "1828490",
    "end": "1834310"
  },
  {
    "text": "from Apache Kinesis and we also have a bunch of historical data on history and",
    "start": "1834310",
    "end": "1840070"
  },
  {
    "text": "that historical data is very important for us to be able to predict if the user is going to click so we want to create a",
    "start": "1840070",
    "end": "1847600"
  },
  {
    "text": "data set with those three pieces of data impression user data and correct and query that data set from Amazon latina",
    "start": "1847600",
    "end": "1855480"
  },
  {
    "text": "since at absorber is integrated with the glue data catalog you could also use the",
    "start": "1855480",
    "end": "1861190"
  },
  {
    "text": "redshift spectrum you can use your own version of Apache presto but I think that ena became very very",
    "start": "1861190",
    "end": "1867130"
  },
  {
    "text": "popular today we certain a lot of our customers we decided to use that for our for our demo so at this point I'm",
    "start": "1867130",
    "end": "1875470"
  },
  {
    "text": "switching the apps over user interface and if you would connect to ups over to",
    "start": "1875470",
    "end": "1882070"
  },
  {
    "text": "the AWS marketplace this would be the user interface that you will see and here you can see a list of connectors so",
    "start": "1882070",
    "end": "1889900"
  },
  {
    "text": "I could bring data from s3 from Kinesis from Kafka and let's see just here what",
    "start": "1889900",
    "end": "1895720"
  },
  {
    "text": "does it mean to use the connector so for example for Kinesis I just choose a region I choose a stream and once I",
    "start": "1895720",
    "end": "1903220"
  },
  {
    "text": "click on continue a datasource would be created and not to solver so what's",
    "start": "1903220",
    "end": "1908950"
  },
  {
    "text": "what's actually a data source so a data source and this is a list of data sources that they have in my in my",
    "start": "1908950",
    "end": "1916900"
  },
  {
    "text": "environment in my demo environment I think source is visually visualization of your data so if I'm clicking on",
    "start": "1916900",
    "end": "1924100"
  },
  {
    "text": "impressions for example app server automatically generated the schema so",
    "start": "1924100",
    "end": "1929140"
  },
  {
    "text": "this is on the left you can see that this data set has four fields the time stamp the impression ID the user ID and",
    "start": "1929140",
    "end": "1936340"
  },
  {
    "text": "the campaign ID and for each field in the",
    "start": "1936340",
    "end": "1941530"
  },
  {
    "text": "for example this community I can also see more statistics to understand what's actually in my law data so that would be",
    "start": "1941530",
    "end": "1948130"
  },
  {
    "text": "for the first stage of our data exploration this campaign ad appears in",
    "start": "1948130",
    "end": "1953380"
  },
  {
    "text": "100 percent of the events if there are 13 distinct campaigns in the data and this value is the most common with 51",
    "start": "1953380",
    "end": "1960790"
  },
  {
    "text": "per point 81% the nice thing here is the",
    "start": "1960790",
    "end": "1966190"
  },
  {
    "text": "the nice thing here is that I can actually go and zoom in on a specific range of time and then all the",
    "start": "1966190",
    "end": "1973780"
  },
  {
    "text": "statistics would be really would be updated for that time frame so right now I can see I can see that there are only",
    "start": "1973780",
    "end": "1980350"
  },
  {
    "text": "11 campaigns in this in this range so that would be the first first that now I",
    "start": "1980350",
    "end": "1986680"
  },
  {
    "text": "have my my data source and I want to create an ETL I want to send the data to",
    "start": "1986680",
    "end": "1992620"
  },
  {
    "text": "Athena so I'm going to click on this button here on the top right called ad output and once I do have a list of",
    "start": "1992620",
    "end": "2000330"
  },
  {
    "text": "output connectors that I can send the data to I'm gonna choose Amazon Athena",
    "start": "2000330",
    "end": "2006440"
  },
  {
    "text": "it's gonna name it and I got to a page",
    "start": "2006440",
    "end": "2015090"
  },
  {
    "text": "that lets me Mack my raw data from the data source to the Athena table so let's",
    "start": "2015090",
    "end": "2021660"
  },
  {
    "text": "see let's just add a few columns you see how exactly it's done so I can choose",
    "start": "2021660",
    "end": "2027090"
  },
  {
    "text": "for example those four fields we had in the impressions select them and those",
    "start": "2027090",
    "end": "2032910"
  },
  {
    "text": "were now added so data dot campaign ID would be mapped to a campaign ID column",
    "start": "2032910",
    "end": "2039240"
  },
  {
    "text": "within Athena I'm gonna click on preview and this is the table that's actually",
    "start": "2039240",
    "end": "2045120"
  },
  {
    "text": "going to will create in Athena you can see it before actually learning the detail but if I'm going to click here on",
    "start": "2045120",
    "end": "2052230"
  },
  {
    "text": "one I'm gonna we're already going to finish the entire the entire process so",
    "start": "2052230",
    "end": "2058070"
  },
  {
    "text": "I think if some Thing Thing is placing myself in the place of the user the",
    "start": "2058070",
    "end": "2064470"
  },
  {
    "text": "first thing they're gonna ask what kind of data transformation can I do enough solver and you have two ways to run your",
    "start": "2064470",
    "end": "2071460"
  },
  {
    "text": "data transformation one is by using the user interface that has a lot of baked-in",
    "start": "2071460",
    "end": "2077358"
  },
  {
    "text": "functions that you can use the other option is to use sequin and I clicked",
    "start": "2077359",
    "end": "2084200"
  },
  {
    "text": "here on this SQL button and you see that for the user in for the mapping that I",
    "start": "2084200",
    "end": "2089628"
  },
  {
    "text": "did with the form view now I have an SQL command and if I'm going to run the SQL command and pretty much I could work",
    "start": "2089629",
    "end": "2096679"
  },
  {
    "text": "with the SQL experience and I can work with the visual experience and go back back and forth between them and each",
    "start": "2096679",
    "end": "2102920"
  },
  {
    "text": "change will be applied to to the other right now what I want to do is paste a",
    "start": "2102920",
    "end": "2109489"
  },
  {
    "text": "sequel that I prepared before this demo and sequel implements the use case of",
    "start": "2109489",
    "end": "2116269"
  },
  {
    "text": "joining the impressions with the clicks with historical user data so since there are 25 lines here let's let's try to",
    "start": "2116269",
    "end": "2123499"
  },
  {
    "text": "break it down into pieces the first piece leaving from here is",
    "start": "2123499",
    "end": "2128599"
  },
  {
    "text": "that I'm just doing a select on top of the impressions data so the impressions stream coming from kinesics is my main",
    "start": "2128599",
    "end": "2136339"
  },
  {
    "text": "source and what I'm doing is I'm going to enrich that source by doing a left out of join once with something called",
    "start": "2136339",
    "end": "2143569"
  },
  {
    "text": "click data and then with another view called user data so well did I got this",
    "start": "2143569",
    "end": "2149809"
  },
  {
    "text": "click data and user data how did I add them into my into my query I declared",
    "start": "2149809",
    "end": "2156769"
  },
  {
    "text": "them here using a we'd syntax which is very common to do SQL and click data is",
    "start": "2156769",
    "end": "2162289"
  },
  {
    "text": "defined as the mapping between the impression ID and the last time there",
    "start": "2162289",
    "end": "2168049"
  },
  {
    "text": "was a click and for the user data I can map the user ID to features like the",
    "start": "2168049",
    "end": "2174769"
  },
  {
    "text": "number of times the user click the data in the last 90 days the last time there was a click the last device the last",
    "start": "2174769",
    "end": "2182630"
  },
  {
    "text": "device ID but the cool part here is the touch so that is actually maintaining a window and like that so how I can bring",
    "start": "2182630",
    "end": "2189589"
  },
  {
    "text": "the last 90 days of user data I could change that 180 days basically I could",
    "start": "2189589",
    "end": "2196069"
  },
  {
    "text": "use any time range time range to bring historical user data the reason that",
    "start": "2196069",
    "end": "2201650"
  },
  {
    "text": "syntax that operation works in observer is that behind the scenes every time you",
    "start": "2201650",
    "end": "2207680"
  },
  {
    "text": "write the syntax absolutely automatically creates an index and when you're doing this left outer join here on the bottom",
    "start": "2207680",
    "end": "2215120"
  },
  {
    "text": "you join in with the index and you're not joining with the actual data honestly and that's how those joint can",
    "start": "2215120",
    "end": "2221810"
  },
  {
    "text": "work in a very high high performance and with let's say obedient keys for a joint",
    "start": "2221810",
    "end": "2227960"
  },
  {
    "text": "would not be an performance issue in in absorber so I hope that part made sense",
    "start": "2227960",
    "end": "2235730"
  },
  {
    "text": "so any transformation in SQL can be done in observer and if that transformation",
    "start": "2235730",
    "end": "2241190"
  },
  {
    "text": "would also be applied to the visual interface so any state for transformation here would would work and",
    "start": "2241190",
    "end": "2247040"
  },
  {
    "text": "now actually go I want to go ahead and run it and see the data in a team so I clicked on run I'm going to choose the",
    "start": "2247040",
    "end": "2254390"
  },
  {
    "text": "buckets I understand distorted data on observer is integrated with my attina account so I'm going to",
    "start": "2254390",
    "end": "2261170"
  },
  {
    "text": "create a table called webinar table and I'm going to click on deploy so the one",
    "start": "2261170",
    "end": "2269720"
  },
  {
    "text": "thing I couldn't really demo is what's happening now behind the scenes so I'm going back to the deck and explaining",
    "start": "2269720",
    "end": "2275750"
  },
  {
    "text": "the process currently running in the background so what observe is ETL is",
    "start": "2275750",
    "end": "2281180"
  },
  {
    "text": "doing actually has two steps the first step is reading the raw data applying",
    "start": "2281180",
    "end": "2286880"
  },
  {
    "text": "all the transformation that you just that we just added in and creating small",
    "start": "2286880",
    "end": "2293510"
  },
  {
    "text": "per cache files on Amazon s3 and creating the table structure in the glue data catalog once this phase is complete",
    "start": "2293510",
    "end": "2301640"
  },
  {
    "text": "it's already possible to use the tape to use a teener to query that data set and",
    "start": "2301640",
    "end": "2306950"
  },
  {
    "text": "the data would be here very fast because this is streaming ETL the challenge here",
    "start": "2306950",
    "end": "2312830"
  },
  {
    "text": "and that's a unique challenge to data ETL is that you need to optimize the file system and we mentioned some of the",
    "start": "2312830",
    "end": "2319880"
  },
  {
    "text": "best practices that you need to follow one of the most important best practices is the size of the files that you came",
    "start": "2319880",
    "end": "2326390"
  },
  {
    "text": "on history the performance you're going to get with small size small files of of",
    "start": "2326390",
    "end": "2332120"
  },
  {
    "text": "parkette will be completely different if you're going to create bigger files that are more optimized for engines like",
    "start": "2332120",
    "end": "2337760"
  },
  {
    "text": "Athena so observer has the another process called compaction and that process takes those small files",
    "start": "2337760",
    "end": "2344090"
  },
  {
    "text": "it combines them together it applies some more best practices that they didn't mention here and that's why",
    "start": "2344090",
    "end": "2350390"
  },
  {
    "text": "the data in the table he's being optimized to improve performance every time you're creating up more optimized",
    "start": "2350390",
    "end": "2357590"
  },
  {
    "text": "files you also need to tell glue because when a tina is growing the data it's actually asking glue where are my files",
    "start": "2357590",
    "end": "2364070"
  },
  {
    "text": "which files should I read I read the small files or should I read the bigger files so observer automatically does",
    "start": "2364070",
    "end": "2370670"
  },
  {
    "text": "that that sync with glue I'm going back",
    "start": "2370670",
    "end": "2375680"
  },
  {
    "text": "to two apps over here there is let's say if if you would want to drill down in what's happening behind your idea so",
    "start": "2375680",
    "end": "2382520"
  },
  {
    "text": "these are the streams that were created the impressions and the clicks that we did not create at the stream that we are",
    "start": "2382520",
    "end": "2388070"
  },
  {
    "text": "reading from those are the indexes that observable implicitly creates do you know how to do the join and in the end",
    "start": "2388070",
    "end": "2394430"
  },
  {
    "text": "there is the output into a Tina if I'm going to move into a Tina right now",
    "start": "2394430",
    "end": "2399650"
  },
  {
    "text": "you'll notice that there isn't a table called webinar table I'm going to refresh my view in a tina and right now",
    "start": "2399650",
    "end": "2408920"
  },
  {
    "text": "there is a new table called webinar TBL and it has the exact same columns that",
    "start": "2408920",
    "end": "2414020"
  },
  {
    "text": "we defined in our sequel if I'm going to preview that table in a second you see",
    "start": "2414020",
    "end": "2422120"
  },
  {
    "text": "that data is already has already been populated into the into the table oh",
    "start": "2422120",
    "end": "2429730"
  },
  {
    "text": "yeah there we go my console that's stuck here for a second so you can see that the query",
    "start": "2433660",
    "end": "2440060"
  },
  {
    "text": "took 2.71 and we just retrieved the first ten columns of that data set now I",
    "start": "2440060",
    "end": "2445520"
  },
  {
    "text": "can start querying on the data set and understand if the features that I prepared for machine learning make any",
    "start": "2445520",
    "end": "2451660"
  },
  {
    "text": "make any sense switch it back to the to the deck there",
    "start": "2451660",
    "end": "2458360"
  },
  {
    "text": "is one more part I want to explain so so far we covered observer regular ETL into",
    "start": "2458360",
    "end": "2464780"
  },
  {
    "text": "a tena including joints including joins and aggregations the other very common",
    "start": "2464780",
    "end": "2470600"
  },
  {
    "text": "use case we see with that you know today is updates and deletes so if you want to",
    "start": "2470600",
    "end": "2475670"
  },
  {
    "text": "replicate an on-premise database into Amazon Athena if you want to enforce",
    "start": "2475670",
    "end": "2480859"
  },
  {
    "text": "compliance in a team you want to be able to do updates and deletes and in this case you're seeing an SQL that sending",
    "start": "2480859",
    "end": "2488569"
  },
  {
    "text": "data to Athena which is sometimes let's say sometimes I'm going to see a new",
    "start": "2488569",
    "end": "2493640"
  },
  {
    "text": "visitor ID so in this case I'm looking at visitors data if I'm going to see if new visitor ID I'm going to do an insert",
    "start": "2493640",
    "end": "2500450"
  },
  {
    "text": "into Athena but if I'm gonna have already seen that visitor ID in the past",
    "start": "2500450",
    "end": "2505480"
  },
  {
    "text": "that I'm going to update the tip so in this case we are counting sorry we are",
    "start": "2505480",
    "end": "2511609"
  },
  {
    "text": "counting the number of pages seen in Athena so far the total number of bounces the total number of cities that",
    "start": "2511609",
    "end": "2518540"
  },
  {
    "text": "user visited from and the way app server does this behind the scene is again the",
    "start": "2518540",
    "end": "2524660"
  },
  {
    "text": "same croc concept of streaming ETL and then compaction but in this case",
    "start": "2524660",
    "end": "2530750"
  },
  {
    "text": "compaction is not only creating bigger files from smaller files it's actually",
    "start": "2530750",
    "end": "2536780"
  },
  {
    "text": "applying those updates and deletes to your historical data so the table that you have in Athena only contains data",
    "start": "2536780",
    "end": "2543890"
  },
  {
    "text": "per key and not the entire original whole data so wearing that table can",
    "start": "2543890",
    "end": "2549380"
  },
  {
    "text": "work much faster and you have to show you exactly what I mean by saying much faster I recreated that table so you can",
    "start": "2549380",
    "end": "2558260"
  },
  {
    "text": "see here that I have a table called webinar TBL updates and I have the visitor count I'm gonna increased I'm",
    "start": "2558260",
    "end": "2567650"
  },
  {
    "text": "gonna zoom in to a full visitor ID the count number of pages the bounces the",
    "start": "2567650",
    "end": "2572839"
  },
  {
    "text": "cities so all that is in my table and when I go ahead and actually queried that table I'm doing the Select star so",
    "start": "2572839",
    "end": "2580069"
  },
  {
    "text": "I want to query all the data all the columns in the table and I and I run it what I see is that query took only 3.93",
    "start": "2580069",
    "end": "2589880"
  },
  {
    "text": "seconds and I only scat 95 and 29 kilobytes of data so I didn't I need to",
    "start": "2589880",
    "end": "2595490"
  },
  {
    "text": "query the entire data I just query data by key and that's the kind of the proof that observer rewrites",
    "start": "2595490",
    "end": "2602119"
  },
  {
    "text": "the data see would it need to scan all of it again and again",
    "start": "2602119",
    "end": "2607420"
  },
  {
    "text": "pretty much the end of over demo we finished both the data set example and",
    "start": "2608650",
    "end": "2614780"
  },
  {
    "text": "the updates and deletes example and what what I want to answer is one very common",
    "start": "2614780",
    "end": "2622040"
  },
  {
    "text": "question we get so how when should I use",
    "start": "2622040",
    "end": "2627200"
  },
  {
    "text": "app server and when should I use spark and when when is one better than the other and I think that spark does a very",
    "start": "2627200",
    "end": "2634880"
  },
  {
    "text": "good job today for data engineering departments but spark is not simple you",
    "start": "2634880",
    "end": "2640970"
  },
  {
    "text": "need to know the ins and outs of spark just like you need to know the ins and outs of Hadoop in order to do data like",
    "start": "2640970",
    "end": "2646849"
  },
  {
    "text": "ETL will spark but without software you don't need to know all that you can just use the SQL you can use user interface",
    "start": "2646849",
    "end": "2653420"
  },
  {
    "text": "and data analysts would not use part but would use a app server to create their",
    "start": "2653420",
    "end": "2658579"
  },
  {
    "text": "data it would be a simpler experience on top of that observer as that indexing",
    "start": "2658579",
    "end": "2664160"
  },
  {
    "text": "engine baked into the platform so doing joins or doing updates and deletes can work and can work very fast and you",
    "start": "2664160",
    "end": "2671630"
  },
  {
    "text": "wouldn't need any additional infrastructure in order to do it you also don't need to know all the best",
    "start": "2671630",
    "end": "2677900"
  },
  {
    "text": "practices so the the query performance you're gonna get from attina using observer is gonna be the best that you",
    "start": "2677900",
    "end": "2684380"
  },
  {
    "text": "can can get just because we are applying all the best practices that AWS recommends with sparked best ideas you",
    "start": "2684380",
    "end": "2691310"
  },
  {
    "text": "would need to implement those best practices on your own and the last difference is that without server",
    "start": "2691310",
    "end": "2697579"
  },
  {
    "text": "everything is streaming so you're always going to get a fresh copy of the data with spark you will need to implement",
    "start": "2697579",
    "end": "2703220"
  },
  {
    "text": "swipe and batch in a separate kind of way so those are the key topics to",
    "start": "2703220",
    "end": "2709130"
  },
  {
    "text": "compare both the platforms actually most of observer customers they are using both our server and a party spark so the",
    "start": "2709130",
    "end": "2716569"
  },
  {
    "text": "two platforms can live side by side and from here we're going to move to the Q&A section and see what kind of questions",
    "start": "2716569",
    "end": "2723800"
  },
  {
    "text": "you might have great Thank You Ori my name is Igor Alexeyev and my partner",
    "start": "2723800",
    "end": "2729680"
  },
  {
    "text": "solution architect at AWS and data and analytics team and I'll be helping out",
    "start": "2729680",
    "end": "2735260"
  },
  {
    "text": "today with questions so it once again save a royal or a great presentation",
    "start": "2735260",
    "end": "2742040"
  },
  {
    "text": "and everybody if you can post your questions into the question window would be great and I already see some",
    "start": "2742040",
    "end": "2747410"
  },
  {
    "text": "questions coming in so first question I think it's for Horry how is absol reprised the observer has a",
    "start": "2747410",
    "end": "2756320"
  },
  {
    "text": "compute based model so you paper a pail hour just like you would with DC - so",
    "start": "2756320",
    "end": "2762770"
  },
  {
    "text": "basically there is a mark up on top of the ec2 price that you would pay and under the hood observer is using spot",
    "start": "2762770",
    "end": "2769850"
  },
  {
    "text": "instances easy to spot instances you pay for the price of the spot instance and you pay for the app server software and",
    "start": "2769850",
    "end": "2776690"
  },
  {
    "text": "of course you only pay for what you actually use so the platform includes auto scaling you can actually see the",
    "start": "2776690",
    "end": "2783620"
  },
  {
    "text": "pricing in the AWS marketplace great thank you very and I think next question",
    "start": "2783620",
    "end": "2789800"
  },
  {
    "text": "is for you again can we use other databases than it than Amazon on AWS can",
    "start": "2789800",
    "end": "2799460"
  },
  {
    "text": "we use other databases then Amazon so I'm not sure if you're meaning if the",
    "start": "2799460",
    "end": "2804500"
  },
  {
    "text": "intention is input databases or output databases so if you want to bring in",
    "start": "2804500",
    "end": "2809780"
  },
  {
    "text": "input databases at the moment we are doing it with by adding VMs AWS data",
    "start": "2809780",
    "end": "2815480"
  },
  {
    "text": "migration service and app server can read the inputs generated by DMS and create tables in Athena and there are",
    "start": "2815480",
    "end": "2822920"
  },
  {
    "text": "also several databases some of there are Amazon servers are not Amazon that you can send the data to as part of your ETL",
    "start": "2822920",
    "end": "2831160"
  },
  {
    "text": "great so it means basically what database is supported by DMS will be supported by app solar awesome next",
    "start": "2831160",
    "end": "2839600"
  },
  {
    "text": "question how is it possible to try up solver so if you go to the AWS",
    "start": "2839600",
    "end": "2846020"
  },
  {
    "text": "marketplace you can start the free trial that's a 14 days trial and doing that",
    "start": "2846020",
    "end": "2851450"
  },
  {
    "text": "trial you get a lot of service from us so we train you on up solver we help you think about your use cases and how it's",
    "start": "2851450",
    "end": "2858500"
  },
  {
    "text": "best to implement them and we make sure that you're getting into something that works on a production scale and then you",
    "start": "2858500",
    "end": "2865700"
  },
  {
    "text": "decide whether I want you also want to buy the platform great thank you next question I think",
    "start": "2865700",
    "end": "2872120"
  },
  {
    "text": "it's still for Yuri I understand that using a thinner and the under this ql we can query it s3 and",
    "start": "2872120",
    "end": "2879860"
  },
  {
    "text": "data sources how would I know which metadata and model to query so when",
    "start": "2879860",
    "end": "2885650"
  },
  {
    "text": "you're writing the ETL so I'm actually gonna switch back to this view and write",
    "start": "2885650",
    "end": "2893240"
  },
  {
    "text": "on silicon sequa so over here I'm selecting several columns the left side",
    "start": "2893240",
    "end": "2899120"
  },
  {
    "text": "is my source field and the right side would be the tape the column in Athena so in this case I'm gonna have column",
    "start": "2899120",
    "end": "2905720"
  },
  {
    "text": "win time inflation ID so by adjusting the SQL that you write your you're",
    "start": "2905720",
    "end": "2910790"
  },
  {
    "text": "controlling the schema that you're going to create in Athena and if I'm going to",
    "start": "2910790",
    "end": "2915860"
  },
  {
    "text": "switch to the forms view you on the side you can also look at the data types and that's that's basically great thank you",
    "start": "2915860",
    "end": "2924650"
  },
  {
    "text": "next question is for your again is it possible to add up solver to AWS billing",
    "start": "2924650",
    "end": "2931000"
  },
  {
    "text": "yes definitely so when you are doing your buying up server from the marketplace you're buying you're",
    "start": "2931000",
    "end": "2938990"
  },
  {
    "text": "actually adding app server to your AWS bill you can pay for observer on demand you can do reserved models you can ask",
    "start": "2938990",
    "end": "2946190"
  },
  {
    "text": "us for private offers and you can pay on a monthly basis on a yearly basis you",
    "start": "2946190",
    "end": "2951230"
  },
  {
    "text": "kind of have all the options you have an AWS observer through the marketplace great thank you next question how is the",
    "start": "2951230",
    "end": "2959960"
  },
  {
    "text": "ETL handled which component and the follow-up question to that where does the ETL comes into the data Lake is it",
    "start": "2959960",
    "end": "2966440"
  },
  {
    "text": "before store into data lake or after storing into data Lake so it's actually",
    "start": "2966440",
    "end": "2971840"
  },
  {
    "text": "both so when observer connects to let's say Kafka its serializes all the data",
    "start": "2971840",
    "end": "2978290"
  },
  {
    "text": "from Kafka into an s3 bucket and then every operation that you're doing is back to s3 even if you are loading to",
    "start": "2978290",
    "end": "2985340"
  },
  {
    "text": "that shift we are first writing the data to s3 by creating an architecture that transforms data from one s3 for the two",
    "start": "2985340",
    "end": "2992540"
  },
  {
    "text": "enough and other s3 folder we can there a guarantee exactly once processing which means you'll never get duplicate",
    "start": "2992540",
    "end": "2999200"
  },
  {
    "text": "data and you'll never get missing data by running enough so very clear great thank you next question for your",
    "start": "2999200",
    "end": "3007390"
  },
  {
    "text": "again what and of customer data does absolve a store so observer that doesn't store any",
    "start": "3007390",
    "end": "3016080"
  },
  {
    "text": "customer data so the way it works is that server stores all the data on s3 that's bucket on s3 is in within the",
    "start": "3016080",
    "end": "3024330"
  },
  {
    "text": "customer account so let's say you you deployed observer into your account and",
    "start": "3024330",
    "end": "3029790"
  },
  {
    "text": "absolutely employee doesn't even have access to the data that you have on this tree the only data sent to the app so of a",
    "start": "3029790",
    "end": "3037320"
  },
  {
    "text": "cloud is billing information and monitoring information since we are supporting your deployment remotely but",
    "start": "3037320",
    "end": "3044970"
  },
  {
    "text": "we don't get any pieces of data into the observer cloud and that was very",
    "start": "3044970",
    "end": "3050130"
  },
  {
    "text": "important for seven for example in Ivan's source and when he went to approve that server with the security",
    "start": "3050130",
    "end": "3056490"
  },
  {
    "text": "and big with the person in charge of the security and bigger yeah this so this is",
    "start": "3056490",
    "end": "3063390"
  },
  {
    "text": "great for achieving all kind of compliance as with when handling PCI and PII right exactly it's like it's always",
    "start": "3063390",
    "end": "3069930"
  },
  {
    "text": "like on premise but on cloud great I",
    "start": "3069930",
    "end": "3075120"
  },
  {
    "text": "think next question you already answered but so how is the data integration achieved what data integration tool is",
    "start": "3075120",
    "end": "3081870"
  },
  {
    "text": "used to do this you already mentioned the data migration service or now if you want to add anything here so I think",
    "start": "3081870",
    "end": "3088140"
  },
  {
    "text": "there asked the question is like our app server is doing the data processing the way absorber can process data at scale",
    "start": "3088140",
    "end": "3095430"
  },
  {
    "text": "like sever from Ireland stones mentioned is by writing the entire platform on",
    "start": "3095430",
    "end": "3100860"
  },
  {
    "text": "Scala and to do a fully decoupled architecture so all the processing is",
    "start": "3100860",
    "end": "3106860"
  },
  {
    "text": "done on s on ec2 not using any local storage and all the data storage is done",
    "start": "3106860",
    "end": "3111960"
  },
  {
    "text": "on s3 and that's how you can really scale and scale the platform and of",
    "start": "3111960",
    "end": "3117000"
  },
  {
    "text": "course if the intention was what kind of data integration this app server has so for example DMS is such an integration",
    "start": "3117000",
    "end": "3124470"
  },
  {
    "text": "and we have some out-of-the-box connectors like after Kinesis data lakes and those kind of things great next",
    "start": "3124470",
    "end": "3133230"
  },
  {
    "text": "question what's included in up solvers connector to athena so first of all the",
    "start": "3133230",
    "end": "3140340"
  },
  {
    "text": "the fact that you can get the data in there with using streaming the second",
    "start": "3140340",
    "end": "3145710"
  },
  {
    "text": "is that you have compaction so all the best practices really all the best practices that way Hasan mentioned in",
    "start": "3145710",
    "end": "3151470"
  },
  {
    "text": "the beginning in in slide are already built into the platform and the ability",
    "start": "3151470",
    "end": "3156540"
  },
  {
    "text": "to do deletes and updates so as always said Athena isn't a sequel engine it's not a database you can do insert update",
    "start": "3156540",
    "end": "3163410"
  },
  {
    "text": "and delete but customers do want to do updates and deletes so observers add that functionality in the ETL layer",
    "start": "3163410",
    "end": "3171290"
  },
  {
    "text": "great thank you and you just mentioned before Kinesis so you have a question",
    "start": "3171290",
    "end": "3176460"
  },
  {
    "text": "about Kinesis can you join multiple",
    "start": "3176460",
    "end": "3181610"
  },
  {
    "text": "Kinesis streams absolutely so you can join any data so everything becomes a",
    "start": "3181610",
    "end": "3188700"
  },
  {
    "text": "data source in observer you can you can join any two data sources together zucchini sis and Kafka Kinesis and",
    "start": "3188700",
    "end": "3194460"
  },
  {
    "text": "Genesis Genesis and s/3 s/3 industry so any combination would work great thank",
    "start": "3194460",
    "end": "3201270"
  },
  {
    "text": "you and next question can you can absolve us work with redshift spectrum a",
    "start": "3201270",
    "end": "3206610"
  },
  {
    "text": "spark over EMR those are two tools but yes so you can sense observer puts all",
    "start": "3206610",
    "end": "3214260"
  },
  {
    "text": "the metadata in the glue data catalog actually once you create a table in the tonight's already automatically created",
    "start": "3214260",
    "end": "3220020"
  },
  {
    "text": "in redshift spectrum and the same idea goes for let's tow over EMR great so",
    "start": "3220020",
    "end": "3226800"
  },
  {
    "text": "basically because you support s3 and glow catalog through those two services you support spectrum and redshift in NMR",
    "start": "3226800",
    "end": "3234810"
  },
  {
    "text": "right exactly in I won't stores by the way they're using the same copy of the data and they",
    "start": "3234810",
    "end": "3241830"
  },
  {
    "text": "are querying the data with Athena with spectrum and with spark using one copy",
    "start": "3241830",
    "end": "3246900"
  },
  {
    "text": "of the data using glue as the metadata store great and speaking of spark how is",
    "start": "3246900",
    "end": "3254400"
  },
  {
    "text": "absolve are different from data breaks so data breaks is a spark spark platform",
    "start": "3254400",
    "end": "3262710"
  },
  {
    "text": "so data breaks you would use a data engineer would be able to use data breaks to implement in ETL but once you",
    "start": "3262710",
    "end": "3269340"
  },
  {
    "text": "want to go to a person that doesn't know all the ins and outs of spark you could not use database it's not",
    "start": "3269340",
    "end": "3274900"
  },
  {
    "text": "would not be a simple platformer that's what reason the ease-of-use side of things other than that observer also as",
    "start": "3274900",
    "end": "3281560"
  },
  {
    "text": "the indexing engine built-in so I would say observer can join multiple sources of data like Genesis and Kinesis the",
    "start": "3281560",
    "end": "3288190"
  },
  {
    "text": "question we had before at the scale which is about 10 to 15 times larger than the other bricks and maybe I'm",
    "start": "3288190",
    "end": "3295510"
  },
  {
    "text": "going to too deep here but the indexing technology that the observer built is is",
    "start": "3295510",
    "end": "3300970"
  },
  {
    "text": "breakthrough technology when it comes to completion and after that can keep compressed data in memory and use it for",
    "start": "3300970",
    "end": "3307480"
  },
  {
    "text": "join while every other and every other vendor that we know is also data beliefs",
    "start": "3307480",
    "end": "3313420"
  },
  {
    "text": "needs to keep the uncompressed data in memory so they absolutely can keep ten to fifteen more term or bigger states in",
    "start": "3313420",
    "end": "3320830"
  },
  {
    "text": "memory great and actually there's a question about indexes and how absorber",
    "start": "3320830",
    "end": "3327280"
  },
  {
    "text": "handles the data does absorber download the download the data and then it works",
    "start": "3327280",
    "end": "3332590"
  },
  {
    "text": "on it locally where is it creating dynamic indexes so those indexes are",
    "start": "3332590",
    "end": "3339550"
  },
  {
    "text": "actually stored again back to s3 so you don't need to store anything locally",
    "start": "3339550",
    "end": "3344560"
  },
  {
    "text": "those the indexes are I'll store the next three and then they are loaded into memory and when you are actually running",
    "start": "3344560",
    "end": "3351670"
  },
  {
    "text": "the ETL the fact that those indexes are so compressed mean that you can just keep a lot of them in memory when you're",
    "start": "3351670",
    "end": "3358210"
  },
  {
    "text": "doing great here so you read from this three processing memory and then write back to history yes exactly",
    "start": "3358210",
    "end": "3365400"
  },
  {
    "text": "excellent next question can you share the comparative analysis I think it's",
    "start": "3365400",
    "end": "3371500"
  },
  {
    "text": "going to be part of the presentation right yes exactly and next question what",
    "start": "3371500",
    "end": "3377440"
  },
  {
    "text": "is the recommendation your recommendation for the hybrid environment what do you mean by hybrid",
    "start": "3377440",
    "end": "3383560"
  },
  {
    "text": "environment so I presume the person means a mixture or an on-prem and cloud",
    "start": "3383560",
    "end": "3392610"
  },
  {
    "text": "so absorber it's clear connectors to connect to CAF courage to HDFS let you",
    "start": "3392610",
    "end": "3398350"
  },
  {
    "text": "put ingest data from on-premise into the cloud so if I would connect to an edge",
    "start": "3398350",
    "end": "3403390"
  },
  {
    "text": "EFS the folder on premise or to a CAF the topic I would bring the data that folder into the cloud and there is",
    "start": "3403390",
    "end": "3409569"
  },
  {
    "text": "actually a public case tidy describing a customer that thought that on the AWS big data block if your source is not a",
    "start": "3409569",
    "end": "3417339"
  },
  {
    "text": "storage the first we will probably asked you to to ingest the data on your own to",
    "start": "3417339",
    "end": "3423010"
  },
  {
    "text": "astray and after we would pick it up from there great yeah and I will tell",
    "start": "3423010",
    "end": "3428319"
  },
  {
    "text": "you w as typically you know ingesting data into a three is not expensive it's",
    "start": "3428319",
    "end": "3433480"
  },
  {
    "text": "one taken out so that's something to remember all right I think next question is actually for",
    "start": "3433480",
    "end": "3440440"
  },
  {
    "text": "sever what is the total size of the data that you had in the presentation in your",
    "start": "3440440",
    "end": "3445630"
  },
  {
    "text": "use case five point three and abides while we're receiving about four petabytes a month",
    "start": "3445630",
    "end": "3453359"
  },
  {
    "text": "great great thank you thank you Sarah and I think I think we handled this",
    "start": "3453359",
    "end": "3458859"
  },
  {
    "text": "question already how do I get started with up solver I think you already answered that rate yes all right and",
    "start": "3458859",
    "end": "3468460"
  },
  {
    "text": "next question is I think it's for Ori how do I get sorry what is the",
    "start": "3468460",
    "end": "3473740"
  },
  {
    "text": "difference between AWS glue in observable observer I think the comparison is again regarding so glue",
    "start": "3473740",
    "end": "3481029"
  },
  {
    "text": "would be server the spark so if I would want to implement my ETS using spark I",
    "start": "3481029",
    "end": "3486519"
  },
  {
    "text": "would use glue and if I want like a visual self-service kind of way to do it",
    "start": "3486519",
    "end": "3492099"
  },
  {
    "text": "I would probably use the absorber absolutely would provide an easier experience but it would not be smart",
    "start": "3492099",
    "end": "3498130"
  },
  {
    "text": "based I glue great thank you and then next question I think it's",
    "start": "3498130",
    "end": "3503440"
  },
  {
    "text": "still Ferrari can you talk about the data retention on your stream in pipelines yes definitely so every object",
    "start": "3503440",
    "end": "3511839"
  },
  {
    "text": "in observer whether it's a data source or an output like the ones we created you can go to properties and define",
    "start": "3511839",
    "end": "3518650"
  },
  {
    "text": "retention and then observer would automatically delete data after that retention the nice thing about giving up",
    "start": "3518650",
    "end": "3526329"
  },
  {
    "text": "server the ability to manage retention is it can actually delete it can make sure that it's ok to delete file",
    "start": "3526329",
    "end": "3532329"
  },
  {
    "text": "and there isn't some kind of Ethier job which is dependent on that file so it kind of which would be a smarter",
    "start": "3532329",
    "end": "3538299"
  },
  {
    "text": "deletion process comparing to delete the entire phone by date great thank you and I think the",
    "start": "3538299",
    "end": "3545529"
  },
  {
    "text": "next question was about up solver and as3 comparing two data breaks and Delta like on s3 yes so in the context of data",
    "start": "3545529",
    "end": "3553869"
  },
  {
    "text": "like so both Delta ik and the update and delete solution in observer I showed today try to add updates and deletes",
    "start": "3553869",
    "end": "3560709"
  },
  {
    "text": "into the into the lake I think the difference is that the Delta ik is a new",
    "start": "3560709",
    "end": "3566319"
  },
  {
    "text": "database API on top of the leg so for data rec you can do insert update and",
    "start": "3566319",
    "end": "3571359"
  },
  {
    "text": "delete and then query Delta but then you would need to integrate a teener into",
    "start": "3571359",
    "end": "3577019"
  },
  {
    "text": "into Delta and you would need to change your ingest ingestion to do insert",
    "start": "3577019",
    "end": "3582249"
  },
  {
    "text": "updates and deletes and it kind of creates a lock in just like just like a data warehouse so I would compare data",
    "start": "3582249",
    "end": "3588069"
  },
  {
    "text": "Lake to a kind of a data warehouse solution although the data is stored on a data Lake without solver you don't",
    "start": "3588069",
    "end": "3595329"
  },
  {
    "text": "need to make any of those changes and since we are only doing manipulations in clue we are not creating any custom",
    "start": "3595329",
    "end": "3601839"
  },
  {
    "text": "process then you could query the data from Athena from Specter you don't need",
    "start": "3601839",
    "end": "3606969"
  },
  {
    "text": "any special observer Athena integration for that I think that when we mentioned",
    "start": "3606969",
    "end": "3612849"
  },
  {
    "text": "the benefits of observer in his presentation he talked about data like a",
    "start": "3612849",
    "end": "3617890"
  },
  {
    "text": "gene and about absorber keeping the whole is the best practices of what's",
    "start": "3617890",
    "end": "3623319"
  },
  {
    "text": "creating a locking and what's not creating a locking in the observer solution doesn't create a locking for",
    "start": "3623319",
    "end": "3629319"
  },
  {
    "text": "updates and deletes great thank you next question for again house how do you",
    "start": "3629319",
    "end": "3636160"
  },
  {
    "text": "handle data privacy issues for HIPAA G GPR etc with data masking and encryption",
    "start": "3636160",
    "end": "3643079"
  },
  {
    "text": "so so the the the best answer I have is the observer is like on premise on cloud",
    "start": "3643079",
    "end": "3649509"
  },
  {
    "text": "so on Plumas is considered HIPAA compliant so observer employees would",
    "start": "3649509",
    "end": "3654819"
  },
  {
    "text": "not have access to the data so in such a case for a customer that needs HIPAA compliance absolutely would it be",
    "start": "3654819",
    "end": "3661420"
  },
  {
    "text": "deployed within their cloud account so no data could leave that that account",
    "start": "3661420",
    "end": "3666699"
  },
  {
    "text": "you could also implement things like masking as part of your ETL that's a",
    "start": "3666699",
    "end": "3672699"
  },
  {
    "text": "different different part of that of the same question great",
    "start": "3672699",
    "end": "3678060"
  },
  {
    "text": "thank you next question is for again how is missing data handled for example how",
    "start": "3678060",
    "end": "3684000"
  },
  {
    "text": "does absolute determination is random so",
    "start": "3684000",
    "end": "3689940"
  },
  {
    "text": "the think the first part which you you have to try to believe it is there is no",
    "start": "3689940",
    "end": "3695010"
  },
  {
    "text": "missing data so since observers architect architecture is s 3 to s 3 we are",
    "start": "3695010",
    "end": "3700859"
  },
  {
    "text": "guaranteeing exactly once and you wouldn't get duplicate or missing or",
    "start": "3700859",
    "end": "3706230"
  },
  {
    "text": "missing data with your with your ETL you can always go and check both sides so",
    "start": "3706230",
    "end": "3712440"
  },
  {
    "text": "query the number of events in CAFTA and on the other side try to use for example we once used Athena to make sure to",
    "start": "3712440",
    "end": "3719940"
  },
  {
    "text": "prove the data floating in in a complete",
    "start": "3719940",
    "end": "3725099"
  },
  {
    "text": "way into Athena but it's not a problem we see with customers data is complete great thank you",
    "start": "3725099",
    "end": "3732180"
  },
  {
    "text": "next question can Claude watch data be fed to Athena what kind of compression is done before being able to feed the",
    "start": "3732180",
    "end": "3738960"
  },
  {
    "text": "data to Athena in as its price in the pays depends on the size of ingested",
    "start": "3738960",
    "end": "3744210"
  },
  {
    "text": "data so right now observer doesn't have a connector to be in the cloud watch",
    "start": "3744210",
    "end": "3749250"
  },
  {
    "text": "data into Athena you could do it manually by dumping the data on s3 and",
    "start": "3749250",
    "end": "3754500"
  },
  {
    "text": "so in pricing it's not really effect if this from the app server side since you",
    "start": "3754500",
    "end": "3760500"
  },
  {
    "text": "are paying per compute so it's really kind of the volume of the volume of data",
    "start": "3760500",
    "end": "3766070"
  },
  {
    "text": "one thing that I didn't say before and maybe is important to say is that absolute sends all monitoring metrics to",
    "start": "3766070",
    "end": "3773130"
  },
  {
    "text": "cloud watch the cloud watch is one of the observer targets but I see that these questions in the context of an input to ops are great and I think we're",
    "start": "3773130",
    "end": "3781020"
  },
  {
    "text": "running out of time so this is going to be the last question does observer support batch loads or only streaming so",
    "start": "3781020",
    "end": "3788849"
  },
  {
    "text": "observer so it supports batch loads but the way it works is that it's it's doing the implementation in streaming so for",
    "start": "3788849",
    "end": "3795720"
  },
  {
    "text": "example server said that they have aggregated outputs so for example you want to output data once an hour and",
    "start": "3795720",
    "end": "3802589"
  },
  {
    "text": "then query that aggregated data to reduce cost and query latency so",
    "start": "3802589",
    "end": "3808920"
  },
  {
    "text": "observer can do batch operation but under the hood it's using streaming it's processing any of every event at a time",
    "start": "3808920",
    "end": "3815940"
  },
  {
    "text": "and using indexing to to join with historical data great thank you very",
    "start": "3815940",
    "end": "3821220"
  },
  {
    "text": "much Orion sever and ROI I think at this point we're going to conclude the webinar if you have any next steps",
    "start": "3821220",
    "end": "3828150"
  },
  {
    "text": "you've seen the previous slides you can take that on those links the deck will be shared with you thank everyone for",
    "start": "3828150",
    "end": "3833730"
  },
  {
    "text": "joining thank you everyone by way",
    "start": "3833730",
    "end": "3838910"
  }
]