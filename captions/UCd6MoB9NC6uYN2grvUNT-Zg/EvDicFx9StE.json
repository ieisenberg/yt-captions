[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "text": "Hey look everyone I'm a greg koala so I lead our business development team they",
    "start": "50",
    "end": "6330"
  },
  {
    "text": "WS for our database analytic services and joining me today is Ryan and Elliot",
    "start": "6330",
    "end": "11700"
  },
  {
    "text": "from Equinox Fitness so in terms of the",
    "start": "11700",
    "end": "16800"
  },
  {
    "text": "agenda those of you that might be new to Amazon redshift I just have a quick like introduction up front and then I'll go",
    "start": "16800",
    "end": "24180"
  },
  {
    "text": "through some recent updates those you get a little more experienced in terms of redshift and then you know majority",
    "start": "24180",
    "end": "29400"
  },
  {
    "text": "of the times I'll play I take like 15 minutes or so the majority of time will be your equinox talk about their case study and",
    "start": "29400",
    "end": "35040"
  },
  {
    "text": "some of the lessons learned that they had in terms of their migration to a cloud data warehouse and I will this",
    "start": "35040",
    "end": "40770"
  },
  {
    "text": "call out as a as an advertisement if you're sticking around till five I have a whole hour at five o'clock to go into",
    "start": "40770",
    "end": "45930"
  },
  {
    "text": "more details and redshift so if you're interested you can see me at five o'clock as well so I think like when we",
    "start": "45930",
    "end": "53730"
  },
  {
    "start": "53000",
    "end": "53000"
  },
  {
    "text": "think about analytics and kind of where it sits in terms of overall AWS",
    "start": "53730",
    "end": "58739"
  },
  {
    "text": "portfolio surely be lots of you know sessions later and throughout the day or you know follow us we can do but the",
    "start": "58739",
    "end": "65700"
  },
  {
    "text": "point is they're gonna think about we have this broad breadth of services in",
    "start": "65700",
    "end": "70890"
  },
  {
    "text": "terms of ingesting data storing it and analyzing it and so whether you have",
    "start": "70890",
    "end": "75930"
  },
  {
    "text": "streaming data or clickstream data or you know ingesting from Internet of Things devices or just transactional",
    "start": "75930",
    "end": "82680"
  },
  {
    "text": "systems you know that's this idea that you know moving away from a single monolithic architecture where you just",
    "start": "82680",
    "end": "89070"
  },
  {
    "text": "had you know a data warehouse and that was it or maybe a Hadoop cluster - you know decoupling compute and storage",
    "start": "89070",
    "end": "95119"
  },
  {
    "text": "thinking about what's the right web service to match the use case and having a rich set a collection of components to",
    "start": "95119",
    "end": "102210"
  },
  {
    "text": "pull from provides a lot of choice that can helpfully match in terms of cost and performance and really innovation right",
    "start": "102210",
    "end": "108509"
  },
  {
    "text": "and today you know we're going to talk a little bit in depth on Amazon redshift but if there's questions you know other",
    "start": "108509",
    "end": "114659"
  },
  {
    "text": "it about services I'm happy to take them you know off in the hallway afterwards as well but I think like if you're new",
    "start": "114659",
    "end": "120659"
  },
  {
    "text": "to redshift I'm just curious like how many folks in the audience used redshift today show of hands so maybe hopefully half or so so that",
    "start": "120659",
    "end": "128369"
  },
  {
    "text": "might be new so Amazon redshift was the first cloud native data warehouse we",
    "start": "128369",
    "end": "133800"
  },
  {
    "text": "launched back in 2012 and really we took the idea that you know for you know",
    "start": "133800",
    "end": "139170"
  },
  {
    "text": "analytics it turns out that sequel is still pretty popular language right and visualizing it through bi tools and so",
    "start": "139170",
    "end": "146010"
  },
  {
    "text": "we want to be able to deliver you know really fast performance give you that",
    "start": "146010",
    "end": "151200"
  },
  {
    "text": "ability to you know not have to pay the upfront costs you know in a new pricing area so if you're familiar with the data",
    "start": "151200",
    "end": "158760"
  },
  {
    "text": "warehouse space I mean it's not unusual to have tens of thousands if not forty thousands of dollars per terabyte per",
    "start": "158760",
    "end": "165180"
  },
  {
    "text": "year and so with redshift you know it can be as low as $1,000 per terabyte per year and so I would argue if you kind of",
    "start": "165180",
    "end": "171630"
  },
  {
    "text": "look at some of the the analysts impressed and things like that that really this this you know Amazon",
    "start": "171630",
    "end": "177330"
  },
  {
    "text": "redshift was the first to really bring the performance and scalability of an MPP or massively parallel processing",
    "start": "177330",
    "end": "184320"
  },
  {
    "text": "data warehouse at a price point which really made it advantageous for you know anyone to adopt right and the fact that",
    "start": "184320",
    "end": "191760"
  },
  {
    "text": "you can start small so you don't have to pre provision ahead of time you can simply say I need you know a couple",
    "start": "191760",
    "end": "196890"
  },
  {
    "text": "hundred gigabytes of data but then you know go all the way up to petabytes of data right and so we have customers that",
    "start": "196890",
    "end": "202290"
  },
  {
    "text": "you know will maybe start a new project with redshift for analytics right and",
    "start": "202290",
    "end": "207540"
  },
  {
    "text": "then figure out well I've got additional data additional data and generally we think that you know average customer you",
    "start": "207540",
    "end": "213239"
  },
  {
    "text": "know roughly doubles the right ship usage per year because they keep collecting more data and finding new use",
    "start": "213239",
    "end": "218700"
  },
  {
    "text": "cases and this is sort of like net new projects but over time we saw that customers started saying well I've got",
    "start": "218700",
    "end": "223980"
  },
  {
    "text": "these you know data warehouse appliances from all the kind of energy I'm sure we could could name and you know there was",
    "start": "223980",
    "end": "231270"
  },
  {
    "text": "nothing unusual about the workload that couldn't fit into redshift and so thinking about the cost and flexibility and so moving this trend from you know",
    "start": "231270",
    "end": "239070"
  },
  {
    "text": "new use cases to actually doing migrations has become I think the the latest trend that we've seen over the last last few years and then you know I",
    "start": "239070",
    "end": "246180"
  },
  {
    "text": "think in terms of security I think it's really an important piece because you know in the analytic space I mean",
    "start": "246180",
    "end": "251760"
  },
  {
    "text": "thinking about your customer data you're you know which can be PCI data could be HIPAA compliant data that you know that",
    "start": "251760",
    "end": "259709"
  },
  {
    "text": "needs to be secured and encrypted and access control and so I think this the security piece I think is oftentimes you",
    "start": "259709",
    "end": "265680"
  },
  {
    "text": "know a key reason that customers look to redshift to be one of their first workloads in AWS because",
    "start": "265680",
    "end": "270960"
  },
  {
    "text": "they can store all their data and whether it's you know health care life sciences financial services US",
    "start": "270960",
    "end": "276449"
  },
  {
    "text": "government agencies government agencies across the globe have really been embracing redshift and I think you know",
    "start": "276449",
    "end": "282900"
  },
  {
    "text": "the access control security and the compliance certifications that redshift",
    "start": "282900",
    "end": "288240"
  },
  {
    "text": "can deliver has oftentimes been a deciding factor when when looking at a cloud data warehouse in the other part",
    "start": "288240",
    "end": "295470"
  },
  {
    "start": "295000",
    "end": "295000"
  },
  {
    "text": "in terms of some of my introduction you know so you know thinking about the history of data warehouses you know",
    "start": "295470",
    "end": "300900"
  },
  {
    "text": "typically you'd have some kind of local disk and then you'd wanted to load the data into the warehouse and have to go through this extract transform and load",
    "start": "300900",
    "end": "307620"
  },
  {
    "text": "process and you know on the other side there was a trend toward you might call it a data Lake right where you know",
    "start": "307620",
    "end": "314669"
  },
  {
    "text": "primarily started off with HDFS in terms of on-premise Hadoop clusters and",
    "start": "314669",
    "end": "319919"
  },
  {
    "text": "offered like low cost scalability and be able to do more schema on read where I",
    "start": "319919",
    "end": "325680"
  },
  {
    "text": "don't have to transform and load the data into a database but I can just read the file as it is on HDFS for instance",
    "start": "325680",
    "end": "332310"
  },
  {
    "text": "right and in the past number of years we've seen an evolution where you know to be honest you know at least the",
    "start": "332310",
    "end": "338190"
  },
  {
    "text": "customers I talked to you know don't really use HDFS anymore right you would just look at s3 as that store for data",
    "start": "338190",
    "end": "345150"
  },
  {
    "text": "right whether it's clickstream data log data and this idea of building your data like on s3 has become a really popular",
    "start": "345150",
    "end": "352919"
  },
  {
    "text": "for customers like FINRA for instance and financial services to essentially load up to 80 billion transactions per",
    "start": "352919",
    "end": "360419"
  },
  {
    "text": "day into s3 directly right for their market surveillance analytics and so if",
    "start": "360419",
    "end": "366210"
  },
  {
    "text": "I think about this merging of a data warehouse where I have you know predefined schemas I have you know",
    "start": "366210",
    "end": "372120"
  },
  {
    "text": "transforming and loading data into the data warehouse but you had this idea of having a data lake on s3 you know what",
    "start": "372120",
    "end": "378240"
  },
  {
    "text": "the trend we see is merging of these two two points right and so now with redshift and a feature we call spectrum",
    "start": "378240",
    "end": "384539"
  },
  {
    "text": "you can actually query data on s3 directly without having to load it into",
    "start": "384539",
    "end": "389699"
  },
  {
    "text": "your data warehouse and so we create an external table in redshift where the data is not local to redshift but the",
    "start": "389699",
    "end": "396479"
  },
  {
    "text": "data sits on s3 and the data is not in a proprietary form like a redshift block format it sits in",
    "start": "396479",
    "end": "403050"
  },
  {
    "text": "CSV or JSON or CSV or sorry or Avro park' or RC some of the columnar formats",
    "start": "403050",
    "end": "411270"
  },
  {
    "text": "so that not only can i you know be i-team query it through standard sequel but my data science team can use spark",
    "start": "411270",
    "end": "418080"
  },
  {
    "text": "or the Hadoop stack or another process to access the same data and so having the ability to have you know the",
    "start": "418080",
    "end": "424560"
  },
  {
    "text": "spectrum layer that seamlessly scales without having to pre allocate the compute size becomes really interesting",
    "start": "424560",
    "end": "429960"
  },
  {
    "text": "as we think about this combination of the data Lake and data warehousing and not having to compromise in between but",
    "start": "429960",
    "end": "436320"
  },
  {
    "text": "being able to use the best of best of both and really I think that it's been this kind of approach in both",
    "start": "436320",
    "end": "442590"
  },
  {
    "text": "performance the cost and ease of use has been you know been seen a lot of tremendous adoption of redshift over the",
    "start": "442590",
    "end": "449490"
  },
  {
    "text": "years here's just a you know a standard you know some of the names that that currently our customers are redshift you",
    "start": "449490",
    "end": "456570"
  },
  {
    "text": "know I think in terms of my perspective I've been with AWS almost five years and I think early on you know many of my",
    "start": "456570",
    "end": "462600"
  },
  {
    "text": "customers that saw it on on redshift kind of came from born the cloud or startups because they really you know",
    "start": "462600",
    "end": "468210"
  },
  {
    "text": "needed this powerful analytics capability but didn't necessarily have the the staff to roll out of data",
    "start": "468210",
    "end": "473400"
  },
  {
    "text": "warehouse right and so redshift became really appealing but now I think you know many times my conversations around",
    "start": "473400",
    "end": "479450"
  },
  {
    "text": "enterprise customers might call them you know fortune 500 you know lights of ng DoCoMo in Japan in terms of",
    "start": "479450",
    "end": "485310"
  },
  {
    "text": "telecommunications that you know need the cost savings and flexibility of a data warehouse but choose to use",
    "start": "485310",
    "end": "490370"
  },
  {
    "text": "redshift to be able to deliver that in the cloud and you know depending upon how much weight you give to kind of",
    "start": "490370",
    "end": "496680"
  },
  {
    "text": "analyst reports and I'm sure that you know there's lots of different analysts a lots of different things but you know",
    "start": "496680",
    "end": "501780"
  },
  {
    "text": "whether it's for store or Gartner you know we don't necessarily break out you know redshift earnings and customer base",
    "start": "501780",
    "end": "508260"
  },
  {
    "text": "separately but you know some of the analysts like for store do some estimation by talking to their customers",
    "start": "508260",
    "end": "513630"
  },
  {
    "text": "and you know some of the data points from foresters report is you know right shift actually has the largest cloud",
    "start": "513630",
    "end": "519930"
  },
  {
    "text": "adoption for data warehouse right and across the globe and you know the number that Forrester says is around 5,000",
    "start": "519930",
    "end": "527190"
  },
  {
    "text": "production customers using redshift across the world right and then not only that but some of the biggest pie",
    "start": "527190",
    "end": "534089"
  },
  {
    "text": "are the biggest cloud data warehouses run redshift so I use this example of DoCoMo earlier so ng DoCoMo has almost",
    "start": "534089",
    "end": "541499"
  },
  {
    "text": "16 petabytes of call detail records into a single redshift cluster in Japan right",
    "start": "541499",
    "end": "547589"
  },
  {
    "text": "and some you know rankings in terms of how we kind of look at redshift features",
    "start": "547589",
    "end": "553019"
  },
  {
    "text": "and functionality compared to other players in the market and you know interesting enough one of the great",
    "start": "553019",
    "end": "558089"
  },
  {
    "text": "benefits of redshift as well is this the number of partners that we that we enable as well so you know oftentimes",
    "start": "558089",
    "end": "563459"
  },
  {
    "text": "you know thinking about the cost savings and flexibility and having a cloud native data warehouse you know you still",
    "start": "563459",
    "end": "569279"
  },
  {
    "text": "go through some selection and if redshift makes sense you know you don't have to change necessarily the end-user",
    "start": "569279",
    "end": "574680"
  },
  {
    "text": "interaction so if you're using tableau or MicroStrategy or your VI tool I mean",
    "start": "574680",
    "end": "579689"
  },
  {
    "text": "right shift supports you know all of those you know obviously it has open standards the ODBC and JDBC but knowing",
    "start": "579689",
    "end": "586199"
  },
  {
    "text": "that your existing bi reports you know you might have to change the connection from Oracle or whatever to redshift but",
    "start": "586199",
    "end": "591720"
  },
  {
    "text": "they should work right same on the data integration side now it's not always as easy as just changing a connection",
    "start": "591720",
    "end": "597149"
  },
  {
    "text": "driver but tools like informatica have native support for redshift and so if your ETL logic is written and you want",
    "start": "597149",
    "end": "603720"
  },
  {
    "text": "to lever leverage a lot of that you know it could be as simple as you know changing the endpoint to redshift but",
    "start": "603720",
    "end": "609269"
  },
  {
    "text": "you know there's always some details to work through in terms of you know being able to get that optimized as well and",
    "start": "609269",
    "end": "614999"
  },
  {
    "text": "then in terms of you know help in terms of system integrators right shift has a large and vibrant consulting partner",
    "start": "614999",
    "end": "621329"
  },
  {
    "text": "ecosystem that can help you because you know in terms of migrations you know oftentimes the the easy part is saying",
    "start": "621329",
    "end": "627360"
  },
  {
    "text": "hey move my data into redshift the hard part becomes you know managing the business process and the change and you",
    "start": "627360",
    "end": "632819"
  },
  {
    "text": "know moving from one database the other does require so in some cases some help to be able to do that and that's kind of",
    "start": "632819",
    "end": "637949"
  },
  {
    "text": "the role that our system integrator partners can help play so that's kind of like my quick recap of redshift I just",
    "start": "637949",
    "end": "643410"
  },
  {
    "text": "want to go through a few new features if you're not aware the things that are currently launched in redshift be sometimes given the pace of change in",
    "start": "643410",
    "end": "649350"
  },
  {
    "text": "AWS some of the new features might get lost and again I'll go pretty quickly through this and I have a whole session",
    "start": "649350",
    "end": "654660"
  },
  {
    "text": "later this afternoon to go in a little bit more detail but the first one I would just say for those that make sense to have the high-performance nvme SSDs",
    "start": "654660",
    "end": "661980"
  },
  {
    "text": "we launched the densest compute to line last November at the same price as the",
    "start": "661980",
    "end": "667350"
  },
  {
    "text": "previous generation so this is you know as you can imagine it's the latest Intel processors using super fast and vme SSDs",
    "start": "667350",
    "end": "674290"
  },
  {
    "text": "and you know the new announcement here I think I would just call out that two weeks ago we launched in the console",
    "start": "674290",
    "end": "680170"
  },
  {
    "text": "that if you're on our older generation DC ones just you know you'll be able to",
    "start": "680170",
    "end": "685510"
  },
  {
    "text": "you know continue your reserved instance for DC ones but move to DC twos right without having to open up support ticket",
    "start": "685510",
    "end": "691959"
  },
  {
    "text": "or talking to anyone you can just you know whatever you're up front payment you made or whatever reserved instance plan you're on you'll be able to just",
    "start": "691959",
    "end": "698380"
  },
  {
    "text": "move seamlessly to DC twos and definitely I recommend although DC ones are still in the console like like start",
    "start": "698380",
    "end": "704290"
  },
  {
    "text": "with DC to is like DC ones as you can imagine or older generation and you know new workloads today and if you have an",
    "start": "704290",
    "end": "709930"
  },
  {
    "text": "existing cluster on DC one we'd like to talk to you about how you can move to the DC two and then in terms of you know",
    "start": "709930",
    "end": "716410"
  },
  {
    "text": "our direction for redshift I would just say that you know we are fully committed we meaning AWS fully committed to making",
    "start": "716410",
    "end": "723990"
  },
  {
    "text": "Amazon redshift clearly the best cloud data warehouse in the market and we have",
    "start": "723990",
    "end": "729700"
  },
  {
    "text": "a number of enhancements I'll come on no talk on here but the number of you know things that are coming in the coming months and essentially our efforts",
    "start": "729700",
    "end": "736330"
  },
  {
    "text": "around performance ease-of-use and integration with the data Lake right so here's a few quotes from customers about",
    "start": "736330",
    "end": "741640"
  },
  {
    "text": "you know their experience in terms of performance whether it's Liberty Mutual or DoCoMo in terms of the performance",
    "start": "741640",
    "end": "746800"
  },
  {
    "text": "gains that they've seen from the legacy systems but in terms of new features one thing I would call out is that you know",
    "start": "746800",
    "end": "752920"
  },
  {
    "text": "redshift has this workload management right where I can set up queues and say okay I've got a set of dashboard users",
    "start": "752920",
    "end": "758860"
  },
  {
    "text": "bi users and ETL processes for instance and you want to have different resources at different times but we launched",
    "start": "758860",
    "end": "765760"
  },
  {
    "text": "something called short query acceleration where you know if I have these kind of you know bi users that",
    "start": "765760",
    "end": "771310"
  },
  {
    "text": "expect you know dashboards reports to turn in two or three seconds as opposed to minutes for maybe analytical queries",
    "start": "771310",
    "end": "776920"
  },
  {
    "text": "I can just bypass my my workload management settings and and make the the",
    "start": "776920",
    "end": "783010"
  },
  {
    "text": "optimized or determine hey this is the shorter query and just automatically run that faster right and so you know this",
    "start": "783010",
    "end": "789790"
  },
  {
    "text": "idea of being able to deliver more queries per second or more queries per hour and not having to you know go into",
    "start": "789790",
    "end": "796209"
  },
  {
    "text": "redshift itself but be able to have machine learning determine this has become really beneficial",
    "start": "796209",
    "end": "801490"
  },
  {
    "text": "many customers that can get this kind of bi or analytic you know the BI users expect sub-second response time you know",
    "start": "801490",
    "end": "807370"
  },
  {
    "text": "to automatically be able to take advantage of short query acceleration so today you have to enable it in the",
    "start": "807370",
    "end": "812860"
  },
  {
    "text": "console itself and then and then coming weeks we're actually going to automatically enable this for your cluster where you can just take",
    "start": "812860",
    "end": "818110"
  },
  {
    "text": "advantage of this machine learning algorithm to help you know define the workload management settings and then",
    "start": "818110",
    "end": "823990"
  },
  {
    "start": "822000",
    "end": "822000"
  },
  {
    "text": "the next one I just talked about is resultset caching so again kind of on this bi user or maybe someone using a",
    "start": "823990",
    "end": "829450"
  },
  {
    "text": "tableau dashboard if you can you know it's no secret that if I can pull a results set from cache right in memory",
    "start": "829450",
    "end": "835570"
  },
  {
    "text": "cache I don't have to you know go back to disk or to s3 to read it and so being able to have that results that cache in",
    "start": "835570",
    "end": "841899"
  },
  {
    "text": "some cases had you know really tremendous impact on on customers dashboard performance for those know you",
    "start": "841899",
    "end": "847630"
  },
  {
    "text": "know the query has to be eligible for caching you can go through some details of how that's determined but you know having the results set in the cache and",
    "start": "847630",
    "end": "854140"
  },
  {
    "text": "having redshift just maintain that for you you know can have you know in some cases you know 5x performance increase",
    "start": "854140",
    "end": "860350"
  },
  {
    "text": "10x right queries can go from 10 seconds to less than a second so giving that you",
    "start": "860350",
    "end": "865570"
  },
  {
    "text": "know the boost that in-memory cache can provide and it's just part of redshift that you know again is included in in",
    "start": "865570",
    "end": "870880"
  },
  {
    "text": "some of our latest updates and then in terms of commit time improvements and so you know the funny thing about a",
    "start": "870880",
    "end": "877570"
  },
  {
    "text": "database is you also want to update it while you're reading it right and so now if you notice you don't really have the",
    "start": "877570",
    "end": "882940"
  },
  {
    "text": "Y access defined but you know just kind of speaking to come of our improvements you know from you know we like to look",
    "start": "882940",
    "end": "889089"
  },
  {
    "text": "at our customer data and feedback and you know moving to this like real-time data warehouse or having more continuous",
    "start": "889089",
    "end": "894550"
  },
  {
    "text": "updates and you know here's just some benchmarks that we looked at across fleet-wide I mean of course you know your workload might depend upon how",
    "start": "894550",
    "end": "900850"
  },
  {
    "text": "you're using redshift but we continue to make improvements in terms of making commits faster right so typically in a",
    "start": "900850",
    "end": "908200"
  },
  {
    "text": "columnar database we want to like batch commits right but you know we also want to give you that that ease of use just",
    "start": "908200",
    "end": "914500"
  },
  {
    "text": "to be able to use your continuous ETL processes into redshift and we don't really have this kind of goes out to",
    "start": "914500",
    "end": "920529"
  },
  {
    "text": "Mars ship but if I look at the fleet metrics and we debated adding in like some August but we're gonna hold off till reinvent it can honey continue this",
    "start": "920529",
    "end": "926860"
  },
  {
    "text": "and you know roughly you'll see about another 50 percent improvement in the coming weeks by some updates that we're",
    "start": "926860",
    "end": "932680"
  },
  {
    "text": "putting and I would just copy out you know your performance maybe different but we're looking at fleet-wide metrics but happy to go into",
    "start": "932680",
    "end": "937779"
  },
  {
    "text": "details of your cluster as well and then on query performance improvements and",
    "start": "937779",
    "end": "942970"
  },
  {
    "start": "940000",
    "end": "940000"
  },
  {
    "text": "particularly in terms of hash joins turns out to be a majority of joins across data sets and redshift when we",
    "start": "942970",
    "end": "948640"
  },
  {
    "text": "look across the fleet you know previously in redshift we were out we determined you know from our",
    "start": "948640",
    "end": "953709"
  },
  {
    "text": "analysis if you're over allocating memory for hash joins and so we've done some engineering work to make sure that",
    "start": "953709",
    "end": "959770"
  },
  {
    "text": "we just allocate the minimum amount required to be able to essentially make the joints faster right and whether in",
    "start": "959770",
    "end": "966610"
  },
  {
    "text": "your workloads or looking at some of the industry benchmarks like TPC H or D s you know we've seen significant",
    "start": "966610",
    "end": "972580"
  },
  {
    "text": "improvement in terms of those industry benchmarks but returns to joining data sets together and again this is an area",
    "start": "972580",
    "end": "979180"
  },
  {
    "text": "that we continue to focus on and then being able to have you know this workload management set up but if I'm",
    "start": "979180",
    "end": "985450"
  },
  {
    "text": "gonna do a large right into that cluster you know I don't want to be bottlenecked into the workload management so I can",
    "start": "985450",
    "end": "991450"
  },
  {
    "text": "actually skip the queues automatically so if it's going to be you know a large updates gonna happen I don't want to",
    "start": "991450",
    "end": "997270"
  },
  {
    "text": "impact my production reporting and so we can automatically put that in a lower priority queue in terms of some of the",
    "start": "997270",
    "end": "1002640"
  },
  {
    "text": "enhancements in terms of workload management as well and then on ease of use here's just a few quotes from",
    "start": "1002640",
    "end": "1008760"
  },
  {
    "text": "customers on there their feedback on on using redshift but I'll say like one of the newest launches that happened",
    "start": "1008760",
    "end": "1014880"
  },
  {
    "start": "1012000",
    "end": "1012000"
  },
  {
    "text": "recently which is just in the past couple of weeks there's something now in the console called redshift advisor and",
    "start": "1014880",
    "end": "1021510"
  },
  {
    "text": "essentially you know the data was always there so if you know we have this github location where we have like admin",
    "start": "1021510",
    "end": "1027750"
  },
  {
    "text": "scripts that you can do some you know investigation of your cluster or something slow we get lots of metrics or",
    "start": "1027750",
    "end": "1033449"
  },
  {
    "text": "being captured and what customers told us was hey like it's great I can run this script but would even greater if you could just show me in the console",
    "start": "1033449",
    "end": "1039720"
  },
  {
    "text": "right and then take action upon it and so the first iteration of this is live like in your console now now it's not in",
    "start": "1039720",
    "end": "1045900"
  },
  {
    "text": "every region so there are some specifics and the release in terms of which regions are supported but essentially it",
    "start": "1045900",
    "end": "1050940"
  },
  {
    "text": "gives you that that either from cost or performance an easy way to see okay am I using redshift to its fullest or there",
    "start": "1050940",
    "end": "1056700"
  },
  {
    "text": "are things that I could you know may be easily changed I could have big difference in terms of getting faster",
    "start": "1056700",
    "end": "1061860"
  },
  {
    "text": "queries or getting lower cost and so having that just in the console itself I think has been been you know pretty",
    "start": "1061860",
    "end": "1068550"
  },
  {
    "text": "be there with customers and then not only that in terms of some of the administration settings but then also just how how's my cluster doing I'm my",
    "start": "1068550",
    "end": "1076620"
  },
  {
    "text": "underutilizing and over utilizing it so before you'd have to look at things like CPU and memory utilization but",
    "start": "1076620",
    "end": "1082650"
  },
  {
    "text": "essentially now we have you know query throughput and query duration right as metrics you can just look at in the",
    "start": "1082650",
    "end": "1088170"
  },
  {
    "text": "console you know throughput you know depends you know upon your your view it's you know because wretched is a",
    "start": "1088170",
    "end": "1093630"
  },
  {
    "text": "columnar database we typically talk about you know throughput you know queries per hour queries per minute and",
    "start": "1093630",
    "end": "1098670"
  },
  {
    "text": "so if I look at these two in terms of you know if queries are running they aggregate running slower faster using",
    "start": "1098670",
    "end": "1105420"
  },
  {
    "text": "but then how many queries per minute am I getting kind of gives you that nice nice check to say you know regardless of",
    "start": "1105420",
    "end": "1111750"
  },
  {
    "text": "CPU in memory like how how quickly am I getting queries through and so being able to visualize that directly in the",
    "start": "1111750",
    "end": "1117120"
  },
  {
    "text": "console and then you know the last part is you know integration with data Lake is kind of our third pillar and again",
    "start": "1117120",
    "end": "1123810"
  },
  {
    "text": "you know our belief is that over time you know the data warehouse in the data Lake will start to merge and we have early efforts through spectrum feature",
    "start": "1123810",
    "end": "1130560"
  },
  {
    "text": "to read park' and or RC files directly and here is some call-outs from a couple of customers that are leveraging you",
    "start": "1130560",
    "end": "1136710"
  },
  {
    "text": "know this feature today to read s3 directly but some recent spectrum enhancements and so you know support for",
    "start": "1136710",
    "end": "1143850"
  },
  {
    "start": "1140000",
    "end": "1140000"
  },
  {
    "text": "more data types so we added scalar JSON and ion formats we now have you know a very popular request as you could",
    "start": "1143850",
    "end": "1150210"
  },
  {
    "text": "imagine support for native date format our date type in spectrum so you know you think about like like there's a CSV",
    "start": "1150210",
    "end": "1156720"
  },
  {
    "text": "file have a date well no it's a CSV file right but being able to still have a date based upon the data type and",
    "start": "1156720",
    "end": "1163290"
  },
  {
    "text": "depending upon how your s3 buckets are set up you can have cross account roles for I am to be able to access data and",
    "start": "1163290",
    "end": "1170010"
  },
  {
    "text": "other s3 buckets right and the last one you know well the last bullet point of let's call out which is the copy from",
    "start": "1170010",
    "end": "1176280"
  },
  {
    "text": "Park a and oh I see so you know redshift has this copy command where if I didn't want to have the data locally in",
    "start": "1176280",
    "end": "1182100"
  },
  {
    "text": "redshift I'd have to use copy command to invoke it to load into local disk well now you know copy command can support",
    "start": "1182100",
    "end": "1187920"
  },
  {
    "text": "Park a no are see files the alternative to that as I said though is that you know you can just have the Park a no RC",
    "start": "1187920",
    "end": "1193620"
  },
  {
    "text": "on s3 and read it directly without having to load it but there could be you know times where I might want to load it and so you know having that copy command",
    "start": "1193620",
    "end": "1200400"
  },
  {
    "text": "support Park a no RC has been a popular so you know request that's now available and then the second-to-last point all",
    "start": "1200400",
    "end": "1206340"
  },
  {
    "text": "this kind of spend two slides on is nested data support so now you know if you think about you know using the dot",
    "start": "1206340",
    "end": "1212340"
  },
  {
    "start": "1207000",
    "end": "1207000"
  },
  {
    "text": "notation and particularly if it's nested JSON or nested park' files you know how can i express this this richness of the",
    "start": "1212340",
    "end": "1219030"
  },
  {
    "text": "nested data format without having to flatten it out into a table structure so by leveraging spectrum you know you can",
    "start": "1219030",
    "end": "1225420"
  },
  {
    "text": "essentially you know query through you know using the dot notation to easily query this nested data format and if",
    "start": "1225420",
    "end": "1232530"
  },
  {
    "text": "this features it's been in preview for number of months which is our word of saying beta we're actually rolling it",
    "start": "1232530",
    "end": "1237870"
  },
  {
    "text": "out to the fleet this week and next and we'll have an announcement you know public announcement here in the coming days to kind of officially launch this",
    "start": "1237870",
    "end": "1244140"
  },
  {
    "text": "but this should already be enabled in your cluster today if not here shortly and you know querying this nested data",
    "start": "1244140",
    "end": "1249930"
  },
  {
    "text": "format you know using sequel plus plus and the dot notation obviously has been a frequent request particularly as you",
    "start": "1249930",
    "end": "1255630"
  },
  {
    "text": "look at you know non-traditional data sources like log data that might come nested itself and obviously there's a",
    "start": "1255630",
    "end": "1262710"
  },
  {
    "text": "big you know implication for performance right so if I don't have to flatten the",
    "start": "1262710",
    "end": "1267900"
  },
  {
    "text": "data out in particularly if it's JSON flatten it out meaning you know map it directly to relational tables not only",
    "start": "1267900",
    "end": "1274560"
  },
  {
    "text": "is it easier from an ingestion perspective I don't have to ETL the data into redshift I can just query it",
    "start": "1274560",
    "end": "1279960"
  },
  {
    "text": "directly from the spectrum layer on s3 but then there's also performance differences right because I don't have",
    "start": "1279960",
    "end": "1285480"
  },
  {
    "text": "to because I don't have to put it into relational tables I don't have to worry in some cases joining the data together and I can just query it natively and so",
    "start": "1285480",
    "end": "1292320"
  },
  {
    "text": "there's lots of lots more to come in terms of performance increases and and enhancements to spectrum but knowing",
    "start": "1292320",
    "end": "1298560"
  },
  {
    "text": "that I can simply create an external table and redshift pointed to a nested JSON file in query it using you know dot",
    "start": "1298560",
    "end": "1304290"
  },
  {
    "text": "notation has become you know a pretty popular request for customers and it's a feature that's now available to be able",
    "start": "1304290",
    "end": "1310410"
  },
  {
    "text": "to further this integration of the data lake and and in data warehousing worlds that we see so I know it's kind of",
    "start": "1310410",
    "end": "1316740"
  },
  {
    "text": "whirlwind I mean I will do an update that's when it provides some some key updates on key new features but maybe",
    "start": "1316740",
    "end": "1322860"
  },
  {
    "text": "the more interesting part is to hear you know beyond just hearing about you know redshift and how great it is to actually hear from a customer on how they looked",
    "start": "1322860",
    "end": "1329520"
  },
  {
    "text": "at this change and some of the benefits and lessons learned that I had so yes Ryan in and Elliott",
    "start": "1329520",
    "end": "1334980"
  },
  {
    "text": "your here's to talk about your your experience Thanks thanks great all",
    "start": "1334980",
    "end": "1340529"
  },
  {
    "text": "right so I'm Elliott this is my associate Ryan and we're gonna take you through our journey to redshift and data",
    "start": "1340529",
    "end": "1347669"
  },
  {
    "text": "Lake strategy there we go",
    "start": "1347669",
    "end": "1356820"
  },
  {
    "text": "all right so how many Equinox members do we have in the room oh good yes nice",
    "start": "1356820",
    "end": "1363450"
  },
  {
    "text": "guys get it so for those of you who aren't familiar with Equinox we are a",
    "start": "1363450",
    "end": "1369299"
  },
  {
    "text": "fitness and lifestyle brand that are devoted to movement nutrition and regeneration so we're best known for",
    "start": "1369299",
    "end": "1376440"
  },
  {
    "text": "our flagship brand Equinox we have about 98 locations as of now with several",
    "start": "1376440",
    "end": "1384179"
  },
  {
    "text": "beautiful locations in Chicago we also own blink Fitness which is a low cost",
    "start": "1384179",
    "end": "1389639"
  },
  {
    "text": "gym offering pure yoga soul cycle which is a boutiques studio cycling experience",
    "start": "1389639",
    "end": "1396919"
  },
  {
    "text": "furthermore which is a media arm as well as Equinox hotels and we'll be opening our first flagship hotel in Hudson Yards",
    "start": "1396919",
    "end": "1405000"
  },
  {
    "text": "New York City in the spring of next year so altogether we have over 200 locations",
    "start": "1405000",
    "end": "1411149"
  },
  {
    "text": "in our portfolio in every major city in London as well as Canada so yeah just",
    "start": "1411149",
    "end": "1419159"
  },
  {
    "text": "give you an idea of our scale so I have a question for you Ellie how complicated is B we have our gyms we walk into the",
    "start": "1419159",
    "end": "1426179"
  },
  {
    "text": "gym lift some weights put them down do it like this puppy does that's it all right we have some members how",
    "start": "1426179",
    "end": "1431580"
  },
  {
    "text": "complicated could it be yeah so if you really try hard you can make anything",
    "start": "1431580",
    "end": "1437370"
  },
  {
    "start": "1435000",
    "end": "1435000"
  },
  {
    "text": "complicated and you know Equinox has done a really good job at it so obviously we have the complexity of our scale you know with over ninety eight",
    "start": "1437370",
    "end": "1444059"
  },
  {
    "text": "equinoxes and in total 200 plus brick-and-mortar locations plus digital every one of our gyms has multiple lines",
    "start": "1444059",
    "end": "1451080"
  },
  {
    "text": "of business which are all run you know very very efficiently from personal training to Pilates to group fitness we",
    "start": "1451080",
    "end": "1456539"
  },
  {
    "text": "also quietly operate the nation's largest corporate spotting because we have a spawn every one of our locations",
    "start": "1456539",
    "end": "1462990"
  },
  {
    "text": "and being a large business we have all the central supporting functions from",
    "start": "1462990",
    "end": "1468990"
  },
  {
    "text": "digital product to see our to marketing to finance and the other",
    "start": "1468990",
    "end": "1474870"
  },
  {
    "start": "1474000",
    "end": "1474000"
  },
  {
    "text": "thing about it too is that it's all connected so we have our consumer products team that are building and use your applications constantly and a",
    "start": "1474870",
    "end": "1482370"
  },
  {
    "text": "number of these are also connecting to Apple health as well so data points are flowing every which way",
    "start": "1482370",
    "end": "1488070"
  },
  {
    "text": "additionally our equipment is tracking as well so pursuit is gamified cycling",
    "start": "1488070",
    "end": "1493740"
  },
  {
    "text": "experience and so this basically makes it something a little bit more fun than you sitting in a room and cycling away",
    "start": "1493740",
    "end": "1500130"
  },
  {
    "text": "so you're actually able to see your results in real time as you are cycling and so our bikes actually collect",
    "start": "1500130",
    "end": "1506010"
  },
  {
    "text": "multiple data points per second per bike per Club so we end up with hundreds of",
    "start": "1506010",
    "end": "1511110"
  },
  {
    "text": "millions of data points per day we also track what's happening our cardio machines and our digital scale and we're",
    "start": "1511110",
    "end": "1518550"
  },
  {
    "text": "actually working now to look into location tracking to see what parts of the gym are used in the frequency yeah",
    "start": "1518550",
    "end": "1526740"
  },
  {
    "text": "so a little bit about our data journey which is probably the interesting part so the history of data at Equinox you",
    "start": "1526740",
    "end": "1534030"
  },
  {
    "text": "know so we built our first official data warehouse type application back in 2007",
    "start": "1534030",
    "end": "1540270"
  },
  {
    "text": "2008 it was called life upon on it's not fitness its life our brand motto",
    "start": "1540270",
    "end": "1546929"
  },
  {
    "text": "so most like most traditional data warehouse implementations it's",
    "start": "1546929",
    "end": "1552059"
  },
  {
    "text": "cylindrical as shown in the picture it was very traditional you know like back",
    "start": "1552059",
    "end": "1557370"
  },
  {
    "start": "1555000",
    "end": "1555000"
  },
  {
    "text": "in the 2000s and prior like you'd pick some commercial off the shelf tools decide if you're gonna run an Oracle or",
    "start": "1557370",
    "end": "1563190"
  },
  {
    "text": "sequel server and you picked a religion we picked Kimball you know and we were rigorously Kimball you know I'm a good",
    "start": "1563190",
    "end": "1569520"
  },
  {
    "text": "friend of Joe caserta who's a big Kimball guy so yeah we we stuck to",
    "start": "1569520",
    "end": "1575160"
  },
  {
    "text": "Kimball we have a beautiful like dimensional data warehouse and if you want to read more about Kimball it's",
    "start": "1575160",
    "end": "1581130"
  },
  {
    "text": "right there there's still some good stuff to be learned from that methodology so life is good you know we",
    "start": "1581130",
    "end": "1587130"
  },
  {
    "start": "1585000",
    "end": "1585000"
  },
  {
    "text": "had reliable reporting we had analytics customer profile that empowered like our digital products and CRM and email",
    "start": "1587130",
    "end": "1593610"
  },
  {
    "text": "marketing so it was a good system it lived a decade and served us very well",
    "start": "1593610",
    "end": "1600050"
  },
  {
    "text": "but that's me right there the direct we had a couple challenges",
    "start": "1600050",
    "end": "1607100"
  },
  {
    "text": "you know aside from being a ten-year-old systems because things get gangly as they approach that age you know we had a",
    "start": "1607100",
    "end": "1613980"
  },
  {
    "text": "lot of direct integrations with our applications you know a rapidly growing business the data warehouse was like system of record of a lot of things",
    "start": "1613980",
    "end": "1620640"
  },
  {
    "text": "related to our customers first place where data really gets hydrated a place",
    "start": "1620640",
    "end": "1626160"
  },
  {
    "text": "where data gets kind of integrated so a lot of our applications start using our data binding directly to our poor data",
    "start": "1626160",
    "end": "1632130"
  },
  {
    "text": "warehouse and causing all sorts of problems for us we something I'm gonna",
    "start": "1632130",
    "end": "1637290"
  },
  {
    "text": "talk a lot about is a sdlc so doing anything like a modern you know kind of",
    "start": "1637290",
    "end": "1643680"
  },
  {
    "text": "de plus test and deployment pipeline on like a traditional data warehouse implementation is not trivial like where",
    "start": "1643680",
    "end": "1650190"
  },
  {
    "text": "am I gonna get another big set of infrastructure and you know kind of bootstrap all like this commercial",
    "start": "1650190",
    "end": "1655260"
  },
  {
    "text": "software which really isn't friendly for doing it so those two things coupled like the tight coupling as well as our",
    "start": "1655260",
    "end": "1661170"
  },
  {
    "text": "sdlc challenges meet us a crew functional debt because we couldn't change quickly enough we had no place to",
    "start": "1661170",
    "end": "1666900"
  },
  {
    "text": "put new data or sequel server was already several like terabytes and there's lots of terabytes waiting",
    "start": "1666900",
    "end": "1672360"
  },
  {
    "text": "outside to come in and there was no good way to accomplish that our data science",
    "start": "1672360",
    "end": "1678210"
  },
  {
    "text": "and our analysts were frustrated because of our methodology as well as our constraints on development and",
    "start": "1678210",
    "end": "1684180"
  },
  {
    "text": "introducing new data and we had lots of expensive commercial software so to",
    "start": "1684180",
    "end": "1689400"
  },
  {
    "text": "solve this we bought more expensive commercial stuff so we bought a tower",
    "start": "1689400",
    "end": "1695970"
  },
  {
    "start": "1691000",
    "end": "1691000"
  },
  {
    "text": "data cluster and now about four years ago so we ended up getting several apps",
    "start": "1695970",
    "end": "1702420"
  },
  {
    "text": "up in beta we were able to address some of our like large data set semi-structured workloads a little bit",
    "start": "1702420",
    "end": "1708750"
  },
  {
    "text": "we found that the platform required a lot of platform specific knowledge like everybody had to go to like Tara do like",
    "start": "1708750",
    "end": "1714930"
  },
  {
    "text": "DBA school and developer school if I got how to develop on it limited integration",
    "start": "1714930",
    "end": "1720210"
  },
  {
    "text": "unless you paid Tara date even more money so their goal is to really get everything in Tara data so that like you spend more money and buy bigger clusters",
    "start": "1720210",
    "end": "1726780"
  },
  {
    "text": "and it was ultimately very expensive like I think right you know mentioned you know 10x or you know kind of cost",
    "start": "1726780",
    "end": "1734010"
  },
  {
    "text": "savings you know we will get in a little bit more but we actually found the maintenance and support just like",
    "start": "1734010",
    "end": "1739429"
  },
  {
    "text": "Tutera data that we were paying we were able to cut that in 1/5 not just the original purchase by moving to redshift",
    "start": "1739429",
    "end": "1748330"
  },
  {
    "text": "so at this time we were like kind of like in a quagmire of like do we move forward with Terra data you know I you",
    "start": "1748690",
    "end": "1755480"
  },
  {
    "text": "know kind of grown up like building apps in the cloud and using redshift and I just you know said we should probably",
    "start": "1755480",
    "end": "1762169"
  },
  {
    "text": "just evaluate our goals stop and see what we're trying to accomplish it's always a good idea when you're you know evaluating making a change or you know",
    "start": "1762169",
    "end": "1769640"
  },
  {
    "text": "kind of on a doing a project so we're trying to build business value we're trying to get things done quicker for",
    "start": "1769640",
    "end": "1775400"
  },
  {
    "text": "our customers we are trying to reduce cost and we are going all in in the public cloud we want",
    "start": "1775400",
    "end": "1783409"
  },
  {
    "text": "to build technology to differentiates so spend less time on like administration and and stuff like that and spend time",
    "start": "1783409",
    "end": "1789289"
  },
  {
    "text": "building good software and we want to embrace like modern engineering principles so we can build immortal",
    "start": "1789289",
    "end": "1794960"
  },
  {
    "text": "systems you know which we're not really worried if a server or something goes down so I'm gonna play the devil on your",
    "start": "1794960",
    "end": "1802130"
  },
  {
    "start": "1800000",
    "end": "1800000"
  },
  {
    "text": "shoulder why don't we just do what the new school says we don't really need a data warehouse we can just throw it all in a data lake and then just use late",
    "start": "1802130",
    "end": "1808460"
  },
  {
    "text": "buying strategy it should work perfectly right that's sorta right so the you know that",
    "start": "1808460",
    "end": "1815990"
  },
  {
    "text": "approach works for a lot of things but not for everything so you know like in",
    "start": "1815990",
    "end": "1821120"
  },
  {
    "start": "1820000",
    "end": "1820000"
  },
  {
    "text": "you know data warehouse versus a data Lake as Greg said it's getting a little blurred there's new advancements and",
    "start": "1821120",
    "end": "1827299"
  },
  {
    "text": "storage technologies and tools like spectrum and stuff but really the way we differentiated is our data warehouses",
    "start": "1827299",
    "end": "1832370"
  },
  {
    "text": "for reliable high assaleh reporting developer and analyst friendly you know",
    "start": "1832370",
    "end": "1838909"
  },
  {
    "text": "kind of workloads and analytics as well as efficiency for specific types of data pipelines pipelines which tend to be",
    "start": "1838909",
    "end": "1845720"
  },
  {
    "text": "more mutable where you're gonna have updates business data that changes these",
    "start": "1845720",
    "end": "1850760"
  },
  {
    "text": "things are a little bit more complicated to accomplish in a pure data Lake strategy so data leaks on the other hand",
    "start": "1850760",
    "end": "1857179"
  },
  {
    "text": "are very good at large immutable data sets think about logs think about POS systems that once you make a transaction",
    "start": "1857179",
    "end": "1863780"
  },
  {
    "text": "it goes on the books and never gets touched again as well as semi-structured and unstructured",
    "start": "1863780",
    "end": "1868830"
  },
  {
    "text": "your data so test this out we had like a",
    "start": "1868830",
    "end": "1873840"
  },
  {
    "start": "1872000",
    "end": "1872000"
  },
  {
    "text": "small internal like project POC just in our spare time we called a project Cosmo",
    "start": "1873840",
    "end": "1880080"
  },
  {
    "text": "and Cosmo is a little weird robot who lives in the clouds you know picture him there we decided to reap lat form or one",
    "start": "1880080",
    "end": "1888450"
  },
  {
    "text": "tower data app related to lead analytics to redshift and Amazon s3 this is one",
    "start": "1888450",
    "end": "1893940"
  },
  {
    "text": "that was a little challenging to us because again we wanted to tap into some of those new data sources such as clickstream analytics right so we were",
    "start": "1893940",
    "end": "1902909"
  },
  {
    "text": "able to do this in two weeks we found a very productive we were very happy with it and it worked",
    "start": "1902909",
    "end": "1908010"
  },
  {
    "text": "so our beloved Teradata cluster was went",
    "start": "1908010",
    "end": "1914490"
  },
  {
    "text": "bye-bye it turns out there's not much of a secondary market for a lightly used Terra data clusters so it unfortunately",
    "start": "1914490",
    "end": "1920610"
  },
  {
    "text": "went to technology Salvage or maybe it was made into a robot yeah so that's",
    "start": "1920610",
    "end": "1928200"
  },
  {
    "text": "when we decided to make the Jarvis data warehouse from the proof-of-concept we found that it worked so well so we",
    "start": "1928200",
    "end": "1933899"
  },
  {
    "start": "1932000",
    "end": "1932000"
  },
  {
    "text": "decided to go all in on it and so this includes our data warehouse or data Lake and our data services and with our data",
    "start": "1933899",
    "end": "1941519"
  },
  {
    "text": "warehouse the architecture that we built for it is that we're essentially going to have a number of apps and different",
    "start": "1941519",
    "end": "1947970"
  },
  {
    "text": "services that are bringing data in we have our own homegrown maximum framework that Elliott will get into but we also",
    "start": "1947970",
    "end": "1954779"
  },
  {
    "text": "use informatica to basically transfer that data into Jarvis redshift which",
    "start": "1954779",
    "end": "1959940"
  },
  {
    "text": "there we're also doing some light elt and making that data you know ready for",
    "start": "1959940",
    "end": "1966210"
  },
  {
    "text": "a fact and dimension tables we also do some lighter transformations through from active Maximilian as well but for",
    "start": "1966210",
    "end": "1972570"
  },
  {
    "text": "the big transformations that will be that's what we save EMR for so at Equinox we really love Apache spark PI",
    "start": "1972570",
    "end": "1979019"
  },
  {
    "text": "spark is absolutely fantastic so once we get into Jarvis we know that we do have",
    "start": "1979019",
    "end": "1984029"
  },
  {
    "text": "a presentation layer that we have to service down the road and those involve apps that we are building on our data",
    "start": "1984029",
    "end": "1989850"
  },
  {
    "text": "analytics team apps that are consumer product team is building and also third-party apps that may be fueled by",
    "start": "1989850",
    "end": "1995190"
  },
  {
    "text": "that data as well and so what we did in the middle layer is built these data Mart's and API is to help service that",
    "start": "1995190",
    "end": "2000860"
  },
  {
    "text": "and maybe lighten some of the compute on redshift itself for the semi structured unstructured maybe there's",
    "start": "2000860",
    "end": "2007669"
  },
  {
    "text": "more immutable data sets we actually send a lot of those straight to s3 itself and with the introduction of",
    "start": "2007669",
    "end": "2013010"
  },
  {
    "text": "redshift spectrum were able to use AWS glue to define the data which essentially sits there is a virtual end",
    "start": "2013010",
    "end": "2019160"
  },
  {
    "text": "memory description of that data and then allows you to once you're in redshift we're that data straight from s3 which",
    "start": "2019160",
    "end": "2025429"
  },
  {
    "text": "has worked very very well for us and so underlying all this we have our data quality and monitoring services that I",
    "start": "2025429",
    "end": "2031760"
  },
  {
    "text": "will touch on sure so I won't spend too much on the slide you know but you know",
    "start": "2031760",
    "end": "2037669"
  },
  {
    "text": "I think we know what redshift is and you know just from our perspective the things that we find very helpful or the",
    "start": "2037669",
    "end": "2043370"
  },
  {
    "text": "fact that it's mostly Postgres compatible it's fasting performing ease",
    "start": "2043370",
    "end": "2048679"
  },
  {
    "text": "of maintenance although it is still an instance based product we perform very little in terms of maintenance",
    "start": "2048679",
    "end": "2054648"
  },
  {
    "text": "operations on our clusters and you know we're just finding low barriers for our developers and analysts so a lot of",
    "start": "2054649",
    "end": "2062658"
  },
  {
    "start": "2062000",
    "end": "2062000"
  },
  {
    "text": "people ask like you know what our data models look like again we came from a very religious you know Kimball",
    "start": "2062659",
    "end": "2069350"
  },
  {
    "text": "methodology camp so you know there's somewhat like pragmatic star schemas that's the way we call it you know since",
    "start": "2069350",
    "end": "2074358"
  },
  {
    "text": "redshift is a distributed system it's not a relational database you don't have to do a lot of those techniques because",
    "start": "2074359",
    "end": "2081108"
  },
  {
    "text": "they are really optimized for relational databases and you know so basically we",
    "start": "2081109",
    "end": "2086628"
  },
  {
    "text": "have flattened like event tables we don't do all the weird you know stuff",
    "start": "2086629",
    "end": "2091669"
  },
  {
    "text": "like junk dimensions bridge tables attribute dimensions and Amazon redshift is columnar so wide tables are totally",
    "start": "2091669",
    "end": "2098150"
  },
  {
    "text": "okay and of course distributed joins can be more expensive so reducing joins unless it's like a data management issue",
    "start": "2098150",
    "end": "2105130"
  },
  {
    "text": "you know is a good idea and we do have dimensions we're pretty rational and conservative we save them for like",
    "start": "2105130",
    "end": "2111080"
  },
  {
    "text": "really business data that has its own kind of lifecycle like people and employees and facilities and assets and",
    "start": "2111080",
    "end": "2118310"
  },
  {
    "text": "we are very conservative with our use of type two dimensions so really from a",
    "start": "2118310",
    "end": "2123440"
  },
  {
    "text": "paradigm perspective and why we find so much productivity is our developers build a data pipeline that gets a good",
    "start": "2123440",
    "end": "2129890"
  },
  {
    "text": "answer and then we just put it in a table we build some governance around that and figure out on a daily or intraday and we're done so",
    "start": "2129890",
    "end": "2136760"
  },
  {
    "text": "like we're able to bring new data assets to life much quicker than we did before",
    "start": "2136760",
    "end": "2144130"
  },
  {
    "text": "so in terms of processing and Amazon redshift something around touched on we do light transformations within redshift",
    "start": "2144460",
    "end": "2151310"
  },
  {
    "text": "we have a proprietary system that will soon open source called Maximilian it's a Python module where we essentially run",
    "start": "2151310",
    "end": "2158510"
  },
  {
    "text": "and dynamically build like sequel scripts to execute our alt scripts all",
    "start": "2158510",
    "end": "2163790"
  },
  {
    "text": "of the big Crunch's and semi structured data processes happen outside of redshift with some small exceptions and",
    "start": "2163790",
    "end": "2169840"
  },
  {
    "text": "really we do that to reserve query capacity like our redshift is really for the analysts and the data scientists so",
    "start": "2169840",
    "end": "2176240"
  },
  {
    "text": "anything we can do to get more of them in and give them better performance the better so I really like proactively try",
    "start": "2176240",
    "end": "2181940"
  },
  {
    "text": "to reduce kind of like ETL bloat and you know kind of capacity being taken from",
    "start": "2181940",
    "end": "2188510"
  },
  {
    "text": "our redshift system just to touch on our",
    "start": "2188510",
    "end": "2194270"
  },
  {
    "text": "data lake one of the reasons that we really got into it are well there's a number of them so really want to utilize",
    "start": "2194270",
    "end": "2201650"
  },
  {
    "text": "the high-performance low-cost blob storage that s3 has we also want to make",
    "start": "2201650",
    "end": "2206900"
  },
  {
    "text": "a functioning analytics store they're one of our beliefs here at Equinox is that we really want to be s3 first with",
    "start": "2206900",
    "end": "2212060"
  },
  {
    "text": "a lot of the data that's coming in if it ends up in redshift that's fine but let's just make sure that we get it there in s3 so we want to make sure that",
    "start": "2212060",
    "end": "2219800"
  },
  {
    "text": "this was also functional so we just didn't want to make it a dumping ground to send all the data there as I likes to say we want to have a nice data Lake not",
    "start": "2219800",
    "end": "2226790"
  },
  {
    "text": "a data swamp very cliche but yes yes and we also want to employ flexible late",
    "start": "2226790",
    "end": "2233240"
  },
  {
    "text": "bind strategies where applicable not all the data was going to be you know defined in AWS glue and then have",
    "start": "2233240",
    "end": "2238400"
  },
  {
    "text": "redshift spectrum where Athena you know run queries on it from there but we're applicable we could use it",
    "start": "2238400",
    "end": "2244250"
  },
  {
    "text": "there and then for the times that we do we can employ the quick setup for those",
    "start": "2244250",
    "end": "2249380"
  },
  {
    "text": "external tables you know we've had times where we've started collecting data and that day within minutes we're actually",
    "start": "2249380",
    "end": "2254990"
  },
  {
    "text": "able to query the data because building the external table to find that data is extremely fast and so lastly the other",
    "start": "2254990",
    "end": "2262340"
  },
  {
    "text": "thing that we have here is it's very easily - very easy to implement just disaster recovery at adji 4s3 it's",
    "start": "2262340",
    "end": "2270500"
  },
  {
    "text": "just a configuration change that you have to make on the bucket itself so once the data is in there change the",
    "start": "2270500",
    "end": "2275569"
  },
  {
    "text": "configuration and you're good to go and so what we store here is the very",
    "start": "2275569",
    "end": "2281450"
  },
  {
    "start": "2279000",
    "end": "2279000"
  },
  {
    "text": "first thing we actually put in here was our clickstream data so this is very amused at a-- it's essentially a log of",
    "start": "2281450",
    "end": "2286819"
  },
  {
    "text": "all the data that's coming from our web applications iOS app or Android app we",
    "start": "2286819",
    "end": "2292400"
  },
  {
    "text": "also have internal apps that are sending data to it as well and we originally projected it into redshift and found out",
    "start": "2292400",
    "end": "2298520"
  },
  {
    "text": "that once the analysts found out about it they actually really loved it and because they loved it they wanted more",
    "start": "2298520",
    "end": "2303950"
  },
  {
    "text": "of it and so originally we only had maybe 40 columns worth of data in there and they wanted more and more and we",
    "start": "2303950",
    "end": "2310220"
  },
  {
    "text": "what we found is that we had to keep going back to the drawing board changing the ETL scripts adding those columns in there and then along the way",
    "start": "2310220",
    "end": "2316579"
  },
  {
    "text": "our collation provider was also deprecating columns that were in there too so the data that they sent us were 550",
    "start": "2316579",
    "end": "2323329"
  },
  {
    "text": "600 columns wide and then they were getting rid of column 350 right in the middle of the data set and so it was",
    "start": "2323329",
    "end": "2330589"
  },
  {
    "text": "very hard to pivot whenever we needed to add stuff or make a change based on the data that was being sent and so what we",
    "start": "2330589",
    "end": "2336200"
  },
  {
    "text": "started doing there is taking all the data that was being fed from our collection provider and saving the",
    "start": "2336200",
    "end": "2341660"
  },
  {
    "text": "entire data set in Part A and our data Lake and then from there we were able to define an AWS glu table on top of it and",
    "start": "2341660",
    "end": "2348920"
  },
  {
    "text": "pick whichever columns we wanted so if we wanted columns 1 2 3 499 and 525 we",
    "start": "2348920",
    "end": "2356030"
  },
  {
    "text": "could just pick those specific columns not have to define everything along the way and so now when someone wants a new",
    "start": "2356030",
    "end": "2362540"
  },
  {
    "text": "column because all the data is being collected and put in Parque anyways the last four it will go and make a change",
    "start": "2362540",
    "end": "2367940"
  },
  {
    "text": "in they do as glue takes 20 seconds save it and it's automatically available there to be queried in redshift spectrum",
    "start": "2367940",
    "end": "2374089"
  },
  {
    "text": "and so we've seen a lot of success with that it makes it extremely flexible when we need to make those changes and the",
    "start": "2374089",
    "end": "2381230"
  },
  {
    "text": "performance is absolutely fantastic the other thing that we've been doing with it is our cycling logs so as mentioned",
    "start": "2381230",
    "end": "2387500"
  },
  {
    "text": "pursuit is sending hundreds of millions of data points every day and so we need",
    "start": "2387500",
    "end": "2392540"
  },
  {
    "text": "to have some kind of operations around those bikes and make sure that when someone gets on a bike that it's going",
    "start": "2392540",
    "end": "2398569"
  },
  {
    "text": "to operate the way that they want it to some time is bikes need maintenance and that's just a fact of life so what we",
    "start": "2398569",
    "end": "2404420"
  },
  {
    "text": "actually do is we've defined that data and AWS glue as well and we have Athena that runs a query over it and actually",
    "start": "2404420",
    "end": "2410749"
  },
  {
    "text": "sends those results and a CSV to slack and so people from our cycling",
    "start": "2410749",
    "end": "2416019"
  },
  {
    "text": "operations team can actually see which bikes are starting to maybe have gaps in",
    "start": "2416019",
    "end": "2421099"
  },
  {
    "text": "the data that's sent or the wattage is above what's normal and they just get a daily file and slack that they can go",
    "start": "2421099",
    "end": "2426799"
  },
  {
    "text": "and check that data other data sources that we have are from our club management software data and then any",
    "start": "2426799",
    "end": "2432829"
  },
  {
    "text": "other data service that enhances our our services as well and so making it all",
    "start": "2432829",
    "end": "2440359"
  },
  {
    "text": "work here's some tools and tips so AWS glue is absolutely mandatory you will need it for the data that's an s3 you'll",
    "start": "2440359",
    "end": "2446839"
  },
  {
    "text": "just need a way to define that data and then from there you have your options so you can use Amazon Athena EMR or",
    "start": "2446839",
    "end": "2453079"
  },
  {
    "text": "redshift spectrum what we've found is that when we need to do maybe an ad hoc query that's serverless",
    "start": "2453079",
    "end": "2459499"
  },
  {
    "text": "will use Athena for it and so that's where we used it for our cycling logs EMR is great for the big transformations",
    "start": "2459499",
    "end": "2466249"
  },
  {
    "text": "so we use that for our clickstream data sets and then the other thing we do as well is we use redshift spectrum for",
    "start": "2466249",
    "end": "2473329"
  },
  {
    "text": "when we need to join that data in s3 two data sources that are also in redshift so our analysts are actually joining",
    "start": "2473329",
    "end": "2478999"
  },
  {
    "text": "together our data from our clickstream data sources in s3 to may be member profile information that's sitting in",
    "start": "2478999",
    "end": "2485269"
  },
  {
    "text": "our redshift cluster and just some tips here where where you can try and",
    "start": "2485269",
    "end": "2491479"
  },
  {
    "text": "leverage self-described high compression park' files so the header of the file is actually in the file itself so you don't",
    "start": "2491479",
    "end": "2498589"
  },
  {
    "text": "have to get rid of the header like you would for a CSV and that's what makes it available for you to basically pick",
    "start": "2498589",
    "end": "2504259"
  },
  {
    "text": "whichever column you want depending on its end to see where applicable you can really lighten the compute load on",
    "start": "2504259",
    "end": "2510170"
  },
  {
    "text": "Amazon redshift by utilizing EMR or athina so that's why we use it for our clickstream data and then something",
    "start": "2510170",
    "end": "2517249"
  },
  {
    "text": "that's really cool that you can do here is for data that's sitting at s3 in redshift spectrum you can run all the",
    "start": "2517249",
    "end": "2523549"
  },
  {
    "text": "same commands that you would in redshift so if you need to do an unload you can actually unload from s3 to s3 and do a",
    "start": "2523549",
    "end": "2530299"
  },
  {
    "text": "transformation in the process and we actually use this for our clickstream data because it's such raw data we want to",
    "start": "2530299",
    "end": "2537170"
  },
  {
    "text": "roll up the data and make it easier for the analyst to go through and pick out insights from it so we actually have",
    "start": "2537170",
    "end": "2542720"
  },
  {
    "text": "some scripts that are unload scripts that roll up the data by just simply",
    "start": "2542720",
    "end": "2547880"
  },
  {
    "text": "doing an unload statement and defining a new table and AWS Kaleo and then lastly one of the things that",
    "start": "2547880",
    "end": "2553400"
  },
  {
    "text": "makes it big one of the things you can do very easily are comparison queries so if you need to",
    "start": "2553400",
    "end": "2559190"
  },
  {
    "text": "compare two days of data if you're saving daily snapshots of a database from redshift into s3 you can just go",
    "start": "2559190",
    "end": "2565520"
  },
  {
    "text": "back and query off two different folders of data see the comparison and you know",
    "start": "2565520",
    "end": "2571010"
  },
  {
    "text": "see what the difference is between those two days and so here's actually a sample Atos blue definition that we have on our",
    "start": "2571010",
    "end": "2578570"
  },
  {
    "start": "2574000",
    "end": "2574000"
  },
  {
    "text": "clickstream data so we defined our table and then every time we need to bring in",
    "start": "2578570",
    "end": "2583580"
  },
  {
    "text": "a new column name there's just an area to edit your schema you add it in there say what your data type is and you're",
    "start": "2583580",
    "end": "2588830"
  },
  {
    "text": "good to go and you can actually see at the very bottom here we have DT which is our date",
    "start": "2588830",
    "end": "2594140"
  },
  {
    "text": "time so if your data structure and s3 actually includes an equal sign whatever is before that equals sign which we have",
    "start": "2594140",
    "end": "2601460"
  },
  {
    "text": "DT so that's our date time so DT equals we use your year year month month day",
    "start": "2601460",
    "end": "2606680"
  },
  {
    "text": "day and so that allows us to actually pick out the specific folders that we want to query so if we have terabytes",
    "start": "2606680",
    "end": "2614150"
  },
  {
    "text": "upon terabytes of data for our clothes room from our collection provider and s3 if we just provide a very specific date",
    "start": "2614150",
    "end": "2621920"
  },
  {
    "text": "time then we can query just that so if we want to query 50 megabytes instead of 10 terabytes it's very easy to do so so",
    "start": "2621920",
    "end": "2633440"
  },
  {
    "text": "something were pretty excited about again I got back to like the SDLC thing that I brought up before is automation",
    "start": "2633440",
    "end": "2640160"
  },
  {
    "text": "and DevOps on redshift so like you know a lot of analytics systems have difficulty even cloud-based ones like",
    "start": "2640160",
    "end": "2646280"
  },
  {
    "text": "adopting kind of modern like kind of like testing and pipeline and deployment",
    "start": "2646280",
    "end": "2652640"
  },
  {
    "text": "pipeline operations like modern DevOps principles so we've really worked to try to achieve that to increase our time the",
    "start": "2652640",
    "end": "2659120"
  },
  {
    "text": "benefit so we've built some homegrown tools from dag execution to ham bot",
    "start": "2659120",
    "end": "2665660"
  },
  {
    "text": "which is our data quality monitoring and sweet four-hour redshift that's three data Lake some ops monitoring we use run",
    "start": "2665660",
    "end": "2673520"
  },
  {
    "text": "deck for scheduling and Jenkins is kind of like our orchestration flow for all of our deployment and testing pipelines",
    "start": "2673520",
    "end": "2680800"
  },
  {
    "text": "so how do we do deployments like how do you do automated regression on a data",
    "start": "2680800",
    "end": "2686030"
  },
  {
    "start": "2682000",
    "end": "2682000"
  },
  {
    "text": "warehouse it seems very challenging mainly because like if you're testing an app you're doing unit testing integration testing you know you can",
    "start": "2686030",
    "end": "2692330"
  },
  {
    "text": "fixture data is not a big thing you know or at least not giant generally when you",
    "start": "2692330",
    "end": "2698960"
  },
  {
    "text": "have a data warehouse solution the fixture data is the whole database or all the source systems so it can be very",
    "start": "2698960",
    "end": "2704270"
  },
  {
    "text": "difficult to try to do automated tests so the way that we do it is we",
    "start": "2704270",
    "end": "2709340"
  },
  {
    "text": "essentially our Jenkins flow will spin up ephemeral Amazon redshift clusters which could be empty could be of some",
    "start": "2709340",
    "end": "2716330"
  },
  {
    "text": "specific date with an own state or yesterday you know if we want to do",
    "start": "2716330",
    "end": "2721640"
  },
  {
    "text": "quick like hotfix deployment regressions so what happens is we do that then we split up docker containers of our",
    "start": "2721640",
    "end": "2728060"
  },
  {
    "text": "Maximillian assets we run all of our major transformations because we're",
    "start": "2728060",
    "end": "2733130"
  },
  {
    "text": "using s3 as many of our sources or at least like a necessary forward strategy the original data is sitting there ready",
    "start": "2733130",
    "end": "2739610"
  },
  {
    "text": "we've run our hand bot checks and if all goes well we you know essentially allow things to merge to master and currently",
    "start": "2739610",
    "end": "2747200"
  },
  {
    "text": "we're working on schema deployments as well and for fun you know in the spirit",
    "start": "2747200",
    "end": "2753020"
  },
  {
    "text": "of automation and again to make our developers happy for slack users out there we allow our developers to create",
    "start": "2753020",
    "end": "2759530"
  },
  {
    "text": "EMR clusters and even redshift clusters and ec2 themselves so we've created a bot named Vincent who allows our",
    "start": "2759530",
    "end": "2765740"
  },
  {
    "text": "operators to you know create clusters kill clusters check the status of clusters get connection information all",
    "start": "2765740",
    "end": "2771980"
  },
  {
    "text": "through slack so it makes them much easier it makes it much easier for them to use in the console they don't have to",
    "start": "2771980",
    "end": "2777290"
  },
  {
    "text": "bother our admins and you know it reduces the console access requirement by giving them like kind of a sandbox of",
    "start": "2777290",
    "end": "2783770"
  },
  {
    "text": "what they can do and I just want you to notice sure he's actually representing him and so every hero needs a villain so",
    "start": "2783770",
    "end": "2791090"
  },
  {
    "text": "we decided to go a little bit further with the bots and this has actually bought two bought communication that we have this is Maximilian he's the villain",
    "start": "2791090",
    "end": "2797990"
  },
  {
    "text": "so every time someone brings out the cluster and they're working with it and I believe someone may forget about it and may fail",
    "start": "2797990",
    "end": "2804180"
  },
  {
    "text": "it may be sitting in that waiting stage so he actually comes in every day at a certain time and says that he's going to",
    "start": "2804180",
    "end": "2809580"
  },
  {
    "text": "destroy these clusters hopefully someone comes in and saves him if they do not he destroys them but the big benefit here",
    "start": "2809580",
    "end": "2816390"
  },
  {
    "text": "takeaway is that you don't always have to have someone going through and monitoring you know how long these",
    "start": "2816390",
    "end": "2822330"
  },
  {
    "text": "clusters are running if someone needs them they'll come in there and do the job for you so in terms of results you",
    "start": "2822330",
    "end": "2830220"
  },
  {
    "start": "2830000",
    "end": "2830000"
  },
  {
    "text": "know things are really good we're very happy with redshift and our overall move you know we've seen huge you know more",
    "start": "2830220",
    "end": "2837180"
  },
  {
    "text": "than the cost savings we've seen huge increases in productivity we view platforms and production alized our",
    "start": "2837180",
    "end": "2843240"
  },
  {
    "text": "entire system in our first two apps in four months and we finished our platform",
    "start": "2843240",
    "end": "2848670"
  },
  {
    "text": "in under a year as I said it's very dependable faster time the benefits since now since redshift is a service",
    "start": "2848670",
    "end": "2856140"
  },
  {
    "text": "with a full api we can do really cool things in terms of automated regression to make sure that when we're doing",
    "start": "2856140",
    "end": "2862350"
  },
  {
    "text": "changes to our system that we don't have unintended consequences or break anything and again the huge cost savings over two",
    "start": "2862350",
    "end": "2868350"
  },
  {
    "text": "Teradata we are paying 20% approximately of the maintenance and support that we",
    "start": "2868350",
    "end": "2874500"
  },
  {
    "text": "were paying with our Tara data and sequel server legacy so you know that",
    "start": "2874500",
    "end": "2881610"
  },
  {
    "text": "alone not to mention the original software expenditure software and hardware expenditure so worked out so",
    "start": "2881610",
    "end": "2887490"
  },
  {
    "start": "2887000",
    "end": "2887000"
  },
  {
    "text": "well we helped out our cousin blink and we built them a brand new blank data warehouse as well and that only took us",
    "start": "2887490",
    "end": "2893700"
  },
  {
    "text": "four months I was a little cheating because they do run a few of our systems and they're not that dissimilar but",
    "start": "2893700",
    "end": "2899330"
  },
  {
    "text": "they're very happy and everything's going very well there and just in terms",
    "start": "2899330",
    "end": "2906540"
  },
  {
    "start": "2904000",
    "end": "2904000"
  },
  {
    "text": "of lessons learned you know try one possibly use an s3 data leak approach whenever possible you know it can do",
    "start": "2906540",
    "end": "2914160"
  },
  {
    "text": "great things in terms of sharing data across clusters across systems across technologies as well as when you're",
    "start": "2914160",
    "end": "2919350"
  },
  {
    "text": "trying to automate testing and regression strive to be couple again going back to the architecture diagram",
    "start": "2919350",
    "end": "2925740"
  },
  {
    "text": "we try one possible to give people lakeshore Mart's that are versioned or versioned api's versus having them reach",
    "start": "2925740",
    "end": "2932340"
  },
  {
    "text": "into our system plan for flexibility we had problems with our clickstream data glue came",
    "start": "2932340",
    "end": "2938630"
  },
  {
    "text": "along and spectrum and we we jumped on board as quickly as we could",
    "start": "2938630",
    "end": "2943869"
  },
  {
    "text": "one size doesn't fit all you know use the right tool for the job you know a hammer when you need a hammer and again",
    "start": "2944349",
    "end": "2950450"
  },
  {
    "text": "automate everything leverage automated test and deployment in your analytic environment and I'll leave this up for a",
    "start": "2950450",
    "end": "2957380"
  },
  {
    "text": "second we are always hiring and looking for smart people so if you like cool stuff come see Equinox",
    "start": "2957380",
    "end": "2964730"
  },
  {
    "text": "careers and thank you thanks a great",
    "start": "2964730",
    "end": "2976820"
  },
  {
    "text": "story thank you I think we have about 10 minutes or so so if there's some",
    "start": "2976820",
    "end": "2982010"
  },
  {
    "text": "questions I'm happy to take them for the Xtreme inutes here's what I guess",
    "start": "2982010",
    "end": "2988640"
  },
  {
    "text": "there's a maybe I can walk around and see if I can guess you have a question",
    "start": "2988640",
    "end": "2994099"
  },
  {
    "text": "come run up and find me and I can have the mic for",
    "start": "2994099",
    "end": "2998440"
  },
  {
    "text": "yeah thank you everyone for letting my question go first thank you for a",
    "start": "3004150",
    "end": "3009190"
  },
  {
    "text": "beautiful presentation very informative one of the slides you mentioned that we",
    "start": "3009190",
    "end": "3015100"
  },
  {
    "text": "try to be you know especially coming from the Kimball pure Kimball approach we try to be conservative for type two",
    "start": "3015100",
    "end": "3022780"
  },
  {
    "text": "you said so does it mean that you try to minimize the use of it and used type one",
    "start": "3022780",
    "end": "3028180"
  },
  {
    "text": "instead or can you please elaborate more on that yeah yeah so so I think we use a combination of techniques at first we",
    "start": "3028180",
    "end": "3034540"
  },
  {
    "text": "really make sure that we need it we don't like kind of greenfield build it and they will come sort of thing we do",
    "start": "3034540",
    "end": "3040870"
  },
  {
    "text": "have a few entities which are kind of like log data like slowly changing more traditional but you know what we do use",
    "start": "3040870",
    "end": "3048670"
  },
  {
    "text": "as a technique since we can have flattened data structures and we want to avoid joins you know because it's a",
    "start": "3048670",
    "end": "3054730"
  },
  {
    "text": "distributed system we do at times flatten data into our event tables",
    "start": "3054730",
    "end": "3060250"
  },
  {
    "text": "themselves so if there is kind of like point in time information it's used a",
    "start": "3060250",
    "end": "3065320"
  },
  {
    "text": "lot by analysts or in like our standard reports we will sometimes roll it into",
    "start": "3065320",
    "end": "3070870"
  },
  {
    "text": "the event table itself pragmatism so",
    "start": "3070870",
    "end": "3076320"
  },
  {
    "text": "I've used it for about a year now or over a year one of the things I",
    "start": "3076320",
    "end": "3081730"
  },
  {
    "text": "encountered difficulty is that we can define all kinds of constraints on your",
    "start": "3081730",
    "end": "3087040"
  },
  {
    "text": "tables but they're not enforced yeah primary keys or foreign keys yes it's a good point a great question for Gregg is",
    "start": "3087040",
    "end": "3097120"
  },
  {
    "text": "- when will red chip start enforcing those constraints primary or foreign",
    "start": "3097120",
    "end": "3102970"
  },
  {
    "text": "keys yeah I don't know where Craig is I can tell you our point of view one okay so I'll give you my quick take on it",
    "start": "3102970",
    "end": "3109690"
  },
  {
    "text": "okay you know like especially after having like a very large like relational database one of the first things we did",
    "start": "3109690",
    "end": "3114790"
  },
  {
    "text": "to improve performance was turn off all the constraints so and that happens a lot",
    "start": "3114790",
    "end": "3120400"
  },
  {
    "text": "what we do is we actually use our hand bot test again because all of our systems are flattened for the most part so you know when we do have like I can",
    "start": "3120400",
    "end": "3127960"
  },
  {
    "text": "strain it like again we're linking out to a person or an employer an asset we essentially run assertions on the table",
    "start": "3127960",
    "end": "3135070"
  },
  {
    "text": "throughout the day or at night to short we have integrity so like oh we don't have like an employee referenced who",
    "start": "3135070",
    "end": "3140910"
  },
  {
    "text": "doesn't exist that creates alerts opens Pedro Duty tickets or reports people in slack so we",
    "start": "3140910",
    "end": "3146610"
  },
  {
    "text": "monitor it you know kind of app to the fact after the date has been created all",
    "start": "3146610",
    "end": "3154770"
  },
  {
    "text": "the manual intervention anyway because your ETL would reject yeah you're gonna have it either way right either makes it",
    "start": "3154770",
    "end": "3161370"
  },
  {
    "text": "in or sitting then some reject you stuff",
    "start": "3161370",
    "end": "3189690"
  },
  {
    "text": "and second of all but we're just some of the biggest problems you guys face and how did you overcome it when migrating",
    "start": "3189690",
    "end": "3196350"
  },
  {
    "text": "from Tara data to your Amazon solution stuff yeah so it's a good question so it",
    "start": "3196350",
    "end": "3203160"
  },
  {
    "text": "took us about a year with an asterisk and that is only because we it took a little bit longer but we had a hiatus",
    "start": "3203160",
    "end": "3208260"
  },
  {
    "text": "for some other initiative right but all totaled in terms of dedicated person",
    "start": "3208260",
    "end": "3213540"
  },
  {
    "text": "effort to migrate it took us about a year in terms of the impedance that we",
    "start": "3213540",
    "end": "3218880"
  },
  {
    "text": "had between Tara data and redshift we found that redshift was way easier again a lot less platform-specific knowledge",
    "start": "3218880",
    "end": "3225690"
  },
  {
    "text": "than Tara data and data structures much simpler in redshift entire data a",
    "start": "3225690",
    "end": "3231060"
  },
  {
    "text": "teradata likes normalization and all stuff like that so we found everything easier - one",
    "start": "3231060",
    "end": "3236520"
  },
  {
    "text": "thing that we just had to be mindful of moving sequel server and Tara dater to redshift which is something that they've",
    "start": "3236520",
    "end": "3241740"
  },
  {
    "text": "made huge strides on was concurrency you know and and you know just being aware",
    "start": "3241740",
    "end": "3247530"
  },
  {
    "text": "of it we've never had a concurrency issue because of our planning and the way that we use red shift in the way",
    "start": "3247530",
    "end": "3252570"
  },
  {
    "text": "that we use the lakeshore strategy but we were just very mindful of it throughout the process you know Tara",
    "start": "3252570",
    "end": "3258390"
  },
  {
    "text": "data and sequel server or sequel server being a relational database is a different animal so we just thought",
    "start": "3258390",
    "end": "3263940"
  },
  {
    "text": "about it and we never had any problems maybe I have time for maybe one more",
    "start": "3263940",
    "end": "3269640"
  },
  {
    "text": "question and we're happy to take that outside as well but thanks great presentation guys how did you guys",
    "start": "3269640",
    "end": "3275550"
  },
  {
    "text": "decide when to use spectrum versus going ahead and loading that directly into your cluster yeah so that actually came",
    "start": "3275550",
    "end": "3284280"
  },
  {
    "text": "upon came upon me specifically because the analysts were like we want this and",
    "start": "3284280",
    "end": "3290840"
  },
  {
    "text": "the entire process that we had of putting into redshift you know just every time we put something new in there",
    "start": "3290840",
    "end": "3297270"
  },
  {
    "text": "the next question would be this is a great data point can we have it you know for like the past year and so then it",
    "start": "3297270",
    "end": "3303720"
  },
  {
    "text": "would be like oh man we got to go back and back date all this data too and so it just became this process that had",
    "start": "3303720",
    "end": "3309450"
  },
  {
    "text": "something back dated update ETL Script and it was it was something that we were just kind of like there's got to be an",
    "start": "3309450",
    "end": "3314970"
  },
  {
    "text": "easier way to do this and so we explored some options came across back to him and we're like let's just go for it and so we had also come across the park'",
    "start": "3314970",
    "end": "3323220"
  },
  {
    "text": "file format which has done wonders and so we just kind of went for it tested it",
    "start": "3323220",
    "end": "3329010"
  },
  {
    "text": "out we're like let's see if this actually works and it worked beautifully and so since then we've just trying to",
    "start": "3329010",
    "end": "3334590"
  },
  {
    "text": "begin trying to get more and more stuff in there so if you have some clickstream data sources that maybe you want to test",
    "start": "3334590",
    "end": "3340710"
  },
  {
    "text": "out with anything that's mutable really well I imagine you'll find pretty good strides with it like schema mutable I'd",
    "start": "3340710",
    "end": "3347460"
  },
  {
    "text": "say something that evolves over time because like our old pipeline we used to take like our clickstream data we're on",
    "start": "3347460",
    "end": "3352920"
  },
  {
    "text": "a spark job rip it down to like 60 name columns shove it into redshift and like",
    "start": "3352920",
    "end": "3358410"
  },
  {
    "text": "every time they're like oh and attribute dropped off the job would break whenever they want in an attribute they weren't looking at prior we'd have to recast",
    "start": "3358410",
    "end": "3365450"
  },
  {
    "text": "forever like a lot of data so this is just taking that out of the equation we don't have to worry about it at all we",
    "start": "3365450",
    "end": "3371760"
  },
  {
    "text": "just changed a table definition and something magical happens plus it deals with sparse or evolving data so if a",
    "start": "3371760",
    "end": "3378720"
  },
  {
    "text": "column drops out of the file it doesn't care yeah and it's there for historical x' yeah that's the other nice thing too",
    "start": "3378720",
    "end": "3384510"
  },
  {
    "text": "is you can actually plan ahead with spectrum and AWS glue as well so if I know that there's you know a new data",
    "start": "3384510",
    "end": "3390870"
  },
  {
    "text": "point that an app is going to be collecting I can actually set up the data feeds to start passing in that new data point I can tell AWS glue that that",
    "start": "3390870",
    "end": "3397530"
  },
  {
    "text": "new data point is going to come and then even before it arrives the column is",
    "start": "3397530",
    "end": "3402870"
  },
  {
    "text": "just empty so let's say I'm going on vacation or something I've set this up and something goes live while I'm out of",
    "start": "3402870",
    "end": "3408900"
  },
  {
    "text": "office then that data automatically starts coming in so you can even plan ahead before this stuff makes it in so",
    "start": "3408900",
    "end": "3415140"
  },
  {
    "text": "once it starts being collected it automatically is projected into a spectrum as well all right",
    "start": "3415140",
    "end": "3423060"
  },
  {
    "text": "I think there's any more questions I think we're I so I'm happy to meet in the hallway but thanks Ryan Elliot for",
    "start": "3423060",
    "end": "3430230"
  },
  {
    "text": "your time and thanks again for participating and again if you're just more on details I have my own session at",
    "start": "3430230",
    "end": "3435270"
  },
  {
    "text": "five o'clock so I might be the only one in the room at that hour but if you want to come by again otherwise thanks to everyone attending and thanks for a",
    "start": "3435270",
    "end": "3441180"
  },
  {
    "text": "great presentation",
    "start": "3441180",
    "end": "3443839"
  }
]