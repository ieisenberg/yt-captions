[
  {
    "text": "good afternoon you guys excited yeah let",
    "start": "2330",
    "end": "7680"
  },
  {
    "text": "me hear something here okay good it's good it's the first day of reinvent and",
    "start": "7680",
    "end": "13460"
  },
  {
    "text": "you know there's always a lot of energy it's a nice big crowd so I appreciate",
    "start": "13460",
    "end": "20220"
  },
  {
    "text": "you guys all coming here and spending the afternoon afternoon with us it's a little hot so take this off my name is",
    "start": "20220",
    "end": "31140"
  },
  {
    "text": "Michael Michael Shaw and I'm the engineering manager for AWS glue and",
    "start": "31140",
    "end": "36860"
  },
  {
    "text": "today I'm going to be speaking to you about how you can build analytics pipelines with glue and then I have with",
    "start": "36860",
    "end": "45390"
  },
  {
    "text": "me my co-presenter Arrupe who's the VP of engineering at realtor.com and he'll",
    "start": "45390",
    "end": "51480"
  },
  {
    "text": "be talking about the success they've had in putting glue and production in their organization so quickly I'm going to",
    "start": "51480",
    "end": "61620"
  },
  {
    "text": "give you an overview of glue to start and this is a 300 level presentation so",
    "start": "61620",
    "end": "67049"
  },
  {
    "text": "I'm actually going to get into some of the details on the things that we've done over the last year year and a half",
    "start": "67049",
    "end": "72420"
  },
  {
    "text": "to improve glue as well as the new features that we've added and then I'll",
    "start": "72420",
    "end": "77670"
  },
  {
    "text": "hand it over to a Rupe to talk about how they've put it into practice all right",
    "start": "77670",
    "end": "83820"
  },
  {
    "text": "so let's get into it a year ago we introduced glue to the family of AWS",
    "start": "83820",
    "end": "91320"
  },
  {
    "text": "services we introduced it as a fully managed server 'less extract transform",
    "start": "91320",
    "end": "97110"
  },
  {
    "text": "and load service or ETL for short now I",
    "start": "97110",
    "end": "102299"
  },
  {
    "text": "know that's a mouthful and there are plenty of other ETL services out there today and were at the time so why did we",
    "start": "102299",
    "end": "108600"
  },
  {
    "text": "build it well it turned out that even though there's so many ETL services out",
    "start": "108600",
    "end": "113729"
  },
  {
    "text": "there and there were very few that were focused on you our customers that were",
    "start": "113729",
    "end": "119460"
  },
  {
    "text": "developers in the audience we as developers found that the other tool",
    "start": "119460",
    "end": "124680"
  },
  {
    "text": "weren't really there for what we needed and you guys agreed and so we kicked",
    "start": "124680",
    "end": "130020"
  },
  {
    "text": "this off about a year ago at that time we had about thousands of customers and running thousands of jobs",
    "start": "130020",
    "end": "136420"
  },
  {
    "text": "daily and this was the slide that I used last year describing the lighthouse",
    "start": "136420",
    "end": "143140"
  },
  {
    "text": "customers that braved the early days of glue with us both feet in first today",
    "start": "143140",
    "end": "151720"
  },
  {
    "text": "we've grown much beyond that this is just a smattering of customers that we",
    "start": "151720",
    "end": "158020"
  },
  {
    "text": "have in production today there's a lot more and you our customers have helped",
    "start": "158020",
    "end": "164320"
  },
  {
    "text": "us grow on the left-hand side what you see is the number of jobs and crawlers",
    "start": "164320",
    "end": "169420"
  },
  {
    "text": "that run on a monthly basis since we launched and glue and you can see that their workloads have been exploding and",
    "start": "169420",
    "end": "176080"
  },
  {
    "text": "on the right-hand side which you see is a number of regions that glue is in we're at 12 regions today and many more",
    "start": "176080",
    "end": "181900"
  },
  {
    "text": "to come by the end of the year so besides just running glue and making",
    "start": "181900",
    "end": "188560"
  },
  {
    "text": "sure that it scales for you and fixing all the bugs we've been busy listening to you as well we built a number of",
    "start": "188560",
    "end": "195970"
  },
  {
    "text": "major features over the last year not just adding you know incremental things",
    "start": "195970",
    "end": "201130"
  },
  {
    "text": "but brand new things that make it easy for you to run your ETL jobs we do a lot",
    "start": "201130",
    "end": "206260"
  },
  {
    "text": "more of the undifferentiated heavy lifting that we always intended to do so",
    "start": "206260",
    "end": "212350"
  },
  {
    "text": "let's take a look at what glue actually looks like it's got three main",
    "start": "212350",
    "end": "217990"
  },
  {
    "text": "components okay the first component is data catalog",
    "start": "217990",
    "end": "223709"
  },
  {
    "text": "it's a metadata store a centralized metadata store for all your analytics",
    "start": "223709",
    "end": "229239"
  },
  {
    "text": "data in AWS and beyond with catalogs you",
    "start": "229239",
    "end": "234340"
  },
  {
    "text": "have crawlers we provide crawlers that actually open up the data sets that you have crack them open extract the",
    "start": "234340",
    "end": "241030"
  },
  {
    "text": "metadata extract the structures extract the organization and then actually load all of that metadata into the catalog",
    "start": "241030",
    "end": "248590"
  },
  {
    "text": "for you so you don't have to do it the catalog is hive minister compatible so",
    "start": "248590",
    "end": "255850"
  },
  {
    "text": "other services analytic services like redshift and Athena and EMR can",
    "start": "255850",
    "end": "262450"
  },
  {
    "text": "integrate with the catalog to understand what there query and actually go after the data and query it so you can use it across a",
    "start": "262450",
    "end": "269450"
  },
  {
    "text": "number of different services another major component for Glu is its",
    "start": "269450",
    "end": "274670"
  },
  {
    "text": "serverless job execution system okay at the core and this is a key value",
    "start": "274670",
    "end": "281060"
  },
  {
    "text": "proposition that we have at the core of our job execution system is an open",
    "start": "281060",
    "end": "286130"
  },
  {
    "text": "platform Apache spark that runs all of your jobs you submit your ETL jobs",
    "start": "286130",
    "end": "292010"
  },
  {
    "text": "written in Python or Scala just give us the job we provision all the machinery",
    "start": "292010",
    "end": "298970"
  },
  {
    "text": "that's necessary to run those jobs we configure the network's we spin up the",
    "start": "298970",
    "end": "304520"
  },
  {
    "text": "machines we shut them down and all you do is pay for the time that the job ran okay and if you don't like writing code",
    "start": "304520",
    "end": "312710"
  },
  {
    "text": "no problem we actually give you a tools that based on the the data that you have",
    "start": "312710",
    "end": "319580"
  },
  {
    "text": "in the data catalog won't automatically generate the code that you need to do transformations from one source format",
    "start": "319580",
    "end": "326510"
  },
  {
    "text": "to another one source to another destination the third component of glue",
    "start": "326510",
    "end": "333080"
  },
  {
    "text": "is its orchestration system okay here what we give you is a system that allows",
    "start": "333080",
    "end": "338690"
  },
  {
    "text": "you to stitch together multiple jobs to accomplish a larger task you can stitch them together in a linear fashion or any",
    "start": "338690",
    "end": "345380"
  },
  {
    "text": "dag any arbitrary directed acyclic graph of jobs that you want to put together we",
    "start": "345380",
    "end": "352610"
  },
  {
    "text": "allow you to monitor this we check for failures we do retries and we also have",
    "start": "352610",
    "end": "359060"
  },
  {
    "text": "ways of integrating the orchestration system with external services with an AWS and beyond so that in a nutshell is",
    "start": "359060",
    "end": "366950"
  },
  {
    "text": "what glue I was was offered as and has continued to offer ass but beyond just",
    "start": "366950",
    "end": "374360"
  },
  {
    "text": "the ETL components our customers you have used glues and glue in ways that we didn't anticipate to begin with so we",
    "start": "374360",
    "end": "382820"
  },
  {
    "text": "originally gave you the api's that were needed to actually stitch together your own notebooks sitting on your laptops or",
    "start": "382820",
    "end": "388250"
  },
  {
    "text": "your desktops to the underlying serverless infrastructure that we had we",
    "start": "388250",
    "end": "394310"
  },
  {
    "text": "found you using that a lot and so what we did is we actually added integration with Amazon saij maker notebooks and so now if you",
    "start": "394310",
    "end": "401310"
  },
  {
    "text": "want to do data science and data exploration you can do it seamlessly without ever spinning up a server",
    "start": "401310",
    "end": "407960"
  },
  {
    "text": "completely managed by AWS and here's",
    "start": "407960",
    "end": "414810"
  },
  {
    "text": "what you're saying okay so there are things that we anticipated that you would do here's what's emerging",
    "start": "414810",
    "end": "421319"
  },
  {
    "text": "and based on the uses that we see for for glue one major use case that is",
    "start": "421319",
    "end": "426509"
  },
  {
    "text": "emerged is the one around building and managing data Lakes data lakes are a centralized repository",
    "start": "426509",
    "end": "432990"
  },
  {
    "text": "of structured and unstructured data that companies are putting together to manage",
    "start": "432990",
    "end": "438030"
  },
  {
    "text": "all of their data and analytics in one place we see many of our customers",
    "start": "438030",
    "end": "443460"
  },
  {
    "text": "managing this data with AWS glue and then analyzing that data with one of",
    "start": "443460",
    "end": "448590"
  },
  {
    "text": "many different analytic services in AWS they use it with Amazon EMR to query the",
    "start": "448590",
    "end": "454259"
  },
  {
    "text": "data in hive or spark they use it to load data into redshift and run sequel",
    "start": "454259",
    "end": "459659"
  },
  {
    "text": "queries customers are also telling us that glue is cost-effective it's pretty",
    "start": "459659",
    "end": "465780"
  },
  {
    "text": "simple to get a small team of engineers to get it this up and running at the fraction of a cost of what it would take",
    "start": "465780",
    "end": "471960"
  },
  {
    "text": "with a traditional traditional ETL tool they're also telling us that glue is fast faster than a lot of the",
    "start": "471960",
    "end": "477840"
  },
  {
    "text": "traditional ETL tools so how do we get there well we've been busy we've been",
    "start": "477840",
    "end": "485219"
  },
  {
    "text": "busy and making improvements in three major areas the first is around",
    "start": "485219",
    "end": "490590"
  },
  {
    "text": "scalability and performance and this is a scalability and performance of the crawler and the ETL job system and I'll",
    "start": "490590",
    "end": "496710"
  },
  {
    "text": "dig into that next the second thing we've done is we've added a number of",
    "start": "496710",
    "end": "501719"
  },
  {
    "text": "API s and to open up what's going on under under the hood in our jobs",
    "start": "501719",
    "end": "507990"
  },
  {
    "text": "we're actually revealing metrics on how the jobs are run so that you can profile them and optimize them and then finally",
    "start": "507990",
    "end": "515610"
  },
  {
    "text": "we've added a number of interoperability features to make glue work well with all",
    "start": "515610",
    "end": "522570"
  },
  {
    "text": "of the other services outside of glue and today I'm going to be announcing a new job type called Python shell that",
    "start": "522570",
    "end": "529949"
  },
  {
    "text": "actually enhances the Interop ability that glue provides alright so",
    "start": "529949",
    "end": "537029"
  },
  {
    "text": "let's get to let's get started let's talk about a simple example that I'm going to be using throughout the talk",
    "start": "537029",
    "end": "543240"
  },
  {
    "text": "okay imagine you have some data let's say it's data like like the github",
    "start": "543240",
    "end": "549120"
  },
  {
    "text": "public timeline data it's the data that's you know publicly available about all the the events that github sees on",
    "start": "549120",
    "end": "557370"
  },
  {
    "text": "the public API is that they have and they make this publicly available what a lot of our customers are doing is taking",
    "start": "557370",
    "end": "563130"
  },
  {
    "text": "this kind of data these are just log you know events and so on and they're",
    "start": "563130",
    "end": "568380"
  },
  {
    "text": "organizing it in s3 inside of buckets and instead of putting all you know hundreds of thousands or millions of",
    "start": "568380",
    "end": "573960"
  },
  {
    "text": "files into one bucket what they're doing is organizing it into directories hierarchies",
    "start": "573960",
    "end": "579029"
  },
  {
    "text": "we call them hive style partitions but really they're just filesystem hierarchies and the reason they do this",
    "start": "579029",
    "end": "584790"
  },
  {
    "text": "is so that they can get to some subsets of the data very quickly so it's easier",
    "start": "584790",
    "end": "590250"
  },
  {
    "text": "to discover the datasets that they care about and what they do is they take the raw data and they transform it somehow",
    "start": "590250",
    "end": "596060"
  },
  {
    "text": "either reducing it or just changing the format into something that's optimized for analytics on the right-hand side in",
    "start": "596060",
    "end": "602430"
  },
  {
    "text": "this particular case we're maintaining the same hierarchy the hierarchy here is in terms of years month and day but you",
    "start": "602430",
    "end": "608730"
  },
  {
    "text": "could use any other hierarchy like a region for example and the format in the",
    "start": "608730",
    "end": "615420"
  },
  {
    "text": "target format is park' which is typically optimized to typically useful for analytics so what happens if you try",
    "start": "615420",
    "end": "623520"
  },
  {
    "text": "to crawl the source data here well here's what it looks like very quickly what you can see is that the crawlers",
    "start": "623520",
    "end": "629700"
  },
  {
    "text": "pick up the top-level attributes in your data as well as all the various nested attributes that you have in there in",
    "start": "629700",
    "end": "635970"
  },
  {
    "text": "this particular case there are hundreds of nested fields in there you don't have to figure that out it figures it out for",
    "start": "635970",
    "end": "642000"
  },
  {
    "text": "you it also keeps track of how the files are grouped into hierarchies in terms of",
    "start": "642000",
    "end": "647700"
  },
  {
    "text": "their partitions and it does that automatically and discovers it and registers it in data catalog well over",
    "start": "647700",
    "end": "656040"
  },
  {
    "text": "the last year we've been hard at work making this much better and more scalable it's crawlers are now seven",
    "start": "656040",
    "end": "663660"
  },
  {
    "text": "times faster than they used to be on average we can run over nearly 900 million files",
    "start": "663660",
    "end": "669470"
  },
  {
    "text": "in a day definitely can handle hundreds of thousands if not millions of",
    "start": "669470",
    "end": "674720"
  },
  {
    "text": "partitions and of course your mileage is going to vary depending on the size and complexity of the data as well as the",
    "start": "674720",
    "end": "681769"
  },
  {
    "text": "partition structures that you have this is what glue ETL scripts look like these",
    "start": "681769",
    "end": "689540"
  },
  {
    "text": "are the generated scripts that come out directly from from the service on the left hand side which you'll see is a",
    "start": "689540",
    "end": "696199"
  },
  {
    "text": "high-level overview of the script and the transforms that are being run on the right hand side is the actual script",
    "start": "696199",
    "end": "702709"
  },
  {
    "text": "that you can either just run or take a look at debug profile modify do whatever",
    "start": "702709",
    "end": "710929"
  },
  {
    "text": "you want with okay there's a lot of boilerplate stuff inside of the script as well as various transformations that",
    "start": "710929",
    "end": "718999"
  },
  {
    "text": "are automatically put in there to do a lot of the cleaning and transformation to get into the target format to",
    "start": "718999",
    "end": "728209"
  },
  {
    "text": "understand what the script is doing it's important to actually understand how the",
    "start": "728209",
    "end": "733220"
  },
  {
    "text": "system is built so glue is actually built on top of an apache spark core",
    "start": "733220",
    "end": "738949"
  },
  {
    "text": "okay at the bottom and Apache spark which you get our rdd's or resilient",
    "start": "738949",
    "end": "745369"
  },
  {
    "text": "data sets or resilient distributed data sets these are basically data structures that allow you to put data in them and",
    "start": "745369",
    "end": "751309"
  },
  {
    "text": "then run operations over them in a fault tolerant way Apache spark itself has",
    "start": "751309",
    "end": "757189"
  },
  {
    "text": "built an additional data structure on top of this called data frames and data",
    "start": "757189",
    "end": "762829"
  },
  {
    "text": "frames are optimized for doing sequel like analytics okay that's what spark is",
    "start": "762829",
    "end": "768129"
  },
  {
    "text": "what we've done is we've actually added a parallel set of infrastructure or code",
    "start": "768129",
    "end": "774790"
  },
  {
    "text": "libraries in particular where we have a parallel data structure called dynamic",
    "start": "774790",
    "end": "781339"
  },
  {
    "text": "frames that are optimized for ETL and then a bunch of transforms on top of",
    "start": "781339",
    "end": "786559"
  },
  {
    "text": "dynamic frames that make it easy to do the data cleaning and restructuring that you need so let's dig into that a little",
    "start": "786559",
    "end": "793339"
  },
  {
    "text": "bit and see how that helps date of data frames and dynamic frames data frames a basic core data structure",
    "start": "793339",
    "end": "800980"
  },
  {
    "text": "for spark sequel they're like structured tables we all know what tables look like you need a schema upfront each row has",
    "start": "800980",
    "end": "808660"
  },
  {
    "text": "the same structure and when you know that every row has the same structure what you can do is you can optimize the",
    "start": "808660",
    "end": "814749"
  },
  {
    "text": "layout of that structure for sequel like analytics it works really well makes it go super fast if on the other hand what",
    "start": "814749",
    "end": "823480"
  },
  {
    "text": "you're doing is ETL this structure actually gets in the way especially if you're looking at unclean incomplete",
    "start": "823480",
    "end": "829749"
  },
  {
    "text": "semi structured data that's often coming out of logs for example Avro data or",
    "start": "829749",
    "end": "834790"
  },
  {
    "text": "JSON data or things that just you know didn't anticipate recording but you recorded and now you got to go analyze",
    "start": "834790",
    "end": "840519"
  },
  {
    "text": "it so what dynamic frames do is that they actually store the structure of",
    "start": "840519",
    "end": "846100"
  },
  {
    "text": "each record with the record itself so every record can be different from the previous there's no need to have a",
    "start": "846100",
    "end": "852850"
  },
  {
    "text": "schema up front there's no need to do a pass over the data to compute the schema before you put it all in you simply",
    "start": "852850",
    "end": "858610"
  },
  {
    "text": "start sucking in the data and start running operations over it and that's why it's useful for ETL it's gonna go",
    "start": "858610",
    "end": "864399"
  },
  {
    "text": "slower for sequel a lot slower that's not what it's intended for and these things are not competing with one",
    "start": "864399",
    "end": "870399"
  },
  {
    "text": "another they actually go hand-in-hand they're complementary you can take a dynamic frame and turn it into a data",
    "start": "870399",
    "end": "875679"
  },
  {
    "text": "frame and vice versa so how fast are",
    "start": "875679",
    "end": "881769"
  },
  {
    "text": "these things well when we initially started they were good you know they were we were really designing it to do",
    "start": "881769",
    "end": "888129"
  },
  {
    "text": "one pass over the data instead of multiple passes over the data but they're now even gooder they're much",
    "start": "888129",
    "end": "893800"
  },
  {
    "text": "faster than they used to be about four times faster your since since we first launched the service you can run over a",
    "start": "893800",
    "end": "900699"
  },
  {
    "text": "terabyte in under 1.5 hours with a 10 DP you cluster dpu is our unit of scaling",
    "start": "900699",
    "end": "907269"
  },
  {
    "text": "it's our unit of capacity it corresponds to four V CPUs and sixteen gigabytes of",
    "start": "907269",
    "end": "915160"
  },
  {
    "text": "memory okay and of course if you increase the number of DP you still go faster if you reduce the number of deep",
    "start": "915160",
    "end": "922209"
  },
  {
    "text": "use it's going to go slower and it's the runtimes are going to vary based on the data formats you're reading and writing",
    "start": "922209",
    "end": "927670"
  },
  {
    "text": "in this particular case we reading JSON and writing part a is in the example and of course it's going to",
    "start": "927670",
    "end": "934089"
  },
  {
    "text": "vary depending on what you're doing in the script and the complexity of that script here's another cool thing that",
    "start": "934089",
    "end": "941019"
  },
  {
    "text": "we've done with dynamic frames if your data is organized into these hive style partitions you don't need to scan over",
    "start": "941019",
    "end": "947949"
  },
  {
    "text": "your data every time you run a query if you only care about a subset of the data let's imagine you only care about one",
    "start": "947949",
    "end": "954189"
  },
  {
    "text": "month's worth of data or one day's worth of data you can just basically specify oops you can specify which month year or",
    "start": "954189",
    "end": "964360"
  },
  {
    "text": "which partitions you care about with a like a where clause effectively that gets pushed down into the infrastructure",
    "start": "964360",
    "end": "970629"
  },
  {
    "text": "we only read over the files that are necessary and then scan those so on the right hand side what you see here is an",
    "start": "970629",
    "end": "977410"
  },
  {
    "text": "experiment where we run over a different number of months that are covered in that time line and you see that the",
    "start": "977410",
    "end": "984249"
  },
  {
    "text": "performance scales linearly what this shows you is that there's very little overhead and being able to push these",
    "start": "984249",
    "end": "989470"
  },
  {
    "text": "predicates down the predicates are the filter clauses all right so that's the",
    "start": "989470",
    "end": "997870"
  },
  {
    "text": "scalability of performance of our system we continue to push on the scalability of our system and make it better and and",
    "start": "997870",
    "end": "1005639"
  },
  {
    "text": "bigger and faster but that's just not enough turns out that you know you give",
    "start": "1005639",
    "end": "1012990"
  },
  {
    "text": "us a script sometimes it just doesn't perform the way you expect it to okay this was especially frustrating when we",
    "start": "1012990",
    "end": "1018749"
  },
  {
    "text": "first started where you got very little information other than it worked or it didn't work and so we listened to you we",
    "start": "1018749",
    "end": "1025319"
  },
  {
    "text": "opened up what was happening under the covers under the hood and we're now",
    "start": "1025319",
    "end": "1030589"
  },
  {
    "text": "sending back metrics that tell you how the job is performing to understand what",
    "start": "1030589",
    "end": "1035730"
  },
  {
    "text": "these metrics - it's important to understand how glue actually does its execution glues execution is based on",
    "start": "1035730",
    "end": "1042659"
  },
  {
    "text": "Apache spark okay what glue does is it takes these Apache spark scripts they're",
    "start": "1042659",
    "end": "1048329"
  },
  {
    "text": "written in Python or Scott Scala and it sends them to the driver the driver then",
    "start": "1048329",
    "end": "1053820"
  },
  {
    "text": "basically breaks up these scripts into stages stages are basically you know",
    "start": "1053820",
    "end": "1059159"
  },
  {
    "text": "parts of the program that need to be run one after another each stage is then broken up into",
    "start": "1059159",
    "end": "1066389"
  },
  {
    "text": "quote-unquote shards or partitions it's different from the hive style partitions I was talking about earlier but just",
    "start": "1066389",
    "end": "1071919"
  },
  {
    "text": "shards or partitions you take a table for example and break it up into a hundred partitions or take a table and",
    "start": "1071919",
    "end": "1077980"
  },
  {
    "text": "break it up into a thousand partitions just depending on you know how many partitions you have it's a data parallel",
    "start": "1077980",
    "end": "1085899"
  },
  {
    "text": "processing system so these shards can be processed you know all at the same time in parallel for each stage and each",
    "start": "1085899",
    "end": "1094179"
  },
  {
    "text": "partition spark has a task and the job of the driver is to allocate these tasks",
    "start": "1094179",
    "end": "1101649"
  },
  {
    "text": "to physical nodes or physical executors that actually run these tasks so the job",
    "start": "1101649",
    "end": "1106899"
  },
  {
    "text": "here for the driver is a taste these take these tasks that are all kind of ready to be executed and pack them onto",
    "start": "1106899",
    "end": "1113200"
  },
  {
    "text": "executors as soon as they come in okay so that's the scheduling that it does for every dpu that you ask to that you",
    "start": "1113200",
    "end": "1122860"
  },
  {
    "text": "ask glue to run a job for we allocate two executors it's just a standard",
    "start": "1122860",
    "end": "1128259"
  },
  {
    "text": "mapping that we have and what you'll notice here is that the throughput of our system the sort of optimal",
    "start": "1128259",
    "end": "1134440"
  },
  {
    "text": "throughput of our system is limited by the number of partitions or shards that you have in your program okay or in your",
    "start": "1134440",
    "end": "1141970"
  },
  {
    "text": "data set so how do you run these metrics well you simply go to the console and",
    "start": "1141970",
    "end": "1149169"
  },
  {
    "text": "say turn on the metrics you can do this also through the CLI or the SDK by passing in a flag - enable metrics",
    "start": "1149169",
    "end": "1156730"
  },
  {
    "text": "pretty simple we actually derive our metrics from the underlying Apache spark",
    "start": "1156730",
    "end": "1163570"
  },
  {
    "text": "metrics that the cluster gives us we have metrics on the driver as well as",
    "start": "1163570",
    "end": "1169360"
  },
  {
    "text": "per executors we return aggregate metrics as well as instantaneous metrics",
    "start": "1169360",
    "end": "1174399"
  },
  {
    "text": "as the job is running we send those metrics to Amazon Cloud Watch on it on a",
    "start": "1174399",
    "end": "1180519"
  },
  {
    "text": "periodic basis roughly every 30 seconds and the kinds of things that we send back are memory usage and the amount of",
    "start": "1180519",
    "end": "1186700"
  },
  {
    "text": "data that an executor read or wrote the CPU load on a driver or execute",
    "start": "1186700",
    "end": "1192490"
  },
  {
    "text": "the amount of bytes that were shuffled or sent back and forth as they were communicating among executors we also",
    "start": "1192490",
    "end": "1199090"
  },
  {
    "text": "send back the number of quote/unquote needed executors these are basically the amount of tasks they're you know in",
    "start": "1199090",
    "end": "1205750"
  },
  {
    "text": "flight that could be scheduled if you had those number of executors so it",
    "start": "1205750",
    "end": "1212440"
  },
  {
    "text": "gives you an idea of how far you can scale the system for your particular job",
    "start": "1212440",
    "end": "1217860"
  },
  {
    "text": "so let's take two examples on how you know these metrics can help a simple",
    "start": "1217860",
    "end": "1223270"
  },
  {
    "text": "example here is when you're processing lots of small files with just Sage with traditional regular data frames this",
    "start": "1223270",
    "end": "1229300"
  },
  {
    "text": "happens for example if you're taking streaming data and you're collecting it say through Kinesis or some other",
    "start": "1229300",
    "end": "1235929"
  },
  {
    "text": "streaming system and storing these things into files often these files will be you know you know some portion of",
    "start": "1235929",
    "end": "1242650"
  },
  {
    "text": "time so they'll be 0 or 0 length or small files and then suddenly you'll get a burst of activity and then they'll get",
    "start": "1242650",
    "end": "1248440"
  },
  {
    "text": "small again if you have in a hundreds of thousands or millions of files it turns",
    "start": "1248440",
    "end": "1254440"
  },
  {
    "text": "out the way data frames work that there's a single task per file and there's metadata or overhead that you",
    "start": "1254440",
    "end": "1262179"
  },
  {
    "text": "have to keep track of the driver has to keep track of per task and so what ends up happening is that all this metadata",
    "start": "1262179",
    "end": "1269140"
  },
  {
    "text": "gets sent to the driver the driver can't keep track of it and immediately you can",
    "start": "1269140",
    "end": "1274480"
  },
  {
    "text": "see on the graph here is that the driver memory goes beyond a critical boundary and then you run into an out of memory",
    "start": "1274480",
    "end": "1280000"
  },
  {
    "text": "error you can see similar things on executors when executors run into out of memory errors and you can use this to go",
    "start": "1280000",
    "end": "1286480"
  },
  {
    "text": "and figure out where that out of memory happened and why it might have happened in this particular case it's because the",
    "start": "1286480",
    "end": "1292809"
  },
  {
    "text": "driver was overwhelmed well no problem turn on dynamic frames instead you know",
    "start": "1292809",
    "end": "1300220"
  },
  {
    "text": "we can actually read all the same formats that spark Ken data frames can and dynamic frames have this ability to",
    "start": "1300220",
    "end": "1306220"
  },
  {
    "text": "automatically group lots of files into fewer tasks that automation allows the",
    "start": "1306220",
    "end": "1313900"
  },
  {
    "text": "driver to keep the number of tasks small enough so that it fits into memory and",
    "start": "1313900",
    "end": "1319170"
  },
  {
    "text": "your job now finishes actually rather than just you know breaking and shutting",
    "start": "1319170",
    "end": "1324880"
  },
  {
    "text": "down with memory are if you don't like the way dynamic frames does the grouping you",
    "start": "1324880",
    "end": "1330050"
  },
  {
    "text": "still run into out of memory hours you can adjust the grouping parameters yourself by just right it going into the code and and playing around with it",
    "start": "1330050",
    "end": "1338440"
  },
  {
    "text": "another example is optimizing parallelism with with with glue metrics",
    "start": "1339040",
    "end": "1346160"
  },
  {
    "text": "in this particular case what we're doing is we're processing large gzip files and B zip is a format that's splittable what",
    "start": "1346160",
    "end": "1354200"
  },
  {
    "text": "that means is that you can actually index into a b zip file into the middle of it and read some portions of it so",
    "start": "1354200",
    "end": "1360710"
  },
  {
    "text": "dynamic frames will automatically kind of index into it and depending on you know how many executors are available",
    "start": "1360710",
    "end": "1366050"
  },
  {
    "text": "and in this particular case there's two metrics that are worth noting there's the redline this is you know run",
    "start": "1366050",
    "end": "1372920"
  },
  {
    "text": "with ten dpu that tells you the maximum number of executors that the system has you said I want to you know run this",
    "start": "1372920",
    "end": "1378920"
  },
  {
    "text": "thing with ten GPU you get something like 17 executors there's some number of executors that are overhead and the",
    "start": "1378920",
    "end": "1386570"
  },
  {
    "text": "Green Line is the instantaneous number of executors that could be used at any particular time in the program and you",
    "start": "1386570",
    "end": "1393200"
  },
  {
    "text": "can see it going up and down the interesting thing here is that you know when you run it at 10 DP you there's",
    "start": "1393200",
    "end": "1399110"
  },
  {
    "text": "actually a lot more executors that it could use that are just waiting there's are tasks that you know executors that",
    "start": "1399110",
    "end": "1406040"
  },
  {
    "text": "are that it could use because there's a bunch of tasks that are just waiting to be scheduled and so you could actually",
    "start": "1406040",
    "end": "1411980"
  },
  {
    "text": "get this system to go run a lot faster if you bumped up the number of GPUs so",
    "start": "1411980",
    "end": "1420980"
  },
  {
    "text": "if you bump it up to 15 DP yous now all of a sudden the number of active executors number of executors that are",
    "start": "1420980",
    "end": "1427160"
  },
  {
    "text": "actually running with matches or tracks very closely the number of you know",
    "start": "1427160",
    "end": "1433220"
  },
  {
    "text": "potential executors that you would need to run the program so going beyond 15 DP you isn't going to again you get you",
    "start": "1433220",
    "end": "1439370"
  },
  {
    "text": "additional performance improvement the point here that I'm trying to make is before all these metrics were there it",
    "start": "1439370",
    "end": "1445040"
  },
  {
    "text": "was just basically trial and error you'd run a job you'd waste some cash you know",
    "start": "1445040",
    "end": "1450230"
  },
  {
    "text": "you'd run it again with more DP you you'd see if it helped if not you'd eventually do a binary search and you",
    "start": "1450230",
    "end": "1455570"
  },
  {
    "text": "know settle on where it ought to be and who knows how things would change as your datasets changed now you actually actually can see what's going on",
    "start": "1455570",
    "end": "1462080"
  },
  {
    "text": "underneath and adjust it on the fly as you need all right so that's some of the",
    "start": "1462080",
    "end": "1470150"
  },
  {
    "text": "cool stuff that we have there are other metrics and other use cases around you can do keep track of you know how much",
    "start": "1470150",
    "end": "1476120"
  },
  {
    "text": "data is sent across the network and so on to debug other kinds of problems that",
    "start": "1476120",
    "end": "1481520"
  },
  {
    "text": "you run into when you're running jobs but I want to switch focus now to talk about interoperability first let's talk",
    "start": "1481520",
    "end": "1489320"
  },
  {
    "text": "about orchestration so when we launched a year ago we said ok you got jobs you",
    "start": "1489320",
    "end": "1495170"
  },
  {
    "text": "have triggers you can compose jobs using triggers so when a job completes you",
    "start": "1495170",
    "end": "1500930"
  },
  {
    "text": "trigger another job or you can trigger another set of jobs and so you can create a dag of jobs and dependencies",
    "start": "1500930",
    "end": "1507290"
  },
  {
    "text": "and build very complex flows so from our experience you know people are used to",
    "start": "1507290",
    "end": "1513080"
  },
  {
    "text": "building lots of complex flows to get complex tasks done for example you know handling your daily analytics pipeline",
    "start": "1513080",
    "end": "1519260"
  },
  {
    "text": "typically involves dozens of tasks right in practice",
    "start": "1519260",
    "end": "1525590"
  },
  {
    "text": "we didn't give you enough controls to actually build any complex workflows what we saw are that customers were you",
    "start": "1525590",
    "end": "1533750"
  },
  {
    "text": "know transforming data sets from one format to another and running crawlers",
    "start": "1533750",
    "end": "1539080"
  },
  {
    "text": "to keep track of what was added and what was deleted from the source and then",
    "start": "1539080",
    "end": "1544430"
  },
  {
    "text": "when it gets to the destination what was added and what was deleted and there was no good way to actually stitch this",
    "start": "1544430",
    "end": "1550160"
  },
  {
    "text": "stuff together and so what they did was they used schedules to say ok every five minutes I'm gonna run a crawl I'm gonna",
    "start": "1550160",
    "end": "1556100"
  },
  {
    "text": "hope that in those five minutes I get all the new data that we saw crawls typically take four or five minutes so",
    "start": "1556100",
    "end": "1562310"
  },
  {
    "text": "five minutes later I'm gonna run a job and when I run that job that job typically takes 10 to 20 minutes so you",
    "start": "1562310",
    "end": "1568820"
  },
  {
    "text": "know 20 minutes after that I'm gonna run another crawl and then you know I'll be ready to you know send my reports to you",
    "start": "1568820",
    "end": "1575480"
  },
  {
    "text": "know Athena or redshift and run those reports and they put a lot of slop in in between to deal with you know timing",
    "start": "1575480",
    "end": "1581690"
  },
  {
    "text": "issues and of course if you had anything that went poorly you know god help you because you couldn't fix this right",
    "start": "1581690",
    "end": "1589420"
  },
  {
    "text": "so we added a lot more building blocks to orchestration so an orchestration we have sort of three main entities",
    "start": "1589669",
    "end": "1596039"
  },
  {
    "text": "crawlers jobs and triggers that will that's what we started with and we had schedules we added a lot more event",
    "start": "1596039",
    "end": "1603029"
  },
  {
    "text": "types and the ability to integrate with external services we also added more",
    "start": "1603029",
    "end": "1610679"
  },
  {
    "text": "control flow mechanisms in particular more complicated conditions that tell you which direction you can send the",
    "start": "1610679",
    "end": "1616740"
  },
  {
    "text": "data flow and we also added more ways of controlling how to how to deal with",
    "start": "1616740",
    "end": "1623879"
  },
  {
    "text": "delays and timeouts and jobs so to integrate externally with you know",
    "start": "1623879",
    "end": "1632129"
  },
  {
    "text": "other services we actually send notifications when a dot job is complete or crawler is complete into Amazon Cloud",
    "start": "1632129",
    "end": "1640350"
  },
  {
    "text": "Watch we publish these notifications which will then trigger Amazon Cloud",
    "start": "1640350",
    "end": "1645419"
  },
  {
    "text": "watch events and you can use those events to then control downstream workflows we added two new condition",
    "start": "1645419",
    "end": "1653370"
  },
  {
    "text": "types any and all any is basically an or a bunch of upstream jobs get finished",
    "start": "1653370",
    "end": "1659730"
  },
  {
    "text": "and if any of them finish you can then trigger the trigger which then runs a workflow for a downstream job all is",
    "start": "1659730",
    "end": "1667500"
  },
  {
    "text": "basically an and all of the upstream you know jobs have to finish before the",
    "start": "1667500",
    "end": "1673470"
  },
  {
    "text": "trigger condition is met in which case you can then run the downstream workflow",
    "start": "1673470",
    "end": "1678889"
  },
  {
    "text": "we also added States for failures for jobs not just completions because we all",
    "start": "1678889",
    "end": "1684990"
  },
  {
    "text": "know that jobs do more than just complete so we notify you when things fail and there's an event for that when",
    "start": "1684990",
    "end": "1692610"
  },
  {
    "text": "you purposely stopped a job when you hit a timeout we now allow you to control",
    "start": "1692610",
    "end": "1697710"
  },
  {
    "text": "the job timeout value before it was just set for you and we have a new type of",
    "start": "1697710",
    "end": "1702960"
  },
  {
    "text": "notification where you can set a delay you know I expect this job to run for 15",
    "start": "1702960",
    "end": "1709200"
  },
  {
    "text": "minutes and if it goes beyond that please notify me so then I can run another workflow for it to compensate",
    "start": "1709200",
    "end": "1714330"
  },
  {
    "text": "for the fact that this is running too slow so you can do a lot of things with all these things with all these new",
    "start": "1714330",
    "end": "1720629"
  },
  {
    "text": "features that we've added most importantly you can run the standard workflow and that we hope that you would that we saw",
    "start": "1720629",
    "end": "1728730"
  },
  {
    "text": "coming up so as new data arrives you can trigger a lambda the lambda will then start the crawl after you start the",
    "start": "1728730",
    "end": "1736140"
  },
  {
    "text": "crawl it runs when it's finished it'll publish an event to cloud watch which reads our triggers another lambda which",
    "start": "1736140",
    "end": "1742830"
  },
  {
    "text": "can then run another job and then you can kind of stitch this together with cloud watch lambda another crawl any",
    "start": "1742830",
    "end": "1748980"
  },
  {
    "text": "subsequent analytics that you have to do and then get your workflow done in a tight deadline in lockstep rather than",
    "start": "1748980",
    "end": "1755400"
  },
  {
    "text": "hoping that it gets done but this is just a very simple example now you can actually build much more complex flows",
    "start": "1755400",
    "end": "1761010"
  },
  {
    "text": "this way another thing that we found that you were doing okay all of this",
    "start": "1761010",
    "end": "1767310"
  },
  {
    "text": "talk is about what you guys were doing and how we're learning from you if you haven't figured it out by now but",
    "start": "1767310",
    "end": "1772380"
  },
  {
    "text": "another thing that we found that you were doing was that you were using Python shell I mean you were using the Python components of SPARC to actual",
    "start": "1772380",
    "end": "1782430"
  },
  {
    "text": "actually control ETL and other operations outside of just spark and",
    "start": "1782430",
    "end": "1788600"
  },
  {
    "text": "this seemed very puzzling I was like why are you doing that well it turns out not all of your ETL in",
    "start": "1788600",
    "end": "1795840"
  },
  {
    "text": "a very complex flow always fits into the SPARC containers right the kinds of things that you want to do it sparks",
    "start": "1795840",
    "end": "1801780"
  },
  {
    "text": "sometimes you want to run a sequel query on redshift something sometimes you might want to run a sequel query on",
    "start": "1801780",
    "end": "1807810"
  },
  {
    "text": "presto and EMR or hive and you know the only thing that you had is a primitive",
    "start": "1807810",
    "end": "1814460"
  },
  {
    "text": "with glue was a spark cluster so we said why don't we give you something where",
    "start": "1814460",
    "end": "1820530"
  },
  {
    "text": "you don't actually have to pay for the cost of a cluster even though it's not doing anything for you so instead we're",
    "start": "1820530",
    "end": "1826920"
  },
  {
    "text": "just giving you a Python shell it's a new job type it's a new cost-effective",
    "start": "1826920",
    "end": "1832230"
  },
  {
    "text": "ETL primitive for small to medium sized tasks where a cluster is just overkill",
    "start": "1832230",
    "end": "1837680"
  },
  {
    "text": "okay people can use this for running sequel based our controlling sequel",
    "start": "1837680",
    "end": "1844680"
  },
  {
    "text": "based ETL so running queries on redshift making sure that they run successfully",
    "start": "1844680",
    "end": "1850170"
  },
  {
    "text": "and then running the next and so on you can use it to build things that are hard",
    "start": "1850170",
    "end": "1855360"
  },
  {
    "text": "to build inside a spark like some connectors for third-party services that have complicated you know API",
    "start": "1855360",
    "end": "1861270"
  },
  {
    "text": "semantics for example if you want to go get data out of SFTP or if you want to go get data out of some you know a web",
    "start": "1861270",
    "end": "1868560"
  },
  {
    "text": "service some software as a service app and whatnot to get that data into s3 and then you can coordinate that you know",
    "start": "1868560",
    "end": "1875010"
  },
  {
    "text": "this Python shell with all the other orchestration features that we have the",
    "start": "1875010",
    "end": "1880230"
  },
  {
    "text": "other thing that you can do with Python shell is run a bunch of medium-sized ml tasks it turns out that Python shell",
    "start": "1880230",
    "end": "1886610"
  },
  {
    "text": "fits that sweet spot for these types of tasks kinds of tasks where you kind of",
    "start": "1886610",
    "end": "1892950"
  },
  {
    "text": "need it a sizable memory but not an infinite amount of memory and you need like a bunch of commute compute but not",
    "start": "1892950",
    "end": "1899610"
  },
  {
    "text": "an infinite amount of compute happens all the time and so here you know here's",
    "start": "1899610",
    "end": "1904710"
  },
  {
    "text": "sort of a perfect sort of a tool for that let me tell you a little bit about",
    "start": "1904710",
    "end": "1912780"
  },
  {
    "text": "the specs for a Python shell Python shell comes with basically a standard",
    "start": "1912780",
    "end": "1917850"
  },
  {
    "text": "Python 2.7 environment I know you're gonna ask me when is Python 3 coming it's coming soon ok but we'll be",
    "start": "1917850",
    "end": "1926160"
  },
  {
    "text": "releasing Python 2 7 first it comes with a bunch of goodies Bodo 3 is there so that you can hint a",
    "start": "1926160",
    "end": "1933510"
  },
  {
    "text": "bunch of AWS ap is a AWS you can use the need the AWS CLI it comes with standard",
    "start": "1933510",
    "end": "1940890"
  },
  {
    "text": "libraries like numpy sy pi and pandas for doing scientific computing machine",
    "start": "1940890",
    "end": "1945900"
  },
  {
    "text": "learning a bunch of libraries to connect to Postgres and redshift and my sequel",
    "start": "1945900",
    "end": "1952350"
  },
  {
    "text": "and so on here's some of the cool things about Python shell 1 even if we haven't",
    "start": "1952350",
    "end": "1957660"
  },
  {
    "text": "don't have one provisioned and ready for you if we have to you know provision it from scratch it still takes only 20 seconds much much cheaper than spinning",
    "start": "1957660",
    "end": "1964800"
  },
  {
    "text": "up a cluster we still support the V configuring to attaching to your V pcs and there's absolutely no runtime limit",
    "start": "1964800",
    "end": "1972240"
  },
  {
    "text": "you can run it for a day a week a month a year it's up to you ok unlike other",
    "start": "1972240",
    "end": "1979850"
  },
  {
    "text": "systems that we have it comes in two different sizes the one dpu size is you",
    "start": "1979850",
    "end": "1987090"
  },
  {
    "text": "know 4 V CPUs and 16 gigabytes of memory this is first sort of those medium-sized tasks and then 116 TPU which gives you",
    "start": "1987090",
    "end": "1994890"
  },
  {
    "text": "one gigabyte of memory this is for your control plane or control flow type tasks it's 44 cents a DP you hour in terms of",
    "start": "1994890",
    "end": "2002180"
  },
  {
    "text": "pricing it's the same pricing that we charge for all of our other job types it's per second billing but here's the",
    "start": "2002180",
    "end": "2007730"
  },
  {
    "text": "most important thing it's a one minute minimum okay not a 10-minute minimum so",
    "start": "2007730",
    "end": "2013400"
  },
  {
    "text": "really therefore a lot of your lightweight tasks you're not paying for you're definitely not paying for the",
    "start": "2013400",
    "end": "2018560"
  },
  {
    "text": "spin-up or the teardown time but you're also not playing any overhead for very short jobs that you have to run and so",
    "start": "2018560",
    "end": "2024440"
  },
  {
    "text": "this will be coming soon right after re-invent I'll be available for all of you to try it out here's a simple",
    "start": "2024440",
    "end": "2032690"
  },
  {
    "text": "example of what you can do with Python shell collaborative filtering so the problem here is you know how do we take",
    "start": "2032690",
    "end": "2040150"
  },
  {
    "text": "how do we take you know a bunch of reviews that we have from Amazon you can actually go get this off the web these",
    "start": "2040150",
    "end": "2047240"
  },
  {
    "text": "are the amazon.com retail customer reviews for a variety of products across the website it says as of 2013 I believe",
    "start": "2047240",
    "end": "2054500"
  },
  {
    "text": "so it's a little old and we took all the reviews from the video category and based on those reviews you know if you",
    "start": "2054500",
    "end": "2060679"
  },
  {
    "text": "get a new customer how do you decide you know what to recommend to them well",
    "start": "2060680",
    "end": "2066080"
  },
  {
    "text": "there's a lot of different ways to do collaborative filtering but a popular way is to actually compute a huge matrix",
    "start": "2066080",
    "end": "2071899"
  },
  {
    "text": "okay and this is why this is a this is a good you know example for a Python show",
    "start": "2071900",
    "end": "2076970"
  },
  {
    "text": "this matrix is in this particular case for just the video category is the customer product matrix every row is a",
    "start": "2076970",
    "end": "2083720"
  },
  {
    "text": "customer every column is a product and every entry is the rating that that customer gave for that particular",
    "start": "2083720",
    "end": "2089990"
  },
  {
    "text": "product right now this is a very noisy and large matrix and what you want to do",
    "start": "2089990",
    "end": "2095240"
  },
  {
    "text": "is sort of boil it down to its essence really you know the major components of",
    "start": "2095240",
    "end": "2100340"
  },
  {
    "text": "where all the the power or the the information in that matrix is and separate that out from all of the noise",
    "start": "2100340",
    "end": "2106460"
  },
  {
    "text": "that's in that matrix a common way to do that is to take that matrix and create a",
    "start": "2106460",
    "end": "2111650"
  },
  {
    "text": "low dimensional or low-rank approximation of that matrix and then with that low-rank approximation use",
    "start": "2111650",
    "end": "2118910"
  },
  {
    "text": "that approximation to predict what a new customer want or could want to would would want",
    "start": "2118910",
    "end": "2125680"
  },
  {
    "text": "to see from the product list that you have the important point here is not the machine learning and the and the and the",
    "start": "2125680",
    "end": "2132100"
  },
  {
    "text": "the transforms the SVD is the transform that does that singular value decomposition the important point is",
    "start": "2132100",
    "end": "2138400"
  },
  {
    "text": "that the SVD requires a bunch of memory and it requires a bunch of CPU and some",
    "start": "2138400",
    "end": "2144460"
  },
  {
    "text": "time to iterate over that to get you the answers and in this case you can actually just you know very quickly get",
    "start": "2144460",
    "end": "2150940"
  },
  {
    "text": "that up and running using the SyFy sparse matrix library that comes packaged with Python shell and the SVD",
    "start": "2150940",
    "end": "2157990"
  },
  {
    "text": "computation that comes packaged with Python shell there's basically four steps here we used a redshift to copy",
    "start": "2157990",
    "end": "2164200"
  },
  {
    "text": "the data into a redshift table run a sequel query to extract the customer product pairs we loaded those pairs into",
    "start": "2164200",
    "end": "2171820"
  },
  {
    "text": "a matrix and then ran the SVD you know the redshift sequel queries took less",
    "start": "2171820",
    "end": "2176920"
  },
  {
    "text": "than twenty seconds it was nothing really and then you know the computation is where all the work was and this is for",
    "start": "2176920",
    "end": "2183430"
  },
  {
    "text": "calculating a 1000 dimension or 1000 rank matrix the matrix size original",
    "start": "2183430",
    "end": "2191859"
  },
  {
    "text": "matrix size was 202 you know 70 minutes",
    "start": "2191859",
    "end": "2198970"
  },
  {
    "text": "or about an hour to run this and the other point is that it cost 60 cents less than a dollar to actually do all of",
    "start": "2198970",
    "end": "2204910"
  },
  {
    "text": "this and so you really don't need a cluster you just need a medium sized machine to go and go and run this on and",
    "start": "2204910",
    "end": "2210820"
  },
  {
    "text": "that's what this job type is for there's so much more that we've added to glue I",
    "start": "2210820",
    "end": "2217030"
  },
  {
    "text": "just don't have the time to go over them but there are three important things that I'd like to touch upon one is that",
    "start": "2217030",
    "end": "2222670"
  },
  {
    "text": "now we have native support in dynamic frames for dynamodb tables what this",
    "start": "2222670",
    "end": "2228640"
  },
  {
    "text": "means is for is with a single line of code you can read a dynamodb table in",
    "start": "2228640",
    "end": "2235330"
  },
  {
    "text": "parallel into a dynamic frame turn it to a data frame like Ice about earlier and",
    "start": "2235330",
    "end": "2240880"
  },
  {
    "text": "then run your sequel queries over that data frame so it allows you to run sequel over dynamodb data in situ you",
    "start": "2240880",
    "end": "2248500"
  },
  {
    "text": "don't have to go dump it to s3 or anything like that just run a sequel query try it out it's already available",
    "start": "2248500",
    "end": "2255410"
  },
  {
    "text": "we have encryption for those of you that are really sensitive to how you store",
    "start": "2255410",
    "end": "2260450"
  },
  {
    "text": "your data on how you run your systems we encrypt at the platform level inside the",
    "start": "2260450",
    "end": "2266809"
  },
  {
    "text": "cluster we encrypt the communication that's going back and forth from executors and we also encrypt the data",
    "start": "2266809",
    "end": "2279589"
  },
  {
    "text": "at rest with customer managed AWS KMS keys",
    "start": "2279589",
    "end": "2285789"
  },
  {
    "text": "we're also compliant with three different certainly with three different certifications right now GDP our",
    "start": "2287170",
    "end": "2293000"
  },
  {
    "text": "compliance HIPAA and baa and more compliance certifications are about to",
    "start": "2293000",
    "end": "2298369"
  },
  {
    "text": "come all right so there's a bunch of related breakouts tomorrow and the day",
    "start": "2298369",
    "end": "2305450"
  },
  {
    "text": "after and I couldn't fit any more on the slide about AWS clue so if you want to dig in to some of the things that I",
    "start": "2305450",
    "end": "2311480"
  },
  {
    "text": "talked about get into the details try them out I recommend that you attend and with that I'd like to hand it over to",
    "start": "2311480",
    "end": "2318440"
  },
  {
    "text": "aruch thank you very much [Applause]",
    "start": "2318440",
    "end": "2328329"
  },
  {
    "text": "Thanks behold good afternoon people my name is Aurora",
    "start": "2328329",
    "end": "2333710"
  },
  {
    "text": "and I'm the vice president of engineering at realtor.com and today I'm going to talk to you about how we use",
    "start": "2333710",
    "end": "2340759"
  },
  {
    "text": "AWS glue at realtor.com so realtor.com is part of NewsCorp and",
    "start": "2340759",
    "end": "2351400"
  },
  {
    "text": "we call ourselves the home of home search so our mission is to empower",
    "start": "2351400",
    "end": "2357140"
  },
  {
    "text": "people by making all things home simple efficient and enjoyable so when you when",
    "start": "2357140",
    "end": "2362809"
  },
  {
    "text": "you're looking for the home that you want to buy this is the place that you have to go in terms of you know how",
    "start": "2362809",
    "end": "2370279"
  },
  {
    "text": "popular the site is we have the highest engagement people have about 1.5 times",
    "start": "2370279",
    "end": "2378440"
  },
  {
    "text": "the pageviews 1.3 times longer visits and it's a very motivated audience",
    "start": "2378440",
    "end": "2386990"
  },
  {
    "text": "that's trying to buy a home and in terms of raw statistics we have about sixty",
    "start": "2386990",
    "end": "2393349"
  },
  {
    "text": "three unique million uniques every month and two billion plus page views so every",
    "start": "2393349",
    "end": "2400039"
  },
  {
    "text": "click every page view ends up in our data platform as a record so you can",
    "start": "2400039",
    "end": "2408019"
  },
  {
    "text": "well imagine the volume and scale that we deal with for us in the data",
    "start": "2408019",
    "end": "2414499"
  },
  {
    "text": "analytics platform we have moved completely away from our old data center",
    "start": "2414499",
    "end": "2420410"
  },
  {
    "text": "by sequel server infrastructure into AWS and almost all the data is today on s3",
    "start": "2420410",
    "end": "2428720"
  },
  {
    "text": "and if you think about it when data shows up from sources they are either",
    "start": "2428720",
    "end": "2434150"
  },
  {
    "text": "showing up near real time through API based integration or people are dropping",
    "start": "2434150",
    "end": "2439190"
  },
  {
    "text": "files or we are extracting files what we have done is we have carved out parts of",
    "start": "2439190",
    "end": "2445430"
  },
  {
    "text": "our s3 buckets which are which service file drop the way the file came in we",
    "start": "2445430",
    "end": "2451609"
  },
  {
    "text": "also have the archive region where we archive the files then we have the area",
    "start": "2451609",
    "end": "2456980"
  },
  {
    "text": "from which we start our process and that's our raw data layer from raw",
    "start": "2456980",
    "end": "2462930"
  },
  {
    "text": "we take it to different levels of curation at the beginning we have what",
    "start": "2462930",
    "end": "2468849"
  },
  {
    "text": "we call the process data transactional to make the data more accessible we want",
    "start": "2468849",
    "end": "2474970"
  },
  {
    "text": "to put it in a format where the metadata is known where people can run queries so",
    "start": "2474970",
    "end": "2481660"
  },
  {
    "text": "what we do is regardless of the way the data came in we put it into a columnar",
    "start": "2481660",
    "end": "2487809"
  },
  {
    "text": "compressed format we put it into park' format makes it very easy for us to query the data and then from processed",
    "start": "2487809",
    "end": "2496029"
  },
  {
    "text": "data transactional we take it to business data layer where we put the business overlay the business metadata",
    "start": "2496029",
    "end": "2502299"
  },
  {
    "text": "on top of the data and finally if there are datasets that we have to feed other",
    "start": "2502299",
    "end": "2507369"
  },
  {
    "text": "systems or reports we aggregate whatever we need to do filter and create the",
    "start": "2507369",
    "end": "2513609"
  },
  {
    "text": "access data layer so what we have done is as we go from raw data to process",
    "start": "2513609",
    "end": "2521200"
  },
  {
    "text": "data transactional we are trying to build a template driven transformation",
    "start": "2521200",
    "end": "2527589"
  },
  {
    "text": "we do not want to have every data set that comes in have its own distinct",
    "start": "2527589",
    "end": "2533680"
  },
  {
    "text": "pipeline but we want a template based solution so that instead of writing new",
    "start": "2533680",
    "end": "2539680"
  },
  {
    "text": "pipelines we are configuring new pipelines at runtime so the problem is",
    "start": "2539680",
    "end": "2544900"
  },
  {
    "text": "how do we generalize the ETL process to handle these generic data sets and",
    "start": "2544900",
    "end": "2550119"
  },
  {
    "text": "generic volumes that we get how do we solve for patterns that we see over and",
    "start": "2550119",
    "end": "2555369"
  },
  {
    "text": "over again things like deeply nested JSON attributes JSON attributes with",
    "start": "2555369",
    "end": "2561519"
  },
  {
    "text": "rows in it which needs to be explore it out and finally make the data easy for us to",
    "start": "2561519",
    "end": "2567849"
  },
  {
    "text": "query we were using EMR hive as the",
    "start": "2567849",
    "end": "2573309"
  },
  {
    "text": "basis of what we did and a lot of the functionalities was coded as user-defined functions so the challenges",
    "start": "2573309",
    "end": "2581289"
  },
  {
    "text": "were of course we had to maintain the cluster for EMR and the UDF code that we",
    "start": "2581289",
    "end": "2586599"
  },
  {
    "text": "wrote and used definitely required a lot of maintenance over time so if you think about the",
    "start": "2586599",
    "end": "2595090"
  },
  {
    "text": "problem that we are trying to solve you have raw data and we allowed raw data in two different distinct format CSV with",
    "start": "2595090",
    "end": "2602560"
  },
  {
    "text": "header where we know the metadata or JSON where the metadata is embedded",
    "start": "2602560",
    "end": "2608260"
  },
  {
    "text": "within the data and what we are trying to do is convert it into process data transactional there in park' format and",
    "start": "2608260",
    "end": "2616500"
  },
  {
    "text": "during this process few things are happening one is we are validating the",
    "start": "2616500",
    "end": "2622780"
  },
  {
    "text": "structure of the input the data types of the fields that are coming in we are",
    "start": "2622780",
    "end": "2628060"
  },
  {
    "text": "relational izing the data we are taking multivalued columns and exploring them",
    "start": "2628060",
    "end": "2634390"
  },
  {
    "text": "out into multiple rows in child table we are also taking deeply nested JSON",
    "start": "2634390",
    "end": "2641260"
  },
  {
    "text": "attributes and aggregating them up at the top level and optionally detect",
    "start": "2641260",
    "end": "2647110"
  },
  {
    "text": "duplication of Records in the input set so that's what we are trying to do final",
    "start": "2647110",
    "end": "2653290"
  },
  {
    "text": "output would be in park' format the way we do it today is the orchestration is",
    "start": "2653290",
    "end": "2659920"
  },
  {
    "text": "being done from AWS data pipeline for every data set we have our own app ID it",
    "start": "2659920",
    "end": "2667120"
  },
  {
    "text": "kicks off our process on ECS and on ECS",
    "start": "2667120",
    "end": "2672280"
  },
  {
    "text": "the first thing that our script does is it goes out and gets the configuration",
    "start": "2672280",
    "end": "2678250"
  },
  {
    "text": "for this particular pipeline namely what are the columns that we need to validate what are the fields that require",
    "start": "2678250",
    "end": "2684520"
  },
  {
    "text": "explosion what are the fields that require compaction then we have another",
    "start": "2684520",
    "end": "2689980"
  },
  {
    "text": "service called the last good key service that we have built which keeps track of what was the last partition last day",
    "start": "2689980",
    "end": "2697090"
  },
  {
    "text": "last hour that has been successfully partition processed so that the next",
    "start": "2697090",
    "end": "2702880"
  },
  {
    "text": "time and the script is running it knows to go to the last good key service for this app find out where should I start",
    "start": "2702880",
    "end": "2709330"
  },
  {
    "text": "from and go from that point onwards so once the state is done then from Amazon",
    "start": "2709330",
    "end": "2717160"
  },
  {
    "text": "ECS the script goes and puts a softlock on the source and the target so that",
    "start": "2717160",
    "end": "2722800"
  },
  {
    "text": "another process running at the same time doesn't end up working together and against each other",
    "start": "2722800",
    "end": "2729059"
  },
  {
    "text": "finally the configuration in JSON format is parsed and the template code gets",
    "start": "2729059",
    "end": "2735639"
  },
  {
    "text": "filled in this is where glue starts at magic it retrieves the data from the",
    "start": "2735639",
    "end": "2741129"
  },
  {
    "text": "input partition based on what the last good key service said it said it",
    "start": "2741129",
    "end": "2746709"
  },
  {
    "text": "performs a data drive validation on the columns that are prescribed in the configuration performs flattening it",
    "start": "2746709",
    "end": "2754719"
  },
  {
    "text": "relation Eliza's and explodes the multivalued columns into child tables and it saves the result in part a format",
    "start": "2754719",
    "end": "2762849"
  },
  {
    "text": "and writes it out to s3 again so that's what is happening with you finally we update our last good tea",
    "start": "2762849",
    "end": "2770619"
  },
  {
    "text": "service to say that okay one more partition got processed so the next time we run we have the bookmark there",
    "start": "2770619",
    "end": "2777689"
  },
  {
    "text": "finally we use AWS blue crawler to crawl the new data set that was just created",
    "start": "2777689",
    "end": "2785409"
  },
  {
    "text": "and the partitions new partitions just become discoverable finally we unlock",
    "start": "2785409",
    "end": "2791799"
  },
  {
    "text": "both the source and the targets so this is how we do it today however we are moving forward with a new",
    "start": "2791799",
    "end": "2801419"
  },
  {
    "text": "way of doing things so we are gradually moving away from data pipeline into air",
    "start": "2801419",
    "end": "2807639"
  },
  {
    "text": "flow to manage complex dag based dependency management and instead of ECS",
    "start": "2807639",
    "end": "2814599"
  },
  {
    "text": "we kick that off a Python shell so we are a beta user for that and PI from",
    "start": "2814599",
    "end": "2822009"
  },
  {
    "text": "Python shell we do exactly what we used to do from ecs go call config service",
    "start": "2822009",
    "end": "2828069"
  },
  {
    "text": "last good key service go and lock the source target run all the things using",
    "start": "2828069",
    "end": "2835569"
  },
  {
    "text": "glue this is where we need a bigger cluster and hence we're going with glue not Python shell and finally so these",
    "start": "2835569",
    "end": "2844899"
  },
  {
    "text": "are the three areas where we were on ECS now with serverless we are even going",
    "start": "2844899",
    "end": "2850839"
  },
  {
    "text": "beyond having two hats and or ECS don't instead we just go straight",
    "start": "2850839",
    "end": "2857040"
  },
  {
    "text": "and use Python shell the next thing that",
    "start": "2857040",
    "end": "2862470"
  },
  {
    "text": "we do using blue today is profiling now that we have data in Parque format in",
    "start": "2862470",
    "end": "2869220"
  },
  {
    "text": "our process data transactional layer we want to profile this data we want to",
    "start": "2869220",
    "end": "2874380"
  },
  {
    "text": "know how many records came in what are the data types what are the various main",
    "start": "2874380",
    "end": "2880350"
  },
  {
    "text": "max standard deviation of each again the ML package is in Glu spark they help you",
    "start": "2880350",
    "end": "2888000"
  },
  {
    "text": "do all sorts of descriptive statistics and those get written out again into s3",
    "start": "2888000",
    "end": "2893240"
  },
  {
    "text": "so whenever we run we run out we run the descriptive statistics put that into s3",
    "start": "2893240",
    "end": "2899730"
  },
  {
    "text": "as well and once the glue crawler has gone through this new data set it",
    "start": "2899730",
    "end": "2906330"
  },
  {
    "text": "appears as an Athena table and in front of that we have tableau so people can",
    "start": "2906330",
    "end": "2911730"
  },
  {
    "text": "actually quickly go in and see how the profile data that we just brought in",
    "start": "2911730",
    "end": "2917340"
  },
  {
    "text": "looks like so not only do we use glue to actually move the data transform the",
    "start": "2917340",
    "end": "2924720"
  },
  {
    "text": "data but also so that we can profile the data and have the output accessible so",
    "start": "2924720",
    "end": "2932220"
  },
  {
    "text": "the way we look at Python shell we look at AWS glue for what it is it's a server",
    "start": "2932220",
    "end": "2939000"
  },
  {
    "text": "list distributed computer so you use it so that you can process the data we look",
    "start": "2939000",
    "end": "2945960"
  },
  {
    "text": "at the glue Python shell though as the edge node just like what you used to have you used to have a cluster and then",
    "start": "2945960",
    "end": "2952290"
  },
  {
    "text": "you need to have an edge node from which you trigger jobs from where you did some of the light transformation unzipping of",
    "start": "2952290",
    "end": "2959610"
  },
  {
    "text": "files tar extractions small data sets being parque converted to a different",
    "start": "2959610",
    "end": "2965460"
  },
  {
    "text": "date format like parque so we see blue Python shell fitting that particular",
    "start": "2965460",
    "end": "2971520"
  },
  {
    "text": "mold where we consider it as an edge node to a bigger glue cluster so that's",
    "start": "2971520",
    "end": "2977070"
  },
  {
    "text": "how we think of it so beyond just",
    "start": "2977070",
    "end": "2983070"
  },
  {
    "text": "processing of data why we exist at realtor.com is to bring the best consumer experience",
    "start": "2983070",
    "end": "2990830"
  },
  {
    "text": "to our users and who we call consumers and then connect them with the real",
    "start": "2990830",
    "end": "2996920"
  },
  {
    "text": "estate agents so for that we have created analytical profile services so",
    "start": "2996920",
    "end": "3003670"
  },
  {
    "text": "we look at users the clickstream that they generate the listings they are",
    "start": "3003670",
    "end": "3010060"
  },
  {
    "text": "viewed we bring that together to create an analytical profile for every consumer",
    "start": "3010060",
    "end": "3016690"
  },
  {
    "text": "so for every cookie user or signed-in user we go in and we say ok these are",
    "start": "3016690",
    "end": "3022210"
  },
  {
    "text": "the events that they have performed within the product these are the listings they're viewed so that we",
    "start": "3022210",
    "end": "3028780"
  },
  {
    "text": "create a batch profile every few hours and for that we use Athena to do the CTS",
    "start": "3028780",
    "end": "3037330"
  },
  {
    "text": "or Athena to build that out however from that the serving layer is",
    "start": "3037330",
    "end": "3043330"
  },
  {
    "text": "on dynamodb so to move data between s3 where Athena has brought it in to dynamo",
    "start": "3043330",
    "end": "3052150"
  },
  {
    "text": "what we do is we use AWS glue ETL and",
    "start": "3052150",
    "end": "3057570"
  },
  {
    "text": "once the data the batch profile is in dynamo what we also have is we have",
    "start": "3057570",
    "end": "3064350"
  },
  {
    "text": "clickstream coming in every five minutes on s3 we have an AWS Glu ETF which is",
    "start": "3064350",
    "end": "3073000"
  },
  {
    "text": "running near real-time and updating the consumer Analytics profile so you have",
    "start": "3073000",
    "end": "3078670"
  },
  {
    "text": "the batch which tells you you have seen these homes and then as you progress the near real-time blue ETL keeps on",
    "start": "3078670",
    "end": "3086260"
  },
  {
    "text": "updating things in Amazon so in in dynamo so that's how we keep our",
    "start": "3086260",
    "end": "3092140"
  },
  {
    "text": "consumer Analytics profile updated and this is the basis of multiple solutions",
    "start": "3092140",
    "end": "3099010"
  },
  {
    "text": "data products like personalization being able to predict user behavior by looking",
    "start": "3099010",
    "end": "3104650"
  },
  {
    "text": "at their history and their profile what they have done in the past so having",
    "start": "3104650",
    "end": "3110830"
  },
  {
    "text": "said that when I look back what have been the benefits of using AWS glue",
    "start": "3110830",
    "end": "3116100"
  },
  {
    "text": "first of all the speed of implementation has gone the developer productivity because you",
    "start": "3116100",
    "end": "3122630"
  },
  {
    "text": "have a lot of inbuilt transforms so for the explosion of multivalued attributes",
    "start": "3122630",
    "end": "3129259"
  },
  {
    "text": "into multiple rows in child table for that UDF we had 200 plus lines of code",
    "start": "3129259",
    "end": "3134839"
  },
  {
    "text": "that we had to maintain now with the relational eyes transformation that's",
    "start": "3134839",
    "end": "3140150"
  },
  {
    "text": "available in Glu it's less than 10 lines and something that we don't manage the",
    "start": "3140150",
    "end": "3146240"
  },
  {
    "text": "operations team they love the serverless aspect of Clue and finally we found that",
    "start": "3146240",
    "end": "3151640"
  },
  {
    "text": "the performance boost that we got for pushing into dynamo DB was very",
    "start": "3151640",
    "end": "3157160"
  },
  {
    "text": "significant things that you know we could get water 10x improving performance of course we helped by",
    "start": "3157160",
    "end": "3162700"
  },
  {
    "text": "finding the sweet spot of how much it can push through have multiple Glu jobs",
    "start": "3162700",
    "end": "3168259"
  },
  {
    "text": "running at the same time and partitioning the input dataset but regardless about a 10x improvement",
    "start": "3168259",
    "end": "3175420"
  },
  {
    "text": "finally I would be remiss if I did not acknowledge the engineering team my team that has actually worked on these things",
    "start": "3175420",
    "end": "3182150"
  },
  {
    "text": "in the last year as well as the Amazon team the account team as well as the",
    "start": "3182150",
    "end": "3187690"
  },
  {
    "text": "product team for Glu and the engineering team for Glu for actually helping us",
    "start": "3187690",
    "end": "3194269"
  },
  {
    "text": "through this whole adoption process thank you very much [Applause]",
    "start": "3194269",
    "end": "3202750"
  },
  {
    "text": "so we're happy to take questions now and at 3:45 the two of us as well as one",
    "start": "3206170",
    "end": "3214730"
  },
  {
    "text": "additional speaker from 3m we'll be happy to answer questions at that time",
    "start": "3214730",
    "end": "3220910"
  },
  {
    "text": "as well okay it's a great question I",
    "start": "3220910",
    "end": "3228890"
  },
  {
    "text": "believe right now you don't get much so the question was how much local disk you get in the Python shell I think right",
    "start": "3228890",
    "end": "3235070"
  },
  {
    "text": "now we're giving you something like six gigabytes of local disk depending on how the you know how this varies we might",
    "start": "3235070",
    "end": "3241580"
  },
  {
    "text": "actually end up bumping it up but we just we need to get some feedback from customers before we we get an idea of",
    "start": "3241580",
    "end": "3247310"
  },
  {
    "text": "how how do you how people will use it good",
    "start": "3247310",
    "end": "3255130"
  },
  {
    "text": "yes so the question about why we had to build the orchestration and other things",
    "start": "3261560",
    "end": "3266970"
  },
  {
    "text": "around it reason is we had already started before glues orchestration became so now we use",
    "start": "3266970",
    "end": "3273900"
  },
  {
    "text": "two features of glue the crawler aspect of it the metadata the transformation",
    "start": "3273900",
    "end": "3279089"
  },
  {
    "text": "the next step would be the orchestration",
    "start": "3279089",
    "end": "3283130"
  },
  {
    "text": "can you speak up I could I only heard PII",
    "start": "3288890",
    "end": "3293480"
  },
  {
    "text": "I'm sorry can you why don't you come over",
    "start": "3296569",
    "end": "3301220"
  },
  {
    "text": "well how do you classify the data that is going into the day delay we have very",
    "start": "3306900",
    "end": "3316680"
  },
  {
    "text": "little personalized data it's mostly cookie users so based on the cookie ID however we do have some amount of",
    "start": "3316680",
    "end": "3325100"
  },
  {
    "text": "information about logged in users who have signed up and for that there are",
    "start": "3325100",
    "end": "3331530"
  },
  {
    "text": "several requirements like there are certain as attributes that we cannot use",
    "start": "3331530",
    "end": "3336570"
  },
  {
    "text": "for targeting so we do not bring that into the profile we just suppress those",
    "start": "3336570",
    "end": "3341700"
  },
  {
    "text": "even before it goes into the profile first name last name email is fine",
    "start": "3341700",
    "end": "3349410"
  },
  {
    "text": "because that they've signed that they have allowed us to use it yeah",
    "start": "3349410",
    "end": "3356030"
  },
  {
    "text": "yes",
    "start": "3360420",
    "end": "3362630"
  },
  {
    "text": "yeah so we have looked at ECS we have looked at darker but this one works",
    "start": "3378460",
    "end": "3384550"
  },
  {
    "text": "right out of the gates of wife and bother and not very expensive either",
    "start": "3384550",
    "end": "3390510"
  },
  {
    "text": "yeah so for me it's the edge node where I have to prep the data before I",
    "start": "3395070",
    "end": "3401230"
  },
  {
    "text": "actually do the processing so this is clearly that yeah so let me just add a",
    "start": "3401230",
    "end": "3406750"
  },
  {
    "text": "little bit to that the intention around Python shell was to you know fill in the",
    "start": "3406750",
    "end": "3412690"
  },
  {
    "text": "gaps where you know the existing infrastructure for just doing the eto was just too heavyweight it's a generic",
    "start": "3412690",
    "end": "3420880"
  },
  {
    "text": "programming environment so I expect people to use it in all kinds of different ways",
    "start": "3420880",
    "end": "3425950"
  },
  {
    "text": "we are not going after the the particular points in the space where",
    "start": "3425950",
    "end": "3431560"
  },
  {
    "text": "where say lambda and ECS are going after ECS you can give whatever you know",
    "start": "3431560",
    "end": "3439020"
  },
  {
    "text": "container you want to them with lambda it's again a different set of additional",
    "start": "3439020",
    "end": "3444970"
  },
  {
    "text": "programming languages as well as spin up and shutdown time and and and various other restrictions that are there so",
    "start": "3444970",
    "end": "3450960"
  },
  {
    "text": "we're going after a different point in space one is large there there is literally two doing building",
    "start": "3450960",
    "end": "3457330"
  },
  {
    "text": "applications on the cloud we are really focused around you know how do we enable ETL right and and and data leaks sorry",
    "start": "3457330",
    "end": "3466330"
  },
  {
    "text": "go ahead",
    "start": "3466330",
    "end": "3468540"
  },
  {
    "text": "huh",
    "start": "3486100",
    "end": "3489100"
  },
  {
    "text": "yeah yeah so there's so the question is roughly around scalability of crawlers",
    "start": "3496710",
    "end": "3502619"
  },
  {
    "text": "so there's one point that I talked about in terms of the number of files and partitions you can handle but there's",
    "start": "3502619",
    "end": "3507960"
  },
  {
    "text": "another sort of access in crawlers which is the number of sources and number of",
    "start": "3507960",
    "end": "3513030"
  },
  {
    "text": "targets that you can go after and we have some workarounds right now that help you pack targets into a crawler so",
    "start": "3513030",
    "end": "3521160"
  },
  {
    "text": "crawlers do not have a limitation on the number of sources and targets that you have though you know you only get one",
    "start": "3521160",
    "end": "3528720"
  },
  {
    "text": "type of trigger with the crawlers if you want different types of triggers you have to have different crawlers things",
    "start": "3528720",
    "end": "3534599"
  },
  {
    "text": "all of these limitations are slowly going to be removed you know the ultimate vision we're not there yet is",
    "start": "3534599",
    "end": "3540740"
  },
  {
    "text": "you just tell us what you want to crawl and when you want it to run and you don't have to do anything else right",
    "start": "3540740",
    "end": "3547380"
  },
  {
    "text": "there is no limitation on you know how many machines and and cert and you know DP use that are with a crawl all that",
    "start": "3547380",
    "end": "3553380"
  },
  {
    "text": "stuff gets kind of hidden away for you we're getting there in steps it will get",
    "start": "3553380",
    "end": "3558810"
  },
  {
    "text": "better but I'm happy to kind of talk to you you know offline to help you kind of figure all that stuff out yeah give me",
    "start": "3558810",
    "end": "3566280"
  },
  {
    "text": "one second there was somebody over here okay go ahead",
    "start": "3566280",
    "end": "3571190"
  },
  {
    "text": "great question so for us in terms of what we have seen in terms of the",
    "start": "3596480",
    "end": "3602610"
  },
  {
    "text": "performance we have the DP use number of DP use that we start the job with and",
    "start": "3602610",
    "end": "3607880"
  },
  {
    "text": "operations team keeps track of whether we need to bump theta in what you saw from a whole today about right sizing",
    "start": "3607880",
    "end": "3615420"
  },
  {
    "text": "the cluster in terms of DP you now we have added insight into how it's",
    "start": "3615420",
    "end": "3621030"
  },
  {
    "text": "performing and what should be the actual number of DP use that we submit the job",
    "start": "3621030",
    "end": "3626880"
  },
  {
    "text": "with okay we have time for one more sure so that means that when our",
    "start": "3626880",
    "end": "3640620"
  },
  {
    "text": "customers leave we're compliant with gdpr there's another level of gd-r gd-r",
    "start": "3640620",
    "end": "3646500"
  },
  {
    "text": "compliance which is when you are running on top of us and you want when your",
    "start": "3646500",
    "end": "3651810"
  },
  {
    "text": "customers leave you delete the appropriate things those components are not there yet and those you have to",
    "start": "3651810",
    "end": "3656940"
  },
  {
    "text": "build on top right okay thank you guys thank you there's going to be a meet the",
    "start": "3656940",
    "end": "3662160"
  },
  {
    "text": "speaker session at 3:45 later I think it's it's on your schedule I'll be there",
    "start": "3662160",
    "end": "3667530"
  },
  {
    "text": "a route will be there and also Timothy from 3m will be there please join us at that time",
    "start": "3667530",
    "end": "3673150"
  },
  {
    "text": "[Applause]",
    "start": "3673150",
    "end": "3675578"
  }
]