[
  {
    "text": "okay hey everybody hey I'm Dave I'm a Solutions Architect at a double yes and",
    "start": "60",
    "end": "5580"
  },
  {
    "text": "this is SRV 318 which is a session about research at PNL which is the Pacific",
    "start": "5580",
    "end": "11429"
  },
  {
    "text": "Northwest National Lab I got Mike and Ralph here who are going to talk about some of the stuff that they've been building and working on one of the best",
    "start": "11429",
    "end": "17730"
  },
  {
    "text": "parts about being an SI is being able to talk with customers and learn about the things that they're building and I think",
    "start": "17730",
    "end": "23670"
  },
  {
    "text": "this is a really great presentation about some real-world examples of how people are solving problems using AWS so",
    "start": "23670",
    "end": "28949"
  },
  {
    "text": "well in any other delay we turn it over to to Mike and Ralphie Rowe talk about what they're doing all right thanks Dave",
    "start": "28949",
    "end": "35620"
  },
  {
    "text": "[Applause] so thanks so my name is Mike garden",
    "start": "35620",
    "end": "41730"
  },
  {
    "text": "Ellie this is Ralph Perko Ralph and I are both software engineers at P&L Pacific Northwest National Laboratory as",
    "start": "41730",
    "end": "48329"
  },
  {
    "text": "Dave said and Ralph and I both joined the lab about seven years ago and our primary focus or emphasis there the jobs",
    "start": "48329",
    "end": "56010"
  },
  {
    "text": "that we do I guess is leading development efforts driving capabilities and then pushing for the adoption of new",
    "start": "56010",
    "end": "61949"
  },
  {
    "text": "and emerging technologies all in support of the government customers that we support what we're going to talk to you",
    "start": "61949",
    "end": "67740"
  },
  {
    "text": "about today is how we leverage that skill set along with our talented researchers to actually push or enable",
    "start": "67740",
    "end": "74340"
  },
  {
    "text": "research at the lab so that we can try to accommodate all the missions and problems again for the customers that we",
    "start": "74340",
    "end": "79799"
  },
  {
    "text": "support so a little bit about the lab system so I don't know how much any of you know about do we Labs Department of",
    "start": "79799",
    "end": "86250"
  },
  {
    "text": "Energy labs but PNL is one of 17 DOA labs that serves the u.s. for a little",
    "start": "86250",
    "end": "92250"
  },
  {
    "text": "over six decades now and what the labs take on is they take on large-scale long-term R&D efforts they're typically",
    "start": "92250",
    "end": "99240"
  },
  {
    "text": "beyond the scope of academia and private industry and that's why they were created they were created to actually",
    "start": "99240",
    "end": "104490"
  },
  {
    "text": "complement the roles of academia and industry and so a lot of the research",
    "start": "104490",
    "end": "109740"
  },
  {
    "text": "that gets done at the lab or at the labs is requires equipment and",
    "start": "109740",
    "end": "115170"
  },
  {
    "text": "instrumentation that is typically beyond what academia can acquire and then beyond the risk tolerance for corporate",
    "start": "115170",
    "end": "121710"
  },
  {
    "text": "industry or corporate research labs so a little bit about pnnl itself so our main",
    "start": "121710",
    "end": "127320"
  },
  {
    "text": "campus is in Richland Washington it's really close to the Hanford Site if you're familiar with that and then we",
    "start": "127320",
    "end": "132720"
  },
  {
    "text": "have other satellite office location so we have an office in Seattle we have an office in Sequim then we were",
    "start": "132720",
    "end": "138060"
  },
  {
    "text": "in office in Portland and then we have some folks out in the DC area as well and then these stats are old but we just",
    "start": "138060",
    "end": "143970"
  },
  {
    "text": "kind of want to give you an idea about a little bit about the lab itself so we had in 2016 we had about a billion",
    "start": "143970",
    "end": "149760"
  },
  {
    "text": "dollars in R&D expenditures we had a little over a thousand publications various patents some wards and then we",
    "start": "149760",
    "end": "156810"
  },
  {
    "text": "have roughly about 4,400 staff most of which are engineers and scientists but then we also have other non-technical",
    "start": "156810",
    "end": "162930"
  },
  {
    "text": "staff that are also part of the process there so a little bit about software engineering at pian and also I mean",
    "start": "162930",
    "end": "169950"
  },
  {
    "text": "first and foremost were a research lab that's of course for most of our emphasises most of the staff are focused",
    "start": "169950",
    "end": "175260"
  },
  {
    "text": "on research and innovation but not typically operational systems and our",
    "start": "175260",
    "end": "180600"
  },
  {
    "text": "software engineers then really come in and help complement that we try to help enable and drive the research",
    "start": "180600",
    "end": "186540"
  },
  {
    "text": "capabilities one of the challenges that we faced often at the lab is access to resources and when I say resources I",
    "start": "186540",
    "end": "193500"
  },
  {
    "text": "mean physical infrastructure so physical Hardware virtual environments staff and",
    "start": "193500",
    "end": "199140"
  },
  {
    "text": "when I say staff I mean people that have the skill sets needed to really augment and supplement and help us configure and",
    "start": "199140",
    "end": "204480"
  },
  {
    "text": "enable those physical environments and that's where a DBS has been a huge game-changer for us it's really made a",
    "start": "204480",
    "end": "211709"
  },
  {
    "text": "big difference for us in our ability to focus on the research and mission problems instead of getting frustrated",
    "start": "211709",
    "end": "217230"
  },
  {
    "text": "with actually trying to acquire the the physical hardware or infrastructure we need to support this mission space and",
    "start": "217230",
    "end": "223320"
  },
  {
    "text": "then in addition to that in particular software engineering it also has really helped with a lot of our software practices and processes so where we can",
    "start": "223320",
    "end": "229920"
  },
  {
    "text": "we try to follow agile like processes and it really has helped with that so in",
    "start": "229920",
    "end": "236700"
  },
  {
    "text": "addition to the the environments that we need one of the other challenges that we face is oftentimes our researchers work",
    "start": "236700",
    "end": "242310"
  },
  {
    "text": "in isolation or independently and what I mean by that is they work independent of the software engineers the DevOps folks",
    "start": "242310",
    "end": "249150"
  },
  {
    "text": "the minimal amount of office folks that we have and other technical staff that can really help augment the capabilities",
    "start": "249150",
    "end": "255239"
  },
  {
    "text": "and skill sets that they need and that is that makes sense because they're the primary focuses the innovation and novel",
    "start": "255239",
    "end": "261930"
  },
  {
    "text": "concepts it's not trying to build deployable applied solutions and in a lot of times then if there is",
    "start": "261930",
    "end": "269060"
  },
  {
    "text": "something that has the potential to be deployed meaning like it's a its capability that one of our collaborators",
    "start": "269060",
    "end": "274910"
  },
  {
    "text": "wants or one of our government sponsors wants the time frames or the timelines that it takes to get it there is a",
    "start": "274910",
    "end": "280340"
  },
  {
    "text": "challenge and oftentimes is a bit of a frustration for both parties both researchers and engine and engineers and",
    "start": "280340",
    "end": "286730"
  },
  {
    "text": "then of course sometimes unfortunately that also then leads to products or capabilities as it really isn't what the",
    "start": "286730",
    "end": "291920"
  },
  {
    "text": "customer wanted and when I say customer I mean like our government customers so",
    "start": "291920",
    "end": "297020"
  },
  {
    "text": "what are we doing about it or what are we trying to do in addition to leveraging AWS is so we are trying to",
    "start": "297020",
    "end": "302480"
  },
  {
    "text": "improve the collaboration our primary solution now is to try to improve that part of it so of course they leverage",
    "start": "302480",
    "end": "308090"
  },
  {
    "text": "AWS where we can for the environment parts but then work more closely or upfront with the researchers so we kind",
    "start": "308090",
    "end": "314210"
  },
  {
    "text": "of build this relationship a partnership between our researchers and engineers and that's really what our software",
    "start": "314210",
    "end": "320180"
  },
  {
    "text": "engineers are trying to do now is to be more engaged and upfront with the researchers so that they can actually",
    "start": "320180",
    "end": "325340"
  },
  {
    "text": "focus on their missions and their problem spaces and if needed then we can jump in our software engineer teams can",
    "start": "325340",
    "end": "331010"
  },
  {
    "text": "jump in and help them create or build deployable solutions and that again is where AWS has really made a big",
    "start": "331010",
    "end": "336800"
  },
  {
    "text": "difference for us this really has been a turning point where both sides of that both our research and engineers can then",
    "start": "336800",
    "end": "342410"
  },
  {
    "text": "leverage the environment in the platforms to focus on the problems and it really has dramatically improved our",
    "start": "342410",
    "end": "348170"
  },
  {
    "text": "collaboration and then it really helps facilitate again some of the processing that we want or the the methods that we",
    "start": "348170",
    "end": "354590"
  },
  {
    "text": "take in building the solutions so that we have more control over the timeframes our estimates for those time timeframes",
    "start": "354590",
    "end": "360020"
  },
  {
    "text": "and then even potentially the transitions that we might have so a little bit about we want to talk a",
    "start": "360020",
    "end": "365990"
  },
  {
    "text": "little bit about moving to the cloud and some of the steps that we took there some of the concerns and frustrations we",
    "start": "365990",
    "end": "371090"
  },
  {
    "text": "had and then kind of where we are today so as I mentioned we a lot of the reasons that we made the aggressive push",
    "start": "371090",
    "end": "376880"
  },
  {
    "text": "to the cloud as we were just simply getting frustrated with how long it was taking for us to do things our access to",
    "start": "376880",
    "end": "382970"
  },
  {
    "text": "resources I mean literally in some cases it would take us weeks to get a virtual machine which was really frustrating",
    "start": "382970",
    "end": "388910"
  },
  {
    "text": "when we were just trying to do some basic vetting and experimentation of some of the concepts and then of course",
    "start": "388910",
    "end": "394160"
  },
  {
    "text": "it one of the other drivers was of course then the timeframes are the timelines that we had or that we wanted",
    "start": "394160",
    "end": "399770"
  },
  {
    "text": "to have so that we could provide our customers with better estimates of how long things would take some of the upfront concerns",
    "start": "399770",
    "end": "406130"
  },
  {
    "text": "we had is probably similar to others as we were unaware of some of the costing or how the costing would work or what",
    "start": "406130",
    "end": "411800"
  },
  {
    "text": "types of cost we would incur by using that environment and then the other part was vendor lock-in so we we were getting",
    "start": "411800",
    "end": "418400"
  },
  {
    "text": "a lot of I don't know if I'd say pushback but it was also concerns from those that we work with and support so",
    "start": "418400",
    "end": "423889"
  },
  {
    "text": "like our management teams and even the sponsors we work with in terms of what if we want to transition it back to the",
    "start": "423889",
    "end": "429050"
  },
  {
    "text": "physical environment what if we want to transition it to our environment are you gonna be locked into the services and",
    "start": "429050",
    "end": "434630"
  },
  {
    "text": "capabilities that you're building on top of within the AWS cloud environment so our initial approach was just a forklift",
    "start": "434630",
    "end": "441169"
  },
  {
    "text": "approach if you will we basically took what we had solution wise or capability wise and just pushed it out into the cloud it really seemed like the the most",
    "start": "441169",
    "end": "448639"
  },
  {
    "text": "say the safest approach for us with the minimal minimal amount of risk but what that led to though is it led to us",
    "start": "448639",
    "end": "455360"
  },
  {
    "text": "really missing out on a lot of the reason that you go to the cloud which is the services and other offerings that it",
    "start": "455360",
    "end": "460970"
  },
  {
    "text": "has in addition to that we really didn't we really didn't eliminate a lot of the operational pains and headaches that we",
    "start": "460970",
    "end": "466909"
  },
  {
    "text": "had it eliminated a lot of the uptime and reliability concerns and issues that we had with our own physical",
    "start": "466909",
    "end": "472190"
  },
  {
    "text": "environments but it didn't eliminate a lot of the day-to-day operational headaches that we were encountering so",
    "start": "472190",
    "end": "478250"
  },
  {
    "text": "since then so what we've done is we really have kind of shift our focus to a clod first approach where we can of",
    "start": "478250",
    "end": "484580"
  },
  {
    "text": "course utilizing whatever possible serverless components or service technologies that are available out in",
    "start": "484580",
    "end": "491030"
  },
  {
    "text": "AWS and then in partnership with some of the other folks at the lab one of our folks our cloud strategist David that's",
    "start": "491030",
    "end": "496969"
  },
  {
    "text": "here we we try to improve the education and knowledge and understanding of cloud environments that's been one of our",
    "start": "496969",
    "end": "502940"
  },
  {
    "text": "challenges as a lot of the folks that we work with just simply not knowing what it is what concerns there are with it",
    "start": "502940",
    "end": "508490"
  },
  {
    "text": "what the risks are those kinds of things and so we really try to improve people's understanding of where and how we use it",
    "start": "508490",
    "end": "514789"
  },
  {
    "text": "and then all that's internal to P&L but then also do the same with our government customers so what Ralph and I",
    "start": "514789",
    "end": "524540"
  },
  {
    "text": "want to do is we want to actually walk through a use case where we feel like we tried to take a cloud first strategy we",
    "start": "524540",
    "end": "531529"
  },
  {
    "text": "really want to try to put collaboration with our researchers up at the forefront and identify some of the",
    "start": "531529",
    "end": "538600"
  },
  {
    "text": "ways or the steps that we took to get there and so this use case is it's an",
    "start": "538600",
    "end": "544480"
  },
  {
    "text": "image classification pipeline that we built in conjunction with our researchers and the the primary goal for",
    "start": "544480",
    "end": "550810"
  },
  {
    "text": "this use case was a few things so our researchers really want to have a platformer environment where they could",
    "start": "550810",
    "end": "556600"
  },
  {
    "text": "quickly evaluate assess analyse augment change these image classification models",
    "start": "556600",
    "end": "563230"
  },
  {
    "text": "that they'd built one of the challenges that they often face is that as they step through this process as I kind of",
    "start": "563230",
    "end": "569260"
  },
  {
    "text": "identified there's a lot of things in their way to be able to do that part of it is of course the physical environments and then the other part of",
    "start": "569260",
    "end": "575230"
  },
  {
    "text": "it is the collaboration and interactions that they want to have with others as part of this effort the other parts of",
    "start": "575230",
    "end": "580810"
  },
  {
    "text": "this is that if we could actually get something established and built and demonstrable to of course the",
    "start": "580810",
    "end": "586330"
  },
  {
    "text": "researchers themselves but also others that maybe have interest in this pipeline our government customers want",
    "start": "586330",
    "end": "591610"
  },
  {
    "text": "us to be able to transition it into their environment and so we were very cognizant of that as well what limitations and expectations that they",
    "start": "591610",
    "end": "597700"
  },
  {
    "text": "might have along the way so some of the key requirements for the pipeline beyond",
    "start": "597700",
    "end": "602800"
  },
  {
    "text": "just the high level collaborative and evaluation pieces that we were going after was they wanted to handle both",
    "start": "602800",
    "end": "608470"
  },
  {
    "text": "static and streaming imagery so they wanted to be able to take existing training sets that they had imagery",
    "start": "608470",
    "end": "614110"
  },
  {
    "text": "training sets and upload them they also wanted us to be able to tap into streaming sources where possible and",
    "start": "614110",
    "end": "619270"
  },
  {
    "text": "pull those in they also were very concerned about how scalable and robust",
    "start": "619270",
    "end": "624430"
  },
  {
    "text": "and flexible it is that's probably stating the obvious but in our physical environments this is oftentimes something that takes a lot of pain and",
    "start": "624430",
    "end": "630220"
  },
  {
    "text": "effort whereas you know with AWS or any of us in like environment this was less of a concern for us they want it to be",
    "start": "630220",
    "end": "637030"
  },
  {
    "text": "easily deployed and maintained and this is an ongoing issue that we always encounter both again internally and for",
    "start": "637030",
    "end": "643660"
  },
  {
    "text": "other environments so one of the we are starting to adopt DevOps or DevOps like processes but I feel and ralphing to",
    "start": "643660",
    "end": "650560"
  },
  {
    "text": "feel like we're a little behind there we're really trying to dramatically improve that part but it is something that we still struggle with and then",
    "start": "650560",
    "end": "657400"
  },
  {
    "text": "they wanted it extensible and by sensible I mean they wanted to be able to add change modify update the image",
    "start": "657400",
    "end": "663880"
  },
  {
    "text": "classification instantiations that they had out this environment so that they could quickly assess and reevaluate how",
    "start": "663880",
    "end": "670000"
  },
  {
    "text": "effective or not effective they were and then lastly we were always we always kind of have an i or they want us to",
    "start": "670000",
    "end": "675700"
  },
  {
    "text": "have an eye on ways that we could leverage this approach or this effort to improve collaboration so this is a hi",
    "start": "675700",
    "end": "682780"
  },
  {
    "text": "this image is a high-level depiction of the actual pipeline itself and we actually had an intermix of AWS services",
    "start": "682780",
    "end": "690370"
  },
  {
    "text": "and then technologies that we are comfortable with and use a lot and so what you see here is of course that",
    "start": "690370",
    "end": "696190"
  },
  {
    "text": "intermix between database offerings and then things like Apache knife I elasticsearch and tensorflow serving",
    "start": "696190",
    "end": "703060"
  },
  {
    "text": "which is actually what was being used for the image classification piece and when we when we went through this",
    "start": "703060",
    "end": "710290"
  },
  {
    "text": "process with them of trying to establish this part of the part of the interactions and breakdowns of what we",
    "start": "710290",
    "end": "715630"
  },
  {
    "text": "were building was was it a good example of where we were I wouldn't again say maybe getting pushback but we were",
    "start": "715630",
    "end": "721360"
  },
  {
    "text": "getting questioned about why we were leveraging so many AWS services instead of using things like Kafka or Hadoop or",
    "start": "721360",
    "end": "728580"
  },
  {
    "text": "relational databases and our main push was that it when we can easily introduce",
    "start": "728580",
    "end": "733660"
  },
  {
    "text": "those things and those are very powerful technologies but it would incorporate or include the administrative overhead and",
    "start": "733660",
    "end": "739330"
  },
  {
    "text": "some of the other frustrations that we typically have just simply because we aren't staffed appropriately and so when",
    "start": "739330",
    "end": "746410"
  },
  {
    "text": "we when we went about this effort with our researchers we kind of broke the we broke the bill down into two different",
    "start": "746410",
    "end": "751420"
  },
  {
    "text": "parts so we had the left side of the diagram which is where the engineers",
    "start": "751420",
    "end": "757210"
  },
  {
    "text": "came in and built the processing and ingest pipelines and and then also where we were how we were how and where we",
    "start": "757210",
    "end": "763480"
  },
  {
    "text": "were storing both the imagery and the metadata and then on the right side is where our researchers came in and they",
    "start": "763480",
    "end": "768820"
  },
  {
    "text": "were heavily involved with building out the actual tensorflow instantiations along with some of the integration",
    "start": "768820",
    "end": "774460"
  },
  {
    "text": "points between knife I and elasticsearch now our engineers didn't get involved and were involved with them on some of",
    "start": "774460",
    "end": "780610"
  },
  {
    "text": "these pieces so that we can do things like docker eyes or tensor flow instantiations refactor refine some of",
    "start": "780610",
    "end": "786339"
  },
  {
    "text": "the knife eye integrations and then work through some of the elasticsearch index and mappings and things like that so as",
    "start": "786339",
    "end": "794589"
  },
  {
    "text": "I mentioned we often are going to walk through each one of these components and kind of and actually how they fit into the mix or how they fit in",
    "start": "794589",
    "end": "800030"
  },
  {
    "text": "pipeline and so the first piece here is Apache knife i we use Apache knife I heavily at the lab across a wide range",
    "start": "800030",
    "end": "807080"
  },
  {
    "text": "of domains and problem spaces and we use it just for a lot of the flexibility and",
    "start": "807080",
    "end": "812330"
  },
  {
    "text": "ease of use that it has along with some of the nice out-of-the-box things that that it comes with one of those being",
    "start": "812330",
    "end": "818180"
  },
  {
    "text": "this visual component which allows us to prototype pipelines and demonstrate them to analysts and managers where it would",
    "start": "818180",
    "end": "824720"
  },
  {
    "text": "maybe otherwise be difficult to show or highlight to them how we're going to be processing this data so a little bit",
    "start": "824720",
    "end": "831320"
  },
  {
    "text": "about knife I so Apache knife I was actually developed by the NSA about ten years ago and it's been of course then",
    "start": "831320",
    "end": "838430"
  },
  {
    "text": "battle tested over that time and now has since been open sourced we started using and I think around the dot to version",
    "start": "838430",
    "end": "845060"
  },
  {
    "text": "something like that we started using it pretty early on and now it's actually used quite heavily at the lab and we",
    "start": "845060",
    "end": "851030"
  },
  {
    "text": "want to just highlight a few pieces or functions about knife a that we appreciate and leverage a lot which is",
    "start": "851030",
    "end": "856820"
  },
  {
    "text": "of course the process and distribution of data message routing and distribution is phenomenal the ETL piece is great",
    "start": "856820",
    "end": "864590"
  },
  {
    "text": "like with with knife I we don't have to go through a lot of the building or creating a boilerplate code that you",
    "start": "864590",
    "end": "870410"
  },
  {
    "text": "would often do in sort in an ETL pipeline retrieving transforming augmenting storing all those things",
    "start": "870410",
    "end": "877430"
  },
  {
    "text": "really comes out of the box and is really easy to use and then the ease of installation setup and then actually",
    "start": "877430",
    "end": "883790"
  },
  {
    "text": "just building things out is is great it really does open the technology to",
    "start": "883790",
    "end": "889070"
  },
  {
    "text": "outside of our engineers and researchers a lot of our analysts and testers and things use it as well just because it makes their life easier and then some of",
    "start": "889070",
    "end": "896030"
  },
  {
    "text": "the just out-of-the-box functionality that it comes with it like back pressuring and queuing is great especially in our internal environments",
    "start": "896030",
    "end": "902570"
  },
  {
    "text": "where we have some issues with stability and up times of some of our downstream systems and then lastly the the visual",
    "start": "902570",
    "end": "909890"
  },
  {
    "text": "component is great it really is a powerful piece to have or I guess aspect",
    "start": "909890",
    "end": "915680"
  },
  {
    "text": "to have where you can see what's going on how much data is being processed where it's being backed up and what",
    "start": "915680",
    "end": "920930"
  },
  {
    "text": "other issues you are actually observing me in in the flow now this is a this is",
    "start": "920930",
    "end": "926990"
  },
  {
    "text": "a little snippet from the ingest portion of the pipeline so as I mentioned knife eye is one of the pearl",
    "start": "926990",
    "end": "933350"
  },
  {
    "text": "pieces in our image classification pipeline and what you see here is a few things you see three what are called",
    "start": "933350",
    "end": "938630"
  },
  {
    "text": "processors those processors being consumed Kafka put SNS an image cache",
    "start": "938630",
    "end": "944120"
  },
  {
    "text": "filter now consume Kafka input SNS are actually out-of-the-box processors there's literally I think hundreds now",
    "start": "944120",
    "end": "950990"
  },
  {
    "text": "that come with the platform a lot of which are things like AWS service connections or processors RDS is",
    "start": "950990",
    "end": "958400"
  },
  {
    "text": "relational databases message queues file systems all those things so there's a really robust suite of processors that",
    "start": "958400",
    "end": "964520"
  },
  {
    "text": "you can simply just drop on the flow and start using the image cast filter is actually one that we custom-built so we",
    "start": "964520",
    "end": "971000"
  },
  {
    "text": "will oftentimes build custom processors depending on how complex the flows would",
    "start": "971000",
    "end": "976490"
  },
  {
    "text": "be if we used out-of-the-box them and/or if we need a specific logic that's that's better suited for us to build a",
    "start": "976490",
    "end": "982700"
  },
  {
    "text": "custom one the other things that you see on here are connectors and those are the errors of actually connecting the",
    "start": "982700",
    "end": "988160"
  },
  {
    "text": "processors to themselves and what those are used for those are actually used for the data routing and also the queuing so",
    "start": "988160",
    "end": "994640"
  },
  {
    "text": "if there is an issue with a downstream component where it gets hung up or it's not available or simply the flows",
    "start": "994640",
    "end": "1000100"
  },
  {
    "text": "getting backed up the messages will back up on these queues and then it just some",
    "start": "1000100",
    "end": "1006130"
  },
  {
    "text": "some tuning tips that we've observed or encountered or at least these are kind of our go-to when we deploy knife out on",
    "start": "1006130",
    "end": "1011620"
  },
  {
    "text": "AWS so we typically use a c4m for ec2 instances they they seem to meet our workloads pretty well for scaling we",
    "start": "1011620",
    "end": "1019000"
  },
  {
    "text": "will often times go vertical meaning we will just scale up to ec2 instance knife",
    "start": "1019000",
    "end": "1024250"
  },
  {
    "text": "I does have clustering but oftentimes we don't really need it and as I mentioned a lot of our use cases don't require 24",
    "start": "1024250",
    "end": "1031240"
  },
  {
    "text": "24 by 7 are always up we can we can sustain some down times typically and",
    "start": "1031240",
    "end": "1036699"
  },
  {
    "text": "then we try to keep the the CPU load CPU load around 50 to 60 percent we found that on ec2 at least this is kind of the",
    "start": "1036700",
    "end": "1043600"
  },
  {
    "text": "sweet spot you know on our physical instantiation so we can go higher than that but when we do that out on AWS",
    "start": "1043600",
    "end": "1048970"
  },
  {
    "text": "we've encountered at least four previous versions of 95 some issues and then we will disable and tweak some of the",
    "start": "1048970",
    "end": "1054190"
  },
  {
    "text": "features or settings one of those settings we often tweak is the provenance piece we don't usually use it",
    "start": "1054190",
    "end": "1059650"
  },
  {
    "text": "and if you turn it or change it to volatile it dramatically improves your processing power or the amount of",
    "start": "1059650",
    "end": "1065710"
  },
  {
    "text": "messages you can and then we typically use SSDs to back the queues just for performance reasons",
    "start": "1065710",
    "end": "1072299"
  },
  {
    "text": "that seems to work well and then we always go back to the ninth I'd best practices because they're constantly evolving evolving and changing that",
    "start": "1072299",
    "end": "1078989"
  },
  {
    "text": "piece and so then this is this is just a basic or a generic example of the actual",
    "start": "1078989",
    "end": "1086220"
  },
  {
    "text": "knife I portion of this flow where we were consuming references to images in",
    "start": "1086220",
    "end": "1091499"
  },
  {
    "text": "the consume Kafka processor where we're grabbing messages off the topic we were applying custom filters in our custom",
    "start": "1091499",
    "end": "1098999"
  },
  {
    "text": "image cache filter processor and then creating SNS payloads so that it could be distributed downstream and further",
    "start": "1098999",
    "end": "1104700"
  },
  {
    "text": "processed and so with that I'm going to hand it over to Ralf and he's going to continue to walk through the pipeline",
    "start": "1104700",
    "end": "1110989"
  },
  {
    "text": "okay as Mike said just to review quickly so we bring the data in with Apache",
    "start": "1115700",
    "end": "1122190"
  },
  {
    "text": "knife I and then we're filtering in this use case we're only interested in koala bears we just want to know about koala",
    "start": "1122190",
    "end": "1127769"
  },
  {
    "text": "bears so we're filtering out the koala bears putting those JSON messages onto SNS and that in going on with our flow",
    "start": "1127769",
    "end": "1134970"
  },
  {
    "text": "why are we using SNS because we're using lambda and we need something to automatically trigger that lambda",
    "start": "1134970",
    "end": "1142679"
  },
  {
    "text": "function and there's a variety of of services you can use to do that SNS that fit in really well with our",
    "start": "1142679",
    "end": "1148470"
  },
  {
    "text": "existing flows that we have already we do a lot with message queuing and whatnot so this fit in well another",
    "start": "1148470",
    "end": "1156450"
  },
  {
    "text": "approach we could have taken was a dynamo DB event inserting the record there and done and it did it that way",
    "start": "1156450",
    "end": "1163230"
  },
  {
    "text": "that would have worked as well but this just seemed to fit in a little better with our other flows as well so what's",
    "start": "1163230",
    "end": "1169980"
  },
  {
    "text": "an SNS message look like well a lot of this is just boilerplate but we care about that little green part there where",
    "start": "1169980",
    "end": "1175230"
  },
  {
    "text": "as Mike mentioned we're filtering just on koala bears so that's what that payload looks like and this was this is",
    "start": "1175230",
    "end": "1181109"
  },
  {
    "text": "what ultimately what's going to get passed on to the to the lambda function",
    "start": "1181109",
    "end": "1186408"
  },
  {
    "text": "now moving on to lambda so as we make our progression through this flow so why",
    "start": "1186559",
    "end": "1192419"
  },
  {
    "text": "lambda and why are we using this why aren't we using Apache knife a knife right for that that that's a great",
    "start": "1192419",
    "end": "1199670"
  },
  {
    "text": "questions so before this we had pretty much used knife I we would take we'd set",
    "start": "1199670",
    "end": "1206420"
  },
  {
    "text": "up and I find ec2 and and things worked well also we were concerned about vendor",
    "start": "1206420",
    "end": "1211550"
  },
  {
    "text": "lock-in as Mike mentioned as well we didn't want to do anything that was going to corner us into only using AWS",
    "start": "1211550",
    "end": "1217730"
  },
  {
    "text": "it would be something that we couldn't put in a different in my environment or use different services with and so we",
    "start": "1217730",
    "end": "1224180"
  },
  {
    "text": "were a little cautious with that but this is a specific use case where the knife I approached wasn't going to work",
    "start": "1224180",
    "end": "1230540"
  },
  {
    "text": "simply we did a bake-off between knife I and and lambda and hands down lambda",
    "start": "1230540",
    "end": "1237910"
  },
  {
    "text": "knocked out the park one of the first things we observed and it wasn't that knife I couldn't handle it it was due to",
    "start": "1237910",
    "end": "1244550"
  },
  {
    "text": "the nature of the use case where we're downloading images of unknown size and then those need to be in memory and then",
    "start": "1244550",
    "end": "1250190"
  },
  {
    "text": "they need to be written out and whatnot it would we just wouldn't need so many instances of knife I to handle it it was",
    "start": "1250190",
    "end": "1257090"
  },
  {
    "text": "beyond what we were willing to really take on and this goes back to some of the points that Mike made about that",
    "start": "1257090",
    "end": "1263120"
  },
  {
    "text": "operational overhead and and the support and whatnot we're just not in that business of operationally supporting",
    "start": "1263120",
    "end": "1268430"
  },
  {
    "text": "large clusters of infrastructure and so with that one of the first things we noticed with lambda when we were testing",
    "start": "1268430",
    "end": "1275660"
  },
  {
    "text": "this is you know you hook that up to Kafka and if you're familiar with Kafka you get an offset and if you haven't hooked up to your Kafka topic in a while",
    "start": "1275660",
    "end": "1282350"
  },
  {
    "text": "then you have a backlog if data is still being written to it so with the lambda approach you know you get this backlog",
    "start": "1282350",
    "end": "1287780"
  },
  {
    "text": "of data and immediately lambda is able to spin up all the instances required to then help get you through that back load",
    "start": "1287780",
    "end": "1294080"
  },
  {
    "text": "backlog and that was really helpful while within with the ec2 approach we were limited by the number of ec2",
    "start": "1294080",
    "end": "1300110"
  },
  {
    "text": "instances we had and the abit in the volume that that could handle and so right off the bat that was just a great",
    "start": "1300110",
    "end": "1306920"
  },
  {
    "text": "great thing to see and then um in this case here it lambda was considerably",
    "start": "1306920",
    "end": "1313070"
  },
  {
    "text": "less expensive as well so to be clear you know cost isn't the only thing we take into account if it's not just a",
    "start": "1313070",
    "end": "1319850"
  },
  {
    "text": "dollar figure there even if lambda was more expensive than the ec2 approach we",
    "start": "1319850",
    "end": "1324860"
  },
  {
    "text": "may have still gone with it just because of that operational component that we that we keep talking about there so",
    "start": "1324860",
    "end": "1332780"
  },
  {
    "text": "again this is this was a role I think a fundamental shift in our thinking when",
    "start": "1332780",
    "end": "1339380"
  },
  {
    "text": "we went with lambda rather than knife I it wasn't that we didn't want to go server list we were excited to do it we",
    "start": "1339380",
    "end": "1345230"
  },
  {
    "text": "had been playing with technologies we just didn't have that use case to justify it and this finally gave us that use case to just go out and do it so",
    "start": "1345230",
    "end": "1353020"
  },
  {
    "text": "moving on from there so what's our lambda function doing it well we get out",
    "start": "1353020",
    "end": "1358280"
  },
  {
    "text": "we get the notification we pull out the image URL we go ahead and download that image from the internet",
    "start": "1358280",
    "end": "1364280"
  },
  {
    "text": "well first we do check to see if we already have it and then we convert and strip the convert the image to jpg strip",
    "start": "1364280",
    "end": "1370670"
  },
  {
    "text": "out any exif data or header data that might be in the image and then we write that metadata to Amazon to DynamoDB",
    "start": "1370670",
    "end": "1377630"
  },
  {
    "text": "excuse me we write the image to s3 and then write a notification to sqs and i'm",
    "start": "1377630",
    "end": "1384140"
  },
  {
    "text": "just going to walk through these each of these really quick just to discuss why we chose those particular technologies",
    "start": "1384140",
    "end": "1389360"
  },
  {
    "text": "excuse me so with dynamo it's a no",
    "start": "1389360",
    "end": "1397040"
  },
  {
    "text": "sequel database now I know it's more than that but for us that's what we're using for internally we also use Cassandra and HBase we're familiar with",
    "start": "1397040",
    "end": "1404060"
  },
  {
    "text": "the technology and this worked really well for the throughput that we needed and because the record is simply just a",
    "start": "1404060",
    "end": "1409070"
  },
  {
    "text": "hashed URL for the key and then a bunch of metadata and it's really fast for lookups it can handle the throughput it",
    "start": "1409070",
    "end": "1415340"
  },
  {
    "text": "work it worked well s3 we use s3 more and more it's really become our go-to",
    "start": "1415340",
    "end": "1420380"
  },
  {
    "text": "storage infrastructure and really helps complement our internal Hadoop infrastructure as well and I'll talk a",
    "start": "1420380",
    "end": "1425540"
  },
  {
    "text": "little bit more about that in a few slides but s3 with all the built-in capability with events and encryption",
    "start": "1425540",
    "end": "1433370"
  },
  {
    "text": "and versioning and replication and multi region support it's hard to compete with",
    "start": "1433370",
    "end": "1438530"
  },
  {
    "text": "that and we leverage that more and more in fact we even will use s3 as a rather",
    "start": "1438530",
    "end": "1443810"
  },
  {
    "text": "than using a message queue at times or whatnot we're just learning you know do we really need the uptime or the",
    "start": "1443810",
    "end": "1449570"
  },
  {
    "text": "capability of a queue can we use something like s3 instead and we're trying to use it abuse it in every way",
    "start": "1449570",
    "end": "1455660"
  },
  {
    "text": "we can and we're we have had a lot of luck with a lot of success with s3 so",
    "start": "1455660",
    "end": "1460850"
  },
  {
    "text": "and then finally sqs and that's where the notification is going to the researchers let them know that a new image",
    "start": "1460850",
    "end": "1466100"
  },
  {
    "text": "to download and it's ready for them to to look at and again back to the comment with SNS this just fits in with our",
    "start": "1466100",
    "end": "1472610"
  },
  {
    "text": "other flows we have a lot of data flows out there and they're all we have a messaging backbone in most of them were",
    "start": "1472610",
    "end": "1478700"
  },
  {
    "text": "some kind of messaging queue system is either handling notifications or the payload we've leaned a lot on Kafka in",
    "start": "1478700",
    "end": "1485510"
  },
  {
    "text": "the past we had significant amount of Kafka infrastructure that we used but more and more we've just been moving",
    "start": "1485510",
    "end": "1491330"
  },
  {
    "text": "away from that just don't have the the bandwidth to support all that so with",
    "start": "1491330",
    "end": "1497690"
  },
  {
    "text": "that what they hit the spacebar when you do that okay all right so here's just a",
    "start": "1497690",
    "end": "1503990"
  },
  {
    "text": "little bit of a lambic code example just a snippet of code don't try to compile this it won't work right so there's just an excerpt pulling",
    "start": "1503990",
    "end": "1511520"
  },
  {
    "text": "out the the parts that are important there so it's just it's just Java and in this case we're not even implementing a",
    "start": "1511520",
    "end": "1518450"
  },
  {
    "text": "lambda interface you just have to make sure you have the right method signature which we do and we're expecting an SNS",
    "start": "1518450",
    "end": "1525020"
  },
  {
    "text": "event and so we you know we grab that message from the SNS invent and it's",
    "start": "1525020",
    "end": "1530210"
  },
  {
    "text": "JSON and then we parse that out we usually use something like Google JSON or jackson xml and their JSON JSON",
    "start": "1530210",
    "end": "1538460"
  },
  {
    "text": "library to do that once we have the image URL we download the image and then",
    "start": "1538460",
    "end": "1544039"
  },
  {
    "text": "write it back out we're using the image i/o API this is just pure Java that's in the Java X package if you're familiar",
    "start": "1544039",
    "end": "1549740"
  },
  {
    "text": "with that the way we're writing that out as a JPEG it doesn't include any of the",
    "start": "1549740",
    "end": "1555830"
  },
  {
    "text": "meta data or exif header data so we get a sanitized image then on the other end which is nice from there we go ahead and",
    "start": "1555830",
    "end": "1563270"
  },
  {
    "text": "put that object into s3 we add a record Dynamo with the hashed URL and then",
    "start": "1563270",
    "end": "1569630"
  },
  {
    "text": "finally send a notification on to the researchers to let them know that a new image is ready a quick note if you're",
    "start": "1569630",
    "end": "1575990"
  },
  {
    "text": "wondering why were you doing sqs here rather than a s3 event it's because at",
    "start": "1575990",
    "end": "1583909"
  },
  {
    "text": "the time when we originally designed this and wrote it because of our environment with different researchers",
    "start": "1583909",
    "end": "1588950"
  },
  {
    "text": "and the buckets and whatnot it's just we wanted that granular control and so we decided we would just write that message",
    "start": "1588950",
    "end": "1594500"
  },
  {
    "text": "ourselves but we do use s3 events quite a bit and and now as things would have progressed",
    "start": "1594500",
    "end": "1601289"
  },
  {
    "text": "we could go back and actually just replace this with remove that code and replace it with an s3 event but it works",
    "start": "1601289",
    "end": "1607110"
  },
  {
    "text": "so just let it chug along okay now we're back to our roadmap and our big overview",
    "start": "1607110",
    "end": "1614429"
  },
  {
    "text": "here talking about collaboration as Mike was talking about so this really was",
    "start": "1614429",
    "end": "1619909"
  },
  {
    "text": "great how these three technologies came together to provide this collaborative",
    "start": "1619909",
    "end": "1624960"
  },
  {
    "text": "environment for us between us the engineering the researchers and very loosely coupled they get a notification",
    "start": "1624960",
    "end": "1630720"
  },
  {
    "text": "that a new image is available they can fetch the metadata from dynamo and then grab the image itself from s3 other they",
    "start": "1630720",
    "end": "1638220"
  },
  {
    "text": "are using Apache knife as well Mike talked about knife I pretty much anywhere data is going in and out of our",
    "start": "1638220",
    "end": "1644039"
  },
  {
    "text": "system there's gonna be a knife I there we use it quite a bit and with the built-in AWS capability that makes it",
    "start": "1644039",
    "end": "1650070"
  },
  {
    "text": "very easy to use and also it's helped a lot with any beers around vendor lock-in",
    "start": "1650070",
    "end": "1655799"
  },
  {
    "text": "which I think I really have been unfounded for us at least we have no we've had no concerns with that knife I",
    "start": "1655799",
    "end": "1661230"
  },
  {
    "text": "has definitely helped with that because we can read from a Kafka topic as quickly and as easily as we can do from an SNS queue so the message we they",
    "start": "1661230",
    "end": "1669570"
  },
  {
    "text": "grabbed that that notification the image they pass it off to tensorflow serving",
    "start": "1669570",
    "end": "1675210"
  },
  {
    "text": "that classification information comes back where they write it to elasticsearch and then there's a",
    "start": "1675210",
    "end": "1680460"
  },
  {
    "text": "research UI that's been put on top of this for analysts to look at it and evaluate and then have that iterative cycle where they can tweak the models",
    "start": "1680460",
    "end": "1686700"
  },
  {
    "text": "and whatnot and continue to work on that that classification work so an elastic",
    "start": "1686700",
    "end": "1691830"
  },
  {
    "text": "search has also been a great collaboration point for us as well ok so",
    "start": "1691830",
    "end": "1697530"
  },
  {
    "text": "some lessons learned this this is great success story for us because not only is",
    "start": "1697530",
    "end": "1703080"
  },
  {
    "text": "it a pretty sophisticated pipeline and it's doing a lot but the overhead has",
    "start": "1703080",
    "end": "1708809"
  },
  {
    "text": "the operational overhead has been greatly minimized and that's and again",
    "start": "1708809",
    "end": "1714210"
  },
  {
    "text": "we keep talking about that it's fantastic for scaling in this use case lambda was the obvious choice",
    "start": "1714210",
    "end": "1719610"
  },
  {
    "text": "it's very performant when the functions are loaded or warm if you're not familiar with lambda if they're cold you",
    "start": "1719610",
    "end": "1725460"
  },
  {
    "text": "initially hit them and they have to be loaded in into memory and there can be a little bit of a hit if you're not",
    "start": "1725460",
    "end": "1732150"
  },
  {
    "text": "if you're not used to that then you will you'll wonder why is my function running so slow so just know once it gets in",
    "start": "1732150",
    "end": "1738090"
  },
  {
    "text": "memory and it's warmed up it runs it runs really fast really well just straight Java that's all we're using to key situations where we're",
    "start": "1738090",
    "end": "1745680"
  },
  {
    "text": "leaning on lambda the the low-cost pilot efforts and the high-volume throughput I have another slide on this to talk a",
    "start": "1745680",
    "end": "1751830"
  },
  {
    "text": "little bit more about excuse me a little bit more about that and then we already",
    "start": "1751830",
    "end": "1761640"
  },
  {
    "text": "talked about the cold storage it starts the legacy code versus new development this really has to do with the jar size",
    "start": "1761640",
    "end": "1767400"
  },
  {
    "text": "so when we build our lambda functions we use maven and then we take advantage of the shade plugin to both build our uber",
    "start": "1767400",
    "end": "1774030"
  },
  {
    "text": "jar and then also minimize the dependencies that get included to only those that are required we ran into a",
    "start": "1774030",
    "end": "1779460"
  },
  {
    "text": "use Kay another situation on another flow or this we were trying to use some existing NLP libraries and natural",
    "start": "1779460",
    "end": "1787230"
  },
  {
    "text": "language processing to enrich some text data and they were just way too large as an uber jar so we had to then go back",
    "start": "1787230",
    "end": "1793020"
  },
  {
    "text": "and just really pull them apart pull out the features we wanted then and also I guess on that use case as well that'd be",
    "start": "1793020",
    "end": "1799080"
  },
  {
    "text": "a case where lambda didn't work because the memory requirements for some of these NLP functions were significant and",
    "start": "1799080",
    "end": "1805200"
  },
  {
    "text": "then that kind of put that cost over the edge of what we where we wanted to go a quick note on SNS this again is from a",
    "start": "1805200",
    "end": "1812850"
  },
  {
    "text": "different use case that 256k limit on the message size we did run into a case where we were writing the payload CSS NS",
    "start": "1812850",
    "end": "1820920"
  },
  {
    "text": "and like 98% of the time the size was fine but then we hit this edge case and",
    "start": "1820920",
    "end": "1826770"
  },
  {
    "text": "expectedly where some of the messages were over 256 K we were scratching our head you know do we just drop them on",
    "start": "1826770",
    "end": "1832440"
  },
  {
    "text": "the floor like what do we do with these because we're using SNS so we don't have to worry about these types of things and",
    "start": "1832440",
    "end": "1837510"
  },
  {
    "text": "turned out the solution was really quite simple for us using an Apache knife I we could detect the size of the payload and",
    "start": "1837510",
    "end": "1843990"
  },
  {
    "text": "then from there route the message to s3 and rather than putting the payload in SNS just put a",
    "start": "1843990",
    "end": "1849990"
  },
  {
    "text": "pointer to the message in SNS and then tweaking the lambda function to know to look in s3 instead of in the payload to",
    "start": "1849990",
    "end": "1859170"
  },
  {
    "text": "be able to you know determine that was was simple and then just one last little",
    "start": "1859170",
    "end": "1864420"
  },
  {
    "text": "tip that's help for us has come your lambda function combined your code into one lambda function so we were at",
    "start": "1864420",
    "end": "1871140"
  },
  {
    "text": "one point trying to make them really granular and have you know all these well-defined loosely coupled functions",
    "start": "1871140",
    "end": "1877590"
  },
  {
    "text": "but then you get charged per function call so the more you can kind of pack into a single function call that could",
    "start": "1877590",
    "end": "1884790"
  },
  {
    "text": "be more cost-effective even if it if it grates against your software engineering",
    "start": "1884790",
    "end": "1889890"
  },
  {
    "text": "design best practices and design patterns which it does on mine but nonetheless sometimes you just have to",
    "start": "1889890",
    "end": "1895740"
  },
  {
    "text": "do that so okay so when do we go with the ec2 based solution and when do we go",
    "start": "1895740",
    "end": "1901650"
  },
  {
    "text": "with lambda so I'm just gonna walk through this slide really quickly here so first let me have a caveat let me",
    "start": "1901650",
    "end": "1907290"
  },
  {
    "text": "just say this caveat we have pre purchased ec2 reserved instances so",
    "start": "1907290",
    "end": "1913200"
  },
  {
    "text": "sometimes that does play into whether or not we would go with ec2 or lambda because we don't want to have ec2",
    "start": "1913200",
    "end": "1918240"
  },
  {
    "text": "instances laying around that are unused so even if we would like to use lambda sometimes we just go with what we've",
    "start": "1918240",
    "end": "1924690"
  },
  {
    "text": "pre-purchased so with that you know starting with instance count ec2",
    "start": "1924690",
    "end": "1930240"
  },
  {
    "text": "instance account cost as a function of instance count you know as you go right-to-left there from left to right",
    "start": "1930240",
    "end": "1935310"
  },
  {
    "text": "as we increase than ordered instances the cost goes up accordingly performance then is also you know a",
    "start": "1935310",
    "end": "1941490"
  },
  {
    "text": "function of cpu count yet more cpus take advantage of those your performance increases but for us the perceived",
    "start": "1941490",
    "end": "1948540"
  },
  {
    "text": "operational overhead really grows exponentially I should have been more of a proper parabola really of that support",
    "start": "1948540",
    "end": "1955770"
  },
  {
    "text": "that's required to really manage these systems this again comes back to that operational component at the lab that we",
    "start": "1955770",
    "end": "1962580"
  },
  {
    "text": "are trying to avoid and so and I put perceived because you have the hard",
    "start": "1962580",
    "end": "1967650"
  },
  {
    "text": "numbers you know so and so spend so many hours you know to to work on something but then you have that intangible factor",
    "start": "1967650",
    "end": "1974700"
  },
  {
    "text": "maybe we don't talk about all that much that's stress like you know you have this cluster running out there you know this experiments running it's gonna be",
    "start": "1974700",
    "end": "1980580"
  },
  {
    "text": "running all weekend long it's a three-day weekend it's in the back your mind should you go check it I don't want to check my email you know and so those",
    "start": "1980580",
    "end": "1986460"
  },
  {
    "text": "little things were being that were you know we're not an operational shop you know the ability to then kind of let",
    "start": "1986460",
    "end": "1994050"
  },
  {
    "text": "that go and has been has just been great been really helpful so",
    "start": "1994050",
    "end": "1999900"
  },
  {
    "text": "the lambda based solution you know cost in in performance or hand in hand you pay for what you use but then that",
    "start": "1999900",
    "end": "2005960"
  },
  {
    "text": "operational support is really a constant so you have that initial hit in the beginning where you do development in",
    "start": "2005960",
    "end": "2011330"
  },
  {
    "text": "the debugging and the the testing and you got to get everything tweaked and the parameters but then after that",
    "start": "2011330",
    "end": "2017240"
  },
  {
    "text": "there's really not much to it and it's it's it doesn't matter if there's one function running or a thousand it's on",
    "start": "2017240",
    "end": "2024320"
  },
  {
    "text": "AWS I mean they're the ones who are supporting this infrastructure so you don't really have to worry about it which is that's just been great and",
    "start": "2024320",
    "end": "2030920"
  },
  {
    "text": "that's been really that that sweet spot for us that we've been taking advantage of really like that on the note of the",
    "start": "2030920",
    "end": "2038720"
  },
  {
    "text": "lambda development cycle you know one of the things just that we've run into is just making sure that your lambda",
    "start": "2038720",
    "end": "2044450"
  },
  {
    "text": "functions are really well tested before you deploy them because you don't have a lot of visibility into the lambda",
    "start": "2044450",
    "end": "2049790"
  },
  {
    "text": "environment and so oftentimes a lambda function will fail for some reason it can be really hard to figure out why it",
    "start": "2049790",
    "end": "2056388"
  },
  {
    "text": "failed and so making sure you know you really unit test your code really hard",
    "start": "2056389",
    "end": "2061460"
  },
  {
    "text": "in your code locally on your system prior to deploying it to lambda will",
    "start": "2061460",
    "end": "2067128"
  },
  {
    "text": "help you out a lot okay exploring other",
    "start": "2067129",
    "end": "2072138"
  },
  {
    "text": "server list technologies this is just a quick quick thing on Amazon Athena we've",
    "start": "2072139",
    "end": "2077240"
  },
  {
    "text": "been using Athena more and more in the last few months it's really become a great way for us to explore data in",
    "start": "2077240",
    "end": "2083388"
  },
  {
    "text": "Amazon s3 and as I said earlier that's become our landing spot for new data",
    "start": "2083389",
    "end": "2090020"
  },
  {
    "text": "sources as they come in and we get sample data you know we get data from sensors the lab does a lot of different research in a lot of different areas and",
    "start": "2090020",
    "end": "2096830"
  },
  {
    "text": "we can you know from sponsors our government customers and whatnot so having something like Athena that we've",
    "start": "2096830",
    "end": "2105110"
  },
  {
    "text": "been using more to explore that data run you know even if it's just simple queries aggregations summaries",
    "start": "2105110",
    "end": "2111430"
  },
  {
    "text": "correlating the data with other data sources extracted doing data extractions for some timeframe",
    "start": "2111430",
    "end": "2117560"
  },
  {
    "text": "these are all the things we've been using Athena for and it's really complements our internal Hadoop cluster so prior to this we would data would",
    "start": "2117560",
    "end": "2124790"
  },
  {
    "text": "come even if we ingested the data out on AWS we would still bring it into our internal Hadoop cluster so analysts",
    "start": "2124790",
    "end": "2130640"
  },
  {
    "text": "could look at and researchers could run jobs and functions and we still do that we still have an eternal infrastructure but we can be a",
    "start": "2130640",
    "end": "2138380"
  },
  {
    "text": "lot more strategic about what data we bring in now we don't bring in you know the everything the kitchen sink we can",
    "start": "2138380",
    "end": "2145430"
  },
  {
    "text": "be more strategic about that or bring in just maybe like you know just a subset of the data or whatnot",
    "start": "2145430",
    "end": "2151819"
  },
  {
    "text": "if you're familiar with hive and Hadoop you will be right at home with Athena if",
    "start": "2151819",
    "end": "2157040"
  },
  {
    "text": "you're familiar with SQL it won't take you long at all to get up to speed on Athena and get going on it",
    "start": "2157040",
    "end": "2163190"
  },
  {
    "text": "you just create a simple schema point it to an s3 directory and if it's one of",
    "start": "2163190",
    "end": "2168980"
  },
  {
    "text": "the supported data types doesn't matter if the data zipped up or or or not well",
    "start": "2168980",
    "end": "2174619"
  },
  {
    "text": "it does depend on the zip type it needs to be a gzip or one of the supported zip types but anyway it can it you know just",
    "start": "2174619",
    "end": "2182540"
  },
  {
    "text": "like Hadoop if you remember familiar with Hadoop it just magically queries your data and then yeah so we were using",
    "start": "2182540",
    "end": "2190099"
  },
  {
    "text": "you know we were writing our own schemas for a while and then we found AWS glue and with their crawlers and B know that",
    "start": "2190099",
    "end": "2195530"
  },
  {
    "text": "point those to an s3 directory and have it extract out a schema for us for their support of data types has been has been",
    "start": "2195530",
    "end": "2201770"
  },
  {
    "text": "great we do a lot of a lot of work with JSON so it handles JSON pretty well we",
    "start": "2201770",
    "end": "2206869"
  },
  {
    "text": "we do have a little bit ax XML data that hasn't the web crawlers don't like that as much so we still have to kind of do",
    "start": "2206869",
    "end": "2212540"
  },
  {
    "text": "that one by hand but it's been it's it's more and more we're leaning on these",
    "start": "2212540",
    "end": "2217700"
  },
  {
    "text": "technologies so alright with that I'm gonna hand it off back to Mike and he's going to talk about moving forward with",
    "start": "2217700",
    "end": "2223730"
  },
  {
    "text": "serverless at the lab in helping enable research soon thanks Rob so we've been",
    "start": "2223730",
    "end": "2229880"
  },
  {
    "text": "talking a lot about the efforts that we're putting into or the collaborations are trying to have with our researchers",
    "start": "2229880",
    "end": "2235250"
  },
  {
    "text": "and constantly trying to look at how we can improve these research environments in these environments that we can position the researchers so they can do",
    "start": "2235250",
    "end": "2242000"
  },
  {
    "text": "this novel new innovative work and then give it a path for if there is a case where we have something that we want to",
    "start": "2242000",
    "end": "2247790"
  },
  {
    "text": "transition or we're try to deploy in another space and so what I want to",
    "start": "2247790",
    "end": "2253369"
  },
  {
    "text": "highlight here is I want to at least highlight some of the things that we're doing this is a new effort that we're working on and what we're trying to do",
    "start": "2253369",
    "end": "2258619"
  },
  {
    "text": "is we're trying to build out a basic data Lake and that may be seems straightforward to some of you in the",
    "start": "2258619",
    "end": "2264079"
  },
  {
    "text": "room but one of the key challenges that our researchers face is active time's access to data it really is of",
    "start": "2264079",
    "end": "2269700"
  },
  {
    "text": "course difficult for them to build some of these capabilities if they don't have access to the data that they need to",
    "start": "2269700",
    "end": "2275070"
  },
  {
    "text": "work through some of these problems and so some of the things that we're doing is of course trying to load up this",
    "start": "2275070",
    "end": "2280230"
  },
  {
    "text": "environment with data but then we're also trying to get people to additionally tag metadata to these data",
    "start": "2280230",
    "end": "2286260"
  },
  {
    "text": "sources so that one you know what the source is to you maybe have some better understanding of what's in there so",
    "start": "2286260",
    "end": "2291630"
  },
  {
    "text": "things like known events or known occurrences of things so if you're working of course for things like cyber data or social media data whatever the",
    "start": "2291630",
    "end": "2298140"
  },
  {
    "text": "case may be you have some insight into what's available to you as a researcher and what events and things you have so",
    "start": "2298140",
    "end": "2303869"
  },
  {
    "text": "that if you are wanting to vet one of your capabilities to something like an algorithm against this data you have a",
    "start": "2303869",
    "end": "2309990"
  },
  {
    "text": "little bit you have more insight into what's there and in a process for being able to do that some of the other things",
    "start": "2309990",
    "end": "2315390"
  },
  {
    "text": "that we've done is we're building out mechanisms for them to actually do things like start/stop replay this data",
    "start": "2315390",
    "end": "2322349"
  },
  {
    "text": "so that they can wire up their algorithms to it and use that as a method to do this evaluation or vetting",
    "start": "2322349",
    "end": "2328050"
  },
  {
    "text": "process some of the other key pieces here though or that not only is it giving them more ability to do that and",
    "start": "2328050",
    "end": "2334440"
  },
  {
    "text": "having our engineers in the mix it's also give making it easier for our samhitas like our subject matter experts",
    "start": "2334440",
    "end": "2340020"
  },
  {
    "text": "or our domain experts whether there's a lab or somewhere else to be able to take a look at these capabilities and help us",
    "start": "2340020",
    "end": "2345510"
  },
  {
    "text": "really shape them we're all we're always trying to improve this research engineering collaboration but we're also",
    "start": "2345510",
    "end": "2351000"
  },
  {
    "text": "trying to make it easier to bring in some of our this means in domain experts like I mentioned so that they can help",
    "start": "2351000",
    "end": "2356250"
  },
  {
    "text": "us shape these problems upfront that's also one of the other challenges we have and then of course as ralph mentioned we're leveraging other technologies",
    "start": "2356250",
    "end": "2363150"
  },
  {
    "text": "dominative us things like Athena and glue to additionally augment that environment and provide researchers with",
    "start": "2363150",
    "end": "2368700"
  },
  {
    "text": "other capabilities or tools so that they can do things like data expose exploration analysis and extraction and",
    "start": "2368700",
    "end": "2375330"
  },
  {
    "text": "then leverage things like glue for like the schema generation things like that as ralph mentioned then the other parts",
    "start": "2375330",
    "end": "2380609"
  },
  {
    "text": "of this is because of the way we're building it and how we're going about it this should make any capabilities that",
    "start": "2380609",
    "end": "2385740"
  },
  {
    "text": "we build more suitable or hopefully easier to transition in other places and a good example of this is one of our",
    "start": "2385740",
    "end": "2392010"
  },
  {
    "text": "researchers researchers was really work recently working on an detection algorithm and they built it in",
    "start": "2392010",
    "end": "2398779"
  },
  {
    "text": "MATLAB and one of the things that we did then is we paired them with one of our engineers they work together they ported",
    "start": "2398779",
    "end": "2404509"
  },
  {
    "text": "it to Java and then we helped them actually put both a spring boot and lambda wrapper around it so that they",
    "start": "2404509",
    "end": "2410480"
  },
  {
    "text": "can deploy it out in this environment and then we could use that as a mechanism to see if this environment",
    "start": "2410480",
    "end": "2415700"
  },
  {
    "text": "would support our ability to integrate their algorithm to some of the data sources that we had available and it worked really well the outcome of that",
    "start": "2415700",
    "end": "2423079"
  },
  {
    "text": "was obviously initially good and then the other part to it is that now they have a mechanism in a pattern other",
    "start": "2423079",
    "end": "2428150"
  },
  {
    "text": "researchers as well to see how we ported and distributed algorithms that they built out into this space and leverage",
    "start": "2428150",
    "end": "2433999"
  },
  {
    "text": "that is a way to to move there hopefully more effectively progress there things forward and/or see what types of things",
    "start": "2433999",
    "end": "2439579"
  },
  {
    "text": "they new need to do to improve them along the way so and then lastly Ralph",
    "start": "2439579",
    "end": "2444799"
  },
  {
    "text": "and I want to go through a few lessons learned that we've had along the way both in terms of trying to be more or",
    "start": "2444799",
    "end": "2451039"
  },
  {
    "text": "focus more on a cloud first environment and then really focusing on the use of things like serverless technologies that",
    "start": "2451039",
    "end": "2457519"
  },
  {
    "text": "it abuse provides so as I mentioned we really are leaning more and more on it serverless technologies a lot of which",
    "start": "2457519",
    "end": "2464930"
  },
  {
    "text": "is their resource constraints and frustrations that we've had both with physical hardware and people a lot of",
    "start": "2464930",
    "end": "2470450"
  },
  {
    "text": "our government customers are now really asking us for this - they really want us to build things where appropriate or we",
    "start": "2470450",
    "end": "2475730"
  },
  {
    "text": "can using those technologies because they have a lot of the same challenges that we do they have challenges with",
    "start": "2475730",
    "end": "2482210"
  },
  {
    "text": "getting the physical resources and staff to maintain and keep the upkeep of the capabilities it deployed and developed",
    "start": "2482210",
    "end": "2489049"
  },
  {
    "text": "within their space and then we want to make it easier for our engineers and researchers of course to do this",
    "start": "2489049",
    "end": "2494900"
  },
  {
    "text": "collaboration but also to establish these these patterns or blueprints if you will and processes so that we can",
    "start": "2494900",
    "end": "2500960"
  },
  {
    "text": "streamline our approaches to these efforts in addition you know we want to",
    "start": "2500960",
    "end": "2507049"
  },
  {
    "text": "focus on solving problems like Mike said so you know in the use case we went through that our goal was not just to",
    "start": "2507049",
    "end": "2513739"
  },
  {
    "text": "create a cool data flow or have a lot of infrastructure it was really to enable research with",
    "start": "2513739",
    "end": "2519780"
  },
  {
    "text": "so they could do image classification and work through those models and whatnot and so AWS more and more it",
    "start": "2519780",
    "end": "2525840"
  },
  {
    "text": "allows us to do that it allows us to lean to to really focus on the capabilities we're trying to provide",
    "start": "2525840",
    "end": "2531359"
  },
  {
    "text": "rather than the actual then rather than maintaining a ton of infrastructure and",
    "start": "2531359",
    "end": "2537570"
  },
  {
    "text": "again you know leveraging the AWS environment provides more and I mean the",
    "start": "2537570",
    "end": "2542700"
  },
  {
    "text": "number of tools is almost its examining I don't know how much you've been on AWS",
    "start": "2542700",
    "end": "2548910"
  },
  {
    "text": "every time you go out there it's like a new tool and just being able to leverage all those new services and whatnot has",
    "start": "2548910",
    "end": "2554099"
  },
  {
    "text": "been has been really helpful we've been really pleased with the performance now",
    "start": "2554099",
    "end": "2560099"
  },
  {
    "text": "that we're familiar with all these services we can use them to cobble together solutions quickly even if it's",
    "start": "2560099",
    "end": "2566340"
  },
  {
    "text": "just to kind of test a test an idea or a scenario or an experiment or pilot and effort so brute force if you will our",
    "start": "2566340",
    "end": "2574470"
  },
  {
    "text": "way through an effort to see if it works and then go back and refine it from there just being able to tie these services",
    "start": "2574470",
    "end": "2579900"
  },
  {
    "text": "together and then you have to find what's most cost-effective for your use cases so every environments different",
    "start": "2579900",
    "end": "2586109"
  },
  {
    "text": "every problem is different every company and organization is different so you got to figure out what works when it's going",
    "start": "2586109",
    "end": "2591930"
  },
  {
    "text": "to be economical and when it's going to work well for you and when it when it's not so we talked about a couple cases where you know one where it was and one",
    "start": "2591930",
    "end": "2598380"
  },
  {
    "text": "where it wasn't and for us definitely that operational support element is a",
    "start": "2598380",
    "end": "2603810"
  },
  {
    "text": "big deal and so is the scaling so those are the two spots where for us if if",
    "start": "2603810",
    "end": "2610080"
  },
  {
    "text": "there's gonna be a lot of operational support required or we have massive scaling requirement lambda is gonna be a",
    "start": "2610080",
    "end": "2615540"
  },
  {
    "text": "really really good choice for us our go-to stack internally is these",
    "start": "2615540",
    "end": "2622080"
  },
  {
    "text": "technologies listed here we we do of course use others we use we didn't mention RDS but we do use RDS in fact",
    "start": "2622080",
    "end": "2628740"
  },
  {
    "text": "that the diagram where the data lambdas writing to sort of like an echo there's",
    "start": "2628740",
    "end": "2634710"
  },
  {
    "text": "just me okay the the the diagram where",
    "start": "2634710",
    "end": "2640200"
  },
  {
    "text": "the data's being written into lambda we've also used that we've also used RDS Aurora with that and that worked as well",
    "start": "2640200",
    "end": "2646080"
  },
  {
    "text": "I was a different different environment but you know some other technologies you",
    "start": "2646080",
    "end": "2652759"
  },
  {
    "text": "know like elasticsearch and whatnot you know just these are these given a problem we'll go to these also we use",
    "start": "2652759",
    "end": "2658759"
  },
  {
    "text": "because we're java shop you know we spring quite a bit spring boot to set up web services and whatnot a lot of rest",
    "start": "2658759",
    "end": "2665509"
  },
  {
    "text": "services we do as well also take advantage of the built-in events and triggers that are in some of the",
    "start": "2665509",
    "end": "2671539"
  },
  {
    "text": "services like s3 or dynamo it's like it's built-in capability that you're",
    "start": "2671539",
    "end": "2677749"
  },
  {
    "text": "getting you know almost for free you know imagine if you will you know writing code to you know watch a",
    "start": "2677749",
    "end": "2683599"
  },
  {
    "text": "directory and if you get a new file you got to send a notification you know that ad rent event-driven programming model",
    "start": "2683599",
    "end": "2689209"
  },
  {
    "text": "well you get that built in with with with AWS and some of their services so",
    "start": "2689209",
    "end": "2694400"
  },
  {
    "text": "take advantage of that and then we find ourselves abandoning a lot of our",
    "start": "2694400",
    "end": "2699529"
  },
  {
    "text": "favorite technologies in favor of AWS because just because of the simplicity",
    "start": "2699529",
    "end": "2705979"
  },
  {
    "text": "and using them so like we were we were big users of Kafka for a long time but",
    "start": "2705979",
    "end": "2711769"
  },
  {
    "text": "just again that operational support of a Kafka cluster and the zoo keepers and whatnot and so with back to die fi you",
    "start": "2711769",
    "end": "2719299"
  },
  {
    "text": "know knife I we find you know site to site knife I has a setting nice site to",
    "start": "2719299",
    "end": "2724339"
  },
  {
    "text": "site that you can figure where one knife I can talk to another if if that's all that we're doing that works really well",
    "start": "2724339",
    "end": "2730069"
  },
  {
    "text": "and we don't even need a messaging backbone in there or we'll use sqs instead Kafka is really great when we",
    "start": "2730069",
    "end": "2737180"
  },
  {
    "text": "got a high volume and you have a lot of different clients wanting to pull off",
    "start": "2737180",
    "end": "2743660"
  },
  {
    "text": "that same topic so that's still a use case that we we use that for so and then",
    "start": "2743660",
    "end": "2751989"
  },
  {
    "text": "we're trying to request people actually change the way that you do things more possible a good example of that is of",
    "start": "2751989",
    "end": "2757849"
  },
  {
    "text": "course this collaboration effort between our researchers and engineers and having them do things like hey if you're working on an effort you're working on a",
    "start": "2757849",
    "end": "2764390"
  },
  {
    "text": "capability and you're collecting data along the way try to apply approaches even if their basic best practices and",
    "start": "2764390",
    "end": "2771289"
  },
  {
    "text": "things like this data like where you are storing maybe multiple copies of the data so that we can have the raw enriched maybe even more copies",
    "start": "2771289",
    "end": "2778950"
  },
  {
    "text": "not just simply so that we can replay data reproduce it if we find issues and again this environment really supports",
    "start": "2778950",
    "end": "2785130"
  },
  {
    "text": "that it's pretty cheap to do that it's really cheap for us to have lots of copies of things so that we can get back to the raw if we need to and then the",
    "start": "2785130",
    "end": "2791880"
  },
  {
    "text": "other thing is just having them do things like tag and add events and other metadata and identify things like time",
    "start": "2791880",
    "end": "2797490"
  },
  {
    "text": "frames sort of another researchers to come in or wants to come in they can and makes that a lot easier for them to come",
    "start": "2797490",
    "end": "2803220"
  },
  {
    "text": "into this environment and actually bet or check their things out or maybe even augment or improve upon a pre-existing",
    "start": "2803220",
    "end": "2809069"
  },
  {
    "text": "capability and then lastly it's just adding other functionality that really helps them or maybe it would help others",
    "start": "2809069",
    "end": "2814440"
  },
  {
    "text": "this like start/stop replay that kind of stuff so you know with that we really",
    "start": "2814440",
    "end": "2819510"
  },
  {
    "text": "wanted to thank everybody for the opportunity and being able to speak to you here Ralph and I care a great deal about the missions and the problem",
    "start": "2819510",
    "end": "2825780"
  },
  {
    "text": "spaces that we support for our government customers in the in abeyance environment has really made a big",
    "start": "2825780",
    "end": "2830910"
  },
  {
    "text": "difference for us it's really changed our life if you will in our ability to approach some of the capabilities we",
    "start": "2830910",
    "end": "2836940"
  },
  {
    "text": "have and also really allows us to focus on the problems at hand really really made a huge difference for us and so",
    "start": "2836940",
    "end": "2843690"
  },
  {
    "text": "with that we want to open it if we have a few more minutes here if anybody has any questions",
    "start": "2843690",
    "end": "2849650"
  }
]