[
  {
    "start": "0",
    "end": "700"
  },
  {
    "text": "I am Balwanth Bobilli, Specialist Solutions Architect with AWS. And my topic today is",
    "start": "700",
    "end": "6733"
  },
  {
    "start": "6733",
    "end": "6932"
  },
  {
    "text": "about the Amazon EventBridge pipes integration with Amazon Timesteam for Analytics.",
    "start": "6933",
    "end": "11333"
  },
  {
    "start": "11333",
    "end": "13466"
  },
  {
    "text": "I will talk briefly about Timestream for LiveAnalytics and EventBridge pipes. And how this new",
    "start": "13466",
    "end": "19599"
  },
  {
    "text": "integration will help you to ingest Time series data efficiently. I will also perform a demo on how",
    "start": "19600",
    "end": "25666"
  },
  {
    "start": "25666",
    "end": "25699"
  },
  {
    "text": "this integration can be set up and also be validated",
    "start": "25700",
    "end": "31833"
  },
  {
    "text": "Quick Introduction about Amazon Timestream for LiveAnalytics, it is a fast, scalable and serverless time-series",
    "start": "31833",
    "end": "38166"
  },
  {
    "start": "38166",
    "end": "38199"
  },
  {
    "text": "database that makes it straightforward and cost effective too store, analyze trillions",
    "start": "38200",
    "end": "44333"
  },
  {
    "text": "of events per day. You can use Time stream for Analytics for use cases like",
    "start": "44333",
    "end": "50633"
  },
  {
    "start": "50633",
    "end": "50666"
  },
  {
    "text": "monitoring, 100s of millions of IoT devices, industrial equipments, gaming sessions,",
    "start": "50666",
    "end": "56399"
  },
  {
    "start": "56400",
    "end": "56866"
  },
  {
    "text": "streaming video sessions, financial, log analytics, and many more. With",
    "start": "56866",
    "end": "62966"
  },
  {
    "start": "62966",
    "end": "62999"
  },
  {
    "text": "Timestream for LiveAnalytics, you can ingest 10's of gigabytes of timeseries data",
    "start": "63000",
    "end": "68833"
  },
  {
    "start": "68833",
    "end": "69233"
  },
  {
    "text": "per minute and run SQL queries on terabytes of timeseries data in seconds. With high",
    "start": "69233",
    "end": "75266"
  },
  {
    "start": "75266",
    "end": "75298"
  },
  {
    "text": "availability. Now coming to EventBridge pipes, it is intended for point-to-point",
    "start": "75300",
    "end": "81866"
  },
  {
    "start": "81866",
    "end": "81898"
  },
  {
    "text": "integration between supported sources and targets, which support for advanced transformation, enrichment and",
    "start": "81900",
    "end": "87933"
  },
  {
    "start": "87933",
    "end": "87965"
  },
  {
    "text": "filtering. So you do not need another service for performing transformation or enrichments. EventBridge Pipes ",
    "start": "87966",
    "end": "94266"
  },
  {
    "start": "94266",
    "end": "94566"
  },
  {
    "text": "reduces the need of specialized knowledge and integration code when developing, event-driven architectures ",
    "start": "94566",
    "end": "100499"
  },
  {
    "start": "100500",
    "end": "100800"
  },
  {
    "text": "fostering consistency across your company's application. Most of Timeseries use",
    "start": "100800",
    "end": "106833"
  },
  {
    "text": "cases require data pipeline to support ingestion of real-time data using data stream and then processing",
    "start": "106833",
    "end": "113066"
  },
  {
    "start": "113066",
    "end": "113466"
  },
  {
    "text": "and storing them into Timestream for LiveAnalytics. Previously you might require out-of-box adapters",
    "start": "113466",
    "end": "119532"
  },
  {
    "text": "such as Apache Flink adapter or customized code to transfer Timeseries data from supported",
    "start": "119533",
    "end": "125533"
  },
  {
    "start": "125533",
    "end": "125565"
  },
  {
    "text": "AWS sources to Timestream for LiveAnalytics. With this new integration you have flexible,",
    "start": "125566",
    "end": "131566"
  },
  {
    "text": "low-code, no-code configuration-based solution to ingest data into Timestream for LiveAnalytics.",
    "start": "131566",
    "end": "137465"
  },
  {
    "start": "137466",
    "end": "137766"
  },
  {
    "text": "The supported sources for the integration are Kinesis data streams, Amazon DynamoDB, Amazon MQ, Amazon SQS,",
    "start": "137766",
    "end": "144199"
  },
  {
    "start": "144200",
    "end": "144400"
  },
  {
    "text": "which is Simple Queue Service. I will move on to show the",
    "start": "144400",
    "end": "150465"
  },
  {
    "start": "150466",
    "end": "152099"
  },
  {
    "text": "demo. I'll show within few clicks how we can create Timestream for LiveAnalytics database and table. Then I'll show how you",
    "start": "152100",
    "end": "158200"
  },
  {
    "text": "how to configure EventBridge Pipes to ingest data from KNSS. For this demo I will use Kinesis",
    "start": "158200",
    "end": "164300"
  },
  {
    "start": "164300",
    "end": "164333"
  },
  {
    "text": "data generator to generate sample data. So the data will be going into kinesis data streams.",
    "start": "164333",
    "end": "170133"
  },
  {
    "start": "170133",
    "end": "170366"
  },
  {
    "text": "Then we will use configured pipes to seeing configured Pipes to directly ",
    "start": "170366",
    "end": "176765"
  },
  {
    "text": "getting data ingested into Amazon Timestream for LiveAnalytics . Amazon Kinesis data generator",
    "start": "176766",
    "end": "182766"
  },
  {
    "text": "which we're using to generate data is a UI that simplifies how you send as data to Amazon Kinesis streams",
    "start": "182766",
    "end": "188866"
  },
  {
    "start": "188866",
    "end": "189299"
  },
  {
    "text": "or even Amazon Kinesis Firehose. Using the Amazon Kinesis data generator, you can create templates",
    "start": "189300",
    "end": "195600"
  },
  {
    "text": "for your data, create random values to use for your data and save the templates for future use.",
    "start": "195600",
    "end": "201100"
  },
  {
    "start": "201100",
    "end": "202433"
  },
  {
    "text": "I'm in Amazon Timestream console to start with as mentioned, I'll create a database and table. So I'm in",
    "start": "202433",
    "end": "208466"
  },
  {
    "start": "208466",
    "end": "208499"
  },
  {
    "text": "the database tab, I'll go ahead and create a database. A couple of options in",
    "start": "208500",
    "end": "214566"
  },
  {
    "start": "214566",
    "end": "214599"
  },
  {
    "text": "database configurations, one is standard database and another is sample database, with sample database,",
    "start": "214600",
    "end": "220333"
  },
  {
    "start": "220333",
    "end": "220898"
  },
  {
    "text": "we can get like a couple of sample data. That is IoT data or DevOps data, but for this demo",
    "start": "220900",
    "end": "227000"
  },
  {
    "text": "I'm going to choose a standard database. And I'm going to give the database name",
    "start": "227000",
    "end": "232666"
  },
  {
    "start": "232666",
    "end": "233099"
  },
  {
    "text": "as a_vehiclemetrics",
    "start": "233100",
    "end": "235766"
  },
  {
    "start": "235766",
    "end": "242466"
  },
  {
    "text": "and for encryption, I'm going to leave it as a default",
    "start": "242466",
    "end": "248899"
  },
  {
    "text": "encryption. And if I want to add tags, we can definitely",
    "start": "248900",
    "end": "254900"
  },
  {
    "start": "254900",
    "end": "254933"
  },
  {
    "text": "add tags here, test test. So that's all. ",
    "start": "254933",
    "end": "260999"
  },
  {
    "start": "261000",
    "end": "261032"
  },
  {
    "text": "Within few options, once we click on create database, as we should be able",
    "start": "261033",
    "end": "267099"
  },
  {
    "start": "267100",
    "end": "267133"
  },
  {
    "text": "to create a database. Ok, let me find my database",
    "start": "267133",
    "end": "272766"
  },
  {
    "start": "272766",
    "end": "273298"
  },
  {
    "text": "this is my database.",
    "start": "273300",
    "end": "274465"
  },
  {
    "start": "274466",
    "end": "281299"
  },
  {
    "text": "I'll go ahead and start creating the table, I will switch to tables tab and create a click on create table.",
    "start": "281300",
    "end": "287432"
  },
  {
    "start": "287433",
    "end": "288233"
  },
  {
    "text": "I can go ahead and give a name for the table,  vehiclemetrics",
    "start": "288233",
    "end": "294233"
  },
  {
    "text": "vehiclemetrics",
    "start": "294233",
    "end": "300499"
  },
  {
    "start": "300500",
    "end": "301633"
  },
  {
    "text": "Then move on to schema settings. Timestream is not a rigid schema, so you don't have to",
    "start": "301633",
    "end": "307866"
  },
  {
    "start": "307866",
    "end": "307899"
  },
  {
    "text": "to pre-defined your columns, but as part of the schema settings, you can choose which part, which",
    "start": "307900",
    "end": "314100"
  },
  {
    "text": "is the partitioning key, you would like to use for this particular table. There are couple",
    "start": "314100",
    "end": "320200"
  },
  {
    "text": "of options. One is custom partitioning and the other one is default partition. For the default partition,",
    "start": "320200",
    "end": "326233"
  },
  {
    "text": "it basically goes through the default Timestream column, which is measure_name.",
    "start": "326233",
    "end": "332033"
  },
  {
    "start": "332033",
    "end": "332798"
  },
  {
    "text": "We recommend always to use customer-defined partition key so that the key",
    "start": "332800",
    "end": "338832"
  },
  {
    "start": "338833",
    "end": "338866"
  },
  {
    "text": "which is highly cardinal and often used for querying. There are",
    "start": "338866",
    "end": "345031"
  },
  {
    "start": "345033",
    "end": "345065"
  },
  {
    "text": "a couple basic concepts in Timestream for LiveAnalytics. One is dimension",
    "start": "345066",
    "end": "350499"
  },
  {
    "start": "350500",
    "end": "351266"
  },
  {
    "text": "and the other is measure. So, to understand this let's take a simple example.",
    "start": "351266",
    "end": "356332"
  },
  {
    "start": "356333",
    "end": "357633"
  },
  {
    "text": "let's say there is a device that is capturing humidity, temperature and pressure. The device",
    "start": "357633",
    "end": "363666"
  },
  {
    "start": "363666",
    "end": "363698"
  },
  {
    "text": "ID or device location, which consistently remains the same will be dimensions and whatever",
    "start": "363700",
    "end": "370066"
  },
  {
    "start": "370066",
    "end": "370099"
  },
  {
    "text": "the device is capturing humidity, temperature and pressure. Those become the measures. So, the dimensions",
    "start": "370100",
    "end": "376366"
  },
  {
    "start": "376366",
    "end": "376399"
  },
  {
    "text": "are the metadata of the data and the measures are the ones which will help you perform mathematical",
    "start": "376400",
    "end": "382300"
  },
  {
    "start": "382300",
    "end": "382565"
  },
  {
    "text": "aggregations or calculation. So, that is changing over the",
    "start": "382566",
    "end": "388699"
  },
  {
    "text": "time, those are the measures. So, for this demo, I was planning to ingest vehicle metrics.",
    "start": "388700",
    "end": "395233"
  },
  {
    "start": "395233",
    "end": "395466"
  },
  {
    "text": "So, I will go ahead and choose vehicle_id as my partition key.",
    "start": "395466",
    "end": "400866"
  },
  {
    "start": "400866",
    "end": "401632"
  },
  {
    "text": "This is an high cardinal column and generally this is used for all of the queries.",
    "start": "401633",
    "end": "408233"
  },
  {
    "text": "So, that's why I chose vehicle_id as my partition key. Now, moving on to the data retention,",
    "start": "408233",
    "end": "414599"
  },
  {
    "start": "414600",
    "end": "415400"
  },
  {
    "text": "there are a couple of settings. We have to set here. One is memory store retention, and the other",
    "start": "415400",
    "end": "421466"
  },
  {
    "text": "is magnetic store retention. I'll quickly talk about what these are. So, memory store, in Timestream for LiveAnalytics",
    "start": "421466",
    "end": "427499"
  },
  {
    "start": "427500",
    "end": "427533"
  },
  {
    "text": "is designed for point-in-time queries and high ingestion throughput. In magnetic,",
    "start": "427533",
    "end": "434166"
  },
  {
    "text": "on the other side, memory is not more optimized than magnetic, that",
    "start": "434166",
    "end": "440166"
  },
  {
    "text": "just is designed for different purposes. magnetic is still, up efficieient for like",
    "start": "440166",
    "end": "446198"
  },
  {
    "text": "analytical queries and it's used for archival purposes.",
    "start": "446200",
    "end": "451300"
  },
  {
    "start": "451300",
    "end": "452666"
  },
  {
    "text": "So, I'll go and uh, keep this our retention's by default, but you can absolutely change, the",
    "start": "452666",
    "end": "458698"
  },
  {
    "text": "memory retention starting from 1 hour to like 12 months and from my magnetic retention, starting",
    "start": "458700",
    "end": "464733"
  },
  {
    "start": "464733",
    "end": "464766"
  },
  {
    "text": "from 1 day to 200 years. Now, what",
    "start": "464766",
    "end": "470931"
  },
  {
    "text": "happens when you set the retention's, based ased on the time of the data, the data is automatically",
    "start": "470933",
    "end": "477233"
  },
  {
    "start": "477233",
    "end": "477266"
  },
  {
    "text": "moved from memory store to magnetic store and once retention is reached on the magnetic store, the data is automatically",
    "start": "477266",
    "end": "483866"
  },
  {
    "start": "483866",
    "end": "483899"
  },
  {
    "text": "purged out. Okay, you can enable magnetic store writes also, if you're loading",
    "start": "483900",
    "end": "490133"
  },
  {
    "text": "data, older than memory storage, the data  directly goes and sits in magnetic store, which you can enable by keeping this",
    "start": "490133",
    "end": "496333"
  },
  {
    "start": "496333",
    "end": "496366"
  },
  {
    "text": "button. So, I'm am keeping that as default and",
    "start": "496366",
    "end": "502466"
  },
  {
    "text": "rest of the options, so I'm not going to touch , for this demo, but you can definitely enable backups",
    "start": "502466",
    "end": "508499"
  },
  {
    "text": "as part of the table creation and also add tags. Okay,",
    "start": "508500",
    "end": "514700"
  },
  {
    "start": "514700",
    "end": "517500"
  },
  {
    "text": "now I have my table. Now on the tables tab in the Timestream console",
    "start": "517500",
    "end": "523932"
  },
  {
    "text": "page. Here, we can see the memory story attention, the magnetic store retention, how much have we configured",
    "start": "523933",
    "end": "529933"
  },
  {
    "text": "and if you want to configure the backup, you can configure the backups from here. We can see, we have",
    "start": "529933",
    "end": "536199"
  },
  {
    "start": "536200",
    "end": "536233"
  },
  {
    "text": "used custom defined partition key and we can also check the, what is our partition key, which is vehicle_id",
    "start": "536233",
    "end": "542899"
  },
  {
    "text": "in this use case. We can also start querying the tables, I click on actions",
    "start": "542900",
    "end": "549066"
  },
  {
    "start": "549066",
    "end": "549099"
  },
  {
    "text": "and I can start querying the table.",
    "start": "549100",
    "end": "551899"
  },
  {
    "start": "551900",
    "end": "556633"
  },
  {
    "text": "Yeah, I see zero rows because we haven't ingested any data so for. Let's go ahead and",
    "start": "556633",
    "end": "562666"
  },
  {
    "start": "562666",
    "end": "562699"
  },
  {
    "text": "set up the EventBridge Pipes and then we can pump in some data and come back to this console",
    "start": "562700",
    "end": "568966"
  },
  {
    "start": "568966",
    "end": "568999"
  },
  {
    "text": "and validate. To switching to EventBridge Pipes",
    "start": "569000",
    "end": "575066"
  },
  {
    "text": "within the Timestream console, have a navigation button here. If I click on this",
    "start": "575066",
    "end": "581099"
  },
  {
    "start": "581100",
    "end": "581133"
  },
  {
    "text": "it automatically takes me to the EventBridge pipes page, for where I can start clicking",
    "start": "581133",
    "end": "587199"
  },
  {
    "text": "on create pipe and start creating the EventBridge Pipes configurations.",
    "start": "587200",
    "end": "593200"
  },
  {
    "start": "593200",
    "end": "594766"
  },
  {
    "text": "And to start with, I have to give a name for the pipes, I'll just give vehiclemetrics_pipes",
    "start": "594766",
    "end": "601066"
  },
  {
    "text": "here and I have to choose source, so as I mentioned",
    "start": "601066",
    "end": "607298"
  },
  {
    "start": "607300",
    "end": "607333"
  },
  {
    "text": "before, I'm going to choose kinesis as my source for the demo. I've already",
    "start": "607333",
    "end": "613166"
  },
  {
    "start": "613166",
    "end": "613899"
  },
  {
    "text": "created a kinesis stream, which is making vehiclemetrics,",
    "start": "613900",
    "end": "617066"
  },
  {
    "start": "617066",
    "end": "621365"
  },
  {
    "text": "I'll choose batch sizes as 100, which is default  anyhow",
    "start": "621366",
    "end": "627499"
  },
  {
    "start": "627500",
    "end": "629100"
  },
  {
    "text": " I'm not going to choose batchwindow  since those are optional and the concurrent batches",
    "start": "629100",
    "end": "635233"
  },
  {
    "start": "635233",
    "end": "635266"
  },
  {
    "text": "for share, if you didn't, but there are those are definitely options, if you'd like to configure it. On on partial batch item failures,",
    "start": "635266",
    "end": "640998"
  },
  {
    "start": "641000",
    "end": "641300"
  },
  {
    "text": "I'll click on automatic_bisect. What happens by clicking on this",
    "start": "641300",
    "end": "646365"
  },
  {
    "start": "646366",
    "end": "647366"
  },
  {
    "text": "is it automatically halves each batch and  retries each half until all the records are processed",
    "start": "647366",
    "end": "653732"
  },
  {
    "text": "or only failed messages are remained in the batch. So setting automatic_bisect is recommended.",
    "start": "653733",
    "end": "660465"
  },
  {
    "start": "660466",
    "end": "662899"
  },
  {
    "text": "So the next step is setting up the filtering and enrichment. For this demo, we don't need filtering",
    "start": "662900",
    "end": "668933"
  },
  {
    "start": "668933",
    "end": "668966"
  },
  {
    "text": "and enrichment options. So we set it up directly the target. You can choose to apply",
    "start": "668966",
    "end": "675199"
  },
  {
    "text": "filtering and enrichment based on your requirements. If you",
    "start": "675200",
    "end": "681300"
  },
  {
    "text": "add an enrichment layer, the difference services support different levels",
    "start": "681300",
    "end": "686000"
  },
  {
    "start": "686000",
    "end": "687433"
  },
  {
    "text": " of batching. So  we have a lot of flexibility. Now I'm",
    "start": "687433",
    "end": "693633"
  },
  {
    "start": "693633",
    "end": "693666"
  },
  {
    "text": "going onto the target. The target is Amazon Timestream for LiveAnalytics table",
    "start": "693666",
    "end": "700066"
  },
  {
    "text": "So I choose that accordingly and the databases",
    "start": "700066",
    "end": "706199"
  },
  {
    "text": "is a_vehiclemetrics. And within the database, we have the vehiclemetrics,",
    "start": "706200",
    "end": "712433"
  },
  {
    "start": "712433",
    "end": "712466"
  },
  {
    "text": "created. Next is time field type. Here you",
    "start": "712466",
    "end": "718499"
  },
  {
    "start": "718500",
    "end": "718533"
  },
  {
    "text": "have option to choose timestamp_format or epoch based on your requirement.",
    "start": "718533",
    "end": "724133"
  },
  {
    "start": "724133",
    "end": "724666"
  },
  {
    "text": "But for this demo, I'm going to choose  timestamp_format, which will become the default time",
    "start": "724666",
    "end": "730666"
  },
  {
    "start": "730666",
    "end": "730699"
  },
  {
    "text": "value. And I'm going to set the format as ",
    "start": "730700",
    "end": "735066"
  },
  {
    "start": "735066",
    "end": "738466"
  },
  {
    "text": "like this. And the data properties",
    "start": "738466",
    "end": "744166"
  },
  {
    "start": "744166",
    "end": "744866"
  },
  {
    "text": "of kinesis is event are the field inside the data object like your original message to kinesis. So in this case,",
    "start": "744866",
    "end": "750966"
  },
  {
    "text": "it will be time vehicle ID miles. This value, I'm going to set it as $.data.connectionTime",
    "start": "750966",
    "end": "757099"
  },
  {
    "text": "So connection time key is what we send",
    "start": "757100",
    "end": "763433"
  },
  {
    "start": "763433",
    "end": "763466"
  },
  {
    "text": "it to kinesis and the way we access here in the EventBridge pipes is  $.data.connectionTime",
    "start": "763466",
    "end": "770066"
  },
  {
    "text": " And now moving on to version value. For version value, you can use latest",
    "start": "770066",
    "end": "776365"
  },
  {
    "start": "776366",
    "end": "776399"
  },
  {
    "text": "value to upsert data points in Timestream for LiveAnalytics. You can only update or add measures.",
    "start": "776400",
    "end": "782900"
  },
  {
    "text": "Dimensions cannot be updated. Each kinesis record includes a value, which is ",
    "start": "782900",
    "end": "789500"
  },
  {
    "start": "789500",
    "end": "789800"
  },
  {
    "text": "approximatearrivaltimestmap, which is what we plan to set it here. So whenever a stream successfully receives",
    "start": "789800",
    "end": "796166"
  },
  {
    "text": "and stores a record, that means it's a latest record. And we use that",
    "start": "796166",
    "end": "802266"
  },
  {
    "start": "802266",
    "end": "802299"
  },
  {
    "text": "to handle the updates. So let me put that to your accordingly",
    "start": "802300",
    "end": "806466"
  },
  {
    "start": "806466",
    "end": "808332"
  },
  {
    "text": "$.data.approximateArrivalTimestamp. Next we set up the data model.",
    "start": "808333",
    "end": "813899"
  },
  {
    "start": "813900",
    "end": "814466"
  },
  {
    "text": "This is where we specify, which keys within the kinesis message you would like to access and how you would like to store",
    "start": "814466",
    "end": "820666"
  },
  {
    "start": "820666",
    "end": "820699"
  },
  {
    "text": "the Timestream for analytics. So here we have two options. One is visual builder and another is JSON editor,",
    "start": "820700",
    "end": "827000"
  },
  {
    "start": "827000",
    "end": "827233"
  },
  {
    "text": "visual builder is flexible to give which keys you would like to access from the kinesis and how you would like to store them in Timestream for LiveAnalytics.",
    "start": "827233",
    "end": "833465"
  },
  {
    "start": "833466",
    "end": "834032"
  },
  {
    "text": "But for this demo, I'm going to choose as JSON editor. I've already have the JSON created for my use case. I'll",
    "start": "834033",
    "end": "840166"
  },
  {
    "text": "quickly walk through what this JSON contents. So there is a dimension's mapping and there is multi-measure",
    "start": "840166",
    "end": "846666"
  },
  {
    "text": "mapping. The dimension mapping is where you specify with the list of dimensions and multi-mission mapping is where you specify",
    "start": "846666",
    "end": "852732"
  },
  {
    "text": "the list of measures. In the dimension's mapping, you specify like which key",
    "start": "852733",
    "end": "858899"
  },
  {
    "text": "are you accessing within the kinesis data object and how you would like to store them in the Timestream for LiveAnalytics",
    "start": "858900",
    "end": "865066"
  },
  {
    "start": "865066",
    "end": "865099"
  },
  {
    "text": "and also the data type. Similarly, for measures, we have three measures,",
    "start": "865100",
    "end": "871133"
  },
  {
    "start": "871133",
    "end": "871166"
  },
  {
    "text": "miles, engine status, battery and the respective data type. I am going to attach",
    "start": "871166",
    "end": "877166"
  },
  {
    "start": "877166",
    "end": "877199"
  },
  {
    "text": "Timestream documentation link where you can build JSON based on your requirement.",
    "start": "877200",
    "end": "883633"
  },
  {
    "start": "883633",
    "end": "884299"
  },
  {
    "text": "Now that we have the data model, before creating the pipe, we have to do the pipe settings.",
    "start": "884300",
    "end": "889766"
  },
  {
    "start": "889766",
    "end": "890432"
  },
  {
    "text": "I would like to keep my pipe active as soon as it's created, but if you would like to disable like as soon as",
    "start": "890433",
    "end": "896498"
  },
  {
    "text": "the data and make some changes later on and then come back and activate it, that's definitely an option",
    "start": "896500",
    "end": "902300"
  },
  {
    "start": "902300",
    "end": "902666"
  },
  {
    "text": "and I move on to the permissions, you can give existing IAM role or you can let  EventBridge pipe create a new role",
    "start": "902666",
    "end": "908832"
  },
  {
    "text": "with minimum permission required for it to read the data from kinesis and write the data to Timestream for LiveAnalytics",
    "start": "908833",
    "end": "915166"
  },
  {
    "text": "and I'm gonna leave the encryption as default. Now coming to the",
    "start": "915166",
    "end": "921266"
  },
  {
    "start": "921266",
    "end": "921299"
  },
  {
    "text": "retry policies and I am going to set it as 30 minutes, so the",
    "start": "921300",
    "end": "927666"
  },
  {
    "text": "records older than 30 minutes are automatically sent to dlq and this totally depends on the use case to",
    "start": "927666",
    "end": "933732"
  },
  {
    "start": "933733",
    "end": "933766"
  },
  {
    "text": "use this. For retry attempt, I'm gonna set it as a 10 times,",
    "start": "933766",
    "end": "939599"
  },
  {
    "start": "939600",
    "end": "941199"
  },
  {
    "text": "so any transient issues that will be ",
    "start": "941200",
    "end": "947333"
  },
  {
    "start": "947333",
    "end": "947733"
  },
  {
    "text": "10 times. After the retry attempt, the any persistent issues",
    "start": "947733",
    "end": "953798"
  },
  {
    "start": "953800",
    "end": "954166"
  },
  {
    "text": "that data is automatically moved to dlq. Now coming to dlq, you ",
    "start": "954166",
    "end": "960499"
  },
  {
    "text": "can toggle the Dead-letter queue and uh I'm gonna select SQS queue",
    "start": "960500",
    "end": "966600"
  },
  {
    "text": "which I have already created. Then coming to logs and then I will stick with default",
    "start": "966600",
    "end": "973032"
  },
  {
    "start": "973033",
    "end": "973066"
  },
  {
    "text": "log option which is CloudWatch logs. And you can choose different",
    "start": "973066",
    "end": "979232"
  },
  {
    "start": "979233",
    "end": "979699"
  },
  {
    "text": "log level error, info and trace. Info gives like all the information so for this demo",
    "start": "979700",
    "end": "985733"
  },
  {
    "start": "985733",
    "end": "985766"
  },
  {
    "text": "I'm gonna choose info and would like to include exclusion data to see",
    "start": "985766",
    "end": "991832"
  },
  {
    "text": " I have any of the issues so I can capture the SDK requests and response us on everything",
    "start": "991833",
    "end": "997899"
  },
  {
    "text": "so if I would have more details to troubleshoot the issue. Coming to Cloud",
    "start": "997900",
    "end": "1003900"
  },
  {
    "start": "1003900",
    "end": "1003933"
  },
  {
    "text": "Watch logs it automatically creates a new log group unless you would like to specify an existing one ",
    "start": "1003933",
    "end": "1010032"
  },
  {
    "start": "1010033",
    "end": "1010066"
  },
  {
    "text": "and if you'd like to specify tags you can specify tags and that's it you can",
    "start": "1010066",
    "end": "1015832"
  },
  {
    "start": "1015833",
    "end": "1016299"
  },
  {
    "text": "just go ahead and create pipe and that will create and enable the pipe.",
    "start": "1016300",
    "end": "1021066"
  },
  {
    "start": "1021066",
    "end": "1022399"
  },
  {
    "text": "Now we have the EventBridge pipes created an up and running and we can go ahead and ingest data using this is kinesis data",
    "start": "1022400",
    "end": "1028400"
  },
  {
    "start": "1028400",
    "end": "1028433"
  },
  {
    "text": "generator and validate the data using Timestream for LiveAnalytics query editor. I've already installed",
    "start": "1028433",
    "end": "1034933"
  },
  {
    "start": "1034933",
    "end": "1035298"
  },
  {
    "text": " Kinesis data generator in my account though CloudFormation templates and logged in through UI which is what you see here",
    "start": "1035300",
    "end": "1041633"
  },
  {
    "start": "1041633",
    "end": "1041666"
  },
  {
    "text": "I have pre-selected the region and the stream which is what we configured in the EventBridge pipes. And accordingly the datamodel",
    "start": "1041666",
    "end": "1047732"
  },
  {
    "start": "1047733",
    "end": "1047766"
  },
  {
    "text": "like how we have configured  in EventBridge Pipes connectiontime and the dimensions and the measures we could test the",
    "start": "1047766",
    "end": "1053766"
  },
  {
    "text": "template and we could set up multiple templates. I'll start sending the data",
    "start": "1053766",
    "end": "1059866"
  },
  {
    "start": "1059866",
    "end": "1059898"
  },
  {
    "text": "the data is being streamed. Let's go ahead and validate here.",
    "start": "1059900",
    "end": "1065300"
  },
  {
    "start": "1065300",
    "end": "1066166"
  },
  {
    "text": "I'm in the Timestream for LiveAnalytics query editor. Now we see the",
    "start": "1066166",
    "end": "1071966"
  },
  {
    "start": "1071966",
    "end": "1075132"
  },
  {
    "text": "data is being ingested and that can help us validate",
    "start": "1075133",
    "end": "1081366"
  },
  {
    "start": "1081366",
    "end": "1081398"
  },
  {
    "text": " data is being ingested from EventBridge Pipes",
    "start": "1081400",
    "end": "1084800"
  },
  {
    "start": "1084800",
    "end": "1105933"
  },
  {
    "text": "With that we can conclude our demo and if you have any further questions reach out to the email displayed here ",
    "start": "1105933",
    "end": "1112166"
  },
  {
    "text": "really appreciate your time today. Thank you so much.",
    "start": "1112166",
    "end": "1114799"
  }
]