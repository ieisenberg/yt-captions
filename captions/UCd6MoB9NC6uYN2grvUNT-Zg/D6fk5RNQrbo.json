[
  {
    "start": "0",
    "end": "19000"
  },
  {
    "text": "In this video, you'll see how you can integrate a serverless\ndata lake into your modern data architecture.",
    "start": "129",
    "end": "4862"
  },
  {
    "text": "Using various AWS Analytics services,",
    "start": "5197",
    "end": "7783"
  },
  {
    "text": "you can rapidly ingest and transform vast amounts of structured\nand unstructured data from data-streaming sources,",
    "start": "7783",
    "end": "13500"
  },
  {
    "text": "query and visualize the data to get meaningful insights,\nand apply machine learning to your data lake.",
    "start": "13501",
    "end": "18351"
  },
  {
    "start": "19000",
    "end": "41000"
  },
  {
    "text": "The process begins with data ingestion.",
    "start": "19345",
    "end": "21511"
  },
  {
    "text": "For our purposes, we’ll represent ingestion using data already exported\nfrom our Amazon Relational Database Service (Amazon RDS)",
    "start": "21868",
    "end": "29105"
  },
  {
    "text": "to an Amazon Simple Storage Service (Amazon S3)\nstaging bucket.",
    "start": "29105",
    "end": "32762"
  },
  {
    "text": "Starting in AWS CloudShell, we’ll copy the data from the staging bucket\ninto a new S3 bucket created for our data lake.",
    "start": "33571",
    "end": "40084"
  },
  {
    "start": "41000",
    "end": "168000"
  },
  {
    "text": "Let’s go to our S3 bucket to see\nif the data was copied into it.",
    "start": "41510",
    "end": "44536"
  },
  {
    "text": "The “tickets” object contains our data on ticket sales\ncollected from professional baseball and football games.",
    "start": "45059",
    "end": "50210"
  },
  {
    "text": "The Amazon Database Migration Service (DMS)\nfolder is our data source.",
    "start": "50924",
    "end": "55029"
  },
  {
    "text": "These folders contain the copied data.",
    "start": "56084",
    "end": "58034"
  },
  {
    "text": "The next step in setting up our architecture\nis to apply data transformations.",
    "start": "59112",
    "end": "63184"
  },
  {
    "text": "For that, we’re going to use AWS Glue.",
    "start": "63465",
    "end": "65670"
  },
  {
    "text": "AWS Glue is a fully managed data catalog and\nETL (extract, transform, and load) service.",
    "start": "68519",
    "end": "74602"
  },
  {
    "text": "To create our AWS Glue Data Catalog,\nwe’ll add a crawler.",
    "start": "75953",
    "end": "79509"
  },
  {
    "text": "The crawler will comb through our data store\nto determine the schema for our data,",
    "start": "85248",
    "end": "88949"
  },
  {
    "text": "and then create metadata tables\nin our data catalog.",
    "start": "88949",
    "end": "91620"
  },
  {
    "text": "For the path destination,\nwe’ll specify the S3 bucket.",
    "start": "92543",
    "end": "95504"
  },
  {
    "text": "We'll create a new AWS Identity and Access\nManagement (AWS IAM) role for this crawler.",
    "start": "100811",
    "end": "106516"
  },
  {
    "text": "We'll leave the run frequency as run on demand.",
    "start": "109406",
    "end": "111750"
  },
  {
    "text": "Let's add a database for the crawler’s output.",
    "start": "113237",
    "end": "115386"
  },
  {
    "text": "Let’s change the configuration option to add new columns to the table\nif the crawler detects schema changes in the data store.",
    "start": "119781",
    "end": "125557"
  },
  {
    "text": "Now let's finish configuration\nand run the crawler.",
    "start": "127979",
    "end": "130519"
  },
  {
    "text": "We can see that 15 tables were added.",
    "start": "138000",
    "end": "139935"
  },
  {
    "text": "Let’s navigate to the database\nwe created to see them.",
    "start": "140222",
    "end": "142703"
  },
  {
    "text": "Next, we’ll do a data validation exercise\non one of the tables.",
    "start": "146466",
    "end": "149770"
  },
  {
    "text": "In this case, the crawler could not determine names\nfor the database columns when crawling the S3 data bucket",
    "start": "150531",
    "end": "155549"
  },
  {
    "text": "and simply provided numerical placeholders.",
    "start": "155549",
    "end": "157748"
  },
  {
    "text": "Let's edit the schema to input appropriate\ncolumn names for the data they represent.",
    "start": "159620",
    "end": "163452"
  },
  {
    "text": "Let's save our changes.",
    "start": "166104",
    "end": "167124"
  },
  {
    "start": "168000",
    "end": "244000"
  },
  {
    "text": "Once all our database tables are validated,\nwe can convert the database tables into Parquet format.",
    "start": "169207",
    "end": "174089"
  },
  {
    "text": "Let's go to AWS Glue Studio to create some ETL jobs that will establish\nnew portal locations in our S3 bucket for the Parquet tables.",
    "start": "175407",
    "end": "182702"
  },
  {
    "text": "Each job will use the ticketdata database as the source.",
    "start": "184593",
    "end": "187453"
  },
  {
    "text": "The first job will transform the sport_team table.",
    "start": "188157",
    "end": "190724"
  },
  {
    "text": "In the Apply Mapping node, we'll adjust the\nid data type from string to double.",
    "start": "194800",
    "end": "199295"
  },
  {
    "text": "Next, we’ll change the output format to Parquet.",
    "start": "201572",
    "end": "204063"
  },
  {
    "text": "For the S3 target location,\nwe’ll choose our bucket and the tickets object.",
    "start": "206286",
    "end": "210000"
  },
  {
    "text": "We’ll also specify the folder that will store the Parquet files\nand the table that corresponds to this job.",
    "start": "214560",
    "end": "219341"
  },
  {
    "text": "Let's also rename the job to match the table.",
    "start": "221576",
    "end": "223773"
  },
  {
    "text": "In the Job details tab,\nwe'll select the IAM role for this job.",
    "start": "225983",
    "end": "229407"
  },
  {
    "text": "Let's disable the job bookmark\nsince we don't need it.",
    "start": "234358",
    "end": "236827"
  },
  {
    "text": "Now let's save and run the job.",
    "start": "239002",
    "end": "240740"
  },
  {
    "start": "244000",
    "end": "406000"
  },
  {
    "text": "We can review the job in the monitoring tab.",
    "start": "245466",
    "end": "247613"
  },
  {
    "text": "Here we can see that one job,\nwhich is the one we just created, is running.",
    "start": "248727",
    "end": "252180"
  },
  {
    "text": "While our first job is running,\nlet's create jobs for our other tables.",
    "start": "253570",
    "end": "256935"
  },
  {
    "text": "We can clone our first job and adjust the settings\nrather than configure a new job from scratch.",
    "start": "257728",
    "end": "262232"
  },
  {
    "text": "We’ll adjust the job name\nand select the next table to work with.",
    "start": "264548",
    "end": "267465"
  },
  {
    "text": "A useful feature of AWS Glue is that it creates\na name folder for each of the tables automatically.",
    "start": "269244",
    "end": "274125"
  },
  {
    "text": "For the sport_location table,\nwe’ll keep the same data type settings.",
    "start": "276778",
    "end": "280302"
  },
  {
    "text": "Next, we’ll revise the folder location of the URL.",
    "start": "282362",
    "end": "285116"
  },
  {
    "text": "Now we’ll confirm the IAM role and other settings,\nand then save and run the job.",
    "start": "288000",
    "end": "292047"
  },
  {
    "text": "Let's repeat the cloning process\nfor a third job.",
    "start": "298167",
    "end": "300453"
  },
  {
    "text": "Now let's go to the Monitoring page\nto view the status of these jobs.",
    "start": "302865",
    "end": "306000"
  },
  {
    "text": "All five jobs were successful.",
    "start": "308915",
    "end": "310507"
  },
  {
    "text": "Let's go to our S3 bucket\nto see the Parquet portal location.",
    "start": "311123",
    "end": "314190"
  },
  {
    "text": "Here we can see the newly created Parquet\nobject with folders for all five tables.",
    "start": "317533",
    "end": "321405"
  },
  {
    "text": "Looking in one of the folders,\nwe also see the type is Parquet.",
    "start": "322146",
    "end": "325260"
  },
  {
    "text": "Next, we'll return to AWS Glue\nand create a crawler for Parquet files.",
    "start": "326627",
    "end": "330583"
  },
  {
    "text": "We’ll retain the default source type settings.",
    "start": "342228",
    "end": "344409"
  },
  {
    "text": "For the path destination, we’ll specify\nthe S3 bucket holding our tickets object.",
    "start": "345417",
    "end": "349479"
  },
  {
    "text": "Let's also ensure the dms_parquet folder\nis included in the path.",
    "start": "351696",
    "end": "355212"
  },
  {
    "text": "We'll choose the appropriate IAM role.",
    "start": "363661",
    "end": "365630"
  },
  {
    "text": "We’ll choose our ticketdata database.",
    "start": "373832",
    "end": "375602"
  },
  {
    "text": "We can also add a prefix to make it easier\nto interpret table data.",
    "start": "378469",
    "end": "381710"
  },
  {
    "text": "Let's finish and run our crawler.",
    "start": "382575",
    "end": "384227"
  },
  {
    "text": "When the crawler finishes running, we’ll be able to see\nall the Parquet tables added to this data lake.",
    "start": "394371",
    "end": "398761"
  },
  {
    "text": "For now, let’s move on to the next step of our Data Lake integration:\ndata querying and visualization.",
    "start": "399088",
    "end": "404198"
  },
  {
    "start": "406000",
    "end": "514000"
  },
  {
    "text": "Let's go to Amazon Athena,\nwhere we'll query our data.",
    "start": "406686",
    "end": "409310"
  },
  {
    "text": "Athena is a serverless query service that makes it easy\nto analyze data in Amazon S3 using standard SQL.",
    "start": "411988",
    "end": "417679"
  },
  {
    "text": "As you can see, Athena automatically identified the AWS Data Catalog\nas the data source and ticket data as the database.",
    "start": "419039",
    "end": "425833"
  },
  {
    "text": "Let's run a query against the Parquet tables\nto generate results on the sporting event information.",
    "start": "427159",
    "end": "431372"
  },
  {
    "text": "The results identify the sporting events,\nevent dates and times, team names, and locations.",
    "start": "433160",
    "end": "437664"
  },
  {
    "text": "We can create a “view” from this query.",
    "start": "438975",
    "end": "440834"
  },
  {
    "text": "A view is a logical table that updates\neach time we run the query.",
    "start": "441322",
    "end": "444625"
  },
  {
    "text": "As you can see, Athena generated\na second query for the view we created.",
    "start": "448480",
    "end": "452175"
  },
  {
    "text": "Let's run a third query\nfor sporting ticket information.",
    "start": "453479",
    "end": "456228"
  },
  {
    "text": "We can save this query\nso we can run it again anytime.",
    "start": "459895",
    "end": "462634"
  },
  {
    "text": "We’ll call it sporting_event_ticket_info.",
    "start": "463485",
    "end": "465655"
  },
  {
    "text": "Now let's run the query again.",
    "start": "473604",
    "end": "475143"
  },
  {
    "text": "The results show information such as seat\nlocation, ticket prices, and ticketholders.",
    "start": "475771",
    "end": "480000"
  },
  {
    "text": "Let’s create a view from this query as well.",
    "start": "481137",
    "end": "483167"
  },
  {
    "text": "We can query the view we just created to see\nthe average ticket price for each sport.",
    "start": "490900",
    "end": "494684"
  },
  {
    "text": "Let's save this query.",
    "start": "498973",
    "end": "500143"
  },
  {
    "text": "Of course, we can create a view\nfrom this query if we want to reuse it.",
    "start": "510725",
    "end": "513776"
  },
  {
    "start": "514000",
    "end": "599000"
  },
  {
    "text": "With our architecture set up\nand some queries saved,",
    "start": "515138",
    "end": "517736"
  },
  {
    "text": "we can now visualize our data using interactive business\nintelligence dashboards in Amazon QuickSight.",
    "start": "517742",
    "end": "522864"
  },
  {
    "text": "We'll begin by creating a new dataset using Athena,\nsince that is where we made our SQL queries.",
    "start": "524714",
    "end": "529167"
  },
  {
    "text": "We’ll select our ticketdata folder as the\ndata source, and then validate the connection.",
    "start": "531000",
    "end": "534733"
  },
  {
    "text": "Now we'll select a table to visualize.",
    "start": "539266",
    "end": "541371"
  },
  {
    "text": "We’ll select the sporting_event_ticket_info\nquery we saved.",
    "start": "542660",
    "end": "545696"
  },
  {
    "text": "Once the query is ready, we can select fields\nto visualize and how we want to see the information.",
    "start": "552614",
    "end": "557155"
  },
  {
    "text": "Here we see the ticket price sum as a currency.",
    "start": "560337",
    "end": "562503"
  },
  {
    "text": "Let's add another dashboard to visualize ticket\nprices versus event dates.",
    "start": "563903",
    "end": "567580"
  },
  {
    "text": "For this, we’ll use a vertical bar graph.",
    "start": "568033",
    "end": "569981"
  },
  {
    "text": "It's easy to add data to the graph.",
    "start": "574069",
    "end": "575717"
  },
  {
    "text": "Simply drag fields to the field wells\nat the top of the page.",
    "start": "576000",
    "end": "579000"
  },
  {
    "text": "Amazon QuickSight also produces insights to\nhelp you monitor your data and detect any anomalies.",
    "start": "579919",
    "end": "584688"
  },
  {
    "text": "An additional step we can take to integrate our data lake\nis to apply Machine Learning (ML) models to it.",
    "start": "586268",
    "end": "591567"
  },
  {
    "text": "Let's go to the Amazon SageMaker console.",
    "start": "592868",
    "end": "595185"
  },
  {
    "start": "599000",
    "end": "728000"
  },
  {
    "text": "In SageMaker, we'll create a Jupyter notebook\ninstance that integrates with our data lake in Athena.",
    "start": "600000",
    "end": "604861"
  },
  {
    "text": "Let's name our notebook.",
    "start": "607305",
    "end": "608500"
  },
  {
    "text": "For this notebook,\nwe need to create a new IAM role.",
    "start": "613035",
    "end": "615649"
  },
  {
    "text": "It's optional to specify which S3 buckets\nto grant access to.",
    "start": "617765",
    "end": "621078"
  },
  {
    "text": "We'll allow all S3 buckets.",
    "start": "621632",
    "end": "623427"
  },
  {
    "text": "Let's open the IAM role and grant access to Athena,\nwhere we've stored our data queries.",
    "start": "624315",
    "end": "628577"
  },
  {
    "text": "We'll attach a new policy\ngranting full access to Athena.",
    "start": "629374",
    "end": "632137"
  },
  {
    "text": "Let's complete our notebook setup.",
    "start": "638255",
    "end": "639722"
  },
  {
    "text": "Now we'll open the notebook in Jupyter and run it in a Conda Python 3\nenvironment preconfigured for deep learning frameworks.",
    "start": "646422",
    "end": "652585"
  },
  {
    "text": "We'll run the command to install the Athena\nJava Database Connectivity (JDBC) driver.",
    "start": "654864",
    "end": "659656"
  },
  {
    "text": "Next, we'll use the JDBC connection to Athena\nand populate the Pandas DataFrame.",
    "start": "663639",
    "end": "667977"
  },
  {
    "text": "First let's confirm the region configuration.",
    "start": "669147",
    "end": "671267"
  },
  {
    "text": "Now we'll run the command to input the S3\ndata bucket that houses our Athena queries",
    "start": "677236",
    "end": "681473"
  },
  {
    "text": "and include the region where it’s located.",
    "start": "681483",
    "end": "683095"
  },
  {
    "text": "The output is our DataFrame,\nwhich we can use to plot our data.",
    "start": "684974",
    "end": "687884"
  },
  {
    "text": "We’ll specify our ticketdata database\nto upload our Parquet files.",
    "start": "691575",
    "end": "695017"
  },
  {
    "text": "Finally, let's run a command to plot the data.",
    "start": "697798",
    "end": "700134"
  },
  {
    "text": "Here you can see the populated plot.",
    "start": "706002",
    "end": "707725"
  },
  {
    "text": "SageMaker notebooks enable easy deployment of hundreds\nof pretrained algorithms for applying ML to data sets.",
    "start": "708751",
    "end": "714518"
  },
  {
    "text": "You've just seen how you can integrate a serverless\ndata lake into your modern data architecture.",
    "start": "715894",
    "end": "719867"
  },
  {
    "text": "You can learn more about this topic in the\ndescription and links for this video.",
    "start": "721066",
    "end": "724168"
  },
  {
    "text": "Thanks for watching.\nNow it’s your turn to try.",
    "start": "724411",
    "end": "726380"
  }
]