[
  {
    "start": "0",
    "end": "75000"
  },
  {
    "text": "all right well let's get started first of all thanks to everyone for sharing at",
    "start": "140",
    "end": "5160"
  },
  {
    "text": "least part of your lunch hour with us today we're gonna go into a deep dive of Amazon Neptune but before we do how many",
    "start": "5160",
    "end": "12330"
  },
  {
    "text": "of you watched the keynote this morning yeah it's a really exciting day for AWS",
    "start": "12330",
    "end": "18060"
  },
  {
    "text": "database services with the announcement of Amazon quantum ledger database as well as Amazon time stream it was really",
    "start": "18060",
    "end": "25740"
  },
  {
    "text": "just a year ago that we were in the similar boat announced as a preview service in Andy's keynote so understand",
    "start": "25740",
    "end": "33300"
  },
  {
    "text": "exactly you know what that means but the how many of you today are using a graph",
    "start": "33300",
    "end": "38370"
  },
  {
    "text": "database okay fantastic how many of you are using property graph",
    "start": "38370",
    "end": "45000"
  },
  {
    "text": "and Apache tinker pop a few how many you are using RDF Sparkle a few as well and",
    "start": "45000",
    "end": "53820"
  },
  {
    "text": "then the tough question how many of you are using Amazon Neptune already okay",
    "start": "53820",
    "end": "59670"
  },
  {
    "text": "fantastic fantastic so we are gonna have some time at the end for questions so we're you know please you know feel free",
    "start": "59670",
    "end": "66600"
  },
  {
    "text": "to ask us your your deepest darkest questions about Neptune and we'll do our best to answer them so but with that why",
    "start": "66600",
    "end": "73500"
  },
  {
    "text": "don't we go ahead and get started so we're going to cover a little bit of an overview about building applications and",
    "start": "73500",
    "end": "80420"
  },
  {
    "start": "75000",
    "end": "136000"
  },
  {
    "text": "the different types of graph models and some of the characteristics of Neptune",
    "start": "80420",
    "end": "85590"
  },
  {
    "text": "and then my colleague Bruce McGarry is going to go into the details of our underlying storage layer sort of what is",
    "start": "85590",
    "end": "92909"
  },
  {
    "text": "it behind Neptune that gives us the features for enterprise availability and",
    "start": "92909",
    "end": "98729"
  },
  {
    "text": "scalability if you're interested in learning more about graphs or about",
    "start": "98729",
    "end": "104909"
  },
  {
    "text": "Amazon Neptune there are other opportunities here at reinvent and so we have some things that are more hands-on",
    "start": "104909",
    "end": "111360"
  },
  {
    "text": "in terms of looking at specific performance tuning pieces so folks that are using Neptune today you might really",
    "start": "111360",
    "end": "117810"
  },
  {
    "text": "benefit from that session some of our engineers are there they can walk you through some of the things that we've",
    "start": "117810",
    "end": "122939"
  },
  {
    "text": "learned about how to get the best performance out of Neptune and the various clients that we have and then we",
    "start": "122939",
    "end": "128879"
  },
  {
    "text": "have also have other options to get started with Neptune's definitely plenty of things to learn but",
    "start": "128879",
    "end": "137900"
  },
  {
    "start": "136000",
    "end": "192000"
  },
  {
    "text": "this is kind of where we start off is you know what kinds of applications can",
    "start": "137900",
    "end": "143210"
  },
  {
    "text": "benefit from a graph database and the answer is relatively simple at first its",
    "start": "143210",
    "end": "150980"
  },
  {
    "text": "graph databases are really when you want to traverse relationships and if this is",
    "start": "150980",
    "end": "157400"
  },
  {
    "text": "what is the primary data access pattern in your application then a graph",
    "start": "157400",
    "end": "162860"
  },
  {
    "text": "database is something that you may want to consider the kinds of data that we",
    "start": "162860",
    "end": "167900"
  },
  {
    "text": "see which we think of as rich and highly connected is characterized by lots of different things it comes from many",
    "start": "167900",
    "end": "174500"
  },
  {
    "text": "different sources it's produced at different rates it's heterogeneous in terms of the schema and the formats but",
    "start": "174500",
    "end": "182900"
  },
  {
    "text": "the thing that unites all of these use cases is that the real value that you get in the applications is thinking",
    "start": "182900",
    "end": "189620"
  },
  {
    "text": "about the relationships and there's lots and lots of different use cases for",
    "start": "189620",
    "end": "195650"
  },
  {
    "text": "connected data when we launched Neptune we expected customers to build social",
    "start": "195650",
    "end": "203810"
  },
  {
    "text": "networking applications to use graphs to model how people interact we expected",
    "start": "203810",
    "end": "209000"
  },
  {
    "text": "people to build recommendation engines to look at how they can provide in-game offers or product recommendations in",
    "start": "209000",
    "end": "216350"
  },
  {
    "text": "their applications as part of their user experience and we expected people to build fraud applications to help",
    "start": "216350",
    "end": "222010"
  },
  {
    "text": "understand what are the attributes across different accounts and different transactions than may be indicative of",
    "start": "222010",
    "end": "227930"
  },
  {
    "text": "patterns of fraud but you know an interesting thing happens when you build",
    "start": "227930",
    "end": "233540"
  },
  {
    "text": "a purpose-built database and give developers a high performance tool for",
    "start": "233540",
    "end": "239180"
  },
  {
    "text": "specialized types of database applications is that they start to discover and do really interesting and",
    "start": "239180",
    "end": "244190"
  },
  {
    "text": "exciting things with it so yes we do have customers in production with Neptune for their social",
    "start": "244190",
    "end": "251180"
  },
  {
    "text": "networks there's actually a Nike session that's gonna start in a few minutes talking about how Nikes migrating their",
    "start": "251180",
    "end": "257660"
  },
  {
    "text": "social platform to use Neptune underneath and we have customers that are using Neptune to make",
    "start": "257660",
    "end": "263030"
  },
  {
    "text": "recommendations and to recommendation engines as well as fraud but we've also seen customers use graph",
    "start": "263030",
    "end": "270640"
  },
  {
    "text": "as a technique to take on other types of problems things that are more strategic to their businesses to allow them to",
    "start": "270640",
    "end": "276640"
  },
  {
    "text": "make connections between datasets that they haven't been able to connect previously and so we've also seen",
    "start": "276640",
    "end": "282640"
  },
  {
    "text": "customers have build knowledge graph applications to help people find and retrieve information to be able to",
    "start": "282640",
    "end": "288010"
  },
  {
    "text": "answer new questions customers are using it in the life sciences to discover new treatments for existing approved drugs",
    "start": "288010",
    "end": "294280"
  },
  {
    "text": "or also to look at how to bring different medical record systems and hospital systems together to give care",
    "start": "294280",
    "end": "302710"
  },
  {
    "text": "treatment practitioners a common view of their data and of course network security as well so lots and lots of",
    "start": "302710",
    "end": "309790"
  },
  {
    "text": "different use cases we walk through a couple of examples of how you can use these techniques so if we start with a",
    "start": "309790",
    "end": "318250"
  },
  {
    "start": "315000",
    "end": "392000"
  },
  {
    "text": "simple social network here but you can see we have Bill and Bill nose of Alice",
    "start": "318250",
    "end": "323560"
  },
  {
    "text": "and Bob but we also have some other data in our organization that we want to bring to bear we also have product",
    "start": "323560",
    "end": "331450"
  },
  {
    "text": "purchase history so we can see what these users purchased we have something that someone else as well and in",
    "start": "331450",
    "end": "337900"
  },
  {
    "text": "addition to that we have some shared interest information so we understand",
    "start": "337900",
    "end": "344460"
  },
  {
    "text": "what these users have expressed as their interest and by using a technique that",
    "start": "344460",
    "end": "350260"
  },
  {
    "text": "we call triadic closure a triangle is the smallest fully connected subgraph in a graph we can identify new edges that",
    "start": "350260",
    "end": "358810"
  },
  {
    "text": "would create triangles and this is the basis for the start of a recommendation",
    "start": "358810",
    "end": "363940"
  },
  {
    "text": "algorithm so by adding an edge between Sara and product we can make a recommendation that Sara may want to",
    "start": "363940",
    "end": "371169"
  },
  {
    "text": "purchase this product because people who also were interested in sports purchased",
    "start": "371169",
    "end": "376390"
  },
  {
    "text": "that product and again on the right hand side you can see that there's an individual bill who has two friends in",
    "start": "376390",
    "end": "383020"
  },
  {
    "text": "common but there's not yet an edge connecting them so we can also use the same technique to make a recommendation",
    "start": "383020",
    "end": "391980"
  },
  {
    "start": "392000",
    "end": "486000"
  },
  {
    "text": "sometimes you want to help people find things they answer more so",
    "start": "393030",
    "end": "398650"
  },
  {
    "text": "fistic ated questions and knowledge graphs are a technique you can use to do",
    "start": "398650",
    "end": "404500"
  },
  {
    "text": "that a knowledge graph uses a graph model to encode domain information and",
    "start": "404500",
    "end": "409560"
  },
  {
    "text": "to link together different sets of source data and that allows you to",
    "start": "409560",
    "end": "414940"
  },
  {
    "text": "traverse that to answer questions so here we start with an example that's taken from the World Wide Web Consortium",
    "start": "414940",
    "end": "420880"
  },
  {
    "text": "or the w3c z-- primer on knowledge graphs and it has a very famous piece of",
    "start": "420880",
    "end": "426820"
  },
  {
    "text": "artwork the Mona Lisa very famous artist Leonardo da Vinci and where it's located what we're gonna also going to add some",
    "start": "426820",
    "end": "433510"
  },
  {
    "text": "other information so we're going to add some social network information so here",
    "start": "433510",
    "end": "440080"
  },
  {
    "text": "we have some shared interests again we have friendship connections and then",
    "start": "440080",
    "end": "445570"
  },
  {
    "text": "we're also going to add information about Geographic references and travel",
    "start": "445570",
    "end": "451030"
  },
  {
    "text": "type data and so now we can see where this museum is located we can see where",
    "start": "451030",
    "end": "456250"
  },
  {
    "text": "these individuals have traveled and that gives us the basis to start to answer more sophisticated questions so we can",
    "start": "456250",
    "end": "463900"
  },
  {
    "text": "now answer questions like who painted the Mona Lisa or what museum should",
    "start": "463900",
    "end": "471520"
  },
  {
    "text": "Alice visit well in Paris or what other",
    "start": "471520",
    "end": "476770"
  },
  {
    "text": "artists have paintings in the Louvre so knowledge graphs another example of a",
    "start": "476770",
    "end": "482470"
  },
  {
    "text": "powerful technique to use graphs to build applications one of our public",
    "start": "482470",
    "end": "488080"
  },
  {
    "start": "486000",
    "end": "553000"
  },
  {
    "text": "reference customers is there anyone here from Thomson Reuters in the audience so",
    "start": "488080",
    "end": "493240"
  },
  {
    "text": "Thomson Reuters is one of our Neptune reference customers and they have a number of different graph use cases this",
    "start": "493240",
    "end": "498729"
  },
  {
    "text": "one is very interesting what they've done is they've created a new service for their customers they have used the",
    "start": "498729",
    "end": "504849"
  },
  {
    "text": "graph model to model global tax policies and compliance regulations and they've",
    "start": "504849",
    "end": "511270"
  },
  {
    "text": "also used the graph model to represent their customers corporate and legal",
    "start": "511270",
    "end": "516760"
  },
  {
    "text": "entities and by combining these they're able to offer a new service that helps",
    "start": "516760",
    "end": "522700"
  },
  {
    "text": "their customers both understand what their tax obligations are for a given",
    "start": "522700",
    "end": "527800"
  },
  {
    "text": "operating structure and then as a second order to help them understand how they might be able to",
    "start": "527800",
    "end": "533680"
  },
  {
    "text": "optimize their their entity structures in that regards and these kinds of tax",
    "start": "533680",
    "end": "539380"
  },
  {
    "text": "and regs cases also show relatively frequently in the graph space where people are looking at using the power of",
    "start": "539380",
    "end": "546130"
  },
  {
    "text": "graphs and the relationships to help them reduce their cost of regulatory compliance so when we started on the",
    "start": "546130",
    "end": "555340"
  },
  {
    "start": "553000",
    "end": "652000"
  },
  {
    "text": "journey of building Neptune we really started by asking ourselves what are the",
    "start": "555340",
    "end": "561580"
  },
  {
    "text": "options that customers have to process this kind of connected data and the",
    "start": "561580",
    "end": "567910"
  },
  {
    "text": "natural first question is can I use a relational database to process this the",
    "start": "567910",
    "end": "575320"
  },
  {
    "text": "answer is yes absolutely you can but there's a couple of drawbacks and what",
    "start": "575320",
    "end": "581170"
  },
  {
    "text": "we found is that the drawbacks are the way that you express traversals or graph",
    "start": "581170",
    "end": "587650"
  },
  {
    "text": "patterns in over a relational database is in sequel and using sequel joins and",
    "start": "587650",
    "end": "595560"
  },
  {
    "text": "for even relatively simple graph patterns or joins they quickly become",
    "start": "595560",
    "end": "602580"
  },
  {
    "text": "complex sequel queries the types of iya",
    "start": "602580",
    "end": "607870"
  },
  {
    "text": "workloads that graph databases have are very very different than what you see in",
    "start": "607870",
    "end": "613300"
  },
  {
    "text": "relational databases so these kinds of join queries will run very slowly on relational databases and as a result",
    "start": "613300",
    "end": "622170"
  },
  {
    "text": "when you build graph applications on a relational database you spend a lot of time tuning and de normalizing your data",
    "start": "622170",
    "end": "630100"
  },
  {
    "text": "model to get good traversal performance out of it and so that means that as a",
    "start": "630100",
    "end": "635410"
  },
  {
    "text": "developer is someone building a graph application using a relational database it's much slower for you to do that",
    "start": "635410",
    "end": "641560"
  },
  {
    "text": "because every time you want to add a new relationship or express a new traversal",
    "start": "641560",
    "end": "647080"
  },
  {
    "text": "you potentially have to change your data model to do so you can also look at this",
    "start": "647080",
    "end": "654040"
  },
  {
    "text": "same question from a data model perspective so on your left hand side you can see a notional entity",
    "start": "654040",
    "end": "660940"
  },
  {
    "text": "relationship diagram of an HR system and it has lots of really great data in",
    "start": "660940",
    "end": "667089"
  },
  {
    "text": "it it has relationships between employees and departments and other HR information and it works really well for",
    "start": "667089",
    "end": "674350"
  },
  {
    "text": "the application that is intended to serve which is to to build HR or to do",
    "start": "674350",
    "end": "679630"
  },
  {
    "text": "HR processes but if you want to use that information to answer other questions",
    "start": "679630",
    "end": "685440"
  },
  {
    "text": "suppose you wanted to say well what skills do employees need to have to",
    "start": "685440",
    "end": "691089"
  },
  {
    "text": "build a particular product it would be very difficult for you to do that because the the answers are really",
    "start": "691089",
    "end": "697300"
  },
  {
    "text": "trapped in that relational schema so on your right hand side you can see a",
    "start": "697300",
    "end": "702399"
  },
  {
    "text": "notional graph model and what immediately jumps out is that the relationships become first-class objects",
    "start": "702399",
    "end": "710079"
  },
  {
    "text": "in a graph data model and that means that you could very easily and directly address them and query them and that's",
    "start": "710079",
    "end": "716589"
  },
  {
    "text": "really the power of a graph database so",
    "start": "716589",
    "end": "722019"
  },
  {
    "text": "graph databases are both optimized for the kinds of i/o workloads that are",
    "start": "722019",
    "end": "727680"
  },
  {
    "text": "common for graph processing as well as supporting graph query languages and",
    "start": "727680",
    "end": "734170"
  },
  {
    "text": "api's that make it efficient effective and concise for you express these",
    "start": "734170",
    "end": "739899"
  },
  {
    "text": "traversals and to build applications over them now there's two major graph",
    "start": "739899",
    "end": "749980"
  },
  {
    "start": "746000",
    "end": "961000"
  },
  {
    "text": "models and frameworks and if there's really one important message about graph",
    "start": "749980",
    "end": "756070"
  },
  {
    "text": "models that I you know I'd like for you to take home today it's that it's all graph really--it's so there's property",
    "start": "756070",
    "end": "763540"
  },
  {
    "text": "graph and property graphs consist of nodes and node properties and edges and",
    "start": "763540",
    "end": "768970"
  },
  {
    "text": "edge properties and the leading framework for property graphs in open-source is something called apache",
    "start": "768970",
    "end": "775420"
  },
  {
    "text": "tinker pop apache tinker pop provides an imperative traversal language which is",
    "start": "775420",
    "end": "781360"
  },
  {
    "text": "called gremlin that allows you to express graphed reversals and graph patterns over property graphs and the",
    "start": "781360",
    "end": "788949"
  },
  {
    "text": "second model is something that's a standard from the w3c which is called",
    "start": "788949",
    "end": "793990"
  },
  {
    "text": "the resource description framework which is a is a graph model and the resource",
    "start": "793990",
    "end": "799350"
  },
  {
    "text": "description framework or RDF has a declarative graph query language which is called sparkle that allows you to",
    "start": "799350",
    "end": "806339"
  },
  {
    "text": "write graph queries over RDF graphs so",
    "start": "806339",
    "end": "811399"
  },
  {
    "text": "back to it's just graph so customers often ask us you know which model is",
    "start": "811399",
    "end": "817950"
  },
  {
    "text": "right for which application and the answer is really there are some",
    "start": "817950",
    "end": "823050"
  },
  {
    "text": "differences particularly in the way the syntax for edge properties in property",
    "start": "823050",
    "end": "828330"
  },
  {
    "text": "graph versus RDF uses something called reification to express them but",
    "start": "828330",
    "end": "833820"
  },
  {
    "text": "conceptually you can implement almost any application with either model what",
    "start": "833820",
    "end": "840300"
  },
  {
    "text": "we do find is that if you're coming from a relational database background into",
    "start": "840300",
    "end": "845640"
  },
  {
    "text": "the graph space it's often a more straightforward transition or you find it easier to start thinking from move",
    "start": "845640",
    "end": "852959"
  },
  {
    "text": "from thinking about rows and tables to thinking about nodes and node properties and edges and edge properties also if",
    "start": "852959",
    "end": "861270"
  },
  {
    "text": "your application itself as a user interacts with it they're building a graph traversal so if you think about a",
    "start": "861270",
    "end": "867690"
  },
  {
    "text": "social networking application someone selects an interest in a friend and that's basically traversing out an edge",
    "start": "867690",
    "end": "874230"
  },
  {
    "text": "and making another traversal the imperative nature of Gremlin can be a really natural fit for those kinds of",
    "start": "874230",
    "end": "881400"
  },
  {
    "text": "applications on the other hand RDF was originally defined to describe resources",
    "start": "881400",
    "end": "889080"
  },
  {
    "text": "on the web and so if you've ever looked at RDF you'll see lots of things that look like URLs inside of them they're",
    "start": "889080",
    "end": "895800"
  },
  {
    "text": "actually called internationalized resource identifiers or IR eyes and they give RDF a very strong sense of identity",
    "start": "895800",
    "end": "903570"
  },
  {
    "text": "so RDF you know carries this background with it and it also has standardized",
    "start": "903570",
    "end": "909000"
  },
  {
    "text": "data serialization formats so there's lots of datasets that are publicly available from sources like open",
    "start": "909000",
    "end": "916440"
  },
  {
    "text": "government data life sciences data there's things wiki data for example has",
    "start": "916440",
    "end": "921720"
  },
  {
    "text": "an RDF version and so if your application uses these kinds of data sets to help bootstrap it or as part of",
    "start": "921720",
    "end": "929700"
  },
  {
    "text": "its processing RDF can be a very natural fit and in addition because of its history as",
    "start": "929700",
    "end": "936330"
  },
  {
    "text": "describing web resources there's lots of different languages language features",
    "start": "936330",
    "end": "941520"
  },
  {
    "text": "and supporting standards for RDF such that if your application is focused on",
    "start": "941520",
    "end": "947540"
  },
  {
    "text": "data canonicalization or building common data models from multiple sources",
    "start": "947540",
    "end": "953190"
  },
  {
    "text": "there's lots of different features that enable you to do that so different models different strengths",
    "start": "953190",
    "end": "958980"
  },
  {
    "text": "all graph so let's look at a particular example now you're not really intended",
    "start": "958980",
    "end": "965160"
  },
  {
    "start": "961000",
    "end": "1041000"
  },
  {
    "text": "to make any sense of this diagram other than to say that this is a structure from something called the Lehigh",
    "start": "965160",
    "end": "970500"
  },
  {
    "text": "University benchmark which is the oldest and most well known graph benchmark and it consists of a simulated university",
    "start": "970500",
    "end": "979110"
  },
  {
    "text": "system where universities have professors and and students and students take courses and there's graduate",
    "start": "979110",
    "end": "984930"
  },
  {
    "text": "students and so on and so forth and you'll also see on your left-hand side",
    "start": "984930",
    "end": "990330"
  },
  {
    "text": "sort of a blowout of what the professor graph hierarchy looks like and so we're",
    "start": "990330",
    "end": "996960"
  },
  {
    "text": "gonna look at an example here and so there's actually 14 different queries in the Lehigh University",
    "start": "996960",
    "end": "1003350"
  },
  {
    "text": "benchmark this if you're familiar with it this happens to be lumen query number two it asks the question it's a",
    "start": "1003350",
    "end": "1009710"
  },
  {
    "text": "conjunctive join query it asks the question find all the graduate students who received an undergraduate degree",
    "start": "1009710",
    "end": "1016280"
  },
  {
    "text": "from the same University and if we want to think about this conceptually this is how it would look as a query so we start",
    "start": "1016280",
    "end": "1023930"
  },
  {
    "text": "with the graduate students we find where they're members of departments we find the organization's the the universities",
    "start": "1023930",
    "end": "1029870"
  },
  {
    "text": "that those departments belong to and then we find the graduate students that also have edges to those universities so",
    "start": "1029870",
    "end": "1038120"
  },
  {
    "text": "that's conceptually how we want to approach this query now we can look at it in Gremlin now you recall that",
    "start": "1038120",
    "end": "1046579"
  },
  {
    "start": "1041000",
    "end": "1111000"
  },
  {
    "text": "gremlin is an imperative language so at the top the GV is basically saying start",
    "start": "1046580",
    "end": "1052820"
  },
  {
    "text": "with my whole graph and find me vertices that have the label graduate student and",
    "start": "1052820",
    "end": "1059770"
  },
  {
    "text": "then store them off in effectively a temporary variable so this as a student construct now that we found",
    "start": "1059770",
    "end": "1067850"
  },
  {
    "text": "all the graduate students we're going to start our traversal so we go on the outbound edges for the graduate students",
    "start": "1067850",
    "end": "1074270"
  },
  {
    "text": "to find all of the universities then we travel traverse the incoming",
    "start": "1074270",
    "end": "1080960"
  },
  {
    "text": "edges to find the departments that are part of those universities and then we",
    "start": "1080960",
    "end": "1088550"
  },
  {
    "text": "go back out to find all of the graduate students that are members of the department and on the last step we",
    "start": "1088550",
    "end": "1096860"
  },
  {
    "text": "effectively take the intersection of what we started with all the graduate students in the graph with all of the",
    "start": "1096860",
    "end": "1102140"
  },
  {
    "text": "graduate students that we found from the universities and that's how we would evaluate this query in gremlin",
    "start": "1102140",
    "end": "1110500"
  },
  {
    "start": "1111000",
    "end": "1163000"
  },
  {
    "text": "now sparkle is a declarative graph query language so if you're familiar with",
    "start": "1111100",
    "end": "1116450"
  },
  {
    "text": "sequel Sparkle will look a little bit familiar across the top you'll see those IR eyes that I mentioned these are",
    "start": "1116450",
    "end": "1122870"
  },
  {
    "text": "namespaces and so this is giving the data a strong sense of identity and then we start in our select statement the",
    "start": "1122870",
    "end": "1130040"
  },
  {
    "text": "first three lines inside below the wear are essentially binding variables which are start with a question mark in",
    "start": "1130040",
    "end": "1136760"
  },
  {
    "text": "sparkle to graduate students to universities and to departments and then",
    "start": "1136760",
    "end": "1144500"
  },
  {
    "text": "we start our conductive joins so we find that where the students are members of the departments where the departments",
    "start": "1144500",
    "end": "1149780"
  },
  {
    "text": "are organizations of the university and where the students have a university degree have a degree from that",
    "start": "1149780",
    "end": "1154910"
  },
  {
    "text": "University so same question very different way to express the answer so",
    "start": "1154910",
    "end": "1164420"
  },
  {
    "start": "1163000",
    "end": "1257000"
  },
  {
    "text": "the second thing that we did from a product perspective is we looked at what are the options that customers have to",
    "start": "1164420",
    "end": "1171650"
  },
  {
    "text": "use a graph database and one of the things that really struck us is that we talked to lots of customers that had",
    "start": "1171650",
    "end": "1178550"
  },
  {
    "text": "done a very successful proof-of-concept or prototype with the graph database and they found that the languages were",
    "start": "1178550",
    "end": "1186260"
  },
  {
    "text": "natural they liked the way that you could think about expressing a problem in a graph structure but when they tried",
    "start": "1186260",
    "end": "1193880"
  },
  {
    "text": "to take these proofs of concepts from a POC status into production they started to",
    "start": "1193880",
    "end": "1200779"
  },
  {
    "text": "encounter some challenges and the kinds of challenges that they found where this that they spent a lot of time trying to",
    "start": "1200779",
    "end": "1209059"
  },
  {
    "text": "maintain query performance as the data scale increased so it's very hard to do that that caused a very high ops workload and",
    "start": "1209059",
    "end": "1217000"
  },
  {
    "text": "then as they wanted to get enterprise features like high availability read",
    "start": "1217000",
    "end": "1222799"
  },
  {
    "text": "replication or encryption at rest what they found is that these features were not available in community or open",
    "start": "1222799",
    "end": "1228950"
  },
  {
    "text": "source editions and they needed to have Enterprise licensing which was relatively expensive and then finally",
    "start": "1228950",
    "end": "1235970"
  },
  {
    "text": "well there are alternatives that support both the RDF and property graph models there sends to be a very strong",
    "start": "1235970",
    "end": "1242419"
  },
  {
    "text": "performance bias so if you choose a solution that is primarily a property",
    "start": "1242419",
    "end": "1247490"
  },
  {
    "text": "graph solution perhaps there's an RDF connector but it doesn't perform very",
    "start": "1247490",
    "end": "1252500"
  },
  {
    "text": "well or vice-versa if you chose an RDF based solution so so",
    "start": "1252500",
    "end": "1259070"
  },
  {
    "start": "1257000",
    "end": "1364000"
  },
  {
    "text": "for that context we built Amazon Neptune and Amazon Neptune is designed for graph",
    "start": "1259070",
    "end": "1265730"
  },
  {
    "text": "use cases that need to have very high throughput graph query answering with",
    "start": "1265730",
    "end": "1271519"
  },
  {
    "text": "low latency and to give you a sense we think about two major classes of read",
    "start": "1271519",
    "end": "1278149"
  },
  {
    "text": "operations so we think about OLTP base graph queries which we defined as",
    "start": "1278149",
    "end": "1283730"
  },
  {
    "text": "parameterize graphed reversals of less than three hops or parameterize graph patterns and for these kinds of OLTP",
    "start": "1283730",
    "end": "1292370"
  },
  {
    "text": "traversals Neptunes throughput is to support up to 10,000 OLTP queries per",
    "start": "1292370",
    "end": "1299179"
  },
  {
    "text": "server per second with less than 50 milliseconds of latency and then we",
    "start": "1299179",
    "end": "1305809"
  },
  {
    "text": "support horizontal scaling through read replication with up to 15 different low",
    "start": "1305809",
    "end": "1311809"
  },
  {
    "text": "latency read replicas for the second classic queries which we think of as",
    "start": "1311809",
    "end": "1317779"
  },
  {
    "text": "OLAP queries or complex reads and we define them as parameterised traversals",
    "start": "1317779",
    "end": "1323809"
  },
  {
    "text": "of three or more hops on bound graph patterns combinations of those with",
    "start": "1323809",
    "end": "1329210"
  },
  {
    "text": "complex filters target throughput is to support up to 100 per server per second but the latest",
    "start": "1329210",
    "end": "1338270"
  },
  {
    "text": "sees will vary and the latency will vary from hundreds of milliseconds all the way up to minutes or more because the",
    "start": "1338270",
    "end": "1345230"
  },
  {
    "text": "ability to evaluate a complex read query very much depends on the shape of the",
    "start": "1345230",
    "end": "1350570"
  },
  {
    "text": "data that you have underneath and how much data do you have to touch to evaluate that query now complex reads of",
    "start": "1350570",
    "end": "1357380"
  },
  {
    "text": "course have the same sort of horizontal scaling capabilities that you get for OLTP queries so this is an overview of",
    "start": "1357380",
    "end": "1366770"
  },
  {
    "start": "1364000",
    "end": "1539000"
  },
  {
    "text": "sort of neptunes high-level architecture the core of Neptune is this blue",
    "start": "1366770",
    "end": "1372410"
  },
  {
    "text": "rectangle so Neptune is a purpose-built storage engine that's optimized for",
    "start": "1372410",
    "end": "1379610"
  },
  {
    "text": "processing graph queries and it's",
    "start": "1379610",
    "end": "1384650"
  },
  {
    "text": "durable and acid with immediate consistency we support both the tinker",
    "start": "1384650",
    "end": "1390680"
  },
  {
    "text": "pop and gremlin stack as well as the RDF Sparkle stock so every Neptune instance",
    "start": "1390680",
    "end": "1396620"
  },
  {
    "text": "has a WebSocket server connection that you can use to connect any of your tinker pop gremlin WebSocket clients as",
    "start": "1396620",
    "end": "1404060"
  },
  {
    "text": "well as a gremlin REST API we provide a rest interface that implements the sparkle protocol 1/1 which you can use",
    "start": "1404060",
    "end": "1410840"
  },
  {
    "text": "with your rdf sparkle clients and then we also provide some functionality for managing the database so we provide a",
    "start": "1410840",
    "end": "1418490"
  },
  {
    "text": "bulk load capability which provides a fast non-transactional parallel bulk",
    "start": "1418490",
    "end": "1425300"
  },
  {
    "text": "load for data that you have stored in s3 you can use this through the rest interface you post JSON documents to the",
    "start": "1425300",
    "end": "1432980"
  },
  {
    "text": "rest interface these JSON documents specify load configuration so they specify s3 bucket I am credentials",
    "start": "1432980",
    "end": "1440560"
  },
  {
    "text": "different properties for property graphs we support a CSV based serialization",
    "start": "1440560",
    "end": "1447130"
  },
  {
    "text": "where nodes and node properties of serialize in one set of CSV files and",
    "start": "1447130",
    "end": "1453550"
  },
  {
    "text": "edge and edge properties are serialized in another set of CSV files for RDF we",
    "start": "1453550",
    "end": "1460100"
  },
  {
    "text": "support for different audio serialization so we support n triple turtle in quads and rdf/xml graph see",
    "start": "1460100",
    "end": "1467430"
  },
  {
    "text": "realizations in general to get the best performance from the bulk loader because",
    "start": "1467430",
    "end": "1472830"
  },
  {
    "text": "it does use parallelism internally we recommend having larger segment sizes of",
    "start": "1472830",
    "end": "1479040"
  },
  {
    "text": "data in at your s3 buckets so typically we recommend two hundred Meg's or more to give you the best write throughput",
    "start": "1479040",
    "end": "1485520"
  },
  {
    "text": "with using the bulk loader we also provide some database management functionalities so we provide the",
    "start": "1485520",
    "end": "1491850"
  },
  {
    "text": "ability to have list and view running queries the ability to cancel running queries and all of this sits on top of a",
    "start": "1491850",
    "end": "1500010"
  },
  {
    "text": "cloud native storage layer which is leveraged from technology originally developed for other instance-based",
    "start": "1500010",
    "end": "1505500"
  },
  {
    "text": "database services at AWS and i'm not going to go into this in too much detail because Bruce is gonna really deep dive",
    "start": "1505500",
    "end": "1511320"
  },
  {
    "text": "on it in just a moment but this layer is what gives us our multi AZ high",
    "start": "1511320",
    "end": "1517320"
  },
  {
    "text": "availability read replication we support encryption at rest and it's also enabled",
    "start": "1517320",
    "end": "1523020"
  },
  {
    "text": "us to have a relatively fast path for things like compliance certification so we achieved us HIPAA compliance at the",
    "start": "1523020",
    "end": "1530790"
  },
  {
    "text": "end of August so it's really a purpose-built database riding on a cloud native storage layer that supports these",
    "start": "1530790",
    "end": "1537030"
  },
  {
    "text": "open source and standard api's it's of course packaged as a fully managed",
    "start": "1537030",
    "end": "1542520"
  },
  {
    "start": "1539000",
    "end": "1572000"
  },
  {
    "text": "service that has all of the things that you'd expect from AWS so we have SDK and CLI support so you can manage it",
    "start": "1542520",
    "end": "1549330"
  },
  {
    "text": "completely via those mechanisms or via the console we support cloud formation we support today encryption at rest",
    "start": "1549330",
    "end": "1556200"
  },
  {
    "text": "encryption in transit you can manage your backup and restore functionality so it's as a fully managed graph database",
    "start": "1556200",
    "end": "1562860"
  },
  {
    "text": "it takes away that operational burden that you see as the graph databases grow",
    "start": "1562860",
    "end": "1567870"
  },
  {
    "text": "and it becomes hard to maintain query performance from a security perspective",
    "start": "1567870",
    "end": "1574850"
  },
  {
    "start": "1572000",
    "end": "1609000"
  },
  {
    "text": "Neptune supports deployment in virtual private cloud so we only support VPC deployment we support client encryption",
    "start": "1574850",
    "end": "1582180"
  },
  {
    "text": "using HTTP and TLS one - for connecting to the gremlin WebSocket clients or the",
    "start": "1582180",
    "end": "1587970"
  },
  {
    "text": "rest endpoints you can choose to use Amazon key management service or kms to",
    "start": "1587970",
    "end": "1593760"
  },
  {
    "text": "use your own keys to manage to encrypt the data arrests and you can also choose to an a well I am based authentication so you",
    "start": "1593760",
    "end": "1600870"
  },
  {
    "text": "can choose to enable that for access to the gremlin web socket or any of the rest services it's just a float quick",
    "start": "1600870",
    "end": "1611220"
  },
  {
    "start": "1609000",
    "end": "1620000"
  },
  {
    "text": "flip of some of the customers that we had as public launch customers for the service and today we're generally",
    "start": "1611220",
    "end": "1622710"
  },
  {
    "start": "1620000",
    "end": "1644000"
  },
  {
    "text": "available we were announced in May we were announced originally in four regions we've expanded the two",
    "start": "1622710",
    "end": "1629010"
  },
  {
    "text": "additional European regions so we launched in London in October we launched in Frankfurt this month and",
    "start": "1629010",
    "end": "1636000"
  },
  {
    "text": "then we're planning on a continued region expansion so with that I'd like to turn it over to my colleague Bruce",
    "start": "1636000",
    "end": "1641460"
  },
  {
    "text": "Megrahi to talk about our enterprise features all right thank you very much",
    "start": "1641460",
    "end": "1650490"
  },
  {
    "start": "1644000",
    "end": "1660000"
  },
  {
    "text": "Brad I'm excited to be here and pleased to be able to share with you all some of",
    "start": "1650490",
    "end": "1656370"
  },
  {
    "text": "what's under the hood of Neptune and show you how things work so I'm going to",
    "start": "1656370",
    "end": "1661500"
  },
  {
    "start": "1660000",
    "end": "1870000"
  },
  {
    "text": "be speaking about Neptune's distributed storage architecture and how we achieve",
    "start": "1661500",
    "end": "1667350"
  },
  {
    "text": "performance availability and durability simultaneously with minimal trade-offs",
    "start": "1667350",
    "end": "1674190"
  },
  {
    "text": "all delivered as a managed service so Neptune is built on a scale out replica",
    "start": "1674190",
    "end": "1680309"
  },
  {
    "text": "architecture and by that we mean that there's a primary engine node which is servicing normally a dedicated write",
    "start": "1680309",
    "end": "1687870"
  },
  {
    "text": "workload although it can serve mixed or read workloads as well it's built with a gremlin and Sparkle endpoint and they",
    "start": "1687870",
    "end": "1695100"
  },
  {
    "text": "each have their respective query processing stacks and those are built on a shared transaction management and",
    "start": "1695100",
    "end": "1701510"
  },
  {
    "text": "caching layer for caching the pages out of the storage for in-memory query",
    "start": "1701510",
    "end": "1707610"
  },
  {
    "text": "optimization these engine nodes sit on",
    "start": "1707610",
    "end": "1712649"
  },
  {
    "text": "top of a shared storage volume to the engine nodes it just appears as a",
    "start": "1712649",
    "end": "1717809"
  },
  {
    "text": "sequence of 10 gigabyte segments it's very easy to program and access to in",
    "start": "1717809",
    "end": "1723799"
  },
  {
    "text": "reality under the hood behind the network there's a whole storage management layer that's hiding the fact",
    "start": "1723799",
    "end": "1730649"
  },
  {
    "text": "that the data in the ten gigabytes segments is actually striped across hundreds of",
    "start": "1730649",
    "end": "1736090"
  },
  {
    "text": "storage nodes and furthermore those storage nodes are partitioned into three",
    "start": "1736090",
    "end": "1741910"
  },
  {
    "text": "different AZ's to guarantee availability with two copies of every segment in each",
    "start": "1741910",
    "end": "1747430"
  },
  {
    "text": "of the AZ's and the segments are distributed in a random manner non",
    "start": "1747430",
    "end": "1753340"
  },
  {
    "text": "sequentially across each of the a disease that has a number of benefits one of which is hotspot rebalance which",
    "start": "1753340",
    "end": "1760600"
  },
  {
    "text": "is very important for guaranteeing consistency right throughput and read",
    "start": "1760600",
    "end": "1766540"
  },
  {
    "text": "throughput and hotspot rebalance is where one of the storage nodes may have some hot segments that are accessed by",
    "start": "1766540",
    "end": "1773500"
  },
  {
    "text": "many of the engine nodes so it's being overloaded on its network card in that",
    "start": "1773500",
    "end": "1779020"
  },
  {
    "text": "case the management layer will automatically detect this overload and shift some of the segments to nearby",
    "start": "1779020",
    "end": "1784570"
  },
  {
    "text": "storage nodes which are under lower Network load and that's all possible",
    "start": "1784570",
    "end": "1790270"
  },
  {
    "text": "because of the organization of the storage into ten gigabytes segments and stripe nature there's another important",
    "start": "1790270",
    "end": "1796240"
  },
  {
    "text": "feature which we'll get into in quite a bit more detail which is the ability for fast database recovery so you can",
    "start": "1796240",
    "end": "1803110"
  },
  {
    "text": "imagine compared to striping this on a single disk when you're restoring the volume at some point in time we can",
    "start": "1803110",
    "end": "1809740"
  },
  {
    "text": "parallel that work asynchronously across all these storage nodes so we can do it",
    "start": "1809740",
    "end": "1814960"
  },
  {
    "text": "much faster there's another subtlety about Neptune storage layer and that is",
    "start": "1814960",
    "end": "1820090"
  },
  {
    "text": "that the log application is actually embedded down in the storage layer rather than in the engine node which is",
    "start": "1820090",
    "end": "1826720"
  },
  {
    "text": "the case in many databases so the engine nodes only have to ship the log down and",
    "start": "1826720",
    "end": "1833440"
  },
  {
    "text": "those are basically just containing the deltas that are happening on the pages rather than shipping pool pages so that",
    "start": "1833440",
    "end": "1839320"
  },
  {
    "text": "massively reduces the amount of network traffic that the storage layer needs to",
    "start": "1839320",
    "end": "1844570"
  },
  {
    "text": "handle and for the primary for that matter as well so that results in much less work on the engine for",
    "start": "1844570",
    "end": "1850470"
  },
  {
    "text": "synchronizing with the storage layer so the engine can stay focused on servicing the transactions in the transaction",
    "start": "1850470",
    "end": "1857440"
  },
  {
    "text": "layer it also minimizes network traffic and after all the network bandwidth is",
    "start": "1857440",
    "end": "1864070"
  },
  {
    "text": "at the day the bottleneck for read and write throughput another aspect and",
    "start": "1864070",
    "end": "1874249"
  },
  {
    "start": "1870000",
    "end": "1979000"
  },
  {
    "text": "benefit of having six-way replication is the ability to survive an AZ plus one",
    "start": "1874249",
    "end": "1881590"
  },
  {
    "text": "additional failure and the way we do that is with the six copies across three",
    "start": "1881590",
    "end": "1887269"
  },
  {
    "text": "availability zones we work on a right quorum of four out of six and a Recor of three out of six",
    "start": "1887269",
    "end": "1893779"
  },
  {
    "text": "and indeed in large fleets it's possible to have all kinds of different failures",
    "start": "1893779",
    "end": "1899629"
  },
  {
    "text": "discs and segments failures are common you can lose an instance you can even lose an entire AZ either through loss of",
    "start": "1899629",
    "end": "1906259"
  },
  {
    "text": "network connectivity or through losing power and the reason we want to be able",
    "start": "1906259",
    "end": "1912409"
  },
  {
    "text": "to survive a z plus one is a Z events are relatively uncommon but when they do",
    "start": "1912409",
    "end": "1917690"
  },
  {
    "text": "happen they tend to occur longer so the likelihood that you have one additional even just losing one segment is",
    "start": "1917690",
    "end": "1923570"
  },
  {
    "text": "relatively higher when you have a longer outage on an AZ so we need to be able to survive those types of scenarios so",
    "start": "1923570",
    "end": "1931190"
  },
  {
    "text": "Neptune storage layer is continuously monitoring for these kind of failures and then it repairs itself by detecting",
    "start": "1931190",
    "end": "1937369"
  },
  {
    "text": "missing segments and through peer-to-peer gossiping it rebuilds those missing segments on the fly",
    "start": "1937369",
    "end": "1943419"
  },
  {
    "text": "automatically so we have two scenarios here on the Left we have a scenario where we've lost with the small X a disk",
    "start": "1943419",
    "end": "1950929"
  },
  {
    "text": "or a segment of a disk and in addition that same segment is is lost on two",
    "start": "1950929",
    "end": "1956779"
  },
  {
    "text": "other instances where we loose the whole storage node that's represented by the medium-sized X so we've lost three of",
    "start": "1956779",
    "end": "1962749"
  },
  {
    "text": "the segments but with the other three we're still able to maintain read quorum and continue to operate the database on",
    "start": "1962749",
    "end": "1968539"
  },
  {
    "text": "the right hand side we have an example of losing entire AZ and in this case we",
    "start": "1968539",
    "end": "1973940"
  },
  {
    "text": "can continue to service writes because we can maintain a write quorum a four out of six so still we often get the",
    "start": "1973940",
    "end": "1981769"
  },
  {
    "start": "1979000",
    "end": "2084000"
  },
  {
    "text": "question you know why do we need six copies there's actually two reasons for that and you know one of the reasons",
    "start": "1981769",
    "end": "1988429"
  },
  {
    "text": "obviously is availability we just hit on that in the previous slide so the question is why wouldn't you just do",
    "start": "1988429",
    "end": "1994159"
  },
  {
    "text": "three copies you know one in each AZ the answer is if you look at the upper portion of slide we lose a z3 for a long",
    "start": "1994159",
    "end": "2000770"
  },
  {
    "text": "period of time represented by the lighter colored red dots and then at",
    "start": "2000770",
    "end": "2006140"
  },
  {
    "text": "some point where a z3 is down we lose say just a single segment on a z1 then",
    "start": "2006140",
    "end": "2011450"
  },
  {
    "text": "you'd lose your your record be down for a period of time on the other hand if we",
    "start": "2011450",
    "end": "2016549"
  },
  {
    "text": "have two copies in each AZ when a c3 goes down and we lose a single segment in a z1 we can still maintain a Recor",
    "start": "2016549",
    "end": "2024200"
  },
  {
    "text": "of three out of six there's another equally important if not more important",
    "start": "2024200",
    "end": "2029710"
  },
  {
    "text": "consequence of storing six copies of data and it's a little bit counterintuitive you might think well",
    "start": "2029710",
    "end": "2035659"
  },
  {
    "text": "storing six copies of data is a lot of overhead you know how would that help performance the answer is that the",
    "start": "2035659",
    "end": "2041450"
  },
  {
    "text": "performance is actually primarily driven by Network latency the engine nodes need to have acknowledgment from the storage",
    "start": "2041450",
    "end": "2048108"
  },
  {
    "text": "layer that what they've written is durable right well network latencies",
    "start": "2048109",
    "end": "2054320"
  },
  {
    "text": "have an exponential distribution and indeed in the tails of the exponential distribution Layton sees can become",
    "start": "2054320",
    "end": "2060888"
  },
  {
    "text": "untolerable long so what the storage layer does is instead of waiting for all",
    "start": "2060889",
    "end": "2065898"
  },
  {
    "text": "six of the copies to be acknowledged in the case of a read it simply waits until three so the three longest latencies on",
    "start": "2065899",
    "end": "2073460"
  },
  {
    "text": "the network are simply hidden by the storage layer similar thing happens with writes were in a four out of six quorum",
    "start": "2073460",
    "end": "2079970"
  },
  {
    "text": "we can hide two of the longest Network latencies another benefit of the",
    "start": "2079970",
    "end": "2087470"
  },
  {
    "start": "2084000",
    "end": "2136000"
  },
  {
    "text": "distributed nature of Amazon storage layer is the ability to do continuous backup and the way this works is every",
    "start": "2087470",
    "end": "2094820"
  },
  {
    "text": "single these segments is continuously being backed up in the background Neptune takes snapshots of the segments",
    "start": "2094820",
    "end": "2101450"
  },
  {
    "text": "and because the segments are relatively small Neptune could snapshot on a frequent basis in the meantime Neptune is",
    "start": "2101450",
    "end": "2108800"
  },
  {
    "text": "shipping the redo logs in between segments snapshots over to s3 so they're durable there as well when a restore",
    "start": "2108800",
    "end": "2117080"
  },
  {
    "text": "happens Neptune loads the first snapshot for each of the segments and then begins",
    "start": "2117080",
    "end": "2123530"
  },
  {
    "text": "applying the redo logs to catch up with the latest recovery point",
    "start": "2123530",
    "end": "2130170"
  },
  {
    "text": "so when Neptune does a restore in this manner this all happens parallel and asynchronously and it has an additional",
    "start": "2130170",
    "end": "2137700"
  },
  {
    "start": "2136000",
    "end": "2235000"
  },
  {
    "text": "benefit not just is back up parallel and fast but when you need to recover it's almost instant",
    "start": "2137700",
    "end": "2143580"
  },
  {
    "text": "so first let's contrast how that would happen in more of a traditional database in a traditional database the data is",
    "start": "2143580",
    "end": "2150360"
  },
  {
    "text": "maybe striped on one disk in a very bad case or across a number of disks but",
    "start": "2150360",
    "end": "2155550"
  },
  {
    "text": "certainly not in a storage Network layer like Neptune at least in a traditional database typically a backup is achieved",
    "start": "2155550",
    "end": "2163770"
  },
  {
    "text": "by doing checkpoints periodically say every several minutes when a restore happens the database cannot be",
    "start": "2163770",
    "end": "2169860"
  },
  {
    "text": "operational until all of the redo log has been applied and that often is done in a single threaded manner so you can",
    "start": "2169860",
    "end": "2176700"
  },
  {
    "text": "be down for several minutes Neptune on the other hand has a little trick in the storage layer the fact is that normal",
    "start": "2176700",
    "end": "2184530"
  },
  {
    "text": "reads actually are following almost the same mechanism so when you do a read in Neptune the engine node requests a page",
    "start": "2184530",
    "end": "2192060"
  },
  {
    "text": "from the storage layer it's entirely possible in the storage layer that not",
    "start": "2192060",
    "end": "2197340"
  },
  {
    "text": "all of the redo logs have been applied to the page yet the Neptune will catch up on the fly and return the page and",
    "start": "2197340",
    "end": "2203910"
  },
  {
    "text": "this all happens really fast because the storage layer is composed of SSDs and very powerful storage nodes so we can",
    "start": "2203910",
    "end": "2210360"
  },
  {
    "text": "achieve that and it actually ends up being a very small portion of the total latency of requests now if you'll note",
    "start": "2210360",
    "end": "2216900"
  },
  {
    "text": "that since Neptune can tolerate doing reads on pages where the redo logs have",
    "start": "2216900",
    "end": "2222150"
  },
  {
    "text": "not yet been fully applied then you'll see that we can actually begin crash",
    "start": "2222150",
    "end": "2227310"
  },
  {
    "text": "recovery immediately there's no reason to wait until the redo log is applied we can just start operation immediately",
    "start": "2227310",
    "end": "2235460"
  },
  {
    "start": "2235000",
    "end": "2282000"
  },
  {
    "text": "there's another interesting feature that's quite useful which is database backtrack because we have small segments",
    "start": "2236210",
    "end": "2242310"
  },
  {
    "text": "and a history of them and can recover quickly you can ask Neptune to simply rewind at some point in time and that'll",
    "start": "2242310",
    "end": "2249000"
  },
  {
    "text": "happen almost instantaneously furthermore the rewind is non-destructive so you can rewind a",
    "start": "2249000",
    "end": "2255540"
  },
  {
    "text": "point in time check the status of the database rewind again or even fast-forward until you find the point in",
    "start": "2255540",
    "end": "2261840"
  },
  {
    "text": "time where you are comfortable with the status state of the database so if you",
    "start": "2261840",
    "end": "2267150"
  },
  {
    "text": "for example have some unintentional inserts or deletes or an unintentional book load or raw or load the wrong data",
    "start": "2267150",
    "end": "2272520"
  },
  {
    "text": "or the wrong shape of the data then you can just use this backtrack mechanism to go back and to the point in time before",
    "start": "2272520",
    "end": "2279660"
  },
  {
    "text": "the error occurred Neptune's storage",
    "start": "2279660",
    "end": "2285900"
  },
  {
    "start": "2282000",
    "end": "2326000"
  },
  {
    "text": "layer supports up to 64 terabyte volumes and it's not really necessary to",
    "start": "2285900",
    "end": "2291060"
  },
  {
    "text": "allocate up in front the amount of storage you intend to use so Neptune auto increments the storage in 10",
    "start": "2291060",
    "end": "2298829"
  },
  {
    "text": "gigabyte units and only allocates the amount that you actually need at any point in time if Neptune detects that",
    "start": "2298829",
    "end": "2306150"
  },
  {
    "text": "the storage volume needs to be grown it's done automatically and there's no impact on the performance similarly from",
    "start": "2306150",
    "end": "2314160"
  },
  {
    "text": "a user perspective if you want to create a snapshot in addition to the backups that are happening in the background that's also done with no performance on",
    "start": "2314160",
    "end": "2322020"
  },
  {
    "text": "impact because of the nature of the storage layer so now let's take a closer",
    "start": "2322020",
    "end": "2329010"
  },
  {
    "start": "2326000",
    "end": "2485000"
  },
  {
    "text": "look at the Redrup luca's which are the other side of the story for performance and availability this story goes",
    "start": "2329010",
    "end": "2335280"
  },
  {
    "text": "hand-in-hand with what's happening in the storage layer so in addition to the",
    "start": "2335280",
    "end": "2341250"
  },
  {
    "text": "Neptune primary you can allocate up to five additional read replicas instances",
    "start": "2341250",
    "end": "2346680"
  },
  {
    "text": "and these will work in synchronization with the Neptune primary to provide",
    "start": "2346680",
    "end": "2352980"
  },
  {
    "text": "additional read throughput a very important factor is replica lag and",
    "start": "2352980",
    "end": "2359099"
  },
  {
    "text": "Neptune is able to provide very low replica lag relative to most databases typically on the order of less than 10",
    "start": "2359099",
    "end": "2365640"
  },
  {
    "text": "milliseconds the reason for that is because of the way the storage layer is architected we're simply shipping redo",
    "start": "2365640",
    "end": "2372660"
  },
  {
    "text": "logs which are very minimal in size across the network just the deltas instead of the full pages those same",
    "start": "2372660",
    "end": "2378660"
  },
  {
    "text": "redo logs are shipped from primary to the replicas because they're small in size there's very little network traffic",
    "start": "2378660",
    "end": "2384720"
  },
  {
    "text": "necessary for the replicas to say instinct is stay in sync with the master",
    "start": "2384720",
    "end": "2390450"
  },
  {
    "text": "what happens the replicas will check if any of the pages",
    "start": "2390450",
    "end": "2396240"
  },
  {
    "text": "that are in scash need redo logs applied as they come in and then it'll do so and",
    "start": "2396240",
    "end": "2401580"
  },
  {
    "text": "advance the read view to stay in sync with the master if on the other hand the",
    "start": "2401580",
    "end": "2406740"
  },
  {
    "text": "replica needs to get something that's not cached and fetch it out of the storage layer those are automatically in",
    "start": "2406740",
    "end": "2413970"
  },
  {
    "text": "sync with the master because they've already been committed to the storage layer so that's the performance side of",
    "start": "2413970",
    "end": "2420390"
  },
  {
    "text": "the story with read replicas there's also an availability side which is equally important for availability",
    "start": "2420390",
    "end": "2426230"
  },
  {
    "text": "Neptune has a management layer that's continuously monitoring the health of the engine notes if it detects a problem",
    "start": "2426230",
    "end": "2433200"
  },
  {
    "text": "with any of them then it replaces them automatically in the case of a primary",
    "start": "2433200",
    "end": "2438710"
  },
  {
    "text": "Neptune will choose a replica and you're able to specify which replicas and specify the failover order and then",
    "start": "2438710",
    "end": "2446310"
  },
  {
    "text": "it'll fail over in less than 60 seconds and in fact typically less than 30 seconds for a failover and you can use",
    "start": "2446310",
    "end": "2454080"
  },
  {
    "text": "this mechanism actually as a nice little feature to upgrade the primary so if you",
    "start": "2454080",
    "end": "2459300"
  },
  {
    "text": "want to have a more powerful instance because your workload is growing over time you allocate a powerful replica you",
    "start": "2459300",
    "end": "2466950"
  },
  {
    "text": "warm it up and then you specify it as the failover node and then at a point in time you're choosing say when traffic is",
    "start": "2466950",
    "end": "2473880"
  },
  {
    "text": "very low you simply force a failover and then that replicas the more powerful replicas becomes the new primary and",
    "start": "2473880",
    "end": "2480390"
  },
  {
    "text": "that all happens in a very managed automatic fashion so finally with all",
    "start": "2480390",
    "end": "2487320"
  },
  {
    "start": "2485000",
    "end": "2526000"
  },
  {
    "text": "this complexity happening under the hood it's important to be able to monitor and understand what's going on so we provide",
    "start": "2487320",
    "end": "2494790"
  },
  {
    "text": "AWS cloud trail which essentially logs all of the Neptune API calls to s3 we",
    "start": "2494790",
    "end": "2502619"
  },
  {
    "text": "also provide event notification using SNS subscriptions via either AWS CLI or",
    "start": "2502619",
    "end": "2509339"
  },
  {
    "text": "through the AWS SDK and finally we ship metrics to AWS cloud watch constantly so",
    "start": "2509339",
    "end": "2517589"
  },
  {
    "text": "you can understand what's happening in the storage layer what's happening in the instance nodes and how your gremlin and Sparkle queries are performing so",
    "start": "2517589",
    "end": "2527760"
  },
  {
    "text": "that's all I have and with that I'll turn it back over to Brad to talk about how to get started thank Bruce so I know a lots of",
    "start": "2527760",
    "end": "2536200"
  },
  {
    "text": "you are already using graph databases we have a bunch of different resources that you can use to learn more to get started",
    "start": "2536200",
    "end": "2541870"
  },
  {
    "text": "so I just wanted to flip through those very quickly so in the last couple weeks we have released two different blog",
    "start": "2541870",
    "end": "2548680"
  },
  {
    "start": "2542000",
    "end": "2594000"
  },
  {
    "text": "posts on the AWS database blogs and this is the start of a series that we're",
    "start": "2548680",
    "end": "2554110"
  },
  {
    "text": "going to be doing about using graph databases and what they've done is they've started showing how to use",
    "start": "2554110",
    "end": "2561490"
  },
  {
    "text": "Neptune in the context of using sage maker Jupiter notebooks as a way to",
    "start": "2561490",
    "end": "2566620"
  },
  {
    "text": "query and interactively view the data and so there's examples there about just getting started there's a second one",
    "start": "2566620",
    "end": "2573700"
  },
  {
    "text": "that shows an example using a air routes data set which is quite common for",
    "start": "2573700",
    "end": "2579190"
  },
  {
    "text": "learning how to use pachi tinker pop tremblin and so this would be an option for you to take out and for the",
    "start": "2579190",
    "end": "2584950"
  },
  {
    "text": "discriminating slide viewer you'll be able to see that this was done by one of our uk-based solutions architects with",
    "start": "2584950",
    "end": "2592240"
  },
  {
    "text": "the visualize spelling there so another way to get started is to use cloud",
    "start": "2592240",
    "end": "2597700"
  },
  {
    "start": "2594000",
    "end": "2628000"
  },
  {
    "text": "formation so we of course support cloud formation you this is a actually a screenshot from our Doc's you can use",
    "start": "2597700",
    "end": "2605020"
  },
  {
    "text": "the designer or you can get started right away using and launching this script so this is a very easy way for",
    "start": "2605020",
    "end": "2610330"
  },
  {
    "text": "you to get a Neptune instance started the blog's that I talked about come with",
    "start": "2610330",
    "end": "2616270"
  },
  {
    "text": "cloud formation scripts that not only provision Neptune instances but they also will provision your stage maker",
    "start": "2616270",
    "end": "2621640"
  },
  {
    "text": "Jupiter notebooks as well pictures",
    "start": "2621640",
    "end": "2628980"
  },
  {
    "start": "2628000",
    "end": "2680000"
  },
  {
    "text": "another thing that you can do is we have several different github repositories and so we have a Amazon Neptune samples",
    "start": "2629520",
    "end": "2637790"
  },
  {
    "text": "github repository on the AWS or the AWS samples lab and there's a number of",
    "start": "2637790",
    "end": "2644490"
  },
  {
    "text": "different examples there all of the source code with the blog post is available we have an example about how",
    "start": "2644490",
    "end": "2650310"
  },
  {
    "text": "to do collaborative filtering which is a recommendation engine approach we now even since I've taken the screenshot in",
    "start": "2650310",
    "end": "2657480"
  },
  {
    "text": "the last week we've added new samples so we now have one about how to use IMDB to find six degrees of Kevin Bacon if",
    "start": "2657480",
    "end": "2664140"
  },
  {
    "text": "you're interested in having fun with that you know as well as an example about how to use glue and Athena to take",
    "start": "2664140",
    "end": "2671700"
  },
  {
    "text": "data that you have stored in s3 turn it into a format that you can ingest into Neptune and start working with it in",
    "start": "2671700",
    "end": "2678600"
  },
  {
    "text": "that data set and finally we also have some tools that are available so we have",
    "start": "2678600",
    "end": "2685170"
  },
  {
    "start": "2680000",
    "end": "2730000"
  },
  {
    "text": "a separate github repo in the AWS labs repo that hosts currently in a tool to",
    "start": "2685170",
    "end": "2692520"
  },
  {
    "text": "convert graph ml which is an xml-based serialization of graphs used for property graph into the CSV format that",
    "start": "2692520",
    "end": "2699300"
  },
  {
    "text": "Neptune uses we also now have a Java program that will do a parallel export",
    "start": "2699300",
    "end": "2705150"
  },
  {
    "text": "of a Neptune instance so if you want to take all the data in your Neptune instance and and export it we have that",
    "start": "2705150",
    "end": "2711300"
  },
  {
    "text": "and then additionally we have this one has also been updated we have an example",
    "start": "2711300",
    "end": "2716670"
  },
  {
    "text": "about how to use glue and use glue to ETL data into Neptune so there's lots of resources that are available and we're",
    "start": "2716670",
    "end": "2722280"
  },
  {
    "text": "continuously working to build more and we really like to hear from you guys about what you'd like to see and how you can how we can make it easier for you to",
    "start": "2722280",
    "end": "2729180"
  },
  {
    "text": "work with Neptune so with that I'm gonna take a few questions one of our senior engineers Mike",
    "start": "2729180",
    "end": "2734760"
  },
  {
    "start": "2730000",
    "end": "2742000"
  },
  {
    "text": "personick is in the back here and he has a microphone so if anybody feels like they don't want to yell out we have a",
    "start": "2734760",
    "end": "2740760"
  },
  {
    "text": "microphone so raise your hand and before we do I'm just going to flip this up to take a picture this is my contact",
    "start": "2740760",
    "end": "2746760"
  },
  {
    "start": "2742000",
    "end": "2770000"
  },
  {
    "text": "information and Bruce's contact information and you know at the service team we want to hear about how Neptune",
    "start": "2746760",
    "end": "2752430"
  },
  {
    "text": "is working for you and we want to hear what you're doing with it so you know do please feel free to reach out and let us",
    "start": "2752430",
    "end": "2757590"
  },
  {
    "text": "know so with that let's let me go ahead and take a question over here on the right",
    "start": "2757590",
    "end": "2763180"
  },
  {
    "start": "2770000",
    "end": "2817000"
  },
  {
    "text": "so the question was if I think how close am i if I think of Neptune as blaze",
    "start": "2770410",
    "end": "2776930"
  },
  {
    "text": "graph implemented over the Aurora storage engine you are standing next to Mike personick and myself who previously",
    "start": "2776930",
    "end": "2783590"
  },
  {
    "text": "to AWS did work for blaze graph and Neptune uses lots of different open",
    "start": "2783590",
    "end": "2788870"
  },
  {
    "text": "source packages as part of it that's part of the nature of supporting both tinkerer pop and RDF and Sparkle but if",
    "start": "2788870",
    "end": "2795380"
  },
  {
    "text": "you've ever operated a blaze graph instance at large scale you'll notice some really important operational",
    "start": "2795380",
    "end": "2801590"
  },
  {
    "text": "differences and so you know there's lots of things that are unique to Neptune that you make it a much more scalable",
    "start": "2801590",
    "end": "2809150"
  },
  {
    "text": "and easier to operate solution all right",
    "start": "2809150",
    "end": "2815349"
  },
  {
    "start": "2817000",
    "end": "2865000"
  },
  {
    "text": "so you mentioned graph joins are slow on relational databases right but where do",
    "start": "2817270",
    "end": "2824720"
  },
  {
    "text": "people get into trouble when they try to apply relational thinking to or to",
    "start": "2824720",
    "end": "2834020"
  },
  {
    "text": "Neptune like what kind of queries like are they concurrent queries that you",
    "start": "2834020",
    "end": "2839120"
  },
  {
    "text": "would have thought run in O of one but they don't are they like going back to your graduate student lookup that sly",
    "start": "2839120",
    "end": "2846590"
  },
  {
    "text": "that you had are they you know if you had a record of every student in history",
    "start": "2846590",
    "end": "2851810"
  },
  {
    "text": "and he had a trillion students you know",
    "start": "2851810",
    "end": "2856300"
  },
  {
    "text": "connected in a sparse graph like what where do customers are in a trouble what",
    "start": "2857350",
    "end": "2862850"
  },
  {
    "text": "do you recommend where do you draw the line yeah I think one thing that customers do when they move from a",
    "start": "2862850",
    "end": "2869810"
  },
  {
    "start": "2865000",
    "end": "2926000"
  },
  {
    "text": "relational database into graph is they they think a little bit too strictly",
    "start": "2869810",
    "end": "2875450"
  },
  {
    "text": "about modeling their data from a schema perspective and so they think about that a little bit more than a necessarily",
    "start": "2875450",
    "end": "2881360"
  },
  {
    "text": "need to from a graph perspective from a query side you know it's really about how much data",
    "start": "2881360",
    "end": "2888560"
  },
  {
    "text": "the query has to touch to evaluate it so there's cases where you know if you retrieve nodes we have a relatively",
    "start": "2888560",
    "end": "2895670"
  },
  {
    "text": "simple query which you have to materialise large number of properties it's really no way around the fact that",
    "start": "2895670",
    "end": "2900920"
  },
  {
    "text": "you actually have to read that data off the storage layer so those are cases where you know you'll see slower performance and the others are cases",
    "start": "2900920",
    "end": "2906320"
  },
  {
    "text": "where you know you'd see a large degree of fan-out in your data and so those can",
    "start": "2906320",
    "end": "2911420"
  },
  {
    "text": "be managed you can take a look at them but there's certainly cases that you can use to optimize let's see let me does",
    "start": "2911420",
    "end": "2918890"
  },
  {
    "text": "they want to take one from this side",
    "start": "2918890",
    "end": "2921760"
  },
  {
    "start": "2926000",
    "end": "2982000"
  },
  {
    "text": "yeah so the question is do we plan to support cipher for those of you don't know cipher is a declarative graph query",
    "start": "2926140",
    "end": "2933290"
  },
  {
    "text": "language over property graph that was originally developed by neo4j today we do not support cipher we don't have firm",
    "start": "2933290",
    "end": "2940640"
  },
  {
    "text": "plans to support cipher we are very interested in declarative graph query over property graph in general there are",
    "start": "2940640",
    "end": "2947330"
  },
  {
    "text": "some ongoing initiatives both within the w3c as well as the gql initiative that",
    "start": "2947330",
    "end": "2952760"
  },
  {
    "text": "you may be familiar with and then there is also open cipher and there's a",
    "start": "2952760",
    "end": "2958760"
  },
  {
    "text": "translator between cipher and gremlins so that's certainly an option but as of today we don't support cipher",
    "start": "2958760",
    "end": "2964750"
  },
  {
    "text": "I'll take one more from over here thinking that so what's the Neptune",
    "start": "2964750",
    "end": "2973700"
  },
  {
    "text": "strategy to use for this multi region replication like this for something like a multi master like what dynamodb has",
    "start": "2973700",
    "end": "2981320"
  },
  {
    "text": "right now yeah so the question was is the net if I could paraphrase what is",
    "start": "2981320",
    "end": "2986630"
  },
  {
    "start": "2982000",
    "end": "3041000"
  },
  {
    "text": "the Neptune storage layer or something like dynamodb x' multi master functionality yeah so so the Neptune",
    "start": "2986630",
    "end": "2993920"
  },
  {
    "text": "storage layer is very different than dynamodb storage layer in terms of multi master functionality it is something",
    "start": "2993920",
    "end": "3001390"
  },
  {
    "text": "that we're thinking about on the Neptune roadmap but it's much further along most",
    "start": "3001390",
    "end": "3007000"
  },
  {
    "text": "likely towards the end of next year timeframe he's asking about cross region replication - yeah so and we today do",
    "start": "3007000",
    "end": "3014350"
  },
  {
    "text": "not support cross region replication we plan to support a cross region backup and restore in most likely the first",
    "start": "3014350",
    "end": "3022160"
  },
  {
    "text": "half of next year and then we will support eventually multi-region replication although again that's in the",
    "start": "3022160",
    "end": "3028220"
  },
  {
    "text": "further out timeline alright let me switch back over to this just to make",
    "start": "3028220",
    "end": "3033770"
  },
  {
    "text": "Michael ran around as much as possible",
    "start": "3033770",
    "end": "3037180"
  },
  {
    "start": "3041000",
    "end": "3088000"
  },
  {
    "text": "yeah so the question was do we plan on supporting geospatial types or geo sparkl we'd have a lot of customers that",
    "start": "3041049",
    "end": "3047720"
  },
  {
    "text": "are interested in geospatial and graph I think our current thinking is that we'll support something like a z-index type of",
    "start": "3047720",
    "end": "3055940"
  },
  {
    "text": "geospatial encoding initially and then look at demand to support a full geo",
    "start": "3055940",
    "end": "3061339"
  },
  {
    "text": "sparkl implementation as you're likely where do Sparkle has lots of different aspects to it all right let me just take",
    "start": "3061339",
    "end": "3069859"
  },
  {
    "text": "one right over there what kind of learning curve or your customers",
    "start": "3069859",
    "end": "3075740"
  },
  {
    "text": "experiencing when it comes to learning gremlin if their relational developers they're coming from SQL they hit gremlin",
    "start": "3075740",
    "end": "3081910"
  },
  {
    "text": "what kind of learning curve you seen and are you doing anything other than some of the things that you showed that maybe",
    "start": "3081910",
    "end": "3086960"
  },
  {
    "text": "shorten that learning curve for them yeah so I think one of the interesting things about gremlin is that there are",
    "start": "3086960",
    "end": "3093470"
  },
  {
    "start": "3088000",
    "end": "3161000"
  },
  {
    "text": "many many ways to write different traversals in gremlin and they are",
    "start": "3093470",
    "end": "3098900"
  },
  {
    "text": "effectively equivalent traversals but they can have different performance characteristics so that's something that",
    "start": "3098900",
    "end": "3104539"
  },
  {
    "text": "you know people run into we have one of our data architects is actually wrote",
    "start": "3104539",
    "end": "3109819"
  },
  {
    "text": "the book on gremlin about you know the practical guide to gremlins so we use that as a tool to help customers",
    "start": "3109819",
    "end": "3115880"
  },
  {
    "text": "understand how to use it the some of the blog posts and guides are sort of building out more examples in that area",
    "start": "3115880",
    "end": "3121760"
  },
  {
    "text": "so we're trying to both create more content that lets developers learn how to use gremlin as well as you know we're",
    "start": "3121760",
    "end": "3128779"
  },
  {
    "text": "also interested in direct engagement so if you have particular questions you know we're happy to work with you about them well and add to what Brad said I",
    "start": "3128779",
    "end": "3135289"
  },
  {
    "text": "mean we've worked with customers and data labs where they've actually had the feedback after they get over the initial learning curve of gremlin they actually",
    "start": "3135289",
    "end": "3141740"
  },
  {
    "text": "find it really a refreshing way to develop they don't have to deal with all the schema and some of the things that go with with relational alright so go",
    "start": "3141740",
    "end": "3149690"
  },
  {
    "text": "back try the front and then great yeah so the question is how far",
    "start": "3149690",
    "end": "3163670"
  },
  {
    "start": "3161000",
    "end": "3246000"
  },
  {
    "text": "away are we from getting users access to the Neptune logs to help with debugging we have a pretty strongly focused story",
    "start": "3163670",
    "end": "3170810"
  },
  {
    "text": "on increasing transparency into this the query engine and so we've recently released support for query hints which",
    "start": "3170810",
    "end": "3178369"
  },
  {
    "text": "allow you to specify different hints to the optimizer to take on different patterns we will be providing support",
    "start": "3178369",
    "end": "3185570"
  },
  {
    "text": "for xquery explanations and query plans for you to take a look at what's the",
    "start": "3185570",
    "end": "3191390"
  },
  {
    "text": "plans or what are the plans that are being generated and how can you didn't",
    "start": "3191390",
    "end": "3196520"
  },
  {
    "text": "look at the plan and try and understand how to adjust your query and then we're interested in feedback about sort of telemetry in general so we've been",
    "start": "3196520",
    "end": "3202700"
  },
  {
    "text": "improving the kinds of error messages that you get for example with I am authentication so that you can understand that you know it's that your",
    "start": "3202700",
    "end": "3209300"
  },
  {
    "text": "credentials have expired and that's why you're getting an error and rather than something in general so we're definitely have a strong story and a focus on",
    "start": "3209300",
    "end": "3215599"
  },
  {
    "text": "increasing transparency there and we'd be very interested in feedback so let me",
    "start": "3215599",
    "end": "3220640"
  },
  {
    "text": "take in the back in the black shirt",
    "start": "3220640",
    "end": "3224318"
  },
  {
    "text": "yes it's a we're working on a repeatable read like snapshot view for four",
    "start": "3230550",
    "end": "3236890"
  },
  {
    "text": "transactions between the regions how you",
    "start": "3236890",
    "end": "3243310"
  },
  {
    "text": "would recommend to deploy it across regions yeah so we have a we have a couple different customers that are",
    "start": "3243310",
    "end": "3249460"
  },
  {
    "start": "3246000",
    "end": "3313000"
  },
  {
    "text": "working on cross region patterns and they effectively fall into a couple different categories so some customers",
    "start": "3249460",
    "end": "3256060"
  },
  {
    "text": "are using DynamoDB global tables as their way to replicate data across",
    "start": "3256060",
    "end": "3262380"
  },
  {
    "text": "regions and then they're reading from those global tables to keep information in sync other customers are essentially",
    "start": "3262380",
    "end": "3269800"
  },
  {
    "text": "bifurcating they're ingestion and using that as the way that they replicate across regions the cross region backup",
    "start": "3269800",
    "end": "3277210"
  },
  {
    "text": "restoration that I mentioned earlier for use cases where the cross region mode is for disaster recovery that will help",
    "start": "3277210",
    "end": "3284230"
  },
  {
    "text": "enable those kinds of use cases something like in these extremes you",
    "start": "3284230",
    "end": "3290140"
  },
  {
    "text": "know the to stadium yeah you can't so you can use Kinesis so in the in the model where your bifurcating the",
    "start": "3290140",
    "end": "3296080"
  },
  {
    "text": "ingestion you can use Canisius streams than to send that across to another region",
    "start": "3296080",
    "end": "3302670"
  },
  {
    "text": "I'm just take the do we have a browser",
    "start": "3303090",
    "end": "3309130"
  },
  {
    "text": "like a new photo browser from the developer perspective to debug something quickly yes so today Neptune only",
    "start": "3309130",
    "end": "3315400"
  },
  {
    "text": "provides the WebSocket and the rest endpoints the Jupiter notebooks that I mentioned earlier are a step towards",
    "start": "3315400",
    "end": "3321910"
  },
  {
    "text": "letting you sort of explore the data we do plan to second Phonographic of a",
    "start": "3321910",
    "end": "3327460"
  },
  {
    "text": "prospective yeah so we do plan to support an in console query capability that lets you issue queries and see",
    "start": "3327460",
    "end": "3332620"
  },
  {
    "text": "results in the data initially that'll take the form of a sort of a tabular result set view and then it will also",
    "start": "3332620",
    "end": "3338320"
  },
  {
    "text": "add a graphical view we're trying to straddle the line between being a really fast effective graph database that",
    "start": "3338320",
    "end": "3344590"
  },
  {
    "text": "provides visualizations to help developers versus providing visualizations that you would use to build an application and have you guys",
    "start": "3344590",
    "end": "3352720"
  },
  {
    "text": "tried using tinker pop spark plug in - look I'm naked with Neptune it'll be",
    "start": "3352720",
    "end": "3358499"
  },
  {
    "text": "like trident there's some issues and I try to download the open source we we have had customers that have tried and",
    "start": "3358499",
    "end": "3364799"
  },
  {
    "text": "reported that's successfully used we haven't tried it ourselves so we'd be happy to follow up and try and see what",
    "start": "3364799",
    "end": "3370259"
  },
  {
    "text": "the gaps are thank you so the question",
    "start": "3370259",
    "end": "3398839"
  },
  {
    "start": "3403000",
    "end": "3455000"
  },
  {
    "text": "yeah so the question was a the attended workshop and they were talking about the fact that a you might execute a query",
    "start": "3403609",
    "end": "3411569"
  },
  {
    "text": "and the actual query that was executed may be different than what you expressed in the gremlin traversal and that's",
    "start": "3411569",
    "end": "3416789"
  },
  {
    "text": "absolutely correct so Neptune does you know is optimized and has a query optimizer for both the gremlin and the",
    "start": "3416789",
    "end": "3422640"
  },
  {
    "text": "sparkle stack and that's how we use what we use to get good performance out of their queries",
    "start": "3422640",
    "end": "3428900"
  },
  {
    "text": "the so we're definitely working on it's a challenge for sure I'm so thank you",
    "start": "3435569",
    "end": "3443730"
  },
  {
    "text": "all right let me go back to this side of bread yes so the question is are we",
    "start": "3443730",
    "end": "3457779"
  },
  {
    "start": "3455000",
    "end": "3556000"
  },
  {
    "text": "supporting any object graph mapping frameworks so actually one of our",
    "start": "3457779",
    "end": "3463599"
  },
  {
    "text": "partners who was here earlier has done some work to extend the spring framework",
    "start": "3463599",
    "end": "3471609"
  },
  {
    "text": "for gremlin to work effectively with Neptune so from that's an options from",
    "start": "3471609",
    "end": "3477039"
  },
  {
    "text": "the gremlin side and there are a couple of RDF options as well so it's not something that we are directly",
    "start": "3477039",
    "end": "3482380"
  },
  {
    "text": "supporting officially but I know that lots of customers are interested in using object graph mapping type of",
    "start": "3482380",
    "end": "3487990"
  },
  {
    "text": "techniques we have some customers that are looking at sort of migrating to Neptune from other solutions and porting",
    "start": "3487990",
    "end": "3494049"
  },
  {
    "text": "through their object graph mapping layer",
    "start": "3494049",
    "end": "3497700"
  },
  {
    "text": "and 14 idiots",
    "start": "3505299",
    "end": "3511079"
  },
  {
    "text": "oh I think please follow up with us on that because I think we'd like to",
    "start": "3511079",
    "end": "3516319"
  },
  {
    "text": "understand what the gaps are let's see",
    "start": "3516319",
    "end": "3522650"
  },
  {
    "start": "3556000",
    "end": "3600000"
  },
  {
    "text": "yeah so the question was if I can paraphrase and let me know if I'm incorrect so one of the use cases for",
    "start": "3556339",
    "end": "3562559"
  },
  {
    "text": "graph is to bring many different datasets together and when you bring datasets together they often have",
    "start": "3562559",
    "end": "3567809"
  },
  {
    "text": "varying degrees of quality and to use them in a production application and you need to do quality control and make sure",
    "start": "3567809",
    "end": "3573779"
  },
  {
    "text": "that your data's high quality so there's a couple of different approaches that people use some people which I view is",
    "start": "3573779",
    "end": "3578819"
  },
  {
    "text": "sort of the more traditional relational approaches they do it on the ETL process and they'll you know do do the quality",
    "start": "3578819",
    "end": "3585089"
  },
  {
    "text": "control in terms of the validation on the ETL another approach that people on the graph side do is they'll import the",
    "start": "3585089",
    "end": "3591779"
  },
  {
    "text": "data into the graph as is and they'll use the graph to explore and try and understand where the quality gaps are",
    "start": "3591779",
    "end": "3597420"
  },
  {
    "text": "and then they'll make new versions of that data in the graph and use that as part of their application in the back",
    "start": "3597420",
    "end": "3608150"
  },
  {
    "text": "yeah so it wasn't that we're not efficient up to three hops I actually I'm getting this single that we should",
    "start": "3626690",
    "end": "3633030"
  },
  {
    "text": "continue this discussion outside so we'll be out available to talk so thank you very much",
    "start": "3633030",
    "end": "3638070"
  },
  {
    "text": "[Applause]",
    "start": "3638070",
    "end": "3641749"
  }
]