[
  {
    "text": "hi",
    "start": "2560",
    "end": "3040"
  },
  {
    "text": "my name is waleek pari i'm a storage",
    "start": "3040",
    "end": "5040"
  },
  {
    "text": "specialist solution architect working at",
    "start": "5040",
    "end": "6640"
  },
  {
    "text": "aws",
    "start": "6640",
    "end": "8000"
  },
  {
    "text": "let's look at a demo of data sync in",
    "start": "8000",
    "end": "9840"
  },
  {
    "text": "action we'll go through a use case of",
    "start": "9840",
    "end": "11840"
  },
  {
    "text": "migrating 10 000",
    "start": "11840",
    "end": "13200"
  },
  {
    "text": "kilowatt size very small files as some",
    "start": "13200",
    "end": "16000"
  },
  {
    "text": "of you might be aware",
    "start": "16000",
    "end": "17119"
  },
  {
    "text": "small file migrations can be quite",
    "start": "17119",
    "end": "18800"
  },
  {
    "text": "challenging because of the file and",
    "start": "18800",
    "end": "20240"
  },
  {
    "text": "metadata operational overhead",
    "start": "20240",
    "end": "22160"
  },
  {
    "text": "that is required that can lead to low",
    "start": "22160",
    "end": "23840"
  },
  {
    "text": "copy operation speeds",
    "start": "23840",
    "end": "26160"
  },
  {
    "text": "in this demo we have a nfs share mounted",
    "start": "26160",
    "end": "29039"
  },
  {
    "text": "on a linux host",
    "start": "29039",
    "end": "30320"
  },
  {
    "text": "which has the 10 000 very small files",
    "start": "30320",
    "end": "32880"
  },
  {
    "text": "presented from an nfs server",
    "start": "32880",
    "end": "36559"
  },
  {
    "text": "we are first going to use a simple copy",
    "start": "36559",
    "end": "39040"
  },
  {
    "text": "script which uses the s3 copy command",
    "start": "39040",
    "end": "41840"
  },
  {
    "text": "located on the linux host to read the 10",
    "start": "41840",
    "end": "44320"
  },
  {
    "text": "000 small files from the nfs server",
    "start": "44320",
    "end": "46719"
  },
  {
    "text": "and copy them to an amazon s3 bucket",
    "start": "46719",
    "end": "50559"
  },
  {
    "text": "then in the second part of the demo we",
    "start": "50559",
    "end": "52320"
  },
  {
    "text": "have deployed a data sync agent and",
    "start": "52320",
    "end": "54160"
  },
  {
    "text": "configured a data sync transfer task",
    "start": "54160",
    "end": "56399"
  },
  {
    "text": "to read the same 10 000 files from the",
    "start": "56399",
    "end": "58320"
  },
  {
    "text": "nfs share",
    "start": "58320",
    "end": "59520"
  },
  {
    "text": "as you can see datasync reads the data",
    "start": "59520",
    "end": "61840"
  },
  {
    "text": "directly from the nfs server",
    "start": "61840",
    "end": "63840"
  },
  {
    "text": "then we are going to start the data",
    "start": "63840",
    "end": "65518"
  },
  {
    "text": "transfer where the task",
    "start": "65519",
    "end": "67920"
  },
  {
    "text": "will use data sync to copy the 10 000",
    "start": "67920",
    "end": "70240"
  },
  {
    "text": "small files",
    "start": "70240",
    "end": "71439"
  },
  {
    "text": "and the file metadata such as user and",
    "start": "71439",
    "end": "73520"
  },
  {
    "text": "group details",
    "start": "73520",
    "end": "74479"
  },
  {
    "text": "permissions and time stamps and saw them",
    "start": "74479",
    "end": "76560"
  },
  {
    "text": "in the same amazon s3 bucket for the",
    "start": "76560",
    "end": "78560"
  },
  {
    "text": "objects",
    "start": "78560",
    "end": "79360"
  },
  {
    "text": "that it creates okay let's start the",
    "start": "79360",
    "end": "82240"
  },
  {
    "text": "demo",
    "start": "82240",
    "end": "83360"
  },
  {
    "text": "we are logged into the linux host that",
    "start": "83360",
    "end": "85200"
  },
  {
    "text": "will be used in this demonstration",
    "start": "85200",
    "end": "87360"
  },
  {
    "text": "this host has an nfs mount point called",
    "start": "87360",
    "end": "89680"
  },
  {
    "text": "nfs underscore source",
    "start": "89680",
    "end": "91600"
  },
  {
    "text": "which stores the 10 000 small files that",
    "start": "91600",
    "end": "93840"
  },
  {
    "text": "will be used",
    "start": "93840",
    "end": "94640"
  },
  {
    "text": "in this demonstration let's go ahead and",
    "start": "94640",
    "end": "97360"
  },
  {
    "text": "look at some of the file share details",
    "start": "97360",
    "end": "99920"
  },
  {
    "text": "now",
    "start": "99920",
    "end": "100640"
  },
  {
    "text": "let's verify the number of files stored",
    "start": "100640",
    "end": "102799"
  },
  {
    "text": "in this file share",
    "start": "102799",
    "end": "105840"
  },
  {
    "text": "we can see it shows 1001 small files",
    "start": "107280",
    "end": "111520"
  },
  {
    "text": "now let's look at some of the files we",
    "start": "111520",
    "end": "113600"
  },
  {
    "text": "are storing in this file share",
    "start": "113600",
    "end": "115360"
  },
  {
    "text": "to view the attribute details such as",
    "start": "115360",
    "end": "117200"
  },
  {
    "text": "size and type",
    "start": "117200",
    "end": "119759"
  },
  {
    "text": "as you can see there are lots of very",
    "start": "119759",
    "end": "121439"
  },
  {
    "text": "small kilobyte-sized csv gif image files",
    "start": "121439",
    "end": "125840"
  },
  {
    "text": "stored",
    "start": "126840",
    "end": "128000"
  },
  {
    "text": "now let's look at the simple copy script",
    "start": "128000",
    "end": "130800"
  },
  {
    "text": "i'm going to use",
    "start": "130800",
    "end": "132000"
  },
  {
    "text": "to copy this file share data to my",
    "start": "132000",
    "end": "134720"
  },
  {
    "text": "target amazon s3 bucket",
    "start": "134720",
    "end": "138239"
  },
  {
    "text": "in this script i use the aws s3 copy",
    "start": "138959",
    "end": "141840"
  },
  {
    "text": "command",
    "start": "141840",
    "end": "142560"
  },
  {
    "text": "to copy data from the local nfs",
    "start": "142560",
    "end": "145440"
  },
  {
    "text": "underscore source mount point",
    "start": "145440",
    "end": "147360"
  },
  {
    "text": "to my target amazon s3 bucket called",
    "start": "147360",
    "end": "150480"
  },
  {
    "text": "nfs target data where i'm going to store",
    "start": "150480",
    "end": "154640"
  },
  {
    "text": "this data under a prefix",
    "start": "154640",
    "end": "156160"
  },
  {
    "text": "folder called s3 cli copy",
    "start": "156160",
    "end": "159920"
  },
  {
    "text": "let's go ahead and start the data",
    "start": "159920",
    "end": "161519"
  },
  {
    "text": "transfer script",
    "start": "161519",
    "end": "163120"
  },
  {
    "text": "in the second window below we can",
    "start": "163120",
    "end": "164879"
  },
  {
    "text": "monitor the performance of how fast a",
    "start": "164879",
    "end": "166800"
  },
  {
    "text": "small file data transfer is copying to",
    "start": "166800",
    "end": "168840"
  },
  {
    "text": "s3",
    "start": "168840",
    "end": "171840"
  },
  {
    "text": "if we fast forward to the end of the",
    "start": "174080",
    "end": "175680"
  },
  {
    "text": "data transfer we see that the average",
    "start": "175680",
    "end": "178000"
  },
  {
    "text": "data transfer speed was about 5",
    "start": "178000",
    "end": "179760"
  },
  {
    "text": "megabytes per second",
    "start": "179760",
    "end": "181360"
  },
  {
    "text": "you might say that's not very fast well",
    "start": "181360",
    "end": "183760"
  },
  {
    "text": "that's because we are transferring 10",
    "start": "183760",
    "end": "185440"
  },
  {
    "text": "000 very small kilobyte size files",
    "start": "185440",
    "end": "187840"
  },
  {
    "text": "which generally tends to be a low",
    "start": "187840",
    "end": "189519"
  },
  {
    "text": "bandwidth operation",
    "start": "189519",
    "end": "190879"
  },
  {
    "text": "compared to transferring large megabyte",
    "start": "190879",
    "end": "193200"
  },
  {
    "text": "or gigabyte size files",
    "start": "193200",
    "end": "194879"
  },
  {
    "text": "where you could get transfer speeds in",
    "start": "194879",
    "end": "196640"
  },
  {
    "text": "the hundreds of megabytes per second",
    "start": "196640",
    "end": "198800"
  },
  {
    "text": "now let's look at how long the data",
    "start": "198800",
    "end": "200640"
  },
  {
    "text": "transfer took",
    "start": "200640",
    "end": "202080"
  },
  {
    "text": "it took one minute and five seconds to",
    "start": "202080",
    "end": "204080"
  },
  {
    "text": "complete",
    "start": "204080",
    "end": "205440"
  },
  {
    "text": "take note of this value as we'll compare",
    "start": "205440",
    "end": "207760"
  },
  {
    "text": "this time",
    "start": "207760",
    "end": "208560"
  },
  {
    "text": "with how long aws data sync will take to",
    "start": "208560",
    "end": "211200"
  },
  {
    "text": "transfer the same",
    "start": "211200",
    "end": "212000"
  },
  {
    "text": "data to the same amazon s3 target bucket",
    "start": "212000",
    "end": "216560"
  },
  {
    "text": "let's go ahead and check out the amazon",
    "start": "216560",
    "end": "218799"
  },
  {
    "text": "s3 bucket",
    "start": "218799",
    "end": "219760"
  },
  {
    "text": "that we transfer the data to as we can",
    "start": "219760",
    "end": "223120"
  },
  {
    "text": "see",
    "start": "223120",
    "end": "223519"
  },
  {
    "text": "some data is being copied across now",
    "start": "223519",
    "end": "226400"
  },
  {
    "text": "let's go and check out how many files",
    "start": "226400",
    "end": "228000"
  },
  {
    "text": "have been copied across",
    "start": "228000",
    "end": "230959"
  },
  {
    "text": "as we can see 1001 objects have been",
    "start": "232000",
    "end": "234560"
  },
  {
    "text": "copied across",
    "start": "234560",
    "end": "240319"
  },
  {
    "text": "now let's navigate to the aws data sync",
    "start": "240319",
    "end": "242879"
  },
  {
    "text": "console",
    "start": "242879",
    "end": "245360"
  },
  {
    "text": "here i have pre-created a data transfer",
    "start": "245519",
    "end": "247680"
  },
  {
    "text": "task to migrate the same",
    "start": "247680",
    "end": "249120"
  },
  {
    "text": "10 000 source files from the nfs server",
    "start": "249120",
    "end": "252480"
  },
  {
    "text": "to the same target amazon s3 bucket but",
    "start": "252480",
    "end": "255280"
  },
  {
    "text": "into a different prefix folder",
    "start": "255280",
    "end": "257040"
  },
  {
    "text": "let's start the data transfer task",
    "start": "257040",
    "end": "260639"
  },
  {
    "text": "when you start the data transfer task",
    "start": "260639",
    "end": "262960"
  },
  {
    "text": "you can choose to override",
    "start": "262960",
    "end": "264479"
  },
  {
    "text": "any of the existing options if you need",
    "start": "264479",
    "end": "266400"
  },
  {
    "text": "to such as",
    "start": "266400",
    "end": "267520"
  },
  {
    "text": "those shown in our demo we won't so",
    "start": "267520",
    "end": "270639"
  },
  {
    "text": "let's go ahead and click on start",
    "start": "270639",
    "end": "274080"
  },
  {
    "text": "we can then go view the task execution",
    "start": "275360",
    "end": "278479"
  },
  {
    "text": "progress",
    "start": "278479",
    "end": "279280"
  },
  {
    "text": "this will give us information on how our",
    "start": "279280",
    "end": "281360"
  },
  {
    "text": "data transfer is progressing",
    "start": "281360",
    "end": "283280"
  },
  {
    "text": "and how much data has been transferred",
    "start": "283280",
    "end": "287040"
  },
  {
    "text": "let's fast forward to the end and we can",
    "start": "294320",
    "end": "296560"
  },
  {
    "text": "see the task has completed successfully",
    "start": "296560",
    "end": "299120"
  },
  {
    "text": "note that it took 22 seconds to run",
    "start": "299120",
    "end": "301680"
  },
  {
    "text": "compared to the one minute and five",
    "start": "301680",
    "end": "303199"
  },
  {
    "text": "seconds",
    "start": "303199",
    "end": "304000"
  },
  {
    "text": "of the copy script and the average",
    "start": "304000",
    "end": "306000"
  },
  {
    "text": "bandwidth achieved",
    "start": "306000",
    "end": "307120"
  },
  {
    "text": "was 29 megabytes per second which is a",
    "start": "307120",
    "end": "309680"
  },
  {
    "text": "lot higher than the five megabytes a",
    "start": "309680",
    "end": "311280"
  },
  {
    "text": "second that the copy script achieved",
    "start": "311280",
    "end": "313680"
  },
  {
    "text": "aws data sync not only transferred and",
    "start": "313680",
    "end": "316240"
  },
  {
    "text": "accelerated the transfer of the data",
    "start": "316240",
    "end": "318160"
  },
  {
    "text": "it copied across the file metadata to",
    "start": "318160",
    "end": "320080"
  },
  {
    "text": "the s3 object",
    "start": "320080",
    "end": "321520"
  },
  {
    "text": "let's go ahead and see what that looks",
    "start": "321520",
    "end": "322880"
  },
  {
    "text": "like",
    "start": "322880",
    "end": "325840"
  },
  {
    "text": "if we click on an object we can see",
    "start": "326800",
    "end": "328960"
  },
  {
    "text": "seven metadata elements that were copied",
    "start": "328960",
    "end": "331039"
  },
  {
    "text": "across",
    "start": "331039",
    "end": "332800"
  },
  {
    "text": "these include items such as timestamps",
    "start": "332800",
    "end": "335039"
  },
  {
    "text": "and permissions of the file",
    "start": "335039",
    "end": "336560"
  },
  {
    "text": "which look like this on the object",
    "start": "336560",
    "end": "340320"
  },
  {
    "text": "so what does that mean let's visualize",
    "start": "342000",
    "end": "344160"
  },
  {
    "text": "it with the help of file gateway",
    "start": "344160",
    "end": "346160"
  },
  {
    "text": "where you can present data stored in",
    "start": "346160",
    "end": "348560"
  },
  {
    "text": "amazon s3 bucket",
    "start": "348560",
    "end": "350240"
  },
  {
    "text": "back to your clients through a file",
    "start": "350240",
    "end": "351600"
  },
  {
    "text": "share in this case we'll present our",
    "start": "351600",
    "end": "354000"
  },
  {
    "text": "target bucket",
    "start": "354000",
    "end": "354880"
  },
  {
    "text": "back to our linux host as an nfs file",
    "start": "354880",
    "end": "357360"
  },
  {
    "text": "share",
    "start": "357360",
    "end": "358000"
  },
  {
    "text": "so we can view and compare the benefits",
    "start": "358000",
    "end": "359840"
  },
  {
    "text": "of being able to copy",
    "start": "359840",
    "end": "361360"
  },
  {
    "text": "the file metadata across to save time i",
    "start": "361360",
    "end": "364240"
  },
  {
    "text": "have already created this nfs share",
    "start": "364240",
    "end": "366400"
  },
  {
    "text": "using file gateway as shown where the",
    "start": "366400",
    "end": "368479"
  },
  {
    "text": "file gateway points to my target bucket",
    "start": "368479",
    "end": "370720"
  },
  {
    "text": "which hosts the 10 000 files that we",
    "start": "370720",
    "end": "373120"
  },
  {
    "text": "copied across",
    "start": "373120",
    "end": "374240"
  },
  {
    "text": "using both the copy script and aws data",
    "start": "374240",
    "end": "376800"
  },
  {
    "text": "sync method",
    "start": "376800",
    "end": "379360"
  },
  {
    "text": "let's go back to our linux host and",
    "start": "379520",
    "end": "381600"
  },
  {
    "text": "mount this file share",
    "start": "381600",
    "end": "383120"
  },
  {
    "text": "on our linux host as a new mount point",
    "start": "383120",
    "end": "385440"
  },
  {
    "text": "called nfs underscore target",
    "start": "385440",
    "end": "389280"
  },
  {
    "text": "now let's verify that has mounted",
    "start": "389680",
    "end": "392840"
  },
  {
    "text": "successfully",
    "start": "392840",
    "end": "395440"
  },
  {
    "text": "as you can see the nfs underscore target",
    "start": "395440",
    "end": "397840"
  },
  {
    "text": "mount is present",
    "start": "397840",
    "end": "399360"
  },
  {
    "text": "now let's look at some of the data",
    "start": "399360",
    "end": "401039"
  },
  {
    "text": "presented in this file share",
    "start": "401039",
    "end": "404400"
  },
  {
    "text": "we can see that is storing the data we",
    "start": "404400",
    "end": "406400"
  },
  {
    "text": "copied across firstly using the copy",
    "start": "406400",
    "end": "408319"
  },
  {
    "text": "script",
    "start": "408319",
    "end": "409039"
  },
  {
    "text": "into the s3 cli copy folder",
    "start": "409039",
    "end": "412080"
  },
  {
    "text": "and then using adabs data sync into the",
    "start": "412080",
    "end": "414759"
  },
  {
    "text": "datasync-copy folder",
    "start": "414759",
    "end": "418160"
  },
  {
    "text": "now let's look at some of the source",
    "start": "418160",
    "end": "419680"
  },
  {
    "text": "details of the original satin.jiff file",
    "start": "419680",
    "end": "422240"
  },
  {
    "text": "which is stored on the source nfs file",
    "start": "422240",
    "end": "424160"
  },
  {
    "text": "share as a point of reference",
    "start": "424160",
    "end": "426319"
  },
  {
    "text": "now let's look at what file metadata was",
    "start": "426319",
    "end": "428479"
  },
  {
    "text": "preserved by using the s3 copy script",
    "start": "428479",
    "end": "430800"
  },
  {
    "text": "to transfer the data to s3",
    "start": "430800",
    "end": "434478"
  },
  {
    "text": "as you can see it didn't preserve any of",
    "start": "435599",
    "end": "437680"
  },
  {
    "text": "the timestamps or permissions of the",
    "start": "437680",
    "end": "439520"
  },
  {
    "text": "original",
    "start": "439520",
    "end": "440479"
  },
  {
    "text": "satin.jiff file lastly",
    "start": "440479",
    "end": "443759"
  },
  {
    "text": "let's look at the file metadata that was",
    "start": "443759",
    "end": "445599"
  },
  {
    "text": "preserved using adobe's data sync to",
    "start": "445599",
    "end": "447919"
  },
  {
    "text": "transfer the same data",
    "start": "447919",
    "end": "449440"
  },
  {
    "text": "to the same target amazon s3 bucket",
    "start": "449440",
    "end": "452560"
  },
  {
    "text": "as you can see the time stamps and",
    "start": "452560",
    "end": "454479"
  },
  {
    "text": "permissions match that of the original",
    "start": "454479",
    "end": "456319"
  },
  {
    "text": "source satin gif file",
    "start": "456319",
    "end": "459680"
  },
  {
    "text": "apart from the acceleration of the",
    "start": "459680",
    "end": "461199"
  },
  {
    "text": "performance it provided encryption",
    "start": "461199",
    "end": "463520"
  },
  {
    "text": "data verification and preserved the file",
    "start": "463520",
    "end": "465520"
  },
  {
    "text": "metadata not to mention",
    "start": "465520",
    "end": "467360"
  },
  {
    "text": "that it also provided a simple dashboard",
    "start": "467360",
    "end": "469599"
  },
  {
    "text": "to monitor and report on the progress",
    "start": "469599",
    "end": "471919"
  },
  {
    "text": "now take a moment to think about these",
    "start": "471919",
    "end": "473759"
  },
  {
    "text": "benefits aws data sync can help you",
    "start": "473759",
    "end": "476319"
  },
  {
    "text": "simplify and accelerate your data",
    "start": "476319",
    "end": "478080"
  },
  {
    "text": "migration journey to aws",
    "start": "478080",
    "end": "479919"
  },
  {
    "text": "for a variety of use cases",
    "start": "479919",
    "end": "493840"
  }
]