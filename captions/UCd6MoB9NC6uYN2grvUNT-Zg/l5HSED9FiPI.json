[
  {
    "text": "well good afternoon everyone welcome to the very last session of reinvent very",
    "start": "5240",
    "end": "10920"
  },
  {
    "text": "happy to see so many of you stuck around until now uh trust that you've all had a good conference learned a lot and",
    "start": "10920",
    "end": "17320"
  },
  {
    "text": "hopefully be back next year so let me start by introducing myself uh I'm David Brown I am the uh general manager for",
    "start": "17320",
    "end": "24160"
  },
  {
    "text": "elastic load balancing uh been with Amazon for about seven years coming up on seven years had the privilege of",
    "start": "24160",
    "end": "30119"
  },
  {
    "text": "working on ec2 in the early days and then moved over to elastic load balancing for about the last year uh so",
    "start": "30119",
    "end": "35760"
  },
  {
    "text": "I've got some co-presenters today so Sean mackley he's product manager for Route 53 he'll be talking about the rout",
    "start": "35760",
    "end": "42360"
  },
  {
    "text": "53 section of today's talk and then Paul Kerney who is from one of our customers infospace uh he's been with infospace",
    "start": "42360",
    "end": "49000"
  },
  {
    "text": "since 2012 uh and actually led a migration of their data center to the cloud uh in just five months so in five",
    "start": "49000",
    "end": "56000"
  },
  {
    "text": "months they transferred all of their search functionality into AWS and used elb and Route 53 pretty much all the",
    "start": "56000",
    "end": "62280"
  },
  {
    "text": "features we have available for using multiple regions as well as multiple availability zones so we'll we'll get to",
    "start": "62280",
    "end": "68119"
  },
  {
    "text": "them shortly I'm going to start off with the elb section so welcome to the talk as you can see as was late on the",
    "start": "68119",
    "end": "75600"
  },
  {
    "text": "slide everything fails all the time uh you've probably heard Verna say this uh",
    "start": "75600",
    "end": "81040"
  },
  {
    "text": "it's probably not new you may have seen it a few times during the invent and the idea is not that everything is unstable",
    "start": "81040",
    "end": "86560"
  },
  {
    "text": "and going to fail all the time but that we should actually have at least an approach to availability that says let's",
    "start": "86560",
    "end": "91640"
  },
  {
    "text": "assume that everything is going to fail all the time a LEL paranoia is helpful in designing and architecting highly",
    "start": "91640",
    "end": "97360"
  },
  {
    "text": "available systems so if you start out with that sort of Mind view what you're going to do is make sure that you have",
    "start": "97360",
    "end": "102560"
  },
  {
    "text": "no single points of failure in your system a single point of failure is anything in the system where if it fails",
    "start": "102560",
    "end": "108840"
  },
  {
    "text": "their entire application is going to go down and with elastic load balancy in arounde 53 uh we provide several",
    "start": "108840",
    "end": "115200"
  },
  {
    "text": "components and features that allow you to remove single points of failures in your system whether it's making sure",
    "start": "115200",
    "end": "121159"
  },
  {
    "text": "that you don't have a dependency on a single instance making sure that you don't have a dependency on a single availability zone or making sure even",
    "start": "121159",
    "end": "127880"
  },
  {
    "text": "that you don't have a dependency potentially on a single region uh elastic load balancing and rounde 53 will help you so let me Begin by talking",
    "start": "127880",
    "end": "135640"
  },
  {
    "text": "about elastic load balancing so what is elastic load balancing well to State the obvious elastic load balancing is a load",
    "start": "135640",
    "end": "141920"
  },
  {
    "text": "balancer uh it balances traffic across your ec2 instances making sure that each instance receives the same amount of",
    "start": "141920",
    "end": "148519"
  },
  {
    "text": "traffic from your your application secondly it's elastic uh in the case of elastic load balancing that means that",
    "start": "148519",
    "end": "155040"
  },
  {
    "text": "we'll scale the load balance to meet the incoming traffic uh for your application so as traffic increases uh the load",
    "start": "155040",
    "end": "162080"
  },
  {
    "text": "balance will actually scale out in size and scale up in size and be able to handle that load it's secure obviously",
    "start": "162080",
    "end": "168000"
  },
  {
    "text": "support https SSL it's integrated what we mean by that is you know integrated",
    "start": "168000",
    "end": "173519"
  },
  {
    "text": "with other AWS services so some of them are cloudwatch ship metrics via cloudwatch water scaling",
    "start": "173519",
    "end": "180360"
  },
  {
    "text": "together with elastic load balancing you can automatically scale up and down the instances behind the load balancer Route",
    "start": "180360",
    "end": "186400"
  },
  {
    "text": "53 which we obviously going to be talking about today so integrate it into the environment and it's cost effective",
    "start": "186400",
    "end": "193480"
  },
  {
    "text": "it's significantly cheaper to use an elastic load balancing then run your own load balancing Solution on ec2 if you",
    "start": "193480",
    "end": "199879"
  },
  {
    "text": "had to scan up a few instances and put a load balancer on there you'd be paying more than using a load balancer to give you an idea it's currently about 2 and a",
    "start": "199879",
    "end": "206480"
  },
  {
    "text": "half cents per hour regardless of the size of your load balancer and it's less than a penny per gig that transfers",
    "start": "206480",
    "end": "212959"
  },
  {
    "text": "through your load balancer obviously you have the data processing fee that ec2 charges for outbound traffic as well um",
    "start": "212959",
    "end": "218599"
  },
  {
    "text": "but very very cheap to run so this is the standard s sort of implementation that you see when",
    "start": "218599",
    "end": "224879"
  },
  {
    "text": "customers use an elastic load balances so in this picture we got two availability zones Zone 1 a and 1 B and",
    "start": "224879",
    "end": "231599"
  },
  {
    "text": "we've got three instances in each Zone and as you can see in the on the leftmost side of the slide uh you can",
    "start": "231599",
    "end": "237480"
  },
  {
    "text": "see we got an elastic load balancer that's receiving requests from clients and those requests have been load",
    "start": "237480",
    "end": "243040"
  },
  {
    "text": "balanced over the backend instances and then we actually do support in VPC virtual private Cloud we do support",
    "start": "243040",
    "end": "249239"
  },
  {
    "text": "internal load balancing so you can actually have a load balancer that will only accept traffic inside your VPC and",
    "start": "249239",
    "end": "254560"
  },
  {
    "text": "that's what we've demonstrating with the second low balancer so the way we structured The Talk today is we're talking about three",
    "start": "254560",
    "end": "260720"
  },
  {
    "text": "levels of availability the first of those levels is instance availability really making sure that you you know",
    "start": "260720",
    "end": "267000"
  },
  {
    "text": "spread your traffic across multiple instances and the loss of a single instance is not going to cause problems for your application the second is zonal",
    "start": "267000",
    "end": "274479"
  },
  {
    "text": "availability so we RAR encourage customers to use multiple availability zones for all of the applications so",
    "start": "274479",
    "end": "280800"
  },
  {
    "text": "we're going to tell you how elastic load balancing helps you do that and finally Regional availability how do you go",
    "start": "280800",
    "end": "287080"
  },
  {
    "text": "about deploying your application to multiple regions and making sure that traffic is routed correctly uh to each",
    "start": "287080",
    "end": "292600"
  },
  {
    "text": "of the regions that you might be using so let's start with instance availability so it really is the first",
    "start": "292600",
    "end": "298960"
  },
  {
    "text": "step that everybody does when they go about making their application highly available I'm sure you've all seen this",
    "start": "298960",
    "end": "304520"
  },
  {
    "text": "diagram it's probably the first architecture you had for most of your applications one E2 instance you wrote",
    "start": "304520",
    "end": "309960"
  },
  {
    "text": "some code you stuck it on there and you hit it with a browser uh it's probably not the most available architecture that",
    "start": "309960",
    "end": "316720"
  },
  {
    "text": "you can have and I I wouldn't recommend anybody go with that so what you want to do is put a low balancer in front of",
    "start": "316720",
    "end": "322199"
  },
  {
    "text": "that now we low balance in over three machines we're still in one availability Zone but what's Happening Here is the",
    "start": "322199",
    "end": "328039"
  },
  {
    "text": "low balance is accepting the income in traffic and actually spreading that traffic across those three machines and",
    "start": "328039",
    "end": "333400"
  },
  {
    "text": "it does that equally as I said incoming request loads shared across the Stree",
    "start": "333400",
    "end": "339600"
  },
  {
    "text": "machines and one thing we we haven't spoken a lot about in the past but it's an important part of elastic lad",
    "start": "339600",
    "end": "345360"
  },
  {
    "text": "balancing you need to understand this we use the least cons algorithm and what least cons essentially means is that the",
    "start": "345360",
    "end": "351080"
  },
  {
    "text": "traffic is going to be distributed equally across each of those backend machines lease cons let's zoom in a",
    "start": "351080",
    "end": "357440"
  },
  {
    "text": "little lease cons will focus on instances that have the fewest number of outstanding connections so when we",
    "start": "357440",
    "end": "364600"
  },
  {
    "text": "sending requests to an instance the instance that is currently processing the fewest requests will get the next",
    "start": "364600",
    "end": "370840"
  },
  {
    "text": "request as soon as an instance completes the request it gets the next request so it does a very good job in smoothing the",
    "start": "370840",
    "end": "376639"
  },
  {
    "text": "request load and the other thing it does very well is reacting to any instance that may have higher latency or slower",
    "start": "376639",
    "end": "382280"
  },
  {
    "text": "response times because obviously if it's got a slower response time it's taking longer to process a request so as a",
    "start": "382280",
    "end": "387560"
  },
  {
    "text": "result given that least cons s requests to the machine that has the fewest number Avail number currently in process",
    "start": "387560",
    "end": "393800"
  },
  {
    "text": "that machine will just get fewer requests so even if your instances were different sizes which I wouldn't recommend you do the smaller instance",
    "start": "393800",
    "end": "400680"
  },
  {
    "text": "would still get fewer requests than the larger instances obviously instances fail and",
    "start": "400680",
    "end": "408199"
  },
  {
    "text": "with elastic load balancing they need to be seamless they can be seamlessly replaced and this might be application",
    "start": "408199",
    "end": "413319"
  },
  {
    "text": "failure or an underlying ec2 issue and the way that this works is so same diagram uh but now we're going to fail",
    "start": "413319",
    "end": "419639"
  },
  {
    "text": "of these ec2 instances so as you can see instance failure happens it might be ec2",
    "start": "419639",
    "end": "425160"
  },
  {
    "text": "fails or your application has an issue elastic load balancing actually has a heartbeat that actually sends requests",
    "start": "425160",
    "end": "431199"
  },
  {
    "text": "to your load balancer every couple of seconds and you can Implement either a TCP heartbeat or you can do an HTTP",
    "start": "431199",
    "end": "437280"
  },
  {
    "text": "heartbeat HTTP you actually write some code to return a 200 request back and says hey I'm healthy and we shift",
    "start": "437280",
    "end": "444560"
  },
  {
    "text": "traffic away and then you can see the remaining instances take that load so this way any failure of an instance will",
    "start": "444560",
    "end": "451120"
  },
  {
    "text": "automatically be shifted away and the remaining healthy instances handle the request load so a couple more details so",
    "start": "451120",
    "end": "458120"
  },
  {
    "text": "obviously health checks are used to determine the health of the instances behind the load balancer uh we do",
    "start": "458120",
    "end": "463360"
  },
  {
    "text": "support TCP and HTTP and you can also specify a port as well as a path uh for",
    "start": "463360",
    "end": "469039"
  },
  {
    "text": "HTTP you can customize the frequency and failure rate so you you may have decide you want to see one health check every",
    "start": "469039",
    "end": "474759"
  },
  {
    "text": "minute um we do allow down to 6 seconds with a certain number of failures uh being indicating that the machine should",
    "start": "474759",
    "end": "481080"
  },
  {
    "text": "be taken out of service uh it's difficult to give you advice on this but you do want to consider the depth and",
    "start": "481080",
    "end": "486879"
  },
  {
    "text": "accuracy of your health checks uh health checks that are very shallow in that they don't go deep into your stack or or",
    "start": "486879",
    "end": "493000"
  },
  {
    "text": "really assess the health maybe just turn around to 200 um that you may find times where you have situations where your",
    "start": "493000",
    "end": "499199"
  },
  {
    "text": "application might be impaired but the health check didn't really detect it and and sort of the inverse of that health checks that are too deep where you're",
    "start": "499199",
    "end": "505560"
  },
  {
    "text": "going all the way down to your database or maybe a whole lot of other dependencies you may find that dependency failures deep within the",
    "start": "505560",
    "end": "511879"
  },
  {
    "text": "stack end up taking machines out of service and you end up with a whole lot of false positives where you really",
    "start": "511879",
    "end": "516959"
  },
  {
    "text": "didn't want to take the server out of service but because you had a a database failure or maybe just a a small slowdown",
    "start": "516959",
    "end": "522120"
  },
  {
    "text": "on the database side you ended up taking all your machines out of service so it's really something that I'd encourage you to think about your initial",
    "start": "522120",
    "end": "527760"
  },
  {
    "text": "implementation and then watch your false positives and your false negatives very very closely and when you get them think",
    "start": "527760",
    "end": "533320"
  },
  {
    "text": "about your health check did we did we detect this correctly did we miss something here and then go and edit your health check over time until you find",
    "start": "533320",
    "end": "539160"
  },
  {
    "text": "the health check that's best suited for your application and then obviously if there no healthy instances behind the",
    "start": "539160",
    "end": "545000"
  },
  {
    "text": "load balancer so they all failing a health check or you don't have any machines behind your load balancer at all uh we will return a 50503 error um",
    "start": "545000",
    "end": "552600"
  },
  {
    "text": "back to the customer or back to the client sorry so we've spoken a lot about",
    "start": "552600",
    "end": "558240"
  },
  {
    "text": "availability uh with our leison algorithm and our health checks let's move on to scaling so by integrating",
    "start": "558240",
    "end": "564880"
  },
  {
    "text": "with auto scaling uh we we can basically adjust the instances behind the load Balan either scaling them up or scaling",
    "start": "564880",
    "end": "571079"
  },
  {
    "text": "them down dynamically depending on the metric that you choose so that could be the CPU metric uh on your backend",
    "start": "571079",
    "end": "578079"
  },
  {
    "text": "instances it could maybe be the surge quue on the load balancer it could be latency that the load balancer is showing you it could be a custom metric",
    "start": "578079",
    "end": "584720"
  },
  {
    "text": "that you currently reporting to cloudwatch so you can you use all those metrics and basically decide how you want to scale your Fleet so by way of",
    "start": "584720",
    "end": "592000"
  },
  {
    "text": "illustration we've got a load balancer here with a few ec2 instances uh traffic increases so this might be maybe you've",
    "start": "592000",
    "end": "598839"
  },
  {
    "text": "got a you know a video streaming application certain times people watch TV or certain times people shop if you're shopping application like Amazon",
    "start": "598839",
    "end": "605760"
  },
  {
    "text": "and you you know that load starts to increase so now we want to deal with this so Autos scaling will automatically launch a few more instances and you can",
    "start": "605760",
    "end": "613200"
  },
  {
    "text": "see the load equals out and the load balancer just simly accepts those new instances and traffic will now be spread",
    "start": "613200",
    "end": "618640"
  },
  {
    "text": "across five machines instead of three but obviously that load's going to decrease that Peak is not going to last",
    "start": "618640",
    "end": "624200"
  },
  {
    "text": "forever and when that Peak drops off uh you want to make sure that you don't waste money because now what we're doing",
    "start": "624200",
    "end": "629839"
  },
  {
    "text": "is running with a utilization that's too low so we're actually spending too much money here so aut scaling will once",
    "start": "629839",
    "end": "635320"
  },
  {
    "text": "again watch that metric and see that we've tripped the low threshold and it'll scale down and you'll you'll be back to where we originally",
    "start": "635320",
    "end": "641839"
  },
  {
    "text": "started let's just zoom in on auto scaling a little so as I said it uh automatically scales instances up or",
    "start": "641839",
    "end": "647880"
  },
  {
    "text": "down depending on whatever metric you choose custom scaling metrics I spoke a bit about that you can literally find a",
    "start": "647880",
    "end": "654639"
  },
  {
    "text": "metric that works best for you uh it reduces costs uh obvious what it does",
    "start": "654639",
    "end": "660160"
  },
  {
    "text": "there and the other thing it will do that I haven't mentioned is it'll automatically replace a failed instance so you do have the option to say if one",
    "start": "660160",
    "end": "666760"
  },
  {
    "text": "of my back in instances is failing a health check a scaling please terminate it and add a new one so it's going to",
    "start": "666760",
    "end": "671880"
  },
  {
    "text": "take care of all that management for you so we spoke about instance",
    "start": "671880",
    "end": "677519"
  },
  {
    "text": "availability we looked at lease cons we looked at health checks we looked at scaling let's move on to availability",
    "start": "677519",
    "end": "683360"
  },
  {
    "text": "zes so I'm sure at this part of the conference you don't need me to tell you what an availability zone is I'm hoping you all understand that",
    "start": "683360",
    "end": "689959"
  },
  {
    "text": "uh just in case it's a distinct location uh completely isolated from other zones",
    "start": "689959",
    "end": "696760"
  },
  {
    "text": "within within one of our regions and as you know we have nine regions around the world with 25 availability zones so",
    "start": "696760",
    "end": "702800"
  },
  {
    "text": "we're talking about the white dots on this image critically important uh whenever I",
    "start": "702800",
    "end": "709639"
  },
  {
    "text": "speak to a customer and I hear that they're in a single availability zone or we even get the sense from talking to them that they might be in one zone one",
    "start": "709639",
    "end": "715680"
  },
  {
    "text": "of the first things I tell them is when are you going to move to multiple zones it's it's really sort of you know the unit of uh availability inside AWS we",
    "start": "715680",
    "end": "724200"
  },
  {
    "text": "guarantee that no Zone will fail for the same reason at the same time and we work very very hard to make sure that we can",
    "start": "724200",
    "end": "729480"
  },
  {
    "text": "keep that guarantee uh so if you have any applications that are in a single zone and maybe your reasoning is it",
    "start": "729480",
    "end": "734839"
  },
  {
    "text": "might be cost maybe it's complexity of your design I'd really encourage you to work out how do you get to multiple",
    "start": "734839",
    "end": "740639"
  },
  {
    "text": "availability zones even if it increases your cost a little it's still going to mean that your application is going to be highly available in some of sort of",
    "start": "740639",
    "end": "747480"
  },
  {
    "text": "the worst scenarios uh one thing to consider is if you're in multiple zones you want to consider uh",
    "start": "747480",
    "end": "754720"
  },
  {
    "text": "how many dependencies do you have between your zones for example if you're in multiple zones but you rely on a master database it's in a single zone",
    "start": "754720",
    "end": "761279"
  },
  {
    "text": "and you don't have a good failover uh scenario uh it's really single point of failure on one zone um you know within",
    "start": "761279",
    "end": "767519"
  },
  {
    "text": "Amazon uh we need to provide guarantee on the isolation between zones we work very hard to make sure that we have no",
    "start": "767519",
    "end": "774000"
  },
  {
    "text": "dependencies between zones so service teams will Implement services that stay within a single zone and don't have",
    "start": "774000",
    "end": "779440"
  },
  {
    "text": "dependencies between these zones and it's critical for us guaranteeing that the zones remain isolated and highly",
    "start": "779440",
    "end": "786519"
  },
  {
    "text": "available so what I've done here is just taken my original image which showed the one load balancer with a single zone and",
    "start": "786519",
    "end": "792079"
  },
  {
    "text": "now we just have well six instances three in each zone three instances of zone is a is a good good place to be uh",
    "start": "792079",
    "end": "798760"
  },
  {
    "text": "you don't need to two is okay as well but three is pretty good in that you can lose one instance and you're still not",
    "start": "798760",
    "end": "804519"
  },
  {
    "text": "down to one you can kind of sleep at night if they're three if they're two and you lose one you probably need to get up and launch the last one because",
    "start": "804519",
    "end": "810880"
  },
  {
    "text": "if you lose the next one it might not be a great situation so talk a little bit about DNS",
    "start": "810880",
    "end": "818680"
  },
  {
    "text": "um each elastic load balancing uses DNS and each of your DNS records you'll have one DNS record for a load balancer and",
    "start": "818680",
    "end": "825360"
  },
  {
    "text": "that DNS record will contain multiple IP addresses explain a little bit further",
    "start": "825360",
    "end": "830920"
  },
  {
    "text": "here so you can see the load balancers got an IP address on the one side do one IP address on the other side do2 and",
    "start": "830920",
    "end": "837320"
  },
  {
    "text": "what your DNS record would serve both these IPS the one will come out you know",
    "start": "837320",
    "end": "842680"
  },
  {
    "text": "return 50% of the time and the other one will be return 50% of the time so clients resolving DNS are going to send",
    "start": "842680",
    "end": "848000"
  },
  {
    "text": "their traffic to each one of these zones 50% of the time and that's the way we basically spread traffic across multiple",
    "start": "848000",
    "end": "854639"
  },
  {
    "text": "availability zones we use DNS round robin as I just",
    "start": "854639",
    "end": "859959"
  },
  {
    "text": "said that's called DNS round robin weighted round robin we basically you know if there three zones then each one will receive 33% of the traffic and we",
    "start": "859959",
    "end": "867079"
  },
  {
    "text": "rely on clients resolving those IPS and making sure that they don't stick to one",
    "start": "867079",
    "end": "872720"
  },
  {
    "text": "IP expect DNS records to change elastic load balancing will change DNS records",
    "start": "872720",
    "end": "878959"
  },
  {
    "text": "when the load B needs to scale up uh and there's some other situations where you might have an IP changing what I mean by",
    "start": "878959",
    "end": "884480"
  },
  {
    "text": "that is not the domain name but the actual contents of the records the a records um so so be very careful or",
    "start": "884480",
    "end": "890040"
  },
  {
    "text": "actually don't take your DNS records from elb and go and put them into a static configuration whether it's a",
    "start": "890040",
    "end": "896240"
  },
  {
    "text": "maybe a proxy or a firewall or something out there know that you need to rely on the DNS",
    "start": "896240",
    "end": "903240"
  },
  {
    "text": "record obviously I saw a few faces looking at me there when I spoke about clients resolving IPS U using multiple",
    "start": "904880",
    "end": "912279"
  },
  {
    "text": "availability zones does bring a few challenges one thing we do see is that uh clients will resolve an IP and then",
    "start": "912279",
    "end": "920040"
  },
  {
    "text": "stick to that zone a lot of java applications by default will just the first time they resolve an IP they just say okay I got the IP let's just use",
    "start": "920040",
    "end": "926440"
  },
  {
    "text": "that there is a flag in Java you can set when you start the M that says re resolve DNS don't just keep to the IP",
    "start": "926440",
    "end": "932160"
  },
  {
    "text": "but a lot of cell phones will do this if you're using a CN a CDN sorry uh they often do that as well uh if you have a",
    "start": "932160",
    "end": "938680"
  },
  {
    "text": "very small number of clients you might experience what we call Zone imbalance and you can see in this diagram one of",
    "start": "938680",
    "end": "943959"
  },
  {
    "text": "these zones you know each one of the zones is receiving different amounts of traffic at different points in time because the way clients are resolving",
    "start": "943959",
    "end": "950199"
  },
  {
    "text": "the IPS the other problem is you may not always have the same number of instances",
    "start": "950199",
    "end": "955240"
  },
  {
    "text": "in each of your availability zones so in this image you can see my one zone has two instances and the other one has",
    "start": "955240",
    "end": "960360"
  },
  {
    "text": "three and this while you know we recommend maintaining balanced zones there are periods of time you may have",
    "start": "960360",
    "end": "965720"
  },
  {
    "text": "just had an instance fail in one of your zones and now what we have is one of the zones is taking 50% of the traffic with",
    "start": "965720",
    "end": "971759"
  },
  {
    "text": "two instances and the other one's taking 50% of the traffic with three instances and that's not a great place but that",
    "start": "971759",
    "end": "978279"
  },
  {
    "text": "problem's been solved last week you may have seen we launched cross Zone load balancing that",
    "start": "978279",
    "end": "983399"
  },
  {
    "text": "allows you to dist distribute your traffic across all of the instances in each of your availability zones",
    "start": "983399",
    "end": "989079"
  },
  {
    "text": "regardless of where the traffic came into uh which zone the client sent the traffic to so it solves both of these",
    "start": "989079",
    "end": "994920"
  },
  {
    "text": "problems now the Zone with two instances is receiving one fth of the traffic on",
    "start": "994920",
    "end": "1000680"
  },
  {
    "text": "each of those instances and the Zone with three is also seeing one F of the traffic on each of those instances so the fact that you lost an instance in",
    "start": "1000680",
    "end": "1007560"
  },
  {
    "text": "zone two really what happens is zone two just starts to receive a smaller amount of traffic as much traffic as it can",
    "start": "1007560",
    "end": "1013600"
  },
  {
    "text": "handle the other thing it does your requests per Zone they suddenly become a very online and this was great to see we",
    "start": "1013600",
    "end": "1020920"
  },
  {
    "text": "had a couple of customers that actually tweeted one of our customers managed to tweet about 15 minutes after the announcement it was pretty impressive",
    "start": "1020920",
    "end": "1027038"
  },
  {
    "text": "you turned it on and immediately seen zones that W weren't you know were seen",
    "start": "1027039",
    "end": "1032079"
  },
  {
    "text": "imbalance immediately converge pretty much into a single line so this makes implementing uh you know multi-az",
    "start": "1032079",
    "end": "1038760"
  },
  {
    "text": "application significantly easier I've mentioned most of the things on this Slide the one thing I haven't mentioned is we we will not charge you for",
    "start": "1038760",
    "end": "1045000"
  },
  {
    "text": "bandwidth between zones as you know ec2 currently charges I think it's still a penny per gig uh for traffic between",
    "start": "1045000",
    "end": "1051400"
  },
  {
    "text": "availability zones if you use an elastic load balancer for this you will not be charged for any inter Zone bandwidth",
    "start": "1051400",
    "end": "1057280"
  },
  {
    "text": "either to or from the load balancer so if you're talking to an internal load balancer and you end up going to a",
    "start": "1057280",
    "end": "1062799"
  },
  {
    "text": "different Zone you won't be charged for that and if your low balancer talks to a backend instance in a different Zone you won't be charged for that either so a",
    "start": "1062799",
    "end": "1069559"
  },
  {
    "text": "significant feature for elb and something that we've seen really good results from and hearing good feedback from",
    "start": "1069559",
    "end": "1075039"
  },
  {
    "text": "customers so that's availability Zone redundancy you say elb makes it it easy to use multiple availability zones and",
    "start": "1075039",
    "end": "1081080"
  },
  {
    "text": "how important it is that we use multiple availability zones in our architectures let's move on to Regional",
    "start": "1081080",
    "end": "1087880"
  },
  {
    "text": "availability so elastic load balancing in around 53 have integrated and support a single application across multiple",
    "start": "1087880",
    "end": "1095320"
  },
  {
    "text": "regions I've put this slide up before and now you can see of the eight regions plus the go Cloud region that we have",
    "start": "1095320",
    "end": "1101919"
  },
  {
    "text": "available um around the world but it would be great if we made it really easy for you to host your application across",
    "start": "1101919",
    "end": "1107720"
  },
  {
    "text": "multiple regions and make sure that you know requests from your customers can be writed intelligently to your",
    "start": "1107720",
    "end": "1112799"
  },
  {
    "text": "applications in these regions so let me hand over to Sean who's going to take us through more of the Route 53",
    "start": "1112799",
    "end": "1119440"
  },
  {
    "text": "details thanks Dave so Amazon Route 53 what is it Well Route 53 is aws's",
    "start": "1119880",
    "end": "1127039"
  },
  {
    "text": "authoritative DNS service we also do something else that's very significant though which is we are our own health",
    "start": "1127039",
    "end": "1132760"
  },
  {
    "text": "checking service I'll talk more in a couple slides about how those two pieces work together for availability for our",
    "start": "1132760",
    "end": "1137919"
  },
  {
    "text": "customers Route 53 is a service itself is built to be highly available and scalable with",
    "start": "1137919",
    "end": "1143240"
  },
  {
    "text": "multiple layers of redundancy both in our data and and control plane we serve DNS queries out of 46 locations around",
    "start": "1143240",
    "end": "1150159"
  },
  {
    "text": "the world for example and Route 53 also offers a variety of routing tools and",
    "start": "1150159",
    "end": "1155760"
  },
  {
    "text": "health checking tools that allow customers to build their own flexible high performance and highly available",
    "start": "1155760",
    "end": "1161159"
  },
  {
    "text": "architectures on AWS Route 53 and elb work closely",
    "start": "1161159",
    "end": "1167799"
  },
  {
    "text": "together route 53 provides the DNS service for elb for one thing Route 53",
    "start": "1167799",
    "end": "1173760"
  },
  {
    "text": "also has the concept of health checks of elbs themselves so for every elastic load balancer that is provisioned Route",
    "start": "1173760",
    "end": "1180240"
  },
  {
    "text": "53 automatically creates a health check of the load balancer itself additionally",
    "start": "1180240",
    "end": "1186240"
  },
  {
    "text": "that health check then takes into account the health of all of the ec2 instances behind that low balancer to get a view of the entire stack behind",
    "start": "1186240",
    "end": "1193400"
  },
  {
    "text": "that low balancer in a single region that allows you to treat that region as a single unit of failure or something of",
    "start": "1193400",
    "end": "1200640"
  },
  {
    "text": "uh for availability that you can fail away from this allows you to build multi- region architectures and backup",
    "start": "1200640",
    "end": "1206280"
  },
  {
    "text": "architectures for higher availability we call this DNS failover",
    "start": "1206280",
    "end": "1212760"
  },
  {
    "text": "DNS failover consists of two main components first is the health checking piece I talked a bit about how we health",
    "start": "1212760",
    "end": "1217880"
  },
  {
    "text": "check every elb instance that is launched we also allow you to create your own health checks in Route 53 of",
    "start": "1217880",
    "end": "1224440"
  },
  {
    "text": "other types of endpoints so for example you can health check directly an e two instance you can also health check your",
    "start": "1224440",
    "end": "1230960"
  },
  {
    "text": "own application running in your own data center behind an IP address for example we then use the results of those",
    "start": "1230960",
    "end": "1237799"
  },
  {
    "text": "health checks is this endpoint up or is it down to drive DNS routing Behavior specifically we only vend the",
    "start": "1237799",
    "end": "1245320"
  },
  {
    "text": "DNS answers corresponding to an endpoint that we perceive as healthy this allows you to build failover into your",
    "start": "1245320",
    "end": "1253360"
  },
  {
    "text": "application let's talk a bit about how Route 53 is built our DNS uh DNS",
    "start": "1254000",
    "end": "1259039"
  },
  {
    "text": "failover feature first is the principle of constant work now to explain what that is I'll first talk about what it",
    "start": "1259039",
    "end": "1265720"
  },
  {
    "text": "isn't or what the alternative is which would be a work on failure model that would mean something like if",
    "start": "1265720",
    "end": "1271840"
  },
  {
    "text": "my application is overloaded or if there's an outage I'm going to make a bunch of API calls I'm going to",
    "start": "1271840",
    "end": "1277880"
  },
  {
    "text": "configure new DNS records maybe provision new load balancers launch new ec2 instances all of those things are",
    "start": "1277880",
    "end": "1284720"
  },
  {
    "text": "API calls that actually have to succeed in order for them to have any effect on the uh the issue that's taking place as",
    "start": "1284720",
    "end": "1291799"
  },
  {
    "text": "failure may be more widespread in your application those calls can themselves Bunch up in a queue or even fail to",
    "start": "1291799",
    "end": "1297679"
  },
  {
    "text": "complete compounding the issue and exponentially increasing the time it takes to resolve the issue Route 53",
    "start": "1297679",
    "end": "1304000"
  },
  {
    "text": "doesn't do that Route 53 maintains a constant level of work both in our health checking so we have the same",
    "start": "1304000",
    "end": "1309880"
  },
  {
    "text": "volume and frequency of health checks whether your application is healthy or uh down and same DNS activity so we we",
    "start": "1309880",
    "end": "1317679"
  },
  {
    "text": "continue to of DS queries the same way we don't actually even touch our own control plane when failover happens",
    "start": "1317679",
    "end": "1323480"
  },
  {
    "text": "there's no API calls involved and that holds true whether the failure is localized or even",
    "start": "1323480",
    "end": "1331120"
  },
  {
    "text": "widespread next throughout 53 checks every endpoint whether that's an elastic load balancer or ec2 instance from a",
    "start": "1331919",
    "end": "1339400"
  },
  {
    "text": "Global Network we check from within every AWS region every Republic AWS region so eight regions today why do we",
    "start": "1339400",
    "end": "1346120"
  },
  {
    "text": "do this well for your end customers health of an endpoint doesn't just mean the application is up and running in",
    "start": "1346120",
    "end": "1352039"
  },
  {
    "text": "that endpoint it also means as an end user can I reach that over the internet now internet connectivity around the",
    "start": "1352039",
    "end": "1358159"
  },
  {
    "text": "world isn't always uh given and so we actually develop a composite view from each location that we health check of",
    "start": "1358159",
    "end": "1364880"
  },
  {
    "text": "whether that application is reachable from that location this becomes particularly",
    "start": "1364880",
    "end": "1370919"
  },
  {
    "text": "important in cases of network partition so hypothetical example would be that you're running an application in South",
    "start": "1370919",
    "end": "1377039"
  },
  {
    "text": "America AWS region there as well as one or more regions outside of South America say Us",
    "start": "1377039",
    "end": "1382799"
  },
  {
    "text": "East now imagine the case which can happen where large region say South",
    "start": "1382799",
    "end": "1387840"
  },
  {
    "text": "America experiences poor internet connectivity to the rest of the world for a period of time this is a network",
    "start": "1387840",
    "end": "1393440"
  },
  {
    "text": "partition scenario and in this case Route 53 is designed to do the right thing for your end users both inside the",
    "start": "1393440",
    "end": "1399400"
  },
  {
    "text": "partition and out so for example inside of South America rout 53's Health",
    "start": "1399400",
    "end": "1404720"
  },
  {
    "text": "checking Fleet located in South America will detect your endpoint and that location as healthy they can see it from",
    "start": "1404720",
    "end": "1411360"
  },
  {
    "text": "everywhere else in the world however they will see that endpoint is unavailable that Health Data will then",
    "start": "1411360",
    "end": "1417039"
  },
  {
    "text": "drive how Route 53 in all of our Edge locations those inside the partition and out will respond to queries for your",
    "start": "1417039",
    "end": "1423480"
  },
  {
    "text": "your endpoints those within South America will be routed to the South America endpoint those outside will be",
    "start": "1423480",
    "end": "1429159"
  },
  {
    "text": "routed to your other healthy endpoint which is in North",
    "start": "1429159",
    "end": "1433400"
  },
  {
    "text": "America Next DNS failover is designed to be entirely automatic meaning that once",
    "start": "1435159",
    "end": "1440279"
  },
  {
    "text": "you've configured it up front you've set up your DNS records and any health checks then the actual failover is",
    "start": "1440279",
    "end": "1446520"
  },
  {
    "text": "completely automatic no intervention required fire and forget basically that failover takes approximately 150 seconds",
    "start": "1446520",
    "end": "1454000"
  },
  {
    "text": "now that number comes from two components first is we health check at an interval of 30 seconds between health",
    "start": "1454000",
    "end": "1460240"
  },
  {
    "text": "check observations then we require three consecutive observations to return",
    "start": "1460240",
    "end": "1466720"
  },
  {
    "text": "either failure or health to for Route 53 to determine yes that endpoint is genu genuinely up or genuinely down so three",
    "start": "1466720",
    "end": "1474200"
  },
  {
    "text": "consecutive observations uh will take roughly 60 to 90 seconds next there's one minute or 60 seconds of DNS TTL that",
    "start": "1474200",
    "end": "1481799"
  },
  {
    "text": "is how long that DNS record is cached in the DNS resolvers around the world we recommend a maximum of 60 seconds uh for",
    "start": "1481799",
    "end": "1489520"
  },
  {
    "text": "ctls on your DNS records when you're using DNS failover so add those up you get a total of 150 seconds from the time",
    "start": "1489520",
    "end": "1495960"
  },
  {
    "text": "that the endpoint actually fails to the time which traffic is now flowing to your alternate or backup location now",
    "start": "1495960",
    "end": "1502840"
  },
  {
    "text": "contrast that with a more traditional manual failover method where for example you're using an external or you third",
    "start": "1502840",
    "end": "1509120"
  },
  {
    "text": "party monitoring service if it detects failure it triggers some sort of alarm could be an SMS a page email that goes",
    "start": "1509120",
    "end": "1516120"
  },
  {
    "text": "to a live person on your team an operator could be during the day could also obviously be during the middle of",
    "start": "1516120",
    "end": "1521480"
  },
  {
    "text": "the night that person then has to log into some console manually make some DNS updates maybe launch new instances and",
    "start": "1521480",
    "end": "1528799"
  },
  {
    "text": "then because those are API calls those obviously have to succeed before they have any effect then you have to wait",
    "start": "1528799",
    "end": "1534159"
  },
  {
    "text": "for those changes to propagate now you can imagine that in the vast majority of cases that's going to take a lot longer than two and a half minutes or 150",
    "start": "1534159",
    "end": "1542880"
  },
  {
    "text": "seconds so again route 53's DNS failover doesn't touch the control plane at all there's no API calls involved every one",
    "start": "1544600",
    "end": "1551919"
  },
  {
    "text": "of our 46 Edge locations is independently pulling the health results from our Global Fleet of Health checkers",
    "start": "1551919",
    "end": "1559399"
  },
  {
    "text": "next you don't have to wait for the API API requests to succeed and um lastly again it's entirely within the data",
    "start": "1559399",
    "end": "1565360"
  },
  {
    "text": "plane which um rout 53 data plane we do actually um maintain 100% SLA on our",
    "start": "1565360",
    "end": "1570760"
  },
  {
    "text": "data plane okay so let's walk through a simple customer example of how they can",
    "start": "1570760",
    "end": "1578679"
  },
  {
    "text": "easily get started using DNS failover let's take a hypothetical eCommerce site we'll call them example.com they're",
    "start": "1578679",
    "end": "1584240"
  },
  {
    "text": "running an application stack behind an elastic load balancer as Dave described in multiple availability zones in one",
    "start": "1584240",
    "end": "1591480"
  },
  {
    "text": "region let's say the customer wants to have a backup in case either their own application goes down across multiple",
    "start": "1591480",
    "end": "1597039"
  },
  {
    "text": "availability zones we've had customers tell us they're concerned about you their own bad software deployment for",
    "start": "1597039",
    "end": "1602159"
  },
  {
    "text": "example affecting their app in multiple azs or again uh you know some parts of the world having poor connectivity to",
    "start": "1602159",
    "end": "1607960"
  },
  {
    "text": "that region very simple solution is to",
    "start": "1607960",
    "end": "1613159"
  },
  {
    "text": "develop a set a static backup site something like you could imagine a gone fishing page fail well type of P page",
    "start": "1613159",
    "end": "1618880"
  },
  {
    "text": "maintains the company's brand presence online maybe have an alternate contact method email phone number something to",
    "start": "1618880",
    "end": "1625360"
  },
  {
    "text": "keep the company's brand uh online even if the dynamic application goes down how this looks in Route 53 is you can figure",
    "start": "1625360",
    "end": "1632520"
  },
  {
    "text": "a your primary DNS record to point to your Dynamic application behind the elb Route 53 will automatically health check",
    "start": "1632520",
    "end": "1639679"
  },
  {
    "text": "that elb for you you also configure a secondary DNS record pointing to the static website which you can host uh",
    "start": "1639679",
    "end": "1646640"
  },
  {
    "text": "very cost effectively and reliably on Amazon S3 if the health check from Route 53 to",
    "start": "1646640",
    "end": "1652000"
  },
  {
    "text": "your elb indicates that your application is up no traffic will flow to your backup",
    "start": "1652000",
    "end": "1657520"
  },
  {
    "text": "website however if the Route 53 health check begins to fail indicating that your application is down in that region",
    "start": "1657520",
    "end": "1663600"
  },
  {
    "text": "traffic will automatically be rerouted to that static backup",
    "start": "1663600",
    "end": "1668360"
  },
  {
    "text": "website now a static site doesn't actually have to mean the entire website if you can imagine a site like",
    "start": "1668919",
    "end": "1674039"
  },
  {
    "text": "amazon.com that actually consists of many different components some of which are dynamically ated for that user for",
    "start": "1674039",
    "end": "1679679"
  },
  {
    "text": "example personalized recommendations other components are static now for each of these Dynamic components as possible",
    "start": "1679679",
    "end": "1687120"
  },
  {
    "text": "to create a static backup so for example you could imagine replacing the dynamic uh list of recommended products for you",
    "start": "1687120",
    "end": "1693919"
  },
  {
    "text": "know users that have browsed with things that you've browsed uh with a static top",
    "start": "1693919",
    "end": "1699519"
  },
  {
    "text": "10 list of you know top 10 products this week right each of those components can then be treated as a unit of",
    "start": "1699519",
    "end": "1706600"
  },
  {
    "text": "failure okay now let's talk about running in multiple regions Route 53 has made this possible",
    "start": "1706760",
    "end": "1713000"
  },
  {
    "text": "for a while with our feature called latency based routing this allows you to actually get physically closer to your",
    "start": "1713000",
    "end": "1718440"
  },
  {
    "text": "end users distributed around the world the way you do that is run your application in multiple AWS regions",
    "start": "1718440",
    "end": "1723720"
  },
  {
    "text": "around the world latency based routing will then route your end users to the endpoint to the AWS region that provides",
    "start": "1723720",
    "end": "1730519"
  },
  {
    "text": "the lowest latency to that end user location genu or generally that corresponds to also being geographically",
    "start": "1730519",
    "end": "1736360"
  },
  {
    "text": "closest but not always and so we're out 3 actually does measure the internet latency essentially from every point in",
    "start": "1736360",
    "end": "1741399"
  },
  {
    "text": "the world to each of our um data centers or AWS regions and so we will route your end users to the fastest location um in",
    "start": "1741399",
    "end": "1749240"
  },
  {
    "text": "terms of Internet latency we have a variety of customers",
    "start": "1749240",
    "end": "1755720"
  },
  {
    "text": "using latency based routing one example is in the advertising space of fine systems um and online advertising for",
    "start": "1755720",
    "end": "1762399"
  },
  {
    "text": "bidding and and serving ads it's important to shave milliseconds off of of each transaction to get your bid in",
    "start": "1762399",
    "end": "1768519"
  },
  {
    "text": "faster for the ad and so a fine systems is one of many customers of ours that are using latency based rating to run uh",
    "start": "1768519",
    "end": "1774760"
  },
  {
    "text": "physically closer to those ad exchanges physically closer to those end users Route 53 is latency based routing also",
    "start": "1774760",
    "end": "1781320"
  },
  {
    "text": "has the advantage it's very easy to to implement using our API or console and uh much lower prices than other",
    "start": "1781320",
    "end": "1787679"
  },
  {
    "text": "traditional uh goo or directional DNS solutions from from many other",
    "start": "1787679",
    "end": "1793120"
  },
  {
    "text": "providers okay this also can give you greater reliability by pairing latency based routing with DNS failover so in",
    "start": "1793120",
    "end": "1801039"
  },
  {
    "text": "that case Route 53 will only route your end users to a region where your application is healthy and is reachable",
    "start": "1801039",
    "end": "1807159"
  },
  {
    "text": "over the Internet so back to the example.com let's say they want faster page load for customers distributed",
    "start": "1807159",
    "end": "1814000"
  },
  {
    "text": "around the world so they decide to launch their application in multiple AWS regions they use latency based routing",
    "start": "1814000",
    "end": "1820440"
  },
  {
    "text": "to Route their end users to the closest region in terms of latency and then by turning on DNS failover they ensure that",
    "start": "1820440",
    "end": "1826440"
  },
  {
    "text": "these end users are only routed to a region where their application is",
    "start": "1826440",
    "end": "1831200"
  },
  {
    "text": "healthy how this looks in practice is they create multiple primary records in Route 53 again uh because these are",
    "start": "1831519",
    "end": "1838799"
  },
  {
    "text": "elastic load balancers Route 53 automatically Provisions a health check for you and takes that Health Data into",
    "start": "1838799",
    "end": "1844120"
  },
  {
    "text": "account when you um select you want to use DNS failover in the event that one of these",
    "start": "1844120",
    "end": "1850559"
  },
  {
    "text": "health checks indicates that the application stack is down in a region Route 53 will automatically shift the traffic that would have gone to that",
    "start": "1850559",
    "end": "1856480"
  },
  {
    "text": "region to the next close closest region again in terms of Internet latency for each end",
    "start": "1856480",
    "end": "1862880"
  },
  {
    "text": "user these two fail over approaches are not mutually exclusive quite the contrary you can combine uh latency",
    "start": "1864200",
    "end": "1871279"
  },
  {
    "text": "based routing with a static website uh backup and in that case the static website backup would only come into play",
    "start": "1871279",
    "end": "1878080"
  },
  {
    "text": "if your application were down or unreachable across all",
    "start": "1878080",
    "end": "1882240"
  },
  {
    "text": "regions DNS failover is quite easy to configure via both the the API or the",
    "start": "1883519",
    "end": "1889080"
  },
  {
    "text": "console for elastic load balancing it's actually literally one Mouse click to",
    "start": "1889080",
    "end": "1894279"
  },
  {
    "text": "start evaluating the health of the low balancer and then it's just a matter of creating the DNS records pointing to the",
    "start": "1894279",
    "end": "1899840"
  },
  {
    "text": "alternate locations that you would want traffic routed to if this application were down behind that",
    "start": "1899840",
    "end": "1905960"
  },
  {
    "text": "elb now I'll hand it over to Paul to talk about how infospace is using elb in rout 53 thanks",
    "start": "1906600",
    "end": "1914000"
  },
  {
    "text": "Sean so share a little bit about our story at infospace and how Route 53 and",
    "start": "1914000",
    "end": "1920159"
  },
  {
    "text": "elastic load balancing helps us to deliver a highly available solution to our",
    "start": "1920159",
    "end": "1927120"
  },
  {
    "text": "customers infospace has been around for a while since like the 1900s and uh what we do our primary",
    "start": "1927120",
    "end": "1935480"
  },
  {
    "text": "product is a search product and the way that we uh we add value we have we",
    "start": "1935480",
    "end": "1941200"
  },
  {
    "text": "leverage our uh relationships with major search engines Google Yahoo Bing and some others and for given search query",
    "start": "1941200",
    "end": "1949440"
  },
  {
    "text": "we'll submit that query to those search engines gather all the results from all",
    "start": "1949440",
    "end": "1954480"
  },
  {
    "text": "of them we'll then combine them dup them and order them in a way that we think",
    "start": "1954480",
    "end": "1959840"
  },
  {
    "text": "provides value to the to the consumer inject in some of our own custom content and then return those results back to",
    "start": "1959840",
    "end": "1966840"
  },
  {
    "text": "the user our application stack is a",
    "start": "1966840",
    "end": "1972799"
  },
  {
    "text": "Microsoft Windows and asp.net application and that search project that",
    "start": "1972799",
    "end": "1978639"
  },
  {
    "text": "I described we deliver in two different Vehicles one is through a set of uh",
    "start": "1978639",
    "end": "1985440"
  },
  {
    "text": "search sites such as dogpile.com uh web fetcher WebCrawler metacrawler these are",
    "start": "1985440",
    "end": "1992039"
  },
  {
    "text": "sites that we own and operate we have end users who come to us in their browser and the other way that we do",
    "start": "1992039",
    "end": "1997760"
  },
  {
    "text": "this is through our search API so we have distribution Partners around the world who want to integrate search",
    "start": "1997760",
    "end": "2003080"
  },
  {
    "text": "results into their website with a way to monetize them we have an API that they can then consume to get those",
    "start": "2003080",
    "end": "2011879"
  },
  {
    "text": "results so between the search sites and the search API there's three different",
    "start": "2012480",
    "end": "2017880"
  },
  {
    "text": "types of users that we see that come to us the first are the search site users",
    "start": "2017880",
    "end": "2023360"
  },
  {
    "text": "so these are uh you know people sitting at home they open up their browser they type in dogpile.com they're distributed",
    "start": "2023360",
    "end": "2030200"
  },
  {
    "text": "all over the world um and and we see about 400 million queries per month that come from",
    "start": "2030200",
    "end": "2037120"
  },
  {
    "text": "just these uh search site users the second set of users are the search API partners and so these are",
    "start": "2037120",
    "end": "2044039"
  },
  {
    "text": "people who are typically operating either out of the cloud or out of a data center they tend to be more geographically um combined um mainly",
    "start": "2044039",
    "end": "2051919"
  },
  {
    "text": "clustered in the US and the EU we see about two billion queries per month that come from uh our search API",
    "start": "2051919",
    "end": "2059878"
  },
  {
    "text": "partners and then the third type of user are what we call click users now these",
    "start": "2059879",
    "end": "2065158"
  },
  {
    "text": "are people who are actually clicking on a search result um the reason we call these out a little bit differently is if",
    "start": "2065159",
    "end": "2070878"
  },
  {
    "text": "you're a search site user if you came to dog pile a click user is the same as a search site user but if you're a API",
    "start": "2070879",
    "end": "2077079"
  },
  {
    "text": "partner the user when they initially submit that search request the partner is who touches us so",
    "start": "2077079",
    "end": "2085320"
  },
  {
    "text": "that's who we're resolving our DNS record against but as soon as the user clicks on a result that goes back to us",
    "start": "2085320",
    "end": "2091800"
  },
  {
    "text": "for logging and then we redirect them onto their destination so that second request actually comes from a different",
    "start": "2091800",
    "end": "2098000"
  },
  {
    "text": "location than the first request in the case of a search API partner so the way this looks in our AWS",
    "start": "2098000",
    "end": "2104960"
  },
  {
    "text": "implementation is that we are uh currently deployed in uh four AWS",
    "start": "2104960",
    "end": "2111200"
  },
  {
    "text": "regions three in the United States and and one in Europe and we have multiple availability",
    "start": "2111200",
    "end": "2117880"
  },
  {
    "text": "zones that we run in each of these uh regions so if you're a search API",
    "start": "2117880",
    "end": "2123320"
  },
  {
    "text": "partner of ours you're located around the world when you connect to us through the C name that we give you that's uh",
    "start": "2123320",
    "end": "2130240"
  },
  {
    "text": "created and hosted through Route 53 you're then routed automatically to the least latent region from your",
    "start": "2130240",
    "end": "2139320"
  },
  {
    "text": "location likewise if you come to one of our search sites you will also get routed to the least latent region from",
    "start": "2139320",
    "end": "2147119"
  },
  {
    "text": "your location and then finally if you're a click user depending on where you",
    "start": "2147119",
    "end": "2152160"
  },
  {
    "text": "might be you'll also when you click on one of those result links get routed to the region that is the Le least latent",
    "start": "2152160",
    "end": "2159319"
  },
  {
    "text": "and the great thing about all this that we've already learned and been able to Leverage is that when",
    "start": "2159319",
    "end": "2165440"
  },
  {
    "text": "we find a new cluster of users maybe a large growth of users in Brazil we can",
    "start": "2165440",
    "end": "2171520"
  },
  {
    "text": "quickly bring on a new Region online create a new latency based rout routing",
    "start": "2171520",
    "end": "2177319"
  },
  {
    "text": "record and those users will then start to get to use that new region",
    "start": "2177319",
    "end": "2183040"
  },
  {
    "text": "automatically so when David introduced The Talk he said that we had completed a migration to AWS earlier this year um we",
    "start": "2185680",
    "end": "2193920"
  },
  {
    "text": "owned an OP or we we had two Colo locations in the United States in starting in January through June we",
    "start": "2193920",
    "end": "2200960"
  },
  {
    "text": "migrated out of those two data centers our Search application now runs 100% in AWS we're deployed in four regions we",
    "start": "2200960",
    "end": "2208599"
  },
  {
    "text": "currently serve about 4 and a half billion requests a month out of out of AWS and all of that is run on about 500",
    "start": "2208599",
    "end": "2214960"
  },
  {
    "text": "ec2 instances behind uh 50 load balancers and about 70 hosted zones",
    "start": "2214960",
    "end": "2220960"
  },
  {
    "text": "inside of Route 53 this diagram shows kind of our",
    "start": "2220960",
    "end": "2227800"
  },
  {
    "text": "reference architecture so when we this uh kind of depicts what architecture looks like",
    "start": "2227800",
    "end": "2234280"
  },
  {
    "text": "within one availability Zone and as we this is our cookie cutter approach so using chef and other AWS API calls we",
    "start": "2234280",
    "end": "2243240"
  },
  {
    "text": "can replicate this very quickly in multiple availability zones or into new regions",
    "start": "2243240",
    "end": "2249520"
  },
  {
    "text": "so as we were getting ready to do the migration we were already in these two",
    "start": "2251520",
    "end": "2257359"
  },
  {
    "text": "Colo locations and we had lots of um expensive Hardware gear that helped us",
    "start": "2257359",
    "end": "2263480"
  },
  {
    "text": "do distribution of traffic between our two Colo locations and we were a little bit wary",
    "start": "2263480",
    "end": "2270680"
  },
  {
    "text": "in the beginning that a simple AWS call to API call to set up latency based",
    "start": "2270680",
    "end": "2275760"
  },
  {
    "text": "routing could actually replace like all this expensive gear and so we wanted to test that out before we actually put all",
    "start": "2275760",
    "end": "2282560"
  },
  {
    "text": "our eggs in that basket and so we came up with a we already had a way of testing that we called fire and forget",
    "start": "2282560",
    "end": "2288560"
  },
  {
    "text": "and we leveraged that um to test latency based routing and I'll walk you through how that worked so in fire forget you",
    "start": "2288560",
    "end": "2294400"
  },
  {
    "text": "have two systems you have your production system you have a system that's under test when a user makes a request to the",
    "start": "2294400",
    "end": "2301240"
  },
  {
    "text": "production system the first thing that that system does is it fires off an asynchronous request looks identical to",
    "start": "2301240",
    "end": "2308760"
  },
  {
    "text": "the original incoming request and that gets fired off to the system under test it's done with a very short timeout so",
    "start": "2308760",
    "end": "2315599"
  },
  {
    "text": "the request is pretty much dropped as soon as it's sent but then both systems get to process that",
    "start": "2315599",
    "end": "2321599"
  },
  {
    "text": "request the only difference being that the only response that goes back to the user comes from the production system",
    "start": "2321599",
    "end": "2327440"
  },
  {
    "text": "what this lets us do in lots of different scenarios is take traffic that is shaped exactly like our production",
    "start": "2327440",
    "end": "2334440"
  },
  {
    "text": "traffic and put it on a new system and see how it be behaves so when we did this for latency",
    "start": "2334440",
    "end": "2341560"
  },
  {
    "text": "based routing we started off with uh systems in our two",
    "start": "2341560",
    "end": "2348880"
  },
  {
    "text": "collocation data centers and we spun up parts of our",
    "start": "2348880",
    "end": "2354359"
  },
  {
    "text": "Search application in two different AWS regions we created some latency based routing records in Route 53 that pointed",
    "start": "2354359",
    "end": "2362400"
  },
  {
    "text": "to those two regions we turned them on turned on fire and forget we watched the traffic spin",
    "start": "2362400",
    "end": "2368040"
  },
  {
    "text": "up in both of those regions and then we killed all the instances in one of those",
    "start": "2368040",
    "end": "2373640"
  },
  {
    "text": "regions and no joke 150 seconds later all the traffic swung over uh it was",
    "start": "2373640",
    "end": "2380240"
  },
  {
    "text": "like magic um and it was literally 150 seconds we we could consistently repeat",
    "start": "2380240",
    "end": "2385880"
  },
  {
    "text": "that at 150 seconds so the results of our high",
    "start": "2385880",
    "end": "2391720"
  },
  {
    "text": "availability experiment in AWS which where is no longer an experiment for us",
    "start": "2391720",
    "end": "2396760"
  },
  {
    "text": "we can have Regional failover in 150 seconds very consistently we've decreased our latency",
    "start": "2396760",
    "end": "2403680"
  },
  {
    "text": "by 25% to our users worldwide Route 53 also allows us in",
    "start": "2403680",
    "end": "2410480"
  },
  {
    "text": "cases where we might be um experiencing trouble between a partner and AWS maybe",
    "start": "2410480",
    "end": "2416400"
  },
  {
    "text": "a network routing problem we can actually just use Route 53 to Route them to a different region so bypassing the",
    "start": "2416400",
    "end": "2424040"
  },
  {
    "text": "latency based routing but instead just point them to a different region until they get their Network route sorted",
    "start": "2424040",
    "end": "2430000"
  },
  {
    "text": "out and in the end it ended up replacing a whole bunch of expensive Network gear from our data",
    "start": "2430000",
    "end": "2436040"
  },
  {
    "text": "centers so next steps for us are continue to expand into additional",
    "start": "2436040",
    "end": "2441359"
  },
  {
    "text": "regions um and we're also looking at integrating application metrics from our",
    "start": "2441359",
    "end": "2447720"
  },
  {
    "text": "app into um into uh Route 53 calls that",
    "start": "2447720",
    "end": "2454400"
  },
  {
    "text": "we might be able to then say um here's an example if if in one",
    "start": "2454400",
    "end": "2459560"
  },
  {
    "text": "region a user comes to us because Route 53 says that that's the least latent but perhaps we're having network troubles",
    "start": "2459560",
    "end": "2465440"
  },
  {
    "text": "trying to talk to Yahoo to get search results we might want to influence where those users are actually routed so maybe",
    "start": "2465440",
    "end": "2471599"
  },
  {
    "text": "not the least latent because their total experience overall would be better served out of a different",
    "start": "2471599",
    "end": "2477720"
  },
  {
    "text": "region that's it with that I'll turn it back to Dave great thanks",
    "start": "2477720",
    "end": "2485520"
  },
  {
    "text": "Paul so let's just summarize we spoke about three levels of availability the first of those we said",
    "start": "2485520",
    "end": "2491560"
  },
  {
    "text": "was instance availability we looked at how elastic load balancing can help you balance your load across multiple",
    "start": "2491560",
    "end": "2496839"
  },
  {
    "text": "instances using least cons as the load balancing algorithm together with health checks to react to any instances that",
    "start": "2496839",
    "end": "2502119"
  },
  {
    "text": "may become unhealthy as well as um we moved on to zonal availability where we",
    "start": "2502119",
    "end": "2508280"
  },
  {
    "text": "said you know we should not be in one availability Zone let's make sure we have our applications in multiple zones",
    "start": "2508280",
    "end": "2513839"
  },
  {
    "text": "and we looked at how elastic low balancing together with the new feature cross Zone L balancing allows you to do that effectively and finally as Sean and",
    "start": "2513839",
    "end": "2520560"
  },
  {
    "text": "Paul described to us Regional availability where you're able to actually run your application in",
    "start": "2520560",
    "end": "2525720"
  },
  {
    "text": "multiple regions and use R 53 uh to load balance to send traffic to multip them",
    "start": "2525720",
    "end": "2531720"
  },
  {
    "text": "multiple of them so we we've been through a lot of content we haven't shown you too many slides on how to do",
    "start": "2531720",
    "end": "2536880"
  },
  {
    "text": "this I would encourage you if you want to know about any of these Concepts it's all documented in both the elb and Route",
    "start": "2536880",
    "end": "2542319"
  },
  {
    "text": "53 documentation so spend some time reading that and then there are a lot of helpful blogs tutorials online that will",
    "start": "2542319",
    "end": "2548119"
  },
  {
    "text": "also help you get this set up thank you very much for listening very happy that so many of you stuck around we'd be",
    "start": "2548119",
    "end": "2553280"
  },
  {
    "text": "happy to take some questions",
    "start": "2553280",
    "end": "2556960"
  }
]