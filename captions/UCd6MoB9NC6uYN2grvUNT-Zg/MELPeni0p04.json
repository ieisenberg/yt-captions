[
  {
    "text": "alright looks like we're gonna get started here hello everybody my name is rainy originally I'm a principal solutions architect and today I'm going",
    "start": "530",
    "end": "7950"
  },
  {
    "text": "to be presenting probably the longest title slide on a show on Kinesis so",
    "start": "7950",
    "end": "13349"
  },
  {
    "text": "we're going to talk about best practices with Kinesis we're gonna go over some of the overview of Canadian streaming first",
    "start": "13349",
    "end": "20189"
  },
  {
    "text": "just to make sure everybody is up to speed on what we're going to be talking about deeper we're gonna build on yeah",
    "start": "20189",
    "end": "27539"
  },
  {
    "text": "on each step of the way showing how we can leverage Kinesis data streams integrating that with Kinesis data",
    "start": "27539",
    "end": "33239"
  },
  {
    "text": "firehose and building out solutions to not only respond to data in real time but also land that data already pre",
    "start": "33239",
    "end": "40140"
  },
  {
    "text": "processed and and curated into your data Lake solution that you're working on and",
    "start": "40140",
    "end": "45780"
  },
  {
    "text": "then finally at the end of this the slide deck there'll be a link for a takeaway notebook so you can do any of",
    "start": "45780",
    "end": "51960"
  },
  {
    "text": "the demos that we did here today through the the slides and so let's start with just streaming data overview really",
    "start": "51960",
    "end": "58109"
  },
  {
    "text": "quickly so the reason why real-time streaming has come about is because we want to respond faster to our customers",
    "start": "58109",
    "end": "64350"
  },
  {
    "text": "needs that may be building out mobile applications and being able to respond through notifications in real time it",
    "start": "64350",
    "end": "70590"
  },
  {
    "text": "could be generating clickstream data and capturing that and reporting on and utilizing that for feedback back into",
    "start": "70590",
    "end": "77759"
  },
  {
    "text": "your products it could be doing log analysis so capturing all of the log data you have from your applications and",
    "start": "77759",
    "end": "83670"
  },
  {
    "text": "providing insight on that many of you may be interested in smart cities IOT",
    "start": "83670",
    "end": "88920"
  },
  {
    "text": "solutions all of these have a real time component to them and this is really where Kinesis can help you deliver those",
    "start": "88920",
    "end": "95490"
  },
  {
    "text": "solutions and when we talk about the time frames for each of these types of",
    "start": "95490",
    "end": "101100"
  },
  {
    "text": "workloads we're talking in not only milliseconds of times where people typically see real-time streaming is",
    "start": "101100",
    "end": "106619"
  },
  {
    "text": "instantly having this data back but it could also be down into the minutes range to do some ETL processing before",
    "start": "106619",
    "end": "113670"
  },
  {
    "text": "landing that data into your data like and a little bit of everything in between so hopefully maybe some of these",
    "start": "113670",
    "end": "119759"
  },
  {
    "text": "use cases fit you know what you're looking to deliver egde with Kinesis as well but what we found in building these",
    "start": "119759",
    "end": "127079"
  },
  {
    "text": "real-time streaming solutions are there are some real challenges to to overcome to make them work consistently",
    "start": "127079",
    "end": "133590"
  },
  {
    "text": "to stop being such high touch applications and allow them to be automated out more a lot of these",
    "start": "133590",
    "end": "139380"
  },
  {
    "text": "solutions have tended to be typical typically difficult to set up they're",
    "start": "139380",
    "end": "144540"
  },
  {
    "text": "hard to scale as your business scales having that high availability therefore you requires a number of components to",
    "start": "144540",
    "end": "151590"
  },
  {
    "text": "be available durably coordination things of that nature and that ends up leading to being very expensive to maintain as",
    "start": "151590",
    "end": "158519"
  },
  {
    "text": "you're you're deploying and scaling these solutions out and this is really why we came out with Kinesis and with",
    "start": "158519",
    "end": "166769"
  },
  {
    "text": "Kinesis we have a number of services within the suite of tools here we start with the Kinesis data streams this is",
    "start": "166769",
    "end": "173190"
  },
  {
    "text": "going to be your typical producer-consumer semantics of sending data to a stream and then processing it",
    "start": "173190",
    "end": "180209"
  },
  {
    "text": "with a consumer on the back end next up we have the Kinesis data firehose the firehose is a solution that allows you",
    "start": "180209",
    "end": "187860"
  },
  {
    "text": "to figure out what persistent data store you want to land that data to it could be s3 it could be elasticsearch you",
    "start": "187860",
    "end": "195269"
  },
  {
    "text": "could be into redshift into your data warehouse solution and point it to that and have that data delivered directly to",
    "start": "195269",
    "end": "200700"
  },
  {
    "text": "there we also have Kinesis analytics and this has the ability to in real time",
    "start": "200700",
    "end": "207079"
  },
  {
    "text": "inject itself into the real time stream to do aggregations analysis on that data",
    "start": "207079",
    "end": "212670"
  },
  {
    "text": "leveraging either sequel or java applications to do that processing and",
    "start": "212670",
    "end": "218340"
  },
  {
    "text": "maybe curate and and persist to another stream of data and then we also have support for video streams as well but",
    "start": "218340",
    "end": "226109"
  },
  {
    "text": "for the majority of the talk today I'm going to be focusing mostly on how you can leverage both Kinesis data streams",
    "start": "226109",
    "end": "232590"
  },
  {
    "text": "and Kinesis data firehose and all of the different configuration options that are there that you could leverage for your",
    "start": "232590",
    "end": "239400"
  },
  {
    "text": "various workloads that you're going after and hopefully what you'll find out of this is there's some real key",
    "start": "239400",
    "end": "245130"
  },
  {
    "text": "benefits to using a managed service like Kinesis because there's a lot of things that you will no longer have to worry",
    "start": "245130",
    "end": "251340"
  },
  {
    "text": "about things like provisioning infrastructure scaling these solutions",
    "start": "251340",
    "end": "256530"
  },
  {
    "text": "up as your business scales if there is no streaming going through the system at",
    "start": "256530",
    "end": "262650"
  },
  {
    "text": "a time you're not incurring costs because you're not utilizing those services and you'll see that they're very highly",
    "start": "262650",
    "end": "268230"
  },
  {
    "text": "available as well as secure you can encrypt your data at rest and it provides the ability to ensure that the",
    "start": "268230",
    "end": "274740"
  },
  {
    "text": "stream of data keeps going and when we talk about streaming you know we need to",
    "start": "274740",
    "end": "281190"
  },
  {
    "text": "get some of the terminology out there so that everybody's familiar with it what we're going to be talking about is on",
    "start": "281190",
    "end": "286200"
  },
  {
    "text": "the producer side there's a number of tools and utilities for ingesting data into your streaming solution and with",
    "start": "286200",
    "end": "294450"
  },
  {
    "text": "Kinesis specifically we really break it up into sort of three different categories we've got the you know",
    "start": "294450",
    "end": "300720"
  },
  {
    "text": "toolkits and libraries that we have support for the CLI is the SDK we have a Kinesis agent that you can leverage",
    "start": "300720",
    "end": "307169"
  },
  {
    "text": "which is an agent you can install on your systems to be able to aggregate the",
    "start": "307169",
    "end": "312479"
  },
  {
    "text": "data at the source and then persist out in batches as well as the Kinesis producer library that can do similar",
    "start": "312479",
    "end": "318210"
  },
  {
    "text": "functionality when we're talking about some of those workloads like smart cities or log and aggregation and",
    "start": "318210",
    "end": "325950"
  },
  {
    "text": "analysis we can leverage our IOT service as well as the cloud watch events and",
    "start": "325950",
    "end": "331169"
  },
  {
    "text": "logs as you're pushing data there to push them through the real-time streaming solution to provide new",
    "start": "331169",
    "end": "336510"
  },
  {
    "text": "insight there and then there's a whole slew of third-party offerings in the Big Data space there's a number of open",
    "start": "336510",
    "end": "342300"
  },
  {
    "text": "source tools commercial tools out there I'm going to call out a few of them here that we see a lot of use for them is the",
    "start": "342300",
    "end": "348810"
  },
  {
    "text": "the integrations with log4j if you're building java applications maybe you're using flew more fluent d to to pull that",
    "start": "348810",
    "end": "356100"
  },
  {
    "text": "log data off of those source systems and throw it into the stream now on the",
    "start": "356100",
    "end": "361260"
  },
  {
    "text": "backend side of that for that consumption of those of that data we again break it down into three different",
    "start": "361260",
    "end": "367800"
  },
  {
    "text": "categories we have with Kinesis as I talked about with Kinesis data analytics you can use an z sequel or you could use",
    "start": "367800",
    "end": "375300"
  },
  {
    "text": "a blink service to pick up that data and process it we also have a Kinesis client",
    "start": "375300",
    "end": "382229"
  },
  {
    "text": "library that will allow you to create your own applications off of it and scale those out through leveraging ec2",
    "start": "382229",
    "end": "388919"
  },
  {
    "text": "instances potentially auto scaling and have that client library process that data what we're going to be focusing on",
    "start": "388919",
    "end": "395550"
  },
  {
    "text": "more though today is to really show somewhat of a server list process to build out these real-time streams and",
    "start": "395550",
    "end": "402710"
  },
  {
    "text": "we'll be using AWS lambda as the consumer of the the Kinesis fire data",
    "start": "402710",
    "end": "408860"
  },
  {
    "text": "stream that we're leveraging and then again just like there is a number of third-party open source ingestion",
    "start": "408860",
    "end": "414980"
  },
  {
    "text": "mechanisms there's also a number of third-party sources for consumption of that data the most popular out there",
    "start": "414980",
    "end": "421520"
  },
  {
    "text": "probably today is Apache spark being able to utilize spark streaming off of a Kinesis stream to process that data but",
    "start": "421520",
    "end": "429470"
  },
  {
    "text": "there's a whole slew of them as you move forward things like Apache storm and data bricks to do that processing as",
    "start": "429470",
    "end": "435920"
  },
  {
    "text": "well and so to kind of get started I'd like to start with the Kinesis data",
    "start": "435920",
    "end": "441290"
  },
  {
    "text": "streams side of processing and we'll walk through what it takes and and some of the the guidelines around building",
    "start": "441290",
    "end": "448690"
  },
  {
    "text": "best practices into building out your streaming solutions and so as I mentioned Kinesis data streams it's very",
    "start": "448690",
    "end": "456020"
  },
  {
    "text": "easy to get started with we'll walk through what it looks like to create that through the console and then start leveraging that that stream it's real",
    "start": "456020",
    "end": "463310"
  },
  {
    "text": "time it has elastic performance you can utilize application auto scaling to increase the number of shards that",
    "start": "463310",
    "end": "469640"
  },
  {
    "text": "you're using as the the number of messages through the system increases and so walking through what the console",
    "start": "469640",
    "end": "477920"
  },
  {
    "text": "looks like today what you would do is you would go into the Kinesis console and you're gonna create a data stream you're going to give it a name because",
    "start": "477920",
    "end": "485300"
  },
  {
    "text": "we need to know where we're putting that data and then finally you have really one lover that you're working with here",
    "start": "485300",
    "end": "492080"
  },
  {
    "text": "and that's gonna be the number of shards that you want this data stream to have and the shard itself is a unit of scale",
    "start": "492080",
    "end": "499460"
  },
  {
    "text": "for determining how much data can go through that stream at any given time and so you can see we've got a an",
    "start": "499460",
    "end": "507500"
  },
  {
    "text": "estimator there so if you know the types of data that you're pushing through there you'll be able to figure out how",
    "start": "507500",
    "end": "513500"
  },
  {
    "text": "many shards you need for this is it for the particular stream and I'll show a diagram that will explain this a little",
    "start": "513500",
    "end": "519080"
  },
  {
    "text": "better here shortly so some of the key metrics to think about when you're building these Kinesis data streaming",
    "start": "519080",
    "end": "525290"
  },
  {
    "text": "solutions are you can have up to one megabyte or a thousand records per second per shard so if you're thinking",
    "start": "525290",
    "end": "532130"
  },
  {
    "text": "about the types of systems that you're building real-time screaming you'll need to have an understanding of how much data you're",
    "start": "532130",
    "end": "537560"
  },
  {
    "text": "pushing through so that you're not throttling at the stream based on ingestion that you're working with and",
    "start": "537560",
    "end": "545140"
  },
  {
    "text": "on the consumer side the consumers are going to make a call called get records",
    "start": "545140",
    "end": "550280"
  },
  {
    "text": "and that's going to pull that stream / shard to get that data to start",
    "start": "550280",
    "end": "555800"
  },
  {
    "text": "consuming each of the shards within the stream and so the data will be returned and with a single consumer on a standard",
    "start": "555800",
    "end": "564080"
  },
  {
    "text": "Kinesis data stream you can have two megabytes per second per shard through there and you can have up to five",
    "start": "564080",
    "end": "571720"
  },
  {
    "text": "transactions per second per shard so on a single on a single consumer you can",
    "start": "571720",
    "end": "577790"
  },
  {
    "text": "pull about every 200 milliseconds that's where it stands today but what happens",
    "start": "577790",
    "end": "583400"
  },
  {
    "text": "if I want to have a number of consumers on there so now what will happen is and things you'll need to be aware of is the",
    "start": "583400",
    "end": "589160"
  },
  {
    "text": "the throughput of each of those consumer applications goes down to about 400 kbps",
    "start": "589160",
    "end": "594250"
  },
  {
    "text": "as well as the propagation delay in how often each one of those consumers can",
    "start": "594250",
    "end": "600830"
  },
  {
    "text": "poll will be affected as well so if I have five consumer applications on this",
    "start": "600830",
    "end": "607070"
  },
  {
    "text": "particular stream each one of those consumer applications will only be able to poll for that data once a second so",
    "start": "607070",
    "end": "613850"
  },
  {
    "text": "those are some of the limitations within the way standard consumers work today and so some of the I mentioned pitfalls",
    "start": "613850",
    "end": "622670"
  },
  {
    "text": "in the initial slide some of the pitfalls to be aware of are things like poison messages so in this example here",
    "start": "622670",
    "end": "629900"
  },
  {
    "text": "I have 300 records that have been pushed through the Kinesis data stream and I have a lambda consumer on that when you",
    "start": "629900",
    "end": "636410"
  },
  {
    "text": "configure a consumer you're going to say what the batch size is for the number of Records to pull back from the stream to",
    "start": "636410",
    "end": "643370"
  },
  {
    "text": "process each time and in this case I'm saying I want to pull back 200 messages so it's going to invoke a function from",
    "start": "643370",
    "end": "651740"
  },
  {
    "text": "lambda to do that processing and let's say for some reason it airs out processing one of those messages within",
    "start": "651740",
    "end": "658130"
  },
  {
    "text": "the batch so it won't have the ability to checkpoint back to the service to",
    "start": "658130",
    "end": "663410"
  },
  {
    "text": "allow the stream to continue processing that data and what will happen is when it invoke a new lambda function it's going to",
    "start": "663410",
    "end": "670320"
  },
  {
    "text": "invoke it with that same 200 records and you're as you can see going to get the same problem it's going to have that",
    "start": "670320",
    "end": "676980"
  },
  {
    "text": "poison pill message so you're really going to want to be aware of how that lambda function how that Kinesis",
    "start": "676980",
    "end": "683670"
  },
  {
    "text": "consumer is processing those messages and be aware of that and that will continue on until that message expires",
    "start": "683670",
    "end": "690960"
  },
  {
    "text": "out of the data stream which would be anywhere from the 24-hour period that is",
    "start": "690960",
    "end": "696030"
  },
  {
    "text": "the default for Kinesis data streams up to seven days so one of the things that you really want to be cognizant of is",
    "start": "696030",
    "end": "702990"
  },
  {
    "text": "capture and log the exceptions to be able to process those poison pill messages at a later time so if we take",
    "start": "702990",
    "end": "710070"
  },
  {
    "text": "that same example example and now in the lambda function maybe we have a try-catch or we're catching the air that",
    "start": "710070",
    "end": "716430"
  },
  {
    "text": "we can't process that message let's just log that out to cloud watch logs maybe we write it out to be picked up by",
    "start": "716430",
    "end": "724140"
  },
  {
    "text": "an alarm or some other you know mechanism that you have and continue on processing to return the successful",
    "start": "724140",
    "end": "730740"
  },
  {
    "text": "response from that batch of Records that you're executing on because now what",
    "start": "730740",
    "end": "736470"
  },
  {
    "text": "will happen is once I've processed those 200 records I can go and take that other hundred records in that batch and invoke",
    "start": "736470",
    "end": "742980"
  },
  {
    "text": "the new lambda function to process the rest of that stream and so from a lambda",
    "start": "742980",
    "end": "752580"
  },
  {
    "text": "perspective how do we create these consumers and I'm always using the examples of the the console but but keep",
    "start": "752580",
    "end": "759930"
  },
  {
    "text": "in mind typically you're going to do this in script you might be leveraging cloud formation templates you might be",
    "start": "759930",
    "end": "766170"
  },
  {
    "text": "leveraging the newly newish announced cdk2 to have infrastructure as code but when",
    "start": "766170",
    "end": "773160"
  },
  {
    "text": "you're starting to gain you know just kick the tires on how some of these services work it's always good to just you know go through the council and",
    "start": "773160",
    "end": "779520"
  },
  {
    "text": "figure out how some of these things work so in this example I'm gonna leverage",
    "start": "779520",
    "end": "785820"
  },
  {
    "text": "one of the existing blueprints that exists out there it's the Kinesis process record in",
    "start": "785820",
    "end": "790890"
  },
  {
    "text": "python i chose Python it could have been no js' any number of languages there and",
    "start": "790890",
    "end": "796430"
  },
  {
    "text": "what I'm going to do is I'm going to give that function a name because this is the consumer I want",
    "start": "796430",
    "end": "801540"
  },
  {
    "text": "utilize for that Kinesis datastream I give it a roll that's going to allow me get record access on that stream in this",
    "start": "801540",
    "end": "810149"
  },
  {
    "text": "example I have a sample Kinesis iam roll and then you're going to select the",
    "start": "810149",
    "end": "816930"
  },
  {
    "text": "stream that it's going to process that data from so so as you can see here I want to grab it from that demo stream",
    "start": "816930",
    "end": "823079"
  },
  {
    "text": "that I just created earlier and I want to have a batch size of a hundred records at a time and now I can go in",
    "start": "823079",
    "end": "829350"
  },
  {
    "text": "there and and start that lambda function and it will start pulling and consuming that data",
    "start": "829350",
    "end": "834509"
  },
  {
    "text": "the last piece there is the starting position which could be important depending on how you're leveraging the",
    "start": "834509",
    "end": "840930"
  },
  {
    "text": "applications so as you're writing records out to the stream it's keeping a",
    "start": "840930",
    "end": "846060"
  },
  {
    "text": "pointer of where it has finished processing in the consumption of that particular stream on the consumer side",
    "start": "846060",
    "end": "852540"
  },
  {
    "text": "so I can say you know what this is a brand new consumer I don't care about the data that was at the very beginning",
    "start": "852540",
    "end": "858389"
  },
  {
    "text": "just start with the latest messages that are in here and start processing that data or if I want to make sure that I",
    "start": "858389",
    "end": "864420"
  },
  {
    "text": "start with everything that's in there I can select the trim horizon and pick up from the beginning of the stream as",
    "start": "864420",
    "end": "870389"
  },
  {
    "text": "opposed to at the end of the stream and so what you'll have from a code perspective is you'll have a very simple",
    "start": "870389",
    "end": "877380"
  },
  {
    "text": "lambda handler here and what you can do is the event contains the number of records based on that batch size that",
    "start": "877380",
    "end": "884370"
  },
  {
    "text": "you can loop through decode that payload and then start doing your processing within there so this is really the the",
    "start": "884370",
    "end": "892670"
  },
  {
    "text": "the totality of default codes you need to get started with a Kinesis consumer",
    "start": "892670",
    "end": "899699"
  },
  {
    "text": "leveraging lambda everything else will be taken care of for you and as we",
    "start": "899699",
    "end": "905100"
  },
  {
    "text": "talked about the Kinesis data streams standard consumers we had some of those limitations you could only process you",
    "start": "905100",
    "end": "912449"
  },
  {
    "text": "know five transactions per second per shard only two megabytes per second per",
    "start": "912449",
    "end": "918329"
  },
  {
    "text": "shard across the board and so a lot of our customers we wanted they wanted to add more than just you know five",
    "start": "918329",
    "end": "924750"
  },
  {
    "text": "consumers they may have you know 20 consumers that they want to have leveraged that same data stream and not",
    "start": "924750",
    "end": "930990"
  },
  {
    "text": "look at things like fan-out strategies to push data into other and and processed the data that way so",
    "start": "930990",
    "end": "938250"
  },
  {
    "text": "what we announced were enhanced fan-out consumers and what that does is these",
    "start": "938250",
    "end": "944610"
  },
  {
    "text": "consumers are no longer going to be polling they're going to be requesting",
    "start": "944610",
    "end": "949830"
  },
  {
    "text": "to subscribe to that shard and start processing that data so what happens is",
    "start": "949830",
    "end": "955650"
  },
  {
    "text": "you'll see us subscribe to the shard it's going to leverage in HTTP to mechanism to start GATT aggregating that",
    "start": "955650",
    "end": "962640"
  },
  {
    "text": "data and as new messages come in it's going to persist that shard to disk so",
    "start": "962640",
    "end": "968730"
  },
  {
    "text": "we have the durability there and then it's going to start pushing those records in an array to that consumer to",
    "start": "968730",
    "end": "975360"
  },
  {
    "text": "be processed and so what does that look like if I start adding multiple",
    "start": "975360",
    "end": "981840"
  },
  {
    "text": "consumers on there before we had all of those limitations with the enhanced fan-out",
    "start": "981840",
    "end": "987150"
  },
  {
    "text": "I can register a number of consumers each of those consumers are going to have their own enhanced fan-out pipe",
    "start": "987150",
    "end": "993660"
  },
  {
    "text": "where that data is being delivered to where that subscription occurs from that",
    "start": "993660",
    "end": "998670"
  },
  {
    "text": "consumer and it's going to leverage up the full two megabytes per second for",
    "start": "998670",
    "end": "1003800"
  },
  {
    "text": "that consumer itself instead of sharing those resources like the standard consumers were so if I have another",
    "start": "1003800",
    "end": "1010220"
  },
  {
    "text": "consumer on here it's going to go through that same process a new efo pipe is going to be generated and we can",
    "start": "1010220",
    "end": "1017390"
  },
  {
    "text": "start streaming that data to that enhanced consumer and so with lambda",
    "start": "1017390",
    "end": "1024290"
  },
  {
    "text": "because lambda was a polling mechanism when we created that blueprint what we",
    "start": "1024290",
    "end": "1029420"
  },
  {
    "text": "can do is make some modifications to that original code that we started with",
    "start": "1029420",
    "end": "1034550"
  },
  {
    "text": "and what what will happen is the lambda service will subscribe to that stream just like a consumer application would",
    "start": "1034550",
    "end": "1042380"
  },
  {
    "text": "and then based on that record size it's going to batch those up on the lambda",
    "start": "1042380",
    "end": "1047630"
  },
  {
    "text": "service and invoke a function for you so you really don't have to change anything within your lambda code to take",
    "start": "1047630",
    "end": "1054260"
  },
  {
    "text": "advantage of that what will have to happen though is you will need to register as an enhanced fan-out consumer",
    "start": "1054260",
    "end": "1061070"
  },
  {
    "text": "and so the way you know typically you would go about that is I showed an",
    "start": "1061070",
    "end": "1067040"
  },
  {
    "text": "example with the CLI here but you can leverage the SDKs as well is you're going to register that consumer based on",
    "start": "1067040",
    "end": "1074130"
  },
  {
    "text": "a name that you're going to give it and then point it to the stream Arn that you want to register it with and what will",
    "start": "1074130",
    "end": "1080940"
  },
  {
    "text": "happen is on that Kinesis data stream you'll see in the enhanced fan-out tab",
    "start": "1080940",
    "end": "1086310"
  },
  {
    "text": "there that you do have a consumer registered so now I can go back to that",
    "start": "1086310",
    "end": "1092370"
  },
  {
    "text": "lamda code that I just built and select a consumer in that second option there",
    "start": "1092370",
    "end": "1097650"
  },
  {
    "text": "of that last update consumer and now I've converted that lambda function from",
    "start": "1097650",
    "end": "1102870"
  },
  {
    "text": "a standard consumer to an enhanced fan-out consumer so that'll give you them the ability to scale beyond the",
    "start": "1102870",
    "end": "1109860"
  },
  {
    "text": "limitations of the standard consumer so then typically the next questions that",
    "start": "1109860",
    "end": "1115170"
  },
  {
    "text": "come out of this is so when do I use which one right there's a lot of different reasons why you would pick and",
    "start": "1115170",
    "end": "1121140"
  },
  {
    "text": "enhance consumer fan-out consumer from a standard consumer but I've kind of really broke it down into these few",
    "start": "1121140",
    "end": "1127050"
  },
  {
    "text": "options here a lot of times you want to leverage Kinesis data streams because you want to have that real time",
    "start": "1127050",
    "end": "1132540"
  },
  {
    "text": "streaming element and separation of services but maybe you don't have a lot of services maybe you don't have a lot",
    "start": "1132540",
    "end": "1138870"
  },
  {
    "text": "of consumers connected to that particular stream look at using the standard consumers and see if you're",
    "start": "1138870",
    "end": "1144600"
  },
  {
    "text": "getting the performance you're looking for you may not have latency sensitive solutions and maybe you're really",
    "start": "1144600",
    "end": "1149790"
  },
  {
    "text": "wanting to optimize on cost but maybe you do want to be able to access that data faster be able to scale out the",
    "start": "1149790",
    "end": "1156240"
  },
  {
    "text": "number of consumers then you're going to want to look at the enhanced fan-out solution for consumption",
    "start": "1156240",
    "end": "1164000"
  },
  {
    "text": "so as some of you may know yeah we",
    "start": "1169150",
    "end": "1174380"
  },
  {
    "text": "really take care and understanding about what our customers are looking for in",
    "start": "1174380",
    "end": "1179480"
  },
  {
    "text": "the services that we build and you know up to 90% of our roadmap is driven by feedback and so when Kinesis data",
    "start": "1179480",
    "end": "1187220"
  },
  {
    "text": "streams first came about and and I was one of the lucky few as a client as well",
    "start": "1187220",
    "end": "1193490"
  },
  {
    "text": "before before I was here at Amazon I was using Kinesis data data streams to just",
    "start": "1193490",
    "end": "1200150"
  },
  {
    "text": "aggregate data in memory wait a particular period of time and then aggregate that out to s3 for the the",
    "start": "1200150",
    "end": "1207340"
  },
  {
    "text": "initial creation of the data like that I was building and what what the Kinesis",
    "start": "1207340",
    "end": "1213620"
  },
  {
    "text": "team found was I wasn't the only one doing this there's a number of customers that were duplicating that same",
    "start": "1213620",
    "end": "1219260"
  },
  {
    "text": "functionality maybe buffering data for the number of megabytes they're leveraging buffering data based on a",
    "start": "1219260",
    "end": "1224810"
  },
  {
    "text": "time frame and then persisting that out to help simplify that persistent process and so that's really where Kinesis date",
    "start": "1224810",
    "end": "1232070"
  },
  {
    "text": "a firehose came konista state a firehose has all the same semantics when publishing data to it as a Kinesis data",
    "start": "1232070",
    "end": "1239240"
  },
  {
    "text": "stream but the difference is you're not building custom code for the consumers of that particular stream what you're",
    "start": "1239240",
    "end": "1246770"
  },
  {
    "text": "doing is pointing to a persistent store and letting you know letting the data",
    "start": "1246770",
    "end": "1252170"
  },
  {
    "text": "buffer in the firehose before it gets persisted out and the three options that",
    "start": "1252170",
    "end": "1257240"
  },
  {
    "text": "we have today are s3 for object storage to build out that data Lake strategy you",
    "start": "1257240",
    "end": "1263090"
  },
  {
    "text": "can load directly to Amazon redshift through that data stream elasticsearch",
    "start": "1263090",
    "end": "1268700"
  },
  {
    "text": "and then with one of our partners Splunk we can push the data into a Splunk instance as well and so some of the key",
    "start": "1268700",
    "end": "1277610"
  },
  {
    "text": "features of leveraging this fire hose is you have that raw data coming in and a lot of times with maybe that consumer",
    "start": "1277610",
    "end": "1284390"
  },
  {
    "text": "aspect you're doing some work to enrich the data maybe filter that data with the",
    "start": "1284390",
    "end": "1290600"
  },
  {
    "text": "Kinesis data firehose you're going to have the ability to in those batches that were persisting out execute a",
    "start": "1290600",
    "end": "1297590"
  },
  {
    "text": "lambda transformation on them and do things like maybe in the data stream so in this case I have",
    "start": "1297590",
    "end": "1303889"
  },
  {
    "text": "raw data coming in that has an IP address in it and maybe I want to do some geo IP lookups",
    "start": "1303889",
    "end": "1309200"
  },
  {
    "text": "to add on that metadata that it was you know from Boston Massachusetts I can leverage lambda with the Kinesis",
    "start": "1309200",
    "end": "1316879"
  },
  {
    "text": "data firehose to enrich that data before it persists of whatever data store I'm using but I can also filter that data",
    "start": "1316879",
    "end": "1324230"
  },
  {
    "text": "too maybe I'm pushing log data in and I don't really care about you know the",
    "start": "1324230",
    "end": "1329690"
  },
  {
    "text": "info level logging the warning any of that stuff but I just want to capture all of the airs and then maybe send it",
    "start": "1329690",
    "end": "1335269"
  },
  {
    "text": "back to the app dev team and say fix this right you know I can take the Kinesis data fire hose apply that lambda",
    "start": "1335269",
    "end": "1342710"
  },
  {
    "text": "transformation to filter out only the air types that get persisted to the data store that I'm working with and finally",
    "start": "1342710",
    "end": "1350179"
  },
  {
    "text": "I can also convert the data stream so maybe I'm capturing Apache logs",
    "start": "1350179",
    "end": "1355429"
  },
  {
    "text": "Pachi logs that have a row format that isn't really structured I wouldn't even",
    "start": "1355429",
    "end": "1360559"
  },
  {
    "text": "say it's semi structured maybe a semi structured give or take but I want to",
    "start": "1360559",
    "end": "1365659"
  },
  {
    "text": "convert that data into a JSON document before I land it into my data Lake the",
    "start": "1365659",
    "end": "1370940"
  },
  {
    "text": "data warehouse I can use that same lambda functionality to convert that row",
    "start": "1370940",
    "end": "1376369"
  },
  {
    "text": "based data into a JSON document and finally one of the features that we just",
    "start": "1376369",
    "end": "1383090"
  },
  {
    "text": "released a few months ago I believe it was is the ability to do record format conversion and this this becomes really",
    "start": "1383090",
    "end": "1389899"
  },
  {
    "text": "critical when you're building out a data like strategy and you you want to leverage a columnar format to improve",
    "start": "1389899",
    "end": "1396259"
  },
  {
    "text": "the performance of the analytics engines that you're querying with on the back end and so with record format conversion",
    "start": "1396259",
    "end": "1402799"
  },
  {
    "text": "it's going to rely on leveraging our glue data catalog and and for those that",
    "start": "1402799",
    "end": "1408019"
  },
  {
    "text": "aren't familiar with what our glue data catalog is it's a hive compliant metadata repository for the information",
    "start": "1408019",
    "end": "1416269"
  },
  {
    "text": "that you want to persist into your data Lake and so the data the glue data catalog will have a schema for what that",
    "start": "1416269",
    "end": "1424009"
  },
  {
    "text": "messaging looks like through the firehose and I can convert that data if it's in JSON format into park' or o or",
    "start": "1424009",
    "end": "1432559"
  },
  {
    "text": "into o RC so depending on which columnar format you're leveraging you can do that and",
    "start": "1432559",
    "end": "1438609"
  },
  {
    "text": "some of the benefits of that is I can really simplify a lot of my ETL processing not having to leverage maybe",
    "start": "1438609",
    "end": "1445840"
  },
  {
    "text": "spinning up an EMR cluster using glue jobs to execute these things and if I have some simpler formats and not having",
    "start": "1445840",
    "end": "1453489"
  },
  {
    "text": "to do a lot of ETL transformation I can do that directly in the stream and land that data in park' format so it's ready",
    "start": "1453489",
    "end": "1460570"
  },
  {
    "text": "to be utilized by you know Athena or redshift thru spectrum or or EMR any",
    "start": "1460570",
    "end": "1467350"
  },
  {
    "text": "number of partner tools that we have out there and the nice thing is if any for any reason in any of those records fail",
    "start": "1467350",
    "end": "1474249"
  },
  {
    "text": "to be processed those records can be pushed into a prefix in the s3 bucket in a failed status so you can go and",
    "start": "1474249",
    "end": "1480850"
  },
  {
    "text": "investigate that deeper and then another option that came out again just a few",
    "start": "1480850",
    "end": "1486909"
  },
  {
    "text": "months ago is the ability to have custom s3 prefixes for that data running through your Kinesis data firehose so if",
    "start": "1486909",
    "end": "1494529"
  },
  {
    "text": "anybody's use the Kinesis data firehose before the the standard format of delivering data into s3 would be in a",
    "start": "1494529",
    "end": "1502539"
  },
  {
    "text": "year date month format with just those integer values within there and so they",
    "start": "1502539",
    "end": "1509440"
  },
  {
    "text": "weren't hive compliant prefixes to allow easy partition ability of the data for",
    "start": "1509440",
    "end": "1517720"
  },
  {
    "text": "many of these engines and so in this example here I can have that JSON data I can define the custom prefix and I'll",
    "start": "1517720",
    "end": "1524889"
  },
  {
    "text": "show you what that looks like in a moment and now when that data lands instead of just being in year month",
    "start": "1524889",
    "end": "1529960"
  },
  {
    "text": "format I'll have it in year equals of the year month equals the month date",
    "start": "1529960",
    "end": "1534999"
  },
  {
    "text": "equals the day and so on so that that's yet another step that I don't have to take to do some conversion after I've",
    "start": "1534999",
    "end": "1541690"
  },
  {
    "text": "streamed that data I can do all of that in line and so you'll have hive compatible partitioning for the naming",
    "start": "1541690",
    "end": "1548859"
  },
  {
    "text": "conventions of the data you're landing for your data like when leveraging s3 and that was a lot of words and no you",
    "start": "1548859",
    "end": "1556960"
  },
  {
    "text": "know if you're you do have a if you are going on a data Lake journey you know",
    "start": "1556960",
    "end": "1562210"
  },
  {
    "text": "you may understand some of the best practices around that in storing your data and a lot of times you may hear",
    "start": "1562210",
    "end": "1569210"
  },
  {
    "text": "gold silver bronze or you may hear raw data processed data curated data well a",
    "start": "1569210",
    "end": "1577040"
  },
  {
    "text": "lot of times you want to save that raw data and just a minute ago I said well do all these conversions and who cares",
    "start": "1577040",
    "end": "1583250"
  },
  {
    "text": "about the raw data well one of the things they've added is the ability to have a source record backup for that",
    "start": "1583250",
    "end": "1590390"
  },
  {
    "text": "Kinesis firehose so if I do a lot of that conversion converting into park' maybe stripping out some columns doing",
    "start": "1590390",
    "end": "1596930"
  },
  {
    "text": "all of that data manipulation that ETL inline i can still store the raw data in",
    "start": "1596930",
    "end": "1603590"
  },
  {
    "text": "another location so in this example here maybe I have a lambda transformation calling out to an enrichment service I",
    "start": "1603590",
    "end": "1610610"
  },
  {
    "text": "want to process that data put it in parque format but then finally I want to",
    "start": "1610610",
    "end": "1616670"
  },
  {
    "text": "be able to come back in and store that raw data for consumption by Yahoo ever",
    "start": "1616670",
    "end": "1623570"
  },
  {
    "text": "it could be your bi folks it could be a data scientist looking at the raw data",
    "start": "1623570",
    "end": "1628610"
  },
  {
    "text": "for a machine learning model they're looking to go after any number of of reasons to store that raw data and one",
    "start": "1628610",
    "end": "1636590"
  },
  {
    "text": "thing I don't have on on the slides but to really kind of call out is as you're storing that raw data right now I have",
    "start": "1636590",
    "end": "1643040"
  },
  {
    "text": "it shown in s3 you can always add our lifecycle transitions from the different",
    "start": "1643040",
    "end": "1649040"
  },
  {
    "text": "storage tiers that we have so maybe you keep that raw data in for 30 days 90 days but maybe then push that data into",
    "start": "1649040",
    "end": "1656840"
  },
  {
    "text": "glacier for long-term storage maybe you have a compliance reason that you need to do that you can set those lifecycle",
    "start": "1656840",
    "end": "1663800"
  },
  {
    "text": "policies in s3 so that you don't have to manually go manipulate and mess with the data right we want to really work on",
    "start": "1663800",
    "end": "1670880"
  },
  {
    "text": "having low touch performance of building streaming solutions into a data Lake and",
    "start": "1670880",
    "end": "1676990"
  },
  {
    "text": "so now what does that look like well the first thing I need to do as I showed you",
    "start": "1676990",
    "end": "1682280"
  },
  {
    "text": "I want to convert the data into park' and into doing that I need to use the glue data catalog so if I go into glue",
    "start": "1682280",
    "end": "1688880"
  },
  {
    "text": "again seoi sdk this always applies I can come in and I can create a database just",
    "start": "1688880",
    "end": "1694940"
  },
  {
    "text": "giving it a name web logs because I'm going to be processing some Apache web logs I can then use something like",
    "start": "1694940",
    "end": "1700880"
  },
  {
    "text": "Athena or I can use the Korea table API call with glue to create the DDL of my parquet version of those",
    "start": "1700880",
    "end": "1708409"
  },
  {
    "text": "streaming logs and so what you'll see here is I've given it a number of attributes that it has columns for",
    "start": "1708409",
    "end": "1714230"
  },
  {
    "text": "request path the request size the host address and then I've also included the",
    "start": "1714230",
    "end": "1719630"
  },
  {
    "text": "partition information inside of there so I want to partition this data by year-month-day an hour now that I have",
    "start": "1719630",
    "end": "1727190"
  },
  {
    "text": "that definition defined in the glue data catalog you're gonna see a table sorry a",
    "start": "1727190",
    "end": "1733220"
  },
  {
    "text": "database I can use that database I'm an ant table and specify that I'm",
    "start": "1733220",
    "end": "1739549"
  },
  {
    "text": "leveraging parquet and when I dig into the details of that I'll have all of that information up front to see where",
    "start": "1739549",
    "end": "1746720"
  },
  {
    "text": "I'm going to process that data in a particular s3 bucket I'm going to see that it's parquet I can see the full",
    "start": "1746720",
    "end": "1752779"
  },
  {
    "text": "schema and partitions of that table where I'm going to use Kinesis fire hose to land that data so I can pre prep all",
    "start": "1752779",
    "end": "1760789"
  },
  {
    "text": "of that stuff so now just like I did with the Kinesis data stream before",
    "start": "1760789",
    "end": "1766309"
  },
  {
    "text": "where I created it I gave it a name but in the data stream I was defining shards",
    "start": "1766309",
    "end": "1771320"
  },
  {
    "text": "because fire hose isn't have to work with that mechanism it's working on",
    "start": "1771320",
    "end": "1776840"
  },
  {
    "text": "persistent storage I'm going to then have an option for either directly putting data to this but even better yet",
    "start": "1776840",
    "end": "1784789"
  },
  {
    "text": "to be able to chain these solutions together I can select that data stream that I defined earlier and say anything",
    "start": "1784789",
    "end": "1791630"
  },
  {
    "text": "that gets streamed to there push it into this fire hose for me to do some work on it as well and the benefit of that is",
    "start": "1791630",
    "end": "1798440"
  },
  {
    "text": "with the Kinesis data stream I can hang off consumers to do real-time notification back to applications",
    "start": "1798440",
    "end": "1805390"
  },
  {
    "text": "real-time processing for any of that data coming in but I can also capture",
    "start": "1805390",
    "end": "1810799"
  },
  {
    "text": "all of that real-time data in the fire hose to persist out for later usage and",
    "start": "1810799",
    "end": "1819080"
  },
  {
    "text": "so the next step I'm going to do is I'm going to enable that record transformation because what I'm going to",
    "start": "1819080",
    "end": "1824779"
  },
  {
    "text": "be pushing through here is an Apache log and if we remember from a few slides ago those logs are in this row based format",
    "start": "1824779",
    "end": "1832399"
  },
  {
    "text": "and I want to take that I want to filter out some of the call that came in there to match the DDL that",
    "start": "1832399",
    "end": "1839120"
  },
  {
    "text": "I had created in the glue table and then converted into JSON so I can do the",
    "start": "1839120",
    "end": "1845360"
  },
  {
    "text": "automatic record transformation into parquet and so there's a lambda function with you know 15 lines of code to do",
    "start": "1845360",
    "end": "1852559"
  },
  {
    "text": "that processing and now I want to enable record conversion so I here I can enable",
    "start": "1852559",
    "end": "1859669"
  },
  {
    "text": "it and I have the solution options for parquet or leveraging oor C and then I",
    "start": "1859669",
    "end": "1868220"
  },
  {
    "text": "want to finally give it the location of where I want to land that data and show it the table metadata that I'm",
    "start": "1868220",
    "end": "1874370"
  },
  {
    "text": "leveraging in the catalog by selecting that web logs database selecting the P",
    "start": "1874370",
    "end": "1879799"
  },
  {
    "text": "underscore streaming underscore logs table and I want to use that as the latest version of the that I have in",
    "start": "1879799",
    "end": "1886100"
  },
  {
    "text": "there to match up when it's doing that Park a conversion because I'm doing the",
    "start": "1886100",
    "end": "1893149"
  },
  {
    "text": "park a conversion typically all of these would be enabled if I was just creating a standard data firehose here I only",
    "start": "1893149",
    "end": "1899809"
  },
  {
    "text": "have s3 enabled because support for that Park a conversion is only for s3 today so the other three options will be",
    "start": "1899809",
    "end": "1906529"
  },
  {
    "text": "grayed out for you I give it the bucket so I know where I want that bucket to be",
    "start": "1906529",
    "end": "1913039"
  },
  {
    "text": "I want to leverage those custom prefixes because I don't want just the year date",
    "start": "1913039",
    "end": "1918529"
  },
  {
    "text": "month in the prefixes I want to specifically call out year equals and",
    "start": "1918529",
    "end": "1923600"
  },
  {
    "text": "there's a variable that I can leverage called timestamp which is the approximate arrival timestamp of the",
    "start": "1923600",
    "end": "1929000"
  },
  {
    "text": "record that came into the Kinesis stream and I can use that by grabbing the year",
    "start": "1929000",
    "end": "1934279"
  },
  {
    "text": "the month of the day and I can also create there's another variable in there called random string so if I want to",
    "start": "1934279",
    "end": "1941149"
  },
  {
    "text": "generate a random string in that path when I'm landing it I can do that as well and when I do select these custom",
    "start": "1941149",
    "end": "1948559"
  },
  {
    "text": "prefixes I'll have to select an air prefix as well so this would be for any",
    "start": "1948559",
    "end": "1953600"
  },
  {
    "text": "of those Park a converted records that failed for any reason I can put those in",
    "start": "1953600",
    "end": "1961250"
  },
  {
    "text": "this failed prefix and decide how I want to process those you know secondarily after that",
    "start": "1961250",
    "end": "1968070"
  },
  {
    "text": "and now that I've done all of these different conversions I've basically thrown the kitchen sink at you of all of",
    "start": "1968070",
    "end": "1974970"
  },
  {
    "text": "the options that exist for the Kinesis data firehose I want to keep in mind that I also want to keep that source",
    "start": "1974970",
    "end": "1980279"
  },
  {
    "text": "data that that I had go into the stream originally and so I can also enable that",
    "start": "1980279",
    "end": "1986129"
  },
  {
    "text": "source record backup I'm going to select a bucket and then I'm going to select a prefix of where I want that data to land",
    "start": "1986129",
    "end": "1991919"
  },
  {
    "text": "and all of that once the the firehose is created will be taken care of for you from a configuration perspective you can",
    "start": "1991919",
    "end": "1998940"
  },
  {
    "text": "then just focus on the business of pushing your data from whatever ingestion sources you have and will take",
    "start": "1998940",
    "end": "2005419"
  },
  {
    "text": "care of making sure all of that process happens downstream and then with the",
    "start": "2005419",
    "end": "2011720"
  },
  {
    "text": "firehouse as I mentioned because we're not working with shards anymore we're working with buffering of those records",
    "start": "2011720",
    "end": "2017419"
  },
  {
    "text": "before persisting you're going to have a buffer size that you can define and you're gonna have a buffer interval and",
    "start": "2017419",
    "end": "2023899"
  },
  {
    "text": "the way it would work is whichever one it hits first is when that data is going",
    "start": "2023899",
    "end": "2029330"
  },
  {
    "text": "to get dumped to the persistent store that you've selected so if I have a 128",
    "start": "2029330",
    "end": "2034369"
  },
  {
    "text": "megabytes of data I filled up in less than 300 seconds that data would be persisted but if not we don't want to",
    "start": "2034369",
    "end": "2041239"
  },
  {
    "text": "leave that data just sitting in the buffer never filling up maybe because I'm not streaming a lot of data to it once I have any data inner if I hit that",
    "start": "2041239",
    "end": "2048770"
  },
  {
    "text": "five hour 30-second mark I'm gonna sorry 300 second mark I'm gonna persist that",
    "start": "2048770",
    "end": "2054408"
  },
  {
    "text": "data out so then I'm going to show you",
    "start": "2054409",
    "end": "2060049"
  },
  {
    "text": "this lovely picture of creating the stream that's going to go take all of those configuration items make sure",
    "start": "2060049",
    "end": "2065658"
  },
  {
    "text": "everything's ready for you it takes a couple of minutes to get the stream set up now maybe 30 to 30 seconds to 90",
    "start": "2065659",
    "end": "2071480"
  },
  {
    "text": "seconds and you're ready to go so now I can start streaming that data and after I hit those buffer interval",
    "start": "2071480",
    "end": "2078049"
  },
  {
    "text": "limits I then can take a look at you",
    "start": "2078049",
    "end": "2083388"
  },
  {
    "text": "know querying that data with things like Athena so now that data is in my Glu",
    "start": "2083389",
    "end": "2090530"
  },
  {
    "text": "data catalog I can see the web logs that I created that database that I created",
    "start": "2090530",
    "end": "2095809"
  },
  {
    "text": "the P underscore streaming logs table that I created and then through all of",
    "start": "2095809",
    "end": "2101000"
  },
  {
    "text": "that relation I have the results out leveraging Athena one of our server",
    "start": "2101000",
    "end": "2106580"
  },
  {
    "text": "lists ad-hoc query engines so that I can completely build out this server list",
    "start": "2106580",
    "end": "2112220"
  },
  {
    "text": "data like architecture and so to kind of",
    "start": "2112220",
    "end": "2117650"
  },
  {
    "text": "you know summarize all of the different options that we've leveraged there what",
    "start": "2117650",
    "end": "2122750"
  },
  {
    "text": "you can do is really build these server list data like congestion architectures by just leveraging the Kinesis suite of",
    "start": "2122750",
    "end": "2129109"
  },
  {
    "text": "tools here I have an Apache web server I can side load the Kinesis agent and",
    "start": "2129109",
    "end": "2134990"
  },
  {
    "text": "point to those Apache logs for the web server that I'm leveraging and I can",
    "start": "2134990",
    "end": "2140510"
  },
  {
    "text": "push those logs that in that raw form to the Kinesis data stream from there maybe",
    "start": "2140510",
    "end": "2148070"
  },
  {
    "text": "I have a consumer that I want to take a look at and filter out maybe just the",
    "start": "2148070",
    "end": "2153380"
  },
  {
    "text": "500 errors and you know report them to simple notification service or aggregate",
    "start": "2153380",
    "end": "2159050"
  },
  {
    "text": "those out and persist them somewhere else or feed those back into a web application a dashboard whatever the",
    "start": "2159050",
    "end": "2164840"
  },
  {
    "text": "case may be I have the capability of doing that but then as I mentioned I can",
    "start": "2164840",
    "end": "2170930"
  },
  {
    "text": "now connect a Kinesis data firehose to that Kinesis data stream through just",
    "start": "2170930",
    "end": "2177230"
  },
  {
    "text": "configuration so there's no infrastructure to maintain there's no patching you're doing you're literally",
    "start": "2177230",
    "end": "2182599"
  },
  {
    "text": "just configuring these resources that we're managing for you to do these actions I can utilize those lambda",
    "start": "2182599",
    "end": "2190640"
  },
  {
    "text": "functions for record transformation I've got a processing service that's doing some work once that comes back in I can",
    "start": "2190640",
    "end": "2199070"
  },
  {
    "text": "persist and convert that record into park' format leveraging those custom s3 prefixes so",
    "start": "2199070",
    "end": "2206810"
  },
  {
    "text": "now I have data in s3 ready to be utilized in that data Lake fashion and I",
    "start": "2206810",
    "end": "2212930"
  },
  {
    "text": "can also store that raw original Apache log data and that data can then be",
    "start": "2212930",
    "end": "2218599"
  },
  {
    "text": "utilized just like the park' data in your data Lake strategy that you're leveraging",
    "start": "2218599",
    "end": "2225670"
  },
  {
    "text": "and so I wanted to talk about all of these different options but I also did want to show finally that not only did",
    "start": "2230910",
    "end": "2239290"
  },
  {
    "text": "we build these for our customers to leverage we also utilize these in-house as well things like AWS metering the s3",
    "start": "2239290",
    "end": "2246190"
  },
  {
    "text": "events so we weren't only the creator of these but we also all clients of the",
    "start": "2246190",
    "end": "2251230"
  },
  {
    "text": "services that we build as well so we want to make sure that that they're meeting the expectations that you have when building these solutions so with",
    "start": "2251230",
    "end": "2260680"
  },
  {
    "text": "that that's all I had for today there's a demo link up there everyone take your pictures maybe I can jump in and it's a",
    "start": "2260680",
    "end": "2269410"
  },
  {
    "text": "jupiter notebook that's going to walk through each one of these steps that I walk through leveraging Python so that",
    "start": "2269410",
    "end": "2275680"
  },
  {
    "text": "you can go and exercise these in your own accounts and see how all of these things work so with that thank you very",
    "start": "2275680",
    "end": "2282070"
  },
  {
    "text": "much appreciate the time and enjoy the rest of the conference",
    "start": "2282070",
    "end": "2286800"
  }
]