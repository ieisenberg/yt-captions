[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "hey everyone thanks for attending our talk I know it's pretty late in the day you guys got some gambling to do some",
    "start": "299",
    "end": "5700"
  },
  {
    "text": "party to do so my name is Jason this is my close friend and colleague on this",
    "start": "5700",
    "end": "10860"
  },
  {
    "text": "project a parsec money and we also worked on this project with Leo Tam this",
    "start": "10860",
    "end": "16289"
  },
  {
    "text": "was originally a project for a class at Stanford University so we're both PhD students there and I'm in medical",
    "start": "16289",
    "end": "23220"
  },
  {
    "text": "imaging and machine learning Burien APAR works on machine learning in finance and",
    "start": "23220",
    "end": "28830"
  },
  {
    "text": "so this was a project that we worked with professors Jeff Coleman and andreas Pepe from the computer science",
    "start": "28830",
    "end": "34530"
  },
  {
    "text": "department there and we also were in contact with a ophthalmologist there whose name is Robert Chang and so this",
    "start": "34530",
    "end": "41879"
  },
  {
    "text": "project is about trying to take images of the eye and we're trying to play the",
    "start": "41879",
    "end": "47280"
  },
  {
    "text": "role of a radiologist and we're gonna use techniques from deep learning to automatically grade the severity of a",
    "start": "47280",
    "end": "53610"
  },
  {
    "text": "disease in these images of the eye so",
    "start": "53610",
    "end": "58800"
  },
  {
    "start": "58000",
    "end": "58000"
  },
  {
    "text": "the particular disease we're looking at is called diabetic retinopathy and this is a disease of the eye which affects up",
    "start": "58800",
    "end": "65549"
  },
  {
    "text": "close to 50% of diabetics across the world and this is almost 100 million",
    "start": "65549",
    "end": "71220"
  },
  {
    "text": "people across the world and a lot of these people are you know underserved",
    "start": "71220",
    "end": "76409"
  },
  {
    "text": "they're not actually they can't afford the health care and they can't afford to see an ophthalmologist and so what happens is a lot of these people fall",
    "start": "76409",
    "end": "82470"
  },
  {
    "text": "through the cracks and there's a need for automated screening tools to allow people to go in for screening and to",
    "start": "82470",
    "end": "88770"
  },
  {
    "text": "allow for more cheaper reviewing of these images so that you don't always have to go to an ophthalmologist who has",
    "start": "88770",
    "end": "95460"
  },
  {
    "text": "this panel up to five minutes for viewing an image at some you know a higher amount of cost and so what this",
    "start": "95460",
    "end": "103860"
  },
  {
    "text": "also allows you to do is to diagnose the disease earlier in the process and the disease process and that means that you",
    "start": "103860",
    "end": "110040"
  },
  {
    "text": "can intervene right when the disease is starting to lead to blindness and that",
    "start": "110040",
    "end": "115619"
  },
  {
    "text": "means that you can deliver more effective care and so having an automated system to be able to grade and",
    "start": "115619",
    "end": "122159"
  },
  {
    "text": "understand these images is really critical to trying to take the care for",
    "start": "122159",
    "end": "127409"
  },
  {
    "text": "diabetic retinopathy to the next level where we can really closely monitor to monitor the state",
    "start": "127409",
    "end": "132900"
  },
  {
    "text": "the disease throughout the lifespan of the subject so the the problem at hand",
    "start": "132900",
    "end": "139650"
  },
  {
    "text": "here is to take a fundus image so the fundus is just what we call the back of",
    "start": "139650",
    "end": "145170"
  },
  {
    "text": "the eye so this includes the retina your optic nerve and things like that and we're taking an image of that using a special camera and I'll show you an",
    "start": "145170",
    "end": "151860"
  },
  {
    "text": "image on the next slide of that and so once we have this image we want to figure out how severe is the disease on",
    "start": "151860",
    "end": "158579"
  },
  {
    "text": "this image and so the hallmarks of the disease are things like bleeds and other things that appear in the retina and so",
    "start": "158579",
    "end": "166819"
  },
  {
    "text": "you want to grade these images on a scale from zero to four where zero is",
    "start": "166819",
    "end": "171930"
  },
  {
    "text": "normal and four severe and this is a classification problem and more",
    "start": "171930",
    "end": "177299"
  },
  {
    "text": "specifically it's an ordinal classification problem and it's important to treat errors between making",
    "start": "177299",
    "end": "183870"
  },
  {
    "text": "a mistake between say oh you know say the true person is a normal and you say is a four that's obviously a really bad",
    "start": "183870",
    "end": "189359"
  },
  {
    "text": "mistake if you say he's a 1 that's a an okay mistake so some of the problems that we face in this project was how to",
    "start": "189359",
    "end": "197099"
  },
  {
    "text": "encode that sort of that sort of loss function into into training the network",
    "start": "197099",
    "end": "202230"
  },
  {
    "text": "and so the performance metric we use is something called quadratic weighted",
    "start": "202230",
    "end": "207299"
  },
  {
    "text": "Kappa and this is something that penalizes greater mistakes so if you have a mistake of a difference of four",
    "start": "207299",
    "end": "213569"
  },
  {
    "text": "then you're going to be squaring the the penalty associated with that and so here are examples of the images that are",
    "start": "213569",
    "end": "220819"
  },
  {
    "start": "219000",
    "end": "219000"
  },
  {
    "text": "acquired and so these are fundus images acquired with a fundus camera and you can see on the left is an example of a",
    "start": "220819",
    "end": "227940"
  },
  {
    "text": "normal eye and you can see this bright red sari bright circle white white",
    "start": "227940",
    "end": "233220"
  },
  {
    "text": "circle on the right side of the image is where the optic nerve attaches to your exterior retina and so from this",
    "start": "233220",
    "end": "240900"
  },
  {
    "text": "radiates a bunch of blood vessels so you can see these red structures curving around the eye and these are the blood",
    "start": "240900",
    "end": "247109"
  },
  {
    "text": "vessels that feed blood to your retina and the dark thing in the center is your fovea so that's where all your cones are",
    "start": "247109",
    "end": "252419"
  },
  {
    "text": "where you have your focused vision which I used to read and look at things and so",
    "start": "252419",
    "end": "258660"
  },
  {
    "text": "what happens with that diabetic retinopathy is if you have diabetes you have an elevated glucose level",
    "start": "258660",
    "end": "264150"
  },
  {
    "text": "you're in your blood and this causes the vessels the walls of your blood vessels to thin and sometimes they can actually",
    "start": "264150",
    "end": "270810"
  },
  {
    "text": "crack and bleed this leads to hemorrhages into your retina and so you",
    "start": "270810",
    "end": "275940"
  },
  {
    "text": "can see on the right image which is an example of a severe patient you can see",
    "start": "275940",
    "end": "281610"
  },
  {
    "text": "that there's a bunch of red blotches in the image and these are actually blood that has leaked out from the the vessels",
    "start": "281610",
    "end": "287610"
  },
  {
    "text": "and the positive themselves onto the retina and so once they start to encroach on the dark metal spot the",
    "start": "287610",
    "end": "292830"
  },
  {
    "text": "fovea that's when you can start seeing these you know black spots in your",
    "start": "292830",
    "end": "297870"
  },
  {
    "text": "vision and you actually become blind if they affect it to a severe level so our",
    "start": "297870",
    "end": "305639"
  },
  {
    "text": "data comes from the cattle competition and this was a competition funded by the California Health Care Foundation as",
    "start": "305639",
    "end": "311130"
  },
  {
    "text": "well as AI PACs and our data consists of 35,000 training images and 54,000 test",
    "start": "311130",
    "end": "316650"
  },
  {
    "text": "images so the test set is quite a bit bigger and this is often common in medical medical problems because you're",
    "start": "316650",
    "end": "322800"
  },
  {
    "text": "trying to say that you have an algorithm that generalizes to a much larger population come than your training set",
    "start": "322800",
    "end": "329150"
  },
  {
    "text": "interestingly this data set this data is very high-resolution compared to other computer vision tasks and on the order",
    "start": "329150",
    "end": "336750"
  },
  {
    "text": "of something like 4 megapixels so that's the first Chandler's challenge we faced",
    "start": "336750",
    "end": "342240"
  },
  {
    "start": "339000",
    "end": "339000"
  },
  {
    "text": "with with this problem it was trying to tackle this problem so in something like an image net problem challenge which is",
    "start": "342240",
    "end": "349889"
  },
  {
    "text": "a general computer vision challenge that they run every year where they're trying to recognize general objects things like",
    "start": "349889",
    "end": "355800"
  },
  {
    "text": "cats dogs planes cars things like that and what they're using what they're using there is kind of like thumbnail",
    "start": "355800",
    "end": "362250"
  },
  {
    "text": "images of what the what the object is and so a lot of the techniques that were",
    "start": "362250",
    "end": "367889"
  },
  {
    "text": "using come from things that are architectures and techniques that have been learned in that that challenge and",
    "start": "367889",
    "end": "374520"
  },
  {
    "text": "we're trying to apply them to this problem here and so the first comment we face is how do we adapt these techniques",
    "start": "374520",
    "end": "380430"
  },
  {
    "text": "to high resolution so here are just a",
    "start": "380430",
    "end": "387810"
  },
  {
    "text": "bunch of thumbnails showing the various different classes of severity of the disease here and so this is just to",
    "start": "387810",
    "end": "394289"
  },
  {
    "text": "highlight the point that we really do need the high resolution just so you can kind of see like the zero one and to kind of look similar to",
    "start": "394289",
    "end": "400110"
  },
  {
    "text": "each other might be hard to see on these images so you can't really tell it is between zero through two they all kind",
    "start": "400110",
    "end": "405960"
  },
  {
    "text": "of look the same when you're looking at low resolution images event threes and fours you can see there's kind of like a",
    "start": "405960",
    "end": "412580"
  },
  {
    "text": "yellow spot you can see above the fovea and the dark spot that's actually a fat",
    "start": "412580",
    "end": "417630"
  },
  {
    "text": "deposit and in the in the for you can see there's sound of kind of like a action kind of gray things and those are",
    "start": "417630",
    "end": "425160"
  },
  {
    "text": "actually scars from treatment so you can kind of see differentiate the more severe forms of disease that is classed",
    "start": "425160",
    "end": "430860"
  },
  {
    "text": "as Susan fours but from zero to two you really need the high-res data to to see what's going on right so they're yours",
    "start": "430860",
    "end": "439890"
  },
  {
    "text": "these are the sorts of lesions that are associated with the disease so the blood",
    "start": "439890",
    "end": "445919"
  },
  {
    "text": "vessel will crack and things like fat will deposit into the until right now these are called the exudates or the yellow spots that you see and then",
    "start": "445919",
    "end": "452520"
  },
  {
    "text": "there's also hemorrhages which are bleeds or that then clot in forming to form into blotches on the on the retina",
    "start": "452520",
    "end": "461300"
  },
  {
    "text": "so there's a bunch of criteria associated with how you actually grade these things these images and we don't",
    "start": "461300",
    "end": "468750"
  },
  {
    "text": "specifically encode these in the in the training procedure but we want the algorithm to learn what these these",
    "start": "468750",
    "end": "476430"
  },
  {
    "text": "guidelines are from just from the data itself and so as with any data centers",
    "start": "476430",
    "end": "482880"
  },
  {
    "text": "it's subject to how well it was labeled and there is some disagreement among",
    "start": "482880",
    "end": "487919"
  },
  {
    "text": "amongst doctors about how to create these images even though there were guidelines here some of these are kind",
    "start": "487919",
    "end": "493169"
  },
  {
    "text": "of vague and it's some of the decisions made by doctors varies and so we spoke",
    "start": "493169",
    "end": "498900"
  },
  {
    "text": "to Robert Chang who was the doctor that we worked with on this project and he confirmed some of the things that we",
    "start": "498900",
    "end": "504390"
  },
  {
    "text": "pointed out like hey this actually looks like you know more severe than what it was labeled and then he was like yeah that's actually true so there's actually",
    "start": "504390",
    "end": "510930"
  },
  {
    "text": "definitely problems with the data and we sort of have to live with it at this stage also there so this image this data",
    "start": "510930",
    "end": "519900"
  },
  {
    "text": "set also has a lot of artifacts associated with the images and these are so what happens is when the patient",
    "start": "519900",
    "end": "527430"
  },
  {
    "text": "comes in he'll get these photos take them with a care so you have all the sorts of artifacts and related with photography so things",
    "start": "527430",
    "end": "534670"
  },
  {
    "text": "like lens flare things like so that you have to use of flash so things like reflections off the front of the eye so",
    "start": "534670",
    "end": "541030"
  },
  {
    "text": "those leads to like the haloing effects that you kind of see and see the so these are all sorts of variances in the",
    "start": "541030",
    "end": "547300"
  },
  {
    "text": "image that the network has to learn to be a robust to and that's what makes this challenge especially child",
    "start": "547300",
    "end": "552670"
  },
  {
    "text": "especially difficult compared to maybe literature methods that in this space where they were using very pristine",
    "start": "552670",
    "end": "559300"
  },
  {
    "text": "images and using standard image processing techniques to try to do diagnosis right so our performance",
    "start": "559300",
    "end": "567130"
  },
  {
    "text": "metric is something called quadratic weighted cap by and this is something that has a squared penalty law so for",
    "start": "567130",
    "end": "572200"
  },
  {
    "text": "example if the true class of an image is one then if we predict that it's one",
    "start": "572200",
    "end": "578620"
  },
  {
    "text": "that we don't have any penalty associated with that if we predict that it's a two then there's a penalty of one",
    "start": "578620",
    "end": "584200"
  },
  {
    "text": "if we predict that it's a three then it's a penalty of two squared which is four and so on and so on as you go as you predict zero or four so",
    "start": "584200",
    "end": "594490"
  },
  {
    "text": "this is one of the main challenges here for the problem how do we encode this sort of squared error into training the",
    "start": "594490",
    "end": "601690"
  },
  {
    "text": "network another big problem that we faced was and this is also common in the",
    "start": "601690",
    "end": "606880"
  },
  {
    "text": "medical space it was the severe class imbalance associated with having way too many normals so most people are normal",
    "start": "606880",
    "end": "613060"
  },
  {
    "text": "and most people will have good eyes and so over seventy percent of our data is actually in the zero class and so if you",
    "start": "613060",
    "end": "620020"
  },
  {
    "text": "sort of try trick try to train the network naively without consideration of",
    "start": "620020",
    "end": "625990"
  },
  {
    "text": "this class imbalance it sort of tries to just predict all zeroes because you predict all zeroes you have seventy",
    "start": "625990",
    "end": "631000"
  },
  {
    "text": "percent accuracy just by that and obviously that's not really helpful in any way also commenting medical data",
    "start": "631000",
    "end": "638410"
  },
  {
    "text": "sets is that we have too few examples so in this case you saw that for the more severe forms of disease threes and fours",
    "start": "638410",
    "end": "644350"
  },
  {
    "text": "we only have we have less than a thousand images something like 600 or 700 hundred percent images some 500",
    "start": "644350",
    "end": "651160"
  },
  {
    "text": "images and so yeah so what we'll do here",
    "start": "651160",
    "end": "656560"
  },
  {
    "text": "is try to take advantage of using things techniques like transfer learning to sort of scrap the issue of our limited",
    "start": "656560",
    "end": "662550"
  },
  {
    "text": "data so we did explore conventional techniques where you sort of hand design it's a bunch of features so you want to",
    "start": "662550",
    "end": "669090"
  },
  {
    "start": "663000",
    "end": "663000"
  },
  {
    "text": "and these images that important things are things like trying to identify where the vessels are because the the bleeds",
    "start": "669090",
    "end": "675450"
  },
  {
    "text": "and stuff usually typically are close to the vessels because that's where they originate from and then you want to find",
    "start": "675450",
    "end": "682740"
  },
  {
    "text": "out where the the bright spots are because those could be fat deposits or but that could also be the optic disk so",
    "start": "682740",
    "end": "687780"
  },
  {
    "text": "these are the sorts of features that you try to learn from and you can build simple models for like SVM from those",
    "start": "687780",
    "end": "693450"
  },
  {
    "text": "features and generally it didn't work very well so we moved on to more I guess",
    "start": "693450",
    "end": "701220"
  },
  {
    "text": "modern approaches with a deep learning convolutional neural networks so in",
    "start": "701220",
    "end": "706440"
  },
  {
    "start": "703000",
    "end": "703000"
  },
  {
    "text": "order to feed data to a convolutional neural neural network you need to well",
    "start": "706440",
    "end": "712470"
  },
  {
    "text": "one thing you can do is to try to normalize the data as much as you can before hand and so this is instead of",
    "start": "712470",
    "end": "719070"
  },
  {
    "text": "like trying to have the network learn some of these variances you can try to a lie or normalize out these variances",
    "start": "719070",
    "end": "725460"
  },
  {
    "text": "beforehand so you can sort of use human knowledge to prevent so that you don't",
    "start": "725460",
    "end": "730710"
  },
  {
    "text": "have to rely on that network to learn these more easier parts of the things that you can account for yourself and so",
    "start": "730710",
    "end": "736260"
  },
  {
    "start": "736000",
    "end": "736000"
  },
  {
    "text": "specifically we do something called registration so this is like if you have a name age what happens is you have like",
    "start": "736260",
    "end": "742500"
  },
  {
    "text": "a circle in this image and that's where the fundus is and this can be anywhere inside you know this black square of the",
    "start": "742500",
    "end": "748200"
  },
  {
    "text": "image and so we have to find where that is and sort of Center them across all our all the data that we have and so we",
    "start": "748200",
    "end": "753810"
  },
  {
    "text": "do that with a simple image processing technique called top circles which is just essentially gives you the center",
    "start": "753810",
    "end": "759750"
  },
  {
    "text": "and radius of the circle and then we can then shift the circle across all the images so that they're all hanging on",
    "start": "759750",
    "end": "765030"
  },
  {
    "text": "top of each other at the same spot we also do color correction so for",
    "start": "765030",
    "end": "770970"
  },
  {
    "text": "different people of different races they'll have different color retinas so I think this is probably a Caucasian",
    "start": "770970",
    "end": "776520"
  },
  {
    "text": "person and so we can we sort of try to correct for that a little bit as much as",
    "start": "776520",
    "end": "782700"
  },
  {
    "text": "we can so that they're closer in you between different people we also do a",
    "start": "782700",
    "end": "788130"
  },
  {
    "text": "standard normalization to zero mean and unit variance just so that",
    "start": "788130",
    "end": "793380"
  },
  {
    "text": "it's well more well suited for processing by that convolutional neural",
    "start": "793380",
    "end": "799050"
  },
  {
    "start": "799000",
    "end": "799000"
  },
  {
    "text": "network so what our cop confidence or convolutional neural network so a comrade is basically one branch of of",
    "start": "799050",
    "end": "807140"
  },
  {
    "text": "deep learning or neural network architectures so there's many different types of convolutional neural network",
    "start": "807140",
    "end": "813900"
  },
  {
    "text": "architectures and these are particularly well suited for vision tasks and so since this is a vision vision task this",
    "start": "813900",
    "end": "821010"
  },
  {
    "text": "is the kind of architecture that we pursued for this problem and so there",
    "start": "821010",
    "end": "828290"
  },
  {
    "text": "most of the the sorts of architectures for these come from the image net",
    "start": "828290",
    "end": "834540"
  },
  {
    "text": "challenge and so you may have heard of things like res net or Google net or alex net way back in the day in 2012 and",
    "start": "834540",
    "end": "842030"
  },
  {
    "text": "so the these are sort of the ones the hallmark networks that have been very",
    "start": "842030",
    "end": "847950"
  },
  {
    "text": "successful for large-scale image recognition challenges where you're trying to identify a thousand different",
    "start": "847950",
    "end": "853020"
  },
  {
    "text": "objects from each other and so these datasets are things on the order of what",
    "start": "853020",
    "end": "858090"
  },
  {
    "text": "1 million images and so because of that they can train a lot more print we can have models that are a lot larger and",
    "start": "858090",
    "end": "863940"
  },
  {
    "text": "deeper than with a smaller data set and so those models can be up to like a hundred million parameters in size so in",
    "start": "863940",
    "end": "871260"
  },
  {
    "text": "our case we had to take in consideration how much smaller a gate it was that is on the order of you know tens of thousand fifty thousand two hundred",
    "start": "871260",
    "end": "877230"
  },
  {
    "text": "thousand images if you take the training gauges the test data as well but anyway",
    "start": "877230",
    "end": "882860"
  },
  {
    "text": "so we use something called the network and network architecture which was published by million in 2014 and this is",
    "start": "882860",
    "end": "890010"
  },
  {
    "text": "an architecture that has a much reduced set of parameters to learn and we reduce it even further for our problem and the",
    "start": "890010",
    "end": "897960"
  },
  {
    "text": "key thing here is that it avoids something called fully connected layers which least well is generally a large",
    "start": "897960",
    "end": "903540"
  },
  {
    "text": "percentage of what you have to learn in traditional networks like Alex Knight and so so what we do is we sort of take",
    "start": "903540",
    "end": "912930"
  },
  {
    "text": "the the base lower layers of the network and we keep that structure and we modify",
    "start": "912930",
    "end": "919770"
  },
  {
    "text": "this higher-level structure so that it's adapted to a five class classification problem as opposed to a",
    "start": "919770",
    "end": "924779"
  },
  {
    "text": "thousand class classification problem right so as I was saying before data is",
    "start": "924779",
    "end": "931230"
  },
  {
    "text": "data is the problem here so we have so little data that we can't really train the whole network by itself because",
    "start": "931230",
    "end": "937889"
  },
  {
    "text": "there's still so many parameters 2.2 million parameters for you know 35,000 images that we have examples of and so",
    "start": "937889",
    "end": "944879"
  },
  {
    "text": "what we can do is we can take the knowledge learn from another problem like image net and transfer learning",
    "start": "944879",
    "end": "950879"
  },
  {
    "text": "which is reduced means we take literally the numbers that are so that we're learned in that problem and carry it",
    "start": "950879",
    "end": "956100"
  },
  {
    "text": "over to this problem and we use that as an initialization point for the network and so a lot of the heuristic reasoning",
    "start": "956100",
    "end": "965639"
  },
  {
    "text": "for this is because in most vision tasks the lower level features are associated with sort of basic image understanding",
    "start": "965639",
    "end": "970949"
  },
  {
    "text": "so things like trying to find the edges in image image trying to identify gradients in the image things like that",
    "start": "970949",
    "end": "976350"
  },
  {
    "text": "so these are all very low level image processing tasks and so these are",
    "start": "976350",
    "end": "982199"
  },
  {
    "text": "effectively thought to be transferable across many different problems and so we",
    "start": "982199",
    "end": "993089"
  },
  {
    "text": "also so similarly for that reason because those those lower level features are fairly consistent we don't learn",
    "start": "993089",
    "end": "999089"
  },
  {
    "text": "them what we use we don't adjust them very much we use a variable learning way rate across the network",
    "start": "999089",
    "end": "1005350"
  },
  {
    "text": "similarly we try to use other deep",
    "start": "1005350",
    "end": "1011269"
  },
  {
    "text": "learning tricks like dropout and early stopping as well as data augmentation to combat this problem of overfitting",
    "start": "1011269",
    "end": "1018499"
  },
  {
    "text": "because we have so many parameters and reduce the amount of data so so right so",
    "start": "1018499",
    "end": "1028308"
  },
  {
    "start": "1022000",
    "end": "1022000"
  },
  {
    "text": "what we wanted to do is we want to work on the high resolution images and that's because we know that the the disease is",
    "start": "1028309",
    "end": "1034010"
  },
  {
    "text": "about it's about finding these small little bleeds in the image and but there are other things that we can do before",
    "start": "1034010",
    "end": "1039860"
  },
  {
    "text": "we try to tackle the the higher resolution images which require more resources and so this project is kind of",
    "start": "1039860",
    "end": "1046399"
  },
  {
    "text": "like so when we did this project we were given three thousand dollars in credits for the class from the AWS educate",
    "start": "1046399",
    "end": "1052490"
  },
  {
    "text": "program and so this project is all about how do you budget that you know very effective manner so that you can do",
    "start": "1052490",
    "end": "1058040"
  },
  {
    "text": "deep learning in as cost effective manner as possible and so what we our",
    "start": "1058040",
    "end": "1063500"
  },
  {
    "text": "idea was we wanted to strategy at strategizes as much as we could at low resolution and so what we realized was",
    "start": "1063500",
    "end": "1070250"
  },
  {
    "text": "that there are parts of the problem that we can model at low resolution before we",
    "start": "1070250",
    "end": "1075440"
  },
  {
    "text": "try to tackle the high resolution stuff and so this is things like trying to accurately model the performance metrics",
    "start": "1075440",
    "end": "1080570"
  },
  {
    "text": "so we want to use a mean squared error loss function ideally because this is more accurately models that quadratic",
    "start": "1080570",
    "end": "1086900"
  },
  {
    "text": "created kappa performance metric that we're trying to target and there's we can also deal with the issue of trying",
    "start": "1086900",
    "end": "1093140"
  },
  {
    "text": "to model the statistics of the population and particularly the class and balance problem that is the",
    "start": "1093140",
    "end": "1098230"
  },
  {
    "text": "dominance of zeroes in our in our data set and so we can wear our idea was we",
    "start": "1098230",
    "end": "1104240"
  },
  {
    "text": "can train the low resolution really fast and really cheaply on AWS and we can",
    "start": "1104240",
    "end": "1109310"
  },
  {
    "text": "then figure explore a bunch of different ideas about how to you know attack of these two problem modeling the",
    "start": "1109310",
    "end": "1114980"
  },
  {
    "text": "performance and modeling the statistics and then once we found the correct solution a low resolution then we can just straightly apply this method out",
    "start": "1114980",
    "end": "1121250"
  },
  {
    "text": "the methodology we learned or the strategy we learned to the high res and so in this way we kind of we kind of",
    "start": "1121250",
    "end": "1126980"
  },
  {
    "text": "have a cost effective solution to figure out the best way to de train the high resolution images so what we do is we",
    "start": "1126980",
    "end": "1134240"
  },
  {
    "text": "start out with the we start out with the",
    "start": "1134240",
    "end": "1140840"
  },
  {
    "text": "the full network and we take the lower layers and we just freeze them a title we don't do anything with them we just leave them as they are and so we're",
    "start": "1140840",
    "end": "1147950"
  },
  {
    "text": "essentially just trying to learn three little small convolution layers on top of all of the features learn all the",
    "start": "1147950",
    "end": "1153530"
  },
  {
    "text": "features are carried over from image net and so what we did was we tried to start",
    "start": "1153530",
    "end": "1160610"
  },
  {
    "text": "off with you know the thing closes the problem that we're trying to solve we want a quadratic error with mean squared error and we want a sampling",
    "start": "1160610",
    "end": "1167180"
  },
  {
    "text": "distribution that matches that you did the true distribution of the population and this doesn't learn at all it tries",
    "start": "1167180",
    "end": "1172490"
  },
  {
    "text": "to break ellezi rows similarly for if you change the sampling rate or sampling so that it's even",
    "start": "1172490",
    "end": "1178790"
  },
  {
    "text": "across all the classes it doesn't do anything as well and it's actually been shown that using a mean squared error or",
    "start": "1178790",
    "end": "1183980"
  },
  {
    "text": "loss one this is a an ill-posed problem for training these sorts of networks so a",
    "start": "1183980",
    "end": "1190700"
  },
  {
    "text": "more traditional loss function to use in object recognition is something called negative log likelihood and this is a penalty that associate has no additional",
    "start": "1190700",
    "end": "1198500"
  },
  {
    "text": "penalty associated with different classes so if you say a zero is a four that's the same as saying that zero is a one it doesn't matter to it and this is it",
    "start": "1198500",
    "end": "1204740"
  },
  {
    "text": "makes sense for things like you know cats versus dogs or playing versus cats and stuff and any mistake is all the",
    "start": "1204740",
    "end": "1209900"
  },
  {
    "text": "same as old and I don't really know how to distinguish between which is the worst mistake or what but so in this",
    "start": "1209900",
    "end": "1217280"
  },
  {
    "text": "case it's not it doesn't quite model the problem we want but it is able to learn something here it doesn't reach a Kappa",
    "start": "1217280",
    "end": "1222350"
  },
  {
    "text": "of 0.1 instead of zero and this is what the true sampling distribution so we",
    "start": "1222350",
    "end": "1229190"
  },
  {
    "text": "then change the sampling distribution so that it's even across all the classes so that you know it's able to get a flavor of all the different classes able to",
    "start": "1229190",
    "end": "1235370"
  },
  {
    "text": "learn something about the threes and fours even though we don't have too many of them otherwise and so it is able to",
    "start": "1235370",
    "end": "1241070"
  },
  {
    "text": "get something somewhere some progress there with a capital of 0.29 okay so",
    "start": "1241070",
    "end": "1247040"
  },
  {
    "text": "once we're at that stage then we say okay okay let's we've learned these top",
    "start": "1247040",
    "end": "1252950"
  },
  {
    "text": "little layers these small top layers a bit now let's take the whole bottom bottom stack of the network and do",
    "start": "1252950",
    "end": "1259340"
  },
  {
    "text": "something with them so we apply really low learning rate to them 1/100 of the scale of the higher low level layers and",
    "start": "1259340",
    "end": "1264980"
  },
  {
    "text": "so we start with what we had before which is negative log likelihood as the class function and then an even sound",
    "start": "1264980",
    "end": "1271100"
  },
  {
    "text": "like distribution we trained the whole network now and so now we can just by",
    "start": "1271100",
    "end": "1276560"
  },
  {
    "text": "doing that you can get a huge jump in performance going to 0.42 and then what we do is during the training procedure",
    "start": "1276560",
    "end": "1282320"
  },
  {
    "text": "we then slowly morph the sampling distribution from an even sampling scheme to the true sampling scheme so",
    "start": "1282320",
    "end": "1287390"
  },
  {
    "text": "we're sort of slowly easy it it slowly easing it into the process of seeing more and more zeros and so that it",
    "start": "1287390",
    "end": "1293780"
  },
  {
    "text": "doesn't really forget about all the threes and fours that I learned about and when we were initially sampling them with a lot more problem a lot higher",
    "start": "1293780",
    "end": "1301070"
  },
  {
    "text": "likelihood and when you do this you have another jump in performance similarly we",
    "start": "1301070",
    "end": "1308450"
  },
  {
    "text": "have we then augment our cost function with the mean squared error cost function so we have both negative log",
    "start": "1308450",
    "end": "1314780"
  },
  {
    "text": "likelihood and means err penalty when we sort of shift the weight from negative log likelihood",
    "start": "1314780",
    "end": "1320389"
  },
  {
    "text": "toward mean squared error and again slowly through the training process we slowly morph the cost function more and",
    "start": "1320389",
    "end": "1328159"
  },
  {
    "text": "more towards mean squared error and so then now we have arrived arrived at the original situation that we wanted to do",
    "start": "1328159",
    "end": "1333259"
  },
  {
    "text": "which is a good criterion that models the performance metric we want so that has quadratic weighted error has a has a",
    "start": "1333259",
    "end": "1341690"
  },
  {
    "text": "quadratic weighted penalty and we have the sampling distribution that is",
    "start": "1341690",
    "end": "1346759"
  },
  {
    "text": "representative of the true population and so this is another ways to another jumping performance and so you can see",
    "start": "1346759",
    "end": "1354950"
  },
  {
    "start": "1354000",
    "end": "1354000"
  },
  {
    "text": "that the sort of progression we went through here we went from you know the first set of so you know this we",
    "start": "1354950",
    "end": "1360919"
  },
  {
    "text": "explored a lot more ideas than what's just shown here but this is effectively the methodology that we learned after experimenting with a lot of cheap low",
    "start": "1360919",
    "end": "1367669"
  },
  {
    "text": "cost training nodes of low resolution and so then you can just directly apply this you know stepwise stages of",
    "start": "1367669",
    "end": "1374539"
  },
  {
    "text": "training and you can apply this to the high resolution and you can see that there's almost a parallel performance",
    "start": "1374539",
    "end": "1380769"
  },
  {
    "text": "increased as you follow this training strategy of course there's also a huge gap there just by going from low res the",
    "start": "1380769",
    "end": "1386659"
  },
  {
    "text": "high res and that's because you're actually lowering the image features that are associated with the disease which is you know trying to find these little top leaves and stuff in the in",
    "start": "1386659",
    "end": "1393019"
  },
  {
    "text": "NBI so with that I'll pass the part who will talk about our computing and some of",
    "start": "1393019",
    "end": "1398750"
  },
  {
    "start": "1395000",
    "end": "1395000"
  },
  {
    "text": "more novel ideas we had thanks okay so",
    "start": "1398750",
    "end": "1405529"
  },
  {
    "text": "so why Jason described so far was essentially a conventional convolution neural network and we're not the issues",
    "start": "1405529",
    "end": "1413809"
  },
  {
    "text": "as Jason again mentioned was that we have this whole problem with the image",
    "start": "1413809",
    "end": "1419330"
  },
  {
    "text": "size you know images that we want to use for diagnosis are about 2 K by 2 K which we",
    "start": "1419330",
    "end": "1425629"
  },
  {
    "text": "actually trained using images of size 2 to 4 by 2 to 4 and then also with 1k by",
    "start": "1425629",
    "end": "1430850"
  },
  {
    "text": "1k so it a lot of our a lot of our design decisions are based on",
    "start": "1430850",
    "end": "1436429"
  },
  {
    "text": "essentially the computing constraints and so what I'll describe in the next few slides would be the computing",
    "start": "1436429",
    "end": "1442519"
  },
  {
    "text": "infrastructure that we've used for the experiments so far and after that I'll mainly talk about the new architecture",
    "start": "1442519",
    "end": "1449059"
  },
  {
    "text": "that we came up that that uses in all of the stuff mentioned so far as well as some",
    "start": "1449059",
    "end": "1454400"
  },
  {
    "text": "some other sauce and so that's the that's the pattern for the rest for the",
    "start": "1454400",
    "end": "1460130"
  },
  {
    "text": "remainder of the talk so in terms of the compute in terms of the computing set up",
    "start": "1460130",
    "end": "1466120"
  },
  {
    "text": "what we have is we've used AWS extensively partly because you know we",
    "start": "1466120",
    "end": "1471380"
  },
  {
    "text": "were supported by the educate program and so we had credits and in particular we've used the Elastic Compute cloud",
    "start": "1471380",
    "end": "1477020"
  },
  {
    "text": "service we've made use of all kinds of facilities with this service you know GP",
    "start": "1477020",
    "end": "1482600"
  },
  {
    "text": "nodes have been used extensively and the main reason is that for computer vision tasks where you want to apply the same",
    "start": "1482600",
    "end": "1489860"
  },
  {
    "text": "operation there are a lot of different places in the image you won't need higher hub you need a lot of parallel processing and with GPUs you can",
    "start": "1489860",
    "end": "1496130"
  },
  {
    "text": "essentially get a 10x 100x speed up reviews to be PC which is virtual private cluster we use start cluster to",
    "start": "1496130",
    "end": "1503570"
  },
  {
    "text": "set this up and we've used EBS optimized instances because as I'll talk about next",
    "start": "1503570",
    "end": "1508760"
  },
  {
    "text": "we were very constrained on the bandwidth if you wanted to let's say run ten experiments it was sort of no good",
    "start": "1508760",
    "end": "1513860"
  },
  {
    "text": "way to do it and you know this is one architecture that we use also to keep the costs down we use something called",
    "start": "1513860",
    "end": "1520490"
  },
  {
    "text": "spot nodes which is essentially like nodes offered by Amazon and pretty cheap rates I guess some of you already know",
    "start": "1520490",
    "end": "1526670"
  },
  {
    "text": "about it so and they can be taken away from you at that discretion so in terms",
    "start": "1526670",
    "end": "1532460"
  },
  {
    "text": "of the noise review single GPU node for the smaller images two to four by two to four images and for the bigger images",
    "start": "1532460",
    "end": "1539450"
  },
  {
    "text": "which are 1k by 1k which is still down sampled from the 2k by 2k images that we want to use we've",
    "start": "1539450",
    "end": "1546500"
  },
  {
    "text": "used the multi-gpu nodes which is G 2.8 X large we've also used storage options",
    "start": "1546500",
    "end": "1553610"
  },
  {
    "text": "from Amazon EBS basically for all our training data s3 for snapshotting which I'll talk about next in terms in terms",
    "start": "1553610",
    "end": "1561290"
  },
  {
    "text": "of the software we've used Python for processing data processing pretty much for all our computing needs and then",
    "start": "1561290",
    "end": "1568220"
  },
  {
    "text": "specifically for training these deep neural networks we've used the library torch which is one of the fastest",
    "start": "1568220",
    "end": "1576350"
  },
  {
    "text": "libraries today so so here's our computing setup what we have is we have",
    "start": "1576350",
    "end": "1582440"
  },
  {
    "text": "data residing gv2 EBS volume and we simply use that",
    "start": "1582440",
    "end": "1587510"
  },
  {
    "text": "data to and we attach that lets using your multi-gpu annoyed and perform our",
    "start": "1587510",
    "end": "1594020"
  },
  {
    "text": "experiments on that so we have this set up that this sort of a setup that we use for developing our code and then if we",
    "start": "1594020",
    "end": "1601370"
  },
  {
    "text": "wanna run experiments while continuing development on on the other side on the",
    "start": "1601370",
    "end": "1606860"
  },
  {
    "text": "right side of the slide you can see we just create a replica of this data so the thing is we keep updating this data",
    "start": "1606860",
    "end": "1613190"
  },
  {
    "text": "you know through that through the as the product project progress and so",
    "start": "1613190",
    "end": "1619070"
  },
  {
    "text": "snapshotting was great because you know we transfer bulk of the data initially and then you only transfer little",
    "start": "1619070",
    "end": "1625429"
  },
  {
    "text": "increments at a time as you make changes to your data set and so that data data to there which is again a GP to EBS we",
    "start": "1625429",
    "end": "1632120"
  },
  {
    "text": "use that for conducting all our experiments and so specifically what what we did was we set up a cluster",
    "start": "1632120",
    "end": "1637400"
  },
  {
    "text": "using star cluster on ec2 so there are a bunch of nodes there and there would be",
    "start": "1637400",
    "end": "1643340"
  },
  {
    "text": "a master node sitting in between word to which we hook up the data and then this",
    "start": "1643340",
    "end": "1649730"
  },
  {
    "text": "master node NFS shares all the date with the the data with all the connected",
    "start": "1649730",
    "end": "1654980"
  },
  {
    "text": "nodes so each of those notes model one through model tan would be like I said with our multi-gpu node and then all the",
    "start": "1654980",
    "end": "1660920"
  },
  {
    "text": "data would be transferred so so yeah all these nodes would also be spot nodes bu",
    "start": "1660920",
    "end": "1667730"
  },
  {
    "text": "is a different focus star cluster which would help you get spot nodes across different availability zones if needed",
    "start": "1667730",
    "end": "1674720"
  },
  {
    "text": "the particular thing to notice works I think I press the wrong button",
    "start": "1674720",
    "end": "1681610"
  },
  {
    "text": "okay so the particular thing to note is that the the connection between the data",
    "start": "1687560",
    "end": "1694040"
  },
  {
    "text": "and the masternode needs to be pretty strong that is you know it needs to have a high bandwidth because really the data",
    "start": "1694040",
    "end": "1699860"
  },
  {
    "text": "travels from the EBS to the master and then it's distributed through via the star network to the connected nodes so",
    "start": "1699860",
    "end": "1706790"
  },
  {
    "text": "that connection needs to be EBS optimized so we use those sorts of things all the nodes are spot nodes",
    "start": "1706790",
    "end": "1712760"
  },
  {
    "text": "obviously and so you know this sort of an architecture has a natural limit to",
    "start": "1712760",
    "end": "1719300"
  },
  {
    "text": "can limit to it because the EBS can only yield about 200 MB is a second so if we",
    "start": "1719300",
    "end": "1726230"
  },
  {
    "text": "wanna let's say have more than 10 experiments in parallel how do we go about that so one way was to just maybe",
    "start": "1726230",
    "end": "1733310"
  },
  {
    "text": "go from a GP to to something like provisioned I ops SSD which allows you",
    "start": "1733310",
    "end": "1738740"
  },
  {
    "text": "to have much higher throughput unfortunately that's a really expensive",
    "start": "1738740",
    "end": "1744430"
  },
  {
    "text": "solution the other solution is to now use EFS that's a relatively recent thing we haven't explored that much but in our",
    "start": "1744430",
    "end": "1751610"
  },
  {
    "text": "case what we then do is we just simply use all our development data we",
    "start": "1751610",
    "end": "1757190"
  },
  {
    "text": "temporarily just halt the development and we just replicate that sort of a cluster here so that's basically the",
    "start": "1757190",
    "end": "1764030"
  },
  {
    "text": "most number of models we need to train about 20 models and this helps us iterate quite fast cheaply because we're",
    "start": "1764030",
    "end": "1770660"
  },
  {
    "text": "using part nodes and we do a lot of these experiments on smaller images and",
    "start": "1770660",
    "end": "1776150"
  },
  {
    "text": "then simply transfer the methodology over to bigger images so specifically we",
    "start": "1776150",
    "end": "1786200"
  },
  {
    "text": "view it is the single GPU instances these have 4 gigs of memory on the GPU",
    "start": "1786200",
    "end": "1791240"
  },
  {
    "text": "and so if your images let's say of size 2 2 4 by 2 2 4 then you can fit a batch",
    "start": "1791240",
    "end": "1798530"
  },
  {
    "text": "of about 128 images you know you can process a batch of 120 images all at",
    "start": "1798530",
    "end": "1803840"
  },
  {
    "text": "once through this neural network and so the main so the main problem in going",
    "start": "1803840",
    "end": "1810440"
  },
  {
    "text": "from this size image is 2 to 4 by 2 to 4 which is heavily downsample to something",
    "start": "1810440",
    "end": "1815510"
  },
  {
    "text": "like 1k by 1k which is what you know which is at least what we really want to do",
    "start": "1815510",
    "end": "1820890"
  },
  {
    "text": "is that you know if you're increasing let's say by a factor of four along each dimension if you're increasing the size",
    "start": "1820890",
    "end": "1826530"
  },
  {
    "text": "effectively by sixteen you can only fit eight or so images now on the GPU right",
    "start": "1826530",
    "end": "1831900"
  },
  {
    "text": "and so then the obvious solution becomes that okay graduate on two nodes which",
    "start": "1831900",
    "end": "1838110"
  },
  {
    "text": "have more GPUs so we use multi GPU know it's effectively having four GPUs",
    "start": "1838110",
    "end": "1843390"
  },
  {
    "text": "therefore 16 gigs of memory and what we do is we use data parallelism so just",
    "start": "1843390",
    "end": "1849990"
  },
  {
    "text": "you know we just train the same model on four different nodes and with accounting",
    "start": "1849990",
    "end": "1855480"
  },
  {
    "text": "for some overhead we can go from a bad size to about a bad size of twenty eight to thirty to something of that sort so",
    "start": "1855480",
    "end": "1862230"
  },
  {
    "text": "this helps us make the training feasible because with a small bad size the",
    "start": "1862230",
    "end": "1867600"
  },
  {
    "text": "gradients are really just too noisy and the network does not train so this is something that that is useful lately",
    "start": "1867600",
    "end": "1875750"
  },
  {
    "text": "Amazon has also come up with these people instances I hear we haven't had a chance to explore them but they actually",
    "start": "1875750",
    "end": "1882600"
  },
  {
    "text": "offer x16 GPUs at the same instance so that's something really cool and you",
    "start": "1882600",
    "end": "1888270"
  },
  {
    "text": "know we're looking forward to play with that okay so with that let me get into",
    "start": "1888270",
    "end": "1896090"
  },
  {
    "text": "what we call the hybrid architecture so what Jason described so far was a",
    "start": "1896090",
    "end": "1902340"
  },
  {
    "text": "methodology to run at most let's say 1k by 1k images through a neural network",
    "start": "1902340",
    "end": "1908220"
  },
  {
    "text": "and give some sort of a prediction so in this diagram on the right hand side of",
    "start": "1908220",
    "end": "1913320"
  },
  {
    "text": "the slide what the the network model which is label the main network that is",
    "start": "1913320",
    "end": "1918360"
  },
  {
    "text": "called that is the model that Jason spoke about it so far so what we have is we have the original image which is 2 K",
    "start": "1918360",
    "end": "1924570"
  },
  {
    "text": "by 2 K which is on the lower left side of the slide and then we down sample that 2 1k by 1k so we lose some features",
    "start": "1924570",
    "end": "1932520"
  },
  {
    "text": "right there and then we run it through the main network so in this setup though",
    "start": "1932520",
    "end": "1938280"
  },
  {
    "text": "in the hybrid architecture what we've done is we've taken the original high resolution image and then try to encode",
    "start": "1938280",
    "end": "1946290"
  },
  {
    "text": "some more signals from the image into the final prediction and",
    "start": "1946290",
    "end": "1952480"
  },
  {
    "text": "specifically what we try to do is that we try to do extra supervision so in some sense this is in contrast orders to",
    "start": "1952480",
    "end": "1961000"
  },
  {
    "text": "the general direction in which machine learning is going so machine learning people I mean these days machine learning folks try to go more",
    "start": "1961000",
    "end": "1968260"
  },
  {
    "text": "and more towards unsupervised learning whereas what we are trying to do here is go towards a more supervised approach",
    "start": "1968260",
    "end": "1975299"
  },
  {
    "text": "specifically what we want to do is so we only have about tens of thousands of images right this is medical imaging we",
    "start": "1975299",
    "end": "1983049"
  },
  {
    "text": "don't have access to millions of images as in other potential applications of deep learning so we try to we want to",
    "start": "1983049",
    "end": "1991240"
  },
  {
    "text": "get the most out of an image and so we try to encode the human knowledge and the specific way we do that here is by",
    "start": "1991240",
    "end": "1998530"
  },
  {
    "text": "marking lesions on the image and then telling the network that hey look at the",
    "start": "1998530",
    "end": "2003900"
  },
  {
    "text": "look at the probabilities of lesions in these specific tiles of the image and then merge that with the prediction of",
    "start": "2003900",
    "end": "2011070"
  },
  {
    "text": "the other network the main network and do a collective prediction so that's the general idea of the hybrid network",
    "start": "2011070",
    "end": "2018530"
  },
  {
    "text": "specifically what we're doing here is we've divided up the image into tiles into 64 tiles now each of these tiles is",
    "start": "2018530",
    "end": "2026250"
  },
  {
    "text": "at full resolution and we traded it there and we predict using the lesion detector the green network there whether",
    "start": "2026250",
    "end": "2034230"
  },
  {
    "text": "or not this this style of the original image has any lesions okay and so you",
    "start": "2034230",
    "end": "2042419"
  },
  {
    "text": "could there are like ten different types of lesions that affect the diagnosis of",
    "start": "2042419",
    "end": "2047970"
  },
  {
    "text": "diabetic retinopathy we've done this for one very specific lesion called hemorrhages which I'll talk about next",
    "start": "2047970",
    "end": "2054358"
  },
  {
    "text": "and mainly the lesion detector now will output 64 probabilities right",
    "start": "2054359",
    "end": "2060480"
  },
  {
    "text": "corresponding to each of those tiles and that probability indicates the chance of",
    "start": "2060480",
    "end": "2065820"
  },
  {
    "text": "having a lesion in that particular tile so the network on the right side sees the entire image at once make a cut",
    "start": "2065820",
    "end": "2072658"
  },
  {
    "text": "makes a collective prediction from that and the network on the left-hand side is doing sort of this localized analysis",
    "start": "2072659",
    "end": "2079648"
  },
  {
    "text": "and trying to generate you know in some sense very interpretable features which",
    "start": "2079649",
    "end": "2085080"
  },
  {
    "text": "are come mine go make a prediction okay so",
    "start": "2085080",
    "end": "2096020"
  },
  {
    "text": "to train the self a lesion detector you know we obviously need to mark data",
    "start": "2096020",
    "end": "2102860"
  },
  {
    "text": "because unlike the traditional model which just tells us what the grade of",
    "start": "2102860",
    "end": "2108470"
  },
  {
    "text": "every images now what you want to know is effectively we want to know the grade of every tile on the image right so and",
    "start": "2108470",
    "end": "2115880"
  },
  {
    "text": "so what we did was we built an entire toolkit around this lesion detector so",
    "start": "2115880",
    "end": "2123290"
  },
  {
    "text": "we built basically a web viewer and what this helps us do is to look at any image",
    "start": "2123290",
    "end": "2130550"
  },
  {
    "start": "2127000",
    "end": "2127000"
  },
  {
    "text": "apply lots of transformations color correction codes and then we can drop",
    "start": "2130550",
    "end": "2136970"
  },
  {
    "text": "boxes around different kinds of features so this is a viewer we made and then what you see in this image here is you",
    "start": "2136970",
    "end": "2144050"
  },
  {
    "text": "know these red boxes correspond to a particular type of lesion called hemorrhages the yellow ones correspond",
    "start": "2144050",
    "end": "2150350"
  },
  {
    "text": "to something called exudates this and this these are not any lesions these are just features that are present",
    "start": "2150350",
    "end": "2157010"
  },
  {
    "text": "in every eye the middle stuff and the optic disk so we did this for several",
    "start": "2157010",
    "end": "2162290"
  },
  {
    "text": "images about 200 of them and you know as you can see every image which is of a",
    "start": "2162290",
    "end": "2168530"
  },
  {
    "text": "certain non normal category can have many different lesions so this image you",
    "start": "2168530",
    "end": "2174530"
  },
  {
    "text": "can see it has a lot of experience and some hemorrhages on the left upper side and then what we did was we use this so",
    "start": "2174530",
    "end": "2184220"
  },
  {
    "text": "for each of the lesions that we marked we then extracted a patch from the",
    "start": "2184220",
    "end": "2189470"
  },
  {
    "start": "2185000",
    "end": "2185000"
  },
  {
    "text": "original image such that if we sample from anywhere in that patch the lesion",
    "start": "2189470",
    "end": "2195200"
  },
  {
    "text": "would be covered so you know it's just a simple way of obtaining positive samples",
    "start": "2195200",
    "end": "2201710"
  },
  {
    "text": "for our lesion detector and then for the negative samples for a lesion detector",
    "start": "2201710",
    "end": "2207380"
  },
  {
    "text": "that is sly that is tiles that do not have any lesions we simply took all the",
    "start": "2207380",
    "end": "2213830"
  },
  {
    "text": "class zero examples and randomly sampled from there randomly sampled tiles from there so that helps you obtain the negative",
    "start": "2213830",
    "end": "2222109"
  },
  {
    "start": "2220000",
    "end": "2220000"
  },
  {
    "text": "class so in sum we've only done this for a particular type of lesion hemorrhages",
    "start": "2222109",
    "end": "2228520"
  },
  {
    "text": "moreover we've only done this for 200 images which is barely anything our",
    "start": "2228520",
    "end": "2233630"
  },
  {
    "text": "training data was 35,000 images but this is really why we could do you know working on this in school and for the",
    "start": "2233630",
    "end": "2240530"
  },
  {
    "text": "negatives obviously we have a lot of data and we use pretty much the same",
    "start": "2240530",
    "end": "2245780"
  },
  {
    "text": "methodology that Jason spoke about we trained a neural network to predict you know positive or negative class and that",
    "start": "2245780",
    "end": "2252770"
  },
  {
    "text": "and the final accuracy we get from this sorrowful lesion detector is about 99%",
    "start": "2252770",
    "end": "2257780"
  },
  {
    "text": "for negatives and 76% for positives now keep in mind that this number 99% looks",
    "start": "2257780",
    "end": "2265130"
  },
  {
    "text": "really good and this 76 percent does not look as good in fact that 99% is a real",
    "start": "2265130",
    "end": "2272000"
  },
  {
    "text": "problem because you know a lot of the tiles that we sample even from diseased",
    "start": "2272000",
    "end": "2278210"
  },
  {
    "text": "images do not have lesions in every single tile so as a result of this when",
    "start": "2278210",
    "end": "2283430"
  },
  {
    "text": "you're really sampling tiles from your images most of the samples like something like 100 or thousand s-21 are",
    "start": "2283430",
    "end": "2290690"
  },
  {
    "text": "just negatives they have no lesions and so that 999 % accuracy for negatives",
    "start": "2290690",
    "end": "2295970"
  },
  {
    "text": "actually the one percent that are misclassified creates so much bigger problem for us than the 24% for",
    "start": "2295970",
    "end": "2301880"
  },
  {
    "text": "positives that are misclassified as negatives so in any case but you know we",
    "start": "2301880",
    "end": "2307069"
  },
  {
    "text": "were able to do this by marking just 200 images you know so with that another you",
    "start": "2307069",
    "end": "2317630"
  },
  {
    "text": "know with that we just we just trained this whole architecture and there was",
    "start": "2317630",
    "end": "2323030"
  },
  {
    "text": "some like we had to manipulate manipulate the sizes a little bit but",
    "start": "2323030",
    "end": "2328609"
  },
  {
    "text": "you know I'll probably not discuss that here great you know there were some adjustment that we needed to make around",
    "start": "2328609",
    "end": "2334160"
  },
  {
    "text": "the sizes to make this work and then we had this fused network which we just",
    "start": "2334160",
    "end": "2340069"
  },
  {
    "text": "trained and so with this entire approach",
    "start": "2340069",
    "end": "2345559"
  },
  {
    "start": "2343000",
    "end": "2343000"
  },
  {
    "text": "of training this separate lesion detector we basically get",
    "start": "2345559",
    "end": "2352030"
  },
  {
    "text": "extra data point of the right here so we were at about 0.79 without doing any of",
    "start": "2352140",
    "end": "2359829"
  },
  {
    "text": "the stuff by just using the simple model that Jason described and by essentially",
    "start": "2359829",
    "end": "2365529"
  },
  {
    "text": "going through all this work was off specifically inviting the network to look out for these lesions we were able",
    "start": "2365529",
    "end": "2372549"
  },
  {
    "text": "to get to a score of about point eight to a cap of point eight two now first of",
    "start": "2372549",
    "end": "2378430"
  },
  {
    "text": "all that that increase looks small for the amount of work that went in it I",
    "start": "2378430",
    "end": "2384670"
  },
  {
    "text": "mean the thing is you just need to scale this up now we've done the amount only two hundred images keep that in mind you",
    "start": "2384670",
    "end": "2390579"
  },
  {
    "text": "know there ders and we've also done it for a very specific kind of lesion just hemorrhages and so you clear like you",
    "start": "2390579",
    "end": "2398170"
  },
  {
    "text": "know this could potentially be replicated and you could obtain like much better accuracy we've just we just",
    "start": "2398170",
    "end": "2404529"
  },
  {
    "text": "haven't tried to optimize this to that to that so yeah so that's the and I",
    "start": "2404529",
    "end": "2411640"
  },
  {
    "text": "should mention that the main reason why we looked at hemorrhages was that we",
    "start": "2411640",
    "end": "2417339"
  },
  {
    "text": "figured that our accuracies from the conventional architecture conventional architecture that Jason described were",
    "start": "2417339",
    "end": "2424059"
  },
  {
    "text": "pretty good except for the onset of diabetic retinopathy now onset is",
    "start": "2424059",
    "end": "2430059"
  },
  {
    "text": "actually the critical point clinically that's a very relevant stage and in particular at onset it is really",
    "start": "2430059",
    "end": "2437499"
  },
  {
    "text": "these hemorrhages which are important to factor in to your diagnosis and so that",
    "start": "2437499",
    "end": "2442749"
  },
  {
    "text": "was the specific reason why we wanted to let the network know effectively what",
    "start": "2442749",
    "end": "2447819"
  },
  {
    "text": "these hemorrhages are and that was the reason for choosing those lesions in",
    "start": "2447819",
    "end": "2454420"
  },
  {
    "text": "terms of training this architecture you know typically the way you go about",
    "start": "2454420",
    "end": "2462059"
  },
  {
    "text": "training yourself an architecture as you do a forward drop which means you provide your image and you run it",
    "start": "2462089",
    "end": "2468039"
  },
  {
    "text": "through the two networks and then you store all these intermediate values so that you can do something called back",
    "start": "2468039",
    "end": "2474789"
  },
  {
    "text": "prop which is just you know running the gradients through both these networks now keep in mind there are 64 of these",
    "start": "2474789",
    "end": "2482589"
  },
  {
    "text": "tiles each one of them is running through an individual network so effectively like",
    "start": "2482589",
    "end": "2488020"
  },
  {
    "text": "this is just impossible for it to do on any GPU even with the one and even with",
    "start": "2488020",
    "end": "2493299"
  },
  {
    "text": "a bad size of one you cannot do it on a single GPU so what we've done so far is",
    "start": "2493299",
    "end": "2500470"
  },
  {
    "text": "not trained this jointly ideally you would want to back prop all the way through the two networks the lesion and",
    "start": "2500470",
    "end": "2507609"
  },
  {
    "text": "the main along with the fuse obviously but we haven't done that because it's",
    "start": "2507609",
    "end": "2514089"
  },
  {
    "text": "just computationally it's just infeasible and so right now we are looking into distributed training which",
    "start": "2514089",
    "end": "2520900"
  },
  {
    "text": "is a particularly attractive way of you know jointly training and even if we",
    "start": "2520900",
    "end": "2527140"
  },
  {
    "text": "don't train the lesion detector jointly with the main we can at least train the fuse and the main network together so I",
    "start": "2527140",
    "end": "2534760"
  },
  {
    "text": "said this is basically our most advanced model at this point",
    "start": "2534760",
    "end": "2539849"
  },
  {
    "text": "finally there are like lots of different techniques that you know some of them we",
    "start": "2540660",
    "end": "2547119"
  },
  {
    "start": "2541000",
    "end": "2541000"
  },
  {
    "text": "haven't tried some of them we've tried but with less success and there are yet",
    "start": "2547119",
    "end": "2553480"
  },
  {
    "text": "there is that you know may not be particularly clinically relevant so for",
    "start": "2553480",
    "end": "2560859"
  },
  {
    "text": "example one very interesting set of techniques is the supervised unsupervised learning where you try to",
    "start": "2560859",
    "end": "2566200"
  },
  {
    "text": "use all the unlabeled data you have so in our case we could just pretend like the test cases are only is just",
    "start": "2566200",
    "end": "2573790"
  },
  {
    "text": "unlabeled data we don't use those the labels corresponding to that data so we could we have techniques like pseudo",
    "start": "2573790",
    "end": "2579220"
  },
  {
    "text": "labeling and we've considered using if you haven't actually gotten around to getting back to word distillation is a",
    "start": "2579220",
    "end": "2587290"
  },
  {
    "text": "technique in which so-and-so's distillation is a technique in which instead of having our targets as simply",
    "start": "2587290",
    "end": "2595540"
  },
  {
    "text": "class 1 class 2 or class 3 what we try to do as we say make a diffuse prediction so we try to",
    "start": "2595540",
    "end": "2601960"
  },
  {
    "text": "say it is 0.2 plus 1.6 class 2 and let's",
    "start": "2601960",
    "end": "2607059"
  },
  {
    "text": "say another point 2 plus 3 so we are giving it a more diffused prediction and",
    "start": "2607059",
    "end": "2612750"
  },
  {
    "text": "so distillation is a technique which is relevant to our problem our classes the classes that we're",
    "start": "2612750",
    "end": "2619380"
  },
  {
    "text": "trying to predict are not just simply independent they have this gradation that you a patient goes from zero to one",
    "start": "2619380",
    "end": "2626520"
  },
  {
    "text": "to two to three to four and so you can encode that knowledge in distillation and what we found was you get the same",
    "start": "2626520",
    "end": "2633569"
  },
  {
    "text": "results with distillation just that you can actually train a lot faster we have lots of other techniques you know RNN",
    "start": "2633569",
    "end": "2640200"
  },
  {
    "text": "scum recurrent networks combined with CN n stood for model some called attention",
    "start": "2640200",
    "end": "2647400"
  },
  {
    "text": "cnn's we also considered looking at both eyes simultaneously so that a little",
    "start": "2647400",
    "end": "2653010"
  },
  {
    "text": "chart on the right side it shows the correlation between the grades of the",
    "start": "2653010",
    "end": "2658289"
  },
  {
    "text": "left and the right eye and so in this way you could make a joint prediction",
    "start": "2658289",
    "end": "2663329"
  },
  {
    "text": "for both the eyes together and that is actually bound to improve your score but",
    "start": "2663329",
    "end": "2669599"
  },
  {
    "text": "it's something that's again not relevant clinically because doctors at least",
    "start": "2669599",
    "end": "2674789"
  },
  {
    "text": "ophthalmologists that's me that we've spoken to they try to make predictions in like you know for one eye at a time",
    "start": "2674789",
    "end": "2680430"
  },
  {
    "text": "they avoid sort of using the correlation and finally on some link is always an",
    "start": "2680430",
    "end": "2685890"
  },
  {
    "text": "option to boost your scores so I just had a quickly summarize what we've",
    "start": "2685890",
    "end": "2693230"
  },
  {
    "start": "2690000",
    "end": "2690000"
  },
  {
    "text": "learned you know from this talk from this project so mainly we've tried to",
    "start": "2693230",
    "end": "2701430"
  },
  {
    "text": "come up with I guess a cheaper way of training we're pretty prog students so",
    "start": "2701430",
    "end": "2707130"
  },
  {
    "text": "you know we are a lot of our emphasis has been around trying to get by with least resources so we've used part notes",
    "start": "2707130",
    "end": "2714930"
  },
  {
    "text": "we've tried to develop a methodology for",
    "start": "2714930",
    "end": "2720180"
  },
  {
    "text": "low res images then scale it up to high level images and so yeah I mean that's",
    "start": "2720180",
    "end": "2725609"
  },
  {
    "text": "that's something which we feel has to be important you know if you want to make this sort of a thing practically another",
    "start": "2725609",
    "end": "2731940"
  },
  {
    "text": "thing is that in in a domain like medical imaging you're always going to",
    "start": "2731940",
    "end": "2737490"
  },
  {
    "text": "be short of data so the hybrid architecture tries to sort of attack",
    "start": "2737490",
    "end": "2742740"
  },
  {
    "text": "that from a different point of view which says that okay if you don't have much data one option obviously is to",
    "start": "2742740",
    "end": "2748020"
  },
  {
    "text": "just go out and collect more data or label more images but then you also have this option of taking your existing data and",
    "start": "2748020",
    "end": "2754830"
  },
  {
    "text": "then providing more supervision so we feel this is broadly a class of techniques that would be more relevant",
    "start": "2754830",
    "end": "2761460"
  },
  {
    "text": "for data hungry domains and medical imaging is definitely one of them we are never going to have a mill like millions",
    "start": "2761460",
    "end": "2768450"
  },
  {
    "text": "of images for diabetic retinopathy this is impossible so you know this is one attractive option there the final I",
    "start": "2768450",
    "end": "2775590"
  },
  {
    "text": "guess takeaway about which we haven't really spoken too much is the clinical importance we feel that it's you know so",
    "start": "2775590",
    "end": "2784380"
  },
  {
    "text": "the way this problem was posed as a five class problem with quadratic weighted cap is the performance metric that",
    "start": "2784380",
    "end": "2791880"
  },
  {
    "text": "really comes from kagu because we were taking part in that competition but then you know it started out as a class",
    "start": "2791880",
    "end": "2797970"
  },
  {
    "text": "project we went and participated in the cattle competition and then from there on we've been working to figure out like",
    "start": "2797970",
    "end": "2804090"
  },
  {
    "text": "how can this technology really come into the hospitals and that was the main reason to work with the doctors at",
    "start": "2804090",
    "end": "2809190"
  },
  {
    "text": "Stanford and so what we've learned is it's like the way this problem is forced",
    "start": "2809190",
    "end": "2815400"
  },
  {
    "text": "as a five class problem is not very relevant what is really critical today is to distinguish when a purse person",
    "start": "2815400",
    "end": "2822990"
  },
  {
    "text": "goes from zero to one to do so it really just becomes like a three-way classification problem and another",
    "start": "2822990",
    "end": "2830940"
  },
  {
    "text": "problem in this in this particular data set was that a lot of the force that is",
    "start": "2830940",
    "end": "2836220"
  },
  {
    "text": "the worst conditions images that we had were already scarred with lasers now",
    "start": "2836220",
    "end": "2843150"
  },
  {
    "text": "laser is a so is a type of treatment that people do if you're already in stage four but that leads to very",
    "start": "2843150",
    "end": "2850470"
  },
  {
    "text": "evident marks you know on the image of the eye and so our classifier is basically just picking up you know those",
    "start": "2850470",
    "end": "2857370"
  },
  {
    "text": "laser marks and that's sort of like cheating because when a person comes on as a four you know you don't really you",
    "start": "2857370",
    "end": "2864390"
  },
  {
    "text": "don't you don't really have the laser marks a priority and so that's something we are trying to figure out like how to",
    "start": "2864390",
    "end": "2869970"
  },
  {
    "text": "attack that get more forests we're also looking at like other images so we can do joint prediction so we are looking at",
    "start": "2869970",
    "end": "2876930"
  },
  {
    "text": "sorry other diseases so for example we've just started looking at glaucoma and we're thinking a freaking sort of",
    "start": "2876930",
    "end": "2882380"
  },
  {
    "text": "make like a collective prediction using not just different types of images but using different types of imaging",
    "start": "2882380",
    "end": "2887540"
  },
  {
    "text": "modalities as well so there's something called OCD finally we are looking at",
    "start": "2887540",
    "end": "2892670"
  },
  {
    "text": "longitudinal analysis but the data for that is just not available so by longitudinal I mean that you know you",
    "start": "2892670",
    "end": "2900170"
  },
  {
    "text": "can you want to study a person's images through time and figure out how he is deteriorating it probably makes the task",
    "start": "2900170",
    "end": "2906740"
  },
  {
    "text": "also easier for the classifier but the data for that is just we haven't been",
    "start": "2906740",
    "end": "2913100"
  },
  {
    "text": "able to get a hands on that so okay so with that you know we'd like to thank",
    "start": "2913100",
    "end": "2919210"
  },
  {
    "text": "AWS they supported us they were responsible for AWS educate who is",
    "start": "2919210",
    "end": "2924560"
  },
  {
    "text": "responsible for I guess making this project happen in the classroom and then",
    "start": "2924560",
    "end": "2929990"
  },
  {
    "text": "after that we've been supported by through their research trance also want",
    "start": "2929990",
    "end": "2935060"
  },
  {
    "text": "to thank professor Robert Chang was an ophthalmologist at Stanford he's someone",
    "start": "2935060",
    "end": "2940400"
  },
  {
    "text": "v4 quite closely with and you know we're about to graduate so he's the one who's going to take this project forward there",
    "start": "2940400",
    "end": "2946670"
  },
  {
    "text": "and finally Jeff Wolman and andreas pepe who are both professors in the CS",
    "start": "2946670",
    "end": "2952160"
  },
  {
    "text": "Department and you know they've provided some really invaluable insights into the problem okay thanks",
    "start": "2952160",
    "end": "2958990"
  },
  {
    "text": "[Applause]",
    "start": "2958990",
    "end": "2964720"
  }
]