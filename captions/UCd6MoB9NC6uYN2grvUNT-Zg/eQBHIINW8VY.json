[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "okay I'm gonna get started now how you guys doing pretty good yeah you guys",
    "start": "140",
    "end": "7080"
  },
  {
    "text": "ready to glue today my name is the May whole shop and today",
    "start": "7080",
    "end": "13910"
  },
  {
    "text": "I'm gonna be showing you how you can build data transformation pipelines with AWS glue I'm a manager on the service I",
    "start": "13910",
    "end": "22340"
  },
  {
    "start": "22000",
    "end": "91000"
  },
  {
    "text": "take my jacket off get too cold or hot around here so today I'll be giving you",
    "start": "22340",
    "end": "32758"
  },
  {
    "text": "a quick introduction to AWS glue I'll start diving into how you can",
    "start": "32759",
    "end": "38280"
  },
  {
    "text": "construct an ETL flow in glue from raw unfiltered data to an ETL flow running",
    "start": "38280",
    "end": "46079"
  },
  {
    "text": "in production and for easy steps then I'm going to actually dive in into the",
    "start": "46079",
    "end": "53160"
  },
  {
    "text": "under the underlying features that glue provides that makes this actually",
    "start": "53160",
    "end": "58590"
  },
  {
    "text": "possible and hopefully you'll learn how to customize your own AWS glue scripts by listening to this talk and in the",
    "start": "58590",
    "end": "66810"
  },
  {
    "text": "latter 1/4 of this talk I'm going to ask one of our early customers Merck keith's",
    "start": "66810",
    "end": "72270"
  },
  {
    "text": "mullah over here is going to be speaking for them and they'll relate to you some of their experiences in using glue and",
    "start": "72270",
    "end": "79799"
  },
  {
    "text": "how quickly and how easy it was to get a ETL pipeline running and production they",
    "start": "79799",
    "end": "85979"
  },
  {
    "text": "were able to do it in less than a week",
    "start": "85979",
    "end": "89420"
  },
  {
    "start": "91000",
    "end": "131000"
  },
  {
    "text": "so what is Glu Glu is a fully managed server 'less extract transform and load",
    "start": "91610",
    "end": "100409"
  },
  {
    "text": "service or ETL for short now there are plenty of ETL tools out there but glue",
    "start": "100409",
    "end": "106770"
  },
  {
    "text": "is one of the few that's designed for developers and built by developers we've",
    "start": "106770",
    "end": "113369"
  },
  {
    "text": "got thousands and thousands of customers on our platform running thousands and",
    "start": "113369",
    "end": "118380"
  },
  {
    "text": "thousands of jobs on a daily basis here are some of our customers today some of",
    "start": "118380",
    "end": "124469"
  },
  {
    "text": "our early customers and as I said Merck will be speaking about their experiences using glue at the end of this talk",
    "start": "124469",
    "end": "131330"
  },
  {
    "start": "131000",
    "end": "313000"
  },
  {
    "text": "in our ecosystem in AWS we actually have a number of partners that do ETL there",
    "start": "131330",
    "end": "138230"
  },
  {
    "text": "are many tools already available it's just just a smattering of the ETL tools of shown on the redshift partner page",
    "start": "138230",
    "end": "144610"
  },
  {
    "text": "but it turns out that our customers are still developing a lot of ETL by hand by",
    "start": "144610",
    "end": "153020"
  },
  {
    "text": "actually hand coding their ETL why is this this is the reason by the way that we built glue ok it's because every one",
    "start": "153020",
    "end": "161240"
  },
  {
    "text": "of these tools that I showed you in the previous slide does something extremely well does that one thing extremely well",
    "start": "161240",
    "end": "168680"
  },
  {
    "text": "ok but your ETL job in your organization or your company isn't just about that",
    "start": "168680",
    "end": "174680"
  },
  {
    "text": "one thing and when you want to go beyond that one thing typically you end up having to build it on your own and",
    "start": "174680",
    "end": "180950"
  },
  {
    "text": "that's why you use code code is powerful it's flexible you can do a lot of things",
    "start": "180950",
    "end": "186710"
  },
  {
    "text": "with it there very little limitations to code code is modular you can actually",
    "start": "186710",
    "end": "192560"
  },
  {
    "text": "build it in a way where you can share it with your friends across your organization and more importantly your",
    "start": "192560",
    "end": "199760"
  },
  {
    "text": "organization is already it does already has a bunch of developers that know how to deal with code they're building",
    "start": "199760",
    "end": "205640"
  },
  {
    "text": "applications for you they have a pipeline in place for managing and developing code so why not manage and",
    "start": "205640",
    "end": "212660"
  },
  {
    "text": "develop your ETL scripts and your ETL jobs the same way there are familiar tools there IDs for debugging and",
    "start": "212660",
    "end": "220010"
  },
  {
    "text": "editing you've got version control for tracking changes you've got a testing pipeline",
    "start": "220010",
    "end": "226070"
  },
  {
    "text": "continuous integration and deployment and so on and so in this talk I'll be",
    "start": "226070",
    "end": "231709"
  },
  {
    "text": "speaking to all the developers in the room over here I'll be describing some of the core data structures and",
    "start": "231709",
    "end": "237260"
  },
  {
    "text": "algorithms that AWS has the IP it has internally that will help you make your",
    "start": "237260",
    "end": "243320"
  },
  {
    "text": "life a lot easier and if you're not a developer hopefully you'll walk away",
    "start": "243320",
    "end": "248450"
  },
  {
    "text": "knowing a lot more about the underlying IP that we provide that powers or ETL",
    "start": "248450",
    "end": "253459"
  },
  {
    "text": "jobs today all right so even though everybody's coding that doesn't mean",
    "start": "253459",
    "end": "259340"
  },
  {
    "text": "coding is easy in particular writing ETL code is especially hard and the reason it's hard",
    "start": "259340",
    "end": "266300"
  },
  {
    "text": "is because the underlying data is changing it's changing very quickly the",
    "start": "266300",
    "end": "272389"
  },
  {
    "text": "underlying data structures the schemas are changing the data formats are changing as you often in an agile",
    "start": "272389",
    "end": "279139"
  },
  {
    "text": "environment need to add or change the sources that you have and the data grows and grows pretty fast and so what this",
    "start": "279139",
    "end": "286789"
  },
  {
    "text": "means is that developers have to constantly change and adapt their scripts and it makes hand coding",
    "start": "286789",
    "end": "292580"
  },
  {
    "text": "extremely error-prone and brittle so what does their EWS kahl√∫a do it",
    "start": "292580",
    "end": "297740"
  },
  {
    "text": "actually does a lot of that have the undifferentiated heavy lifting that you need to manage these changes so that",
    "start": "297740",
    "end": "304610"
  },
  {
    "text": "developers can focus on the high-level things that they want to get done for their ETL code and we do the rest",
    "start": "304610",
    "end": "310460"
  },
  {
    "text": "underneath all right so that's sort of",
    "start": "310460",
    "end": "315770"
  },
  {
    "start": "313000",
    "end": "433000"
  },
  {
    "text": "the premise for AWS glue let me kind of walk you through the three main components of AWS glue first we have a",
    "start": "315770",
    "end": "322490"
  },
  {
    "text": "data catalog and it's there for helping you discover and understand the data sources that you have we have crawlers",
    "start": "322490",
    "end": "329900"
  },
  {
    "text": "that are associated with data catalog that when you point them to your datasets will automatically extract the",
    "start": "329900",
    "end": "334940"
  },
  {
    "text": "structure and store all that information including statistics into the catalog for you the catalog is hive meta store",
    "start": "334940",
    "end": "344180"
  },
  {
    "text": "compatible which means it is now in you can integrate it with a lot of various tools including a number of AWS analytic",
    "start": "344180",
    "end": "351830"
  },
  {
    "text": "services like EMR athina redshift spectrum and so on the second component",
    "start": "351830",
    "end": "359870"
  },
  {
    "text": "is the job authoring and ETL system and that's what I'm mostly going to be",
    "start": "359870",
    "end": "365090"
  },
  {
    "text": "talking about today the first thing it does is it actually lets you get started quickly with a ETL",
    "start": "365090",
    "end": "372259"
  },
  {
    "text": "flow it generates code for you if you point to tables inside of the data catalog the code it generates is in a",
    "start": "372259",
    "end": "379699"
  },
  {
    "text": "familiar language Python and particularly PI spark that works with",
    "start": "379699",
    "end": "384949"
  },
  {
    "text": "Apache spark underneath vanilla Apache spark so it's an environment that many",
    "start": "384949",
    "end": "391250"
  },
  {
    "text": "developers are already familiar with and then we offer tools that allow you to edit",
    "start": "391250",
    "end": "396960"
  },
  {
    "text": "and debug your code as well as explore the data sets that you want to go and analyze we also have a number of new and",
    "start": "396960",
    "end": "404910"
  },
  {
    "text": "interesting primitives that make this easy and that's what I'll be talking about today the third thing that we",
    "start": "404910",
    "end": "411720"
  },
  {
    "text": "provide is a job execution system that serverless you give us your code your ETL code we turn it into a job and then",
    "start": "411720",
    "end": "419280"
  },
  {
    "text": "we run it for you you never have to spin up any machines and manage their lifecycle configure them",
    "start": "419280",
    "end": "424889"
  },
  {
    "text": "it's basically give us and go you can also schedule these jobs in a very",
    "start": "424889",
    "end": "429960"
  },
  {
    "text": "flexible way and monitor their progress alright so what are people using glue",
    "start": "429960",
    "end": "435840"
  },
  {
    "start": "433000",
    "end": "456000"
  },
  {
    "text": "for what are our customers doing with glue one of the main things that they're",
    "start": "435840",
    "end": "440910"
  },
  {
    "text": "doing is actually loading data warehouses they get all their data from a variety of different places integrate",
    "start": "440910",
    "end": "446340"
  },
  {
    "text": "it together with AWS glue structure it and then put it into redshift so then you can analyze it analyze that later",
    "start": "446340",
    "end": "453210"
  },
  {
    "text": "data later another thing that they do is",
    "start": "453210",
    "end": "458820"
  },
  {
    "start": "456000",
    "end": "481000"
  },
  {
    "text": "they actually are trying to build data lakes with glue instead of warehousing their data here they'll crawl all their",
    "start": "458820",
    "end": "464760"
  },
  {
    "text": "data index all their information and then make that data available and ready",
    "start": "464760",
    "end": "470460"
  },
  {
    "text": "for analysis through one of many analytic services that we have EMR Athena and redshift spectrum including a",
    "start": "470460",
    "end": "477510"
  },
  {
    "text": "bunch of other BI tools that work on top of them all right so now given these",
    "start": "477510",
    "end": "484530"
  },
  {
    "start": "481000",
    "end": "549000"
  },
  {
    "text": "components I'm going to show you how you can construct an ETL flow and four easy steps the first step is to simply point",
    "start": "484530",
    "end": "493380"
  },
  {
    "text": "the crawlers and data catalog to your data and the crawlers will automatically figure things out for you I'll give you",
    "start": "493380",
    "end": "499409"
  },
  {
    "text": "an example of that shortly next once you have your data all kind of indexed",
    "start": "499409",
    "end": "504810"
  },
  {
    "text": "inside of your catalog you can spit use you can specify mappings with the AWS glue console that will then generate an",
    "start": "504810",
    "end": "511770"
  },
  {
    "text": "ETL flow it will actually generate some code for you you can edit that code using development endpoints and an",
    "start": "511770",
    "end": "519000"
  },
  {
    "text": "environment which is exactly the same environment that you get when you run your jobs and then finally you can",
    "start": "519000",
    "end": "524850"
  },
  {
    "text": "schedule and trigger your jobs and a dynamic now you don't have to do all of these",
    "start": "524850",
    "end": "529930"
  },
  {
    "text": "things to get started you can just do one of them for example just catalog your data and start using an analytic service or you can just use",
    "start": "529930",
    "end": "538270"
  },
  {
    "text": "the underlying you know Python and spark infrastructure that we have the point",
    "start": "538270",
    "end": "543340"
  },
  {
    "text": "here is that we have all the primitives that you need to go from raw data to a production job now throughout this talk",
    "start": "543340",
    "end": "551680"
  },
  {
    "text": "I'm going to be working with a specific example where we do where we convert a",
    "start": "551680",
    "end": "557470"
  },
  {
    "text": "bunch of JSON data from JSON to CSV it's a typical process that you see in",
    "start": "557470",
    "end": "563890"
  },
  {
    "text": "both building data lakes as well as loading data warehouses and so it's a canonical example that you can take home",
    "start": "563890",
    "end": "569170"
  },
  {
    "text": "and try after yourself the example here is going to be working with github archive data this is basically the",
    "start": "569170",
    "end": "576400"
  },
  {
    "text": "archive of all the events in the public events that github makes available for",
    "start": "576400",
    "end": "581560"
  },
  {
    "text": "you online they actually generate these these files on an hourly basis and make",
    "start": "581560",
    "end": "587230"
  },
  {
    "text": "them available and you can go download them and analyze them god bless these guys because there's a lot of wonderful public data there you can grab that data",
    "start": "587230",
    "end": "594550"
  },
  {
    "text": "through a variety of mechanisms in this case what I've shown you is you can go and download it through AWS lambda and",
    "start": "594550",
    "end": "599650"
  },
  {
    "text": "put it in that Amazon s3 bucket you crawl it using glue crawlers",
    "start": "599650",
    "end": "604780"
  },
  {
    "text": "they'll reconstruct the structure you convert the data using our ETL system",
    "start": "604780",
    "end": "610060"
  },
  {
    "text": "and then the output is actually put back into Amazon s3 in another set of buckets where then it's available for a variety",
    "start": "610060",
    "end": "617170"
  },
  {
    "text": "of analytic services to run queries over okay so this is the example that we're working with digging down a little bit",
    "start": "617170",
    "end": "625000"
  },
  {
    "start": "623000",
    "end": "683000"
  },
  {
    "text": "one of the things that we want to do here in our example is I want to organize the data imagine you have terabytes if not",
    "start": "625000",
    "end": "631480"
  },
  {
    "text": "petabytes of data you don't want to just have one big bucket with all your files to run about it'd be kind of a mess it'd",
    "start": "631480",
    "end": "636670"
  },
  {
    "text": "be hard to keep track of and so what a lot of people do is organize this stuff in terms of directory hierarchies and s3",
    "start": "636670",
    "end": "643320"
  },
  {
    "text": "okay so that way they can actually you know manage the lifecycle of that data find that data easily and more",
    "start": "643320",
    "end": "650440"
  },
  {
    "text": "importantly if you're actually querying this data with Athena or redshift spectrum you can very quickly you know",
    "start": "650440",
    "end": "655960"
  },
  {
    "text": "narrow down data that you care and ignore a bunch of other things so on",
    "start": "655960",
    "end": "661200"
  },
  {
    "text": "the left hand under yes on the left hand side so my left hand your left to you",
    "start": "661200",
    "end": "667950"
  },
  {
    "text": "see the source data in JSON format and what we're going to be doing is filtering that data for specific types of Defense transforming them into CSV",
    "start": "667950",
    "end": "675060"
  },
  {
    "text": "format and we're going to try to get them into exactly the same hierarchy so that then you can analyze it with them a number of other analytic services this",
    "start": "675060",
    "end": "684690"
  },
  {
    "start": "683000",
    "end": "723000"
  },
  {
    "text": "is what the data looks like and it's a bit of an eye chart but that's the point okay imagine you have this raw data and you",
    "start": "684690",
    "end": "691680"
  },
  {
    "text": "had to come up with a table structure for it so that you could actually query it it'd be pretty hard there are about",
    "start": "691680",
    "end": "698100"
  },
  {
    "text": "35 actually more than 35 now 37 event types that github publishes they have",
    "start": "698100",
    "end": "703770"
  },
  {
    "text": "some sort sort of common structure but there's an unique payload for every event and the structure for the the",
    "start": "703770",
    "end": "711210"
  },
  {
    "text": "payload varies by event type and so trying to kind of comb through this data to come up with the structure is",
    "start": "711210",
    "end": "717350"
  },
  {
    "text": "incredibly tedious and often people can get it wrong and it'll break a lot of your analyses so the first step is to",
    "start": "717350",
    "end": "725970"
  },
  {
    "start": "723000",
    "end": "837000"
  },
  {
    "text": "actually crawl this data using the crawler so what the crawler will do when you point in it at this data is come up",
    "start": "725970",
    "end": "731340"
  },
  {
    "text": "with this beautiful schema that you see in the bottom left ok over here you'll",
    "start": "731340",
    "end": "736710"
  },
  {
    "text": "see eight columns here sorry this is a little bit dense here but those are the",
    "start": "736710",
    "end": "742350"
  },
  {
    "text": "sort of top level comma columns that all the the the event types share it detects",
    "start": "742350",
    "end": "748140"
  },
  {
    "text": "the data type as well and if it's a structure you can actually double click on that structure and over here we're",
    "start": "748140",
    "end": "753870"
  },
  {
    "text": "showing you the actual complex schema that comes out from unit unifying all",
    "start": "753870",
    "end": "759330"
  },
  {
    "text": "the various payloads across all the different event types that we have doing so creates about 200 more or more fields",
    "start": "759330",
    "end": "766980"
  },
  {
    "text": "I didn't actually count them all doing it by hand as I said is going to be too hard crawlers also collect a number of",
    "start": "766980",
    "end": "774000"
  },
  {
    "text": "other statistics about the data a number of rows you know where the data is and",
    "start": "774000",
    "end": "780390"
  },
  {
    "text": "they also classify the data into what type of data it is what format it is whether it has compression and so on and",
    "start": "780390",
    "end": "787410"
  },
  {
    "text": "that way if you're using any one of these other services it knows how to kind of decode that data and start analyzing it another",
    "start": "787410",
    "end": "794429"
  },
  {
    "text": "interesting thing that crawlers do is at the bottom you'll notice that we've actually figured out the partition",
    "start": "794429",
    "end": "800999"
  },
  {
    "text": "structure and added them as columns to our tables we know that though data is",
    "start": "800999",
    "end": "806759"
  },
  {
    "text": "organized when we're crawling it by year month and day just like the picture that I showed you before we figure that out",
    "start": "806759",
    "end": "813119"
  },
  {
    "text": "or crawlers figured that automatically it's not actually information in the data but is information in the way it's",
    "start": "813119",
    "end": "818189"
  },
  {
    "text": "organized and we make it available so you don't have to go manage it and as a new data shows up and new partitions to",
    "start": "818189",
    "end": "824429"
  },
  {
    "text": "show up we call them partitions crawlers will automatically kind of populate that",
    "start": "824429",
    "end": "830759"
  },
  {
    "text": "information the catalog for you all right so once you've got this crawled",
    "start": "830759",
    "end": "837619"
  },
  {
    "start": "837000",
    "end": "887000"
  },
  {
    "text": "you can now start converting that data basically go to the add job wizard inside of our service and what it will",
    "start": "837619",
    "end": "845099"
  },
  {
    "text": "do is it'll give you a schema of the data on the left-hand side that you can play around with in this particular case",
    "start": "845099",
    "end": "851009"
  },
  {
    "text": "we're only pulling out some subset of the columns that that we want for our analyses in this particular case ID and",
    "start": "851009",
    "end": "857699"
  },
  {
    "text": "type and some payload information and the arrows basically are things that you",
    "start": "857699",
    "end": "862799"
  },
  {
    "text": "can you can configure yourself to map the source to the destination and on the",
    "start": "862799",
    "end": "868439"
  },
  {
    "text": "destination side you can play around with the order of columns you can play around with the types so you can cast",
    "start": "868439",
    "end": "874889"
  },
  {
    "text": "something that was a string into an end or vice versa you can change the names and the target",
    "start": "874889",
    "end": "880409"
  },
  {
    "text": "and so on and then once you've kind of configured this the way that you want on the console you hit go and what it does",
    "start": "880409",
    "end": "889229"
  },
  {
    "start": "887000",
    "end": "1037000"
  },
  {
    "text": "is generates a script for you and I'm going to kind of walk through the script for you I know it's again a bit of an",
    "start": "889229",
    "end": "895259"
  },
  {
    "text": "eye chart but I'll kind of describe the individual components of the script and how it works at the top of the script",
    "start": "895259",
    "end": "903049"
  },
  {
    "text": "it's just a bunch of boilerplate it's just stuff that you need to go and get things running like libraries and",
    "start": "903049",
    "end": "909839"
  },
  {
    "text": "context that you have to set up the first thing that you do is you actually initialize a thing called a job bookmark",
    "start": "909839",
    "end": "916279"
  },
  {
    "text": "ok a bookmark is a way for the script to know where it left off the last time it",
    "start": "916279",
    "end": "922499"
  },
  {
    "text": "ran so it doesn't reprocess all the data and all the state that it had beforehand it just picks up from where it's and where",
    "start": "922499",
    "end": "928610"
  },
  {
    "text": "it left off and I'll talk more about bookmarks and how they work at the end of this talk the next thing you'll see",
    "start": "928610",
    "end": "935450"
  },
  {
    "text": "is a bunch of annotations there are actually just comments in the code from",
    "start": "935450",
    "end": "941390"
  },
  {
    "text": "the purpose perspective of Pi spark it's just ignored okay from the perspective of glue and the glue service we actually",
    "start": "941390",
    "end": "949760"
  },
  {
    "text": "look at those comments and those comments are what is going to generate the graph on the left hand side which",
    "start": "949760",
    "end": "955820"
  },
  {
    "text": "describes the transformations from source to destination the next thing",
    "start": "955820",
    "end": "961460"
  },
  {
    "text": "that we do is we actually read the data into a data frame okay a data frame is",
    "start": "961460",
    "end": "966950"
  },
  {
    "text": "basically a data structure that represents the underlying data and it's",
    "start": "966950",
    "end": "972770"
  },
  {
    "text": "designed for doing ETL operations now for this particular transformation from source JSON to CSV the system has",
    "start": "972770",
    "end": "980750"
  },
  {
    "text": "decided that it needs to do a certain set of data cleaning transformations and restructuring transformations and so",
    "start": "980750",
    "end": "987110"
  },
  {
    "text": "it's added those things after that and then finally when you get the resulting data frame you write it back out to the",
    "start": "987110",
    "end": "993860"
  },
  {
    "text": "destination which is another sink or a location in s3 okay now this particular",
    "start": "993860",
    "end": "1000250"
  },
  {
    "text": "code if you go run it what it's going to do is it's going to just transform the data from that source to that particular",
    "start": "1000250",
    "end": "1005770"
  },
  {
    "text": "target format that we just that we did in the previous step but it's not going to create the hive style partitions that",
    "start": "1005770",
    "end": "1012160"
  },
  {
    "text": "we want on the target to be able to go and actually go and do any you know queries fast the other thing it won't do",
    "start": "1012160",
    "end": "1019210"
  },
  {
    "text": "is and won't actually do any filtering it'll actually only do all the projections or pick out all the columns",
    "start": "1019210",
    "end": "1025150"
  },
  {
    "text": "that you want but if you want to only pick out certain types of events it's not going to do that either so you can",
    "start": "1025150",
    "end": "1031150"
  },
  {
    "text": "edit the code right here in the console but sometimes that's a little bit hard it's not a tool that you might be used",
    "start": "1031150",
    "end": "1036430"
  },
  {
    "text": "to so we offer to you as a developer and",
    "start": "1036430",
    "end": "1041490"
  },
  {
    "start": "1037000",
    "end": "1091000"
  },
  {
    "text": "developer endpoint or a development endpoint what a development endpoint is",
    "start": "1041490",
    "end": "1048188"
  },
  {
    "text": "is basically an environment a glue based spark environment that's set up for you",
    "start": "1048189",
    "end": "1054040"
  },
  {
    "text": "and that's constantly up and running where you can interact send your spark jobs or your so excuse",
    "start": "1054040",
    "end": "1060570"
  },
  {
    "text": "me your spark scripts too and get your answers back very quickly and supports",
    "start": "1060570",
    "end": "1066420"
  },
  {
    "text": "an interface that a lot of IDs and notebooks are used to so you can basically attach your ID IDE directly to",
    "start": "1066420",
    "end": "1074280"
  },
  {
    "text": "the developer endpoint and then you can start debugging your scripts just like you're normally used to and the IDE that",
    "start": "1074280",
    "end": "1081270"
  },
  {
    "text": "you have okay you can even test it out over you know example data sets or",
    "start": "1081270",
    "end": "1087330"
  },
  {
    "text": "sample data sets that you have beyond just debugging scripts you can",
    "start": "1087330",
    "end": "1093390"
  },
  {
    "text": "also connect notebooks to your developer endpoint and when you do that you get a",
    "start": "1093390",
    "end": "1101940"
  },
  {
    "text": "whole new set of rich functionality you can start to experiment and explore data sets that you have at the top on the",
    "start": "1101940",
    "end": "1110250"
  },
  {
    "text": "left-hand side oops on the left hand side what we're showing you is you can actually write PI",
    "start": "1110250",
    "end": "1115740"
  },
  {
    "text": "spark scripts and interactively get answers in this particular case we're",
    "start": "1115740",
    "end": "1121020"
  },
  {
    "text": "just showing you schema that's coming back or if you register the data as tables in in the catalog then what ends",
    "start": "1121020",
    "end": "1130770"
  },
  {
    "text": "up happening is that you can just use spark sequel and run sequel point commands directly over your data and",
    "start": "1130770",
    "end": "1135780"
  },
  {
    "text": "then start charting and exploring your data that way once you've kind of gotten",
    "start": "1135780",
    "end": "1141720"
  },
  {
    "text": "your scripts the way you want then you basically upload them to s3 and create",
    "start": "1141720",
    "end": "1147990"
  },
  {
    "text": "or register an ETL job with AWS glue and the job basically encapsulates the",
    "start": "1147990",
    "end": "1153000"
  },
  {
    "text": "script it runs that script for you once you've got the job registered with the",
    "start": "1153000",
    "end": "1159270"
  },
  {
    "start": "1156000",
    "end": "1215000"
  },
  {
    "text": "system then you can trigger the job based on a number of different conditions we have several different",
    "start": "1159270",
    "end": "1166890"
  },
  {
    "text": "trigger types or event types that you can trigger a job on on the left hand side what I'm showing you is a way of",
    "start": "1166890",
    "end": "1173850"
  },
  {
    "text": "configuring it to schedule your job on a particular time you can use a cron expression for example you can just run",
    "start": "1173850",
    "end": "1182070"
  },
  {
    "text": "a job on demand you can also run jobs based on the completion of another job",
    "start": "1182070",
    "end": "1189000"
  },
  {
    "text": "and so you can actually up all these very complicated pipelines or simple pipelines of you know jobs",
    "start": "1189000",
    "end": "1196560"
  },
  {
    "text": "kind of getting kicking other jobs off with various conditions like ands and ORS in there you can also pass",
    "start": "1196560",
    "end": "1204270"
  },
  {
    "text": "parameters to your jobs so that your with triggers so that your jobs have",
    "start": "1204270",
    "end": "1209580"
  },
  {
    "text": "some context when they're running",
    "start": "1209580",
    "end": "1213049"
  },
  {
    "start": "1215000",
    "end": "1312000"
  },
  {
    "text": "finally when we actually run these jobs we run them in our service you never",
    "start": "1215420",
    "end": "1221970"
  },
  {
    "text": "actually have to provision configure or manage servers we'll give you a serverless interface for running these",
    "start": "1221970",
    "end": "1227700"
  },
  {
    "text": "jobs and the cool thing is we actually do all the configuration that you need to do for attaching to VP C's for",
    "start": "1227700",
    "end": "1236070"
  },
  {
    "text": "getting to your data sources or getting access based on roles that you have for",
    "start": "1236070",
    "end": "1241710"
  },
  {
    "text": "say your AWS resources like s3 buckets we also maintain the same security and",
    "start": "1241710",
    "end": "1249390"
  },
  {
    "text": "isolation that you would have if you're running these things on clusters in your account so if you have a job that runs",
    "start": "1249390",
    "end": "1255900"
  },
  {
    "text": "in a particular V PC or with a particular role it's going to run on a different set of virtual resources then",
    "start": "1255900",
    "end": "1262590"
  },
  {
    "text": "a job that's in another V PC year with another role so they don't actually intermingle so your data and your code",
    "start": "1262590",
    "end": "1269370"
  },
  {
    "text": "is separated by those boundaries it won't be intermingled with other customers or other jobs even if it's",
    "start": "1269370",
    "end": "1276540"
  },
  {
    "text": "within the same account okay customers can also specify the amount of capacity",
    "start": "1276540",
    "end": "1282810"
  },
  {
    "text": "that you want to use for a job and you do that based on this abstract computing unit called a data processing unit or a",
    "start": "1282810",
    "end": "1288360"
  },
  {
    "text": "GPU that's how we price our jobs and so you can basically tell us how many dpu you want to use to go and run a particular",
    "start": "1288360",
    "end": "1294690"
  },
  {
    "text": "job and we'll spin up those resources automatically for you and will automatically scale those resources then",
    "start": "1294690",
    "end": "1301200"
  },
  {
    "text": "finally you only pay for the resources that you consume we've now gone to per second billing and we still have a we",
    "start": "1301200",
    "end": "1307830"
  },
  {
    "text": "have we have a 10-minute minimum for all of your jobs okay all right well so now",
    "start": "1307830",
    "end": "1315000"
  },
  {
    "text": "you know how to get a script running into production and what happens underneath what happens if you want to",
    "start": "1315000",
    "end": "1320490"
  },
  {
    "text": "go and customize your script what are all those things there's book Mart and dynamic frames doing well to try to",
    "start": "1320490",
    "end": "1326250"
  },
  {
    "text": "understand that you need to first we need to first kind of get into what spark is and how it works okay",
    "start": "1326250",
    "end": "1333480"
  },
  {
    "text": "Apache spark if you guys aren't familiar with it is a parallel scale out data",
    "start": "1333480",
    "end": "1338700"
  },
  {
    "text": "processing engine it's got fault tolerance and built in so if things fail it restarts your your jobs for you",
    "start": "1338700",
    "end": "1346140"
  },
  {
    "text": "internally and then we also have another layer of restarting and so on more",
    "start": "1346140",
    "end": "1351300"
  },
  {
    "text": "importantly it's got a flexible interface you can not only write scripts but you also can do SQL sequel with",
    "start": "1351300",
    "end": "1360680"
  },
  {
    "text": "spark and you can actually intermingle intermingle these things freely so it's actually quite flexible there's a rich",
    "start": "1360680",
    "end": "1367620"
  },
  {
    "text": "ecosystem of use cases around machine learning people are doing graph analytics with spark people are doing",
    "start": "1367620",
    "end": "1374010"
  },
  {
    "text": "sequel analytics they're doing ETL with spark and do a lot at the but at the",
    "start": "1374010",
    "end": "1379980"
  },
  {
    "text": "core of spark what they have are these data structures called rdd's they're basically collections of data that are",
    "start": "1379980",
    "end": "1385920"
  },
  {
    "text": "resilient are resilient data set distributed data sets that's where they stand for and on top of rdd's they've",
    "start": "1385920",
    "end": "1391620"
  },
  {
    "text": "built these things called data frames and on top of data frames they have a whole bunch of transforms",
    "start": "1391620",
    "end": "1396950"
  },
  {
    "text": "that are basically sequel based pac---- of transforms that spark sequel offers well what we've done is we've built an",
    "start": "1396950",
    "end": "1404130"
  },
  {
    "text": "analogous set of libraries that work with all of this code we have these things called data frames and I'll talk",
    "start": "1404130",
    "end": "1410100"
  },
  {
    "text": "about that next and our glue ETL libraries on top of data frames allow",
    "start": "1410100",
    "end": "1415170"
  },
  {
    "text": "you to integrate with all of the other AWS glue components like data catalog",
    "start": "1415170",
    "end": "1420980"
  },
  {
    "text": "orchestration or scheduling code generation bookmarks as well as",
    "start": "1420980",
    "end": "1426060"
  },
  {
    "text": "integrate with other AWS services natively like s3 RDS and we have more",
    "start": "1426060",
    "end": "1432570"
  },
  {
    "text": "ETL transforms connectors and formats that spark sequel doesn't we also have",
    "start": "1432570",
    "end": "1439290"
  },
  {
    "text": "this new data structure called dynamic frames and I want to get into that next to kind of understand how dynamic frames",
    "start": "1439290",
    "end": "1445950"
  },
  {
    "start": "1442000",
    "end": "1528000"
  },
  {
    "text": "work it kind of we first have to kind of explain how data frame works data frames",
    "start": "1445950",
    "end": "1452010"
  },
  {
    "text": "are the core data structure for spark sequel they're like structured tables in a database system you kind of need to have the schema",
    "start": "1452010",
    "end": "1459490"
  },
  {
    "text": "defined upfront for data frames okay each row in a data frame has the same",
    "start": "1459490",
    "end": "1466660"
  },
  {
    "text": "structure and so what this means is if you're trying to load data and you don't know the schema data frames have to",
    "start": "1466660",
    "end": "1472600"
  },
  {
    "text": "first read all your data and compute the schema and then they can bring it in so you have to do two passes over your data",
    "start": "1472600",
    "end": "1478410"
  },
  {
    "text": "the beautiful thing about this is that if you're working over structured data especially data data that looks tabular",
    "start": "1478410",
    "end": "1484680"
  },
  {
    "text": "and you want to do sequel like analytics as things perfect for doing that but for",
    "start": "1484680",
    "end": "1491200"
  },
  {
    "text": "ETL so it's not quite ideal and so what we",
    "start": "1491200",
    "end": "1496510"
  },
  {
    "text": "did is we introduced dynamic frames that relaxed some of the assumptions about having a schema upfront and having a",
    "start": "1496510",
    "end": "1502690"
  },
  {
    "text": "having the same structure per row they're like data frames but designed",
    "start": "1502690",
    "end": "1508090"
  },
  {
    "text": "for ETL they're designed for processing semi structured data so what I mean what do I mean by semi structured data the",
    "start": "1508090",
    "end": "1515530"
  },
  {
    "text": "data that I showed you earlier the public github timeline you got lots of different event types and the payloads",
    "start": "1515530",
    "end": "1521290"
  },
  {
    "text": "payload structure varies by each different type of event okay and so what",
    "start": "1521290",
    "end": "1527200"
  },
  {
    "text": "ends up happening over here I'm going to show you how we actually represent this data and and dynamic frames what dynamic",
    "start": "1527200",
    "end": "1535810"
  },
  {
    "start": "1528000",
    "end": "1674000"
  },
  {
    "text": "frames are are basically a collection of dynamic records okay and I've given you a sort of a cartoonish version of the",
    "start": "1535810",
    "end": "1543340"
  },
  {
    "text": "public you know github timeline where you have three different event types a create event a push event and a pull",
    "start": "1543340",
    "end": "1549880"
  },
  {
    "text": "event the top line their ID and type is",
    "start": "1549880",
    "end": "1554890"
  },
  {
    "text": "shared amongst though all of these events and the bottom line the payload is actually different and very different",
    "start": "1554890",
    "end": "1560200"
  },
  {
    "text": "from event type to event type for a data frame if you wanted to store all this information you'd have to have a",
    "start": "1560200",
    "end": "1566110"
  },
  {
    "text": "structure that's sort of the union of all of these structures and so you'd have a lot of columns or fields that",
    "start": "1566110",
    "end": "1574060"
  },
  {
    "text": "would be empty okay and so it would look a little bit like the shape at the",
    "start": "1574060",
    "end": "1579610"
  },
  {
    "text": "bottom for dynamic frames and dynamic records dynamic records basically are",
    "start": "1579610",
    "end": "1586810"
  },
  {
    "text": "self describing so you don't actually need to have a schema up front storing this inside of a dynamic frame",
    "start": "1586810",
    "end": "1592159"
  },
  {
    "text": "each record actually stores its own schema and so it can vary from record to record to record what this means is it's",
    "start": "1592159",
    "end": "1599720"
  },
  {
    "text": "really easy to do a whole bunch of restructuring transforms modification",
    "start": "1599720",
    "end": "1604759"
  },
  {
    "text": "transforms and in some cases or in many cases for semi structured data this",
    "start": "1604759",
    "end": "1611299"
  },
  {
    "text": "representation can be more compact than a data frame row an interesting",
    "start": "1611299",
    "end": "1617389"
  },
  {
    "text": "consequence of all of this of course is that many simple flows or even complex flows as long as you don't force a",
    "start": "1617389",
    "end": "1623269"
  },
  {
    "text": "schema computation can be done as single pass so if you have terabytes of data that you want to convert petabytes of",
    "start": "1623269",
    "end": "1629720"
  },
  {
    "text": "data that you want to convert you don't have to go over it twice another thing",
    "start": "1629720",
    "end": "1634789"
  },
  {
    "text": "that I want you to note here is that I've done something very subtle here the IDS in the first two records are strings",
    "start": "1634789",
    "end": "1641779"
  },
  {
    "text": "and the ID and the third record is actually an integer okay SPARC will basically read that and just",
    "start": "1641779",
    "end": "1648830"
  },
  {
    "text": "convert it to whatever it thinks it should convert it to but you may not want that well data frames because each",
    "start": "1648830",
    "end": "1654590"
  },
  {
    "text": "record is sort of you know self-describing we keep that intact and then you can",
    "start": "1654590",
    "end": "1660350"
  },
  {
    "text": "decide later on what you want to do with it so in the dynamic frame schema here at the bottom you'll see on the left",
    "start": "1660350",
    "end": "1666230"
  },
  {
    "text": "hand side there are two different versions of ID one that's an integer and one that's a string alright so what can",
    "start": "1666230",
    "end": "1673639"
  },
  {
    "text": "you do with dynamic frames well we have over 15 transforms and adding more and",
    "start": "1673639",
    "end": "1679190"
  },
  {
    "start": "1674000",
    "end": "1735000"
  },
  {
    "text": "more one of the simplest things that you can do is you can decide how you want to resolve your choice between integer or",
    "start": "1679190",
    "end": "1685279"
  },
  {
    "text": "string and by the way you can have all kinds of choices you can have a choice between a string and a struct an array",
    "start": "1685279",
    "end": "1690409"
  },
  {
    "text": "and a string and so on and it's not so easy to decide what to do with the data frame right are you gonna stringify",
    "start": "1690409",
    "end": "1696619"
  },
  {
    "text": "you're gonna array it here you get to actually specify what you're going to do through the resolve choice transform",
    "start": "1696619",
    "end": "1701809"
  },
  {
    "text": "it's cool little transform you should try it out another one which is kind of a superset lets you resolve choices and",
    "start": "1701809",
    "end": "1709489"
  },
  {
    "text": "do a lot more things like restructure is apply mapping okay so apply mapping will",
    "start": "1709489",
    "end": "1716029"
  },
  {
    "text": "allow you to take structured or semi structured data or nested data and flatten it in any way you want pick",
    "start": "1716029",
    "end": "1722390"
  },
  {
    "text": "the columns that you want rename it basically implements that mapping that we saw on the screen and you can",
    "start": "1722390",
    "end": "1729470"
  },
  {
    "text": "actually go backwards as well you can go from a flat structure to a nested structure a third and so when am our",
    "start": "1729470",
    "end": "1737870"
  },
  {
    "start": "1735000",
    "end": "1831000"
  },
  {
    "text": "most complicated and most interesting transform is relational eyes we tried to",
    "start": "1737870",
    "end": "1743480"
  },
  {
    "text": "actually originally build relational eyes with data frames we found out that it was really hard and very inefficient",
    "start": "1743480",
    "end": "1750200"
  },
  {
    "text": "what it does is it takes any arbitrary schema that you have in a",
    "start": "1750200",
    "end": "1755210"
  },
  {
    "text": "semi-structured schema and turns it into a collection of tables and columns so that you can load it into a data",
    "start": "1755210",
    "end": "1761510"
  },
  {
    "text": "warehouse without the need for support for various choices or without having to",
    "start": "1761510",
    "end": "1767840"
  },
  {
    "text": "have support for arrays what we do is we basically create a column for every path",
    "start": "1767840",
    "end": "1775100"
  },
  {
    "text": "from the root to the leaf and for every array we separate it out into an",
    "start": "1775100",
    "end": "1780560"
  },
  {
    "text": "auxiliary cable that auxiliary table will have multiple elements for that array and it'll join back up with the",
    "start": "1780560",
    "end": "1787850"
  },
  {
    "text": "original table using a primary key foreign key relationship okay and the",
    "start": "1787850",
    "end": "1793040"
  },
  {
    "text": "relational relational eyes transform does this on the fly as it's seeing the data come through right it's pretty cool",
    "start": "1793040",
    "end": "1800810"
  },
  {
    "text": "it adds new columns and types and tables on the fly and we'll load it into redshift it tracks the keys and foreign",
    "start": "1800810",
    "end": "1807350"
  },
  {
    "text": "keys so that you never duplicate those keys again and it's interesting by",
    "start": "1807350",
    "end": "1813500"
  },
  {
    "text": "actually flattening the data and making them columnar and putting into a column or data warehouse which is designed for",
    "start": "1813500",
    "end": "1819170"
  },
  {
    "text": "handling column columnar data your sequel queries now can go basically orders of magnitude faster than if you",
    "start": "1819170",
    "end": "1825500"
  },
  {
    "text": "were just natively processing the json data with one of these other analytic services there are a bunch of other",
    "start": "1825500",
    "end": "1832850"
  },
  {
    "start": "1831000",
    "end": "1865000"
  },
  {
    "text": "useful transforms that we have to DF converts a dynamic frame to a data frame so if we don't have a transform you you",
    "start": "1832850",
    "end": "1840500"
  },
  {
    "text": "know we don't have a transform that you need you can convert it to a data frame and just run your transforms in spark we",
    "start": "1840500",
    "end": "1847100"
  },
  {
    "text": "have spigots you basically insert them in your flow and then you can get samples of your data that are flowing through into s3 so you can debug we've",
    "start": "1847100",
    "end": "1854630"
  },
  {
    "text": "got unbox which will take column and parse it so for example if you have a JSON column in CSV you can",
    "start": "1854630",
    "end": "1861190"
  },
  {
    "text": "unbox that column and start querying that we've got a bunch more you should try it out so what's the bottom line",
    "start": "1861190",
    "end": "1867580"
  },
  {
    "start": "1865000",
    "end": "1917000"
  },
  {
    "text": "here well it turns out that we can go fast because of this remember I told you",
    "start": "1867580",
    "end": "1872980"
  },
  {
    "text": "that we only have to do one pass here is a workload where we convert that JSON to",
    "start": "1872980",
    "end": "1878620"
  },
  {
    "text": "CSV that github timeline data we filter out for just the pull events just the",
    "start": "1878620",
    "end": "1884290"
  },
  {
    "text": "events that are just you know where people are talking about pull ok and then we write it back out in CSV and",
    "start": "1884290",
    "end": "1890820"
  },
  {
    "text": "whether we're converting a day a month or a year worth of data on average we're",
    "start": "1890820",
    "end": "1896020"
  },
  {
    "text": "basically getting 2x the performance of native spark just vanilla Apache spark",
    "start": "1896020",
    "end": "1902560"
  },
  {
    "text": "ok now if you're forced to compute a schema for whatever operation that you want to do we're going to go about as",
    "start": "1902560",
    "end": "1909370"
  },
  {
    "text": "fast as spark does as well so we're not not really losing a lot of performance and in these particular cases you",
    "start": "1909370",
    "end": "1915340"
  },
  {
    "text": "actually gain a bunch of performance another thing that we do is we have some",
    "start": "1915340",
    "end": "1920680"
  },
  {
    "start": "1917000",
    "end": "2049000"
  },
  {
    "text": "optimizations for dealing with streaming data so imagine you have data coming in from Kinesis firehose this data comes in",
    "start": "1920680",
    "end": "1927550"
  },
  {
    "text": "sort of sporadically that's what streaming data is like and so you'll end up with a lot of small files that are empty or just a few records in there and",
    "start": "1927550",
    "end": "1934180"
  },
  {
    "text": "then you'll have files with some big you know lots of records in there and it",
    "start": "1934180",
    "end": "1939820"
  },
  {
    "text": "turns out spark right now has a lot of overhead per file it has it schedules a",
    "start": "1939820",
    "end": "1946120"
  },
  {
    "text": "task per file and the result of that task is all of all those tasks is sent",
    "start": "1946120",
    "end": "1951970"
  },
  {
    "text": "back to a central master or a driver and so if you have hundreds of thousands if",
    "start": "1951970",
    "end": "1958180"
  },
  {
    "text": "not millions of tasks and they're all really small the scheduling overhead is high for doing that work because you're",
    "start": "1958180",
    "end": "1963760"
  },
  {
    "text": "kind of you know communicating for doing very little work and there's memory head overhead for keeping track of you know",
    "start": "1963760",
    "end": "1970870"
  },
  {
    "text": "all the results which are basically null right and so as you scale the number of",
    "start": "1970870",
    "end": "1977530"
  },
  {
    "text": "partitions and number of files in this particular case we're actually going over Twitter data ok at some point spark",
    "start": "1977530",
    "end": "1983800"
  },
  {
    "text": "basically just you know poops out it just doesn't have an enough memory to",
    "start": "1983800",
    "end": "1989410"
  },
  {
    "text": "keep going and you know at around 320 thousand files 160 partitions we're going about",
    "start": "1989410",
    "end": "1997090"
  },
  {
    "text": "seven times faster than spark vanilla spark that is okay two point one why are",
    "start": "1997090",
    "end": "2002820"
  },
  {
    "text": "we able to do this one because we integrate with data catalog and so we have a fairly good really good idea with",
    "start": "2002820",
    "end": "2009059"
  },
  {
    "text": "about all of the partitions and their structure so we don't actually have to go and scan through and list all the",
    "start": "2009059",
    "end": "2016289"
  },
  {
    "text": "files and you know create partitions and so on another thing that we do is we",
    "start": "2016289",
    "end": "2022559"
  },
  {
    "text": "also have a bunch of statistics about those files how many there are per partition and so what we can do is we",
    "start": "2022559",
    "end": "2028710"
  },
  {
    "text": "can actually group those files per task so you can have multiple files handled",
    "start": "2028710",
    "end": "2034080"
  },
  {
    "text": "by a single task and so you don't overwhelm that coordinator and as a result we can get to about a million",
    "start": "2034080",
    "end": "2039419"
  },
  {
    "text": "over a million files in this particular example and we're probably we probably",
    "start": "2039419",
    "end": "2044490"
  },
  {
    "text": "can scale further I just we just didn't finish the experiment to see how far we could go all right so that's dynamic",
    "start": "2044490",
    "end": "2053250"
  },
  {
    "start": "2049000",
    "end": "2217000"
  },
  {
    "text": "frames in a nutshell I encourage you to go and try them out and I want to kind of switch gears a little bit and talk",
    "start": "2053250",
    "end": "2058858"
  },
  {
    "text": "about bookmarks remember I said there are these things called job bookmarks that let the script when you're running",
    "start": "2058859",
    "end": "2064950"
  },
  {
    "text": "from one iteration to the next remember what happened that's what a job bookmark is if you're trying to periodically run",
    "start": "2064950",
    "end": "2071099"
  },
  {
    "text": "a job for example you want to go and practice your process one partition one day's worth of data at a time you don't",
    "start": "2071099",
    "end": "2078450"
  },
  {
    "text": "want to reprocess all the input just to be able to go and process the next day's worth of worth of data all right you",
    "start": "2078450",
    "end": "2085800"
  },
  {
    "text": "want to remember what you process so that the next day you can process just the new stuff more importantly you don't",
    "start": "2085800",
    "end": "2091200"
  },
  {
    "text": "want to also duplicate the output over and over again now you can kind of build",
    "start": "2091200",
    "end": "2096780"
  },
  {
    "text": "this stuff by hand and you can kind of you know come up with ways where you look at the timestamp and sorry that it's time of",
    "start": "2096780",
    "end": "2103950"
  },
  {
    "text": "day it is and kind of come up with a partition that you want to go in process but again it's brittle if you ever",
    "start": "2103950",
    "end": "2109859"
  },
  {
    "text": "change your partitioning structure it won't work and you have to go and you know change all of your scripts again so",
    "start": "2109859",
    "end": "2117990"
  },
  {
    "text": "what we offer are bookmarks and what they do is they basically keep track on a per job basis",
    "start": "2117990",
    "end": "2123630"
  },
  {
    "text": "of the state of the previous runs there basically per job checkpoints they",
    "start": "2123630",
    "end": "2129420"
  },
  {
    "text": "persist the state of all the readers or sources in your script the state of transforms and the state of all the",
    "start": "2129420",
    "end": "2136799"
  },
  {
    "text": "sinks or the targets in your script some of the things that you can do with job bookmarks is process files on a daily",
    "start": "2136799",
    "end": "2143279"
  },
  {
    "text": "basis for our github archive example you can process Ken a cease fire hose data",
    "start": "2143279",
    "end": "2149220"
  },
  {
    "text": "on an hourly basis it looks very similar you can track timestamps on tables and",
    "start": "2149220",
    "end": "2156210"
  },
  {
    "text": "databases or primary keys to see which rows you've already consumed so that you don't Rican soom the same rows and you",
    "start": "2156210",
    "end": "2162509"
  },
  {
    "text": "can also attract the keys and foreign keys that you generate when you normalize your data in particular for the relation wise transform you can also",
    "start": "2162509",
    "end": "2175259"
  },
  {
    "text": "play around with bookmarks where you turn them on and off of course when they're turned off what you're doing is just basically when they're turned on",
    "start": "2175259",
    "end": "2182369"
  },
  {
    "text": "what you're basically doing is just advancing the state of your computation when they're disabled you might want to",
    "start": "2182369",
    "end": "2188069"
  },
  {
    "text": "do this just kind of debug your script to see what's going on all right we don't actually look at the bookmark",
    "start": "2188069",
    "end": "2193470"
  },
  {
    "text": "information it's just ignore it and you just process your entire table there's a",
    "start": "2193470",
    "end": "2198690"
  },
  {
    "text": "third option it's a cool little option which is called pause which means go back to the previous run rerun it but",
    "start": "2198690",
    "end": "2204180"
  },
  {
    "text": "don't advance the state and you might want to do this for example if one of your fought one of your you know runs",
    "start": "2204180",
    "end": "2209309"
  },
  {
    "text": "broke and you wanted to see what the output looked like without having to kind of you know advance the state by",
    "start": "2209309",
    "end": "2214440"
  },
  {
    "text": "yourself so it's a debugging trick that you can use how does this all work well",
    "start": "2214440",
    "end": "2220319"
  },
  {
    "text": "we could keep track of all in every file that we have run for every job and you",
    "start": "2220319",
    "end": "2226470"
  },
  {
    "text": "know make sure that you know look at those files again but that's going to be very complicated and it's gonna require a lot of state so we've got to avoid",
    "start": "2226470",
    "end": "2233099"
  },
  {
    "text": "that space blow up the simplest way to do that is to use time stance time",
    "start": "2233099",
    "end": "2238230"
  },
  {
    "text": "stamps to filter out previously processed input so imagine you're on run number two over there and a run number",
    "start": "2238230",
    "end": "2244650"
  },
  {
    "text": "two started you know listing all your files at t2 and then process them then",
    "start": "2244650",
    "end": "2250200"
  },
  {
    "text": "on the next run only the files that were created after t2 are the ones that you need to look at",
    "start": "2250200",
    "end": "2256090"
  },
  {
    "text": "those are the new files and because that's three files there can't be modified you can look at the timestamps and say you can you know exactly which",
    "start": "2256090",
    "end": "2262510"
  },
  {
    "text": "ones are the new files so that works in most of the cases okay especially when s3 behaves in a strongly",
    "start": "2262510",
    "end": "2269650"
  },
  {
    "text": "consistent way but it doesn't work every once in a while when s3 becomes an eventually consistent system because you",
    "start": "2269650",
    "end": "2275890"
  },
  {
    "text": "can actually have files appear in the next run that you didn't see in the previous run even though their time",
    "start": "2275890",
    "end": "2281260"
  },
  {
    "text": "stamps are younger or sorry older excuse me then when the previous run started so what we",
    "start": "2281260",
    "end": "2288430"
  },
  {
    "text": "do to deal with this inconsistency is we actually keep track of all the files that we saw and an inconsistency window",
    "start": "2288430",
    "end": "2295150"
  },
  {
    "text": "right before the previous run listed all those files and we keep sort of a small",
    "start": "2295150",
    "end": "2300790"
  },
  {
    "text": "Delta and that's configurable and in that and that's basically an exclusion lists of file that we keep track of that",
    "start": "2300790",
    "end": "2307870"
  },
  {
    "text": "you can filter out the next time you run and the subsequent run you basically",
    "start": "2307870",
    "end": "2312970"
  },
  {
    "text": "start listing all your files from the the previous timestamp - this Delta this",
    "start": "2312970",
    "end": "2318100"
  },
  {
    "text": "consistency window these are all the files that you list and then you eliminate the files that you've already",
    "start": "2318100",
    "end": "2323110"
  },
  {
    "text": "seen but keep the files that you haven't seen and you process them and this works really well and this is pretty",
    "start": "2323110",
    "end": "2328660"
  },
  {
    "text": "complicated to get right you don't want to be building this yourself and we provide that underneath for you and this",
    "start": "2328660",
    "end": "2334450"
  },
  {
    "text": "is just for sources we do similar things for transforms and syncs all right so I'm gonna wrap it up now I've shown you",
    "start": "2334450",
    "end": "2340240"
  },
  {
    "start": "2337000",
    "end": "2355000"
  },
  {
    "text": "the four easy steps that you need to build a production ETL flow of described dynamic frames and job bookmarks and how",
    "start": "2340240",
    "end": "2347680"
  },
  {
    "text": "they work and hopefully with these with this knowledge you can go and actually build your own scripts now and get or",
    "start": "2347680",
    "end": "2354610"
  },
  {
    "text": "get going with glue I want to make two additional announcements one we're soon",
    "start": "2354610",
    "end": "2359650"
  },
  {
    "start": "2355000",
    "end": "2384000"
  },
  {
    "text": "going to have Scala support so all your scripts that were in Python right now can do a lot of orchestration sort of",
    "start": "2359650",
    "end": "2366640"
  },
  {
    "text": "you know building these pipelines but a lot of the heavy lifting that you need to do to process your data often needs",
    "start": "2366640",
    "end": "2372100"
  },
  {
    "text": "to be done in Scala and now you're gonna be able to do that with the Scala support there's also two new regions",
    "start": "2372100",
    "end": "2377710"
  },
  {
    "text": "that are coming up asia-pacific or Tokyo and Ireland so you can run your jobs",
    "start": "2377710",
    "end": "2383080"
  },
  {
    "text": "from there alright with that said I want to hand it over to Keith who's going to describe how he",
    "start": "2383080",
    "end": "2388640"
  },
  {
    "start": "2384000",
    "end": "2405000"
  },
  {
    "text": "used glue to get Merc started very quickly and easily thank you so good",
    "start": "2388640",
    "end": "2403220"
  },
  {
    "text": "afternoon everyone my name is Keith Lola I come to you from Merc outside the US",
    "start": "2403220",
    "end": "2410240"
  },
  {
    "start": "2405000",
    "end": "2416000"
  },
  {
    "text": "were actually known as MSD so for those of you who come from outside the US so",
    "start": "2410240",
    "end": "2418609"
  },
  {
    "text": "we actually started looking at glue I think today was announced the way it was",
    "start": "2418609",
    "end": "2425930"
  },
  {
    "text": "today it was made public and one of our biggest challenges was trying to get all",
    "start": "2425930",
    "end": "2431930"
  },
  {
    "text": "the ancillary data we needed for our enterprise software delivery management system up to the system itself so that",
    "start": "2431930",
    "end": "2438710"
  },
  {
    "text": "it can be used and for those of you that maybe don't come from a large enterprise",
    "start": "2438710",
    "end": "2445059"
  },
  {
    "text": "it's the system that we use helps people do the things like Environment",
    "start": "2445059",
    "end": "2450170"
  },
  {
    "text": "Management to understand you know what makes them Bob if they're available in",
    "start": "2450170",
    "end": "2455630"
  },
  {
    "text": "life cycles so maybe they want to use them there during one of their projects or things like that so it actually does",
    "start": "2455630",
    "end": "2462619"
  },
  {
    "text": "a whole bunch of other stuff like deployment planning and scope management",
    "start": "2462619",
    "end": "2468079"
  },
  {
    "text": "but I'm here to talk about glue so our",
    "start": "2468079",
    "end": "2473690"
  },
  {
    "text": "challenge started with we didn't have many integrations at all with with our",
    "start": "2473690",
    "end": "2479269"
  },
  {
    "text": "enterprise system we had a lot of the BI teams that wanted access to this to the",
    "start": "2479269",
    "end": "2485779"
  },
  {
    "text": "data in the software delivery management system itself plus the data that we were going to",
    "start": "2485779",
    "end": "2491040"
  },
  {
    "text": "then actually align it to the context of an application and these projects you",
    "start": "2491040",
    "end": "2496830"
  },
  {
    "text": "know as most people probably recognize a project as a certain time frame they",
    "start": "2496830",
    "end": "2501870"
  },
  {
    "text": "want it they want it now the system they were using to deliver it you know we needed to get it somehow from the raw",
    "start": "2501870",
    "end": "2509640"
  },
  {
    "text": "state and up to the up to the system itself so they could use it in a",
    "start": "2509640",
    "end": "2515130"
  },
  {
    "text": "relatively short timeframe so this",
    "start": "2515130",
    "end": "2521160"
  },
  {
    "start": "2518000",
    "end": "2669000"
  },
  {
    "text": "brings us to summary of our challenges so we had this short timeline to get this data layer together so my team and",
    "start": "2521160",
    "end": "2528720"
  },
  {
    "text": "I were or a little bit of not troublemakers but we we like to push the",
    "start": "2528720",
    "end": "2536100"
  },
  {
    "text": "envelope so and I saw something like glue and we had the opportunity to actually get a data layer together for",
    "start": "2536100",
    "end": "2542640"
  },
  {
    "text": "our system it certainly seemed interesting the one thing that you have",
    "start": "2542640",
    "end": "2547710"
  },
  {
    "text": "to recognize about my team is we are not ETL developers we work with a lot of the",
    "start": "2547710",
    "end": "2553320"
  },
  {
    "text": "project teams that do do ETL development and the different technologies inside",
    "start": "2553320",
    "end": "2558780"
  },
  {
    "text": "the ecosystem but we are not pure developers most of us have some sort of",
    "start": "2558780",
    "end": "2564330"
  },
  {
    "text": "background in our education or prior experience where we've done some codeine",
    "start": "2564330",
    "end": "2570500"
  },
  {
    "text": "additionally the only support that we really had available because I mean we",
    "start": "2570990",
    "end": "2576390"
  },
  {
    "text": "were using glue the day after so our cloud services team helped us get it",
    "start": "2576390",
    "end": "2581880"
  },
  {
    "text": "connected the day came out set up in our enterprise and then I probably started",
    "start": "2581880",
    "end": "2588540"
  },
  {
    "text": "pinging mail my whole and his team on a Wednesday asking about some individual questions and things but from there it",
    "start": "2588540",
    "end": "2597270"
  },
  {
    "text": "was interesting because we started exploring a lot of our data that we needed to bring in and as you can",
    "start": "2597270",
    "end": "2603420"
  },
  {
    "text": "imagine we're an enterprise to enterprise company and we've been around for I forget what the slide says over",
    "start": "2603420",
    "end": "2609510"
  },
  {
    "text": "honor and I think a hundred over a hundred years let's just say so we have",
    "start": "2609510",
    "end": "2615300"
  },
  {
    "text": "a lot of legacy systems we have even I think some frame but probably not so you can",
    "start": "2615300",
    "end": "2623560"
  },
  {
    "text": "imagine a different amount of data that we needed to bring in to align to an application and it was definitely a",
    "start": "2623560",
    "end": "2628869"
  },
  {
    "text": "different people had in the spreadsheets a ton team sites that existing databases",
    "start": "2628869",
    "end": "2633940"
  },
  {
    "text": "and we have a seem to be system of course so one of the things I started with looking at is hoping that I can",
    "start": "2633940",
    "end": "2641350"
  },
  {
    "text": "find some way not to have to move the data because everyone typically takes",
    "start": "2641350",
    "end": "2646930"
  },
  {
    "text": "the data wants to warehouse it but then we have multiple copies of the data everywhere so when I saw the you know we",
    "start": "2646930",
    "end": "2654070"
  },
  {
    "text": "had a easy way to actually catalog the data we started pushing to that Avenue a",
    "start": "2654070",
    "end": "2660340"
  },
  {
    "text": "little bit more but because we're a legacy because we have a lot of legacy systems we had to keep the warehousing",
    "start": "2660340",
    "end": "2666610"
  },
  {
    "text": "option available to us so what we came",
    "start": "2666610",
    "end": "2671740"
  },
  {
    "start": "2669000",
    "end": "2911000"
  },
  {
    "text": "up with was and by no means was this something that we started day one we did",
    "start": "2671740",
    "end": "2677980"
  },
  {
    "text": "take a very step-by-step approach what our sister what our back-end system but",
    "start": "2677980",
    "end": "2684280"
  },
  {
    "text": "our sorry data layer actually looks like now is we have here's where we have a",
    "start": "2684280",
    "end": "2689380"
  },
  {
    "text": "number of our different sources whether they're coming from the team side or whatnot we have a lambda function that will",
    "start": "2689380",
    "end": "2696070"
  },
  {
    "text": "prepare it and drop it in the s3 bucket we do have some local DBS and then we",
    "start": "2696070",
    "end": "2702490"
  },
  {
    "text": "have some databases that run in RDS so the key for us was being able to being",
    "start": "2702490",
    "end": "2709210"
  },
  {
    "text": "able to get all this into one place and the group called the glue crawler I think actually on the run Wednesday we",
    "start": "2709210",
    "end": "2719200"
  },
  {
    "text": "were already moving data we had some raw data files we had some data files here",
    "start": "2719200",
    "end": "2724270"
  },
  {
    "text": "and the very first thing we did was what males showed you in the presentation we",
    "start": "2724270",
    "end": "2730270"
  },
  {
    "text": "set up a crawler we let Lou make the trench like glue make the script and we",
    "start": "2730270",
    "end": "2737530"
  },
  {
    "text": "sent it to a database just so we could see how everything was moving forward and try to understand the transforms",
    "start": "2737530",
    "end": "2743470"
  },
  {
    "text": "because you know as I said like my team's not developers so you know I do",
    "start": "2743470",
    "end": "2749800"
  },
  {
    "text": "have some teammates in the room Bryan and Jeff over here and Rajesh was",
    "start": "2749800",
    "end": "2755740"
  },
  {
    "text": "really good with sword procedures he can do it wonders with the stored procedure so we pushed it through day one and said let's",
    "start": "2755740",
    "end": "2763630"
  },
  {
    "text": "see what a stored procedure looks like so the group the glue crawler I did that",
    "start": "2763630",
    "end": "2769410"
  },
  {
    "text": "my team wasn't ready we didn't have a lot of resource available I did that myself I do have a little bit of a",
    "start": "2769410",
    "end": "2776619"
  },
  {
    "text": "development background but you know as we grow up through the enterprise I'm",
    "start": "2776619",
    "end": "2781680"
  },
  {
    "text": "now more in a leadership position so",
    "start": "2781680",
    "end": "2787599"
  },
  {
    "text": "once we had the crawler in place to update the catalog then we started to",
    "start": "2787599",
    "end": "2793089"
  },
  {
    "text": "understand a little bit more about that so the day one first run was basically just getting that one script over to",
    "start": "2793089",
    "end": "2799630"
  },
  {
    "text": "glue and we established that RDS database so Rajesh can do his store first you know establish a store",
    "start": "2799630",
    "end": "2806200"
  },
  {
    "text": "procedure but what we do what we've done from there is we've taken those store",
    "start": "2806200",
    "end": "2812109"
  },
  {
    "text": "procedures and we've the limit eliminated them and starting I'm just doing this just doing",
    "start": "2812109",
    "end": "2818500"
  },
  {
    "text": "it writing glue right in the PI spark script we do tend to we do we have",
    "start": "2818500",
    "end": "2825550"
  },
  {
    "text": "extended it just back to Python just trying to leverage some of the PI spark libraries itself so from here then you",
    "start": "2825550",
    "end": "2837579"
  },
  {
    "text": "know we'll use the glue transform see it and leverage the catalog we do do we'd",
    "start": "2837579",
    "end": "2844630"
  },
  {
    "text": "have done some calculated data and transactional data from the source so we're actually adding on to the data",
    "start": "2844630",
    "end": "2851579"
  },
  {
    "text": "because how it relates to the and solution and and then we have a lambda",
    "start": "2851579",
    "end": "2858400"
  },
  {
    "text": "function that will then send it send it up to our enterprise software delivery management system so the one thing",
    "start": "2858400",
    "end": "2866410"
  },
  {
    "text": "you'll notice here is we recognize that our bi teams so here's how we give the bi teams access to to this system so",
    "start": "2866410",
    "end": "2873819"
  },
  {
    "text": "after we feed the enterprise delivery management system actually in parallel they have access to read the RDS",
    "start": "2873819",
    "end": "2881290"
  },
  {
    "text": "database but what we're trying to help them with is is actually just reading the catalogue itself so they have access via",
    "start": "2881290",
    "end": "2889180"
  },
  {
    "text": "their bi tools to use the same catalogue everyone else does because ultimately what we do anyway is where the crawler",
    "start": "2889180",
    "end": "2897250"
  },
  {
    "text": "is actually just reading the catalogue enough is actually just reading their IDs database and updating the catalogue",
    "start": "2897250",
    "end": "2902730"
  },
  {
    "text": "so we provide the option to use the RDS database at this point more from a",
    "start": "2902730",
    "end": "2908860"
  },
  {
    "text": "legacy aspect so for us being able",
    "start": "2908860",
    "end": "2917320"
  },
  {
    "text": "leverage the existing resources was important finding something it was quick",
    "start": "2917320",
    "end": "2922840"
  },
  {
    "text": "and easy to use that was important in blue it was it's pretty easy to use and",
    "start": "2922840",
    "end": "2928480"
  },
  {
    "text": "from what I can see there's a lot you can do with it even extend it beyond we",
    "start": "2928480",
    "end": "2935410"
  },
  {
    "text": "didn't have to set up a single server and we didn't have to go to a single shared service to ask them you know to",
    "start": "2935410",
    "end": "2942010"
  },
  {
    "text": "write code for us or provide requirements for them to do that so our",
    "start": "2942010",
    "end": "2947740"
  },
  {
    "text": "timeline was really short we were within inside of a week moving the data through",
    "start": "2947740",
    "end": "2954520"
  },
  {
    "text": "to our system and since then we've just been perfecting it to make it faster to",
    "start": "2954520",
    "end": "2962020"
  },
  {
    "text": "do more in line and less in what and less in the database itself adapting",
    "start": "2962020",
    "end": "2969670"
  },
  {
    "text": "into data sources this was an interesting one so my my manager actually gave me a new file from a part",
    "start": "2969670",
    "end": "2975550"
  },
  {
    "text": "of the company and said you know here it is how long is it going to take you I said about 15 minutes he said I'm gonna",
    "start": "2975550",
    "end": "2983770"
  },
  {
    "text": "tell him an hour I'm like 15 minutes it's gonna take me 15 minutes but you",
    "start": "2983770",
    "end": "2989800"
  },
  {
    "text": "know it's not it's a it's actually really easy to move it through so now we",
    "start": "2989800",
    "end": "2996700"
  },
  {
    "text": "have it from we have a single source of data we're on our way to make it more of",
    "start": "2996700",
    "end": "3002130"
  },
  {
    "text": "a data Lake and getting people to actually utilize the catalog is key to that so I can get rid of the RDS",
    "start": "3002130",
    "end": "3009000"
  },
  {
    "text": "database and look it's they may be like dynamodb is what we're looking at Nexus and the biggest win for",
    "start": "3009000",
    "end": "3015969"
  },
  {
    "text": "us was our projects got what they wanted so they got extended data about their",
    "start": "3015969",
    "end": "3021459"
  },
  {
    "text": "environments and about their applications that came from multiple different sources and they can see it in",
    "start": "3021459",
    "end": "3027729"
  },
  {
    "text": "one place and they can focus on managing their application and not having to manage you",
    "start": "3027729",
    "end": "3035199"
  },
  {
    "text": "know moving the data around so glue for us was an interesting timing interesting",
    "start": "3035199",
    "end": "3041130"
  },
  {
    "text": "offering at the right time and from where I can see you know sky's the limit",
    "start": "3041130",
    "end": "3047859"
  },
  {
    "text": "so thanks [Applause]",
    "start": "3047859",
    "end": "3054050"
  }
]