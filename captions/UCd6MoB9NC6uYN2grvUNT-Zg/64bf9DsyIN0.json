[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "good morning my name is Paul Sears I'm a Solutions Architect with AWS on the",
    "start": "709",
    "end": "6870"
  },
  {
    "text": "partner organization and I work with big data partners we're happy you're here today and I'm joined today with Daniel",
    "start": "6870",
    "end": "14820"
  },
  {
    "text": "woodlands and Anna Kepler from via set and we're going to basically discuss",
    "start": "14820",
    "end": "21330"
  },
  {
    "text": "optimizing storage on big data workloads as well as listen to their story of a",
    "start": "21330",
    "end": "27269"
  },
  {
    "text": "big data so we're gonna get started here so what we call Big Data incorporates",
    "start": "27269",
    "end": "34170"
  },
  {
    "start": "30000",
    "end": "112000"
  },
  {
    "text": "many kind of workloads we're going to cover five workloads today we're gonna go through them pretty fast so we have",
    "start": "34170",
    "end": "41040"
  },
  {
    "text": "Hadoop with Amazon EMR data warehouse with Amazon redshift streaming Amazon",
    "start": "41040",
    "end": "47340"
  },
  {
    "text": "Kinesis no sequel of Amazon Amazon DynamoDB and search with Amazon Elastic",
    "start": "47340",
    "end": "54210"
  },
  {
    "text": "search so we're actually going to be focusing on the other versions of these",
    "start": "54210",
    "end": "59399"
  },
  {
    "text": "for example if you wanted to run your own Hadoop or your own I called it cloud cloud era or Hortonworks or map R or you",
    "start": "59399",
    "end": "67170"
  },
  {
    "text": "did your own data warehouse with Vertica ter data we're going to focus on those choices today to allow you to figure out",
    "start": "67170",
    "end": "74040"
  },
  {
    "text": "or determine the optimal storage you should be using for those workloads so if you're using Amazon services the",
    "start": "74040",
    "end": "82020"
  },
  {
    "text": "advantage of that is that a managed for you you don't need to worry about your storage provisioning you don't need to worry about your performance your",
    "start": "82020",
    "end": "87990"
  },
  {
    "text": "throughput because all that's managed for you you don't need to worry about patching or anything like that so you just can use the services build your",
    "start": "87990",
    "end": "95520"
  },
  {
    "text": "applications do your big data applications analytics and and so on but if you want to do it yourself then I'm",
    "start": "95520",
    "end": "101939"
  },
  {
    "text": "gonna help you at least give you an introduction and to think about the different kinds of i/o patterns you're",
    "start": "101939",
    "end": "107250"
  },
  {
    "text": "gonna see with these workloads that will then allow you to choose the right kind of storage so with the Big Data we kinda",
    "start": "107250",
    "end": "116159"
  },
  {
    "start": "112000",
    "end": "162000"
  },
  {
    "text": "volved from with modern Hadoop coming around you know late 90s early thousands",
    "start": "116159",
    "end": "122490"
  },
  {
    "text": "with as commodity of hardware because back then and your data centers you had",
    "start": "122490",
    "end": "127500"
  },
  {
    "text": "these large you know San or dreck attached storage appliances",
    "start": "127500",
    "end": "132640"
  },
  {
    "text": "the disks were very expensive the service were very expensive so Hadoop came around with design of using very",
    "start": "132640",
    "end": "140520"
  },
  {
    "text": "inexpensive commodity hardware making it replicated local times for weed",
    "start": "140520",
    "end": "146650"
  },
  {
    "text": "durability and that way you can save the costs and scale up some large clusters and get your performance around loot for",
    "start": "146650",
    "end": "155380"
  },
  {
    "text": "your big data applications and projects so it really kind of awarded the network as storage as much as possible because",
    "start": "155380",
    "end": "161560"
  },
  {
    "text": "that was expensive so now with the cloud that all changes so network storage",
    "start": "161560",
    "end": "168130"
  },
  {
    "start": "162000",
    "end": "198000"
  },
  {
    "text": "which we're going to talk about today gives you persistence it gives you the ability to decouple your compute from",
    "start": "168130",
    "end": "175330"
  },
  {
    "text": "your storage so in the old way you had to upgrade your hardware to get more",
    "start": "175330",
    "end": "180459"
  },
  {
    "text": "disks or add more capacity beyond a certain point it also the ability to do things like snapshots and you also can",
    "start": "180459",
    "end": "188470"
  },
  {
    "text": "use things like ephemeral storage in certain use cases this can be very fast but it's very temporary storage and will",
    "start": "188470",
    "end": "196390"
  },
  {
    "text": "kind of go through that in a few minutes so to briefly recap our building blocks",
    "start": "196390",
    "end": "202360"
  },
  {
    "start": "198000",
    "end": "235000"
  },
  {
    "text": "of storage on AWS we have our object storage which is Amazon s3 and Amazon",
    "start": "202360",
    "end": "208000"
  },
  {
    "text": "glacier we have our file based storage which is Amazon EFS this is more or less",
    "start": "208000",
    "end": "214299"
  },
  {
    "text": "like a giant NFS server that scales automatically for you and highly durable and then we have Amazon or our block",
    "start": "214299",
    "end": "221440"
  },
  {
    "text": "services with Amazon EF EBS and the block storage is the raw discs you would",
    "start": "221440",
    "end": "227110"
  },
  {
    "text": "see normally like you would on a server in this case there will be attached over the network so let's go ahead and get",
    "start": "227110",
    "end": "233470"
  },
  {
    "text": "deeper into what EBS is so block storage",
    "start": "233470",
    "end": "238720"
  },
  {
    "text": "we have two offerings I mentioned EBS and also I mentioned ephemeral storage so the instant store is a thermal",
    "start": "238720",
    "end": "246790"
  },
  {
    "text": "storage and then we have our EBS volumes that are SSD backed and we have these",
    "start": "246790",
    "end": "253360"
  },
  {
    "text": "are come into two flavors gp2 and i/o one and they're based on performance",
    "start": "253360",
    "end": "258970"
  },
  {
    "text": "characteristics throughput in terms of I ops and then we have our EBS HDD back",
    "start": "258970",
    "end": "264760"
  },
  {
    "text": "volumes what our magnetic storage and these volumes come in two flavors st1 sc1 based on the overall throughput",
    "start": "264760",
    "end": "272680"
  },
  {
    "text": "you need for that storage so what is instance store in store is the actual",
    "start": "272680",
    "end": "279009"
  },
  {
    "text": "disks attached to the instance that your are the hardware the servers that you're",
    "start": "279009",
    "end": "284410"
  },
  {
    "text": "innocent runs on and this is transient storage it's on persistence now if you",
    "start": "284410",
    "end": "289419"
  },
  {
    "text": "reboot your instance you still get access to your data on that storage however if you terminate your instance",
    "start": "289419",
    "end": "295900"
  },
  {
    "text": "or if the host has a problem of failure you will lose that data so it's meant to",
    "start": "295900",
    "end": "302919"
  },
  {
    "text": "be in ephemeral and temporary it's usually very fast because it's directly attached to the server the data is not",
    "start": "302919",
    "end": "308650"
  },
  {
    "text": "replicated by default there's no additional durability to it and there's no support for things like snapshots and",
    "start": "308650",
    "end": "314080"
  },
  {
    "text": "it does come in SSD and HDD HD formats so how does this compare to EBS so EBS",
    "start": "314080",
    "end": "321699"
  },
  {
    "text": "is our block stored as a service you get it over basically it's over the network",
    "start": "321699",
    "end": "327759"
  },
  {
    "text": "storage designed with five nines availability and you access it via API",
    "start": "327759",
    "end": "333280"
  },
  {
    "text": "calls so you can attach and detach volumes to your instances using API",
    "start": "333280",
    "end": "338650"
  },
  {
    "text": "calls and we'll see some advantage of this shortly in our discussion and the key here is EBS is over the network so",
    "start": "338650",
    "end": "347340"
  },
  {
    "text": "EBS is a service it's not the actual disks themselves you're not getting",
    "start": "347340",
    "end": "352659"
  },
  {
    "text": "physical disks assigned to your instance you're going to get part of a giant pool",
    "start": "352659",
    "end": "358539"
  },
  {
    "text": "of capacity available to you about that are shared out as volumes that you then attach to your instances so the volumes",
    "start": "358539",
    "end": "366970"
  },
  {
    "start": "364000",
    "end": "560000"
  },
  {
    "text": "and EBS they persist regardless of your instance state so you can terminate your",
    "start": "366970",
    "end": "372820"
  },
  {
    "text": "instance you can create new ones and your storage is still available still there so this allows you to decouple",
    "start": "372820",
    "end": "379180"
  },
  {
    "text": "your compute from your storage let's say you decided in a certain instance size and your performance is okay and then",
    "start": "379180",
    "end": "387250"
  },
  {
    "text": "your data size gets bigger or you need more more CPU power you can detach that storage get a different instance size",
    "start": "387250",
    "end": "393969"
  },
  {
    "text": "rebuild your app your instance and then attach that storage back and have your data available so that's that kind of",
    "start": "393969",
    "end": "399860"
  },
  {
    "text": "key thing and this it does you can attach and detach within the same availability zone okay keep on here as",
    "start": "399860",
    "end": "405770"
  },
  {
    "text": "well so when you build your applications you have a durability within AZ and you can move the instance the volumes around",
    "start": "405770",
    "end": "412669"
  },
  {
    "text": "two instances within an AZ and one more",
    "start": "412669",
    "end": "418520"
  },
  {
    "text": "information here we go okay so so when you have so many options comes down to",
    "start": "418520",
    "end": "423740"
  },
  {
    "text": "how do you really choose the right volume type you want to use so we're going to get into specific io patterns",
    "start": "423740",
    "end": "430819"
  },
  {
    "text": "shortly but if you really don't have any preference gp2 are EBS SSD volumes",
    "start": "430819",
    "end": "439939"
  },
  {
    "text": "general-purpose volumes it's probably a really good choice has really good or good performance I ops up to 10,000 of",
    "start": "439939",
    "end": "448810"
  },
  {
    "text": "based on that profile and your volume size and if the cost cost as Miller Road",
    "start": "448810",
    "end": "454909"
  },
  {
    "text": "and so if you're if you don't really care GP to is a good choice if you really need a lot of high I ops then you",
    "start": "454909",
    "end": "463009"
  },
  {
    "text": "can go with io1 volumes and these can your eye ups are greater than a thousand",
    "start": "463009",
    "end": "468830"
  },
  {
    "text": "or sorry under eighty thousand but more than ten thousand you can actually provision I ops and guarantee a certain",
    "start": "468830",
    "end": "474110"
  },
  {
    "text": "amount of performance from these volumes now sometimes you may need to go either",
    "start": "474110",
    "end": "480169"
  },
  {
    "text": "higher higher than that so on our I three volumes these are ephemeral store SSDs but to the new nvme SSDs and you",
    "start": "480169",
    "end": "489710"
  },
  {
    "text": "can get three million I ops on these particular volume the data is not permanent so if you shut off your",
    "start": "489710",
    "end": "497000"
  },
  {
    "text": "instance or something happens to it you may have to recreate your data but it's a very high performance on the thermal",
    "start": "497000",
    "end": "503479"
  },
  {
    "text": "storage and on the other side we have four HD but HDD back volumes we have our",
    "start": "503479",
    "end": "511069"
  },
  {
    "text": "SC one which is a lower throughput average throughput but it's also a lower",
    "start": "511069",
    "end": "516409"
  },
  {
    "text": "cost and so we also have our st one volumes higher average throughput and",
    "start": "516409",
    "end": "522940"
  },
  {
    "text": "higher burst throughput as well as a little more performance than you would on NC one so you can kind of see the",
    "start": "522940",
    "end": "530449"
  },
  {
    "text": "contrast here between s these HDD DS and you can make a choice",
    "start": "530449",
    "end": "536400"
  },
  {
    "text": "here based on what you need for your particular application and your innocence size and type and also we have",
    "start": "536400",
    "end": "543780"
  },
  {
    "text": "ephemeral storage for hard HDD volumes the d2 instances for example this is a",
    "start": "543780",
    "end": "550790"
  },
  {
    "text": "higher performance in terms of throughput but it's gain is ephemeral and it will the data data is not",
    "start": "550790",
    "end": "557280"
  },
  {
    "text": "persistent so you need to deal with those persistent issues so if you look at the landscape of big data analytics",
    "start": "557280",
    "end": "564110"
  },
  {
    "start": "560000",
    "end": "612000"
  },
  {
    "text": "it was a big move to put move data on to Amazon s3 which is our super durable storage 11",
    "start": "564110",
    "end": "570630"
  },
  {
    "text": "lines of durability and if you also map out other analytics applications and",
    "start": "570630",
    "end": "577020"
  },
  {
    "text": "moving down the hole the whole set of workloads recover here data warehouse streaming and catalog very nixing no",
    "start": "577020",
    "end": "584850"
  },
  {
    "text": "sequel databases and search all fit really well in ec2 instance store or",
    "start": "584850",
    "end": "590850"
  },
  {
    "text": "Amazon EBS there are some in some limited uses for Amazon EFS for example",
    "start": "590850",
    "end": "597870"
  },
  {
    "text": "if using grid computing you may need have some sort of shared file system where EFS could be a good choice but in",
    "start": "597870",
    "end": "603570"
  },
  {
    "text": "general most workloads that you're going to deploy on ec2 are gonna fall into",
    "start": "603570",
    "end": "609030"
  },
  {
    "text": "either using instance store or EBS so we're going to dive into Hadoop really",
    "start": "609030",
    "end": "615000"
  },
  {
    "start": "612000",
    "end": "677000"
  },
  {
    "text": "quick so the key thing about Hadoop is that uses a file system called HDFS and",
    "start": "615000",
    "end": "621200"
  },
  {
    "text": "when it was designed it will design with commodity Hardware in mind the disks",
    "start": "621200",
    "end": "626370"
  },
  {
    "text": "really weren't very fast the network wasn't always very fast as well so it did everything as sequential i/o only",
    "start": "626370",
    "end": "632910"
  },
  {
    "text": "and it's a very large block size now I'm talking about just straight Hadoop itself HDFS and so the block sizes for",
    "start": "632910",
    "end": "641580"
  },
  {
    "text": "data reads and writes would be either in blocks or 64 megabytes or humm 20 megabytes or 256 depending on the how",
    "start": "641580",
    "end": "649110"
  },
  {
    "text": "you set up in and configure your Hadoop cluster so the data is distributed across all nodes in the cluster well",
    "start": "649110",
    "end": "656070"
  },
  {
    "text": "it's distributed and it's it's basically replicated three times you if you want",
    "start": "656070",
    "end": "661200"
  },
  {
    "text": "to have a chunk of data they're gonna have actually two extra copies of it so that means your cluster Hadoop would be three times bigger than",
    "start": "661200",
    "end": "669090"
  },
  {
    "text": "you really know wouldn't need to do if you just did it on a local server but again because it's it's sequential i/o",
    "start": "669090",
    "end": "674840"
  },
  {
    "text": "Hadoop works really well with whoops sorry I can't go here I do per drill",
    "start": "674840",
    "end": "680850"
  },
  {
    "start": "677000",
    "end": "741000"
  },
  {
    "text": "well with our HD vacuum htd back volumes now you also can do what's called an",
    "start": "680850",
    "end": "686250"
  },
  {
    "text": "HDFS which is Hadoop compatible file system and this is what we do with Amazon EMR it is you map the API call so",
    "start": "686250",
    "end": "693510"
  },
  {
    "text": "Amazon s3 into the H the HDFS API calls and long as you can do that you can run",
    "start": "693510",
    "end": "700220"
  },
  {
    "text": "HDFS on any type of file system you want to as long as you have a compatibility",
    "start": "700220",
    "end": "705900"
  },
  {
    "text": "you even can run it on Amazon EFS as I mentioned here EMR with support for s3 a",
    "start": "705900",
    "end": "714120"
  },
  {
    "text": "and s3 n because Amazon s3 is has some the eventual consistency issues so we",
    "start": "714120",
    "end": "721590"
  },
  {
    "text": "using things like s3 a and f3 n you can map and you can also deal with some of",
    "start": "721590",
    "end": "726900"
  },
  {
    "text": "those consistency challenges you have Amazon Athena which is a presto under",
    "start": "726900",
    "end": "732000"
  },
  {
    "text": "the covers it talks to Amazon basically talk to HDFS through a through the HDFS",
    "start": "732000",
    "end": "740430"
  },
  {
    "text": "interface so looking at Amazon s3 as your HDFS this is if you would launch",
    "start": "740430",
    "end": "746880"
  },
  {
    "start": "741000",
    "end": "751000"
  },
  {
    "text": "your EMR cluster you would likely choose have your your HDFS on s3 there are some",
    "start": "746880",
    "end": "752340"
  },
  {
    "text": "advantages you can scale out horizontally without any data distribution data distribution issues it",
    "start": "752340",
    "end": "758730"
  },
  {
    "text": "just scales in scales and scales you don't need to have any separate disaster",
    "start": "758730",
    "end": "763830"
  },
  {
    "text": "recovery because Amazon s3 has 11 nines of durability it's gonna be far greater you can do in any type of data center or",
    "start": "763830",
    "end": "770400"
  },
  {
    "text": "do it yourself on EBS and it's also really a really good place for transient",
    "start": "770400",
    "end": "776370"
  },
  {
    "text": "clusters clusters that are running part time so for example if you're storing your data in HDFS your clusters need to",
    "start": "776370",
    "end": "783210"
  },
  {
    "text": "be available for that namespace for the data be available so they need to run as long as well that data if you have it in",
    "start": "783210",
    "end": "788550"
  },
  {
    "text": "s3 and you're accessing s3 be HDFS 3m iam RFS that data can be can persist and",
    "start": "788550",
    "end": "795210"
  },
  {
    "text": "be available outside of a running Hadoop cluster but there also some managers as well so I mentioned earlier",
    "start": "795210",
    "end": "801120"
  },
  {
    "start": "798000",
    "end": "861000"
  },
  {
    "text": "the eventual consistency challenge of s3 is a challenge for HDFS so for example",
    "start": "801120",
    "end": "809220"
  },
  {
    "text": "if you do a rename operation what it does in terms of Hadoop or HDFS it",
    "start": "809220",
    "end": "815520"
  },
  {
    "text": "actually just renames the the node in the data structure itself for the for the file system and you're done",
    "start": "815520",
    "end": "820560"
  },
  {
    "text": "and Amazon s3 it actually has to copy the data to a new file so it's not an atomic operation and this has a",
    "start": "820560",
    "end": "826740"
  },
  {
    "text": "challenge with the data becoming consist eventually consistent there are ways of",
    "start": "826740",
    "end": "832010"
  },
  {
    "text": "helping along you can use things like dynamo DB behind the scenes to keep track of metadata which is actually what",
    "start": "832010",
    "end": "838110"
  },
  {
    "text": "a MRFs does there's also list operations on large numbers of files and s3 could",
    "start": "838110",
    "end": "844020"
  },
  {
    "text": "be a little problematic and security the there there doesn't only support I am I",
    "start": "844020",
    "end": "850320"
  },
  {
    "text": "am roles very well and you're seeing that s3 a is starting to address some of",
    "start": "850320",
    "end": "856140"
  },
  {
    "text": "these security challenges working with Amazon s3 as your store for HDFS so when",
    "start": "856140",
    "end": "863160"
  },
  {
    "start": "861000",
    "end": "975000"
  },
  {
    "text": "you want you to use HDFS on EBS so one of the key things here is if you really",
    "start": "863160",
    "end": "868560"
  },
  {
    "text": "want high ops and performance you're going to deploy HD HDFS on top of an EBS",
    "start": "868560",
    "end": "874470"
  },
  {
    "text": "set of volumes so it also depends on your cluster types if you're gonna",
    "start": "874470",
    "end": "880170"
  },
  {
    "text": "running long-running clusters then you're in managing your space namespace then running your data on your any HDFS",
    "start": "880170",
    "end": "888180"
  },
  {
    "text": "on EBS makes a lot of sense also if you're running in a distribution like cloud era or Hortonworks or map R then",
    "start": "888180",
    "end": "894330"
  },
  {
    "text": "you're likely going to need to leverage those file systems particularly in map r2 which will require you to run your",
    "start": "894330",
    "end": "900690"
  },
  {
    "text": "over abs versus s3 so now that you understand that Hadoop is sequential i/o",
    "start": "900690",
    "end": "909000"
  },
  {
    "text": "and you can optimize how you optimize your decision and what type of storage you use for for this i/o so you can use",
    "start": "909000",
    "end": "917700"
  },
  {
    "text": "Amazon ec2 instance store addi two instances for example and you can get three gigabits gigabytes a second",
    "start": "917700",
    "end": "924270"
  },
  {
    "text": "throughput but basically with Hadoop the the network and the disk really aren't",
    "start": "924270",
    "end": "932130"
  },
  {
    "text": "the bottleneck usually get you compute power so you're actually going to hit your compute ceilings faster they're going to hit",
    "start": "932130",
    "end": "938010"
  },
  {
    "text": "your storage ceilings so in this case using Amazon st-1 volumes are probably a",
    "start": "938010",
    "end": "943320"
  },
  {
    "text": "better choice overall you can have a really good burst rate with st-1 the",
    "start": "943320",
    "end": "949200"
  },
  {
    "text": "especially views with a two terabyte volume size you also can also can lower your replication factor from three times",
    "start": "949200",
    "end": "955860"
  },
  {
    "text": "the two times using EBS because EBS automatically replicates your data at",
    "start": "955860",
    "end": "961170"
  },
  {
    "text": "one time within an availability zone so you get additional durability because EBS and that way you don't need to have",
    "start": "961170",
    "end": "966870"
  },
  {
    "text": "additional copies through the HDFS of that data and so in here you'll say st 1",
    "start": "966870",
    "end": "973620"
  },
  {
    "text": "st ones are good choices ok so moving on to data warehouses Amazon redshift is",
    "start": "973620",
    "end": "979380"
  },
  {
    "start": "975000",
    "end": "1048000"
  },
  {
    "text": "our column our data warehouse a managed database that if you wanted to use that",
    "start": "979380",
    "end": "984450"
  },
  {
    "text": "you don't really need to worry about storage you just pick whether you want a storage dense nodes or you want compute",
    "start": "984450",
    "end": "990750"
  },
  {
    "text": "dense notes for your cluster but if you're gonna run Vertica for example Vertica is a memory mapped database it",
    "start": "990750",
    "end": "997770"
  },
  {
    "text": "does all its changes in memory and then flushes those changes in the disk so it's actually a really really good",
    "start": "997770",
    "end": "1003560"
  },
  {
    "text": "candidate for sequential i/o SC one st one storage are good options for running",
    "start": "1003560",
    "end": "1009590"
  },
  {
    "text": "a vertical cluster because they don't really the cluster doesn't really worry so much about performance of the i/o if",
    "start": "1009590",
    "end": "1015500"
  },
  {
    "text": "you're you looking at terror data chair data is and also data warehouse but it's row oriented and not column are based",
    "start": "1015500",
    "end": "1021140"
  },
  {
    "text": "and so it's sniffily more random i/o in this case for random i/o you're gonna",
    "start": "1021140",
    "end": "1027470"
  },
  {
    "text": "want to focus on GP 2 i/o 1 volumes or even the i2 i3 ephemeral storage",
    "start": "1027470",
    "end": "1033470"
  },
  {
    "text": "instances for the higher performance if you really want to work on you know if that's a concern for you but so for roll",
    "start": "1033470",
    "end": "1040010"
  },
  {
    "text": "bro based terror data its GP 2 or SSD based storage and for Vertica it's hard",
    "start": "1040010",
    "end": "1046189"
  },
  {
    "text": "disks or HD back volumes so look at no sequel we have Amazon DynamoDB we also",
    "start": "1046190",
    "end": "1054890"
  },
  {
    "text": "can look at Cassandra and MongoDB now I'm wanting to give you a brief overview",
    "start": "1054890",
    "end": "1061250"
  },
  {
    "text": "of these because we actually have a many we actually have a number of sessions this week focusing deep into no sequel on EBS so",
    "start": "1061250",
    "end": "1069310"
  },
  {
    "text": "I'll share those with you in a minute so with uh where's my slide again so with",
    "start": "1069310",
    "end": "1077680"
  },
  {
    "start": "1076000",
    "end": "1137000"
  },
  {
    "text": "no sequel the i/o pattern is predominately random it's gonna be 4k extents those till the Block Street",
    "start": "1077680",
    "end": "1083560"
  },
  {
    "text": "you're using a 4k in size much smaller than in HDFS sequential read or write of",
    "start": "1083560",
    "end": "1089170"
  },
  {
    "text": "a 64 megabytes are larger so the ion pattern for those sequel was a lot of 4k",
    "start": "1089170",
    "end": "1094180"
  },
  {
    "text": "extents that I mapped a randomly around in memory and on disk supposed to be a random i/o pattern so in this case for",
    "start": "1094180",
    "end": "1101020"
  },
  {
    "text": "random i/o SSDs Excel were over HTTP and in this case EGP to il-12 really good",
    "start": "1101020",
    "end": "1108840"
  },
  {
    "text": "options for in a general sense for no sequel reloads and you can you can match",
    "start": "1108840",
    "end": "1116830"
  },
  {
    "text": "your profile so you have you will get into a little more into us during a further discussion but you can write",
    "start": "1116830",
    "end": "1123850"
  },
  {
    "text": "size dropped your instance with the network band network bandwidth and EBS bandwidth available to get their",
    "start": "1123850",
    "end": "1129220"
  },
  {
    "text": "performance on there so also you can use snapshot to the EBS as well so again using GP to is a or il one it's a good",
    "start": "1129220",
    "end": "1137440"
  },
  {
    "text": "choice you can use instance doors if you want and there are some challenges with that particularly if using Cassandra",
    "start": "1137440",
    "end": "1143680"
  },
  {
    "text": "it's it's not uncommon to use and just insist or with Cassandra but when you",
    "start": "1143680",
    "end": "1148690"
  },
  {
    "text": "have to rebuild your cluster you got to replay all your data and you got retain more logs to be able do so so there's",
    "start": "1148690",
    "end": "1154930"
  },
  {
    "text": "some a balance there as well in most cases a lot of customers are seeing the advantage of going to GP to il-1 over",
    "start": "1154930",
    "end": "1162520"
  },
  {
    "text": "using a ephemeral store SSD so there's a",
    "start": "1162520",
    "end": "1167620"
  },
  {
    "text": "couple sessions here for the deep dive on to know sequel and EBS that are to",
    "start": "1167620",
    "end": "1174130"
  },
  {
    "text": "today and one on Thursday if you want to get deeper into this particular you know",
    "start": "1174130",
    "end": "1179890"
  },
  {
    "text": "how to optimize for no sequel on EBS so now we're gonna dive into search in this",
    "start": "1179890",
    "end": "1185110"
  },
  {
    "start": "1182000",
    "end": "1212000"
  },
  {
    "text": "case Splunk is our use case it could be also elk or anything else but the the patterns are kind of the same thing here",
    "start": "1185110",
    "end": "1191620"
  },
  {
    "text": "it's everything is stored as an index it's its raw data okay so you're going",
    "start": "1191620",
    "end": "1196660"
  },
  {
    "text": "to give the data in and you're going to index it and you're stored into a bunch of buckets so the",
    "start": "1196660",
    "end": "1202460"
  },
  {
    "text": "buckets are basically divided up on a time range of the incoming data how hot",
    "start": "1202460",
    "end": "1208040"
  },
  {
    "text": "it is how cold it is you know how recent the data is you're partitioning or your buckets around that so normally we have",
    "start": "1208040",
    "end": "1215000"
  },
  {
    "start": "1212000",
    "end": "1317000"
  },
  {
    "text": "hot and warm buckets and this is more recent data very reader very recent or",
    "start": "1215000",
    "end": "1220370"
  },
  {
    "text": "more recent data some customers have done 30 days for the hot and warm some done week depending on data growth data",
    "start": "1220370",
    "end": "1227270"
  },
  {
    "text": "needs and then you have cold buckets in cold buckets or more for your longer",
    "start": "1227270",
    "end": "1232640"
  },
  {
    "text": "storage longer-term searches maybe you don't refresh those as often maybe you don't need to search those as often so",
    "start": "1232640",
    "end": "1238340"
  },
  {
    "text": "the profile of those is a little different so with hot bucket hot warm your search is a variant very random i/o",
    "start": "1238340",
    "end": "1244250"
  },
  {
    "text": "so you don't know words can be is a lot of data coming in it could be anywhere in those buckets in those partitions so",
    "start": "1244250",
    "end": "1250730"
  },
  {
    "text": "you will need a lot of random i/o for that if you're doing a search that's bounded it's a much longer time frame",
    "start": "1250730",
    "end": "1256610"
  },
  {
    "text": "it's gonna crawl through all the data has available in that case it's give me a lot more a lot more sequential i/o on",
    "start": "1256610",
    "end": "1262700"
  },
  {
    "text": "that for fro we also have a button called frozen this is more for your archival of your search data that you",
    "start": "1262700",
    "end": "1269059"
  },
  {
    "text": "don't access anymore and this data you actually don't search it but you may roll it into a a cold",
    "start": "1269059",
    "end": "1274070"
  },
  {
    "text": "stored or frozen bucket just to have it for retention for some kind of regulatory or whatever reason for that",
    "start": "1274070",
    "end": "1280520"
  },
  {
    "text": "and so for hot warm general recommendations our GP to IO one and",
    "start": "1280520",
    "end": "1287270"
  },
  {
    "text": "then for cold or BSC one st one because cold give me a lot more sequential i/o therefore the performance the throughput",
    "start": "1287270",
    "end": "1294350"
  },
  {
    "text": "of our HDD back volumes are adequate for good enough for running cold buckets and",
    "start": "1294350",
    "end": "1300169"
  },
  {
    "text": "then for frozen it's a good candidate for Amazon glacier glacier designed for archival and so it will store the data",
    "start": "1300169",
    "end": "1306500"
  },
  {
    "text": "in glacier and you can have it available for for whatever attention curves you need to have on that as I mentioned",
    "start": "1306500",
    "end": "1313010"
  },
  {
    "text": "sequential i/o for writes and then the reads depends on your search so as I mentioned earlier so the next discussion",
    "start": "1313010",
    "end": "1320299"
  },
  {
    "start": "1317000",
    "end": "1419000"
  },
  {
    "text": "for workload is streaming now we have Amazon Kinesis which is our managed streaming service for you it's very easy",
    "start": "1320299",
    "end": "1326690"
  },
  {
    "text": "to use you don't need to worry about things like storage or really provisioning just basically how fast you want how much day do you",
    "start": "1326690",
    "end": "1333040"
  },
  {
    "text": "want to get but if you did want to run your own streaming infrastructure like",
    "start": "1333040",
    "end": "1338590"
  },
  {
    "text": "with Apache Kafka we're going to talk about it here and so Kafka is a pub/sub",
    "start": "1338590",
    "end": "1344620"
  },
  {
    "text": "system pub/sub system where you have producers of data and consumers data in",
    "start": "1344620",
    "end": "1349870"
  },
  {
    "text": "the middle you have Kafka nodes brokers that are going to be basically reading",
    "start": "1349870",
    "end": "1355660"
  },
  {
    "text": "data in and spitting it out so fundamentally it's just you partition",
    "start": "1355660",
    "end": "1361450"
  },
  {
    "text": "your data based on your topics and and you whatever method you want to break your data up there but it's going to",
    "start": "1361450",
    "end": "1367810"
  },
  {
    "text": "write all that data into sequential log files so you're going to end up having a large number of log files that are",
    "start": "1367810",
    "end": "1373810"
  },
  {
    "text": "sequential writes throughout your your architecture for how many nodes you have on your cluster now the consumers are",
    "start": "1373810",
    "end": "1381370"
  },
  {
    "text": "going to affect fetched chunks of data and there are in in general the use case is going to be you know st-1 is a good",
    "start": "1381370",
    "end": "1389530"
  },
  {
    "text": "volume a sequential storage is a very good good choice for this and your network is more often your bottleneck",
    "start": "1389530",
    "end": "1395380"
  },
  {
    "text": "than your storage because Kaufmann can get very very busy with a lot of producers a lot of consumers networkers",
    "start": "1395380",
    "end": "1401230"
  },
  {
    "text": "often gonna be the ceiling you're going to hit first there are some cases if you have a lot of high random i/o for",
    "start": "1401230",
    "end": "1406960"
  },
  {
    "text": "example if you do a lot of log compaction or if you have a high number of partitions then you might want to use",
    "start": "1406960",
    "end": "1412480"
  },
  {
    "text": "gp2 for those volumes but for in the general use case using st-1 is recommended okay so I've covered the",
    "start": "1412480",
    "end": "1422110"
  },
  {
    "start": "1419000",
    "end": "1450000"
  },
  {
    "text": "five data workloads and at a kind of a high level and now I'm gonna turn over to Anna and and Daniel to discuss their",
    "start": "1422110",
    "end": "1429880"
  },
  {
    "text": "story and experiences with with my Sam",
    "start": "1429880",
    "end": "1434700"
  },
  {
    "text": "alright guys how's it going oops this is Anna Kepler we're both here",
    "start": "1435210",
    "end": "1442210"
  },
  {
    "text": "from viasat and we're excited to be talking to you about what we've been doing on Amazon and specifically on EBS",
    "start": "1442210",
    "end": "1450840"
  },
  {
    "start": "1450000",
    "end": "1493000"
  },
  {
    "text": "so first a little bit about ViaSat we do a lot of different things but all those",
    "start": "1450840",
    "end": "1456160"
  },
  {
    "text": "things kind of focus around satellite communications where were the first true",
    "start": "1456160",
    "end": "1461890"
  },
  {
    "text": "broadband satellite is P we have Guinness Book of World Records the",
    "start": "1461890",
    "end": "1467620"
  },
  {
    "text": "highest bandwidth satellite in the world that goes into service in a couple of months we're currently the only Wi-Fi",
    "start": "1467620",
    "end": "1476170"
  },
  {
    "text": "provider that's capable of streaming video to an airplane so if you've streaming a video on a plane that's probably yes we have a lot of other",
    "start": "1476170",
    "end": "1485370"
  },
  {
    "text": "military technology that I really won't talk about today a lot of its secret but",
    "start": "1485370",
    "end": "1491110"
  },
  {
    "text": "there's a lot of cool stuff there as well for our industry working as an isp",
    "start": "1491110",
    "end": "1498940"
  },
  {
    "start": "1493000",
    "end": "1558000"
  },
  {
    "text": "is naturally a conservative but we've really I think push the pushing the",
    "start": "1498940",
    "end": "1503980"
  },
  {
    "text": "boundaries of what you can really do in the cloud we've been a happy Amazon customer since 2010 and historically",
    "start": "1503980",
    "end": "1513160"
  },
  {
    "text": "that was physical data centers for our primary server primary customers with",
    "start": "1513160",
    "end": "1519460"
  },
  {
    "text": "AWS for analytics and special use cases and when you just want to throw up a",
    "start": "1519460",
    "end": "1524530"
  },
  {
    "text": "server real quick but our our next-gen ground system actually runs majority of",
    "start": "1524530",
    "end": "1531970"
  },
  {
    "text": "the management plane and some parts of the control plane in AWS so we're very we're very invested in",
    "start": "1531970",
    "end": "1539740"
  },
  {
    "text": "enable us and we really like it today we have over 150 accounts in Amazon 300",
    "start": "1539740",
    "end": "1545350"
  },
  {
    "text": "BBC's across seven different regions we have redundant AWS Direct Connect links",
    "start": "1545350",
    "end": "1550990"
  },
  {
    "text": "on the East Coast and West Coast with multiple peering locations so we're very plugged in to Amazon and doing some cool",
    "start": "1550990",
    "end": "1557500"
  },
  {
    "text": "stuff there and so specifically our team we were kind of the analytics big data",
    "start": "1557500",
    "end": "1564340"
  },
  {
    "start": "1558000",
    "end": "1718000"
  },
  {
    "text": "team at the company and we built a product that we call the data bus",
    "start": "1564340",
    "end": "1569470"
  },
  {
    "text": "internally so the idea of the data bus is that we have a lot of systems in our",
    "start": "1569470",
    "end": "1575890"
  },
  {
    "text": "network that produce what we often call data exhaust so we might have traffic",
    "start": "1575890",
    "end": "1581320"
  },
  {
    "text": "shapers that enforce policy and our users and ensure that everyone gets a fair time on the Internet",
    "start": "1581320",
    "end": "1587530"
  },
  {
    "text": "of course we're satellite network so we have to at some point convert that satellite waveform and",
    "start": "1587530",
    "end": "1593320"
  },
  {
    "text": "actual internet packets and we have to implement the Mac layer 2 on our network",
    "start": "1593320",
    "end": "1599620"
  },
  {
    "text": "so there's actual you know services that implement that because we're satellite",
    "start": "1599620",
    "end": "1605560"
  },
  {
    "text": "ISP and we have a large amount of and there's a large amount of latency built into that we have traffic acceleration",
    "start": "1605560",
    "end": "1613360"
  },
  {
    "text": "to try to mask that latency and then of course this is a large ISP there's a ton of networking equipment a ton of",
    "start": "1613360",
    "end": "1619960"
  },
  {
    "text": "management devices a whole bunch of extra stuff necessary to run a network and with that data we obviously we want",
    "start": "1619960",
    "end": "1629050"
  },
  {
    "text": "to use it for monitoring and alerting on that so the people that develop these",
    "start": "1629050",
    "end": "1634180"
  },
  {
    "text": "tools and operate them want to be able to see how well they are performing want to be able to see when they fail get",
    "start": "1634180",
    "end": "1641530"
  },
  {
    "text": "alerts all that basic stuff but when what we really wanted to be able to do",
    "start": "1641530",
    "end": "1646750"
  },
  {
    "text": "with the data bus was connect not only to that use case but to all the other",
    "start": "1646750",
    "end": "1651760"
  },
  {
    "text": "things that people want to do with that data so for example we do security scanning",
    "start": "1651760",
    "end": "1657010"
  },
  {
    "text": "so we both scan on our network looking for intruders as well as on the on our",
    "start": "1657010",
    "end": "1663250"
  },
  {
    "text": "customers network looking for things like botnets we we use this data to do",
    "start": "1663250",
    "end": "1671410"
  },
  {
    "text": "to do growth projection so there's some knobs and dials we can turn on our",
    "start": "1671410",
    "end": "1676750"
  },
  {
    "text": "satellite to allocate different bandwidth to different parts of the country and we use this same data to",
    "start": "1676750",
    "end": "1681880"
  },
  {
    "text": "project our growth over time and look where we should do that we also pull the",
    "start": "1681880",
    "end": "1688270"
  },
  {
    "text": "data in to make it available so that it can be pulled for customer care so if",
    "start": "1688270",
    "end": "1694390"
  },
  {
    "text": "somebody calls us and hat not having a great time they can look at the performance of our network potentially",
    "start": "1694390",
    "end": "1700630"
  },
  {
    "text": "identify an area a reason for which they are not getting great performance and",
    "start": "1700630",
    "end": "1705930"
  },
  {
    "text": "the data bus is all about bringing these together and",
    "start": "1705930",
    "end": "1710730"
  },
  {
    "text": "basically building a what we like to call data switchboard that connects all of these data sources to all of these producers",
    "start": "1711179",
    "end": "1718769"
  },
  {
    "start": "1718000",
    "end": "1795000"
  },
  {
    "text": "so let's talk a little bit about what the architecture looks like so if I set",
    "start": "1718769",
    "end": "1725470"
  },
  {
    "text": "a somewhat unique in that we are largely a vertically integrated company we we build the modems that the users use we",
    "start": "1725470",
    "end": "1732850"
  },
  {
    "text": "build a lot of the services that I mentioned are actually developed as applications in-house by our",
    "start": "1732850",
    "end": "1738519"
  },
  {
    "text": "organization and so we built our team builds client libraries that allow",
    "start": "1738519",
    "end": "1745470"
  },
  {
    "text": "others within our organization to plug in metrics collection into their application so we're half languages in",
    "start": "1745470",
    "end": "1753159"
  },
  {
    "text": "different service excuse me we have services in different languages so we have libraries for Python for C++ for",
    "start": "1753159",
    "end": "1759549"
  },
  {
    "text": "Java and we have ways of getting generic metrics plug-ins for collecti some of",
    "start": "1759549",
    "end": "1767379"
  },
  {
    "text": "the things that we are monitoring are third-party devices these are largely things like network switches so we use",
    "start": "1767379",
    "end": "1772840"
  },
  {
    "text": "Polar's in those cases to pull metrics from them and then we also have a special collection mechanism that we use",
    "start": "1772840",
    "end": "1778779"
  },
  {
    "text": "for collecting data from all of the user terminals the idea here is to reduce the amount of bandwidth used as much as",
    "start": "1778779",
    "end": "1784929"
  },
  {
    "text": "possible it's every bit that we use to manage the network is a bit that we",
    "start": "1784929",
    "end": "1790330"
  },
  {
    "text": "can't sell to our customers to actually use on satellite so all of those",
    "start": "1790330",
    "end": "1796809"
  },
  {
    "start": "1795000",
    "end": "1837000"
  },
  {
    "text": "different producers and there are actually more than this those all connect to the kind of services that we",
    "start": "1796809",
    "end": "1803169"
  },
  {
    "text": "call the data bus at the core of that is Apache Kafka and Kafka itself requires",
    "start": "1803169",
    "end": "1810009"
  },
  {
    "text": "zookeeper for doing quorum we have our own service that we call the stream service that's just it's a it just as",
    "start": "1810009",
    "end": "1817359"
  },
  {
    "text": "administration so you can create delete update streams view them change security",
    "start": "1817359",
    "end": "1822489"
  },
  {
    "text": "policies on them things like that and then we also integrate with the confluence schema registry which helps",
    "start": "1822489",
    "end": "1829239"
  },
  {
    "text": "ensure that that there's well structured",
    "start": "1829239",
    "end": "1834460"
  },
  {
    "text": "data between the producers and the consumers a large part of like I said is",
    "start": "1834460",
    "end": "1840809"
  },
  {
    "text": "integrating metrics and alerting so we have a fleet of auto deployed for ders",
    "start": "1840809",
    "end": "1847679"
  },
  {
    "text": "that take data that's being produced into these kafka streams and write them into open th DB for showing monitoring",
    "start": "1847679",
    "end": "1855570"
  },
  {
    "text": "metrics and we kind of build that in to provide kind of a core value to the",
    "start": "1855570",
    "end": "1861539"
  },
  {
    "text": "people that are producing data to us but we also send it this data consumers to",
    "start": "1861539",
    "end": "1866730"
  },
  {
    "start": "1863000",
    "end": "1902000"
  },
  {
    "text": "all different places data goes to Befana often through open th DB we send it we",
    "start": "1866730",
    "end": "1874799"
  },
  {
    "text": "have a internal service that also can generate alerts off that data and send",
    "start": "1874799",
    "end": "1880289"
  },
  {
    "text": "them to X matters or page of duty we we do analytics on the data we pull it into",
    "start": "1880289",
    "end": "1886320"
  },
  {
    "text": "our kind of data warehouse and our data Sciences do fun stuff with it and there all kinds of there are all kinds of",
    "start": "1886320",
    "end": "1894210"
  },
  {
    "text": "applications that are custom built applications that do other more interesting things with the data alright",
    "start": "1894210",
    "end": "1903240"
  },
  {
    "start": "1902000",
    "end": "2086000"
  },
  {
    "text": "so I showed there that our core of our product is Apache Kafka but we kind of",
    "start": "1903240",
    "end": "1910200"
  },
  {
    "text": "our story of moving into EBS it really starts in when open TCB which is itself",
    "start": "1910200",
    "end": "1917039"
  },
  {
    "text": "built on top of a no sequel database called HBase so HBase comes from the",
    "start": "1917039",
    "end": "1923789"
  },
  {
    "text": "Hadoop ecosystem use HDFS the file system and the fundamental data structure that it provides is the log",
    "start": "1923789",
    "end": "1930570"
  },
  {
    "text": "structured merge tree LSM is a somewhat complex algorithm but essentially I like",
    "start": "1930570",
    "end": "1936870"
  },
  {
    "text": "to think of it as a distributed write back cache so every time you write into",
    "start": "1936870",
    "end": "1942840"
  },
  {
    "text": "HBase you are going to ask Paul mentioned earlier with I believe it was Vertica you're going to write into",
    "start": "1942840",
    "end": "1949620"
  },
  {
    "text": "memory and then that memory is going to be periodically flushed to disk so those",
    "start": "1949620",
    "end": "1956549"
  },
  {
    "text": "periodic flushes there's a lot of reasons that HBase will flush to disk it",
    "start": "1956549",
    "end": "1961710"
  },
  {
    "text": "will flush based on time so just periodically it will flush based on the",
    "start": "1961710",
    "end": "1968039"
  },
  {
    "text": "amount of per region size reasons in this case are what HBase calls like a partition a",
    "start": "1968039",
    "end": "1975520"
  },
  {
    "text": "chunk of data or a bucket is another term for it it will base it will flush",
    "start": "1975520",
    "end": "1980680"
  },
  {
    "text": "based on the total amount of memory that it's using and it will flush based on its right ahead log size effectively",
    "start": "1980680",
    "end": "1988120"
  },
  {
    "text": "with the deep which is what you can see here is that there are a lot of reasons that that HBase flushes to disk and",
    "start": "1988120",
    "end": "1994350"
  },
  {
    "text": "those are kind of all operating at the same time and when we originally",
    "start": "1994350",
    "end": "1999580"
  },
  {
    "text": "deployed this application we're running on D to excels and we were having an issue where we were getting inconsistent",
    "start": "1999580",
    "end": "2006800"
  },
  {
    "text": "IO latency when it came to doing these flushes so most of the time things would",
    "start": "2006800",
    "end": "2012090"
  },
  {
    "text": "be okay but sometimes if several of these flushes kind of overlapped at the same time and we got unlucky well then",
    "start": "2012090",
    "end": "2018510"
  },
  {
    "text": "the IO would back up we wouldn't flush on time and that would prevent us that would affect the availability of our",
    "start": "2018510",
    "end": "2024540"
  },
  {
    "text": "system we weren't quite sure where this was coming from we had a theory that it was we were on D",
    "start": "2024540",
    "end": "2030780"
  },
  {
    "text": "to s or shared instances maybe it's something to do with the do with that and we weren't really able to experiment",
    "start": "2030780",
    "end": "2037860"
  },
  {
    "text": "with with differences we've kind of stuck deploying a big making a big",
    "start": "2037860",
    "end": "2043530"
  },
  {
    "text": "change at once so we decided to try out moving to sp1 EBS as our backing data",
    "start": "2043530",
    "end": "2051840"
  },
  {
    "text": "store and this really enabled us to experiment with different instance sizes",
    "start": "2051840",
    "end": "2057600"
  },
  {
    "text": "and to figure out kind of where our bottleneck is and very quickly we",
    "start": "2057600",
    "end": "2062669"
  },
  {
    "text": "noticed much more consistent performance out of EBS and our experience EBS",
    "start": "2062669",
    "end": "2068220"
  },
  {
    "text": "provides us kind of with the provision amount of i/o kind of all day every day",
    "start": "2068220",
    "end": "2073679"
  },
  {
    "text": "which is exactly what we are looking for and of course we were able to quickly experiment with different instance types",
    "start": "2073679",
    "end": "2079020"
  },
  {
    "text": "we could try we could just stop the instance change it to a for Excel and rerun our tests and see how that",
    "start": "2079020",
    "end": "2084658"
  },
  {
    "text": "impacted our performance and I'm going to hand it over to Anna to talk about",
    "start": "2084659",
    "end": "2089669"
  },
  {
    "start": "2086000",
    "end": "2129000"
  },
  {
    "text": "Kafka so as Daniel mentioned earlier in the",
    "start": "2089669",
    "end": "2096868"
  },
  {
    "text": "core about streaming platform that we build we have Kafka and it's important for us to design it to achieve a really",
    "start": "2096869",
    "end": "2103230"
  },
  {
    "text": "consistent io throughput and since we are also operating the system and report to support also it's important for us to",
    "start": "2103230",
    "end": "2109860"
  },
  {
    "text": "spend this little amount of time maintaining this system as well so it is Paul mention earlier publish/subscribe",
    "start": "2109860",
    "end": "2115950"
  },
  {
    "text": "system we have multiple brokers in the core of a process and the data and replicating the data you have producers",
    "start": "2115950",
    "end": "2122490"
  },
  {
    "text": "on one side and we have consumers on the other side reading this data forward in",
    "start": "2122490",
    "end": "2128520"
  },
  {
    "text": "its server applications it is designed for the high throughput and all the data",
    "start": "2128520",
    "end": "2134910"
  },
  {
    "start": "2129000",
    "end": "2177000"
  },
  {
    "text": "on all the brokers needs to be located on a dedicated disk even configurations there are specific configurations locks",
    "start": "2134910",
    "end": "2142320"
  },
  {
    "text": "that's there there you could specify there your data is located so Kafka could properly process it and achieve",
    "start": "2142320",
    "end": "2147510"
  },
  {
    "text": "that I high fruit put and Woolf instant storage and EBS volumes can work well in",
    "start": "2147510",
    "end": "2154890"
  },
  {
    "text": "majority of cases for Kafka and originally we did I ran out of kanji to extralife systems however later with",
    "start": "2154890",
    "end": "2162630"
  },
  {
    "text": "lessons learned from HBase we decided that we will switch the our Kafka cluster also to a savant volumes you",
    "start": "2162630",
    "end": "2169740"
  },
  {
    "text": "know over over concept we'll use cases where we found that what benefits we",
    "start": "2169740",
    "end": "2175890"
  },
  {
    "text": "actually found from doing that so a little more to the core of a Kafka it is",
    "start": "2175890",
    "end": "2182220"
  },
  {
    "start": "2177000",
    "end": "2228000"
  },
  {
    "text": "majority of the data processing is sequential i/o you have producers that append data to log in the sequential",
    "start": "2182220",
    "end": "2189359"
  },
  {
    "text": "matter their each message associated with specific offset they have consumers that read from specific offsets and",
    "start": "2189359",
    "end": "2197250"
  },
  {
    "text": "could be in the middle of your topic could be obtained as the data comes in but most most of the time they read all",
    "start": "2197250",
    "end": "2205260"
  },
  {
    "text": "this data in a sequential pattern you start since it's streaming data so it's most of the time interested to process",
    "start": "2205260",
    "end": "2212850"
  },
  {
    "text": "it and it as it comes in so the sequential i/o and multiple consumers they're independent of each other",
    "start": "2212850",
    "end": "2218910"
  },
  {
    "text": "so you could have different applications using the same topic and doing different",
    "start": "2218910",
    "end": "2224660"
  },
  {
    "text": "operations on it so in our case most of",
    "start": "2224660",
    "end": "2231630"
  },
  {
    "text": "all occasions nopal mentioned earlier there's some applications to do a lot of law",
    "start": "2231630",
    "end": "2237390"
  },
  {
    "text": "compaction there's a lot of random i/o in our case we have 90% of non bursty",
    "start": "2237390",
    "end": "2242940"
  },
  {
    "text": "sequential access it is 24/7 we see the",
    "start": "2242940",
    "end": "2248099"
  },
  {
    "text": "consistent fruit food from estimate volumes that we've switched our Kafka to",
    "start": "2248099",
    "end": "2253289"
  },
  {
    "text": "and we've obviously selection of EBS optimized well you know instances it is",
    "start": "2253289",
    "end": "2259890"
  },
  {
    "text": "important when you go to estimate volumes different have different producers and consumers that you could",
    "start": "2259890",
    "end": "2265980"
  },
  {
    "text": "see here and most of them do have that sequential access and that's what we see so some of the special cases that we",
    "start": "2265980",
    "end": "2273180"
  },
  {
    "text": "really saw there benefited from switching to Estevan volumes we do",
    "start": "2273180",
    "end": "2280079"
  },
  {
    "text": "sequential access IO fruit put really well however periodically you do need to",
    "start": "2280079",
    "end": "2285359"
  },
  {
    "text": "do some maintenance on your Kafka so you have one broker if it becomes overloaded of your data so you need to de move data",
    "start": "2285359",
    "end": "2290579"
  },
  {
    "text": "around to achieve a really good overall performance of your system so we wake up",
    "start": "2290579",
    "end": "2296549"
  },
  {
    "text": "card asset you design this file with petitioner assignments and you move data around in with generally do it in",
    "start": "2296549",
    "end": "2303900"
  },
  {
    "text": "batches so now we don't um we don't interfere with normal data throughput a twisty no",
    "start": "2303900",
    "end": "2310980"
  },
  {
    "text": "normal operations on our brokers and so when we do that we see this bursting",
    "start": "2310980",
    "end": "2316349"
  },
  {
    "text": "patterns and when we were on easy to actualize we often saw that some of our folders that producers the Tri data to",
    "start": "2316349",
    "end": "2324990"
  },
  {
    "text": "have come see some performance degradation however about we run is maintenance operations after switching",
    "start": "2324990",
    "end": "2332039"
  },
  {
    "text": "to a Steve on we service bursting and still the normal pattern normal flow of",
    "start": "2332039",
    "end": "2337589"
  },
  {
    "text": "operations has not been interrupted so we were able to still achieve normal performance on our Kafka while running",
    "start": "2337589",
    "end": "2343890"
  },
  {
    "text": "maintenance and without making any unhappy customers in our",
    "start": "2343890",
    "end": "2349990"
  },
  {
    "text": "and getting support calls sir similar",
    "start": "2349990",
    "end": "2355020"
  },
  {
    "start": "2352000",
    "end": "2472000"
  },
  {
    "text": "bursting patterns that they see in some scenarios besides the maintenance it's a natural degradation and I for example",
    "start": "2355020",
    "end": "2361360"
  },
  {
    "text": "the top graph here is a little Direct Connect maintenance of something's happening on and this or some data drops",
    "start": "2361360",
    "end": "2368170"
  },
  {
    "text": "and as soon as the data network came back up a lot of affluent collectors that cache this data while the network",
    "start": "2368170",
    "end": "2374440"
  },
  {
    "text": "is down started flooding the brokers with lots of data and we didn't drop any",
    "start": "2374440",
    "end": "2380290"
  },
  {
    "text": "data everything's been written really well we work our instances were able to achieve high bursts and process all the",
    "start": "2380290",
    "end": "2388600"
  },
  {
    "text": "data we went to normal operations within a couple of hours a similar situation",
    "start": "2388600",
    "end": "2394300"
  },
  {
    "text": "this year our stream processing jobs somewhere streams are really heavy on data and we use a lot of spark",
    "start": "2394300",
    "end": "2399880"
  },
  {
    "text": "processing and jobs do go down there are some health checks that we have on that",
    "start": "2399880",
    "end": "2405160"
  },
  {
    "text": "and sometimes it needs to get restarted and great thing about cough cough and your job restarts you could pick up the",
    "start": "2405160",
    "end": "2412380"
  },
  {
    "text": "from the point from an offset that you stopped processing it and keep carrying on and process your data but you need to",
    "start": "2412380",
    "end": "2418450"
  },
  {
    "text": "catch up to the current point of where your data is currently and because your producers kept writing your data while",
    "start": "2418450",
    "end": "2424810"
  },
  {
    "text": "your job was down so we see this job trying to catch up and write so to go a",
    "start": "2424810",
    "end": "2431560"
  },
  {
    "text": "little bit more why we see the high input as a higher extra effect on our",
    "start": "2431560",
    "end": "2436750"
  },
  {
    "text": "cafes so our spark jobs process the data they do something that and we write it back to Kafka so we call them loop backs",
    "start": "2436750",
    "end": "2443920"
  },
  {
    "text": "so someone else could it could be a data aggregation data enrichments and so we",
    "start": "2443920",
    "end": "2449800"
  },
  {
    "text": "still prefer to write it back on Kafka so we see this right like reading and",
    "start": "2449800",
    "end": "2455140"
  },
  {
    "text": "then writing back onto Kafka so like bursting patterns with this even as jobs do you go down periodically that said",
    "start": "2455140",
    "end": "2462940"
  },
  {
    "text": "normal like don't drop any data we do see this happens generally we don't really have to worry about it we know",
    "start": "2462940",
    "end": "2469119"
  },
  {
    "text": "the system is designed well to handle this and so some of them really",
    "start": "2469119",
    "end": "2475560"
  },
  {
    "start": "2472000",
    "end": "2552000"
  },
  {
    "text": "important maintenance the use case that we saw when we were running our Kafka on due to extra large",
    "start": "2475560",
    "end": "2482619"
  },
  {
    "text": "instances and originally we started as a small amount of data so we didn't really",
    "start": "2482619",
    "end": "2488380"
  },
  {
    "text": "see our maintenance operations taking a lot of time however once we reach this",
    "start": "2488380",
    "end": "2493810"
  },
  {
    "text": "terabytes and terabytes of data multiple customers and they saw some of it brokers had degraded performance or they",
    "start": "2493810",
    "end": "2502750"
  },
  {
    "text": "were going down in order for us to replace the broker they had to move all this data to other brokers in order to",
    "start": "2502750",
    "end": "2510820"
  },
  {
    "text": "replace it so that sometimes would take us 24 hours if not more to move their data because Kafka throttles a little",
    "start": "2510820",
    "end": "2516940"
  },
  {
    "text": "bit your data or flowed into other brokers in order to not to interfere",
    "start": "2516940",
    "end": "2522220"
  },
  {
    "text": "with the rest of the sort of give everyone equal performance on your",
    "start": "2522220",
    "end": "2528520"
  },
  {
    "text": "brokers Howard so switching to a Steve on volumes here's what we saw we could",
    "start": "2528520",
    "end": "2533860"
  },
  {
    "text": "detach our volumes of data replace our broker reattach our volumes and our",
    "start": "2533860",
    "end": "2540310"
  },
  {
    "text": "brokers back in operations in under 30 minutes it was amazing maintenance wise",
    "start": "2540310",
    "end": "2546010"
  },
  {
    "text": "especially when you have to do it in the middle of a night that's really a big",
    "start": "2546010",
    "end": "2551109"
  },
  {
    "text": "use case for us so some of them besides their classroom some of instance in",
    "start": "2551109",
    "end": "2556930"
  },
  {
    "text": "volumes with reiha running so as I mentioned we started cough cough due to extra large switch to m4 to extra lies",
    "start": "2556930",
    "end": "2564160"
  },
  {
    "text": "with estaban volumes there is an Iranian in for two extra lies Kafka really doesn't use a lot of RAM so m4 has been",
    "start": "2564160",
    "end": "2571630"
  },
  {
    "text": "working really well for us and we're still able to achieve the overall throughput of those type of instances",
    "start": "2571630",
    "end": "2577030"
  },
  {
    "text": "the time series databases to use the open GSD beam so we run two type of nodes there one knows that really",
    "start": "2577030",
    "end": "2584230"
  },
  {
    "text": "process and store the data and that's where we see m42 extra light of a Steven Williams however we have the what's",
    "start": "2584230",
    "end": "2591580"
  },
  {
    "text": "called reader knows those are just query our data and return it back to the users too like Ravana and other applications",
    "start": "2591580",
    "end": "2600280"
  },
  {
    "text": "and in those scenarios the GP to the random I or type of appropriate volumes",
    "start": "2600280",
    "end": "2607300"
  },
  {
    "text": "is what we use there so a masters cluster don't do a lot of disk storage in our",
    "start": "2607300",
    "end": "2612650"
  },
  {
    "text": "masters quest it's mainly processing our jobs like a lot about spark jobs around in their full reduce running in master's",
    "start": "2612650",
    "end": "2618320"
  },
  {
    "text": "cluster other applications of our users are doing to process the data so since",
    "start": "2618320",
    "end": "2624470"
  },
  {
    "text": "we don't use and utilize any disk storage there is no need for us really to have Estevan volumes or other type of",
    "start": "2624470",
    "end": "2632810"
  },
  {
    "text": "storage volume so jp2 works really well for us and I believe its size it's",
    "start": "2632810",
    "end": "2637880"
  },
  {
    "text": "rather small there as well however the instance size and 410 extra",
    "start": "2637880",
    "end": "2643040"
  },
  {
    "text": "lines you started with m42 extra large and then we ran into having 50 different",
    "start": "2643040",
    "end": "2649040"
  },
  {
    "text": "instances perform small operations and really had this different a ssin about resources across instances I thought",
    "start": "2649040",
    "end": "2655640"
  },
  {
    "text": "yeah that's not really good so we have to go move on over to like really large instances so we could have as resource a",
    "start": "2655640",
    "end": "2662000"
  },
  {
    "text": "full utilization of all the resources at the scene and then they have multiple in-house applications as well as Griffin",
    "start": "2662000",
    "end": "2668600"
  },
  {
    "text": "and zookeeper and those really don't need a lot of memory they well scale",
    "start": "2668600",
    "end": "2674780"
  },
  {
    "text": "them for light works really well for us we have gp2 since most of us",
    "start": "2674780",
    "end": "2679790"
  },
  {
    "text": "applications utilize the render my own type of pattern operations so gonna pass",
    "start": "2679790",
    "end": "2686780"
  },
  {
    "start": "2685000",
    "end": "2765000"
  },
  {
    "text": "it back to they know some lessons so we talked a little bit about what we've",
    "start": "2686780",
    "end": "2692830"
  },
  {
    "text": "what we've done on EBS and why but we've we've learned some lessons along the way and Paul touched on this a little bit",
    "start": "2692830",
    "end": "2699710"
  },
  {
    "text": "and a big takeaway is that it's it's really easy to over focus on IO",
    "start": "2699710",
    "end": "2705200"
  },
  {
    "text": "especially because what we're doing is is building a a big data system right",
    "start": "2705200",
    "end": "2710330"
  },
  {
    "text": "and data is i/o so disk IO is probably going to be our bottleneck well as we as",
    "start": "2710330",
    "end": "2717740"
  },
  {
    "text": "we remove that bottleneck through focus and engineering around it you can expose",
    "start": "2717740",
    "end": "2722870"
  },
  {
    "text": "other issues and this has been our experience I I mentioned earlier that we",
    "start": "2722870",
    "end": "2729110"
  },
  {
    "text": "have redundant we have redundant Direct Connect links connecting to our backbone network well",
    "start": "2729110",
    "end": "2735860"
  },
  {
    "text": "that was not always the case we used to have VPN connections over",
    "start": "2735860",
    "end": "2741349"
  },
  {
    "text": "the internet and those network connections proved less reliable than the actual storage and would often",
    "start": "2741349",
    "end": "2747529"
  },
  {
    "text": "become themselves a bottleneck so as we encountered those bottlenecks we had to work with our networking teams to deploy",
    "start": "2747529",
    "end": "2753979"
  },
  {
    "text": "Direct Connect connectivity and particularly we operate in u.s. East so getting that in u.s. East was a big one",
    "start": "2753979",
    "end": "2759950"
  },
  {
    "text": "and then it also is with the issue with the number of simultaneous connections was I wanted to dive into a little bit",
    "start": "2759950",
    "end": "2765410"
  },
  {
    "start": "2765000",
    "end": "2825000"
  },
  {
    "text": "more in depth so we've we encountered this this issue in in production where",
    "start": "2765410",
    "end": "2773509"
  },
  {
    "text": "we started seeing a flood of network connections our clients kind of make a",
    "start": "2773509",
    "end": "2778700"
  },
  {
    "text": "test connection before they connect to the system to make sure that they're going to be able to connect and uh when",
    "start": "2778700",
    "end": "2784970"
  },
  {
    "text": "there'd be a large-scale restart say somebody doing a software upgrade on a bunch of servers we would see a flood of",
    "start": "2784970",
    "end": "2792319"
  },
  {
    "text": "those connections and then the connections would fail and then they would be retried and etc etc and then",
    "start": "2792319",
    "end": "2799880"
  },
  {
    "text": "you'd see eventually you'd see kernel log saying the kernel is detecting that this is probably a DDoS attack that's",
    "start": "2799880",
    "end": "2806269"
  },
  {
    "text": "happening of course it's it's not but we eventually are overloading the ability",
    "start": "2806269",
    "end": "2811940"
  },
  {
    "text": "of Kafka to accept new connections and this was a bottleneck that we didn't",
    "start": "2811940",
    "end": "2817369"
  },
  {
    "text": "really even consider when we were first designed the system so how do we deal with that well first off you're seeing a",
    "start": "2817369",
    "end": "2824239"
  },
  {
    "text": "nice graph there and we had to actually start collecting those metrics on TCP",
    "start": "2824239",
    "end": "2830299"
  },
  {
    "start": "2825000",
    "end": "2887000"
  },
  {
    "text": "connections because we didn't have visibility across the cluster into that when we first when we first fell into",
    "start": "2830299",
    "end": "2837049"
  },
  {
    "text": "this problem so we had to collect new metrics on the amount of connections that we have established and we have",
    "start": "2837049",
    "end": "2843259"
  },
  {
    "text": "them in all different like established and the different TCP states that connections can be in we had to delve",
    "start": "2843259",
    "end": "2851029"
  },
  {
    "text": "more deeply into understanding how those worked and probably most importantly we",
    "start": "2851029",
    "end": "2856369"
  },
  {
    "text": "had to optimize our client code to reduce the amount of connections that it was making so optimize the actual",
    "start": "2856369",
    "end": "2864619"
  },
  {
    "text": "libraries to reduce the amount of load on the servers more so than just throw up to get us out of this problem",
    "start": "2864619",
    "end": "2871250"
  },
  {
    "text": "specifically I don't show it here we actually to prevent the issue we started by just throwing a whole bunch of",
    "start": "2871250",
    "end": "2877070"
  },
  {
    "text": "servers at the problem and which is fantastic in Amazon when you run into a problem you can do that and that will",
    "start": "2877070",
    "end": "2884450"
  },
  {
    "text": "get you out of an issue for tonight at least so overall our results have been",
    "start": "2884450",
    "end": "2891610"
  },
  {
    "start": "2887000",
    "end": "2940000"
  },
  {
    "text": "really dependable i/o performance out of EBS our copper cluster doesn't do a",
    "start": "2891610",
    "end": "2897680"
  },
  {
    "text": "whole lot steady-state it's about seven megabytes per second but it does that all day every day people use the",
    "start": "2897680",
    "end": "2903230"
  },
  {
    "text": "internet 24/7 so we get metrics about it 24/7 we've seen really good bursting",
    "start": "2903230",
    "end": "2908930"
  },
  {
    "text": "performance this graph is from our from our open th DB cluster and you can see",
    "start": "2908930",
    "end": "2915200"
  },
  {
    "text": "it kind of Peaks are out at around 125 megabytes per second which is the limit applied to the instance so if we we",
    "start": "2915200",
    "end": "2922700"
  },
  {
    "text": "would need bigger instances to get more to scale out of that but that's plenty for our for our yeast casing we've",
    "start": "2922700",
    "end": "2927920"
  },
  {
    "text": "really conceived seeing that we can hit that 100 and we can saturate the instance limits pretty much on command",
    "start": "2927920",
    "end": "2934280"
  },
  {
    "text": "which is really fantastic without a lot of really getting exactly what we're offered what we're promised there so",
    "start": "2934280",
    "end": "2942710"
  },
  {
    "start": "2940000",
    "end": "2998000"
  },
  {
    "text": "we've also been able to achieve some cost savings because by moving to EBS we",
    "start": "2942710",
    "end": "2949910"
  },
  {
    "text": "feel comfortable reducing our default replication from 3 to 2 that's opposed",
    "start": "2949910",
    "end": "2955280"
  },
  {
    "text": "to the default there are some higher there are some applications that use a higher availability so in doing so the",
    "start": "2955280",
    "end": "2963710"
  },
  {
    "text": "total cost of our system that is comparing d2 to excels to the m42 excels",
    "start": "2963710",
    "end": "2969740"
  },
  {
    "text": "plus St ones that we talked about earlier that increases the total cost of our cluster by about 12% but when you",
    "start": "2969740",
    "end": "2978860"
  },
  {
    "text": "look at it in terms of what's actually usable storage space that decreases our",
    "start": "2978860",
    "end": "2984710"
  },
  {
    "text": "cost of per usable gigabyte by 25% I mean this is just pure straight up what",
    "start": "2984710",
    "end": "2990500"
  },
  {
    "text": "our AWS bill storage costs are going to be it does not account anything for you",
    "start": "2990500",
    "end": "2995540"
  },
  {
    "text": "know produce maintenance costs or faster turnaround things like that and of course a big advantage was",
    "start": "2995540",
    "end": "3002280"
  },
  {
    "start": "2998000",
    "end": "3040000"
  },
  {
    "text": "decoupling the instances from the storage I mentioned earlier that we were able to experiment with different",
    "start": "3002280",
    "end": "3007410"
  },
  {
    "text": "instance types without having to do an extensive data moving process we're able",
    "start": "3007410",
    "end": "3013680"
  },
  {
    "text": "to replace data quickly before it would take a minimum of eight hours of data to",
    "start": "3013680",
    "end": "3019590"
  },
  {
    "text": "replicate everything off of the note and now it takes 20-30 minutes to detach",
    "start": "3019590",
    "end": "3026600"
  },
  {
    "text": "redeploy and attach which for us has been just it's it's just great to be",
    "start": "3026600",
    "end": "3033420"
  },
  {
    "text": "able to well you have a poor performing instance just replace it and move on with your life and I think I'm going to",
    "start": "3033420",
    "end": "3040620"
  },
  {
    "start": "3040000",
    "end": "3132000"
  },
  {
    "text": "pass it back to Paul to wrap up for us okay so a few things you can look dive",
    "start": "3040620",
    "end": "3047850"
  },
  {
    "text": "more into I mentioned earlier there are a number of sessions that were available for no sequel particularly in particular",
    "start": "3047850",
    "end": "3053910"
  },
  {
    "text": "there's a whole bunch of EBS sessions and storage sessions this week if you really want to add more into different",
    "start": "3053910",
    "end": "3059760"
  },
  {
    "text": "aspects of the storage components and our services by all means you can",
    "start": "3059760",
    "end": "3065070"
  },
  {
    "text": "explore some of those also we have a Kafka confluent QuickStart so if you want to spin up a copper cluster that's",
    "start": "3065070",
    "end": "3072270"
  },
  {
    "text": "pretty much you you go to the QuickStart and you just click a few buttons and select a few things but builds a V PC",
    "start": "3072270",
    "end": "3078750"
  },
  {
    "text": "for you it deploys everything for you you can actually do this and try it out yourself if you want to end up deploying",
    "start": "3078750",
    "end": "3084240"
  },
  {
    "text": "your own your cluster with a confluent and Kafka so and then a C I think",
    "start": "3084240",
    "end": "3090810"
  },
  {
    "text": "there's one more slide here but it's okay so that's all we have for this it's",
    "start": "3090810",
    "end": "3096180"
  },
  {
    "text": "one reiterate that there are so many advantages of using EBS for your storage and just pay attention to the i/o",
    "start": "3096180",
    "end": "3102270"
  },
  {
    "text": "patterns of your workloads and you can then choose the right storage for those as you see there are many use cases for",
    "start": "3102270",
    "end": "3108990"
  },
  {
    "text": "using st1 sc1 volumes compared to the SSD the GP 2's",
    "start": "3108990",
    "end": "3114000"
  },
  {
    "text": "but GP twos are still a great general-purpose storage so if you don't weigh don't care that's probably your first choice to try",
    "start": "3114000",
    "end": "3120960"
  },
  {
    "text": "out and then go from there so I want to thank Daniel Anna for joining us today",
    "start": "3120960",
    "end": "3125970"
  },
  {
    "text": "and really hearing their story so thank you everybody [Applause]",
    "start": "3125970",
    "end": "3134420"
  }
]