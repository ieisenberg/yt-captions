[
  {
    "start": "0",
    "end": "88000"
  },
  {
    "text": "my name is Craig Rachel I'm a solution architect from the public from the Wellington office I'm in the public",
    "start": "0",
    "end": "5250"
  },
  {
    "text": "sector team and today we'll be talking about building service ETL pipelines with AWS glue now I even worked for AWS",
    "start": "5250",
    "end": "12870"
  },
  {
    "text": "for my entire life that might come as a surprise to you but before I before I joined OWS I worked for a bunch of um",
    "start": "12870",
    "end": "18690"
  },
  {
    "text": "public sector agencies and part of my day job there was to actually build ETL pipelines and and I've worked in roles",
    "start": "18690",
    "end": "26189"
  },
  {
    "text": "like developing the pipe developing the code provisioning the infrastructure that run the pipeline's",
    "start": "26189",
    "end": "31500"
  },
  {
    "text": "to actually fix the pipeline's when they break architecting the pipelines so I've",
    "start": "31500",
    "end": "36840"
  },
  {
    "text": "got some sort of personal experience with what the challenges involved in them in ETL tasks actually are and what",
    "start": "36840",
    "end": "43920"
  },
  {
    "text": "I'd like to do today is actually put you ask you to put yourself in the shoes of someone who's actually been who's been",
    "start": "43920",
    "end": "50190"
  },
  {
    "text": "given that job of actually marrying up the disparate and diverse set of data sources that we need to deal with in",
    "start": "50190",
    "end": "56789"
  },
  {
    "text": "most enterprises and actually servicing the needs of data analysts and data scientists on the other end of the",
    "start": "56789",
    "end": "62760"
  },
  {
    "text": "street on other end of the pipe line and your your task as the CTL expert is - I",
    "start": "62760",
    "end": "68549"
  },
  {
    "text": "guess abstract the whale of gnarly and arcane and obscure details involved in",
    "start": "68549",
    "end": "74070"
  },
  {
    "text": "translating and transforming data from one format to another so that you're in consumers those data scientists and our",
    "start": "74070",
    "end": "80820"
  },
  {
    "text": "analysts are blissfully unaware of all the the hijinks and gymnastics you've had to do along the process so what I'd",
    "start": "80820",
    "end": "89759"
  },
  {
    "start": "88000",
    "end": "553000"
  },
  {
    "text": "like to do is just start with a with a typical flow of what a modern ETL pipeline would look like running on AWS",
    "start": "89759",
    "end": "96439"
  },
  {
    "text": "so the first thing is the is generation sorry first thing is generating the data",
    "start": "96439",
    "end": "102540"
  },
  {
    "text": "sources now the typical sort of generators have data saw this data sources in the prizes of things like",
    "start": "102540",
    "end": "108500"
  },
  {
    "text": "transactional legacy systems ERP systems things like si P more and more we're",
    "start": "108500",
    "end": "113670"
  },
  {
    "text": "seeing things like web logs capturing information about the citizens and the consumers they're actually hitting the",
    "start": "113670",
    "end": "119460"
  },
  {
    "text": "websites if you're working in the utility space you might be seeing some more sensor networks they're feeding",
    "start": "119460",
    "end": "124829"
  },
  {
    "text": "data into the data pipelines the next part of your system is the collection side and you might see services like you",
    "start": "124829",
    "end": "132060"
  },
  {
    "text": "might see polling services running on see to going out to the enterprise systems and pulling down real file",
    "start": "132060",
    "end": "137370"
  },
  {
    "text": "shares or from um from databases a modern system might be dealing with using technologies like Amazon fire hose",
    "start": "137370",
    "end": "143730"
  },
  {
    "text": "to pull it pull in real time data Kinesis streams you might also see",
    "start": "143730",
    "end": "149550"
  },
  {
    "text": "things like AWS DMS so if you're doing things like database replication replicating a source database running",
    "start": "149550",
    "end": "156320"
  },
  {
    "text": "on-premises to a target database running an AWS you might see that as well you'll",
    "start": "156320",
    "end": "161730"
  },
  {
    "text": "all or just they'll always see Amazon s3 so it can be it's a very versatile service a lot of customers that I'm",
    "start": "161730",
    "end": "167460"
  },
  {
    "text": "working with are syncing data from their own premises systems into s3 buckets on a regular basis pushing that data to AWS",
    "start": "167460",
    "end": "174450"
  },
  {
    "text": "and you might also see snowballs where they're available so you can use snowballs to do bog transfers of data so",
    "start": "174450",
    "end": "182220"
  },
  {
    "text": "it's on the collection side and on the storage side it's a it's a fairly traditional picture as well s3 figures",
    "start": "182220",
    "end": "187530"
  },
  {
    "text": "large as a storage medium forum for bog data it figured as you'll see later in",
    "start": "187530",
    "end": "193140"
  },
  {
    "text": "the story at figures large in the actual ETL process as well you'll see that services like Amazon either yes various",
    "start": "193140",
    "end": "199590"
  },
  {
    "text": "flavors of engines running within that yes and you'll see customers running their own databases on ec2 as well so",
    "start": "199590",
    "end": "206040"
  },
  {
    "text": "that's that's your storage layer I'm going to skip over the extract and transform Lakers I don't that's going to",
    "start": "206040",
    "end": "211500"
  },
  {
    "text": "be the exciting part of the presentation and I'm going to go to the analyze layer now the sort of services you might see",
    "start": "211500",
    "end": "216720"
  },
  {
    "text": "in the analyze layer are things like Amazon Athena which are access a query engine against data you might see Amazon",
    "start": "216720",
    "end": "223290"
  },
  {
    "text": "EMR so a lot of my customers are running things like spark clusters on top of the EMR to run things like our functions",
    "start": "223290",
    "end": "230100"
  },
  {
    "text": "across some large data sets using frames some customers might be running redshift",
    "start": "230100",
    "end": "235380"
  },
  {
    "text": "and richard redshift spectrum as well and some might be running Connexus analytics when they're dealing with real time data and then lastly we've got the",
    "start": "235380",
    "end": "242490"
  },
  {
    "text": "final cut the end consumers of our pipeline the the data scientists and data analysts the data scientist will be",
    "start": "242490",
    "end": "248190"
  },
  {
    "text": "happily ensconced in there our studio environments working working with data frames running in spark you might have",
    "start": "248190",
    "end": "254100"
  },
  {
    "text": "data analysts who are doing things like running tableau or quick site to do queries SQL oriented queries you might",
    "start": "254100",
    "end": "259890"
  },
  {
    "text": "have business users they're running things like dashboards against the data as well so there's a variety of applications",
    "start": "259890",
    "end": "265380"
  },
  {
    "text": "variety of them visualization tools that I mean users",
    "start": "265380",
    "end": "270570"
  },
  {
    "text": "can use now I promise to go back to ETL and what's actually in ETL with services",
    "start": "270570",
    "end": "275760"
  },
  {
    "text": "like EMR and lambda and Amazon Kinesis so that's pretty much the state of play",
    "start": "275760",
    "end": "282990"
  },
  {
    "text": "at the moment as far as ETL technology goes you might be wondering so what's wrong with that was the problem well",
    "start": "282990",
    "end": "289320"
  },
  {
    "text": "I've got it I've got a solution stack that looks okay it's all cloud-based but the challenges associated with using",
    "start": "289320",
    "end": "294990"
  },
  {
    "text": "those tools in there ETL stage is actually the volumes of data that they're dealing with some some of my",
    "start": "294990",
    "end": "300360"
  },
  {
    "text": "customers have seen situations where their data growth or data requirements and storage requirements have gone from",
    "start": "300360",
    "end": "306060"
  },
  {
    "text": "a linear function to a two am an exponential function they're talking about how many years does it take for",
    "start": "306060",
    "end": "311970"
  },
  {
    "text": "their data volumes to double and that's being driven by things like increasing time resolution that data increasing",
    "start": "311970",
    "end": "318210"
  },
  {
    "text": "sources of data things like web analytics feeding into their data warehouse what more ways of engaging with citizens generating massive amounts",
    "start": "318210",
    "end": "325199"
  },
  {
    "text": "of data the other thing that's driving this is the need to or that's complicating this is the need to",
    "start": "325199",
    "end": "330449"
  },
  {
    "text": "incorporate disparate data sets you've got data sets they're coming from the general ecosystem not just from the",
    "start": "330449",
    "end": "336030"
  },
  {
    "text": "agency's internal systems so you've got communities of services that trying to",
    "start": "336030",
    "end": "341039"
  },
  {
    "text": "feed into a single a single data lake you've also got things like vendor systems vendor formats that are changing",
    "start": "341039",
    "end": "347370"
  },
  {
    "text": "and driving the UM the need to perform ETL processes and and organizations",
    "start": "347370",
    "end": "352979"
  },
  {
    "text": "don't stand still either so some of my customers have actually got a current system and what they call a heritage system which is a system that hasn't",
    "start": "352979",
    "end": "359250"
  },
  {
    "text": "actually been shut down but it's been kept there in case they need to run forensic analysis or or some sort of um",
    "start": "359250",
    "end": "365099"
  },
  {
    "text": "machine learning job in future so there's there's multiple generations of systems are floating around there all",
    "start": "365099",
    "end": "370770"
  },
  {
    "text": "generating data oftentimes in different formats and the challenge that ETL has is to actually",
    "start": "370770",
    "end": "376260"
  },
  {
    "text": "synthesize all of these in a unified way and then lastly you might have been to some presentations today talking about",
    "start": "376260",
    "end": "382199"
  },
  {
    "text": "machine learning so that need to have highly highly refined training sets for",
    "start": "382199",
    "end": "387479"
  },
  {
    "text": "doing machine learning is and also also a driver now as I said I asked you to have a look at to think about the",
    "start": "387479",
    "end": "393720"
  },
  {
    "text": "problem from an ETL software developers perspective and what well I'd like to share with you as",
    "start": "393720",
    "end": "399840"
  },
  {
    "text": "well I often businesses analysts would tell me hey we've got this new data set from agency",
    "start": "399840",
    "end": "405280"
  },
  {
    "text": "X or from this international body and we'd like you to start incorporating their data warehouse by the way it's",
    "start": "405280",
    "end": "411610"
  },
  {
    "text": "sitting on this file share or are sitting in s3 and that's pretty much what you're going to see just an obscure",
    "start": "411610",
    "end": "416920"
  },
  {
    "text": "bunch of directories you think great how am I going to make sense of this what I'll do is I'll step into one of those directories and have a look and bring it",
    "start": "416920",
    "end": "422800"
  },
  {
    "text": "up in the editor and see what it is so you open up a directory oh my god more directories more obscurely named",
    "start": "422800",
    "end": "428560"
  },
  {
    "text": "directories what they make would he do then so you persist you keep on drilling down through the hierarchies finally you",
    "start": "428560",
    "end": "434350"
  },
  {
    "text": "get to a piece of um something you can read and obviously this is a piece of Jason but then the question that might",
    "start": "434350",
    "end": "439420"
  },
  {
    "text": "pop into your mind is well I've done this a random sampling my data how can I know that this is actually the",
    "start": "439420",
    "end": "445030"
  },
  {
    "text": "consistent scheme and that's being applied throughout the entire data set because potentially be talking about terabytes of data so this is sort of the",
    "start": "445030",
    "end": "452080"
  },
  {
    "text": "problems that ETL engineers have are actually facing on a regular basis it's not the end of the story where mentioned",
    "start": "452080",
    "end": "458440"
  },
  {
    "text": "are the volumes the growth of data volumes and additional data sources but the other challenge is that these are",
    "start": "458440",
    "end": "464140"
  },
  {
    "text": "these data sources and the mappings from these data sources to the target formats are almost unique to every organization",
    "start": "464140",
    "end": "470590"
  },
  {
    "text": "and whenever you have uniqueness that literally means there's a human involved in writing code to manage that diversity",
    "start": "470590",
    "end": "476920"
  },
  {
    "text": "so managing all that those variations in hand coding as a challenge the formats",
    "start": "476920",
    "end": "482500"
  },
  {
    "text": "change over time so as I mentioned that the organization's evolve the data sources evolve these most perhaps more",
    "start": "482500",
    "end": "489340"
  },
  {
    "text": "than them so how do you actually deal with these changes over time as well and",
    "start": "489340",
    "end": "495910"
  },
  {
    "text": "even recognize that the changes have happened proactively rather than waiting for your ETL pipeline to break so what",
    "start": "495910",
    "end": "503590"
  },
  {
    "text": "what the consequence of this is the ETL assumes a much larger footprint in you",
    "start": "503590",
    "end": "509140"
  },
  {
    "text": "in your organization that really deserves ultimate it's a piece of plumbing it's a little bit like the the aviation security queue that you have to",
    "start": "509140",
    "end": "515710"
  },
  {
    "text": "go through on them on your way to your holiday in Tahiti it should just be a minor part a necessary part but it's not",
    "start": "515710",
    "end": "521620"
  },
  {
    "text": "the reason you're taking the trip then a lot of organizations is a whole culture or a whole whole team that builds up",
    "start": "521620",
    "end": "526900"
  },
  {
    "text": "around ETL and really the focus where you want to be is on the the rewarding part the analysis our consumption of",
    "start": "526900",
    "end": "532930"
  },
  {
    "text": "that data so the challenge that we're trying to solve is how to actually shrink that ETL",
    "start": "532930",
    "end": "538180"
  },
  {
    "text": "process down so it's let's less of a burden for organizations to deal with and this is where we come in to glue so",
    "start": "538180",
    "end": "544870"
  },
  {
    "text": "what I've done is I've suggested here that either be as glue is potentially the UM the ones who you need to use for",
    "start": "544870",
    "end": "550660"
  },
  {
    "text": "most of your ETL requirements okay so why why would you what's the benefit of",
    "start": "550660",
    "end": "556540"
  },
  {
    "start": "553000",
    "end": "764000"
  },
  {
    "text": "glue what's its secret sauce that makes it so powerful the first thing is that it has the ability to act to",
    "start": "556540",
    "end": "561730"
  },
  {
    "text": "automatically discover and cattle catalogue your data so I can scan your data and put it into a hive compatible",
    "start": "561730",
    "end": "567910"
  },
  {
    "text": "metadata schema if you've been to some of the other big data presentations you would have heard that MIT arm hive",
    "start": "567910",
    "end": "573070"
  },
  {
    "text": "compatible metadata schema mentioned many times because it's the the engine that drives things like Athena redshift",
    "start": "573070",
    "end": "579760"
  },
  {
    "text": "spectrum it's the piece of information that those query engines need to operate so it's glue will actually generate that",
    "start": "579760",
    "end": "586300"
  },
  {
    "text": "data catalogue for you the other thing is that once it's cataloged that makes that data of merely aquaria ball buy",
    "start": "586300",
    "end": "592510"
  },
  {
    "text": "services like Athena and redshift spectrum and the EM are so they can leverage that catalog to actually run",
    "start": "592510",
    "end": "598150"
  },
  {
    "text": "the queries directly on the data while it's sitting on this three there's no need to actually load it up into a",
    "start": "598150",
    "end": "604089"
  },
  {
    "text": "relational database or an OLAP type database you can run the queries on in place directly against that data and",
    "start": "604089",
    "end": "610750"
  },
  {
    "text": "that catalog is that secret intra is that secret piece of information that needs the other thing that glue does is",
    "start": "610750",
    "end": "617080"
  },
  {
    "text": "it generates the code for you or at least gives generates the first iteration of the code for you so it will",
    "start": "617080",
    "end": "622330"
  },
  {
    "text": "generate your ETL code that will click it will they clean your code will generate cleaning logic for you",
    "start": "622330",
    "end": "627400"
  },
  {
    "text": "enrichment logic for you mapping logic and choice resolution logic for you the",
    "start": "627400",
    "end": "632770"
  },
  {
    "text": "data is adaptable so once it's actually been generated by the mapping tool you can actually go and edit that and you in",
    "start": "632770",
    "end": "638620"
  },
  {
    "text": "a lot of cases customers will with sophisticated requirements will use the the generators just to do produce the",
    "start": "638620",
    "end": "645700"
  },
  {
    "text": "first iteration and then take it from there they'll get it to do the donkey work under null they'll add code at",
    "start": "645700",
    "end": "650829"
  },
  {
    "text": "themselves from that point on and and the other thing is that glue generate doesn't generate code in the in obscure",
    "start": "650829",
    "end": "656470"
  },
  {
    "text": "language or any proprietary schemas it's actually just using PI spark or Scala",
    "start": "656470",
    "end": "661930"
  },
  {
    "text": "when you weren't actually generating the code so that means portability if you need to take if you",
    "start": "661930",
    "end": "667329"
  },
  {
    "text": "that you want to take your Pais barcode and runner than you know or cloud or a different cloud then you can do that you",
    "start": "667329",
    "end": "674170"
  },
  {
    "text": "can actually take this source code it's been checked into github and you can run that code in your choice of environment",
    "start": "674170",
    "end": "680259"
  },
  {
    "text": "that's quite important to some of the customers some of my customers in New Zealand at least and it runs the job",
    "start": "680259",
    "end": "687040"
  },
  {
    "text": "service so that's a really powerful one a powerful feature of glue because I",
    "start": "687040",
    "end": "693459"
  },
  {
    "text": "mentioned that as an ETL developer your job was thinking about schemas but often",
    "start": "693459",
    "end": "698829"
  },
  {
    "text": "times your job as an ETL developer was actually thinking about infrastructure as well worried about how fast your",
    "start": "698829",
    "end": "704230"
  },
  {
    "text": "saying is how how many I ops you can generate there that you seen when you're dealing with this and making sure you",
    "start": "704230",
    "end": "710110"
  },
  {
    "text": "got the right operating system versions when you're running service that problem sort of disappears because what's",
    "start": "710110",
    "end": "715420"
  },
  {
    "text": "happening is that the jobs are running within an AWS managed cluster for you so you're not having to worry about",
    "start": "715420",
    "end": "721420"
  },
  {
    "text": "provisioning all that infrastructure and we'll talk a little bit later about how that how that world between the AWS",
    "start": "721420",
    "end": "727209"
  },
  {
    "text": "manager and your particular data resources is bridged but just suffice to say at the moment that them that",
    "start": "727209",
    "end": "733179"
  },
  {
    "text": "database is taken care of the provisioning the resources to run those ETL jobs for you",
    "start": "733179",
    "end": "738819"
  },
  {
    "text": "and lastly Googler gives you the ability to schedule and trigger your job so you can do the the classics or the time cron",
    "start": "738819",
    "end": "745059"
  },
  {
    "text": "based scheduling but you can also set up quite complex dependency chains where jobs jobs the completion of jobs",
    "start": "745059",
    "end": "752110"
  },
  {
    "text": "triggers subsequent jobs you can have all conditions around conditions that say once this job a and job and job B",
    "start": "752110",
    "end": "758619"
  },
  {
    "text": "complete the I can start job C so there's a lot of flexibility there as well now I'm going to move fairly",
    "start": "758619",
    "end": "766480"
  },
  {
    "start": "764000",
    "end": "906000"
  },
  {
    "text": "quickly through some of the some of the slides that the slides that shows screenshots of glue because rather than",
    "start": "766480",
    "end": "772779"
  },
  {
    "text": "getting it to squeaky erisa them fairly hard to read them screenshot so I'm",
    "start": "772779",
    "end": "777790"
  },
  {
    "text": "going to actually show you a live demonstration where I'm going to do some glue jobs for you so you'll see it and",
    "start": "777790",
    "end": "783339"
  },
  {
    "text": "much much more interactively what it looks like so this going back to the",
    "start": "783339",
    "end": "789100"
  },
  {
    "text": "theory though how the way to my data so the the typical flow is four steps now I'm in practice we describe those four",
    "start": "789100",
    "end": "795519"
  },
  {
    "text": "steps in practice and most organizations there's a fear that event or iteration and experiment that goes on but for now we'll just say",
    "start": "795519",
    "end": "802339"
  },
  {
    "text": "there's there's four steps that are completed sequentially the first step is crawling and crawling is actually the",
    "start": "802339",
    "end": "808129"
  },
  {
    "text": "process of discovering what the schemer is in your data and that really is a simple matter of actually pointing",
    "start": "808129",
    "end": "813290"
  },
  {
    "text": "pointing glue at the data an s3 bucket or a database schema a database running",
    "start": "813290",
    "end": "820069"
  },
  {
    "text": "on either yes for example and telling it to find out what is the schema behind this and then guru glue will launch the",
    "start": "820069",
    "end": "825499"
  },
  {
    "text": "crawler and actually discover that schema and publish it into the metadata catalog the next stage is mapping just",
    "start": "825499",
    "end": "832819"
  },
  {
    "text": "um but that there so mapping is the process of taking your source schema that the discovered schema to your",
    "start": "832819",
    "end": "839689"
  },
  {
    "text": "target schema so as in each field ithi el designer you'll have some schema in",
    "start": "839689",
    "end": "844850"
  },
  {
    "text": "mind that you want to map to so you're going from a source kheema to a target schema and that mapping process can be",
    "start": "844850",
    "end": "851059"
  },
  {
    "text": "as simple as this renaming a few fields mate doing a few type conversions to a much more sophisticated restructuring of",
    "start": "851059",
    "end": "856850"
  },
  {
    "text": "the data doing things like relational lysing data that is in relational by its nature and we'll touch on that a little",
    "start": "856850",
    "end": "864319"
  },
  {
    "text": "bit during the demonstration you can then edit and explore the data so once you've actually done a mapping you can",
    "start": "864319",
    "end": "870319"
  },
  {
    "text": "decide to actually do further you can run query tools directly against that you can start running Athena or against",
    "start": "870319",
    "end": "875749"
  },
  {
    "text": "at running queries to discover more about the UM the actual structure of the data and you can iterate on that process",
    "start": "875749",
    "end": "881959"
  },
  {
    "text": "so you can build together chains of ETL processes to reach your target state and",
    "start": "881959",
    "end": "886970"
  },
  {
    "text": "then lastly you can actually schedule these jobs so once you've gone past the on demand experimentation type of",
    "start": "886970",
    "end": "892939"
  },
  {
    "text": "process and you're ready for things to actually go into production status then you can start setting up schedules to",
    "start": "892939",
    "end": "898339"
  },
  {
    "text": "run these things on a regular basis to reflect that the the timeline under which the source data sets being changed",
    "start": "898339",
    "end": "906610"
  },
  {
    "start": "906000",
    "end": "1190000"
  },
  {
    "text": "so let's go through those four steps one by one just to look just quickly but I'll just highlight some of the theory",
    "start": "906610",
    "end": "913309"
  },
  {
    "text": "involved just so that you prepare when we get into the demo so first of all crawlers now crawlers are mentioned that",
    "start": "913309",
    "end": "919699"
  },
  {
    "text": "either to automatically discover your data and one of the key elements of the or the key value adds with glue is it",
    "start": "919699",
    "end": "925910"
  },
  {
    "text": "has a whole library of built-in classifiers and those classifiers will understand common formats and see",
    "start": "925910",
    "end": "931970"
  },
  {
    "text": "those in the mix but you can also write your own classifiers so if you're if you've got some decades-old data format that was",
    "start": "931970",
    "end": "940570"
  },
  {
    "text": "invented by someone who's long since retired and it was designed for human readability and not so much for",
    "start": "940570",
    "end": "947110"
  },
  {
    "text": "automated processing you can actually write custom classifiers to help parse that using regular using tools like",
    "start": "947110",
    "end": "953710"
  },
  {
    "text": "regular expressions and you can build those into your Python automatically if you need to so I mentioned that they are",
    "start": "953710",
    "end": "961900"
  },
  {
    "text": "some of the major data for data classifiers that the cloud that the crawlers will work with the first um the",
    "start": "961900",
    "end": "968290"
  },
  {
    "text": "first thing to notice the crawlers can either use JDBC connections as their source so there opens it up to things",
    "start": "968290",
    "end": "973870"
  },
  {
    "text": "like any of the Amazon RDS databases it opens it up to redshift it also opens up",
    "start": "973870",
    "end": "979570"
  },
  {
    "text": "which isn't on the slide here but it also has opens up to the possibility of querying databases managed by by the",
    "start": "979570",
    "end": "985780"
  },
  {
    "text": "customers running on ec2 or even on premises it also clauses can also deal",
    "start": "985780",
    "end": "992260"
  },
  {
    "text": "with object connections and these are just pointers to s3 buckets and then the crawlers will go away and recurse down",
    "start": "992260",
    "end": "998560"
  },
  {
    "text": "through the bucket structure through the folder structure and discover what's actually underneath the classifiers that",
    "start": "998560",
    "end": "1004290"
  },
  {
    "text": "built-in will will arm things for the open there's built-in classifiers for all the open source databases that can",
    "start": "1004290",
    "end": "1010140"
  },
  {
    "text": "query their data dictionaries to discover the date of the underlying tables and there's also support for",
    "start": "1010140",
    "end": "1015560"
  },
  {
    "text": "common open source compression formats cut columnar compression formats like Parque Avro and ork obviously works with",
    "start": "1015560",
    "end": "1022800"
  },
  {
    "text": "Jason that supports a number of um compression types B's gzip type compression types and variations on that",
    "start": "1022800",
    "end": "1029339"
  },
  {
    "text": "and regular CSV where would be that CSV and then um obviously supports custom",
    "start": "1029339",
    "end": "1034980"
  },
  {
    "text": "graph expressions that's where they a classifier looks like I'm not going to",
    "start": "1034980",
    "end": "1042079"
  },
  {
    "text": "belabor the UM the syntax here but suffice tips are pointing my laser of the humanly the UM the grok expressions",
    "start": "1042080",
    "end": "1050250"
  },
  {
    "text": "are more or less just the generalization of generalization about regular expressions so anyone that knows regular",
    "start": "1050250",
    "end": "1056220"
  },
  {
    "text": "expressions could will be fairly up to speed with writing their own grok expressions there's also built in",
    "start": "1056220",
    "end": "1061320"
  },
  {
    "text": "expressions for things like common timestamp formats so if your time stance on and in the ISO standard format you can write",
    "start": "1061320",
    "end": "1067970"
  },
  {
    "text": "your use a building block to build up your own timestamp parsers and there's a little example on the UM on the slide",
    "start": "1067970",
    "end": "1075049"
  },
  {
    "text": "there about crawling out an Apache weblog so things like log levels meshes",
    "start": "1075049",
    "end": "1080870"
  },
  {
    "text": "prefixes the other thing that crawls can do is to detect partitions and",
    "start": "1080870",
    "end": "1087380"
  },
  {
    "text": "partitions are a really powerful feature of um when you organize your data on this three what I mean by partitions is",
    "start": "1087380",
    "end": "1094220"
  },
  {
    "text": "they're really common pattern for example is to organize your your s3 based data entered into folder",
    "start": "1094220",
    "end": "1101330"
  },
  {
    "text": "structures that are organized into years months and days and that gives you a very convenient way of organizing your",
    "start": "1101330",
    "end": "1107480"
  },
  {
    "text": "data but also applying a lifecycle policy so that all the data ages out or moves down to two other cloud storage",
    "start": "1107480",
    "end": "1113509"
  },
  {
    "text": "formats so what I'm what I what the glue is able to do is recursive or the",
    "start": "1113509",
    "end": "1119059"
  },
  {
    "text": "structures and actually use a heuristic to discover ask questions like is that folder actually a year is am i seeing",
    "start": "1119059",
    "end": "1126980"
  },
  {
    "text": "that same year repeated over and over again and Ken what can I reliably infer that that folder name that says year",
    "start": "1126980",
    "end": "1133759"
  },
  {
    "text": "equals 2018 is actually an attribute of the underlying data but says that this data is actually one of the Year",
    "start": "1133759",
    "end": "1139820"
  },
  {
    "text": "attribute for my underlying data is 2018 likewise with the the month and the year and the day components underneath so",
    "start": "1139820",
    "end": "1147019"
  },
  {
    "text": "what ends up happening is that glue can infer that the that that folder structure is actually not just a bit of",
    "start": "1147019",
    "end": "1153559"
  },
  {
    "text": "decoration and housekeeping it's actually put an integral part of the data itself and that will extend the",
    "start": "1153559",
    "end": "1159799"
  },
  {
    "text": "scheme and that discovers with that folder structure and on the on the slide there you'll see a little example there",
    "start": "1159799",
    "end": "1165289"
  },
  {
    "text": "with our months and days didn't excuse me the I'll step through these quickly",
    "start": "1165289",
    "end": "1172429"
  },
  {
    "text": "so you'll see those in the demo that with the do glue data catalog and the table properties very hard to read there",
    "start": "1172429",
    "end": "1178519"
  },
  {
    "text": "and you'll see a live one in the moment anyway that the table properties bring up stable statistics the columns there's",
    "start": "1178519",
    "end": "1184669"
  },
  {
    "text": "discovered and the data formats of those columns",
    "start": "1184669",
    "end": "1188740"
  },
  {
    "start": "1190000",
    "end": "1390000"
  },
  {
    "text": "so how do I build the ETL so the first the glue will actually guide you through",
    "start": "1190299",
    "end": "1196760"
  },
  {
    "text": "the ETL process it will generate the code for you you through a job authoring tool it will let you graphically",
    "start": "1196760",
    "end": "1203240"
  },
  {
    "text": "manipulate the mappings it generates code for you and you can actually see a",
    "start": "1203240",
    "end": "1209419"
  },
  {
    "text": "little bit of code there you'll see it in the demo on the moment it's quite quite readable as it's actually got",
    "start": "1209419",
    "end": "1214669"
  },
  {
    "text": "documentation within it the functions are relatively clear it's declarative there's not many condition there's no",
    "start": "1214669",
    "end": "1220610"
  },
  {
    "text": "loops or conditions in there now one piece of theory I will mention is that glue hat you glue uses the concept of",
    "start": "1220610",
    "end": "1226669"
  },
  {
    "text": "dynamic frames and dynamic frames as a way of actually dealing with surprises in your data because you might your",
    "start": "1226669",
    "end": "1232880"
  },
  {
    "text": "drill realize that once you've actually catalogued your data there's no guarantee it's still going that's not going to change data sources changed all",
    "start": "1232880",
    "end": "1239330"
  },
  {
    "text": "the time unexpectedly so what dynamic frames let you do is set a policy for how to deal with those",
    "start": "1239330",
    "end": "1244760"
  },
  {
    "text": "surprises so those those surprises could be for example that the data type suddenly changes from an integer to a",
    "start": "1244760",
    "end": "1251570"
  },
  {
    "text": "string so the you can set of dynamic frames are able to keep track of those",
    "start": "1251570",
    "end": "1256640"
  },
  {
    "text": "changes of the row level and actually set a policy for whether it should cast the data for a mum from the source to",
    "start": "1256640",
    "end": "1262640"
  },
  {
    "text": "the target whether it should know that or whether it should drop it out of the mapping altogether",
    "start": "1262640",
    "end": "1268630"
  },
  {
    "text": "the the other and the key piece of functionality and glue that lets you do that there's a function called resolve",
    "start": "1273809",
    "end": "1279910"
  },
  {
    "text": "choice and result choice will let you will let you take these these variations in the data and actually decides where",
    "start": "1279910",
    "end": "1287020"
  },
  {
    "text": "that you do that mapping or the caste operation or in fact they even split into two separate columns so you can",
    "start": "1287020",
    "end": "1292420"
  },
  {
    "text": "leave it as a task for the downstream process to our to deal with the fact that there's actually a string",
    "start": "1292420",
    "end": "1297490"
  },
  {
    "text": "representation of value and an integer version of the data planning around so that so you're not losing data during",
    "start": "1297490",
    "end": "1304240"
  },
  {
    "text": "the mapping process there's also the apply mapping function which is the the",
    "start": "1304240",
    "end": "1309640"
  },
  {
    "text": "function that does the renaming flattens out structures nested structures does",
    "start": "1309640",
    "end": "1315040"
  },
  {
    "text": "the actual reordering and type conversions of data and there's a quite",
    "start": "1315040",
    "end": "1321010"
  },
  {
    "text": "a magical function called relational eyes which will actually take nested arrays and turn them into flatten them",
    "start": "1321010",
    "end": "1326440"
  },
  {
    "text": "out into a relational model so it will actually build secondary tables for you build the foreign key relationships for",
    "start": "1326440",
    "end": "1332020"
  },
  {
    "text": "you so you can take a nested nested arrays within structures and turn them into a relational model that your",
    "start": "1332020",
    "end": "1337929"
  },
  {
    "text": "relational query tools can happily deal with and there's some eto code there I'm",
    "start": "1337929",
    "end": "1343660"
  },
  {
    "text": "just gonna quit quick moving so we don't um during that time developer endpoints because you're running these running",
    "start": "1343660",
    "end": "1350650"
  },
  {
    "text": "glue in a cluster that's that's provision on demand you might be wondering so as a developer how to actually interact with this how to",
    "start": "1350650",
    "end": "1356950"
  },
  {
    "text": "actually run my zip love notebook against glue when the cluster comes and goes on based on jobs and the answer is",
    "start": "1356950",
    "end": "1362650"
  },
  {
    "text": "that will provision endpoints for you to point your your notebooks at so that you",
    "start": "1362650",
    "end": "1367900"
  },
  {
    "text": "can run that sort of coding expression evaluation print type loops interactively and lastly the all the",
    "start": "1367900",
    "end": "1377590"
  },
  {
    "text": "code associated with glue itself the libraries that make up glue are published in github you can eat there's",
    "start": "1377590",
    "end": "1384220"
  },
  {
    "text": "also sample code publishing github and you can actually use github to share the code that you develop I'm just going to",
    "start": "1384220",
    "end": "1392170"
  },
  {
    "start": "1390000",
    "end": "1502000"
  },
  {
    "text": "run quickly through the execution mechanism I mentioned that that glue creates a bridge between your VP CS and",
    "start": "1392170",
    "end": "1398440"
  },
  {
    "text": "your compute instances and the way it does that is by dropping and then let them elastic net work in the face into",
    "start": "1398440",
    "end": "1403690"
  },
  {
    "text": "your VP C so as far as your VP C's can blue is actually sitting as a network interface within your V PC which means",
    "start": "1403690",
    "end": "1410889"
  },
  {
    "text": "it can talk to your databases who can talk to your on-premises systems over direct connect it gives it access to our",
    "start": "1410889",
    "end": "1416529"
  },
  {
    "text": "s3 endpoints so it means that you've got that the happy mix between the service",
    "start": "1416529",
    "end": "1421539"
  },
  {
    "text": "environments and access to BBC based resources I'll skip over the the pricing",
    "start": "1421539",
    "end": "1430090"
  },
  {
    "text": "unit but I'm just it's um not really relevant to the to the demonstration but",
    "start": "1430090",
    "end": "1435309"
  },
  {
    "text": "the deep use of the unit of cost because you're not actually provisioning an instance type a DP you as a measurement",
    "start": "1435309",
    "end": "1441849"
  },
  {
    "text": "of resource provisioning so you set a cap on the number of DP use which is a 4 gigabyte sorry for V PC years since 4 V",
    "start": "1441849",
    "end": "1449499"
  },
  {
    "text": "CPUs and 16 gigabytes of RAM you say that's the cap I'm prepared to run with and I'm going to actually execute my job",
    "start": "1449499",
    "end": "1456099"
  },
  {
    "text": "up to that cap when you actually built when the job actually runs the bill that you get is actually based on the male",
    "start": "1456099",
    "end": "1461919"
  },
  {
    "text": "resource you're consuming the deep use you consume and they're but they're consumed on a per second basis there is",
    "start": "1461919",
    "end": "1468220"
  },
  {
    "text": "a 10-minute limit lower limit but you only can build for what you consume so there's there's not like a standard EMR",
    "start": "1468220",
    "end": "1473919"
  },
  {
    "text": "cluster where you're actually choosing instance types in the number of instances there's also costs associated",
    "start": "1473919",
    "end": "1480549"
  },
  {
    "text": "with the with the metadata catalog itself but the 3 the free limits are actually so high that I've never seen a",
    "start": "1480549",
    "end": "1486759"
  },
  {
    "text": "customer actually get a bill for their um their schema objects so they're quite generous in that regard and I think I've",
    "start": "1486759",
    "end": "1494289"
  },
  {
    "text": "mentioned job composition already so that's that's the mechanism by which you can chain together glue jobs to create a",
    "start": "1494289",
    "end": "1500139"
  },
  {
    "text": "sequence of our dependency chain so let me switch over to the demo at this point and show you what it looks like in",
    "start": "1500139",
    "end": "1505809"
  },
  {
    "start": "1502000",
    "end": "1832000"
  },
  {
    "text": "practice",
    "start": "1505809",
    "end": "1507960"
  },
  {
    "text": "okay thanks so the scenario on the paint is that we've got a we've got a business",
    "start": "1512120",
    "end": "1517880"
  },
  {
    "text": "analyst who said who's come to me as an ETL developer and said I want you to produce a graph like this all right I",
    "start": "1517880",
    "end": "1525250"
  },
  {
    "text": "want you to produce this graph for me and the example is that it's ground",
    "start": "1525250",
    "end": "1530480"
  },
  {
    "text": "transportation statistics in the city of New York and well that's what I've been asked to do is produce a day by day",
    "start": "1530480",
    "end": "1536720"
  },
  {
    "text": "break down for a particular month showing whether the whether the various transport providers or the number of",
    "start": "1536720",
    "end": "1541940"
  },
  {
    "text": "trips provided by different transport providers in New York for that month and there's actually three separate",
    "start": "1541940",
    "end": "1547940"
  },
  {
    "text": "transport providers that vinum that feeding us data there's our yellow cabs Green shuttles and Anubis and what what",
    "start": "1547940",
    "end": "1557450"
  },
  {
    "text": "you're actually tasked with as a as an ETL analyst there's the job of actually discovering what that data looks like so",
    "start": "1557450",
    "end": "1564200"
  },
  {
    "text": "the the the BA is told you that you just go and have a look in this tree and see what's actually being pushed into s3 so",
    "start": "1564200",
    "end": "1572419"
  },
  {
    "text": "we look at the consult the caper sauce and I've got a bucket here and I've got",
    "start": "1572419",
    "end": "1583220"
  },
  {
    "text": "the folder here that's got um got two subfolders in it green in yellow and so",
    "start": "1583220",
    "end": "1589850"
  },
  {
    "text": "they're obviously the the cabinet that the names of the cab companies the sorry green shuttles in yellow data over look",
    "start": "1589850",
    "end": "1595520"
  },
  {
    "text": "in yellow let's have a look at what's in there so it's a whole bunch of obscurely named files so if I open one of those",
    "start": "1595520",
    "end": "1602270"
  },
  {
    "text": "and he'll look at it this is what I might see so the old Jason question is",
    "start": "1602270",
    "end": "1607549"
  },
  {
    "text": "what's the schema that's being used in this there's actually 25 million records in this some in this bucket",
    "start": "1607549",
    "end": "1613100"
  },
  {
    "text": "what is that schema so what I'll do is I I can use glue to help to come to the rescue",
    "start": "1613100",
    "end": "1619809"
  },
  {
    "text": "just get my",
    "start": "1621399",
    "end": "1624730"
  },
  {
    "text": "I'm going to write a cruller",
    "start": "1633430",
    "end": "1636780"
  },
  {
    "text": "so I say add cruller",
    "start": "1639320",
    "end": "1642700"
  },
  {
    "text": "and call it yellow cruller",
    "start": "1645700",
    "end": "1649138"
  },
  {
    "text": "it's no I'm gonna use s3 as my data source and I'm going to point it at this bucket that I just had look at so I've",
    "start": "1652200",
    "end": "1663090"
  },
  {
    "text": "been pointing at the yellow folder so I can't last me for my other data stores",
    "start": "1663090",
    "end": "1668909"
  },
  {
    "text": "that's enough now it's going to ask me about a policy and I am role to run and that's an important security aspect that",
    "start": "1668909",
    "end": "1674820"
  },
  {
    "text": "blue the I'm going to point at an existing role and this role is actually has the permissions that tells this glue",
    "start": "1674820",
    "end": "1681870"
  },
  {
    "text": "job it's allowed to read that bucket so it's going to it's got a performance certificate series of get up.get object",
    "start": "1681870",
    "end": "1687750"
  },
  {
    "text": "operations on that data I need to make sure it's authorized to do that I choose run on demand and I just choose",
    "start": "1687750",
    "end": "1695340"
  },
  {
    "text": "that the the database I want to drop the actual discovered schema into and at",
    "start": "1695340",
    "end": "1700710"
  },
  {
    "text": "that point I could actually run the job now I'm not going to do that because it takes it takes about a minute or so to",
    "start": "1700710",
    "end": "1705990"
  },
  {
    "text": "UM for a quality that to actually execute but I'll show you what it would produce it as an output so I go into the",
    "start": "1705990",
    "end": "1714570"
  },
  {
    "text": "UM into the tables section and I'm filtering this list of tables by the the",
    "start": "1714570",
    "end": "1719639"
  },
  {
    "text": "database name which is sub it's um an 18 and this sum the century down here called yellow is the name of the you'll",
    "start": "1719639",
    "end": "1727019"
  },
  {
    "text": "see the location there matches out our discovered the folder we're pointing out this page here shows what the discovered",
    "start": "1727019",
    "end": "1733590"
  },
  {
    "text": "schemer is so it's discovered all these attributes of our arm of our data and you'll see down the bottom here there's",
    "start": "1733590",
    "end": "1739649"
  },
  {
    "text": "there's all the usual data types like anything string but there's a structure here called fare info and if I click on",
    "start": "1739649",
    "end": "1744929"
  },
  {
    "text": "the struct tab there you'll see that the the fare schema details have actually popped up so it's actually drilled down",
    "start": "1744929",
    "end": "1751260"
  },
  {
    "text": "into the sub structures as well so now I've got an entry now I've got a now but I've discovered that data there",
    "start": "1751260",
    "end": "1759360"
  },
  {
    "text": "that's just one of my providers that's the yellow cabs no the next thing I need to do is think about the other two",
    "start": "1759360",
    "end": "1764549"
  },
  {
    "text": "providers uber and green now haven't got time to actually build a crawler for those but green cabs for example is um",
    "start": "1764549",
    "end": "1770490"
  },
  {
    "text": "in this example is actually a CSV data set rube is actually in a database in the reward database so what I'm going to",
    "start": "1770490",
    "end": "1777659"
  },
  {
    "text": "do is actually try and build a an ETL job that's going to pull together all those datasets into one single data",
    "start": "1777659",
    "end": "1784860"
  },
  {
    "text": "unified that table like an endpoint my query yet so I",
    "start": "1784860",
    "end": "1790590"
  },
  {
    "text": "think I'm just about our time here so let me just show you what the UM what",
    "start": "1790590",
    "end": "1797010"
  },
  {
    "text": "the actual target table looks like I'm going to go into Athena and when I as an",
    "start": "1797010",
    "end": "1803520"
  },
  {
    "text": "ETL designer what I would do is actually create the the target table format in Athena and I've called the ground trips",
    "start": "1803520",
    "end": "1810630"
  },
  {
    "text": "and if you just look at there that's",
    "start": "1810630",
    "end": "1815640"
  },
  {
    "text": "actual was schema on the left-hand side here that I've designed as an ETL designer",
    "start": "1815640",
    "end": "1820710"
  },
  {
    "text": "now the having designed that too and what I've done here is I put the provider as an attribute at that actual",
    "start": "1820710",
    "end": "1826380"
  },
  {
    "text": "um ground trips table so that allows me to unify all my sources into a single single source yeah excuse me",
    "start": "1826380",
    "end": "1834330"
  },
  {
    "start": "1832000",
    "end": "2054000"
  },
  {
    "text": "the other thing that's done it's excuse",
    "start": "1834330",
    "end": "1839790"
  },
  {
    "text": "me standardize the date/time attribute which is because it's a time series it's really important though I can do that so",
    "start": "1839790",
    "end": "1846180"
  },
  {
    "text": "I'll just show you one other one other aspect of glue let's go back to to glue",
    "start": "1846180",
    "end": "1855360"
  },
  {
    "text": "and then to write it to write the actual ETL job it really is as simple as going",
    "start": "1855360",
    "end": "1860700"
  },
  {
    "text": "into going to jobs and saying add job",
    "start": "1860700",
    "end": "1866660"
  },
  {
    "text": "give it a name",
    "start": "1866660",
    "end": "1870140"
  },
  {
    "text": "choosing the role so it's going to be writing into an s3 bucket as well as reading one so that that role needs to",
    "start": "1872640",
    "end": "1878490"
  },
  {
    "text": "be had permission to do that I'm going to ask it to propose a script for me and so it's going to generate the code for",
    "start": "1878490",
    "end": "1884610"
  },
  {
    "text": "me go on to the next tab and I'm going",
    "start": "1884610",
    "end": "1892500"
  },
  {
    "text": "to get it to choose a data source",
    "start": "1892500",
    "end": "1895970"
  },
  {
    "text": "that's quite a bit crazy",
    "start": "1897590",
    "end": "1901190"
  },
  {
    "text": "here we go",
    "start": "1908780",
    "end": "1911620"
  },
  {
    "text": "and I've gotta tell to look at the yellow data set and then the data target",
    "start": "1924780",
    "end": "1931590"
  },
  {
    "text": "I'm going to tell tilt to use the ground trips schema that I've defined an Athena there's my target and then it pops up",
    "start": "1931590",
    "end": "1940710"
  },
  {
    "text": "this the Sun this mapping page that I showed you earlier and this gives you the opportunity to take the source",
    "start": "1940710",
    "end": "1945750"
  },
  {
    "text": "columns and actually met them to rename them and change the dad types in the target columns and actually flatten out",
    "start": "1945750",
    "end": "1951780"
  },
  {
    "text": "structures so you've got a fare and info structure I can pull out sub attributes of that and put that into the target",
    "start": "1951780",
    "end": "1957990"
  },
  {
    "text": "schema so at that point it's going to propose a cut a set of code for me and",
    "start": "1957990",
    "end": "1964560"
  },
  {
    "text": "that's the actual ETL job itself so it's it's so quite readable Pais Barkin on",
    "start": "1964560",
    "end": "1970320"
  },
  {
    "text": "the left-hand side it will generate a diagram showing me what all the transforms involved in the in running",
    "start": "1970320",
    "end": "1975990"
  },
  {
    "text": "the job are now I'm not going to run the job now it takes about takes about eight minutes 25 million records it's reasonably quick",
    "start": "1975990",
    "end": "1983070"
  },
  {
    "text": "but the end result is if I just jump back to Athena is we can have a look at",
    "start": "1983070",
    "end": "1988410"
  },
  {
    "text": "some of the some of the modified the actual ETL data I've got this this",
    "start": "1988410",
    "end": "1994170"
  },
  {
    "text": "ground troops yellow view defined in Athena and it's running the query now so",
    "start": "1994170",
    "end": "2000980"
  },
  {
    "text": "that's actually querying the querying the detailed data and you can see it's",
    "start": "2000980",
    "end": "2006500"
  },
  {
    "text": "actually provided they move the yellow data the tagged in the yellow provider mapped the date times and as converted",
    "start": "2006500",
    "end": "2013520"
  },
  {
    "text": "over the trip distance and fairs now once that data is in that format it opens up a whole lot of possibilities",
    "start": "2013520",
    "end": "2018980"
  },
  {
    "text": "with the by running in terms of views I can build on that data so this is regular old SQL expressions at this",
    "start": "2018980",
    "end": "2025070"
  },
  {
    "text": "point and if I run this query here you can see I've got a view here that shows",
    "start": "2025070",
    "end": "2030380"
  },
  {
    "text": "some really interesting statistics about these providers for the yellow cab company the average distance of a trip",
    "start": "2030380",
    "end": "2036230"
  },
  {
    "text": "the average cost per mile the average fare and the 95th percentile fare so",
    "start": "2036230",
    "end": "2041900"
  },
  {
    "text": "again some idea about what's the maximum it would be likely to pay so it's a very powerful engine that's actually running",
    "start": "2041900",
    "end": "2047180"
  },
  {
    "text": "interactively there's no optimizations he interactively against millions of millions of Records sitting sitting just",
    "start": "2047180",
    "end": "2053720"
  },
  {
    "text": "in this three so just to wrap up what what I'm what we've done with this demonstration showing how with glue you can take you",
    "start": "2053720",
    "end": "2060679"
  },
  {
    "start": "2054000",
    "end": "2092000"
  },
  {
    "text": "can discover data sitting in the variety of sources s3 buckets CSV Jason Aurora",
    "start": "2060680",
    "end": "2066290"
  },
  {
    "text": "databases produce a target schema matching what your analysts want to see and divide an ETL job to map from those",
    "start": "2066290",
    "end": "2073250"
  },
  {
    "text": "source those source data sets into a unified data set which can then run traditional SQL queries and your choice",
    "start": "2073250",
    "end": "2079879"
  },
  {
    "text": "of visualization tool advanced so thanks very much if you could just fill in your",
    "start": "2079880",
    "end": "2084909"
  },
  {
    "text": "registry it's re your um feedback cards on the way out and just swipe on the on",
    "start": "2084910",
    "end": "2090440"
  },
  {
    "text": "the exit thanks very much everyone",
    "start": "2090440",
    "end": "2094359"
  }
]