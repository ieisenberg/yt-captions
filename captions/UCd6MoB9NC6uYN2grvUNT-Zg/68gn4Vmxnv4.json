[
  {
    "start": "0",
    "end": "100000"
  },
  {
    "text": "welcome everybody uh last session of the day we hope you're going to we're not going to hold you too long before your",
    "start": "320",
    "end": "5920"
  },
  {
    "text": "um beers so going getting started quickly um I'm Rahul FAA and I'm going",
    "start": "5920",
    "end": "11920"
  },
  {
    "text": "to speak uh on Amazon EMR do a deep dive on some of the Amazon elastic map ruce uh features and services um benefits and",
    "start": "11920",
    "end": "19640"
  },
  {
    "text": "with me I have Andrew yep my name is Andrew Jorgenson um from Twitter I'm going to talk to you guys how we use EMR",
    "start": "19640",
    "end": "25480"
  },
  {
    "text": "uh to scale to handle five billion sessions per day uh with on answers thank you Andrew um this is the agenda",
    "start": "25480",
    "end": "33200"
  },
  {
    "text": "we're going to cover in short just do a quick introduction on EMR and some of the features around it then we're going",
    "start": "33200",
    "end": "38440"
  },
  {
    "text": "to talk about how you can leverage Amazon S3 which is an object store with EMR we're going to talk about some of",
    "start": "38440",
    "end": "43920"
  },
  {
    "text": "the design patterns with EMR on ews like how customers are using EMR in which situations are they using it and in the",
    "start": "43920",
    "end": "50360"
  },
  {
    "text": "last we're going to talk about EMR optimizations you can do around storing your data with EMR what are the various",
    "start": "50360",
    "end": "57039"
  },
  {
    "text": "file formats file choices or even compression and after that we'll have",
    "start": "57039",
    "end": "62239"
  },
  {
    "text": "Andre talk about answers and how they're using some of the architectural principles to handle 5 billion shens per",
    "start": "62239",
    "end": "68360"
  },
  {
    "text": "day and in the last leave with you some takeaways which you can use from this session so let's get",
    "start": "68360",
    "end": "74240"
  },
  {
    "text": "started uh as you might know Amazon EMR is a managed Hadoop uh framework",
    "start": "74240",
    "end": "80079"
  },
  {
    "text": "available for you guys to use in the AWS Cloud now Hadoop is a great framework to process your data but managing Hadoop",
    "start": "80079",
    "end": "86920"
  },
  {
    "text": "itself is not that easy Hadoop itself has its own set of complexity no set of problems which you have to handle um in",
    "start": "86920",
    "end": "94000"
  },
  {
    "text": "case you want to focus on your data set you would rather focus on your data set processing not on managing your cluster",
    "start": "94000",
    "end": "99280"
  },
  {
    "text": "in that aspect there are six benefits of EMR we're going to talk about or look into that as we go through this uh",
    "start": "99280",
    "end": "105159"
  },
  {
    "start": "100000",
    "end": "100000"
  },
  {
    "text": "presentations further first with Amazon EMR you could launch a cluster by click",
    "start": "105159",
    "end": "110439"
  },
  {
    "text": "a few buttons within few minutes you can get started with a manage hero cluster fully managed hero cluster on AWS second",
    "start": "110439",
    "end": "118000"
  },
  {
    "text": "you don't have to invest in figuring out how how much infrastructure you need and you have to pay the investment in upfront you could start with the cluster",
    "start": "118000",
    "end": "124759"
  },
  {
    "text": "do your work and only pay for the hours or only pay the time you use for the infrastructure with your AWS or with",
    "start": "124759",
    "end": "130640"
  },
  {
    "text": "your Amazon EMR cluster second it does allow you to by using a single AP",
    "start": "130640",
    "end": "136000"
  },
  {
    "text": "operation you can actually add compute capacity to your EMR cluster so in case you are running short on the cluster or",
    "start": "136000",
    "end": "141680"
  },
  {
    "text": "if your job is taking longer you can use that AP operation to add more notes to your cluster to give you additional",
    "start": "141680",
    "end": "147360"
  },
  {
    "text": "compute capacity not only that you can use something called spot instances we'll talk about that also further on",
    "start": "147360",
    "end": "153160"
  },
  {
    "text": "how you can use that to lower your cost effectively for processing with Amazon EMR finally with EMR you're not going to",
    "start": "153160",
    "end": "160640"
  },
  {
    "text": "spend much time trying to debug monitor what happening with your job you're rather going to focus on the processing of a job and we'll talk about how using",
    "start": "160640",
    "end": "166879"
  },
  {
    "text": "some um um benefits such as Cloud watch such as ganglia and Etc we do allow you to Pro for you to monitor your cluster",
    "start": "166879",
    "end": "174000"
  },
  {
    "text": "or monitor your jobs it does provide you ability also to limit on what can be accessed to your cluster by using",
    "start": "174000",
    "end": "180200"
  },
  {
    "text": "something called security groups so that your cluster is secure it runs within a VPC which is basically virtual private",
    "start": "180200",
    "end": "186480"
  },
  {
    "text": "Cloud which is isol isolated section of AWS Cloud for you to use securely and",
    "start": "186480",
    "end": "192040"
  },
  {
    "text": "finally you as end user control every single aspect of the cluster we have something called bootstrap actions for",
    "start": "192040",
    "end": "198000"
  },
  {
    "text": "instance in which case you are not finding the framework available with Amazon EMR you can use the bootstrap",
    "start": "198000",
    "end": "203319"
  },
  {
    "text": "actions to do additional work or to install additional Frameworks available within Hadoop such as Presto and Spark",
    "start": "203319",
    "end": "209400"
  },
  {
    "text": "and we touch upon that briefly also so going started Amazon EMR we said is easy to",
    "start": "209400",
    "end": "216159"
  },
  {
    "start": "214000",
    "end": "214000"
  },
  {
    "text": "launch well you can launch with Amazon EMR cluster in three different ways first you can go to the AWS GUI and you",
    "start": "216159",
    "end": "222439"
  },
  {
    "text": "can launch the cluster by using few clicks there or you can use the command line AWS CLI also to launch the Clusters",
    "start": "222439",
    "end": "229400"
  },
  {
    "text": "um recently Le there was a feature where you can pre-configure the kind of instances the kind of clusters you want to launch and you can launch the",
    "start": "229400",
    "end": "235720"
  },
  {
    "text": "Clusters using that pre-configured values within a single AWS CLI command rather than typing the entire command",
    "start": "235720",
    "end": "241640"
  },
  {
    "text": "here finally we'll also hear from Andrew how actually they're using um EMR apis",
    "start": "241640",
    "end": "247519"
  },
  {
    "text": "which AWS SDK to launch that clusters on demand so what you could do is you could as a part of your job flow as a part of",
    "start": "247519",
    "end": "253720"
  },
  {
    "text": "your workflow you could actually in you could start a cluster within a part of your job flow do the processing work and terminate the cluster all using within",
    "start": "253720",
    "end": "260160"
  },
  {
    "text": "the single apis without ever a human touching any processing stream in there not only that when you launch EMR",
    "start": "260160",
    "end": "268240"
  },
  {
    "start": "266000",
    "end": "266000"
  },
  {
    "text": "clusters you could choose to do uh have all the logs available to you for xsy S3",
    "start": "268240",
    "end": "273919"
  },
  {
    "text": "which is displayed on the top right here that all your Hardo job logs cluster locks and process locks are available",
    "start": "273919",
    "end": "280039"
  },
  {
    "text": "for you to look at from the GUI itself you can use the AWS uh EMR console GUI",
    "start": "280039",
    "end": "286120"
  },
  {
    "text": "to even browse the logs from S3 directly in the console itself without you having to do SSH into the cluster to look at",
    "start": "286120",
    "end": "292000"
  },
  {
    "text": "the jobs to figure out what is going wrong with your job what is happening in there we also integrate with Amazon",
    "start": "292000",
    "end": "297240"
  },
  {
    "text": "Cloud watch and we provide you metrics on what's the cluster utilization what each node is doing or even if the",
    "start": "297240",
    "end": "303759"
  },
  {
    "text": "cluster is Idle or not at that given point in time so imagine a workflow where you can say that if my cluster has",
    "start": "303759",
    "end": "309000"
  },
  {
    "text": "been idle which is at a 5 minute interval and it has been idle for three sequ subsequent checks I'm going to",
    "start": "309000",
    "end": "315400"
  },
  {
    "text": "terminate my cluster or if my cluster is doing additional if my hdfs storage is",
    "start": "315400",
    "end": "320759"
  },
  {
    "text": "at 80% level I'm going to resize the cluster to add more sdfs storage and rebalance my cluster so now I have",
    "start": "320759",
    "end": "326720"
  },
  {
    "text": "greater cluster capacity for my work to be continued without interruption and all of this could be done using apis",
    "start": "326720",
    "end": "332560"
  },
  {
    "text": "using cloudwatch or using other pattern available as you mentioned that EMR provides you different uh metrics",
    "start": "332560",
    "end": "339440"
  },
  {
    "text": "depending on which version of Haro you're using to integrate within cloudwatch and you can use those with SNS even to send alarm or even to",
    "start": "339440",
    "end": "346000"
  },
  {
    "text": "generate some um actions based on those cloudwatch uh",
    "start": "346000",
    "end": "351160"
  },
  {
    "text": "notifications hu which is another um framework within um Hadoop Community",
    "start": "352720",
    "end": "358360"
  },
  {
    "text": "it's stands for Hadoop user experience what it allows you is to do it look at your cluster from a u web GOI",
    "start": "358360",
    "end": "365960"
  },
  {
    "text": "perspective there are few aspects of what Hue which EMR makes it available when you create a EMR cluster you can",
    "start": "365960",
    "end": "372240"
  },
  {
    "text": "actually choose to install Hue as a framework and the moment you install hu as a framework or as additional",
    "start": "372240",
    "end": "377440"
  },
  {
    "text": "component on your cluster it gives you ability such as to browse your uh file system and the file system could be your",
    "start": "377440",
    "end": "384120"
  },
  {
    "text": "hdfs uh which stands for Hado distributed file system or you could even browse your data sets of file",
    "start": "384120",
    "end": "389840"
  },
  {
    "text": "object sitting in Amazon S3 which is Object Store using the same Hue interface from here as as displayed here",
    "start": "389840",
    "end": "397160"
  },
  {
    "text": "if you look at the interface it shows you that how somebody is looking at a data set sitting in S3 not in",
    "start": "397160",
    "end": "402280"
  },
  {
    "text": "hgfs second what it also allows you to do is run your queries such as hi Pig",
    "start": "402280",
    "end": "408720"
  },
  {
    "text": "you can run those queries hi queries or Pig uh commands all from the Hue interface without ever touching your",
    "start": "408720",
    "end": "414280"
  },
  {
    "text": "cluster and you can just run the submit the jobs to the cluster and it will display the results back to you and",
    "start": "414280",
    "end": "420120"
  },
  {
    "text": "finally it also has a metadata manager um if that might be available on the other side if you know about hi meta",
    "start": "420120",
    "end": "426080"
  },
  {
    "text": "store which is basically uh way to store all your schemas on on top of your data you can actually browse the hi meta",
    "start": "426080",
    "end": "432160"
  },
  {
    "text": "store also from here using the meta store manager link not only that you can",
    "start": "432160",
    "end": "437280"
  },
  {
    "text": "actually access your job logs even job statuses similar to what the SD logs provided earlier but here you can",
    "start": "437280",
    "end": "443919"
  },
  {
    "text": "actually look at all the jobs you ran on your Yan um on using Yan and you can browse them and see the status of each",
    "start": "443919",
    "end": "450240"
  },
  {
    "text": "job how much complete they are and access their locks also as the locks are being written to the",
    "start": "450240",
    "end": "455400"
  },
  {
    "text": "cluster and with h you can actually go to one step further where you don't have to basically log into the cluster to do",
    "start": "455400",
    "end": "461360"
  },
  {
    "text": "any work but you can just use the uh UI web go is provided with EMR and with you",
    "start": "461360",
    "end": "466639"
  },
  {
    "text": "to basically do entire work work orchestration on the AMR cluster finally one more thing you don't",
    "start": "466639",
    "end": "474639"
  },
  {
    "start": "472000",
    "end": "472000"
  },
  {
    "text": "have to do with emis you don't have to worry about what instance types you have to choose or what instance what infrastructure you have to invest in to",
    "start": "474639",
    "end": "480080"
  },
  {
    "text": "build your hoop cluster there are multiple instance families available for you to do your work four of which EMR",
    "start": "480080",
    "end": "486240"
  },
  {
    "text": "supports um in general notion or general categorization is General uh M1 M1 and",
    "start": "486240",
    "end": "493280"
  },
  {
    "text": "M3 family which are General um mediums I think medium optimized family CPU optimized which are CC uh CC1 C2 and C3",
    "start": "493280",
    "end": "501440"
  },
  {
    "text": "families memory optimized which are M3 and R3 and storage optimize which are",
    "start": "501440",
    "end": "506879"
  },
  {
    "text": "d2s and R2 if you might have own or if you might uh d2s were launched a week",
    "start": "506879",
    "end": "513159"
  },
  {
    "text": "back from today and d2s support very high sequential iio for workloads running workloads like Hadoop so if your",
    "start": "513159",
    "end": "520159"
  },
  {
    "text": "workloads are aligned to something like in case you're just doing General batch retail processing you can start with",
    "start": "520159",
    "end": "526120"
  },
  {
    "text": "something like M1 and M3 in case you're doing some processes like machine learning or you're doing some very high iterative workloads of computer",
    "start": "526120",
    "end": "532399"
  },
  {
    "text": "optimized CPU optimize workloads you can use the uh CPU instance families you're",
    "start": "532399",
    "end": "537480"
  },
  {
    "text": "running workloads like Presto Impala spark you can use memory optimized families to give you that additional",
    "start": "537480",
    "end": "544079"
  },
  {
    "text": "responsiveness when you're running those queries on the cluster and finally if you're running something like HB which needs dense storage which needs High",
    "start": "544079",
    "end": "550680"
  },
  {
    "text": "storage or workloads where you have to use hdfs to keep writing reading data from there you can use like d2s which",
    "start": "550680",
    "end": "556800"
  },
  {
    "text": "provides you magnetic 48 tbte of storage in mag magnetic disc for instance or i2s",
    "start": "556800",
    "end": "563279"
  },
  {
    "text": "which provide you 6.4 terabyt of SSD storage and each of these configurations",
    "start": "563279",
    "end": "568680"
  },
  {
    "text": "you can create a CL cluster and if that does not meet your need you can actually destroy the cluster and you can recreate a new cluster on a different instance",
    "start": "568680",
    "end": "574800"
  },
  {
    "text": "family and rerun your workload so that's tuned towards that workload and all of that which will not be possible if",
    "start": "574800",
    "end": "580240"
  },
  {
    "text": "you're building a single Hadoop cluster to do each of these type of jobs you'll have to invest a hard time trying to figure out optimize every single aspect",
    "start": "580240",
    "end": "587240"
  },
  {
    "text": "of the cluster to keep up with different workloads in there uh we mentioned about",
    "start": "587240",
    "end": "593000"
  },
  {
    "text": "the uh resizable cluster that once you figure out what the type of instance families you're working on and once you",
    "start": "593000",
    "end": "598920"
  },
  {
    "text": "get started Ed with that workload let's say you start with 10 instances and you realize that 10 instances are not enough for your work to be done you could call",
    "start": "598920",
    "end": "606279"
  },
  {
    "text": "a single AP operation to resize a cluster to any arbitrary amount of um notes you can uh you want to have in",
    "start": "606279",
    "end": "611959"
  },
  {
    "text": "your cluster you could there are three different task groups within an EMR cluster uh first is called a master",
    "start": "611959",
    "end": "617440"
  },
  {
    "text": "Group which runs the master services within Hadoop for things like name node resource manager and Etc there's",
    "start": "617440",
    "end": "623560"
  },
  {
    "text": "something called core nodes these are the nodes which runs uh data if you think of data no there were hdfs data",
    "start": "623560",
    "end": "629519"
  },
  {
    "text": "notes lies and then something called task noes or task groups where only the task noes run these are like the Yan um",
    "start": "629519",
    "end": "636639"
  },
  {
    "text": "you know task notes a node manager per se so you could call that AP operation",
    "start": "636639",
    "end": "642839"
  },
  {
    "text": "to rechange your cluster size on each of these groups ex the master node has to have one today but you can have arbitary",
    "start": "642839",
    "end": "649120"
  },
  {
    "text": "number of task groups or um task groups and you can have any number of instances",
    "start": "649120",
    "end": "654480"
  },
  {
    "text": "within those task groups itself the way it lends in economical way to process your job with an EMR is that let's say",
    "start": "654480",
    "end": "661160"
  },
  {
    "start": "658000",
    "end": "658000"
  },
  {
    "text": "that you started with a job and you know that the job running on um eight clusters for example eight nodes can",
    "start": "661160",
    "end": "666839"
  },
  {
    "text": "meet your workload within 4 hours so you're running a you're running a workload which can do the work which can",
    "start": "666839",
    "end": "672040"
  },
  {
    "text": "do the processing within 4 hours on eight nodes using something called spot",
    "start": "672040",
    "end": "678079"
  },
  {
    "text": "instances which is basically Amazon ec2 offers you instances at a lower market",
    "start": "678079",
    "end": "683639"
  },
  {
    "text": "value when they're not being used so you can bid on them and once you bid on them the bid prices could be up to 86 %",
    "start": "683639",
    "end": "689839"
  },
  {
    "text": "generally lower on recent instances on average they're lower than the on demand pricing so now what you could do is you",
    "start": "689839",
    "end": "696079"
  },
  {
    "text": "could add for example create another task group and say that I'm going to bid at half the price of what are my on",
    "start": "696079",
    "end": "702000"
  },
  {
    "text": "demand pricing is and now your job is going to get completed first because Hadoop scale is linearly so instead of 4",
    "start": "702000",
    "end": "707880"
  },
  {
    "text": "hours it's going to get finished in 2 hours it's going to lead you to up to 25 or 30% of savings just by using this uh",
    "start": "707880",
    "end": "715560"
  },
  {
    "text": "just by using this pattern with Amazon EMR the one idea behind that is that you never want to use spot instances with",
    "start": "715560",
    "end": "721760"
  },
  {
    "text": "your core group because Core Group do store some data in there and if your tasks uh if the core groups uh if the",
    "start": "721760",
    "end": "728639"
  },
  {
    "text": "spot instances get taken away then you might end up losing data in that uh in that instance the best practice around",
    "start": "728639",
    "end": "734680"
  },
  {
    "text": "that would be that you can create as many task groups as you want and for each of the task groups you can uh quote",
    "start": "734680",
    "end": "740079"
  },
  {
    "text": "your own pricing so whenever the pricing becomes available Beyond The Spot instance current pricing they will get",
    "start": "740079",
    "end": "745720"
  },
  {
    "text": "added to your cluster and be available for you to run your job a process a job within that infrastructure within that",
    "start": "745720",
    "end": "751240"
  },
  {
    "text": "cluster sorry and um Andrew also is going to talk about how they actually use BT",
    "start": "751240",
    "end": "756800"
  },
  {
    "text": "instances today later and to complete their work in in the processing of their handling five billion sessions per",
    "start": "756800",
    "end": "765000"
  },
  {
    "start": "764000",
    "end": "764000"
  },
  {
    "text": "day the one more thing we mention about that you are in the complete control of the cluster so when you start Amazon EMA",
    "start": "765199",
    "end": "771000"
  },
  {
    "text": "cluster there are few Frameworks we which we offer out of the box a Frameworks being Hive Pig uh ganglia",
    "start": "771000",
    "end": "778120"
  },
  {
    "text": "edas uh hunk which is a product by Splunk in case any of these products there are other products which you want",
    "start": "778120",
    "end": "783880"
  },
  {
    "text": "to or other hadu Frameworks which you want to run on your cluster there's something called bootstrap actions which is hosted repository on GitHub and you",
    "start": "783880",
    "end": "790800"
  },
  {
    "text": "can choose any of these bootstrap actions to run any of these Frameworks as listed here on your cluster these",
    "start": "790800",
    "end": "796279"
  },
  {
    "text": "Frameworks could be something as recent as spark Presto acumulo or even drill",
    "start": "796279",
    "end": "802760"
  },
  {
    "text": "and you can get started with them when you run when you use the bootstop actions to run these Frameworks on your cluster as in when you resize cluster",
    "start": "802760",
    "end": "810040"
  },
  {
    "text": "these bootstrap actions also do run on the uh new notes itself so they become part of your new framework and you can",
    "start": "810040",
    "end": "815399"
  },
  {
    "text": "easily start using those new capacity on with these Frameworks directly in there you can even create your own bootstrap",
    "start": "815399",
    "end": "821600"
  },
  {
    "text": "actions think of it something like um a script you're going to give it to a cluster say that whenever my cluster is",
    "start": "821600",
    "end": "827519"
  },
  {
    "text": "created run the script and EMR will take care of basically running this script on every single node within your",
    "start": "827519",
    "end": "834639"
  },
  {
    "text": "cluster not only that you can also use the bootstrap actions today to configure the cluster values for example you want",
    "start": "834680",
    "end": "840399"
  },
  {
    "text": "to allocate more memory to Yan less memory to Yan you want to change the number of slots or change the number of",
    "start": "840399",
    "end": "846240"
  },
  {
    "text": "containers Yan should allocate for you want to change the application factor of hdfs all of these values which can be",
    "start": "846240",
    "end": "852279"
  },
  {
    "text": "controlled by any of these configuration file within Hadoop you can change the parameter values of these when you",
    "start": "852279",
    "end": "857839"
  },
  {
    "text": "create the cluster by simply providing the name of the files you want to uh name of the config file and the name of",
    "start": "857839",
    "end": "862959"
  },
  {
    "text": "the parameter you want to change to so when the cluster gets operated or created the file will automatically be",
    "start": "862959",
    "end": "868720"
  },
  {
    "text": "EV wrden based on the command you provided so for instance um example being that if I'm not using hdfs with",
    "start": "868720",
    "end": "874560"
  },
  {
    "text": "the EMR cluster to do any storage I might as well lower the application factor of hdfs from uh 3 to one by",
    "start": "874560",
    "end": "881480"
  },
  {
    "text": "default which could save you some time which could save you some processing hours if you're not using hdfs just just",
    "start": "881480",
    "end": "886600"
  },
  {
    "text": "example but there could be multiple ways other ways you can optimize",
    "start": "886600",
    "end": "891639"
  },
  {
    "text": "it um one more integration we want to talk about I mean Kinesis Amazon Kinesis",
    "start": "891880",
    "end": "897000"
  },
  {
    "start": "892000",
    "end": "892000"
  },
  {
    "text": "which is a realtime in uh service EMR also allows you to read data",
    "start": "897000",
    "end": "904199"
  },
  {
    "text": "from Kinesis so you can use family framework such as Hive Pig streaming map",
    "start": "904199",
    "end": "909880"
  },
  {
    "text": "reduce Etc to read data from Kinesis imagine that you're putting data into scale at Kinesis in real time and with",
    "start": "909880",
    "end": "917040"
  },
  {
    "text": "hiy you can actually run a a SQL query on data which is streaming and that is quite powerful like just being able to",
    "start": "917040",
    "end": "923320"
  },
  {
    "text": "use the same patterns what you have to run exactly apply that on the streaming data and you don't have to do anything",
    "start": "923320",
    "end": "928759"
  },
  {
    "text": "for that all you have to do is when you create a EMR cluster it already has the input output format or storage Handler",
    "start": "928759",
    "end": "934920"
  },
  {
    "text": "for Kinesis you just go create a table in Hive and say that the storage format I'm using is basically a Kinesis storage",
    "start": "934920",
    "end": "941639"
  },
  {
    "text": "format and it will ofly do the work for you doing the work for checkpointing doing the work for Recovery or even",
    "start": "941639",
    "end": "946759"
  },
  {
    "text": "moving around with that data so those were mostly around some of",
    "start": "946759",
    "end": "952279"
  },
  {
    "text": "the EMR Amazon EMR features and now we're going to talk about Amazon EMR and how does it integrates with S3 and how",
    "start": "952279",
    "end": "958680"
  },
  {
    "text": "you can use that LE and how you can leverage that integration to run your jobs effectively on",
    "start": "958680",
    "end": "964839"
  },
  {
    "text": "AWS so Amazon S3 is a object store um there are trillions of object stores in",
    "start": "964839",
    "end": "971079"
  },
  {
    "start": "966000",
    "end": "966000"
  },
  {
    "text": "Amazon S3 and Amazon S3 is designed by default for 119 of durability so you",
    "start": "971079",
    "end": "976759"
  },
  {
    "text": "don't have to worry about replicating your data when you put data in s Amazon S3 because the data is by default it's",
    "start": "976759",
    "end": "982639"
  },
  {
    "text": "designed to give you that kind of durability by default and second the one of the main design pattern which Amazon",
    "start": "982639",
    "end": "989199"
  },
  {
    "text": "apply or S3 provides you is that now you can decouple your storage and compute",
    "start": "989199",
    "end": "994639"
  },
  {
    "text": "think of it in a model like hdfs where you have one hdfs and you have a Hado cluster running on top of hdfs now if",
    "start": "994639",
    "end": "1002000"
  },
  {
    "text": "you're stuck due to hdfs for example let's say you're at 80% of Your Capacity but yet you're using only 50% of the",
    "start": "1002000",
    "end": "1008519"
  },
  {
    "text": "compute capacity of the cluster you still you are still going to scale your uh cluster beyond that 80% and you're",
    "start": "1008519",
    "end": "1015480"
  },
  {
    "text": "going to waste the capacity you're not using but with principal what with the object storage like S3 you can create",
    "start": "1015480",
    "end": "1021880"
  },
  {
    "text": "clusters of size which can match your job exactly and when the job is done you can terminate that cluster only pay for",
    "start": "1021880",
    "end": "1028038"
  },
  {
    "text": "the time you use to compute and not pay for any storage cost with that S3 allows",
    "start": "1028039",
    "end": "1033640"
  },
  {
    "text": "you to create multiple clusters which can point to the same S3 data set so you can use that data set to process whether",
    "start": "1033640",
    "end": "1039520"
  },
  {
    "text": "be it running a map reduce be running a hi query be running a big query and all of that could be run on separate cluster",
    "start": "1039520",
    "end": "1045600"
  },
  {
    "text": "but yet pointing to the same data store not only that give S3 is a persist in storage it also allows you to terminate",
    "start": "1045600",
    "end": "1051919"
  },
  {
    "text": "your cluster when your job is done so if you're running a job only for one hour a day you can create EMR cluster run the",
    "start": "1051919",
    "end": "1058200"
  },
  {
    "text": "job for an hour terminate the cluster your data Still Remains on S3 and next day you can again recreate the cluster",
    "start": "1058200",
    "end": "1063400"
  },
  {
    "text": "process the job still not worry about the data in there so with Amazon Emi and S3",
    "start": "1063400",
    "end": "1068840"
  },
  {
    "text": "integration you can resize or you can even shut down a cluster without any data loss which is typically difficult",
    "start": "1068840",
    "end": "1074440"
  },
  {
    "text": "to do in a um in um hadu cluster as such without S3 as a storage and second you",
    "start": "1074440",
    "end": "1080880"
  },
  {
    "text": "can point multiple clusters to to do different kind of processing on the same data set sitting in",
    "start": "1080880",
    "end": "1087720"
  },
  {
    "text": "S3 um not only that we provide easier integration for you to use with EMR",
    "start": "1087720",
    "end": "1093960"
  },
  {
    "start": "1088000",
    "end": "1088000"
  },
  {
    "text": "cluster so when you create EMR clusters we have Amazon EMR file system which integrates natively with S3 you as a",
    "start": "1093960",
    "end": "1100919"
  },
  {
    "text": "user don't have to do anything specifically to B utilize that uh integration you just have to choose when",
    "start": "1100919",
    "end": "1107280"
  },
  {
    "text": "you create the EMR cluster to say that I want to utilize emrfs with my um with my",
    "start": "1107280",
    "end": "1112480"
  },
  {
    "text": "EMR cluster the some of the benefits is provide is first it can give you the",
    "start": "1112480",
    "end": "1117799"
  },
  {
    "text": "consistency if you know about S3 sorry today um S3 does not provide you read",
    "start": "1117799",
    "end": "1123159"
  },
  {
    "text": "after write consistency in Us East one but with emrfs you don't have to worry about which cons which regon is",
    "start": "1123159",
    "end": "1128880"
  },
  {
    "text": "consistent or not it will take care of the consistency it also gives you faster",
    "start": "1128880",
    "end": "1134360"
  },
  {
    "text": "listing uh the way emrfs works is basically it creates a Dynamo DB table and think of that dyn table DV table as",
    "start": "1134360",
    "end": "1140880"
  },
  {
    "text": "index of your data set so each and every operation rather than going directly against us S3 goes to the Dynamo DB",
    "start": "1140880",
    "end": "1146880"
  },
  {
    "text": "table first so which means that you're going to see instead of doing a listing on all the SD objects if I can do all",
    "start": "1146880",
    "end": "1152240"
  },
  {
    "text": "the listing of Dynamo DB objects which is going to be much faster operation in principle you're going to see lower",
    "start": "1152240",
    "end": "1157720"
  },
  {
    "text": "latencies in your uh job start times and Amazon emrfs also allows you",
    "start": "1157720",
    "end": "1164559"
  },
  {
    "text": "take advantage of S3 server side and client encryption sub side encryption",
    "start": "1164559",
    "end": "1169919"
  },
  {
    "text": "being S3 is doing the work for you encrypting your data set client side encryption being that you as a user are",
    "start": "1169919",
    "end": "1175559"
  },
  {
    "text": "providing the keys and emrfs can do the encryption for you before putting the data into S3 um depending on workload if",
    "start": "1175559",
    "end": "1182039"
  },
  {
    "text": "you're running a HIPPA kind of workload um or of that nature where the data is sensitive in nature you could choose",
    "start": "1182039",
    "end": "1187600"
  },
  {
    "text": "depend you know depending on requirements either of them and emfs will support it the way CLI side",
    "start": "1187600",
    "end": "1193440"
  },
  {
    "text": "encryption works is on the other side when you're writing data to S3 you will create a master key that master key",
    "start": "1193440",
    "end": "1199600"
  },
  {
    "text": "could be stored in something like Amazon KMS or your own key provider and when you create when you do the encrypt the",
    "start": "1199600",
    "end": "1204880"
  },
  {
    "text": "object it's going to create a key for that object write the key as a part of the object header and both of them get",
    "start": "1204880",
    "end": "1211000"
  },
  {
    "text": "stored in S3 when you create emrfs you tell it that okay my um use the client",
    "start": "1211000",
    "end": "1216240"
  },
  {
    "text": "side encryption and EMR will go to KMS or get the master key from you read the object envelope from the envelope get",
    "start": "1216240",
    "end": "1222840"
  },
  {
    "text": "the key temporary key decrypt it back and decrypt using that key now decrypt your data back to you and the this",
    "start": "1222840",
    "end": "1228840"
  },
  {
    "text": "scenario you're controlling the entire Keys uh the encryption keys from your side you could choose to do a server side encryption in which you don't have",
    "start": "1228840",
    "end": "1235280"
  },
  {
    "text": "to worry about any encryption but your data in S3 will be encrypted also I talked about um how",
    "start": "1235280",
    "end": "1243320"
  },
  {
    "text": "emrfs uses Dynamo DB which is Illustrated here that every operation you run against S3 gets recorded into",
    "start": "1243320",
    "end": "1249600"
  },
  {
    "text": "Dynamo DB so now if I have to do a listing of anything I can actually use Dynamo DB for that listing rather than",
    "start": "1249600",
    "end": "1255240"
  },
  {
    "text": "doing a get uh list operation on S3 and as you can you can see this leads to something like 3x or 4X 5x you know",
    "start": "1255240",
    "end": "1262039"
  },
  {
    "text": "something in that frange is going to give you faster latencies in your job start times which means that now when you run a job it going to probably start",
    "start": "1262039",
    "end": "1268679"
  },
  {
    "text": "three three or four times faster it's not going to run any faster but it's going to start faster based on the",
    "start": "1268679",
    "end": "1275200"
  },
  {
    "text": "existing times and all of this is transparent to you as end user you as end user just",
    "start": "1275200",
    "end": "1281440"
  },
  {
    "text": "have to say that enable emrfs on my cluster and just using the same S3 notation endpoint so you can say that",
    "start": "1281440",
    "end": "1287600"
  },
  {
    "text": "rather than having my on something like hdfs it's just S3 colon colon notation and it will take care of every single",
    "start": "1287600",
    "end": "1293520"
  },
  {
    "text": "aspects behind it now S3 is still a remote object",
    "start": "1293520",
    "end": "1299799"
  },
  {
    "start": "1296000",
    "end": "1296000"
  },
  {
    "text": "storage which means that every time you're reading or writing from S3 the data is going over the network which leads to a few things that if you're",
    "start": "1299799",
    "end": "1306200"
  },
  {
    "text": "running alterative workload let's say that you run a high query and the high query is writing data set into uh S3 and",
    "start": "1306200",
    "end": "1314159"
  },
  {
    "text": "H query in general theoretically could be compromised on multiple map jobs so each if each of the map rce job is just",
    "start": "1314159",
    "end": "1320080"
  },
  {
    "text": "writing data temporary intermediate output to S3 is going to make a round trip to the network to read in the next",
    "start": "1320080",
    "end": "1325480"
  },
  {
    "text": "job itself at that point in time you can optimize it um to write the data instead",
    "start": "1325480",
    "end": "1330840"
  },
  {
    "text": "to hdfs and write only the final output to S3 so you could have change jobs job",
    "start": "1330840",
    "end": "1335919"
  },
  {
    "text": "number one reads from S3 wres to hdfs job number two reads from hdfs wres to",
    "start": "1335919",
    "end": "1340960"
  },
  {
    "text": "hdfs till job number one which reads from hdfs but writes a final output to",
    "start": "1340960",
    "end": "1346400"
  },
  {
    "text": "uh S3 so you can get a faster uh round trip time in there if you're running a",
    "start": "1346400",
    "end": "1351600"
  },
  {
    "text": "disio intensive workloads or even if you're running Frameworks like hbas for instance you would need to use um you",
    "start": "1351600",
    "end": "1359080"
  },
  {
    "text": "would need to leverage hdfs because they don't run on S3 principally today uh at that point in time you can look at",
    "start": "1359080",
    "end": "1364760"
  },
  {
    "text": "storage optimized instances like D2 which can provide you very high sequential uh throughput for workloads",
    "start": "1364760",
    "end": "1370120"
  },
  {
    "text": "like Hado uh there's one another thing uh S3 disc CP in case um it's think of",
    "start": "1370120",
    "end": "1377520"
  },
  {
    "text": "it it's like a parallel copy operation what you could say is that if your data set is sitting in S3 and if you're going",
    "start": "1377520",
    "end": "1383120"
  },
  {
    "text": "to run repeated operations on top of the data sets on a cluster you could rather run S3 disc CP to transfer the data set",
    "start": "1383120",
    "end": "1390080"
  },
  {
    "text": "from S3 onto hdfs and then run your uh iterative workloads or repeated workloads on top of the data set sitting",
    "start": "1390080",
    "end": "1396279"
  },
  {
    "text": "in a in hdfs that's going to give you a little bit more uh better response times per se you can even even use S3 D CP to",
    "start": "1396279",
    "end": "1404159"
  },
  {
    "text": "combine files into uh combine multiple small files into one larger files and we're going to talk about that also",
    "start": "1404159",
    "end": "1409960"
  },
  {
    "text": "later on how you can use some of the options with sdcp to do that",
    "start": "1409960",
    "end": "1415440"
  },
  {
    "text": "effectively so those are the two things we talked about first uh some of the EMR features and we talked about some of the",
    "start": "1417720",
    "end": "1423799"
  },
  {
    "text": "U EMR integration with Amazon S3 now we're going to talk about some of the design principles or design patterns",
    "start": "1423799",
    "end": "1429360"
  },
  {
    "text": "with Amazon EMR and how people are actually using EMR as a service on AWS the first one which is uh quite",
    "start": "1429360",
    "end": "1438159"
  },
  {
    "start": "1435000",
    "end": "1435000"
  },
  {
    "text": "popular and what we see is people using EMR as a batch processing system your data sets get generated and return into",
    "start": "1438159",
    "end": "1444559"
  },
  {
    "text": "S3 um it could be Json file could be text delimited could be whatever it might be and then people",
    "start": "1444559",
    "end": "1450919"
  },
  {
    "text": "create daily EMR cluster to process their job and use Frameworks like high Pig Etc to transform the data and you",
    "start": "1450919",
    "end": "1458279"
  },
  {
    "text": "know and move it to somewhere else a common example of this being Yelp um as you know that Yelp is a local",
    "start": "1458279",
    "end": "1464960"
  },
  {
    "text": "online business review store they create two 250 Amazon EMR",
    "start": "1464960",
    "end": "1470039"
  },
  {
    "text": "jobs daily and they process 30 tabt of data sitting on S3 using those 250 EMR",
    "start": "1470039",
    "end": "1476919"
  },
  {
    "text": "clusters each of these clusters can do its own processing can do its own job can do its own um optimization and they",
    "start": "1476919",
    "end": "1484440"
  },
  {
    "text": "take this data from S3 itself and they use it uh you know they use a batch processing mechanism here to process the",
    "start": "1484440",
    "end": "1491080"
  },
  {
    "text": "data sets or process the reviews which we write on Yelp um in a batch",
    "start": "1491080",
    "end": "1497039"
  },
  {
    "start": "1497000",
    "end": "1497000"
  },
  {
    "text": "fashion the next next one being a long running cluster here contrary to the first one where you create the job for",
    "start": "1497039",
    "end": "1502960"
  },
  {
    "text": "the where you create the cluster for the duration of the job you need and terate the cluster after that what you rather",
    "start": "1502960",
    "end": "1508120"
  },
  {
    "text": "do here is that you have one cluster on which your persistent data stored like Edge Bas is running which can serve",
    "start": "1508120",
    "end": "1513919"
  },
  {
    "text": "requests which can serve data to your front- end apis and then you have a second cluster which is taking your data",
    "start": "1513919",
    "end": "1520279"
  },
  {
    "text": "from S3 processing it doing the ETL work on it doing the transformation doing the whatever changes you need aggregation",
    "start": "1520279",
    "end": "1527080"
  },
  {
    "text": "and writing the data back to S3 and then you can use something like for example let's say that hbas bulk load to load",
    "start": "1527080",
    "end": "1532440"
  },
  {
    "text": "the data from S3 into hbas so now you have a um no SQL store running on a",
    "start": "1532440",
    "end": "1538039"
  },
  {
    "text": "persistent cluster which is 24x7 and then you have a temporary cluster which is being created as and when data becomes available in S3 and",
    "start": "1538039",
    "end": "1545000"
  },
  {
    "text": "you need to process it the third one that people actually",
    "start": "1545000",
    "end": "1550240"
  },
  {
    "start": "1548000",
    "end": "1548000"
  },
  {
    "text": "use EMR clusters to interactively query a lot of data set sitting in S3 without any processing so in this case what",
    "start": "1550240",
    "end": "1556919"
  },
  {
    "text": "you're doing is that your data set is already sitting in S3 you are just creating a EMR cluster with framework",
    "start": "1556919",
    "end": "1562600"
  },
  {
    "text": "like spark with framework like Presto or even Impala um and you use that cluster",
    "start": "1562600",
    "end": "1568679"
  },
  {
    "text": "to run queries against data set setting in S3 uh with Impala today you don't have to transfer data to hdfs sorry",
    "start": "1568679",
    "end": "1574640"
  },
  {
    "text": "about that I kind of missed that but with Presto and Spark you can actually point to data sets which are sitting in",
    "start": "1574640",
    "end": "1580480"
  },
  {
    "text": "S3 and run a queries directly using those Frameworks without having to transfer data ever from S3 back onto",
    "start": "1580480",
    "end": "1587039"
  },
  {
    "text": "your hdfs cluster when you run a query for example in Presto or you run a query in spark SQL the data goes directly from",
    "start": "1587039",
    "end": "1594640"
  },
  {
    "text": "S3 to the Clusters uh process memory it does not even touch the hdfs dis in there it could touch the local disc for",
    "start": "1594640",
    "end": "1601120"
  },
  {
    "text": "swap space and Etc but it doesn't go to principally to hdfs so you're going to see a very fast interactive query",
    "start": "1601120",
    "end": "1606440"
  },
  {
    "text": "pattern with that and there's a Blog by Netflix where they talk about how they're using Presto running on EMR to",
    "start": "1606440",
    "end": "1613760"
  },
  {
    "text": "query to build an interactive querying mechanism on a 10 paby data set sitting in S3 I would encourage you guys to take a",
    "start": "1613760",
    "end": "1619960"
  },
  {
    "text": "look at it it's a very impressive blog in that sense on what they have done here the idea here is that you are",
    "start": "1619960",
    "end": "1626360"
  },
  {
    "text": "paying the cost of um S3 which is around 3 cents starting with uh 3 cents per GB",
    "start": "1626360",
    "end": "1631440"
  },
  {
    "text": "per month to store that data set you only create the Clusters when you need to query it you control when you're",
    "start": "1631440",
    "end": "1637159"
  },
  {
    "text": "creating the Clusters and whenever the moment your work is done you can destroy the cluster and not pay for any compute",
    "start": "1637159",
    "end": "1642320"
  },
  {
    "text": "capacity at that point in time which leads to a very economical and cost- effective architecture if you need to run ad hog explor of data sets um with u",
    "start": "1642320",
    "end": "1650240"
  },
  {
    "text": "S3 and Amazon EMR so onto the final part uh we talk",
    "start": "1650240",
    "end": "1657480"
  },
  {
    "text": "about some of the storage optimizations and these St optimizations principally apply on the third pattern where you",
    "start": "1657480",
    "end": "1663480"
  },
  {
    "text": "don't to talk your you're going to store your data set in S3 and you're going to create some interactive Computing",
    "start": "1663480",
    "end": "1668720"
  },
  {
    "text": "framework like Presto spark or spark SQL um to query the data set back from um S3",
    "start": "1668720",
    "end": "1676360"
  },
  {
    "text": "directly the first thing uh which is file formats on Broad classification that support of two types of file",
    "start": "1676360",
    "end": "1683039"
  },
  {
    "start": "1677000",
    "end": "1677000"
  },
  {
    "text": "formats within Hadoop the first one be grow oriented row oriented file formats are basically if you think of a logical",
    "start": "1683039",
    "end": "1688960"
  },
  {
    "text": "table um with the color coded beings the uh columns each the way how these rows or",
    "start": "1688960",
    "end": "1696200"
  },
  {
    "text": "each columns get store is what is determined whether it's a row or column oriented in row oriented all the rows",
    "start": "1696200",
    "end": "1702440"
  },
  {
    "text": "are stored first as is so Row one followed by row two followed by Row three logically and this is this is a design",
    "start": "1702440",
    "end": "1708960"
  },
  {
    "text": "format which has been used by databases for quite some time uh if you choose a DEX file if you choose um AO data files",
    "start": "1708960",
    "end": "1715880"
  },
  {
    "text": "all of these these two for example these two principal uh data formats or these two commonly used data formats within uh",
    "start": "1715880",
    "end": "1721519"
  },
  {
    "text": "Hadoop they store data sets at rows so all your rows get stored serly so Row one followed by row two followed by Row",
    "start": "1721519",
    "end": "1727760"
  },
  {
    "text": "three and Etc uh two more popular columnar formats orc",
    "start": "1727760",
    "end": "1733720"
  },
  {
    "text": "um I think optimized record colmer not object probably and and U par what they",
    "start": "1733720",
    "end": "1740480"
  },
  {
    "text": "do is they rather than storing data as rows they sto they they split rows into a groups and they Store The Columns of",
    "start": "1740480",
    "end": "1747159"
  },
  {
    "text": "those groups together first so it'll be like columns one of group one column two of group two column three of group um",
    "start": "1747159",
    "end": "1754360"
  },
  {
    "text": "three the idea why columnus stores is important because if you're running analytics queries most of the time",
    "start": "1754360",
    "end": "1759519"
  },
  {
    "text": "you're running analytics queries on a very large white table and but when you run the query you only use maybe a",
    "start": "1759519",
    "end": "1765559"
  },
  {
    "text": "column maybe two column maybe three column out of that and if you store your data set in a colum format that format",
    "start": "1765559",
    "end": "1770840"
  },
  {
    "text": "can practically skip all the other 97 um columns it can only read The Columns it need to read and it can it can give you",
    "start": "1770840",
    "end": "1777919"
  },
  {
    "text": "it can much better suit it for our analytical workloads but if you want to use um some format which can work with",
    "start": "1777919",
    "end": "1785080"
  },
  {
    "text": "other systems also at that point in time you have to look into something like text for example if you want to take your data set load into red shift you",
    "start": "1785080",
    "end": "1791279"
  },
  {
    "text": "have to use uh text files for instance Json text files whatever it might be the choice of what you use between",
    "start": "1791279",
    "end": "1799480"
  },
  {
    "start": "1797000",
    "end": "1797000"
  },
  {
    "text": "them uh it's a delicate balance between which tool you want prefer whether you prefer High whether you prefer Impala",
    "start": "1799480",
    "end": "1805320"
  },
  {
    "text": "whether you prefer presso because somehow each of these tools have a better integration with either of the",
    "start": "1805320",
    "end": "1810600"
  },
  {
    "text": "formats for example um Hive has better integration with uh orc Impala and",
    "start": "1810600",
    "end": "1816880"
  },
  {
    "text": "Presto probably has better integration with park today from a support standpoint um you could also mix and",
    "start": "1816880",
    "end": "1824480"
  },
  {
    "text": "match which is quite interesting uh for example you can use a to Define schema only but rather than using AO data files",
    "start": "1824480",
    "end": "1831200"
  },
  {
    "text": "you can use Park to store your data so which means that your schema is defined by AO and AO gives you the interoperability between different uh",
    "start": "1831200",
    "end": "1837960"
  },
  {
    "text": "programing languages also like Beyond Java itself and you can use par to give you the uh flexibility in how you run",
    "start": "1837960",
    "end": "1844000"
  },
  {
    "text": "analytical queries the one thing you definitely don't want to do you don't want to store your data as a single Json or XML object",
    "start": "1844000",
    "end": "1851840"
  },
  {
    "text": "because had realized the principle of being able to split your data if you store your entire data set as a single",
    "start": "1851840",
    "end": "1857240"
  },
  {
    "text": "Json object Hado does not know how to split it and that's going to lead to a lot of trouble what you want to do is",
    "start": "1857240",
    "end": "1862480"
  },
  {
    "text": "you want to store your data probably as a Json records which is different than a Json object what it means that each",
    "start": "1862480",
    "end": "1868000"
  },
  {
    "text": "individual line within your within your file is a single Json object and it's a complete Json object within that not",
    "start": "1868000",
    "end": "1873480"
  },
  {
    "text": "part of a bigger Json object itself second uh compression whether you want",
    "start": "1873480",
    "end": "1879600"
  },
  {
    "text": "to do a block or file compression either ways I mean compression is a good idea going forward which talks about small",
    "start": "1879600",
    "end": "1885039"
  },
  {
    "start": "1884000",
    "end": "1884000"
  },
  {
    "text": "files that you want to avoid small small files because what happens generally",
    "start": "1885039",
    "end": "1890279"
  },
  {
    "text": "that when you run a Hado task when you run any of these tasks in general one",
    "start": "1890279",
    "end": "1895320"
  },
  {
    "text": "single task processes one file and rather than having a huge a large",
    "start": "1895320",
    "end": "1901320"
  },
  {
    "text": "splitable file if you have like multiples of millions of small files",
    "start": "1901320",
    "end": "1906840"
  },
  {
    "text": "which mean that the the framework you're using will be forced to create millions of jobs or millions of Tas to read data",
    "start": "1906840",
    "end": "1912559"
  },
  {
    "text": "from each individual file but if you have like let's say thousands of big files then the framework can only run",
    "start": "1912559",
    "end": "1918559"
  },
  {
    "text": "thousand jobs and each of those jobs can basically read data sets across that in parallel which is going to give you way effective speed or way effective",
    "start": "1918559",
    "end": "1924960"
  },
  {
    "text": "processing in that scenario it's only not only that it's also going to save you the round Tripes it has to make to",
    "start": "1924960",
    "end": "1930799"
  },
  {
    "text": "read the small files you probably might know onways the um problem which Hadoop name node had around the small files",
    "start": "1930799",
    "end": "1937480"
  },
  {
    "text": "because each of the small files means that name node had to build that indexing had to build that storage around uh around the small",
    "start": "1937480",
    "end": "1944440"
  },
  {
    "text": "files in case you do land up with small files there are two things you could do the",
    "start": "1944440",
    "end": "1949960"
  },
  {
    "start": "1948000",
    "end": "1948000"
  },
  {
    "text": "first thing you can run something called S3 DCP we talked about this earlier it's a distributed copy that's what it really",
    "start": "1949960",
    "end": "1956159"
  },
  {
    "text": "stands for but here your Source or Target could be S3 and when you run S3 disc CP there's one option called um I",
    "start": "1956159",
    "end": "1964600"
  },
  {
    "text": "don't know I don't see it here but an option called pattern and group by or Target sze sorry the pattern tells you",
    "start": "1964600",
    "end": "1971559"
  },
  {
    "text": "that based on what input file patterns names I'm going to group my files into one larger file and targets file or",
    "start": "1971559",
    "end": "1978120"
  },
  {
    "text": "Target size determines how large the single file size should be you could choose something like 64 MB 2 128 MB or",
    "start": "1978120",
    "end": "1985440"
  },
  {
    "text": "Etc in case none of that if even that is not an option you could look into uh",
    "start": "1985440",
    "end": "1990519"
  },
  {
    "text": "changing the hdfs block size and that's something you can do using the bootstrap actions we described earlier when you",
    "start": "1990519",
    "end": "1996240"
  },
  {
    "text": "create a cluster you can just create a small bootstrap action to lower the hdfs block size and even probably the",
    "start": "1996240",
    "end": "2001679"
  },
  {
    "text": "application to better give you better uh um better ion um better match between",
    "start": "2001679",
    "end": "2007240"
  },
  {
    "text": "your files SI and the cluster processing uh scale",
    "start": "2007240",
    "end": "2012880"
  },
  {
    "text": "finally when your data sets in S3 most of the time you're transfering data over the network and if you use your",
    "start": "2012880",
    "end": "2019039"
  },
  {
    "text": "compression it's going to help you your job because most of the times your job is not going to be CPU bounded is probably going to be bounded by the",
    "start": "2019039",
    "end": "2025200"
  },
  {
    "text": "network and with compression what you're doing is you're lowering the size of data you're going to get transfer over the network but you're moving the load",
    "start": "2025200",
    "end": "2032519"
  },
  {
    "text": "certain bit from Network to the CPU so it can do a compress and UNC on the Fly",
    "start": "2032519",
    "end": "2038760"
  },
  {
    "text": "it's going to give you both effective processing and faster job times and you can also choose to compress mapper and",
    "start": "2038760",
    "end": "2044200"
  },
  {
    "text": "reducer output which in general by default is enabled what it really means that for example if you run a map job",
    "start": "2044200",
    "end": "2050240"
  },
  {
    "text": "the map phase also writes its data onto the local disk and then reducer phase finally writes the output into sgfs or",
    "start": "2050240",
    "end": "2056919"
  },
  {
    "text": "S3 somewhere and if you use a compression which could be any compression you can use in here it's",
    "start": "2056919",
    "end": "2062200"
  },
  {
    "text": "going to basically write data faster back to disk and read data faster from dis because most of the times the the",
    "start": "2062200",
    "end": "2067800"
  },
  {
    "text": "bottleneck in that case is the disk speed or the disc throughput at that point in time uh most of the Amazon EMR jobs are",
    "start": "2067800",
    "end": "2076040"
  },
  {
    "text": "by default have compression enabled and I think they do do have lzo enabled with Hadoop 1 and with Hadoop 2 I think",
    "start": "2076040",
    "end": "2082358"
  },
  {
    "text": "that's Snappy compression which is enabled by default you could pick to choose it or you can change it as you",
    "start": "2082359",
    "end": "2087440"
  },
  {
    "text": "deem appropriate four type of different compressions um primally in use zzip",
    "start": "2087440",
    "end": "2094280"
  },
  {
    "start": "2089000",
    "end": "2089000"
  },
  {
    "text": "bzip 2 lzo and snappy we referred earlier to and snappy snappy is what it",
    "start": "2094280",
    "end": "2099839"
  },
  {
    "text": "uses Hadoop 2 uses by default on EMR lzo is what is by default on Hadoop 1 on",
    "start": "2099839",
    "end": "2105800"
  },
  {
    "text": "EMR which compression scheme you choose probably depends on a lot of factors but here are some of the guidance you can",
    "start": "2105800",
    "end": "2111880"
  },
  {
    "text": "actually use to depending on what compression you have to use if you use lzo remember that you have to pre-index",
    "start": "2111880",
    "end": "2117240"
  },
  {
    "text": "the data otherwise it's not splitable which means that if you compress your data set lzo you need Hado",
    "start": "2117240",
    "end": "2123800"
  },
  {
    "text": "is going to read that entire file as one large one single uh from one single task but if you pre-index your data Hado can",
    "start": "2123800",
    "end": "2130200"
  },
  {
    "text": "split that file and process that in parallel so these are some of the",
    "start": "2130200",
    "end": "2135480"
  },
  {
    "text": "guidelines which you can use or these are some of the pointers you can use to choosing which compression you need uh whe which compression you actually use",
    "start": "2135480",
    "end": "2141320"
  },
  {
    "text": "in effective will depend on whether you need to interoperate with other uh systems whether you are running into",
    "start": "2141320",
    "end": "2146560"
  },
  {
    "text": "time sensitive workload whether you're processing very huge amounts of data and you want to you know um make the data",
    "start": "2146560",
    "end": "2152200"
  },
  {
    "text": "sitze smaller as it get transferred between EMR and S3 in case um you really don't have any",
    "start": "2152200",
    "end": "2159520"
  },
  {
    "text": "recommendation you just want to start with it um using gz compression is pretty is probably a pretty good point",
    "start": "2159520",
    "end": "2165760"
  },
  {
    "text": "to start with and with that we'll invite Andrew to talk about um answers and how",
    "start": "2165760",
    "end": "2171760"
  },
  {
    "text": "they're handling 5 billion sessions per day using EMR go ahead Andrew than thanks R um so as you said I'm going to",
    "start": "2171760",
    "end": "2178200"
  },
  {
    "text": "talk to you guys about how we've scaled up using EMR to handle five billion sessions per day so answers is an opinionated mobile",
    "start": "2178200",
    "end": "2185720"
  },
  {
    "start": "2183000",
    "end": "2183000"
  },
  {
    "text": "analytics product we allow uh give the ability to mobile app developers to answer questions like",
    "start": "2185720",
    "end": "2190839"
  },
  {
    "text": "how's my app growing with metrics like daily active users and monthly active users or how much is my app crashing",
    "start": "2190839",
    "end": "2196680"
  },
  {
    "text": "what is the thing that I should be doing fixing on my app right now to make my users happier how are my users engaging",
    "start": "2196680",
    "end": "2203040"
  },
  {
    "text": "with my application through like session duration and retention we can tell them how their users are",
    "start": "2203040",
    "end": "2208800"
  },
  {
    "text": "doing so all of this data comes in we got five billion sessions per day which translates to about a trillion analytics",
    "start": "2208800",
    "end": "2215440"
  },
  {
    "text": "events per month this is a lot of data so we're architected in what's called a",
    "start": "2215440",
    "end": "2221280"
  },
  {
    "start": "2219000",
    "end": "2219000"
  },
  {
    "text": "Lambda architecture on the left hand side here we have the devices sending us data the devices come in we get split",
    "start": "2221280",
    "end": "2226839"
  },
  {
    "text": "into two streams the first the top one is the batch layer the batch layer is responsible for keeping a immutable",
    "start": "2226839",
    "end": "2234000"
  },
  {
    "text": "appendonly raw set of the data and then we layer on Technologies like EMR that",
    "start": "2234000",
    "end": "2239200"
  },
  {
    "text": "make it really easy to do batch compute on that data to provide metrics for our customers on the bottom there we have",
    "start": "2239200",
    "end": "2245640"
  },
  {
    "text": "the speed layer the speed layer play is responsible for processing events as they come into our system this provides",
    "start": "2245640",
    "end": "2251599"
  },
  {
    "text": "a real-time view of data to our users um which is they really like because I can see their data right now and they don't",
    "start": "2251599",
    "end": "2257200"
  },
  {
    "text": "get any stale data at query time you can answer any question by combining the batch and the",
    "start": "2257200",
    "end": "2263319"
  },
  {
    "text": "speed layer together in this talk I'm mostly going to be talking about the batch layer um I",
    "start": "2263319",
    "end": "2269560"
  },
  {
    "text": "suggest you all go after this talk and just Google Lambda architecture the first result is should be a really good",
    "start": "2269560",
    "end": "2275079"
  },
  {
    "text": "um summary of some of the trade-offs and some of the benef benefits of uh of architecting your system in sort of a Lambda",
    "start": "2275079",
    "end": "2282440"
  },
  {
    "start": "2282000",
    "end": "2282000"
  },
  {
    "text": "fashion um so as the data comes in it comes in in small packets right so all",
    "start": "2282440",
    "end": "2288079"
  },
  {
    "text": "these files are in really small about 2 and a half megabyte files and as rules is saying this is really bad",
    "start": "2288079",
    "end": "2293920"
  },
  {
    "text": "performance-wise for Hadoop we really want to consume the files in much larger chunks so we use S3 disc copy to take",
    "start": "2293920",
    "end": "2301960"
  },
  {
    "text": "all of those small 2 and a half megabyte files and turn them into much fewer larger roughly around 400 to 500",
    "start": "2301960",
    "end": "2309359"
  },
  {
    "text": "megabyte files and this improves our job startup time immensely we then go ahead and Trigger",
    "start": "2309359",
    "end": "2315240"
  },
  {
    "text": "all of our computations which all run on their own ephemeral EMR clusters and",
    "start": "2315240",
    "end": "2320880"
  },
  {
    "text": "this may sound really expensive but we use spot instances pretty much exclusively in order to save a lot on",
    "start": "2320880",
    "end": "2326440"
  },
  {
    "text": "that compute cost we can start n number of computations to compute all of those metrics you saw in the first uh",
    "start": "2326440",
    "end": "2333720"
  },
  {
    "text": "slide uh at really um with without spending a lot of money on um a compute",
    "start": "2333720",
    "end": "2340920"
  },
  {
    "text": "cost uh after the jobs are done they then write their results out back out to S3 so we are using cascading which is a",
    "start": "2340920",
    "end": "2349079"
  },
  {
    "text": "Java abstraction on map produce um but we've also played around with things like calog which is a closure DSL on top",
    "start": "2349079",
    "end": "2355280"
  },
  {
    "text": "of cascading we've also played around with scalding which is a Scala DSL on top of",
    "start": "2355280",
    "end": "2360520"
  },
  {
    "text": "cascading the point here is that the flexibility of EMR lets you bring the tools that make you most effective to",
    "start": "2360520",
    "end": "2367599"
  },
  {
    "text": "the platform and I think this is really powerful if you're used to working in closure typically you want to keep you",
    "start": "2367599",
    "end": "2374319"
  },
  {
    "text": "working in closure so you can use something like calog to stay effective and stay fast and agile while uh",
    "start": "2374319",
    "end": "2381880"
  },
  {
    "text": "providing value for customers and batch processing all your data um for S3 disc copy we use LZ um",
    "start": "2381880",
    "end": "2390000"
  },
  {
    "text": "some of the reasons for that is LZ is fast um it's uh sacrifices some in",
    "start": "2390000",
    "end": "2395599"
  },
  {
    "text": "compression ratio but at the end of the day um not having to worry about uh",
    "start": "2395599",
    "end": "2401160"
  },
  {
    "text": "processing time to compress and decompress the data while we're processing it is really valuable lzo is splitable so as R was",
    "start": "2401160",
    "end": "2408800"
  },
  {
    "text": "talking about if you index your files you can take those large files and split and have um EMR split them up for you so",
    "start": "2408800",
    "end": "2416720"
  },
  {
    "text": "the mappers can work on chunks of the larger file and that can improve performance this is just a way to tune",
    "start": "2416720",
    "end": "2422480"
  },
  {
    "text": "uh tune your jobs and um gain a little bit more performance out of them lzo is built in I think this is one",
    "start": "2422480",
    "end": "2428720"
  },
  {
    "text": "of the most important points um just by adding like one command line argument to S3 disc copy you can put lzo compression",
    "start": "2428720",
    "end": "2437040"
  },
  {
    "text": "in and this is awesome so I don't have to as a developer I don't have to think about um you know how am I going to",
    "start": "2437040",
    "end": "2442160"
  },
  {
    "text": "compress this data how am I going to ingest this compressed data I can let EMR handle all of",
    "start": "2442160",
    "end": "2448560"
  },
  {
    "text": "that and lzo improves performance so as R was saying uh you can press your data",
    "start": "2448640",
    "end": "2453920"
  },
  {
    "text": "down you reduce the amount of uh Network overhead you pay for transferring your information from S3 into your EMR",
    "start": "2453920",
    "end": "2461520"
  },
  {
    "text": "clusters so another thing we do to improve performance is snapshotting so take a computation like monthly active",
    "start": "2462520",
    "end": "2468480"
  },
  {
    "start": "2463000",
    "end": "2463000"
  },
  {
    "text": "users so if you think about how that computation is structured we have to look at all of the device all of the",
    "start": "2468480",
    "end": "2474160"
  },
  {
    "text": "unique devices over the course of a month right and you can conceivably have a job that takes 30 command line",
    "start": "2474160",
    "end": "2480920"
  },
  {
    "text": "arguments ingests all 30 days worth of data and then computes the set of devices that we've seen in a given month",
    "start": "2480920",
    "end": "2487400"
  },
  {
    "text": "but that's really expensive instead what we do is our jobs uh take the data in",
    "start": "2487400",
    "end": "2493560"
  },
  {
    "text": "from the day figure out what devices that they've seen and then write a snapshot out to",
    "start": "2493560",
    "end": "2499040"
  },
  {
    "text": "S3 this data usually consists of some sort of a timestamp from the last time we've seen the device and then the",
    "start": "2499040",
    "end": "2504839"
  },
  {
    "text": "unique device ID so that way the next job that runs can prune out old devices that are haven't been seen for more than",
    "start": "2504839",
    "end": "2510480"
  },
  {
    "text": "a month and then continue to interleave in the data from that day to compute the",
    "start": "2510480",
    "end": "2516000"
  },
  {
    "text": "actual result so this this improves performance a lot as you can tell um just by not having to to suck in 30 days",
    "start": "2516000",
    "end": "2523040"
  },
  {
    "text": "worth of data in order to do these computations back filling so as a show of hands who's ever released a bug to",
    "start": "2523040",
    "end": "2531520"
  },
  {
    "start": "2526000",
    "end": "2526000"
  },
  {
    "text": "production so this is really awesome uh so this is basically a safety net and this is kind of uh speaks to the",
    "start": "2531520",
    "end": "2537880"
  },
  {
    "text": "properties of the batch layer and that it's uh immutable and aend only so if I run a job with the same arguments twice",
    "start": "2537880",
    "end": "2543800"
  },
  {
    "text": "I should theoretically get the same result each time what this means is if I release a bug on say Friday like I'm",
    "start": "2543800",
    "end": "2549480"
  },
  {
    "text": "getting ready to go drinking with my friends you know I hit that merge button on GitHub it goes up and then the job",
    "start": "2549480",
    "end": "2554960"
  },
  {
    "text": "gets deployed it runs all weekend and now I go look at it on Monday and my data is corrupt so by using EMR and using the",
    "start": "2554960",
    "end": "2562720"
  },
  {
    "text": "spot inses and the ephemeral clusters we can start jobs for each of the days that we have corrupt data and go back after",
    "start": "2562720",
    "end": "2569839"
  },
  {
    "text": "we fixed the bug go back and recompute all of those days worth of data and this safety net is really valuable",
    "start": "2569839",
    "end": "2578920"
  },
  {
    "start": "2578000",
    "end": "2578000"
  },
  {
    "text": "um for job scheduling we use storm which is a little bit of an unconventional use of storm uh we basically built cron on",
    "start": "2578920",
    "end": "2585559"
  },
  {
    "text": "top of storm in order to manage our EMR jobs um so basically what happens is here is the orchestrator uses the uh EMR",
    "start": "2585559",
    "end": "2593040"
  },
  {
    "text": "API to start jobs to monitor them and then when they finish to either chain",
    "start": "2593040",
    "end": "2598119"
  },
  {
    "text": "other jobs together or kick off other tasks that we need to do after the the data has been",
    "start": "2598119",
    "end": "2604079"
  },
  {
    "text": "processed um there's other tools for doing this when we started these Services didn't exist so I recommend you",
    "start": "2604079",
    "end": "2609760"
  },
  {
    "text": "guys go check out data Pipeline and simple workflow Service as a really awesome Services um but I think speaking",
    "start": "2609760",
    "end": "2615480"
  },
  {
    "text": "of the flexibility of EMR you can even use just something as simple as cran to uh schedule your jobs depending on how",
    "start": "2615480",
    "end": "2621599"
  },
  {
    "text": "complicated your workflow is so after the batch view is computed",
    "start": "2621599",
    "end": "2626839"
  },
  {
    "start": "2625000",
    "end": "2625000"
  },
  {
    "text": "our registrator will then go and uh kick off what we call a data pump and what",
    "start": "2626839",
    "end": "2631880"
  },
  {
    "text": "this is doing is it's forklifting the data out of S3 and putting it into some queriable database in our case is",
    "start": "2631880",
    "end": "2637520"
  },
  {
    "text": "cassander because we're working with time series data but this could be pretty much",
    "start": "2637520",
    "end": "2642599"
  },
  {
    "text": "anything so what if my data is not an S3 so say I have some data in something",
    "start": "2642599",
    "end": "2648520"
  },
  {
    "start": "2643000",
    "end": "2643000"
  },
  {
    "text": "like MySQL or mongod DB and I want to inter leave that data in with my S3 data say it's going to provide a little more",
    "start": "2648520",
    "end": "2654839"
  },
  {
    "text": "value for customers I can do some more interesting metrics around that sort of data than I can otherwise I could write",
    "start": "2654839",
    "end": "2661359"
  },
  {
    "text": "something like a python script to pull the data down from whatever the database is store back in S3 now it's really",
    "start": "2661359",
    "end": "2667640"
  },
  {
    "text": "easily easy to get at by EMR but actually you can query databases",
    "start": "2667640",
    "end": "2672760"
  },
  {
    "text": "from EMR and from map produce so cascading has a bunch of uh libraries for doing this but I think this is",
    "start": "2672760",
    "end": "2678640"
  },
  {
    "text": "really powerful in that you can combine all of your different data sources and use the flexibility of EMR to provide",
    "start": "2678640",
    "end": "2684520"
  },
  {
    "text": "the best uh uh best value that you can for your customers and compute the the",
    "start": "2684520",
    "end": "2690000"
  },
  {
    "text": "data that you really want to compute and then you can write it out to something like back out to S3 I think the real power of of em R is",
    "start": "2690000",
    "end": "2697559"
  },
  {
    "text": "that they abstract away all of the minutia of having to manage the hdfs",
    "start": "2697559",
    "end": "2703000"
  },
  {
    "text": "cluster having to worry about how do you scale your nodes up how do I make sure they can talk to each other it abstracts",
    "start": "2703000",
    "end": "2709119"
  },
  {
    "text": "all that stuff out out so you as a developer can really focus on the things that you really care about which is",
    "start": "2709119",
    "end": "2715359"
  },
  {
    "text": "Computing the data that is going to bring the most value to your customers uh and that's why we've U really",
    "start": "2715359",
    "end": "2720599"
  },
  {
    "text": "fortunate and really awesome that the team uh the EMR team is so awesome um and that they have this really cool",
    "start": "2720599",
    "end": "2725960"
  },
  {
    "text": "product thank you thank you",
    "start": "2725960",
    "end": "2731800"
  },
  {
    "text": "Andrew that was pretty cool uh one thing I didn't talk about they actually use uh",
    "start": "2735079",
    "end": "2740440"
  },
  {
    "text": "EMR apis within storm to create EMR cluster and that was one of thing you're talking about that it's quite easy for",
    "start": "2740440",
    "end": "2745839"
  },
  {
    "text": "you to launch EMR clusters you could use uh CLI to launch clusters by Crown job or take Andrew's route and use tomor to",
    "start": "2745839",
    "end": "2752839"
  },
  {
    "text": "launch clusters either ways uh but just to reiterate on some of the takeway um as we go",
    "start": "2752839",
    "end": "2758720"
  },
  {
    "text": "along first use Amazon S3 as your persistent data store that gives you the ability to",
    "start": "2758720",
    "end": "2765680"
  },
  {
    "start": "2759000",
    "end": "2759000"
  },
  {
    "text": "decou your storage and your compute and you can choose your compute as you wish for your job and not worry about how",
    "start": "2765680",
    "end": "2772079"
  },
  {
    "text": "you're going to tune your individual Custer to meet hundreds of needs you can focus on on your job itself and not worry about tuning the job",
    "start": "2772079",
    "end": "2779319"
  },
  {
    "text": "second uh EMR supports spot instances so always look into using spot instances",
    "start": "2779319",
    "end": "2786160"
  },
  {
    "text": "for um if Bringing Down the work cost of your workload the spot inst are available for",
    "start": "2786160",
    "end": "2792559"
  },
  {
    "text": "you on a recent basis on average of 86% lower than on demand instances then",
    "start": "2792559",
    "end": "2798440"
  },
  {
    "text": "which means that if you use them effectively you probably can pretty much process your job somewhere at probably half the cost or even lower than that",
    "start": "2798440",
    "end": "2805240"
  },
  {
    "text": "which will typically be on using on demand pricing use Amazon Reserve instances if",
    "start": "2805240",
    "end": "2810559"
  },
  {
    "text": "you have a steady workload if you're running a persistent cluster with hpas if you know a cluster is always going to come nightly and do this much of",
    "start": "2810559",
    "end": "2817160"
  },
  {
    "text": "processing um and always does this much of processing you can use Reserve instances which further lower the price",
    "start": "2817160",
    "end": "2822760"
  },
  {
    "text": "you pay for instances from ec2 you can create cloudwatch metrics and you can",
    "start": "2822760",
    "end": "2828359"
  },
  {
    "text": "even create an entire workflow saying that if my cluster has been idle for let's say x y z amount of time then I'm",
    "start": "2828359",
    "end": "2835520"
  },
  {
    "text": "going to create a workflow which is going to call a resize cluster and a rebalance operation on my",
    "start": "2835520",
    "end": "2841040"
  },
  {
    "text": "hdfs um or if the cluster has been idle then I'm going to terminate the cluster",
    "start": "2841040",
    "end": "2846200"
  },
  {
    "text": "and recreate the CL when there's a need to be second in case you're doing um we",
    "start": "2846200",
    "end": "2851440"
  },
  {
    "text": "do have some um discon program tailored depending on your users so if you're using EMR in which case you're spending",
    "start": "2851440",
    "end": "2858000"
  },
  {
    "text": "more than 10 uh K $10,000 us on EMR then you can definitely try to contact or",
    "start": "2858000",
    "end": "2863119"
  },
  {
    "text": "definitely contact your Amazon um account representative so you can get a better uh pricing with respect to your",
    "start": "2863119",
    "end": "2868920"
  },
  {
    "text": "investment on EMR and that should be all for today thank you um we are not holding you long we still have 12",
    "start": "2868920",
    "end": "2874640"
  },
  {
    "text": "minutes to go so that with the last session in case you have any Q&A we'll just take it offline please stop by here",
    "start": "2874640",
    "end": "2880319"
  },
  {
    "text": "and we can um work with that thank you again",
    "start": "2880319",
    "end": "2885720"
  }
]