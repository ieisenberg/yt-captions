[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "alright well let's get started so a little bit about me my name is Peter",
    "start": "890",
    "end": "7350"
  },
  {
    "text": "Bacchus I lead the event and data pipeline team at Netflix provider netflix i spend time at yella yahoo",
    "start": "7350",
    "end": "15299"
  },
  {
    "text": "paypal work in variety of different areas everything from payments to ad serving behavioral targeting real-time",
    "start": "15299",
    "end": "22890"
  },
  {
    "text": "analytics my background primarily is the should be systems large-scale infrastructure outside of that advice",
    "start": "22890",
    "end": "31289"
  },
  {
    "text": "several startups oddly enough in the data security and container space so",
    "start": "31289",
    "end": "36780"
  },
  {
    "text": "it's a little bit about me so i wanted",
    "start": "36780",
    "end": "41910"
  },
  {
    "start": "39000",
    "end": "88000"
  },
  {
    "text": "to talk a little bit about Keystone Keystone is our data pipeline at netflix",
    "start": "41910",
    "end": "47070"
  },
  {
    "text": "it's what we consider a paved path all applications and services are able to",
    "start": "47070",
    "end": "52379"
  },
  {
    "text": "produce and publish a man star pipeline and then we handle everything from that point down essentially we're responsible",
    "start": "52379",
    "end": "58890"
  },
  {
    "text": "for all the heavy lifting so that all the other teams don't have to do it so instead of you spinning up your own",
    "start": "58890",
    "end": "63960"
  },
  {
    "text": "pipeline or writing your own code to do that my team is responsible for that what I hope to cover in today's",
    "start": "63960",
    "end": "69900"
  },
  {
    "text": "conversation architecture design principles some of the trade-offs we had to make and the decisions why as well as",
    "start": "69900",
    "end": "77700"
  },
  {
    "text": "current state of technologies that were using also i'll throw in there as a",
    "start": "77700",
    "end": "82799"
  },
  {
    "text": "bonus and best practices some pro tips as i like to call them so as i mentioned",
    "start": "82799",
    "end": "89400"
  },
  {
    "start": "88000",
    "end": "165000"
  },
  {
    "text": "my team is responsible for data pipeline what does that mean we responsible for",
    "start": "89400",
    "end": "95520"
  },
  {
    "text": "publishing collecting processing aggregating and moving data at cloud",
    "start": "95520",
    "end": "101130"
  },
  {
    "text": "scale so that's relatively common term people throw out a lot so what does it",
    "start": "101130",
    "end": "107009"
  },
  {
    "text": "mean to be cloud scale for us we do 550 billion events a day at peak we're at",
    "start": "107009",
    "end": "116100"
  },
  {
    "text": "eight and a half million and 21 gb per second on average we are six and a half",
    "start": "116100",
    "end": "122460"
  },
  {
    "text": "million we push over a petabyte through",
    "start": "122460",
    "end": "127710"
  },
  {
    "text": "our pipeline on a daily basis and we have hundreds of different event",
    "start": "127710",
    "end": "133189"
  },
  {
    "text": "types now we started this process for Keystone in q4 of last year at that time",
    "start": "133189",
    "end": "141349"
  },
  {
    "text": "we were at three and a half million a second three hundred billion now as I",
    "start": "141349",
    "end": "147709"
  },
  {
    "text": "mentioned six and a half million 550 it's essentially doubled over the last year there's actually another talk if",
    "start": "147709",
    "end": "153799"
  },
  {
    "text": "you guys don't care about Kafka going on from netflix about cost management and they should be highlighting the increase",
    "start": "153799",
    "end": "160010"
  },
  {
    "text": "in my spin here shortly so if you guys catch up is actually pretty good pretty good to talk so i want to take a few",
    "start": "160010",
    "end": "167420"
  },
  {
    "start": "165000",
    "end": "273000"
  },
  {
    "text": "minutes and look at how we arrived to where we're at today when we first",
    "start": "167420",
    "end": "173090"
  },
  {
    "text": "started our needs were fairly simple we had applications and services at",
    "start": "173090",
    "end": "178190"
  },
  {
    "text": "published events we wanted to be able to collect those and write them to s3 so",
    "start": "178190",
    "end": "183590"
  },
  {
    "text": "our pipeline looked very slim one we based on apache chakra and to serve this pretty well for a while as time went on",
    "start": "183590",
    "end": "191510"
  },
  {
    "text": "we started getting more and more requests for at least once as well as",
    "start": "191510",
    "end": "197870"
  },
  {
    "text": "real time so we introduced a branch off main line that's he traffic so about",
    "start": "197870",
    "end": "205160"
  },
  {
    "text": "twenty percent of traffic went through the real-time pipe so we use that we",
    "start": "205160",
    "end": "210650"
  },
  {
    "text": "routed to elasticsearch and druid people consume directly off of kafka as well applications so if you look at it though",
    "start": "210650",
    "end": "217760"
  },
  {
    "text": "it adds a lot of complexity to what we were doing and there's also a lot of duplication and functionality so",
    "start": "217760",
    "end": "225709"
  },
  {
    "text": "original pipeline was best effort so there was some loss not a lot but there was something you didn't have any",
    "start": "225709",
    "end": "233269"
  },
  {
    "text": "persistence we didn't keep messages if if an instance died those messages swin",
    "start": "233269",
    "end": "238340"
  },
  {
    "text": "away so at the beginning of q4 we sat down his team and said how do we",
    "start": "238340",
    "end": "244699"
  },
  {
    "text": "simplify this for two reasons one from an operational management perspective we're essentially managing two pipelines",
    "start": "244699",
    "end": "251379"
  },
  {
    "text": "it serves slightly different use cases the other was from a complexity in the",
    "start": "251379",
    "end": "257120"
  },
  {
    "text": "business side we had teams coming to the saying hey where should i send events do",
    "start": "257120",
    "end": "262909"
  },
  {
    "text": "i send it to chuck la die sent it to Kafka if you see you have events going from chocolate into Kafka so there's a lot of",
    "start": "262909",
    "end": "269960"
  },
  {
    "text": "confusion as far as what people should be doing so I've q4 we sat down and said",
    "start": "269960",
    "end": "277580"
  },
  {
    "start": "273000",
    "end": "372000"
  },
  {
    "text": "how do we simplify and this is what we came up with this is Keystone if you",
    "start": "277580",
    "end": "284000"
  },
  {
    "text": "notice chuck was gone we've essentially flattened our pipeline so we no longer",
    "start": "284000",
    "end": "291050"
  },
  {
    "text": "have multiple paths in its fan in fan out there are some common themes router",
    "start": "291050",
    "end": "299000"
  },
  {
    "text": "that we had before but those are also different null dig down a little bit deeper than those as we go along there's",
    "start": "299000",
    "end": "305270"
  },
  {
    "text": "a couple reasons why we decided to do this one of them was there's a huge huge",
    "start": "305270",
    "end": "310550"
  },
  {
    "text": "community around Kafka majority of folks raised their hand when they said who's",
    "start": "310550",
    "end": "316940"
  },
  {
    "text": "using or looking at Kafka so as a very vibrant community the other had offered",
    "start": "316940",
    "end": "322610"
  },
  {
    "text": "us the durability that we needed as more and more teams were saying we need at least once in fact at this point we're",
    "start": "322610",
    "end": "329690"
  },
  {
    "text": "starting a team saying we want exactly once and with their previous pipeline we wouldn't been able to do that we could",
    "start": "329690",
    "end": "335990"
  },
  {
    "text": "have built it but we would have been duped duplicating functionality that there's an open source project that",
    "start": "335990",
    "end": "341300"
  },
  {
    "text": "already did and so what we decided to do is put our efforts into helping the",
    "start": "341300",
    "end": "347210"
  },
  {
    "text": "community which in turn would help us ideally we don't want to be a snowflake",
    "start": "347210",
    "end": "352780"
  },
  {
    "text": "everybody talks about some snow flakes are great they're unique except when you're dealing with infrastructure like",
    "start": "352780",
    "end": "358880"
  },
  {
    "text": "no one is really that different where you have to bear the burden of going in",
    "start": "358880",
    "end": "364850"
  },
  {
    "text": "at it alone and that was our philosophy and that's one of the reasons why we decided to leverage Kafka so let's take",
    "start": "364850",
    "end": "373790"
  },
  {
    "start": "372000",
    "end": "568000"
  },
  {
    "text": "a look at the first part of our infrastructure the fronting Kafka what we have here is we've split it into two",
    "start": "373790",
    "end": "380780"
  },
  {
    "text": "we have normal priority and high priority normal priority is a majority",
    "start": "380780",
    "end": "385910"
  },
  {
    "text": "of our events we have a we have two replicas and 12 hours of retention on",
    "start": "385910",
    "end": "392110"
  },
  {
    "text": "the high priority we have three replicas and 24 hours",
    "start": "392110",
    "end": "397300"
  },
  {
    "text": "retention the reason why we have the",
    "start": "397300",
    "end": "402430"
  },
  {
    "text": "difference in retention is what risk where are we willing to tolerate if there's some downstream event that",
    "start": "402430",
    "end": "409539"
  },
  {
    "text": "occurs the entire intent of our fronting Kafka clusters is to essentially behave",
    "start": "409539",
    "end": "415449"
  },
  {
    "text": "as a buffer consumed all the events that gets thrown at you if something happens",
    "start": "415449",
    "end": "421210"
  },
  {
    "text": "with s3 or elasticsearch or any anything downstream we have 12 to 24 hours to",
    "start": "421210",
    "end": "428860"
  },
  {
    "text": "reconcile those issues before we start losing data so let's take a look at",
    "start": "428860",
    "end": "438900"
  },
  {
    "text": "instance types we ended up going the d2x l's primarily large disk a large memory",
    "start": "438900",
    "end": "447840"
  },
  {
    "text": "medium network the one thing that we did notice so if you guys are looking at them once you get around 18 mb per",
    "start": "447840",
    "end": "456129"
  },
  {
    "text": "second replication lag starts to show up the other thing that we had to do when",
    "start": "456129",
    "end": "462699"
  },
  {
    "text": "we went with Kafka so for those who have used Kafka for those who haven't Kafka's",
    "start": "462699",
    "end": "468009"
  },
  {
    "text": "not necessarily designed to running cloud and that was one of the things that when we were making our decision",
    "start": "468009",
    "end": "474840"
  },
  {
    "text": "how do we do how do we deal what do we need to do in order to make it more native to cloud so right now we don't",
    "start": "474840",
    "end": "483430"
  },
  {
    "text": "have an answer we over provision so we over provision in all regions to handle",
    "start": "483430",
    "end": "489400"
  },
  {
    "text": "peak traffic as well as if we need to evacuate any region we need we need to",
    "start": "489400",
    "end": "494860"
  },
  {
    "text": "be able to sustain that traffic as well so our current footprint is about 1,500",
    "start": "494860",
    "end": "500050"
  },
  {
    "text": "brokers spread across three regions and that's the fronting Kafka piece one of",
    "start": "500050",
    "end": "509500"
  },
  {
    "text": "the things that we've done is zone aware replica assignment so this is currently",
    "start": "509500",
    "end": "516010"
  },
  {
    "text": "an internal version that we have we have a pull request I believe we just",
    "start": "516010",
    "end": "521198"
  },
  {
    "text": "submitted an architecture doc as well there's a couple benefits that we got by doing this right now we're in multiple",
    "start": "521199",
    "end": "528880"
  },
  {
    "text": "az's so from an availability perspective if we lose an AZ we're still okay the other",
    "start": "528880",
    "end": "535000"
  },
  {
    "text": "is if we have multiple Kafka instances on one physical host and that goes away",
    "start": "535000",
    "end": "541089"
  },
  {
    "text": "this also allows us to survive that one of the added benefits of doing this is",
    "start": "541089",
    "end": "547210"
  },
  {
    "text": "around maintenance prior to being able to take out a Z's at a time when we did",
    "start": "547210",
    "end": "553029"
  },
  {
    "text": "upgrades we had to do node by node by node and so now when we need to do",
    "start": "553029",
    "end": "559570"
  },
  {
    "text": "maintenance any upgrades we could do it at the AZ level we take out an AZ upgrade throw back in next piece is the",
    "start": "559570",
    "end": "571330"
  },
  {
    "start": "568000",
    "end": "615000"
  },
  {
    "text": "control plane so data movement is done by the data plane and it's managed by",
    "start": "571330",
    "end": "576820"
  },
  {
    "text": "the control plane so if we look at the control and data plane job manager",
    "start": "576820",
    "end": "583360"
  },
  {
    "text": "manages state the control playing reconciles from source of truth for us that's cough gun zookeeper and one of",
    "start": "583360",
    "end": "591700"
  },
  {
    "text": "the things we do is we manage trade-offs based on requirements latency throughput resource utilization the other thing",
    "start": "591700",
    "end": "598870"
  },
  {
    "text": "that we have in the control and data plane is we have the ability to auto scale so well we don't have that at the",
    "start": "598870",
    "end": "605589"
  },
  {
    "text": "kafka side one of the things that we wanted to do is at the routing layer we wanted to be able to auto scale and the",
    "start": "605589",
    "end": "612430"
  },
  {
    "text": "control plane allows us to do that so let's take a look at the router you",
    "start": "612430",
    "end": "618130"
  },
  {
    "start": "615000",
    "end": "1011000"
  },
  {
    "text": "notice that I had same image because I'm not that good at images and the previous the original diagram is this one so this",
    "start": "618130",
    "end": "626589"
  },
  {
    "text": "routing service is slightly different one of the things that we also discuss",
    "start": "626589",
    "end": "631870"
  },
  {
    "text": "at the time we did have a routing service however it didn't do things like checkpointing and so if we were going to",
    "start": "631870",
    "end": "637750"
  },
  {
    "text": "a pipeline that did at least once having",
    "start": "637750",
    "end": "643180"
  },
  {
    "text": "any part of that have lost defeated the purpose so once again not wanting to be",
    "start": "643180",
    "end": "649060"
  },
  {
    "text": "a snowflake we took a step back said okay we could either build this or we can look and see what else there is and",
    "start": "649060",
    "end": "654670"
  },
  {
    "text": "so part of our evaluation we ended up looking at a couple different things for",
    "start": "654670",
    "end": "660730"
  },
  {
    "text": "the routing service we ended up with Samsa Sam two works really well with Kafka and",
    "start": "660730",
    "end": "666619"
  },
  {
    "text": "so what it looks like we're using docker Sam's a bicycle RDS and we have",
    "start": "666619",
    "end": "672709"
  },
  {
    "text": "checkpointing clusters for Kafka so if we were playing buzzword bingo we're",
    "start": "672709",
    "end": "679639"
  },
  {
    "text": "really close we got a couple more let's dig in a little bit so the way this",
    "start": "679639",
    "end": "685069"
  },
  {
    "text": "functions is each job is its own containers and then Penta container and",
    "start": "685069",
    "end": "691279"
  },
  {
    "text": "its assigned specific partitions that it's Nick assume from as well as which",
    "start": "691279",
    "end": "697790"
  },
  {
    "text": "sink you notice we had s3 elasticsearch Kafka so that is configuration that gets",
    "start": "697790",
    "end": "704809"
  },
  {
    "text": "passed to it so the job managers responsibility is to write optimized static configurations the infrastructure",
    "start": "704809",
    "end": "712129"
  },
  {
    "text": "continuously reconciles definitions and then we use unique IDs to Matt to sign",
    "start": "712129",
    "end": "719689"
  },
  {
    "text": "two nodes and zookeeper so this is kind of high-level what we do simplified",
    "start": "719689",
    "end": "725529"
  },
  {
    "text": "State configuration goes in RDS it gets past the other thing that we did with",
    "start": "725529",
    "end": "731209"
  },
  {
    "text": "this the ability to auto scale I mention that earlier so by having each job have",
    "start": "731209",
    "end": "739779"
  },
  {
    "text": "partitions assigned to it that it consumes from if we need to scale up on",
    "start": "739779",
    "end": "745129"
  },
  {
    "text": "the kafka side if we need to add more partitions then for routing all we need to do is launch another job and have it",
    "start": "745129",
    "end": "751819"
  },
  {
    "text": "pick up those configurations and so now this allows us to go up and down at the",
    "start": "751819",
    "end": "756889"
  },
  {
    "text": "routing layer so let's take a look at",
    "start": "756889",
    "end": "763069"
  },
  {
    "text": "some more numbers because numbers numbers numbers numbers are good so if we look at the number of long-running",
    "start": "763069",
    "end": "770089"
  },
  {
    "text": "containers that we have for s3 we're running 5800 containers about 500 c34",
    "start": "770089",
    "end": "777499"
  },
  {
    "text": "XL's so for the routing service were using the c3 for XL's elasticsearch",
    "start": "777499",
    "end": "783439"
  },
  {
    "text": "there's about 850 long-running containers and the kafka routing were at about 3,200 long-term containers so in",
    "start": "783439",
    "end": "791209"
  },
  {
    "text": "total we're running about 10 thousand containers for a routing service right now and",
    "start": "791209",
    "end": "796390"
  },
  {
    "text": "about 850 ish instances alright so let's",
    "start": "796390",
    "end": "805210"
  },
  {
    "text": "drill down a little bit more container footprint about 225 gig in memory is",
    "start": "805210",
    "end": "811150"
  },
  {
    "text": "average it's what we assign about one cpu share one to twelve partitions so",
    "start": "811150",
    "end": "819430"
  },
  {
    "text": "depending we have topics that have only one partition we have so you may have a job with one partition and then we",
    "start": "819430",
    "end": "826150"
  },
  {
    "text": "periodically report health to the infrastructure so if we look at averages",
    "start": "826150",
    "end": "834600"
  },
  {
    "text": "about 1.8 gig per container average memories between 7 and 25 gig CPU is",
    "start": "834600",
    "end": "843040"
  },
  {
    "text": "only eight percent so we're not CPU bound we're actually we're actually saturating network because the job of",
    "start": "843040",
    "end": "850810"
  },
  {
    "text": "the routing service is consumed as fast as you can and move it on right",
    "start": "850810",
    "end": "855880"
  },
  {
    "text": "durability is there in case there's an issue but the intent is to move it to its long-term location as fast as",
    "start": "855880",
    "end": "862810"
  },
  {
    "text": "possible so network and 256 network out about 156",
    "start": "862810",
    "end": "869340"
  },
  {
    "text": "more numbers so we write to s3 every 200",
    "start": "873150",
    "end": "879270"
  },
  {
    "text": "mb or two minutes whichever comes first our average upload to s3 is about 200",
    "start": "879270",
    "end": "886690"
  },
  {
    "text": "milliseconds if we look at producer to router latency 50 their 70th percentile",
    "start": "886690",
    "end": "894430"
  },
  {
    "text": "is under a second 92 seconds overall we're averaging about two seconds so",
    "start": "894430",
    "end": "899500"
  },
  {
    "text": "from the point that it produced the event is produced to the point that it goes to the router is two and a half",
    "start": "899500",
    "end": "904780"
  },
  {
    "text": "seconds and if we look at Kafka to router lag 90th percentiles about five",
    "start": "904780",
    "end": "910330"
  },
  {
    "text": "seconds how many folks here have used",
    "start": "910330",
    "end": "917680"
  },
  {
    "text": "Sansa how many folks here that abuse ends are running it with yarn you don't",
    "start": "917680",
    "end": "925810"
  },
  {
    "text": "have to anymore so one of the trade-offs once again when we decided to go with",
    "start": "925810",
    "end": "931090"
  },
  {
    "text": "Sansa was it had dependency that runs under yarn and we did not want to run",
    "start": "931090",
    "end": "937300"
  },
  {
    "text": "and manage yarn for a routing service there's also a dependence there's also",
    "start": "937300",
    "end": "943270"
  },
  {
    "text": "potential where you get split brain and so it's not truly a che and so once",
    "start": "943270",
    "end": "949000"
  },
  {
    "text": "again we wanted to minimize those things so these JIRA's we're running this in",
    "start": "949000",
    "end": "954790"
  },
  {
    "text": "production we have a forked version we've submitted fixes so we'll see what",
    "start": "954790",
    "end": "960310"
  },
  {
    "text": "happens but Sam's of 41 basically is the one that allows us to run in containers",
    "start": "960310",
    "end": "966220"
  },
  {
    "text": "so standalone the other one that was really big for us was 775 that one we",
    "start": "966220",
    "end": "973180"
  },
  {
    "text": "were having out of memory issues the way it does its prefetch buffer so we also",
    "start": "973180",
    "end": "981460"
  },
  {
    "text": "submitted patches for those for those",
    "start": "981460",
    "end": "986680"
  },
  {
    "text": "who are Sam's of users there is a meetup in I think the 13th next week linkedin",
    "start": "986680",
    "end": "995050"
  },
  {
    "text": "will be talking and a lot more detail about the routing service so it'd be a good opportunity even if you guys aren't",
    "start": "995050",
    "end": "1001260"
  },
  {
    "text": "using sams and in stream processing it's it's I think",
    "start": "1001260",
    "end": "1006379"
  },
  {
    "text": "it's 630 on the 13th so cool so let's",
    "start": "1006379",
    "end": "1014600"
  },
  {
    "start": "1011000",
    "end": "1167000"
  },
  {
    "text": "take a look at the kafka piece the kafka kafka so this is where we are doing a",
    "start": "1014600",
    "end": "1021739"
  },
  {
    "text": "lot of our real-time it's similar to the previous branch except kind of a little bit more simplified so three typical",
    "start": "1021739",
    "end": "1030350"
  },
  {
    "text": "areas custom mantis spark and I'll touch on them so custom application could be",
    "start": "1030350",
    "end": "1035569"
  },
  {
    "text": "anything you write a service consume off Kafka and now if you go the retention",
    "start": "1035569",
    "end": "1044538"
  },
  {
    "text": "period in these Kafka clusters vary based on needs so while we want to",
    "start": "1044539",
    "end": "1051679"
  },
  {
    "text": "minimize in the front team these are up to the different teams and development",
    "start": "1051679",
    "end": "1057260"
  },
  {
    "text": "development teams to decide what they need what their needs are so if we look at mantis mantis is an internal stream",
    "start": "1057260",
    "end": "1065570"
  },
  {
    "text": "processing system that was developed at Netflix we recently open source Penza which is a part of that but it's does",
    "start": "1065570",
    "end": "1075019"
  },
  {
    "text": "real-time or historical streams it also has progressive processing it does push",
    "start": "1075019",
    "end": "1082429"
  },
  {
    "text": "pull or mixture so you could have a single job be configured to change based",
    "start": "1082429",
    "end": "1087830"
  },
  {
    "text": "on stage it's also cloud native so both jobs and clusters have the ability to",
    "start": "1087830",
    "end": "1093830"
  },
  {
    "text": "scale up and down spark streaming spark",
    "start": "1093830",
    "end": "1099919"
  },
  {
    "text": "streaming users all right couple more Sam's up in spark streaming so when we",
    "start": "1099919",
    "end": "1108710"
  },
  {
    "text": "were evaluating our routing service we actually looked at spark streaming as well spark within netflix has taken is a",
    "start": "1108710",
    "end": "1118340"
  },
  {
    "text": "little bit more along on the batch side there are teams at netflix that are",
    "start": "1118340",
    "end": "1124940"
  },
  {
    "text": "using spark streaming in this context that i showed but it's not it's still",
    "start": "1124940",
    "end": "1129950"
  },
  {
    "text": "kind of a leaf system one of the things we looked at 12 performance numbers",
    "start": "1129950",
    "end": "1136159"
  },
  {
    "text": "weren't quite their back pressure we worked you know we worked with the community we worked with data",
    "start": "1136159",
    "end": "1141920"
  },
  {
    "text": "bricks on campaigning for back pressure work we did notice a significant",
    "start": "1141920",
    "end": "1147470"
  },
  {
    "text": "increase in performance between what's 12 and 13 so so yeah so we'll see where",
    "start": "1147470",
    "end": "1153650"
  },
  {
    "text": "spark streaming goes there seems to be a lot of at least internally especially folks that are using the batch contacts",
    "start": "1153650",
    "end": "1159860"
  },
  {
    "text": "were starting to see more and more requests for people that that are interested in being able to do things both in batch in real time so Netflix is",
    "start": "1159860",
    "end": "1171410"
  },
  {
    "text": "primarily Java shop so if you have an application ring Java this is how you",
    "start": "1171410",
    "end": "1177800"
  },
  {
    "text": "would be able to send events to us now",
    "start": "1177800",
    "end": "1182950"
  },
  {
    "text": "well it is mostly a Java shop doesn't mean it's only a Java shop so there are",
    "start": "1182950",
    "end": "1188150"
  },
  {
    "text": "other things so what we do is we also provide a proxy service so you have a python application you can make a call",
    "start": "1188150",
    "end": "1195650"
  },
  {
    "text": "and send us events so our wire format extensible currently support JSON only",
    "start": "1195650",
    "end": "1204520"
  },
  {
    "text": "we are quickly going to pick up abra support the interests thing about this",
    "start": "1204520",
    "end": "1213350"
  },
  {
    "text": "is so when we started the process we decided to try to make this as",
    "start": "1213350",
    "end": "1218990"
  },
  {
    "text": "frictionless as possible the intent is we want to be able new pipeline run in",
    "start": "1218990",
    "end": "1224990"
  },
  {
    "text": "parallel with the old and then be able to cut over right because that's how do you move 550 billion event without",
    "start": "1224990",
    "end": "1231400"
  },
  {
    "text": "causing impact one of the things we did there is at the producer level we made",
    "start": "1231400",
    "end": "1238160"
  },
  {
    "text": "it so that we did dual rights so we ended up shadowing traffic and so we were able to have hot pipeline and the",
    "start": "1238160",
    "end": "1246260"
  },
  {
    "text": "other one that we were building out which was Keystone run in parallel the",
    "start": "1246260",
    "end": "1251570"
  },
  {
    "text": "one thing we did is we decided to change wire format because it allowed us future flexibility and this so kind of a pro",
    "start": "1251570",
    "end": "1258230"
  },
  {
    "text": "tip this is one that's the change caused a serialization bug it affected a",
    "start": "1258230",
    "end": "1265480"
  },
  {
    "text": "handful of teams but it went from being fairly frictionless to us now having to",
    "start": "1265480",
    "end": "1270740"
  },
  {
    "text": "work them and have them pick up new versions and things of that sort so there's a lot",
    "start": "1270740",
    "end": "1276340"
  },
  {
    "text": "more effort that we could have probably avoided so in hindsight we should have probably held off on making this change",
    "start": "1276340",
    "end": "1283090"
  },
  {
    "text": "until further down after we had cut over but that's just it's one of those things",
    "start": "1283090",
    "end": "1288250"
  },
  {
    "text": "where in hindsight like I said it's you got to figure out what you're optimizing for and we try to optimize not only for",
    "start": "1288250",
    "end": "1293740"
  },
  {
    "text": "frictionless but also for future flexibility and we ended up introducing an issue so a couple things we're doing",
    "start": "1293740",
    "end": "1302440"
  },
  {
    "text": "at the producer level so our philosophy is that if our service is down we should",
    "start": "1302440",
    "end": "1310450"
  },
  {
    "text": "never stop what that instance is serving we should not have any impact the other",
    "start": "1310450",
    "end": "1317080"
  },
  {
    "text": "thing is if our service is not available we shouldn't keep other services new instances from coming up and when we are",
    "start": "1317080",
    "end": "1325150"
  },
  {
    "text": "restored it should automatically start sending events back so fail but never",
    "start": "1325150",
    "end": "1335740"
  },
  {
    "text": "block cool so want to shift a little bit",
    "start": "1335740",
    "end": "1343810"
  },
  {
    "text": "talked about some of the architecture pieces some of the philosophies I'm gonna shift a little bit and some of the",
    "start": "1343810",
    "end": "1350860"
  },
  {
    "text": "monitoring and dashboarding that we're implementing so this is one of the areas that you can never have enough and so",
    "start": "1350860",
    "end": "1360940"
  },
  {
    "text": "we're spending a significant amount of time and effort making sure that we have the visibility that we need so one of",
    "start": "1360940",
    "end": "1369040"
  },
  {
    "text": "the things we've done so this is one of our dashboards so you have the ability at a high level 10,000 foot level sit",
    "start": "1369040",
    "end": "1376000"
  },
  {
    "text": "there and view what's going on quickly we can drill down from overall to",
    "start": "1376000",
    "end": "1383080"
  },
  {
    "text": "brokers to topics so this is important for us to have that visibility",
    "start": "1383080",
    "end": "1389410"
  },
  {
    "text": "especially you start talking about thousands of instances 100 hundreds upon",
    "start": "1389410",
    "end": "1395230"
  },
  {
    "text": "hundreds of topics and billions of events so we've spent a significant",
    "start": "1395230",
    "end": "1400870"
  },
  {
    "text": "amount of time on these they're still early more where to do but it's critical that you have",
    "start": "1400870",
    "end": "1406940"
  },
  {
    "text": "insights into what's going on so for",
    "start": "1406940",
    "end": "1412760"
  },
  {
    "start": "1410000",
    "end": "1486000"
  },
  {
    "text": "those who've used Kafka this is probably familiar ish concept linkedin has I",
    "start": "1412760",
    "end": "1418400"
  },
  {
    "text": "think they've done to talk on this it's kind of an auditor type service so we",
    "start": "1418400",
    "end": "1423950"
  },
  {
    "text": "have our own and basically what we do is we do bro for monitoring consumer",
    "start": "1423950",
    "end": "1429620"
  },
  {
    "text": "monitoring heart beating as well as test and so on broker we alert on offline",
    "start": "1429620",
    "end": "1435190"
  },
  {
    "text": "from zookeeper consumer any lags stuck or unconsumed partitions as well as",
    "start": "1435190",
    "end": "1442700"
  },
  {
    "text": "heart beating the one thing that it does not do is it does not track the flow of",
    "start": "1442700",
    "end": "1448640"
  },
  {
    "text": "messages in the pipeline so that's one area that we want to work on to be able to track messages as they go through the",
    "start": "1448640",
    "end": "1455780"
  },
  {
    "text": "pipeline on the benchmarking you're able to produce tens of thousands of messages",
    "start": "1455780",
    "end": "1463010"
  },
  {
    "text": "single instance we will be open sourcing this along with a few other things here",
    "start": "1463010",
    "end": "1468380"
  },
  {
    "text": "shortly so so here's some of the monitoring their as I mentioned this is",
    "start": "1468380",
    "end": "1474800"
  },
  {
    "text": "broker consumer you could see lag stuck",
    "start": "1474800",
    "end": "1480620"
  },
  {
    "text": "consumers unconsumed partitions offsets",
    "start": "1480620",
    "end": "1485470"
  },
  {
    "start": "1486000",
    "end": "1586000"
  },
  {
    "text": "so great we see what's going on we see something's not working what's typical",
    "start": "1486160",
    "end": "1491750"
  },
  {
    "text": "way you deal with something get an alert you look at it you look at some graphs",
    "start": "1491750",
    "end": "1496910"
  },
  {
    "text": "you login or we have some automation so Winston is a new internal service that's",
    "start": "1496910",
    "end": "1506630"
  },
  {
    "text": "being developed by one of the other platform teams it's an automation engine and so what Winston does it collects",
    "start": "1506630",
    "end": "1513080"
  },
  {
    "text": "diagnostic information and then it acts on that so the intent of Winston is to",
    "start": "1513080",
    "end": "1518210"
  },
  {
    "text": "make things self heal there's a lot of times where you have very specific run",
    "start": "1518210",
    "end": "1524270"
  },
  {
    "text": "books for how you resolve an issue and people do it over and over and over the intent here is take the human part out",
    "start": "1524270",
    "end": "1531770"
  },
  {
    "text": "of that if Winston doesn't know what to do then you get notified",
    "start": "1531770",
    "end": "1538120"
  },
  {
    "text": "so a couple keys here it reduces mean time to recover once again getting paged",
    "start": "1538250",
    "end": "1544950"
  },
  {
    "text": "for something that you could have some service take care for you and overall developer productivity there is a",
    "start": "1544950",
    "end": "1552300"
  },
  {
    "text": "meet-up so for folks that are interested in automation remediation we are",
    "start": "1552300",
    "end": "1558210"
  },
  {
    "text": "supposed to meet up in November mid-november for that as well so if anyone's in the Bay Area so this is a",
    "start": "1558210",
    "end": "1565650"
  },
  {
    "text": "typical winston alert so what happened here was a broker was reported to be",
    "start": "1565650",
    "end": "1572250"
  },
  {
    "text": "offline Winston took a look and said cool it's a nun running state but it's",
    "start": "1572250",
    "end": "1578370"
  },
  {
    "text": "not there so let's restart it so no required zero interaction from us as we",
    "start": "1578370",
    "end": "1587970"
  },
  {
    "text": "mentioned about visualizations this is kind of our start into this so we wanted",
    "start": "1587970",
    "end": "1593790"
  },
  {
    "text": "to have a way to be able to quickly visualize the help of our systems so",
    "start": "1593790",
    "end": "1599910"
  },
  {
    "text": "here's a view one of the region's us east of our clusters so everything looks",
    "start": "1599910",
    "end": "1605250"
  },
  {
    "text": "mostly green you see one of them log trace it's starting to turn a little red what this does allow us to do is to",
    "start": "1605250",
    "end": "1611970"
  },
  {
    "text": "quickly look at it and drill down so we could go from clusters to brokers and we",
    "start": "1611970",
    "end": "1618900"
  },
  {
    "text": "could see what's going on if there's any brokers or having issues and then we",
    "start": "1618900",
    "end": "1625170"
  },
  {
    "text": "could drill down to topics so this is a quick way for us to visualize what's",
    "start": "1625170",
    "end": "1632430"
  },
  {
    "text": "happening in our systems so if we look",
    "start": "1632430",
    "end": "1641610"
  },
  {
    "start": "1636000",
    "end": "1850000"
  },
  {
    "text": "at near-term right now we haven't done a",
    "start": "1641610",
    "end": "1646860"
  },
  {
    "text": "lot around performance tuning and optimizations on the samsa job container",
    "start": "1646860",
    "end": "1653670"
  },
  {
    "text": "piece we think with proper been packing we could have significant improvement",
    "start": "1653670",
    "end": "1659530"
  },
  {
    "text": "same thing on the kafka side so right now we're looking at EBS I think there",
    "start": "1659530",
    "end": "1666350"
  },
  {
    "text": "was a crowd strike was one who just did a talk on using EBS back for Cassandra",
    "start": "1666350",
    "end": "1671540"
  },
  {
    "text": "so we're looking at gp2 we've run some preliminary testing on it and we",
    "start": "1671540",
    "end": "1676910"
  },
  {
    "text": "actually had much better replication performance less lag out of sync partitions then we do with the d2's so",
    "start": "1676910",
    "end": "1683600"
  },
  {
    "text": "that'll be something that I want to experiment over the next quarter to see what kind of performance gains we can",
    "start": "1683600",
    "end": "1688760"
  },
  {
    "text": "game there so self-service we showed",
    "start": "1688760",
    "end": "1696500"
  },
  {
    "text": "some of the visualizations and one of the things that we as a team want to do",
    "start": "1696500",
    "end": "1702110"
  },
  {
    "text": "is we want to remove ourselves from being critical path or blocking for developers we want to provide tooling to",
    "start": "1702110",
    "end": "1708650"
  },
  {
    "text": "allow our developers to do what they need to do you need to create a topic you're able to do that you need to",
    "start": "1708650",
    "end": "1714110"
  },
  {
    "text": "create new routing rules you can do that yourself so building self-service tools",
    "start": "1714110",
    "end": "1719780"
  },
  {
    "text": "around what we have right now it's still very much people requesting for us to do",
    "start": "1719780",
    "end": "1726500"
  },
  {
    "text": "things and not just that's that becomes a blocking think it's we become a gating factor schemas and registries we don't",
    "start": "1726500",
    "end": "1737780"
  },
  {
    "text": "do schemas right now yeah good look we've been talking about certain types",
    "start": "1737780",
    "end": "1745100"
  },
  {
    "text": "of events being able to support schemas registering event once again this kind",
    "start": "1745100",
    "end": "1750560"
  },
  {
    "text": "of falls into the self-service piece you go in register event provide your schema the issue we want to try to solve is",
    "start": "1750560",
    "end": "1758860"
  },
  {
    "text": "creating a contract between producers and consumers right now we find",
    "start": "1758860",
    "end": "1763970"
  },
  {
    "text": "ourselves in situations where someone will change something and it breaks down stream and no one knows so this is a big",
    "start": "1763970",
    "end": "1771290"
  },
  {
    "text": "area that we'll be focusing on over the next quarter discovering visualization",
    "start": "1771290",
    "end": "1778960"
  },
  {
    "text": "so the one area that we have a pretty",
    "start": "1778960",
    "end": "1785240"
  },
  {
    "text": "big gap is people knowing exactly what events are flowing through the system",
    "start": "1785240",
    "end": "1790880"
  },
  {
    "text": "it's critical now but as we move a little bit further on when we start",
    "start": "1790880",
    "end": "1796130"
  },
  {
    "text": "going more stream processing this becomes more critical so one of the things we want to focus on is how do I",
    "start": "1796130",
    "end": "1802040"
  },
  {
    "text": "discover what's there right now there's really no good way for people to take a",
    "start": "1802040",
    "end": "1808400"
  },
  {
    "text": "step back and say oh okay this event already exists or we have this so I could use this so extending the work",
    "start": "1808400",
    "end": "1816080"
  },
  {
    "text": "that we've done on visualizations as well as adding a discovery service we",
    "start": "1816080",
    "end": "1824480"
  },
  {
    "text": "mentioned open sourcing auditor that will be happy named shortly we're also",
    "start": "1824480",
    "end": "1830750"
  },
  {
    "text": "going to be putting together a series of blog posts kind of digging into each of these areas a little bit deeper so",
    "start": "1830750",
    "end": "1838340"
  },
  {
    "text": "that's best practices instance types performance numbers so look out for",
    "start": "1838340",
    "end": "1844430"
  },
  {
    "text": "those will be doing that over the next next month or so as well long term so",
    "start": "1844430",
    "end": "1854510"
  },
  {
    "start": "1850000",
    "end": "1988000"
  },
  {
    "text": "over the next 12 months one of the things that we're looking at doing is moving more towards a real-time data",
    "start": "1854510",
    "end": "1861380"
  },
  {
    "text": "stream and stream processing Network so what does that mean right now they're starting to be a lot more a lot more",
    "start": "1861380",
    "end": "1868390"
  },
  {
    "text": "desire around Kafka and so what we're",
    "start": "1868390",
    "end": "1873590"
  },
  {
    "text": "looking at doing is how do we offer a general messaging service that it's",
    "start": "1873590",
    "end": "1879170"
  },
  {
    "text": "multi-tenant handles queues streams notifications same thing on the stream",
    "start": "1879170",
    "end": "1885530"
  },
  {
    "text": "processing side we want a general-purpose multi tennis stream processing system that enables people to",
    "start": "1885530",
    "end": "1890780"
  },
  {
    "text": "do things like joins filters any of their et al work normalization whatever",
    "start": "1890780",
    "end": "1898070"
  },
  {
    "text": "the case may be over multiple data data sets if you look at Keystone Keystone",
    "start": "1898070",
    "end": "1904700"
  },
  {
    "text": "really is just a product that's built on top of these services right so right now",
    "start": "1904700",
    "end": "1910250"
  },
  {
    "text": "it's a paved path but the paved path is kind of limited as far as where you can",
    "start": "1910250",
    "end": "1915380"
  },
  {
    "text": "send things people may not want to produce an event and have it go to s3 year they may want to just elasticsearch",
    "start": "1915380",
    "end": "1921170"
  },
  {
    "text": "or they may want to do point-to-point communication and so what we're looking at evolving",
    "start": "1921170",
    "end": "1926600"
  },
  {
    "text": "into is more of providing those underlying services with Keystone being",
    "start": "1926600",
    "end": "1931640"
  },
  {
    "text": "a product that's built on top of those on the stream processing that's actually",
    "start": "1931640",
    "end": "1940670"
  },
  {
    "text": "one area that's a little bit fuzzier in the sense that there's a lot of competing technologies out there one of",
    "start": "1940670",
    "end": "1948380"
  },
  {
    "text": "the challenges that we have is how do we have something that's able to handle six and a half million events per second",
    "start": "1948380",
    "end": "1955510"
  },
  {
    "text": "consume them and do something with it the other is this goes back to the",
    "start": "1955510",
    "end": "1960980"
  },
  {
    "text": "visualization and discovery if I take an event and I ain't rich it and I",
    "start": "1960980",
    "end": "1967280"
  },
  {
    "text": "republish it within the pipeline how do I discover that it's there someone may",
    "start": "1967280",
    "end": "1972740"
  },
  {
    "text": "have already done a transformation for something that's critical and I don't have to do that now so that's some of",
    "start": "1972740",
    "end": "1979040"
  },
  {
    "text": "the areas that we're looking at over the next I'd say six to twelve months is evolving more into more general purpose",
    "start": "1979040",
    "end": "1986210"
  },
  {
    "text": "messaging and stream processing",
    "start": "1986210",
    "end": "1989649"
  }
]