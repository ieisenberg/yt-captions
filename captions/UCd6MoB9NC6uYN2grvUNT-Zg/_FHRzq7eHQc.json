[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "good afternoon Vegas how's everybody doing today very nice I can see that lots of",
    "start": "5160",
    "end": "13200"
  },
  {
    "text": "people is still living because you know Vegas",
    "start": "13200",
    "end": "18920"
  },
  {
    "text": "so today I'm going to talk about how to maximize the performance of S3 uh in this presentation we're going",
    "start": "18920",
    "end": "25960"
  },
  {
    "text": "to provide you some uh best practices in terms of architecture how to perform our gets your puts and how to Leverage The",
    "start": "25960",
    "end": "33200"
  },
  {
    "text": "Power of other services that AWS provide together with uh S3 and also I have here together with me",
    "start": "33200",
    "end": "40480"
  },
  {
    "text": "a customer that is going to present to you his use case at the end of the the presentation",
    "start": "40480",
    "end": "47199"
  },
  {
    "text": "right uh just make sure uh is there any Murphy in this",
    "start": "47199",
    "end": "52760"
  },
  {
    "text": "room no no just making sure because I have some Demos in the presentation and I don't want Murphy's Law to be in my",
    "start": "52760",
    "end": "59120"
  },
  {
    "text": "session so that's fine let's roll um",
    "start": "59120",
    "end": "65799"
  },
  {
    "start": "63000",
    "end": "91000"
  },
  {
    "text": "so comparing 2013 against 2014 we had a",
    "start": "65799",
    "end": "71479"
  },
  {
    "text": "137% increase in the awss street usage that's huge and we're not including the",
    "start": "71479",
    "end": "78799"
  },
  {
    "text": "usage of uh AWS uh objects it means that",
    "start": "78799",
    "end": "83920"
  },
  {
    "text": "this increase is being made by you guys thank you that's nice",
    "start": "83920",
    "end": "91799"
  },
  {
    "start": "91000",
    "end": "153000"
  },
  {
    "text": "so here's the agenda of our presentation we're going to begin with",
    "start": "92000",
    "end": "97560"
  },
  {
    "text": "architecture where I'm going to share with you uh some best practice on key",
    "start": "97560",
    "end": "103759"
  },
  {
    "text": "naming how to choose the best region to put your objects and uh considering list is when",
    "start": "103759",
    "end": "111600"
  },
  {
    "text": "using your S3 also I'm going to talk after that about how to optimize your put okay and",
    "start": "111600",
    "end": "120079"
  },
  {
    "text": "how to use multiart uploads to leverage the benefits of pars and we have a demo after that",
    "start": "120079",
    "end": "128399"
  },
  {
    "text": "hopefully and uh right after that we're going to talk about how to optimize gats",
    "start": "128399",
    "end": "134360"
  },
  {
    "text": "how to use AWS Cloud front to Leverage The Power of S3 uh how to perform range",
    "start": "134360",
    "end": "141040"
  },
  {
    "text": "based GS and also hopefully another demo and at the end we're going to have",
    "start": "141040",
    "end": "147680"
  },
  {
    "text": "our customer case presented by big data Corp uh to run",
    "start": "147680",
    "end": "152800"
  },
  {
    "text": "so let's begin architecture uh there are a few",
    "start": "152800",
    "end": "159159"
  },
  {
    "start": "153000",
    "end": "199000"
  },
  {
    "text": "variables that we need to take into consideration when we're we're thinking about start using S3 for a certain",
    "start": "159159",
    "end": "166840"
  },
  {
    "text": "workload or to redesign some workload that we already have one of the things that we take into",
    "start": "166840",
    "end": "173640"
  },
  {
    "text": "consideration is performance and when we think about performance we need to think",
    "start": "173640",
    "end": "178720"
  },
  {
    "text": "about uh how close this S3 region is to your users or the workload we are you",
    "start": "178720",
    "end": "184440"
  },
  {
    "text": "are planning to you know put inside AWS and also put it as closest as",
    "start": "184440",
    "end": "192000"
  },
  {
    "text": "possible at the workload that you are currently running inside AWS so this is",
    "start": "192000",
    "end": "198760"
  },
  {
    "text": "performance the sorry the other things to think about are legal and Regulatory",
    "start": "198760",
    "end": "204560"
  },
  {
    "start": "199000",
    "end": "229000"
  },
  {
    "text": "compl uh compliance because we have some certain workloads that requires that the data",
    "start": "204560",
    "end": "211720"
  },
  {
    "text": "resides inside the country the data is not allowed to live the country that is",
    "start": "211720",
    "end": "216920"
  },
  {
    "text": "a reasonable uh consideration to think about and also that costs Vari by region",
    "start": "216920",
    "end": "223120"
  },
  {
    "text": "so certain regions have lower costs than things like",
    "start": "223120",
    "end": "229040"
  },
  {
    "start": "229000",
    "end": "278000"
  },
  {
    "text": "that uh thinking about architecture one of the most important things that we",
    "start": "229080",
    "end": "235760"
  },
  {
    "text": "always discuss with our customers is the naming is skem",
    "start": "235760",
    "end": "241720"
  },
  {
    "text": "so if you want consistent performance from your",
    "start": "242120",
    "end": "247360"
  },
  {
    "text": "bucket if you want your bucket to be capable of routinely exceeding 100 DPS",
    "start": "247360",
    "end": "254760"
  },
  {
    "text": "and 100 DPS is a lot uh we advise you always to take a",
    "start": "254760",
    "end": "261000"
  },
  {
    "text": "look at our public documentation where we have uh considerations around request rate and performance",
    "start": "261000",
    "end": "267320"
  },
  {
    "text": "considerations there we have lots of explanations and tons of tips about how",
    "start": "267320",
    "end": "273440"
  },
  {
    "text": "to create the proper name scheme and things like that",
    "start": "273440",
    "end": "279639"
  },
  {
    "start": "278000",
    "end": "445000"
  },
  {
    "text": "TPS what does the acronym TPS means TPS means transactions per second simple as",
    "start": "279639",
    "end": "287880"
  },
  {
    "text": "that so let's talk about here an hypothetical application where we have a",
    "start": "287880",
    "end": "293280"
  },
  {
    "text": "bucket we have a mobile device actually hundreds of thousands of",
    "start": "293280",
    "end": "300320"
  },
  {
    "text": "mobile device using your application hopefully and we have behind it also",
    "start": "300320",
    "end": "307039"
  },
  {
    "text": "inside AWS a few ec2 servers running your workers your application your apis",
    "start": "307039",
    "end": "313479"
  },
  {
    "text": "or whatever you need to to run to fulfill your application needs so the mobile",
    "start": "313479",
    "end": "322880"
  },
  {
    "text": "application for example uploads an image to your bucket and this image needs to be",
    "start": "322880",
    "end": "329280"
  },
  {
    "text": "analyzed it or something like that then after this image is an analyzed your ec2",
    "start": "329280",
    "end": "337360"
  },
  {
    "text": "workers generated three other informations that are put on S3 so far we already performed five",
    "start": "337360",
    "end": "347199"
  },
  {
    "text": "TPS against S three and also after these three new information generated by our",
    "start": "347199",
    "end": "353680"
  },
  {
    "text": "workers are put on S3 your mobile application retrieves three new",
    "start": "353680",
    "end": "359680"
  },
  {
    "text": "information from S3 so to fulfill a single event from your mobile",
    "start": "359680",
    "end": "365840"
  },
  {
    "text": "application was needed eight transactions from S3 so thinking about this",
    "start": "365840",
    "end": "374120"
  },
  {
    "text": "application and from the previous slide where we mentioned that uh uh if you",
    "start": "374120",
    "end": "379360"
  },
  {
    "text": "want your bucket to deliver to you 100",
    "start": "379360",
    "end": "384240"
  },
  {
    "text": "TPS in this scenario if we divide 100 TP s by eight events to F to",
    "start": "384400",
    "end": "393080"
  },
  {
    "text": "eight transactions per second to fulfill an event uh in this",
    "start": "393080",
    "end": "398560"
  },
  {
    "text": "scenario you would be able to deliver 12.5 events per second okay without",
    "start": "398560",
    "end": "404800"
  },
  {
    "text": "doing nothing okay just using the regular",
    "start": "404800",
    "end": "410039"
  },
  {
    "text": "S3 okay and when you're application reaches something like 100,000 users at",
    "start": "410039",
    "end": "417000"
  },
  {
    "text": "10 events per hour you would require from your bucket 224 TPS whoa that's",
    "start": "417000",
    "end": "425360"
  },
  {
    "text": "time to think about review your naming scheme and see and make sure that everything is correctly set",
    "start": "425360",
    "end": "432800"
  },
  {
    "text": "up and that's what and that's where the previous slide where I pointed you to",
    "start": "432800",
    "end": "438160"
  },
  {
    "text": "the public documentation that we have on the AWS website comes handy",
    "start": "438160",
    "end": "444680"
  },
  {
    "start": "445000",
    "end": "482000"
  },
  {
    "text": "right so let's talk about Distributing your key names",
    "start": "445000",
    "end": "451120"
  },
  {
    "text": "usually we see something like this okay please don't do this don't put",
    "start": "453599",
    "end": "460000"
  },
  {
    "text": "a consistent naming scheme in front of your object we advise you to never ever",
    "start": "460000",
    "end": "467199"
  },
  {
    "text": "do that okay even if you're expecting your application to not deliver hire",
    "start": "467199",
    "end": "473120"
  },
  {
    "text": "topr or something like that if you can rely from the moment zero in your",
    "start": "473120",
    "end": "478800"
  },
  {
    "text": "application using the best practice use them if you do this this is going to",
    "start": "478800",
    "end": "485280"
  },
  {
    "text": "happen the previous application that I mentioned your application starts growing okay then you have your workers",
    "start": "485280",
    "end": "492479"
  },
  {
    "text": "behind the scenes your your mobile application working then you have your bucket partition behind by AWS and when",
    "start": "492479",
    "end": "501440"
  },
  {
    "text": "the transaction starts coming from your mobile devices to S3 this is going to",
    "start": "501440",
    "end": "507520"
  },
  {
    "text": "happen due to the bad naming scheme all of the transactions are going to hit the",
    "start": "507520",
    "end": "512640"
  },
  {
    "text": "same partition and this is not going to deliver you the expected throughput so",
    "start": "512640",
    "end": "518760"
  },
  {
    "text": "you start to suffering from the throughput okay so what we advise is add",
    "start": "518760",
    "end": "525640"
  },
  {
    "text": "some Randomness to the beginning of your key names when you do something like that as",
    "start": "525640",
    "end": "532440"
  },
  {
    "text": "you can see we have random numbers random algorithms at the beginning of our key names",
    "start": "532440",
    "end": "539959"
  },
  {
    "text": "behind the scenes if you take a look at the same application all of your transactions will be able to be",
    "start": "539959",
    "end": "545920"
  },
  {
    "text": "distributed evenly across the partitions so you have your partitions and the",
    "start": "545920",
    "end": "551480"
  },
  {
    "text": "transactions you'll be able to hit different partitions during the the curreny of an",
    "start": "551480",
    "end": "559040"
  },
  {
    "text": "event okay um also we can talk about other uh",
    "start": "559040",
    "end": "567839"
  },
  {
    "start": "562000",
    "end": "648000"
  },
  {
    "text": "techniques to uh distribute key names not only adding some random names at the beginning of uh your",
    "start": "567839",
    "end": "574959"
  },
  {
    "text": "keys uh usually we have some customers that to store objects as a hash of their",
    "start": "574959",
    "end": "581200"
  },
  {
    "text": "name so they just take the object name compute a hash over the object name and",
    "start": "581200",
    "end": "587120"
  },
  {
    "text": "store the object as the computed hash this is a very reasonable way of",
    "start": "587120",
    "end": "593160"
  },
  {
    "text": "storing objects in S3 so using the example that we have over there the dead mouse and the lineing mix MP3 you would",
    "start": "593160",
    "end": "599959"
  },
  {
    "text": "store this object or something this uh long string that we have over there also",
    "start": "599959",
    "end": "605440"
  },
  {
    "text": "we can use uh the option that we gave you guys in the previous slide uh calculating some hash using the object",
    "start": "605440",
    "end": "613480"
  },
  {
    "text": "name then you take the few first share something like the four five or six",
    "start": "613480",
    "end": "619160"
  },
  {
    "text": "First share from the generated hash you prepare the generated hash in the object",
    "start": "619160",
    "end": "624200"
  },
  {
    "text": "name and then you store it in S3 is also a viable option",
    "start": "624200",
    "end": "630839"
  },
  {
    "text": "and also we have the EPO time reverse so you just take the current time reverse",
    "start": "631200",
    "end": "637639"
  },
  {
    "text": "it and prepend to the object name is also another viable option but this is",
    "start": "637639",
    "end": "644279"
  },
  {
    "text": "good this is awesome but sometimes using this can be an anti",
    "start": "644279",
    "end": "652480"
  },
  {
    "start": "648000",
    "end": "676000"
  },
  {
    "text": "pattern when you have to resort to life cycle policies or list with prefix",
    "start": "652480",
    "end": "659600"
  },
  {
    "text": "computers or maintaining Tubb nails of images or when you need to recover a",
    "start": "659600",
    "end": "666040"
  },
  {
    "text": "file with it uh original name it can become a problem but no problem you're",
    "start": "666040",
    "end": "672240"
  },
  {
    "text": "not alone in there we have a solution for",
    "start": "672240",
    "end": "676320"
  },
  {
    "text": "this you can use you can prepend to the object name what we call a folder we can",
    "start": "679120",
    "end": "686440"
  },
  {
    "text": "put a folder and put the random object name inside this",
    "start": "686440",
    "end": "691880"
  },
  {
    "text": "folder this way you're going to be able to the benefit of um transactions per",
    "start": "691880",
    "end": "698480"
  },
  {
    "text": "second that S3 is going to provide you by adding some Randomness at the beginning of your objects but but also",
    "start": "698480",
    "end": "706040"
  },
  {
    "text": "you would be able to still resort to lists and other operations in your S3",
    "start": "706040",
    "end": "711279"
  },
  {
    "text": "bucket that needs to rely on the name of the objects that we have",
    "start": "711279",
    "end": "717320"
  },
  {
    "start": "718000",
    "end": "773000"
  },
  {
    "text": "uh so like I told you guys Distributing ke names even when you think you don't need",
    "start": "720680",
    "end": "728880"
  },
  {
    "text": "it it's always a good idea okay even in when you're using the best",
    "start": "728880",
    "end": "736000"
  },
  {
    "text": "practice it can take some time to S3 to reflect the changes that you made to",
    "start": "736000",
    "end": "741600"
  },
  {
    "text": "your bucket Okay and like I told told you uh 100 TPS",
    "start": "741600",
    "end": "748639"
  },
  {
    "text": "is a lot that's a lot of transactions to exceed 100 TPS it's a",
    "start": "748639",
    "end": "755399"
  },
  {
    "text": "lot but even so in case you expect more than that once again the documentation",
    "start": "755399",
    "end": "763600"
  },
  {
    "text": "that we have on AWS has tons of tips for you guys to create the best naming",
    "start": "763600",
    "end": "768839"
  },
  {
    "text": "scheme for your user case scenario okay and you're not alone uh when you're",
    "start": "768839",
    "end": "777000"
  },
  {
    "start": "773000",
    "end": "854000"
  },
  {
    "text": "revi reviewing uh any design that you already have or",
    "start": "777000",
    "end": "782279"
  },
  {
    "text": "you're planning the design of the new application that your company is",
    "start": "782279",
    "end": "787360"
  },
  {
    "text": "planning to launch on the market or you have some crazy idea to create a startup",
    "start": "787360",
    "end": "792399"
  },
  {
    "text": "anything you're not alone like verer vogo said today in our presentation AWS",
    "start": "792399",
    "end": "798560"
  },
  {
    "text": "is not just AWS we are much more with our partners and our Marketplace so",
    "start": "798560",
    "end": "805040"
  },
  {
    "text": "before starting to create something from the scratch please take a look at our",
    "start": "805040",
    "end": "811600"
  },
  {
    "text": "Marketplace okay and see if we don't already have any other solution that can",
    "start": "811600",
    "end": "817839"
  },
  {
    "text": "fulfill your requirements",
    "start": "817839",
    "end": "821680"
  },
  {
    "text": "okay and for any solution that you find on the AWS Marketplace you don't have to",
    "start": "823040",
    "end": "830240"
  },
  {
    "text": "start paying from the moment zero almost all of them they provide you something",
    "start": "830240",
    "end": "835720"
  },
  {
    "text": "like a free trial period that you can start using without paying nothing and",
    "start": "835720",
    "end": "840839"
  },
  {
    "text": "some of them provide some like premium they provide their product but with less",
    "start": "840839",
    "end": "847440"
  },
  {
    "text": "features and something like that and you can pay everything together in your AWS bill you don't have to worry",
    "start": "847440",
    "end": "855959"
  },
  {
    "start": "854000",
    "end": "887000"
  },
  {
    "text": "about that so we talked about architecture so let's talk about now um how to optimize",
    "start": "855959",
    "end": "864639"
  },
  {
    "text": "for uh puts usually when you're putting your",
    "start": "864639",
    "end": "869759"
  },
  {
    "text": "objects into S3 you just use the regular SDK the console the CLI whatever and",
    "start": "869759",
    "end": "875720"
  },
  {
    "text": "you're putting objects into S3 but usually when you're using the SDK people are resorting to the standard way of",
    "start": "875720",
    "end": "882360"
  },
  {
    "text": "putting objects into S3 they're not using the mot part up load okay and here",
    "start": "882360",
    "end": "888279"
  },
  {
    "start": "887000",
    "end": "943000"
  },
  {
    "text": "I would like to talk about with you guys about the benefits of paralyzing your",
    "start": "888279",
    "end": "893399"
  },
  {
    "text": "puts okay uh when you paralyze your puts you're able to increase the aggregated",
    "start": "893399",
    "end": "900160"
  },
  {
    "text": "through put by paralyzing your uploads on higher bandwith networks and when you",
    "start": "900160",
    "end": "907079"
  },
  {
    "text": "do that you're able to move the button act to where it belongs the io button act to where it belongs so it goes to",
    "start": "907079",
    "end": "913120"
  },
  {
    "text": "the network network will take care of that",
    "start": "913120",
    "end": "918160"
  },
  {
    "text": "right and also when you use multiart uploads let's say that you're uploading a 1 gig file to",
    "start": "918160",
    "end": "925440"
  },
  {
    "text": "AWS if you're doing it in the standard way if in the middle of the transaction",
    "start": "925440",
    "end": "931120"
  },
  {
    "text": "the load fails you have to start it from the scratch again so you lost all of the",
    "start": "931120",
    "end": "937680"
  },
  {
    "text": "bytes that you already uploaded and using M part",
    "start": "937680",
    "end": "943199"
  },
  {
    "start": "943000",
    "end": "1001000"
  },
  {
    "text": "uploads you can benefit from faster and flexible uploads you can break your object into a",
    "start": "943279",
    "end": "951319"
  },
  {
    "text": "set of parts and upload them sequentially or",
    "start": "951319",
    "end": "957560"
  },
  {
    "text": "parly and and after all of the parts are into S3 S3 is able to present all of the",
    "start": "957759",
    "end": "965120"
  },
  {
    "text": "parts together to compose their original and single object and another cool benefit of",
    "start": "965120",
    "end": "972120"
  },
  {
    "text": "parallel uploads is that it allows your application to pause and resume the",
    "start": "972120",
    "end": "977360"
  },
  {
    "text": "uploads of your objects so that's very nice so if you have to upload again the",
    "start": "977360",
    "end": "983720"
  },
  {
    "text": "same example that one gig file and you at the moment that you already uploaded 5 00 gigs 500 megabytes you can pause",
    "start": "983720",
    "end": "992720"
  },
  {
    "text": "the upload and resume transferring the other parts afterwards at any moment that you want",
    "start": "992720",
    "end": "1000560"
  },
  {
    "text": "right but when we talk about mode part that loads we need to think about how do",
    "start": "1002079",
    "end": "1009480"
  },
  {
    "text": "I choose the minimum part we always advise your customers to",
    "start": "1009480",
    "end": "1014839"
  },
  {
    "text": "start with parts of 25 or 50 megabytes when we're talking about higher",
    "start": "1014839",
    "end": "1021120"
  },
  {
    "text": "bandwidth networks but uh when you're considering your mobile applications you",
    "start": "1021120",
    "end": "1026558"
  },
  {
    "text": "should start trying with 10 megabytes or so something like that this is not a rule of T right but this is a good",
    "start": "1026559",
    "end": "1035280"
  },
  {
    "text": "start and also you need to strike a balance between the part size and the",
    "start": "1036199",
    "end": "1041880"
  },
  {
    "text": "number of Parts because if you increase the number of Parts you won't benefit of",
    "start": "1041880",
    "end": "1048480"
  },
  {
    "text": "the m part uploads because you put too much overload on your network and if",
    "start": "1048480",
    "end": "1054240"
  },
  {
    "text": "you're uploading fewer parts and larger in size you also won't benefit of",
    "start": "1054240",
    "end": "1060120"
  },
  {
    "text": "multiart upload because if the upload of one of the parts fail you will have to transer a larger part of the object",
    "start": "1060120",
    "end": "1067320"
  },
  {
    "text": "again so we always recommend Benchmark for your",
    "start": "1067320",
    "end": "1073519"
  },
  {
    "start": "1073000",
    "end": "1133000"
  },
  {
    "text": "scenario and uh S3 also provides you the option to use use encryption when you're",
    "start": "1074000",
    "end": "1080280"
  },
  {
    "text": "transferring when you're using the https endpoint so when you're using SSL you're",
    "start": "1080280",
    "end": "1086080"
  },
  {
    "text": "likely to become CPU constraint because the encryption itself uses CPU power",
    "start": "1086080",
    "end": "1091520"
  },
  {
    "text": "encryption is a CPU intensive process so Amazon S3 recommends that you",
    "start": "1091520",
    "end": "1098760"
  },
  {
    "text": "whenever you need to use encryption on your files use as 256 to optimize for",
    "start": "1098760",
    "end": "1105960"
  },
  {
    "text": "both security and performance and to not become a CPU constraint in terms of",
    "start": "1105960",
    "end": "1113679"
  },
  {
    "text": "encryption you can Leverage The Power of ASN Hardware on your host to improve the",
    "start": "1113679",
    "end": "1119760"
  },
  {
    "text": "performance we have tons of2 instances that already provide this benefit so you",
    "start": "1119760",
    "end": "1125520"
  },
  {
    "text": "can use them to leverage the response times in your application when you need",
    "start": "1125520",
    "end": "1131000"
  },
  {
    "text": "to encrypt the files so Showtime",
    "start": "1131000",
    "end": "1137840"
  },
  {
    "start": "1133000",
    "end": "1319000"
  },
  {
    "text": "let me present you here a quick demo um in this demo it's a very simple demo",
    "start": "1139440",
    "end": "1146799"
  },
  {
    "text": "where I create a dummy file with 500",
    "start": "1146799",
    "end": "1152120"
  },
  {
    "text": "um uh megabytes and in this file I will break",
    "start": "1152120",
    "end": "1157440"
  },
  {
    "text": "this file in uh 20 parts so we will upload each part having 25 megabytes to",
    "start": "1157440",
    "end": "1163559"
  },
  {
    "text": "S3",
    "start": "1163559",
    "end": "1166559"
  },
  {
    "text": "actually the first one is using 10 parts so the first test we're uploading",
    "start": "1171120",
    "end": "1177400"
  },
  {
    "text": "a 500 megabytes in 10 chunks sequentially so it's doing one part",
    "start": "1177400",
    "end": "1184320"
  },
  {
    "text": "after the order let's see how long is it going to take to transfer a 500",
    "start": "1184320",
    "end": "1190600"
  },
  {
    "text": "megabytes file to S3 if Murphy isn't doing nothing right",
    "start": "1190600",
    "end": "1198240"
  },
  {
    "text": "now it should complete oops completed so that's a little game right almost uh",
    "start": "1198240",
    "end": "1204520"
  },
  {
    "text": "what can you say 700% or something like that so to transfer the same exact file",
    "start": "1204520",
    "end": "1211799"
  },
  {
    "text": "in 10 chunks using sequential uploads it took 21 seconds and the same file using",
    "start": "1211799",
    "end": "1219080"
  },
  {
    "text": "parallel uploads it took 3 seconds just 3 seconds and what I was telling you guys",
    "start": "1219080",
    "end": "1226120"
  },
  {
    "text": "about um Benchmark for your best case scenario",
    "start": "1226120",
    "end": "1232840"
  },
  {
    "text": "let's try to transfer the same file again but using 20 chunks not 10 chunks",
    "start": "1232840",
    "end": "1240039"
  },
  {
    "text": "let's use 20 chunks and let's see what happens in this case",
    "start": "1240039",
    "end": "1246279"
  },
  {
    "text": "20 seconds never took so",
    "start": "1260600",
    "end": "1264000"
  },
  {
    "text": "long Murph is",
    "start": "1267559",
    "end": "1271120"
  },
  {
    "text": "working okay so it variz probably do some Network variation or something like",
    "start": "1282880",
    "end": "1288360"
  },
  {
    "text": "that but as you can see using 10 parts it took 21 seconds in sequential and 3",
    "start": "1288360",
    "end": "1295559"
  },
  {
    "text": "seconds in parallel and the same file using 20 chunks uh we decrease the we",
    "start": "1295559",
    "end": "1301240"
  },
  {
    "text": "increase the time from 3 seconds in parallel to 5 seconds in parallel so",
    "start": "1301240",
    "end": "1306400"
  },
  {
    "text": "that's why we always advise you Benchmark for your use case you start",
    "start": "1306400",
    "end": "1311480"
  },
  {
    "text": "with the numbers that we suggest you guys and then adjust up or down according to your scenario right",
    "start": "1311480",
    "end": "1320000"
  },
  {
    "text": "um good so let's talk about how to optimize for",
    "start": "1320159",
    "end": "1327158"
  },
  {
    "text": "GS there's a few ways of how optimize S3 forgets you can Leverage The Power of",
    "start": "1329400",
    "end": "1336039"
  },
  {
    "text": "our uh content deliver Network called cloudfront where we",
    "start": "1336039",
    "end": "1341159"
  },
  {
    "text": "have if AWS didn't launch any other Pop I believe it's five 55 Edge locations",
    "start": "1341159",
    "end": "1348000"
  },
  {
    "text": "around the world world so you can use uh uh Cloud front in front of your S3",
    "start": "1348000",
    "end": "1353919"
  },
  {
    "text": "bucket to distribute your content using the nearest location to your user so it",
    "start": "1353919",
    "end": "1359440"
  },
  {
    "text": "will speed up a lot the distribution of your content okay S3 Cloud front basically",
    "start": "1359440",
    "end": "1367080"
  },
  {
    "text": "caches the objects of S3 that you transferring the nearest location to the user so it will be it will provide a",
    "start": "1367080",
    "end": "1374240"
  },
  {
    "text": "faster and reliable experience to your user right and you also reduce the",
    "start": "1374240",
    "end": "1380039"
  },
  {
    "text": "number of GS that you issue against S3 because once the object gets cached in",
    "start": "1380039",
    "end": "1386279"
  },
  {
    "text": "the nearest Edge location you will save a few round trips from cloud front to S3",
    "start": "1386279",
    "end": "1392240"
  },
  {
    "text": "from the user to S3 actually okay and Cloud front provides",
    "start": "1392240",
    "end": "1398279"
  },
  {
    "text": "low latency end points so downloading the same object",
    "start": "1398279",
    "end": "1403520"
  },
  {
    "text": "from S3 even if the user is closes close to the region that you have your S3 bucket",
    "start": "1403520",
    "end": "1410200"
  },
  {
    "text": "using S3 or Cloud front you provide using Cloud front instead of S3 directly",
    "start": "1410200",
    "end": "1415720"
  },
  {
    "text": "will provide a better experience for your user and also it provides a higher",
    "start": "1415720",
    "end": "1421600"
  },
  {
    "text": "transfer hate and Cloud front you can use it to distribute content from S3 not",
    "start": "1421600",
    "end": "1427320"
  },
  {
    "text": "not only images or stack files like JS files CSS files or whatever static files",
    "start": "1427320",
    "end": "1433559"
  },
  {
    "text": "you're putting on stre you can use also uh Cloud front to distribute a live",
    "start": "1433559",
    "end": "1438720"
  },
  {
    "text": "content or uh OnDemand video content so let's present another demo",
    "start": "1438720",
    "end": "1444240"
  },
  {
    "start": "1442000",
    "end": "1573000"
  },
  {
    "text": "for you guys where I show the difference in performance",
    "start": "1444240",
    "end": "1449720"
  },
  {
    "text": "between um S3 and Cloud front basically in this",
    "start": "1449720",
    "end": "1456440"
  },
  {
    "text": "demo I'm going to issue um few gats against five gats",
    "start": "1456440",
    "end": "1461559"
  },
  {
    "text": "basically against S three and five gats against Cloud front to the same object and here you can see the difference",
    "start": "1461559",
    "end": "1468559"
  },
  {
    "text": "oops wrong demo",
    "start": "1468559",
    "end": "1471919"
  },
  {
    "text": "sorry this is",
    "start": "1473960",
    "end": "1477240"
  },
  {
    "text": "it so I'm basically trying to get a one andb sample file from S3 and it took 3.8",
    "start": "1483159",
    "end": "1491000"
  },
  {
    "text": "seconds 3.6 seconds and now three 5 Seconds",
    "start": "1491000",
    "end": "1499240"
  },
  {
    "text": "two more",
    "start": "1499720",
    "end": "1502200"
  },
  {
    "text": "times and you can see the difference between downloading the file from S3 and downloading the file from cloud front",
    "start": "1506399",
    "end": "1512960"
  },
  {
    "text": "that's another huge gam so always try to consider using Cloud",
    "start": "1512960",
    "end": "1519039"
  },
  {
    "text": "front in front of S3 to download the files that you need from my street",
    "start": "1519039",
    "end": "1525960"
  },
  {
    "text": "but also when you take talk about gats you can paralyze your GS too because the same way you paralyze motor part uploads",
    "start": "1535720",
    "end": "1542360"
  },
  {
    "text": "you can paralyze your gats and you can benefit from the multi-threaded performance for",
    "start": "1542360",
    "end": "1548159"
  },
  {
    "text": "downloading object from S3 and the same benefits that you gain when you use multipart uploads using",
    "start": "1548159",
    "end": "1555080"
  },
  {
    "text": "range based gets you can compensate for unreel iable",
    "start": "1555080",
    "end": "1560720"
  },
  {
    "text": "networks and you benefit for multi- traded parallelism and just make sure",
    "start": "1560720",
    "end": "1566840"
  },
  {
    "text": "that you you're align your ranges uh with your parts okay and uh let's show",
    "start": "1566840",
    "end": "1574880"
  },
  {
    "text": "another demo for you guys in this demo I do the same thing that I did for the multipart upload but I do using uh range",
    "start": "1574880",
    "end": "1583520"
  },
  {
    "text": "based GS so the same two objects that I generated with you guys in the same demo",
    "start": "1583520",
    "end": "1590000"
  },
  {
    "text": "on S3 now I'm going to run this demo and we will try to download this objects using",
    "start": "1590000",
    "end": "1596799"
  },
  {
    "text": "sequential and parallel gats so let's do",
    "start": "1596799",
    "end": "1603440"
  },
  {
    "text": "it so first we're going to we're trying to download a 500 megabytes object in 20",
    "start": "1603440",
    "end": "1612360"
  },
  {
    "text": "chunks sequentially so it took 9 seconds",
    "start": "1614279",
    "end": "1620159"
  },
  {
    "text": "so that's a huge game",
    "start": "1626440",
    "end": "1632159"
  },
  {
    "text": "again so imagine",
    "start": "1633840",
    "end": "1638080"
  },
  {
    "start": "1634000",
    "end": "1683000"
  },
  {
    "text": "bringing S3 with range-based GS and",
    "start": "1638960",
    "end": "1644919"
  },
  {
    "text": "uh Cloud front toer application the huge gains that you can bring to your",
    "start": "1644919",
    "end": "1650640"
  },
  {
    "text": "application to your end user experience that's very",
    "start": "1650640",
    "end": "1657120"
  },
  {
    "text": "nice but also when we're talking about getting content from S3 there's another",
    "start": "1658000",
    "end": "1663720"
  },
  {
    "text": "operation that is heavily used called list so sometimes you have to get the",
    "start": "1663720",
    "end": "1670159"
  },
  {
    "text": "object that your customer stored see all the lists of the file that are storying",
    "start": "1670159",
    "end": "1675519"
  },
  {
    "text": "on S3 getting a list of logs viewing inventories or things like that starting",
    "start": "1675519",
    "end": "1680960"
  },
  {
    "text": "Keys based on metadata what should you do in this case you can also paralyze your list",
    "start": "1680960",
    "end": "1690679"
  },
  {
    "start": "1683000",
    "end": "1729000"
  },
  {
    "text": "operations if you have a list of your keys you can paralyze them and benefit from the same gains from multiart of",
    "start": "1690679",
    "end": "1697039"
  },
  {
    "text": "loads and range based gaps but also what you could do you could do a secondary index instead of",
    "start": "1697039",
    "end": "1705039"
  },
  {
    "text": "relying on the S3 list operation which which is a heavy operation you could use",
    "start": "1705039",
    "end": "1710559"
  },
  {
    "text": "a secondary list and a secondary list is able to provide you sorting by metadata",
    "start": "1710559",
    "end": "1716640"
  },
  {
    "text": "search and have objects by time stamp it provides you uh features that uh the",
    "start": "1716640",
    "end": "1724519"
  },
  {
    "text": "plain S3 list operation is not able to provide you so inside AWS we have a few services",
    "start": "1724519",
    "end": "1732679"
  },
  {
    "start": "1729000",
    "end": "1782000"
  },
  {
    "text": "that can help you guys to bring the power of second list indexes into your application either using Dynamo DB RDS",
    "start": "1732679",
    "end": "1740640"
  },
  {
    "text": "Cloud search ec2 or any other database or solution",
    "start": "1740640",
    "end": "1748200"
  },
  {
    "text": "that you might have running inside ofu whatever you like but uh when you use",
    "start": "1748200",
    "end": "1754399"
  },
  {
    "text": "secondary indexes you don't have to rely on S3 for the heavy lease operations that we have and you can bring lots of",
    "start": "1754399",
    "end": "1761159"
  },
  {
    "text": "features that the plain this operation don't offer right thank you guys for your time I'm",
    "start": "1761159",
    "end": "1768440"
  },
  {
    "text": "going to leave you with tan right now that he's going to explain his use case about all of the practice that we talked",
    "start": "1768440",
    "end": "1775279"
  },
  {
    "text": "during this session and explain to you how he started and where he is right now",
    "start": "1775279",
    "end": "1781320"
  },
  {
    "text": "thank",
    "start": "1781320",
    "end": "1783518"
  },
  {
    "start": "1782000",
    "end": "1818000"
  },
  {
    "text": "you hello hi everyone uh good afternoon I'm torren and I'm the founder and CEO",
    "start": "1787159",
    "end": "1794399"
  },
  {
    "text": "of Big Data carp where a company based in Brazil which we we're we've been in the market for about 3 years and our",
    "start": "1794399",
    "end": "1802240"
  },
  {
    "text": "focus is mostly on uh data capture and data generation so we do a lot of",
    "start": "1802240",
    "end": "1807360"
  },
  {
    "text": "crawling a lot of things like that and that sort of directly ties into the",
    "start": "1807360",
    "end": "1812600"
  },
  {
    "text": "problem that we were facing and and how we use that tree to solve it",
    "start": "1812600",
    "end": "1818200"
  },
  {
    "text": "so basically we have a very large scale crawling process where at least once a",
    "start": "1818200",
    "end": "1825279"
  },
  {
    "text": "week we're hitting tens of millions of websites and capturing the HTML content of these websites and this generates",
    "start": "1825279",
    "end": "1832159"
  },
  {
    "text": "hundreds of billions of web pages and that amount of data is is pretty insane",
    "start": "1832159",
    "end": "1839000"
  },
  {
    "text": "to handle it's it's about 500 terabytes per week of raw data data that we",
    "start": "1839000",
    "end": "1844679"
  },
  {
    "text": "generate and have to have to process and parse and do things with and for a while",
    "start": "1844679",
    "end": "1851480"
  },
  {
    "text": "we were working we were trying to work with databases and trying to do some stuff with it and it really wasn't",
    "start": "1851480",
    "end": "1858039"
  },
  {
    "text": "working out very well and it's interesting for me because it's a very different use case from what you would",
    "start": "1858039",
    "end": "1864120"
  },
  {
    "text": "normally think about s3e because usually when you're talking to people about s stre they they're talking about okay",
    "start": "1864120",
    "end": "1869799"
  },
  {
    "text": "let's upload a file and then we'll stream it out or send it out to to several users we have static files that",
    "start": "1869799",
    "end": "1875639"
  },
  {
    "text": "people are downloading multiple times and oura our use case was actually a little bit different because instead of",
    "start": "1875639",
    "end": "1882840"
  },
  {
    "text": "doing a lot of getting on the file instead of getting the file several times and distributing it to a lot of",
    "start": "1882840",
    "end": "1888679"
  },
  {
    "text": "users we had these hundreds of thousands of distributed put requests you know we",
    "start": "1888679",
    "end": "1894919"
  },
  {
    "text": "were saving a ton of files we're saving a ton of objects into S3 that we then needed to do postprocessing and and",
    "start": "1894919",
    "end": "1904080"
  },
  {
    "text": "um work with and so we started out we read the documentation we got in touch",
    "start": "1904080",
    "end": "1909840"
  },
  {
    "text": "with the guys at at AWS and we were talk discussing with them our idea and",
    "start": "1909840",
    "end": "1916559"
  },
  {
    "text": "uh we started working on it and we started building an architecture for",
    "start": "1916559",
    "end": "1921919"
  },
  {
    "text": "it and once we started testing it and actually running some test processes",
    "start": "1921919",
    "end": "1928799"
  },
  {
    "text": "with it we found out that it wasn't working at all we were when when we",
    "start": "1928799",
    "end": "1934039"
  },
  {
    "text": "originally started out our our idea was to store each individual HTML page as a",
    "start": "1934039",
    "end": "1940720"
  },
  {
    "text": "single object on S3 and if you look at it from a postprocessing standpoint that",
    "start": "1940720",
    "end": "1946320"
  },
  {
    "text": "actually makes a lot of sense because if I have each Page's individual HTML stored on S3 then I can apply EMR or any",
    "start": "1946320",
    "end": "1953559"
  },
  {
    "text": "other of the services to really process that data really easily and do a lot of fun stuff with it",
    "start": "1953559",
    "end": "1959679"
  },
  {
    "text": "but um that was actually kind of an issue",
    "start": "1959679",
    "end": "1965399"
  },
  {
    "text": "because we were trying to do billions of put requests and over 300 Sim 300,000",
    "start": "1965399",
    "end": "1973760"
  },
  {
    "text": "simultaneous put requests into our S3 buckets and um when you consider Philipi was",
    "start": "1973760",
    "end": "1982679"
  },
  {
    "text": "talking about when when he started the presentation that as stre can easily support 100 transactions per second we",
    "start": "1982679",
    "end": "1989200"
  },
  {
    "text": "were a couple of orders of magnitude above that and that kind of became an",
    "start": "1989200",
    "end": "1996440"
  },
  {
    "text": "issue um actually the first time that we ran the process we got uh an email saying okay you're you",
    "start": "1996440",
    "end": "2004760"
  },
  {
    "text": "know you're kind of doing you you shouldn't be doing what you're doing we are we we we are let's let's sit down",
    "start": "2004760",
    "end": "2011320"
  },
  {
    "text": "and talk and you know try to work out what what you're trying to do here because that's not going to work and",
    "start": "2011320",
    "end": "2017559"
  },
  {
    "text": "it's understandable right I mean we were using name hashing based on gids uids",
    "start": "2017559",
    "end": "2025399"
  },
  {
    "text": "um how I don't know how many of you are familiar with it uh with GES I think",
    "start": "2025399",
    "end": "2030480"
  },
  {
    "text": "they are sort of the there were sort of the def default mode for generating a a",
    "start": "2030480",
    "end": "2035600"
  },
  {
    "text": "random number to put in front of the of the the key name so we thought okay so we should be pretty safe there uh and",
    "start": "2035600",
    "end": "2044600"
  },
  {
    "text": "you know just putting the data there and then when we needed to process it we would run our processes running list",
    "start": "2044600",
    "end": "2050320"
  },
  {
    "text": "requests get all the file names and the object names that we needed to process and trying to work with that and as I",
    "start": "2050320",
    "end": "2056118"
  },
  {
    "text": "was saying that really didn't work out even though we we were getting a decent compression rate and even though we were",
    "start": "2056119",
    "end": "2062118"
  },
  {
    "text": "getting uh some results it it quickly became obvious to us that that wouldn't scale and that would wouldn't scale on a",
    "start": "2062119",
    "end": "2068800"
  },
  {
    "text": "performance but that also wouldn't scale on a cost perspective and this is something that we really don't think",
    "start": "2068800",
    "end": "2074280"
  },
  {
    "text": "about when when you're but when you're doing a lot of puts that's something that you really need to consider because",
    "start": "2074280",
    "end": "2079560"
  },
  {
    "text": "put requests are quite more expensive than get requests and when you multiply",
    "start": "2079560",
    "end": "2085079"
  },
  {
    "text": "the price of a put request by the number of puts that we're actually trying to",
    "start": "2085079",
    "end": "2090440"
  },
  {
    "text": "do we would be paying about $500,000 a month simply to perform those put",
    "start": "2090440",
    "end": "2096520"
  },
  {
    "text": "requests and that's kind of untenable um in the long run so we had to change our architecture a little bit",
    "start": "2096520",
    "end": "2102960"
  },
  {
    "text": "and we talked a lot with the guys from S3 and with the guys from AWS about that with the Architects and",
    "start": "2102960",
    "end": "2109920"
  },
  {
    "text": "everyone and we ended up changing our architecture to instead of store each",
    "start": "2109920",
    "end": "2117240"
  },
  {
    "text": "individual page as an object in S3 we actually decided to S each individual",
    "start": "2117240",
    "end": "2122800"
  },
  {
    "text": "website that we were crawling so we built a more complex object structure in order order to actually reduce the",
    "start": "2122800",
    "end": "2129160"
  },
  {
    "text": "number of put requests and and afterwards the number of get requests and number of lists that we had to do on",
    "start": "2129160",
    "end": "2134960"
  },
  {
    "text": "top of that and that actually brought with it a a lot a lot",
    "start": "2134960",
    "end": "2141280"
  },
  {
    "text": "more side benefits that we really didn't expect when we started out so first of all we had a huge gain in compression so",
    "start": "2141280",
    "end": "2147680"
  },
  {
    "text": "we we we started out when we were compressing the individual pages with about 60 60 65% compression rate for the",
    "start": "2147680",
    "end": "2154960"
  },
  {
    "text": "individual web pages but once we started bringing all the web pages from from the same s website together and compressing",
    "start": "2154960",
    "end": "2161520"
  },
  {
    "text": "that as a whole we actually ended up with 90% or even more than that compression which really makes a dent",
    "start": "2161520",
    "end": "2167880"
  },
  {
    "text": "when you're talking about storing millions and millions of websites we changed the naming scheme so",
    "start": "2167880",
    "end": "2174560"
  },
  {
    "text": "once again naming scheme is crucial and uh it's one of the the most important",
    "start": "2174560",
    "end": "2181040"
  },
  {
    "text": "things for you to work with uh guids are actually a lousy choice for you to",
    "start": "2181040",
    "end": "2187040"
  },
  {
    "text": "randomize your names because guids usually start with the same prefix or a very similar pref prefix if you're",
    "start": "2187040",
    "end": "2193800"
  },
  {
    "text": "generating them sequentially so that ends up destroying the name scheme it it",
    "start": "2193800",
    "end": "2199079"
  },
  {
    "text": "results in pretty much the same problem that that uh Philippi showed with using the date as uh you know the year of the",
    "start": "2199079",
    "end": "2205480"
  },
  {
    "text": "day that you end up hitting all the same partitions and you have a problem there we actually were able to bring down the",
    "start": "2205480",
    "end": "2213160"
  },
  {
    "text": "distrib the the the number of put and get requests that we needed to do from these billions and billions of requests",
    "start": "2213160",
    "end": "2219599"
  },
  {
    "text": "to a few million or a few tens of millions of requests and we actually got much better put request distribution",
    "start": "2219599",
    "end": "2226359"
  },
  {
    "text": "because we only have to save a website once we were finished crawling it so we",
    "start": "2226359",
    "end": "2231520"
  },
  {
    "text": "didn't have all these individual objects trying to be ridden at the very same time to an S3 bucket but rather we got a",
    "start": "2231520",
    "end": "2238359"
  },
  {
    "text": "distribution over time because individual websites will have individual different numbers of web pages so it's",
    "start": "2238359",
    "end": "2243960"
  },
  {
    "text": "much much less likely that we'll have all of our distributed servers trying to ride to our S3 bucket at the same time",
    "start": "2243960",
    "end": "2251200"
  },
  {
    "text": "so that really improved performance as well and the last thing that we did in order to um improve the the the results and",
    "start": "2251200",
    "end": "2260319"
  },
  {
    "text": "improve our process was to start working with secondary indexes and that was not only to reduce",
    "start": "2260319",
    "end": "2267280"
  },
  {
    "text": "the number of list requests that we wanted to do on top of the data but actually to allow us to perform more",
    "start": "2267280",
    "end": "2273240"
  },
  {
    "text": "complex operations on top of the data that that we wanted to store so we are St storing our secondary index in a",
    "start": "2273240",
    "end": "2279400"
  },
  {
    "text": "database that carries with it a lot of metadata about that object that stores stored on S tree so that that allows us",
    "start": "2279400",
    "end": "2286520"
  },
  {
    "text": "to do all sorts of segmentation before we actually go out and read the object so I can take all the individual",
    "start": "2286520",
    "end": "2293960"
  },
  {
    "text": "websites and I can say okay let let me let me only process the websites that whose domain name ends in do MX or",
    "start": "2293960",
    "end": "2302160"
  },
  {
    "text": "something like that and I can only read I I can read only those objects and I don't have to list my bucket and look at",
    "start": "2302160",
    "end": "2308280"
  },
  {
    "text": "the individual objects to know that because that's stored in an auxiliary database and that database has the the",
    "start": "2308280",
    "end": "2314200"
  },
  {
    "text": "the pointers to the objects inside s stre so that really helped us out further along the path um just to show",
    "start": "2314200",
    "end": "2322280"
  },
  {
    "start": "2321000",
    "end": "2368000"
  },
  {
    "text": "you guys real quickly this is the architecture that we operate today",
    "start": "2322280",
    "end": "2327720"
  },
  {
    "text": "so basically we have a reserved inst instance that orchestrates the entire process we use sqs to do all the",
    "start": "2327720",
    "end": "2334359"
  },
  {
    "text": "messaging between the individual processes and every time that once we've crawled",
    "start": "2334359",
    "end": "2342040"
  },
  {
    "text": "the web page we'll store the raw data on the the raw data for the websites on S3",
    "start": "2342040",
    "end": "2348800"
  },
  {
    "text": "comp compressed with the structure that I mentioned and will'll store the links",
    "start": "2348800",
    "end": "2354920"
  },
  {
    "text": "to the the the IDS of the objects and all the metadata associated with them on a mongodb cluster which allows us to do",
    "start": "2354920",
    "end": "2361960"
  },
  {
    "text": "all the segmentation and the filtering that I was mentioning a little bit earlier",
    "start": "2361960",
    "end": "2368960"
  },
  {
    "start": "2368000",
    "end": "2431000"
  },
  {
    "text": "so just to quickly um go over these things first and foremost pay attention to the",
    "start": "2368960",
    "end": "2376079"
  },
  {
    "text": "naming scheme it is really really important um it it's really important",
    "start": "2376079",
    "end": "2382520"
  },
  {
    "text": "not only to make it easier for you you know to avoid all the the the transactions",
    "start": "2382520",
    "end": "2388280"
  },
  {
    "text": "hitting the the same place but also it makes it easier for you to work with the objects later on to apply policies to",
    "start": "2388280",
    "end": "2393880"
  },
  {
    "text": "them to the lead a thund them at the same time and to do those things um GDs are",
    "start": "2393880",
    "end": "2402000"
  },
  {
    "text": "terrible uh if you come from a Windows background as I do you're we're used to",
    "start": "2402000",
    "end": "2409200"
  },
  {
    "text": "using GDs as IDs for a lot of the things that we do and that's a really bad idea",
    "start": "2409200",
    "end": "2414240"
  },
  {
    "text": "when you're dealing with S3 because the First characters tend to always be the same so you're always going to be",
    "start": "2414240",
    "end": "2419359"
  },
  {
    "text": "incurring that that problem so we actually built a special randomizer that",
    "start": "2419359",
    "end": "2425160"
  },
  {
    "text": "generates the first four letters of the objects that we want to store um this is really important and",
    "start": "2425160",
    "end": "2432599"
  },
  {
    "start": "2431000",
    "end": "2625000"
  },
  {
    "text": "then use the using the secondary indexes instead of listing the objects it allows",
    "start": "2432599",
    "end": "2438319"
  },
  {
    "text": "you to do the segmentation it makes it much easier for you to do parallel processing especially if you're running",
    "start": "2438319",
    "end": "2444160"
  },
  {
    "text": "a large scale process on a ton of servers and you want to you you can simply grab down those objects or you",
    "start": "2444160",
    "end": "2449960"
  },
  {
    "text": "can put the name of the objects on on the cube from the database and then work with it you don't have to be running",
    "start": "2449960",
    "end": "2455920"
  },
  {
    "text": "list commands against your but at all the time and it really makes the code simpler and since it's simpler it's",
    "start": "2455920",
    "end": "2462119"
  },
  {
    "text": "easier to maintain it gives you a lot less headaches it becomes much um easier",
    "start": "2462119",
    "end": "2467839"
  },
  {
    "text": "to work with and once again uh puts and lists",
    "start": "2467839",
    "end": "2473640"
  },
  {
    "text": "are very easy to overlook expense when we design our original architecture of",
    "start": "2473640",
    "end": "2478880"
  },
  {
    "text": "storing the individual web pages as objects on S3 we really didn't stop and",
    "start": "2478880",
    "end": "2485400"
  },
  {
    "text": "do the math on the amount of money that would be spending on put requests and that was a very uh as I mentioned that",
    "start": "2485400",
    "end": "2492000"
  },
  {
    "text": "that was a very bad idea for us because when once we actually sat down and put",
    "start": "2492000",
    "end": "2497680"
  },
  {
    "text": "it in the paper we we figured out okay this this will be completely um nobody would be happy with",
    "start": "2497680",
    "end": "2505000"
  },
  {
    "text": "us right Amazon wouldn't be happy with us because we wouldn't be a able to pay our bill at the end of the month we",
    "start": "2505000",
    "end": "2510400"
  },
  {
    "text": "wouldn't be happy because you know we'll be out of money to run the company uh so",
    "start": "2510400",
    "end": "2517079"
  },
  {
    "text": "uh you know think about what you're trying to do think think about the puts in the list the list requests as well",
    "start": "2517079",
    "end": "2523280"
  },
  {
    "text": "especially if you're doing uh processing that's not related to Distributing files to a lot of people that's that's",
    "start": "2523280",
    "end": "2529240"
  },
  {
    "text": "something that's really important to look at and one other thing that's actually not on the slide there is uh",
    "start": "2529240",
    "end": "2535319"
  },
  {
    "text": "something that Philipi mentioned earlier that you should always keep your data as close to the process in place as",
    "start": "2535319",
    "end": "2542520"
  },
  {
    "text": "possible I mean we talk a lot about Cloud front and putting the data close to the users but in our in our case",
    "start": "2542520",
    "end": "2548559"
  },
  {
    "text": "we're not actually sending the data out to any users we're using it internally so we have to keep it as close as",
    "start": "2548559",
    "end": "2554839"
  },
  {
    "text": "possible to the region you know if I'm using I'm going to use S3 in the same region where I'm going to host my",
    "start": "2554839",
    "end": "2560720"
  },
  {
    "text": "servers and that can lead to a five to 10 Time Performance gain when you're",
    "start": "2560720",
    "end": "2566480"
  },
  {
    "text": "accessing and working with that data so it it's it is really important for you to keep the data close not only to your",
    "start": "2566480",
    "end": "2572280"
  },
  {
    "text": "users but to where you're going to run your processing on it and with that I'm",
    "start": "2572280",
    "end": "2577400"
  },
  {
    "text": "going to hand it over again to Philippi thank you so thank",
    "start": "2577400",
    "end": "2585039"
  },
  {
    "text": "you so thank you guys uh wrapping it wrapping it up uh we talked about uh",
    "start": "2587599",
    "end": "2593319"
  },
  {
    "text": "architecture on top of S3 how to uh use our best practices and tips to uh you know optimize the performance or",
    "start": "2593319",
    "end": "2600599"
  },
  {
    "text": "maximize the performance of your S3 usage and uh we talked about help to optimize for puts for gets and we had",
    "start": "2600599",
    "end": "2606839"
  },
  {
    "text": "the customer case off to run here you have our contacts and we'll be happy to talk with you outside we're going to",
    "start": "2606839",
    "end": "2612920"
  },
  {
    "text": "have a QA station over there so if you have some questions for me or you're interested in tance case just come to",
    "start": "2612920",
    "end": "2620520"
  },
  {
    "text": "talk to us outside thank you [Applause]",
    "start": "2620520",
    "end": "2626459"
  }
]