[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "hello and welcome to databases on AWS as part of the AWS innovate online",
    "start": "60",
    "end": "5400"
  },
  {
    "text": "conference my name is Blair Leighton and I'm the business development manager for database services in asia-pacific I'll",
    "start": "5400",
    "end": "11639"
  },
  {
    "text": "be taking you through an overview of the manage database services on AWS today I will also talk about how you can migrate",
    "start": "11639",
    "end": "17820"
  },
  {
    "text": "your databases to AWS and from one database to another AWS consists of our",
    "start": "17820",
    "end": "23100"
  },
  {
    "text": "global infrastructure with 16 geographic regions across the world all interconnected with our network backbone",
    "start": "23100",
    "end": "28980"
  },
  {
    "text": "then in each region we have our network services such as virtual private cloud and cloud front the three main service",
    "start": "28980",
    "end": "35790"
  },
  {
    "text": "areas you will typically consume when building applications on AWS our our compute storage and database services",
    "start": "35790",
    "end": "42510"
  },
  {
    "text": "today we are going to focus on the manage database services Amazon RDS Amazon ElastiCache Amazon DynamoDB",
    "start": "42510",
    "end": "50219"
  },
  {
    "text": "Amazon redshift and the AWS migration service all these services that I just",
    "start": "50219",
    "end": "55860"
  },
  {
    "text": "mentioned managed database services so why would you use a managed database service instead of running your own",
    "start": "55860",
    "end": "62340"
  },
  {
    "text": "database on premise or even on ec2 well have a think about how much time your DBAs and other IT staff spend on",
    "start": "62340",
    "end": "69510"
  },
  {
    "text": "provisioning hardware and storage installing upgrading and patching software doing documentation licensing",
    "start": "69510",
    "end": "76650"
  },
  {
    "text": "and training backup recovery data load and unload and especially security then",
    "start": "76650",
    "end": "82259"
  },
  {
    "text": "ask yourself is that really adding value to the business looking at this in",
    "start": "82259",
    "end": "87330"
  },
  {
    "text": "another way here is what you do if you have to host your database on-premises you need to manage all aspects of your",
    "start": "87330",
    "end": "93600"
  },
  {
    "text": "environment from power high voltage a conditioning and networking to application optimization and everything",
    "start": "93600",
    "end": "100049"
  },
  {
    "text": "in between if you host your databases in ec2 then AWS can cake take care of the",
    "start": "100049",
    "end": "105960"
  },
  {
    "text": "infrastructure components from the data center up to the operating system installation but you will still have to",
    "start": "105960",
    "end": "111240"
  },
  {
    "text": "perform the operating system patches database installation database packages backups manage high availability scaling",
    "start": "111240",
    "end": "118110"
  },
  {
    "text": "and application optimization if you choose a managed database service such",
    "start": "118110",
    "end": "123840"
  },
  {
    "text": "as Amazon relational database service or RDS then the only thing you need to",
    "start": "123840",
    "end": "128849"
  },
  {
    "text": "worry about is your application optimization with AWS taking care of everything else this allows you to focus",
    "start": "128849",
    "end": "135630"
  },
  {
    "text": "on what really matters for your business imagine you have a competitor who is in",
    "start": "135630",
    "end": "140790"
  },
  {
    "text": "the same market as you with the same resources but your competitor is managing their own environments and you",
    "start": "140790",
    "end": "146970"
  },
  {
    "text": "are using managed services from AWS you will be able to focus on your application and be more agile delivering",
    "start": "146970",
    "end": "153300"
  },
  {
    "text": "new capabilities to your customers this will make you stand out from your competitors now I'm going to talk in",
    "start": "153300",
    "end": "159780"
  },
  {
    "start": "158000",
    "end": "192000"
  },
  {
    "text": "more detail about AWS database services starting with Amazon RDS RDS given its",
    "start": "159780",
    "end": "166860"
  },
  {
    "text": "name is obviously focused on relational databases it offers a managed service for my sequel ms sequel server oracle",
    "start": "166860",
    "end": "173489"
  },
  {
    "text": "database Postgres and maria DB amazon aurora is Amazon's own database platform",
    "start": "173489",
    "end": "179100"
  },
  {
    "text": "and comes in two editions one with my sequel compatibility and the other with Postgres compatibility as a managed",
    "start": "179100",
    "end": "185910"
  },
  {
    "text": "service RDS is simple and faster scale gives you fast predictable performance and you only pay for what you consume",
    "start": "185910",
    "end": "192680"
  },
  {
    "text": "some of the key features of RDS are the ability to provision a database in six",
    "start": "192680",
    "end": "198329"
  },
  {
    "text": "minutes why is it six minutes and not five well it's because I've tested it and that's what I got the number when I",
    "start": "198329",
    "end": "203959"
  },
  {
    "text": "tested it you can provision a multi AZ database with a few mouse clicks",
    "start": "203959",
    "end": "209549"
  },
  {
    "text": "this configures a master database in one availability zone and a standby database in another availability zone when we",
    "start": "209549",
    "end": "216690"
  },
  {
    "text": "replicate the data synchronously between the master and standby if there is a failure in the hardware networking",
    "start": "216690",
    "end": "222959"
  },
  {
    "text": "storage etcetera then RDS will automatically failover to a standby instance with about 60 to 90 seconds",
    "start": "222959",
    "end": "229769"
  },
  {
    "text": "downtime you can scale a database up or down using the same mechanism with 60 to",
    "start": "229769",
    "end": "234780"
  },
  {
    "text": "90 seconds downtime two RDS will take the standby offline resize it and then bring it back online again once it is",
    "start": "234780",
    "end": "241680"
  },
  {
    "text": "caught up then RDS initiates a failure failover from the primary to the standby the old primary is then resized and",
    "start": "241680",
    "end": "248700"
  },
  {
    "text": "configured as the standby if a patch will take longer then the 60 to 90 second failover window RDS will",
    "start": "248700",
    "end": "255480"
  },
  {
    "text": "automatically include a failover as part of the patching process to minimize the downtime required adding a read replica to the database",
    "start": "255480",
    "end": "263070"
  },
  {
    "text": "can be a multiple step process as error-prone with RDS you can create a",
    "start": "263070",
    "end": "268300"
  },
  {
    "text": "read replica with a few mouse clicks and Aria supports up to 15 read replicas for my sequel Postgres and Aurora RDS",
    "start": "268300",
    "end": "276639"
  },
  {
    "text": "creates daily snapshots of your databases at a time you configure and keeps them for up to 30 to 5 days when",
    "start": "276639",
    "end": "283539"
  },
  {
    "text": "you create an RDS instance you specify the time you want the backup to occur and how long you want the backups to be",
    "start": "283539",
    "end": "289930"
  },
  {
    "text": "kept for the default is 7 days you can also create user snapshots at any time",
    "start": "289930",
    "end": "295630"
  },
  {
    "text": "that are kept until you explicitly delete them all of the backups are kept in s3 with 11 nines of durability the",
    "start": "295630",
    "end": "302919"
  },
  {
    "text": "database logs are sent to s3 every five minutes - this enables RDS to recover a database",
    "start": "302919",
    "end": "309070"
  },
  {
    "text": "to any point in time by restoring a snapshot to a new instance and applying the logs up until the failure or the",
    "start": "309070",
    "end": "315940"
  },
  {
    "text": "user error occurred the majority of AWS services allow you to get metrics down",
    "start": "315940",
    "end": "321669"
  },
  {
    "text": "to one minute interval intervals however RDS was the first service to introduce",
    "start": "321669",
    "end": "327370"
  },
  {
    "text": "detailed metrics that go down to one-second intervals you can also secure your data with a single click to enable",
    "start": "327370",
    "end": "334120"
  },
  {
    "text": "encryption at rest SSL is supported to encrypt data in transit - with the",
    "start": "334120",
    "end": "339639"
  },
  {
    "text": "option to make it mandatory on Postgres and sequel server for SSL connections now I'd like to talk about Amazon a war",
    "start": "339639",
    "end": "347440"
  },
  {
    "text": "that is built upon the RDS platform so what is Amazon Aurora it's a my sequel",
    "start": "347440",
    "end": "354880"
  },
  {
    "text": "compatible and Postgres compatible relational database platform with the",
    "start": "354880",
    "end": "359919"
  },
  {
    "text": "performance and availability of the commercial databases delivered as a managed service AWS announced the my",
    "start": "359919",
    "end": "367000"
  },
  {
    "text": "sequel compatible addition at reinvent in 2014 and the Postgres compatible",
    "start": "367000",
    "end": "372550"
  },
  {
    "text": "addition at reinvent in 2016 if you're going to build a new database for the",
    "start": "372550",
    "end": "378070"
  },
  {
    "text": "cloud then you don't want to use the same architecture that has been around for the since the 70s",
    "start": "378070",
    "end": "383940"
  },
  {
    "text": "AWS decided to build a new database platform using a service orientated architecture the storage layer has been",
    "start": "383940",
    "end": "391479"
  },
  {
    "text": "broken out into a separate database optimized storage service that receives the database changes in the form of log",
    "start": "391479",
    "end": "397659"
  },
  {
    "text": "entries not filesystem this is a huge difference and the main benefit of the Aurora platform it is",
    "start": "397659",
    "end": "405370"
  },
  {
    "text": "also integrated with other AWS services to help manage an Amazon Aurora such as",
    "start": "405370",
    "end": "410830"
  },
  {
    "text": "ec2 Amazon DynamoDB Amazon simple workflow and Amazon route 53 backups are",
    "start": "410830",
    "end": "418150"
  },
  {
    "text": "continuously taken from the storage service and integrated with Amazon s3 with eleven nines of durability you can",
    "start": "418150",
    "end": "425290"
  },
  {
    "text": "also create a manual snapshot and save that in s3 too this slide demonstrates the large",
    "start": "425290",
    "end": "432280"
  },
  {
    "start": "429000",
    "end": "522000"
  },
  {
    "text": "differences between the storage operations of RDS in a multi AZ environment and Amazon Aurora with the",
    "start": "432280",
    "end": "438760"
  },
  {
    "text": "same failover capability on the left hand side you can see the RDS my sequel",
    "start": "438760",
    "end": "444400"
  },
  {
    "text": "environment and on the right Amazon Aurora my sequel has a primary instance and a standby instance in another",
    "start": "444400",
    "end": "450910"
  },
  {
    "text": "availability zone think of that as a data center the primary database has to",
    "start": "450910",
    "end": "455920"
  },
  {
    "text": "write to an EBS volume which is replicated by EBS in the same AC at the",
    "start": "455920",
    "end": "460990"
  },
  {
    "text": "same time the same write is sent across to the standby and the same right to the EBS is applied there bin log files and",
    "start": "460990",
    "end": "468190"
  },
  {
    "text": "other metadata is also kept up-to-date across both instances on Amazon Aurora",
    "start": "468190",
    "end": "473680"
  },
  {
    "text": "there is a primary instance and a read replica there is no need for a standby instance that can't be accessed like in",
    "start": "473680",
    "end": "480130"
  },
  {
    "text": "my Xia inradius my sequel this is because read replicas become failover targets in Aurora the storage is located",
    "start": "480130",
    "end": "488470"
  },
  {
    "text": "across six locations in three availability zones and kept consistent by the storage service there is a quorum",
    "start": "488470",
    "end": "495130"
  },
  {
    "text": "of four needed for an acknowledge right and three for a read providing a highly resilient storage tier yet as discussed",
    "start": "495130",
    "end": "503050"
  },
  {
    "text": "before the database just needs to send the log or change information to the storage service and replicate some basic",
    "start": "503050",
    "end": "509500"
  },
  {
    "text": "information to the read replicas this greatly reduces the amount of i/o that Aurora needs to do for a given",
    "start": "509500",
    "end": "515919"
  },
  {
    "text": "transaction load increasing the scale that Aurora can handle when compared to RDS and traditional databases we only",
    "start": "515920",
    "end": "522909"
  },
  {
    "start": "522000",
    "end": "800000"
  },
  {
    "text": "have a short amount of time and there are so many features of Aurora so I just want to go over a few key features as",
    "start": "522910",
    "end": "528750"
  },
  {
    "text": "mentioned the storage is highly available with six copies across three ACS and backups are continuous 2s3 with the Reid",
    "start": "528750",
    "end": "536740"
  },
  {
    "text": "replicas operating on the same storage is the master instance there is minimal lag typically less than 20 milliseconds",
    "start": "536740",
    "end": "542650"
  },
  {
    "text": "this solves a big problem for my sequel users who often have read replicas that lag into minutes when there is more than",
    "start": "542650",
    "end": "549340"
  },
  {
    "text": "2,000 die offs of activity on the primary database the storage is auto-scaling it simply adds 10 gigabytes",
    "start": "549340",
    "end": "556690"
  },
  {
    "text": "segments as they are needed the storage service also identifies hotspots and moves data around to maximize throughput",
    "start": "556690",
    "end": "562960"
  },
  {
    "text": "and minimize contextual recovery is single threaded with my sequel where as",
    "start": "562960",
    "end": "568480"
  },
  {
    "text": "Aurora does parallel distributed and asynchronous recovery that greatly decreases the time that your database",
    "start": "568480",
    "end": "573970"
  },
  {
    "text": "becomes available after a restart or a crash this is possible because the",
    "start": "573970",
    "end": "579400"
  },
  {
    "text": "storage engine can apply the log changes to each story segment in parallel related to recovery time is survivable",
    "start": "579400",
    "end": "587020"
  },
  {
    "text": "caches when you restart a relational database the cache is part of the database process so the data is lost",
    "start": "587020",
    "end": "593110"
  },
  {
    "text": "however in Aurora the cache is a separate process that survives a database process restart this means that",
    "start": "593110",
    "end": "600640"
  },
  {
    "text": "when you restart the database the data is already available in the cache preventing the surge and disk activity",
    "start": "600640",
    "end": "605860"
  },
  {
    "text": "that is normally needed to bring the data into memory before the application to perform at its peak performance again",
    "start": "605860",
    "end": "612130"
  },
  {
    "text": "normal databases suffer from such brownouts as the data is read from the disks into memory this all contributes",
    "start": "612130",
    "end": "619450"
  },
  {
    "text": "to faster more predictable failover speaking of faster when Aurora is",
    "start": "619450",
    "end": "625600"
  },
  {
    "text": "compared to my sequel on the same hardware at scale it is up to 5 times faster it should be noted that this is",
    "start": "625600",
    "end": "632200"
  },
  {
    "text": "not a single query running but a lot of connections running lots of queries for",
    "start": "632200",
    "end": "637450"
  },
  {
    "text": "Postgres it's more than 2 times faster on the same hardware if you would like",
    "start": "637450",
    "end": "642460"
  },
  {
    "text": "to know more AWS has published the benchmark information and how to replicate the my sequel bank smarts to",
    "start": "642460",
    "end": "648850"
  },
  {
    "text": "on the AWS website when you are using RDS at scale it is common to have large",
    "start": "648850",
    "end": "656230"
  },
  {
    "text": "instances with such as our 3/8 X large with 32 V CPUs 256 gigabytes of RAM and",
    "start": "656230",
    "end": "663580"
  },
  {
    "text": "provisional storage customers then have the same instance and storage for the primary standby and",
    "start": "663580",
    "end": "669820"
  },
  {
    "text": "at least one read replicas with Aurora only needing one set of storage and a",
    "start": "669820",
    "end": "675370"
  },
  {
    "text": "read replicas for failover it can be significantly cheaper than RDS my sequel",
    "start": "675370",
    "end": "680500"
  },
  {
    "text": "is scale when you take into account that Aurora is also faster than my sequel it is common for customers to find that",
    "start": "680500",
    "end": "686920"
  },
  {
    "text": "they can have the instance size on Aurora 2 making further cost reductions",
    "start": "686920",
    "end": "692940"
  },
  {
    "text": "there have been a series of recent improvements with Aurora that with the Aurora platform that you can read here",
    "start": "692940",
    "end": "699310"
  },
  {
    "text": "I'll call out a few of them farce DDL allows for adding a new column to my",
    "start": "699310",
    "end": "706390"
  },
  {
    "text": "sequel table without having the typical table copy operation that happens with my sequel today zero downtime patching",
    "start": "706390",
    "end": "713800"
  },
  {
    "text": "looks for a window of low activity suspends the connections performs the patch and then restores the connections",
    "start": "713800",
    "end": "720190"
  },
  {
    "text": "this happens so fast that most users would not even notice having the small",
    "start": "720190",
    "end": "726070"
  },
  {
    "text": "t2 instances and the medium instances available for Amazon Aurora allows you",
    "start": "726070",
    "end": "731380"
  },
  {
    "text": "to run your dev test environments and small databases at a much lower cost Amazon Aurora is now available in all",
    "start": "731380",
    "end": "738970"
  },
  {
    "text": "three AZ regions with recent regions including London Montreal Ohio and San",
    "start": "738970",
    "end": "744490"
  },
  {
    "text": "Francisco looking across the RTS engines you can see some of the major features",
    "start": "744490",
    "end": "750880"
  },
  {
    "text": "of the RDS platform and how they are supported by each engine in this slide Aurora offers a lot more storage than",
    "start": "750880",
    "end": "758290"
  },
  {
    "text": "all the other engines with 64 terabytes available instead of 6 terabytes for the Lingga Linux based databases and 4",
    "start": "758290",
    "end": "765070"
  },
  {
    "text": "terabytes for sequel server as a result of having less storage sequel server also has less I ops available sequel",
    "start": "765070",
    "end": "771790"
  },
  {
    "text": "server has no ability to resize the storage without a backup and restore process this is done for a standard dot",
    "start": "771790",
    "end": "778209"
  },
  {
    "text": "back file exported to s3 and then imported with a new instance with larger",
    "start": "778209",
    "end": "783790"
  },
  {
    "text": "attached storage these gaps across the RDS platform are all things we are looking to improve so stay tuned for",
    "start": "783790",
    "end": "790660"
  },
  {
    "text": "future announcements on to ElastiCache this is our managed caching service for",
    "start": "790660",
    "end": "798310"
  },
  {
    "text": "memcache and Redis why is in-memory caching becoming more important today everything is becoming",
    "start": "798310",
    "end": "804900"
  },
  {
    "start": "800000",
    "end": "870000"
  },
  {
    "text": "connected with the rise of IOT from phones and tablets to cars air conditioners and even toasters there is",
    "start": "804900",
    "end": "812400"
  },
  {
    "text": "a demand from real real time performance with online games EdTech ecommerce and",
    "start": "812400",
    "end": "818790"
  },
  {
    "text": "especially social apps for example if your parents are on holiday in another part of the world and they upload some",
    "start": "818790",
    "end": "825090"
  },
  {
    "text": "photos to Facebook you expect them to be instantly available for you to view and load as quickly as possible this is a",
    "start": "825090",
    "end": "831660"
  },
  {
    "text": "difficult problem to solve load from these types of applications can be spiky",
    "start": "831660",
    "end": "837450"
  },
  {
    "text": "- for example a promotion for a game or an online sale at a web store will drive",
    "start": "837450",
    "end": "842550"
  },
  {
    "text": "many people to an app or website in many cases the database becomes the bottleneck and overall application",
    "start": "842550",
    "end": "848550"
  },
  {
    "text": "performance in these situations to help alleviate reads from the database the best thing to cache are small frequently",
    "start": "848550",
    "end": "855150"
  },
  {
    "text": "accessed items such as products in a catalog session information from a game and top articles on a new site this can",
    "start": "855150",
    "end": "861930"
  },
  {
    "text": "greatly reduce the demand on a database and then increase the scale of the application without needing to increase",
    "start": "861930",
    "end": "867030"
  },
  {
    "text": "the size of the database server the - caching solutions offered by the",
    "start": "867030",
    "end": "873510"
  },
  {
    "start": "870000",
    "end": "941000"
  },
  {
    "text": "ElastiCache service are memcache and Redis so let's take a look at each engine memcache is an in-memory cache",
    "start": "873510",
    "end": "881070"
  },
  {
    "text": "with a multi-threaded architecture that can consume multiple cores it doesn't support any persistence model so you",
    "start": "881070",
    "end": "887250"
  },
  {
    "text": "have to design your application to deal with cache failures memcache only allows a storing of string values so for some",
    "start": "887250",
    "end": "894000"
  },
  {
    "text": "users this can be a deal-breaker it does scale well well horizontally by adding a few nodes and shouting the data across",
    "start": "894000",
    "end": "901020"
  },
  {
    "text": "each node Redis is a more advanced in-memory cache but it is single",
    "start": "901020",
    "end": "906660"
  },
  {
    "text": "threaded it supports read replicas and in version 3 read asserted support for clustering to scale out the persistence",
    "start": "906660",
    "end": "913530"
  },
  {
    "text": "model allows you to take snapshots and create new instances or clusters from these snapshots atomic operations allow",
    "start": "913530",
    "end": "920430"
  },
  {
    "text": "you to use it for counters and other critical operations there are also advanced data types such as ordered sets",
    "start": "920430",
    "end": "927510"
  },
  {
    "text": "that are great for game leaderboards top article article lists etc some customers",
    "start": "927510",
    "end": "932580"
  },
  {
    "text": "even use the pub subnet switching feature like a queue and we see most customers using Redis these",
    "start": "932580",
    "end": "938220"
  },
  {
    "text": "days instead of memcache it's important to take advantage of the AWS and Intel",
    "start": "938220",
    "end": "944430"
  },
  {
    "start": "941000",
    "end": "985000"
  },
  {
    "text": "partnership for any workload on AWS and unless the cache is no exception Intel works with AWS on each new AWS",
    "start": "944430",
    "end": "952470"
  },
  {
    "text": "instance family to inherit the benefits of both the latest Intel CPU and the chipset platform together with AWS",
    "start": "952470",
    "end": "958950"
  },
  {
    "text": "improvements along the way to the new AWS instances are often much cheaper",
    "start": "958950",
    "end": "964530"
  },
  {
    "text": "than the old ones too therefore it's important to upgrade to the latest instances to maximize your performance",
    "start": "964530",
    "end": "970230"
  },
  {
    "text": "and lower your costs for example with a lesser cash Redis this means you can",
    "start": "970230",
    "end": "976560"
  },
  {
    "text": "achieve 34% greater throughput using m4 instances versus m3 in some instances",
    "start": "976560",
    "end": "982140"
  },
  {
    "text": "and at a lower cost dynamodb is a managed no sequel database",
    "start": "982140",
    "end": "988980"
  },
  {
    "start": "985000",
    "end": "1113000"
  },
  {
    "text": "with low latency and highly durable storage it's designed for Tier one applications",
    "start": "988980",
    "end": "994590"
  },
  {
    "text": "it offers massive and seamless scalability with some customers writing in excess of a million writes per second",
    "start": "994590",
    "end": "1001730"
  },
  {
    "text": "to a single table DynamoDB offers an amazing feature of providing consistent",
    "start": "1001730",
    "end": "1008170"
  },
  {
    "text": "single-digit millisecond latency at any scale even when scaling size or",
    "start": "1008170",
    "end": "1013370"
  },
  {
    "text": "throughput requirements the storage is located in facilities in all regions",
    "start": "1013370",
    "end": "1019400"
  },
  {
    "text": "even those with two AZ's to provide highly available and durable storage it's easy to use with simple api's to",
    "start": "1019400",
    "end": "1027890"
  },
  {
    "text": "get and set information it supports both key value and document models and there",
    "start": "1027890",
    "end": "1033500"
  },
  {
    "text": "are no table size or through port limits the blue line on this diagram represents",
    "start": "1033500",
    "end": "1040459"
  },
  {
    "text": "the number of requests coming into dynamodb with various peaks and troughs along the way the green line represents",
    "start": "1040459",
    "end": "1047780"
  },
  {
    "text": "the average latency of dynamodb during the same period this demonstrates that no matter what the number of requests",
    "start": "1047780",
    "end": "1053750"
  },
  {
    "text": "coming in to dynamodb are that the average latency stays consistent and predictable this also applies when you",
    "start": "1053750",
    "end": "1060560"
  },
  {
    "text": "are scaling up or down the provision through put-on DynamoDB or operating on the provision",
    "start": "1060560",
    "end": "1066140"
  },
  {
    "text": "it's some of the popular use cases for dynamodb our EdTech IOT gaming and",
    "start": "1066140",
    "end": "1073670"
  },
  {
    "text": "mobile web applications a consistent use case across many different industries is",
    "start": "1073670",
    "end": "1078890"
  },
  {
    "text": "user profile and login management often companies will hold a large promotion or",
    "start": "1078890",
    "end": "1085310"
  },
  {
    "text": "event that requires many users to log in all at the same time for example a gaming company could send out a",
    "start": "1085310",
    "end": "1091160"
  },
  {
    "text": "notification telling users to play the game now and they could receive a gem or a telco could launch the latest iPhone",
    "start": "1091160",
    "end": "1097150"
  },
  {
    "text": "each of these situations will create a logon storm that relational databases would struggle to deal with DynamoDB is",
    "start": "1097150",
    "end": "1104510"
  },
  {
    "text": "able to scale up for the promotion and then back down again afterwards so you only pay for the throughput that you",
    "start": "1104510",
    "end": "1110660"
  },
  {
    "text": "need when you need it now we move on to Amazon redshift a relational data",
    "start": "1110660",
    "end": "1117230"
  },
  {
    "start": "1113000",
    "end": "1195000"
  },
  {
    "text": "warehouse that is massively parallel and scales to petabytes of data this managed",
    "start": "1117230",
    "end": "1122930"
  },
  {
    "text": "database service offers both magnetic and solid-state disk options depending on your performance and storage",
    "start": "1122930",
    "end": "1128270"
  },
  {
    "text": "requirements costing 1,000 per terabyte per year it is significantly cheaper",
    "start": "1128270",
    "end": "1133520"
  },
  {
    "text": "than other data warehouses and you can even start at 25 cents an hour",
    "start": "1133520",
    "end": "1138700"
  },
  {
    "text": "some of redshifts key capabilities are being completely scalable going from 160",
    "start": "1138700",
    "end": "1146810"
  },
  {
    "text": "gigabytes to up to 2 petabytes of information it's fast with parallel",
    "start": "1146810",
    "end": "1152690"
  },
  {
    "text": "execution on compressed sorted data on optimized Hardware it's also inexpensive",
    "start": "1152690",
    "end": "1158600"
  },
  {
    "text": "because you can start at that 25 cents an hour or on or 1,000 Terra per dollars",
    "start": "1158600",
    "end": "1163880"
  },
  {
    "text": "per terabyte per year from a managed service perspective it's easy to provision backup restore patch and scale",
    "start": "1163880",
    "end": "1171110"
  },
  {
    "text": "and on security you can load and store and scripted data you can have SSL for",
    "start": "1171110",
    "end": "1177200"
  },
  {
    "text": "data in transit and you can have audit logging - it's also innovative with over",
    "start": "1177200",
    "end": "1183050"
  },
  {
    "text": "a hundred new features since launch it has a large ecosystem with major data",
    "start": "1183050",
    "end": "1189050"
  },
  {
    "text": "integration and visualization IVs supporting red ship with a large consulting partner base I'd like to talk",
    "start": "1189050",
    "end": "1196340"
  },
  {
    "start": "1195000",
    "end": "1282000"
  },
  {
    "text": "about a use case for weblog analytics from our power company amazon.com as most of you have",
    "start": "1196340",
    "end": "1202600"
  },
  {
    "text": "used that Mazon comm you would know the website is heavily customized for each user based on previous purchases",
    "start": "1202600",
    "end": "1208480"
  },
  {
    "text": "browsing history and other information the amount of data required to manage",
    "start": "1208480",
    "end": "1214029"
  },
  {
    "text": "this process is more than one petabyte with two terabytes added every day and the largest table at four hundred",
    "start": "1214029",
    "end": "1220480"
  },
  {
    "text": "terabytes this workload is growing at 67 percent year-on-year the legacy data",
    "start": "1220480",
    "end": "1226299"
  },
  {
    "text": "warehouse could query across one weeks worth of data in an hour when using Hadoop they were able to get that to one",
    "start": "1226299",
    "end": "1233559"
  },
  {
    "text": "month's worth of data in the same amount of time so that's quite a good improvement once the team migrated to",
    "start": "1233559",
    "end": "1240760"
  },
  {
    "text": "Amazon redshift they were able to query 15 months worth of data which is one petabyte in 14 minutes they were also",
    "start": "1240760",
    "end": "1248350"
  },
  {
    "text": "able to load five billion rows of data in 10 minutes a large join of 21 billion",
    "start": "1248350",
    "end": "1253659"
  },
  {
    "text": "rows with 10 billion rows used to take three days on hive but now came down to",
    "start": "1253659",
    "end": "1258820"
  },
  {
    "text": "two hours with redshift even the load pipeline that ran on Oracle technology came down from 90",
    "start": "1258820",
    "end": "1264370"
  },
  {
    "text": "hours to 8 hours to deliver on this performance the team uses 64 clusters",
    "start": "1264370",
    "end": "1270580"
  },
  {
    "text": "with a total of 800 nodes across all the clusters and 13 powered petabytes of provisioned storage yet because redshift",
    "start": "1270580",
    "end": "1277630"
  },
  {
    "text": "is a managed service they only have two DBAs this brings us to the end of the",
    "start": "1277630",
    "end": "1284409"
  },
  {
    "start": "1282000",
    "end": "1422000"
  },
  {
    "text": "section on managed database services on AWS in summary I discussed the benefits of Amazon RDS for relational databases",
    "start": "1284409",
    "end": "1291870"
  },
  {
    "text": "including Amazon Aurora platform I covered Amazon ElastiCache that supports both memcache and Redis to help",
    "start": "1291870",
    "end": "1298840"
  },
  {
    "text": "speed up and scale your applications Amazon DynamoDB which is our no sequel database with consistent latency and",
    "start": "1298840",
    "end": "1305679"
  },
  {
    "text": "near infinite scalability that supports both key-value and document models finally Amazon redshift is a petabyte",
    "start": "1305679",
    "end": "1312730"
  },
  {
    "text": "scale parallel Kalima data warehouse there are some reasons you may not want",
    "start": "1312730",
    "end": "1319090"
  },
  {
    "text": "to use the database services I just mentioned but that doesn't mean you can't use AWS if your database is on",
    "start": "1319090",
    "end": "1325570"
  },
  {
    "text": "Windows or Linux you can run it on ec2 there are many AM is available from the",
    "start": "1325570",
    "end": "1332110"
  },
  {
    "text": "technology partners such as our called databases I'm a sequel server MongoDB Vertica and from the marketplace",
    "start": "1332110",
    "end": "1339160"
  },
  {
    "text": "for licensed products such as teradata there are also white papers on how to run Oracle database em a sequel server",
    "start": "1339160",
    "end": "1346150"
  },
  {
    "text": "MongoDB Cassandra and others on AWS including both RDS and ec2 where",
    "start": "1346150",
    "end": "1352510"
  },
  {
    "text": "applicable so why would you use ez - well there might not be the AWS managed",
    "start": "1352510",
    "end": "1358780"
  },
  {
    "text": "service for the database you want to use such as Cassandra you also might need more control than what RDS offers for",
    "start": "1358780",
    "end": "1365200"
  },
  {
    "text": "the integration with other software or local file system access sometimes you might need to use the latest ec2",
    "start": "1365200",
    "end": "1371770"
  },
  {
    "text": "instance that is not available on the managed service yet or you might need more than 6 terabytes of storage that",
    "start": "1371770",
    "end": "1377860"
  },
  {
    "text": "RDS offers at this stage one of those new instance types is the x1 instance",
    "start": "1377860",
    "end": "1384400"
  },
  {
    "text": "it's designed for large-scale in-memory applications in the cloud and is ideal for M memory databases like sa",
    "start": "1384400",
    "end": "1391180"
  },
  {
    "text": "P Hana and big data processing applications like SPARC and presto the",
    "start": "1391180",
    "end": "1396820"
  },
  {
    "text": "instance is powered by intel xeon e7 Haswell processes with up to 100 and 228",
    "start": "1396820",
    "end": "1402430"
  },
  {
    "text": "virtual CPUs and 2 terabytes of RAM we just announced that we'll be increasing",
    "start": "1402430",
    "end": "1408130"
  },
  {
    "text": "the amount of ram in the x1 family with a new x1 e32 large instance with 4 terabytes of RAM",
    "start": "1408130",
    "end": "1416200"
  },
  {
    "text": "and we are also working on instances with 8 to 16 terabytes of RAM whether",
    "start": "1416200",
    "end": "1423340"
  },
  {
    "start": "1422000",
    "end": "1539000"
  },
  {
    "text": "you want to migrate your applications to AWS and use ec2 or the managed services",
    "start": "1423340",
    "end": "1428800"
  },
  {
    "text": "you have to work out how to migrate the databases securely and ideally with minimal downtime the AWS daybut database",
    "start": "1428800",
    "end": "1435730"
  },
  {
    "text": "migration service or DMS allows you to perform homogeneous and heterogeneous migrations between the very various",
    "start": "1435730",
    "end": "1442810"
  },
  {
    "text": "databases on the slide here the migrations can be carried out with change data capture or CDC this allows",
    "start": "1442810",
    "end": "1450310"
  },
  {
    "text": "the source database to remain active during the migration keeping your application up and running until the",
    "start": "1450310",
    "end": "1456160"
  },
  {
    "text": "target database has caught up let's take a look how it works in more detail on",
    "start": "1456160",
    "end": "1463080"
  },
  {
    "text": "the left hand side you can see the existing applications database on premise and the users connecting to",
    "start": "1463080",
    "end": "1469480"
  },
  {
    "text": "that on the right hand side is AWS with an empty database ready for the migration in the middle is a networking",
    "start": "1469480",
    "end": "1475600"
  },
  {
    "text": "we're using a VPN in this example but it could be using our direct connect option to to start the migration we need a",
    "start": "1475600",
    "end": "1483640"
  },
  {
    "text": "replication instance which is running the database migration service then once",
    "start": "1483640",
    "end": "1490450"
  },
  {
    "text": "that has started you need to provide the connection information for the source and target databases next you need to",
    "start": "1490450",
    "end": "1497080"
  },
  {
    "text": "choose what data you want to migrate to AWS DMS allows you to choose specific",
    "start": "1497080",
    "end": "1502510"
  },
  {
    "text": "tables schemas or the whole database then sit back and let DMS do the rest",
    "start": "1502510",
    "end": "1508650"
  },
  {
    "text": "it creates the tables loads the data and best of all keeps them synchronized for",
    "start": "1508650",
    "end": "1514420"
  },
  {
    "text": "as long as you need that replication capability which keeps the source and target databases in sync allows",
    "start": "1514420",
    "end": "1520090"
  },
  {
    "text": "customers to switch applications over to point AWS at their leisure",
    "start": "1520090",
    "end": "1526110"
  },
  {
    "text": "this means that DMS eliminates the need for high stakes extended outages to",
    "start": "1526110",
    "end": "1531370"
  },
  {
    "text": "migrate production data into the cloud DMS provides a graceful switchover",
    "start": "1531370",
    "end": "1536500"
  },
  {
    "text": "capability DMS is also very cost effective with no upfront costs with the",
    "start": "1536500",
    "end": "1544750"
  },
  {
    "text": "AWS database migration service you pay for the migration instance that moves your data from your source database to",
    "start": "1544750",
    "end": "1550990"
  },
  {
    "text": "your target database each database migration instance includes storage sufficient to support the needs of the",
    "start": "1550990",
    "end": "1556630"
  },
  {
    "text": "replication engines such as swap space logs and cache inbound data transfer is",
    "start": "1556630",
    "end": "1562360"
  },
  {
    "text": "free additional charges only apply if you decide to allocate additional storage for migration logs or when you",
    "start": "1562360",
    "end": "1569140"
  },
  {
    "text": "replicate your data to a database in another region or on-premises the",
    "start": "1569140",
    "end": "1575230"
  },
  {
    "text": "database migration service currently supports the t2 and C for instance classes t2 instances are low cost",
    "start": "1575230",
    "end": "1581770"
  },
  {
    "text": "standard instances designed to provide a baseline level of CPU performance with the ability to burst above the baseline",
    "start": "1581770",
    "end": "1587640"
  },
  {
    "text": "very suitable for developing configuring and testing your database migration process and for periodic database",
    "start": "1587640",
    "end": "1593770"
  },
  {
    "text": "migration tasks that can benefit from the CPU boast capability see four instances are designed to",
    "start": "1593770",
    "end": "1600950"
  },
  {
    "text": "deliver the highest level of processor performance and achieve significant higher packet per second performance",
    "start": "1600950",
    "end": "1606980"
  },
  {
    "text": "lower jitter and lower network latency you should use C or for instances if you",
    "start": "1606980",
    "end": "1612590"
  },
  {
    "text": "are migrating large databases and are looking to minimize the migration time while DMS migrates the data we are",
    "start": "1612590",
    "end": "1620720"
  },
  {
    "start": "1618000",
    "end": "1667000"
  },
  {
    "text": "finding more and more people are looking to switch database engines so what happens if you are converting from one",
    "start": "1620720",
    "end": "1626540"
  },
  {
    "text": "database engine to another where does that magic happen this is where the AWS",
    "start": "1626540",
    "end": "1632150"
  },
  {
    "text": "schema conversion tool steps in SCT supports the migration of Oracle",
    "start": "1632150",
    "end": "1638060"
  },
  {
    "text": "database Microsoft sequel server mariya DB Postgres and my sequel to any",
    "start": "1638060",
    "end": "1643520"
  },
  {
    "text": "of the open source engines we don't support the migration to Oracle database or Microsoft sequel server because both",
    "start": "1643520",
    "end": "1649580"
  },
  {
    "text": "Oracle and Microsoft provide their own tools for the job and most customers want to migrate to open-source databases",
    "start": "1649580",
    "end": "1655280"
  },
  {
    "text": "anyway SCT also supports the migration of Teradata Oracle netezza greenplum mo",
    "start": "1655280",
    "end": "1662570"
  },
  {
    "text": "single server and Vertica data warehouses to Amazon redshift ESET also",
    "start": "1662570",
    "end": "1668270"
  },
  {
    "start": "1667000",
    "end": "1688000"
  },
  {
    "text": "supports a schema copy function I'm sure many of you have been in the situation where an old system doesn't have any",
    "start": "1668270",
    "end": "1674060"
  },
  {
    "text": "documentation or even the DDL to create the database that means the only definition of the database is the live",
    "start": "1674060",
    "end": "1679940"
  },
  {
    "text": "production system SCT can create an exact copy of such systems to help",
    "start": "1679940",
    "end": "1685100"
  },
  {
    "text": "migrate them to AWS the other useful feature it has is the ability to make",
    "start": "1685100",
    "end": "1691040"
  },
  {
    "start": "1688000",
    "end": "1710000"
  },
  {
    "text": "RDS recommendations including whether you can use cheaper versions of the database for example if you're using",
    "start": "1691040",
    "end": "1697430"
  },
  {
    "text": "Oracle database Enterprise Edition but SCT finds that we are not using any Enterprise Edition features then it will",
    "start": "1697430",
    "end": "1703700"
  },
  {
    "text": "recommend that you can use standard Edition 1 or standard ition 2 when migrating to RDS on this slide you can",
    "start": "1703700",
    "end": "1711980"
  },
  {
    "start": "1710000",
    "end": "1766000"
  },
  {
    "text": "see what the CS et interface looks like on the left side you have the source database information and on the right",
    "start": "1711980",
    "end": "1718190"
  },
  {
    "text": "side you have the database information for the target in the middle at the top is a list of issues that schema",
    "start": "1718190",
    "end": "1724850"
  },
  {
    "text": "conversion tool has found and below that are two windows one for the source program unit and one for the target this",
    "start": "1724850",
    "end": "1731510"
  },
  {
    "text": "allows you to see what the problem is and make the appropriate change the object that SCT",
    "start": "1731510",
    "end": "1737510"
  },
  {
    "text": "migrates are listed on the far right this includes all basic things like table indexes and view definitions but",
    "start": "1737510",
    "end": "1745130"
  },
  {
    "text": "also the complex packages and store procedures another trick that SCT has up",
    "start": "1745130",
    "end": "1750800"
  },
  {
    "text": "its sleeve is the ability to process your application source code and fix the sequel statements inline this feature",
    "start": "1750800",
    "end": "1757010"
  },
  {
    "text": "can greatly accelerate application and database migrations especially for poorly written applications with sequel",
    "start": "1757010",
    "end": "1763160"
  },
  {
    "text": "code written throughout them the final thing I want to talk about with respect",
    "start": "1763160",
    "end": "1768830"
  },
  {
    "start": "1766000",
    "end": "1801000"
  },
  {
    "text": "to the schema conversion tool is its most important feature it should also be the first thing you do when you consider",
    "start": "1768830",
    "end": "1774650"
  },
  {
    "text": "a migration of a database that is the assessment report feature once you",
    "start": "1774650",
    "end": "1780620"
  },
  {
    "text": "connect the schema conversion tool to the source database and select what target database you want SCT can produce",
    "start": "1780620",
    "end": "1786500"
  },
  {
    "text": "the assessment report that will list all the issues it found the number of occurrences of them and why it is a",
    "start": "1786500",
    "end": "1792080"
  },
  {
    "text": "problem this is an essential tool for understanding the complexity of a database migration and for assisting",
    "start": "1792080",
    "end": "1799130"
  },
  {
    "text": "with project planning if you are interested in learning more about what I discussed today you have several options",
    "start": "1799130",
    "end": "1805460"
  },
  {
    "start": "1801000",
    "end": "1832000"
  },
  {
    "text": "available to you to gain more confidence and hands-on experience with AWS watch our instructional videos and",
    "start": "1805460",
    "end": "1811640"
  },
  {
    "text": "explore the saf pace self place the labs additionally you can attend our instructor-led classes by qualified AWS",
    "start": "1811640",
    "end": "1818750"
  },
  {
    "text": "instructors and learn how to design deploy and operate highly available cost-effective and secure operations on",
    "start": "1818750",
    "end": "1824750"
  },
  {
    "text": "AWS and finally remember to validate your technical expertise with AWS certification this brings me to the end",
    "start": "1824750",
    "end": "1832970"
  },
  {
    "text": "of my session I hope you found it useful do remember to complete the survey by visiting the link on the screen today",
    "start": "1832970",
    "end": "1838520"
  },
  {
    "text": "with that I would like to thank you for attending AWS innovate",
    "start": "1838520",
    "end": "1844389"
  },
  {
    "text": "you",
    "start": "1845910",
    "end": "1847970"
  }
]