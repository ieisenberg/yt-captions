[
  {
    "text": "good evening everyone thank you for coming my name is Bruno Faria and I'm here Meyer solutions architect with AWS",
    "start": "30",
    "end": "6720"
  },
  {
    "text": "and today I have with me ratash Ritesh and he's a senior product manager at",
    "start": "6720",
    "end": "12870"
  },
  {
    "text": "Vanguard and for our session today we're going to be talking about migrating big",
    "start": "12870",
    "end": "17910"
  },
  {
    "text": "data applications to AWS so part of our agenda here today we're going to",
    "start": "17910",
    "end": "23430"
  },
  {
    "text": "deconstruct some of the on-premise environment and they also check out some of the challenges that on premise",
    "start": "23430",
    "end": "29519"
  },
  {
    "text": "solutions might have then we're going to talk about migrating each individual component into a lasagna amar",
    "start": "29519",
    "end": "36480"
  },
  {
    "text": "as well as other AWS analytic services and then last off ritesh we'll be finishing off and",
    "start": "36480",
    "end": "42960"
  },
  {
    "text": "telling us how Vanguard performed their migration over to AWS Amazon EMR now I'm",
    "start": "42960",
    "end": "50879"
  },
  {
    "text": "gonna run the through this a bit quick so I would like maybe to leave a few minutes for you guys to have any",
    "start": "50879",
    "end": "56219"
  },
  {
    "text": "questions that you may want to ask so if you can bear with me and I'm gonna run through this and hopefully we can cover",
    "start": "56219",
    "end": "62730"
  },
  {
    "text": "everything here today so we're going to start off with deconstructing the",
    "start": "62730",
    "end": "68460"
  },
  {
    "text": "on-premise environments and let's take a look for example of a cluster of a 1u",
    "start": "68460",
    "end": "74610"
  },
  {
    "text": "server that has maybe 32 to 64 gigabytes of memory 6 to 9 terabytes of hard drive",
    "start": "74610",
    "end": "82140"
  },
  {
    "text": "as well as 12 cores a server with those packs might usually run for three to",
    "start": "82140",
    "end": "87930"
  },
  {
    "text": "four thousand dollars and that's also not accounting for your network switches your racks that you have to purchase and",
    "start": "87930",
    "end": "94170"
  },
  {
    "text": "also the open source hadoop distribution or a commercial one if you're using that and also one of the big factors that we",
    "start": "94170",
    "end": "101189"
  },
  {
    "text": "have here is the storage take something like HDFS which by default is three",
    "start": "101189",
    "end": "107340"
  },
  {
    "text": "times the replication factor and for that you need three times the amount of storage needed also we usually typically",
    "start": "107340",
    "end": "116460"
  },
  {
    "text": "see it on premise environment where every single workload type actually runs on the same cluster so take your",
    "start": "116460",
    "end": "122729"
  },
  {
    "text": "large-scale ETL your machine learning your no sequel data warehouse you see",
    "start": "122729",
    "end": "128190"
  },
  {
    "text": "all these workloads and applications all operating within the same cluster which this leads us",
    "start": "128190",
    "end": "133830"
  },
  {
    "text": "this situation we see here well here we have a swim lane of jobs and if we refer here to the left side there are five",
    "start": "133830",
    "end": "140610"
  },
  {
    "text": "different analytics applications or workload types we can see up at the top particular hours in the day so if we",
    "start": "140610",
    "end": "147690"
  },
  {
    "text": "take over here at the beginning portion a day for a few hours there the cluster is over utilize where you have every",
    "start": "147690",
    "end": "154380"
  },
  {
    "text": "single one of those workloads running at the same time now at other times of the day there's only two workload running",
    "start": "154380",
    "end": "161040"
  },
  {
    "text": "which that makes it for an underutilized cluster so he releases over utilize at",
    "start": "161040",
    "end": "166170"
  },
  {
    "text": "peak hours and underutilized situation for the cluster for an on-premise",
    "start": "166170",
    "end": "171270"
  },
  {
    "text": "solution there's also the role of the Big Data administrator take for example that big data or Hadoop",
    "start": "171270",
    "end": "176459"
  },
  {
    "text": "atom admin they need to be able to get hardware and be able to replace hard drive backing up the solution taking",
    "start": "176459",
    "end": "183660"
  },
  {
    "text": "care of disaster recovery amongst tuning jobs configuration management and many other things so this you need an entire",
    "start": "183660",
    "end": "191190"
  },
  {
    "text": "team to be able to dedicate their work hours for that so let's talk about some",
    "start": "191190",
    "end": "197100"
  },
  {
    "text": "of the challenges some of the main challenges that you have with this on pram or a managed environment one of the",
    "start": "197100",
    "end": "203310"
  },
  {
    "text": "things is a tightly coupled compute and memory storage a tight copley storage",
    "start": "203310",
    "end": "209790"
  },
  {
    "text": "with your compute resources so that means that usually if you need a scale simply based to accommodate more storage",
    "start": "209790",
    "end": "216000"
  },
  {
    "text": "for your cluster let's say you need more HDFS you're going to lead into a situation where your compute and memory",
    "start": "216000",
    "end": "222150"
  },
  {
    "text": "resources are underutilized so this also taken back to the swimlane of jobs that",
    "start": "222150",
    "end": "227370"
  },
  {
    "text": "we saw earlier can be a cluster that's over utilized at peak hours and underutilized at other times in the day",
    "start": "227370",
    "end": "232890"
  },
  {
    "text": "the end result here is that it is high cost and low efficiency also system",
    "start": "232890",
    "end": "239550"
  },
  {
    "text": "management difficulties with an on parameter or managed environment you need to be able to have durable and",
    "start": "239550",
    "end": "245870"
  },
  {
    "text": "disaster recovery durable storage and disaster recovery for it and part of that is actually a lot of work",
    "start": "245870",
    "end": "252030"
  },
  {
    "text": "especially as your data grows so when you're talking about terabytes or even petabytes of data we all know that it's",
    "start": "252030",
    "end": "258030"
  },
  {
    "text": "not going to be easy for you to be able to keep that as a durable storage or as well as managing disaster recovery",
    "start": "258030",
    "end": "263760"
  },
  {
    "text": "solutions for it also managing the distributor applications which is also not an easy",
    "start": "263760",
    "end": "268770"
  },
  {
    "text": "task itself and managing multiple environments like you EQA your dev and test environments all of these are",
    "start": "268770",
    "end": "275250"
  },
  {
    "text": "things that adds on to the complexity of being able to manage this environment yourself and which requires you to have",
    "start": "275250",
    "end": "282000"
  },
  {
    "text": "an entire team dedicated for that so let's talk about now migrating these",
    "start": "282000",
    "end": "287280"
  },
  {
    "text": "components into AWS for migrating these workloads some of the key migration",
    "start": "287280",
    "end": "292650"
  },
  {
    "text": "technologies some of the key migration considerations we have here the first one that we're going to be",
    "start": "292650",
    "end": "298350"
  },
  {
    "text": "talking about is to do not lift and shift and what we mean by that is many time a customer might look at a",
    "start": "298350",
    "end": "304800"
  },
  {
    "text": "migration as simply well let me provision a equivalent cluster of ec2",
    "start": "304800",
    "end": "309900"
  },
  {
    "text": "instances where it has the same amount of disk space same amount of memory as well as same CPU and that's going to be",
    "start": "309900",
    "end": "317490"
  },
  {
    "text": "it and then I simply need to lift and ship that over and I'm done well it's",
    "start": "317490",
    "end": "322590"
  },
  {
    "text": "not usually the case so if you're migrating to technology such as Amazon EMR or other analytic services within",
    "start": "322590",
    "end": "328979"
  },
  {
    "text": "AWS you should not lift and shift instead you should they construct your workloads and use the right tool for the",
    "start": "328979",
    "end": "335430"
  },
  {
    "text": "job we all know there's the best amount of applications in the Big Data realm and these applications can do many",
    "start": "335430",
    "end": "342030"
  },
  {
    "text": "things but there is usually one particular application that does something better than others which we'll talk about later on index which is going",
    "start": "342030",
    "end": "350099"
  },
  {
    "text": "to be a primary focus here on this session today is about decoupling your compute and your storage with Amazon s3",
    "start": "350099",
    "end": "357020"
  },
  {
    "text": "so Amazon s3 is pretty much the bread and butter for many of the analytic services as a backing store for it and",
    "start": "357020",
    "end": "364050"
  },
  {
    "text": "we're going to be talking about how you should be decoupling that storage for building an application or a design",
    "start": "364050",
    "end": "370020"
  },
  {
    "text": "that's actually low cost and scalable first thing let's deconstruct workloads",
    "start": "370020",
    "end": "377520"
  },
  {
    "text": "and take a look at some of the different analytic types and frameworks now there's actually best amount where a",
    "start": "377520",
    "end": "383760"
  },
  {
    "text": "large amount of different workload types but what I can fit in the slides I kind",
    "start": "383760",
    "end": "389190"
  },
  {
    "text": "of put on the the core ones that we talked about here today so first we have an example of a batch workload which",
    "start": "389190",
    "end": "395520"
  },
  {
    "text": "that might include a daily or weekly reporting tool and that might take minutes or hours to run you also have your",
    "start": "395520",
    "end": "402880"
  },
  {
    "text": "interactive workload which might include something like using spark or so an",
    "start": "402880",
    "end": "409360"
  },
  {
    "text": "example of using for that is a service dashboard and that and again a it takes",
    "start": "409360",
    "end": "414850"
  },
  {
    "text": "minutes to run and you could use something like redshift Amazon Athena as well as presto and spark for it next we",
    "start": "414850",
    "end": "422230"
  },
  {
    "text": "also have stream workload types which takes milliseconds to seconds to run and those usually might include your spark",
    "start": "422230",
    "end": "428170"
  },
  {
    "text": "streaming application or flank Amazon camellia Kinesis analytics and so on and",
    "start": "428170",
    "end": "434620"
  },
  {
    "text": "one of the newest one that I decided to add it here is also your AI or artificial intelligence workloads which",
    "start": "434620",
    "end": "440680"
  },
  {
    "text": "may take milliseconds and minutes to run and that includes something maybe like fraud detection or some kind of",
    "start": "440680",
    "end": "446380"
  },
  {
    "text": "prediction text-to-speech and so on so we're not going to touch too much on that but I just felt like you should",
    "start": "446380",
    "end": "451750"
  },
  {
    "text": "also list as it's been a pretty hot topic lately is about machine learning as well as artificial intelligence so",
    "start": "451750",
    "end": "459910"
  },
  {
    "text": "translating the use cases with the right tools as I mentioned earlier many applications can do the same thing but",
    "start": "459910",
    "end": "466480"
  },
  {
    "text": "that doesn't mean that you should use that one single application to perform all of your workloads so what we talked",
    "start": "466480",
    "end": "472330"
  },
  {
    "text": "about here take for example here are some of the applications that we have within amazonia Mar that might include",
    "start": "472330",
    "end": "478000"
  },
  {
    "text": "hive big spark sequel streaming and so on some of these will fall under your",
    "start": "478000",
    "end": "483160"
  },
  {
    "text": "batch category also interactive amongst other things now one of the things that",
    "start": "483160",
    "end": "489580"
  },
  {
    "text": "I'd like to talk about in particular that we start off is a storage layer so underneath over here back in all of this",
    "start": "489580",
    "end": "495850"
  },
  {
    "text": "we have our storage in this case Amazon EMR it works really well with Amazon s3",
    "start": "495850",
    "end": "501220"
  },
  {
    "text": "through what we call EMR FS and this is what we're going to be talking about next which is the storage that you",
    "start": "501220",
    "end": "507640"
  },
  {
    "text": "should be using for an environment as well as talking about best practices for it and what you should use that as well",
    "start": "507640",
    "end": "513400"
  },
  {
    "text": "as optimizations and tunings they can apply for your storage or using Amazon s3 for your storage so first off there",
    "start": "513400",
    "end": "521229"
  },
  {
    "text": "are many storage layers that you can choose from take it for example um are over here where you can choose between",
    "start": "521229",
    "end": "526240"
  },
  {
    "text": "s3 redshift Kinesis RDS and so on we're not going to be talking about all of",
    "start": "526240",
    "end": "531910"
  },
  {
    "text": "these storage today and we primarily going to be concentrated on Amazon s3 since that is the bread and",
    "start": "531910",
    "end": "537980"
  },
  {
    "text": "butter for many of the analytic services in which we'll show later on down some slides so why Amazon s3 as your",
    "start": "537980",
    "end": "545329"
  },
  {
    "text": "persistent data store here are some of the different benefits that Amazon s3 will provide for a first office 3 it is",
    "start": "545329",
    "end": "552829"
  },
  {
    "text": "natively supported by all the big data frameworks like spark I've presto and others and also a big important factor",
    "start": "552829",
    "end": "559459"
  },
  {
    "text": "here is that it allows you to decouple your storage and compute so there's no more need for you to run compute and",
    "start": "559459",
    "end": "566110"
  },
  {
    "text": "nodes for your storage so this means that you need to scale your storage and",
    "start": "566110",
    "end": "571130"
  },
  {
    "text": "you're using s3 for that you no longer need to be thinking about any nodes to your cluster s3 is scalable",
    "start": "571130",
    "end": "577040"
  },
  {
    "text": "automatically so you don't have to worry about none none of that it also allows you to run this transient versus",
    "start": "577040",
    "end": "583670"
  },
  {
    "text": "long-running clusters so taking transient clusters which is a cluster that you run for a job and it gets",
    "start": "583670",
    "end": "589610"
  },
  {
    "text": "terminated once that job is done if your data is being stored in s3 there's no need for you to keep that cluster around",
    "start": "589610",
    "end": "596029"
  },
  {
    "text": "if you're now running a job so again you can have a very cost-effective and scalable environment for them and one of",
    "start": "596029",
    "end": "603170"
  },
  {
    "text": "the other things that comes with that is that there are multiple heterogeneous applications and services that can also",
    "start": "603170",
    "end": "608540"
  },
  {
    "text": "use that same data take for example a redshift spectrum or glue athina all of",
    "start": "608540",
    "end": "614810"
  },
  {
    "text": "these are services that actually uses data directly from s3 which allows you to be also maybe you're running a",
    "start": "614810",
    "end": "621110"
  },
  {
    "text": "particular machine learning from an EMR cluster while you're doing your ad hoc queries from Athena elsewhere all using",
    "start": "621110",
    "end": "627589"
  },
  {
    "text": "that same data set also has three is used for eleven nines of durability and",
    "start": "627589",
    "end": "633190"
  },
  {
    "text": "you keep with that a very low cost for your infrastructure now what about HDFS as well as data",
    "start": "633190",
    "end": "640160"
  },
  {
    "text": "tearing HDFS is still available in EMR if you choose to use some of the use",
    "start": "640160",
    "end": "645320"
  },
  {
    "text": "cases for you to still use HDFS it can be for your very frequently accessed data or your hot data so if you have a",
    "start": "645320",
    "end": "652339"
  },
  {
    "text": "use case such as that you could for example store your data in HDFS and those results you can then move it over",
    "start": "652339",
    "end": "658550"
  },
  {
    "text": "to s3 once the job is complete also you can use your Amazon glacier for cold",
    "start": "658550",
    "end": "665480"
  },
  {
    "text": "data or storage archiving and as well as the other different kind of data tiers for Amazon s3 so talking",
    "start": "665480",
    "end": "673820"
  },
  {
    "text": "about the couple storage and compute that's the example that I was talking about earlier here in the middle we have our compute and take for example service",
    "start": "673820",
    "end": "680960"
  },
  {
    "text": "like Amazon EMR redshift spectrum and Athena all of these can be using that same data set from s3 directly while if",
    "start": "680960",
    "end": "689270"
  },
  {
    "text": "you're needing to have a persistent meta store you can also today use your external meta star ending there RDS or",
    "start": "689270",
    "end": "695870"
  },
  {
    "text": "actually use our newest feature for EMR one of our newest feature which is keeping your data catalog using glue",
    "start": "695870",
    "end": "702470"
  },
  {
    "text": "data catalog for your meta store which will be one of the points that we'll be talking about here shortly what are some",
    "start": "702470",
    "end": "709370"
  },
  {
    "text": "of the tips for s3 to make it optimized for you to run as three as your data",
    "start": "709370",
    "end": "714680"
  },
  {
    "text": "store first of the some of the considerations we have here is partitioning compression as well as file",
    "start": "714680",
    "end": "720650"
  },
  {
    "text": "formats so when you partition your data you're actually reduced amount the data the amount of data that needs to be",
    "start": "720650",
    "end": "726620"
  },
  {
    "text": "scanned so that's going to not only improve the performance of your job but if you're using something like Amazon",
    "start": "726620",
    "end": "732770"
  },
  {
    "text": "Athena where you'll pay for how much data gets read or how much data gets scan for a service like that you're also",
    "start": "732770",
    "end": "738980"
  },
  {
    "text": "going to optimize the cost for it so this has a two-way street here that not only are optimizing the job itself the",
    "start": "738980",
    "end": "746270"
  },
  {
    "text": "performance but you're also optimizing the cost for it also you should avoid you should avoid small files or a large",
    "start": "746270",
    "end": "753440"
  },
  {
    "text": "amount of small files all that same as three locations so you should optimize your file sizes which typically means",
    "start": "753440",
    "end": "760070"
  },
  {
    "text": "you should keep them closer to the HDFS block size which might be 128 megabytes so fewer files mention the HDFS block",
    "start": "760070",
    "end": "768500"
  },
  {
    "text": "size that means that there will be fewer calls to as three fewer listings and better performance overall next you",
    "start": "768500",
    "end": "775760"
  },
  {
    "text": "should compress your data to also minimize the amount of bandwidth going from ec2 to s3 and that also it's going",
    "start": "775760",
    "end": "782750"
  },
  {
    "text": "to improve not only the performance before something like Athena where you're paying for what you read once again optimizing the cost for it",
    "start": "782750",
    "end": "789530"
  },
  {
    "text": "and lastly you should utilize a file format like Parque Oro RC which can also",
    "start": "789530",
    "end": "795170"
  },
  {
    "text": "improve the performance of your reads so here are some quick examples of data",
    "start": "795170",
    "end": "800540"
  },
  {
    "text": "part stationing where we can do an altar table where you can do an MMS seek a",
    "start": "800540",
    "end": "805579"
  },
  {
    "text": "repair table for a hive compatible format but these are just an example to outline how to partition the data for",
    "start": "805579",
    "end": "813199"
  },
  {
    "text": "file formats in particular talking about column or formats we have parquet and or RC and that includes automatically",
    "start": "813199",
    "end": "821179"
  },
  {
    "text": "compression or better compression for your data set also it is column or based which means it is read optimized if",
    "start": "821179",
    "end": "827959"
  },
  {
    "text": "you're having a particular column as part of your filtering and it is also integrated with index and stats for row",
    "start": "827959",
    "end": "834410"
  },
  {
    "text": "formats we have a verb which is also good for compression it is row based",
    "start": "834410",
    "end": "839689"
  },
  {
    "text": "optimized and is also integrated with indexes and stats and last we have a text format that includes CSV or your",
    "start": "839689",
    "end": "847369"
  },
  {
    "text": "JSON file formats but for a text format it's usually going to be slow not very",
    "start": "847369",
    "end": "854179"
  },
  {
    "text": "you know we might may or may not have compression but the good thing about is that is very easy to use and it's very",
    "start": "854179",
    "end": "860329"
  },
  {
    "text": "malleable and generic so here's some quick to example craze that we ran to",
    "start": "860329",
    "end": "866389"
  },
  {
    "text": "kind of a list the difference performance from the tube so here both of these are actually data stored in s3",
    "start": "866389",
    "end": "872660"
  },
  {
    "text": "but here if we do a select count for a CSV file format we can see that the",
    "start": "872660",
    "end": "877670"
  },
  {
    "text": "first example using text format it took 36 seconds and 15.9 gigabytes of data",
    "start": "877670",
    "end": "883490"
  },
  {
    "text": "was scan now the same X actually query but run in front of your park' file",
    "start": "883490",
    "end": "888649"
  },
  {
    "text": "format it took only 5 seconds and only 4.9 gigabytes of data scan so here this",
    "start": "888649",
    "end": "894559"
  },
  {
    "text": "kind of outlines the difference between using a column or format versus something else like CSV or TSV and so on",
    "start": "894559",
    "end": "903069"
  },
  {
    "text": "and to wrap up some of the final some of our file considerations here is that for",
    "start": "903069",
    "end": "909139"
  },
  {
    "text": "scanning if you're actually looking for particular columns you should maybe look into a column or format like bar K or I",
    "start": "909139",
    "end": "915740"
  },
  {
    "text": "see if you have read access that's based on a row format where you're utilizing era just a portion of your rows then",
    "start": "915740",
    "end": "923149"
  },
  {
    "text": "something like Avro or a row optimize format might be good for that and as far as write performance goals usually it's",
    "start": "923149",
    "end": "929870"
  },
  {
    "text": "going to pen on your use case but text usually tends to be very slow",
    "start": "929870",
    "end": "935470"
  },
  {
    "text": "so talking about the external hive Mehta store which I mentioned earlier with EMR",
    "start": "935470",
    "end": "940520"
  },
  {
    "text": "today if you're trying to use an external meta store you have two options one is to use an external RDS instance",
    "start": "940520",
    "end": "947960"
  },
  {
    "text": "and the other one is to use the data catalog Gloup Gloup data catalog feature and with glue data catalog you have the",
    "start": "947960",
    "end": "956060"
  },
  {
    "text": "search meta store and you can also keep your scheme and version management and you can utilize blue scroller crawlers",
    "start": "956060",
    "end": "962390"
  },
  {
    "text": "to be able to detect schema changes whenever something happens with your datum and also the other good thing",
    "start": "962390",
    "end": "968360"
  },
  {
    "text": "about the glue data catalog for your external meta store is that it allows multiple services to share that exact",
    "start": "968360",
    "end": "974660"
  },
  {
    "text": "same meta store and that includes services like Amazon Athena as well as red ships spectrum and EMR and glue",
    "start": "974660",
    "end": "982010"
  },
  {
    "text": "itself all of these services can share the exact same meta store for your jobs and to use glue as your external meta",
    "start": "982010",
    "end": "989000"
  },
  {
    "text": "store is simply a check of a button through the AWS console if you're launching the closer through it or you",
    "start": "989000",
    "end": "994400"
  },
  {
    "text": "could also just type in the parameter option for that if you're launching the cluster via the SDK or CLI but here the",
    "start": "994400",
    "end": "1002380"
  },
  {
    "text": "the one thing that I would like to as a side note is that these iam are just getting updated so quickly that just",
    "start": "1002380",
    "end": "1010090"
  },
  {
    "text": "last week we actually announced we released our newest EMR release which includes presto support for using glues",
    "start": "1010090",
    "end": "1017920"
  },
  {
    "text": "data catalog so here we have hive and spark but just note that last week we also released press o support for it so",
    "start": "1017920",
    "end": "1027069"
  },
  {
    "text": "change a bit of pace let's talk about cost optimization and in particular about EMR and how you can optimize your",
    "start": "1027070",
    "end": "1034180"
  },
  {
    "text": "cost and build a scalable environment so one of the newer features with EMR is",
    "start": "1034180",
    "end": "1039579"
  },
  {
    "text": "the instance fleet features which allows for advanced spot provision of ec2 instances I'm not sure how many of you",
    "start": "1039579",
    "end": "1046449"
  },
  {
    "text": "already experienced with using spot instances but previously a lot of our customers had issues with it because",
    "start": "1046449",
    "end": "1052390"
  },
  {
    "text": "sometimes it requires you to know what is the bid history what's going to be",
    "start": "1052390",
    "end": "1057610"
  },
  {
    "text": "the buy the best bid that you can make on a particular spot and so on so by",
    "start": "1057610",
    "end": "1063310"
  },
  {
    "text": "introducing the instance fleet feature it pretty much takes care of all that provision",
    "start": "1063310",
    "end": "1068470"
  },
  {
    "text": "for you where it's going to pick out of a list of availability zones something that he can make the best spot request",
    "start": "1068470",
    "end": "1074620"
  },
  {
    "text": "for your cluster also this takes us to the back to our swimlane of jobs to",
    "start": "1074620",
    "end": "1081549"
  },
  {
    "text": "where now you got to make a decision of running a transient cluster versus a long-running cluster so if we look back",
    "start": "1081549",
    "end": "1087820"
  },
  {
    "text": "our swimlane of jobs we see that two perfect candidates for a long-running cluster would be here no sequel or",
    "start": "1087820",
    "end": "1094510"
  },
  {
    "text": "something like your real-time streaming application that needs to be running at all times so for these you can have",
    "start": "1094510",
    "end": "1100000"
  },
  {
    "text": "leveraged a transient cluster and you spot instances for it in this case wow",
    "start": "1100000",
    "end": "1105250"
  },
  {
    "text": "I'm sorry for these you can use a long-running cluster and also have reserved instances so on but then we",
    "start": "1105250",
    "end": "1112179"
  },
  {
    "text": "have our transient a cluster use cases which those are your batch reporting or your analytics that runs in some few",
    "start": "1112179",
    "end": "1118690"
  },
  {
    "text": "times in the day for those you can take advantage of using your spot instances and only pay for what you use and to add",
    "start": "1118690",
    "end": "1125860"
  },
  {
    "text": "to this another newer feature that's been released for Amazon ec2 as well as",
    "start": "1125860",
    "end": "1131260"
  },
  {
    "text": "DMR is the concept of you pay per second billing so before you had to launch an",
    "start": "1131260",
    "end": "1138130"
  },
  {
    "text": "instance and you usually pay for the entire hour once you launch an instance for UMR but this is no longer the case",
    "start": "1138130",
    "end": "1143740"
  },
  {
    "text": "which now you pay literally for the minute that you're using an instance so if you launch an instance you can use",
    "start": "1143740",
    "end": "1149020"
  },
  {
    "text": "for two minutes and you're going to pay for just those two minutes which brings us to our auto scaling feature which",
    "start": "1149020",
    "end": "1155740"
  },
  {
    "text": "GEOMAR has also automatically auto auto scaling built into it to where you can",
    "start": "1155740",
    "end": "1160780"
  },
  {
    "text": "configure cloud watch metrics to be able to scale out and scale in your cluster or add and remove nodes based on",
    "start": "1160780",
    "end": "1167289"
  },
  {
    "text": "particular metrics that might be the amount of memory that the cluster is utilizing the amount of containers you have running and once again all of this",
    "start": "1167289",
    "end": "1174880"
  },
  {
    "text": "is you're only paying for what you use and to wrap up over here with one of our",
    "start": "1174880",
    "end": "1180580"
  },
  {
    "text": "final slides is the securities security and governance and auditing and here are some of the different features that we",
    "start": "1180580",
    "end": "1187450"
  },
  {
    "text": "have and the different services now one thing to know about that is just last week we actually also released a newer",
    "start": "1187450",
    "end": "1195039"
  },
  {
    "text": "feature for EMR two different security features which one of them is a security feature that allows for",
    "start": "1195039",
    "end": "1201840"
  },
  {
    "text": "kerberos native Kerber support in EMR so today you can just with a click of a button in a configuration setting you",
    "start": "1201840",
    "end": "1208320"
  },
  {
    "text": "can kerberized all the applications within a within EMR and be able to integrate that with something like",
    "start": "1208320",
    "end": "1213870"
  },
  {
    "text": "Active Directory and the other feature that we also release is a storage based permissions for EMR FS which now you can",
    "start": "1213870",
    "end": "1221700"
  },
  {
    "text": "have so an OS user inside the EMR cluster whenever the operating system",
    "start": "1221700",
    "end": "1227070"
  },
  {
    "text": "gets logged in or SSH it's going to assume its own I M roles and get the",
    "start": "1227070",
    "end": "1232470"
  },
  {
    "text": "permissions of that particular M role so you no longer have to have you know a cluster dedicated for different",
    "start": "1232470",
    "end": "1238380"
  },
  {
    "text": "permissions so you can have now EMR FS or s3 fine granted permissions with it",
    "start": "1238380",
    "end": "1243750"
  },
  {
    "text": "so with that said I'm gonna hand off now to Ritesh and he's going to be covering",
    "start": "1243750",
    "end": "1248880"
  },
  {
    "text": "about Vanguard's migration story bruna",
    "start": "1248880",
    "end": "1256260"
  },
  {
    "text": "thank you for that short introduction rita Shahir work in cloud analytics",
    "start": "1256260",
    "end": "1262200"
  },
  {
    "text": "Services team as a program manager it's part of cloud chief technology officer",
    "start": "1262200",
    "end": "1268830"
  },
  {
    "text": "within Vanguard before I take you all",
    "start": "1268830",
    "end": "1273960"
  },
  {
    "text": "through the Vanguard journey for migrating to EMR I want to quit show of",
    "start": "1273960",
    "end": "1279840"
  },
  {
    "text": "hands for three questions right first one how many of you know Vanguard as a",
    "start": "1279840",
    "end": "1285210"
  },
  {
    "text": "company ok good second question how many of you use",
    "start": "1285210",
    "end": "1292290"
  },
  {
    "text": "Hadoop on Prem today third question and",
    "start": "1292290",
    "end": "1298080"
  },
  {
    "text": "the last question how many of you are either in process of migrating or",
    "start": "1298080",
    "end": "1303260"
  },
  {
    "text": "considering migration to EMR a lot of her ok so I'm gonna get the first",
    "start": "1303260",
    "end": "1311730"
  },
  {
    "text": "question I answered for people who don't know Wanger first thing first Vanguard",
    "start": "1311730",
    "end": "1316799"
  },
  {
    "text": "is one of the world's largest investment management company core purpose is to",
    "start": "1316799",
    "end": "1323669"
  },
  {
    "text": "take a stand for all investors treat them fairly and give them the best",
    "start": "1323669",
    "end": "1329730"
  },
  {
    "text": "chance for investment success that's what one God stands for",
    "start": "1329730",
    "end": "1336400"
  },
  {
    "text": "while I walk you through the winegar journey we have three sections that we'll go through first we'll go through",
    "start": "1336400",
    "end": "1343990"
  },
  {
    "text": "deconstructing on premise what unpromising up infrastructure second",
    "start": "1343990",
    "end": "1350420"
  },
  {
    "text": "section would be d coming or deconstructing or demystifying EMR and",
    "start": "1350420",
    "end": "1355700"
  },
  {
    "text": "then third what's next for Vanguard",
    "start": "1355700",
    "end": "1360700"
  },
  {
    "text": "quick raise of hands also four how many of you have seen this kind of Hadoop",
    "start": "1361090",
    "end": "1367640"
  },
  {
    "text": "cluster workload at your place at your company big small any size right this is",
    "start": "1367640",
    "end": "1373550"
  },
  {
    "text": "a typical setup for a Hadoop cluster where you have different types of data",
    "start": "1373550",
    "end": "1381050"
  },
  {
    "text": "stores running on premise or off premise and data for it is brought into a Hadoop",
    "start": "1381050",
    "end": "1389060"
  },
  {
    "text": "cluster and then you have different kind of analytics done on Hadoop cluster",
    "start": "1389060",
    "end": "1396470"
  },
  {
    "text": "using different components right you have a data analyst you have a data",
    "start": "1396470",
    "end": "1401810"
  },
  {
    "text": "engineer or a data Wrangler or you have a data scientist accessing this one",
    "start": "1401810",
    "end": "1407960"
  },
  {
    "text": "Hadoop cluster where you have large volume of data and you typically end up",
    "start": "1407960",
    "end": "1415010"
  },
  {
    "text": "using components like core Hadoop hive you see scoop to bring that data in",
    "start": "1415010",
    "end": "1423550"
  },
  {
    "text": "sentry if you use cloud era you use that to secure your cluster for from an",
    "start": "1423550",
    "end": "1430130"
  },
  {
    "text": "authentication and authorization perspective many of your analysts and",
    "start": "1430130",
    "end": "1435440"
  },
  {
    "text": "scientists use Impala through a Hadoop user experience interface and then you",
    "start": "1435440",
    "end": "1442550"
  },
  {
    "text": "may have enabled spark Python and tableau for your analysts and consumers",
    "start": "1442550",
    "end": "1448880"
  },
  {
    "text": "of analyst analytics next why migrate",
    "start": "1448880",
    "end": "1454970"
  },
  {
    "text": "from on-premise to AWS most of you may have run in one of these situations",
    "start": "1454970",
    "end": "1462850"
  },
  {
    "text": "where you your infrastructure for compute and storage is tightly coupled",
    "start": "1462850",
    "end": "1470110"
  },
  {
    "text": "second your cluster gets over utilized at peak hours and underutilized at non",
    "start": "1470110",
    "end": "1477440"
  },
  {
    "text": "peak hours there is cross impact from different workloads a data scientist",
    "start": "1477440",
    "end": "1484370"
  },
  {
    "text": "running a experiment versus an analyst trying to just analyze sets of data you",
    "start": "1484370",
    "end": "1490610"
  },
  {
    "text": "may have cross impacts many a times having one cluster humongous one gets",
    "start": "1490610",
    "end": "1498230"
  },
  {
    "text": "you into a situation of not having a disaster recovery kind of environment if needed and the last one on this section",
    "start": "1498230",
    "end": "1506090"
  },
  {
    "text": "is you have dedicated teams to maintain this cluster day-in day-out for upgrades",
    "start": "1506090",
    "end": "1513080"
  },
  {
    "text": "adding new hardware software patches etc",
    "start": "1513080",
    "end": "1519010"
  },
  {
    "text": "more reasons many a times big enterprise companies run into long procurement",
    "start": "1519010",
    "end": "1525860"
  },
  {
    "text": "cycles for procuring the hardware and attaching it to your Hadoop cluster long",
    "start": "1525860",
    "end": "1532760"
  },
  {
    "text": "setup times to add that hardware complex upgrade coordination because it's a",
    "start": "1532760",
    "end": "1539870"
  },
  {
    "text": "multi-tenant environment you have to convince everybody to migrate to a newer",
    "start": "1539870",
    "end": "1545090"
  },
  {
    "text": "version of Hadoop at the same time which all leads to high total cost of",
    "start": "1545090",
    "end": "1551120"
  },
  {
    "text": "ownership and difficult to charge back to line of business on what's the actual",
    "start": "1551120",
    "end": "1557330"
  },
  {
    "text": "cost they are incurring for running a Hadoop cluster moving on to the second",
    "start": "1557330",
    "end": "1565340"
  },
  {
    "text": "section or demystifying EMR workloads let's start with the first important",
    "start": "1565340",
    "end": "1574340"
  },
  {
    "text": "thing that one got dead right they were before moving to EMR they set up a foundational requirements they wanted to",
    "start": "1574340",
    "end": "1581210"
  },
  {
    "text": "achieve let's start with secure when we wanted to move to EMR we wanted our data",
    "start": "1581210",
    "end": "1590990"
  },
  {
    "text": "to be secured in a way that data being moved to EMR would be encrypted in",
    "start": "1590990",
    "end": "1596780"
  },
  {
    "text": "flight and when it lands on s3 we wanted it encrypted",
    "start": "1596780",
    "end": "1601840"
  },
  {
    "text": "second piece we wanted a flexible workload where we could have our compute",
    "start": "1601840",
    "end": "1608980"
  },
  {
    "text": "be customizable based on what workload it is being used for third one would be",
    "start": "1608980",
    "end": "1616270"
  },
  {
    "text": "reduced the administration so embrace managed services and managed way of",
    "start": "1616270",
    "end": "1622870"
  },
  {
    "text": "working in EMR all of that leading to lower total cost of ownership where you",
    "start": "1622870",
    "end": "1630730"
  },
  {
    "text": "paper usage bruno mentioned they have introduced per second billing you can get to a lower total cost of ownership",
    "start": "1630730",
    "end": "1639390"
  },
  {
    "text": "it also provides we wonderful control where we wanted our infrastructure to be",
    "start": "1639390",
    "end": "1647279"
  },
  {
    "text": "spun up or spun down as code and not a physical hardware that needs to be spun",
    "start": "1647279",
    "end": "1654460"
  },
  {
    "text": "up or spun down and last we wanted a scalable compute and storage when we",
    "start": "1654460",
    "end": "1662860"
  },
  {
    "text": "meant scalable we wanted to scale the storage irrespective of having to scale the compute Hardware next one we are",
    "start": "1662860",
    "end": "1673539"
  },
  {
    "text": "going to dive start diving deeper into different workloads let's start with an",
    "start": "1673539",
    "end": "1679149"
  },
  {
    "text": "ingestion workload for EMR here you would see different data stores just like Hadoop data flowing into EMR and",
    "start": "1679149",
    "end": "1687880"
  },
  {
    "text": "EMR being the single point through which the data would get written into s3 which",
    "start": "1687880",
    "end": "1694210"
  },
  {
    "text": "would be our data leak where all the data resides we utilized EMR s3 as a",
    "start": "1694210",
    "end": "1701500"
  },
  {
    "text": "derelict all the data was encrypted using TMS and all the data was secured",
    "start": "1701500",
    "end": "1709390"
  },
  {
    "text": "through iam policies we use step actions and step api's to launch our OC",
    "start": "1709390",
    "end": "1717130"
  },
  {
    "text": "workflows we also used lot of alternative mechanisms to bring data in",
    "start": "1717130",
    "end": "1723309"
  },
  {
    "text": "we use snowball we use at Unity which is a change data capture tool and we also",
    "start": "1723309",
    "end": "1729520"
  },
  {
    "text": "use scoop to ingest data from a processing perspective",
    "start": "1729520",
    "end": "1734690"
  },
  {
    "text": "use we use hive our spark or combination of both based on the needs and we use",
    "start": "1734690",
    "end": "1743000"
  },
  {
    "text": "cloud watch in conjunction with Splunk to monitor all the injection processes",
    "start": "1743000",
    "end": "1748130"
  },
  {
    "text": "that are happening we also learnt lessons our land along our journey will",
    "start": "1748130",
    "end": "1755840"
  },
  {
    "text": "we quickly learned segregation of infrastructure and code is important we",
    "start": "1755840",
    "end": "1761750"
  },
  {
    "text": "need to keep our core to come for compute separate from the business logic",
    "start": "1761750",
    "end": "1769120"
  },
  {
    "text": "lessons learned around instance type and cluster sizing that we would use each",
    "start": "1769120",
    "end": "1774710"
  },
  {
    "text": "workload to bring data would require a different cluster sizing and instance",
    "start": "1774710",
    "end": "1779750"
  },
  {
    "text": "type exercise we decided to office Kate all known PII DDoS and segregation the",
    "start": "1779750",
    "end": "1788270"
  },
  {
    "text": "data based on different business lines diving further deeper into conceptual",
    "start": "1788270",
    "end": "1796370"
  },
  {
    "text": "diagram this diagram shows three different compute mechanisms first one",
    "start": "1796370",
    "end": "1803480"
  },
  {
    "text": "is we spin up ephemeral EMR clusters for ingestion",
    "start": "1803480",
    "end": "1808670"
  },
  {
    "text": "we have FTP nodes and then we have persistent FTP nodes let's start with an",
    "start": "1808670",
    "end": "1816350"
  },
  {
    "text": "example where you would have a relational database we ingest that data using EMR scooped and loaded into s3",
    "start": "1816350",
    "end": "1826840"
  },
  {
    "text": "while doing that we also braid hive tables on top of it and catalog it using",
    "start": "1826840",
    "end": "1833780"
  },
  {
    "text": "hive meta store another workload where we need real-time or semi semi real-time",
    "start": "1833780",
    "end": "1841310"
  },
  {
    "text": "data we use at unity to replicate that data into post grass SQL and then we",
    "start": "1841310",
    "end": "1849440"
  },
  {
    "text": "utilize the EMR cluster to process it and put it into s3 and build hive tables",
    "start": "1849440",
    "end": "1856610"
  },
  {
    "text": "on top of it we also have FTP processes",
    "start": "1856610",
    "end": "1862460"
  },
  {
    "text": "where we use data to be pulled in using ephemeral FTP",
    "start": "1862460",
    "end": "1868320"
  },
  {
    "text": "nones and then put it into s3 bucket this FTP is secure FTP or SCP so that",
    "start": "1868320",
    "end": "1876120"
  },
  {
    "text": "data is encrypted and transit and then we do have third-party vendors that ship",
    "start": "1876120",
    "end": "1883740"
  },
  {
    "text": "data to angered and we have an internal team that takes that data and pushes it",
    "start": "1883740",
    "end": "1889980"
  },
  {
    "text": "into s3 using persistent ec2 ftp nodes",
    "start": "1889980",
    "end": "1896000"
  },
  {
    "text": "now moving on to the second section a different workload it's around analytics",
    "start": "1896000",
    "end": "1901230"
  },
  {
    "text": "workload typically you will have an EMR cluster that works with data in s3 and",
    "start": "1901230",
    "end": "1908460"
  },
  {
    "text": "the consumers are data analysts data Wranglers and data scientists typically",
    "start": "1908460",
    "end": "1916530"
  },
  {
    "text": "you will have a persistent EMR cluster that would auto scale you would use hive",
    "start": "1916530",
    "end": "1923070"
  },
  {
    "text": "presto as your query engines and spark",
    "start": "1923070",
    "end": "1928740"
  },
  {
    "text": "and Python for distributed processing we use u and tableau as user interactive",
    "start": "1928740",
    "end": "1936000"
  },
  {
    "text": "tools Zeppelin and Jupiter based notebooks that we are about to introduce",
    "start": "1936000",
    "end": "1941070"
  },
  {
    "text": "for interactive and collaborative data exploration all the authorization is",
    "start": "1941070",
    "end": "1947640"
  },
  {
    "text": "done using hive sequel art and I am bucket policies on Amazon s3 continuing",
    "start": "1947640",
    "end": "1956310"
  },
  {
    "text": "the same theme what did we learn we we quickly learned that data governance is",
    "start": "1956310",
    "end": "1962040"
  },
  {
    "text": "and very important thing we should have the metadata lineage and other data",
    "start": "1962040",
    "end": "1967770"
  },
  {
    "text": "governance capability is enabled on the platform auditing who ran what query who",
    "start": "1967770",
    "end": "1974220"
  },
  {
    "text": "accessed what data is very important instance type and cluster sizing for",
    "start": "1974220",
    "end": "1980790"
  },
  {
    "text": "different workloads is important and lastly but most important integration of",
    "start": "1980790",
    "end": "1987360"
  },
  {
    "text": "all these user tools with processing engines we learnt a lot while",
    "start": "1987360",
    "end": "1993450"
  },
  {
    "text": "establishing all that at Vanguard diving deeper or one level deep into a",
    "start": "1993450",
    "end": "2000620"
  },
  {
    "text": "conceptual diagram we will have query clusters that get spun up using control-m and these query",
    "start": "2000620",
    "end": "2009080"
  },
  {
    "text": "clusters would interact with hive meta store for information about the tables",
    "start": "2009080",
    "end": "2016250"
  },
  {
    "text": "and would you utilize s3 for your data we also enable tabular integration using",
    "start": "2016250",
    "end": "2024050"
  },
  {
    "text": "these query clusters so that it can be used for visualization the other",
    "start": "2024050",
    "end": "2030740"
  },
  {
    "text": "workload that I talked about was around data science EMR clusters these EMR",
    "start": "2030740",
    "end": "2036320"
  },
  {
    "text": "clusters would typically run SPARC Python and Zeppelin and they would work",
    "start": "2036320",
    "end": "2041810"
  },
  {
    "text": "with hive presto and s3 as the storage and here are the three different actors",
    "start": "2041810",
    "end": "2051350"
  },
  {
    "text": "or data analysts there are anglers and Lyra scientists working with appropriate",
    "start": "2051350",
    "end": "2057159"
  },
  {
    "text": "components of the EMR workload last",
    "start": "2057160",
    "end": "2062690"
  },
  {
    "text": "section what's next for a vanguard most important thing real time data ingestion",
    "start": "2062690",
    "end": "2068320"
  },
  {
    "text": "enhanced data processing pipelines visualization and analytics use case",
    "start": "2068320",
    "end": "2074679"
  },
  {
    "text": "experimentation with AI and machine learning those are the four areas when God's going to start focusing on in",
    "start": "2074680",
    "end": "2081350"
  },
  {
    "text": "future and it's going to use plenty of other AWS services to move down this",
    "start": "2081350",
    "end": "2088460"
  },
  {
    "text": "path at this point I'll give it two back to Bruno all right thank you very much",
    "start": "2088460",
    "end": "2098090"
  },
  {
    "text": "Ritesh thank you also at this moment I would like to give you a moment for any questions or anything particularly like",
    "start": "2098090",
    "end": "2104840"
  },
  {
    "text": "to talk about and feel free I don't believe we have a microphone for you",
    "start": "2104840",
    "end": "2109940"
  },
  {
    "text": "guys but feel free to approach here if you have any questions and we can talk about it thanks again everyone for",
    "start": "2109940",
    "end": "2115550"
  },
  {
    "text": "coming and once again please remember to write this in a mobile app",
    "start": "2115550",
    "end": "2121060"
  },
  {
    "text": "[Applause]",
    "start": "2121060",
    "end": "2123090"
  }
]