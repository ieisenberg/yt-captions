[
  {
    "start": "0",
    "end": "204000"
  },
  {
    "text": "hello everyone welcome thanks for coming I'm Dave Killian I'm an engineer at snapchat and I work on the storage",
    "start": "30",
    "end": "6450"
  },
  {
    "text": "infrastructure team and I'm here today to tell you about how we migrated a critical storage use case over to",
    "start": "6450",
    "end": "13110"
  },
  {
    "text": "dynamodb so before I get into that I'll explain a little bit about what snapshots about so you could think of",
    "start": "13110",
    "end": "19980"
  },
  {
    "text": "snapchat as communicating with pictures or videos we call snapchat messages snaps and snaps are ephemeral so that",
    "start": "19980",
    "end": "27210"
  },
  {
    "text": "means they get deleted by defaults and because messages get deleted by default a moment I capture doesn't have to be",
    "start": "27210",
    "end": "35250"
  },
  {
    "text": "super amazing or important and to share with my friends so when I'm interacting with my friends on snapchat it tends to",
    "start": "35250",
    "end": "41430"
  },
  {
    "text": "feel a lot more like an in-person conversation that I might with other other messaging platforms and because",
    "start": "41430",
    "end": "48719"
  },
  {
    "text": "there's fewer barriers to sharing and most the content is pretty recent snapchat just tends to have a more in",
    "start": "48719",
    "end": "53879"
  },
  {
    "text": "the moment feel to it so a part of snapchat is the stories feature so all",
    "start": "53879",
    "end": "59609"
  },
  {
    "text": "you need to know for now is that that's one of the most popular ways people communicate on snapchat which is why",
    "start": "59609",
    "end": "65670"
  },
  {
    "text": "back on New Year's Eve last year I was super nervous I was worried because we",
    "start": "65670",
    "end": "71640"
  },
  {
    "text": "just migrated this story juiced kiss over to Dinah what do you be and I knew what was coming so we had these huge",
    "start": "71640",
    "end": "80220"
  },
  {
    "text": "traffic spikes every midnight on New Year's Eve and the new system we'd built",
    "start": "80220",
    "end": "85530"
  },
  {
    "text": "was about to get put to its ultimate test so load on the system was going to be several times higher than we'd ever seen",
    "start": "85530",
    "end": "91530"
  },
  {
    "text": "before we were pretty sure it was going to work but we couldn't be positive but",
    "start": "91530",
    "end": "97860"
  },
  {
    "text": "first let's go all the way back to the beginning of the project so I joined snapchat and around this time I joined",
    "start": "97860",
    "end": "105299"
  },
  {
    "text": "the storage infrastructure team in Seattle when that office was starting up and to do some context on snaps so we've",
    "start": "105299",
    "end": "111780"
  },
  {
    "text": "grown a lot over the years and we're at really high scale now and we spend",
    "start": "111780",
    "end": "117060"
  },
  {
    "text": "hundreds of millions of dollars per year on our back-end cloud services so reducing costs was a big part of our",
    "start": "117060",
    "end": "124290"
  },
  {
    "text": "team's goals also snapchat runs primarily in Google cloud platform so we",
    "start": "124290",
    "end": "132090"
  },
  {
    "text": "run in App Engine there and primarily and we use a lot of the managed services available to us",
    "start": "132090",
    "end": "137780"
  },
  {
    "text": "there but around this time snapchat had made a decision that strategically we",
    "start": "137780",
    "end": "143660"
  },
  {
    "text": "wanted to be on multiple cloud providers in the long run given the scale we were at and other reasons so our team figured",
    "start": "143660",
    "end": "152000"
  },
  {
    "text": "maybe we could kill two birds with one stone we figured there must be some storage use cases at snapchat that given",
    "start": "152000",
    "end": "160940"
  },
  {
    "text": "their characteristics would be a better fit for the trade-offs of other storage services so we collected a lot of data",
    "start": "160940",
    "end": "170150"
  },
  {
    "text": "about snapchat story juice cases did a bunch of number crunching did a bunch of",
    "start": "170150",
    "end": "175370"
  },
  {
    "text": "analysis and there was one use case that kept surfacing to the top over and over again and that was story inboxes so it",
    "start": "175370",
    "end": "185150"
  },
  {
    "text": "still is not an easy call to pick this use case to migrate there was a lot of risks involved with it there was",
    "start": "185150",
    "end": "190760"
  },
  {
    "text": "definitely easier use cases we could have migrated but before we get into that I'll just explain what the stories",
    "start": "190760",
    "end": "196549"
  },
  {
    "text": "feature is real quick and then take you through the technical backend of the story in boxes piece ok so user",
    "start": "196549",
    "end": "204920"
  },
  {
    "start": "204000",
    "end": "204000"
  },
  {
    "text": "experience of posting a story so when you open snapchat it opens up to the camera screen so this is what I usually",
    "start": "204920",
    "end": "211340"
  },
  {
    "text": "see when I open the camera screen my feet there and then I might take a picture of something and you get to edit it in various ways this is a snap from",
    "start": "211340",
    "end": "218319"
  },
  {
    "text": "last Thanksgiving in my family and then you choose who you send it to and you",
    "start": "218319",
    "end": "223760"
  },
  {
    "text": "can send it in individual people or you can choose to send it to your story which means all your friends will get to",
    "start": "223760",
    "end": "230569"
  },
  {
    "text": "see it for the next 24 hours after which it goes away so here's what viewing",
    "start": "230569",
    "end": "236060"
  },
  {
    "text": "stories looks like I open the app it goes to the camera screen I swipe over to the right and I can see all my",
    "start": "236060",
    "end": "242930"
  },
  {
    "text": "friends stories there are all my friends that have active stories and then I",
    "start": "242930",
    "end": "248060"
  },
  {
    "text": "might tap on one and then it'll play back their story snaps in chronological order like a story ok so now that we",
    "start": "248060",
    "end": "255799"
  },
  {
    "text": "understand a little bit more about the user experience let's get into the technical aspect so first what's the",
    "start": "255799",
    "end": "261079"
  },
  {
    "text": "story in box so this is a pretty simple concept every story post has an idea",
    "start": "261079",
    "end": "267260"
  },
  {
    "text": "with it and every user has an inbox and that inbox just holds all the story post",
    "start": "267260",
    "end": "273350"
  },
  {
    "text": "IDs that are incoming to that user so my inbox is going to contain all the ideas all my friends story posts for the past",
    "start": "273350",
    "end": "279620"
  },
  {
    "text": "24 hours and the data model is pretty simple too so the row is keyed off a user identifier and there's a column in",
    "start": "279620",
    "end": "286070"
  },
  {
    "text": "there that has a serialized list of all these story at post IDs",
    "start": "286070",
    "end": "291730"
  },
  {
    "start": "293000",
    "end": "293000"
  },
  {
    "text": "so here's architecture I'll take you through posting a story all this area in",
    "start": "295000",
    "end": "300440"
  },
  {
    "text": "the cloud here is Google cloud platform at the time so keep that in mind so most of these are managed services there so",
    "start": "300440",
    "end": "307070"
  },
  {
    "text": "say I'm posting this story it sends a request to the backend it hits an instance in App Engine we associate an",
    "start": "307070",
    "end": "313730"
  },
  {
    "text": "ID with that story post this is just an arbitrary ID I'm creating here we persist the media itself and some",
    "start": "313730",
    "end": "320210"
  },
  {
    "text": "metadata about the story and a storage system there and then we in queue a",
    "start": "320210",
    "end": "325430"
  },
  {
    "text": "series of tasks and this is kind of simplifying it really we in queue one task that's going to in cute asks but",
    "start": "325430",
    "end": "331100"
  },
  {
    "text": "this these tasks are going to be in charge of adding that story post ID to all my friends in boxes but here we just",
    "start": "331100",
    "end": "338360"
  },
  {
    "text": "in queue that and we return back to the user once it's successfully in queued and it says their story was posted",
    "start": "338360",
    "end": "344210"
  },
  {
    "text": "successfully and they're good to go so I'll zoom in on what these fan out",
    "start": "344210",
    "end": "349790"
  },
  {
    "start": "348000",
    "end": "348000"
  },
  {
    "text": "tasks look like so this is happening behind the scenes after the user posts a",
    "start": "349790",
    "end": "355010"
  },
  {
    "text": "story so in this example we have three tasks those will get run by that same",
    "start": "355010",
    "end": "360110"
  },
  {
    "text": "fleet and then those tasks will interact with the story inboxes persistence layer to add that ID to all my friends in",
    "start": "360110",
    "end": "366320"
  },
  {
    "text": "boxes and then once the tasks run they're longer in the queue and that process is done I'll zoom in further on",
    "start": "366320",
    "end": "373310"
  },
  {
    "text": "this so the same example there's three tasks running this first one is going to add that ID to some subset of my friends",
    "start": "373310",
    "end": "380840"
  },
  {
    "text": "and the way it does that is basically just to read modify write transaction so we start a transaction we read the row",
    "start": "380840",
    "end": "387260"
  },
  {
    "text": "we deserialize the existing list of story IDs we add the new story post ID to the end we put that column back in",
    "start": "387260",
    "end": "395210"
  },
  {
    "text": "the row write the row back to storage commit the transaction that's it and then so we would do that for all",
    "start": "395210",
    "end": "401030"
  },
  {
    "text": "three of these friends here and then these other tasks are going to do the exact same thing for other subset of",
    "start": "401030",
    "end": "406520"
  },
  {
    "text": "subsets of my friends and by the time it's done they all have this new story post ID in their inboxes so I'll take",
    "start": "406520",
    "end": "416930"
  },
  {
    "start": "415000",
    "end": "415000"
  },
  {
    "text": "you through the read side of this so say my brother opens the device goes over to a story screen it's going to send a",
    "start": "416930",
    "end": "424910"
  },
  {
    "text": "request to update stories there's a caching layer in front of these storage systems so we first retrieve his inbox",
    "start": "424910",
    "end": "431450"
  },
  {
    "text": "you can see my new story post IDs there we use that inbox to fetch some other",
    "start": "431450",
    "end": "436790"
  },
  {
    "text": "metadata about some of these stories and then we construct a response send it back to the device and then it can",
    "start": "436790",
    "end": "442310"
  },
  {
    "text": "rerender the story screen with all the new posts and then if you taps on my story it's going to retrieve that media",
    "start": "442310",
    "end": "448010"
  },
  {
    "text": "from a media delivery system we have okay so we understand a little bit about",
    "start": "448010",
    "end": "455120"
  },
  {
    "text": "the architecture here so what was it about story in boxes that made it look so attractive so when we estimated cost",
    "start": "455120",
    "end": "461740"
  },
  {
    "text": "across a bunch of storage systems this particular one it showed up that we",
    "start": "461740",
    "end": "467690"
  },
  {
    "text": "would eliminate almost all the costs on DynamoDB so that was really surprising and we weren't sure what to make of it",
    "start": "467690",
    "end": "474980"
  },
  {
    "text": "we figured it might be wrong and even if it wasn't wrong there's maybe something we can learn about this so we dug into",
    "start": "474980",
    "end": "480380"
  },
  {
    "text": "the details so explain here more about that so this is just some properties of",
    "start": "480380",
    "end": "487010"
  },
  {
    "start": "482000",
    "end": "482000"
  },
  {
    "text": "this use case in relation to the Dynamo building model so one thing we're built for is total data stored and in this use",
    "start": "487010",
    "end": "493820"
  },
  {
    "text": "case is basically nothing compared to the other costs and that's because the data is ephemeral primarily also because",
    "start": "493820",
    "end": "499910"
  },
  {
    "text": "these story post IDs don't take up much data but the inboxes are not",
    "start": "499910",
    "end": "505850"
  },
  {
    "text": "proportional to how long I've been a user so they don't accumulate data over time since they're only the posts from",
    "start": "505850",
    "end": "511400"
  },
  {
    "text": "the past 24 hours so the size of the table might fluctuate with how many",
    "start": "511400",
    "end": "517039"
  },
  {
    "text": "posts are active across the whole system or might grow with user growth over time but it's not going to accumulate data",
    "start": "517040",
    "end": "523430"
  },
  {
    "text": "depending on how long someone been a user so network bandwidth was a new cost",
    "start": "523430",
    "end": "528460"
  },
  {
    "text": "since we were trying to cross cloud providers to storage here we're getting billed for",
    "start": "528460",
    "end": "534250"
  },
  {
    "text": "work ban with both ways so that was not a trivial cost but not the bulk of it",
    "start": "534250",
    "end": "539410"
  },
  {
    "text": "either similar with reeds they were significant but the real story here is right so this is a really right heavy",
    "start": "539410",
    "end": "546550"
  },
  {
    "text": "use case and remember that's because every post we're doing an inbox right for every one of your friends so there's",
    "start": "546550",
    "end": "552670"
  },
  {
    "text": "a multiplying effect there now so rights were the bulk of the cost and they're",
    "start": "552670",
    "end": "560050"
  },
  {
    "text": "also the most resource intensive so we're really going to focus on rights and we talked about costs so I'll talk a",
    "start": "560050",
    "end": "571000"
  },
  {
    "start": "569000",
    "end": "569000"
  },
  {
    "text": "little bit about dynamos billing model in case you're not familiar and this also kind of helped us explain why it",
    "start": "571000",
    "end": "577990"
  },
  {
    "text": "might be a lot cheaper on dynamo basically if you imagine a billing model where you just build per row written",
    "start": "577990",
    "end": "583870"
  },
  {
    "text": "that be that blue line and the cost per write doesn't change with how big the row is but with dynamos billing model",
    "start": "583870",
    "end": "591010"
  },
  {
    "text": "it's more of a throughput based billing model so you can imagine it like you're billed per kilobyte written and so the",
    "start": "591010",
    "end": "597460"
  },
  {
    "text": "larger the row the more expensive it's going to be per writes and story inboxes kind of falls in this lower left",
    "start": "597460",
    "end": "603640"
  },
  {
    "text": "quadrant here so these tend to be really small rows so this is kind of how we rationalize to ourselves yeah okay so it",
    "start": "603640",
    "end": "610390"
  },
  {
    "text": "makes sense that this would be a lot cheaper because we're kind of benefiting from only writing a small amount of data",
    "start": "610390",
    "end": "617850"
  },
  {
    "text": "another thing we really need to understand as we talk about costs here is with dynamo we're not actually",
    "start": "620640",
    "end": "628000"
  },
  {
    "text": "getting billed for how much of that write throughput we use it's how much we allocate so we pick an allocation and",
    "start": "628000",
    "end": "635350"
  },
  {
    "text": "that's that would be that orange line at the top that we need that green line would be just like an example daily",
    "start": "635350",
    "end": "641770"
  },
  {
    "text": "pattern of the write throughput we consume so we need to pick an allocation that will meet those Peaks and then",
    "start": "641770",
    "end": "647980"
  },
  {
    "text": "we're billed for the allocation not the green lines so we're build more based on that orange line there and so at night",
    "start": "647980",
    "end": "653860"
  },
  {
    "text": "we have unused capacity but we are being billed still for that capacity to be allocated okay so it makes more sense to",
    "start": "653860",
    "end": "662800"
  },
  {
    "start": "660000",
    "end": "660000"
  },
  {
    "text": "us that this will be a lot cheaper there was a lot going for picking this use case one is",
    "start": "662800",
    "end": "667840"
  },
  {
    "text": "scale so we do millions of rights per second with this use case and that kind of magnifies the cost savings in terms",
    "start": "667840",
    "end": "675520"
  },
  {
    "text": "of savings as I said we estimated we'd eliminate most of it and its millions",
    "start": "675520",
    "end": "680950"
  },
  {
    "text": "per year complexity wise this use case is also easier to migrate because it's",
    "start": "680950",
    "end": "687160"
  },
  {
    "text": "ephemeral so instead of having a copy data from an old system to a new system we could just dual write to the old",
    "start": "687160",
    "end": "693460"
  },
  {
    "text": "system and new system and then 24 hours later they're both in sync more or less also since all these writes",
    "start": "693460",
    "end": "701980"
  },
  {
    "text": "happen asynchronously and those fan out tasks we have a little leeway with",
    "start": "701980",
    "end": "707230"
  },
  {
    "text": "introducing some new latency or failures and retries there because small delays in that fan-out",
    "start": "707230",
    "end": "712630"
  },
  {
    "text": "path aren't really noticed by users unless they're they're pretty significant because people don't know",
    "start": "712630",
    "end": "717850"
  },
  {
    "text": "exactly when their friend posted a story so they don't know if there was a small increase in delays when they first see",
    "start": "717850",
    "end": "723460"
  },
  {
    "text": "that story so there's a lot going for this use case but there's a lot of open",
    "start": "723460",
    "end": "729430"
  },
  {
    "text": "questions concerns at the same time a really big one was is making cross cloud",
    "start": "729430",
    "end": "735250"
  },
  {
    "text": "calls really going to be viable especially for a use case of this scale we weren't sure how much latency we were",
    "start": "735250",
    "end": "742750"
  },
  {
    "text": "to introduce how much how many failures we were worried about how often that network path would go down and what it",
    "start": "742750",
    "end": "749080"
  },
  {
    "text": "would look like for such an outage we also didn't know if her estimates were right it was actually kind of",
    "start": "749080",
    "end": "754330"
  },
  {
    "text": "complicated estimating and the inputs to our estimates involve other estimates so there's definitely a chance that the",
    "start": "754330",
    "end": "760180"
  },
  {
    "text": "return on this project wouldn't be there we weren't sure we wanted to migrate such a critical piece of our",
    "start": "760180",
    "end": "766600"
  },
  {
    "text": "infrastructure for such an important feature as the first thing we migrated there was definitely easier cases we",
    "start": "766600",
    "end": "773200"
  },
  {
    "text": "could have started with I would just you know kind of dipped our toes in the water and finally we just figured there's",
    "start": "773200",
    "end": "778660"
  },
  {
    "text": "things that we're totally missing that we're not even thinking of here so at",
    "start": "778660",
    "end": "784120"
  },
  {
    "text": "this point there's really not much more data we could collect and there's only one way we can answer all these",
    "start": "784120",
    "end": "789850"
  },
  {
    "text": "questions and so this is going to be we're going to enter sort of the building phase of the project here so we",
    "start": "789850",
    "end": "798520"
  },
  {
    "start": "798000",
    "end": "798000"
  },
  {
    "text": "wanted to build a production shadow experiments and there's some portent things we wanted to get out of this one we wanted",
    "start": "798520",
    "end": "805390"
  },
  {
    "text": "to mirror real production operations so if we had built an artificial load test or something we might just be baking in",
    "start": "805390",
    "end": "811930"
  },
  {
    "text": "our false assumptions into that test so - we wanted to run it at full scale",
    "start": "811930",
    "end": "817200"
  },
  {
    "text": "because at full scale there's just things that might happen or we might see or problems that might arise that we",
    "start": "817200",
    "end": "823300"
  },
  {
    "text": "won't see otherwise and three we wanted to run it for a significant period of time so we wanted to see it go through",
    "start": "823300",
    "end": "830620"
  },
  {
    "text": "those occasional outages or just really weird unusual events and have confidence that it's not going to cause a",
    "start": "830620",
    "end": "836440"
  },
  {
    "text": "catastrophic event or a feedback loop or something like that so we enter a",
    "start": "836440",
    "end": "842170"
  },
  {
    "start": "842000",
    "end": "842000"
  },
  {
    "text": "building phase here and I'll summarize it pretty quickly so fortunately it's snapchat most of our storage use cases",
    "start": "842170",
    "end": "848410"
  },
  {
    "text": "go through a small number of really high level storage interfaces so we were able",
    "start": "848410",
    "end": "853720"
  },
  {
    "text": "to leverage that and plug in underneath that so we built a storage framework where we could plug in different storage capabilities and we could go to",
    "start": "853720",
    "end": "860470"
  },
  {
    "text": "different storage backends like DynamoDB so we got that working we built the DynamoDB plugin for it got that working",
    "start": "860470",
    "end": "867550"
  },
  {
    "text": "we built this read and write shadow mode where without changing business logic we could shadow reads and writes and",
    "start": "867550",
    "end": "873670"
  },
  {
    "text": "control the percentage that of traffic that we're doing that with so we got this out to production and we started",
    "start": "873670",
    "end": "879730"
  },
  {
    "text": "ramping it up we ramped it up to 100% so we're excited to see all these benefits",
    "start": "879730",
    "end": "886600"
  },
  {
    "text": "we were looking for so one was cost so that's one of the first ones we wanted to see and it was not panning out it was",
    "start": "886600",
    "end": "895420"
  },
  {
    "text": "a lot more expensive than we expected it was multiple times higher than we estimated not sure why latency so we",
    "start": "895420",
    "end": "902890"
  },
  {
    "text": "actually done before this production experiment some much simpler experiments we just did a bunch of round trips to",
    "start": "902890",
    "end": "908680"
  },
  {
    "text": "Dynamo across cloud providers and we measured tens of milliseconds in latency there so that'd be totally workable but",
    "start": "908680",
    "end": "916120"
  },
  {
    "text": "it was way worse than that it was hundreds of milliseconds latency didn't know why so keep in mind here we're",
    "start": "916120",
    "end": "921760"
  },
  {
    "text": "going across cloud providers availability also not what we expected",
    "start": "921760",
    "end": "928230"
  },
  {
    "text": "so at least we failed fast so at this point of the project was kind of a low",
    "start": "928230",
    "end": "933820"
  },
  {
    "text": "point but at the same time when we're surprised by results we figure there's probably something we can",
    "start": "933820",
    "end": "939100"
  },
  {
    "text": "learn here and we were excited to dig into the data and so the first thing we saw when we started digging in was this",
    "start": "939100",
    "end": "946600"
  },
  {
    "text": "right off the bat this is from the DynamoDB console so this is our right",
    "start": "946600",
    "end": "951880"
  },
  {
    "start": "947000",
    "end": "947000"
  },
  {
    "text": "throughput over a 24-hour period now we're used to seeing a particular pattern every day at snapchat and across",
    "start": "951880",
    "end": "959440"
  },
  {
    "text": "a lot of use cases it looks similar and this one looks way different and in",
    "start": "959440",
    "end": "964570"
  },
  {
    "text": "particular you can see there's this weird drop in the middle of the day and that happens every single day and this",
    "start": "964570",
    "end": "972490"
  },
  {
    "text": "is strange too because we had never looked at this graph before like we're typically typically looking at per call",
    "start": "972490",
    "end": "978010"
  },
  {
    "text": "rate graphs so we're kind of like turning over a rock here and looking underneath for the first time so we look",
    "start": "978010",
    "end": "984310"
  },
  {
    "text": "closer at this graph and we see this drop and you might notice something's odd about it it happens at midnight and",
    "start": "984310",
    "end": "991200"
  },
  {
    "text": "that's UTC time so we figure there must be something weird going on with the",
    "start": "991200",
    "end": "996340"
  },
  {
    "text": "code here so we dig into the code and look for a some piece of code that might be dealing with midnight and we find",
    "start": "996340",
    "end": "1004350"
  },
  {
    "text": "something that looks like this so I didn't talk a lot about how we actually",
    "start": "1004350",
    "end": "1010650"
  },
  {
    "start": "1006000",
    "end": "1006000"
  },
  {
    "text": "remove expired IDs from these inboxes but all you all you really need to know is that we calculate this first time",
    "start": "1010650",
    "end": "1018030"
  },
  {
    "text": "it'll be removed so the way we calculate that is it's basically midnight of next day so when I post a story it'll expire",
    "start": "1018030",
    "end": "1026010"
  },
  {
    "text": "in 24 hours but the first time we remove it is the following midnight I'll also",
    "start": "1026010",
    "end": "1032670"
  },
  {
    "text": "note here this is totally independent of how we removed the the media and metadata for or sorry the media for the",
    "start": "1032670",
    "end": "1041430"
  },
  {
    "text": "actual story and other metadata so those are all removed shortly after the story expires so the inbox itself works a",
    "start": "1041430",
    "end": "1047819"
  },
  {
    "text": "little bit differently and also since there's expired IDs in these rows we",
    "start": "1047820",
    "end": "1054930"
  },
  {
    "text": "filter those out on the read side so users never see these expired IDs we're going to show you a visualization here",
    "start": "1054930",
    "end": "1060900"
  },
  {
    "text": "of this effect of the way we expire these or the way we actually purge them",
    "start": "1060900",
    "end": "1066500"
  },
  {
    "text": "so one of these story or sorry this is an inbox over a few days so say this is my",
    "start": "1066500",
    "end": "1072150"
  },
  {
    "text": "inbox and every one of my friends post is one of these bars and it's green when it's active and then it turns to orange",
    "start": "1072150",
    "end": "1078060"
  },
  {
    "text": "one is expired so there's posts that are overlapping and so at any given time there's a",
    "start": "1078060",
    "end": "1084210"
  },
  {
    "text": "number of active and I'm going to overlay what my Inbox might look like at different times throughout the day so",
    "start": "1084210",
    "end": "1091440"
  },
  {
    "text": "you can see right before midnight my inbox has eight IDs in it and half of them are expired and then shortly after",
    "start": "1091440",
    "end": "1099180"
  },
  {
    "text": "midnight those get removed and the inbox size has a 50% drop and all four of the",
    "start": "1099180",
    "end": "1105870"
  },
  {
    "text": "IDS in it are active and then you can see the rest of the day we start to see that bloat again the inbox keeps getting",
    "start": "1105870",
    "end": "1111720"
  },
  {
    "text": "larger with more expired IDs and then it drops again the following midnight so",
    "start": "1111720",
    "end": "1118050"
  },
  {
    "text": "when we understood this better we look back at that other graph and it makes a lot more sense so that daily drop was",
    "start": "1118050",
    "end": "1123540"
  },
  {
    "text": "about 50% every day sure enough and because UTC midnight happens to be right",
    "start": "1123540",
    "end": "1129420"
  },
  {
    "text": "after our daily peak that means we go through our peak every day with inboxes",
    "start": "1129420",
    "end": "1135030"
  },
  {
    "text": "that are almost twice as big as they could be if they didn't have those expired IDs in them and because that's",
    "start": "1135030",
    "end": "1140760"
  },
  {
    "text": "hitting our peak it actually is almost doubling our costs but that's not the",
    "start": "1140760",
    "end": "1147480"
  },
  {
    "text": "only thing that made our estimates off so even if we change the way this works they'd still be off so here's another",
    "start": "1147480",
    "end": "1153720"
  },
  {
    "text": "thing that had happened we had a bad input to one of our cost estimates we",
    "start": "1153720",
    "end": "1160350"
  },
  {
    "start": "1154000",
    "end": "1154000"
  },
  {
    "text": "calculated right through put as number of writes which we had historical data on and we multiplied rights by average",
    "start": "1160350",
    "end": "1167400"
  },
  {
    "text": "row size to get throughput and we knew that this was problematic but I think it's slipped into our estimates somehow",
    "start": "1167400",
    "end": "1172740"
  },
  {
    "text": "and I'll explain why that's problematic so basically the write throughput is skewed by the more active users so the",
    "start": "1172740",
    "end": "1180690"
  },
  {
    "text": "largest inboxes they take up the most right throughput but they all soar the",
    "start": "1180690",
    "end": "1186960"
  },
  {
    "text": "ones written to the most often so if you imagine my inbox tends to have n IDs in",
    "start": "1186960",
    "end": "1193590"
  },
  {
    "text": "it and that just it tends to be n from day to day how many times is my inbox written per day its",
    "start": "1193590",
    "end": "1201860"
  },
  {
    "text": "because there's n IDs in it those all had to have been added in the past 24 hours so really if you think of my inbox",
    "start": "1201860",
    "end": "1209360"
  },
  {
    "text": "just throughput per day its n writes a day times n IDs per right so it's N",
    "start": "1209360",
    "end": "1214940"
  },
  {
    "text": "squared IDs are written to storage every day another way to get this across if my",
    "start": "1214940",
    "end": "1221600"
  },
  {
    "text": "inbox is 10 times larger than the average users inbox it's going to contribute to cost a hundred times more",
    "start": "1221600",
    "end": "1226910"
  },
  {
    "text": "than the average user and that's just not reflected when we use average row size in our estimate we we have to use",
    "start": "1226910",
    "end": "1234260"
  },
  {
    "text": "something like average size of a write",
    "start": "1234260",
    "end": "1238270"
  },
  {
    "text": "so we weren't sure how to deal with this so we just kind of let it let this digest and we looked at the other",
    "start": "1239380",
    "end": "1244460"
  },
  {
    "start": "1244000",
    "end": "1244000"
  },
  {
    "text": "problem so one latency this turned out to be poor persistent connection reuse so when we were making a request to",
    "start": "1244460",
    "end": "1251750"
  },
  {
    "text": "dinamo way too often we had to create a new connection so you can see on this",
    "start": "1251750",
    "end": "1257300"
  },
  {
    "text": "diagram it's it's just showing client-server round trips to establish a",
    "start": "1257300",
    "end": "1262820"
  },
  {
    "text": "TCP connection followed by an SSL session it requires a lot of round trips",
    "start": "1262820",
    "end": "1268310"
  },
  {
    "text": "this is going across cloud providers and that can add tens or even hundreds of milliseconds in latency so why was our",
    "start": "1268310",
    "end": "1276470"
  },
  {
    "text": "persistent connection were used so bad this actually had to do with snapchats architecture so snapchat deploys it",
    "start": "1276470",
    "end": "1282890"
  },
  {
    "start": "1281000",
    "end": "1281000"
  },
  {
    "text": "back-end that's mostly one large monolith there's hundreds of thousands of instances in this back-end there each",
    "start": "1282890",
    "end": "1289550"
  },
  {
    "text": "pretty small and a key thing about them is that they're running business logic",
    "start": "1289550",
    "end": "1295160"
  },
  {
    "text": "for hundreds of different endpoints so a lot of things besides the stories feature and because of that only a few",
    "start": "1295160",
    "end": "1302600"
  },
  {
    "text": "of these instances had active requests to dine about any given time and because",
    "start": "1302600",
    "end": "1308660"
  },
  {
    "text": "only a few connections were active a lot of them stayed idle which gave them a lot more opportunity to just get closed",
    "start": "1308660",
    "end": "1313940"
  },
  {
    "text": "and idle out and so then later we'd have to create new connections so to tackle",
    "start": "1313940",
    "end": "1321080"
  },
  {
    "text": "this we put a proxy between the monolith and dynamodb we put it near our",
    "start": "1321080",
    "end": "1329000"
  },
  {
    "start": "1323000",
    "end": "1323000"
  },
  {
    "text": "instances and we we have thought of doing this early in the project but it actually requires two",
    "start": "1329000",
    "end": "1334119"
  },
  {
    "text": "Forked a Davis SDK we didn't think we need it before but now we realize we do",
    "start": "1334119",
    "end": "1339189"
  },
  {
    "text": "need it so we went ahead and fork the SDK to get this working and we had to fork it because this is actually a",
    "start": "1339189",
    "end": "1345039"
  },
  {
    "text": "managed proxy and it required us to use some special api's and with this we just",
    "start": "1345039",
    "end": "1351459"
  },
  {
    "text": "get to concentrate all that traffic down a small number of very active connections so the proxy has far fewer",
    "start": "1351459",
    "end": "1358419"
  },
  {
    "text": "instances running to proxy those across then the monolith deployment and then",
    "start": "1358419",
    "end": "1363489"
  },
  {
    "text": "each of those just has there's there's just hundreds of connections to Diana instead of potentially millions okay so",
    "start": "1363489",
    "end": "1372719"
  },
  {
    "text": "our experiment didn't really go as planned but it wasn't really a blocker our cost estimates were way worse than",
    "start": "1372719",
    "end": "1379149"
  },
  {
    "text": "we expected but we'd still save money if we move forward at this point the lane C",
    "start": "1379149",
    "end": "1384309"
  },
  {
    "text": "wasn't we expected but it was manageable and we think this proxy would help but at the same time we learned a lot so",
    "start": "1384309",
    "end": "1391509"
  },
  {
    "text": "running this experiment and we figured it'd be a good time to kind of re-evaluate based on what we learned so",
    "start": "1391509",
    "end": "1397899"
  },
  {
    "text": "there's some new optimization vectors we can go down here so storage throughput had never really been looked at or",
    "start": "1397899",
    "end": "1403599"
  },
  {
    "start": "1398000",
    "end": "1398000"
  },
  {
    "text": "optimized just because we never really needed to in the past that's just not how our billing model works so right off",
    "start": "1403599",
    "end": "1411159"
  },
  {
    "text": "the bat compressing inboxes is one thing we could do to reduce throughput and we",
    "start": "1411159",
    "end": "1416289"
  },
  {
    "text": "just didn't experiment that and it cuts through put by half there was the expiration we talked about",
    "start": "1416289",
    "end": "1421389"
  },
  {
    "text": "earlier we could tweak the way that the way that worked and that should also cut our costs in half because it'll cut peak",
    "start": "1421389",
    "end": "1427689"
  },
  {
    "text": "throughput in half but at the same time we're starting to see the system through",
    "start": "1427689",
    "end": "1433719"
  },
  {
    "text": "this lens of throughput and we're gaining a visceral understanding of how it works and so we're starting to see a lot of waste in the system that we",
    "start": "1433719",
    "end": "1440409"
  },
  {
    "text": "didn't see before so we're getting some new ideas let me show you it I mean so",
    "start": "1440409",
    "end": "1445779"
  },
  {
    "start": "1444000",
    "end": "1444000"
  },
  {
    "text": "here's a fan-out task again it basically just does a read-modify-write transaction and so through the lens of",
    "start": "1445779",
    "end": "1452379"
  },
  {
    "text": "calls this is just to read and a write straightforward but if you look at this as throughput it's reading five IDs and",
    "start": "1452379",
    "end": "1459399"
  },
  {
    "text": "their writing six IDs back just to add one new IDs so there's really useless",
    "start": "1459399",
    "end": "1465069"
  },
  {
    "text": "operations on 10 of these IDs so why can't this just be writing one new idea so we explored a new data model",
    "start": "1465069",
    "end": "1474029"
  },
  {
    "start": "1471000",
    "end": "1471000"
  },
  {
    "text": "so basically instead of a row per inbox it would be a row per story post ID so",
    "start": "1475170",
    "end": "1484240"
  },
  {
    "text": "my inbox would be a collection of rows instead of one row and the primary key",
    "start": "1484240",
    "end": "1490809"
  },
  {
    "text": "here basically just starts with the user identifier whose inbox it is and that'd",
    "start": "1490809",
    "end": "1496030"
  },
  {
    "text": "be the hash key and dinamo and then following that would be the expiration time of that story post and then",
    "start": "1496030",
    "end": "1501640"
  },
  {
    "text": "following that the actual story post ID so nothing else in these rows it's just a key I'll show you how we could",
    "start": "1501640",
    "end": "1509170"
  },
  {
    "start": "1508000",
    "end": "1508000"
  },
  {
    "text": "interact with that data model so it's a fetching inbox I basically just have to query where the hash key is my user",
    "start": "1509170",
    "end": "1516730"
  },
  {
    "text": "identifier and the sort key is greater than the current time so that'll give me",
    "start": "1516730",
    "end": "1523080"
  },
  {
    "text": "all the IDS in my inbox that haven't expired yet so to add a new ID to an",
    "start": "1523080",
    "end": "1530890"
  },
  {
    "text": "inbox is pretty simple too you just write one new row with the destination user as the hash key and then following",
    "start": "1530890",
    "end": "1538179"
  },
  {
    "text": "that the expiration following that the story post ID so I'll take you through",
    "start": "1538179",
    "end": "1544030"
  },
  {
    "text": "an illustration of how this works and we'll compare the two models and see the efficiency gains we can get from this so",
    "start": "1544030",
    "end": "1550929"
  },
  {
    "start": "1546000",
    "end": "1546000"
  },
  {
    "text": "assume we're running one fan-out task and that task is going to add one new story post ID to a hundred of my friends",
    "start": "1550929",
    "end": "1558370"
  },
  {
    "text": "and assume that every one of my friends has an inbox it already has a hundred IDs in it and one other thing to call",
    "start": "1558370",
    "end": "1566110"
  },
  {
    "text": "out here is that dynamo allows 25 rows per HTTP requests there are 25 writes",
    "start": "1566110",
    "end": "1572020"
  },
  {
    "text": "and so that will come into play here ok so let's so the blue is going to be the old data model here so in terms of",
    "start": "1572020",
    "end": "1578950"
  },
  {
    "text": "writes through the same like either way we had to write to 100 in boxes in terms",
    "start": "1578950",
    "end": "1583960"
  },
  {
    "text": "of reads before 100 now it's 0 we don't do any reads now in order to add to an",
    "start": "1583960",
    "end": "1589120"
  },
  {
    "text": "inbox we're just writing rows in terms of remote calls before we were reading",
    "start": "1589120",
    "end": "1594250"
  },
  {
    "text": "200 I'm sorry we were reading 100 and writing 100 because every one was read modify right now we're just doing no",
    "start": "1594250",
    "end": "1601149"
  },
  {
    "text": "reads and for remote calls each one has 25 rows were writing now IDs per cause",
    "start": "1601149",
    "end": "1607719"
  },
  {
    "text": "getting into more of a throughput based dimension so our IDs per write rather so",
    "start": "1607719",
    "end": "1613659"
  },
  {
    "text": "before every write was writing 101 IDs back now every write or every row right is just one ID so let's look at this for",
    "start": "1613659",
    "end": "1622089"
  },
  {
    "text": "more of a throughput based dimension IDs written before it was over 10,000",
    "start": "1622089",
    "end": "1627580"
  },
  {
    "text": "because we were writing 100 rows and each one had over 100 IDs for IDs read",
    "start": "1627580",
    "end": "1632649"
  },
  {
    "text": "it was also 10,000 but we're not doing any reads anymore but with the new data model we're just",
    "start": "1632649",
    "end": "1639609"
  },
  {
    "text": "writing a hundred ideas across into storage so there's some other benefits",
    "start": "1639609",
    "end": "1647349"
  },
  {
    "text": "of this one there's no contention so before people with really active inboxes",
    "start": "1647349",
    "end": "1653200"
  },
  {
    "text": "really active users there were there is often some contention problems because it was read modify write and if the row",
    "start": "1653200",
    "end": "1660159"
  },
  {
    "text": "had changed before we wrote we did a contention issue failure now the writes are no longer conditional",
    "start": "1660159",
    "end": "1666190"
  },
  {
    "text": "in any read so that problem goes away the amount of resources we use per write",
    "start": "1666190",
    "end": "1672669"
  },
  {
    "text": "or per adding an ID is constant rather than linear with the size of the inbox that also makes estimate estimating",
    "start": "1672669",
    "end": "1680499"
  },
  {
    "text": "earlier so this is kind of a trip this up before with the skewed consumption by active users so things are just easier",
    "start": "1680499",
    "end": "1685989"
  },
  {
    "text": "to reason about also we're optimizing CPU and network bandwidth here so we saw",
    "start": "1685989",
    "end": "1693729"
  },
  {
    "text": "that there was 10,000 IDs going to storage before and now there's 100 so that correlates with network bandwidth",
    "start": "1693729",
    "end": "1699009"
  },
  {
    "text": "usage to which we're paying for now so almost all that just goes away and before we were had to deserialize and",
    "start": "1699009",
    "end": "1706960"
  },
  {
    "text": "serialize all those IDs and almost all that went away too so at this point",
    "start": "1706960",
    "end": "1712200"
  },
  {
    "text": "optimizing for the storage billing model is having these ripple effects were we're optimizing network and CPU - and",
    "start": "1712200",
    "end": "1719799"
  },
  {
    "text": "we looked at these fan-out tasks and we actually pay millions per year to run just the compute for these fan-out tasks",
    "start": "1719799",
    "end": "1726669"
  },
  {
    "text": "so now we're predicting that almost all that should go away because every dimension we look at here seems like almost all that goes away",
    "start": "1726669",
    "end": "1734940"
  },
  {
    "start": "1736000",
    "end": "1736000"
  },
  {
    "text": "all right I haven't talked about garbage collection how we clean up these expire IDs with the new data model so I'll go",
    "start": "1738060",
    "end": "1744250"
  },
  {
    "text": "through that so as we said before out of the queue comes fan-out tasks and with",
    "start": "1744250",
    "end": "1749860"
  },
  {
    "text": "the new data model we just do a batch right so to do cleanup instead of in queueing one task that's fan-out we in",
    "start": "1749860",
    "end": "1756040"
  },
  {
    "text": "queue two tasks the other one is a cleanup task and this is almost the exact same is the fan-out task except",
    "start": "1756040",
    "end": "1763390"
  },
  {
    "text": "for two differences so one we're just doing a batch delete of the same rows instead of a batch put the others that",
    "start": "1763390",
    "end": "1770620"
  },
  {
    "text": "we scheduled is to run 36 hours in the future so we don't schedule it 24 hours",
    "start": "1770620",
    "end": "1776320"
  },
  {
    "text": "in the future because then our peak delete time would be our peak put time and both deletes and puts contribute to",
    "start": "1776320",
    "end": "1782680"
  },
  {
    "text": "write throughput and we're billed for those so if we schedule a 24 hours in the future we'd be doubling our",
    "start": "1782680",
    "end": "1788680"
  },
  {
    "text": "throughput at peak and doubling our costs so by adding 12 more hours we're",
    "start": "1788680",
    "end": "1793870"
  },
  {
    "text": "offsetting the put and delete cycles and so at night when we're not using much of",
    "start": "1793870",
    "end": "1799420"
  },
  {
    "text": "our capacity we're filling that up with the peak deletes so in the meantime the",
    "start": "1799420",
    "end": "1807370"
  },
  {
    "start": "1804000",
    "end": "1804000"
  },
  {
    "text": "production system of course is still running while we're looking at this and user growth is happening and growth of",
    "start": "1807370",
    "end": "1814390"
  },
  {
    "text": "the stories feature is happening and the production system starts running into some scaling limitations and so",
    "start": "1814390",
    "end": "1822390"
  },
  {
    "text": "occasionally when we hit peak there's some problems with it and sometimes there's some ripple effects of that",
    "start": "1822390",
    "end": "1827560"
  },
  {
    "text": "problem they're even affecting other use cases so the stories back-end team which",
    "start": "1827560",
    "end": "1832630"
  },
  {
    "text": "owns this they were able to mitigate those but at the same time we're",
    "start": "1832630",
    "end": "1838450"
  },
  {
    "text": "starting to get worried because New Year's Eve was on its way and a New Year's Eve the low is going to be",
    "start": "1838450",
    "end": "1844510"
  },
  {
    "text": "several times higher than our normal Peaks and if it's having truck trouble now you know what's going to happen on",
    "start": "1844510",
    "end": "1850510"
  },
  {
    "text": "New Year's Eve so this kind of changes the dynamic of our project so initially it was a cost savings initiative now",
    "start": "1850510",
    "end": "1857560"
  },
  {
    "text": "people are starting to look at it like maybe this is going to be a plan B for New Year's Eve or maybe even a plan a so",
    "start": "1857560",
    "end": "1864640"
  },
  {
    "text": "now there's a new sense of urgency to get this done before New Year's Eve and working okay so our new data model looks",
    "start": "1864640",
    "end": "1874990"
  },
  {
    "text": "promising it seems like it's going to drop the cost to compute network bandwidth storage storage throughput but",
    "start": "1874990",
    "end": "1883960"
  },
  {
    "text": "still like last time we could totally be missing things that we aren't going to find out so we try it so we move forward",
    "start": "1883960",
    "end": "1889420"
  },
  {
    "text": "with a shadow experiment to run the new data model and we get it working we",
    "start": "1889420",
    "end": "1894820"
  },
  {
    "text": "changed we built some prototype business logic to have it run through that path let's see what happens so cost so costs",
    "start": "1894820",
    "end": "1903640"
  },
  {
    "start": "1901000",
    "end": "1901000"
  },
  {
    "text": "looked awesome it was way better than before it was right in line with our estimates the new",
    "start": "1903640",
    "end": "1909370"
  },
  {
    "text": "data model had made it a lot cheaper I'll show you the effect of our puts and deletes staggering here so you can see",
    "start": "1909370",
    "end": "1916660"
  },
  {
    "text": "that is working pretty well and so it's cool to see this graph in action so the sum of those two things would be our",
    "start": "1916660",
    "end": "1923200"
  },
  {
    "text": "write throughput so you can see it almost doesn't even have a daily cycle anymore so when we allocate capacity",
    "start": "1923200",
    "end": "1928780"
  },
  {
    "text": "right above that line we're not wasting much much of it so at night we're still usually most at capacity",
    "start": "1928780",
    "end": "1935520"
  },
  {
    "text": "what about latency so we had built that proxy latency was awesome it looks much",
    "start": "1935520",
    "end": "1940990"
  },
  {
    "text": "better than before you can see the drop here when we turn on that proxy went from hundred milliseconds down to tens",
    "start": "1940990",
    "end": "1947080"
  },
  {
    "text": "so that's more of what we expected availability also looked a lot better so",
    "start": "1947080",
    "end": "1953560"
  },
  {
    "start": "1951000",
    "end": "1951000"
  },
  {
    "text": "I think this is because going through the proxy it was something related to that potentially not having to do all",
    "start": "1953560",
    "end": "1959140"
  },
  {
    "text": "those round trips to establish a connection just left less opportunity for something to fail or go wrong but",
    "start": "1959140",
    "end": "1967480"
  },
  {
    "text": "still a question we got asked a lot was isn't latency availability gonna be worse like we're going across storage",
    "start": "1967480",
    "end": "1973410"
  },
  {
    "text": "providers it's got to be worse isn't that going to be a problem so to tackle",
    "start": "1973410",
    "end": "1978940"
  },
  {
    "text": "this question instead of comparing both storage systems side-by-side and looking at those latency and yeah the new system",
    "start": "1978940",
    "end": "1985540"
  },
  {
    "text": "would add some latency because we're making a call across providers we zoomed out and just looked at the metrics we",
    "start": "1985540",
    "end": "1990850"
  },
  {
    "text": "care about for these user experience so if we look at the time it actually takes to deliver a story to a user or if we",
    "start": "1990850",
    "end": "1998290"
  },
  {
    "text": "look at a time for a user to update their stories we're look at those percentiles there was no",
    "start": "1998290",
    "end": "2004110"
  },
  {
    "text": "distinguishable difference between the two so we couldn't detect this difference and distinction it from the",
    "start": "2004110",
    "end": "2010350"
  },
  {
    "text": "noise and those in those graphs all",
    "start": "2010350",
    "end": "2016170"
  },
  {
    "start": "2016000",
    "end": "2016000"
  },
  {
    "text": "right so we're on the home stretch here so we're gonna let the production experiment bake because again we want to",
    "start": "2016170",
    "end": "2022740"
  },
  {
    "text": "see it go through these unusual events and see how it reacts to those in the meantime the story's back-end team takes",
    "start": "2022740",
    "end": "2028740"
  },
  {
    "text": "our prototyping and our what we learned from the experiment and starts integrating with the real business logic they build the duel right and read and",
    "start": "2028740",
    "end": "2035670"
  },
  {
    "text": "and the migration controls for that and the last thing on our plate is kind of estimating how much capacity we need for",
    "start": "2035670",
    "end": "2042690"
  },
  {
    "text": "New Year's Eve so one problem we had with this is when we look back a year",
    "start": "2042690",
    "end": "2048929"
  },
  {
    "start": "2045000",
    "end": "2045000"
  },
  {
    "text": "ago in our graphing system the data at that time is hourly averages so that",
    "start": "2048929",
    "end": "2056340"
  },
  {
    "text": "tells us very little about what that 5-10 minutes spike right after each hour",
    "start": "2056340",
    "end": "2061590"
  },
  {
    "text": "looks like so we can't really use that to extrapolate what the peach this year will look like so we asked around",
    "start": "2061590",
    "end": "2069240"
  },
  {
    "text": "everyone does anyone have fine-grained data from last year that they saved so",
    "start": "2069240",
    "end": "2074429"
  },
  {
    "text": "you know I'm asking HipChat does anyone have this and fortunately someone on the storage team had a graph of one minute",
    "start": "2074429",
    "end": "2082378"
  },
  {
    "text": "granularity from the previous year new years and I'm just showing the few days around New Year's there and you can see we've got these gnarly traffic spikes on",
    "start": "2082379",
    "end": "2090148"
  },
  {
    "text": "new years and the traffic's much higher than the days around it but there's this",
    "start": "2090149",
    "end": "2096600"
  },
  {
    "text": "weird graphing quirk that occasionally bites me when I'm looking at really short Peaks",
    "start": "2096600",
    "end": "2103700"
  },
  {
    "text": "basically I calculated how many minutes are in this graph oh I'm sorry so the way we were going to extrapolate",
    "start": "2103700",
    "end": "2110910"
  },
  {
    "text": "this to near the next New Year's is basically look at the average ratio compared to the new year's peak there",
    "start": "2110910",
    "end": "2117450"
  },
  {
    "text": "and then project that forward based on our current average traffic and we would calculate that way but I look at this",
    "start": "2117450",
    "end": "2126270"
  },
  {
    "text": "graph and there's over 15,000 minutes in this time span cuz this the screenshot was actually from a few weeks of data",
    "start": "2126270",
    "end": "2131990"
  },
  {
    "text": "and then I open up the screenshot and see how many pixels wide it is and it's 940 pixels wide so it's not",
    "start": "2131990",
    "end": "2139109"
  },
  {
    "text": "really possible to represent all these minutes in this graph and so what our",
    "start": "2139109",
    "end": "2144420"
  },
  {
    "text": "graphing system does in this case is it's actually averaging together data points to render them to one pixel so",
    "start": "2144420",
    "end": "2151200"
  },
  {
    "text": "this is actually 17 minutes per pixels so this graph is lying to us on normal",
    "start": "2151200",
    "end": "2157230"
  },
  {
    "text": "traffic days this wouldn't make a big difference because in a 17 minute span each one of those minutes isn't isn't",
    "start": "2157230",
    "end": "2163230"
  },
  {
    "text": "that different from the other minutes but for those Peaks on new years that makes a big difference so they're",
    "start": "2163230",
    "end": "2169170"
  },
  {
    "text": "actually much higher than they look here if this data were still alive and you were to zoom in on this graph you'd",
    "start": "2169170",
    "end": "2176099"
  },
  {
    "text": "actually see that the peak getting larger and larger as you zoom into a narrow time frame and that minutes per",
    "start": "2176099",
    "end": "2181799"
  },
  {
    "text": "pixel gets smaller but at the same time this is all the data we had so we worked",
    "start": "2181799",
    "end": "2187020"
  },
  {
    "text": "with it and so when we added you know a bunch of other estimates like growth",
    "start": "2187020",
    "end": "2192150"
  },
  {
    "text": "between now New Year's and buffering so we wouldn't get throttled and just",
    "start": "2192150",
    "end": "2198270"
  },
  {
    "text": "margin of errors on different inputs we figured we need anywhere between 5 to 12",
    "start": "2198270",
    "end": "2203400"
  },
  {
    "text": "times our normal capacity on new years",
    "start": "2203400",
    "end": "2207920"
  },
  {
    "start": "2210000",
    "end": "2210000"
  },
  {
    "text": "all right so that production shadow experiment was running and we did see it go through some interesting events so",
    "start": "2211880",
    "end": "2219869"
  },
  {
    "text": "one the Cubs won their first World Series in a hundred years and I'm gonna",
    "start": "2219869",
    "end": "2225780"
  },
  {
    "text": "show you a snap I sent some of my colleagues the next day showing the story post spike when that happens and",
    "start": "2225780",
    "end": "2235890"
  },
  {
    "text": "you can even see spikes happening at the end of different innings or plays before",
    "start": "2235890",
    "end": "2241079"
  },
  {
    "text": "that that were bigger than our normal daily peak but when the game ended and",
    "start": "2241079",
    "end": "2246690"
  },
  {
    "text": "it was like a game seven down to the last play epic finale when that game ended our traffic went several times",
    "start": "2246690",
    "end": "2253020"
  },
  {
    "text": "higher than our normal daily peaks and so it was great to see the system go through this we exceeded our capacity by",
    "start": "2253020",
    "end": "2262079"
  },
  {
    "text": "a lot so we were getting throttled by dynamodb one cool thing though was that",
    "start": "2262079",
    "end": "2267480"
  },
  {
    "text": "we didn't brown out anything so our read capacity we actually did not exceed because most people were posting",
    "start": "2267480",
    "end": "2273509"
  },
  {
    "text": "and so this didn't affect reads because that's an independent capacity",
    "start": "2273509",
    "end": "2278609"
  },
  {
    "text": "allocation we set and reads are a lot more important because if people can't refresh the stories that's very",
    "start": "2278609",
    "end": "2284400"
  },
  {
    "text": "noticeable but if these story posts get deleted a couple minutes it's not a huge deal and so we'd actually changed the",
    "start": "2284400",
    "end": "2292109"
  },
  {
    "text": "way our fan-out tasks worked to so instead of like sleeping and backing off and retrying and keeping that task alive",
    "start": "2292109",
    "end": "2298949"
  },
  {
    "text": "while we're getting throttled we would actually schedule a task in the future",
    "start": "2298949",
    "end": "2304259"
  },
  {
    "text": "when any failure happens so if something fails we'll schedule a new task to write those same rows and the amount of time",
    "start": "2304259",
    "end": "2311640"
  },
  {
    "text": "in the future is our back off so we'll keep doubling the amount of time in the future we schedule these tasks so that",
    "start": "2311640",
    "end": "2317729"
  },
  {
    "text": "way if there is a really big events we can keep backing off and retrying for hours without dropping any story",
    "start": "2317729",
    "end": "2323640"
  },
  {
    "text": "deliveries and so in this case we were",
    "start": "2323640",
    "end": "2328680"
  },
  {
    "text": "able to drain this within a few minutes so it actually worked really well and",
    "start": "2328680",
    "end": "2333859"
  },
  {
    "text": "there was there was actually another event I'll talk about where our task",
    "start": "2333859",
    "end": "2339839"
  },
  {
    "text": "queue system we did something that causes to go down actually for an hour and all the story posts during that time",
    "start": "2339839",
    "end": "2346259"
  },
  {
    "text": "backed up and all the deletes that we need to do backed up as well so when that the the task queue system came back",
    "start": "2346259",
    "end": "2352529"
  },
  {
    "text": "online we had this huge flood of traffic to DynamoDB and similar to this the back",
    "start": "2352529",
    "end": "2357839"
  },
  {
    "text": "off kicked in and it worked well it took longer to drain in that case but it worked well so these incidents are",
    "start": "2357839",
    "end": "2364829"
  },
  {
    "text": "really important because every time they happen and there were others as well but every time they happen we're gaining confidence in the system and we're",
    "start": "2364829",
    "end": "2371609"
  },
  {
    "text": "gaining a visceral understanding of how this operates and and we just understand the operational characteristics better",
    "start": "2371609",
    "end": "2379788"
  },
  {
    "text": "all right New Year's Eve so at this point there's nothing more we can do to prepare for New Year's and it's rolling",
    "start": "2380630",
    "end": "2387089"
  },
  {
    "text": "in so things are going to go quick now and I'll take you through that and most of this is us just reacting to things",
    "start": "2387089",
    "end": "2395900"
  },
  {
    "text": "December 20th by this time the story's back-end team had completely integrated",
    "start": "2395959",
    "end": "2403380"
  },
  {
    "text": "the dual writes and reads working with the new system and then your data bottle going at a hundred percent zero percent",
    "start": "2403380",
    "end": "2410430"
  },
  {
    "text": "of users were actually getting results from dynamo though they were still from the old system and at this time snapchat",
    "start": "2410430",
    "end": "2416340"
  },
  {
    "text": "is is planning for new years and you know we're talking about scaling and top",
    "start": "2416340",
    "end": "2421980"
  },
  {
    "text": "risks and that production system is still a top concern so there's some tension there by the 23rd we had 50% of",
    "start": "2421980",
    "end": "2431250"
  },
  {
    "text": "users actually getting their results excuse me their results from dynamo a TBS gives us the green light to use all",
    "start": "2431250",
    "end": "2438450"
  },
  {
    "text": "that capacity we need the 24th bump in the road we we had a nasty bug and we",
    "start": "2438450",
    "end": "2446340"
  },
  {
    "text": "and Christmas Eve we had to push an emergency deployment to fix that got that straightened out Christmas this was",
    "start": "2446340",
    "end": "2454230"
  },
  {
    "text": "interesting because we expected to see really high read throughput from dynamo on Christmas and that's because",
    "start": "2454230",
    "end": "2459360"
  },
  {
    "text": "Christmas Eve is a really active day and so there's a lot of active story posts",
    "start": "2459360",
    "end": "2465480"
  },
  {
    "text": "in the system so the average inbox size is way higher than normal and then Christmas Day is really active so",
    "start": "2465480",
    "end": "2472110"
  },
  {
    "text": "there's a lot more fetching of the inboxes than normal so those kind of multiplied together but a completely",
    "start": "2472110",
    "end": "2478410"
  },
  {
    "text": "unrelated outage of snapchat dampened our peak traffic that day so we never quite got that signal by the 27th at",
    "start": "2478410",
    "end": "2486600"
  },
  {
    "text": "this point 100% of users are getting the results from dynamos so we're actually on the Dynamo system but we're still",
    "start": "2486600",
    "end": "2492600"
  },
  {
    "text": "dual writing and reading from the old system and this is where we cranked up our provision capacity to what we need",
    "start": "2492600",
    "end": "2498000"
  },
  {
    "text": "on New Year's so there's nothing else for us to do at this point all right New",
    "start": "2498000",
    "end": "2503040"
  },
  {
    "text": "Year's Eve so to give you some context at snapshot on New Year's every midnight",
    "start": "2503040",
    "end": "2508320"
  },
  {
    "text": "the strikes there's a big spike in traffic but there's two particular spikes that are are just beastly and",
    "start": "2508320",
    "end": "2515310"
  },
  {
    "text": "that is Paris and New York and so I'm going to focus on those two so those are",
    "start": "2515310",
    "end": "2520380"
  },
  {
    "text": "the ones that we all fear all right so 3 p.m. this is Seattle time",
    "start": "2520380",
    "end": "2525750"
  },
  {
    "text": "so 3 p.m. is going to be Paris midnight so at 1:30 we've watched what's happened",
    "start": "2525750",
    "end": "2531330"
  },
  {
    "text": "there's been several New Year's already they're going to be much smaller than Paris everything looks fine but even our",
    "start": "2531330",
    "end": "2536760"
  },
  {
    "text": "base line story post rate is much higher than normal like higher than a normal peak",
    "start": "2536760",
    "end": "2542750"
  },
  {
    "text": "and that's without even counting the spikes every midnight so we roll into midnight and we're sitting there",
    "start": "2542750",
    "end": "2550170"
  },
  {
    "start": "2548000",
    "end": "2548000"
  },
  {
    "text": "refreshing graphs a few minutes later we see story post spiking as we expect and",
    "start": "2550170",
    "end": "2555440"
  },
  {
    "text": "then a few minutes later we see the story post to delivery latency start increasing so this is basically time",
    "start": "2555440",
    "end": "2562500"
  },
  {
    "text": "between a user posting a story and it being delivered to an inbox so every one of those is measured and that latency is",
    "start": "2562500",
    "end": "2568470"
  },
  {
    "text": "going up a few minutes later we understand the root cause of this we've",
    "start": "2568470",
    "end": "2573990"
  },
  {
    "text": "hit our capacity for our task queue system so we can't execute all the tasks",
    "start": "2573990",
    "end": "2579359"
  },
  {
    "text": "at full bore and some of the tasks are getting backed up so some of these fan-out tasks are sitting in the queue not delivering stories this we didn't",
    "start": "2579359",
    "end": "2587430"
  },
  {
    "text": "expect but by 10:00 after the spike had",
    "start": "2587430",
    "end": "2593220"
  },
  {
    "text": "gone down which freed up enough capacity to drain those backed up deliveries things were trending rapidly back to",
    "start": "2593220",
    "end": "2598829"
  },
  {
    "text": "normal everything's ok at 3:30 p.m. the topic of discussion should we change anything",
    "start": "2598829",
    "end": "2605630"
  },
  {
    "text": "we were concerned because if we hit capacity in Paris what's going to happen when we hit New York which is a much",
    "start": "2605630",
    "end": "2611819"
  },
  {
    "text": "bigger spike now typically changing anything on this day at this time would",
    "start": "2611819",
    "end": "2619079"
  },
  {
    "text": "be dumb but we talked a lot about it and we talked ourselves into one change so",
    "start": "2619079",
    "end": "2627809"
  },
  {
    "text": "we had a knob that could control the number of inbox rights we would do per fan-out task so if we increase this that",
    "start": "2627809",
    "end": "2635849"
  },
  {
    "text": "means we'd be executing fewer tasks and that would free some capacity up in the task queue system we had never used this",
    "start": "2635849",
    "end": "2642630"
  },
  {
    "text": "knob before but we talked through it a",
    "start": "2642630",
    "end": "2648180"
  },
  {
    "text": "lot and we figured you know in most ways this could go wrong it's pretty reversible so we went ahead and did it",
    "start": "2648180",
    "end": "2654930"
  },
  {
    "text": "and it worked ok we saw the task institutions go down things seem to be smooth at 5:15 we talked ourselves into",
    "start": "2654930",
    "end": "2665490"
  },
  {
    "text": "another change we increased our task queue capacity now this we had done a bunch of times",
    "start": "2665490",
    "end": "2671170"
  },
  {
    "text": "or and it always have gone fine so that was one reason to go forward with it five minutes later story delivery times",
    "start": "2671170",
    "end": "2677680"
  },
  {
    "start": "2676000",
    "end": "2676000"
  },
  {
    "text": "start increasing for something like 10% of inbox rights we have no idea why by",
    "start": "2677680",
    "end": "2686799"
  },
  {
    "text": "6:00 p.m. a small percentage of delivers continued increase their delay so the",
    "start": "2686799",
    "end": "2693609"
  },
  {
    "text": "problem is getting worse by seven we understand that this is definitely related to us increasing the capacity so",
    "start": "2693609",
    "end": "2699970"
  },
  {
    "text": "we're kind of regretting that at this point and we also understand there's just nothing we can do about it except",
    "start": "2699970",
    "end": "2705579"
  },
  {
    "text": "wait it out so this is kind of when things were starting to drain at this point at 7:00 p.m. but it wasn't",
    "start": "2705579",
    "end": "2711220"
  },
  {
    "text": "draining very quickly now again this was affecting it wasn't affecting everyone it was it was something like five to",
    "start": "2711220",
    "end": "2717250"
  },
  {
    "text": "seven percent of deliveries were affected and it was kind of comical because if you looked at the delay graph",
    "start": "2717250",
    "end": "2723549"
  },
  {
    "text": "it was trending back down but if you extrapolate it and intersected exactly",
    "start": "2723549",
    "end": "2728650"
  },
  {
    "text": "with New York midnight so we made this exciting for ourselves so 15 minutes",
    "start": "2728650",
    "end": "2736390"
  },
  {
    "text": "before New York hits midnight those finally drain so we get 15 minutes of",
    "start": "2736390",
    "end": "2741730"
  },
  {
    "text": "rest midnight hits in New York there's a huge spike in story posts as expected a",
    "start": "2741730",
    "end": "2749040"
  },
  {
    "text": "few minutes later we see us hit task execution capacity again now note that",
    "start": "2749040",
    "end": "2755530"
  },
  {
    "text": "this capacity is a lot higher and we're using less capacity because of those two changes we made so maybe it was a good",
    "start": "2755530",
    "end": "2761559"
  },
  {
    "text": "thing that we changed this because there have been a lot worse delays if we hadn't so we we still hit this kiss",
    "start": "2761559",
    "end": "2766720"
  },
  {
    "text": "limit despite our changes so we see a small percentage of stories getting delayed but by the time the spike went",
    "start": "2766720",
    "end": "2775299"
  },
  {
    "text": "away like Paris things trended rapidly back down to normal again we were able",
    "start": "2775299",
    "end": "2780880"
  },
  {
    "text": "to drain those so only a small percentage of liveries were affected it was only a few minutes so it wasn't a",
    "start": "2780880",
    "end": "2786280"
  },
  {
    "text": "big deal so we're kind of just summarize this like the rest of the night we had",
    "start": "2786280",
    "end": "2791680"
  },
  {
    "text": "some other spikes but they were much smaller so they weren't a big deal so we had some user impact here that",
    "start": "2791680",
    "end": "2796869"
  },
  {
    "text": "wasn't good but overall it actually went pretty smoothly so the impact we did",
    "start": "2796869",
    "end": "2802540"
  },
  {
    "text": "cause wasn't super noticeable but we were not out of the woods yet so",
    "start": "2802540",
    "end": "2810400"
  },
  {
    "text": "January 2nd if you remember the graph I showed you earlier where we had put some",
    "start": "2810400",
    "end": "2817360"
  },
  {
    "text": "deletes offset so all of New Years Eve we were scheduling deletes for 36 hours",
    "start": "2817360",
    "end": "2823510"
  },
  {
    "text": "in the future so that means we're just going to replay what happened on New",
    "start": "2823510",
    "end": "2828760"
  },
  {
    "text": "Year's Eve on January 2nd and on top of that a lot of those story posts are",
    "start": "2828760",
    "end": "2833830"
  },
  {
    "text": "happening at night on New Year's Eve so those deletes are going to happen at peak on this day so we're adding the two",
    "start": "2833830",
    "end": "2841420"
  },
  {
    "text": "together but we were prepared for this or at least we had thought of it fortunately so we had left our capacity",
    "start": "2841420",
    "end": "2848560"
  },
  {
    "text": "allocated at it's new year's level so there was still a lot of it also one thing we did is we don't schedule it for",
    "start": "2848560",
    "end": "2855190"
  },
  {
    "text": "exactly 36 hours in the future we actually added some jitter so it'll be within a few hour period we randomly",
    "start": "2855190",
    "end": "2862180"
  },
  {
    "text": "pick around 36 hours so that's especially helpful for those really short new year new year's spikes because",
    "start": "2862180",
    "end": "2869350"
  },
  {
    "text": "it'll disperse those few minute periods across a few hours so the traffic was really high on the second but it wasn't",
    "start": "2869350",
    "end": "2876370"
  },
  {
    "text": "as spiky as it was on New Year's Eve so we didn't get those yearly or those hourly spikes so things actually worked",
    "start": "2876370",
    "end": "2883150"
  },
  {
    "text": "fine and we got through this all right January 9th so this is the end this is",
    "start": "2883150",
    "end": "2890830"
  },
  {
    "text": "the point of no return we turned off writes and reads to the old system and",
    "start": "2890830",
    "end": "2896190"
  },
  {
    "text": "here's a graph showing us turning off that use case so this is a handful of",
    "start": "2896190",
    "end": "2901260"
  },
  {
    "text": "storage use cases it's snapchat and you see here the right number of writes for",
    "start": "2901260",
    "end": "2907960"
  },
  {
    "text": "each of those and obviously that top green one is the one we turned off so it was really awesome to see that just go down to zero now in terms of results",
    "start": "2907960",
    "end": "2917310"
  },
  {
    "text": "we've been running this for almost a year now and it's been very stable since",
    "start": "2917310",
    "end": "2923560"
  },
  {
    "text": "I'd say it's probably one of our most stable systems at snapchat in terms of",
    "start": "2923560",
    "end": "2928660"
  },
  {
    "text": "cost savings we eliminated most of the old systems cost between compute and",
    "start": "2928660",
    "end": "2934420"
  },
  {
    "text": "storage costs and that's when taking into account the new",
    "start": "2934420",
    "end": "2939960"
  },
  {
    "text": "bandwidth costs we were paying and I'd love to tell you exactly how much we're saving but I can't disclose that but I",
    "start": "2939960",
    "end": "2947940"
  },
  {
    "text": "can tell you that we would not have put the effort into this project if it would only save a few million a year and we",
    "start": "2947940",
    "end": "2954660"
  },
  {
    "text": "definitely got the result we wanted and it was an impactful result so it was it was successful so just to summarize some",
    "start": "2954660",
    "end": "2962580"
  },
  {
    "text": "threads we saw here so a fine-grained billing model gives you more dimensions",
    "start": "2962580",
    "end": "2967620"
  },
  {
    "text": "to optimize your use case if you need to you saw that we were able to really",
    "start": "2967620",
    "end": "2973140"
  },
  {
    "text": "understand the characteristics of the use case and match it up to that billing model and optimize the heck out of it and also that billing model incentivizes",
    "start": "2973140",
    "end": "2980820"
  },
  {
    "text": "the whole system to be more efficient so you saw our network bandwidth fall away when we optimized it and you saw CPU",
    "start": "2980820",
    "end": "2987120"
  },
  {
    "text": "cost so the the compute costs we paid drops away completely almost also prefer",
    "start": "2987120",
    "end": "2996180"
  },
  {
    "text": "realistic realistic experiments over analysis and estimates so both are important but as you saw our estimates",
    "start": "2996180",
    "end": "3004700"
  },
  {
    "text": "were easily off and a realistic experiment is just gonna take all those variables and into account that you",
    "start": "3004700",
    "end": "3010970"
  },
  {
    "text": "can't think of no matter no matter how hard you try or how clever you think you can be so that's all I have thank you",
    "start": "3010970",
    "end": "3017630"
  },
  {
    "text": "all for coming [Applause]",
    "start": "3017630",
    "end": "3027690"
  }
]