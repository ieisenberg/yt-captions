[
  {
    "text": "hi everyone thank you for coming my name is Tina Adams I'm the Senior Product",
    "start": "0",
    "end": "5670"
  },
  {
    "text": "Manager for Amazon redshift and with me today is adam from Yahoo's mobile team",
    "start": "5670",
    "end": "10700"
  },
  {
    "text": "and before I hand it over to Adam I just wanted to give you guys a quick overview of redshift and just to get a sense of",
    "start": "10700",
    "end": "17850"
  },
  {
    "text": "the room how many people here use red chip today okay so most people so this",
    "start": "17850",
    "end": "23970"
  },
  {
    "text": "will be really fast so for those few of you who don't use redshift we're a fast",
    "start": "23970",
    "end": "29070"
  },
  {
    "text": "simple petabyte scale data warehouse available for less than a thousand dollars per terabyte per year and we",
    "start": "29070",
    "end": "36719"
  },
  {
    "text": "design red chip to be ten times faster and 10 times cheaper than traditional data warehousing solutions and along",
    "start": "36719",
    "end": "43980"
  },
  {
    "text": "with that whole lot simpler so as a managed service we take care of provisioning monitoring maintenance",
    "start": "43980",
    "end": "49289"
  },
  {
    "text": "security backups all those things for you so what makes stretch of so fast we",
    "start": "49289",
    "end": "55739"
  },
  {
    "text": "have local column or storage which significantly reduces io redshift is a clustered data warehouse we have a",
    "start": "55739",
    "end": "61859"
  },
  {
    "text": "leader known which acts as your sequel endpoint and coordinates query execution and we have the compute nodes which we",
    "start": "61859",
    "end": "70380"
  },
  {
    "text": "paralyze and distribute all our queries across the compute nodes all loads I'll back up sri stores resizes and the nice",
    "start": "70380",
    "end": "78180"
  },
  {
    "text": "thing about the clustered architecture is that you can get started small with a single node cluster for 25 cents an hour",
    "start": "78180",
    "end": "84990"
  },
  {
    "text": "and then scale to 120 node cluster with over two petabytes of storage in",
    "start": "84990",
    "end": "90320"
  },
  {
    "text": "addition to being fast red ship is very cost-effective we priced redshift so",
    "start": "90320",
    "end": "95909"
  },
  {
    "text": "that you can analyze all your data and not throw any of it away our pricing is really simple it's just the number of",
    "start": "95909",
    "end": "103079"
  },
  {
    "text": "nodes in your cluster at times the hourly price per node we don't charge for the leader and we have reserved",
    "start": "103079",
    "end": "109020"
  },
  {
    "text": "instances you get a forty percent discount for a one-year RI and a seventy percent discount for a three-year RI so",
    "start": "109020",
    "end": "115110"
  },
  {
    "text": "if you're not on our eyes you should because the payback period is like six seven months these prices also include",
    "start": "115110",
    "end": "123370"
  },
  {
    "text": "three copies of your data to on the nodes and one and s3 and our terabyte",
    "start": "123370",
    "end": "130429"
  },
  {
    "text": "prices are on compressed terabytes so with redshift customers typically see 3x",
    "start": "130430",
    "end": "136160"
  },
  {
    "text": "compression which means if you have 10 terabytes they'll typically compress down to three terabytes and redshift and",
    "start": "136160",
    "end": "141470"
  },
  {
    "text": "they'll be three thousand dollars a year and finally redshift is really easy to",
    "start": "141470",
    "end": "146840"
  },
  {
    "text": "use you can provision a cluster in minutes you can monitor performance in our console we have built-in security",
    "start": "146840",
    "end": "153130"
  },
  {
    "text": "and automated backups that are incremental and continuous and scaling",
    "start": "153130",
    "end": "159200"
  },
  {
    "text": "is really easy resizing your cluster is just a simple point-and-click operation in our console oh there we are and then",
    "start": "159200",
    "end": "172580"
  },
  {
    "text": "finally we have customers of all shapes and sizes across every single industry",
    "start": "172580",
    "end": "177590"
  },
  {
    "text": "and what they really have in common is that they've chosen to use red shift because of our performance or ease of",
    "start": "177590",
    "end": "184100"
  },
  {
    "text": "use and our cost effectiveness and that's my pleasure to hand it over to Adam from yahoo to talk about how",
    "start": "184100",
    "end": "189950"
  },
  {
    "text": "they've been able to take advantage of red shift thank you so much",
    "start": "189950",
    "end": "194890"
  },
  {
    "text": "thanks Tina my name is Adam Savitsky I",
    "start": "198240",
    "end": "203440"
  },
  {
    "text": "am I work in engineering on the mobile data analytics team for growth products",
    "start": "203440",
    "end": "210190"
  },
  {
    "text": "at Yahoo so we call ourselves Sputnik and we have built a data warehouse on",
    "start": "210190",
    "end": "218380"
  },
  {
    "text": "top of redshift I'm going to tell you a little bit about that today so what I'm",
    "start": "218380",
    "end": "223840"
  },
  {
    "text": "going to talk about today is what analytics means at a company like Yahoo how are you tal process works I want to",
    "start": "223840",
    "end": "232569"
  },
  {
    "text": "walk you through a little bit of our redshift architecture I will do some do's and don'ts and best practices and",
    "start": "232569",
    "end": "239950"
  },
  {
    "text": "then finally we'll do a more technical deep dive into how we built our user retention reporting so I want to set the",
    "start": "239950",
    "end": "248620"
  },
  {
    "text": "stage a little bit here Yahoo was a very big company a very large company and",
    "start": "248620",
    "end": "257430"
  },
  {
    "text": "it's also been around for a very long time as you can imagine there are a number of sins of the past but this",
    "start": "257430",
    "end": "266289"
  },
  {
    "text": "quote from Marissa Mayer really really stuck out with me we're returning an iconic iconic company to greatness and",
    "start": "266289",
    "end": "273610"
  },
  {
    "text": "so that's kind of like what I see our mission for the whole company right now",
    "start": "273610",
    "end": "279210"
  },
  {
    "text": "we're going through an immense transition and part of that is bringing analytics into the 21st century so my",
    "start": "279210",
    "end": "287890"
  },
  {
    "text": "team is part of a growth team so we work on scaling existing products optimizing",
    "start": "287890",
    "end": "294690"
  },
  {
    "text": "flows within products and building new products as well but we think of growth",
    "start": "294690",
    "end": "300400"
  },
  {
    "text": "is kind of not something that you do it's something that that compounds after",
    "start": "300400",
    "end": "306160"
  },
  {
    "text": "lots and lots of little wins so we we realized very early on that analytics is",
    "start": "306160",
    "end": "313570"
  },
  {
    "text": "critical to growth so we we basically decided that we need to get first class",
    "start": "313570",
    "end": "318970"
  },
  {
    "text": "analytics this is a quote from one of my",
    "start": "318970",
    "end": "324250"
  },
  {
    "text": "colleagues you can't grow a product that hasn't reached product market fit it's impossible like you can spend all of the",
    "start": "324250",
    "end": "330520"
  },
  {
    "text": "advertising dollars you want on something but it's not going to work unless there's a fit with the product in the market so that's another thing that",
    "start": "330520",
    "end": "337120"
  },
  {
    "text": "we do is we try and use analytics to understand well what's working with this product what isn't what can we change",
    "start": "337120",
    "end": "343060"
  },
  {
    "text": "and how can we build a better product and then like I mentioned earlier",
    "start": "343060",
    "end": "348330"
  },
  {
    "text": "analytics is absolutely critical to growth this was clear to me when I",
    "start": "348330",
    "end": "353680"
  },
  {
    "text": "joined the company about a year ago before that I worked at a startup called message me it was a messaging company a",
    "start": "353680",
    "end": "359919"
  },
  {
    "text": "lot of my co-workers there were ex gaming and in these industries and",
    "start": "359919",
    "end": "365200"
  },
  {
    "text": "startup and especially in gaming analytics is really really crucial it's used heavily to make decisions every day",
    "start": "365200",
    "end": "372060"
  },
  {
    "text": "we we were using red shift it messaged me as well and one of the great things",
    "start": "372060",
    "end": "377590"
  },
  {
    "text": "about working there was everybody in the entire organization from engineers to product managers to analysts to",
    "start": "377590",
    "end": "383590"
  },
  {
    "text": "executives we all had access to the same data so if you can you can play lip",
    "start": "383590",
    "end": "389410"
  },
  {
    "text": "service to say Oh everything's going really really great but you can't say",
    "start": "389410",
    "end": "395260"
  },
  {
    "text": "that if everybody can look at the data and see it's not working and I mean obviously you want things to work but",
    "start": "395260",
    "end": "400960"
  },
  {
    "text": "what it allows you to do is have an honest conversation about your product and it effectively builds a better",
    "start": "400960",
    "end": "407590"
  },
  {
    "text": "culture so when we came to Yahoo we wanted to try and bring some of that with us so to give you an idea of scale",
    "start": "407590",
    "end": "416669"
  },
  {
    "text": "Yahoo processes roughly about 85 billion event per day and that's clickstream",
    "start": "416669",
    "end": "425500"
  },
  {
    "text": "data so this is not in red shift but it's in Hadoop and Hadoop was invented",
    "start": "425500",
    "end": "435520"
  },
  {
    "text": "at Yahoo it's there's a long history of Hadoop there and it's it's very good for",
    "start": "435520",
    "end": "441400"
  },
  {
    "text": "holding tremendous amounts of data but I mean if you compare it till I auto miles driven even Google searches that's not",
    "start": "441400",
    "end": "447610"
  },
  {
    "text": "to say that we have more data than Google necessarily but it just capped off the total number of searches you can see that there really is a tremendous",
    "start": "447610",
    "end": "453430"
  },
  {
    "text": "volume there this is how our audience data breaks down so the vast majority is",
    "start": "453430",
    "end": "460870"
  },
  {
    "text": "still desktop my team is responsible for mobile so mobile represents a small portion of",
    "start": "460870",
    "end": "468470"
  },
  {
    "text": "the PI right now but we've realized that mobile is our absolute number one priority so everything that we do is",
    "start": "468470",
    "end": "474830"
  },
  {
    "text": "about trying to grow our mobile products so like I said to dupe is really really",
    "start": "474830",
    "end": "481729"
  },
  {
    "text": "important at Yahoo everyone uses Hadoop and it's actually worked out really really well for us for a number of",
    "start": "481729",
    "end": "488840"
  },
  {
    "text": "applications but there's 14 clusters for 2,000 nodes three data centers 500",
    "start": "488840",
    "end": "495169"
  },
  {
    "text": "petabytes there some people say it's the biggest to do cluster in the world I don't know if that's true but it",
    "start": "495169",
    "end": "501139"
  },
  {
    "text": "certainly is very large so the de facto way of doing analytics right now with do",
    "start": "501139",
    "end": "508190"
  },
  {
    "text": "is hive at Yahoo so you can ssh into a server you get a command line you type",
    "start": "508190",
    "end": "513320"
  },
  {
    "text": "some sequel the problem there is that hive is not very performant it's it's",
    "start": "513320",
    "end": "519740"
  },
  {
    "text": "really hard to share the results of what you've done I think some teams are using bi tools on top of hive but really the",
    "start": "519740",
    "end": "527959"
  },
  {
    "text": "way the way things get done if they if anybody's looking at the data at all they run a query in hive they look at",
    "start": "527959",
    "end": "534230"
  },
  {
    "text": "the results and they email it to their buddies but like that's really not a very scalable way to share data with an",
    "start": "534230",
    "end": "540200"
  },
  {
    "text": "organization and so it's hard to use and then it's hard for somebody to go in and repeat what you've done or you yourself",
    "start": "540200",
    "end": "545660"
  },
  {
    "text": "if you want to go back and run the same query again you have to remember what your grand so yeah it's like it I've is",
    "start": "545660",
    "end": "558110"
  },
  {
    "text": "its equals great i love i love sequel but like this isn't the most friendly interface for you know a product manager",
    "start": "558110",
    "end": "566029"
  },
  {
    "text": "who's not an engineer right i can write sequel all day or an executive like Marissa Mayer's not going to go into",
    "start": "566029",
    "end": "572270"
  },
  {
    "text": "hive and type out some sequel statement so in addition to hive there are some",
    "start": "572270",
    "end": "579740"
  },
  {
    "text": "other projects going on at Yahoo these among others these are all Hadoop based",
    "start": "579740",
    "end": "587839"
  },
  {
    "text": "so we've actually migrated from hive to hive on tez and that's kind of what's",
    "start": "587839",
    "end": "593029"
  },
  {
    "text": "powering or hive instance right now but there are other implementations of",
    "start": "593029",
    "end": "598779"
  },
  {
    "text": "analytics products running on top of Hadoop I'm not going to talk about those today I'm not as familiar with them but",
    "start": "599270",
    "end": "604860"
  },
  {
    "text": "we are interested in that but we are very much committed to red shift and I'll show you why we did some benchmarks",
    "start": "604860",
    "end": "612480"
  },
  {
    "text": "when we first started out because we wanted to see what one of these calumny Hadoop based analytics tools would do",
    "start": "612480",
    "end": "622170"
  },
  {
    "text": "and the one we chose to look at was Impala from Cloudera and it's very",
    "start": "622170",
    "end": "627450"
  },
  {
    "text": "powerful but like the performance was actually orders of magnitude slower I think this was a fairly even benchmark",
    "start": "627450",
    "end": "633750"
  },
  {
    "text": "in terms of hardware and we did all the optimizations we couldn't we just couldn't get it to be fast enough so it's clear that we were going to go with",
    "start": "633750",
    "end": "641610"
  },
  {
    "text": "either red shift over toca the benchmark shows vertically being slightly faster I",
    "start": "641610",
    "end": "647720"
  },
  {
    "text": "could probably take that with a grain of salt they're effectively equivalent and",
    "start": "647720",
    "end": "653600"
  },
  {
    "text": "red shift ends up being a much easier sell because you don't have to go out and provision hardware you don't have to",
    "start": "653600",
    "end": "659100"
  },
  {
    "text": "purchase you know one of these big boxes you can just log into your console spin up a cluster and you're good to go so",
    "start": "659100",
    "end": "665640"
  },
  {
    "text": "that's why we chose red shift and we've been really happy with it we're currently running on 21 of the DC 18 XL",
    "start": "665640",
    "end": "672840"
  },
  {
    "text": "nodes those are the high memory hi compute nodes pretty much the top of the",
    "start": "672840",
    "end": "679800"
  },
  {
    "text": "line that amazon offers we process on the order of 2 billion events per day on",
    "start": "679800",
    "end": "687090"
  },
  {
    "text": "football sunday is that number goes up like two or three times we handle about",
    "start": "687090",
    "end": "693240"
  },
  {
    "text": "1200 queries per day some of those are human generated somebody going in and",
    "start": "693240",
    "end": "698760"
  },
  {
    "text": "typing them others are dashboards and 27 terabytes of data I think that numbers",
    "start": "698760",
    "end": "705300"
  },
  {
    "text": "probably gone up since I made these slides so it's not massive right but it's very large it's large enough where",
    "start": "705300",
    "end": "712050"
  },
  {
    "text": "you really have to worry about scale and redshift just handles it no problem you",
    "start": "712050",
    "end": "718320"
  },
  {
    "text": "should also know that we recently migrated over to an encrypted cluster just for an added layer of security we",
    "start": "718320",
    "end": "725970"
  },
  {
    "text": "were asked to do it by the security folks so we didn't really see too much of a",
    "start": "725970",
    "end": "731180"
  },
  {
    "text": "performance impact I think mostly because our queries for the most part run in memory so you pay for the decryption once the beginning of the",
    "start": "731180",
    "end": "737630"
  },
  {
    "text": "query and then everything else runs at normal speed but you know if you're in an industry with you know compliance",
    "start": "737630",
    "end": "744139"
  },
  {
    "text": "requirements redshift encryption no problem there we go so I wanted to talk",
    "start": "744139",
    "end": "754970"
  },
  {
    "text": "a little bit about architecture here's a general overflow of our ETL process to",
    "start": "754970",
    "end": "762290"
  },
  {
    "text": "get data to redshift we take it out of Hadoop with pig we then pipe it up to s3",
    "start": "762290",
    "end": "767920"
  },
  {
    "text": "and we use a tool called air flow there which is an open-source ETL tool from",
    "start": "767920",
    "end": "774579"
  },
  {
    "text": "airbnb I'll talk a little bit more about that but air flow then copies the data",
    "start": "774579",
    "end": "779870"
  },
  {
    "text": "out of s3 and puts it into red shift and then we use a bi tool called looker which I'll show you a bit later as well",
    "start": "779870",
    "end": "786370"
  },
  {
    "text": "and that interface is really nicely with redshift and we've been happy with pretty much all of this so in a little",
    "start": "786370",
    "end": "796699"
  },
  {
    "text": "bit more detail this is what the upstream section of our etl looks like so like I said we start with Hadoop we",
    "start": "796699",
    "end": "804110"
  },
  {
    "text": "have clickstream data living there this is a really large data set because it includes desktop data as well for mostly",
    "start": "804110",
    "end": "812000"
  },
  {
    "text": "yahoo mail actually and search as well we use uzi to hourly batch process",
    "start": "812000",
    "end": "818959"
  },
  {
    "text": "events and hoozy runs our pig script which writes out the data to",
    "start": "818959",
    "end": "824209"
  },
  {
    "text": "intermediate storage on HDFS pig will actually upload directly to s3 so if",
    "start": "824209",
    "end": "830269"
  },
  {
    "text": "you're using EMR piece of cake because of our security restrictions our Hadoop",
    "start": "830269",
    "end": "837199"
  },
  {
    "text": "named nodes can't talk to the outside world we have a firewall rule for that so that's how we get around it we we",
    "start": "837199",
    "end": "842750"
  },
  {
    "text": "copy files individually one at a time off of the name nodes onto a gateway machine and then we have a Python script",
    "start": "842750",
    "end": "849649"
  },
  {
    "text": "effectively that uses bodo to upload the data to ask three yeah so we use s3 as a",
    "start": "849649",
    "end": "859250"
  },
  {
    "text": "holding area and kind of a backup for all of our data that's in red chef red chip supports s3",
    "start": "859250",
    "end": "865850"
  },
  {
    "text": "and DynamoDB I believe as sources so if you're going to copy data and you can",
    "start": "865850",
    "end": "871400"
  },
  {
    "text": "use either so then I hopefully you can see this okay but this is what our",
    "start": "871400",
    "end": "878150"
  },
  {
    "text": "downstream ETL process looks like so every hour this is running an air flow by the way and my next slide will show",
    "start": "878150",
    "end": "883970"
  },
  {
    "text": "you what that looks like but we check to see if the data is available in s3 there's a sensor airflow provides an s3",
    "start": "883970",
    "end": "891110"
  },
  {
    "text": "sensor that will ping s3 every 10 minutes and say hey is the data available if it is if it's copied into",
    "start": "891110",
    "end": "896810"
  },
  {
    "text": "redshift that's just a redshift command we then do some sanitization on the data",
    "start": "896810",
    "end": "903730"
  },
  {
    "text": "we then take all of the events and then process them into individual tables",
    "start": "903730",
    "end": "909170"
  },
  {
    "text": "based on which product they are and I'll go into our schema in more detail later but we then have a process that looks at",
    "start": "909170",
    "end": "917030"
  },
  {
    "text": "all of the events figures out which are new installs so is this the first time I've seen a device if so we send it to a",
    "start": "917030",
    "end": "924350"
  },
  {
    "text": "third-party attribution provider who will tell us what the source for that",
    "start": "924350",
    "end": "929570"
  },
  {
    "text": "install was if there was if there was one so it could be organic or it could be an ad click and then we get a ping",
    "start": "929570",
    "end": "936500"
  },
  {
    "text": "back from them so we will we then so we",
    "start": "936500",
    "end": "941750"
  },
  {
    "text": "do to our way we do an hourly in the daily roll-up of the data for summarization purposes and I'll get into",
    "start": "941750",
    "end": "946850"
  },
  {
    "text": "summarization later but you take the raw events and then we build an hourly summary and then we update the table of",
    "start": "946850",
    "end": "956420"
  },
  {
    "text": "all the installs we do some work with parameters how each event can have",
    "start": "956420",
    "end": "961760"
  },
  {
    "text": "parameters we process those and then we do some final cleanup so that happens every hour this is what it looks like in",
    "start": "961760",
    "end": "970940"
  },
  {
    "text": "practice when you're using air flow basically each column represents one",
    "start": "970940",
    "end": "977870"
  },
  {
    "text": "hour and every hour all of those boxes into turn green if they turn red or if",
    "start": "977870",
    "end": "983840"
  },
  {
    "text": "they turn orange it means there's a problem and it'll notify us and tell us that it failed all the steps that I",
    "start": "983840",
    "end": "989450"
  },
  {
    "text": "showed you previously are in this so they run it's uh you define what they call a directed",
    "start": "989450",
    "end": "996829"
  },
  {
    "text": "acyclic graph or a dad and that just it's basically a dependency graph of",
    "start": "996829",
    "end": "1003100"
  },
  {
    "text": "what tasks run when so you can run things in parallel you can run them in sequence you can have things wait for other tasks to complete running and this",
    "start": "1003100",
    "end": "1010300"
  },
  {
    "text": "is kind of our health monitor for everything that's happening if stuffs go if stuff goes wrong it shows up an air",
    "start": "1010300",
    "end": "1015939"
  },
  {
    "text": "flow the nice thing about this too is that all of these dags are coded in",
    "start": "1015939",
    "end": "1021699"
  },
  {
    "text": "Python so we work with easy to like I mentioned I don't know if you've ever worked with Lucy but that's all XML and",
    "start": "1021699",
    "end": "1028089"
  },
  {
    "text": "the configuration is really messy setting up etl and airflow is really really simple I suggest giving it a look",
    "start": "1028089",
    "end": "1034270"
  },
  {
    "text": "it's free open source so this is our schema we actually have more tables",
    "start": "1034270",
    "end": "1040959"
  },
  {
    "text": "because it expanded since I'd made this slide but it gives you the general idea so we have raw data and that's just the",
    "start": "1040959",
    "end": "1047380"
  },
  {
    "text": "most basic data it comes out of a dupe and then we put it in as raw data and these are clickstream events so user",
    "start": "1047380",
    "end": "1053710"
  },
  {
    "text": "clicks on a button that son of that right they open the app that's an event so we in interest of performance we",
    "start": "1053710",
    "end": "1062710"
  },
  {
    "text": "split the data into individual / product tables so male has a table flickr has a",
    "start": "1062710",
    "end": "1068440"
  },
  {
    "text": "table home run is the home page app that has a table these all their individual",
    "start": "1068440",
    "end": "1074409"
  },
  {
    "text": "tables we copy them into one big table where we staged everything and then our et al copies that not one at a time then",
    "start": "1074409",
    "end": "1082500"
  },
  {
    "text": "we I mentioned parameters early the earlier each event has parameters that",
    "start": "1082500",
    "end": "1087610"
  },
  {
    "text": "come in as a big JSON blob so you know at the example I like to give for this",
    "start": "1087610",
    "end": "1093100"
  },
  {
    "text": "is for the fantasy sports app there's you can actually play fantasy sports for",
    "start": "1093100",
    "end": "1098289"
  },
  {
    "text": "baseball or football or all these different sports so like when you when you click you know I want to I want to",
    "start": "1098289",
    "end": "1106720"
  },
  {
    "text": "create a new league right there'll be a parameter on that event for which sport it is and then I mean these are their",
    "start": "1106720",
    "end": "1112240"
  },
  {
    "text": "arbitrary so it's not a fixed schema so what we did was rather than parsing JSON",
    "start": "1112240",
    "end": "1117429"
  },
  {
    "text": "every time you want to see what the parameter values are because that that's actually pretty slow it's just a slow",
    "start": "1117429",
    "end": "1123820"
  },
  {
    "text": "operation because it's it's string parsing right so what we do is we pay for that ahead of time we have an ETL job that when the",
    "start": "1123820",
    "end": "1130850"
  },
  {
    "text": "hourly events come well every hour when the events come in it looks at that JSON explicit into key value pairs and it",
    "start": "1130850",
    "end": "1137870"
  },
  {
    "text": "generates a unique ID for each event so then we have a one-to-many relationship with these parameter bowls so each event",
    "start": "1137870",
    "end": "1144080"
  },
  {
    "text": "then we'll have key value event ID and you can join on the event ID doing it",
    "start": "1144080",
    "end": "1151490"
  },
  {
    "text": "like this where you have one product table for params and events it works out really well because if somebody's",
    "start": "1151490",
    "end": "1158390"
  },
  {
    "text": "looking at you know a smaller product it doesn't have as much data you don't have to then scan a really big table in",
    "start": "1158390",
    "end": "1167180"
  },
  {
    "text": "theory sort keys which I'll get into should take care of this problem but do see performance degradation when you",
    "start": "1167180",
    "end": "1173660"
  },
  {
    "text": "have a really really big table and it's also a bad practice to to just have one monolithic table because if you have a",
    "start": "1173660",
    "end": "1181070"
  },
  {
    "text": "table that's too big you effectively can never copy that table if it's more than fifty percent in your cluster and I'll",
    "start": "1181070",
    "end": "1186560"
  },
  {
    "text": "get into that so keep your table small if you can I think you want to have the cluster two to three times larger than",
    "start": "1186560",
    "end": "1193880"
  },
  {
    "text": "the size of your largest table so from our raw data as you might imagine raw",
    "start": "1193880",
    "end": "1199970"
  },
  {
    "text": "data is it's hard it's slower to parse every single event gets tracked and you",
    "start": "1199970",
    "end": "1206210"
  },
  {
    "text": "don't necessarily need down to the second resolution when you're doing aggregations and analytics so we build",
    "start": "1206210",
    "end": "1213560"
  },
  {
    "text": "an hourly event roll up and a daily event roll up and basically what that does is says okay if a user did the",
    "start": "1213560",
    "end": "1219650"
  },
  {
    "text": "button tap event three times in this hour I don't need to store all three of those events I can just store it once",
    "start": "1219650",
    "end": "1225800"
  },
  {
    "text": "and say the user did it three times during this hour and then the same is true for the daily table this was a",
    "start": "1225800",
    "end": "1231770"
  },
  {
    "text": "really really big optimization for us because when we first started out we were really paying for big table scans",
    "start": "1231770",
    "end": "1237710"
  },
  {
    "text": "on these large raw event tables and when we switched over to the hourly and especially the daily table for us we",
    "start": "1237710",
    "end": "1245060"
  },
  {
    "text": "pretty much only ever need daily resolution unless we want parameters so it was a huge win it went a lot faster",
    "start": "1245060",
    "end": "1252340"
  },
  {
    "text": "we tracked mobile revenue as well we have a summary table for that and we don't have raw data for that you just",
    "start": "1252340",
    "end": "1258560"
  },
  {
    "text": "copy and summarized data and we do the summarization in Hadoop and then we hit telemetry data as",
    "start": "1258560",
    "end": "1264410"
  },
  {
    "text": "well then we build derived tables and I don't know if that's an official term or anything but that's what i call it it's",
    "start": "1264410",
    "end": "1271370"
  },
  {
    "text": "any table that's sort of a derivative of of like just based data so like I said I",
    "start": "1271370",
    "end": "1278780"
  },
  {
    "text": "mentioned it before the installs table that keeps a record of when every device",
    "start": "1278780",
    "end": "1285170"
  },
  {
    "text": "ID was first seen and the conditions that were present at that time so we know that the user was on Android",
    "start": "1285170",
    "end": "1291500"
  },
  {
    "text": "version 4.3 when they installed and we know what they're installed it was and this allows us to do things like build",
    "start": "1291500",
    "end": "1297650"
  },
  {
    "text": "user retention tables which derives from both the install table and the event",
    "start": "1297650",
    "end": "1302900"
  },
  {
    "text": "daily table and i'm going to show you in detail how we do that but basically these support more complicated types of",
    "start": "1302900",
    "end": "1309950"
  },
  {
    "text": "analytics advanced analytics where you're not just interested in seeing you know okay how many events occurred each",
    "start": "1309950",
    "end": "1315500"
  },
  {
    "text": "day you're really you're really trying to do more complicated stuff like final analysis the user retention analysis all",
    "start": "1315500",
    "end": "1324260"
  },
  {
    "text": "sort of stuff and I'll get into an example of the user attention later so",
    "start": "1324260",
    "end": "1329450"
  },
  {
    "text": "we also have a nightly ETL process this runs every night in addition to the hourly ETL process what it does is it",
    "start": "1329450",
    "end": "1337429"
  },
  {
    "text": "checks to see ok are there 24 hours of data available for this day if if there",
    "start": "1337429",
    "end": "1344570"
  },
  {
    "text": "are great we'll proceed if not it'll check it three times once every 30 minutes or something like that and it'll",
    "start": "1344570",
    "end": "1349910"
  },
  {
    "text": "send us an L yer you know if it's not able to do it and then we can jump on it and figure out what happened but the",
    "start": "1349910",
    "end": "1356330"
  },
  {
    "text": "first thing it does is it wipes all data will old broad data we keep 90 days worth of raw data and then as much",
    "start": "1356330",
    "end": "1362870"
  },
  {
    "text": "summarized data as we can fit in our cluster and still get reasonable performance but yeah every day we wipe a",
    "start": "1362870",
    "end": "1369770"
  },
  {
    "text": "raw data older than 90 days then we build that daily summary table I was telling you about we build the user",
    "start": "1369770",
    "end": "1375350"
  },
  {
    "text": "retention table that I always talk about telling you about final table and then we vacuum vacuum every night so I'll get",
    "start": "1375350",
    "end": "1383630"
  },
  {
    "text": "into some do's and don'ts that we've learned along the way some best practices they may they may apply to you",
    "start": "1383630",
    "end": "1389300"
  },
  {
    "text": "in most cases they do but maybe not and I'll try and mention in caveats so",
    "start": "1389300",
    "end": "1395649"
  },
  {
    "text": "the first thing I would say is summarize your data because you really get a dramatic drop in the total volume you're",
    "start": "1395649",
    "end": "1402549"
  },
  {
    "text": "going to pay less for storage you're going to pay less for compute what we do is we basically just say all right give",
    "start": "1402549",
    "end": "1409269"
  },
  {
    "text": "me the distinct values for any given day",
    "start": "1409269",
    "end": "1414879"
  },
  {
    "text": "and then sum up how many events occurred during that day with you know the same action label choose good sort cheese so",
    "start": "1414879",
    "end": "1426549"
  },
  {
    "text": "here's an example of transactions I don't know it's some store somewhere is",
    "start": "1426549",
    "end": "1432429"
  },
  {
    "text": "selling things and they track per customer and transact transaction where",
    "start": "1432429",
    "end": "1440350"
  },
  {
    "text": "it occurred when it happened and how much it was it was for so there's a number of ways you could choose sort",
    "start": "1440350",
    "end": "1446379"
  },
  {
    "text": "keys for this right like one might say alright well I want to choose customer ID and then it means every time you scan",
    "start": "1446379",
    "end": "1453610"
  },
  {
    "text": "on customer ID it's going to be really fast but i don't know maybe maybe your",
    "start": "1453610",
    "end": "1459909"
  },
  {
    "text": "data maybe you're more concerned with looking at data by location so you know that your stores in las vegas are only",
    "start": "1459909",
    "end": "1467470"
  },
  {
    "text": "going to be looking at data for las vegas and stores in San Francisco I'm",
    "start": "1467470",
    "end": "1472840"
  },
  {
    "text": "only going to be looking at data for San Francisco in that case it makes a good location makes a really good sort key",
    "start": "1472840",
    "end": "1478629"
  },
  {
    "text": "because every time there's going to be a filter on sort key so you cut out like a lot of data you know if you have a",
    "start": "1478629",
    "end": "1484779"
  },
  {
    "text": "hundred locations and people only looking at one location great you just cut out ninety nine percent of the data",
    "start": "1484779",
    "end": "1490769"
  },
  {
    "text": "and then so sort keys work kind of in descending priority you can think of it",
    "start": "1490769",
    "end": "1496720"
  },
  {
    "text": "kind of like an order by clause that's effectively how it's ordering your data on disk so the second key is going to be",
    "start": "1496720",
    "end": "1504370"
  },
  {
    "text": "your second most important sort key it's not going to be as effective as the first one but if you know that you'll be",
    "start": "1504370",
    "end": "1510309"
  },
  {
    "text": "filtering on a date range that's often a very good sort key and then if you",
    "start": "1510309",
    "end": "1516070"
  },
  {
    "text": "really are interested in looking at individual customers then maybe you have your customer IDs your third sort key",
    "start": "1516070",
    "end": "1521379"
  },
  {
    "text": "but you get descending descending or diminishing returns I would say so the first one is the most",
    "start": "1521379",
    "end": "1528380"
  },
  {
    "text": "important so think about how your data is structured think about how it's going to be accessed and choose your sort keys",
    "start": "1528380",
    "end": "1533480"
  },
  {
    "text": "that way there are also angel lead sort",
    "start": "1533480",
    "end": "1539090"
  },
  {
    "text": "keys now that's not for a couple months I don't know as much about that but my understanding is that you have more",
    "start": "1539090",
    "end": "1545029"
  },
  {
    "text": "equal weighting to the sort keys so that's a different model entirely I won't talk about it because I'm not intimately familiar with it at Yahoo we",
    "start": "1545029",
    "end": "1552679"
  },
  {
    "text": "use traditional sort keys product is our first sort key because most of the time",
    "start": "1552679",
    "end": "1558049"
  },
  {
    "text": "people are only looking at one product at a time and event data is our second sort key because most of the time people",
    "start": "1558049",
    "end": "1564710"
  },
  {
    "text": "are only interested in the last seven days or the last 30 days they don't need to look at the whole table for all the",
    "start": "1564710",
    "end": "1569929"
  },
  {
    "text": "dates so vacuum nightly or don't say you",
    "start": "1569929",
    "end": "1577760"
  },
  {
    "text": "did but there are some tricks you can do to get around having to vacuum like you",
    "start": "1577760",
    "end": "1583429"
  },
  {
    "text": "can load data in sorted order we don't do that we just vacuumed nightly we if",
    "start": "1583429",
    "end": "1590750"
  },
  {
    "text": "you don't want to bother setting up an ETL tool just use a cron job right just just vacuum nightly do it when nobody is",
    "start": "1590750",
    "end": "1597380"
  },
  {
    "text": "using the cluster it'll that mean the more you do it the faster he runs so and",
    "start": "1597380",
    "end": "1603230"
  },
  {
    "text": "we analyze their data to vacuum an and a lot analyze every night avoid joins were",
    "start": "1603230",
    "end": "1610490"
  },
  {
    "text": "possible now like many schemas require joins and we we use joins pretty heavily",
    "start": "1610490",
    "end": "1616419"
  },
  {
    "text": "at Yahoo but don't do them if you don't have to like if you have a whole",
    "start": "1616419",
    "end": "1621890"
  },
  {
    "text": "normalized schema that you can do you normalize and put everything in one table like wide tables are okay in a",
    "start": "1621890",
    "end": "1628940"
  },
  {
    "text": "columnar database if you basically never have to pay for joins when you do have",
    "start": "1628940",
    "end": "1637070"
  },
  {
    "text": "to join there are some strategies that can help so these are some different",
    "start": "1637070",
    "end": "1644049"
  },
  {
    "text": "distribution examples that's that's how you're going to get your best performance for joining is choosing the right distribution style so basically",
    "start": "1644049",
    "end": "1652490"
  },
  {
    "text": "what you want to do is since redshift is it's basically a sharded environment your joins are going to be fastest when",
    "start": "1652490",
    "end": "1658619"
  },
  {
    "text": "the data is co-located on the same node because then you don't have to pay for the network transfer of data across the",
    "start": "1658619",
    "end": "1664710"
  },
  {
    "text": "network it'll be a lot faster so distribution keys can be used to achieve",
    "start": "1664710",
    "end": "1670259"
  },
  {
    "text": "this so one thing you can do is you can choose a distribution key and then all",
    "start": "1670259",
    "end": "1675539"
  },
  {
    "text": "of the records will be distributed by that key so the example I have here this",
    "start": "1675539",
    "end": "1682559"
  },
  {
    "text": "is I guess a two node cluster right and so these are columns you have one column",
    "start": "1682559",
    "end": "1689789"
  },
  {
    "text": "in two columns and you can see that it's distributed on the what does it",
    "start": "1689789",
    "end": "1697529"
  },
  {
    "text": "distributed on I don't remember my own slide yeah so it's distributed on the",
    "start": "1697529",
    "end": "1709350"
  },
  {
    "text": "second the the one the number in the in a one so all of the records with one in",
    "start": "1709350",
    "end": "1714869"
  },
  {
    "text": "our co-located all the records with three are co-located all the records with five are co-located so yeah okay so",
    "start": "1714869",
    "end": "1720869"
  },
  {
    "text": "these are two tables a and B a is a table be as a table and you can think of like one being the value for the",
    "start": "1720869",
    "end": "1728549"
  },
  {
    "text": "distribution key so if you wanted to join table a.2 table be it'd be really",
    "start": "1728549",
    "end": "1733799"
  },
  {
    "text": "fast to do it because they're distributed on that on that second value because you don't have to move records",
    "start": "1733799",
    "end": "1740639"
  },
  {
    "text": "from node to node the second example would be all distribution where all records get distributed to all nodes so",
    "start": "1740639",
    "end": "1747749"
  },
  {
    "text": "you're not doing any fancy distribution styles where you make sure that records are co-located by the key you just put",
    "start": "1747749",
    "end": "1753929"
  },
  {
    "text": "them everywhere and then records are always co-located which is great but the downside is that it eats up a lot more",
    "start": "1753929",
    "end": "1759600"
  },
  {
    "text": "disk space and then finally even distribution is probably something that",
    "start": "1759600",
    "end": "1766320"
  },
  {
    "text": "you don't want to do if you are if you're joining because it's just kind of randomly distributed like you can see",
    "start": "1766320",
    "end": "1773279"
  },
  {
    "text": "that how do i do the laser pointer yeah so you can see like a one is over here",
    "start": "1773279",
    "end": "1780179"
  },
  {
    "text": "you want to join it to be one what is on a separate note it's just kind of round-robin fashion and I think that's",
    "start": "1780179",
    "end": "1785399"
  },
  {
    "text": "the default don't quote me on that I think that's the default setting choose your distribution keys wisely",
    "start": "1785399",
    "end": "1793110"
  },
  {
    "text": "automate it's pretty simple we managed to do well it's not simple but it's it's",
    "start": "1793890",
    "end": "1801340"
  },
  {
    "text": "kind of it's kind of obvious when you think about it the more you automate the less work you have to do but we run our",
    "start": "1801340",
    "end": "1806710"
  },
  {
    "text": "entire cluster with three engineers and we automate everything that we don't",
    "start": "1806710",
    "end": "1812410"
  },
  {
    "text": "want to do by hand right like if you're running vacuums manually just use ETL or",
    "start": "1812410",
    "end": "1818020"
  },
  {
    "text": "you know even a cron job like I said that so you don't have to worry about it",
    "start": "1818020",
    "end": "1823260"
  },
  {
    "text": "don't fill your cluster I mentioned this earlier but leave more room than you think because if you are doing some kind",
    "start": "1823260",
    "end": "1830440"
  },
  {
    "text": "of really expensive etl operation that affects a large part of the big table",
    "start": "1830440",
    "end": "1836380"
  },
  {
    "text": "and that table is is big enough and it uses enough memory that it spills to",
    "start": "1836380",
    "end": "1842440"
  },
  {
    "text": "disk then suddenly you can run out of disk space you're doing that operation so you should leave two to three times",
    "start": "1842440",
    "end": "1849520"
  },
  {
    "text": "the size of your largest table free on the cluster that's not necessarily",
    "start": "1849520",
    "end": "1854830"
  },
  {
    "text": "obvious and we kind of learned that the hard way people told us this but we ignored it and just filled up the",
    "start": "1854830",
    "end": "1860050"
  },
  {
    "text": "cluster anyway so take my take my word for it workload management I recommend",
    "start": "1860050",
    "end": "1868990"
  },
  {
    "text": "reading up on this if you've never used wlm it's your friend what it's going to",
    "start": "1868990",
    "end": "1874060"
  },
  {
    "text": "allow you to do is create queues for your queries so that not everything runs",
    "start": "1874060",
    "end": "1879220"
  },
  {
    "text": "in the same queue so if you're doing etl people running ad-hoc queries will have",
    "start": "1879220",
    "end": "1885760"
  },
  {
    "text": "to they'll have to wait for your ETL to finish before their queries will run and that sucks because it might take two or",
    "start": "1885760",
    "end": "1891460"
  },
  {
    "text": "three hours for that etl query to finish my default everything just runs in the",
    "start": "1891460",
    "end": "1896860"
  },
  {
    "text": "default queue so using wlm you can set up cues for distinct things put your ETL",
    "start": "1896860",
    "end": "1902590"
  },
  {
    "text": "in 1q have a separate q4 ad hoc and then you won't have resource contention that way and you can actually specify",
    "start": "1902590",
    "end": "1909670"
  },
  {
    "text": "concurrency levels too so you might say all right my default Q should have a concurrency level of 10 no 10 queries at",
    "start": "1909670",
    "end": "1917680"
  },
  {
    "text": "once can run because for the most part they'll be relatively inexpensive compared to like doing a vacuum you can",
    "start": "1917680",
    "end": "1923980"
  },
  {
    "text": "also set time outs to so you can't you can prevent really expensive queries you're taking everything else down and",
    "start": "1923980",
    "end": "1929679"
  },
  {
    "text": "you can allocate memory so this is exactly what we use we have 2 q's you're the default queue and an etl q both get",
    "start": "1929679",
    "end": "1937059"
  },
  {
    "text": "fifty percent of the memory and you need more memory than you think for the ETL q",
    "start": "1937059",
    "end": "1943330"
  },
  {
    "text": "even though it's not always active that memory needs to be reserved for doing expensive operations okay don't now",
    "start": "1943330",
    "end": "1957520"
  },
  {
    "text": "these are adults don't use create table as for anything that's going to stick around is permanent the reason is you",
    "start": "1957520",
    "end": "1965710"
  },
  {
    "text": "don't get the benefits of sort keys when you do that so if you don't know create a table as basically you write that and",
    "start": "1965710",
    "end": "1971799"
  },
  {
    "text": "then you write your select statement after that and it'll create a table based on that relation it'll its",
    "start": "1971799",
    "end": "1977290"
  },
  {
    "text": "permanent it writes it to disk but it you can't there's no way to specify sort keys when you do that so your data won't",
    "start": "1977290",
    "end": "1983679"
  },
  {
    "text": "be sorted and it'll be slow so always",
    "start": "1983679",
    "end": "1988900"
  },
  {
    "text": "use the full create table syntax so you can specify your sort keys it's okay to",
    "start": "1988900",
    "end": "1994600"
  },
  {
    "text": "do that for temporary tables I do it all the time but anything that's going to be like user-facing don't do it don't email",
    "start": "1994600",
    "end": "2001590"
  },
  {
    "text": "sequel around like get a good reporting tool good bi tool like I said we use looker it's really worked well for us",
    "start": "2001590",
    "end": "2009290"
  },
  {
    "text": "tableau mixpanel any any of these will do it's better than emailing sequel",
    "start": "2009290",
    "end": "2014340"
  },
  {
    "text": "around saying hey run this you know there are open-source ones to you can",
    "start": "2014340",
    "end": "2019380"
  },
  {
    "text": "find so now I wanted to get into a deep dive to talk a little bit about user",
    "start": "2019380",
    "end": "2025830"
  },
  {
    "text": "retention because this is one of the more advanced applications of something that we do with redshift so in case you",
    "start": "2025830",
    "end": "2032309"
  },
  {
    "text": "don't know what user retention is or if you've used a different definition in the past I like this one I found it on",
    "start": "2032309",
    "end": "2038309"
  },
  {
    "text": "Quora it's from someone in Amazon he said the percentage of a user segment",
    "start": "2038309",
    "end": "2043980"
  },
  {
    "text": "who used your service in a period a who also used your service in a period b where b is greater than I felt the out",
    "start": "2043980",
    "end": "2050908"
  },
  {
    "text": "it's the inverse of churn might have heard a customer churn before so why do we care about it I mean I like",
    "start": "2050909",
    "end": "2060060"
  },
  {
    "text": "to think of it as probably the most important quality metric for your product and I haven't asked for their",
    "start": "2060060",
    "end": "2065638"
  },
  {
    "text": "kind of because it's not always right there might be something like a KPI that that's more meaningful for your business",
    "start": "2065639",
    "end": "2071280"
  },
  {
    "text": "or your product than retention but it's kind of a really good gauge of how well your product is doing if users are",
    "start": "2071280",
    "end": "2077520"
  },
  {
    "text": "sticking around then it means something's working if they all turn out after seven days then it means that",
    "start": "2077520",
    "end": "2083158"
  },
  {
    "text": "there's something wrong with the core experience if you're building a consumer application or you know even a b2b",
    "start": "2083159",
    "end": "2088800"
  },
  {
    "text": "application these are things you might want to track it's also very very",
    "start": "2088800",
    "end": "2094888"
  },
  {
    "text": "closely related to growth like if you think about growth as a science rather than something that's just mystical and",
    "start": "2094889",
    "end": "2101520"
  },
  {
    "text": "it just happens right like with users stick around then when you add more they",
    "start": "2101520",
    "end": "2109170"
  },
  {
    "text": "stay around longer and your total user counts compound so these are two different products right this is looking",
    "start": "2109170",
    "end": "2114960"
  },
  {
    "text": "at 14-day retention so users who installed at 14 days later how many of",
    "start": "2114960",
    "end": "2120330"
  },
  {
    "text": "them had still stuck around the green one has you know about thirty percent 14-day retention up so after two weeks",
    "start": "2120330",
    "end": "2127080"
  },
  {
    "text": "thirty percent of the original unions are still using it not great I am but I",
    "start": "2127080",
    "end": "2132510"
  },
  {
    "text": "mean if you compare it to the second product product be the blue one it really only gets about four percent retention I mean the graph on the right",
    "start": "2132510",
    "end": "2139470"
  },
  {
    "text": "shows this over time and you know like product be all of the users are gone you know after after 30 days all those word",
    "start": "2139470",
    "end": "2146910"
  },
  {
    "text": "users you worked so hard to get they're just gone and you would have no way of",
    "start": "2146910",
    "end": "2152040"
  },
  {
    "text": "knowing this if there wasn't a tool like red shift and a looker to be able to",
    "start": "2152040",
    "end": "2157380"
  },
  {
    "text": "build these kinds of graphs and I'm going to get into how we do that before I do though i like it i want to show you",
    "start": "2157380",
    "end": "2162930"
  },
  {
    "text": "this is like simulated data but for those two products that i showed you before this is how it compounds like if",
    "start": "2162930",
    "end": "2168660"
  },
  {
    "text": "you add a thousand users a day and then you take their to retention rates like this is how the growth trajectory looks",
    "start": "2168660",
    "end": "2174920"
  },
  {
    "text": "product a is going to grow much much more quickly than product p and the higher that retention number goes the",
    "start": "2174920",
    "end": "2180810"
  },
  {
    "text": "better this graph looks it also translates if you're if you're",
    "start": "2180810",
    "end": "2186270"
  },
  {
    "text": "advertising for your product to bring in new user is which we do it translates into a",
    "start": "2186270",
    "end": "2191470"
  },
  {
    "text": "wasted ad dollars because if ninety percent of your users turn out after a week then well you know all that money",
    "start": "2191470",
    "end": "2197470"
  },
  {
    "text": "you spend is wasted so not only will product a spend less money they'll waste",
    "start": "2197470",
    "end": "2204520"
  },
  {
    "text": "less money on users that disappear they'll have more money left over to turn to put back into advertising and face if they so choose so how do we",
    "start": "2204520",
    "end": "2213070"
  },
  {
    "text": "calculate user retention we use what we call the sputnik method it's not like an",
    "start": "2213070",
    "end": "2218200"
  },
  {
    "text": "official thing is just what we call it it's it's a way to build a",
    "start": "2218200",
    "end": "2223300"
  },
  {
    "text": "multi-dimensional user retention table so our actual table has like 14",
    "start": "2223300",
    "end": "2229000"
  },
  {
    "text": "dimensions device make country app",
    "start": "2229000",
    "end": "2234730"
  },
  {
    "text": "version OS version OS name I actually OS names in this one but basically the idea",
    "start": "2234730",
    "end": "2241360"
  },
  {
    "text": "is to be able to give the ability to slice and dice this however you want so",
    "start": "2241360",
    "end": "2246720"
  },
  {
    "text": "female users in the US between ages 25 and 55 how do they retain you know it's",
    "start": "2246720",
    "end": "2255250"
  },
  {
    "text": "it's actually really amazing what you can do when you have the right dimensionality but this is basically",
    "start": "2255250",
    "end": "2261940"
  },
  {
    "text": "what the table looks like under the hood you have the event date you have the install date the operating all the",
    "start": "2261940",
    "end": "2269200"
  },
  {
    "text": "dimensions and then at the end of the table you keep track of for that event date how many users were active and how",
    "start": "2269200",
    "end": "2275410"
  },
  {
    "text": "large was the cohort so the cohort is the group of users who installed with",
    "start": "2275410",
    "end": "2281740"
  },
  {
    "text": "these same dimension values so you can see that on Monday in the US for Android",
    "start": "2281740",
    "end": "2287770"
  },
  {
    "text": "devices there were 100 users there's this a cohort size of 100 since it's the day they installed they were all active",
    "start": "2287770",
    "end": "2294100"
  },
  {
    "text": "so you have 100 users that were active to on Tuesday there we go on Tuesday",
    "start": "2294100",
    "end": "2301810"
  },
  {
    "text": "some of them didn't come back right so the cohort size is still 100 but only 83 active users then you have another co",
    "start": "2301810",
    "end": "2308410"
  },
  {
    "text": "work down here this table shows two cohorts over two days 75 users Monday us",
    "start": "2308410",
    "end": "2314170"
  },
  {
    "text": "iOS and then on Tuesday I guess all of them returns so there would be a hundred",
    "start": "2314170",
    "end": "2319390"
  },
  {
    "text": "percent retention after day two or um date yeah on day two so this is",
    "start": "2319390",
    "end": "2326020"
  },
  {
    "text": "the sequel that you would write to generate a port or report from that table you sum up the active users you",
    "start": "2326020",
    "end": "2333130"
  },
  {
    "text": "some the cohort size and then you divide them and we're specifying at the end",
    "start": "2333130",
    "end": "2338710"
  },
  {
    "text": "event date is one greater than the installed it so this is day one retention and basically we're just",
    "start": "2338710",
    "end": "2349390"
  },
  {
    "text": "saying okay the today's date it always has to be one day greater than the event",
    "start": "2349390",
    "end": "2355690"
  },
  {
    "text": "did so here's how that calculation works",
    "start": "2355690",
    "end": "2360760"
  },
  {
    "text": "out like let's say that were so we're",
    "start": "2360760",
    "end": "2366580"
  },
  {
    "text": "only interested in day one retention right so we filter out the day zero about we filter out the day zero values",
    "start": "2366580",
    "end": "2372550"
  },
  {
    "text": "you can see they're great out there so we're just left with day 1 values and then the cohort sizes get added the",
    "start": "2372550",
    "end": "2378700"
  },
  {
    "text": "active users get added and then you divide them so that's like there's no granularity to that that's just",
    "start": "2378700",
    "end": "2384820"
  },
  {
    "text": "completely aggregated all the way up but if you wanted to then look at if you",
    "start": "2384820",
    "end": "2390850"
  },
  {
    "text": "wanted to see you know by OS name what the retention was well you just add OS name to the you add OS name to the query",
    "start": "2390850",
    "end": "2400000"
  },
  {
    "text": "and then you group by it and then it's going to give you retention by OS so",
    "start": "2400000",
    "end": "2405280"
  },
  {
    "text": "here's how that would look it just does the same calculation it does it separately though so you divide 83 by",
    "start": "2405280",
    "end": "2411970"
  },
  {
    "text": "100 to get eighty-three percent and 75 x 75 to get one hundred percent for Android so this is what this is what you",
    "start": "2411970",
    "end": "2420340"
  },
  {
    "text": "need if you if you wanted to build something like this you need a summary of events it actually doesn't have to be",
    "start": "2420340",
    "end": "2426850"
  },
  {
    "text": "summarized but it's a lot faster to build the table if you summarize it we",
    "start": "2426850",
    "end": "2432010"
  },
  {
    "text": "use a daily event summary and then we need a user or an install table that keeps track of when users installed and",
    "start": "2432010",
    "end": "2437500"
  },
  {
    "text": "what their characteristics were where what the dimension values were when they installed and then those need to be",
    "start": "2437500",
    "end": "2443230"
  },
  {
    "text": "joined on user I did so like I said avoid joins were possible but this is a case where we have to join in order to",
    "start": "2443230",
    "end": "2449440"
  },
  {
    "text": "build the table so step one is you calculate the cohort",
    "start": "2449440",
    "end": "2456560"
  },
  {
    "text": "sizes you figure out how big the cohorts are and you do it at the most granular level so for every possible cohort that",
    "start": "2456560",
    "end": "2463400"
  },
  {
    "text": "exists how many people were in it they can often be very small like you have two or three people and like some bizarre cohort but that's how we do it",
    "start": "2463400",
    "end": "2471880"
  },
  {
    "text": "then we figure out for each day for each user where you active or not and then we",
    "start": "2471880",
    "end": "2478850"
  },
  {
    "text": "maintain a list of you know all the users who are active in on what days and then we join those two together and then",
    "start": "2478850",
    "end": "2484850"
  },
  {
    "text": "we do aggregations so I'll walk you through how that works so here's the",
    "start": "2484850",
    "end": "2490970"
  },
  {
    "text": "first part where we calculate cohort sizes this is our users table we",
    "start": "2490970",
    "end": "2496910"
  },
  {
    "text": "basically just group by all of the dimensions and then we count all of the",
    "start": "2496910",
    "end": "2501950"
  },
  {
    "text": "users in that match the dimensions and that gives us something like this so",
    "start": "2501950",
    "end": "2508520"
  },
  {
    "text": "this is a two dimension table in addition to install data I guess so",
    "start": "2508520",
    "end": "2513580"
  },
  {
    "text": "we've got two different cohorts right there's the iOS us cohort I guess um",
    "start": "2513580",
    "end": "2518720"
  },
  {
    "text": "when still yahoo installed on october second and the android canada cohort who",
    "start": "2518720",
    "end": "2524600"
  },
  {
    "text": "installed on october first there's your cohort sizes so the next step is you",
    "start": "2524600",
    "end": "2530960"
  },
  {
    "text": "have to figure out who is active and when so we use a temp table to do this",
    "start": "2530960",
    "end": "2536920"
  },
  {
    "text": "we basically select the distinct combinations of user ID and event date",
    "start": "2536920",
    "end": "2543140"
  },
  {
    "text": "so that'll tell you okay these are all the users and this is when they were active and we use like a key trigger",
    "start": "2543140",
    "end": "2548630"
  },
  {
    "text": "event so if you open the app then you're eligible to qualify for active that's how we define it you can define it any",
    "start": "2548630",
    "end": "2554510"
  },
  {
    "text": "way you want you don't need you don't need to do that it can be any event like we don't care about spam events we just",
    "start": "2554510",
    "end": "2560390"
  },
  {
    "text": "say everyone who used the app on this day will always have an app open event so that's that's just how our app is",
    "start": "2560390",
    "end": "2566330"
  },
  {
    "text": "instrumented so then what we do is we",
    "start": "2566330",
    "end": "2572630"
  },
  {
    "text": "get a list of all of the users we create a tab table for that we create a temp table for all of the days okay then what",
    "start": "2572630",
    "end": "2581600"
  },
  {
    "text": "we do is we cross join all of the users we all the dates okay so for every user for",
    "start": "2581600",
    "end": "2588519"
  },
  {
    "text": "every day we're going to output something whether or not they were active basically and inner join on that",
    "start": "2588519",
    "end": "2594339"
  },
  {
    "text": "activity table and we're going to write a one if they were active and we'll",
    "start": "2594339",
    "end": "2599519"
  },
  {
    "text": "write a zero if they weren't active so",
    "start": "2599519",
    "end": "2606420"
  },
  {
    "text": "I'm not going to get into that but this is the this is basically what happens when you do that so this user was active",
    "start": "2606420",
    "end": "2614019"
  },
  {
    "text": "this user was active this user wasn't active okay well it's the same user but on these three days they were active on",
    "start": "2614019",
    "end": "2620140"
  },
  {
    "text": "the first to an active on the third then",
    "start": "2620140",
    "end": "2625869"
  },
  {
    "text": "what you can do is you can sum up from this table you can sum is active you can",
    "start": "2625869",
    "end": "2631569"
  },
  {
    "text": "sum is active and that gives you the total number of active users on that debt so for each event date for all the",
    "start": "2631569",
    "end": "2638319"
  },
  {
    "text": "dimensions how many users were active and that gives you your active users",
    "start": "2638319",
    "end": "2643349"
  },
  {
    "text": "then the last step is really just to join the cohort sizes with the active user counts and then you have your",
    "start": "2643349",
    "end": "2648849"
  },
  {
    "text": "result so at this point i wanted to show a quick demo of how this works to the",
    "start": "2648849",
    "end": "2656140"
  },
  {
    "text": "end user",
    "start": "2656140",
    "end": "2658528"
  },
  {
    "text": "so this is um this is looker this is the tool that we use for our bi this is",
    "start": "2663240",
    "end": "2672210"
  },
  {
    "text": "white labeled so we have our logo up here but alright you might have to look",
    "start": "2672210",
    "end": "2684420"
  },
  {
    "text": "at it in green there we go I guess I'll hold it there yeah so I'm going to zoom",
    "start": "2684420",
    "end": "2691650"
  },
  {
    "text": "in a little bit so it's easier to read but over here on the left you have all those dimensions that I was mentioning",
    "start": "2691650",
    "end": "2697130"
  },
  {
    "text": "country device make these are all the different dimensions that are available so like if I wanted to say all right",
    "start": "2697130",
    "end": "2703980"
  },
  {
    "text": "give me females and then we'll say in",
    "start": "2703980",
    "end": "2710300"
  },
  {
    "text": "the US and then we'll look by device OS",
    "start": "2710300",
    "end": "2717990"
  },
  {
    "text": "name so we've selected our dimensions I put in a product here it automatically",
    "start": "2717990",
    "end": "2724740"
  },
  {
    "text": "gets populated so that the filters always used male dog food is like the",
    "start": "2724740",
    "end": "2730050"
  },
  {
    "text": "internal version of yahoo mail that we dog food so I've got those filters and",
    "start": "2730050",
    "end": "2736200"
  },
  {
    "text": "then down here you have measures that you can select so I can select active users cohort size and percentage",
    "start": "2736200",
    "end": "2742200"
  },
  {
    "text": "retention and I'll run this and it's going to spit out oh right so I'll",
    "start": "2742200",
    "end": "2750000"
  },
  {
    "text": "select days since install I'll say day since install is going to be less than or equal to let's say 14 so we'll look",
    "start": "2750000",
    "end": "2757740"
  },
  {
    "text": "at two weeks of retention and then let's just pick an install date let's say is",
    "start": "2757740",
    "end": "2763890"
  },
  {
    "text": "on the day let's get something 12 weeks ago it's temporary so for users who",
    "start": "2763890",
    "end": "2772350"
  },
  {
    "text": "installed on September eighth what does their 14 and a retention curve look like and this is just in the US for this one",
    "start": "2772350",
    "end": "2778890"
  },
  {
    "text": "product who are female and then we're going to look at how it varies by",
    "start": "2778890",
    "end": "2784290"
  },
  {
    "text": "operating system also rana generally takes about 10 seconds we get really",
    "start": "2784290",
    "end": "2789330"
  },
  {
    "text": "really fast performance for this kind of stuff who data",
    "start": "2789330",
    "end": "2794760"
  },
  {
    "text": "I guess that's probably not a good example then let's look at",
    "start": "2794760",
    "end": "2804920"
  },
  {
    "text": "yeah I don't think there's any male dog food data in here hopefully it'll work for home run there we go so you can see",
    "start": "2813690",
    "end": "2825180"
  },
  {
    "text": "I'm just going to sort it so you can see for the first you know 14 days through",
    "start": "2825180",
    "end": "2830730"
  },
  {
    "text": "install there was a cohort size of five right and then on each subsequent day you can see how many users were active",
    "start": "2830730",
    "end": "2837240"
  },
  {
    "text": "so like this is a very small data set fault but we do this for every single one of our projects over here you can",
    "start": "2837240",
    "end": "2843360"
  },
  {
    "text": "see the okay the retention is you know twenty percent or zero percent that's",
    "start": "2843360",
    "end": "2848990"
  },
  {
    "text": "that's pretty much how we use this tool then there's visualization too you're not going to get a very good",
    "start": "2848990",
    "end": "2854010"
  },
  {
    "text": "visualization but that normally we would set up all of these complicated fancy",
    "start": "2854010",
    "end": "2859920"
  },
  {
    "text": "visualizations will look beautiful and it's really easy to share the way this works if you want it if you were curious",
    "start": "2859920",
    "end": "2865320"
  },
  {
    "text": "is this looker actually write sequel for you so if you look at like how complicated the sequel is there's a",
    "start": "2865320",
    "end": "2871410"
  },
  {
    "text": "modeling layer where you define all this stuff you write it there and then look her right to this so you can imagine",
    "start": "2871410",
    "end": "2876660"
  },
  {
    "text": "like trying to get people to run this stuff and like run it the same way every time it basically be impossible but",
    "start": "2876660",
    "end": "2884190"
  },
  {
    "text": "that's how it all works that's how it comes to fruition but yeah that's how",
    "start": "2884190",
    "end": "2893220"
  },
  {
    "text": "the end that's the end user the consumer of our data products i'm not seeing this",
    "start": "2893220",
    "end": "2898820"
  },
  {
    "text": "so i just wanted to wrap up by talking about like well what's what were some of the big wins for us which isn't showing",
    "start": "2898820",
    "end": "2907890"
  },
  {
    "text": "up",
    "start": "2907890",
    "end": "2910160"
  },
  {
    "text": "that's okay I'll just I'll just talk this is my last slide so basically there's there's really four categories",
    "start": "2916330",
    "end": "2922310"
  },
  {
    "text": "in which like red shift has really really been a big win for us being able to get real-time insights into our data",
    "start": "2922310",
    "end": "2930230"
  },
  {
    "text": "is a tremendously powerful like the dented a big company like Yahoo this would have already been assault problem",
    "start": "2930230",
    "end": "2936670"
  },
  {
    "text": "but it hasn't we were the first one to solve it basically and it's been a huge",
    "start": "2936670",
    "end": "2942020"
  },
  {
    "text": "win I think I think we'll probably just wrap it up there we did want to do a",
    "start": "2942020",
    "end": "2947750"
  },
  {
    "text": "quick Q&A if anybody has any questions we'd be more than happy to answer about anything about Yahoo about redshift",
    "start": "2947750",
    "end": "2956859"
  }
]