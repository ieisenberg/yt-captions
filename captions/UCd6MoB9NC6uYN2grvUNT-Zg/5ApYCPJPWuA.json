[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "all right hello everyone thank you for coming out oh thank you memo all right",
    "start": "4060",
    "end": "10250"
  },
  {
    "text": "so I got a fun topic for you all today I'm gonna be talking about leveling up your machine learning workflows so I",
    "start": "10250",
    "end": "15920"
  },
  {
    "text": "gave a talk earlier in the conference on how to make sense of machine learning targeted more towards getting start with",
    "start": "15920",
    "end": "21200"
  },
  {
    "text": "it what does this mean for you how can you go back to your organization have tangible next steps to actually start performing machine learning so just a",
    "start": "21200",
    "end": "29059"
  },
  {
    "text": "quick show of hands who is at a company currently that has machine learning deployed either as an API their own",
    "start": "29059",
    "end": "35570"
  },
  {
    "text": "services they rolled in-house okay cool so a good number of you that I'll help with we've got a my talk I think we",
    "start": "35570",
    "end": "47990"
  },
  {
    "text": "should good now yeah all right sweet great so again I'm Nick Walsh this is",
    "start": "47990",
    "end": "54860"
  },
  {
    "text": "leveling up your machine learning workflows a little bit about myself I'm a Technical Evangelist at AWS I have the",
    "start": "54860",
    "end": "61130"
  },
  {
    "start": "58000",
    "end": "58000"
  },
  {
    "text": "unique honor of getting to be here and teach you fellow developers all sorts of awesome things and to help empower you",
    "start": "61130",
    "end": "67550"
  },
  {
    "text": "using AWS services I'm super passionate about AI and machine learning I've worked in it pretty extensively in my",
    "start": "67550",
    "end": "72650"
  },
  {
    "text": "past roles and before I helped you write code to spin up infrastructure I wrote code that did everything from analyzing",
    "start": "72650",
    "end": "78500"
  },
  {
    "text": "human brains eggs and sharks so was big in the bio space back in the day but in",
    "start": "78500",
    "end": "85790"
  },
  {
    "text": "addition to that I get to host live shows with developer oriented content on the AWS Channel that's actually getting",
    "start": "85790",
    "end": "92420"
  },
  {
    "text": "streamed on them right now as well as my own personal channel so if you're in any way interested in live video content for developers check us out and I've worked",
    "start": "92420",
    "end": "99320"
  },
  {
    "text": "at some cool companies in the past that have done work in the IML space and as always comments criticism and anything",
    "start": "99320",
    "end": "106010"
  },
  {
    "text": "in between is always welcome on Twitter so let's give a quick roadmap of what we're gonna talk about in today's talk",
    "start": "106010",
    "end": "111950"
  },
  {
    "text": "so first we're gonna talk about AI pipelines in the wild every company has a different pipeline but I have a bit of",
    "start": "111950",
    "end": "118640"
  },
  {
    "text": "a calculus for helping you figure out where you fall into place the world of machine learning is a bit more",
    "start": "118640",
    "end": "123890"
  },
  {
    "text": "structured when it comes to what companies settle on with respect to pipelines and I want to share that with you and I'm going to do that and help",
    "start": "123890",
    "end": "130069"
  },
  {
    "text": "you figure out based on that how to improve your pipeline then after that I'm gonna give you advice that you may",
    "start": "130069",
    "end": "135560"
  },
  {
    "text": "not use right away but you'll be using down the line when you're going to be figuring out what am i hitting the end of this tear that I'm",
    "start": "135560",
    "end": "141290"
  },
  {
    "text": "in and what will I need or what will that moment be that I'll have to make the jump to the next level of services",
    "start": "141290",
    "end": "147319"
  },
  {
    "text": "and I know a lot of that is very big but over the course of this talk you'll you'll get to learn a little bit more about what that means",
    "start": "147319",
    "end": "153610"
  },
  {
    "text": "so first really quickly this is one of the big takeaways from my prior talk when I was explaining machine learning but to all many of you this should be",
    "start": "153610",
    "end": "159830"
  },
  {
    "start": "154000",
    "end": "154000"
  },
  {
    "text": "very familiar so the machine learning process in three steps at the core of it you have to define a problem statement",
    "start": "159830",
    "end": "165080"
  },
  {
    "text": "you have some sort of parameter you're optimizing for and this drives the decision of what algorithm you're going",
    "start": "165080",
    "end": "170569"
  },
  {
    "text": "to use you may have data already but if you have a specific question in mind that you want to solve you may need to",
    "start": "170569",
    "end": "175730"
  },
  {
    "text": "go out and get the data to solve that and so as a result of feeding that data that you have into your algorithm you",
    "start": "175730",
    "end": "180980"
  },
  {
    "text": "will have a model as an output and now machine learning does not provide value to your business until you're actually",
    "start": "180980",
    "end": "186620"
  },
  {
    "text": "using that model to perform predictions we call this inference and so using that model you build a micro service that can",
    "start": "186620",
    "end": "192739"
  },
  {
    "text": "perform inference hopefully it's scalable and you use that to whatever capacity your application needs either an endpoint that it operates internally",
    "start": "192739",
    "end": "199549"
  },
  {
    "text": "or maybe you make a machine learning model that operates externally as a service but this is really the core of machine learning so I just said this but",
    "start": "199549",
    "end": "207410"
  },
  {
    "text": "business value is achieved during inference time and if any of you have ever tried to justify budget a machine",
    "start": "207410",
    "end": "214459"
  },
  {
    "text": "learning can seem like it has a very large hump to get over with respect to collecting the data with respect to performing the training which can seem",
    "start": "214459",
    "end": "220849"
  },
  {
    "text": "like a very big deal but in reality not only is that the value delivered during inference time so you need to go from 0",
    "start": "220849",
    "end": "226730"
  },
  {
    "text": "to 1 in order to actually start getting value from your experiments but ultimately in the long term your",
    "start": "226730",
    "end": "232700"
  },
  {
    "text": "training becomes a small sunk cost if you're using machine learning at scale inference will very quickly",
    "start": "232700",
    "end": "237980"
  },
  {
    "text": "balloon and become the larger cost when compared to training so it's very important to focus on that and so training is still necessary it still",
    "start": "237980",
    "end": "245030"
  },
  {
    "text": "fits the the paradigm I described before but depending on the level of abstraction that your company is taking with respect to AI and M L it may just",
    "start": "245030",
    "end": "251660"
  },
  {
    "text": "not have to be managed by you and sometimes that's okay and I would recommend it and we'll go into what those examples look like so first",
    "start": "251660",
    "end": "259639"
  },
  {
    "start": "259000",
    "end": "259000"
  },
  {
    "text": "there's there's two main parts the AI pipeline and the level of abstraction you'll choose will dictate what tier",
    "start": "259639",
    "end": "265669"
  },
  {
    "text": "you're gonna fall in later with respect to system design and architecture power so first part is training here are steps",
    "start": "265669",
    "end": "272180"
  },
  {
    "text": "that essentially take you from end to end with training now many of you may not do some of these and and you'll see",
    "start": "272180",
    "end": "279590"
  },
  {
    "text": "yourself represented later on in the side deck but this is what what what it will take to go from data in some form",
    "start": "279590",
    "end": "285320"
  },
  {
    "text": "where we're considering that's the entry point right you have data you're storing it somewhere to ultimately having a validated model that you wanted to go in",
    "start": "285320",
    "end": "291320"
  },
  {
    "text": "and send into inference which is the next part so each of these steps again probably pretty familiar but we'll go",
    "start": "291320",
    "end": "296600"
  },
  {
    "text": "through them really quickly you collect the data for later access if it's a lot of data you may need some sort of personalized ETL extract transform load",
    "start": "296600",
    "end": "303770"
  },
  {
    "text": "pipeline especially if you have a huge amounts of data that you need to feed into an algorithm or perform transformations and cleaning on then",
    "start": "303770",
    "end": "310550"
  },
  {
    "text": "you're gonna have pre-processing where you've selected one algorithm you want to use or you know what format you need your data to begin to start wrangling",
    "start": "310550",
    "end": "316190"
  },
  {
    "text": "with and so pre-processing for example for use Python that's probably gonna be PI spark tie into the ETL there or numpy",
    "start": "316190",
    "end": "324710"
  },
  {
    "text": "pandas these libraries we're all used to working with and you know them inside now you want to be able to transform",
    "start": "324710",
    "end": "329900"
  },
  {
    "text": "that data such that I can plug into the algorithm which leads right into training when you plug your data you run it through the algorithm it's going to",
    "start": "329900",
    "end": "336080"
  },
  {
    "text": "output a model that's that's that's gold that's the gold Meg that's what you want and then maybe you're running a bunch of experiments in parallel maybe that's",
    "start": "336080",
    "end": "342320"
  },
  {
    "text": "your first experiment you want to go back make some changes so this isn't exactly linear and all of you probably know this you know you've you've trained",
    "start": "342320",
    "end": "348830"
  },
  {
    "text": "your first model something went terribly wrong or it's not accurate enough you go back and do it again you may run many experiments in parallel you may do hyper",
    "start": "348830",
    "end": "354500"
  },
  {
    "text": "parameter optimization all this falls under training and so these are the",
    "start": "354500",
    "end": "361130"
  },
  {
    "text": "tools that you're used to using every day and so data lakes and databases warehouses on the data side ETL I",
    "start": "361130",
    "end": "367700"
  },
  {
    "text": "mentioned spark but you know MapReduce Hadoop depending on what your company's existing maturity level is for your data",
    "start": "367700",
    "end": "374780"
  },
  {
    "text": "pipeline some systems may already have different tools you may not get the shoes right we think it's extremely",
    "start": "374780",
    "end": "380270"
  },
  {
    "text": "important to build pipeline to build tools for you that meet you where you're at we're not in this to convince you to",
    "start": "380270",
    "end": "386300"
  },
  {
    "text": "use a new tool we want to build tools that augments your existing workflow with pre-processing we build SDKs that",
    "start": "386300",
    "end": "392510"
  },
  {
    "text": "are available to work in the languages and use the libraries you already know and love and on the training side we",
    "start": "392510",
    "end": "398270"
  },
  {
    "text": "know that you're using what a language level frameworks like tensorflow like pi torch you want to to use those and so we meet you there",
    "start": "398270",
    "end": "405050"
  },
  {
    "text": "and we build and optimize those algorithms but I'll talk more about that later next up inference this is one of the",
    "start": "405050",
    "end": "411620"
  },
  {
    "start": "409000",
    "end": "409000"
  },
  {
    "text": "biggest ones and I think that there's a lot of information out there on the training side but I think that inference is one of the less publicly talked about",
    "start": "411620",
    "end": "418730"
  },
  {
    "text": "ones with respect to scaling there are a handful of companies that have to deal with this at insane scale examples being",
    "start": "418730",
    "end": "425110"
  },
  {
    "text": "for internal use Amazon Google uber Facebook etc these are data science and AI first companies and they have had to",
    "start": "425110",
    "end": "431720"
  },
  {
    "text": "deal with this problem at an unprecedented scale but this isn't obvious and it doesn't always come out in tech talks for how the public can",
    "start": "431720",
    "end": "438020"
  },
  {
    "text": "follow in their footsteps so here we see all these these steps deploying that model that we had before a model is just",
    "start": "438020",
    "end": "444950"
  },
  {
    "text": "a static file it's just a bunch of numbers it's weights you need to wrap that as a micro service and then the frameworks make that really easy to do",
    "start": "444950",
    "end": "451370"
  },
  {
    "text": "but you still need to do that the static file doesn't just perform prediction on its own as much as we all wish that could you have data ingestion right so",
    "start": "451370",
    "end": "458600"
  },
  {
    "text": "you have a function but how are you piping the data into it right there's a million different ways you can do that then once you have the data do I need a",
    "start": "458600",
    "end": "465740"
  },
  {
    "text": "pre process it is it coming from one source it gives it many sources what sort of like linting or or filtering do",
    "start": "465740",
    "end": "470930"
  },
  {
    "text": "I need to do on this data then finally you get to perform your prediction and then once that predictions made where",
    "start": "470930",
    "end": "476120"
  },
  {
    "text": "does it have to get served to is it the end of an API response and then you send that right back to whatever called your",
    "start": "476120",
    "end": "481820"
  },
  {
    "text": "micro service are you outputting a entry on a database are you putting it to a static file all these things have to be",
    "start": "481820",
    "end": "488960"
  },
  {
    "text": "considered and if your company doesn't need to do all then that's still okay but it's important understand that they",
    "start": "488960",
    "end": "494900"
  },
  {
    "text": "don't go away they're just abstracted in other services and so this is important not in the immediate future but when you want",
    "start": "494900",
    "end": "501230"
  },
  {
    "text": "to level up and go into the deeper levels having an understanding of what's next will help you ask the right questions to be able to do this so",
    "start": "501230",
    "end": "509120"
  },
  {
    "text": "technologies for this a lot of these should be straightforward but I'm gonna show you a specific examples about how to use this using AWS services that will",
    "start": "509120",
    "end": "516349"
  },
  {
    "text": "make your life extremely easy save you a lot of money and make your data scientists and machine learning practitioners extremely time efficient",
    "start": "516350",
    "end": "522979"
  },
  {
    "text": "which is one of the biggest sources of inefficiency and machine learning today so again some of this I've used pretty",
    "start": "522979",
    "end": "528740"
  },
  {
    "text": "fairly standard language with respect to the names of the technologies but I'm going to the specific services in a bit",
    "start": "528740",
    "end": "534010"
  },
  {
    "text": "so it's important to understand that every single customer leverages machine learning at a different level of",
    "start": "534010",
    "end": "539420"
  },
  {
    "text": "abstraction so your organization may only use an API and so the only",
    "start": "539420",
    "end": "545030"
  },
  {
    "text": "interface point that you had to worry about in both training and inference is in sending sending the API request with",
    "start": "545030",
    "end": "551720"
  },
  {
    "text": "the payload to the model the model of the API is a service is handling the pre-processing it's handling the post-processing it doesn't even think",
    "start": "551720",
    "end": "558230"
  },
  {
    "text": "about training that's already been handled for you so you still fit into that pipeline no matter where you are but we're gonna talk about how you can",
    "start": "558230",
    "end": "565010"
  },
  {
    "text": "think to expand and trip tips you can use no matter where you are in the existing pipeline with machine learning",
    "start": "565010",
    "end": "571400"
  },
  {
    "text": "there's no one-size-fits-all solution and this is for a lot of different reasons one of the biggest pain points",
    "start": "571400",
    "end": "576710"
  },
  {
    "text": "of machine learning is that there is a lot of academic phrasing in the education around it right like software",
    "start": "576710",
    "end": "582770"
  },
  {
    "text": "engineers can be amazing at writing bug-free code but that doesn't always translate into being able to read PhD level papers for state of the art",
    "start": "582770",
    "end": "588800"
  },
  {
    "text": "performance on machine learning algorithms and so education is a huge gap but there's a lot of factors beyond that so let's look at the ones that you",
    "start": "588800",
    "end": "596090"
  },
  {
    "text": "know you probably face very directly these are these are very obvious latency how quickly does this prediction need to",
    "start": "596090",
    "end": "601520"
  },
  {
    "text": "be served is someone uploading a picture that needs to instantly have its text read you have a very low latency",
    "start": "601520",
    "end": "606730"
  },
  {
    "text": "situation there maybe you want that connection to be done on the edge instead of on the cloud maybe you are",
    "start": "606730",
    "end": "613010"
  },
  {
    "text": "you are doing a Knightly and analytics suite basically of your database for the day that has no that is really like a",
    "start": "613010",
    "end": "619160"
  },
  {
    "text": "very high latency tolerance so you can take steps to actually optimize around that workflow to save a lot of money in",
    "start": "619160",
    "end": "625550"
  },
  {
    "text": "that instance next is cost obviously if cost is prohibitive in certain ways you",
    "start": "625550",
    "end": "630860"
  },
  {
    "text": "may be able to alter you it's it's not binary right it's not can I use machine learning can I not there are ways where",
    "start": "630860",
    "end": "636920"
  },
  {
    "text": "you can run a mop you can train a model for three weeks and get 95% accuracy or you could train it maybe in a day for",
    "start": "636920",
    "end": "642440"
  },
  {
    "text": "your use case to get up to 90 percent and maybe that instance is a sufficient trade-off of accuracy to costs and that",
    "start": "642440",
    "end": "649160"
  },
  {
    "text": "will work for your business use case but again every company has different permutation of all of these scalability",
    "start": "649160",
    "end": "654740"
  },
  {
    "text": "also right is this an internal service that's just going to satisfy a few teams internal products or is this something that's scaling out like like uber that",
    "start": "654740",
    "end": "661520"
  },
  {
    "text": "uses this to do predictions every single time you open the app these are all things that make you question how you're going to are",
    "start": "661520",
    "end": "667010"
  },
  {
    "text": "detect your application annexes customizability so you you think you",
    "start": "667010",
    "end": "672050"
  },
  {
    "text": "Nate you name your business problem you think you found maybe an API that works for it but what does that look like in the long term are you going to collect",
    "start": "672050",
    "end": "678440"
  },
  {
    "text": "more data are you going to want to answer more questions that may not be answered by that API will your company",
    "start": "678440",
    "end": "683480"
  },
  {
    "text": "needs change these are all things to consider and they will ultimately dictate what solution you choose for",
    "start": "683480",
    "end": "688880"
  },
  {
    "text": "machine learning so those are those are the hard stops right like we have a specific budget constraint we either",
    "start": "688880",
    "end": "694840"
  },
  {
    "text": "have the latency requirement or we don't but then there's some soft soft blockers too that you need to consider employee",
    "start": "694840",
    "end": "700910"
  },
  {
    "text": "skill set obviously we don't expect everyone to be PhD research scientists because it's not realistic we know that",
    "start": "700910",
    "end": "706400"
  },
  {
    "text": "our customers we talk to them that's not where they are out in the world some of them are like that we have services for them but we believe it's important to",
    "start": "706400",
    "end": "713060"
  },
  {
    "text": "build services and options for every type of customer so depending on what your employee skill set is we have a",
    "start": "713060",
    "end": "719240"
  },
  {
    "text": "machine learning option tailored towards that agility requirements so much like customizability what will the the needs",
    "start": "719240",
    "end": "725870"
  },
  {
    "text": "of this model change content wise how quickly do you need to update it it's just a model they're gonna build once and then just have to serve forever like",
    "start": "725870",
    "end": "733220"
  },
  {
    "text": "your problem is finite it's solved or is this a model that you constantly want to update on on frequent purchases for",
    "start": "733220",
    "end": "738260"
  },
  {
    "text": "frequent data updates right so for example with e-commerce sites you always want your model to serve up to their",
    "start": "738260",
    "end": "743330"
  },
  {
    "text": "recommendation so you want to ingest data and update that model very quickly and then lastly foundational infrastructure a lot of you especially",
    "start": "743330",
    "end": "749270"
  },
  {
    "text": "ones that are at older companies will have existing infrastructure that you have to work around so particularly your",
    "start": "749270",
    "end": "754970"
  },
  {
    "text": "data links or the data bases of choice you don't have the choice to be able to reset those so it comes down to will",
    "start": "754970",
    "end": "761330"
  },
  {
    "text": "this new tool that I want to plug into my system be compatible with the existing tools or the existing data maturity level that my company exhibits",
    "start": "761330",
    "end": "767870"
  },
  {
    "text": "and if it doesn't can I can I remedy that okay so that's a lot of background",
    "start": "767870",
    "end": "773600"
  },
  {
    "text": "on AI pipelines in general and I guarantee you you're all like Nick just tell me how I can save money using",
    "start": "773600",
    "end": "778700"
  },
  {
    "text": "tensorflow Nick how do i scale infinitely right I'm getting to it I'm getting to it so these factors they",
    "start": "778700",
    "end": "784820"
  },
  {
    "text": "create archetypes with respect to how customers play out I talk to lots of customers we all do at AWS and so I find",
    "start": "784820",
    "end": "791120"
  },
  {
    "text": "them falling into four main buckets and these are not the buckets themselves but the reasons why this happens",
    "start": "791120",
    "end": "796280"
  },
  {
    "text": "specifically in ml in a slightly different way from traditional software m/l education is largely quantized what",
    "start": "796280",
    "end": "802820"
  },
  {
    "text": "I mean by that is there is not a great level of granularity to learn gradually a machine learning you have the self",
    "start": "802820",
    "end": "808940"
  },
  {
    "text": "learner or that or someone who learned at a bootcamp they learned a certain amount of information or maybe someone who did a master's degree or has",
    "start": "808940",
    "end": "814430"
  },
  {
    "text": "experience deploying an application and they've they've dug a little bit deeper at a company and then maybe you have",
    "start": "814430",
    "end": "819440"
  },
  {
    "text": "PhDs and research scientists those are worlds apart with respect to machine learning expertise but ultimately",
    "start": "819440",
    "end": "825320"
  },
  {
    "text": "they're all machine learning practitioners and users just in different ways and so because we don't see this gradual distribution of",
    "start": "825320",
    "end": "831440"
  },
  {
    "text": "knowledge throughout that gap the tools are also then as a result going to be purpose-built for those very niche customers and so",
    "start": "831440",
    "end": "838970"
  },
  {
    "text": "that only serves to sort of perpetuate that same problem next is that jumping",
    "start": "838970",
    "end": "844070"
  },
  {
    "text": "abstraction levels can be difficult right it's easy to say ok my developers know API is it's their bread and butter so if we can match this business use",
    "start": "844070",
    "end": "851000"
  },
  {
    "text": "case to this API we're golden but going to the next level isn't just a matter of learning one small thing it's",
    "start": "851000",
    "end": "857000"
  },
  {
    "text": "a matter of learning a lot of different things and this is extremely intimidating and difficult so this is also why we see people falling into",
    "start": "857000",
    "end": "862880"
  },
  {
    "text": "buckets and lastly machine learning services are to access problem in software it's typically you just have",
    "start": "862880",
    "end": "870500"
  },
  {
    "text": "the complexity of the software problem I say just I'm not trying to generalize but ultimately it is like you have traditional computer science problems",
    "start": "870500",
    "end": "876889"
  },
  {
    "text": "now you take all of those and they apply to machine learning and you have another axis of unique problems specific to AI",
    "start": "876889",
    "end": "883430"
  },
  {
    "text": "nml where all the productivity and deployment and CI CD tools that have already existed for software that have",
    "start": "883430",
    "end": "889100"
  },
  {
    "text": "taken so long to builds don't really solve the last mile problems in AI so now you're not just assigning like alright is this a hard technical problem",
    "start": "889100",
    "end": "895459"
  },
  {
    "text": "from a software perspective you have to match that expertise with the machine learning expertise so it becomes a mess really really quickly so based on my",
    "start": "895459",
    "end": "903910"
  },
  {
    "text": "observations this is what I believe the archetypes are for the machine learning pipelines out in the wild at companies",
    "start": "903910",
    "end": "909740"
  },
  {
    "text": "just like all of yours first the software as a service or the API centric model highest level of extraction you",
    "start": "909740",
    "end": "916940"
  },
  {
    "text": "you you have a relationship with the vendor of the API where you do not have to have any machine learning expertise",
    "start": "916940",
    "end": "922550"
  },
  {
    "text": "you are ingesting that that that you are passing in data and ingesting the results that's it they handle the ops",
    "start": "922550",
    "end": "929240"
  },
  {
    "text": "they handle the scaling they handle the and you paper requests typically or no computation time it depends next is",
    "start": "929240",
    "end": "935690"
  },
  {
    "text": "plugging data into existing training and deployment flows so let's say company did research you couldn't find any API",
    "start": "935690",
    "end": "940700"
  },
  {
    "text": "that worked for you or you've moved on for a new business use case you now need to start leveraging algorithms to train",
    "start": "940700",
    "end": "946910"
  },
  {
    "text": "models and now we're not asking you to be research scientists here but we see that there are a large enough number of",
    "start": "946910",
    "end": "952400"
  },
  {
    "text": "open source examples tutorials and similar use cases that exist out there so I start you feel confident to plug",
    "start": "952400",
    "end": "958400"
  },
  {
    "text": "your data into an algorithm follow a workflow guide and be able to actually achieve your own machine learning pipeline at least a very basic version",
    "start": "958400",
    "end": "964790"
  },
  {
    "text": "of it that solves your problem and that's that's a really large group of people next we have the heavy",
    "start": "964790",
    "end": "969980"
  },
  {
    "text": "experimenters typically people move from 2 to 3 whereas people some companies",
    "start": "969980",
    "end": "975500"
  },
  {
    "text": "that say start with API is end up staying there so after a lot of experimentation you find out ways to optimize your workflow you may have",
    "start": "975500",
    "end": "981589"
  },
  {
    "text": "multiple models that train you may have versioning constraints or or very nuance type of deployments with respect to all",
    "start": "981589",
    "end": "989089"
  },
  {
    "text": "of that you need to learn a whole new set of skills and apply those see ICD principles I explained but learn them in",
    "start": "989089",
    "end": "994640"
  },
  {
    "text": "the context of machine learning and we have a lot of features that can help with that and last is the fully custom stack this is you achieve all of the",
    "start": "994640",
    "end": "1001510"
  },
  {
    "text": "benefit of working as close to the metal as possible but you're probably running things that we're running framework",
    "start": "1001510",
    "end": "1007450"
  },
  {
    "text": "versions or you're trying to deploy at scales that are unprecedented where software solutions start to break down and you need to start rolling your own",
    "start": "1007450",
    "end": "1013990"
  },
  {
    "text": "these are the ones that I've described at you know those major companies I said before and those unfortunately the",
    "start": "1013990",
    "end": "1019450"
  },
  {
    "text": "solutions they built out internally are not available to the public that's why we've built sage maker and some other things that I'm excited to",
    "start": "1019450",
    "end": "1024550"
  },
  {
    "text": "talk about so we see these archetypes but it's important to put into perspective so our mission AWS is we are",
    "start": "1024550",
    "end": "1030490"
  },
  {
    "text": "fanatical about customer choice and we are customer obsessed so I just named all these different types of customers",
    "start": "1030490",
    "end": "1035709"
  },
  {
    "text": "that exist out there and we have the task of putting machine learning in the hands of every one of them so you can",
    "start": "1035709",
    "end": "1042339"
  },
  {
    "start": "1042000",
    "end": "1042000"
  },
  {
    "text": "take my word for it or you can look at all the customers we have and I can happily say that more machine learning happens on AWS than anywhere else we",
    "start": "1042339",
    "end": "1049780"
  },
  {
    "text": "have 10 over 10,000 customers of our machine learning services across the stack we have two times number of customer references so you can directly",
    "start": "1049780",
    "end": "1055660"
  },
  {
    "text": "see what what is working for them the improvements they've gotten and lastly tensorflow and most popular frameworks",
    "start": "1055660",
    "end": "1061990"
  },
  {
    "text": "for running machine learning 85% of those workloads running AWS",
    "start": "1061990",
    "end": "1067200"
  },
  {
    "text": "okay so I went across the stack in one of my prior talks at collision here but",
    "start": "1067200",
    "end": "1072670"
  },
  {
    "start": "1068000",
    "end": "1068000"
  },
  {
    "text": "this is the full offering of the Amazon ml stack from top to bottom so quickly let's give a quick rundown of this at",
    "start": "1072670",
    "end": "1079870"
  },
  {
    "text": "the top level we have the AI services these are the api's so essentially you you I will talk in depth about what the",
    "start": "1079870",
    "end": "1086170"
  },
  {
    "text": "abstraction levels are for each of these but very quickly the AI services are paper per API call or per thousand API",
    "start": "1086170",
    "end": "1092500"
  },
  {
    "text": "calls your team needs no machine learning knowledge if your business use case in terms of the data you have and",
    "start": "1092500",
    "end": "1098800"
  },
  {
    "text": "the insights you want from it fit one of these services that is all that you need you just have to be able to query an",
    "start": "1098800",
    "end": "1105190"
  },
  {
    "text": "endpoint we handle the scaling we handle everything else and we're actually consistently updating those models under",
    "start": "1105190",
    "end": "1110770"
  },
  {
    "text": "the hood so when you pay for those api's you're not just paying for the results you're getting now you're paying for the",
    "start": "1110770",
    "end": "1116680"
  },
  {
    "text": "research scientist at AWS that's improving that service and making it more accurate for you going forward very",
    "start": "1116680",
    "end": "1121720"
  },
  {
    "text": "powerful to name a few of them because I'm not going into they really like customer use cases for them recognition for computer vision for both images and",
    "start": "1121720",
    "end": "1128050"
  },
  {
    "text": "video extremely powerful you pass pictures or videos can get you facial detection and give you labels of things",
    "start": "1128050",
    "end": "1133510"
  },
  {
    "text": "in the image extremely powerful text tracked OCR optical character recognition a very difficult problem",
    "start": "1133510",
    "end": "1139720"
  },
  {
    "text": "there are lots of open source libraries like tesseract for example I've worked with them directly in previous lines of work and they're good and they've come a",
    "start": "1139720",
    "end": "1146920"
  },
  {
    "text": "very long way but ultimately they they're very focused on just doing one thing which is pulling the letters and the words off the page in numbers but",
    "start": "1146920",
    "end": "1153340"
  },
  {
    "text": "ultimately we realize business use cases are more nuanced than that when you get that results you shall have to write a bunch of regex to be able to parse that",
    "start": "1153340",
    "end": "1159580"
  },
  {
    "text": "into whatever fragile way you're trying to you have your forms in and so that's really brittle that's not sustainable",
    "start": "1159580",
    "end": "1165430"
  },
  {
    "text": "you don't want that anytime you have a new form you don't have to like write new code to analyze that so we built text track the purpose-built OCR",
    "start": "1165430",
    "end": "1171520"
  },
  {
    "text": "solution that will actually take forms and like a printout of a table and it actually intelligently analyzes that",
    "start": "1171520",
    "end": "1177640"
  },
  {
    "text": "piece of paper and will return structured JSON content as a response it's crazy it's a huge step up from open",
    "start": "1177640",
    "end": "1183550"
  },
  {
    "text": "source and out-of-the-box OCR programs for speech we have text-to-speech with poly voice to text we have both batch",
    "start": "1183550",
    "end": "1189970"
  },
  {
    "text": "processing and stream processing so you can translate in real-time or you can translate as well as a transcribe which",
    "start": "1189970",
    "end": "1196300"
  },
  {
    "text": "is the service for speech and comprehend if you want to get the centum of text so angry neutral happy these are",
    "start": "1196300",
    "end": "1203680"
  },
  {
    "text": "things that are extremely valuable for business insights should we have chopped up program Lex super amazing you can",
    "start": "1203680",
    "end": "1209440"
  },
  {
    "text": "build this to improve your customer use customer interactions automate them reduce calls to your call centers forecast for time series data prediction",
    "start": "1209440",
    "end": "1216700"
  },
  {
    "text": "and personalized which is they like the personal recommendation engines that serve you product suggestions on Amazon",
    "start": "1216700",
    "end": "1222070"
  },
  {
    "text": "like the service recommendations from Netflix after your episode finishes you can have that by plugging in your",
    "start": "1222070",
    "end": "1227320"
  },
  {
    "text": "existing customer data and don't have to worry about training you just have to provide a historical data and the service will respond with nuanced and",
    "start": "1227320",
    "end": "1234670"
  },
  {
    "text": "proper recommendations for your customer extremely powerful sorry one second so",
    "start": "1234670",
    "end": "1244450"
  },
  {
    "text": "before I go into the other ones I figure will watch me dive a little bit deeper and see what what the AI services look",
    "start": "1244450",
    "end": "1249670"
  },
  {
    "start": "1249000",
    "end": "1249000"
  },
  {
    "text": "like so this is the first archetype SAS or AI services typically you have no",
    "start": "1249670",
    "end": "1255220"
  },
  {
    "text": "machine learning talent in-house nor have these our company decided that at this point in time it is a worthwhile investment for you you're probably",
    "start": "1255220",
    "end": "1262390"
  },
  {
    "text": "running a pilot first before you've even considered whether machine learning something your organization wants to commit to in the long term and the AI",
    "start": "1262390",
    "end": "1268690"
  },
  {
    "text": "the way AI services are architected from AWS makes us perfect there's no logistical like legwork in terms of getting set up",
    "start": "1268690",
    "end": "1275920"
  },
  {
    "text": "it's just an endpoint so you can query it with a bunch of pictures or your data for whatever service you need and see with a gut with a gut check hey is this",
    "start": "1275920",
    "end": "1282910"
  },
  {
    "text": "like right for me is it is this worth exploring next it's fully managed and hosted so if",
    "start": "1282910",
    "end": "1288970"
  },
  {
    "text": "it is you don't have to manage how that ever scales you just have to manage your own application code and whenever you're",
    "start": "1288970",
    "end": "1293980"
  },
  {
    "text": "sending out a request to one of our EAP eyes we can guarantee that based on our SLA is you will have the proper latency",
    "start": "1293980",
    "end": "1299380"
  },
  {
    "text": "you will have the proper response times it will be accurate it will not go down no ops to manage obviously everyone loves that makes it easier less calls",
    "start": "1299380",
    "end": "1305890"
  },
  {
    "text": "and pages in the middle of the night and the models are built by experts it really improved we look at all of our",
    "start": "1305890",
    "end": "1311170"
  },
  {
    "text": "customer use cases we see our customers down in the wild there are solutions architects that are evangelists like myself for helping and we see one of the",
    "start": "1311170",
    "end": "1316600"
  },
  {
    "text": "repeatable problems the customers are facing that we see people reinventing the wheel on because there just isn't enough public availability of services",
    "start": "1316600",
    "end": "1323200"
  },
  {
    "text": "and so that's why we've built these purpose-built solutions so here in blue are the services that are abstracted",
    "start": "1323200",
    "end": "1328510"
  },
  {
    "text": "away or the parts of the pipeline in both training and inference so you don't have to deal with them so here all you have to worry is ingesting the response just just pass",
    "start": "1328510",
    "end": "1334899"
  },
  {
    "text": "an API call and just the response you've got everything archetype number two basic train and deploy this is where you",
    "start": "1334899",
    "end": "1341679"
  },
  {
    "start": "1339000",
    "end": "1339000"
  },
  {
    "text": "start getting at the building your own model this is where Sage Maker is going to come in so machine learning services",
    "start": "1341679",
    "end": "1347379"
  },
  {
    "text": "especially from AWS that you're going to be looking at here sage maker which I'll dive a little bit deeper in a second so you make are hosted notebook instances",
    "start": "1347379",
    "end": "1353559"
  },
  {
    "text": "which you've if you've never used a jupiter notebook they're amazing and we have them out of the box with one click now and hosted servers they do a little",
    "start": "1353559",
    "end": "1359440"
  },
  {
    "text": "bit more than that to i'm excited to share we have stage maker ground truth so data labeling is a huge pain so",
    "start": "1359440",
    "end": "1365799"
  },
  {
    "text": "whoever has had to collect data knows that in order to pass to a supervised learning algorithm it needs to be",
    "start": "1365799",
    "end": "1370899"
  },
  {
    "text": "labeled so if i'm trying to make a shoe classifier i would need to pass in a bunch of images and each of them has to",
    "start": "1370899",
    "end": "1375999"
  },
  {
    "text": "be label with the brand of shoe that I want the algorithm to use to make this decision boundary and so you know I",
    "start": "1375999",
    "end": "1381039"
  },
  {
    "text": "think every data scientist at one point in time has to at their desk for a few hours saying like yes-no yes-no ABC it's",
    "start": "1381039",
    "end": "1388179"
  },
  {
    "text": "just it's terrible it's it's extremely inefficient and so ground truth is a part of stage maker in our platform",
    "start": "1388179",
    "end": "1394809"
  },
  {
    "text": "where you can upload images and create a task you know like web client in a web GUI and you can essentially pass in a",
    "start": "1394809",
    "end": "1401979"
  },
  {
    "text": "list of emails it could be people at your company you can also sign up for a Mechanical Turk as a target which is ultimately like a task based labor as a",
    "start": "1401979",
    "end": "1410470"
  },
  {
    "text": "service for for tasks like this and you can just say okay here's the task I want",
    "start": "1410470",
    "end": "1415659"
  },
  {
    "text": "you to look at these images here are examples of labels label them and we have this for multiple types of classification problems so you have",
    "start": "1415659",
    "end": "1422440"
  },
  {
    "text": "binary classification so for example is there a car in this image yes or no and then the user has to just click yes or no it's like a game almost they just go",
    "start": "1422440",
    "end": "1429309"
  },
  {
    "text": "to a URL and anyone who you've sent this to can then feed up the the data automatically they don't understand it",
    "start": "1429309",
    "end": "1435039"
  },
  {
    "text": "so binary classification multi classification um bounding boxes so draw a bounding box around the cars in each",
    "start": "1435039",
    "end": "1441190"
  },
  {
    "text": "of these images and one that's actually very difficult typically semantic segmentation which is like okay so I",
    "start": "1441190",
    "end": "1447070"
  },
  {
    "text": "have this image and I want to have the exact pixels that represent this object so so can you take a brush and",
    "start": "1447070",
    "end": "1452919"
  },
  {
    "text": "essentially go over and call her in those pixels I companies are rolling their own software to perform this labeling because when you do it often",
    "start": "1452919",
    "end": "1458799"
  },
  {
    "text": "enough it becomes a huge blocker it's all available through a GUI that you can deploy to your own company or other other people out in the wild that",
    "start": "1458799",
    "end": "1466179"
  },
  {
    "text": "we have we have vested part we have approved partners we have different levels of tearing depending on what your problem is if you need domain expertise",
    "start": "1466179",
    "end": "1472389"
  },
  {
    "text": "you can even get medical images categorized by doctors with HIPAA compliant policies so we really have",
    "start": "1472389",
    "end": "1479440"
  },
  {
    "text": "something for everyone to help you there here what we're looking at the user may manage the architecture some of it but",
    "start": "1479440",
    "end": "1486220"
  },
  {
    "text": "stage make your handles provisioning it and the important distinction there is that you need to understand that you",
    "start": "1486220",
    "end": "1491649"
  },
  {
    "text": "need a server for training you need to understand that an endpoint is how I'm going to serve my predictions but you",
    "start": "1491649",
    "end": "1496960"
  },
  {
    "text": "don't have to know ops code for spinning that up you don't need to know ec2 Amazon's service server as a service",
    "start": "1496960",
    "end": "1502740"
  },
  {
    "text": "offering to be able to do this you just need to know okay hey sage maker I need four instances to train this because",
    "start": "1502740",
    "end": "1510070"
  },
  {
    "text": "it's gonna be a huge it's gonna be a huge long-running task and I'm gonna write that I'm saying number of instances equals for instance site C for",
    "start": "1510070",
    "end": "1517029"
  },
  {
    "text": "X large you can enter with the Train commands here stage maker estimator outfit and in the background",
    "start": "1517029",
    "end": "1522789"
  },
  {
    "text": "sage maker will spin up those instances without you having to do anything it will pipe all the logs of that event",
    "start": "1522789",
    "end": "1528460"
  },
  {
    "text": "data in real time out to you it will perform the training it will paralyze your algorithm allows your training job",
    "start": "1528460",
    "end": "1534730"
  },
  {
    "text": "if your algorithm allows for it which most of the sage maker ones do and will actually distribute it and and basically",
    "start": "1534730",
    "end": "1539980"
  },
  {
    "text": "perform their MapReduce and pull it all back onto the host server which is the Jupiter notebook so you just performed distributed training across any number",
    "start": "1539980",
    "end": "1547090"
  },
  {
    "text": "of instances of any type with GPUs what I need to know anything else beyond this function right here so we know that ops",
    "start": "1547090",
    "end": "1554230"
  },
  {
    "text": "is a big blocker for a lot of machine learning folks so this is this has helped a lot of people and then ultimately I mentioned before you don't",
    "start": "1554230",
    "end": "1559360"
  },
  {
    "text": "get value until you're performing inference so you can have the best mod on the world you could have 98% accuracy right 99% accuracy but if you can't",
    "start": "1559360",
    "end": "1566110"
  },
  {
    "text": "deploy the service and all their app parts of your application can't use it it's not providing value so we have a one-line function for this if you're",
    "start": "1566110",
    "end": "1571419"
  },
  {
    "text": "building your model on stage maker you do stage make your dot deploy and you pass in the model that you just built and you have an endpoint and we have",
    "start": "1571419",
    "end": "1579129"
  },
  {
    "text": "something called Amazon elastic inference which is a box you can check that will say please scale this endpoint",
    "start": "1579129",
    "end": "1584559"
  },
  {
    "text": "to traffic so that you are never under provisioned so that when you know too many people hit it's it's lagging or it",
    "start": "1584559",
    "end": "1590350"
  },
  {
    "text": "goes down and you're never over provisioned because it will only scale up to the demand that you have instantaneously again like the huge",
    "start": "1590350",
    "end": "1595929"
  },
  {
    "text": "software problems exist in the ops world the existing tools didn't work but in amazon we've built our systems that",
    "start": "1595929",
    "end": "1601460"
  },
  {
    "text": "enable us to do this so what does this look like when it comes down to brass",
    "start": "1601460",
    "end": "1606950"
  },
  {
    "start": "1604000",
    "end": "1604000"
  },
  {
    "text": "tacks like what are the actual gains of using sage maker over just rolling your own frameworks open source frameworks on",
    "start": "1606950",
    "end": "1613730"
  },
  {
    "text": "servers so first is reducing costs 70% cost reduction for using ground truth if",
    "start": "1613730",
    "end": "1619010"
  },
  {
    "text": "you are paying a data scientist and they are on your payroll it is not efficient to have them performing a menial task",
    "start": "1619010",
    "end": "1624920"
  },
  {
    "text": "like data labeling unless there is a specific knowledge need before that to happen but most of the times there's not",
    "start": "1624920",
    "end": "1630080"
  },
  {
    "text": "so 70% of the cost reduction to do that next 75% cost reduction for inference so",
    "start": "1630080",
    "end": "1636650"
  },
  {
    "text": "typically you may package your model you ship it off to your DevOps folks at your team or the team of your company and say",
    "start": "1636650",
    "end": "1642410"
  },
  {
    "text": "hey wrap this as an endpoint put it up for me scale it just do whatever you need by leveraging GPUs under the hood",
    "start": "1642410",
    "end": "1649130"
  },
  {
    "text": "with our infrastructure we can actually automatically perform the scaling and perform it more effectively than you would be able to if you just rolled your",
    "start": "1649130",
    "end": "1655430"
  },
  {
    "text": "own servers or rolled your own open-source variants well against the open source part in a second next is increased performance as part of",
    "start": "1655430",
    "end": "1661850"
  },
  {
    "text": "this parallel eyes ability we have parallelizable GPU parallelizable variants of a lot of algorithms that enable you to scale with extremely high",
    "start": "1661850",
    "end": "1669340"
  },
  {
    "text": "efficiency higher than some of the open source equivalents so the issue with scalability on the training side is",
    "start": "1669340",
    "end": "1674690"
  },
  {
    "text": "always resource utilization so let's say you have this server and it has 10 GPUs and you want to make sure that every one",
    "start": "1674690",
    "end": "1679820"
  },
  {
    "text": "of those GPUs is running at 100% so that you're getting the fastest computation time possible well a lot of these open source frameworks as a function of them",
    "start": "1679820",
    "end": "1686240"
  },
  {
    "text": "being generalizable to work with a lot of different environments or actually not optimized to work on all of them as",
    "start": "1686240",
    "end": "1692600"
  },
  {
    "text": "well that's just a general software trade-off and so as a result you may only ever see peaking at 7080 percent",
    "start": "1692600",
    "end": "1697910"
  },
  {
    "text": "utilization on a GPU so your you have to scale out more even though none of those GPUs will go to one percent it's",
    "start": "1697910",
    "end": "1702950"
  },
  {
    "text": "terrible you it's just inefficiency and so we enable you to have both more",
    "start": "1702950",
    "end": "1708050"
  },
  {
    "text": "efficient access of resource utilization but also linear scaling a lot of a lot of frameworks will fall off after a",
    "start": "1708050",
    "end": "1713990"
  },
  {
    "text": "certain number of GPUs and not continue to scale linearly but ours do to a much higher extent I mentioned a one-click",
    "start": "1713990",
    "end": "1719960"
  },
  {
    "text": "training and deployment you could do it with with the SDK like I said before the dot deploy function or in the console",
    "start": "1719960",
    "end": "1725120"
  },
  {
    "text": "and train wants to run anywhere it's really cool but I'll mention it later because it's more on the full stack side with respect to complex deploy",
    "start": "1725120",
    "end": "1733990"
  },
  {
    "text": "so I mentioned before 85% of tensorflow workflows on the cloud on AWS so 65",
    "start": "1733990",
    "end": "1741770"
  },
  {
    "start": "1734000",
    "end": "1734000"
  },
  {
    "text": "percent is about the scaling efficiency you will get on individual GPUs due to in efficiencies in how the memory",
    "start": "1741770",
    "end": "1747740"
  },
  {
    "text": "handling and distribution is handled by the open source variant of tensorflow this it's not to say tensorflow is bad this is a trade-off that developers had",
    "start": "1747740",
    "end": "1754730"
  },
  {
    "text": "to make when they make a framework that wants to achieve mass popularity if you can make it efficient but you could you",
    "start": "1754730",
    "end": "1760520"
  },
  {
    "text": "can't also make it perfectly efficient and perfectly generalizable when you're using inherently different hardware platforms or something that's so close",
    "start": "1760520",
    "end": "1766850"
  },
  {
    "text": "to the hardware so 65 percent is around what we can see for utilization at large scale out we optimize sensor flow under",
    "start": "1766850",
    "end": "1773090"
  },
  {
    "text": "the hood you have the exact same bindings so when you bring tensorflow code to AWS your code will run just as",
    "start": "1773090",
    "end": "1778940"
  },
  {
    "text": "it normally does but you will be using more of each of the GPUs such that you can either achieve one of a few goals",
    "start": "1778940",
    "end": "1784760"
  },
  {
    "text": "you could use fewer instances or fewer GPUs to achieve the same training time or using the same number of GPUs your",
    "start": "1784760",
    "end": "1791419"
  },
  {
    "text": "models will finish training faster so it's a win-win you get to decide which of those that you want also you can feel",
    "start": "1791419",
    "end": "1796549"
  },
  {
    "text": "comfortable and sleep well at night knowing that you're not paying for a part of the utilization to just be heating the data center okay so sage",
    "start": "1796549",
    "end": "1804080"
  },
  {
    "start": "1803000",
    "end": "1803000"
  },
  {
    "text": "maker it's not an individual service much like all of the above ones I mentioned for api's and there's a reason",
    "start": "1804080",
    "end": "1809929"
  },
  {
    "text": "for that in this level of abstraction you truly need an end-to-end solution such that",
    "start": "1809929",
    "end": "1814970"
  },
  {
    "text": "you can make trade-offs and assumptions for passing off parts of your workflow along the way now there are open-source",
    "start": "1814970",
    "end": "1820580"
  },
  {
    "text": "variants or open source software solutions to try to pick at some of these individual parts the process there's actually what my last company",
    "start": "1820580",
    "end": "1827000"
  },
  {
    "text": "did we tried to pick at individual parts of this but we quickly realized is that if every single company is using a",
    "start": "1827000",
    "end": "1832250"
  },
  {
    "text": "different sort of open-source variant or each part has different connectors or compatibility the customer is never",
    "start": "1832250",
    "end": "1837590"
  },
  {
    "text": "going to be able to achieve some sort of stable and sustainable pipelines so this is why we built sage maker because we need customers need an end-to-end",
    "start": "1837590",
    "end": "1843440"
  },
  {
    "text": "solution to access all these efficiencies so first as pre-built notebooks not only do you not have to worry about spinning up a server and",
    "start": "1843440",
    "end": "1849919"
  },
  {
    "text": "then you know loading up an ami or in an operating system installing Jupiter and so on and so forth you just wanna get started you your data scientist used to",
    "start": "1849919",
    "end": "1856039"
  },
  {
    "text": "four notebooks like you could just type Jupiter notebook in your terminal on it's up when we make it that easy on the cloud you go into stage maker in the",
    "start": "1856039",
    "end": "1861679"
  },
  {
    "text": "console so it's the website and you click hosted notebooks you hit launch and you don't have to launch a beefy",
    "start": "1861679",
    "end": "1867140"
  },
  {
    "text": "notebook and I'll get into why that's the case in a sec you can essentially set up a master slave configuration where you are notebook use like your",
    "start": "1867140",
    "end": "1873500"
  },
  {
    "text": "home base it will prescribe out to other services and other pieces of infrastructure on how to run your code",
    "start": "1873500",
    "end": "1878660"
  },
  {
    "text": "and it'll do it really efficiently such that the only persistent piece of hardware is gonna be that notebook but because you can run it on a small server",
    "start": "1878660",
    "end": "1884600"
  },
  {
    "text": "it can be very cost effective next is algorithm selection so these are algorithms you know and love they exist",
    "start": "1884600",
    "end": "1891050"
  },
  {
    "text": "in various forms something identical across different frameworks scikit-learn has many of these tensorflow Carolus",
    "start": "1891050",
    "end": "1897410"
  },
  {
    "text": "they all have bindings for these some of them use this exact same code under the hood but again we go in we optimize",
    "start": "1897410",
    "end": "1902990"
  },
  {
    "text": "these further we have the hardware they're all gonna run on we're building the bindings so that we can connect them",
    "start": "1902990",
    "end": "1908060"
  },
  {
    "text": "so we go all the way down the stack and optimize these so that you can get better performance than the open source variants but use the code that you",
    "start": "1908060",
    "end": "1913790"
  },
  {
    "text": "already have so you're not getting locked in you're using the same code you already have and any code that you write you can do whatever you want with down",
    "start": "1913790",
    "end": "1919160"
  },
  {
    "text": "the line next is setting up environments this was exactly what I worked in and",
    "start": "1919160",
    "end": "1924470"
  },
  {
    "text": "it's it's hell um like for anyone who's worked in it both on trying to reproduce training results and deploying you know",
    "start": "1924470",
    "end": "1930140"
  },
  {
    "text": "everyone says like oh yeah use docker and docker is one of the foundational technologies that well solve this",
    "start": "1930140",
    "end": "1935240"
  },
  {
    "text": "problem but it's easy to say hey just learn this massive technology and all the pros and cons that come along with it when your machine learning",
    "start": "1935240",
    "end": "1940900"
  },
  {
    "text": "practitioner is already trying to learn ops and then proper software engineering best practices so if we can enable this",
    "start": "1940900",
    "end": "1946610"
  },
  {
    "text": "to have one-click deployment or reproducible environments like we do in Sage Maker we know it provides immense value and it makes it extremely easy to",
    "start": "1946610",
    "end": "1953000"
  },
  {
    "text": "lift and shift when you're going from training to deploying to production that's why we can enable one-line deployments training and tuning this is",
    "start": "1953000",
    "end": "1961040"
  },
  {
    "text": "a part of like algorithm selection you go in the experimental process hyper parameter optimization there's a huge problem right so you know this is the",
    "start": "1961040",
    "end": "1967760"
  },
  {
    "text": "right algorithm you know your data set is nice and clean you know you have the right features but now you just need to scale out your experimentation process",
    "start": "1967760",
    "end": "1973970"
  },
  {
    "text": "so that you can figure out what the optimal set of hyper parameters are and so the state-of-the-art performance has moved over time you know you can just do",
    "start": "1973970",
    "end": "1980480"
  },
  {
    "text": "a proper grid search you could do a random search which at times has shown to be even better but how do we optimize",
    "start": "1980480",
    "end": "1986000"
  },
  {
    "text": "that further how do we make that experience better than running a bunch of nested loops that if it crashes your your job just goes out well you can just",
    "start": "1986000",
    "end": "1992630"
  },
  {
    "text": "parallel eyes all of them we allow you to do that in stage maker hyper parameters just passing lists passing arrays and it'll actually spin all of",
    "start": "1992630",
    "end": "1998510"
  },
  {
    "text": "those jobs up this is what it looks like so I have a",
    "start": "1998510",
    "end": "2005080"
  },
  {
    "text": "factorization machine model here used for recommendation engines it's just imported from Sage maker and",
    "start": "2005080",
    "end": "2010500"
  },
  {
    "text": "here I'm passing the data and feature dimensions out of the shape of it I'm telling you the predictor type it's just",
    "start": "2010500",
    "end": "2016750"
  },
  {
    "text": "one of the many parameters you can look at the docks and I'm determining the number of factors a box I'm just defining my",
    "start": "2016750",
    "end": "2022300"
  },
  {
    "text": "training job this is the hyper parameters right this is very similar to how you already do it but this is enabling Sage maker to take this and",
    "start": "2022300",
    "end": "2027670"
  },
  {
    "text": "apply a very similar syntax to whatever algorithm you're using under the hood one click deployments I about this but I",
    "start": "2027670",
    "end": "2035920"
  },
  {
    "text": "want to just show you instead so what it looks like after you've you've taken that you've define your high parameters you do FM fit it will distribute that",
    "start": "2035920",
    "end": "2043120"
  },
  {
    "text": "perform the training it'll output your model artifact on s3 or wherever you want it to go and then once that's done",
    "start": "2043120",
    "end": "2048460"
  },
  {
    "text": "you can actually just deploy with stage maker got deployer in this case FM not deploy you you tell it the instance type",
    "start": "2048460",
    "end": "2054580"
  },
  {
    "text": "you want you tell it the initial count it couldn't be any easier next scaling",
    "start": "2054580",
    "end": "2062020"
  },
  {
    "text": "and managing the production environment obviously going from 0 to 1 is one of the biggest problems in machine learning but once you're there you will very",
    "start": "2062020",
    "end": "2068110"
  },
  {
    "text": "quickly realize the next question is oh so where's model 2 going where's version 2 of the first model how do we scale this out and parallel eyes",
    "start": "2068110",
    "end": "2074169"
  },
  {
    "text": "how I use more data it's the for everyone thinks it's so far away but it wants you to play that first model you're gonna be right there so",
    "start": "2074169",
    "end": "2080740"
  },
  {
    "text": "auto-scaling health checks automatic handling of node failures failover redundancies spinning of instances",
    "start": "2080740",
    "end": "2085960"
  },
  {
    "text": "backup we've solved the problem a lot of customers don't see as unique business value of solving it again for themselves",
    "start": "2085960",
    "end": "2091149"
  },
  {
    "text": "so please just use the cert that this this functionality is free as a part of sage maker you're still only paying for",
    "start": "2091150",
    "end": "2097270"
  },
  {
    "text": "the compute resources you're using these features are free to use you don't pay an additional part to use our sdk",
    "start": "2097270",
    "end": "2104610"
  },
  {
    "start": "2105000",
    "end": "2105000"
  },
  {
    "text": "okay so archetype 3 i think i'm i got to speed up a little bit but um we have ml",
    "start": "2105990",
    "end": "2111340"
  },
  {
    "text": "services so this is after you've been deployed that first model maybe you've got a bunch maybe you need to paralyze them you're using a lot of the same",
    "start": "2111340",
    "end": "2116470"
  },
  {
    "text": "services you just used but you're probably getting a little more intimately familiar with them maybe your company has expanded your data pipeline",
    "start": "2116470",
    "end": "2122440"
  },
  {
    "text": "and you want to have more sources so you have to perform more complex ETL Amazon glue and you know uh yeah I'm as I'm",
    "start": "2122440",
    "end": "2128470"
  },
  {
    "text": "gluing tables you to do that it enables you to pull in data perform a transformation on it it's completely realist you don't have to worry about",
    "start": "2128470",
    "end": "2134079"
  },
  {
    "text": "what instance that lives on and you basically just say hey here's my source this is what they think is gonna come in yes this is the format I want it to be",
    "start": "2134079",
    "end": "2140049"
  },
  {
    "text": "in and this is the destination you don't to worry about if what framework you're using for ETL or you can roll your own",
    "start": "2140049",
    "end": "2145329"
  },
  {
    "text": "you know you could be a PI spark also we have EMR elastic MapReduce if that's something your company's ready used to do or knowledgeable in and so ultimately",
    "start": "2145329",
    "end": "2152020"
  },
  {
    "text": "this is the deeper end-to-end integration so in the prior example archetype - you're kind of plugging your",
    "start": "2152020",
    "end": "2157089"
  },
  {
    "text": "data into an algorithm that you already knew works this is where maybe your software engineers have learned a little bit more about machine learning you can",
    "start": "2157089",
    "end": "2162970"
  },
  {
    "text": "reason a little bit better about algorithms maybe you've hired your first data scientists these are the services they're gonna want to use to be able to",
    "start": "2162970",
    "end": "2168730"
  },
  {
    "text": "unlock that next level of value for your company and pre-processing obviously it",
    "start": "2168730",
    "end": "2175359"
  },
  {
    "text": "depends on what your service looks like before I mentioned an API but you know you may have lots of different ways this and come in you could have streaming",
    "start": "2175359",
    "end": "2181480"
  },
  {
    "text": "data for your your your model what does that look like right like that's all you have to figure out for your company is it audio data is it pic image data is it",
    "start": "2181480",
    "end": "2188799"
  },
  {
    "text": "video data the world is wild out there but ultimately you still don't have to worry too much about ingesting it you're",
    "start": "2188799",
    "end": "2194589"
  },
  {
    "text": "not opening your own API gateway you're just deploying so you still don't have to worry about the ops on the endpoint",
    "start": "2194589",
    "end": "2199599"
  },
  {
    "text": "it's very handy and lastly post-processing like once your models made a decision as a micro service you still have to resi realize whatever that",
    "start": "2199599",
    "end": "2205839"
  },
  {
    "text": "output is and send it back as an API response or send it somewhere now in the function you you can build for pre-processing you can just define where",
    "start": "2205839",
    "end": "2212109"
  },
  {
    "text": "it wants to go but again you don't have to worry that you accidentally messed up your API gateway or that your service is gonna shut down again what does this",
    "start": "2212109",
    "end": "2220000"
  },
  {
    "start": "2219000",
    "end": "2219000"
  },
  {
    "text": "look like when it comes to numbers and sent dollars and cents training on GPU instances with parallel eyes ability on",
    "start": "2220000",
    "end": "2226270"
  },
  {
    "text": "stage maker is going to achieve a lot of performance gains but more specifically",
    "start": "2226270",
    "end": "2231849"
  },
  {
    "text": "we have a specific class of instances called spot instances so most of you are used to using on-demand instances I say",
    "start": "2231849",
    "end": "2237910"
  },
  {
    "text": "hey I want the server please spin up now I know what your rate is gonna be and then you spin down and then that's the",
    "start": "2237910",
    "end": "2243069"
  },
  {
    "text": "cost you paid now as the skill we operate at we can actually sell at a",
    "start": "2243069",
    "end": "2248170"
  },
  {
    "text": "different market rate based on supply and demand at any one point in time servers at a significant discount so",
    "start": "2248170",
    "end": "2254200"
  },
  {
    "text": "there's a trade-off here these servers may not be able to live forever and they may get interrupted but there are ways",
    "start": "2254200",
    "end": "2259450"
  },
  {
    "text": "to architect your application such that you can train and use these servers for thirty to fifty percent less than the",
    "start": "2259450",
    "end": "2265000"
  },
  {
    "text": "same exact equivalent on so again this works for certain workloads for example batch processing",
    "start": "2265000",
    "end": "2270580"
  },
  {
    "text": "you want to do a nightly analysis of your data set it doesn't need to be timely if it fails you can have it spin back up on another server and continue",
    "start": "2270580",
    "end": "2276730"
  },
  {
    "text": "it may be long-running processes right these are all ideal for spot instances and it's literally just a lift and shift",
    "start": "2276730",
    "end": "2282970"
  },
  {
    "text": "from on-demand to spot and then maybe you need to make a few tweaks if you have extremely long running tests but 30",
    "start": "2282970",
    "end": "2288040"
  },
  {
    "text": "50 percent is nothing as scoff at my co-workers for Shaun Keyes another evangelist he works the AI nml he wrote an awesome guy and being able to use GPU",
    "start": "2288040",
    "end": "2294520"
  },
  {
    "text": "training on these spot instances so you don't have to worry about this system failing really great post you'll get all",
    "start": "2294520",
    "end": "2299860"
  },
  {
    "text": "beginning the slides after so I recommend going to that so yeah people are naturally very excited about how",
    "start": "2299860",
    "end": "2305740"
  },
  {
    "text": "much money they save a spot instances next if you do want to roll your own",
    "start": "2305740",
    "end": "2311500"
  },
  {
    "start": "2307000",
    "end": "2307000"
  },
  {
    "text": "serialization for example say you have a very specific use case but you still want to use sage maker you can still do",
    "start": "2311500",
    "end": "2318340"
  },
  {
    "text": "that you don't have to give up all of the benefits ad maker has to be able to have more control again we're customer",
    "start": "2318340",
    "end": "2323470"
  },
  {
    "text": "choice we want to make sure you can do it you want so for example if you're building this to be a publicly exposed API or even a private API that another",
    "start": "2323470",
    "end": "2329620"
  },
  {
    "text": "team your company is going to use you can define what that serialization will look like for the response you're gonna give you don't have to worry about",
    "start": "2329620",
    "end": "2334810"
  },
  {
    "text": "dumping it to s3 and then figuring out how the other team's gonna ingest it you can actually write that code and write it directly in Sage makers notebook such",
    "start": "2334810",
    "end": "2341050"
  },
  {
    "text": "that you have all that code in one place you can look at that notebook it doesn't need to be in ten different places you",
    "start": "2341050",
    "end": "2347230"
  },
  {
    "text": "see here on the bottom you're defining common parameters for API is like what is that was content type what is the DC",
    "start": "2347230",
    "end": "2352450"
  },
  {
    "text": "raliser and Python here it's using the JSON deserialize er next distributed training I've talked a lot about this",
    "start": "2352450",
    "end": "2358240"
  },
  {
    "start": "2356000",
    "end": "2356000"
  },
  {
    "text": "I've talked about how it scales extremely efficiently and I have 10x listed as the order of magnitude of this cannon increase your training but it can",
    "start": "2358240",
    "end": "2365140"
  },
  {
    "text": "honestly be more than that if you really oh the only things you're bottlenecks here buy or what is your algorithmic efficiency how fast you need your model",
    "start": "2365140",
    "end": "2371410"
  },
  {
    "text": "trains and you know how long is your process to begin with because ultimately",
    "start": "2371410",
    "end": "2377580"
  },
  {
    "text": "and like please excuse me this is not technology alphabet soup this is almost like serverless machine learning on the",
    "start": "2377580",
    "end": "2383590"
  },
  {
    "text": "training site it's much more apt on the inference side but essentially the way you get billed for training",
    "start": "2383590",
    "end": "2388990"
  },
  {
    "text": "traditionally you'd spin up ec2 instances you put your job on it you run it it's done you check it you SSH in okay it's cool alright shut it down",
    "start": "2388990",
    "end": "2394710"
  },
  {
    "text": "you're paying for all of that lag or time on both ends with Sanja maker when you run that fit Fung",
    "start": "2394710",
    "end": "2400190"
  },
  {
    "text": "it provisions the infrastructure and then once it's launched it starts the clock and it has automatically on",
    "start": "2400190",
    "end": "2405890"
  },
  {
    "text": "spin-up time actually thrown together your job as part of that ami it instantly starts running it and the",
    "start": "2405890",
    "end": "2410900"
  },
  {
    "text": "second it actually errors out or it completes it tears down the infrastructure you cannot physically",
    "start": "2410900",
    "end": "2415970"
  },
  {
    "text": "beat sage makers distributed fit in terms of shutting down your for structure efficiently and the best part",
    "start": "2415970",
    "end": "2421940"
  },
  {
    "text": "is I don't have to worry about it right so you just run it like typically training jobs that they get started in the morning and your data scientists",
    "start": "2421940",
    "end": "2427340"
  },
  {
    "text": "around at their desk all day and they're just waiting alright did it finish did it not did it error and it takes hours their time you can run this at any time",
    "start": "2427340",
    "end": "2433160"
  },
  {
    "text": "of day and not have to worry about that and you're only getting billed to the exact seconds and it'll actually tell you that the end of the job it'll say you know hundred eighty seconds and the",
    "start": "2433160",
    "end": "2440660"
  },
  {
    "text": "magic of this is that parallel eyes as much as you want because you don't get billed on a multiplier for the number of",
    "start": "2440660",
    "end": "2446270"
  },
  {
    "text": "simultaneous instances you spit up so for example if I have one instance and it rains in a hundred hours or I can",
    "start": "2446270",
    "end": "2451670"
  },
  {
    "text": "spit up ten instances and let's say it's like perfectly parallelizable it will train in one tenth of time but because",
    "start": "2451670",
    "end": "2457340"
  },
  {
    "text": "the I have used the same whole amount of time in ten spinning for tenth one tenth",
    "start": "2457340",
    "end": "2462440"
  },
  {
    "text": "of time and one spinning for for one hundred percent you get billed the same amount so you can pay the same amount of",
    "start": "2462440",
    "end": "2467960"
  },
  {
    "text": "money to paralyze it as much as you want and pay the same exact amount it's like really really amazing okay next reducing",
    "start": "2467960",
    "end": "2475730"
  },
  {
    "text": "training time automated attributed machine learning I've talked about this I mentioned before you just define as a variable here the number of instances",
    "start": "2475730",
    "end": "2481520"
  },
  {
    "text": "you want the instance type and then when you do say to make your dot fit it just runs it for you it's amazing elastic inference I also mention this",
    "start": "2481520",
    "end": "2488360"
  },
  {
    "start": "2487000",
    "end": "2487000"
  },
  {
    "text": "before essentially what this is is it's a GPU accelerator it's a special device that we've built custom-built to work",
    "start": "2488360",
    "end": "2495770"
  },
  {
    "text": "with our hardware and essentially what this does is when when demand spikes it can use the GPUs that you don't have to",
    "start": "2495770",
    "end": "2502280"
  },
  {
    "text": "manage you just check a box in the console and will accelerate the the inference process you don't have to worry about setting that up you don't",
    "start": "2502280",
    "end": "2507770"
  },
  {
    "text": "have to worry about using an SDK for that it's just a box you check and you can save up to 75% compared to traditional GPUs if you want learn more",
    "start": "2507770",
    "end": "2513980"
  },
  {
    "text": "about that and the pricing around it for the tiers you can check it out at that link ok last last one some of you may be",
    "start": "2513980",
    "end": "2522770"
  },
  {
    "start": "2519000",
    "end": "2519000"
  },
  {
    "text": "approaching this this is definitely the smallest bucket out of all of them this is the custom stack this is inherently",
    "start": "2522770",
    "end": "2528200"
  },
  {
    "text": "very broad because what this means to end up in this stack is that you have faced typically",
    "start": "2528200",
    "end": "2534000"
  },
  {
    "text": "operations deployment or infrastructure demands that have not been met or are not generalizable enough to most other people and so while all those other",
    "start": "2534000",
    "end": "2540570"
  },
  {
    "text": "services that are really optimized you access cost savings they're all built on the exact tools you'll get to use here so none of them are abstracted away but",
    "start": "2540570",
    "end": "2547050"
  },
  {
    "text": "that means that you have the full customized ability to do this and so what are the instances where it would make sense to move to this and I'll get",
    "start": "2547050",
    "end": "2553080"
  },
  {
    "text": "to a little bit later but the common ones are like let's say you're using a tensor flow framework right like they just push the new nightly build and that's not supported on docker",
    "start": "2553080",
    "end": "2559710"
  },
  {
    "text": "containers yet that are stable and you want to use that because it's something for your business well you can build your own docker container and still use",
    "start": "2559710",
    "end": "2565320"
  },
  {
    "text": "sage maker to be able to make that a really efficient workflow there's lots of others you want to deploy at edge you want to deploy on IOT maybe you have a",
    "start": "2565320",
    "end": "2571620"
  },
  {
    "text": "not persistent internet connection that's still needs highly fast latency like on a farm right it doesn't have",
    "start": "2571620",
    "end": "2576780"
  },
  {
    "text": "internet on the middle of a field you can't get an edge device as long as you have internet to deploy to that device they'll actually perform inference on",
    "start": "2576780",
    "end": "2582480"
  },
  {
    "text": "edge these are things that we're helping to solve out in the wild every single day and you may not face it now but a",
    "start": "2582480",
    "end": "2587820"
  },
  {
    "text": "lot like of warehouses like that we've seen lots of instances where this is the case I mentioned glue before simplifying",
    "start": "2587820",
    "end": "2594630"
  },
  {
    "start": "2592000",
    "end": "2592000"
  },
  {
    "text": "your ETL process really simple build your data clock catalog this is gonna say what are my sources of data then",
    "start": "2594630",
    "end": "2599850"
  },
  {
    "text": "you're gonna generate and edit the transformations you say what is the format I need to get this data in to make sure that it gets passed and",
    "start": "2599850",
    "end": "2605310"
  },
  {
    "text": "properly the next service and then you're going to schedule and run your jobs it's a service you don't need to spin up servers to perform glue you just",
    "start": "2605310",
    "end": "2611880"
  },
  {
    "text": "point it at at different instances as a target to receive and a target to send to and you trigger in you you can either",
    "start": "2611880",
    "end": "2618180"
  },
  {
    "text": "manually trigger it or you can set an event trigger much like lambdas so you can say hey every night at midnight please run the CTL job there's lots of",
    "start": "2618180",
    "end": "2624330"
  },
  {
    "text": "other triggers you can run as well next serverless inference a lot of people don't think about this especially in the",
    "start": "2624330",
    "end": "2630690"
  },
  {
    "start": "2627000",
    "end": "2627000"
  },
  {
    "text": "machine learning world so AWS lambda who here has heard of lambda all right good a lot of you so lambda I",
    "start": "2630690",
    "end": "2639330"
  },
  {
    "text": "will go on a limb and say it is probably one of the most cost-effective ways to run inference should your inference",
    "start": "2639330",
    "end": "2645420"
  },
  {
    "text": "workload fit in the parameters and so there's only a few of them and lambda has expanded its capabilities over time such that more and more of your",
    "start": "2645420",
    "end": "2651540"
  },
  {
    "text": "workloads will fit on here so there's a few things you only pay for what time you use great that's why lamb is awesome auto-scaling don't manage any ops",
    "start": "2651540",
    "end": "2657420"
  },
  {
    "text": "beautiful but the considerations time and size on time lambda has a 15 minute",
    "start": "2657420",
    "end": "2662460"
  },
  {
    "text": "timeout I'm not telling you to run your training on land I'm telling you if you have ru inference tasks that can",
    "start": "2662460",
    "end": "2667620"
  },
  {
    "text": "on a CPU in under 15 minutes put it on lambda you'll never have to scale it you'll never have to worry about you",
    "start": "2667620",
    "end": "2674820"
  },
  {
    "text": "know it running over on time any like inefficiencies and how you have your sort of configuration lines like it's always just at the functional level you",
    "start": "2674820",
    "end": "2681150"
  },
  {
    "text": "just have an inference function and ultimately sighs so lambdas have a particular gigabyte size limit so if you",
    "start": "2681150",
    "end": "2687480"
  },
  {
    "text": "have a very large architecture for your model like the large weights file in a neural network for example if you have machine learning libraries they have to",
    "start": "2687480",
    "end": "2693630"
  },
  {
    "text": "import to perform that inference that are very large or if you have a huge payload like a huge video you still use",
    "start": "2693630",
    "end": "2698850"
  },
  {
    "text": "lambdas in that case but you have to figure out some smart ways to dice it up such that you fit under those",
    "start": "2698850",
    "end": "2703950"
  },
  {
    "text": "requirements but again it'll be slam to one of those cost-effective ways to be able to do this you just have to think if it will fit to your workflow build",
    "start": "2703950",
    "end": "2711240"
  },
  {
    "start": "2710000",
    "end": "2710000"
  },
  {
    "text": "once deploy anywhere stage maker neo this is extremely viable so in the past all those constraints I just mentioned",
    "start": "2711240",
    "end": "2716640"
  },
  {
    "text": "or what you think about when you start training your model right you have to think okay well if I'm mala can only be so large I can only use certain",
    "start": "2716640",
    "end": "2721950"
  },
  {
    "text": "framework certain libraries and it really limits what you want to do and let's say you have a model that's extremely successful and you want you",
    "start": "2721950",
    "end": "2727860"
  },
  {
    "text": "need a replicated you need to almost start from scratch and rebuild it to each of those platforms it's essentially completely independent pipelines so you",
    "start": "2727860",
    "end": "2734280"
  },
  {
    "text": "to maker neo by building an end-to-end platform enables you to train once using whatever typical code that you write",
    "start": "2734280",
    "end": "2739890"
  },
  {
    "text": "like the first way you would know code your model not the minified or ik strenuous different ways you would do it",
    "start": "2739890",
    "end": "2745140"
  },
  {
    "text": "you build it once and you can deploy it in any number of ways you can deploy to an edge device you can deploy that model",
    "start": "2745140",
    "end": "2751500"
  },
  {
    "text": "to a server you can deploy it anywhere and it will be able to match a lot of different runtimes match a lot of",
    "start": "2751500",
    "end": "2757080"
  },
  {
    "text": "different file sizes and constraints you have all from the same training so you don't have to think about how you're a multi class deployments or your multi",
    "start": "2757080",
    "end": "2764460"
  },
  {
    "text": "device or endpoint deployment is going to work when you use sage burning sage maker neo it's huge in the IOT space and",
    "start": "2764460",
    "end": "2770370"
  },
  {
    "text": "computed edge and for minifying models ok those are a lot of the tips and",
    "start": "2770370",
    "end": "2775380"
  },
  {
    "text": "tricks that I've learned and I've talked a little bit about how you figure out when you move between tiers but I've",
    "start": "2775380",
    "end": "2780660"
  },
  {
    "text": "really broken it down to two very specific leaps that you'll take first off is caveat I've mentioned before",
    "start": "2780660",
    "end": "2787110"
  },
  {
    "text": "there's no one-size-fits-all solution and you may have a different bottleneck that dictates a reason to jump or a reason that you cannot jump with your",
    "start": "2787110",
    "end": "2793530"
  },
  {
    "text": "organization if you're on the fence about where to start if you were here that your organization you don't know where to start here or if you are on the",
    "start": "2793530",
    "end": "2800250"
  },
  {
    "text": "fence about whether you should jump my advice is start at the higher level of abstraction first and the reason for",
    "start": "2800250",
    "end": "2805499"
  },
  {
    "text": "this is that by design none of your work is going to be wasted so these are not completely independent and separate",
    "start": "2805499",
    "end": "2811319"
  },
  {
    "text": "versions and solutions for machine learning they are nested dolls of levels of abstraction such that if you start at",
    "start": "2811319",
    "end": "2817349"
  },
  {
    "text": "the highest level of abstraction all of that work that your other services are doing you ingest the the results in a",
    "start": "2817349",
    "end": "2823229"
  },
  {
    "text": "file or in a end point will still persist when you move to the lower level so now you're making your own end point",
    "start": "2823229",
    "end": "2828389"
  },
  {
    "text": "and so you're you have to do a little bit more work on the back end but all the work you did to plug that into your application is still going to persist so",
    "start": "2828389",
    "end": "2834029"
  },
  {
    "text": "start high if you ever on the fence and go more in depth so next up I have the two jumps with these three categories",
    "start": "2834029",
    "end": "2839849"
  },
  {
    "text": "the first is going to be when an ia I service may or may not cut it for you when you're first starting or when you want to move on the biggest reason why",
    "start": "2839849",
    "end": "2846809"
  },
  {
    "text": "customers have to jump from a prepackaged AI service to building their own model is because the model accuracy",
    "start": "2846809",
    "end": "2851849"
  },
  {
    "text": "is either not sufficient enough for their specific use case or over time they've built out a mature data pipeline",
    "start": "2851849",
    "end": "2857579"
  },
  {
    "text": "such that they can actually build their own models and be even more accurate than any sort of generalizable solution the biggest factors they learn to be",
    "start": "2857579",
    "end": "2864599"
  },
  {
    "text": "helpful to you in the quality of live department ground truth for labeling notebooks for getting up to speed and orchestrating everything optimize",
    "start": "2864599",
    "end": "2870719"
  },
  {
    "text": "algorithms so you're gonna be running more efficiently than open-source variants and deployment and hosting so we try to make that leap as painless as",
    "start": "2870719",
    "end": "2876630"
  },
  {
    "text": "possible and then the next one is ultimately traffic scaling an infrastructure if you have extremely niche needs or if you have these very",
    "start": "2876630",
    "end": "2883079"
  },
  {
    "text": "specific things that you need that are not generalizable to the higher-level abstract of solutions you have full",
    "start": "2883079",
    "end": "2888239"
  },
  {
    "text": "control you still have access to those optimized frameworks that we run on our architecture you have access to in",
    "start": "2888239",
    "end": "2893519"
  },
  {
    "text": "Forenza and FPGAs which are really efficient hardware that we've developed in-house to be able to optimize your",
    "start": "2893519",
    "end": "2899639"
  },
  {
    "text": "machine learning inference so I'm gunning right up against time but I notice a lot I know that every step in",
    "start": "2899639",
    "end": "2905249"
  },
  {
    "text": "this pathway is asking your company to learn more but we want to meet you halfway we have a vested interest in",
    "start": "2905249",
    "end": "2911069"
  },
  {
    "text": "your success and we want to see you do well so we have ways we can help ml solutions lab is number one from brainstorming if you are interested come",
    "start": "2911069",
    "end": "2917819"
  },
  {
    "text": "talk to us we will help you we will help you figure out this is right for you we don't need you to come to us with the answers we can help you walk and ride",
    "start": "2917819",
    "end": "2924059"
  },
  {
    "text": "their custom modeling training work side by side with us we have various levels of support from enterprise support essays and so on and so forth and",
    "start": "2924059",
    "end": "2930449"
  },
  {
    "text": "machine learning training and certification for formal education so with that I hope you a few things hope you can save some",
    "start": "2930449",
    "end": "2936340"
  },
  {
    "text": "money be more efficient thank you i'm nick walsh and i hope you enjoyed hey",
    "start": "2936340",
    "end": "2941560"
  },
  {
    "text": "Nick we got a couple questions from from twitch cool so one is what's your recommended resource for someone that's",
    "start": "2941560",
    "end": "2948490"
  },
  {
    "text": "just getting started with machine learning on AWS yeah so if you're just getting started with machine learning",
    "start": "2948490",
    "end": "2954160"
  },
  {
    "text": "AWS it kind of depends are you looking to solve a business problems quickly if that is the case I would recommend",
    "start": "2954160",
    "end": "2960130"
  },
  {
    "text": "looking at our API very quickly if you are trying to start learning machine learning from scratch and learning how to build models I would recommend using",
    "start": "2960130",
    "end": "2966280"
  },
  {
    "text": "C to make our hosted notebooks because you'll have to worry about knowing what one even is you just click the button we have kernels and environments built out",
    "start": "2966280",
    "end": "2972250"
  },
  {
    "text": "already and you start using all of those frameworks right out of the box with lots of example code on there for solving complicated problems so and",
    "start": "2972250",
    "end": "2978990"
  },
  {
    "text": "there's another one from sock zero two which is what would you recommend for",
    "start": "2978990",
    "end": "2985360"
  },
  {
    "text": "someone that's just starting out in college or finishing high school and they want to get into data science like",
    "start": "2985360",
    "end": "2991240"
  },
  {
    "text": "what kind of coursework Michigan was someone that's just starting I want to get into the data science what I recommend so it depends on where you",
    "start": "2991240",
    "end": "2998830"
  },
  {
    "text": "want to end up which is always not a easy question to answer in the beginning I feel like most people don't know that more math is always better to know but I",
    "start": "2998830",
    "end": "3006360"
  },
  {
    "text": "would say find projects and and skills that excite you and and set that as your goal post meet it start lower than",
    "start": "3006360",
    "end": "3012300"
  },
  {
    "text": "higher and just keep moving along it's all iterative it all builds on one another so that's my advice",
    "start": "3012300",
    "end": "3018950"
  },
  {
    "text": "all right cool I think we're done so thank you again everyone who came out I know it's late on the third day but I",
    "start": "3019070",
    "end": "3024330"
  },
  {
    "text": "hope you all learned a thing it's you I've got a jet but I'm available here if you ever want to learn more or if you",
    "start": "3024330",
    "end": "3029670"
  },
  {
    "text": "need the slides they should be going out to all of you if you signed up with your email I have the AWS deaf theater if not",
    "start": "3029670",
    "end": "3034770"
  },
  {
    "text": "again reach out to me I'd love to help you out thanks again",
    "start": "3034770",
    "end": "3039200"
  }
]