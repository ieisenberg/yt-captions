[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "hello everyone my name is Abhishek I'm a senior product manager on Amazon Athena thank you for choosing this session",
    "start": "1020",
    "end": "7710"
  },
  {
    "text": "amongst probably 50 other very interesting sessions we appreciate that",
    "start": "7710",
    "end": "14630"
  },
  {
    "text": "we're going to talk about we're gonna take a deep dive on Athena and hopefully my name is after the session you might",
    "start": "14630",
    "end": "20730"
  },
  {
    "text": "say what that didn't sound too hard because the core principle what we've built a tea now on as simplicity that we",
    "start": "20730",
    "end": "28320"
  },
  {
    "text": "want to hide as much complexity as possible when it comes to building a big data system and that's predominantly",
    "start": "28320",
    "end": "35040"
  },
  {
    "text": "what Athena is built on we also have the privilege of welcoming a customer who is",
    "start": "35040",
    "end": "40170"
  },
  {
    "text": "going to talk about their use case and their journey of using Athena all the way from Rio so we have raul from OLX",
    "start": "40170",
    "end": "47850"
  },
  {
    "text": "brazil alex is an online marketplace and they'll talk about how they have used Athena in conjunction with a bunch of",
    "start": "47850",
    "end": "54750"
  },
  {
    "text": "other tools and what do they like and what do they not like about the tool so let's get started so we'll do there's a",
    "start": "54750",
    "end": "61920"
  },
  {
    "start": "58000",
    "end": "212000"
  },
  {
    "text": "lot of topics here so I'll go really quickly hopefully they're all very simple simple things to understand at",
    "start": "61920",
    "end": "71880"
  },
  {
    "text": "the end of it so we launched about a",
    "start": "71880",
    "end": "76920"
  },
  {
    "text": "year ago so tomorrow is actually our happy birthday so Andy launched and Jesse launched our",
    "start": "76920",
    "end": "82650"
  },
  {
    "text": "service last year keynote so we've been we've had a pretty busy year we've since",
    "start": "82650",
    "end": "87960"
  },
  {
    "text": "our launch we have added multiple ways to connect to athena for example we added an asynchronous REST API that you",
    "start": "87960",
    "end": "94380"
  },
  {
    "text": "can use various different SDKs to talk to athena it's a query API we under",
    "start": "94380",
    "end": "99509"
  },
  {
    "text": "added support for a bluest cloud trail that means you can audit every query that goes through the system we added",
    "start": "99509",
    "end": "106500"
  },
  {
    "text": "support for JDBC and ODBC drivers and there have been a bunch of new versions of those drivers that have been out",
    "start": "106500",
    "end": "112470"
  },
  {
    "text": "there we have expanded to 11 public AWS regions we have added support for grok",
    "start": "112470",
    "end": "119509"
  },
  {
    "text": "geospatial data and using an avro formats we also added support for",
    "start": "119509",
    "end": "124799"
  },
  {
    "text": "processing data using encrypted data using server-side encryption or",
    "start": "124799",
    "end": "130289"
  },
  {
    "text": "client-side encryption with keys in kms we integrated with AWS blue and ADA",
    "start": "130289",
    "end": "135989"
  },
  {
    "text": "blue as blue data catalog will talk about that we added support for cloud formation we also added a bunch of other",
    "start": "135989",
    "end": "143550"
  },
  {
    "text": "services also started putting the data in formats that Athena lights on Taobao date on on s3 so for example s3",
    "start": "143550",
    "end": "150780"
  },
  {
    "text": "inventory just released a feature where they actually get their inventory details in Orsi format on s3 and just",
    "start": "150780",
    "end": "157680"
  },
  {
    "text": "you can quickly create tables on the data and visualize or analyze your s3",
    "start": "157680",
    "end": "163110"
  },
  {
    "text": "inventory so since last year we've had a lot of customers triose and and and I've",
    "start": "163110",
    "end": "170850"
  },
  {
    "text": "given us a lot of great feedback if you are in the audience thank you very much there are a lot of times when you have",
    "start": "170850",
    "end": "177030"
  },
  {
    "text": "told us what you like and what you don't like about the service as a new service it's extremely important for us that you",
    "start": "177030",
    "end": "182640"
  },
  {
    "text": "give us a lot of feedback this has been a very very interesting year so we'll talk a lot about how these customers are",
    "start": "182640",
    "end": "189180"
  },
  {
    "text": "using using Athena we have learned a lot more about the various use cases of a Howell Tina is being used some that we",
    "start": "189180",
    "end": "196620"
  },
  {
    "text": "aimed at for some that we didn't we've also integrated with several partners",
    "start": "196620",
    "end": "202680"
  },
  {
    "text": "especially the bi bi tools you'll see some of the tools that you probably use in your day-to-day data on with this",
    "start": "202680",
    "end": "210810"
  },
  {
    "text": "platform here all right so let's look at a bunch of use cases that we have seen since launch the most",
    "start": "210810",
    "end": "217260"
  },
  {
    "start": "212000",
    "end": "356000"
  },
  {
    "text": "predominant one is Athena being used as a query layer on a data leg and and",
    "start": "217260",
    "end": "222269"
  },
  {
    "text": "that's pretty simple what we've seen if you talk to probably hundred customers that have that are building a data Lake",
    "start": "222269",
    "end": "228750"
  },
  {
    "text": "they agree on one thing they agree that s3 can be the data Lake and they would want to store the data in s3 they all",
    "start": "228750",
    "end": "234930"
  },
  {
    "text": "disagree on how they want to access this data you either run you know a lot of them run wanna run spark or open source",
    "start": "234930",
    "end": "242430"
  },
  {
    "text": "applications on EMR a lot of them run redshift they want to access the data out of redshift a lot of them want to",
    "start": "242430",
    "end": "247829"
  },
  {
    "text": "run in a serverless manner like in Athena so I think if you look at our service data analytics s3 data leak",
    "start": "247829",
    "end": "254549"
  },
  {
    "text": "story it's in just everything into s3 and you have a lot of ingestion mechanisms that allow you to ingest the",
    "start": "254549",
    "end": "260130"
  },
  {
    "text": "data you have a common catalog that you can share across these downstream query engine",
    "start": "260130",
    "end": "265590"
  },
  {
    "text": "and then you have a wide variety of query engines to choose from on the downstream side as to how you want to access the data so if you're just using",
    "start": "265590",
    "end": "272580"
  },
  {
    "text": "sequel Athena redshift EMR are good choices if you're doing SPARC with machine learning EMR is a good choice so",
    "start": "272580",
    "end": "279210"
  },
  {
    "text": "on and so forth so this is predominantly the biggest use case that we have seen and when we look at our customers use",
    "start": "279210",
    "end": "285570"
  },
  {
    "text": "case today with all X this also kind of adheres to this this pattern the second",
    "start": "285570",
    "end": "291419"
  },
  {
    "text": "one which was very interesting is we've seen customers and especially SAS",
    "start": "291419",
    "end": "296639"
  },
  {
    "text": "providers embed Athena as the sequel tool behind an API and expose the data",
    "start": "296639",
    "end": "301830"
  },
  {
    "text": "set that they have to external customers so that they can query the data directly so for example in this case Athena is",
    "start": "301830",
    "end": "310110"
  },
  {
    "text": "sitting behind in an API gateway and it basically our customers and all that SAS",
    "start": "310110",
    "end": "315630"
  },
  {
    "text": "provider has to do or their application provider has to do is push data into s3 and their customers can access the data",
    "start": "315630",
    "end": "322800"
  },
  {
    "text": "using sequel also it is a pretty cost effective mechanism right because all you're paying for in this case is the",
    "start": "322800",
    "end": "328979"
  },
  {
    "text": "data that is on on s3 for you to build a product out and expose it to the external environment Athena is only",
    "start": "328979",
    "end": "336389"
  },
  {
    "text": "charged on a per query basis so unless somebody actually runs the query you don't pay anything for Athena you just",
    "start": "336389",
    "end": "342120"
  },
  {
    "text": "be on there's three cars so it is a very cost effective mechanism if you're thinking about a product or if you're thinking about a mechanism to expose",
    "start": "342120",
    "end": "348840"
  },
  {
    "text": "your raw data or your you know aggregated data to your end customers so",
    "start": "348840",
    "end": "353940"
  },
  {
    "text": "they can run sequel analytics so good example is timber timber at i/o they're",
    "start": "353940",
    "end": "360900"
  },
  {
    "start": "356000",
    "end": "500000"
  },
  {
    "text": "they're a start-up they basically do log aggregation and do log enrichment and allow you to quickly process process",
    "start": "360900",
    "end": "368880"
  },
  {
    "text": "your logs in a very easy fashion this is their architecture and they basically wrap their API API is around Athena so",
    "start": "368880",
    "end": "376380"
  },
  {
    "text": "if you look at the bottom layer which is where they're collecting logs they have a mechanism in which they collect logs they take this logs they push this into",
    "start": "376380",
    "end": "383190"
  },
  {
    "text": "s3 they do some sort of ETL to push these logs in to oversee formats",
    "start": "383190",
    "end": "388349"
  },
  {
    "text": "partition the data and then they allow customers to pro to query these logs now",
    "start": "388349",
    "end": "394110"
  },
  {
    "text": "they they want customers to use a much simple language in sequel so what they do is they trap the query and decompose",
    "start": "394110",
    "end": "401070"
  },
  {
    "text": "the query into multiple smaller athina queries and then directly talk to the attina API so essentially their",
    "start": "401070",
    "end": "406830"
  },
  {
    "text": "operational workload in this case becomes as good as building putting data",
    "start": "406830",
    "end": "412530"
  },
  {
    "text": "in the right format in s3 and as their customers grow attina scales along with them very interesting use case the third",
    "start": "412530",
    "end": "420330"
  },
  {
    "text": "one that we have seen is migration from on-premises Hadoop environments and if",
    "start": "420330",
    "end": "425670"
  },
  {
    "text": "you look at a typical on-premise Hadoop environment you would see that there are a bunch of things that are going on there right so there is some ETL either",
    "start": "425670",
    "end": "432450"
  },
  {
    "text": "using spark or hive there's some data wrangling that is going on there's some machine learning with a bunch of open",
    "start": "432450",
    "end": "438480"
  },
  {
    "text": "source tool there's some interactive querying there's some data science and all this is kind of centered around a",
    "start": "438480",
    "end": "444450"
  },
  {
    "text": "hive meta store that allows you to create a common metadata environment so what customers are seeing is there are",
    "start": "444450",
    "end": "451170"
  },
  {
    "text": "there is a wide variety of services on AWS that you can take your on-premise hadoop implementation you can break into",
    "start": "451170",
    "end": "457920"
  },
  {
    "text": "those smaller components and then deploy it on top of AWS for example you could",
    "start": "457920",
    "end": "462960"
  },
  {
    "text": "use AWS glue and Amazon EMR for your ETL glue server less EMR allows you to do a",
    "start": "462960",
    "end": "469380"
  },
  {
    "text": "high level of customization on environment you can use a tina EMR and redshift for interactive querying EMR",
    "start": "469380",
    "end": "476100"
  },
  {
    "text": "carries a bunch of machine learning libraries along with it so you can use that but the common fundamental fill all",
    "start": "476100",
    "end": "481320"
  },
  {
    "text": "these is that being able to use s3 as a date-like you will see me repeating this",
    "start": "481320",
    "end": "487140"
  },
  {
    "text": "diagram a lot because it's essentially what a lot of what we see as commonality across a lot of customers that are",
    "start": "487140",
    "end": "494730"
  },
  {
    "text": "building the data lakes on top of AWS so that's the third use case which is migration from an on-premise cluster the",
    "start": "494730",
    "end": "501930"
  },
  {
    "start": "500000",
    "end": "639000"
  },
  {
    "text": "fourth one which was very interesting is AWS services by themselves produce a lot",
    "start": "501930",
    "end": "507120"
  },
  {
    "text": "of service logs and there is often a need to query these level service logs",
    "start": "507120",
    "end": "512729"
  },
  {
    "text": "to find useful information for example you might be querying cloud trail logs for information that you want to dig out",
    "start": "512730",
    "end": "519719"
  },
  {
    "text": "about let's say API access by a certain certain principle or you might be querying ELB logs so you might be",
    "start": "519720",
    "end": "526020"
  },
  {
    "text": "looking at AWS billing logs so what we see is because these logs are automatically pushed on to s3 Athena",
    "start": "526020",
    "end": "533110"
  },
  {
    "text": "becomes a really easy mechanism for creating these service loads typically customers who are doing these",
    "start": "533110",
    "end": "538540"
  },
  {
    "text": "are not the data analytics people in an organization there are security engineers and people who are managing",
    "start": "538540",
    "end": "544690"
  },
  {
    "text": "their network load balancers or people who are who have some kind of",
    "start": "544690",
    "end": "549730"
  },
  {
    "text": "infrastructure that is on top of a tableau as the architecture for this is fairly simple your logs some gets",
    "start": "549730",
    "end": "556930"
  },
  {
    "text": "deposited on s3 sometimes it's in the format that Athena likes sometimes it might not be in a format that you like",
    "start": "556930",
    "end": "562420"
  },
  {
    "text": "so most people use some sort of a lambda based transform to put this into the right format and by right format I mean",
    "start": "562420",
    "end": "568660"
  },
  {
    "text": "in the right order or in the right layout of your data on Esther you will talk about what is the right layout of",
    "start": "568660",
    "end": "574600"
  },
  {
    "text": "data on s3 and then you allow Athena to be able to query this a slightly more",
    "start": "574600",
    "end": "579700"
  },
  {
    "text": "evolved version of this is let when you have lots of logs for example we have customers that would have in several",
    "start": "579700",
    "end": "586210"
  },
  {
    "text": "terabytes of crowd front logs and they want to go back in history and process those logs they will take these logs in",
    "start": "586210",
    "end": "592900"
  },
  {
    "text": "they will ETL that true true blue converted into columnar format petition this put it back into s3 and then it",
    "start": "592900",
    "end": "599710"
  },
  {
    "text": "becomes available for people to create using Athena now the benefit of this is Athena has zero set-up cost that means",
    "start": "599710",
    "end": "607000"
  },
  {
    "text": "when you create tables or when you put data on s3 Athena itself doesn't charge you you get charged for the data in s3",
    "start": "607000",
    "end": "613000"
  },
  {
    "text": "that's different but Athena doesn't charge you for creating tables adding partitions so you can clearly see a lot",
    "start": "613000",
    "end": "619930"
  },
  {
    "text": "of customers doing like hundreds of DDL statements day over day and then one day when they need to actually query this",
    "start": "619930",
    "end": "625930"
  },
  {
    "text": "data they can easily just go to the table go to the Athena console or the JDBC interface and just query this data",
    "start": "625930",
    "end": "631750"
  },
  {
    "text": "and get the results so really good for ad-hoc wearing of this of log specially",
    "start": "631750",
    "end": "637330"
  },
  {
    "text": "eight with service logs so a lot of you gave us feedback that you guys generate",
    "start": "637330",
    "end": "643090"
  },
  {
    "text": "these logs these are all AWS service logs like cloud trail and he'll be and VPC flow logs",
    "start": "643090",
    "end": "648820"
  },
  {
    "text": "why don't you why are you making me write all of these create table statements do on all of these why don't",
    "start": "648820",
    "end": "655240"
  },
  {
    "text": "you just create a very simple template that allows me to create these logs instantaneously",
    "start": "655240",
    "end": "660779"
  },
  {
    "text": "on top of data and s3 so to do it pretty",
    "start": "660779",
    "end": "666060"
  },
  {
    "text": "quickly we started writing a bunch of blogs where we had CloudFormation templates and you know the most common",
    "start": "666060",
    "end": "672779"
  },
  {
    "text": "queries that you would have pertaining to every service log so for example if you go to the if you just search for AWS",
    "start": "672779",
    "end": "679589"
  },
  {
    "text": "big data log big data blog and click on it Tina you will find this this blog",
    "start": "679589",
    "end": "684870"
  },
  {
    "text": "which talks about visualizing AWS cloud trail logs with Athena glue and quick",
    "start": "684870",
    "end": "690120"
  },
  {
    "text": "side and it gives you the entire pipeline you can launch this pipeline and there are a lot more for example",
    "start": "690120",
    "end": "695160"
  },
  {
    "text": "here's one that processes the cost and usage reports also known as the billing reports on Athena here's one that takes",
    "start": "695160",
    "end": "702779"
  },
  {
    "text": "a backup of DynamoDB data set and takes out and and figures out what are the top",
    "start": "702779",
    "end": "708180"
  },
  {
    "text": "and objects if you're looking at hot spots in your dynamic db2 database here's one that actually looks at cloud",
    "start": "708180",
    "end": "714750"
  },
  {
    "text": "front and the use case here is I want to look at bot activity from my Cloud Print logs so all of these blogs have here's",
    "start": "714750",
    "end": "722670"
  },
  {
    "text": "my create table statement here's what a cloud print log looks like if I want to run a query on this data set here's what",
    "start": "722670",
    "end": "728370"
  },
  {
    "text": "it will it'll take to run a query on this data set if you want to do it one time it's pretty simple create a table",
    "start": "728370",
    "end": "733589"
  },
  {
    "text": "run a query now if you want to do it multiple times maybe on an everyday basis and create a report out of it then",
    "start": "733589",
    "end": "739559"
  },
  {
    "text": "here's a pipeline that will allow you to do it in a much more structured and cost-effective manner and it goes on to",
    "start": "739559",
    "end": "746879"
  },
  {
    "text": "a lot of lot of different things for example this is one on order databases where you can query the the log files",
    "start": "746879",
    "end": "753240"
  },
  {
    "text": "for connection query patterns so on and so forth VPC flow logs for for detecting any kind",
    "start": "753240",
    "end": "761639"
  },
  {
    "text": "of network network activity so on and so forth right so this there's a lot of templates available out there for you to",
    "start": "761639",
    "end": "769309"
  },
  {
    "text": "to follow the reason I put it out here was that if tomorrow you're in situation where you need to go and look at s3",
    "start": "769309",
    "end": "775860"
  },
  {
    "text": "access logs go to the blog you will find a table creation statement there you'll",
    "start": "775860",
    "end": "782430"
  },
  {
    "text": "find some best practices of creating the data set and it'll be a pretty quick ramp up curve for you to query those",
    "start": "782430",
    "end": "788339"
  },
  {
    "text": "logs is we also saw a customers like Airbnb",
    "start": "788339",
    "end": "794190"
  },
  {
    "start": "790000",
    "end": "1009000"
  },
  {
    "text": "use this as an underlying system to build completely new products so for example Airbnb has a open source project",
    "start": "794190",
    "end": "802319"
  },
  {
    "text": "that's on get called stream alert and what did what stream alert does is it allows you to deploy an infrastructure",
    "start": "802319",
    "end": "808290"
  },
  {
    "text": "that gives you real-time alerting based upon out of logs based upon the rules",
    "start": "808290",
    "end": "814740"
  },
  {
    "text": "that you set so it's a very simple scalable service and server less environment there is zero or very",
    "start": "814740",
    "end": "821790"
  },
  {
    "text": "minimal infrastructure requirement for this you can find it on and github today you can easily deploy it to your",
    "start": "821790",
    "end": "827550"
  },
  {
    "text": "infrastructure it will take it'll support a wide variety of data formats it can take data from cloud your data",
    "start": "827550",
    "end": "834569"
  },
  {
    "text": "center or other sources it will support data types like JSON key value store and key log and also supports a variety of",
    "start": "834569",
    "end": "841709"
  },
  {
    "text": "formats and this is all open source you can deploy it on your radio Bluest environment here's an here's an",
    "start": "841709",
    "end": "847560"
  },
  {
    "text": "architecture and the best way to kind of look at it is is go from left to right so on the left hand side are all your",
    "start": "847560",
    "end": "854129"
  },
  {
    "text": "producers on the right hand side at the bottom is your rules engine which is basically looking at real time",
    "start": "854129",
    "end": "859500"
  },
  {
    "text": "processing those rules and then all of this data goes and sits in s3 so if you want to query data that is not real-time",
    "start": "859500",
    "end": "866160"
  },
  {
    "text": "use athena for doing that and that uses the tina API itself so here's an example",
    "start": "866160",
    "end": "872540"
  },
  {
    "text": "in this example if we pulled sq sq for the latest s3 event based notifications",
    "start": "872540",
    "end": "878699"
  },
  {
    "text": "based upon what notifications are there the context of what file are is present",
    "start": "878699",
    "end": "884069"
  },
  {
    "text": "in s3 is present in the notification and based upon that it spins up a lambda function and updates a table or",
    "start": "884069",
    "end": "890490"
  },
  {
    "text": "partition on top of Athena it's fairly simple but the architecture is pretty clean so that's that's on querying AWS",
    "start": "890490",
    "end": "899639"
  },
  {
    "text": "service logs are essentially any kind of locks you can extrapolate that theory to any kind of logs the fifth one is is an",
    "start": "899639",
    "end": "908040"
  },
  {
    "text": "interesting one because we see a lot of people query database backups because either they want to query it for",
    "start": "908040",
    "end": "913800"
  },
  {
    "text": "archival purposes that they have a huge database only 10% of it is actually used and they want to keep the data set are",
    "start": "913800",
    "end": "920449"
  },
  {
    "text": "always queryable for compliance or historical reasons they back this data up on to s 3 and then Athena becomes a",
    "start": "920449",
    "end": "926819"
  },
  {
    "text": "really good way there to set up tables one and Athena becomes a really good way for querying those tables they are blogs on how to do this",
    "start": "926819",
    "end": "933899"
  },
  {
    "text": "for something like sequel server or Oracle on data bluest big data blog as well also we have seen customers push",
    "start": "933899",
    "end": "943290"
  },
  {
    "text": "time-series data on to s3 and then create and use a cleaner to query the time series data now the one critical",
    "start": "943290",
    "end": "950790"
  },
  {
    "text": "thing that we have seen in these Apple in for these customers as they are very latency tolerant that means they're not",
    "start": "950790",
    "end": "956579"
  },
  {
    "text": "looking at a real-time view of your time series data if you were to use get",
    "start": "956579",
    "end": "962759"
  },
  {
    "text": "something that is more real-time out of your data store then you probably want to use no sequel system within with you",
    "start": "962759",
    "end": "970110"
  },
  {
    "text": "know time series data so Athena doesn't play in that a good role there right the latency for a query in Athena is seconds",
    "start": "970110",
    "end": "978060"
  },
  {
    "text": "not milliseconds for that you have to use a no sequel store or something like DynamoDB for for customers that are",
    "start": "978060",
    "end": "983490"
  },
  {
    "text": "basically looking at time series data and they are tolerant to latency or",
    "start": "983490",
    "end": "989160"
  },
  {
    "text": "they're looking at large aggregations of this data that data can directly land in something like Kinesis firehose and",
    "start": "989160",
    "end": "996540"
  },
  {
    "text": "pushed on to s3 and the lambda function can be used to partition it if you was if you're hearing me speak the word",
    "start": "996540",
    "end": "1002959"
  },
  {
    "text": "partition a lot we will we will go deep into them into what that means and how to set it up we also added support for",
    "start": "1002959",
    "end": "1011300"
  },
  {
    "start": "1009000",
    "end": "1107000"
  },
  {
    "text": "geospatial data so you can build geometries on Athena so if your data is",
    "start": "1011300",
    "end": "1016970"
  },
  {
    "text": "an well-known JSON and you can you can create data types and build relationships out of that geospatial",
    "start": "1016970",
    "end": "1022370"
  },
  {
    "text": "data this is fairly new it's about two to three months so we're very excited with how customers are going to use this alright so one question that we get very",
    "start": "1022370",
    "end": "1030709"
  },
  {
    "text": "often is how is Athena different from something like EMR or something like redshift spectrum essentially what we're",
    "start": "1030709",
    "end": "1037339"
  },
  {
    "text": "trying to do is we're trying to extend you different use cases to double down on the whole idea of s3 being a deal",
    "start": "1037339",
    "end": "1044630"
  },
  {
    "text": "like so redshift spectrum is is redshifts way of extending that",
    "start": "1044630",
    "end": "1050120"
  },
  {
    "text": "capability on to s3 so if you are a predominant red chip user and you already have data in redshift and you",
    "start": "1050120",
    "end": "1056210"
  },
  {
    "text": "now want to create run ad-hoc queries on data for on s3 wretched spectrum is a great use",
    "start": "1056210",
    "end": "1062799"
  },
  {
    "text": "case you attina runs presto underneath you don't interact with the presto engine but you interact with the Athena",
    "start": "1062799",
    "end": "1068650"
  },
  {
    "text": "API but if you wanted to run presto on top of EMR because you know you want complete control over the cluster you",
    "start": "1068650",
    "end": "1075400"
  },
  {
    "text": "can do so as well and all those these three systems are essentially doubling down and the whole",
    "start": "1075400",
    "end": "1081190"
  },
  {
    "text": "concept of s3 as a daily right so first customers were using EMR that has always",
    "start": "1081190",
    "end": "1087940"
  },
  {
    "text": "been the case for customer that they have access data from s3 Athena is essentially server less so if you don't",
    "start": "1087940",
    "end": "1093520"
  },
  {
    "text": "want to manage your cluster you want simplicity go for Athena if you're already a redshift user and you're looking a way to ad-hoc to access data",
    "start": "1093520",
    "end": "1101440"
  },
  {
    "text": "in s3 in an ad hoc manner redshift spectrum is probably a good way to look at it all right there are three ways you",
    "start": "1101440",
    "end": "1108850"
  },
  {
    "start": "1107000",
    "end": "1210000"
  },
  {
    "text": "can connect to Athena when we started we only had a JDBC driver and what we",
    "start": "1108850",
    "end": "1114070"
  },
  {
    "text": "realized was we thought Athena is like a ad hoc query tool so people would like",
    "start": "1114070",
    "end": "1119350"
  },
  {
    "text": "to just connect it using a JDBC driver we were very surprised that how many people wanted to query it using an API",
    "start": "1119350",
    "end": "1124720"
  },
  {
    "text": "which told us that people wanted to build server less even driven pipelines to offload reporting on to Athena so we",
    "start": "1124720",
    "end": "1132400"
  },
  {
    "text": "added an API we also added a ODBC driver or an of course you can use Athena on",
    "start": "1132400",
    "end": "1137919"
  },
  {
    "text": "the console now the API that we build unlike the JDBC an ODBC driver which is synchronous in nature that means you",
    "start": "1137919",
    "end": "1144100"
  },
  {
    "text": "send a query you wait for the results you get the results the API is actually a saying that means there are two key",
    "start": "1144100",
    "end": "1151090"
  },
  {
    "text": "api's there there are also api's for like listing query executions listing results listing query history and so on",
    "start": "1151090",
    "end": "1157450"
  },
  {
    "text": "so forth what you would expect from any a tableau a service but essentially the two ones is you start query execution",
    "start": "1157450",
    "end": "1163690"
  },
  {
    "text": "that means you go and run a query you get a query ID then you pull on the query ID to get the results so kindly",
    "start": "1163690",
    "end": "1169330"
  },
  {
    "text": "decouples the execution of the query and running and running the query and we have seen a lot of customers use this",
    "start": "1169330",
    "end": "1175330"
  },
  {
    "text": "with lambda functions to do even driven I have some data that comes in I spin up",
    "start": "1175330",
    "end": "1181000"
  },
  {
    "text": "a lambda function or query the data and I process and I get the results we have seen a lot",
    "start": "1181000",
    "end": "1187299"
  },
  {
    "text": "of this in doing data quality checks before you loading your data into some kind of a database or a data warehouse",
    "start": "1187299",
    "end": "1193739"
  },
  {
    "text": "then there's a lambda function spin sup it runs one quick creates a table runs",
    "start": "1193739",
    "end": "1198759"
  },
  {
    "text": "it runs a query you get the results you see the results look fine is exactly the same number of records that you expected",
    "start": "1198759",
    "end": "1204669"
  },
  {
    "text": "you load the data up really good easy use case for Athena so let's talk a",
    "start": "1204669",
    "end": "1211720"
  },
  {
    "start": "1210000",
    "end": "1499000"
  },
  {
    "text": "little bit about creating tables on Athena right because essentially you know if you if you look at the total you",
    "start": "1211720",
    "end": "1219190"
  },
  {
    "text": "know Athena deep divers creating tables creating partitions and running queries",
    "start": "1219190",
    "end": "1224230"
  },
  {
    "text": "essentially those are the three things you can do on Athena so databases are basically logical concepts they don't",
    "start": "1224230",
    "end": "1231489"
  },
  {
    "text": "have in it's a logical collection of tables now when you start with the thena",
    "start": "1231489",
    "end": "1237249"
  },
  {
    "text": "like other evening of the read lewis service there is a limit on the number of database and tables that you get in your basic account now you can increase",
    "start": "1237249",
    "end": "1243970"
  },
  {
    "text": "that these are soft limits you can increase that by going to a limit",
    "start": "1243970",
    "end": "1248980"
  },
  {
    "text": "increase page and increasing and and requesting us for a limit so you can increase your databases you can increase",
    "start": "1248980",
    "end": "1254859"
  },
  {
    "text": "your tables you can also increase the number of partitions now there Athena uses a combination of hive to run your",
    "start": "1254859",
    "end": "1262090"
  },
  {
    "text": "DTL queries but it also uses Presto to run your sequel queries so but you don't",
    "start": "1262090",
    "end": "1267460"
  },
  {
    "text": "interact with these systems it does interact with the Athena API there are many ways to create tables on Athena",
    "start": "1267460",
    "end": "1272980"
  },
  {
    "text": "first you can go to the console and you can directly write a hive DDL statement",
    "start": "1272980",
    "end": "1278559"
  },
  {
    "text": "you can directly use the Athena API and give it a hive DDL statement to create a table you can use the JDBC and ODBC",
    "start": "1278559",
    "end": "1286210"
  },
  {
    "text": "driver but remember the diagram that I showed you and there was a glue catalog in between Athena",
    "start": "1286210",
    "end": "1291999"
  },
  {
    "text": "redshift spectrum in EMR the reason the glue cart lag is in between is because it's a shared metadata catalog that can",
    "start": "1291999",
    "end": "1298720"
  },
  {
    "text": "share metadata between EMR cluster running presto spark or high redshift spectrum and Athena",
    "start": "1298720",
    "end": "1305200"
  },
  {
    "text": "so once I've created my table in any one of these systems and if I'm using the glue data catalog we'll talk a little",
    "start": "1305200",
    "end": "1311259"
  },
  {
    "text": "bit about the blue Terry catalog but once I've created a table all of these three engines can actually see the table",
    "start": "1311259",
    "end": "1317620"
  },
  {
    "text": "right so that means I can also use glue directly to create tables that means I",
    "start": "1317620",
    "end": "1322960"
  },
  {
    "text": "can directly call the glue API to create a table and Athena will be able to see those tables glue also has a concept of",
    "start": "1322960",
    "end": "1329080"
  },
  {
    "text": "crawlers crawlers are essentially things that can run on your on your data in s3",
    "start": "1329080",
    "end": "1337060"
  },
  {
    "text": "and automatically infer schemas on create tables and automatically give you",
    "start": "1337060",
    "end": "1343390"
  },
  {
    "text": "information about the data that you have already in s3 so it'll can automatically create a table create a petition so on",
    "start": "1343390",
    "end": "1349210"
  },
  {
    "text": "and so forth so you can use the group crawlers to crawl the data that is sitting in s3 create a table and then",
    "start": "1349210",
    "end": "1354730"
  },
  {
    "text": "Athena can query it you can go to EMR using hive create a table and Athena can query it or vice versa",
    "start": "1354730",
    "end": "1360930"
  },
  {
    "text": "you can also use the blue API so if you are for example run adding a million",
    "start": "1360930",
    "end": "1368440"
  },
  {
    "text": "partitions a day the best way to do it is directly calling the glue API to let you load partitions a lot of our",
    "start": "1368440",
    "end": "1375160"
  },
  {
    "text": "customers come to us and say we need higher rates or we need higher rates of",
    "start": "1375160",
    "end": "1380770"
  },
  {
    "text": "how we are running DTL queries and Athena and the best way to do it is to use the glue catalog and directly call",
    "start": "1380770",
    "end": "1387520"
  },
  {
    "text": "the blue API to add partitions or add tables so on and so forth creating",
    "start": "1387520",
    "end": "1395320"
  },
  {
    "text": "databases is our creating tables is pretty simple so Athena only works on a concept of external tables that",
    "start": "1395320",
    "end": "1402160"
  },
  {
    "text": "basically means that where your data definition or where your metadata is stored is different from your data is",
    "start": "1402160",
    "end": "1408130"
  },
  {
    "text": "stored so for example if you look at this create table statement this create table statement must always have an s3",
    "start": "1408130",
    "end": "1415270"
  },
  {
    "text": "prefix at its end that denotes the location of where the data of the data resides in the table now in this case",
    "start": "1415270",
    "end": "1423340"
  },
  {
    "text": "Athena can only create table else on prefixes and every object that you have",
    "start": "1423340",
    "end": "1429310"
  },
  {
    "text": "in the prefix will become a part of that particular table so in this case for",
    "start": "1429310",
    "end": "1434440"
  },
  {
    "text": "example I have a NS 3 structure like this my data locks data 1 and I have a",
    "start": "1434440",
    "end": "1441190"
  },
  {
    "text": "bunch of objects in there if I want to create a table on this this structure I would create a create external table I",
    "start": "1441190",
    "end": "1447760"
  },
  {
    "text": "will give the location that maps to the prefix when I created this data all the objects",
    "start": "1447760",
    "end": "1453430"
  },
  {
    "text": "inside that prefix will get queried that means in the next minute if I added a new object and Dan then ran a query all",
    "start": "1453430",
    "end": "1461020"
  },
  {
    "text": "the new objects will also get queried so all the data inside a particular prefix gets queried also when you create a",
    "start": "1461020",
    "end": "1468340"
  },
  {
    "text": "statement like this there is an Athena data catalog which is basically very",
    "start": "1468340",
    "end": "1473350"
  },
  {
    "text": "similar to a hive meta store that stores this metadata what does it store it stores a table name it stores it stores",
    "start": "1473350",
    "end": "1480850"
  },
  {
    "text": "the location it stores what is the format of the data it stores other other things in like statistics on the data so",
    "start": "1480850",
    "end": "1487960"
  },
  {
    "text": "on and so forth and this you can share we'll talk about that a little bit more this differentiation of where the data",
    "start": "1487960",
    "end": "1493690"
  },
  {
    "text": "lives and where the metadata lives is key to thinking about how you rightly think about Athena because Athena uses",
    "start": "1493690",
    "end": "1500530"
  },
  {
    "start": "1499000",
    "end": "1587000"
  },
  {
    "text": "schema on read versus schema on right so in a traditional database you first define your schema then you ETL the",
    "start": "1500530",
    "end": "1507460"
  },
  {
    "text": "schema and you load the data into the schema and by the process of that you prepared the data for the queries to",
    "start": "1507460",
    "end": "1513310"
  },
  {
    "text": "become really really fast you also route out any inefficiencies in the data for",
    "start": "1513310",
    "end": "1518380"
  },
  {
    "text": "example if you have an invalid column or if you have a column that is null and you don't expect it to be now then you",
    "start": "1518380",
    "end": "1523510"
  },
  {
    "text": "route it out because you're loading the data into a database or into a data warehouse when you're doing schema and",
    "start": "1523510",
    "end": "1528910"
  },
  {
    "text": "writes that has its own advantages Athena doesn't do that at Lima Athena does schema and dream because it is",
    "start": "1528910",
    "end": "1535570"
  },
  {
    "text": "that's why it's really fast that you can take keep your data in s3 just create a schema on the data and the schema is",
    "start": "1535570",
    "end": "1542500"
  },
  {
    "text": "applied to the data when you are actually running the query so for example if I say that this table has is",
    "start": "1542500",
    "end": "1549850"
  },
  {
    "text": "CSV file and I have I want to process each row as a regular expression when I",
    "start": "1549850",
    "end": "1555910"
  },
  {
    "text": "do a create table statement nothing changes no data is moved no data is touched it's basically just the metadata",
    "start": "1555910",
    "end": "1561970"
  },
  {
    "text": "is created now when I run a query that means in that query every regex",
    "start": "1561970",
    "end": "1567100"
  },
  {
    "text": "expression is run through every line of Athena and evaluated at once so the",
    "start": "1567100",
    "end": "1572140"
  },
  {
    "text": "schema is applied on the data both habits advantages and disadvantages I think scheming read is really good for",
    "start": "1572140",
    "end": "1578170"
  },
  {
    "text": "exploration and for experimentation and for quickly getting your data in our you know",
    "start": "1578170",
    "end": "1584849"
  },
  {
    "text": "quickly creating tables in your data these are various sets of formats and",
    "start": "1584849",
    "end": "1590969"
  },
  {
    "start": "1587000",
    "end": "1684000"
  },
  {
    "text": "surveys that we support Assad a C stands for as he realized the DC Eliezer which basically tells Athena what is the kind",
    "start": "1590969",
    "end": "1597989"
  },
  {
    "text": "of underlying data that we have so regex crock is basically a regex pattern that",
    "start": "1597989",
    "end": "1603629"
  },
  {
    "text": "can be named so there are about 200 to 300 different growth patterns that you",
    "start": "1603629",
    "end": "1608669"
  },
  {
    "text": "can find which have which have predefined patterns for specific kind of files so but they basically rejects",
    "start": "1608669",
    "end": "1616139"
  },
  {
    "text": "patterns but they're named regex patterns you can do Joyce on you can do open csv CSV and we also do columnar",
    "start": "1616139",
    "end": "1622950"
  },
  {
    "text": "formats like parquet and oversee Athena is much faster when you're using columnar formats like or C in parking we",
    "start": "1622950",
    "end": "1631379"
  },
  {
    "text": "also have data that is a bru and geospatial we also support a bunch of compression formats I'll leave this for",
    "start": "1631379",
    "end": "1637079"
  },
  {
    "text": "your reference it's also in our Doc's as to which compression format actually gives you the best bang for the buck now",
    "start": "1637079",
    "end": "1642869"
  },
  {
    "text": "if you do compress your data because Athena charges you by the amount of data scan your costs on Athena go down that",
    "start": "1642869",
    "end": "1649979"
  },
  {
    "text": "means even if I have to decompress the data to process the query I will only charge you for the compressed data that",
    "start": "1649979",
    "end": "1657119"
  },
  {
    "text": "is sitting on s3 so if I have a terabyte of data in in text files you converted this into park' that probably became",
    "start": "1657119",
    "end": "1663749"
  },
  {
    "text": "about 110 220 gigs when you run a query on this data I will only charge you for",
    "start": "1663749",
    "end": "1669119"
  },
  {
    "text": "what the compressed data even though I have decompressed to run the query so we",
    "start": "1669119",
    "end": "1674579"
  },
  {
    "text": "are encouraging you to compress the data and put it on is 3 or to compress the data because when I scan less data from",
    "start": "1674579",
    "end": "1681149"
  },
  {
    "text": "s3 I become faster ok let's talk a little bit about the blue data catalog",
    "start": "1681149",
    "end": "1687359"
  },
  {
    "start": "1684000",
    "end": "1871000"
  },
  {
    "text": "so we talked about Athena uses an internal catalog called which is called",
    "start": "1687359",
    "end": "1693329"
  },
  {
    "text": "the data catalog this catalog is the same catalog as the glue data catalog but with a bunch of features that we",
    "start": "1693329",
    "end": "1700079"
  },
  {
    "text": "have added to it and I'll talk about how do you upgrade from an Athena based catalog to a glue based data catalog so",
    "start": "1700079",
    "end": "1707070"
  },
  {
    "text": "the catalog is a single view of your metadata it allows you to classify data it allows you to search through your",
    "start": "1707070",
    "end": "1712799"
  },
  {
    "text": "metadata and it also does things like scheme versioning it is a hive meta store",
    "start": "1712799",
    "end": "1718800"
  },
  {
    "text": "compatible data store that means you can hook it up to things like an Amazon EMR cluster running hive on top of this data",
    "start": "1718800",
    "end": "1726530"
  },
  {
    "text": "again my favorite diagram is here that the purpose of showing you this diagram",
    "start": "1726530",
    "end": "1731880"
  },
  {
    "text": "as the the what we think is the glue data catalog is going to become the",
    "start": "1731880",
    "end": "1737220"
  },
  {
    "text": "central metadata repository for all the data that sits on s3 and outside so the",
    "start": "1737220",
    "end": "1743040"
  },
  {
    "text": "glue data catalog can crawl data from any JDBC source like redshift or RDS or",
    "start": "1743040",
    "end": "1749130"
  },
  {
    "text": "anything that is JDBC compliant and on s3 so it becomes a central metadata store for all your data and then we have",
    "start": "1749130",
    "end": "1756179"
  },
  {
    "text": "a bunch of downstream engines that are hooked up to the to the data catalog that allow you to query the data so as I",
    "start": "1756179",
    "end": "1763020"
  },
  {
    "text": "said everybody most of our customers agree that we want to use s3 or data Lake they disagree as to how they want",
    "start": "1763020",
    "end": "1769380"
  },
  {
    "text": "to process the data and this notion of a data catalog kind of unifies that view",
    "start": "1769380",
    "end": "1775040"
  },
  {
    "text": "so here's a view of the data catalog you would see that you know it has recognized a bunch of a bunch of files",
    "start": "1775040",
    "end": "1781290"
  },
  {
    "text": "from red shift from sequel CSV Park a JSON so on and so forth so how do you",
    "start": "1781290",
    "end": "1787679"
  },
  {
    "text": "populate your data catalog you can populate your data catalog by a bunch of using crawlers or by directly using DDL",
    "start": "1787679",
    "end": "1794490"
  },
  {
    "text": "statement so on and so forth when you pop when crawlers populate a table they also give you a lot of information for",
    "start": "1794490",
    "end": "1800970"
  },
  {
    "text": "example they give you table properties that you can extend with custom information for example you can add",
    "start": "1800970",
    "end": "1806940"
  },
  {
    "text": "business annotation or business information about these table properties they also give you statistics for",
    "start": "1806940",
    "end": "1812490"
  },
  {
    "text": "example how many objects did I crawl so on and so forth and as we progress the data catalog we will keep building more",
    "start": "1812490",
    "end": "1818490"
  },
  {
    "text": "information on top of this they also automatically infer schemas when you run",
    "start": "1818490",
    "end": "1824130"
  },
  {
    "text": "a crawler on the data we can automatically infer the schema so for example in this case we had a nested",
    "start": "1824130",
    "end": "1829800"
  },
  {
    "text": "structure which was a complex structure using the data type struct and we were able to infer these schemas as well",
    "start": "1829800",
    "end": "1836670"
  },
  {
    "text": "we also automatically register partitions the reason I say partitioning",
    "start": "1836670",
    "end": "1842880"
  },
  {
    "text": "a lot because but down stream engines like Athena are able to reduce the amount of data you scan by using partitioning so if",
    "start": "1842880",
    "end": "1850480"
  },
  {
    "text": "partitioning is just like using using a virtual column with a ver class so most",
    "start": "1850480",
    "end": "1856029"
  },
  {
    "text": "people are not creating all the data all the time so by partitioning you can restrict your query on a smaller data",
    "start": "1856029",
    "end": "1861669"
  },
  {
    "text": "set and scan less data and then pay less and become faster so blue crawlers can",
    "start": "1861669",
    "end": "1869409"
  },
  {
    "text": "automatically do this so what grew colors can also give you",
    "start": "1869409",
    "end": "1875380"
  },
  {
    "text": "versioning on your schema so if your schema changes between day one and day two it'll automatically create a new",
    "start": "1875380",
    "end": "1881350"
  },
  {
    "text": "version of this schema and then gives you the ability to do what you need to do to deal with that mutation of the schema you can run the group called",
    "start": "1881350",
    "end": "1888340"
  },
  {
    "text": "crawler in an ad hoc manner you can run it on an event you can also schedule it so so it is a good way to keep your",
    "start": "1888340",
    "end": "1894250"
  },
  {
    "text": "catalog up to date with the data that you have on s3 so how is at the house",
    "start": "1894250",
    "end": "1900880"
  },
  {
    "start": "1899000",
    "end": "1959000"
  },
  {
    "text": "this data how is group crawler doing this so at the core of a group called crawler is something called a classifier",
    "start": "1900880",
    "end": "1905980"
  },
  {
    "text": "which is basically a bunch of growth patterns that are put in an ordered",
    "start": "1905980",
    "end": "1911679"
  },
  {
    "text": "format and what it does it when it looks at a particular data set it gives me a number a certainty number between 0 to 1 as to",
    "start": "1911679",
    "end": "1919450"
  },
  {
    "text": "how certain it is that this particular data set is exactly how I read it right",
    "start": "1919450",
    "end": "1924549"
  },
  {
    "text": "and based upon this you can also add your own custom patterns so for example",
    "start": "1924549",
    "end": "1931269"
  },
  {
    "text": "how does it do partitioning what it does is if you look at this prediction hierarchy I have a month in date that",
    "start": "1931269",
    "end": "1937419"
  },
  {
    "text": "means I partitioned by month and date so I have date 10 date is 15 I can see that the schema on the files on date 10 is",
    "start": "1937419",
    "end": "1944769"
  },
  {
    "text": "this is the same as the schema on the file that is date 15 and then I ask based upon estimation on these",
    "start": "1944769",
    "end": "1951159"
  },
  {
    "text": "classifiers I can did I can say or the crew product can say that this process of Lee is a petition and that's how he",
    "start": "1951159",
    "end": "1957370"
  },
  {
    "text": "loads up the petition you can also write your own ordered set of classifiers as",
    "start": "1957370",
    "end": "1962500"
  },
  {
    "start": "1959000",
    "end": "2102000"
  },
  {
    "text": "growth patterns to tell Gloup Gloup crawlers as how do I want to crawl my",
    "start": "1962500",
    "end": "1967600"
  },
  {
    "text": "data or what do I want to expect when I crawl my data if you have some different data set now this whole idea of is the",
    "start": "1967600",
    "end": "1976269"
  },
  {
    "text": "catalog with Athena or is the catalog with glue can become quite confusing the reason is because when Athena launched the glue",
    "start": "1976269",
    "end": "1983190"
  },
  {
    "text": "catalog was only internal to Athena that means you can only access it with Athena",
    "start": "1983190",
    "end": "1988920"
  },
  {
    "text": "by wire Athena so all the permissioning model did the blue catalog y was via Athena as we are",
    "start": "1988920",
    "end": "1996660"
  },
  {
    "text": "expanding glue to new regions and building all these features and glue we are also you have an option to upgrade",
    "start": "1996660",
    "end": "2003470"
  },
  {
    "text": "from the Athena catalog to the glue catalog there is no data that moves there is no metadata that moves the only",
    "start": "2003470",
    "end": "2010370"
  },
  {
    "text": "thing it changes is with the glue data catalog it becomes a public API and Athena stops authenticating any requests",
    "start": "2010370",
    "end": "2017630"
  },
  {
    "text": "for it all the authentication or the commissioning model for glue catalog then becomes a blue you know glue",
    "start": "2017630",
    "end": "2025670"
  },
  {
    "text": "permissioning model so with the Tina and glue if you have upgraded to an glue data catalog you would have to have both",
    "start": "2025670",
    "end": "2032420"
  },
  {
    "text": "permissions Athena permissions and glue permissions it gives you two levels of permissioning model on top of Athena so",
    "start": "2032420",
    "end": "2038360"
  },
  {
    "text": "a good way to think about this is here was the world when I was on an Athena and an internal data catalog the data",
    "start": "2038360",
    "end": "2044930"
  },
  {
    "text": "catalog authorization was inside the purview of Athena when I migrated it",
    "start": "2044930",
    "end": "2050210"
  },
  {
    "text": "becomes outside and it an Athena becomes sorry the catalog becomes an external service that you can call directly by an",
    "start": "2050210",
    "end": "2056780"
  },
  {
    "text": "API that's when you can start hooking it up to things like EMR retro spectrum and",
    "start": "2056780",
    "end": "2063440"
  },
  {
    "text": "other data sources as well so how do you upgrade so upgrade is a very simple process it's really not changing",
    "start": "2063440",
    "end": "2070360"
  },
  {
    "text": "anything on the data or the metadata it's basically saying that I will stop I",
    "start": "2070360",
    "end": "2077450"
  },
  {
    "text": "can't have two authentication mechanisms to the same data set or the same metadata so it's basically saying I will",
    "start": "2077450",
    "end": "2083690"
  },
  {
    "text": "stop from now onwards I will stop honoring any glue based access from Athena and you must add glue policies to",
    "start": "2083690",
    "end": "2091310"
  },
  {
    "text": "your Athena policies to keep accessing this data so before you upgrade you essentially have to modify your policies",
    "start": "2091310",
    "end": "2098120"
  },
  {
    "text": "to add your new policies and you will be able to do that you'll be able to use glue so now we have a sent one central",
    "start": "2098120",
    "end": "2106970"
  },
  {
    "start": "2102000",
    "end": "2165000"
  },
  {
    "text": "metadata stored per account that means you don't want any I am user",
    "start": "2106970",
    "end": "2112520"
  },
  {
    "text": "to have the ability to just upgrade your daya catalog to the glute kit catalog so",
    "start": "2112520",
    "end": "2117890"
  },
  {
    "text": "we built a new API called glue import catalog to glue and you basically need",
    "start": "2117890",
    "end": "2122990"
  },
  {
    "text": "permissions to this API to be able to upgrade what that me what that does is it intentionally allows an admin to go",
    "start": "2122990",
    "end": "2130280"
  },
  {
    "text": "and upgrade it because once you upgrade from the glue data catalog to the thenar data catalog you upgrade it for an",
    "start": "2130280",
    "end": "2135770"
  },
  {
    "text": "entire account so for example if I have queries that were running on Athena and it had no glue permissions and suddenly",
    "start": "2135770",
    "end": "2142700"
  },
  {
    "text": "somebody went and upgraded the Athena data catalog to the glue data catalog exactly the same thing no change in data",
    "start": "2142700",
    "end": "2148760"
  },
  {
    "text": "no change in internal that means my queries are Tina will stop authenticating that means that Tina",
    "start": "2148760",
    "end": "2154340"
  },
  {
    "text": "queries will fail so the way to do it is go and add glue policies to your Athena",
    "start": "2154340",
    "end": "2159410"
  },
  {
    "text": "policies go create a permissioning model for this API and then upgrade to the glue catalog lots of benefits of",
    "start": "2159410",
    "end": "2167090"
  },
  {
    "start": "2165000",
    "end": "2187000"
  },
  {
    "text": "upgrading you can use crawlers you can share metadata between EMR and Athena",
    "start": "2167090",
    "end": "2172640"
  },
  {
    "text": "and hive spark and presto that is running on an EMR you can also run then",
    "start": "2172640",
    "end": "2178760"
  },
  {
    "text": "the glue API directly if you want to run high concurrency DDL statements so instead of doing all the table add",
    "start": "2178760",
    "end": "2185030"
  },
  {
    "text": "partition you can just do partitioning so let's talk a little bit about partitioning your area",
    "start": "2185030",
    "end": "2190580"
  },
  {
    "start": "2187000",
    "end": "2309000"
  },
  {
    "text": "so what partitioning does is petitioning allows you to read only the data that",
    "start": "2190580",
    "end": "2197120"
  },
  {
    "text": "your query needs essentially partitioning is a virtual column you define partitioning at the time of",
    "start": "2197120",
    "end": "2202760"
  },
  {
    "text": "defining or defining a table and then you in your ver class you will be able",
    "start": "2202760",
    "end": "2208250"
  },
  {
    "text": "to use different filters and based upon different filters I might not read all the data I might read only a small chunk",
    "start": "2208250",
    "end": "2215180"
  },
  {
    "text": "of this data you define partitions at the time of creating the tables Athena uses high base partitioning but",
    "start": "2215180",
    "end": "2221810"
  },
  {
    "text": "especially if your data is text-based that means text is not really a good format for querying data for text-based",
    "start": "2221810",
    "end": "2227630"
  },
  {
    "text": "data I have to read the entire data set to be able to go to a particular column that's not the case with columnar data",
    "start": "2227630",
    "end": "2233300"
  },
  {
    "text": "formats like oversee and parquet so if you're really using text-based data then partitioning gives you a lot of really",
    "start": "2233300",
    "end": "2239840"
  },
  {
    "text": "good advantages and I'll show you some examples so here's a here's an example of the same query it's about on 74",
    "start": "2239840",
    "end": "2245829"
  },
  {
    "text": "of data so on the left-hand side I have a query the first query is just giving a",
    "start": "2245829",
    "end": "2251469"
  },
  {
    "text": "ship date or just asking me information about a particular ship the ship date it",
    "start": "2251469",
    "end": "2256539"
  },
  {
    "text": "takes me about nine seconds because on the non partition table because the table is not partitioned and it scans",
    "start": "2256539",
    "end": "2263529"
  },
  {
    "text": "the entire table when I partition the table it is you know several order of magnitude cheap cheaper because what",
    "start": "2263529",
    "end": "2269799"
  },
  {
    "text": "then I do is I don't look at anything else I would just look at that ship date and go and process data in that ship",
    "start": "2269799",
    "end": "2275979"
  },
  {
    "text": "date the second example is the range range of partitions so I want to get",
    "start": "2275979",
    "end": "2281289"
  },
  {
    "text": "data between these three partitions or and you can see that in the first case it takes me about ten seconds on an on",
    "start": "2281289",
    "end": "2287559"
  },
  {
    "text": "partition table but it takes me really really less 2.73 and you can see that",
    "start": "2287559",
    "end": "2292989"
  },
  {
    "text": "the data scan in the first one I am scanning the entire table seventy four GB of data in the second one I'm",
    "start": "2292989",
    "end": "2298599"
  },
  {
    "text": "scanning some 800 max of data so it is infinitely faster to partition the data and partitioning of data only means how",
    "start": "2298599",
    "end": "2305769"
  },
  {
    "text": "you layout your data and s3 so let's talk a little bit about that also partitioning has a little bit of an",
    "start": "2305769",
    "end": "2311469"
  },
  {
    "start": "2309000",
    "end": "2325000"
  },
  {
    "text": "overhead that means if you if you're always going to scan your entire table probably petitioning is not a great use",
    "start": "2311469",
    "end": "2317709"
  },
  {
    "text": "case for you because I do spend some time fetching all the partitions and then so and finding out the partition",
    "start": "2317709",
    "end": "2323079"
  },
  {
    "text": "that you need to go to right so I talked about partitioning as a way that that",
    "start": "2323079",
    "end": "2329289"
  },
  {
    "start": "2325000",
    "end": "2422000"
  },
  {
    "text": "petitioning is defined at the time of defining a table so in this case imagine I have a table and I want to partition",
    "start": "2329289",
    "end": "2335589"
  },
  {
    "text": "it by two things I want to partition by year and month the way I want to use this the way I would lay out my data on",
    "start": "2335589",
    "end": "2342219"
  },
  {
    "text": "s3 is this is the path to my basic table this is my partition column one this is",
    "start": "2342219",
    "end": "2348339"
  },
  {
    "text": "my partition column two and this is how my data possibly there are other variations that I will talk about but",
    "start": "2348339",
    "end": "2354729"
  },
  {
    "text": "this is how my data should be laid out in s3 so now when you actually use the sequel query and say where is year is",
    "start": "2354729",
    "end": "2362739"
  },
  {
    "text": "equal to 2015 month is equal to eleven it has a unique s3 location associated",
    "start": "2362739",
    "end": "2369579"
  },
  {
    "text": "with that tupple and it only goes into that s3 location and and queries the",
    "start": "2369579",
    "end": "2375130"
  },
  {
    "text": "files that are in that s3 location that's how we are able to scan much less data",
    "start": "2375130",
    "end": "2380170"
  },
  {
    "text": "that's why in previous diagrams you see a lot of people using lambda functions because what they're trying to do is",
    "start": "2380170",
    "end": "2385900"
  },
  {
    "text": "data comes in in one prefix they're trying to take a lambda function and put it in a partition or format so that when",
    "start": "2385900",
    "end": "2391480"
  },
  {
    "text": "they have to query one day's worth of data they're not creating an year's worth of data so here's an here is an",
    "start": "2391480",
    "end": "2398320"
  },
  {
    "text": "example of how this might exactly look like s3 your bucket your table name here",
    "start": "2398320",
    "end": "2403570"
  },
  {
    "text": "is 2017 month is 11 now before you can use partition you also have to tell the",
    "start": "2403570",
    "end": "2409000"
  },
  {
    "text": "data catalog where is this partition so first you have to define that you want the partition which is in your create",
    "start": "2409000",
    "end": "2414730"
  },
  {
    "text": "table statement you have to layout the data in s in s3 you have to also tell me using the DDL statement that where is",
    "start": "2414730",
    "end": "2422080"
  },
  {
    "start": "2422000",
    "end": "2513000"
  },
  {
    "text": "this partition the way I do it is using an alter table command so I can do something like alter table",
    "start": "2422080",
    "end": "2428440"
  },
  {
    "text": "access log which is the name of my table add partition and then I give each partition a value and then associate",
    "start": "2428440",
    "end": "2435220"
  },
  {
    "text": "that value with an s3 path now it doesn't need to be two levels of partitioning you can have any level of",
    "start": "2435220",
    "end": "2441460"
  },
  {
    "text": "partitioning that you want single level six levels but essentially what it is doing is with that unique keys or",
    "start": "2441460",
    "end": "2448090"
  },
  {
    "text": "combination of keys it is associating an s3 namespace or an s3 path to that keys",
    "start": "2448090",
    "end": "2454150"
  },
  {
    "text": "so when I use in a query give me so and so we're here is 2017 and month is 11 it",
    "start": "2454150",
    "end": "2460140"
  },
  {
    "text": "understands that this query is only going to that petition and it goes and scans only that partition right you can",
    "start": "2460140",
    "end": "2467890"
  },
  {
    "text": "do that directly now you can do this about the table indirectly with the blue API you can also do it using an alter",
    "start": "2467890",
    "end": "2474820"
  },
  {
    "text": "table command I'll talk a little bit about this but what if my data is like this right 20 17 11 it doesn't have how",
    "start": "2474820",
    "end": "2483460"
  },
  {
    "text": "do I go back yeah it doesn't have your is equal to 2017 and month is equal to",
    "start": "2483460",
    "end": "2488770"
  },
  {
    "text": "11 what if it doesn't have this that doesn't matter in fact you can still the key or what is",
    "start": "2488770",
    "end": "2496210"
  },
  {
    "text": "the value that you're gonna reference the date of it you are still specifying that key it's the same way as remember",
    "start": "2496210",
    "end": "2503790"
  },
  {
    "text": "my pointer here we go so it's the same way here is 2017 month is 11 and it goes",
    "start": "2503790",
    "end": "2509260"
  },
  {
    "text": "through a unique s3 location it doesn't matter what the location is called in fact if you look at the last so there's also",
    "start": "2509260",
    "end": "2516880"
  },
  {
    "start": "2513000",
    "end": "2556000"
  },
  {
    "text": "command called MSC key repair table so if your data is actually in this format which a lot of people who have used hi",
    "start": "2516880",
    "end": "2522850"
  },
  {
    "text": "before that means it has its key and a value on the s3 path you can just use",
    "start": "2522850",
    "end": "2528280"
  },
  {
    "text": "this command called MSC key repair table and it'll automatically load the audio partitions this is a pretty expensive",
    "start": "2528280",
    "end": "2533740"
  },
  {
    "text": "command because it actually goes through all your s3 namespace and validates all your partitions so if you're just",
    "start": "2533740",
    "end": "2539050"
  },
  {
    "text": "loading one partition don't use this because it will scan all your partitions all the time right so but if your data",
    "start": "2539050",
    "end": "2545830"
  },
  {
    "text": "is in this format and you are like if you have a lot of hive data that is sitting on s3 and you want to immediately load all of that data into",
    "start": "2545830",
    "end": "2551980"
  },
  {
    "text": "Athena you can do an MS Akira pair table and all the partitions will be loaded the second is if I have data in this",
    "start": "2551980",
    "end": "2558250"
  },
  {
    "start": "2556000",
    "end": "2610000"
  },
  {
    "text": "format and I can do all the table add partition I can give it the values and an access location I can do about 99 of",
    "start": "2558250",
    "end": "2565900"
  },
  {
    "text": "this in the same alter table command and what it means is you can actually do it",
    "start": "2565900",
    "end": "2570970"
  },
  {
    "text": "anytime you want the data doesn't need to be present in that s3 namespace it is basically associating a key value a",
    "start": "2570970",
    "end": "2578580"
  },
  {
    "text": "unique set of keys to an s3 location so for example I can even do this right so",
    "start": "2578580",
    "end": "2583810"
  },
  {
    "text": "far let's say my bucket because a lot of people who tell me that look if I use these namespaces and these names in my",
    "start": "2583810",
    "end": "2589630"
  },
  {
    "text": "buckets then it doesn't follow s3 is best practices which are to introduce entropy in your prefix name or in your",
    "start": "2589630",
    "end": "2596380"
  },
  {
    "text": "prefixes and object names so for example if you use a bucket like this I do exactly the same thing all the table",
    "start": "2596380",
    "end": "2602530"
  },
  {
    "text": "here is 2017 month is this and path the table and this this should also this",
    "start": "2602530",
    "end": "2607870"
  },
  {
    "text": "this will also work this is all you need to know about partitioning now when you",
    "start": "2607870",
    "end": "2614290"
  },
  {
    "start": "2610000",
    "end": "2636000"
  },
  {
    "text": "do when you there are many ways of loading of loading partitions you can use lambda you can use MS CK you can use",
    "start": "2614290",
    "end": "2620410"
  },
  {
    "text": "alter table you can do over 99 of them in the same thing as I said a lot of people to do that add the daily",
    "start": "2620410",
    "end": "2626110"
  },
  {
    "text": "partitions they would go and do it at this at the start of the day you can also schedule a crawler to run into",
    "start": "2626110",
    "end": "2631210"
  },
  {
    "text": "automatically or you can directly call the blue API using an SDK of your choice to do it to do something now the way you",
    "start": "2631210",
    "end": "2638770"
  },
  {
    "start": "2636000",
    "end": "2713000"
  },
  {
    "text": "choose your partitions was also important write petitions that you are nested that means you can use one or",
    "start": "2638770",
    "end": "2644440"
  },
  {
    "text": "multiple columns as partitions so that means you can use stead of using a date you can use ear month day and then you might have a",
    "start": "2644440",
    "end": "2651170"
  },
  {
    "text": "customer ID but you want to choose your partitions in such a way that your petitions are roughly balanced and each",
    "start": "2651170",
    "end": "2656450"
  },
  {
    "text": "one of them has roughly an order of 128 Meg's of data when you look at all the",
    "start": "2656450",
    "end": "2662840"
  },
  {
    "text": "partitions because beyond that you are spending a lot more time actually getting and fetching the partition then",
    "start": "2662840",
    "end": "2668030"
  },
  {
    "text": "but then actually going and running a process right so look at your query patterns and define a partitioning but",
    "start": "2668030",
    "end": "2675800"
  },
  {
    "text": "by in a way that you think that if you're running a query on all your on on your data for an entire day every day",
    "start": "2675800",
    "end": "2682700"
  },
  {
    "text": "then a date based partitioning makes sense if you're running your partitions across a customer that for this customer",
    "start": "2682700",
    "end": "2689210"
  },
  {
    "text": "I need to run a query every day then you probably make sense to do your partitions again remember you can change",
    "start": "2689210",
    "end": "2696290"
  },
  {
    "text": "this a lot of our customers do change partitions as well because essentially remember metadata this is all that it's",
    "start": "2696290",
    "end": "2702560"
  },
  {
    "text": "doing is there is a metadata string that says if you give me a rare class with",
    "start": "2702560",
    "end": "2707810"
  },
  {
    "text": "these three partition keys I will go to this particular location so you can change that as well so let's look at a",
    "start": "2707810",
    "end": "2714620"
  },
  {
    "start": "2713000",
    "end": "2757000"
  },
  {
    "text": "couple of other things when you run when you run queries on Athena and how can you improve performance first of all Athena doesn't like small",
    "start": "2714620",
    "end": "2722660"
  },
  {
    "text": "files I think most big data systems doesn't like you don't like small fights it's a reality of our life that most data is produced in small files that's",
    "start": "2722660",
    "end": "2729350"
  },
  {
    "text": "true right so it's very easy for me to stand up here and say that and don't use small files but I understand I've gone",
    "start": "2729350",
    "end": "2735800"
  },
  {
    "text": "through the pain of hearing you tell me that you know it my file comes in KB's because I have an IOT device there that",
    "start": "2735800",
    "end": "2742340"
  },
  {
    "text": "sends data using firehose and dumps it in s3 but here is an example of the same",
    "start": "2742340",
    "end": "2748670"
  },
  {
    "text": "data set on the top it is split across five thousand files and at the bottom is just one file you can see how different",
    "start": "2748670",
    "end": "2755690"
  },
  {
    "text": "the performance looks what people do what we have seen people do is people use glue to aggregate the data and",
    "start": "2755690",
    "end": "2762860"
  },
  {
    "start": "2757000",
    "end": "2774000"
  },
  {
    "text": "create hourly aggregations if that files are really small so concatenate your",
    "start": "2762860",
    "end": "2769340"
  },
  {
    "text": "data and expose that data set on top of Athena and your query will be much faster when you convert your data into",
    "start": "2769340",
    "end": "2776720"
  },
  {
    "text": "oversee and and park' formats in this example we only using perky and perky is a columnar",
    "start": "2776720",
    "end": "2782440"
  },
  {
    "text": "format we run much faster we run much faster because the data is compressed and with our C and M / K which are",
    "start": "2782440",
    "end": "2789670"
  },
  {
    "text": "columnar format not only can I select only the columns that you reference in the query I can also go and get much",
    "start": "2789670",
    "end": "2797440"
  },
  {
    "text": "better at finding the data that you want so for example the same data set the 1.1 terabyte in 1.15 terabyte in text when I",
    "start": "2797440",
    "end": "2805540"
  },
  {
    "text": "convert into parquet using the standard compression that Parker uses gets me about 130 GB so lower s3 footprint as",
    "start": "2805540",
    "end": "2812079"
  },
  {
    "text": "well and if I ran a query on the entire data set the one that took 236 seconds and costed me about $5 takes about 6",
    "start": "2812079",
    "end": "2820900"
  },
  {
    "text": "seconds on parquet and cost me you know multiple fractions lower order by",
    "start": "2820900",
    "end": "2829359"
  },
  {
    "start": "2828000",
    "end": "2887000"
  },
  {
    "text": "clauses if you are using order by clauses a limit clause and the order by clauses will significantly improve your",
    "start": "2829359",
    "end": "2835000"
  },
  {
    "text": "performance most of the times we see customers using order buy and they're only looking for top 10 so if you add a",
    "start": "2835000",
    "end": "2840819"
  },
  {
    "text": "limit to the order by clause I think it will be significantly faster joints there's one trick to doing joints which",
    "start": "2840819",
    "end": "2848349"
  },
  {
    "text": "is if you keep your larger table on the left-hand side of the joint such as the example that is given here so on the",
    "start": "2848349",
    "end": "2855310"
  },
  {
    "text": "part table is actually the bigger table and the line item is actually the smaller table if you just reorder those",
    "start": "2855310",
    "end": "2861190"
  },
  {
    "text": "things from the left to the right your query start to become much faster also",
    "start": "2861190",
    "end": "2867099"
  },
  {
    "text": "because people process a lot of JSON based data there's also a lot of like",
    "start": "2867099",
    "end": "2872770"
  },
  {
    "text": "like like operators people use you can also you regex based operators are much better than like based operators you",
    "start": "2872770",
    "end": "2878980"
  },
  {
    "text": "will see some 20 to 25 percent performance improvement on those all of these are documented on a blog right so",
    "start": "2878980",
    "end": "2885040"
  },
  {
    "text": "you can you can you can look through that last a bunch of things that are coming soon that we are launching in the",
    "start": "2885040",
    "end": "2890829"
  },
  {
    "start": "2887000",
    "end": "2979000"
  },
  {
    "text": "next couple of months so if you if you run into a query resources exhausted",
    "start": "2890829",
    "end": "2896589"
  },
  {
    "text": "error we're basically solving the problem in the next couple of next couple of months",
    "start": "2896589",
    "end": "2902500"
  },
  {
    "text": "what it does what is doing is today presto doesn't have an ability to spill",
    "start": "2902500",
    "end": "2908710"
  },
  {
    "text": "to disk on large aggregations it does all the time and I in memory and with with Athena is is",
    "start": "2908710",
    "end": "2914100"
  },
  {
    "text": "able to just obtuse with Athena we're adding functionality that we'll be able to nurse bill to this so you'll be able",
    "start": "2914100",
    "end": "2919980"
  },
  {
    "text": "to do aggregations on really large data sets as well we are also adding a new optimized power key reader that improves",
    "start": "2919980",
    "end": "2926700"
  },
  {
    "text": "performance especially with complex data types we are adding a lot more on simplified error messaging so if you",
    "start": "2926700",
    "end": "2933390"
  },
  {
    "text": "have if you have used Athena and you've gotten an internal error we are doing",
    "start": "2933390",
    "end": "2938970"
  },
  {
    "text": "lot better with those error messaging we will tell you exactly what there are is and what is the remedial action we",
    "start": "2938970",
    "end": "2944730"
  },
  {
    "text": "choose we chose not to add logs to Athena because essentially what we want to do is give you a remedial action of",
    "start": "2944730",
    "end": "2950220"
  },
  {
    "text": "what you can do with Athena also we wanted to keep and we only a single",
    "start": "2950220",
    "end": "2955980"
  },
  {
    "text": "version of Athena so that you don't have to upgrade from version X to version Y of Athena so all upgrades on Athena and",
    "start": "2955980",
    "end": "2962730"
  },
  {
    "text": "transparent we are very soon upgrading to a version of presto that is based on version 172 but you won't need to change",
    "start": "2962730",
    "end": "2969120"
  },
  {
    "text": "anything your queries just run exactly the same they run faster but there is no",
    "start": "2969120",
    "end": "2974340"
  },
  {
    "text": "upgrade requirement for you that all the upgrade happens automatically with this",
    "start": "2974340",
    "end": "2979680"
  },
  {
    "start": "2979000",
    "end": "2998000"
  },
  {
    "text": "I like to invite a role and role to help us talk through the oil XK star",
    "start": "2979680",
    "end": "2985970"
  },
  {
    "text": "can you hear me can you hear me okay",
    "start": "2990480",
    "end": "2999329"
  },
  {
    "start": "2998000",
    "end": "3600000"
  },
  {
    "text": "nice to meet you all my name is Rob Renteria I had the big data team at",
    "start": "2999329",
    "end": "3004789"
  },
  {
    "text": "Oleg's Brazil I will explain give you a short overview of our experience with",
    "start": "3004789",
    "end": "3010819"
  },
  {
    "text": "Athena throughout I guess last the last year when we implement it so I will do",
    "start": "3010819",
    "end": "3017569"
  },
  {
    "text": "that in three three fold so first I will give you context why we use it",
    "start": "3017569",
    "end": "3023539"
  },
  {
    "text": "second how we did it how implement that stuff and finally I would like to give you some advice some stuff that we",
    "start": "3023539",
    "end": "3030200"
  },
  {
    "text": "learned over since January when we implement it okay first to give you some context who",
    "start": "3030200",
    "end": "3036470"
  },
  {
    "text": "you are we are the biggest online marketplace in Brazil every day millions",
    "start": "3036470",
    "end": "3042019"
  },
  {
    "text": "of Brazilians they come to a platform and they go there to buy or sell they used from pencils to apartments so we",
    "start": "3042019",
    "end": "3050269"
  },
  {
    "text": "are located in region here as you can see in that beautiful picture we are near the Bay Area there and we are part",
    "start": "3050269",
    "end": "3058039"
  },
  {
    "text": "of two world leaders in online marketplace namely shifted and Naspers",
    "start": "3058039",
    "end": "3063220"
  },
  {
    "text": "to give you a perspective the size of the vision and how it's growing just",
    "start": "3063220",
    "end": "3068599"
  },
  {
    "text": "some numbers every month we have 700,000",
    "start": "3068599",
    "end": "3074349"
  },
  {
    "text": "people coming in and publishing content for their first time and to give you an",
    "start": "3074349",
    "end": "3079910"
  },
  {
    "text": "idea also last year 2016 we had one thief of all the cars sales through the",
    "start": "3079910",
    "end": "3086539"
  },
  {
    "text": "platform in Brazil right continuing on the on the contact side so back December",
    "start": "3086539",
    "end": "3094579"
  },
  {
    "text": "last year where we were on the data platform that we had as you can see we have the acquisition of the different",
    "start": "3094579",
    "end": "3102019"
  },
  {
    "text": "data sources on the left on the bottom you have the data warehouse where most",
    "start": "3102019",
    "end": "3107089"
  },
  {
    "text": "of the load was at that time and still is so many visualization many Edoras lot",
    "start": "3107089",
    "end": "3113509"
  },
  {
    "text": "of exploration going on there from all different departments of the company right it's not about only there",
    "start": "3113509",
    "end": "3119280"
  },
  {
    "text": "Engineering and product on the top you see that we have our data like that Ella",
    "start": "3119280",
    "end": "3125160"
  },
  {
    "text": "Lake at that time was being used mostly by the data scientists and there's more",
    "start": "3125160",
    "end": "3130380"
  },
  {
    "text": "group of data analysts to give you an idea of the law that we have at that time we had ingested everyday point two",
    "start": "3130380",
    "end": "3138870"
  },
  {
    "text": "terabytes and every day 50 terabytes consumed as I explained to you we served",
    "start": "3138870",
    "end": "3146520"
  },
  {
    "text": "the whole company every type of use in the company that's something that we want to push and we keep pushing and so",
    "start": "3146520",
    "end": "3152730"
  },
  {
    "text": "different users will load and scan the data the same data and aggregate it in",
    "start": "3152730",
    "end": "3158280"
  },
  {
    "text": "different ways that's why you see that huge difference between ingestion and consumption okay",
    "start": "3158280",
    "end": "3165060"
  },
  {
    "text": "one key message in that in that architecture is that we develop we",
    "start": "3165060",
    "end": "3171270"
  },
  {
    "text": "construct something that was available for everyone in the company to use with",
    "start": "3171270",
    "end": "3176490"
  },
  {
    "text": "this different set of tools in a in aggravation in data aggregation the way",
    "start": "3176490",
    "end": "3181680"
  },
  {
    "text": "that best benefited their jobs okay that's the philosophy behind this",
    "start": "3181680",
    "end": "3186990"
  },
  {
    "text": "architecture going on on the context so to give you a good explaining the story",
    "start": "3186990",
    "end": "3194120"
  },
  {
    "text": "at that time December last year we were observing a high user demand not only",
    "start": "3194120",
    "end": "3200490"
  },
  {
    "text": "the number of users our employees were we were observing a 20% increase year-over-year and that's still the case",
    "start": "3200490",
    "end": "3206790"
  },
  {
    "text": "and we also had a clear push from the company we want every department to use",
    "start": "3206790",
    "end": "3212370"
  },
  {
    "text": "the data to help them make better decisions so data must be part of the decision-making process so that's",
    "start": "3212370",
    "end": "3218790"
  },
  {
    "text": "something that we push a lot that's also part of the goal of my department and at at the same time as you can see on the",
    "start": "3218790",
    "end": "3224940"
  },
  {
    "text": "bottom here we had some very data savvy projects in the pipeline like we were",
    "start": "3224940",
    "end": "3232470"
  },
  {
    "text": "developing a new messaging service we also were expanding the coverage of our",
    "start": "3232470",
    "end": "3239340"
  },
  {
    "text": "tracking user tracking devices and mostly we were developing some machine",
    "start": "3239340",
    "end": "3244740"
  },
  {
    "text": "learn basic user profile' mechanism so we will burn this and how content is created in our platform and also",
    "start": "3244740",
    "end": "3250290"
  },
  {
    "text": "implementing combination through collaborative filtering so what we did",
    "start": "3250290",
    "end": "3256680"
  },
  {
    "text": "so that was December last year and that was in January so one month later we",
    "start": "3256680",
    "end": "3262130"
  },
  {
    "text": "basically we did two things we removed the load on the data warehouse and we removed presto from the",
    "start": "3262130",
    "end": "3271500"
  },
  {
    "text": "picture from the detector and we put a sinner directly there on top of s3",
    "start": "3271500",
    "end": "3278099"
  },
  {
    "text": "serving not only the attackers but all the data products process that we had so",
    "start": "3278099",
    "end": "3283440"
  },
  {
    "text": "that was the move that we did in a short we put that query engine on top of the",
    "start": "3283440",
    "end": "3290790"
  },
  {
    "text": "data Lake and served everybody in the company so and going and giving you so a",
    "start": "3290790",
    "end": "3298859"
  },
  {
    "text": "bit about talking about about the timing so Athena was launched by abhishek",
    "start": "3298859",
    "end": "3305329"
  },
  {
    "text": "exactly at the same time that we were observing that huge demand and we were foreseeing a lot of demand going on",
    "start": "3305329",
    "end": "3311640"
  },
  {
    "text": "throughout the year and so we decide to take it and we implement that stuff in",
    "start": "3311640",
    "end": "3318690"
  },
  {
    "text": "in in two days that was done by January 2017 use today's given the fact that we",
    "start": "3318690",
    "end": "3326119"
  },
  {
    "text": "formatted the data inisfree using some some guidelines ok I will show you that",
    "start": "3326119",
    "end": "3332869"
  },
  {
    "text": "in a short how we implemented that so",
    "start": "3332869",
    "end": "3338579"
  },
  {
    "text": "give you a bit more details here so if you look at our data architecture we",
    "start": "3338579",
    "end": "3344220"
  },
  {
    "text": "have two clusters one which is batch oriented a spot that loads data from the",
    "start": "3344220",
    "end": "3349710"
  },
  {
    "text": "transactional and the external API and we have another cluster that handles all",
    "start": "3349710",
    "end": "3355619"
  },
  {
    "text": "the streaming based jobs ok so those two pipelines they will acquire all the data",
    "start": "3355619",
    "end": "3361410"
  },
  {
    "text": "process it ingest directly into as free and for observing performance games",
    "start": "3361410",
    "end": "3368880"
  },
  {
    "text": "downstream it was important to do what I what I show you there first we had to",
    "start": "3368880",
    "end": "3375960"
  },
  {
    "text": "put that in column form I I hope that at this point by all the stuff",
    "start": "3375960",
    "end": "3381390"
  },
  {
    "text": "I wish I'd told you you understand that is was a key thing to do second we partition the data again that",
    "start": "3381390",
    "end": "3388950"
  },
  {
    "text": "was very important we use year month and day third we use compression that is very",
    "start": "3388950",
    "end": "3394560"
  },
  {
    "text": "important to also use and fourth we after during the tuning process we found",
    "start": "3394560",
    "end": "3401730"
  },
  {
    "text": "out that it was better to keep the file size between two hundred mega megabytes and 800 that was very important to get a",
    "start": "3401730",
    "end": "3408540"
  },
  {
    "text": "good performance right and once we had this data format in this way in the data",
    "start": "3408540",
    "end": "3414840"
  },
  {
    "text": "leak we were able to plug in Athena and we did we had to do some tweaking there",
    "start": "3414840",
    "end": "3420320"
  },
  {
    "text": "we had to raise the number of concurrent queries and also the maximum memory",
    "start": "3420320",
    "end": "3427920"
  },
  {
    "text": "available for the data query that's something that we work with abhishek he helped us to make it through and also we",
    "start": "3427920",
    "end": "3435510"
  },
  {
    "text": "also had to create a job a batch job to get all these tiny smaller stream files",
    "start": "3435510",
    "end": "3441840"
  },
  {
    "text": "and put them together using the the file size constraint that i just explained to you if you look at this story this",
    "start": "3441840",
    "end": "3449520"
  },
  {
    "text": "picture it's clear that most of the job was doing preparing the data right and",
    "start": "3449520",
    "end": "3454580"
  },
  {
    "text": "plugging in Athena was pretty straightforward and what we managed to",
    "start": "3454580",
    "end": "3460740"
  },
  {
    "text": "do that so we as I told you we mentioned that in two days the quality of the serving was was good very good we",
    "start": "3460740",
    "end": "3468060"
  },
  {
    "text": "managed to keep all users going going on with the respiratory and all the ad hoc",
    "start": "3468060",
    "end": "3474480"
  },
  {
    "text": "analysis all that stuff kept going on we also kept all the data products like the",
    "start": "3474480",
    "end": "3479970"
  },
  {
    "text": "message we had set all the stuff that I told you about and also as you know as",
    "start": "3479970",
    "end": "3485160"
  },
  {
    "text": "part of any technical decision you have also to look at other alternatives so what we looked at that time so because",
    "start": "3485160",
    "end": "3491670"
  },
  {
    "text": "timing was an important thing as you know every project time is a key thing to look at so we had two available two",
    "start": "3491670",
    "end": "3499200"
  },
  {
    "text": "things that we thought about first I could we could have implemented that self ourselves we could have taken open",
    "start": "3499200",
    "end": "3504300"
  },
  {
    "text": "source like presto and put that in place but we didn't have the resource you have to expand the team and I didn't have a",
    "start": "3504300",
    "end": "3510210"
  },
  {
    "text": "resource slash time to do that as you can see on right if we look at the data ingested and",
    "start": "3510210",
    "end": "3516380"
  },
  {
    "text": "consumed by from December last year until last month October the rate the",
    "start": "3516380",
    "end": "3522530"
  },
  {
    "text": "daily rate grew by 10 times if you look",
    "start": "3522530",
    "end": "3527630"
  },
  {
    "text": "at from some shot to give you a rough idea it came from 50 terabytes to 600",
    "start": "3527630",
    "end": "3532670"
  },
  {
    "text": "terabytes in less than one year and we knew about that so that's why we need to be fast so in that way growing my team",
    "start": "3532670",
    "end": "3540500"
  },
  {
    "text": "was not an option second we could use the existing data warehouse right you",
    "start": "3540500",
    "end": "3546320"
  },
  {
    "text": "could have it scale it but that will not be very efficient we will be using the",
    "start": "3546320",
    "end": "3551810"
  },
  {
    "text": "wrong tool for the job and will cost us around 100k dollars per month so that's",
    "start": "3551810",
    "end": "3557000"
  },
  {
    "text": "why we were able to put that tool in place and keep things going smoothly so",
    "start": "3557000",
    "end": "3565040"
  },
  {
    "text": "that gives you as I told you a short overview right give you the story behind",
    "start": "3565040",
    "end": "3570770"
  },
  {
    "text": "our implementation Athena I would like to show you what happened after right",
    "start": "3570770",
    "end": "3576650"
  },
  {
    "text": "because that was done January so what as you can see on the bottom part it's",
    "start": "3576650",
    "end": "3584000"
  },
  {
    "text": "almost the same we kept the DW as it is but we just add a new layer for the",
    "start": "3584000",
    "end": "3589460"
  },
  {
    "text": "marketing folks but on the other side you can see that we brought back presto",
    "start": "3589460",
    "end": "3595580"
  },
  {
    "text": "and why was that as I told you we have this this philosophy these boys that we",
    "start": "3595580",
    "end": "3603320"
  },
  {
    "text": "want every department to be able to access the data so I talked every sales market everyone should access the data",
    "start": "3603320",
    "end": "3610250"
  },
  {
    "text": "to enhance their decision-making process and not everyone right nice SQL virus",
    "start": "3610250",
    "end": "3616610"
  },
  {
    "text": "that's the true friend it's it's part of the business and what happens that some",
    "start": "3616610",
    "end": "3621880"
  },
  {
    "text": "some group of users they start creating very non effective queries so we were we",
    "start": "3621880",
    "end": "3628880"
  },
  {
    "text": "start to see with squares that were scanning unnecessary data and as you",
    "start": "3628880",
    "end": "3634850"
  },
  {
    "text": "scan unnecessary data you create unnecessary costs and that became a",
    "start": "3634850",
    "end": "3640280"
  },
  {
    "text": "start to became go out of our control and for that reason we had to put in",
    "start": "3640280",
    "end": "3645770"
  },
  {
    "text": "place a more cost environment where we could observe and constrain the resourcing entendu",
    "start": "3645770",
    "end": "3651820"
  },
  {
    "text": "parasite and we came back with presto throughout the year and that allowed us",
    "start": "3651820",
    "end": "3657700"
  },
  {
    "text": "for some group we still use Athena or data oriented process who sues the thena for some pairing but when we know who is",
    "start": "3657700",
    "end": "3664570"
  },
  {
    "text": "doing that okay so that's that's that's an important thing that we learn throughout the road looking at the whole",
    "start": "3664570",
    "end": "3671560"
  },
  {
    "text": "thing things to consider if you're going down depending on on where you are and",
    "start": "3671560",
    "end": "3676810"
  },
  {
    "text": "that in your project first thing take a look at your data format on the street",
    "start": "3676810",
    "end": "3684490"
  },
  {
    "text": "do that from the beginning I don't think it will only help you if with Athena but",
    "start": "3684490",
    "end": "3690070"
  },
  {
    "text": "also any many other aspect of your data she textured that's the first thing",
    "start": "3690070",
    "end": "3695500"
  },
  {
    "text": "second one a very important take a look at your data size and your team size if",
    "start": "3695500",
    "end": "3700960"
  },
  {
    "text": "you still at smaller size on the data I",
    "start": "3700960",
    "end": "3706120"
  },
  {
    "text": "don't think that most of those that I told you about today will be very relevant and also if you have a small",
    "start": "3706120",
    "end": "3713860"
  },
  {
    "text": "team of people that know how to craft good SQL queries so if you know in your company it's a small team that dot that",
    "start": "3713860",
    "end": "3721360"
  },
  {
    "text": "job you won't face the issue that we had",
    "start": "3721360",
    "end": "3726730"
  },
  {
    "text": "you will be able to go easily throughout but if you are in a larger environment",
    "start": "3726730",
    "end": "3732520"
  },
  {
    "text": "where you need to provide exploratory and ad hoc analysis and reporting to a",
    "start": "3732520",
    "end": "3739120"
  },
  {
    "text": "diverse group of users then you will need a more constrained environment and",
    "start": "3739120",
    "end": "3745000"
  },
  {
    "text": "I mean by that two things on one side you will need to track how much",
    "start": "3745000",
    "end": "3751750"
  },
  {
    "text": "resources you're consuming right on the parasite and second you need also to track the quality of the query if users",
    "start": "3751750",
    "end": "3758110"
  },
  {
    "text": "are waiting too much to get a result back so we'll need an environment that's what we did with presto to watch and",
    "start": "3758110",
    "end": "3764740"
  },
  {
    "text": "track those two things and that's that's how we managed to that's how we are",
    "start": "3764740",
    "end": "3770680"
  },
  {
    "text": "today as I told you if you want to know a bit more about us you have our there",
    "start": "3770680",
    "end": "3777509"
  },
  {
    "text": "there the Lincoln also like to to thank to folks that helped to put that stuff together much hello and Hudson and",
    "start": "3777509",
    "end": "3784699"
  },
  {
    "text": "finally I'd like to give you a big thank you for your time today",
    "start": "3784699",
    "end": "3789830"
  },
  {
    "text": "[Applause]",
    "start": "3789830",
    "end": "3794760"
  }
]