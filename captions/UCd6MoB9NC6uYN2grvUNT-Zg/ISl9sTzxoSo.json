[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "good morning Oh feeling a little low this morning seems",
    "start": "30",
    "end": "5370"
  },
  {
    "text": "like understandable thank you for coming to our session my name is Abhishek Sena I'm a principal",
    "start": "5370",
    "end": "12570"
  },
  {
    "text": "product manager at AWS I manage a couple services Amazon EMR and Amazon Athena at",
    "start": "12570",
    "end": "19050"
  },
  {
    "text": "Chilton Damon courtesy is a big data architect for today's session",
    "start": "19050",
    "end": "26220"
  },
  {
    "text": "it's a deep dive in what's new at EMR so how many of you already use EMR thank",
    "start": "26220",
    "end": "32398"
  },
  {
    "text": "you thank you for your business what we so when we talk about the session we thought about a couple of",
    "start": "32399",
    "end": "37469"
  },
  {
    "text": "things that we wanted to talk to you about we've made a significant amount of releases in this year we wanted to talk",
    "start": "37469",
    "end": "43890"
  },
  {
    "text": "about that and we've also noticed a lot of clear patterns and how customers use",
    "start": "43890",
    "end": "50070"
  },
  {
    "text": "EMR so what we thought what we will build those two patterns and give you",
    "start": "50070",
    "end": "55980"
  },
  {
    "text": "advice on what we have found from our customers as the best practices of running those patterns and what we have",
    "start": "55980",
    "end": "61859"
  },
  {
    "text": "done in the entire year to accelerate or to make it easy to run those patterns on your more a lot of our customers you",
    "start": "61859",
    "end": "69420"
  },
  {
    "text": "would find that some you are probably running one or the other of that pattern so some of this might help you improve",
    "start": "69420",
    "end": "76920"
  },
  {
    "text": "what you're already running some of this might help you add to what you are already running so let's get in it so as",
    "start": "76920",
    "end": "84119"
  },
  {
    "start": "82000",
    "end": "178000"
  },
  {
    "text": "you know that we release almost on a monthly basis we released a new version of EMR almost on a monthly basis where",
    "start": "84119",
    "end": "90780"
  },
  {
    "text": "we have new open source packages in the distribution we have about 16 different",
    "start": "90780",
    "end": "97259"
  },
  {
    "text": "applications on EMR today this year we added Jupiter hub Livi tensorflow and MX",
    "start": "97259",
    "end": "102720"
  },
  {
    "text": "net we also added a significant amount of new features that were contributed back to open source for example the",
    "start": "102720",
    "end": "110369"
  },
  {
    "text": "sparkly be the spark magic and Libby user impersonation with Jupiter hub that's what's going to contribute it",
    "start": "110369",
    "end": "116610"
  },
  {
    "text": "back to open source some of that actually came from you guys where you told us that this was the problem when you were running and we tried to solve",
    "start": "116610",
    "end": "124680"
  },
  {
    "text": "it and contribute back to open source how many of you running HBase on s3 a Mars version of HBase on the registry",
    "start": "124680",
    "end": "131910"
  },
  {
    "text": "except some of you so we also wrote a detailed guide on how to use and when to",
    "start": "131910",
    "end": "139140"
  },
  {
    "text": "use HBase on s3 you can find this online as well so it's a and how to optimize",
    "start": "139140",
    "end": "144930"
  },
  {
    "text": "those workloads so here's a I exam for",
    "start": "144930",
    "end": "150000"
  },
  {
    "text": "you if you can look at if you can look at these applications this is how we track about sixteen to nineteen",
    "start": "150000",
    "end": "156510"
  },
  {
    "text": "different applications that run on top of EMR and our commitment to you is that as long as there is an open source",
    "start": "156510",
    "end": "162780"
  },
  {
    "text": "version that is stable we will try and release it within the next release of",
    "start": "162780",
    "end": "168090"
  },
  {
    "text": "EMR at least within a 30-day time cycle so at least till now we have been able",
    "start": "168090",
    "end": "173280"
  },
  {
    "text": "to do that and hopefully we'll be able to continue on that commitment how many",
    "start": "173280",
    "end": "179070"
  },
  {
    "start": "178000",
    "end": "243000"
  },
  {
    "text": "of you know about the AWS big data blog awesome so for those of you who don't",
    "start": "179070",
    "end": "185490"
  },
  {
    "text": "there is blog like you have Jeff Barse blog and the compute blog and the serverless blog there is a blog on AWS",
    "start": "185490",
    "end": "191700"
  },
  {
    "text": "called a tableau as big data blog you can follow it by clicking subscribe to it and a lot of what we do goes into",
    "start": "191700",
    "end": "199440"
  },
  {
    "text": "this blog as feature releases also a ton of articles written on this are by",
    "start": "199440",
    "end": "205590"
  },
  {
    "text": "customers by solution architects or by big data architects that take EMR along",
    "start": "205590",
    "end": "211410"
  },
  {
    "text": "with other components and help you build systems or help you build complete architectures for example we recently",
    "start": "211410",
    "end": "217620"
  },
  {
    "text": "released a blog where auto scaling policies were used to scale HDFS on a",
    "start": "217620",
    "end": "224340"
  },
  {
    "text": "running cluster using elastic volumes on EBS so as EBS released last equol yems we added that to auto scaling and it it",
    "start": "224340",
    "end": "233130"
  },
  {
    "text": "gets released as a blog so do take a look at what happens on on these blog there are several very interesting",
    "start": "233130",
    "end": "238830"
  },
  {
    "text": "articles on this blog about things that you can do so what do we see customers",
    "start": "238830",
    "end": "245040"
  },
  {
    "start": "243000",
    "end": "308000"
  },
  {
    "text": "running so we essentially see two architectural patterns of what customers run we see customers running transient",
    "start": "245040",
    "end": "251970"
  },
  {
    "text": "clusters that means and this is a fairly common pattern on top of AWS cloud I think the underpinning of this pattern",
    "start": "251970",
    "end": "258870"
  },
  {
    "text": "is that if you have data in s3 running a ten node cluster for 10 hours cost you",
    "start": "258870",
    "end": "264210"
  },
  {
    "text": "exactly the same as running 100 node cluster for one hour so why wouldn't you scale up your cluster to",
    "start": "264210",
    "end": "269280"
  },
  {
    "text": "hundred nodes process your data and then shut the cluster back down we see this a",
    "start": "269280",
    "end": "274290"
  },
  {
    "text": "lot with the with with customers the other pattern that we see is of persistent clusters where you have a",
    "start": "274290",
    "end": "280800"
  },
  {
    "text": "multi-talented persistent workload that never shuts down but scales up and down",
    "start": "280800",
    "end": "285810"
  },
  {
    "text": "using native auto scaling policies that are are an EMR cluster so predominantly",
    "start": "285810",
    "end": "291030"
  },
  {
    "text": "if you look at the workloads people are running on top of EMR we can divide them into these two patterns there's also the",
    "start": "291030",
    "end": "297960"
  },
  {
    "text": "HBase pattern which I think will address separately but most of the use cases",
    "start": "297960",
    "end": "303180"
  },
  {
    "text": "fall into these two patterns so what are the if you look at it from a workload perspective the transient cluster is",
    "start": "303180",
    "end": "311010"
  },
  {
    "text": "used largely for large-scale transformation so you know if you're doing hive or spark with Scala for ETL",
    "start": "311010",
    "end": "317600"
  },
  {
    "text": "that could be one of the use cases for transfer for transient cluster it works very well you have a scheduled job that",
    "start": "317600",
    "end": "324840"
  },
  {
    "text": "runs every R or a couple of hours it spins up a cluster finishes the job spins down the cluster the data is",
    "start": "324840",
    "end": "330930"
  },
  {
    "text": "already in s3 so this transformation pattern can be used in multiple ways it",
    "start": "330930",
    "end": "336330"
  },
  {
    "text": "can be used to clean and compose a lot of data convert the data into from one",
    "start": "336330",
    "end": "341370"
  },
  {
    "text": "format to another it can also be used to load data into other systems or for ETL",
    "start": "341370",
    "end": "346500"
  },
  {
    "text": "into other systems and also for building extracts so for example we see a lot of",
    "start": "346500",
    "end": "352110"
  },
  {
    "text": "you running ml jobs on top of your mark Luster's that build extracts the persistent use case is an interesting",
    "start": "352110",
    "end": "359070"
  },
  {
    "text": "one because we see customers multi 10:18 ad hoc workloads now the ad hoc work",
    "start": "359070",
    "end": "364440"
  },
  {
    "text": "clothes could be coming from a notebook so Jupiter notebook is a pretty common sin IDE that data scientists to use we",
    "start": "364440",
    "end": "373020"
  },
  {
    "text": "often see multiple notebooks attached to a single cluster which Auto scales based upon the amount of load that is pushed",
    "start": "373020",
    "end": "379830"
  },
  {
    "text": "on to the cluster we see a lot of experimentation on a persistent cluster we will have a users log into the",
    "start": "379830",
    "end": "386040"
  },
  {
    "text": "cluster log into edge nodes and run jobs for experimentation for dev and for",
    "start": "386040",
    "end": "391560"
  },
  {
    "text": "testing of those clusters we also see ad-hoc usage of sequel so sparks equal",
    "start": "391560",
    "end": "396990"
  },
  {
    "text": "or presto where somebody is logging into the cluster all through a hue interface and then running and queries on the cluster",
    "start": "396990",
    "end": "402900"
  },
  {
    "text": "those are also used cases on this if you're also doing transformation that is continuous maybe in terms of streaming",
    "start": "402900",
    "end": "409950"
  },
  {
    "text": "or maybe in terms of an hourly ETL there are all fairly good reasons to run a persistent clusters so one of the things",
    "start": "409950",
    "end": "417420"
  },
  {
    "text": "that most of our customers ask us is do does everybody run transient cluster does everybody run persistent cluster I",
    "start": "417420",
    "end": "424290"
  },
  {
    "text": "think a good exact answer to this is depending upon the workload people run a combination of transient and persistent",
    "start": "424290",
    "end": "431550"
  },
  {
    "text": "clusters even in the world that we have seen with persistent clusters we often see customers recycle their clusters for",
    "start": "431550",
    "end": "438450"
  },
  {
    "text": "security updates every couple of every couple of days and especially if you're running if you look at the workload and",
    "start": "438450",
    "end": "444210"
  },
  {
    "text": "persistent clusters there's often an opportunity to recycle those as well so it's not one versus the other",
    "start": "444210",
    "end": "450390"
  },
  {
    "text": "it's basically both of those patterns are perfectly viable for you as long as your workload fits that particular",
    "start": "450390",
    "end": "456330"
  },
  {
    "text": "pattern so what what so let's talk about the first pattern which is the transient",
    "start": "456330",
    "end": "462540"
  },
  {
    "start": "457000",
    "end": "606000"
  },
  {
    "text": "cluster pattern and if we try to condense all the best practices that we",
    "start": "462540",
    "end": "467580"
  },
  {
    "text": "have heard from our customers who run this at scale it would condense to something like this run a stateless as",
    "start": "467580",
    "end": "474120"
  },
  {
    "text": "possible automate everything and try and template as much as possible so what",
    "start": "474120",
    "end": "479880"
  },
  {
    "text": "does that mean it means that if you look at the transient clusters we want to",
    "start": "479880",
    "end": "485790"
  },
  {
    "text": "offload all the metadata or the state of the cluster so one good example of this",
    "start": "485790",
    "end": "490890"
  },
  {
    "text": "is connecting to the hive meta store or if you are using Hugh then connecting taking the Hugh database out of the",
    "start": "490890",
    "end": "497790"
  },
  {
    "text": "other cluster of you're using Kerberos then taking an external key DC so on and so forth there's also use case around",
    "start": "497790",
    "end": "506060"
  },
  {
    "text": "persisting your data industry so your state of your various source data as in your app and your final data set is all",
    "start": "506060",
    "end": "513330"
  },
  {
    "text": "in s3 there's also some best practices around how you submit jobs to build",
    "start": "513330",
    "end": "519419"
  },
  {
    "text": "pipelines essentially you if you can automate the submission of your jobs so that clusters can run on a schedule",
    "start": "519420",
    "end": "525690"
  },
  {
    "text": "you basically have got this pattern down as a pipeline and the last one is you",
    "start": "525690",
    "end": "531630"
  },
  {
    "text": "can use pot to the costume these clusters and a lot of our customers do we'll talk a little bit about and I'll show you some data but",
    "start": "531630",
    "end": "538019"
  },
  {
    "text": "what we see from our and in terms of how the spot usage has grown significantly",
    "start": "538019",
    "end": "544490"
  },
  {
    "text": "one pattern that we have seen as we see this great pattern of transient clusters that is used by a lot of customers but",
    "start": "545120",
    "end": "551880"
  },
  {
    "text": "customers often tell us and specially data teams that are that are catering to the demands of multiple lines of",
    "start": "551880",
    "end": "557760"
  },
  {
    "text": "businesses they tell us that okay we get it we know how to template these clusters but how do I make sure that",
    "start": "557760",
    "end": "564300"
  },
  {
    "text": "hundreds of lines of businesses that are now using EMR have the same template of running this Kuster how do I build a",
    "start": "564300",
    "end": "571040"
  },
  {
    "text": "self-service platform where anybody can spin up a transient cluster and that adheres to the same kind of best",
    "start": "571040",
    "end": "577740"
  },
  {
    "text": "practices that we design it for and how will those manage costs so we'll talk a little bit about that as well",
    "start": "577740",
    "end": "584000"
  },
  {
    "text": "so one of the basics of running stateless is you can take the meta store off the cluster this is a fairly common",
    "start": "584000",
    "end": "590459"
  },
  {
    "text": "use case where you take the meta store off the cluster it improves your startup time it lowers you your cost because you",
    "start": "590459",
    "end": "597089"
  },
  {
    "text": "don't have to you know spin up the metadata server on every machine or",
    "start": "597089",
    "end": "602490"
  },
  {
    "text": "every cluster in just all of the data fairly common pattern we make it pretty easy to do this you can use a",
    "start": "602490",
    "end": "609149"
  },
  {
    "start": "606000",
    "end": "666000"
  },
  {
    "text": "configuration file on your team on your EMR cluster that allows you to connect to a meta store so early this year or",
    "start": "609149",
    "end": "616320"
  },
  {
    "text": "late last year we released a tableau as glue data catalog which is actually a drop-in replacement for the hive",
    "start": "616320",
    "end": "622199"
  },
  {
    "text": "metadata service or for the hive meta store so you can a lot of our customers",
    "start": "622199",
    "end": "627480"
  },
  {
    "text": "have now migrated from using the hive meta store to using the glue data catalog the benefit of the glue data",
    "start": "627480",
    "end": "633720"
  },
  {
    "text": "catalog is it is the shared meta store between services like AMR Athena glue",
    "start": "633720",
    "end": "640470"
  },
  {
    "text": "and Richard Spectrum and what we note from a lot of our data teams is they don't use EMR in only alone they use EMR",
    "start": "640470",
    "end": "647730"
  },
  {
    "text": "with a little bit of Athena some glue a lot of redshift all of these together so if you have a common metadata service",
    "start": "647730",
    "end": "654510"
  },
  {
    "text": "like the glue data catalog which is a drop-in replacement from the hive meta store immensely helps that scenario",
    "start": "654510",
    "end": "663170"
  },
  {
    "text": "so connecting to the glue catalog is also very simple you're basically pointed to a factory you point a you use",
    "start": "665930",
    "end": "674070"
  },
  {
    "start": "666000",
    "end": "796000"
  },
  {
    "text": "the configuration settings that are run on the slide notice that just also an account ID in there that means you can",
    "start": "674070",
    "end": "680940"
  },
  {
    "text": "also connect to a blue data catalog that is sitting in a completely different account and I'll talk about what is",
    "start": "680940",
    "end": "687120"
  },
  {
    "text": "enabling this use case we also released fine-grained access under on objects of",
    "start": "687120",
    "end": "693779"
  },
  {
    "text": "the glue data catalog so for example you can restrict access to a certain catalogue you can restrict access to a",
    "start": "693779",
    "end": "699990"
  },
  {
    "text": "certain table you can restrict access with a regular expression you can also restrict access to a complete catalogue",
    "start": "699990",
    "end": "706910"
  },
  {
    "text": "however remember that this is only restricting access to the glue metadata",
    "start": "706910",
    "end": "712410"
  },
  {
    "text": "elements it's not restricting access to data in s3 that you will still need that still needs to be managed to s3 but B",
    "start": "712410",
    "end": "720120"
  },
  {
    "text": "you need a user needs both access to the glue data catalog and access to s3 to",
    "start": "720120",
    "end": "725700"
  },
  {
    "text": "execute maybe a query or run particular job so it's kind of adding another layer of fine-grained access control that you",
    "start": "725700",
    "end": "732510"
  },
  {
    "text": "will see that you can use to control access also with you would have noticed",
    "start": "732510",
    "end": "738690"
  },
  {
    "text": "the release of a tableau as lake formation this year late formation will",
    "start": "738690",
    "end": "744000"
  },
  {
    "text": "further simplify this and further simplify making this really easy on the top of the glue to layer catalog so look",
    "start": "744000",
    "end": "751140"
  },
  {
    "text": "out for those sessions on the AWS lake lake formation to learn more about that a lot of teams told us that they have",
    "start": "751140",
    "end": "759440"
  },
  {
    "text": "especially in large organizations they have a central data team and multiple lines of businesses and central data",
    "start": "759440",
    "end": "766649"
  },
  {
    "text": "teams want to expose a data catalog to other lines of businesses so that EMR",
    "start": "766649",
    "end": "772380"
  },
  {
    "text": "clusters are running in their account however it was talking to the data sitting in the data teams account and",
    "start": "772380",
    "end": "778949"
  },
  {
    "text": "the catalog sitting in their account so it's it's a very good use case if you want to do cost attribution so you can",
    "start": "778949",
    "end": "786000"
  },
  {
    "text": "have your customers run in separate accounts and the data catalog in one account so we enabled cross account blue",
    "start": "786000",
    "end": "791820"
  },
  {
    "text": "data catalog access again this year pretty useful we also",
    "start": "791820",
    "end": "797939"
  },
  {
    "start": "796000",
    "end": "824000"
  },
  {
    "text": "a bunch of scripts that can help you migrate the hive metal store to the data",
    "start": "797939",
    "end": "803459"
  },
  {
    "text": "catalog there are a couple of ways of doing that there's a bulk copy mode on the script I would recommend that you do",
    "start": "803459",
    "end": "810569"
  },
  {
    "text": "test the script before you use it in production but this isn't me this is maybe a direction that you could use to",
    "start": "810569",
    "end": "817859"
  },
  {
    "text": "take data off your hive meta store and then run it and then use the dual data catalog so let's talk about this other",
    "start": "817859",
    "end": "825720"
  },
  {
    "start": "824000",
    "end": "989000"
  },
  {
    "text": "pattern of using s3 to persist your data we know that reads are fairly fairly",
    "start": "825720",
    "end": "831959"
  },
  {
    "text": "great on Amazon s3 because of the X the throughput that you get and a lot of workloads on running on EMR our",
    "start": "831959",
    "end": "839369"
  },
  {
    "text": "throughput bound except maybe the no sequel ones and there are but Riots tend to be slower because of the way a bunch",
    "start": "839369",
    "end": "846449"
  },
  {
    "text": "of committers are written so committers are essentially pieces of code that write data to s3 and because s3 is not a",
    "start": "846449",
    "end": "853139"
  },
  {
    "text": "POSIX compliant system in the early days the way you would write to s3 is you would write to a temporary file or a",
    "start": "853139",
    "end": "859229"
  },
  {
    "text": "temporary location and then you do a rename of all the files to move it into the right location tends to be really",
    "start": "859229",
    "end": "866099"
  },
  {
    "text": "clunky and slow so if both in open source and EMR FS which is EMRs on",
    "start": "866099",
    "end": "872309"
  },
  {
    "text": "version of how it talks to s3 there are opportunities to improve the performance of this particular committed code so we",
    "start": "872309",
    "end": "880439"
  },
  {
    "text": "recently released a new committer called EMR s3 optimized committer that is available in 5.19 significantly improves",
    "start": "880439",
    "end": "888059"
  },
  {
    "text": "the performance of spark when you write park' data on to s3 so instead of a",
    "start": "888059",
    "end": "893129"
  },
  {
    "text": "two-step process it basically writes it in one step using s3 multi-part uploads those of you familiar with multi-part",
    "start": "893129",
    "end": "899609"
  },
  {
    "text": "uploads multi-part loads are essentially atomic so it essentially uses the multi-part upload to commit data into s3",
    "start": "899609",
    "end": "907349"
  },
  {
    "text": "directly avoiding the two-step process and as you can see in the graphs that",
    "start": "907349",
    "end": "912839"
  },
  {
    "text": "I'm putting up on the board you we see significant performance improvements between the file commit o V 1 and V 2",
    "start": "912839",
    "end": "919849"
  },
  {
    "text": "there's also an open source equivalent of this that is available it's your choice which one you want to use this is",
    "start": "919849",
    "end": "925859"
  },
  {
    "text": "our implementation we also see that this implement with this implementation we",
    "start": "925859",
    "end": "931459"
  },
  {
    "text": "we can switch on speculative execution on s3 if you're using this implementation with spark and park a",
    "start": "931459",
    "end": "937660"
  },
  {
    "text": "speculative execution allows you to has great performance or has benefits when",
    "start": "937660",
    "end": "943189"
  },
  {
    "text": "you have lot of straggler jobs that means if your data has a lot of skew there's going to be straggler jobs and speculative execution improves the",
    "start": "943189",
    "end": "949850"
  },
  {
    "text": "performance of straggler jobs however we have always meant kind of caution to our",
    "start": "949850",
    "end": "956059"
  },
  {
    "text": "customers in in using speculative execution on top of s3 we generally turn it off because it can cause",
    "start": "956059",
    "end": "962119"
  },
  {
    "text": "inconsistencies but now with this new version of VM RFS committer when you're",
    "start": "962119",
    "end": "968329"
  },
  {
    "text": "using spark and writing data to park' you can definitely use speculative con",
    "start": "968329",
    "end": "974019"
  },
  {
    "text": "execution it also shows a pretty good performance when you're using a MRFs",
    "start": "974019",
    "end": "979100"
  },
  {
    "text": "consistent view MRFs consistent view is a part of EMR FS that allows you to maintain consistency",
    "start": "979100",
    "end": "985100"
  },
  {
    "text": "of objects between reads and writes also",
    "start": "985100",
    "end": "990800"
  },
  {
    "start": "989000",
    "end": "1124000"
  },
  {
    "text": "with talking about s3 we also released integration with s3 select so s3 selectors and as a native s3 api that",
    "start": "990800",
    "end": "997999"
  },
  {
    "text": "got released last year that allows you to push a predicate all the way down to an object so we so for example when",
    "start": "997999",
    "end": "1007240"
  },
  {
    "text": "spark runs with s3 spark basically pulls all the data from s3 into ec2 nodes and",
    "start": "1007240",
    "end": "1012459"
  },
  {
    "text": "filters it in memory but spark integration of s3 select we spark does",
    "start": "1012459",
    "end": "1017470"
  },
  {
    "text": "not pull all the data from s3 instead it pushes the predicate all the way down to s3 and then s 3 returns only the",
    "start": "1017470",
    "end": "1024579"
  },
  {
    "text": "resultant data back so the idea is that use with the user of s3 select you get better performance however remember that",
    "start": "1024579",
    "end": "1031480"
  },
  {
    "text": "s3 select has a different cost profile than s3 has so that's why we haven't defaulted all interactions to s3 select",
    "start": "1031480",
    "end": "1039899"
  },
  {
    "text": "default the to all interactions to sd select on EMR clusters but you have a",
    "start": "1039899",
    "end": "1044918"
  },
  {
    "text": "choice of enabling that it's available on CSV and JSON and it's available in",
    "start": "1044919",
    "end": "1050529"
  },
  {
    "text": "spark hive and pressed on top of EMR so we ran some tests we when we compared s3",
    "start": "1050529",
    "end": "1057399"
  },
  {
    "text": "select to stark s3 so this was pesto running TPC open source presto running",
    "start": "1057399",
    "end": "1062620"
  },
  {
    "text": "TPC des hundred queries on s3 and you would see that on some queries we see regression but when most",
    "start": "1062620",
    "end": "1068980"
  },
  {
    "text": "of the queries you see 40 to 60 percent performance improvement remember it adds a different cost profile because there's",
    "start": "1068980",
    "end": "1075340"
  },
  {
    "text": "three select you actually charges you for the amount of data that you get back also",
    "start": "1075340",
    "end": "1080920"
  },
  {
    "text": "there are two things to to be cautious about when you're using s3 select one is if your queries don't filter any data I",
    "start": "1080920",
    "end": "1087730"
  },
  {
    "text": "mean there's basically you're not gonna get a lot of benefit because you're still gonna scan all the data and get",
    "start": "1087730",
    "end": "1092860"
  },
  {
    "text": "charged for that data however also if your data is in compressed format s3",
    "start": "1092860",
    "end": "1098350"
  },
  {
    "text": "select would have to decompress the data before it can flutter so that means sometimes you can get more data than",
    "start": "1098350",
    "end": "1104710"
  },
  {
    "text": "what you had initially on s3 because it has to send the data back up into history so just be cautious of of those",
    "start": "1104710",
    "end": "1110920"
  },
  {
    "text": "two gotchas when you're using s3 select it's available today you can switch it on if you have queries which filter a",
    "start": "1110920",
    "end": "1116410"
  },
  {
    "text": "large volume of data on CSV in JSON I think you will get much better performance and s3 selected as is",
    "start": "1116410",
    "end": "1122230"
  },
  {
    "text": "evident in this chart we also see several ways of submitting jobs to a",
    "start": "1122230",
    "end": "1129090"
  },
  {
    "start": "1124000",
    "end": "1229000"
  },
  {
    "text": "cluster today and especially a transient cluster people use the EMR step API",
    "start": "1129090",
    "end": "1134850"
  },
  {
    "text": "which submits jobs to the cluster we also see AWS step functions so how many",
    "start": "1134850",
    "end": "1140140"
  },
  {
    "text": "of you know what step functions are fantastic so there's a use case that we",
    "start": "1140140",
    "end": "1147280"
  },
  {
    "text": "see where customers use AWS step functions to submit job to something like Libby which has a REST API here's a",
    "start": "1147280",
    "end": "1154990"
  },
  {
    "text": "good example of that you can define your complete state machine in the step function and each step of the step",
    "start": "1154990",
    "end": "1162880"
  },
  {
    "text": "function can trigger a lambda function to submit the job to spa so step",
    "start": "1162880",
    "end": "1168010"
  },
  {
    "text": "functions essentially have three state there is a task state it can invoke a lambda function it can call the Libby server and submit a job the next task",
    "start": "1168010",
    "end": "1176410"
  },
  {
    "text": "actually looks at retrieves the state of the previous job and if the state of the previous job is success it can go into a",
    "start": "1176410",
    "end": "1183070"
  },
  {
    "text": "choice state whether it can take a path X or path y so something like this you can build a complex tree are essentially",
    "start": "1183070",
    "end": "1190720"
  },
  {
    "text": "a dag a relationship tag where you can take each of these elements in the step function and you can run them on an EMR",
    "start": "1190720",
    "end": "1197920"
  },
  {
    "text": "cluster using this edible using the EMR step API or the delivery server very interesting",
    "start": "1197920",
    "end": "1206020"
  },
  {
    "text": "use cases and we've seen a lot of customers will migrate to this serverless way of submitting jobs to the",
    "start": "1206020",
    "end": "1211750"
  },
  {
    "text": "cluster also when these are made these jobs you can go to this EMR spark UI and",
    "start": "1211750",
    "end": "1217780"
  },
  {
    "text": "you can also see these jobs in the list of application history when you click on any of these jobs it also can take you",
    "start": "1217780",
    "end": "1223960"
  },
  {
    "text": "to the exactly logs associated with that particular job on this part on the UI we",
    "start": "1223960",
    "end": "1229900"
  },
  {
    "start": "1229000",
    "end": "1413000"
  },
  {
    "text": "have seen customers do advanced orchestration where they trigger these jobs are the trigger these step functions based upon arrival of the data",
    "start": "1229900",
    "end": "1236230"
  },
  {
    "text": "based upon arrival of a metric or a cloud watch alarm are based on arrival",
    "start": "1236230",
    "end": "1241300"
  },
  {
    "text": "of a manifest file those are all useful trigger faction trigger functions they",
    "start": "1241300",
    "end": "1246790"
  },
  {
    "text": "are all we have also seen orchestration based upon schedule we have seen alerting based on failures which step",
    "start": "1246790",
    "end": "1251800"
  },
  {
    "text": "function provides and we also see in parallel execution where you have multiple streams of these DAGs parallely",
    "start": "1251800",
    "end": "1257020"
  },
  {
    "text": "submitting jobs to any amar class term so very useful so let's talk a little",
    "start": "1257020",
    "end": "1262360"
  },
  {
    "text": "bit about Spartan sensors we see a lot of our customers use part instances to reduce the cost of their EMR clusters",
    "start": "1262360",
    "end": "1269890"
  },
  {
    "text": "very much in the transient use case but also in the persistent use case and I",
    "start": "1269890",
    "end": "1275200"
  },
  {
    "text": "think the fundamental reason is imagine I have a cluster that's running for 14 hours where each node on-demand cost me",
    "start": "1275200",
    "end": "1282010"
  },
  {
    "text": "a dollar and let's say the cluster is running for 10 hours so this cluster is costing me roughly about 140 dollars for",
    "start": "1282010",
    "end": "1288310"
  },
  {
    "text": "the entire operation and now imagine I added 10 more nodes in spot and in my perfect world let's assume that adding",
    "start": "1288310",
    "end": "1295780"
  },
  {
    "text": "10 more nodes I have 20 node cluster now let's say adding 10 more nodes reduces",
    "start": "1295780",
    "end": "1301720"
  },
  {
    "text": "my 14 hours of job time to 7 hours let's assume linear scalability but so the car",
    "start": "1301720",
    "end": "1308950"
  },
  {
    "text": "if you look at the cost of this cluster assuming that all the spot nodes that I got were a 50% discount this is the cost",
    "start": "1308950",
    "end": "1315760"
  },
  {
    "text": "of the cluster that you will get you generally get a higher discount on spot instances about about 80 to 90 percent",
    "start": "1315760",
    "end": "1322840"
  },
  {
    "text": "but I think it's safe to assume that it is at 50 percent discount from the on-demand price so you would see that",
    "start": "1322840",
    "end": "1328210"
  },
  {
    "text": "now by adding spot you have taken down 50 percent less run thyme and 25% less cough so what I tell",
    "start": "1328210",
    "end": "1335500"
  },
  {
    "text": "my customers is the is the best use case or the best pattern for using spot is you run the number of on-demand nodes",
    "start": "1335500",
    "end": "1342850"
  },
  {
    "text": "that covers your worst case SLA for your business if the worst case SLA for your",
    "start": "1342850",
    "end": "1348040"
  },
  {
    "text": "business is 10 hours where you want to finish a certain ETL run those number of on demand nodes and then scale up using",
    "start": "1348040",
    "end": "1354400"
  },
  {
    "text": "the spot nodes and every time you scale up you will always finish a job faster",
    "start": "1354400",
    "end": "1359410"
  },
  {
    "text": "and save cost right because 10 node cluster running for 10 hours",
    "start": "1359410",
    "end": "1365260"
  },
  {
    "text": "it cost you the same as 100 node cluster running for one hour now in on a day when you don't get the spot instances",
    "start": "1365260",
    "end": "1371020"
  },
  {
    "text": "then you already are meeting your worst case SLA this is the same pattern that",
    "start": "1371020",
    "end": "1376720"
  },
  {
    "text": "we tell our customers to use in a persistent use case where you have a persistent cluster that is running maybe",
    "start": "1376720",
    "end": "1382390"
  },
  {
    "text": "that is a 10 no 20 node 50 node cluster that you can get our eyes for this so",
    "start": "1382390",
    "end": "1387460"
  },
  {
    "text": "you get the benefit of ec2 RI pricing on that and then you scale up and down",
    "start": "1387460",
    "end": "1392679"
  },
  {
    "text": "using the spot instances and when those instances get taken away or let's say you don't find them in the spot market",
    "start": "1392679",
    "end": "1398290"
  },
  {
    "text": "well you were always at your you were always provisioning for the worst case scenario so spot using spot every day is",
    "start": "1398290",
    "end": "1405190"
  },
  {
    "text": "essentially gravy on top think that the I thought that will catch on a little",
    "start": "1405190",
    "end": "1410650"
  },
  {
    "text": "bit more but that's okay so what are we hear from customers we hear a couple of",
    "start": "1410650",
    "end": "1416110"
  },
  {
    "text": "things from our customers when they complain about spot and especially we we we hear from customers who have been",
    "start": "1416110",
    "end": "1422500"
  },
  {
    "text": "using spot for a while that they've kept their problems kind of lie into these two distinct buckets one is capacity",
    "start": "1422500",
    "end": "1428919"
  },
  {
    "text": "related and the second one is interruption related the capacity related problems are oh we tried to run",
    "start": "1428919",
    "end": "1434260"
  },
  {
    "text": "an m3 extra-large or c5 for extra-large in u.s. East one and we couldn't find",
    "start": "1434260",
    "end": "1439600"
  },
  {
    "text": "any capacity we waited for 30 minutes and 40 minutes and we couldn't find min capacity or I was trying to launch in an",
    "start": "1439600",
    "end": "1445750"
  },
  {
    "text": "AZ I couldn't find any capacity the other one is my spot cluster was interrupted or I had a cluster that was",
    "start": "1445750",
    "end": "1453040"
  },
  {
    "text": "running the master node and the master node was interrupted try not running the master node and spot that tends to be",
    "start": "1453040",
    "end": "1459010"
  },
  {
    "text": "kill the entire cluster a lot if the spot instance taken away but there was also conditions where you know the spot",
    "start": "1459010",
    "end": "1466210"
  },
  {
    "text": "cluster was interrupted here is real data on the percentage of clusters that",
    "start": "1466210",
    "end": "1473409"
  },
  {
    "text": "are interrupted by spot across a complete month and you will see that it",
    "start": "1473409",
    "end": "1478419"
  },
  {
    "text": "is so low it's less than 0.2 to 3% and this is data that you can see on a on a",
    "start": "1478419",
    "end": "1484570"
  },
  {
    "text": "month by month basis you can see a spike and NLO but this is the percentage of",
    "start": "1484570",
    "end": "1491590"
  },
  {
    "text": "all the spot clusters that run on EMR and there's lots and lots of them how many of you you spot with EMR today in",
    "start": "1491590",
    "end": "1497950"
  },
  {
    "text": "this room about half of you when we looked into all of these interruptions",
    "start": "1497950",
    "end": "1504070"
  },
  {
    "text": "and what percentage of these interruptions were what were the reasons for this interruptions we found three",
    "start": "1504070",
    "end": "1509980"
  },
  {
    "text": "reasons we found that a request did you request it for a type of instance in a",
    "start": "1509980",
    "end": "1515200"
  },
  {
    "text": "certain AZ and it wasn't available we also found that the requested capacity in all a AC sometimes not available and",
    "start": "1515200",
    "end": "1523000"
  },
  {
    "text": "we also found the termination of a single instance sometimes caused the entire cluster to fail we had a bug in",
    "start": "1523000",
    "end": "1530049"
  },
  {
    "text": "how we were provisioning our system and especially on the core nodes where if you take one node away on the core node",
    "start": "1530049",
    "end": "1535510"
  },
  {
    "text": "the entire cluster would fail this wasn't the case with task nodes but we saw our general guidance was don't run",
    "start": "1535510",
    "end": "1541600"
  },
  {
    "text": "core nodes and spotter and task no sunspot if a spot node goes away that you know that's fine it you will recover",
    "start": "1541600",
    "end": "1547240"
  },
  {
    "text": "but people also ran master nodes and core nodes and we saw that even when the cornered if one instance was taken away",
    "start": "1547240",
    "end": "1554200"
  },
  {
    "text": "sometimes it will kill an entire cluster so we have solved that problem it was a very small percentage of the clusters",
    "start": "1554200",
    "end": "1560890"
  },
  {
    "text": "but if you look at that the three issues that I listed down the first two were the majority of the reasons people saw",
    "start": "1560890",
    "end": "1566530"
  },
  {
    "text": "interruption or people who couldn't use pot and we figured that there was there",
    "start": "1566530",
    "end": "1572770"
  },
  {
    "start": "1570000",
    "end": "1696000"
  },
  {
    "text": "was one basic reason so spot is the capacity it's basically a market and if you all invest in the market what do you",
    "start": "1572770",
    "end": "1579250"
  },
  {
    "text": "do you don't invest everything in one stock you try and diversify on the index or you try and diversify across stock so",
    "start": "1579250",
    "end": "1586540"
  },
  {
    "text": "AWS has capacity that is available across a multitude of instances but what we see in terms of interruptions is",
    "start": "1586540",
    "end": "1593880"
  },
  {
    "text": "everybody is trying to get the same instance in the same a Z so here's here's something called the",
    "start": "1593880",
    "end": "1600120"
  },
  {
    "text": "spot bid advisor how many of you know about the spot bid advisor if you don't know about the spot bid advisor I would",
    "start": "1600120",
    "end": "1606390"
  },
  {
    "text": "recommend that you google it it's on the spot website it's also it's a table it's",
    "start": "1606390",
    "end": "1611580"
  },
  {
    "text": "also an API that you could use there is a nice little checkbox button there which talks about instances that EMR",
    "start": "1611580",
    "end": "1618000"
  },
  {
    "text": "supports but essentially if you look at this table you would see that the r44 extra Lodge has 16gb CPUs and 32 gigs of ram",
    "start": "1618000",
    "end": "1625920"
  },
  {
    "text": "sorry 122 GB of RAM and it is very similar to the r44 extra-large and they are almost at the same percentage",
    "start": "1625920",
    "end": "1631950"
  },
  {
    "text": "discount at about 73 and 74 percent but one is at 5% interruption rate and the",
    "start": "1631950",
    "end": "1637350"
  },
  {
    "text": "other one is at 20% interruption rate the 20% interruption rate is because everybody is trying to get the same",
    "start": "1637350",
    "end": "1643260"
  },
  {
    "text": "instance at the same time in the same easy so the price has spiked up but at the same time there are other instances",
    "start": "1643260",
    "end": "1650250"
  },
  {
    "text": "of similar capacity similar discount probably even higher discount that are available that look exactly the same",
    "start": "1650250",
    "end": "1656730"
  },
  {
    "text": "which we are not able to use this so it almost feels like there is an eight-lane",
    "start": "1656730",
    "end": "1662010"
  },
  {
    "text": "highway and for some reason everybody is driving in the same Lane right and then there is congestion so most of our it's",
    "start": "1662010",
    "end": "1670710"
  },
  {
    "text": "very experienced the spot users what they do is they take instance in instance groups they try provisioning on",
    "start": "1670710",
    "end": "1678450"
  },
  {
    "text": "a certain instance type if they don't find it they try provisioning on a different distress type but they have instance flexibility essentially what",
    "start": "1678450",
    "end": "1685890"
  },
  {
    "text": "you're saying is if I want to run my spark job and I need something that has more than 16 gigs of RAM 16 CPUs and",
    "start": "1685890",
    "end": "1692550"
  },
  {
    "text": "hundred and 20 gigs of ram there are lots of choices within a tableau s so we created something called the instance",
    "start": "1692550",
    "end": "1698340"
  },
  {
    "start": "1696000",
    "end": "1871000"
  },
  {
    "text": "fleet API what the instance fleet API does it allows you to mix and match",
    "start": "1698340",
    "end": "1704130"
  },
  {
    "text": "different instance types into one instance group so the API works is this",
    "start": "1704130",
    "end": "1710100"
  },
  {
    "text": "youtell instance fleet I want 160 cores or I want maybe a thousand course on my",
    "start": "1710100",
    "end": "1716100"
  },
  {
    "text": "cluster I want certain amount of RAM and I think that these particular instances",
    "start": "1716100",
    "end": "1721110"
  },
  {
    "text": "will be are good for me they couldn't be from the same family they can be from different family they can be as big as",
    "start": "1721110",
    "end": "1727440"
  },
  {
    "text": "you know multiple families so for example I 1,000 course I'm fine with any for",
    "start": "1727440",
    "end": "1734450"
  },
  {
    "text": "extra-large machine and R 5 r r4 and r3 and m4 and m5 so I've given it five",
    "start": "1734450",
    "end": "1741650"
  },
  {
    "text": "different options so what the instance fleet does is every time if it sees a contention on the M file it will pick",
    "start": "1741650",
    "end": "1748820"
  },
  {
    "text": "the cheapest available instance in an AZ and it will provision it for you it will try and mix and match the instances to",
    "start": "1748820",
    "end": "1755510"
  },
  {
    "text": "the cost of the cluster is the cheapest and the in the chances of you getting interrupted are the lowest you can still",
    "start": "1755510",
    "end": "1761809"
  },
  {
    "text": "get interrupted but the chances of you getting interrupted are the lowest and this information is completely public",
    "start": "1761809",
    "end": "1767450"
  },
  {
    "text": "you remember the spot bit advisor that I showed you which shows you the percentage the chance is the probability of you getting interrupted in a certain",
    "start": "1767450",
    "end": "1775070"
  },
  {
    "text": "AC when you use a certain instance type it basically uses this information but uses it as an API so if you start one",
    "start": "1775070",
    "end": "1781909"
  },
  {
    "text": "day and you programmatically decide that I want these five instances they're all five you can have different EBS volumes",
    "start": "1781909",
    "end": "1787970"
  },
  {
    "text": "attached to them you can have different configuration for instance and I'm gonna use instance fleets to go and and run",
    "start": "1787970",
    "end": "1794419"
  },
  {
    "text": "that and on a day when the price of let's say one single instance shoots up because everybody is trying to use those",
    "start": "1794419",
    "end": "1799820"
  },
  {
    "text": "instance you will be able to diversify across those instances the other thing it does it is what if I do not have any",
    "start": "1799820",
    "end": "1808070"
  },
  {
    "text": "spot capacity in particular in in a particular AC on that particular day it also allows you to fall back to on",
    "start": "1808070",
    "end": "1815210"
  },
  {
    "text": "demand after a certain time out so your general spot pattern will give me a",
    "start": "1815210",
    "end": "1821539"
  },
  {
    "text": "thousand nodes amongst these choices and if you can't just switch me to on demand that is what the API does so with the",
    "start": "1821539",
    "end": "1830690"
  },
  {
    "text": "newer implementation of the instance bleeds and and also some of the new processes and implement new changes that",
    "start": "1830690",
    "end": "1836809"
  },
  {
    "text": "we have made you can see the cluster third time has also improved significantly I think overall our",
    "start": "1836809",
    "end": "1842030"
  },
  {
    "text": "cluster startup time has improved by about 45 percent since the starting of the year we continue to keep working on",
    "start": "1842030",
    "end": "1848510"
  },
  {
    "text": "this so you'll see faster and faster cluster startup time spot was a major reason why our sort of thanks for little",
    "start": "1848510",
    "end": "1853850"
  },
  {
    "text": "slower so we've changed our provisioning algorithm a certain bit now you can see the blue line was what we would see was",
    "start": "1853850",
    "end": "1861470"
  },
  {
    "text": "the all across our clusters and the green line is what we see now pretty much smoothened",
    "start": "1861470",
    "end": "1866779"
  },
  {
    "text": "out and I also significantly lower so",
    "start": "1866779",
    "end": "1871850"
  },
  {
    "start": "1871000",
    "end": "1950000"
  },
  {
    "text": "you so Instant fleets you can mix and match instances between markets you can say half of it I want on demand half of",
    "start": "1871850",
    "end": "1878570"
  },
  {
    "text": "it I want on spot I want from multiple different intercepts so on and so forth so the key",
    "start": "1878570",
    "end": "1884210"
  },
  {
    "text": "to using spot on AWS or the key to using spot an EMR is to diversify across",
    "start": "1884210",
    "end": "1889519"
  },
  {
    "text": "multiple instance types as trust me most of my customers who were actually doing",
    "start": "1889519",
    "end": "1896090"
  },
  {
    "text": "this really well on spot have orchestration that is built around EMR they're just exactly this we've also",
    "start": "1896090",
    "end": "1907730"
  },
  {
    "text": "done a significant amount of work which has been contributed back to open source and spark around decommissioning of spot",
    "start": "1907730",
    "end": "1913549"
  },
  {
    "text": "instances so when this when spot goes away it takes it gives you a two-minute warning and in that two-minute warning",
    "start": "1913549",
    "end": "1920480"
  },
  {
    "text": "we try to safely decommission a job or slightly we tried to remove the particular node so that spark can fail a",
    "start": "1920480",
    "end": "1927559"
  },
  {
    "text": "little bit more gracefully it's not necessary that always that two minutes is enough for spark to fail gracefully",
    "start": "1927559",
    "end": "1933320"
  },
  {
    "text": "but we try this has been contributed back to open node what we do is we blacklist the node so that no other jobs",
    "start": "1933320",
    "end": "1940220"
  },
  {
    "text": "can be taken no other jobs are submitted to that particular node and gracefully decommission the node using the",
    "start": "1940220",
    "end": "1948230"
  },
  {
    "text": "two-minute warning that spot gives up so these are all great best practices when you are running a transient cluster what",
    "start": "1948230",
    "end": "1955340"
  },
  {
    "start": "1950000",
    "end": "2006000"
  },
  {
    "text": "organizations tell us that this is great but I don't want my depth data team to",
    "start": "1955340",
    "end": "1960830"
  },
  {
    "text": "be continuously spinning up clusters I don't want to ship code that everybody needs to use - especially lines for",
    "start": "1960830",
    "end": "1966919"
  },
  {
    "text": "businesses and especially when you have lines of businesses where they are data scientists and analysts who do not need",
    "start": "1966919",
    "end": "1972830"
  },
  {
    "text": "to know the details about how we are spending up clusters are what a multi hone Nick really is all they care about",
    "start": "1972830",
    "end": "1978679"
  },
  {
    "text": "is logging into the clusters or using a something like Jupiter to submit jobs to",
    "start": "1978679",
    "end": "1985190"
  },
  {
    "text": "a particular cluster and their job is done they basically want to shut down the cluster without so how do we template this and how do we build this",
    "start": "1985190",
    "end": "1991580"
  },
  {
    "text": "in a way so that your entire organization can use the template so this year we release integration with",
    "start": "1991580",
    "end": "1998119"
  },
  {
    "text": "with EMR which is built we sorry integration with AWS Service Catalog that allows you to build a self-service",
    "start": "1998119",
    "end": "2005169"
  },
  {
    "text": "platform how many of you know what service catalog is a tableau a service catalog some of you so the edible",
    "start": "2005169",
    "end": "2012189"
  },
  {
    "start": "2006000",
    "end": "2134000"
  },
  {
    "text": "service catalog is essentially a portfolio of products that you can expose to your organization that allows",
    "start": "2012189",
    "end": "2017799"
  },
  {
    "text": "you to allows them to go pick a product and run that product a product can be a",
    "start": "2017799",
    "end": "2022899"
  },
  {
    "text": "cloud formation template or it can be a complete stack so imagine I have a lamp stack and I can expose a lamp stack to",
    "start": "2022899",
    "end": "2029739"
  },
  {
    "text": "my entire organization it also decouples the user of the stack that means the",
    "start": "2029739",
    "end": "2036039"
  },
  {
    "text": "consumer who is actually using the stack from service catalog which is actually provisioning it that means I don't need",
    "start": "2036039",
    "end": "2043179"
  },
  {
    "text": "to give all the users provisioning capabilities one of the things that we often hear from customers is I don't",
    "start": "2043179",
    "end": "2048908"
  },
  {
    "text": "want data scientists to have provisioning capabilities of ec2 instances I just want them to be able to",
    "start": "2048909",
    "end": "2054220"
  },
  {
    "text": "provision ec2 instances spin up to Jupiter notebooks and then run and jobs on top of the clusters I just run hive",
    "start": "2054220",
    "end": "2059829"
  },
  {
    "text": "jobs I don't want them to spin up hundreds and thousands of you instances you can template all of that in Service",
    "start": "2059829",
    "end": "2066069"
  },
  {
    "text": "Catalog and we'll show you a demo on this today as well so you can standardize your",
    "start": "2066069",
    "end": "2072279"
  },
  {
    "text": "deployments you could say that in my organization there are three standards of running EMR clusters one is a five",
    "start": "2072279",
    "end": "2078638"
  },
  {
    "text": "node cluster with these application these EBS volumes this configuration so on and so forth",
    "start": "2078639",
    "end": "2083739"
  },
  {
    "text": "you can import consistency and compliance that means you can enforce what tags need to be associated with it",
    "start": "2083739",
    "end": "2088749"
  },
  {
    "text": "you can enforce what costs need to be associated with it you can limit access you can also do tagging and security",
    "start": "2088749",
    "end": "2096099"
  },
  {
    "text": "controls all baked into one so nobody in your entire lines or businesses or your consumers don't need to know the nitty",
    "start": "2096099",
    "end": "2103150"
  },
  {
    "text": "gritties of running EMR clusters they can essentially just go to the service catalog spin up a product log into the",
    "start": "2103150",
    "end": "2109150"
  },
  {
    "text": "cluster and that's it for the consumer is also really simple because they'd really don't need to come to you to spin",
    "start": "2109150",
    "end": "2115480"
  },
  {
    "text": "up a cluster they can essentially go to the service catalog spin up the clock spin up the cluster they don't need ec2",
    "start": "2115480",
    "end": "2120489"
  },
  {
    "text": "permissions they know that they're working on a templated thing a templated product that has been vetted by you they",
    "start": "2120489",
    "end": "2127180"
  },
  {
    "text": "can automate deployment and it essentially becomes a one-stop shop you can where you can enable a lot of governance",
    "start": "2127180",
    "end": "2134189"
  },
  {
    "start": "2134000",
    "end": "2184000"
  },
  {
    "text": "so from an administrator point of view how does it work so you are the author you author a template this can be a",
    "start": "2134189",
    "end": "2140019"
  },
  {
    "text": "crowd formation template you can add parameters to the template so one common parameters is is where is your job where",
    "start": "2140019",
    "end": "2146259"
  },
  {
    "text": "is your source data where is your source file where do you want your output this can all be parameterised you create a",
    "start": "2146259",
    "end": "2152410"
  },
  {
    "text": "product you build a template and you release the product we'll also show you how you have versions of product you can",
    "start": "2152410",
    "end": "2157689"
  },
  {
    "text": "build build different layers on the product so on and so forth from a consumer point of view they go on",
    "start": "2157689",
    "end": "2163809"
  },
  {
    "text": "to AWS Service Catalog there is a portfolio of products inside there you choose a pet product and you instantiate",
    "start": "2163809",
    "end": "2169719"
  },
  {
    "text": "it pretty simple so what that does is it changes the complete interaction the",
    "start": "2169719",
    "end": "2174759"
  },
  {
    "text": "central date team is having with lines of businesses or with your consumers especially consumers who have different",
    "start": "2174759",
    "end": "2181449"
  },
  {
    "text": "levels of skill sets across the organization so for example here was the situation before when a data scientist",
    "start": "2181449",
    "end": "2187959"
  },
  {
    "start": "2184000",
    "end": "2256000"
  },
  {
    "text": "would come to an IT team and says give me a new cluster you provision a new cluster for them because they might not",
    "start": "2187959",
    "end": "2193390"
  },
  {
    "text": "really know how to do that or they might not want to do that and then you give them access to the cluster they log into",
    "start": "2193390",
    "end": "2200349"
  },
  {
    "text": "the cluster you have complex roles to define those and they get access to the cluster and I have to continuously",
    "start": "2200349",
    "end": "2205660"
  },
  {
    "text": "monitor or build programs that monitor whether the cluster was running toward did I need to shut it down is somebody",
    "start": "2205660",
    "end": "2211449"
  },
  {
    "text": "still running the cluster with the Service Catalog what changes is you have there is a one-time request where data",
    "start": "2211449",
    "end": "2217779"
  },
  {
    "text": "science has I want these libraries this version of SPARC a cluster with this",
    "start": "2217779",
    "end": "2222880"
  },
  {
    "text": "configuration this templates this security settings you build it as a template you expose it on the ADA Bluest",
    "start": "2222880",
    "end": "2228999"
  },
  {
    "text": "management console using the Service Catalog you can also integrate it with things like ServiceNow and the data",
    "start": "2228999",
    "end": "2235359"
  },
  {
    "text": "scientist now goes to the console it basically has a list of products they can use a product and they can expose",
    "start": "2235359",
    "end": "2241390"
  },
  {
    "text": "the product so let us show you a demo of Service Catalog integration with EMR for that in my team",
    "start": "2241390",
    "end": "2248640"
  },
  {
    "text": "all right good morning everybody sorry no problem just want to say a quick",
    "start": "2256240",
    "end": "2261770"
  },
  {
    "text": "thank you to everybody for coming out this morning a real pleasure to have you here and also thank you for being here this little weak for reinvent I know",
    "start": "2261770",
    "end": "2268100"
  },
  {
    "text": "it's possible that it may have been a little bit crazy but even as you know an internal AWS",
    "start": "2268100",
    "end": "2273380"
  },
  {
    "text": "employee I continue to be astounded and blown away by the breadth and scale at which we continue to release new services and it is truly mind-blowing",
    "start": "2273380",
    "end": "2280610"
  },
  {
    "text": "but enough of that thank you I let's go into the demo so what I want to show you today is using Service",
    "start": "2280610",
    "end": "2287000"
  },
  {
    "start": "2281000",
    "end": "2323000"
  },
  {
    "text": "Catalog and I want to give you the point of view from both an administrator from creating a new product in Service",
    "start": "2287000",
    "end": "2292310"
  },
  {
    "text": "Catalog and then as a user what would I would do to provision a new cluster myself so we've got our Service Catalog",
    "start": "2292310",
    "end": "2298220"
  },
  {
    "text": "here and I've created a portfolio inside the Service Catalog which will contain the different products that I want to",
    "start": "2298220",
    "end": "2303560"
  },
  {
    "text": "use I've also customized the Service Catalog a little bit with my beautiful EMR logo up in the left hand corner you",
    "start": "2303560",
    "end": "2309740"
  },
  {
    "text": "can even change the colors if you want to but if I go into the portfolio you can see I've got a couple different",
    "start": "2309740",
    "end": "2314990"
  },
  {
    "text": "products in there I've got a data science EMR and the data analyst EMR if we go into the products list as an admin",
    "start": "2314990",
    "end": "2322160"
  },
  {
    "text": "as well we can see these two products in here and the description of them so the",
    "start": "2322160",
    "end": "2327200"
  },
  {
    "start": "2323000",
    "end": "2375000"
  },
  {
    "text": "data analyst one we're gonna auto you know create a cluster with hives spark and hue for interactive queries on the",
    "start": "2327200",
    "end": "2333110"
  },
  {
    "text": "data science I'd maybe want to have some tensor flow on there some MX Net whatever else you want for your MLA I",
    "start": "2333110",
    "end": "2338710"
  },
  {
    "text": "advanced learning capabilities it's pretty easy to create a new product as Abhishek mentioned you can use a cloud",
    "start": "2338710",
    "end": "2344780"
  },
  {
    "text": "formation template so you have to go ahead and build that ahead of time so what I'd like to sit show is imagine",
    "start": "2344780",
    "end": "2350330"
  },
  {
    "text": "you're building a data Lake you signed up for a lake formation preview but it's not out yes you still need to build your own data Lake and maybe use Athena for a",
    "start": "2350330",
    "end": "2357320"
  },
  {
    "text": "lot of your ad hoc queries but you want to give Presto to your users to allow them to spin up their own presto cluster",
    "start": "2357320",
    "end": "2362330"
  },
  {
    "text": "and you want that cluster to auto scale throughout the day so when people come in at 8 a.m. and they're hitting their refresh button on their dashboard the",
    "start": "2362330",
    "end": "2369380"
  },
  {
    "text": "clusters already scaled up and ready for that level of queries and then at 5 o'clock it scales back down so I've got",
    "start": "2369380",
    "end": "2375440"
  },
  {
    "start": "2375000",
    "end": "2416000"
  },
  {
    "text": "a handy CloudFormation template that I just happened to create back here we can have parameters in the cloud formation",
    "start": "2375440",
    "end": "2381080"
  },
  {
    "text": "template for example the name of the cluster and the maximum size that we want the to allow the cluster to autoscale up to we've got a",
    "start": "2381080",
    "end": "2388130"
  },
  {
    "text": "simple resource in here for the EMR cluster and then here we specify you know all the things that we might want",
    "start": "2388130",
    "end": "2393410"
  },
  {
    "text": "to hide from the user that they don't really need to know about right we can specify the the V PC subnet ID in here",
    "start": "2393410",
    "end": "2399350"
  },
  {
    "text": "the ec2 key name the instance specific specific instance types and then we've",
    "start": "2399350",
    "end": "2404600"
  },
  {
    "text": "also got the auto scaling group in here and this just defines the policy that says okay based on some metrics on the",
    "start": "2404600",
    "end": "2410690"
  },
  {
    "text": "Presto cluster auto scale up in the morning and back down in the evening so pretty straightforward CloudFormation",
    "start": "2410690",
    "end": "2416600"
  },
  {
    "start": "2416000",
    "end": "2453000"
  },
  {
    "text": "template there back in service catalog we can upload that new product we'll create this a name for it here give it a",
    "start": "2416600",
    "end": "2425869"
  },
  {
    "text": "nice little description maybe a cheeky description and then we can say who it's",
    "start": "2425869",
    "end": "2431090"
  },
  {
    "text": "provided by in the next section here we can provide support details as well so if somebody runs into issues running",
    "start": "2431090",
    "end": "2436910"
  },
  {
    "text": "that cluster they know who to get in touch with and then we there upload that file from our local machine or we put it",
    "start": "2436910",
    "end": "2443000"
  },
  {
    "text": "on an s3 bucket and just provide that as the as the link there we can also have",
    "start": "2443000",
    "end": "2448790"
  },
  {
    "text": "different revisions in here so I'll just put an initial revision for now then we do that go ahead and click create and",
    "start": "2448790",
    "end": "2455750"
  },
  {
    "text": "that uploads the product end of the service catalog so in a second here that should show up after a little refresh",
    "start": "2455750",
    "end": "2460940"
  },
  {
    "text": "and now we've got our auto scaling presto cluster we still need to add this to the portfolio though so we go there",
    "start": "2460940",
    "end": "2466550"
  },
  {
    "text": "add that product to the portfolio and we'll just add it to our reinvent demo so you can have one product that adds",
    "start": "2466550",
    "end": "2472880"
  },
  {
    "text": "into multiple portfolios as well right maybe you have a sales portfolio on a marketing portfolio or a data team",
    "start": "2472880",
    "end": "2478460"
  },
  {
    "text": "portfolio you can mix and match all those different products great so now as an administrator I've got a few",
    "start": "2478460",
    "end": "2484670"
  },
  {
    "text": "different products in there and let's see what do user sees when they go into this so they go into their products list",
    "start": "2484670",
    "end": "2490130"
  },
  {
    "text": "and they see the same thing we just saw right and I can decide whether to scale or launch something up if I try to",
    "start": "2490130",
    "end": "2496100"
  },
  {
    "text": "launch that prèsto cluster you can go in there and hit next and you can see the",
    "start": "2496100",
    "end": "2501200"
  },
  {
    "text": "different parameters that I mentioned in the job are provided in here as well but for the purpose of this I actually want",
    "start": "2501200",
    "end": "2506600"
  },
  {
    "text": "to launch this nice data analyst EMR so maybe when they add data analyst and I've got a little spark script to do",
    "start": "2506600",
    "end": "2513380"
  },
  {
    "start": "2509000",
    "end": "2549000"
  },
  {
    "text": "some data conversion right pretty straightforward spark script you can see it's just taking an input location",
    "start": "2513380",
    "end": "2519380"
  },
  {
    "text": "in an output location and converting it to Park a pretty common thing when you're doing data analysis or data",
    "start": "2519380",
    "end": "2524750"
  },
  {
    "text": "engineering is did do that park a conversion right I'm gonna take a look at the Amazon toys reviews or Amazon",
    "start": "2524750",
    "end": "2530810"
  },
  {
    "text": "product reviews data set for toys I've got a nine month old at home and I'm a first-time dad at night I want to use some machine learning to try to figure",
    "start": "2530810",
    "end": "2536690"
  },
  {
    "text": "out what to buy for the little guy still figuring out this dad thing so this part converter script pretty straightforward",
    "start": "2536690",
    "end": "2542600"
  },
  {
    "text": "just gonna read that TSV file in and then write it back out to Parque in the s3 output location that I provide so I",
    "start": "2542600",
    "end": "2550190"
  },
  {
    "start": "2549000",
    "end": "2568000"
  },
  {
    "text": "go in I create my my data analyst cluster I can have different revisions",
    "start": "2550190",
    "end": "2555590"
  },
  {
    "text": "here as well all my of revisions are more functionally based but you could essentially specify different EMR",
    "start": "2555590",
    "end": "2561470"
  },
  {
    "text": "versions in here or different configurations that you want your users to be able to to choose I'm just gonna",
    "start": "2561470",
    "end": "2566720"
  },
  {
    "text": "select the latest revision again here's where we fill in our parameters so we're",
    "start": "2566720",
    "end": "2572120"
  },
  {
    "start": "2568000",
    "end": "2594000"
  },
  {
    "text": "gonna name the cluster I've given the option to allow my user to choose a CPU or a memory intensive cluster so this",
    "start": "2572120",
    "end": "2577700"
  },
  {
    "text": "hold behind-the-scenes I've kind of choose the right kind of nodes that we want to run on since I'm doing data conversion Aldine towards CPU and again",
    "start": "2577700",
    "end": "2584840"
  },
  {
    "text": "we could scale the cluster to whatever point we wanted to we could also auto terminate the EMR cluster so if I just",
    "start": "2584840",
    "end": "2590150"
  },
  {
    "text": "want to run the script and shut the cluster back down as soon as that script is done we could set that to true and",
    "start": "2590150",
    "end": "2595340"
  },
  {
    "start": "2594000",
    "end": "2613000"
  },
  {
    "text": "then I have different job types here so I'm gonna select SPARC and then I have a set of inputs there so those three",
    "start": "2595340",
    "end": "2603470"
  },
  {
    "text": "inputs those are essentially what's going to get passed to a spark submit command right so you got the PI spark script you've got the Tuohy's data set",
    "start": "2603470",
    "end": "2611000"
  },
  {
    "text": "and then the output location as well so I'll go back there hit next we could",
    "start": "2611000",
    "end": "2616730"
  },
  {
    "start": "2613000",
    "end": "2651000"
  },
  {
    "text": "enforce tags maybe for billing purposes or something like that and then we'll review everything and go ahead and click",
    "start": "2616730",
    "end": "2622850"
  },
  {
    "text": "Launch now behind the scenes this is taking that CloudFormation template filling it out with the parameters that",
    "start": "2622850",
    "end": "2628400"
  },
  {
    "text": "we provided as part of the Service Catalog and spinning up that CloudFormation stack you can see a link",
    "start": "2628400",
    "end": "2633410"
  },
  {
    "text": "to it right there if you really wanted to go over and take a look at that and again as Abhishek mentioned this is",
    "start": "2633410",
    "end": "2638930"
  },
  {
    "text": "something where I as a user don't need to have the permissions to actually spin up the EMR cluster right so this",
    "start": "2638930",
    "end": "2645500"
  },
  {
    "text": "behind-the-scenes could use a completely different role that that that user doesn't need anything to get into that",
    "start": "2645500",
    "end": "2651230"
  },
  {
    "start": "2651000",
    "end": "2680000"
  },
  {
    "text": "so that cluster is gonna go in provision and when it's done I've got it over here and my list and you can even provide",
    "start": "2651230",
    "end": "2658280"
  },
  {
    "text": "output parameters as well so that cluster spun up and now I provide the user with a link to maybe the EMR",
    "start": "2658280",
    "end": "2664310"
  },
  {
    "text": "resource manager or the spark history server so let's go take a look at that cluster right now here's my awesome",
    "start": "2664310",
    "end": "2670610"
  },
  {
    "text": "spark cluster and as we mentioned it's got a c5 nodes there so we decided to",
    "start": "2670610",
    "end": "2676340"
  },
  {
    "text": "run compute heavy and that was provisioned automatically behind the scenes if you look at the steps of that",
    "start": "2676340",
    "end": "2681440"
  },
  {
    "text": "cluster you can see the step got submitted to the cluster this is the the SPARC submit script that we pushed up to",
    "start": "2681440",
    "end": "2688100"
  },
  {
    "text": "that cluster and ran the job it took a couple minutes and that's that now one of the nice things about this as well is",
    "start": "2688100",
    "end": "2694340"
  },
  {
    "text": "we can go back to Service Catalog and you say okay I ran my ran my script and",
    "start": "2694340",
    "end": "2699770"
  },
  {
    "text": "now I want to run another script so I'm gonna update my provisioned product I'll choose that latest revision there again",
    "start": "2699770",
    "end": "2706280"
  },
  {
    "start": "2703000",
    "end": "2727000"
  },
  {
    "text": "and we'll go and update the parameters and let's say now that I have that data I want to run a hive query over it to",
    "start": "2706280",
    "end": "2712730"
  },
  {
    "text": "kind of calculate the top toys over the course of that that data set right so we've got this really simple hive",
    "start": "2712730",
    "end": "2718400"
  },
  {
    "text": "scripts that again takes an input location an output location and I'm going to provide those parameters to the",
    "start": "2718400",
    "end": "2723850"
  },
  {
    "text": "to the cloud formation or to the service catalog template as well we'll go ahead we'll hit next and update the the",
    "start": "2723850",
    "end": "2731000"
  },
  {
    "start": "2727000",
    "end": "2769000"
  },
  {
    "text": "Service Catalog product and behind the scenes again this is submitting the CloudFormation template and Confirmation is smart enough to just do a diff",
    "start": "2731000",
    "end": "2736910"
  },
  {
    "text": "between those two versions of the templates that says okay cool the only difference here is I've got a new step so if I go back to my EMR cluster and",
    "start": "2736910",
    "end": "2743780"
  },
  {
    "text": "the demo gods are lovely and they are we can see that now we've got a pending step in that cluster and this is my hive",
    "start": "2743780",
    "end": "2749930"
  },
  {
    "text": "script that I used to you know essentially do that analysis if I look",
    "start": "2749930",
    "end": "2755390"
  },
  {
    "text": "out on s3 I've got my spark output that I converted there my Amazon product",
    "start": "2755390",
    "end": "2760520"
  },
  {
    "text": "reviews dataset and lovely snappy park' format and there's also some hive output",
    "start": "2760520",
    "end": "2766040"
  },
  {
    "text": "that's getting written out there right now and that would be the hive output so",
    "start": "2766040",
    "end": "2771710"
  },
  {
    "start": "2769000",
    "end": "2793000"
  },
  {
    "text": "that's a good way that I can also you know I as a user didn't have to provision that EMR cluster didn't have",
    "start": "2771710",
    "end": "2776840"
  },
  {
    "text": "to log into it just submitted a few things if I wanted to spend up a completely different hive cluster as well",
    "start": "2776840",
    "end": "2782060"
  },
  {
    "text": "maybe I chose a memory heavy cluster I can do that as well and so as a user it's really easy for me to go",
    "start": "2782060",
    "end": "2788510"
  },
  {
    "text": "in here select those things and spin up those clusters without you know too much of a worry if you want something really",
    "start": "2788510",
    "end": "2795770"
  },
  {
    "text": "really end to end there's an awesome post of course on the AWS Big Data blog about using code pipeline and Service",
    "start": "2795770",
    "end": "2801799"
  },
  {
    "text": "Catalog to deploy and and a simple example that I have up here no demo god",
    "start": "2801799",
    "end": "2808010"
  },
  {
    "text": "for me today there we go sample spark pipeline so this is some source code that's in a repository when",
    "start": "2808010",
    "end": "2814970"
  },
  {
    "start": "2809000",
    "end": "2826000"
  },
  {
    "text": "that code and when any commits come in this triggers a code pipeline and essentially goes through does the build",
    "start": "2814970",
    "end": "2820280"
  },
  {
    "text": "does a QA deploy does a test of that and this is all spinning up Service Catalog stuff in cloud formation templates in",
    "start": "2820280",
    "end": "2826309"
  },
  {
    "start": "2826000",
    "end": "2851000"
  },
  {
    "text": "the background and then you've got a live test approval here in the code pipeline as well you can even hit review",
    "start": "2826309",
    "end": "2831700"
  },
  {
    "text": "that's good to me and then this will go out build a new stack using the service",
    "start": "2831700",
    "end": "2836780"
  },
  {
    "text": "catalog again the user doesn't need any access to EMR and you know you can actually do that entire deploy essentially in a serverless fashion",
    "start": "2836780",
    "end": "2843680"
  },
  {
    "text": "using code pipeline pretty awesome stuff that's it for now I think without further ado o invite I'll check back up",
    "start": "2843680",
    "end": "2850339"
  },
  {
    "text": "to continue that was awesome thank you",
    "start": "2850339",
    "end": "2858549"
  },
  {
    "text": "so we talked a lot about the transient cluster workload let's talk a little bit about the persistent user persistent",
    "start": "2858849",
    "end": "2865970"
  },
  {
    "start": "2865000",
    "end": "2975000"
  },
  {
    "text": "cluster workload the most important one and the most often used was a use case for persistent cluster is we see with",
    "start": "2865970",
    "end": "2873170"
  },
  {
    "text": "Jupiter notebooks or with any kind of notebooks where data scientists want to use a notebook for maybe ml for later",
    "start": "2873170",
    "end": "2879950"
  },
  {
    "text": "engineering purposes and they want to connect it to a cluster we see a constant cluster which is multi-talented",
    "start": "2879950",
    "end": "2885890"
  },
  {
    "text": "by multiple notebooks and that can scale up and down what users have told us is",
    "start": "2885890",
    "end": "2891289"
  },
  {
    "text": "that it's really hard to set up Jupiter hub it's really hard to set up Jupiter on top of notebooks and there is a lot",
    "start": "2891289",
    "end": "2896750"
  },
  {
    "text": "of muck around using Jupiter today and allowing all the data scientists within your organization to use a simple to use",
    "start": "2896750",
    "end": "2904329"
  },
  {
    "text": "Jupiter notebook on top of EMR so we're so very recently we released something",
    "start": "2904329",
    "end": "2910670"
  },
  {
    "text": "called EMR notebooks which is a fully managed notebook environment built on",
    "start": "2910670",
    "end": "2917220"
  },
  {
    "text": "Jupiter will talk based on open source Jupiter so we'll talk a little bit about that we'll also see to show you another",
    "start": "2917220",
    "end": "2923310"
  },
  {
    "text": "demo of the Jupiter hub node but the core fundamental around running a persistent workload is again scale up",
    "start": "2923310",
    "end": "2930690"
  },
  {
    "text": "and scale down EMR already provides you native auto scaling policies you can take you can take metrics that are",
    "start": "2930690",
    "end": "2938430"
  },
  {
    "text": "coming from yarn are coming from spark that are aggregated across the cluster you can take metrics that coming from",
    "start": "2938430",
    "end": "2944369"
  },
  {
    "text": "the cluster itself that cluster hardware that are aggregated you can trigger cloud watch alerts and we using cloud",
    "start": "2944369",
    "end": "2949770"
  },
  {
    "text": "word alerts you can call the auto scaling cluster don't forget you can also use spot when you're using auto",
    "start": "2949770",
    "end": "2955710"
  },
  {
    "text": "scaling cluster it becomes a really good for a way to save costs on a long-running cluster where you have the",
    "start": "2955710",
    "end": "2962640"
  },
  {
    "text": "on-demand clusters running using a reserved instance so that's about sixty to seventy percent discount and the",
    "start": "2962640",
    "end": "2968310"
  },
  {
    "text": "scale-up using spot instances so my safely advice on that is scale up and",
    "start": "2968310",
    "end": "2973980"
  },
  {
    "text": "down as much as you can so we it's about three weeks ago we released a completely",
    "start": "2973980",
    "end": "2980070"
  },
  {
    "start": "2975000",
    "end": "3040000"
  },
  {
    "text": "new part of e mark all EMR notebooks essentially we are building a notebook environment that is decoupled from the",
    "start": "2980070",
    "end": "2986460"
  },
  {
    "text": "cluster it's based upon open source Jupiter you'll get the same Jupiter notebook you have to attach a notebook",
    "start": "2986460",
    "end": "2993690"
  },
  {
    "text": "to a cluster to use the notebook or you can also provision a completely new cluster directly from the notebook",
    "start": "2993690",
    "end": "2999720"
  },
  {
    "text": "currently it's only available on the console but we will also make it available by API you can have multiple",
    "start": "2999720",
    "end": "3005990"
  },
  {
    "text": "notebooks or multiple users attach those notebooks to a single cluster that can auto scale that's also possible you can",
    "start": "3005990",
    "end": "3012740"
  },
  {
    "text": "also attach a notebook from a cluster let's say your dev cluster and then reattach detach and then reattach it to",
    "start": "3012740",
    "end": "3020119"
  },
  {
    "text": "a prod cluster you can save the notebooks to s3 we also gonna add",
    "start": "3020119",
    "end": "3025630"
  },
  {
    "text": "integrations with come and get repositories and you can also do tag based permissions that means you could",
    "start": "3025630",
    "end": "3031849"
  },
  {
    "text": "restrict access to the notebook using tags and you can also restrict access to which notebook connects to which cluster",
    "start": "3031849",
    "end": "3038690"
  },
  {
    "text": "using tags so we're very excited about this idea of notebook the notebook is",
    "start": "3038690",
    "end": "3044030"
  },
  {
    "start": "3040000",
    "end": "3056000"
  },
  {
    "text": "completely managed it's you don't have to manage any instances on the notebook you but remember to execute any code and",
    "start": "3044030",
    "end": "3050690"
  },
  {
    "text": "the notebook you will have to attach the notebook to a running cluster or you can directly provision a cluster as well one",
    "start": "3050690",
    "end": "3057270"
  },
  {
    "text": "of the use cases that we see here is with the auto scaling policy that we imagine customers will take a",
    "start": "3057270",
    "end": "3062520"
  },
  {
    "text": "long-running cluster have multiple notebooks associated with the cluster where the cluster would scale up during",
    "start": "3062520",
    "end": "3067890"
  },
  {
    "text": "the day when more jobs are being tested on the cluster and scaled down during a night during the night when the not who",
    "start": "3067890",
    "end": "3073350"
  },
  {
    "text": "are the jobs are being around on a cluster a pretty good example of our water scaling works I took this from one",
    "start": "3073350",
    "end": "3079920"
  },
  {
    "start": "3075000",
    "end": "3094000"
  },
  {
    "text": "of our users on Twitter found it really beautiful that on the left-hand side you can see ganglia graphs of how the cluster is gradually scaling up as the",
    "start": "3079920",
    "end": "3086850"
  },
  {
    "text": "load as the load is increasing on a cluster and then slowly scaling down when the load starts to decrease in the",
    "start": "3086850",
    "end": "3092610"
  },
  {
    "text": "cluster so mr scales in at yarn task completion because scaling in is also as",
    "start": "3092610",
    "end": "3099540"
  },
  {
    "start": "3094000",
    "end": "3131000"
  },
  {
    "text": "important as scale up you can also scale up your EBS volumes or you store your storage so if you're using HDFS which is",
    "start": "3099540",
    "end": "3106860"
  },
  {
    "text": "fine if you're using HDFS if you're using HDFS you can scale out and the HDFS be careful when you're scaling down",
    "start": "3106860",
    "end": "3113250"
  },
  {
    "text": "HDFS hdf that does rebalance when it scales down so that's in important to",
    "start": "3113250",
    "end": "3118350"
  },
  {
    "text": "kind of note you can we've also done several contributions where we can D as",
    "start": "3118350",
    "end": "3123480"
  },
  {
    "text": "I said we can scale down spark slowly or gracefully and we have contributed this",
    "start": "3123480",
    "end": "3129330"
  },
  {
    "text": "back to open source as well one of the things that customers ask us often is when I run a persistent cluster what",
    "start": "3129330",
    "end": "3136800"
  },
  {
    "start": "3131000",
    "end": "3189000"
  },
  {
    "text": "happens when a master node goes down we are working on what was good what we're calling multi master support for email",
    "start": "3136800",
    "end": "3142950"
  },
  {
    "text": "applications that's coming very soon depending upon what application that",
    "start": "3142950",
    "end": "3147960"
  },
  {
    "text": "you're using your there will be different kinds of actions that you can take when a master node goes down so for",
    "start": "3147960",
    "end": "3154050"
  },
  {
    "text": "example you can see this in this chart when yarn goes down if you're using yarn you will have an active standby with",
    "start": "3154050",
    "end": "3160680"
  },
  {
    "text": "automated failover and recovery but if you're using Libby Libby won't automatically fail but really can",
    "start": "3160680",
    "end": "3166800"
  },
  {
    "text": "recover really has options to recover so when the master will have three masters so that's why we're calling it multi",
    "start": "3166800",
    "end": "3172350"
  },
  {
    "text": "master and when Libby kind of fails on one it basically can recover you'll have to start up Libby on another no and at",
    "start": "3172350",
    "end": "3177960"
  },
  {
    "text": "the master node but you'll be able to recover it so that's one of the things that the Lord five customers have asked us when they're running long",
    "start": "3177960",
    "end": "3183460"
  },
  {
    "text": "persistent clusters to give them an ability to use a chi on that on the",
    "start": "3183460",
    "end": "3188620"
  },
  {
    "text": "masternode we've also seen customers ask us for reconfiguration where you'll be able to online reconfigure your EMR",
    "start": "3188620",
    "end": "3195940"
  },
  {
    "start": "3189000",
    "end": "3235000"
  },
  {
    "text": "cluster or change the configuration of your EMR cluster without shutting down and starting a completely new cluster so",
    "start": "3195940",
    "end": "3202240"
  },
  {
    "text": "that's also an API that is coming you will be able to do a rolling restart of the data nodes to prevent data loss and",
    "start": "3202240",
    "end": "3208450"
  },
  {
    "text": "the and one more thing with this is if you if you try to run a configuration and the configuration fails it",
    "start": "3208450",
    "end": "3215020"
  },
  {
    "text": "automatically defaults to the previous configuration so all of this is coming fairly soon but let me show you another",
    "start": "3215020",
    "end": "3221830"
  },
  {
    "text": "demo of EMR notebooks and for that I invite Damon",
    "start": "3221830",
    "end": "3228450"
  },
  {
    "start": "3235000",
    "end": "3249000"
  },
  {
    "text": "alright I'm back so yeah we'll take a look at notebooks if you're at all familiar with EMR which I'm assuming a",
    "start": "3236089",
    "end": "3242449"
  },
  {
    "text": "lot of you are on the left hand side you'll notice this new little section there that's just subtly titled notebooks so if you go in there you can",
    "start": "3242449",
    "end": "3248390"
  },
  {
    "text": "create a notebook and it's pretty straightforward we'll just name it you know Damon's notebook add a description and",
    "start": "3248390",
    "end": "3253969"
  },
  {
    "start": "3249000",
    "end": "3291000"
  },
  {
    "text": "then here we can either choose an existing cluster or we can even create a cluster inside the notebook interface if",
    "start": "3253969",
    "end": "3260689"
  },
  {
    "text": "we do that we just get some basic options to create an AMR cluster it takes whatever the most recent release",
    "start": "3260689",
    "end": "3266509"
  },
  {
    "text": "of EMR is and adds on the necessary applications in order to spin that cluster up you can choose your instance",
    "start": "3266509",
    "end": "3273410"
  },
  {
    "text": "type in this right there as well so maybe you want that P 316 Excel for your favorite GPU applications and and then",
    "start": "3273410",
    "end": "3279949"
  },
  {
    "text": "you could just go ahead and hit start you can also provide tags again maybe for billing purposes or whatnot by",
    "start": "3279949",
    "end": "3286130"
  },
  {
    "text": "default each EMR notebook is tagged to the creator of that notebook so let's",
    "start": "3286130",
    "end": "3292369"
  },
  {
    "text": "switch back to a different account here and what we're gonna do we're actually gonna choose an existing cluster to attach this notebook to so again we",
    "start": "3292369",
    "end": "3299869"
  },
  {
    "text": "could put all that stuff in there and if we want to choose where the notebook eventually gets saved to on s3 we could",
    "start": "3299869",
    "end": "3305269"
  },
  {
    "text": "do that as well but for now we'll just leave that in the default location so we'll go ahead will create that notebook",
    "start": "3305269",
    "end": "3311359"
  },
  {
    "start": "3309000",
    "end": "3348000"
  },
  {
    "text": "behind the scene this is a provisioning that new jupiter notebook and with that comes up in the next second here when",
    "start": "3311359",
    "end": "3318349"
  },
  {
    "text": "you get into that notebook you can start writing your your code in there so when you get in there and that first time you",
    "start": "3318349",
    "end": "3324380"
  },
  {
    "text": "execute the notebook what's going to happen is the notebook is actually going to connect to Apache live II on the EMR",
    "start": "3324380",
    "end": "3329660"
  },
  {
    "text": "cluster and start up a spark application in the EMR cluster so that first command you run on there might take a little bit",
    "start": "3329660",
    "end": "3334939"
  },
  {
    "text": "of time while it goes back to the cluster starts that up but once it does you're good to go and right here you can",
    "start": "3334939",
    "end": "3341150"
  },
  {
    "text": "also get some information that comes back from that and in terms of the yarn application ID maybe you need to do some",
    "start": "3341150",
    "end": "3346309"
  },
  {
    "text": "debugging or dive into the logs or something but on that cluster back there already",
    "start": "3346309",
    "end": "3352189"
  },
  {
    "start": "3348000",
    "end": "3386000"
  },
  {
    "text": "opened up as well so if we open that up we get that clean slate cluster back there or notebook but now we're gonna do",
    "start": "3352189",
    "end": "3359239"
  },
  {
    "text": "some a little bit of data science C type stuff so I mentioned I've got that toys data set and I want to go back there and",
    "start": "3359239",
    "end": "3365839"
  },
  {
    "text": "do the analysis on the toys data set to figure out what I should buy my 9-month I'm a nine-month-old so when",
    "start": "3365839",
    "end": "3371820"
  },
  {
    "text": "you execute commands in here again this is going back to Livi and communicating with the EMR cluster and we can see that",
    "start": "3371820",
    "end": "3377940"
  },
  {
    "text": "we immediately get back some output we went and read in that toys data set and printed out the schema so we know what we're working with we can do a simple",
    "start": "3377940",
    "end": "3384330"
  },
  {
    "text": "count on that data as well when this goes back to the EMR cluster we also get some feedback in the notebook interface",
    "start": "3384330",
    "end": "3390960"
  },
  {
    "start": "3386000",
    "end": "3402000"
  },
  {
    "text": "in terms of what's happening so we know that you know we went back we just had these 10 tasks that we ran and they",
    "start": "3390960",
    "end": "3397200"
  },
  {
    "text": "finished pretty quickly and we have maybe about 5 million reviews that we need to do some analysis of I'll do a",
    "start": "3397200",
    "end": "3403110"
  },
  {
    "text": "little bit of data shuffling here I want to take that data repartition it so I can redistribute it across my cluster a little bit better and then I want to do",
    "start": "3403110",
    "end": "3410160"
  },
  {
    "text": "a quick calculation of the top toy so I can see what to buy if we go back to that cluster that this is running on we",
    "start": "3410160",
    "end": "3418380"
  },
  {
    "text": "can also see that we've got master Corrin tasks nodes so my master in core nodes I put on our fives and then my",
    "start": "3418380",
    "end": "3424470"
  },
  {
    "text": "task nodes I put on c5 with spot so as different notebooks attach this cluster those that those task notes can scale up",
    "start": "3424470",
    "end": "3431760"
  },
  {
    "text": "to about 10 nodes and scale back down to 0 nodes so kind of scales dynamically throughout the day if I go back there",
    "start": "3431760",
    "end": "3438240"
  },
  {
    "start": "3437000",
    "end": "3460000"
  },
  {
    "text": "I've already got some data back and it looks like cards against humanity' is definitely what I should buy my 9-month",
    "start": "3438240",
    "end": "3443430"
  },
  {
    "text": "fold for for christmas so maybe we need to filter this data a little bit right",
    "start": "3443430",
    "end": "3449609"
  },
  {
    "text": "let's do some kids rating so we'll just do a simple we're query there we'll filter anything out that doesn't contain",
    "start": "3449609",
    "end": "3455130"
  },
  {
    "text": "baby or infant let's let's scope this down a little bit right we'll go ahead we'll run this again and as we saw",
    "start": "3455130",
    "end": "3461609"
  },
  {
    "start": "3460000",
    "end": "3476000"
  },
  {
    "text": "before we get the spark job progress so if we look at that again we can see the different tasks as they're progressing",
    "start": "3461609",
    "end": "3467580"
  },
  {
    "text": "through essentially in real time we repartition this to 200 different partitions and you can see it'll fly",
    "start": "3467580",
    "end": "3473520"
  },
  {
    "text": "through those pretty quickly as well as it goes through and computes across the cluster and we've got our output and",
    "start": "3473520",
    "end": "3478619"
  },
  {
    "start": "3476000",
    "end": "3493000"
  },
  {
    "text": "this definitely seems a little bit better a little bit more age-appropriate for me ironically enough as I was",
    "start": "3478619",
    "end": "3484500"
  },
  {
    "text": "working on this demo this VTech sit-to-stand learning walker it's a little thing like the kid can learn to",
    "start": "3484500",
    "end": "3489960"
  },
  {
    "text": "walk on just bought that for my 9 month old about a month ago so it looks like I'm kind of on the right track here so",
    "start": "3489960",
    "end": "3495990"
  },
  {
    "start": "3493000",
    "end": "3516000"
  },
  {
    "text": "if I wanted to I could continue doing some of my ml analysis maybe take this Product ID and find other users",
    "start": "3495990",
    "end": "3502099"
  },
  {
    "text": "I reviewed that product favorably and then what else did they buy so super",
    "start": "3502099",
    "end": "3507229"
  },
  {
    "text": "quick demo of how notebooks work pretty nice interface allows you to just jump in there and be able to use notebooks on",
    "start": "3507229",
    "end": "3513380"
  },
  {
    "text": "on EMR really quickly of course thank you what opposed the demo gods are",
    "start": "3513380",
    "end": "3523670"
  },
  {
    "start": "3516000",
    "end": "3600000"
  },
  {
    "text": "pleased that you did so that concludes our presentation III what we're trying to do here was",
    "start": "3523670",
    "end": "3530390"
  },
  {
    "text": "give you a sense of what we have released in the year and how kind of it blends into our story of what",
    "start": "3530390",
    "end": "3536450"
  },
  {
    "text": "architectures do we see customers running there's some new stuff around notebooks there's some stuff that is coming around reconfiguration and h-a",
    "start": "3536450",
    "end": "3543279"
  },
  {
    "text": "and that we are open for questions if you have any about two minutes yeah",
    "start": "3543279",
    "end": "3552829"
  },
  {
    "text": "question",
    "start": "3552829",
    "end": "3555099"
  },
  {
    "text": "it depends I mean it's not necessary that those fully managed services do depend on EMR so like Athena doesn't",
    "start": "3578050",
    "end": "3586330"
  },
  {
    "text": "show you what it really depends on so and they're also I managed both attend an EMR and Athena has a choice of in a",
    "start": "3586330",
    "end": "3594520"
  },
  {
    "text": "tableau as teams work independently so they have a choice of how quickly they can take it and they can write it they",
    "start": "3594520",
    "end": "3601330"
  },
  {
    "text": "have different kinds of challenges about how quickly they can take a patch so for example just being in the Athena and the",
    "start": "3601330",
    "end": "3608020"
  },
  {
    "text": "EMR team with EMR it's fairly easy to get a new version out right because you're opting into the new version yourself right you you explicitly say I",
    "start": "3608020",
    "end": "3616000"
  },
  {
    "text": "want to use 518 or 519 however on Athena there's a single version so there is a",
    "start": "3616000",
    "end": "3621610"
  },
  {
    "text": "lot of testing and regression and that analysis I mean regression goes on in both ways but in Athena it's a lot more",
    "start": "3621610",
    "end": "3629040"
  },
  {
    "text": "it's a lot more difficult to kind of push near in your versions also there are a lot more careful about adding new",
    "start": "3629040",
    "end": "3634750"
  },
  {
    "text": "versions to this but mostly we as we assume that all the products if you make an improvement in spark or presto or",
    "start": "3634750",
    "end": "3642370"
  },
  {
    "text": "hive all the private will percolate through all the particular products generally I would say 30 to 30 days to a",
    "start": "3642370",
    "end": "3651280"
  },
  {
    "text": "couple of six to eight weeks as well expect",
    "start": "3651280",
    "end": "3656609"
  },
  {
    "text": "I think s3n defaults to the normal protocol but you don't need to use s3 is",
    "start": "3663770",
    "end": "3669240"
  },
  {
    "text": "the open source version so if you just say s3 it'll that should be all fine you don't need to specify I think n goes to",
    "start": "3669240",
    "end": "3676290"
  },
  {
    "text": "s3 a only I've been told that I need we need to vacate here at 10:15 because for",
    "start": "3676290",
    "end": "3682530"
  },
  {
    "text": "the next session so we'll be here outside taking your questions",
    "start": "3682530",
    "end": "3687980"
  }
]