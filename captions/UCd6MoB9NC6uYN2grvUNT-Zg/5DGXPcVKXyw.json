[
  {
    "text": "good afternoon everybody uh thanks for coming to the session the session is deep dive on Amazon EMR best practices",
    "start": "2080",
    "end": "8719"
  },
  {
    "text": "and design patterns my name is John Fritz I'm senior product manager for Amazon EMR and I'm joined by Naveen from",
    "start": "8719",
    "end": "14719"
  },
  {
    "text": "assurion who's a seni senior principal architect um actually before we get started how many people here uh run Hado",
    "start": "14719",
    "end": "21000"
  },
  {
    "text": "per Spark by by a show of hands fair amount that's good um how many folks run it on AWS",
    "start": "21000",
    "end": "27400"
  },
  {
    "text": "today in fair amount and then how many folks in the audience run it on EMR",
    "start": "27400",
    "end": "33879"
  },
  {
    "text": "great so um real quick we'll go over an overview of the presentation uh we'll start out with a quick overview of the",
    "start": "33879",
    "end": "40440"
  },
  {
    "text": "Apache ecosystem available on EMR it might be a refresher for some of you who currently use it but we'll go through",
    "start": "40440",
    "end": "46039"
  },
  {
    "text": "that quickly um then we'll talk a little about using EMR with S3 and other AWS Services quick overview on kind of just",
    "start": "46039",
    "end": "52440"
  },
  {
    "text": "a common practices on how people are connecting um these Hadoop applications running on EMR to the various data",
    "start": "52440",
    "end": "58480"
  },
  {
    "text": "stores in AWS um then we'll go over a few slides on just some best practices around securing",
    "start": "58480",
    "end": "64320"
  },
  {
    "text": "your Hadoop stack both from an IM perspective an application off perspective and an encryption",
    "start": "64320",
    "end": "69560"
  },
  {
    "text": "perspective as well and then a quick uh end on lowering cost with autoscaling which is a new feature that we uh",
    "start": "69560",
    "end": "75439"
  },
  {
    "text": "launched a couple weeks ago um and spot instances as well and then I'll hand the the mic over to naen who will talk um",
    "start": "75439",
    "end": "81479"
  },
  {
    "text": "for about the second half of the presentation on building a data lake at a Shuan using EMR S3 and a variety of",
    "start": "81479",
    "end": "87119"
  },
  {
    "text": "other services if we have any time at the end we'll take a few questions so um here's a quick graph we actually",
    "start": "87119",
    "end": "93759"
  },
  {
    "text": "update this uh diagram every time we do a release we release a new version of EMR around every five weeks or so um",
    "start": "93759",
    "end": "100840"
  },
  {
    "text": "showing actually from about a year ago emr4 uh we relaunched with EMR 4.0 and had uh four or five applications uh fast",
    "start": "100840",
    "end": "108320"
  },
  {
    "text": "forward a year with EMR 5.2 which we released last week or two weeks ago and uh we've got I think over 15 open source",
    "start": "108320",
    "end": "115200"
  },
  {
    "text": "projects and typically the latest version of each actually with spark we were a couple days behind the spark 2 release spark minor versions as well",
    "start": "115200",
    "end": "122119"
  },
  {
    "text": "we're very close behind open source um but for those who don't use EMR a quick kind of view of how we think about the",
    "start": "122119",
    "end": "128599"
  },
  {
    "text": "stack of applications we have at the base layer um we have the data stores typically customers are using Amazon S3",
    "start": "128599",
    "end": "136000"
  },
  {
    "text": "as their input and output data set we'll go over that a little bit more in detail about why we see that um in a bit but we",
    "start": "136000",
    "end": "141440"
  },
  {
    "text": "still actually install hdfs on your clusters as well if you needed to say store some temporary data um or had a",
    "start": "141440",
    "end": "146959"
  },
  {
    "text": "reason why you needed to store data locally on the cluster um from there we use yarn and we install yarn and",
    "start": "146959",
    "end": "152840"
  },
  {
    "text": "actually the EMR cluster is made up of core nodes and task nodes and the master node a core node runs node manager some",
    "start": "152840",
    "end": "159200"
  },
  {
    "text": "of the other Damons and hdfs data nodes the task nodes do not so it makes it easier to scale up and down where if you",
    "start": "159200",
    "end": "164560"
  },
  {
    "text": "do have data in hdfs you don't have to deal with data node rebalancing um we also have hbase um you",
    "start": "164560",
    "end": "171519"
  },
  {
    "text": "know no SQL uh on Hadoop and Phoenix which is SQL over hbase if you needed",
    "start": "171519",
    "end": "176560"
  },
  {
    "text": "say a SQL interface over your data in hbas that doesn't run on yarn and the same thing with Presto which we",
    "start": "176560",
    "end": "182280"
  },
  {
    "text": "typically uh see customers using for low latency SQL on S3 and actually for those of you who don't know Presto it was born",
    "start": "182280",
    "end": "187959"
  },
  {
    "text": "out of Facebook um Netflix I think has a presentation tomorrow talking about how they run a 25 pyte data warehouse using",
    "start": "187959",
    "end": "193560"
  },
  {
    "text": "Presto it's a very powerful tool for low latency SQL uh on top of yarn we've got a bunch",
    "start": "193560",
    "end": "199400"
  },
  {
    "text": "of Frameworks as well that utilize yarn for resource management we have uh map produce which is still people using but",
    "start": "199400",
    "end": "206080"
  },
  {
    "text": "it's more of a legacy U application uh but more for batch Analytics uh TZ which now is the default for Hive",
    "start": "206080",
    "end": "213040"
  },
  {
    "text": "2 on EMR 5 um TZ is a uh you know creates a dag instead of using map ruce",
    "start": "213040",
    "end": "218840"
  },
  {
    "text": "uh uh specifically and uh gets better performance utilizes memory and T sessions a little bit to improve",
    "start": "218840",
    "end": "225319"
  },
  {
    "text": "performance on say faster jobs without a lot of spin-up time uh spark which many of you guys seems like are running today",
    "start": "225319",
    "end": "231640"
  },
  {
    "text": "for in memory machine learning that sort of thing and then we actually just released support for Apache Flink uh",
    "start": "231640",
    "end": "237680"
  },
  {
    "text": "about a month ago and Flink is a newer stream processing framework um people who have been running storm it's more",
    "start": "237680",
    "end": "243360"
  },
  {
    "text": "similar to a storm likee experience you can you know process every record you can do some cool things about if you get",
    "start": "243360",
    "end": "248840"
  },
  {
    "text": "out of order events changing it by event time and other things like that it actually has a great batch interface as",
    "start": "248840",
    "end": "254120"
  },
  {
    "text": "well so we're excited to see uh that application grow and we have it today and then on top of that obviously you",
    "start": "254120",
    "end": "259160"
  },
  {
    "text": "have Hive Pig uh spark ml spark SQL spark streaming and all of that on top of those other Frameworks also would be",
    "start": "259160",
    "end": "266000"
  },
  {
    "text": "remiss not to mention we have a bunch of uis and actually we'll talk about that in a minute of some of the uis that are",
    "start": "266000",
    "end": "271039"
  },
  {
    "text": "available on the cluster um so here's a quick diagram of yarn um so I've",
    "start": "271039",
    "end": "276120"
  },
  {
    "text": "mentioned yarn is uh a core resource manager for a lot of the apps we have on EMR in this diagram you can see this is",
    "start": "276120",
    "end": "282680"
  },
  {
    "text": "how uh yarn might run a spark application um and we like yarn because",
    "start": "282680",
    "end": "288440"
  },
  {
    "text": "uh you can run a variety of different Frameworks on the same cluster if you say you have a multi-tenant use case and",
    "start": "288440",
    "end": "294400"
  },
  {
    "text": "uh yarn does a really great job of managing all of those um uh different applications trading off resources",
    "start": "294400",
    "end": "301000"
  },
  {
    "text": "between the two um and then runs you know containers on all the slave nodes with the right processes for spark it",
    "start": "301000",
    "end": "307280"
  },
  {
    "text": "would be executors for TZ it would be TZ containers um and so on and so forth um",
    "start": "307280",
    "end": "313039"
  },
  {
    "text": "also yarn uh yarn supports cros so you can enable Kos on those clusters uh a",
    "start": "313039",
    "end": "318280"
  },
  {
    "text": "deep dive yarn um has a bunch of different schedulers by default EMR uses the capacity scheduler um now we have",
    "start": "318280",
    "end": "325479"
  },
  {
    "text": "one Q by default so when you submit a job EMR will bring it into that queue and run it however let's say that you had a bunch",
    "start": "325479",
    "end": "331600"
  },
  {
    "text": "of different types of workloads and some were high priority and some weren't you can portion off different uh resources",
    "start": "331600",
    "end": "337840"
  },
  {
    "text": "on the cluster for different cues and priorities of each queue to say look if a job comes into this queue uh give the",
    "start": "337840",
    "end": "343880"
  },
  {
    "text": "resources to that job take them away from from somewhere else um and you but you can adjust all these settings and",
    "start": "343880",
    "end": "349840"
  },
  {
    "text": "actually EMR has an easy to use uh uh configuration API where when you create a cluster you can specify the",
    "start": "349840",
    "end": "356319"
  },
  {
    "text": "configuration file and the key value pairs to overwrite in this case it' probably be in coresite um where you can",
    "start": "356319",
    "end": "362120"
  },
  {
    "text": "change the different schedulers specify the different cues that you need and then create a cluster with um with those",
    "start": "362120",
    "end": "367880"
  },
  {
    "text": "uh configuration settings um I mentioned there's a lot of unclustered uis um and they're all",
    "start": "367880",
    "end": "374280"
  },
  {
    "text": "available on the master node um it by default the EMR security groups which are your firewall settings uh turn off",
    "start": "374280",
    "end": "380680"
  },
  {
    "text": "all the ports except for Port 22 uh to SSH so you can SSH the master node use",
    "start": "380680",
    "end": "385720"
  },
  {
    "text": "port forwarding and have access to a variety of different uis um actually I'll show a few in a quick demo in a",
    "start": "385720",
    "end": "390840"
  },
  {
    "text": "minute um but you have Hugh for uh browsing the hive metastore for your tables uh there's a really good SQL",
    "start": "390840",
    "end": "398280"
  },
  {
    "text": "notebook in there SQL editor you can create uh dags of jobs with Uzi and have a nice display showing the different",
    "start": "398280",
    "end": "404240"
  },
  {
    "text": "parts of the dag um and just a great way to have a nice front end um if you're",
    "start": "404240",
    "end": "409280"
  },
  {
    "text": "not you don't want to use the command line we also have Zeppelin which is a rich notebook as well very common for spark users to be able and data",
    "start": "409280",
    "end": "415319"
  },
  {
    "text": "scientists to write notes uh save them and get restore from other notes and that's sort of thing but for say the",
    "start": "415319",
    "end": "421680"
  },
  {
    "text": "admin rule somebody trying to debug jobs uh Monitor and that sort of thing the spark UI the resource manager UI the",
    "start": "421680",
    "end": "429440"
  },
  {
    "text": "h-based front end if you want to see how your region servers are doing or your Cas Miss ratio that sort of thing TZ to",
    "start": "429440",
    "end": "435440"
  },
  {
    "text": "go see dags that might have been run by uh your hive job using TZ and even Flink showing um kind of a a logical view of",
    "start": "435440",
    "end": "442440"
  },
  {
    "text": "what's going on uh during an execution path and if none of those really meet your use case you can even bootstrap",
    "start": "442440",
    "end": "448639"
  },
  {
    "text": "applications bootstrap actions uh can be specified when you're starting up a cluster and um they run before we start",
    "start": "448639",
    "end": "455960"
  },
  {
    "text": "any of the dam so before Hadoop is installed before anything's installed we'll run all of your scripts and we",
    "start": "455960",
    "end": "461000"
  },
  {
    "text": "actually have on our AWS Big Data blog uh some information on how to install things like Jupiter uh our studio and",
    "start": "461000",
    "end": "467520"
  },
  {
    "text": "some other other things as well you have rud access over all the machines so if there's a library uh that we don't have",
    "start": "467520",
    "end": "473039"
  },
  {
    "text": "or a application we don't have uh you can install it",
    "start": "473039",
    "end": "478080"
  },
  {
    "text": "yourself um this is a common view of having you know EMR being able to access data in a",
    "start": "478319",
    "end": "484840"
  },
  {
    "text": "variety different data sets it's very common for uh complex analytics pipelines to have you know maybe your uh",
    "start": "484840",
    "end": "491120"
  },
  {
    "text": "cold source of Truth in S3 but you might be pulling in data say from Dynamo DB or",
    "start": "491120",
    "end": "496400"
  },
  {
    "text": "streaming data in from something like Apache Kafka or Amazon Kinesis or creating say search indexes in elastic",
    "start": "496400",
    "end": "502080"
  },
  {
    "text": "search um and with all these different applications in EMR you really have all of the open source connectivity uh to",
    "start": "502080",
    "end": "508599"
  },
  {
    "text": "all these different storage services to be able to join data say across two silos to enrich both or maybe run",
    "start": "508599",
    "end": "515399"
  },
  {
    "text": "analytics on top of many different data stores in this case actually EMR recently open sourced our Hive Dynamo DB",
    "start": "515399",
    "end": "521360"
  },
  {
    "text": "connector and you can find it um on the AWS GitHub you can also use it with spark so you can create an external",
    "start": "521360",
    "end": "526880"
  },
  {
    "text": "table but specify that external table over a Dynamo DB and query data or load data directly from hive or spark SQL",
    "start": "526880",
    "end": "533160"
  },
  {
    "text": "into Dynamo DB one thing to take note though is that um the read capacity because keep in mind you have many",
    "start": "533160",
    "end": "538680"
  },
  {
    "text": "different clusters nodes accessing Dynamo DB at once uh to not get throttled or get the performance you",
    "start": "538680",
    "end": "543839"
  },
  {
    "text": "need you might need to bump up the amount of read capacity but doesn't end there you can use scoop on Amazon EMR to",
    "start": "543839",
    "end": "549800"
  },
  {
    "text": "query data out of RDS or Aurora if you need to say pull data out of an operational database and put it into",
    "start": "549800",
    "end": "555200"
  },
  {
    "text": "something like a data warehouse or your data Lake in S3 you can actually build elastic search index using Hadoop tools",
    "start": "555200",
    "end": "561000"
  },
  {
    "text": "and just load them into Amazon elastic search or elastic search on ec2 redshift supports um an open source package in",
    "start": "561000",
    "end": "567519"
  },
  {
    "text": "spark called spark redshift connector utilizing basically a red shift export to S3 and then loading back into your",
    "start": "567519",
    "end": "572640"
  },
  {
    "text": "spark job um many different tools integrate with Kafka and Kinesis uh spark streaming Flink um you can pull",
    "start": "572640",
    "end": "579600"
  },
  {
    "text": "data off of those uh and do real time near realtime stream processing and then finally Amazon uh S3 which is our",
    "start": "579600",
    "end": "586320"
  },
  {
    "text": "drivable Object Store and actually I'll talk about that in a minute but one example here from a customer use case is",
    "start": "586320",
    "end": "591360"
  },
  {
    "text": "Hurst who's using a variety of different storage systems um and kind of using EMR",
    "start": "591360",
    "end": "596600"
  },
  {
    "text": "as the glue between all of them in this case um you know Hurst a large Media company over 200 web properties they",
    "start": "596600",
    "end": "602240"
  },
  {
    "text": "really want to understand how their customers are interacting with those web properties um and doing recommendation",
    "start": "602240",
    "end": "608160"
  },
  {
    "text": "engines analytics on that data so they collect all that clickstream data and they push it through Amazon Kinesis and",
    "start": "608160",
    "end": "613680"
  },
  {
    "text": "they're running a spark streaming cluster on Amazon EMR in this case uh pretty large micro batches of 5 minutes",
    "start": "613680",
    "end": "619560"
  },
  {
    "text": "but still you know faster than your typical offline batch processing to roll up that information and then write it",
    "start": "619560",
    "end": "625920"
  },
  {
    "text": "out to S3 and Json or CSV and actually um there running more complicated analytics but also you could use",
    "start": "625920",
    "end": "631000"
  },
  {
    "text": "something like Kinesis fire hose to get the data directly into S3 if you didn't need a full spark streaming job um to",
    "start": "631000",
    "end": "637279"
  },
  {
    "text": "process that data um once the data is in S3 um they use Amazon EMR once again to",
    "start": "637279",
    "end": "643240"
  },
  {
    "text": "pull the data out of S3 and then use the spark red shift connector to load into Amazon redshift um and also to load that",
    "start": "643240",
    "end": "649720"
  },
  {
    "text": "data and ETL it transform it uh and load the indexes and elastic search as well",
    "start": "649720",
    "end": "655000"
  },
  {
    "text": "so Amazon EMR in this case almost serves as the glue between many of these WS services to stream ingest and then store",
    "start": "655000",
    "end": "661560"
  },
  {
    "text": "in an optimized search or uh data warehouse but also in and of itself is doing some processing and analytics work",
    "start": "661560",
    "end": "667839"
  },
  {
    "text": "um as well um one uh tip I guess with Amazon S3 and",
    "start": "667839",
    "end": "673959"
  },
  {
    "text": "you know Amazon S3 designed for 119s of durability very scalable low cost and is",
    "start": "673959",
    "end": "679320"
  },
  {
    "text": "really uh you know the core idea of decoupling your storage and compute which my guess as many of the EMR users here are doing this today it really",
    "start": "679320",
    "end": "686200"
  },
  {
    "text": "gives you the flexibility of um you know not managing your data layer so managing a very very large hdfs cluster can be",
    "start": "686200",
    "end": "693399"
  },
  {
    "text": "very painful also it's in a single AZ S3 is available across all azs meaning you",
    "start": "693399",
    "end": "698680"
  },
  {
    "text": "can spin up a cluster in any a in the region and access that data immediately also it allows you to shut your cluster",
    "start": "698680",
    "end": "704880"
  },
  {
    "text": "down we found that a lot of folks running hadon Prem have sides as a cluster for hdfs you need all of your",
    "start": "704880",
    "end": "710440"
  },
  {
    "text": "data nodes live to get uh access to data but often times the compute Cycles are idle it's just you know holding on to",
    "start": "710440",
    "end": "716440"
  },
  {
    "text": "the data that you're warehousing so we found that you can shut down down your cluster when you're using Amazon S3 your data is still durable and available",
    "start": "716440",
    "end": "723040"
  },
  {
    "text": "within a couple minutes you can get another cluster back up and process it when you need it however you can also have your cluster on all the time so",
    "start": "723040",
    "end": "728279"
  },
  {
    "text": "it's really up to you from a cost perspective of what makes the most sense based on your jobs um one thing you can",
    "start": "728279",
    "end": "734199"
  },
  {
    "text": "do though if you're turning your clusters up and down and you're say using something like Hive or Presto which need table schemo stored in the",
    "start": "734199",
    "end": "740760"
  },
  {
    "text": "hive meta store is you can use something like Amazon Aurora or RDS to store that table information outside of any one",
    "start": "740760",
    "end": "746880"
  },
  {
    "text": "cluster so when your cluster comes up um you point it at that database and uh you",
    "start": "746880",
    "end": "751959"
  },
  {
    "text": "have your tables back you don't need to recover all of your partitions every single time and actually there's a big data blog post or something in AWS Labs",
    "start": "751959",
    "end": "758720"
  },
  {
    "text": "that utilizes AWS Lambda to update your partition information assuming that new data comes in and you haven't loaded",
    "start": "758720",
    "end": "765000"
  },
  {
    "text": "into a cluster so if you have no cluster active you can use Lambda to uh add add new information to the metast store uh",
    "start": "765000",
    "end": "771519"
  },
  {
    "text": "asynchronously from any cluster a couple of S3 tips um you want",
    "start": "771519",
    "end": "776880"
  },
  {
    "text": "to avoid key names and lexor graphical order like say you have time series data and you're scanning everything through a",
    "start": "776880",
    "end": "782040"
  },
  {
    "text": "month the date times are the same the reason is to improve throughput performance um S3 will store things that",
    "start": "782040",
    "end": "787360"
  },
  {
    "text": "are lexor graphically similar um kind of more grouped onto s similar machines if you have a more diverse lexor graphical",
    "start": "787360",
    "end": "794519"
  },
  {
    "text": "key name for the object it'll distribute across more machines will get better list performance and better throughput",
    "start": "794519",
    "end": "800800"
  },
  {
    "text": "you can use hashing techniques to do that some people flip the date time around and other things like that so common queries uh will have better",
    "start": "800800",
    "end": "807399"
  },
  {
    "text": "performance um obviously compressing your data set will make a difference you know you need to",
    "start": "807399",
    "end": "812639"
  },
  {
    "text": "have one that's splitable in the right way or have the right object size but you're minimizing the uh actual amount",
    "start": "812639",
    "end": "817959"
  },
  {
    "text": "of data transferred from S3 over to the ec2 nodes in your EMR cluster and also one thing we found is that people are",
    "start": "817959",
    "end": "824199"
  },
  {
    "text": "commonly using parket assuming that you don't need to migrate your schema often and that your queries are can can",
    "start": "824199",
    "end": "829680"
  },
  {
    "text": "leverage something like a column or a file format for performance because you're got a wide table you don't need to access as much data the less data",
    "start": "829680",
    "end": "836120"
  },
  {
    "text": "scanned obviously the better performance you're going to get uh one new thing about S3 and actually",
    "start": "836120",
    "end": "841360"
  },
  {
    "text": "before before when people were running hbas um they'd run hbas they'd run hdfs on cluster and have these large hbas",
    "start": "841360",
    "end": "847079"
  },
  {
    "text": "clusters primarily sized for the data stored um last week we uh launched support for hbas using S3 as a data",
    "start": "847079",
    "end": "853800"
  },
  {
    "text": "store so hbed root directory where you would have say um your you know the H files the data in your table some",
    "start": "853800",
    "end": "859480"
  },
  {
    "text": "additional metadata instead of storing it on S3 or sorry in hdfs you're storing it on S3 what that allows you to do is",
    "start": "859480",
    "end": "866240"
  },
  {
    "text": "size the actual cluster for the amount of process uh compute and memory because the region servers will cash some of",
    "start": "866240",
    "end": "872399"
  },
  {
    "text": "this in memory um that you need for performance reasons not necessarily sizing a large cluster because hbase can",
    "start": "872399",
    "end": "878199"
  },
  {
    "text": "store a lot of data is very scalable no SQL um a lot of times your cluster size would be very massive for the amount of",
    "start": "878199",
    "end": "883360"
  },
  {
    "text": "data stored um but a few things we do to increase performance because you know",
    "start": "883360",
    "end": "888519"
  },
  {
    "text": "when uh there's a cash Miss on a read um hbas is going down to S3 and the random IO performance on S3 is obviously not as",
    "start": "888519",
    "end": "895240"
  },
  {
    "text": "good as an SSD is we will uh cache using the hbas bucket cache as much as we can",
    "start": "895240",
    "end": "900800"
  },
  {
    "text": "on the local disc of each node and so typically and actually uh on the next side I'll explain more F's use case but",
    "start": "900800",
    "end": "906800"
  },
  {
    "text": "if you it's almost you have a tiered storage you have the option of saying well you know I don't really need the",
    "start": "906800",
    "end": "912120"
  },
  {
    "text": "fastest performance on two pedabytes of data so I'm not going to cach everything I'll cash kind of the warm working set",
    "start": "912120",
    "end": "917920"
  },
  {
    "text": "that most of the um the reads are accessing but when there's an outlier I can take a little bit less performance",
    "start": "917920",
    "end": "923480"
  },
  {
    "text": "but pay uh you know substantially less so we have that caching go on in the background you can also shut down your",
    "start": "923480",
    "end": "928720"
  },
  {
    "text": "h-based cluster and restore it or in a Dr scenario um let's say that you needed to move your cluster to another a",
    "start": "928720",
    "end": "935079"
  },
  {
    "text": "because all of your data is already there and durable in S3 you can shut down your hbas cluster in 1 a and bring",
    "start": "935079",
    "end": "940120"
  },
  {
    "text": "up another one and have a recovery time of minutes which might typically take um days and a great example of that is",
    "start": "940120",
    "end": "946279"
  },
  {
    "text": "finra's use case they actually posted a blog post on the big data blog talking about an application they built uh on",
    "start": "946279",
    "end": "951920"
  },
  {
    "text": "EMR with hbas on S3 stores around three trillion Market records and they put about a billion of it in each day and",
    "start": "951920",
    "end": "957720"
  },
  {
    "text": "they'll go in depth about this they have a presentation uh tomorrow I highly encourage you to check it out they'll go into more detail than I will here but at",
    "start": "957720",
    "end": "964399"
  },
  {
    "text": "a high level they were running this cluster using hdfs and they managed to save 60% which is a significant amount",
    "start": "964399",
    "end": "970000"
  },
  {
    "text": "of money with the size a 700 terabyte cluster um by taking the data an S3 and",
    "start": "970000",
    "end": "975639"
  },
  {
    "text": "moving or sorry an hdfs moving it to S3 running hbas on EMR and um decoupling",
    "start": "975639",
    "end": "981120"
  },
  {
    "text": "the storage and compute in this case they're bulk loading with Hive um and then doing random reads on this data to",
    "start": "981120",
    "end": "987199"
  },
  {
    "text": "back and interactive application um shifting gears from talking about S3",
    "start": "987199",
    "end": "993040"
  },
  {
    "text": "is a uh storage and some of the other uh storage layers you can access an EMR um talking a little bit about security you",
    "start": "993040",
    "end": "999160"
  },
  {
    "text": "can run uh EMR in a private subnet we see this as a popular uh way to deploy EMR um customers in that case will use a",
    "start": "999160",
    "end": "1006639"
  },
  {
    "text": "S3 endpoint VPC um so the cluster can access the data in S3 directly from the",
    "start": "1006639",
    "end": "1012079"
  },
  {
    "text": "private subnet um and if you have to access say things uh that have public endpoints like say using Hive to query",
    "start": "1012079",
    "end": "1017880"
  },
  {
    "text": "Dynamo DB b or retrieve an AWS KMS key uh any endpoint that's not in a private",
    "start": "1017880",
    "end": "1023199"
  },
  {
    "text": "subnet you'll need to use something like a knat or a managen to be able to access that range um a new feature we actually",
    "start": "1023199",
    "end": "1030720"
  },
  {
    "text": "just launched two weeks ago um is a support for fine grain Access Control by cluster tags today when you create a",
    "start": "1030720",
    "end": "1036360"
  },
  {
    "text": "cluster you have a service role IM Ro that you give to the EMR service to have permission to say create ec2 instances",
    "start": "1036360",
    "end": "1042918"
  },
  {
    "text": "terminate these instances when your cluster is done and then an instance profile that you put on each node um",
    "start": "1042919",
    "end": "1047959"
  },
  {
    "text": "that gives you know each node in the cluster like aess spark executor when it calls out to access data and S3 those",
    "start": "1047959",
    "end": "1053280"
  },
  {
    "text": "permissions um and that already existed but what we allow now is in a policy an IM policy or an IM user policy um to",
    "start": "1053280",
    "end": "1061360"
  },
  {
    "text": "have set a condition on EMR um uh apis in that policy of Grants or and deny",
    "start": "1061360",
    "end": "1068520"
  },
  {
    "text": "depending on whether there's a cluster tag so you can create a cluster and actually and force a user to say when",
    "start": "1068520",
    "end": "1073640"
  },
  {
    "text": "you know user a creates a cluster must be tagged with you know User Group analytics um um and that cluster is",
    "start": "1073640",
    "end": "1079960"
  },
  {
    "text": "created the user can't remove the tag and then only a certain set of users would be able to say add nodes delete",
    "start": "1079960",
    "end": "1085960"
  },
  {
    "text": "the cluster add steps or units of work any of the EMR apis that involve in the clust that involve Interac with the",
    "start": "1085960",
    "end": "1091640"
  },
  {
    "text": "cluster you can limit by cluster tag now um but even furthermore um that's",
    "start": "1091640",
    "end": "1097480"
  },
  {
    "text": "kind of more of a cluster level security um uh way of doing things but often",
    "start": "1097480",
    "end": "1103760"
  },
  {
    "text": "times you might have a multi-tenant cluster with uh many users interacting with say hive you have a bunch of",
    "start": "1103760",
    "end": "1109080"
  },
  {
    "text": "analysts some analysts have access to table a some might have only access to table B and you want to control",
    "start": "1109080",
    "end": "1114400"
  },
  {
    "text": "everything through the Gateway point and so this slide right here and actually this is going to be it's a little bit",
    "start": "1114400",
    "end": "1120000"
  },
  {
    "text": "ahead but in the AWS Big Data blog we'll be posting more information um about how to do this um so this is a little bit of",
    "start": "1120000",
    "end": "1125440"
  },
  {
    "text": "a preview um you can you can can do this by uh configuring Hive server 2 there's a bunch of ways but this new blog post",
    "start": "1125440",
    "end": "1131520"
  },
  {
    "text": "talks about using Apache Ranger which is a open source project that's almost like a policy generator and enforcer for many",
    "start": "1131520",
    "end": "1137960"
  },
  {
    "text": "of the to ecosystem applications um in this example you can use something like H and you can do this today um and link",
    "start": "1137960",
    "end": "1145080"
  },
  {
    "text": "up H using ldap to say your active directory to get all of your users so when a user comes to the terminal um",
    "start": "1145080",
    "end": "1150360"
  },
  {
    "text": "they can log in as themselves um but then the new thing is uh using a cloud formation script from this blog to",
    "start": "1150360",
    "end": "1155880"
  },
  {
    "text": "install Ranger on an ec2 instance and then have um Hive server 2 and hdfs in",
    "start": "1155880",
    "end": "1161600"
  },
  {
    "text": "this example um interact with that uh the policies that you've created in Ranger to say you know if I've logged",
    "start": "1161600",
    "end": "1167400"
  },
  {
    "text": "into Hue as user a can you know user a uh access this table or not and actually",
    "start": "1167400",
    "end": "1172480"
  },
  {
    "text": "if we could switch over to my computer I have uh I can show you a quick preview of of what some of these things look",
    "start": "1172480",
    "end": "1181039"
  },
  {
    "text": "like so here I've set up Hue just like uh that was shown uh communicate with",
    "start": "1184200",
    "end": "1189320"
  },
  {
    "text": "the active directory and hopefully this will log in there we go and for those of you who don't know Hugh it's a it's a very",
    "start": "1189320",
    "end": "1195240"
  },
  {
    "text": "useful uh tool you have a robust SQL editor editor you can um go browse tables in this case is a table with some",
    "start": "1195240",
    "end": "1201919"
  },
  {
    "text": "sample data you actually have a nice preview of what uh some of that data is so it's great for analysts and writing",
    "start": "1201919",
    "end": "1207280"
  },
  {
    "text": "ad hoc queries but there's there's two tables uh in the cluster there's table anal uh table analyst 2 which I logged",
    "start": "1207280",
    "end": "1213120"
  },
  {
    "text": "in as table analy one you can't see which is by Design but if you run it um",
    "start": "1213120",
    "end": "1218200"
  },
  {
    "text": "it'll call out to Ranger and uh see if I have access in the end I'm denied uh from the ranger View and the ranger UI",
    "start": "1218200",
    "end": "1224840"
  },
  {
    "text": "has a rich UI where it actually uses solar on the back end to process basically your audit Trail you can go as",
    "start": "1224840",
    "end": "1231159"
  },
  {
    "text": "a admin user and Ranger see who's been accessing what um and then here as you can see um might take a minute to",
    "start": "1231159",
    "end": "1238559"
  },
  {
    "text": "refresh but earlier I was doing it um you know you can see here who's been denied who's had access and then from uh",
    "start": "1238559",
    "end": "1245440"
  },
  {
    "text": "you know the access here you can control all of your policies with a a rich UI here as you can see for analyst 2 I",
    "start": "1245440",
    "end": "1252039"
  },
  {
    "text": "don't have access to table one and I was denied so it's a it's a very very nice way we can switch back to the slides um",
    "start": "1252039",
    "end": "1258880"
  },
  {
    "text": "it's a very easy way to manage a lot of policies for Hive and they're rolling out more and more support uh for more",
    "start": "1258880",
    "end": "1264760"
  },
  {
    "text": "applications as well um I've got a race through the next slide so I can hand it over to naen but uh I do want to mention",
    "start": "1264760",
    "end": "1272120"
  },
  {
    "text": "that we support now encryption for tez spark and map reduce um and we make it",
    "start": "1272120",
    "end": "1277159"
  },
  {
    "text": "easy using a feature called uh security configurations where and here's a screenshot from the console you can",
    "start": "1277159",
    "end": "1282520"
  },
  {
    "text": "select the AWS KS Keys you want to use or you can provide an encryption materials provider uh with information",
    "start": "1282520",
    "end": "1288200"
  },
  {
    "text": "how to say access keys from your HSM or custom keys and can save this configuration with us and then reference",
    "start": "1288200",
    "end": "1293679"
  },
  {
    "text": "it when creating a cluster and before everything starts up we'll uh encrypt all the local uh drives using Lux we'll",
    "start": "1293679",
    "end": "1299840"
  },
  {
    "text": "configure encrypted shuffle for spark encrypted shuffle for Hadoop and TZ um and then with data in S3 we'll configure",
    "start": "1299840",
    "end": "1306520"
  },
  {
    "text": "emrfs which is as mentioned before RS3 connector to be able to interact with data in S3 that's encrypted client side",
    "start": "1306520",
    "end": "1312760"
  },
  {
    "text": "or server side using uh KMS or S3 manage keys so if encryption is important also EMR is included um under the list of",
    "start": "1312760",
    "end": "1319240"
  },
  {
    "text": "services that are Hipp eligible in AWS so if this is interesting happy to talk more also after the presentation but all",
    "start": "1319240",
    "end": "1324960"
  },
  {
    "text": "this is available today um and you know example of that is NASDAQ uh they presented this uh",
    "start": "1324960",
    "end": "1331960"
  },
  {
    "text": "presentation actually reinvent last year they have a Federated data Lake over Amazon red shift um for a subset",
    "start": "1331960",
    "end": "1338159"
  },
  {
    "text": "aggregate data that's more hot that is using um you know the very powerful performance of red shift um but then",
    "start": "1338159",
    "end": "1343840"
  },
  {
    "text": "using Presto and EMR which is still fast it's not quite as optimized for a lot of complex joins still useful for low",
    "start": "1343840",
    "end": "1349360"
  },
  {
    "text": "latency SQL on a data Lake that's all of their data and it's a little bit easier to store um at low cost because S3 is",
    "start": "1349360",
    "end": "1356080"
  },
  {
    "text": "designed for low cost they can shut their cluster down when an analyst say needs access to all the historical data",
    "start": "1356080",
    "end": "1361240"
  },
  {
    "text": "sometimes they can fire up a presto cluster and give that analyst SQL interface over that data then shut it down and not pay for but in this case um",
    "start": "1361240",
    "end": "1367960"
  },
  {
    "text": "the presentation goes into more on how they manage a bunch of custom keys to use with EMR because they client side",
    "start": "1367960",
    "end": "1373320"
  },
  {
    "text": "encrypt all their data in S3 um so shifting to the final Point um that I",
    "start": "1373320",
    "end": "1379679"
  },
  {
    "text": "want to go through is auto scaling and spot EMR ship support for Autos scaling two weeks ago um and it's available in",
    "start": "1379679",
    "end": "1385919"
  },
  {
    "text": "most regions and we'll be rolling it out to more regions um uh in the next couple",
    "start": "1385919",
    "end": "1391120"
  },
  {
    "text": "months uh basically uh EMR sends metrics to cloudwatch today uh yarn metrics we",
    "start": "1391120",
    "end": "1398799"
  },
  {
    "text": "actually have a few new ones like yarn percentage used memory and um container pending ratio just metrics that have to",
    "start": "1398799",
    "end": "1404960"
  },
  {
    "text": "do with how much is my cluster being utilized and if I added more noes would I actually have uh you know any extra",
    "start": "1404960",
    "end": "1410559"
  },
  {
    "text": "capacity um that gets pumped today and behind the the scenes EMR is configuring cloudwatch alarms and using application",
    "start": "1410559",
    "end": "1417520"
  },
  {
    "text": "autoscaling to when you've you know set a policy saying when yarn memory is utilized 80% add nodes um EMR will go",
    "start": "1417520",
    "end": "1425200"
  },
  {
    "text": "through that and uh then add nodes to your cluster in applications like spark with Dynamic allocation will take",
    "start": "1425200",
    "end": "1430960"
  },
  {
    "text": "advantage of the new nodes that come up and scale those applications out the same thing for going down um and and",
    "start": "1430960",
    "end": "1436640"
  },
  {
    "text": "decreasing the cluster size and actually with this new feature we have uh two scale down behaviors the default one now",
    "start": "1436640",
    "end": "1442960"
  },
  {
    "text": "is we won't scale down your cluster until it's nearing that instance hour boundary you've paid for the hour for the instance having it around for longer",
    "start": "1442960",
    "end": "1449360"
  },
  {
    "text": "isn't going to hurt and maybe a new job will come in that'll utilize that capacity so we've instrumented logic in to not actually terminate the instance",
    "start": "1449360",
    "end": "1455440"
  },
  {
    "text": "until it nears that hourly boundary however we also have Behavior to say you know don't terminate the instance unless",
    "start": "1455440",
    "end": "1460600"
  },
  {
    "text": "all yarn containers are done running on that node and then we'll Blacklist the node and drain off the work and",
    "start": "1460600",
    "end": "1466200"
  },
  {
    "text": "eventually nothing will be running and we'll shut it down so you have more of a kind of cost optimized scale down and a",
    "start": "1466200",
    "end": "1471679"
  },
  {
    "text": "uh you know workflow don't lose any any running tasks option as well and and finally before I conclude um I want to",
    "start": "1471679",
    "end": "1478720"
  },
  {
    "text": "announce that we're are going to be coming out soon with the feature Advanced spot provisioning um that'll be available soon um what it allow you to",
    "start": "1478720",
    "end": "1485559"
  },
  {
    "text": "do it's it has some kind of flavor of the uh spot Fleet feature set if anyone here is familiar with spot Fleet and",
    "start": "1485559",
    "end": "1490760"
  },
  {
    "text": "support for spot blocks as well it'll allow you to specify a set of instances to choose from with a bunch of different",
    "start": "1490760",
    "end": "1496039"
  },
  {
    "text": "bid prices also several different azs and based on the capacity of those instant types in the azs and what's",
    "start": "1496039",
    "end": "1502440"
  },
  {
    "text": "available for the lowest unit cost EMR will provision a mix of these instances um to give you kind of the lowest cost",
    "start": "1502440",
    "end": "1508720"
  },
  {
    "text": "cluster in the a with the uh the most available capacity to avoid any spot interruptions um also spot block support",
    "start": "1508720",
    "end": "1515960"
  },
  {
    "text": "if you know that your job say takes 4 hours you can use a specifi spot block and not actually get that data",
    "start": "1515960",
    "end": "1522399"
  },
  {
    "text": "interrupted so anyway I'll hand the uh the mic over to navine to talk a little bit about how they built the data L with",
    "start": "1522399",
    "end": "1528200"
  },
  {
    "text": "EMR at",
    "start": "1528200",
    "end": "1530720"
  },
  {
    "text": "Aur thanks John hey I'm naen from masurian today we're going to focus on",
    "start": "1534279",
    "end": "1540559"
  },
  {
    "text": "how to leverage Amazon services to build a data Lake uh that with a single UniFi data Lake uh where you can support your",
    "start": "1540559",
    "end": "1547679"
  },
  {
    "text": "today's kind of analysis where you have traditional bi users report users data",
    "start": "1547679",
    "end": "1552720"
  },
  {
    "text": "scientists at the same time keeping the cost and Security in mind",
    "start": "1552720",
    "end": "1559679"
  },
  {
    "text": "how many of you guys know what who is",
    "start": "1560799",
    "end": "1564320"
  },
  {
    "text": "assuan so assuan is the leader in providing customer support and uh",
    "start": "1566039",
    "end": "1571960"
  },
  {
    "text": "provide it's a leader in uh Production Services so when you guys have your devices and if it has support and",
    "start": "1571960",
    "end": "1577679"
  },
  {
    "text": "warranty that's what aan provides today as you can imagine when you guys lose a phone or some kind of",
    "start": "1577679",
    "end": "1583679"
  },
  {
    "text": "electronic device that you are attached to it's quite a hassle to get back to your cont your photos and all of your",
    "start": "1583679",
    "end": "1589760"
  },
  {
    "text": "social media content so assuan basically helps our goal is to make technology",
    "start": "1589760",
    "end": "1594840"
  },
  {
    "text": "seamlessly work and make our customers uh have lives",
    "start": "1594840",
    "end": "1600159"
  },
  {
    "text": "richer so we are assuan supports almost 229 290 million users uh globally and we",
    "start": "1600159",
    "end": "1607279"
  },
  {
    "text": "have a constant Innovation that happens at the assuan and we are Global in uh Us",
    "start": "1607279",
    "end": "1614039"
  },
  {
    "text": "South America Europe and uh Japan",
    "start": "1614039",
    "end": "1619080"
  },
  {
    "text": "aurent has a wide variety of data sources ranging from oilp applications",
    "start": "1619080",
    "end": "1624919"
  },
  {
    "text": "to unstructured data from telepon voice to text claims data notes and social",
    "start": "1624919",
    "end": "1631200"
  },
  {
    "text": "with 290 million customers and over 10 billion interactions and 52 million uh",
    "start": "1631200",
    "end": "1638520"
  },
  {
    "text": "interactions for voice and 24 million claims and unique visitors we have a huge volume of data so which comes down",
    "start": "1638520",
    "end": "1646360"
  },
  {
    "text": "to the typical challenge of the volume velocity and",
    "start": "1646360",
    "end": "1651600"
  },
  {
    "text": "variety when we started this project back in 2015 our data growth was around",
    "start": "1651960",
    "end": "1658600"
  },
  {
    "text": "data size at the time was around 1 paby so in the last year and a half we have actually grown it to three pedabytes and",
    "start": "1658600",
    "end": "1665120"
  },
  {
    "text": "we expect the data to be grown to 8 pedabytes so with this particular with this expectation of data",
    "start": "1665120",
    "end": "1672120"
  },
  {
    "text": "growth what we are trying to have is how do you define a single Unified platform",
    "start": "1672120",
    "end": "1678120"
  },
  {
    "text": "to support today's users especially analytical users so for which what we have laid out is some core fundamental",
    "start": "1678120",
    "end": "1684000"
  },
  {
    "text": "principles that would help and guide us uh while building our data Lake and provisioning it to our end",
    "start": "1684000",
    "end": "1690480"
  },
  {
    "text": "users one of the guiding principle that we have embraced is to store all the Enterprise data in one single location",
    "start": "1690480",
    "end": "1696240"
  },
  {
    "text": "so what it this enables us to do is to reduce the data proliferation and reduces the data footprint uh for the",
    "start": "1696240",
    "end": "1703320"
  },
  {
    "text": "data exposure or data breaches other key area one of the",
    "start": "1703320",
    "end": "1709080"
  },
  {
    "text": "principle that we have leveraged is to provide the data as quickly as possible to analysts or or data scientists to do",
    "start": "1709080",
    "end": "1715640"
  },
  {
    "text": "data Discovery and data analysis and provide value or actionable content or information from the data so for which",
    "start": "1715640",
    "end": "1722240"
  },
  {
    "text": "we have embraced elt as a pattern rather than the traditional ETL pattern and data quality is a key",
    "start": "1722240",
    "end": "1729760"
  },
  {
    "text": "because we have to build trust in the data that we have so uh what we",
    "start": "1729760",
    "end": "1735000"
  },
  {
    "text": "recommend is to handle data quality and data security from get-go when you're",
    "start": "1735000",
    "end": "1740399"
  },
  {
    "text": "designing your data link data security is also a key Foundation uh with the recent changes",
    "start": "1740399",
    "end": "1747720"
  },
  {
    "text": "with brexit in EU and now uh LinkedIn now being banned in Russia",
    "start": "1747720",
    "end": "1754519"
  },
  {
    "text": "security policies are constantly changing so have enough metadata attached to your data when policies",
    "start": "1754519",
    "end": "1760840"
  },
  {
    "text": "change you can Embrace that and mod instead of rebuilding your data Lake you can just configure data Lake to meet to",
    "start": "1760840",
    "end": "1767559"
  },
  {
    "text": "those local security guidelines we talked about velocity and variety and one of the thing that we",
    "start": "1767559",
    "end": "1774399"
  },
  {
    "text": "wanted to embrace is with the recent world with the recent changes how things are moving towards",
    "start": "1774399",
    "end": "1781120"
  },
  {
    "text": "the market with MVP and agile way of Building Products scale is very important for us",
    "start": "1781120",
    "end": "1789240"
  },
  {
    "text": "because an MVP can be successful in a market or cannot be successful in the market but if it is Su successful how do",
    "start": "1789240",
    "end": "1795519"
  },
  {
    "text": "you scale that thing without having to invest your lot of money in infrastructure your capital investment",
    "start": "1795519",
    "end": "1801000"
  },
  {
    "text": "in infrastructure so we wanted to have a system where we can scale on demand without manual",
    "start": "1801000",
    "end": "1806799"
  },
  {
    "text": "intervention and other thing is we would like to have the cost to be minimal so we would like to do it as a pay as ego",
    "start": "1806799",
    "end": "1812240"
  },
  {
    "text": "model to keep all of these things in mind and to keep our operational cost lower and to to support the business",
    "start": "1812240",
    "end": "1819200"
  },
  {
    "text": "agility we have chosen platform as a service as our core Foundation to our",
    "start": "1819200",
    "end": "1825279"
  },
  {
    "text": "architecture to solve the challenge we have chosen this as our logical architecture where you can see on the",
    "start": "1825279",
    "end": "1831240"
  },
  {
    "text": "left hand side all of our olp systems where we can produce the transactional",
    "start": "1831240",
    "end": "1837039"
  },
  {
    "text": "data in the traditional relational databases or even like events coming from your",
    "start": "1837039",
    "end": "1842440"
  },
  {
    "text": "devices and also even from the social media events so we take the data and we",
    "start": "1842440",
    "end": "1847760"
  },
  {
    "text": "have a data extraction layer a data service layer where we collect through a standard interface of obbc or jdbc or a",
    "start": "1847760",
    "end": "1855639"
  },
  {
    "text": "change data control or change data capture like uh SQL replicator or Oracle Golden Gate tools where you can extract",
    "start": "1855639",
    "end": "1862480"
  },
  {
    "text": "the data and encrypt it especially when moving the data into the cloud and uh it could cross a country boundaries so we",
    "start": "1862480",
    "end": "1869279"
  },
  {
    "text": "like to make sure that we are securing the sensitive information so we encrypt",
    "start": "1869279",
    "end": "1874919"
  },
  {
    "text": "the sensitive information and then load it into our data lake so in data Lake where we have S3 as our Central Hub",
    "start": "1874919",
    "end": "1883120"
  },
  {
    "text": "where we purist all the data and we use EMR to process process the data to make",
    "start": "1883120",
    "end": "1888360"
  },
  {
    "text": "that raw data to be meaningful content or information once the processing is done",
    "start": "1888360",
    "end": "1895039"
  },
  {
    "text": "we take the model data and pump it into optimized data sources in this",
    "start": "1895039",
    "end": "1900159"
  },
  {
    "text": "particular scenario we are using U Amazon rad shift and Dynamo DB uh based",
    "start": "1900159",
    "end": "1905320"
  },
  {
    "text": "upon the use cases and the other layer that you see",
    "start": "1905320",
    "end": "1910760"
  },
  {
    "text": "on the right hand side is your data virtualization layer we use this as a single unified layer to control access",
    "start": "1910760",
    "end": "1917279"
  },
  {
    "text": "to the uh users basically we authenticate and authorize the users at single layer rather than configuring",
    "start": "1917279",
    "end": "1922919"
  },
  {
    "text": "security at each of the product level SQL server has it security Oracle hasard security red shift hasard security so",
    "start": "1922919",
    "end": "1929480"
  },
  {
    "text": "things of that nature instead of configuring security at all of these places we have chosen a parameter",
    "start": "1929480",
    "end": "1934559"
  },
  {
    "text": "configuration so we encapsulate all of the data around data virtualization and configure security at one single",
    "start": "1934559",
    "end": "1942440"
  },
  {
    "text": "place and that's just traditional bi reporting tools any tools which supports jdbc obbc or rest API can be used like",
    "start": "1942440",
    "end": "1950440"
  },
  {
    "text": "uh uh click view or tabl or Spotfire or quick sites uh the bottom you see is our",
    "start": "1950440",
    "end": "1957679"
  },
  {
    "text": "Enterprise orchestration layer where we use this uh environment to schedule the",
    "start": "1957679",
    "end": "1963279"
  },
  {
    "text": "jobs within our ecosystem to transform the data and move the data across the different data sources like uh red shift",
    "start": "1963279",
    "end": "1970279"
  },
  {
    "text": "and uh uh and SQL servers or our Amazon",
    "start": "1970279",
    "end": "1975639"
  },
  {
    "text": "audience on the top you see an environment where we have created uh to support traditional uh test data",
    "start": "1975639",
    "end": "1982559"
  },
  {
    "text": "management in today's world when you do your ETL work in uh SQL data warehouses",
    "start": "1982559",
    "end": "1988399"
  },
  {
    "text": "or Oracle data warehouses you have production systems then you have Dev systems and QA systems most of the time",
    "start": "1988399",
    "end": "1994480"
  },
  {
    "text": "people spend quite a bit of time trying to generate the test data test data to move the data into the lower environment",
    "start": "1994480",
    "end": "2001120"
  },
  {
    "text": "for the development team to build and test so with help of Amazon EMR and S3",
    "start": "2001120",
    "end": "2006360"
  },
  {
    "text": "architecture uh we were able to mitigate the uh need for test data management and leverage",
    "start": "2006360",
    "end": "2013840"
  },
  {
    "text": "directly uh able to access data directly from S3 and providing the capability to",
    "start": "2013840",
    "end": "2019120"
  },
  {
    "text": "the users to do their development without impacting the production",
    "start": "2019120",
    "end": "2025840"
  },
  {
    "text": "ecosystem here are the some of the just overlay of the Amazon web services icons on uh of a day L what we've been using",
    "start": "2026880",
    "end": "2033559"
  },
  {
    "text": "so far this is not a comprehensive list for this particular scenario we just put a sample set we use Amazon of fire hose",
    "start": "2033559",
    "end": "2040960"
  },
  {
    "text": "and KES stream for even streaming with the Lambda uh we use Amazon S3 for your central data storage EMR for data",
    "start": "2040960",
    "end": "2048200"
  },
  {
    "text": "processing RSS for external metast stores and other transactional data sets and Amazon red shift for uh your",
    "start": "2048200",
    "end": "2055200"
  },
  {
    "text": "columnar uh uh reporting uh and we use elastic bean",
    "start": "2055200",
    "end": "2061520"
  },
  {
    "text": "stock and elastic catch to provide apis uh from some of the data that has been",
    "start": "2061520",
    "end": "2066960"
  },
  {
    "text": "churned using EMR so now we talked about this brief",
    "start": "2066960",
    "end": "2073118"
  },
  {
    "text": "overview of our data Lake architecture so in this particular section what we're going to focus on is Amazon S3 so during",
    "start": "2073119",
    "end": "2079480"
  },
  {
    "text": "our journey we're going to share some of the key slide key takeaways that we have",
    "start": "2079480",
    "end": "2084760"
  },
  {
    "text": "learned during our journey while injesting data into Amazon",
    "start": "2084760",
    "end": "2090399"
  },
  {
    "text": "S3 and as typically for all storage system we want it to be secure scal able",
    "start": "2090800",
    "end": "2097200"
  },
  {
    "text": "durable and storage so for that we have chosen S3 so while we ingesting data",
    "start": "2097200",
    "end": "2102240"
  },
  {
    "text": "into the data Lake some of the key things that we have to transform the data so that it becomes much more easy",
    "start": "2102240",
    "end": "2108119"
  },
  {
    "text": "to transform using EMR map ruce or even through spark uh especially with the way",
    "start": "2108119",
    "end": "2113720"
  },
  {
    "text": "the five different file file formats are available and some of them are splitable some of them are non-s splitable so some",
    "start": "2113720",
    "end": "2119400"
  },
  {
    "text": "of the key takeaways that uh we have learned during our journey was to handle",
    "start": "2119400",
    "end": "2124440"
  },
  {
    "text": "Carriage returns and line feeds in the data that you have so that it doesn't becomes a split record and the the map",
    "start": "2124440",
    "end": "2130280"
  },
  {
    "text": "ruce doesn't treat it as two different records to process it and we also chose an a specific D limiter and removed the",
    "start": "2130280",
    "end": "2136760"
  },
  {
    "text": "dmit from the rest of the text and uh other thing key important thing that we have learned is how do you",
    "start": "2136760",
    "end": "2143880"
  },
  {
    "text": "handle time we have servers in all over the world and each one has its own local time zone so before we ingesting the",
    "start": "2143880",
    "end": "2149920"
  },
  {
    "text": "data into the data Lake we have chosen to convert it into UTC uh as a single time zone and other thing is based upon",
    "start": "2149920",
    "end": "2157800"
  },
  {
    "text": "the map reduced containers and the size of the noes that you used and the amount of memory that is available on it so",
    "start": "2157800",
    "end": "2163720"
  },
  {
    "text": "file sizes are very key otherwise you're going to get the splits or it's going to wait for resources or you're going to",
    "start": "2163720",
    "end": "2168960"
  },
  {
    "text": "get memory overflow with the jvms so what we have done is we have while",
    "start": "2168960",
    "end": "2174079"
  },
  {
    "text": "ingesting itself we are able to split the files at 128 Meg so we recommend anything greater than 128 Meg and 512",
    "start": "2174079",
    "end": "2181839"
  },
  {
    "text": "mag of a file size compressed using gzip or Snappy is one of the the key things",
    "start": "2181839",
    "end": "2187319"
  },
  {
    "text": "that we have learned uh for performance and for data scanning it's better to",
    "start": "2187319",
    "end": "2192920"
  },
  {
    "text": "always have a partitioning strategy in place where you can choose to partition your data based upon your business",
    "start": "2192920",
    "end": "2199240"
  },
  {
    "text": "activity date or whichever is Meaningful to your data set and other thing uh is that on",
    "start": "2199240",
    "end": "2207079"
  },
  {
    "text": "S3 the object path is case sensitive so if you have multiple developers",
    "start": "2207079",
    "end": "2212440"
  },
  {
    "text": "developing it so we would like to convert it into one single case either convert it to Upper case or a lower case",
    "start": "2212440",
    "end": "2217720"
  },
  {
    "text": "so we have chosen to convert it into lower case so now we know how to get the data",
    "start": "2217720",
    "end": "2222960"
  },
  {
    "text": "into the data like how what are the Transformations that we need to do to get it uh to do for the data to be",
    "start": "2222960",
    "end": "2228839"
  },
  {
    "text": "transformed to get it into the data l so we have used uh python libraries to do",
    "start": "2228839",
    "end": "2234319"
  },
  {
    "text": "uh push the data from on Prem to AWS S3 uh we have leveraged multipot upload",
    "start": "2234319",
    "end": "2241480"
  },
  {
    "text": "strategy and uh while loading the data into the data L to make based not all",
    "start": "2241480",
    "end": "2247079"
  },
  {
    "text": "data is equal right so some data is much more valuable than the other data some data is accessed more often than the",
    "start": "2247079",
    "end": "2252240"
  },
  {
    "text": "other uh so we have different classes in Amazon S3 so one is the standard storage",
    "start": "2252240",
    "end": "2257960"
  },
  {
    "text": "one is reduced redundant storage other is infrequent access so you can choose to set those settings while you're doing",
    "start": "2257960",
    "end": "2264200"
  },
  {
    "text": "the upload process and you also have the life cycle policies uh enabled so that you can move",
    "start": "2264200",
    "end": "2270839"
  },
  {
    "text": "the data from a higher Cost Storage to the lower cost storage from standard to in frequent access once the data is",
    "start": "2270839",
    "end": "2276119"
  },
  {
    "text": "processed and other thing is on S3 one of the recommendation that we have is keep your storage and compute as close",
    "start": "2276119",
    "end": "2283760"
  },
  {
    "text": "to together as possible uh so we don't want you to guys launching an EMR cluster in uh uh EU and keep the data",
    "start": "2283760",
    "end": "2290280"
  },
  {
    "text": "storage in uh us so that gives you better performance if you keep those",
    "start": "2290280",
    "end": "2296240"
  },
  {
    "text": "things local together and the cost will also be much more economical one other thing we have also",
    "start": "2296240",
    "end": "2301480"
  },
  {
    "text": "noticed is on S3 you have a throttling of 100 uh uh",
    "start": "2301480",
    "end": "2307040"
  },
  {
    "text": "puts deletes and gets or 300 uh gets so when you reach that leader you have an",
    "start": "2307040",
    "end": "2312280"
  },
  {
    "text": "option to either expand or to uh request for another bucket and you can partition the data for those",
    "start": "2312280",
    "end": "2318720"
  },
  {
    "text": "buckets and always tag your things so that you can know how to track your cost and for S3 you can also have IP",
    "start": "2318720",
    "end": "2326200"
  },
  {
    "text": "restriction policies where you can say it is bound to certain IP you cannot access it from uh any other IP than your",
    "start": "2326200",
    "end": "2332040"
  },
  {
    "text": "organization IP range and you can use the IM am and ACLS to control the security on it",
    "start": "2332040",
    "end": "2338520"
  },
  {
    "text": "did I go up sorry so now we know how to get the data into the data L we have put the data",
    "start": "2341680",
    "end": "2348960"
  },
  {
    "text": "into the data l so once we wanted to put the data into the data L we needed to have some kind of a logical structure how to organize the data in the data L",
    "start": "2348960",
    "end": "2356119"
  },
  {
    "text": "for those things what we have defined is basically four logical layers of data so in layer one what we calling it as raw",
    "start": "2356119",
    "end": "2362440"
  },
  {
    "text": "data where you keep the data at as a raw format as much as possible possible with minimal amount of transformation so it",
    "start": "2362440",
    "end": "2369760"
  },
  {
    "text": "becomes our system of Truth and uh does the basic minimum uh encryption",
    "start": "2369760",
    "end": "2376520"
  },
  {
    "text": "for your sensitive data transform your data gets it into the data Lake layer sorry Layer Two is basically taking the",
    "start": "2376520",
    "end": "2384440"
  },
  {
    "text": "raw data applying your quality rules and transforming the data to a different",
    "start": "2384440",
    "end": "2390440"
  },
  {
    "text": "partitioning strategy that's more much more faster and economical for your analyst to use so in r data in layer one",
    "start": "2390440",
    "end": "2397640"
  },
  {
    "text": "you would like to choose a partition strategy that would be much more economical to handle your ETL daily Deltas but in Layer Two you might be",
    "start": "2397640",
    "end": "2404760"
  },
  {
    "text": "choosing a different partition strategy based upon what kind of queries you're running so in Layer Two we have chosen",
    "start": "2404760",
    "end": "2409880"
  },
  {
    "text": "orc and park as our file formats in Layer Three is basically taking this all of these oilp system raw",
    "start": "2409880",
    "end": "2417000"
  },
  {
    "text": "data sets and then trying to model them using a data WTH 2.0 pattern so it's a standard pattern available uh it's uh",
    "start": "2417000",
    "end": "2424480"
  },
  {
    "text": "similar to your start scheme in the traditional bi model so data VA helps us with the EMR that's much more optimized",
    "start": "2424480",
    "end": "2431560"
  },
  {
    "text": "to process the data where I don't have to process all the facts uh before the dimensions so in layer three we choose",
    "start": "2431560",
    "end": "2439119"
  },
  {
    "text": "orc as a primary format for people to query the data and we also choose text to write the final uh output so we use",
    "start": "2439119",
    "end": "2446359"
  },
  {
    "text": "this data to push the data into red shift if uh some of this data is required to be much more SL bound and",
    "start": "2446359",
    "end": "2453280"
  },
  {
    "text": "the fourth layer is basically a reference data which is basically a confirmed Dimensions or Master data and",
    "start": "2453280",
    "end": "2458839"
  },
  {
    "text": "we use orc as our file format there now we have the capability of",
    "start": "2458839",
    "end": "2464480"
  },
  {
    "text": "getting the data storing the data and we have logically organized it now we would like to see how we can process the data",
    "start": "2464480",
    "end": "2470760"
  },
  {
    "text": "so as Jonathan has talked about we are those are the best practices right have",
    "start": "2470760",
    "end": "2475839"
  },
  {
    "text": "a single meta store for all of your data so that you know where your data is located what is it structure you can",
    "start": "2475839",
    "end": "2481680"
  },
  {
    "text": "have transient clusters or long running clusters and all the Clusters you can have more than one running cluster at at",
    "start": "2481680",
    "end": "2488079"
  },
  {
    "text": "a time and all of them can be looking at the same as three objects so what happens with this is you don't have to",
    "start": "2488079",
    "end": "2494440"
  },
  {
    "text": "duplicate the data the data stays at one single copy reduces your cost and with",
    "start": "2494440",
    "end": "2499720"
  },
  {
    "text": "the transion Clusters you can compute and then shut it down as soon as you're",
    "start": "2499720",
    "end": "2505480"
  },
  {
    "text": "done Amazon EMR supports much more applications than traditional map produ it supports test spark and one of the",
    "start": "2505480",
    "end": "2512119"
  },
  {
    "text": "application it also supports is presto presto is used for for our uh ad",
    "start": "2512119",
    "end": "2518560"
  },
  {
    "text": "hoc usage for doing some query analysis and we and in this slide we would like",
    "start": "2518560",
    "end": "2523640"
  },
  {
    "text": "to K key attention to why we have a second metast store for Presto and we'll go in details of why we have to do",
    "start": "2523640",
    "end": "2532520"
  },
  {
    "text": "that so at this time with this part basic core fundamental architecture right with Amazon S3 for your storage",
    "start": "2532760",
    "end": "2539280"
  },
  {
    "text": "and EMR as your compute we are able to deploy this particular plattin at asuan",
    "start": "2539280",
    "end": "2544440"
  },
  {
    "text": "in US EU and Japan and some of the stats that we have is more than 50 plus business analyst and",
    "start": "2544440",
    "end": "2550760"
  },
  {
    "text": "Report users are carrying the data from our data link we have more than thousand ad hoc queries runs per day and we have",
    "start": "2550760",
    "end": "2557200"
  },
  {
    "text": "20 plus sources of data in our data Lake and 100 plus of ETL Hive jobs run every single day to convert the data from raw",
    "start": "2557200",
    "end": "2563960"
  },
  {
    "text": "data into quality or modeled data and we have more than 25 plus Park jobs to run some of the uh what we call like a churn",
    "start": "2563960",
    "end": "2571720"
  },
  {
    "text": "prediction or uh or lifetime value for some of the analytical needs and we have",
    "start": "2571720",
    "end": "2577240"
  },
  {
    "text": "more than 2 plus pedabytes in our data l in the next slide we'll talk about",
    "start": "2577240",
    "end": "2583040"
  },
  {
    "text": "some of the lessons learned during EMR so the key thing here is always when",
    "start": "2583040",
    "end": "2590359"
  },
  {
    "text": "you're creating an EMR hi database use S3 S3 path by default it takes the",
    "start": "2590359",
    "end": "2597160"
  },
  {
    "text": "master node IP space and when you destroy the cluster and bring the recluster app in the H meta store you",
    "start": "2597160",
    "end": "2603280"
  },
  {
    "text": "still have the same IP space so to have some you will have some issues with it we have faced some of those issues while",
    "start": "2603280",
    "end": "2608920"
  },
  {
    "text": "we migrating from one environment to another environment so we recommend using S3 path use external tables use a",
    "start": "2608920",
    "end": "2615839"
  },
  {
    "text": "single metast store and you can recover partitions into the data Lake from your S3 into",
    "start": "2615839",
    "end": "2622319"
  },
  {
    "text": "high Met store by running a simple M repair command or you can alter table and add partitions into it you have uh",
    "start": "2622319",
    "end": "2629880"
  },
  {
    "text": "three different end points for S3 normally one is S3 uh s3n and S s3a uh",
    "start": "2629880",
    "end": "2637240"
  },
  {
    "text": "for emrfs we choose we recommend you to choose S3 by default if not if that if",
    "start": "2637240",
    "end": "2643280"
  },
  {
    "text": "application doesn't support S3 you can choose s3n but s3a is not something that we would recommend to use it's not",
    "start": "2643280",
    "end": "2648480"
  },
  {
    "text": "supported at this point of time and you have clusters which are transient and long running uh we have scheduled our",
    "start": "2648480",
    "end": "2655119"
  },
  {
    "text": "clusters on and off uh by bringing up and down using python libraries and our Enterprise",
    "start": "2655119",
    "end": "2661680"
  },
  {
    "text": "scheduler one of the three things about uh that once you have the data when you process the data we would like to",
    "start": "2661760",
    "end": "2668359"
  },
  {
    "text": "compress the output so for compression we have chosen gzip as a highly compression but it's not splitable and",
    "start": "2668359",
    "end": "2674400"
  },
  {
    "text": "snappy for streaming data which has a low compression uh but it's a a splitable some of the key takeaways from",
    "start": "2674400",
    "end": "2681559"
  },
  {
    "text": "highways basically partition your data denormalize your",
    "start": "2681559",
    "end": "2687160"
  },
  {
    "text": "data enable speculative execution we have chosen file format orc",
    "start": "2687160",
    "end": "2692480"
  },
  {
    "text": "Park is also good orc for us is much more interoperable enable vectorization uh enable parallel",
    "start": "2692480",
    "end": "2699480"
  },
  {
    "text": "execution of jobs and uh enable your uh compression of your intermediate output",
    "start": "2699480",
    "end": "2706160"
  },
  {
    "text": "between map produce jobs between the mappers and uh also enable the joints",
    "start": "2706160",
    "end": "2712160"
  },
  {
    "text": "autocon convert joints what will enable is if you have a smaller table to join to a bigger table then the smaller table",
    "start": "2712160",
    "end": "2718800"
  },
  {
    "text": "can be spread across all of the data nodes it doesn't have to do the shuffling of the data and it can keep it",
    "start": "2718800",
    "end": "2723839"
  },
  {
    "text": "in memory and get you the results much faster",
    "start": "2723839",
    "end": "2727880"
  },
  {
    "text": "so now we have learned about processing of the data now we have processed data now how do we enable our business users",
    "start": "2729440",
    "end": "2734880"
  },
  {
    "text": "to access this data for which what we have leveraged this Presto which is a native application on EMR for low",
    "start": "2734880",
    "end": "2741559"
  },
  {
    "text": "latency queries some of the lessons that we learned for Presto was for us orc was",
    "start": "2741559",
    "end": "2747240"
  },
  {
    "text": "much more interoperable while you're working with uh presto presto supports predicate push",
    "start": "2747240",
    "end": "2753280"
  },
  {
    "text": "down so that it doesn't have to bring all of the data into the memory it can say hey if I'm looking for certain range",
    "start": "2753280",
    "end": "2758640"
  },
  {
    "text": "of data it can just look at that partition and get the data back to you so Presto is way faster than high when",
    "start": "2758640",
    "end": "2764680"
  },
  {
    "text": "test it supports low latency queries and uh it is a very memory intensive so we",
    "start": "2764680",
    "end": "2770200"
  },
  {
    "text": "we got better results when we used R3 2x large as our machine type and some of the things that we need",
    "start": "2770200",
    "end": "2776319"
  },
  {
    "text": "to watch out when you're doing Presto is when Hive supports bucketing and prto still is a challenge with bucketing the",
    "start": "2776319",
    "end": "2782680"
  },
  {
    "text": "based upon GitHub latest articles the new releases are going to support bucketing but as of when we were working",
    "start": "2782680",
    "end": "2788079"
  },
  {
    "text": "with it bucketing was a challenge and uh Presta was not able to handle complex data structures like maps that you can",
    "start": "2788079",
    "end": "2794680"
  },
  {
    "text": "Define or structures that you can Define in Hive so prestor was having a challenge but most of those things are",
    "start": "2794680",
    "end": "2799960"
  },
  {
    "text": "getting resolved and other thing is when you're doing huge data set joints based upon your node type how much memory you",
    "start": "2799960",
    "end": "2806359"
  },
  {
    "text": "have the joints there is a memory limit how much you have so an R3 2x large I",
    "start": "2806359",
    "end": "2811720"
  },
  {
    "text": "think it is 30 gigs of your memory if your data size exceeds more than 30 gig then you you need to do much more a",
    "start": "2811720",
    "end": "2817240"
  },
  {
    "text": "little bit more configurations on it and uh Presto still has I mean latest",
    "start": "2817240",
    "end": "2822680"
  },
  {
    "text": "release of presto it has been taken care of but in the older versions of presto you have a challenge of mapping the H",
    "start": "2822680",
    "end": "2829680"
  },
  {
    "text": "data types to presto data types some of those are like floats characters and",
    "start": "2829680",
    "end": "2835480"
  },
  {
    "text": "vasts some of the settings that will give you a better optimal results is how much a single query can take uh a memory",
    "start": "2835480",
    "end": "2842440"
  },
  {
    "text": "across all of your data nodes and what is your timeout of a sing s Le query and the age of the query so age and timeout",
    "start": "2842440",
    "end": "2848760"
  },
  {
    "text": "are the key if your timeout is lower than amount of time it executes the query then you will",
    "start": "2848760",
    "end": "2855040"
  },
  {
    "text": "not get any results so you need to make sure that your age and time out have been set appropriately and also enable",
    "start": "2855040",
    "end": "2860839"
  },
  {
    "text": "uh we have what we have learned is 42% of our memory can be allocated for for",
    "start": "2860839",
    "end": "2865920"
  },
  {
    "text": "each query on a given data Note with the latest release of orc 2 or",
    "start": "2865920",
    "end": "2873559"
  },
  {
    "text": "with the latest of orc drivers uh The Sur days we were able to see much more Improvement in pushing down with the orc",
    "start": "2873559",
    "end": "2881119"
  },
  {
    "text": "files of the searches uh when we are having enable the the bloom filters and optimize the Rader and uh for the Orcs",
    "start": "2881119",
    "end": "2889040"
  },
  {
    "text": "so these are some of the key lessons that we have learned during our journey with the prto and now we'll talk about why we",
    "start": "2889040",
    "end": "2896559"
  },
  {
    "text": "have to create a separate metast store for prto as you guess in the previous slide we talked about hi has certain",
    "start": "2896559",
    "end": "2902119"
  },
  {
    "text": "data types which are not all compatible with the prto in order to make that change you have to create again a table",
    "start": "2902119",
    "end": "2908240"
  },
  {
    "text": "structure in Hive to handle that or in pressor to handle that so what we have done is created an automation routine",
    "start": "2908240",
    "end": "2913960"
  },
  {
    "text": "which takes the hive metast store converts the data types and then puts it into another meta store and recovers all",
    "start": "2913960",
    "end": "2920200"
  },
  {
    "text": "of the partitions from it so in this case what happens is if a presto compatibility is not there those fields",
    "start": "2920200",
    "end": "2926319"
  },
  {
    "text": "will not be available in Presto this will enable you to get that particular data bag with the latest release of EMR",
    "start": "2926319",
    "end": "2931839"
  },
  {
    "text": "0.152 most of these challenges have been resolved",
    "start": "2931839",
    "end": "2937599"
  },
  {
    "text": "so now we'll talk about uh sandboxes how you can enable sandboxes for your development uh resources or QA resources",
    "start": "2938119",
    "end": "2945119"
  },
  {
    "text": "on for your data analysts or data scientists as we talked before Mo the in",
    "start": "2945119",
    "end": "2950720"
  },
  {
    "text": "traditional world you have to do test data management with help of EMR and S3",
    "start": "2950720",
    "end": "2956200"
  },
  {
    "text": "we are able to mitigate that challenge the way we mitigated that challenge here is in the production environment we have",
    "start": "2956200",
    "end": "2962280"
  },
  {
    "text": "the production S3 storage and have its production more clusters you see the maroon S3 storage that is",
    "start": "2962280",
    "end": "2968440"
  },
  {
    "text": "the sandbox storage which is also on S3 but on a different bucket and you have each sandbox can has its own EMR",
    "start": "2968440",
    "end": "2976200"
  },
  {
    "text": "clusters why do we need to have its own EMR clusters right the reason why we need to have an EMR clusters is you can",
    "start": "2976200",
    "end": "2983079"
  },
  {
    "text": "have ywn to configure with your schedules when when users are submitting jobs to the ywn you have to put in your",
    "start": "2983079",
    "end": "2990000"
  },
  {
    "text": "queue name to so that hey use this default queue so that I can have these resources it's very it was a challenge",
    "start": "2990000",
    "end": "2995480"
  },
  {
    "text": "alling for us to train our users so we were able to mitigate that challenge by",
    "start": "2995480",
    "end": "3000599"
  },
  {
    "text": "providing each group of users a standard dedicated yourr cluster for them to analyze their own data and we were able",
    "start": "3000599",
    "end": "3009200"
  },
  {
    "text": "to handle security by having separate metast store for sandbox one for",
    "start": "3009200",
    "end": "3015040"
  },
  {
    "text": "production and one for sandboxes and we have we can have more than one sandbox",
    "start": "3015040",
    "end": "3020599"
  },
  {
    "text": "and each sandbox can have all of the sandboxes will have one single meta store and you can sync your production",
    "start": "3020599",
    "end": "3027119"
  },
  {
    "text": "data metast store to the dev metast store and from security perspective you",
    "start": "3027119",
    "end": "3032680"
  },
  {
    "text": "can make sure uh with the IM IM policies the production es to have only read only",
    "start": "3032680",
    "end": "3038160"
  },
  {
    "text": "and read that access to San 3s boxes so below you'll see a small script that will help you to create an neomar",
    "start": "3038160",
    "end": "3043640"
  },
  {
    "text": "cluster through command line where you can set up a policy while you're launching an neomar cluster at the last",
    "start": "3043640",
    "end": "3049200"
  },
  {
    "text": "line which is called instance profile EMR E2 sandbox one roll one so dur in that role you can specify",
    "start": "3049200",
    "end": "3056359"
  },
  {
    "text": "an IM am policy stating that hey deny all delete object and put out put",
    "start": "3056359",
    "end": "3062400"
  },
  {
    "text": "actions on your production bucket so that makes your production bucket secure at the same time you can have the delete",
    "start": "3062400",
    "end": "3069000"
  },
  {
    "text": "and write objects on your sandbox so that they can have a processed output in the in their localized",
    "start": "3069000",
    "end": "3076119"
  },
  {
    "text": "environments now with all of the users having their own sandboxes now we wanted",
    "start": "3076119",
    "end": "3081319"
  },
  {
    "text": "to have a provision mechanism to manage cost and also have the ability to scale",
    "start": "3081319",
    "end": "3086520"
  },
  {
    "text": "up and scale down our clusters automatically based upon the workload so with the latest release of",
    "start": "3086520",
    "end": "3092440"
  },
  {
    "text": "the Amazon EMR Auto scaling it has some of the uh metrics available in the cloud",
    "start": "3092440",
    "end": "3098480"
  },
  {
    "text": "watch uh as like is the cluster idle number of container spending so those are all out of the box",
    "start": "3098480",
    "end": "3105520"
  },
  {
    "text": "what we have also leveraged is to create a CPU usage as a custom metrics and you can also Define the minimum and the",
    "start": "3105520",
    "end": "3111520"
  },
  {
    "text": "maximum count of your cluster to scale up and scale down and when should it occur when your load is greater than 60%",
    "start": "3111520",
    "end": "3117599"
  },
  {
    "text": "or 80% so here is the sample graph in working that we were able to using ganglia ganglia is uh distributed metric",
    "start": "3117599",
    "end": "3125079"
  },
  {
    "text": "collector which is a native application on EMR where it gives you based upon your EMR jobs running what is the load",
    "start": "3125079",
    "end": "3132559"
  },
  {
    "text": "it is taking and the next graph next to it which is on the other side of the CPU",
    "start": "3132559",
    "end": "3138119"
  },
  {
    "text": "graph talks about your map produce increasing and decreasing based upon the load so we have for this particular a",
    "start": "3138119",
    "end": "3145400"
  },
  {
    "text": "scaling is done through CPU metric so CPU metrix is not available out of the standard Cloud watch metric so we have",
    "start": "3145400",
    "end": "3152040"
  },
  {
    "text": "used ganglia to get those things in the next slide we'll talk about how to use ganglia to get that metrics so ganglia",
    "start": "3152040",
    "end": "3158880"
  },
  {
    "text": "has a URL where you can ask in the response in as a Json where it gives you",
    "start": "3158880",
    "end": "3164000"
  },
  {
    "text": "the CPU usage by minute so we take the data using Lambda we are able to compute",
    "start": "3164000",
    "end": "3169760"
  },
  {
    "text": "the per minute CPU usage and it's a Snapple sample snippet of code where you",
    "start": "3169760",
    "end": "3175079"
  },
  {
    "text": "can can calculate the average minute CPU usage once you have that CPU",
    "start": "3175079",
    "end": "3181558"
  },
  {
    "text": "usage you can create a cloud watch alarm we were able to leverage again the Lambda to create a cloud watch alarm to",
    "start": "3182680",
    "end": "3188760"
  },
  {
    "text": "create to write two Cloud watch as a custom metric of one every minute CPU",
    "start": "3188760",
    "end": "3193960"
  },
  {
    "text": "usage with that we are able to create as a metric to do scale up and scale down",
    "start": "3193960",
    "end": "3201079"
  },
  {
    "text": "here is a sample slide that demonstrates the cost savings that we were able to uh accomplished on an average we're able to",
    "start": "3201079",
    "end": "3207400"
  },
  {
    "text": "get 55% of cost savings during with EMR Auto",
    "start": "3207400",
    "end": "3212839"
  },
  {
    "text": "scaling and that is we also have reserved instances in this particular scenario but if you take reserved",
    "start": "3212839",
    "end": "3218240"
  },
  {
    "text": "instances off you would have 40% of the cost savings so now we'll talk about uh data",
    "start": "3218240",
    "end": "3225720"
  },
  {
    "text": "virtualization so we have used data virtualization as a single layer where you can configure security so this",
    "start": "3225720",
    "end": "3232480"
  },
  {
    "text": "security enables authentication and authorization ation for the end",
    "start": "3232480",
    "end": "3238760"
  },
  {
    "text": "users here is a typical architecture that we have using we have encrypted all of our data",
    "start": "3239040",
    "end": "3245240"
  },
  {
    "text": "at the field level before putting into Amazon S3 once we have the data there we are able to process it and persist in",
    "start": "3245240",
    "end": "3251680"
  },
  {
    "text": "the higher layers we also have Presto for users to query the data at",
    "start": "3251680",
    "end": "3258280"
  },
  {
    "text": "hawklink some of the model data based upon slas have been also moved into red shift now you have at two different",
    "start": "3258280",
    "end": "3263880"
  },
  {
    "text": "places the data one some is in S3 some of that is in uh red shift how does a user know which data is where and how do",
    "start": "3263880",
    "end": "3270680"
  },
  {
    "text": "I do their cross Federation joints so for which what we have leveraged is a a data virtualization layer where you can",
    "start": "3270680",
    "end": "3277720"
  },
  {
    "text": "configure security authentication authorization and uh do data Federation",
    "start": "3277720",
    "end": "3284839"
  },
  {
    "text": "and keep all of these Technologies abstract from the end users so that tomorrow we might find a new technology",
    "start": "3284839",
    "end": "3290160"
  },
  {
    "text": "that helps us so we don't want our users to rewrite their quote so data virtualization helps helps us to write",
    "start": "3290160",
    "end": "3296160"
  },
  {
    "text": "traditional ANC SQL code so here are some of the advantages of using data virtualization so all of",
    "start": "3296160",
    "end": "3303280"
  },
  {
    "text": "the code that you are writing is going to be an SQL compliant and it can be pushed down to the end sources so that",
    "start": "3303280",
    "end": "3310880"
  },
  {
    "text": "if so for example if you're trying to push get data from prestor all of the query goes into prestor prestor does all",
    "start": "3310880",
    "end": "3316359"
  },
  {
    "text": "of the work and brings it back if for certain reason if you want to do data set joints between Presto and red shift",
    "start": "3316359",
    "end": "3322880"
  },
  {
    "text": "so it will try to do push down as much as possible possible to both of those sources get the final output and then do",
    "start": "3322880",
    "end": "3328359"
  },
  {
    "text": "the joint as minimal as possible in the data virtualization so you can also enable colum and role level security in",
    "start": "3328359",
    "end": "3334280"
  },
  {
    "text": "the data virtualization and we can enable authentication against your on premise",
    "start": "3334280",
    "end": "3340119"
  },
  {
    "text": "or your existing L app for uh",
    "start": "3340119",
    "end": "3344440"
  },
  {
    "text": "compliance so all the ad queries and reports are through jdbc and obbc against Amazon red shift and Presto so",
    "start": "3346039",
    "end": "3352359"
  },
  {
    "text": "as you was talking before you don't have to worry about configuring security at multiple sources configuring it one",
    "start": "3352359",
    "end": "3358520"
  },
  {
    "text": "maintaining it one becomes much more economical and feasible so some of the key takeaways",
    "start": "3358520",
    "end": "3363559"
  },
  {
    "text": "that what we have learned during our journey is manage cost from getg goo so create tags to all of your resources",
    "start": "3363559",
    "end": "3370240"
  },
  {
    "text": "manage them accordingly do real-time frictional scaling using uh EMR Auto",
    "start": "3370240",
    "end": "3375720"
  },
  {
    "text": "scaling capabilities align to Core Design patterns like uh platform as a",
    "start": "3375720",
    "end": "3382359"
  },
  {
    "text": "service leverage readily available solution than rebuilding those things and designed for security and compliance",
    "start": "3382359",
    "end": "3388440"
  },
  {
    "text": "from start as we all know the security is going to change and evolve as the privacy of the people has become much",
    "start": "3388440",
    "end": "3394480"
  },
  {
    "text": "more uh uh what I call intrusive or with the latest changes with brexit and uh",
    "start": "3394480",
    "end": "3401760"
  },
  {
    "text": "LinkedIn and fail forward and adjust as needed and Harden your Solutions in One Marketplace and then deploy into other",
    "start": "3401760",
    "end": "3408520"
  },
  {
    "text": "Market spaces with this I'm going to hand it over to",
    "start": "3408520",
    "end": "3413880"
  },
  {
    "text": "Jonathan want to say uh thanks everybody for uh coming to the presentation um remember to complete your evaluations",
    "start": "3413880",
    "end": "3420400"
  },
  {
    "text": "we'll be here to take uh some questions um afterwards as well thank you",
    "start": "3420400",
    "end": "3427279"
  }
]