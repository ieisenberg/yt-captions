[
  {
    "text": "(upbeat music)",
    "start": "0",
    "end": "2583"
  },
  {
    "text": "- Hey, hey, this is Dr. Nashlie Sephus,",
    "start": "3630",
    "end": "5940"
  },
  {
    "text": "and I'm a Principal AI Scientist\nat Amazon Web Services.",
    "start": "5940",
    "end": "9480"
  },
  {
    "text": "I specialize in responsible AI,",
    "start": "9480",
    "end": "11610"
  },
  {
    "text": "and today we're gonna talk to Alicia, Mia,",
    "start": "11610",
    "end": "14580"
  },
  {
    "text": "as well as Valeria,",
    "start": "14580",
    "end": "15780"
  },
  {
    "text": "who work on the responsible AI team",
    "start": "15780",
    "end": "17699"
  },
  {
    "text": "about the work that they've done,",
    "start": "17700",
    "end": "19170"
  },
  {
    "text": "and we're gonna show you",
    "start": "19170",
    "end": "20100"
  },
  {
    "text": "how you can get more involved as well.",
    "start": "20100",
    "end": "21876"
  },
  {
    "text": "(upbeat music)",
    "start": "21876",
    "end": "24060"
  },
  {
    "text": "You were a part of a tutorial\nfor the FAccT Conference,",
    "start": "24060",
    "end": "28020"
  },
  {
    "text": "and so I want you to\ntell us a little bit more",
    "start": "28020",
    "end": "29550"
  },
  {
    "text": "about that tutorial\nand what you did there.",
    "start": "29550",
    "end": "32460"
  },
  {
    "text": "- We wanted to present some\ninformation to the community",
    "start": "32460",
    "end": "35040"
  },
  {
    "text": "about how generative AI",
    "start": "35040",
    "end": "37590"
  },
  {
    "text": "has changed the landscape\nof responsible AI.",
    "start": "37590",
    "end": "41580"
  },
  {
    "text": "So here, you can see an example",
    "start": "41580",
    "end": "43200"
  },
  {
    "text": "of what a large language model does.",
    "start": "43200",
    "end": "45180"
  },
  {
    "text": "It's just gonna choose\nthe next most likely word.",
    "start": "45180",
    "end": "48060"
  },
  {
    "text": "Given that we have\nstudents in the context,",
    "start": "48060",
    "end": "50460"
  },
  {
    "text": "the model is guided more towards books",
    "start": "50460",
    "end": "52920"
  },
  {
    "text": "as the most likely application,",
    "start": "52920",
    "end": "54690"
  },
  {
    "text": "followed by laptops.",
    "start": "54690",
    "end": "56309"
  },
  {
    "text": "And what's really amazing about this",
    "start": "56310",
    "end": "58320"
  },
  {
    "text": "is you can now add books to the context,",
    "start": "58320",
    "end": "61350"
  },
  {
    "text": "and you can generate\nanother word, and another,",
    "start": "61350",
    "end": "63989"
  },
  {
    "text": "until you have a phrase,\na sentence, a novel.",
    "start": "63990",
    "end": "67560"
  },
  {
    "text": "If you were to compare this\nto a more traditional AI model",
    "start": "67560",
    "end": "71399"
  },
  {
    "text": "for something like consumer lending,",
    "start": "71400",
    "end": "74160"
  },
  {
    "text": "we're gonna look at a training\nset of loan applications",
    "start": "74160",
    "end": "78000"
  },
  {
    "text": "where we know the truth,",
    "start": "78000",
    "end": "79410"
  },
  {
    "text": "whether the applicant did or\ndid not pay back the loan,",
    "start": "79410",
    "end": "83400"
  },
  {
    "text": "and we're gonna train a model\nto make binary decisions,",
    "start": "83400",
    "end": "86520"
  },
  {
    "text": "yes or no.",
    "start": "86520",
    "end": "87479"
  },
  {
    "text": "Given a new application,",
    "start": "87480",
    "end": "88890"
  },
  {
    "text": "do we think that this new\napplicant is likely to default?",
    "start": "88890",
    "end": "92070"
  },
  {
    "text": "So a model that can spit out 1s and 0s",
    "start": "92070",
    "end": "95190"
  },
  {
    "text": "in response to a very structured input",
    "start": "95190",
    "end": "97320"
  },
  {
    "text": "of a loan application,",
    "start": "97320",
    "end": "98940"
  },
  {
    "text": "that can still go wrong,",
    "start": "98940",
    "end": "100170"
  },
  {
    "text": "and we've seen it go wrong\nif it doesn't approve loans",
    "start": "100170",
    "end": "104430"
  },
  {
    "text": "from certain communities",
    "start": "104430",
    "end": "105930"
  },
  {
    "text": "the same rate that it\ndoes in other communities.",
    "start": "105930",
    "end": "108120"
  },
  {
    "text": "We have algorithmic solutions",
    "start": "108120",
    "end": "110130"
  },
  {
    "text": "to make those types of\ndecisions more fair.",
    "start": "110130",
    "end": "112680"
  },
  {
    "text": "When we think about a generative model,",
    "start": "112680",
    "end": "114810"
  },
  {
    "text": "like the one you just saw,",
    "start": "114810",
    "end": "116490"
  },
  {
    "text": "it's newer and it's more nuanced",
    "start": "116490",
    "end": "118560"
  },
  {
    "text": "to try to even define\nwhat fairness means there.",
    "start": "118560",
    "end": "121470"
  },
  {
    "text": "- For example,",
    "start": "121470",
    "end": "122640"
  },
  {
    "text": "if someone applies for a\nloan and they're denied,",
    "start": "122640",
    "end": "125190"
  },
  {
    "text": "they're gonna wanna know why.",
    "start": "125190",
    "end": "126780"
  },
  {
    "text": "But if the institution can't explain,",
    "start": "126780",
    "end": "129240"
  },
  {
    "text": "so we're lacking explainability,\nthen that's a problem.",
    "start": "129240",
    "end": "132450"
  },
  {
    "text": "Could you talk a little bit more",
    "start": "132450",
    "end": "133349"
  },
  {
    "text": "about how you classify those\nthings that could go wrong,",
    "start": "133350",
    "end": "136620"
  },
  {
    "text": "and what can we do about it?",
    "start": "136620",
    "end": "138239"
  },
  {
    "text": "- We think about genders,\nwe think about ethnicities,",
    "start": "138240",
    "end": "140550"
  },
  {
    "text": "experiences of people in\ndifferent geographies,",
    "start": "140550",
    "end": "143250"
  },
  {
    "text": "context and the organizational context",
    "start": "143250",
    "end": "145440"
  },
  {
    "text": "where you're making these REI\ndecisions has a big effect.",
    "start": "145440",
    "end": "149190"
  },
  {
    "text": "And a lot of it does have to do",
    "start": "149190",
    "end": "150480"
  },
  {
    "text": "with what you just mentioned,",
    "start": "150480",
    "end": "151590"
  },
  {
    "text": "like what can go wrong and\nwho is harmed in the process.",
    "start": "151590",
    "end": "154860"
  },
  {
    "text": "- It boils down to context",
    "start": "154860",
    "end": "157552"
  },
  {
    "text": "and really understanding your use case,",
    "start": "157552",
    "end": "159660"
  },
  {
    "text": "so that you can better tell:",
    "start": "159660",
    "end": "161970"
  },
  {
    "text": "Is this a risk, a higher\nrisk, a lower risk?",
    "start": "161970",
    "end": "165030"
  },
  {
    "text": "- And there are actions that we can take",
    "start": "165030",
    "end": "166709"
  },
  {
    "text": "to mitigate the risk",
    "start": "166710",
    "end": "167730"
  },
  {
    "text": "at a variety of stages of\ndevelopment of the model.",
    "start": "167730",
    "end": "171000"
  },
  {
    "text": "We can investigate our data.",
    "start": "171000",
    "end": "173040"
  },
  {
    "text": "Is our data representative",
    "start": "173040",
    "end": "174750"
  },
  {
    "text": "of all the customers we expect to use it?",
    "start": "174750",
    "end": "177330"
  },
  {
    "text": "And then your algorithm,",
    "start": "177330",
    "end": "179160"
  },
  {
    "text": "if you do have even a small\namount of underrepresentation",
    "start": "179160",
    "end": "183480"
  },
  {
    "text": "or bias in your data set,",
    "start": "183480",
    "end": "185040"
  },
  {
    "text": "is your algorithm amplifying that bias?",
    "start": "185040",
    "end": "188370"
  },
  {
    "text": "We can measure bias at\nevery training iteration,",
    "start": "188370",
    "end": "190860"
  },
  {
    "text": "so that we're optimizing\nnot only for accuracy,",
    "start": "190860",
    "end": "193260"
  },
  {
    "text": "but also for the distribution of accuracy",
    "start": "193260",
    "end": "195930"
  },
  {
    "text": "across different groups.",
    "start": "195930",
    "end": "197129"
  },
  {
    "text": "Part of the solution",
    "start": "197130",
    "end": "198090"
  },
  {
    "text": "is also making sure you use these AI tools",
    "start": "198090",
    "end": "201360"
  },
  {
    "text": "in the context of a workflow\nthat includes humans",
    "start": "201360",
    "end": "204120"
  },
  {
    "text": "that are also applying their own judgment.",
    "start": "204120",
    "end": "206278"
  },
  {
    "text": "(pensive music)",
    "start": "206278",
    "end": "209028"
  }
]