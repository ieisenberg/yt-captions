[
  {
    "text": "now first of all I'm not going to assume that everyone here is convinced that that database is the right place to be",
    "start": "630",
    "end": "5819"
  },
  {
    "text": "running your data but those so let me talk some of the benefits of moving it to the cloud",
    "start": "5819",
    "end": "11360"
  },
  {
    "text": "first of all performance and scalability now I've used term RDS without sort of",
    "start": "11360",
    "end": "18449"
  },
  {
    "text": "defining whether I should do that first RDS is a relational database service it's a managed service and it's a",
    "start": "18449",
    "end": "23910"
  },
  {
    "text": "service we Devine design to make it as easy as possible to set up a database to operate the database in scale a database",
    "start": "23910",
    "end": "30660"
  },
  {
    "text": "on the cloud RDS supports 14 different",
    "start": "30660",
    "end": "36090"
  },
  {
    "text": "instance types they range from a from a t1 micro which has got a gigabyte of RAM",
    "start": "36090",
    "end": "41160"
  },
  {
    "text": "singles it's V CPU right through to the r3 family which go up to 240 and 4",
    "start": "41160",
    "end": "46440"
  },
  {
    "text": "gigabytes and 32 V CPUs now I've got some customers that for which effort",
    "start": "46440",
    "end": "51449"
  },
  {
    "text": "where they're running s AP Hana and that's not quite big enough they want to run X once you've certainly got the",
    "start": "51449",
    "end": "56879"
  },
  {
    "text": "option available to you but bear in mind that that that that's all available within the RDS family you'll need to",
    "start": "56879",
    "end": "62370"
  },
  {
    "text": "manage those things yourself now one of the nice things about the way about RDS",
    "start": "62370",
    "end": "69000"
  },
  {
    "text": "is that if you're doing that sort of vertical scaling up and down in in essence families you can you can",
    "start": "69000",
    "end": "74549"
  },
  {
    "text": "automate the whole process so open windows sorry ok you can set up your",
    "start": "74549",
    "end": "81810"
  },
  {
    "text": "Maitlis windows so that you can schedule your your scale up and scale down operations to happen",
    "start": "81810",
    "end": "87810"
  },
  {
    "text": "Tapan in a downtime which is a pretty magical sort of thing if you come from an on-premise environment where you're",
    "start": "87810",
    "end": "93450"
  },
  {
    "text": "having to having to provision memory and schedule extended outages the other the other way you can scale",
    "start": "93450",
    "end": "99659"
  },
  {
    "text": "are the SS is through horizontally through read replicas so if you've got a workload that some that's that's very",
    "start": "99659",
    "end": "105479"
  },
  {
    "text": "read intensive you can set up a farm of a wreath as synchronous read replicas the driven off the master database and",
    "start": "105479",
    "end": "112680"
  },
  {
    "text": "just reserve your month based resources actually handling the commit transaction the update transactions now as well as",
    "start": "112680",
    "end": "120030"
  },
  {
    "text": "keeping as well as a lot of what said I'm ok the earlier he's dropping out as",
    "start": "120030",
    "end": "127619"
  },
  {
    "text": "well as making sure your databases can scale there's a set of cloud watch metrics the go alongside RDS that",
    "start": "127619",
    "end": "133660"
  },
  {
    "text": "tell you about the health of your database now depending on the database engine there's between 15 and 18 of them",
    "start": "133660",
    "end": "139090"
  },
  {
    "text": "a lot they include the standard sort of cloud watch hypervisor type metrics with CPU and disk i/o and network but they",
    "start": "139090",
    "end": "145960"
  },
  {
    "text": "also include some metrics into what your database engine is actually doing so things like memory utilization number of",
    "start": "145960",
    "end": "151750"
  },
  {
    "text": "connections read replicas lag the amount of free disk space so and those and",
    "start": "151750",
    "end": "157180"
  },
  {
    "text": "those metrics are down to it down to one minute resolution if you want to an enable enhanced monitoring you can",
    "start": "157180",
    "end": "162970"
  },
  {
    "text": "actually get into one second route resolution and boost that to about 50 different metrics so there's plenty you",
    "start": "162970",
    "end": "168100"
  },
  {
    "text": "can see what's going inside your engines now I mention though this is a managed service now think about your DBA is",
    "start": "168100",
    "end": "174040"
  },
  {
    "text": "there a scarce resource you really don't want them spending their time patching databases applying endless security",
    "start": "174040",
    "end": "179620"
  },
  {
    "text": "patches making sure the backups have have run the night before you really want to make sure that you really will",
    "start": "179620",
    "end": "185170"
  },
  {
    "text": "leave that that task to the RDS service into AWS and that's what we do",
    "start": "185170",
    "end": "191130"
  },
  {
    "text": "developmental and operational flexibility now some of my customers give me the best examples of this as",
    "start": "191130",
    "end": "196330"
  },
  {
    "text": "sometimes then they need to do a major application upgrade and they're quite nervous about it because their schema it",
    "start": "196330",
    "end": "201459"
  },
  {
    "text": "schema changes involved well they might be concerned about the performance X of a major upgrade and they want to make",
    "start": "201459",
    "end": "207490"
  },
  {
    "text": "sure before they launch that do that release on a production system that the things being tested so without es what you can do is just",
    "start": "207490",
    "end": "213820"
  },
  {
    "text": "take a snapshot of your production database and use that to launch a new instance run your tests against that new",
    "start": "213820",
    "end": "220660"
  },
  {
    "text": "instance and then once the tests are completed you shut it down you stop paying for it so it's a really great",
    "start": "220660",
    "end": "226030"
  },
  {
    "text": "great way to achieve some degree of operational flexibility and the last thing is fault tolerance and",
    "start": "226030",
    "end": "231580"
  },
  {
    "text": "availability now with our the years you've got the option of setting up a synchronous active standby replication",
    "start": "231580",
    "end": "238750"
  },
  {
    "text": "schedule that works across across multiple AZ's and it's as simple as",
    "start": "238750",
    "end": "244209"
  },
  {
    "text": "enabling checkbox and RDS now you can achieve much a sort of thing yourself of using MS sequel with always-on",
    "start": "244209",
    "end": "250630"
  },
  {
    "text": "availability groups but I can guarantee you there's a lot more than a single checkbox to turn on it's a lot of work",
    "start": "250630",
    "end": "256230"
  },
  {
    "text": "so that some gives you some sense about what an RDS database and RDS managed",
    "start": "256230",
    "end": "261459"
  },
  {
    "text": "service has got to offer you but how well some things she'd need to bear in mind when you're doing a migration",
    "start": "261459",
    "end": "267500"
  },
  {
    "text": "the first thing is business impact so think about how much downtime can your businesses sustain without starting to",
    "start": "267500",
    "end": "274880"
  },
  {
    "text": "impinge on the ESL A's you have with your customers what are some another another dimension is what's your current",
    "start": "274880",
    "end": "281150"
  },
  {
    "text": "licensing situation with your database software if you've got a whole bunch of perpetual licenses you're able to bring",
    "start": "281150",
    "end": "286310"
  },
  {
    "text": "them to AWS or do you need to use our licensing another thing is support contracts if you're using a third party",
    "start": "286310",
    "end": "291590"
  },
  {
    "text": "support support organization will they be able to support your databases when they move to AWS as well you need to",
    "start": "291590",
    "end": "299510"
  },
  {
    "text": "understand what's going on in your existing databases so I using some fairly complex the other types are you",
    "start": "299510",
    "end": "304520"
  },
  {
    "text": "using materialized views you need to understand a little bit if you're using",
    "start": "304520",
    "end": "309710"
  },
  {
    "text": "blobs for example what's the maximum size that those blobs could get to that can be very helpful to know later on",
    "start": "309710",
    "end": "316510"
  },
  {
    "text": "application complexity so a really crucial thing is to understand where is your application code actually running",
    "start": "316510",
    "end": "323000"
  },
  {
    "text": "how much is running in the database itself in the form of stored procedures or stored packages how much is running",
    "start": "323000",
    "end": "328340"
  },
  {
    "text": "outside of the database this is oftentimes a really good time to look at what your situation is without",
    "start": "328340",
    "end": "334430"
  },
  {
    "text": "third party software as to whether you can start running that software you can switch database engines as part of this",
    "start": "334430",
    "end": "340460"
  },
  {
    "text": "migration for example quite a few of my customers are running ESRI ArcGIS and on",
    "start": "340460",
    "end": "345680"
  },
  {
    "text": "on AMS sequel but they've got the option of moving that to Postgres are they yes because that's fully supported by ESRI",
    "start": "345680",
    "end": "352250"
  },
  {
    "text": "so they can save themselves a lot of licensing license fees by doing that migration as part of the AWS migration",
    "start": "352250",
    "end": "359770"
  },
  {
    "text": "integration points so if you're planning a migration you need to understand how your applications are connecting to the",
    "start": "359770",
    "end": "364970"
  },
  {
    "text": "database when you move that database into AWS you need to make a decision",
    "start": "364970",
    "end": "370970"
  },
  {
    "text": "there you going to move your applications simultaneously before or after because in those before/after",
    "start": "370970",
    "end": "376250"
  },
  {
    "text": "situations you'll be relying on the performance of your connectivity between where the applications are living on",
    "start": "376250",
    "end": "381710"
  },
  {
    "text": "premises and and with an AWS so think about the direct connect and VPN implications of that RPO and rzo so when",
    "start": "381710",
    "end": "390560"
  },
  {
    "text": "you move to AWS there's a whole range of options you've got for improving for enabling higher bail beliefs solutions",
    "start": "390560",
    "end": "396590"
  },
  {
    "text": "so think make sure that you're exploiting those solutions not you're not just relying on your care and tried and tested out dr strategy",
    "start": "396590",
    "end": "403400"
  },
  {
    "text": "which is assumes you've got multiple data centers and lastly skills required",
    "start": "403400",
    "end": "409430"
  },
  {
    "text": "so what what are the skills are going to need a transfer to your organization from the other use AWS so we've taken",
    "start": "409430",
    "end": "416240"
  },
  {
    "text": "those considerations into account and what we designed was was the DMS service and the dear mr. cert the service is",
    "start": "416240",
    "end": "423050"
  },
  {
    "text": "designed to be as easy as possible to migrate your database with minimal downtime that's that's the crucial thing",
    "start": "423050",
    "end": "428240"
  },
  {
    "text": "with minimal downtime it can move it can migrate your database from the most widely used commercial databases so",
    "start": "428240",
    "end": "434480"
  },
  {
    "text": "Oracle and sequel server are those two any number of and quite a few alternative um open source databases so",
    "start": "434480",
    "end": "442220"
  },
  {
    "text": "my sequel Postgres Arora in its my sequel lender or the announced Postgres",
    "start": "442220",
    "end": "449870"
  },
  {
    "text": "engines Maria DB it's recently we announced support for MongoDB conversions as well in DMS to the",
    "start": "449870",
    "end": "457130"
  },
  {
    "text": "DynamoDB and there's also LTP to redshift options as well now let's have",
    "start": "457130",
    "end": "464300"
  },
  {
    "text": "a look at what's actually involved in that in in that migration how do we keep your application running during a migration without input imposing",
    "start": "464300",
    "end": "471290"
  },
  {
    "text": "downtime so this is a really classic setup you've got your on-premises database on the left here you've got a",
    "start": "471290",
    "end": "477380"
  },
  {
    "text": "an Internet can be peeing over an internet connection or we might have a direct connect and then an AWS database",
    "start": "477380",
    "end": "483800"
  },
  {
    "text": "sitting within a V PC so there could be a self managed database on they seat or it could be in the earliest instance and",
    "start": "483800",
    "end": "490250"
  },
  {
    "text": "you've got your application users pointing at the on-premises database the first thing you do is you start a",
    "start": "490250",
    "end": "496250"
  },
  {
    "text": "replication instance within the DMS system and and what that D what that replication instance will do is connect",
    "start": "496250",
    "end": "502730"
  },
  {
    "text": "to both your on-premises database and your target database in AWS the next",
    "start": "502730",
    "end": "510170"
  },
  {
    "text": "thing you do is you select the schemas that you want the migrate you may not want to migrate the entire database you just might mother it my greater set of",
    "start": "510170",
    "end": "516800"
  },
  {
    "text": "schemas over and then you let the the DMS service go through a sequence of",
    "start": "516800",
    "end": "523849"
  },
  {
    "text": "creating the tables in the target database using some built-in rules doing a bulk load of data from the source",
    "start": "523849",
    "end": "530480"
  },
  {
    "text": "database into the target database and there's a high degree of parallel that happens there with multiple tables",
    "start": "530480",
    "end": "535920"
  },
  {
    "text": "being moved moved over simultaneously and then moving into us a CDC or continuous data replication mode after",
    "start": "535920",
    "end": "543180"
  },
  {
    "text": "that whereby we keep track of all the changes in the source and move them automatically to the target now a lot of",
    "start": "543180",
    "end": "549420"
  },
  {
    "text": "DBAs get nervous about that see the CDC process they wonder is the is the replication assistance sitting there",
    "start": "549420",
    "end": "555689"
  },
  {
    "text": "running big queries against the source databases are going to impinge on performance the answer is we've tried to",
    "start": "555689",
    "end": "560819"
  },
  {
    "text": "optimize that as much as possible than the data migration services basically lighting mining the logs for the",
    "start": "560819",
    "end": "566639"
  },
  {
    "text": "transaction logs on the source database to minimize the impact and this is feeding through the deltas over the",
    "start": "566639",
    "end": "573269"
  },
  {
    "text": "internet into the over the connection into the target database and then lastly",
    "start": "573269",
    "end": "579930"
  },
  {
    "text": "once the migration has actually happened then yes and you've run your tests you're happy that the everything's in",
    "start": "579930",
    "end": "585509"
  },
  {
    "text": "sync then simply a cname switching your DNS system to direct your users to the target database now the the DMS is a we",
    "start": "585509",
    "end": "597149"
  },
  {
    "text": "call the Swiss Army knife because it's it's quite versatile it's it can deal with commercial databases to do",
    "start": "597149",
    "end": "602579"
  },
  {
    "text": "homogeneous heterogeneous conversions it handles our OLTP to no sequel",
    "start": "602579",
    "end": "608610"
  },
  {
    "text": "conversions it now handles redshift it's quite a versatile tool but the sweet",
    "start": "608610",
    "end": "613920"
  },
  {
    "text": "spot for DMS is really heterogeneous migrations we in the minimal downtime and where you've got no native solution",
    "start": "613920",
    "end": "620370"
  },
  {
    "text": "now a counter example of that would be say if you're doing say an Oracle to Oracle conversion and you're already",
    "start": "620370",
    "end": "626399"
  },
  {
    "text": "licensed for Golden Gate that might be your best bet there might be a faster and safer option than um than DMS",
    "start": "626399",
    "end": "632449"
  },
  {
    "text": "another situation is if you if say you've got an MS sequel solution that you're wanting to migrate the gainer",
    "start": "632449",
    "end": "637860"
  },
  {
    "text": "homogeneous migration to AWS if you've got a fairly relaxed outage window and",
    "start": "637860",
    "end": "643350"
  },
  {
    "text": "you can afford to just take a backup just do that take a backup of your on-premise database upload it to s3 and",
    "start": "643350",
    "end": "649949"
  },
  {
    "text": "then restore it into an athiest instance and you're up and running and possibly if it's a modest-sized database you can",
    "start": "649949",
    "end": "655889"
  },
  {
    "text": "do that on the weekend but for other scenarios which to be honest - the most common ones we wanted to do we've got",
    "start": "655889",
    "end": "662160"
  },
  {
    "text": "minimal downtime and you're doing a heterogeneous migration DMS is a good",
    "start": "662160",
    "end": "667199"
  },
  {
    "text": "solution if you kept a few cautions you mentioned earlier that's it's helpful to",
    "start": "667199",
    "end": "673060"
  },
  {
    "text": "understand how big the blot if you're using blobs in your database how big they are that can help you tune the the",
    "start": "673060",
    "end": "679420"
  },
  {
    "text": "migration process you need to understand if you're using complex though types in your source system because some of them",
    "start": "679420",
    "end": "685150"
  },
  {
    "text": "might be problematic and we've got I'll cover in the moment how you identify those and also if you've got a high load",
    "start": "685150",
    "end": "691210"
  },
  {
    "text": "under your source databases if it's already under stress and that's what's actually driving your migration even",
    "start": "691210",
    "end": "696940"
  },
  {
    "text": "that that CDC process might need some what might need some tuning to make sure it minimizes the impact if anyone wants",
    "start": "696940",
    "end": "703900"
  },
  {
    "text": "to see me afterwards in the in the booth I'll tell you I'll give you some more stories and some secrets for doing that as well so dear mrs. isn't a magic wand",
    "start": "703900",
    "end": "712890"
  },
  {
    "text": "well I strongly recommend this that you consider what your source database is your target database data",
    "start": "712890",
    "end": "719140"
  },
  {
    "text": "databases go to the user guide and actually look up that combination and will carefully document all the",
    "start": "719140",
    "end": "725020"
  },
  {
    "text": "limitations and all the things that you need to be aware of when you're going from source source to target and and",
    "start": "725020",
    "end": "731170"
  },
  {
    "text": "read that and understand that because it'll save you a lot of time later on just some really common common issues if",
    "start": "731170",
    "end": "738220"
  },
  {
    "text": "you're using the auto increment feature of my sequel databases that won't be replicated to the target I've been",
    "start": "738220",
    "end": "745750"
  },
  {
    "text": "called out with with customers running Oracle databases where they have blobs or mutating blobs on tables with no",
    "start": "745750",
    "end": "752589"
  },
  {
    "text": "primary key that can cause an issue with CDs so you have to fine-tune the the configuration options there as well well",
    "start": "752589",
    "end": "759700"
  },
  {
    "text": "in any case there's a full dictionary of what all the various options are on the documentation now I mentioned that the",
    "start": "759700",
    "end": "765880"
  },
  {
    "text": "DMS tool is a is a good tool for doing your default schema conversion but if",
    "start": "765880",
    "end": "772209"
  },
  {
    "text": "you need to do something more sophisticated like if you want to do type mappings or rename introduce new",
    "start": "772209",
    "end": "777880"
  },
  {
    "text": "naming conventions and your schemers if you want to bring over complex objects like constraints and triggers and",
    "start": "777880",
    "end": "784000"
  },
  {
    "text": "indexes and most particularly if you have need to bring over store procedures so peel sequel transact sequel over to a",
    "start": "784000",
    "end": "790930"
  },
  {
    "text": "target database you need to use the schema conversion tool which is a complementary tool sits alongside DMS",
    "start": "790930",
    "end": "797130"
  },
  {
    "text": "that supports a wide range of options as well here's some of the source option for the scheme of conversion to all the",
    "start": "797130",
    "end": "802720"
  },
  {
    "text": "usual commercial databases recently we added to dynamodb support OLTP",
    "start": "802720",
    "end": "809230"
  },
  {
    "text": "sources - - Aurora redshift options are",
    "start": "809230",
    "end": "814810"
  },
  {
    "text": "also supported from all these commercial all that type databases so it's a very",
    "start": "814810",
    "end": "820270"
  },
  {
    "text": "versatile tool here's some of the outputs you'll get promote from the",
    "start": "820270",
    "end": "825850"
  },
  {
    "text": "schema conversion tool I'm about to go into a demo we'll see we'll see some real world examples here but I what I",
    "start": "825850",
    "end": "831370"
  },
  {
    "text": "strongly recommend here - if you're interested in the migration download the schema conversion tool tomorrow install",
    "start": "831370",
    "end": "838150"
  },
  {
    "text": "it on the PC within your own network pointer that your database you don't need any special database permissions",
    "start": "838150",
    "end": "843550"
  },
  {
    "text": "and get it to run that report and that report will look at all of your your schema objects your tables constraints",
    "start": "843550",
    "end": "850360"
  },
  {
    "text": "indexes sequences and then give you a sense of what how to what extent you be",
    "start": "850360",
    "end": "857050"
  },
  {
    "text": "able to automate that conversion to any one of three or four different target databases we'll do the same thing with",
    "start": "857050",
    "end": "862930"
  },
  {
    "text": "all these store procedures as well so that this is a this is an actual report I read earlier see that's a clean bill",
    "start": "862930",
    "end": "869380"
  },
  {
    "text": "of health with it with the schema objects a little bit of work that's needed will actually quite be the work that's needed on some of these package",
    "start": "869380",
    "end": "875560"
  },
  {
    "text": "procedures and and functions but it's it's a it's a really simple way to get",
    "start": "875560",
    "end": "881410"
  },
  {
    "text": "some sense about how big a task is ahead for you I won't go through all these",
    "start": "881410",
    "end": "886420"
  },
  {
    "text": "that it's a rapidly evolving list even since I did the slide there's some Sybase ASE s be sorry I'm sa P ASC",
    "start": "886420",
    "end": "894459"
  },
  {
    "text": "forming on the side base has been added to the list the version is constantly evolving general theme is that the",
    "start": "894459",
    "end": "900610"
  },
  {
    "text": "supports a wide range of commercial commercial to open-source conversions open-source to open-source conversions",
    "start": "900610",
    "end": "907029"
  },
  {
    "text": "within the conversion tool and then um it also supports as recently as this",
    "start": "907029",
    "end": "914020"
  },
  {
    "text": "year we've added the whole range of all that type databases so Teradata Vertica",
    "start": "914020",
    "end": "920130"
  },
  {
    "text": "the teaser greenplum to redshift conversions I mentioned already the dynamo to MongoDB",
    "start": "920130",
    "end": "926110"
  },
  {
    "text": "conversions signal server data warehouse Oracle data warehouse so I was really putting a lot of investment in this tool",
    "start": "926110",
    "end": "931990"
  },
  {
    "text": "to cover the a wide range of scenario now as for the what does a what does a",
    "start": "931990",
    "end": "937910"
  },
  {
    "text": "notional work primary conversion look like this is a typical sort of task list",
    "start": "937910",
    "end": "942920"
  },
  {
    "text": "from my experience it's actually pretty accurate the big-ticket items in this can you know in a in a migration process",
    "start": "942920",
    "end": "950990"
  },
  {
    "text": "a really application conversion and remediation so if you do have a lot of complex sequel sitting in store",
    "start": "950990",
    "end": "958580"
  },
  {
    "text": "procedures or stored functions you might not be spending the time tweaking some of the work that the convert the ordem",
    "start": "958580",
    "end": "964070"
  },
  {
    "text": "aid work that the schema conversion tool does and then finally there's no escaping the fact you have to test it if",
    "start": "964070",
    "end": "969680"
  },
  {
    "text": "you're gonna convert the schema it's a larger complex undertaking you need to run some regression tests to make sure that the conversions worked okay so let",
    "start": "969680",
    "end": "979010"
  },
  {
    "text": "me show you what it looks like in in practice so here's a here's the test",
    "start": "979010",
    "end": "984290"
  },
  {
    "text": "setup this is a very common sort of migrations or fairly simple migration schema I'm using a sample database",
    "start": "984290",
    "end": "990800"
  },
  {
    "text": "sample organization called stadium sports they're a ticketing ticketing organization they're running in the data",
    "start": "990800",
    "end": "996530"
  },
  {
    "text": "center they've got an Oracle 12c database the demo I'm running has got about 20 gigabytes of data I've got the",
    "start": "996530",
    "end": "1002650"
  },
  {
    "text": "schema conversion tool don't download and install on Prem on the in the target",
    "start": "1002650",
    "end": "1008050"
  },
  {
    "text": "AWS side I've got a target either yes sorry a target Postgres RDS instance",
    "start": "1008050",
    "end": "1013900"
  },
  {
    "text": "it's sitting within the VPC and I've got a direct connect between them now for",
    "start": "1013900",
    "end": "1019150"
  },
  {
    "text": "the purpose of the demo I've just replaced the I'm using a separate V PC the host the the Oracle 12c database I'm",
    "start": "1019150",
    "end": "1026319"
  },
  {
    "text": "using a peering connection rather than direct connect but the principles are exactly the same so what's going to",
    "start": "1026320",
    "end": "1034810"
  },
  {
    "text": "happen is the schema conversion tool will connect to their source database and this is where I'm gonna run my assessment process I'm then going to",
    "start": "1034810",
    "end": "1041170"
  },
  {
    "text": "connect the schema conversion tool to the target database to to create a",
    "start": "1041170",
    "end": "1047319"
  },
  {
    "text": "tweaked schema in the target so I'm not just go for the purpose of the demo I'm not just going to be satisfied with the",
    "start": "1047320",
    "end": "1053500"
  },
  {
    "text": "default conversion to want to mess around with a little bit the next thing is the the DMS instance is going to",
    "start": "1053500",
    "end": "1060460"
  },
  {
    "text": "connect to the source it's going to connect to the target and now I'm going to use the schema conversion tool to actually control that",
    "start": "1060460",
    "end": "1066400"
  },
  {
    "text": "replication instance so this schema can be we'll be the dashboard as such for the actual whole process now that's the the",
    "start": "1066400",
    "end": "1075980"
  },
  {
    "text": "scheme where I'll be converting it's actually a real-world schemer it's not super complex but it's complex enough to sort of proof prove some of the",
    "start": "1075980",
    "end": "1082190"
  },
  {
    "text": "principles so the first thing is the assessment report I think you've hit",
    "start": "1082190",
    "end": "1088040"
  },
  {
    "text": "hitting play D so the this is the schema conversion tool assessment report the first thing I do is launch the project",
    "start": "1088040",
    "end": "1093830"
  },
  {
    "text": "wizard I give it a name and then I tell it that I want to I want to do an",
    "start": "1093830",
    "end": "1098900"
  },
  {
    "text": "assessment of an Oracle source database and the button there says oh I don't want the chat at this stage I wanted to",
    "start": "1098900",
    "end": "1105350"
  },
  {
    "text": "look at other engine options so I want to look at some open source options I enter some my Oracle credentials they",
    "start": "1105350",
    "end": "1112100"
  },
  {
    "text": "need to be specially privileged to this they be able to see the UM see the schema and then enumerate the UM the",
    "start": "1112100",
    "end": "1117260"
  },
  {
    "text": "objects click Next I give me a list of",
    "start": "1117260",
    "end": "1125690"
  },
  {
    "text": "all the scheme is available for assessment I'm gonna pick the DMS sample which is that sports ticketing database",
    "start": "1125690",
    "end": "1130700"
  },
  {
    "text": "and then what the the tool is going to do is it's going to enumerate all of the tables the constraints the indexes",
    "start": "1130700",
    "end": "1137990"
  },
  {
    "text": "sequences in that source database and for each of those objects it's going to assess to what extent those objects",
    "start": "1137990",
    "end": "1144050"
  },
  {
    "text": "can be automatically converted to the target schema and it'll use as a full point scheme so a one end of the extreme",
    "start": "1144050",
    "end": "1150740"
  },
  {
    "text": "it's a fully automated conversion at the other extreme the conversions going to need a little bit of it's going to need",
    "start": "1150740",
    "end": "1155840"
  },
  {
    "text": "significant actions and it's going to indicate that in the assessment report and it will generate the assessment",
    "start": "1155840",
    "end": "1161360"
  },
  {
    "text": "report for three different target schemas three different target engines this is the report for the my sequel",
    "start": "1161360",
    "end": "1168470"
  },
  {
    "text": "conversion earliest my sequel so there's a little bit of work here on the on the",
    "start": "1168470",
    "end": "1173720"
  },
  {
    "text": "on the constraints there's a fair bit of work on procedures and packages this is the significant actions there's another",
    "start": "1173720",
    "end": "1181370"
  },
  {
    "text": "report here for the for a raw and my sequel similar sort of story you kind of",
    "start": "1181370",
    "end": "1186410"
  },
  {
    "text": "expect that because my sequel rora and and my sequel were compatible and then",
    "start": "1186410",
    "end": "1192200"
  },
  {
    "text": "here's the the solution for a my Postgres are the yes so it's actually got a clean bill of health as far as the",
    "start": "1192200",
    "end": "1198320"
  },
  {
    "text": "scheme Rob G is concerned and there's less work involved in the in the procedures so if I was doing this for",
    "start": "1198320",
    "end": "1204500"
  },
  {
    "text": "real this is probably the one I choose as my open source target database and in fact in the demo machine that's happened",
    "start": "1204500",
    "end": "1210530"
  },
  {
    "text": "I've either selected Postgres RDS as my target and now I've stood up a Postgres",
    "start": "1210530",
    "end": "1216110"
  },
  {
    "text": "RDS instance in in my V PC and I'm now about to launch a real conversion so I",
    "start": "1216110",
    "end": "1222320"
  },
  {
    "text": "go back into the schema conversion tool I say I want to create a new project give it a name",
    "start": "1222320",
    "end": "1228820"
  },
  {
    "text": "this time I'll tell it that I wanted to actually I won't want to do a convert I want to be specific about my target I",
    "start": "1228820",
    "end": "1234710"
  },
  {
    "text": "want to do an Oracle to a Postgres RDS conversion so that some that's working",
    "start": "1234710",
    "end": "1248600"
  },
  {
    "text": "so that the schema conversion tool will then interrogate the source database get a list of all the scheme is available",
    "start": "1248600",
    "end": "1253820"
  },
  {
    "text": "for conversion I'm sorry I need to enter my credentials first for the for the",
    "start": "1253820",
    "end": "1259580"
  },
  {
    "text": "Oracle database and I'll have to enter my credentials for the target database as well they're just regular old",
    "start": "1259580",
    "end": "1267230"
  },
  {
    "text": "connection strings put in the URL the database the port number the name of the name of the young instance ID so it's",
    "start": "1267230",
    "end": "1273770"
  },
  {
    "text": "come up with a list of the the source the source schemas and I'll repeat the process connecting to the Postgres",
    "start": "1273770",
    "end": "1280040"
  },
  {
    "text": "database so Bera mine were actually connecting to our target instance and we're about to actually modify its schema or install schema objects on the",
    "start": "1280040",
    "end": "1289580"
  },
  {
    "text": "target so I'm going to select the DMS sample as my source schema let's go here",
    "start": "1289580",
    "end": "1297160"
  },
  {
    "text": "that's my sports ticketing system and the DMS sample I'll click on that it's",
    "start": "1297160",
    "end": "1304040"
  },
  {
    "text": "going to enumerate all of the the objects in the in the in the source schema now in this path I'm just going",
    "start": "1304040",
    "end": "1310700"
  },
  {
    "text": "to focus on the tables and the tables and indexes I'll do the do the stored procedures in the next the next cycle",
    "start": "1310700",
    "end": "1319600"
  },
  {
    "text": "they saw that this is actually done in real time but I'm not doing the demo now but this is a real time real timings and",
    "start": "1321340",
    "end": "1327740"
  },
  {
    "text": "then sped up the film now I'm going to say convert the schema now that's what this going to do is apply its standard",
    "start": "1327740",
    "end": "1334250"
  },
  {
    "text": "built-in rules for a generic Oracle to Postgres ideas conversion so it's going to cook I'm",
    "start": "1334250",
    "end": "1339919"
  },
  {
    "text": "going to perform the standard naming naming um conversions so uppercase Oracle to a lowercase it's going to do",
    "start": "1339919",
    "end": "1346399"
  },
  {
    "text": "the standard mappings of data types and we'll see whether it comes up with so",
    "start": "1346399",
    "end": "1353720"
  },
  {
    "text": "I'm gonna I've switched it so I'm going to choose a table in the sample database",
    "start": "1353720",
    "end": "1360110"
  },
  {
    "text": "called the player database sorry it's in the target database called player and and I'm gonna see what will it's come up",
    "start": "1360110",
    "end": "1367159"
  },
  {
    "text": "with as far as a schema conversion now here you can see there's columns here called ID number and they'd be mapped",
    "start": "1367159",
    "end": "1374119"
  },
  {
    "text": "this double-precision now you see the deviation say all that like that I don't like going numbers to double precision",
    "start": "1374119",
    "end": "1379580"
  },
  {
    "text": "because that's a primary key and that's not a great choice of a primary key so what I'm going to do is is is create a",
    "start": "1379580",
    "end": "1385100"
  },
  {
    "text": "mapping rule custom mapping rule that's going to set up a mapping that says that for all columns than they that have got",
    "start": "1385100",
    "end": "1391970"
  },
  {
    "text": "an ID all table columns they have a name of ID I want them converted to two number in",
    "start": "1391970",
    "end": "1398179"
  },
  {
    "text": "Postgres not to floating-point sorry American Postgres rather than floating point and I do that within the schema",
    "start": "1398179",
    "end": "1404210"
  },
  {
    "text": "conversion tool so I'm just defining out the pattern I want to follow here and I",
    "start": "1404210",
    "end": "1413450"
  },
  {
    "text": "say I say I want to change the data type to their begin could have been another option that's that's a good option in",
    "start": "1413450",
    "end": "1419690"
  },
  {
    "text": "Postgres but I'll just go for for numeric and I'll save that rule they'll",
    "start": "1419690",
    "end": "1427609"
  },
  {
    "text": "they'll handle all my primary keys labeled ID also I've got foreign key references",
    "start": "1427609",
    "end": "1432950"
  },
  {
    "text": "they've all got all got Suffolk ID so it'll say where it says um a ticket will",
    "start": "1432950",
    "end": "1438889"
  },
  {
    "text": "have like the stadium ID as a reference so I want to convert all of those foreign key references as well from from",
    "start": "1438889",
    "end": "1445730"
  },
  {
    "text": "numeric sorry from number to numeric so I'm going to create a rule here that says any column that's got a suffix of",
    "start": "1445730",
    "end": "1452690"
  },
  {
    "text": "ID but that from number two numeric instantly while it's doing that the",
    "start": "1452690",
    "end": "1459320"
  },
  {
    "text": "default rule says that if there's a precision included in the Oracle data type just preserve that precision that's",
    "start": "1459320",
    "end": "1465109"
  },
  {
    "text": "just this this corner case we don't have a precision in the source column that you have to UM there goes the floating-point so you may",
    "start": "1465109",
    "end": "1472740"
  },
  {
    "text": "not need to do this on your own databases so I've prayed that custom rule and now now I've got a custom rule",
    "start": "1472740",
    "end": "1478799"
  },
  {
    "text": "I'm going to rerun the the conversion and see what it comes up with so I click",
    "start": "1478799",
    "end": "1484649"
  },
  {
    "text": "convert schema it's going to overwrite my the potential schema or the",
    "start": "1484649",
    "end": "1490279"
  },
  {
    "text": "prospective schemer I've creds on the right hand side with the the new mappings and have a look at the player",
    "start": "1490279",
    "end": "1497009"
  },
  {
    "text": "here and here it's gone to change the the column type to numeric so it's no longer floating-point so that's actually",
    "start": "1497009",
    "end": "1504090"
  },
  {
    "text": "that's actually the the desired result I was after so this new this new rules",
    "start": "1504090",
    "end": "1509340"
  },
  {
    "text": "been applied to every single table in that schema and the next thing I'm going",
    "start": "1509340",
    "end": "1515159"
  },
  {
    "text": "to do is repeat it is go and have a look at the packages during the conversion I think that's internet so we're gonna",
    "start": "1515159",
    "end": "1522029"
  },
  {
    "text": "convert the packages thank you I'm going",
    "start": "1522029",
    "end": "1527159"
  },
  {
    "text": "to switch them into the assessment view for this packages demo now what the assessment view does is rather than",
    "start": "1527159",
    "end": "1532980"
  },
  {
    "text": "saying here's a list of all the objects and here's the problems with it it says here's a is an issue that's appearing at",
    "start": "1532980",
    "end": "1538470"
  },
  {
    "text": "least once in your conversion and these are all the occurrences of it and there can be a better way of organizing your",
    "start": "1538470",
    "end": "1543779"
  },
  {
    "text": "workload if you're actually going through a migration process so the the",
    "start": "1543779",
    "end": "1549600"
  },
  {
    "text": "example here is there's a list of all my conversions and I'm going to drill down into a particular particular package",
    "start": "1549600",
    "end": "1557070"
  },
  {
    "text": "here called its are a particular function called get event details and this is an example of",
    "start": "1557070",
    "end": "1563669"
  },
  {
    "text": "a clean conversion so that's the source PL sequel and that's the target PG sequel so there's no issues involved in",
    "start": "1563669",
    "end": "1570149"
  },
  {
    "text": "that conversion that's just being a straightforward mapping by the conversion tool so I'm happy with that one I'll have a look at one that's that",
    "start": "1570149",
    "end": "1577110"
  },
  {
    "text": "it's got a problem with it and that's one called transfer ticket and this is the sauce peel sequel and it's told me",
    "start": "1577110",
    "end": "1583919"
  },
  {
    "text": "that there's an issue here that the converted function depends on the time zone settings so let's have a look at",
    "start": "1583919",
    "end": "1589500"
  },
  {
    "text": "what that means they'll go down to the PL sequel and just",
    "start": "1589500",
    "end": "1597400"
  },
  {
    "text": "roll across screen Real Estate's a bit of an issue when you're doing doing this using this tool but it's got its got",
    "start": "1597400",
    "end": "1604809"
  },
  {
    "text": "reference to sustain in there in the PL sequel and Sustaita isn't an intrinsic",
    "start": "1604809",
    "end": "1610120"
  },
  {
    "text": "within Postgres what what it does is it Maps it to a a function that's part of",
    "start": "1610120",
    "end": "1616360"
  },
  {
    "text": "the extension package we include in the in the conversion called Oracle extensions estate and worth saying here",
    "start": "1616360",
    "end": "1623740"
  },
  {
    "text": "is that just be aware that there's a timezone dependency that's been introduced by that mapping and check to make sure that you've got the timezone",
    "start": "1623740",
    "end": "1629470"
  },
  {
    "text": "set correctly so that's a that's an example of a package conversion now once",
    "start": "1629470",
    "end": "1637600"
  },
  {
    "text": "I'm happy I can keep iterating on that package conversion once I'm happy with it the next thing I do is I apply that",
    "start": "1637600",
    "end": "1643660"
  },
  {
    "text": "that schema to apply the schemer I've generated on the right hand side through my rules and my edits to the store",
    "start": "1643660",
    "end": "1649780"
  },
  {
    "text": "packages and I apply that to the target database now it's only at this point that I'm actually going to apply all the",
    "start": "1649780",
    "end": "1655570"
  },
  {
    "text": "DDL that the other produced mine did hit",
    "start": "1655570",
    "end": "1660580"
  },
  {
    "text": "play there it's only once I've actually",
    "start": "1660580",
    "end": "1666940"
  },
  {
    "text": "done the gone through all those rules that actually apply my target my DDL to",
    "start": "1666940",
    "end": "1672820"
  },
  {
    "text": "the target database and this is the point where it creates the tables and stores all the packages ok so there's",
    "start": "1672820",
    "end": "1681250"
  },
  {
    "text": "not much more to see there that just more or less runs immediately and next thing is I'm going to start the CDC task",
    "start": "1681250",
    "end": "1687850"
  },
  {
    "text": "so now make sure start migrating converting real live transactions happening in my source database to my",
    "start": "1687850",
    "end": "1693550"
  },
  {
    "text": "target database I'm going to stay within this scheme a conversion tool to do this I can do it from in the console but it's",
    "start": "1693550",
    "end": "1699490"
  },
  {
    "text": "really convenient to do it within the conversion tool I'll create the CDC task",
    "start": "1699490",
    "end": "1704740"
  },
  {
    "text": "I'll give it a name there's a few attributes here source endpoint target",
    "start": "1704740",
    "end": "1709750"
  },
  {
    "text": "in point I won't go into details about how they set up they're just dumb objects that's that usurp in the console ahead of time crucial thing is I'm going",
    "start": "1709750",
    "end": "1716920"
  },
  {
    "text": "to tell it I want to replicate data changes only there's a few other options there I could have done a bulk low then",
    "start": "1716920",
    "end": "1722200"
  },
  {
    "text": "CDC or I could do CDC by itself I'm a  within the demo itself we've already",
    "start": "1722200",
    "end": "1728200"
  },
  {
    "text": "done the bulk load and all I'm doing is going into CDC modes of replicate and real live",
    "start": "1728200",
    "end": "1733659"
  },
  {
    "text": "transactions as they happen so that task is now running the CDC task is now running the next thing that deals I want",
    "start": "1733659",
    "end": "1741340"
  },
  {
    "text": "to sell tickets so this is actually where I'm doing a real workload on the source database and the way I'm going to",
    "start": "1741340",
    "end": "1747009"
  },
  {
    "text": "generate that is I'm not going to bore you with me getting into a GUI and selling ticket so I'm just going to run",
    "start": "1747009",
    "end": "1752470"
  },
  {
    "text": "a store procedure that's going to randomly generate ticket sales so the",
    "start": "1752470",
    "end": "1758350"
  },
  {
    "text": "this is SQL developer skill developer you if you're using Oracle already this will be painfully familiar and we round",
    "start": "1758350",
    "end": "1765909"
  },
  {
    "text": "the query here to indicate that there's currently there's like five hundred thousand tickets to sell from the pool",
    "start": "1765909",
    "end": "1773200"
  },
  {
    "text": "and the actual number of tickets I've sold so far there's a nice clean zero so",
    "start": "1773200",
    "end": "1779049"
  },
  {
    "text": "you know I'm starting from a clean slate on the source database and now I'm gonna run the execute procedure that's going",
    "start": "1779049",
    "end": "1784779"
  },
  {
    "text": "to generate a series of random ticket sale operations in the source database",
    "start": "1784779",
    "end": "1790768"
  },
  {
    "text": "sorry about my typing and slope so",
    "start": "1796289",
    "end": "1805059"
  },
  {
    "text": "that's underway so I'm staying to generate ticket sales at the moment now",
    "start": "1805059",
    "end": "1810639"
  },
  {
    "text": "will them do is demonstrate to you that those ticket sales are being replicated from Oracle into Postgres doing here",
    "start": "1810639",
    "end": "1818109"
  },
  {
    "text": "play them thank you so I'm going to fire",
    "start": "1818109",
    "end": "1824499"
  },
  {
    "text": "up my Postgres admin tool this time and look at what's happening in Postgres so",
    "start": "1824499",
    "end": "1830619"
  },
  {
    "text": "I brought up the dashboard you can see there's a bit of activity here so these are the actual replicated ticket transactions hitting the target database",
    "start": "1830619",
    "end": "1837100"
  },
  {
    "text": "at this point now I'm going to go into the query tool and run a query to see how many ticket purchases and now in my",
    "start": "1837100",
    "end": "1843669"
  },
  {
    "text": "target database so",
    "start": "1843669",
    "end": "1847830"
  },
  {
    "text": "you can see here that this ticketing transactions there is let's come back with six thousand nine hundred and",
    "start": "1852720",
    "end": "1858970"
  },
  {
    "text": "nineteen ticket sales so that's some that's in the code that the conversion is actually working now now that CDC",
    "start": "1858970",
    "end": "1865330"
  },
  {
    "text": "task is running continuously now and every time there's a ticket sale on the source you'll be immediately replicated",
    "start": "1865330",
    "end": "1870580"
  },
  {
    "text": "over so that's pretty much it that's the that's the nuts and bolts of a standard",
    "start": "1870580",
    "end": "1876730"
  },
  {
    "text": "migration conversion I mentioned the DMS console I won't go in if you know just",
    "start": "1876730",
    "end": "1882070"
  },
  {
    "text": "hit play while I just taught the everything we're doing the schemer conversion tool is actually there's also",
    "start": "1882070",
    "end": "1888130"
  },
  {
    "text": "a parent through the console all the you can see here that there's a there's a CDC task already running you can see",
    "start": "1888130",
    "end": "1894549"
  },
  {
    "text": "that in the console it's got a whole lot of metrics to do with how that task is performing you can see the numbers of",
    "start": "1894549",
    "end": "1900909"
  },
  {
    "text": "incoming changes how much memory it's using so you can might you can you DBAs",
    "start": "1900909",
    "end": "1906010"
  },
  {
    "text": "can very closely monitor how that CDC task is actually performing I mentioned the endpoints these are just um these",
    "start": "1906010",
    "end": "1912549"
  },
  {
    "text": "are just objects that indicate that have the credentials and URLs vias respectively a source and target",
    "start": "1912549",
    "end": "1917679"
  },
  {
    "text": "database you set them up within the console and just refer to them in this SCT tool and that's pretty much it thank",
    "start": "1917679",
    "end": "1925270"
  },
  {
    "text": "you very much [Applause]",
    "start": "1925270",
    "end": "1930859"
  }
]