[
  {
    "start": "0",
    "end": "31000"
  },
  {
    "text": "okay let's go ahead and get started I'm Marvin timer I'm a distinguished engineer at AWS and I'm the lead",
    "start": "760",
    "end": "6879"
  },
  {
    "text": "engineer on this effort and what I'm going to tell you about today is how to use Kinesis if you went to the 100 level",
    "start": "6879",
    "end": "14200"
  },
  {
    "text": "talk last afternoon you learned about the business propositions and values and you got a little bit of a taste of how",
    "start": "14200",
    "end": "20039"
  },
  {
    "text": "to use it and so forth this talk is going to be focused on how to use it and the technical aspects of that so I've",
    "start": "20039",
    "end": "26480"
  },
  {
    "text": "got here a picture of the whole thing overall and I most going to be talking about the part on the right which is how",
    "start": "26480",
    "end": "32238"
  },
  {
    "start": "31000",
    "end": "31000"
  },
  {
    "text": "you actually process uh streams of data um but I briefly want to go through the",
    "start": "32239",
    "end": "37480"
  },
  {
    "text": "whole thing so if you remember Kinesis is about ingesting data and processing it in real time we can take data from an",
    "start": "37480",
    "end": "44760"
  },
  {
    "text": "arbitrary number of sources in arbitrary volumes and that gets ingested at our",
    "start": "44760",
    "end": "50239"
  },
  {
    "text": "AWS uh web service front endpoint that data is then going to be durably stored",
    "start": "50239",
    "end": "55559"
  },
  {
    "text": "across three different availability zones before we say that we've gotten it success from the source and then once",
    "start": "55559",
    "end": "62079"
  },
  {
    "text": "that data is durably available it's available for processing by an arbitrary number of applications that can be",
    "start": "62079",
    "end": "67439"
  },
  {
    "text": "running in parallel to each other now you might have sources from a variety of places you could have like the computers",
    "start": "67439",
    "end": "73960"
  },
  {
    "text": "in your data center sending log information you might have your browser your website uh server sending",
    "start": "73960",
    "end": "80360"
  },
  {
    "text": "clickstream information in you might have sensors from your turbine sending information in you might have your",
    "start": "80360",
    "end": "85520"
  },
  {
    "text": "mobile phone sending information in we don't care but our scale is intended to handle that kind of thing like all of",
    "start": "85520",
    "end": "91680"
  },
  {
    "text": "the phones in the world ideally on the output side you can do whatever you like you could take this",
    "start": "91680",
    "end": "97720"
  },
  {
    "text": "data for example and aggregate it and archive it into S3 you might do symmetric extraction and then put that",
    "start": "97720",
    "end": "104399"
  },
  {
    "text": "into Dynamo DB so that you could have a real-time dashboard showing things you might do some kind of sliding window",
    "start": "104399",
    "end": "110560"
  },
  {
    "text": "analysis and then update the statistics from that into red shift so that you can do more complicated queries or you could",
    "start": "110560",
    "end": "116759"
  },
  {
    "text": "do a variety of other things including sending it to your favorite other place including say a storm",
    "start": "116759",
    "end": "123118"
  },
  {
    "text": "application so what I'm going to spend the rest of the time talking about is three things I'm going to take you",
    "start": "123759",
    "end": "129039"
  },
  {
    "text": "through a tour of the Kinesis Concepts in the context of trying to implement a simplified version of the demo that you",
    "start": "129039",
    "end": "135440"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "saw in verner's uh keynote speech that's a Twitter Trend service I'm going to talk about how you might implement it",
    "start": "135440",
    "end": "141640"
  },
  {
    "text": "using our client libraries and then I'm going to try and situate Kinesis in the broader context of a big data",
    "start": "141640",
    "end": "147599"
  },
  {
    "text": "ecosystem so let's start with the first thing a tour of Kinesis Concepts so let's suppose you wanted to implement a",
    "start": "147599",
    "end": "153760"
  },
  {
    "text": "Twitter Trends site now what that is what I'm talking about here is let's suppose we took the Twitter firose and",
    "start": "153760",
    "end": "160040"
  },
  {
    "text": "we took that into our website and it's going to look at what the top 10 hashtag topics are and then basically users can",
    "start": "160040",
    "end": "166720"
  },
  {
    "text": "go there with HTTP and ask you know what's the top 10 sites using you know it's a standard website right now if you",
    "start": "166720",
    "end": "174000"
  },
  {
    "text": "ignored scale this would be pretty straightforward in AWS just say make it an elastic beant stock application",
    "start": "174000",
    "end": "180920"
  },
  {
    "text": "the problem is is that the data from the fire hose is too big to handle with one box right so the challenge we've got is",
    "start": "180920",
    "end": "187879"
  },
  {
    "start": "185000",
    "end": "185000"
  },
  {
    "text": "we've got to somehow divide up the work among multiple machines in order to be able to process this stuff in parallel",
    "start": "187879",
    "end": "194400"
  },
  {
    "text": "okay and so now that data needs to be processed in parallel and then combined and and uh provided to this to our web",
    "start": "194400",
    "end": "201280"
  },
  {
    "text": "server so that it can go out to the to the client now the solution we're going to take from this is we're going to take",
    "start": "201280",
    "end": "207080"
  },
  {
    "start": "204000",
    "end": "204000"
  },
  {
    "text": "a DAT a page from the map ruce Community from the Community that's already figured out how to do processing of huge",
    "start": "207080",
    "end": "212599"
  },
  {
    "text": "batches of data they use this concept of map reduce we're going to come up with sort of the streaming equivalent of that",
    "start": "212599",
    "end": "219280"
  },
  {
    "text": "so how would we do that well we're going to divide up our work and say what we want to do is is we'll calculate a top",
    "start": "219280",
    "end": "225120"
  },
  {
    "text": "10 locala for all the data coming into each of these machines they'll send in their top estimate their top 10",
    "start": "225120",
    "end": "231480"
  },
  {
    "text": "estimates and that'll get combined into a global top 10 by the the web server",
    "start": "231480",
    "end": "236920"
  },
  {
    "text": "that I've shown over here our elastic beant stock application now the problem with this is the Twitter",
    "start": "236920",
    "end": "242680"
  },
  {
    "text": "data is going to come in sort of randomly to all of these machines I've",
    "start": "242680",
    "end": "247760"
  },
  {
    "text": "shown here as a color the hashtag that's associated with a Tweet now I'm going to assume for purposes of this example that",
    "start": "247760",
    "end": "254519"
  },
  {
    "text": "tweets come in only with a single hashtag Topic in it now as those of you who know Twitter know that tweets",
    "start": "254519",
    "end": "259840"
  },
  {
    "text": "actually have multiple hashtags in it and I'm going to leave it as an exercise to The Listener at the end of this talk",
    "start": "259840",
    "end": "265560"
  },
  {
    "text": "to figure out how you would transform a stream that has records with multiple hashtags into a separate stream a new",
    "start": "265560",
    "end": "271400"
  },
  {
    "text": "stream that has duplicated records with single hashtags that should hopefully be pretty obvious how to do that when I'm",
    "start": "271400",
    "end": "277320"
  },
  {
    "text": "done so what we need now if we're going to take the streaming map reduce ideas we need to somehow partition all of the",
    "start": "277320",
    "end": "284280"
  },
  {
    "text": "data that has the same topic and group it together so it arrives at the same machine so we'll do that by interjecting",
    "start": "284280",
    "end": "291479"
  },
  {
    "text": "a box in the middle that takes the data that's randomly that's coming at us randomly and sorts it by that hashtag",
    "start": "291479",
    "end": "298440"
  },
  {
    "text": "and then sends all of the data with the same hashtag to the same machine like I've shown here right and once I have",
    "start": "298440",
    "end": "304120"
  },
  {
    "text": "that I can combine these now you might ask why didn't I just take the random data in each one and compute a local top",
    "start": "304120",
    "end": "310320"
  },
  {
    "text": "10 there the challenge with that is you're estimating the top 10 because say if I look at the red ones there they're",
    "start": "310320",
    "end": "317360"
  },
  {
    "text": "going to come into all the different machines and so what I'm really doing is estimating a top 10 for some fraction of",
    "start": "317360",
    "end": "322680"
  },
  {
    "text": "the data and depending on skews of the distribution and so forth you're not going to get necessarily an accurate",
    "start": "322680",
    "end": "328720"
  },
  {
    "text": "value of top 10 without sending a substantially larger amount of data over to the green guy so that they can",
    "start": "328720",
    "end": "334680"
  },
  {
    "text": "combine it and get statistically close it's much simpler to do what I've shown here which is to partition it get an",
    "start": "334680",
    "end": "341080"
  },
  {
    "text": "accurate top 10 estimate for all of the data that's in the subset of the red and blue guys on the top guy the green and",
    "start": "341080",
    "end": "347160"
  },
  {
    "text": "the orange ones in the middle and the blue and the gray ones on the bottom so this is just a much easier way of doing it I would argue and again this is the",
    "start": "347160",
    "end": "354440"
  },
  {
    "text": "standard way that we do it in the batch Community we're just applying the same ideas now to the streaming world",
    "start": "354440",
    "end": "361639"
  },
  {
    "text": "so what are the Core Concepts that we need to do this well there's a notion of a data record an individual tweet",
    "start": "361800",
    "end": "368720"
  },
  {
    "text": "there's all of the tweets that are going to go into a single uh type of stream",
    "start": "368720",
    "end": "374120"
  },
  {
    "start": "371000",
    "end": "371000"
  },
  {
    "text": "right and I might have many different streams for any different kinds of things I want to do right we need a",
    "start": "374120",
    "end": "379520"
  },
  {
    "text": "notion of a partition key which is the thing that we're going to partition on in this case it would be the hashtag",
    "start": "379520",
    "end": "385039"
  },
  {
    "text": "topic for the tweets and we need a notion of all of the ones that are going to be grouped together and sent to a",
    "start": "385039",
    "end": "390800"
  },
  {
    "text": "single worker and a worker is basically the thing that's processing one of these shards right so A Shard is all of the",
    "start": "390800",
    "end": "398000"
  },
  {
    "text": "data that has the partition keys that have been grouped together to be sent to one single worker that would typically",
    "start": "398000",
    "end": "404319"
  },
  {
    "text": "be a reducer say if you were thinking of the map ruce Paradigm again right so those are the Core Concepts I",
    "start": "404319",
    "end": "412080"
  },
  {
    "text": "want to add one more thing we order the records in A Shard and we assign a",
    "start": "412080",
    "end": "417319"
  },
  {
    "text": "sequence number to them so there's an exact order that's uh defined when records are ingested into a Kinesis",
    "start": "417319",
    "end": "423319"
  },
  {
    "text": "stream at the beginning and what this is going to allow us to do is there a very nice form of failover by doing basically",
    "start": "423319",
    "end": "429479"
  },
  {
    "text": "checkpoint replay so just to rec uh so how does this relate to Kinesis well",
    "start": "429479",
    "end": "434960"
  },
  {
    "start": "432000",
    "end": "432000"
  },
  {
    "text": "that box in the blue there that blue box is kinesis that's the service that's taking all the data records in and these",
    "start": "434960",
    "end": "442240"
  },
  {
    "text": "yellow guys here are basically a Kinesis application we call it a processing stage you can have many of these",
    "start": "442240",
    "end": "447919"
  },
  {
    "text": "processing stages so let me just recap The Core Concepts for you again there's",
    "start": "447919",
    "end": "453440"
  },
  {
    "start": "450000",
    "end": "450000"
  },
  {
    "text": "a data record in this case it would be a tweet there's a stream that's all the tweets coming from the Twitter fire hose",
    "start": "453440",
    "end": "460280"
  },
  {
    "text": "there's a partition key in this case it's the Twitter topic and we're for the moment assuming every tweet record",
    "start": "460280",
    "end": "465360"
  },
  {
    "text": "belongs to exactly one of these again we would have to actually transform the fire hose into something that has that",
    "start": "465360",
    "end": "472199"
  },
  {
    "text": "because we have to have one single partition key that we're going to separate things spot right there's a Shard which is all the data records",
    "start": "472199",
    "end": "478919"
  },
  {
    "text": "belonging to a set of Twitter topics that will get grouped together and sent to one worker which processes the",
    "start": "478919",
    "end": "484479"
  },
  {
    "text": "records of a shard in sequence number order and that sequence number is what gets assigned to a data record when it's",
    "start": "484479",
    "end": "490319"
  },
  {
    "text": "first ingested by the Kinesis service okay now you could just program directly",
    "start": "490319",
    "end": "497840"
  },
  {
    "text": "against the Kinesis service it's a web service we offer an SDK that lets you get to this thing directly on the put",
    "start": "497840",
    "end": "504360"
  },
  {
    "text": "side for example that's exactly what you would do but on the and you could do the same thing on the get",
    "start": "504360",
    "end": "510800"
  },
  {
    "text": "side and let's talk about what that would look like basically it's almost like",
    "start": "510800",
    "end": "517120"
  },
  {
    "text": "processing a tape you would seek to someplace in the Stream typically if you",
    "start": "517120",
    "end": "522479"
  },
  {
    "text": "were If This Were purely a real-time application you would seek to the end of the stream which I've designated as",
    "start": "522479",
    "end": "527880"
  },
  {
    "text": "latest but you could also seek to someplace else like the beginning of the stream of all the data that's still",
    "start": "527880",
    "end": "533040"
  },
  {
    "text": "there or to some checkpoint Place once you've got an iterator you're going to just simply read forward on that",
    "start": "533040",
    "end": "538760"
  },
  {
    "text": "iterator right right so you'd have a loop here that basically reads records up to some Maximum that you're willing",
    "start": "538760",
    "end": "545000"
  },
  {
    "text": "to accept you'll get that as an array of Records back plus an iterator that's basically your cursor for going forward",
    "start": "545000",
    "end": "550959"
  },
  {
    "text": "in this stream and then you would process those records and you're simply in this infinite Loop now what would",
    "start": "550959",
    "end": "556720"
  },
  {
    "text": "that processing code look like in this case it's pretty simple we just take each of the records and we're going to",
    "start": "556720",
    "end": "562640"
  },
  {
    "text": "update our local in memory data structure that keeps track of our top 10 I'll show you know it's basically a hash",
    "start": "562640",
    "end": "568760"
  },
  {
    "text": "table and then periodically we're going to do output in this case we're going to write it to Dynamo",
    "start": "568760",
    "end": "575240"
  },
  {
    "text": "DB um you could send this data directly to our website I'm going to show I'm I'm",
    "start": "575240",
    "end": "580440"
  },
  {
    "text": "going to illustrate the fact that you might want to put it into Dynamo DB because then if you had a fault tolerant",
    "start": "580440",
    "end": "585959"
  },
  {
    "text": "service over there you wouldn't have to worry about connecting to it if it fails over and so forth or if it's down",
    "start": "585959",
    "end": "591360"
  },
  {
    "text": "briefly what do you do with your data well we're simply going to aggregate we're going to Simply store it in Dynamo",
    "start": "591360",
    "end": "596399"
  },
  {
    "text": "DB and that way it's one less thing that we have to worry about you could do it either way right so periodically we",
    "start": "596399",
    "end": "602279"
  },
  {
    "text": "write to Dynamo DB say every 10 seconds what happens on the other side is this guy is going to sit in an infinite Loop",
    "start": "602279",
    "end": "608360"
  },
  {
    "text": "and he's going to scan that Dynamo DB table there will be an entry for each worker in that table take all of those",
    "start": "608360",
    "end": "614720"
  },
  {
    "text": "top 10 lists that are local and merge them into a global top 10 list and there he's got it that's all there is to it",
    "start": "614720",
    "end": "622120"
  },
  {
    "text": "now that's not quite true that's not all there is to it because you had to still deal with a lot of muck in order to make",
    "start": "622120",
    "end": "627959"
  },
  {
    "start": "623000",
    "end": "623000"
  },
  {
    "text": "this work for example you have to manually create the workers and you have to figure out manually who's going to",
    "start": "627959",
    "end": "633399"
  },
  {
    "text": "work on which Shard you've got to decide how many workers you're going to run in your ec2 instance and you've got to decide how",
    "start": "633399",
    "end": "639720"
  },
  {
    "text": "many of these ec2 instances to run right so you're doing all of this muck by hand",
    "start": "639720",
    "end": "645440"
  },
  {
    "text": "and what we've done is we've created a Kinesis client library that tries to take care of most of that muck for",
    "start": "645440",
    "end": "652560"
  },
  {
    "text": "you and what that library is is it's it's code that you would link into your application so the way you want to think",
    "start": "652800",
    "end": "658959"
  },
  {
    "text": "about Kinesis applications is these are things you run yourself so these are ec2",
    "start": "658959",
    "end": "665399"
  },
  {
    "text": "instance running under your account you launch them you provision the Ami any way you want and that library is linked",
    "start": "665399",
    "end": "672680"
  },
  {
    "text": "in and it then takes care of mediating the interactions with Kinesis so that",
    "start": "672680",
    "end": "677880"
  },
  {
    "text": "then the code you end up writing that's is primarily going to be business processing logic as well as that",
    "start": "677880",
    "end": "683279"
  },
  {
    "text": "configuration code for setting up your processing libraries right so if you had some custom library that was doing say",
    "start": "683279",
    "end": "689600"
  },
  {
    "text": "image Transformations or so forth and it needed a special setup and so forth you would be able to manage it because it's",
    "start": "689600",
    "end": "696040"
  },
  {
    "text": "your Amy that you're creating for this thing right so what this Library does is if",
    "start": "696040",
    "end": "703959"
  },
  {
    "text": "it's an it's it's an East hc2 instance that you run and what it's going to do on Startup is it's going to query you're",
    "start": "703959",
    "end": "710680"
  },
  {
    "start": "705000",
    "end": "705000"
  },
  {
    "text": "going to tell it what stream you want to process and it's going to query Kinesis and do a describ stream web service call",
    "start": "710680",
    "end": "717120"
  },
  {
    "text": "and that's going to tell it how many shards there are now the first guy that gets to it is going to try and create a Dynamo DB",
    "start": "717120",
    "end": "723880"
  },
  {
    "text": "table this will be a table in your account and in that Dynamo DB table there'll be an entry for every Shard",
    "start": "723880",
    "end": "729480"
  },
  {
    "text": "that's in the Stream So these guys if I say had started this thing up with three different ec2 instances they would each",
    "start": "729480",
    "end": "735639"
  },
  {
    "text": "go do a described stream to Kinesis they'd go check the sh the to see if the table exists if it doesn't they create",
    "start": "735639",
    "end": "742160"
  },
  {
    "text": "it otherwise they just look at it and what they then do is is they look at each of those entries and say who's got",
    "start": "742160",
    "end": "748199"
  },
  {
    "text": "that entry who owns it CU what each of those entries has is among other things is a lock and a lease that is acquired",
    "start": "748199",
    "end": "755240"
  },
  {
    "text": "by one of these uh worker instances so what they're all going to do is is they're going to compete to try",
    "start": "755240",
    "end": "760959"
  },
  {
    "text": "and keep get this lock in Dynamo you can do a test and set type of lock so exactly one of these uh ec2 instances",
    "start": "760959",
    "end": "768560"
  },
  {
    "text": "will get the lock for each of the shards and they will then spin up a worker one of those blue ovals in order to process",
    "start": "768560",
    "end": "774440"
  },
  {
    "text": "that Shard they'll also do a certain amount of decentralized load balancing they'll look at how many workers they've",
    "start": "774440",
    "end": "780720"
  },
  {
    "text": "got on themselves and they'll look at how many workers everybody else has by scanning that table and if they see that",
    "start": "780720",
    "end": "786480"
  },
  {
    "text": "they have too many workers they'll relinquish one of their shards so that somebody else can acquire it what that",
    "start": "786480",
    "end": "791880"
  },
  {
    "text": "means is that the system will basically load balance okay now that load balances",
    "start": "791880",
    "end": "797519"
  },
  {
    "text": "across um the ec2 instances that you've spun up if you want to make that elastic",
    "start": "797519",
    "end": "804360"
  },
  {
    "text": "what you do is is you put them in an autoscaling group so now what happens is you've put you've defined an autoscaling",
    "start": "804360",
    "end": "810639"
  },
  {
    "text": "group for yourami that's going to Define you can now Define a cloudwatch metric that's used to control that autoscaling",
    "start": "810639",
    "end": "816880"
  },
  {
    "text": "group and that might look at say the load on those machines and if the load's getting too high for some reason then",
    "start": "816880",
    "end": "822959"
  },
  {
    "text": "what'll happen is it will automatically spin up a new ec2 instance that won't have any workers on it but the client",
    "start": "822959",
    "end": "828720"
  },
  {
    "text": "library is going to go off to that Shard table and try and acquire some shards",
    "start": "828720",
    "end": "833839"
  },
  {
    "text": "now what's going to happen is everybody's looking at that chard table and they're going to see hey there's somebody who's not got as much load as I",
    "start": "833839",
    "end": "839759"
  },
  {
    "text": "do so of the other three they're going to kind of throw a dice and decide which guy they'll each individually throw a",
    "start": "839759",
    "end": "846120"
  },
  {
    "text": "dice and relinquish one of their shards and this guy will basically be able to acquire that Shard and now I've moved",
    "start": "846120",
    "end": "852720"
  },
  {
    "text": "some of the processing over to a new machine so what's going on here is that basically with the client Library we can",
    "start": "852720",
    "end": "859320"
  },
  {
    "text": "automatically do this level of load balancing and expansion and so forth so",
    "start": "859320",
    "end": "864880"
  },
  {
    "start": "860000",
    "end": "860000"
  },
  {
    "text": "what that would mean is if I've got some load Spike we can automatically move it across multiple machines if the load",
    "start": "864880",
    "end": "871160"
  },
  {
    "text": "goes back down the autoscaling group will push you know will essentially squeeze back",
    "start": "871160",
    "end": "877360"
  },
  {
    "text": "down now one thing that one problem that you're going to run into is if the load just keeps growing and growing I'll spin",
    "start": "877360",
    "end": "884320"
  },
  {
    "text": "up new uh ec2 instances in the autoscaling group but at some point I've gotten to the point where there's only",
    "start": "884320",
    "end": "890639"
  },
  {
    "text": "one Shard on each worker right because the way that balancing worked is I keep moving shards off and at some point",
    "start": "890639",
    "end": "896720"
  },
  {
    "text": "there's only going to be one right now if I keep keep growing at that point what's going to happen well the auto",
    "start": "896720",
    "end": "902320"
  },
  {
    "text": "scaling group's going to grow again a new machine's going to show up nothing happens now because nobody's going to",
    "start": "902320",
    "end": "908759"
  },
  {
    "text": "give up their last CH because that would make them the least uh the the least loaded guy right now if you set an alarm",
    "start": "908759",
    "end": "915720"
  },
  {
    "text": "on your cloudwatch metric here for persistent High load then what's going to happen is this guy these guys are",
    "start": "915720",
    "end": "922040"
  },
  {
    "text": "going to report their load in the alarm is going to go off say it goes into an SNS topic now you get a notification",
    "start": "922040",
    "end": "929120"
  },
  {
    "text": "goes to the operator of your service the owner of Twitter trends.com that person's going to look and they're going",
    "start": "929120",
    "end": "935040"
  },
  {
    "text": "to say ah I see I've been gr to the point where I need more capacity in my stream and he's going to go to the AWS",
    "start": "935040",
    "end": "942120"
  },
  {
    "text": "console for Kinesis and he's going to issue a rehard command he's going to split some of the shards so that there",
    "start": "942120",
    "end": "947639"
  },
  {
    "text": "are more shards available so we can now spend up some additional workers so what's now going to happen is a new",
    "start": "947639",
    "end": "953560"
  },
  {
    "text": "Shard will be created this guy down there is going to notice that there's a Shard that he can grab",
    "start": "953560",
    "end": "959440"
  },
  {
    "text": "that's not got a lock on it in The Shard management table he'll grab it and now load goes down there so what's going on",
    "start": "959440",
    "end": "966440"
  },
  {
    "text": "the way to think about the the resharding in Kinesis is a human being is going to decide to add more capacity",
    "start": "966440",
    "end": "973680"
  },
  {
    "text": "into the Kinesis stream and what our client Library does is automatically track that so the applications currently",
    "start": "973680",
    "end": "980959"
  },
  {
    "text": "don't do any recharting themselves what they do is they simply follow the they track the rearing that happens in the",
    "start": "980959",
    "end": "986959"
  },
  {
    "text": "Stream and for the moment that's a manual operation you can imagine eventually we",
    "start": "986959",
    "end": "992199"
  },
  {
    "text": "might try and automate that but we wanted to start with something manual and give people maximum control and then",
    "start": "992199",
    "end": "997680"
  },
  {
    "text": "watch how people use that and see where to go next with",
    "start": "997680",
    "end": "1002600"
  },
  {
    "text": "it okay so I've talked about how things grow and shrink and how I can take care of all the muck and so forth one of the",
    "start": "1002920",
    "end": "1009680"
  },
  {
    "start": "1004000",
    "end": "1004000"
  },
  {
    "text": "big things we support is fall tolerance and there's two kinds of fall tolerance you want to think about one is a single",
    "start": "1009680",
    "end": "1016199"
  },
  {
    "text": "machine goes down and you'd like to have some kind of automated fail over the other one is is a data an availability",
    "start": "1016199",
    "end": "1022800"
  },
  {
    "text": "Zone might go down now if you had put all of your ec2 instances in a single availability Zone you're now",
    "start": "1022800",
    "end": "1029360"
  },
  {
    "text": "dead and you can get around both of these so let's look at how we do that for single uh ec2 instance going down we",
    "start": "1029360",
    "end": "1037240"
  },
  {
    "start": "1034000",
    "end": "1034000"
  },
  {
    "text": "can deal with it directly with the client library and the way that works is let's suppose that machine goes down",
    "start": "1037240",
    "end": "1043360"
  },
  {
    "text": "what's going to happen is these two guys event when that machine goes down the locks that it held on the two shards",
    "start": "1043360",
    "end": "1050000"
  },
  {
    "text": "that I've represented there are eventually going to expire and everybody else is watching that table and they're",
    "start": "1050000",
    "end": "1055480"
  },
  {
    "text": "going to notice there's two shards that don't have a lock and they're going to try and get them each of them will grab",
    "start": "1055480",
    "end": "1061760"
  },
  {
    "text": "one because they're each going to try and move a minimum off of their their current Balan load and so what's going",
    "start": "1061760",
    "end": "1067880"
  },
  {
    "text": "to basically happen is that these shards are going to be acquired and that work",
    "start": "1067880",
    "end": "1073200"
  },
  {
    "text": "is going to go to these two guys and this happens completely automatically now if you want to avoid",
    "start": "1073200",
    "end": "1079039"
  },
  {
    "text": "being hit by an availability outage an availability Zone outage what you need to do is simply have your autoscaling",
    "start": "1079039",
    "end": "1085400"
  },
  {
    "text": "group put all of your ec2 instances in multiple availability zones and that's",
    "start": "1085400",
    "end": "1090880"
  },
  {
    "text": "again something that you can do just specifying off the availab uh off the autoscaling group so we can handle both",
    "start": "1090880",
    "end": "1096919"
  },
  {
    "text": "of these basically automatically for you now you may be wondering well how",
    "start": "1096919",
    "end": "1101960"
  },
  {
    "start": "1101000",
    "end": "1101000"
  },
  {
    "text": "does that fail over actually work oh and by um and and so let's go through that",
    "start": "1101960",
    "end": "1107240"
  },
  {
    "text": "right now so here I've shown you host a processing A Shard let's suppose some",
    "start": "1107240",
    "end": "1112400"
  },
  {
    "text": "records come into this Shard and this guy's merrily processing away what he does is he's grabbed those two records",
    "start": "1112400",
    "end": "1118880"
  },
  {
    "text": "he's got a little counter you can see there that's his cursor into the stream it shows three right now let's suppose",
    "start": "1118880",
    "end": "1124320"
  },
  {
    "text": "some more records come along and so the next time he goes around his while loop he's going to essentially call get next",
    "start": "1124320",
    "end": "1130480"
  },
  {
    "text": "record and he's going to get those guys now at some point he's going to want to do output right so let's suppose after",
    "start": "1130480",
    "end": "1136280"
  },
  {
    "text": "these records he updates his top 10 list to that Dynamo table so he's going to update his Shard entry for that he's",
    "start": "1136280",
    "end": "1142880"
  },
  {
    "text": "also then going to issue a checkpoint to The Shard management table saying I've",
    "start": "1142880",
    "end": "1147960"
  },
  {
    "text": "seen and processed all the records through 10 and I never need to see them again now that last thing there is done",
    "start": "1147960",
    "end": "1155240"
  },
  {
    "text": "automatically for you basically in your processing code and we'll go through this later you'll just say I'm going to ready to do a checkpoint it will manage",
    "start": "1155240",
    "end": "1162080"
  },
  {
    "text": "writing that out to the Dynamo table for you so in this case he's checkpointed so let's see what happens now when we keep",
    "start": "1162080",
    "end": "1168200"
  },
  {
    "text": "going let's suppose some more records come in and we're processing along and some more records come in and",
    "start": "1168200",
    "end": "1175039"
  },
  {
    "text": "we process and now let's suppose that we fail so we go down so what's going to",
    "start": "1175039",
    "end": "1180640"
  },
  {
    "text": "happen is the lease is going to be lost and so you saw down in the table that host a has disappeared host B is going",
    "start": "1180640",
    "end": "1188280"
  },
  {
    "text": "to notice this and they're basically going to spin up a worker after having claimed the lock for it the client",
    "start": "1188280",
    "end": "1194400"
  },
  {
    "text": "Library will automatically go look at that sequence number in there and say okay I know that I need to start",
    "start": "1194400",
    "end": "1200240"
  },
  {
    "text": "processing from here and so I just go forward so that's how our checkpoint replay works now to give you an idea of",
    "start": "1200240",
    "end": "1207039"
  },
  {
    "text": "how long this failover takes our leases typically expire in a few seconds so by",
    "start": "1207039",
    "end": "1212080"
  },
  {
    "text": "the time the lease is expired and another host has noticed it and this has all worked it's typically on the order",
    "start": "1212080",
    "end": "1218000"
  },
  {
    "text": "of about 10 maybe 15 seconds at most depending on whether you have some Network hiccups so you want to think of",
    "start": "1218000",
    "end": "1224559"
  },
  {
    "text": "fail overtime as being something that's on the order of about 10 seconds say right",
    "start": "1224559",
    "end": "1230639"
  },
  {
    "text": "um let's see so what that means now is that um I'm going to be doing a catchup",
    "start": "1231360",
    "end": "1239159"
  },
  {
    "start": "1239000",
    "end": "1239000"
  },
  {
    "text": "after I've I've failed over right if it's a 15-second catchup I can",
    "start": "1239159",
    "end": "1245000"
  },
  {
    "text": "pretty much just do that without any trouble right I mean I just seek to that checkpoint I process through it and I'm",
    "start": "1245000",
    "end": "1250919"
  },
  {
    "text": "I've caught up very quickly but let's suppose we're processing",
    "start": "1250919",
    "end": "1256799"
  },
  {
    "text": "along and um we go down because there's a bug in",
    "start": "1256960",
    "end": "1262640"
  },
  {
    "text": "our software and so in fact we go down and the software on B never actually",
    "start": "1262640",
    "end": "1267880"
  },
  {
    "text": "spins up a worker or for some reason the whole Fleet goes down and we've got to essentially uh go patch a bug and so",
    "start": "1267880",
    "end": "1274960"
  },
  {
    "text": "let's say for a few hours or something we're down or a few minutes depending on how fast you can respond to on this well",
    "start": "1274960",
    "end": "1281000"
  },
  {
    "text": "records keep coming in right the good news is kinesis keeps those records for",
    "start": "1281000",
    "end": "1286120"
  },
  {
    "text": "you so if your sources had some buff limit like 10 minutes or something before they run over their disc uh and",
    "start": "1286120",
    "end": "1293120"
  },
  {
    "text": "have to start dropping data we buffer this for you we buffer 24 hours of this data before we expire it so if you can",
    "start": "1293120",
    "end": "1299600"
  },
  {
    "text": "come back up within 24 hours no data is lost right now of course the challenge is by the time you've come back up this",
    "start": "1299600",
    "end": "1306760"
  },
  {
    "text": "guy's going to spin up they're going to go from their checkpoint they're going to have a huge amount of data to go through right so they're going to be",
    "start": "1306760",
    "end": "1312600"
  },
  {
    "text": "trying to process this as fast as they can and um the question is well how long",
    "start": "1312600",
    "end": "1320760"
  },
  {
    "text": "is it going to take us to catch up that's a complicated question to a",
    "start": "1320760",
    "end": "1326279"
  },
  {
    "text": "certain extent you know the first thing that you might think about is well a standard Shard gives you these limits on",
    "start": "1326279",
    "end": "1332960"
  },
  {
    "text": "capacity for getting data in and out of Kinesis for a given Shard you can put data in at one megabit per second and",
    "start": "1332960",
    "end": "1339240"
  },
  {
    "text": "you can get it out at 2 megabits per second so if you had only one application running then you could actually catch up and you were running",
    "start": "1339240",
    "end": "1345760"
  },
  {
    "text": "at full capacity of your shard you were you were this aggressive person who",
    "start": "1345760",
    "end": "1351000"
  },
  {
    "text": "decided we're going to run at 100% capacity for some reason well then you could catch up basically in the same",
    "start": "1351000",
    "end": "1356360"
  },
  {
    "text": "amount of time that you were down now in reality people don't uh run at full utility because then every time there's",
    "start": "1356360",
    "end": "1362480"
  },
  {
    "text": "any kind of a load Spike you'd have to start rebalancing and resharding and doing everything else so in practice",
    "start": "1362480",
    "end": "1368039"
  },
  {
    "text": "people run it much more at say 50% or something like that right and remember shards we've tried to make them cheap so",
    "start": "1368039",
    "end": "1373919"
  },
  {
    "text": "that's not an unreasonable thing to do right so now if you're running it at half capacity even without the two",
    "start": "1373919",
    "end": "1379840"
  },
  {
    "text": "megabytes there you could or you know if you had one application running now you're going to catch up at three times",
    "start": "1379840",
    "end": "1385679"
  },
  {
    "text": "speed right you're going to have your regular processing and you've got 3x of that to catch up right so it's a variety",
    "start": "1385679",
    "end": "1391480"
  },
  {
    "text": "of things that are going to determine how fast you can catch up you also have to worry about you know how fast is your",
    "start": "1391480",
    "end": "1397679"
  },
  {
    "text": "application processing things as I said if you're running it in a way that's cpub bound on your processing side",
    "start": "1397679",
    "end": "1403200"
  },
  {
    "text": "that's going to determine how fast you can catch up um the good news is basically provisioning more shards",
    "start": "1403200",
    "end": "1410120"
  },
  {
    "text": "solves this problem right so if we make it cheap so that it's not ridiculous to",
    "start": "1410120",
    "end": "1415200"
  },
  {
    "text": "have more shards then what you can do is provision enough shards in order to be able to handle these kinds of spikes and",
    "start": "1415200",
    "end": "1420840"
  },
  {
    "text": "so forth and the nice thing is since we give you Dynamic resharding you don't have to decide this at the beginning and",
    "start": "1420840",
    "end": "1426480"
  },
  {
    "text": "hope that it lasts until the first time that you bring things down basically this just can keep running and you can",
    "start": "1426480",
    "end": "1432080"
  },
  {
    "text": "rehard as you need but rearing only works going forward right I can't sort of reard after a failure and say now I'd",
    "start": "1432080",
    "end": "1439039"
  },
  {
    "start": "1438000",
    "end": "1438000"
  },
  {
    "text": "like to have all the data that's already been uh ingested uh spread across more shards that doesn't work right so you've",
    "start": "1439039",
    "end": "1445919"
  },
  {
    "text": "got to rechart in time to be able to to handle the load going forward so let's talk about a couple of",
    "start": "1445919",
    "end": "1452240"
  },
  {
    "text": "other things uh that you'll want to know as I said you can have multiple applications that run in parallel and",
    "start": "1452240",
    "end": "1458840"
  },
  {
    "text": "they basically are completely independent of each other because they each maintain their own cursor through the data so these guys basically don't",
    "start": "1458840",
    "end": "1466200"
  },
  {
    "text": "interfere right so the one guy can be sitting there processing along and the other guy can be processing along they",
    "start": "1466200",
    "end": "1472520"
  },
  {
    "text": "really don't know about each other there one of them may hiccup may do a failover and so forth it really doesn't matter",
    "start": "1472520",
    "end": "1478720"
  },
  {
    "text": "the order and the timing that they have for processing through the data that's completely part uh independent of each",
    "start": "1478720",
    "end": "1486200"
  },
  {
    "text": "other now you might be wondering why not combine these applications why not have",
    "start": "1487480",
    "end": "1492520"
  },
  {
    "start": "1488000",
    "end": "1488000"
  },
  {
    "text": "one reader of the stream who then calls function calls in memory to you know one thing or the other now obvious viously I",
    "start": "1492520",
    "end": "1499240"
  },
  {
    "text": "showed you earlier in this thing uh you know Twitter Trend suppose somebody had created spot the revolution that tries",
    "start": "1499240",
    "end": "1505399"
  },
  {
    "text": "to detect revolutions going on in the Middle East right by looking at Twitter Trends if that's a separate org they'd",
    "start": "1505399",
    "end": "1511159"
  },
  {
    "text": "obviously be running it separately but you may want to do several things for",
    "start": "1511159",
    "end": "1516640"
  },
  {
    "text": "yourself within the same stream you might have that Twitter Trends thing going on and you might want to be keeping statistics in red shift so you",
    "start": "1516640",
    "end": "1522720"
  },
  {
    "text": "can do more you know lengthy analysis over the last month and you might be wondering why not just combine these two",
    "start": "1522720",
    "end": "1528600"
  },
  {
    "text": "things and just call them in memory the challenge with this is that the checkpointing and the output rates are",
    "start": "1528600",
    "end": "1535320"
  },
  {
    "text": "different from each other and it starts getting to be a headache if you try and combine these let's think about this",
    "start": "1535320",
    "end": "1540440"
  },
  {
    "text": "example if we had Twitter Trends I mentioned this is going to update every 10 seconds to Dynamo DB so every 10",
    "start": "1540440",
    "end": "1546120"
  },
  {
    "text": "seconds I'm going to do an output to a Dynamo TB table and then I'm going to checkpoint and what that means is on",
    "start": "1546120",
    "end": "1551880"
  },
  {
    "text": "fail over I'm going to go back to the last checkpoint from 10 seconds ago and process forwards 10 seconds in order to",
    "start": "1551880",
    "end": "1558000"
  },
  {
    "text": "catch back up right now if I'm doing statistics I may not want to load I may want to aggregate things say at the",
    "start": "1558000",
    "end": "1564600"
  },
  {
    "text": "hourly level uh to keep because I'm going to keep months and months or years of data and so here for this guy I'm",
    "start": "1564600",
    "end": "1571360"
  },
  {
    "text": "going to be outputting say an aggregated thing to Red shift once an hour and I'll be checkpointing once an hour now if I",
    "start": "1571360",
    "end": "1577559"
  },
  {
    "text": "fail over on this guy I'm going to go back a whole hour now if I combine these two things",
    "start": "1577559",
    "end": "1583320"
  },
  {
    "text": "into one worker the question is well when do I checkpoint how do I do this right if I say well let's checkpoint",
    "start": "1583320",
    "end": "1589480"
  },
  {
    "text": "everybody every 10 seconds then I'm going to be trying to upload to Red shift every 10 seconds red shift doesn't",
    "start": "1589480",
    "end": "1595880"
  },
  {
    "text": "like that level of incremental updates it likes big batch updates so that's not going to work very well but if you",
    "start": "1595880",
    "end": "1602399"
  },
  {
    "text": "checkpoint every hour then you're on failover you're going to reprocess an hour of data to catch up that's not very",
    "start": "1602399",
    "end": "1608720"
  },
  {
    "text": "good for current trends right so the easiest thing to do is to Simply split these apart and that works",
    "start": "1608720",
    "end": "1615919"
  },
  {
    "text": "fine all right let's talk about how we actually implement this thing so I'm going to take you through some code it's",
    "start": "1615919",
    "end": "1622159"
  },
  {
    "text": "necessarily going to be at a high level because um I didn't really have enough time to go through all the details so",
    "start": "1622159",
    "end": "1628760"
  },
  {
    "text": "this is more of a tour right the key message that I want to leave you with is we've open source we've created a bunch",
    "start": "1628760",
    "end": "1635840"
  },
  {
    "start": "1634000",
    "end": "1634000"
  },
  {
    "text": "of code that we've open sourced there's the client Library there's also a bunch of library of of code that we've written",
    "start": "1635840",
    "end": "1641880"
  },
  {
    "text": "that's sample code that shows you how to do things like connect to S3 or upload into red shift and so forth and all of",
    "start": "1641880",
    "end": "1649320"
  },
  {
    "text": "that code is available so you can modify it as you like you can go read it in detail and so this is intended just to",
    "start": "1649320",
    "end": "1655679"
  },
  {
    "text": "give you a flavor of everything now in order to create a Kinesis stream you would go through the console there's a",
    "start": "1655679",
    "end": "1661679"
  },
  {
    "text": "programmatic API if you want to do it that way but the most common way we expect would be people would go to the console and simply create a stream the",
    "start": "1661679",
    "end": "1669559"
  },
  {
    "text": "next thing they're going to be asked is how many shards and the primary thing that determines that is basically the",
    "start": "1669559",
    "end": "1675159"
  },
  {
    "start": "1672000",
    "end": "1672000"
  },
  {
    "text": "data volume and the number of puts that you're going to be doing and it's going to ask you about that and it's going to ask you how many applications you want",
    "start": "1675159",
    "end": "1681360"
  },
  {
    "text": "to run in parallel because that'll determine how much of your read capacity you need to provision so It'll ask you",
    "start": "1681360",
    "end": "1687120"
  },
  {
    "text": "all those things and it'll suggest a number of shards there's also some other considerations that you'll need to take",
    "start": "1687120",
    "end": "1693919"
  },
  {
    "text": "take uh take in account and I've mentioned them uh if you've got heavy processing and that's going to be the",
    "start": "1693919",
    "end": "1700039"
  },
  {
    "text": "bottleneck for your system you'll want to take that into account uh and if you have tight uh boundaries on catchup um",
    "start": "1700039",
    "end": "1708519"
  },
  {
    "text": "then you'll want to take that into account say you're doing something like a bulk archive into S3 where you might",
    "start": "1708519",
    "end": "1713799"
  },
  {
    "text": "be checkpointing every hour or something like that then you want to think about well what does it mean to have an hour",
    "start": "1713799",
    "end": "1719000"
  },
  {
    "text": "of catchup that I have to do and how fast do I want to go through that so these are considerations you'll have to",
    "start": "1719000",
    "end": "1724320"
  },
  {
    "text": "take into account for yourself the good news is you can experiment on this thing and you can reard later if you got it",
    "start": "1724320",
    "end": "1729880"
  },
  {
    "text": "wrong or if things grow if you're business grows then you can simply reard so it's not something where you have to",
    "start": "1729880",
    "end": "1736000"
  },
  {
    "text": "get this right the first time so now we've got our stream how are we going to do uh our",
    "start": "1736000",
    "end": "1744120"
  },
  {
    "text": "implementation well the client Library offers you this uh interface called i",
    "start": "1744120",
    "end": "1749679"
  },
  {
    "text": "reccord processor and you're going to implement that interface and there's the usual things you might expect an",
    "start": "1749679",
    "end": "1756320"
  },
  {
    "text": "initialization method and a shutdown method and the key one is process records basically the client library is",
    "start": "1756320",
    "end": "1762799"
  },
  {
    "text": "going to do that loop I described and it's going to just feed records and call this process records it's also going to hand you a checkpoint that you can call",
    "start": "1762799",
    "end": "1769399"
  },
  {
    "text": "whenever you want to checkpoint so what PR records looks like is this for our",
    "start": "1769399",
    "end": "1774559"
  },
  {
    "text": "particular example it's going to get some records in it's going to compute its local top 10 in memory and",
    "start": "1774559",
    "end": "1780600"
  },
  {
    "text": "periodically it's going to decide to do output now key thing that I forgot to mention actually is for that checkpoint",
    "start": "1780600",
    "end": "1787080"
  },
  {
    "text": "replay approach to work I have to be item potent in my output right if my output if I if I don't have a completely",
    "start": "1787080",
    "end": "1794600"
  },
  {
    "text": "item potent processing approach then if I replay things I'm going to a different result right so in this case the way",
    "start": "1794600",
    "end": "1801200"
  },
  {
    "text": "we're making an item potent is we're going to Output to that Dynamo DB table every time we exceed 2,000 tweets this",
    "start": "1801200",
    "end": "1808200"
  },
  {
    "text": "is deterministic right because the records are ordered and so you're going to get the exactly the same sequence of",
    "start": "1808200",
    "end": "1813360"
  },
  {
    "text": "Records whether you played it the first time or the second time or the third time right so this is deterministic and",
    "start": "1813360",
    "end": "1819360"
  },
  {
    "text": "so in this case every 2,000 records we're going to emit to Dynamo DB and we're going to feed in the checkpoint so",
    "start": "1819360",
    "end": "1825000"
  },
  {
    "text": "that it can do a checkpoint after it's updated the Dynamo table that's basically the code you write again",
    "start": "1825000",
    "end": "1831600"
  },
  {
    "text": "you're going to have to create your Ami and the uh uh you know put an an autoscaling group and things like that",
    "start": "1831600",
    "end": "1837480"
  },
  {
    "text": "together but this is the code that you're going to write for that implementation so we think it's fairly",
    "start": "1837480",
    "end": "1842679"
  },
  {
    "text": "straightforward and simple just to show you again you know the compute local top 10 you're going to basically have a hash",
    "start": "1842679",
    "end": "1848399"
  },
  {
    "text": "table and inside of this emit Dynamo DB you'll write to Dynamo and then you'll call checkpoint that's going to do that",
    "start": "1848399",
    "end": "1854960"
  },
  {
    "text": "update of that table The Shard management table that I",
    "start": "1854960",
    "end": "1859919"
  },
  {
    "text": "described so that's how you would write that one application but in fact typically what",
    "start": "1860279",
    "end": "1866760"
  },
  {
    "start": "1865000",
    "end": "1865000"
  },
  {
    "text": "we think is uh what we see customers wanting to do is a variety of things with their data it was mentioned yesterday in the 100 talk um Big Data",
    "start": "1866760",
    "end": "1875159"
  },
  {
    "text": "typically goes through a life cycle where you do some amount of processing local uh in real time and then you TP",
    "start": "1875159",
    "end": "1881039"
  },
  {
    "text": "and in this particular case that would be our Twitter Trends application but you also want to do a variety of other",
    "start": "1881039",
    "end": "1887240"
  },
  {
    "text": "things I already alluded to the fact that it might be nice to have statistics uh so that I could do say monthly",
    "start": "1887240",
    "end": "1892919"
  },
  {
    "text": "analysis of Trends so I'm going to want to take that same data and somehow aggregate it into a statistical form and",
    "start": "1892919",
    "end": "1898720"
  },
  {
    "text": "upload it into redshift and I may want to Archive this stuff maybe in an anonymized form and put it into S3 so",
    "start": "1898720",
    "end": "1905200"
  },
  {
    "text": "that if I discover an interesting Trend with my redshift analysis I can go actually look at the tweets and say hey",
    "start": "1905200",
    "end": "1910880"
  },
  {
    "text": "what was going on here right so really big data applications look more like this where you've got multiple things",
    "start": "1910880",
    "end": "1916760"
  },
  {
    "text": "happening to the same data stream in parallel so what we've done is we've",
    "start": "1916760",
    "end": "1922159"
  },
  {
    "start": "1921000",
    "end": "1921000"
  },
  {
    "text": "created a connector framework so there's the initial client library that lets you build your basic application this is now",
    "start": "1922159",
    "end": "1928919"
  },
  {
    "text": "a second set of software that we provide and it's offered in the form of uh open",
    "start": "1928919",
    "end": "1934039"
  },
  {
    "text": "source sample code that allows you to do these kinds of things like up upload into red shift archive IND S3 and so",
    "start": "1934039",
    "end": "1941039"
  },
  {
    "text": "forth and I'm going to take you through a brief tour of this stuff a key thing I want to emphasize here is the way this",
    "start": "1941039",
    "end": "1947240"
  },
  {
    "text": "is facted is a particular way you could do it a variety of other ways since it's open",
    "start": "1947240",
    "end": "1952440"
  },
  {
    "text": "source you can look at this stuff and you can decide for yourself how you want to tweak it the normal case is that",
    "start": "1952440",
    "end": "1957720"
  },
  {
    "text": "you'll have to write only a small amount of code in order to do things like archive IND S3 but if you don't like the",
    "start": "1957720",
    "end": "1963399"
  },
  {
    "text": "way the code works or if there's something more complicated that you wanted to do that's easy to do because",
    "start": "1963399",
    "end": "1968760"
  },
  {
    "text": "this is open source and we'll have documentation describing it so the key Concepts we provide in this connector",
    "start": "1968760",
    "end": "1974679"
  },
  {
    "text": "framework are listed here this is basically something that you would sub type in order to customize your application there's a notion of an",
    "start": "1974679",
    "end": "1981399"
  },
  {
    "text": "emitter that's basically the the the way that you target a different uh end",
    "start": "1981399",
    "end": "1986840"
  },
  {
    "text": "destination so we have an emitter that we've already written for S3 and for Dynamo and for redshift but you could",
    "start": "1986840",
    "end": "1992720"
  },
  {
    "text": "write your own for example it's very straightforward to write an emitter that would be a spout for a storm for storm",
    "start": "1992720",
    "end": "1999600"
  },
  {
    "text": "right so you could basically use kineses to get to your storm application if you had that by taking this connector code",
    "start": "1999600",
    "end": "2006519"
  },
  {
    "text": "and writing something that looks like like a a spout for storm we have a notion of a buffer and that's where you",
    "start": "2006519",
    "end": "2012360"
  },
  {
    "text": "would aggregate data so if you've got buffer you know if you've got records in and you're say doing a one minute",
    "start": "2012360",
    "end": "2018279"
  },
  {
    "text": "statistics aggregation across all those tweets you'd get those into the buffer that would buffer them up and then at",
    "start": "2018279",
    "end": "2024440"
  },
  {
    "text": "some point those would get emitted to Red shift right we have a notion of a transformer that allows you to take",
    "start": "2024440",
    "end": "2030039"
  },
  {
    "text": "records in and do Transformations on them and we have a notion of a filter that allows you to decide that you're",
    "start": "2030039",
    "end": "2035120"
  },
  {
    "text": "going to take some subset of these uh if you look at if we go into these interfaces in a little more detail",
    "start": "2035120",
    "end": "2041559"
  },
  {
    "text": "they're they're fairly straightforward the Transformer has a method called two class that this is going to be called to",
    "start": "2041559",
    "end": "2047799"
  },
  {
    "text": "get a record from the stream that's uh and convert it into a",
    "start": "2047799",
    "end": "2053118"
  },
  {
    "text": "Java data type that you would then manipulate in your in your connector code right so if I had tweets coming in",
    "start": "2053119",
    "end": "2059240"
  },
  {
    "text": "as a as a record which means a bite array basically I might turn them into a tweet uh into a Twitter record that",
    "start": "2059240",
    "end": "2066280"
  },
  {
    "text": "actually you know shows all of the various fields and so forth the from class is what we use to emit things so",
    "start": "2066280",
    "end": "2072638"
  },
  {
    "text": "the emitter is going to call from class in order to get a set of records of this type T and actually write them to the",
    "start": "2072639",
    "end": "2079679"
  },
  {
    "text": "Final Destination say S3 and the filter is very straight forward keep you know",
    "start": "2079679",
    "end": "2085118"
  },
  {
    "text": "we hand you a record and you either say keep it or not so if you were to look at the Twitter uh the Tweet transformer for",
    "start": "2085119",
    "end": "2091919"
  },
  {
    "start": "2091000",
    "end": "2091000"
  },
  {
    "text": "the Twitter Trends application this thing is going to define a we Define a Twitter record you would have to Define",
    "start": "2091919",
    "end": "2098480"
  },
  {
    "text": "that yourself and uh since tweets are in Json basically you just need for the two",
    "start": "2098480",
    "end": "2104560"
  },
  {
    "text": "class to fire up a Json processor Pro uh parser that is going to take that bite array and turn it into your Twitter",
    "start": "2104560",
    "end": "2111480"
  },
  {
    "text": "record okay on the other side it's pretty straightforward as well you're going to",
    "start": "2111480",
    "end": "2116960"
  },
  {
    "text": "get your record in and you're basically going to create a big string out of a big bite array out of this thing and",
    "start": "2116960",
    "end": "2122839"
  },
  {
    "text": "I've shown a sanitize operation and there one way to transform things would be to say on the out say I'm going to",
    "start": "2122839",
    "end": "2128480"
  },
  {
    "text": "anonymize the data or so forth I'll show you how to do aggregation in in a a short while so that's basically you know",
    "start": "2128480",
    "end": "2135960"
  },
  {
    "text": "in order to write say the S3 archiver you just saw the code now the buffer interface is more",
    "start": "2135960",
    "end": "2143560"
  },
  {
    "text": "complicated and really the main one that's interesting is the consume records um uh call let's suppose we",
    "start": "2143560",
    "end": "2151720"
  },
  {
    "start": "2149000",
    "end": "2149000"
  },
  {
    "text": "wanted to do that aggregation up to Red shift that I mentioned well one way I could do it is um def find a consume",
    "start": "2151720",
    "end": "2158640"
  },
  {
    "text": "records method that I'm going to subtype that takes in a Twitter aggregation record so this would not be the record",
    "start": "2158640",
    "end": "2164280"
  },
  {
    "text": "that I got from my stream this would be an aggregation record and I simply then take the aggregation records and",
    "start": "2164280",
    "end": "2171079"
  },
  {
    "text": "aggregate them together until I output them and those are going to get output now what makes this work is I'm going to",
    "start": "2171079",
    "end": "2177680"
  },
  {
    "text": "have a different Transformer in this case the Transformer that I'm going to use is one that on the input side takes",
    "start": "2177680",
    "end": "2185079"
  },
  {
    "start": "2184000",
    "end": "2184000"
  },
  {
    "text": "a record out of the Kinesis stream which is going to be a Twitter record I'm going to parse it into a Twitter record",
    "start": "2185079",
    "end": "2191160"
  },
  {
    "text": "but then I'm going to create a Twitter aggregation record out of that that only keeps the fields that I'm trying to",
    "start": "2191160",
    "end": "2196240"
  },
  {
    "text": "aggregate right and I'll hand that off into my framework and then on the other side it's going to be dealing strictly",
    "start": "2196240",
    "end": "2202800"
  },
  {
    "text": "in aggregation record so I'm going to hand that in and I'm now going to emit that in a form that I want say for red",
    "start": "2202800",
    "end": "2207960"
  },
  {
    "text": "shift which in this case could be a pipe delimited uh set of",
    "start": "2207960",
    "end": "2213920"
  },
  {
    "text": "strings so the emitter interface you you probably won't be writing emitters very",
    "start": "2214040",
    "end": "2219240"
  },
  {
    "text": "often because we're going to try and provide you lots of useful emitters but again for example if you wanted to write",
    "start": "2219240",
    "end": "2224599"
  },
  {
    "text": "a storm emitter or something to go to your favorite other processing framework real-time processing framework you could",
    "start": "2224599",
    "end": "2230800"
  },
  {
    "text": "write an emitter that allows you to basically use Kinesis as a buffering front end to go into another system like",
    "start": "2230800",
    "end": "2236920"
  },
  {
    "text": "that we're a big believer in the idea that we allow you to do what you want and in fact in the market of real-time",
    "start": "2236920",
    "end": "2242880"
  },
  {
    "text": "processing what we see is that the market is still experimenting with how to do things right there's stor form there's the Berkeley uh um system",
    "start": "2242880",
    "end": "2251240"
  },
  {
    "text": "screaming spark and so forth there's a variety of things and nobody really knows yet which of these are going to be the right way or whether there is going",
    "start": "2251240",
    "end": "2257079"
  },
  {
    "text": "to be one right way to do it and so what we want to do is make it easy for you to essentially hook Kinesis into these",
    "start": "2257079",
    "end": "2263880"
  },
  {
    "text": "other things right and so this connector framework is a first step in that direction",
    "start": "2263880",
    "end": "2270280"
  },
  {
    "text": "okay so let's talk about what that red shift connector actually looks like because it's an interesting example of",
    "start": "2270280",
    "end": "2275920"
  },
  {
    "text": "using more than of using several pieces so what I've shown here is a case where",
    "start": "2275920",
    "end": "2281560"
  },
  {
    "text": "I've got the Twitter stream coming in to my processor and what we're going to do is we're going to aggregate all of the",
    "start": "2281560",
    "end": "2288000"
  },
  {
    "text": "data into S3 and then we're going to upload it into redshift and the reason we want to do that is because redshift",
    "start": "2288000",
    "end": "2293640"
  },
  {
    "text": "likes to do bulk upload right these data warehouses need to do a lot of indexing and pre-computation on data that they",
    "start": "2293640",
    "end": "2299319"
  },
  {
    "text": "ingest and so they really don't like to do this one record at a time so what we're going to do is is our processing",
    "start": "2299319",
    "end": "2305599"
  },
  {
    "text": "stage is going to aggregate a bunch of these tweets and say uh for the example",
    "start": "2305599",
    "end": "2311480"
  },
  {
    "text": "that we showed with verer we do this on a minute basis take all the data for a minute write that into a bulk S3 object",
    "start": "2311480",
    "end": "2318400"
  },
  {
    "text": "and then call Red shift with a pointer to that S3 file and say upload that file",
    "start": "2318400",
    "end": "2324240"
  },
  {
    "text": "now the simple version of that would imply that every one of these shards is going to create one of these S3 records",
    "start": "2324240",
    "end": "2329800"
  },
  {
    "text": "that then is going to get uplift uh uploaded into am into red shift right those are still fairly small files if",
    "start": "2329800",
    "end": "2336400"
  },
  {
    "text": "you want to do the say every minute right you could do it longer but now you've got a longer lead a longer cycle",
    "start": "2336400",
    "end": "2341839"
  },
  {
    "text": "time what you can do is is actually have a two-stage system where we add another Kinesis stream and another processing",
    "start": "2341839",
    "end": "2348599"
  },
  {
    "text": "stage in order to get an upload across all of your shards the way that would work is the first processing stage still",
    "start": "2348599",
    "end": "2355160"
  },
  {
    "text": "writes these S3 files but instead of calling red shift and saying upload that file what it does is it puts the name of",
    "start": "2355160",
    "end": "2361160"
  },
  {
    "text": "the S3 object into a second Kinesis stream and then on the out other side",
    "start": "2361160",
    "end": "2366359"
  },
  {
    "text": "we've got a processor there and this only needs A Single Shard because this is going to be a low throughput stream that guy is going to get all of the",
    "start": "2366359",
    "end": "2372800"
  },
  {
    "text": "records for all of these objects and so what he can do now is say every minute is going across all of the shards just",
    "start": "2372800",
    "end": "2378960"
  },
  {
    "text": "create a manifest file of all those S3 object names hand that to Red shift and red shift will do that in one Fell Swoop",
    "start": "2378960",
    "end": "2385800"
  },
  {
    "text": "so here we've gotten much more batch you know we've supported the batch capabilities of red shift in a way that",
    "start": "2385800",
    "end": "2391800"
  },
  {
    "text": "nicely leverage the fact that we can compose these streams right and hint hint now if you wanted to convert that",
    "start": "2391800",
    "end": "2397839"
  },
  {
    "text": "Twitter stream from multiple hashtags to ones with duplicates in a single hashtag how would you do it just one more stage",
    "start": "2397839",
    "end": "2404480"
  },
  {
    "text": "just like that right all right so that's the end of the tour of these Concepts I want to end by",
    "start": "2404480",
    "end": "2411280"
  },
  {
    "start": "2410000",
    "end": "2410000"
  },
  {
    "text": "reminding you of what the value proposition is it is that we're trying to provide you Kinesis is managed so you",
    "start": "2411280",
    "end": "2418319"
  },
  {
    "text": "don't have to deal with managing all of the durable storage of this data we handle that elasticity and we do it in",
    "start": "2418319",
    "end": "2425480"
  },
  {
    "text": "real time um we we've provided you a bunch of connectors and open source libraries that'll show you how to put",
    "start": "2425480",
    "end": "2431160"
  },
  {
    "text": "this into the larger ecosystem we think it's easy to write um real-time applications in a fairly straightforward",
    "start": "2431160",
    "end": "2437839"
  },
  {
    "text": "way with our libraries and we've tried to make it cheap enough to where you don't have to worry about doing this so",
    "start": "2437839",
    "end": "2443359"
  },
  {
    "text": "with that I'll be happy to take questions",
    "start": "2443359",
    "end": "2448599"
  }
]