[
  {
    "start": "0",
    "end": "102000"
  },
  {
    "text": "[Music]",
    "start": "100",
    "end": "7429"
  },
  {
    "text": "hello everyone welcome to the AWS developer theater we still have a few",
    "start": "7429",
    "end": "13980"
  },
  {
    "text": "seats in the front not many but please come over so I'm Alex Caudill Bonnie I'm a",
    "start": "13980",
    "end": "20880"
  },
  {
    "text": "Technical Evangelist at Amazon Web Services and today I want to tell you a little bit how you can go and do data",
    "start": "20880",
    "end": "26789"
  },
  {
    "text": "collection and data analysis in a very efficient way in a very cost effective way and in a way that allows you to",
    "start": "26789",
    "end": "32750"
  },
  {
    "text": "analyze your data almost in real time or as quickly as possible let's say which is a great way to get insights from your",
    "start": "32750",
    "end": "41700"
  },
  {
    "text": "data so I want to ask you before we get started how many of you have attended more than one or two sessions today",
    "start": "41700",
    "end": "48500"
  },
  {
    "text": "Wow you really like the theater cool and how many of you have ever built a data",
    "start": "48500",
    "end": "56969"
  },
  {
    "text": "analysis dashboard or maybe run a data warehouse cool did you enjoy it was it a",
    "start": "56969",
    "end": "63510"
  },
  {
    "text": "fun experience not many ends okay so I",
    "start": "63510",
    "end": "69090"
  },
  {
    "text": "know I've done that too and today I want to share my experience with this kind of tools and micro services and data",
    "start": "69090",
    "end": "76710"
  },
  {
    "text": "analysis best practices I'm a software engineer I'm a web developer I am into",
    "start": "76710",
    "end": "82049"
  },
  {
    "text": "AI and servers and data analysis I've been in a start-up for four and a half years and analyzing data was my best",
    "start": "82049",
    "end": "88799"
  },
  {
    "text": "like of time like evening duty so it's a lot of fun I've been an AWS customer I",
    "start": "88799",
    "end": "95880"
  },
  {
    "text": "built that that start of on AWS and I recently joined Amazon Web Services so",
    "start": "95880",
    "end": "102229"
  },
  {
    "start": "102000",
    "end": "125000"
  },
  {
    "text": "what I want to tell you today a little bit about the data challenges that you find in this space so we're not talking",
    "start": "102229",
    "end": "109229"
  },
  {
    "text": "about real time or like production databases that back a like a product or",
    "start": "109229",
    "end": "116369"
  },
  {
    "text": "a service we're talking about data analysis okay which means reporting and dashboards it's a different set of data",
    "start": "116369",
    "end": "122549"
  },
  {
    "text": "use case and I want to mention why columnar format",
    "start": "122549",
    "end": "128399"
  },
  {
    "start": "125000",
    "end": "162000"
  },
  {
    "text": "a very good tool that you need to be aware of in this space and what's really",
    "start": "128399",
    "end": "133560"
  },
  {
    "text": "the difference between a data Lake data warehouse how do you handle those are there an alternative kind of use both",
    "start": "133560",
    "end": "138659"
  },
  {
    "text": "and what are the main differences and then how you go and do this data analytics and how you build your",
    "start": "138659",
    "end": "144750"
  },
  {
    "text": "pipelines and dashboards by keeping costs super low and by not thinking about servers and clusters and all those",
    "start": "144750",
    "end": "152090"
  },
  {
    "text": "operational things there is a data scientist or data analyst or just data geek you don't want to think about and",
    "start": "152090",
    "end": "159569"
  },
  {
    "text": "then I'll show you a demo because we do a lot of demos on stage so what are the challenges well in nowadays you don't",
    "start": "159569",
    "end": "168780"
  },
  {
    "start": "162000",
    "end": "332000"
  },
  {
    "text": "really have one source of data as you can imagine you might have clickstream data mobile application or like your",
    "start": "168780",
    "end": "175920"
  },
  {
    "text": "managers trying to sneak a spreadsheet here and there maybe you have infrastructure log coming from ribbon",
    "start": "175920",
    "end": "182459"
  },
  {
    "text": "run like production system maybe a social media analysis from your marketing team so if you're had it",
    "start": "182459",
    "end": "187709"
  },
  {
    "text": "analysis today and it analysts today you need to take take to take care of all these different data sources which is a",
    "start": "187709",
    "end": "194790"
  },
  {
    "text": "challenge of course and they come in two documents maybe there are files maybe they're structured maybe they're not you",
    "start": "194790",
    "end": "201510"
  },
  {
    "text": "have no idea what's in there and we you need the tools to be able to understand what's in there to discover data to",
    "start": "201510",
    "end": "206940"
  },
  {
    "text": "explore and to then go and build your analysis right there are so many",
    "start": "206940",
    "end": "212849"
  },
  {
    "text": "different data bases we like to say that there isn't one database for each like purpose-built database is not one size",
    "start": "212849",
    "end": "219659"
  },
  {
    "text": "fits all database and we'll get to that but that means that you have so many different options you can you can use",
    "start": "219659",
    "end": "226400"
  },
  {
    "text": "something like relational databases you may want it like paas grey or my sequel",
    "start": "226400",
    "end": "232290"
  },
  {
    "text": "or other options or maybe no sequel you may have heard of MongoDB or dynamodb or",
    "start": "232290",
    "end": "237989"
  },
  {
    "text": "redshift and other different kind of databases there maybe you have something",
    "start": "237989",
    "end": "243959"
  },
  {
    "text": "on premise and something in the cloud and you want to merge those different data sources again maybe some of this",
    "start": "243959",
    "end": "250769"
  },
  {
    "text": "data is not really there yet maybe it's coming every five seconds from our ot",
    "start": "250769",
    "end": "255989"
  },
  {
    "text": "device and you have to take care of analyzing that data source which is continuous you just",
    "start": "255989",
    "end": "261340"
  },
  {
    "text": "can't stop analyzing it's always coming new data's coming all the time and it's not about where the data is coming from",
    "start": "261340",
    "end": "267790"
  },
  {
    "text": "so about variety and volume it's also about who is consuming the data who wants to consume the data and the",
    "start": "267790",
    "end": "274870"
  },
  {
    "text": "reports and their analysis they we produce as data analysts so it's not just data scientist or analyst it may be",
    "start": "274870",
    "end": "283240"
  },
  {
    "text": "its business users and managers maybe want to be the dashboard that is internally available in the company to",
    "start": "283240",
    "end": "289389"
  },
  {
    "text": "everyone or maybe you have actually a web or mobile applications that show statistics and data you want to support",
    "start": "289389",
    "end": "295870"
  },
  {
    "text": "those as well so it's about what it is coming from and we will be consuming that data and there are a lot of",
    "start": "295870",
    "end": "301360"
  },
  {
    "text": "challenges in these two spaces and you want to make sure you can ingest and discover and build catalogs and do data",
    "start": "301360",
    "end": "308950"
  },
  {
    "text": "exploration to understand this data maybe you want to curate or validate or",
    "start": "308950",
    "end": "313960"
  },
  {
    "text": "clean these different data sources and the ultimate goal is to find useful",
    "start": "313960",
    "end": "319060"
  },
  {
    "text": "insights for the business and otherwise we wouldn't be here to collect and store",
    "start": "319060",
    "end": "324280"
  },
  {
    "text": "analyze all this data you want to understand it to solve the problem or to provide something useful to your",
    "start": "324280",
    "end": "330370"
  },
  {
    "text": "leadership right we're not talking about small scale data okay if you're talking",
    "start": "330370",
    "end": "335919"
  },
  {
    "text": "about customers and companies doing billions and billions of events every day like like FINRA with now managing",
    "start": "335919",
    "end": "343389"
  },
  {
    "text": "more than 99% of equities and 65% of options in the United States so they",
    "start": "343389",
    "end": "348639"
  },
  {
    "text": "have a lot a lot of data to ingest every day to collect to to discover yeah over",
    "start": "348639",
    "end": "355840"
  },
  {
    "text": "20 petabytes of storage currently being used so it's not a small scale maybe",
    "start": "355840",
    "end": "361030"
  },
  {
    "text": "you're a start-up but especially if you're a startup and you're growing you want to make sure that the tools that",
    "start": "361030",
    "end": "367240"
  },
  {
    "text": "you use today you don't have to change them every three or six or twelve months otherwise you keep reinventing the wheel",
    "start": "367240",
    "end": "373960"
  },
  {
    "text": "and doing the same thing over and over so the tools that I'm showing to you I love to say that we are democratizing",
    "start": "373960",
    "end": "380650"
  },
  {
    "text": "this stuff because what's been run using at this scale you can use it to in the store and since there is no upfront commitment",
    "start": "380650",
    "end": "387960"
  },
  {
    "text": "or no license or no monthly or really fee you can just scale up both the usage",
    "start": "387960",
    "end": "394290"
  },
  {
    "text": "and the cost of this kind of solution so startup is not going to pay as much as we're anticipating but you can go and",
    "start": "394290",
    "end": "400680"
  },
  {
    "text": "scale up without changing the system that's key we like to say that customer",
    "start": "400680",
    "end": "407130"
  },
  {
    "text": "needs come for us we just don't say it we really mean it it's our first daily",
    "start": "407130",
    "end": "412380"
  },
  {
    "text": "thought how can we improve the life for our customers and that and that's why we build all these purpose-built engines or",
    "start": "412380",
    "end": "419160"
  },
  {
    "text": "databases because there is no one database cancer that can solve all your problems so we really like to say that",
    "start": "419160",
    "end": "425460"
  },
  {
    "text": "you have to choose the right tool for the job here right and that's why you see all these different databases like",
    "start": "425460",
    "end": "431220"
  },
  {
    "text": "sequel and a sequel columnar non-cotton are real time or real what's there what",
    "start": "431220",
    "end": "437280"
  },
  {
    "text": "should I use there are a lot so these are a few of the services and databases",
    "start": "437280",
    "end": "444330"
  },
  {
    "start": "439000",
    "end": "781000"
  },
  {
    "text": "that help you manage all the different data sources so in the collect phase",
    "start": "444330",
    "end": "449850"
  },
  {
    "text": "where you want to get data and make sure that the producer of this data can push",
    "start": "449850",
    "end": "454920"
  },
  {
    "text": "it effortlessly into the system you have services like Amazon kinases so we are",
    "start": "454920",
    "end": "460830"
  },
  {
    "text": "it comes with a couple of flavors it's Canisius streams and QC Spyros they're similar interface for the consumer to",
    "start": "460830",
    "end": "467850"
  },
  {
    "text": "producer different ways of storing data and handling and processing data for you",
    "start": "467850",
    "end": "473220"
  },
  {
    "text": "in the backend but they are very very similar and then there is Direct Connect where you can just bind your dated in your",
    "start": "473220",
    "end": "479430"
  },
  {
    "text": "on-premise data center to data obvious cloud and just almost pretend it's the same network in the same virtual",
    "start": "479430",
    "end": "486600"
  },
  {
    "text": "environment and then you have snowball like physical devices that you can kind",
    "start": "486600",
    "end": "491820"
  },
  {
    "text": "of that we deliver to you and you touch it to your network you load your 25 billion petabytes of data in there and",
    "start": "491820",
    "end": "498660"
  },
  {
    "text": "then you ship it back that's much faster than you know moving data over the wire if we have very very large-scale data on",
    "start": "498660",
    "end": "505980"
  },
  {
    "text": "pram that you want to move to the cloud so different options to move and bring your data to the cloud but once it is in",
    "start": "505980",
    "end": "514409"
  },
  {
    "text": "the cloud where do you store it not all the data is equal so maybe you are",
    "start": "514410",
    "end": "520050"
  },
  {
    "text": "already in a like relational it's already in our relational form so maybe",
    "start": "520050",
    "end": "525990"
  },
  {
    "text": "you want to store it into our Amazon RDS database or maybe it's no sequel so you have items not records and they have",
    "start": "525990",
    "end": "533040"
  },
  {
    "text": "completely random structures we love it so maybe you put it into dynamo DB or maybe you want to kind of use our the",
    "start": "533040",
    "end": "540720"
  },
  {
    "text": "father of all the services which is I like to call it like that which is s3 if the result is our simple storage",
    "start": "540720",
    "end": "546540"
  },
  {
    "text": "service object storage where you can just store everything in there as an object whatever it is it's JSON a CSV",
    "start": "546540",
    "end": "553460"
  },
  {
    "text": "random binary file you can store everything in there and then you think about how you want to process and",
    "start": "553460",
    "end": "558660"
  },
  {
    "text": "analyze data on s3 later on I'll show you how to do that if we have data",
    "start": "558660",
    "end": "564870"
  },
  {
    "text": "that's been there for a while and you're pretty sure you're not using it anytime soon you can go and archive it into",
    "start": "564870",
    "end": "570270"
  },
  {
    "text": "Amazon glacier ok glacier like frozen data that you might need but it's not",
    "start": "570270",
    "end": "575880"
  },
  {
    "text": "strictly required if you need it right now it must take 10 seconds to read it maybe it can take a couple of hours it's",
    "start": "575880",
    "end": "582600"
  },
  {
    "text": "maybe at last year's backups you don't care anymore you can put those stuff on glacier there are the tools like cloud",
    "start": "582600",
    "end": "589770"
  },
  {
    "text": "storage elastic search if in your analysis pipeline you need to do like full-text search and other advanced",
    "start": "589770",
    "end": "596640"
  },
  {
    "text": "search capabilities maybe you don't need them all the time but it's good to know they exist so you don't reinvent again a",
    "start": "596640",
    "end": "603060"
  },
  {
    "text": "text search in every project and once the data is stored anywhere you store it",
    "start": "603060",
    "end": "609150"
  },
  {
    "text": "you want to go and analyze it different tools in here maybe you are a Hadoop",
    "start": "609150",
    "end": "615180"
  },
  {
    "text": "Apache Hadoop fan so you want to start your Amazon EMR cluster maybe you have a",
    "start": "615180",
    "end": "620850"
  },
  {
    "text": "real-time analytics use case and if you are already using Amazon Kinesis dreams and fireworks you can even use Amazon",
    "start": "620850",
    "end": "627960"
  },
  {
    "text": "Kinesis analytics which will allow you to do moving window SQL queries on upcoming real-time data",
    "start": "627960",
    "end": "634710"
  },
  {
    "text": "so you can then go and kind of a real-time stream of an",
    "start": "634710",
    "end": "640660"
  },
  {
    "text": "analytics it's it's a strange way of doing it but very efficient and it",
    "start": "640660",
    "end": "645730"
  },
  {
    "text": "allows you to build real-time dashboards in like once icon 10-second window so it is very cool and then you have other",
    "start": "645730",
    "end": "653130"
  },
  {
    "text": "more structure tools like redshift it's data warehousing columnar format",
    "start": "653130",
    "end": "658150"
  },
  {
    "text": "database then you have Athena Athena is kind of special because it's not really",
    "start": "658150",
    "end": "665710"
  },
  {
    "text": "a database it's more like an SQL interface on top of your s3 data okay so",
    "start": "665710",
    "end": "671890"
  },
  {
    "text": "imagine you have CSV or JSON or park' or any kind of structure or semi semi",
    "start": "671890",
    "end": "678100"
  },
  {
    "text": "structured data on s3 you can build you can define like a virtual table on top of it and use Athena to query that data",
    "start": "678100",
    "end": "685330"
  },
  {
    "text": "so no data ingestion into a database no real data it's like it's a schema on",
    "start": "685330",
    "end": "691870"
  },
  {
    "text": "read it's called so you define a schema and then the query engine will will run",
    "start": "691870",
    "end": "698680"
  },
  {
    "text": "your query in parallel in as many machines as needed and you don't pay about a machine there is no cluster or",
    "start": "698680",
    "end": "703810"
  },
  {
    "text": "you don't pay by the amount of computation you pay by how much data you're scanning from s3 which is an",
    "start": "703810",
    "end": "711430"
  },
  {
    "text": "interesting way of thinking about databases and analysis so it's also very cool because if you optimize your data",
    "start": "711430",
    "end": "718029"
  },
  {
    "text": "well enough you will scan fewer data fewer gigabytes or megabytes or petabytes and it means your queries will",
    "start": "718029",
    "end": "725200"
  },
  {
    "text": "be faster and cheaper so very good reason to optimize your database your your data set as well but these are more",
    "start": "725200",
    "end": "733000"
  },
  {
    "text": "like query databases like a different way of queering your data how do you go and do data exploration data discovery",
    "start": "733000",
    "end": "739900"
  },
  {
    "text": "and there are actually building dashboards for example so there are tools like Amazon quick site which I'm",
    "start": "739900",
    "end": "745870"
  },
  {
    "text": "showing you later in this in the demo which allows you to do like point-and-click data exploration on your",
    "start": "745870",
    "end": "752200"
  },
  {
    "text": "data sets whether wherever it comes from it's on redshift and dynamo on RDS on s3",
    "start": "752200",
    "end": "757570"
  },
  {
    "text": "wherever it is you can say hey go and push data from there maybe merge it with other data sources and then you have a",
    "start": "757570",
    "end": "764010"
  },
  {
    "text": "AI driven - bored creation point-and-click experience where you just click on",
    "start": "764010",
    "end": "772019"
  },
  {
    "text": "fields' and maybe you like that piece of the pie you click in there and you dive deep and go and do the exploration very",
    "start": "772019",
    "end": "778769"
  },
  {
    "text": "intuitive I'm going to show you later so a quick parentheses on what columnar",
    "start": "778769",
    "end": "784350"
  },
  {
    "start": "781000",
    "end": "942000"
  },
  {
    "text": "data who knows what is or how it works under the hood call them your data anyone awesome you are in the right",
    "start": "784350",
    "end": "790500"
  },
  {
    "text": "place so there are many open starters like back by Apaches such as Parque or",
    "start": "790500",
    "end": "797009"
  },
  {
    "text": "RC so today we will see mostly Parque why do you need this kind of data format",
    "start": "797009",
    "end": "802889"
  },
  {
    "text": "well you want to optimize the performance of some kind of queries we call analytical queries so maybe in a in",
    "start": "802889",
    "end": "810660"
  },
  {
    "text": "a production database in a service you want to query by key you want to filter by I don't know by by column or you want",
    "start": "810660",
    "end": "819000"
  },
  {
    "text": "to do like production kind of queries but analytical queries are very different because usually you reason on",
    "start": "819000",
    "end": "825600"
  },
  {
    "text": "very large data sets you want to build aggregates averages sums and stuff that are very often column based not really",
    "start": "825600",
    "end": "834209"
  },
  {
    "text": "record based you don't care about the individual records anymore you want to build aggregate statistics so we want to",
    "start": "834209",
    "end": "840360"
  },
  {
    "text": "optimize those those kind of queries and what it looks like under the hood I mean the format itself I'm not going into the",
    "start": "840360",
    "end": "846949"
  },
  {
    "text": "implementation details but intuitively what it looks like is if you think about",
    "start": "846949",
    "end": "852000"
  },
  {
    "text": "records like a BSC our fields in your data model a1 like 1 2 3 4 5 our records",
    "start": "852000",
    "end": "860040"
  },
  {
    "text": "okay what you usually do in traditional raw layout databases that is that you",
    "start": "860040",
    "end": "867660"
  },
  {
    "text": "store a record at a time so here we have a1 b1 c1 that's the first record and",
    "start": "867660",
    "end": "872970"
  },
  {
    "text": "then a2 v2 t c2 and so on what we do in colouring our format is video totally",
    "start": "872970",
    "end": "879569"
  },
  {
    "text": "the opposite we just store the same column altogether and then we stop and then we store another column altogether",
    "start": "879569",
    "end": "885750"
  },
  {
    "text": "and so on so as you can imagine this allows you to to access",
    "start": "885750",
    "end": "892470"
  },
  {
    "text": "and the reason on specific columns in a much more efficient way because you can",
    "start": "892470",
    "end": "897840"
  },
  {
    "text": "say hey give me the sum of field a you don't need to go through the whole file",
    "start": "897840",
    "end": "903330"
  },
  {
    "text": "you can just read the first third of the file and stop there you just read those",
    "start": "903330",
    "end": "908970"
  },
  {
    "text": "later so intuitively it might be more if it is usually more efficient for unalloyed achill queries there is more",
    "start": "908970",
    "end": "916650"
  },
  {
    "text": "because actually at the beginning of this section you have made some meta data as well so if you want to do",
    "start": "916650",
    "end": "922920"
  },
  {
    "text": "queries like count you don't have even to go and scan anything it's already in there so it helps you doing this kind of",
    "start": "922920",
    "end": "930410"
  },
  {
    "text": "aggregation count some averages analytic so you can store those while you build",
    "start": "930410",
    "end": "935820"
  },
  {
    "text": "the columnar file why should you build it at runtime so it helps you with this",
    "start": "935820",
    "end": "941340"
  },
  {
    "text": "kind of queries wide matters because we are going into the Big Data world it means not one gigabyte not tanking about",
    "start": "941340",
    "end": "949350"
  },
  {
    "start": "942000",
    "end": "991000"
  },
  {
    "text": "maybe 200 petabytes of data that you want to analyze every six hours or every 60 seconds if you have a real-time use",
    "start": "949350",
    "end": "957420"
  },
  {
    "text": "case and it's very important that you do this for data exploration okay",
    "start": "957420",
    "end": "964710"
  },
  {
    "text": "building a dashboard is a pain if you have an idea what the data looks like and what's in there and and sometimes",
    "start": "964710",
    "end": "972750"
  },
  {
    "text": "you find insights not by building dashboards but just by playing with the data you have a question in mind and you",
    "start": "972750",
    "end": "978990"
  },
  {
    "text": "want to go and dive deep into it so you want to have some sort of efficient way to do their exploration on the latest",
    "start": "978990",
    "end": "986550"
  },
  {
    "text": "version of the data so very very important there especially for startups",
    "start": "986550",
    "end": "991910"
  },
  {
    "start": "991000",
    "end": "1057000"
  },
  {
    "text": "what what used to happen in the traditional world is that you this I",
    "start": "991910",
    "end": "997410"
  },
  {
    "text": "mean relational data was the only option you can go terabyte to petabyte scale",
    "start": "997410",
    "end": "1002950"
  },
  {
    "text": "maybe not exabyte and you know all the new world that I don't even remember like thousands or millions of petabytes",
    "start": "1002950",
    "end": "1011300"
  },
  {
    "text": "that that's a new scale that was not even conceived when with these tools I work",
    "start": "1011300",
    "end": "1016550"
  },
  {
    "text": "in the first place this came the schema needed to be defined a priori",
    "start": "1016550",
    "end": "1022040"
  },
  {
    "text": "before you even started having data at all which makes sense in some production",
    "start": "1022040",
    "end": "1028579"
  },
  {
    "text": "database but for analytics it's a pain you have a new data type that you want",
    "start": "1028580",
    "end": "1034670"
  },
  {
    "text": "to start collecting and it takes five weeks just to create the table to agree on the data for mine you wasted months",
    "start": "1034670",
    "end": "1041360"
  },
  {
    "text": "okay the goal of these tools was to build operational reporting and to do",
    "start": "1041360",
    "end": "1047930"
  },
  {
    "text": "very high doc analytics the goal is still there we just changed how we do it",
    "start": "1047930",
    "end": "1053900"
  },
  {
    "text": "and how the data is store and what tools we use and then there is the concept of",
    "start": "1053900",
    "end": "1059990"
  },
  {
    "start": "1057000",
    "end": "1164000"
  },
  {
    "text": "data Lake okay data Lake are there not an alternative I rather say they are an",
    "start": "1059990",
    "end": "1065960"
  },
  {
    "text": "extension of your of your data warehouse and with data Lake they live on the side",
    "start": "1065960",
    "end": "1073070"
  },
  {
    "text": "and they allow you to a relational non-relational structured unstructured data all living in the same place in a",
    "start": "1073070",
    "end": "1081530"
  },
  {
    "text": "lot since this data can be distributed okay I can doesn't have to fit into one",
    "start": "1081530",
    "end": "1086570"
  },
  {
    "text": "big fat database it allows you to go up to the exabyte scale if not more there's",
    "start": "1086570",
    "end": "1091970"
  },
  {
    "text": "no theoretical limit there the schema does not need to be defined a priori you",
    "start": "1091970",
    "end": "1097220"
  },
  {
    "text": "can define it afterwards the data is there you define a virtual schema and go and read it scheme on read awesome so we",
    "start": "1097220",
    "end": "1104360"
  },
  {
    "text": "think that problem if they help you fix that part of the problem sometimes you also need schema Priore for like data",
    "start": "1104360",
    "end": "1111230"
  },
  {
    "text": "that changes a lot or data that you need to correlate with historical data so you don't forget the traditional world but",
    "start": "1111230",
    "end": "1117140"
  },
  {
    "text": "think how you can extend it let's say that",
    "start": "1117140",
    "end": "1123549"
  },
  {
    "text": "how can we say even the cost of these tools tend to be much smaller because",
    "start": "1124370",
    "end": "1130450"
  },
  {
    "text": "you don't have a cluster of machines to handle anymore or if you do it in the cloud like on AWS we take care of all",
    "start": "1130450",
    "end": "1137809"
  },
  {
    "text": "the infrastructure and you don't even see physical machines clusters you don't see that anymore and you don't even pay",
    "start": "1137809",
    "end": "1143930"
  },
  {
    "text": "for that so if you have a query that can be parallelized very efficiently in ten thousand machines we are the cluster",
    "start": "1143930",
    "end": "1149930"
  },
  {
    "text": "they're running you give us the query we understand how to paralyze it you just pay by the amount of gigabytes or",
    "start": "1149930",
    "end": "1155960"
  },
  {
    "text": "megabytes very different model very different cost different way of thinking",
    "start": "1155960",
    "end": "1162200"
  },
  {
    "text": "of cost as well as a data analyst the way you do it on on on on AWS you put",
    "start": "1162200",
    "end": "1171020"
  },
  {
    "start": "1164000",
    "end": "1214000"
  },
  {
    "text": "your data on s3 SV is helping you like if the the basis of every other aw",
    "start": "1171020",
    "end": "1178250"
  },
  {
    "text": "service almost or almost every of the services using s3 for some sort of storage capability and again there are",
    "start": "1178250",
    "end": "1186170"
  },
  {
    "text": "different ways to put data in there are different ways of securing to reach compliance to reach auditing so if you",
    "start": "1186170",
    "end": "1192830"
  },
  {
    "text": "are in a regulated environment no problem and they double yes platform",
    "start": "1192830",
    "end": "1198170"
  },
  {
    "text": "also allows you to build very fine-grained access policies for your data analyst for your data for you the",
    "start": "1198170",
    "end": "1204500"
  },
  {
    "text": "consumer of the reports so you can you can you can implement very tight security and auditing policies mmm let's",
    "start": "1204500",
    "end": "1215240"
  },
  {
    "start": "1214000",
    "end": "1555000"
  },
  {
    "text": "go on so how what are the components how you can build this stuff on AWS well mmm",
    "start": "1215240",
    "end": "1222140"
  },
  {
    "text": "the data ingestion phase is very is very interesting because you have tools like",
    "start": "1222140",
    "end": "1227600"
  },
  {
    "text": "kinases virus for this specific data Lake case it's perfect so what you do as a consumer you put",
    "start": "1227600",
    "end": "1233660"
  },
  {
    "text": "records and it's an a sync operation okay one episode of the hood is that you configure Kinesis virus to put data to",
    "start": "1233660",
    "end": "1241640"
  },
  {
    "text": "store data directly on s3 and you have zero lines of code written so far okay",
    "start": "1241640",
    "end": "1246650"
  },
  {
    "text": "it does everything for you what's even more interesting is that",
    "start": "1246650",
    "end": "1252490"
  },
  {
    "text": "Pyro's will not put each record in its own file okay it will act as a buffer so",
    "start": "1252610",
    "end": "1258130"
  },
  {
    "text": "it will ingest data I will take it there for a while you can configure how much data or how for how long",
    "start": "1258130",
    "end": "1264669"
  },
  {
    "text": "based on your Chi and based on your compression needs or your real-time needs and what happens after I know 60",
    "start": "1264669",
    "end": "1271450"
  },
  {
    "text": "seconds for example this is what we're trying to do today and insights in 60 seconds is that it will take all the",
    "start": "1271450",
    "end": "1276880"
  },
  {
    "text": "data collected so far compress it encrypt it eventually converted to",
    "start": "1276880",
    "end": "1282130"
  },
  {
    "text": "columnar format so still zero code lines of code written and put it on this tree so if you build that what you have so",
    "start": "1282130",
    "end": "1289240"
  },
  {
    "text": "far is a system that can take data compress it encrypted transforming into",
    "start": "1289240",
    "end": "1295929"
  },
  {
    "text": "columnar data put it on s3 zero lines of code Riki okay and this is these scales there is",
    "start": "1295929",
    "end": "1301960"
  },
  {
    "text": "no server there's no cluster to manage and here you also pay about how much",
    "start": "1301960",
    "end": "1307419"
  },
  {
    "text": "data is passing so you can have a dev environment a test environment a prod environment yes how much you pay for the",
    "start": "1307419",
    "end": "1312880"
  },
  {
    "text": "dev environment at night if nobody is using it zero okay you pay about how",
    "start": "1312880",
    "end": "1317980"
  },
  {
    "text": "much data is going through very cool this is the ingestion phase ok different",
    "start": "1317980",
    "end": "1323289"
  },
  {
    "text": "ways I think virus is your best alternative because it allows you to put records in real time from a mobile app",
    "start": "1323289",
    "end": "1329289"
  },
  {
    "text": "from a certain web application wherever data is coming from you have the different SDKs CI to CLI tools you can",
    "start": "1329289",
    "end": "1336519"
  },
  {
    "text": "put data in any way you want but it's not just about putting data in there it's also about building a catalog and",
    "start": "1336519",
    "end": "1343450"
  },
  {
    "text": "making sure that people can go and explore and go and understand what data",
    "start": "1343450",
    "end": "1349210"
  },
  {
    "text": "has been put there because you don't want to slow down the people that are producing data but then as a data",
    "start": "1349210",
    "end": "1354370"
  },
  {
    "text": "analyst you on a quick efficient way to make sense of all this data in different formats and different variations of data",
    "start": "1354370",
    "end": "1362740"
  },
  {
    "text": "sources so how you do that aw glue a blue is a fun name but it",
    "start": "1362740",
    "end": "1368980"
  },
  {
    "text": "gives you a hint it's bluing it's a helping you glue all these pieces together about ingestion and analytics",
    "start": "1368980",
    "end": "1375070"
  },
  {
    "text": "so glue will build a data catalog for you there there are crawlers betta crawlers they",
    "start": "1375070",
    "end": "1380270"
  },
  {
    "text": "will go and kind of go on into s3 and see what's being put there and build you",
    "start": "1380270",
    "end": "1385780"
  },
  {
    "text": "beautiful tables and schemas and with versioning and it is really powerful and",
    "start": "1385780",
    "end": "1392960"
  },
  {
    "text": "then you can go and use analytical tools in the processing and analytics phase",
    "start": "1392960",
    "end": "1398360"
  },
  {
    "text": "like quick side maybe you want to enhance this data with some artificial intelligence tools maybe you are",
    "start": "1398360",
    "end": "1404600"
  },
  {
    "text": "collecting I don't know audio and maybe you want to transcribe the audio before you put it into s3 or after you put it",
    "start": "1404600",
    "end": "1411440"
  },
  {
    "text": "into s3 so you have another ETL step of audio transcription and then you can reason on to a scripted text coming from",
    "start": "1411440",
    "end": "1419360"
  },
  {
    "text": "audio may be mobile devices or like I don't know conversational interfaces stuff like that so you can kind of build",
    "start": "1419360",
    "end": "1427580"
  },
  {
    "text": "your own processing and blue will also help you do all the ETL so it comes with",
    "start": "1427580",
    "end": "1432590"
  },
  {
    "text": "pre-built scripts so you say ok I'm at it is coming from these data source I need to convert it this way into this",
    "start": "1432590",
    "end": "1439730"
  },
  {
    "text": "all data source it'll give you a Python script that you can customize you just okay just oh now it looks my scale",
    "start": "1439730",
    "end": "1446810"
  },
  {
    "text": "there's three times a day whatever run it no server is no configuration very awesome security top",
    "start": "1446810",
    "end": "1455750"
  },
  {
    "text": "priority you have all the logging you of all the auditing who did what with Amazon Cloud work with Amazon Cloud",
    "start": "1455750",
    "end": "1462170"
  },
  {
    "text": "trail so it's not a black box that you have no idea what's going on you can go and see which API calls have been called",
    "start": "1462170",
    "end": "1468370"
  },
  {
    "text": "who did what and do auditing and monitoring of the system overall so this",
    "start": "1468370",
    "end": "1473690"
  },
  {
    "text": "is more or less what the landscape looks like I like to stress the world",
    "start": "1473690",
    "end": "1478870"
  },
  {
    "text": "serverless meaning you don't see the servers or the clusters you don't need",
    "start": "1478870",
    "end": "1484040"
  },
  {
    "text": "to go and patch the machines or install updated version of the libraries that you're using so you can forget about all",
    "start": "1484040",
    "end": "1491000"
  },
  {
    "text": "the infrastructure management infrastructure administration part which as the data is a dream",
    "start": "1491000",
    "end": "1497480"
  },
  {
    "text": "so another effect of this service thing is you pay only for what you use again if",
    "start": "1497480",
    "end": "1504850"
  },
  {
    "text": "nobody's using the system you want to pay zero right and we like to say you don't pay for idle I load capacity",
    "start": "1504850",
    "end": "1512220"
  },
  {
    "text": "forget it's not a forget it's not a problem anymore not just that but how",
    "start": "1512220",
    "end": "1517659"
  },
  {
    "text": "okay raise your hand how many times you build a dashboard reporting very beautiful dashboard but then it was a",
    "start": "1517659",
    "end": "1524289"
  },
  {
    "text": "flying because the data the data warehouse was down or it was an error every week right okay since most of the",
    "start": "1524289",
    "end": "1532480"
  },
  {
    "text": "tools we are seeing on our servers we take care of the infrastructure we take care of making sure that the software",
    "start": "1532480",
    "end": "1539049"
  },
  {
    "text": "components are already distributed in different availability zones in different like fault tolerant areas so",
    "start": "1539049",
    "end": "1546399"
  },
  {
    "text": "that if even a whole data center goes down okay there is another part of the system that's up and that can serve your",
    "start": "1546399",
    "end": "1552129"
  },
  {
    "text": "request so very cool again it's not",
    "start": "1552129",
    "end": "1557769"
  },
  {
    "start": "1555000",
    "end": "1596000"
  },
  {
    "text": "about it's not only about storing and ingesting it's not about discovering and the more your go the more you go into",
    "start": "1557769",
    "end": "1565750"
  },
  {
    "text": "the Big Data world the more this concept of dark data emerges which means you",
    "start": "1565750",
    "end": "1572049"
  },
  {
    "text": "have no idea what's in there and if you have no idea what's in there you're not using that data anytime soon so we have",
    "start": "1572049",
    "end": "1578139"
  },
  {
    "text": "first to find a way to solve that data to solve that problem you want to reduce",
    "start": "1578139",
    "end": "1583330"
  },
  {
    "text": "that hopefully to zero you might have data that you don't need anymore you want to archive it or delete it but not",
    "start": "1583330",
    "end": "1589809"
  },
  {
    "text": "being aware of what's in there is no it's not I mean it shouldn't happen",
    "start": "1589809",
    "end": "1595750"
  },
  {
    "text": "anymore a few words few more words about blue it helps you do ETL apps to build a",
    "start": "1595750",
    "end": "1602529"
  },
  {
    "start": "1596000",
    "end": "1649000"
  },
  {
    "text": "data catalog but you see these little spiders here or had they are crawlers so they you can configure glue to go and",
    "start": "1602529",
    "end": "1609639"
  },
  {
    "text": "fetch data from yours three buckets from your all your different s3 objects and",
    "start": "1609639",
    "end": "1615809"
  },
  {
    "text": "it was the goal here is to make sure that the data that you store is immediately available for you you can",
    "start": "1615809",
    "end": "1622539"
  },
  {
    "text": "see the schema you can see the schema version I'm sorry if this if the schemas",
    "start": "1622539",
    "end": "1629019"
  },
  {
    "text": "change - Jim one one be the change and what has changed so it will create new",
    "start": "1629019",
    "end": "1634909"
  },
  {
    "text": "schema versions for you automatically so very interesting tool you can go in scheduled ETL jobs and you generate",
    "start": "1634909",
    "end": "1642679"
  },
  {
    "text": "customizable code code for you Python codes color code depending on what you are more comfortable with",
    "start": "1642679",
    "end": "1650019"
  },
  {
    "start": "1649000",
    "end": "1721000"
  },
  {
    "text": "crawlers are really amazing if the built-in ones are not good enough",
    "start": "1650019",
    "end": "1655759"
  },
  {
    "text": "for you you can even build custom classifiers based on pattern so maybe you're analyzing very structure data but",
    "start": "1655759",
    "end": "1663049"
  },
  {
    "text": "maybe there are no server logs and you want an easy way to match the server log",
    "start": "1663049",
    "end": "1668179"
  },
  {
    "text": "pattern with your crawler okay you can specify the pattern and the crawler widow will match it with the data it's",
    "start": "1668179",
    "end": "1678109"
  },
  {
    "text": "not only about where the data is and what the schema is it's also about what's the format is it Jason",
    "start": "1678109",
    "end": "1684559"
  },
  {
    "text": "is it CSV is in 4k should you care no because there are built-in classifiers",
    "start": "1684559",
    "end": "1690109"
  },
  {
    "text": "even in within the same dataset you might have even different formats some people are pushing CSV some people push",
    "start": "1690109",
    "end": "1695869"
  },
  {
    "text": "in JSON awesome we can deal with that so",
    "start": "1695869",
    "end": "1700909"
  },
  {
    "text": "it's not even about s3 only data and crawl these crawlers from blue can go and fetch your data into from redshift",
    "start": "1700909",
    "end": "1707599"
  },
  {
    "text": "or from an RDS databases database so maybe a positron or my sequel stuff that",
    "start": "1707599",
    "end": "1713629"
  },
  {
    "text": "you need to kind of match and combine and join with your with the rest of the data so this crawlers will go and and take",
    "start": "1713629",
    "end": "1720830"
  },
  {
    "text": "care of that as well similarity is interesting would you say that these two",
    "start": "1720830",
    "end": "1726469"
  },
  {
    "start": "1721000",
    "end": "1763000"
  },
  {
    "text": "schemas are similar so there is a name and ID and an address so it's kind of",
    "start": "1726469",
    "end": "1731869"
  },
  {
    "text": "similar the first one is a structured approach to store the address with two",
    "start": "1731869",
    "end": "1737479"
  },
  {
    "text": "subfields but crawlers will also help you find similarity in different data",
    "start": "1737479",
    "end": "1742969"
  },
  {
    "text": "schemas so they can tell you okay this can this schema this new scheme is our 87% similar so you might want to do an",
    "start": "1742969",
    "end": "1749869"
  },
  {
    "text": "ETL step there you may want to kind of consider it similar enough if you don't",
    "start": "1749869",
    "end": "1755419"
  },
  {
    "text": "care about the address format for example maybe you want to concatenate the three fields and you're good so they will help you take of care of",
    "start": "1755419",
    "end": "1763100"
  },
  {
    "start": "1763000",
    "end": "1805000"
  },
  {
    "text": "that silent problem what about partitioning partitioning really helps you optimizing the",
    "start": "1763100",
    "end": "1770600"
  },
  {
    "text": "efficiency of your queries so if you build partitions maybe my day or by month or by year maybe you don't want to",
    "start": "1770600",
    "end": "1777350"
  },
  {
    "text": "query the whole historical data set if you've been there for five years maybe you want to be early or dashboards or",
    "start": "1777350",
    "end": "1785300"
  },
  {
    "text": "maybe you want to only analyze next month or current month data so partitions can help you focus on a",
    "start": "1785300",
    "end": "1791000"
  },
  {
    "text": "specific subset of the data and being able to go and build or detect",
    "start": "1791000",
    "end": "1798800"
  },
  {
    "text": "partitions if we have already already stored the data in a partition friendly way okay glue will help you with that",
    "start": "1798800",
    "end": "1805250"
  },
  {
    "start": "1805000",
    "end": "1827000"
  },
  {
    "text": "too I already mentioned schema versioning so if the data has changed at some point",
    "start": "1805250",
    "end": "1811240"
  },
  {
    "text": "you want to be aware of what has changed between version 1 and version 2 what changed exactly in the schema when it",
    "start": "1811240",
    "end": "1818330"
  },
  {
    "text": "changed so maybe when you if you want to do very manual queries you can filter the data and make sure you're filtering",
    "start": "1818330",
    "end": "1824690"
  },
  {
    "text": "the exact schema that you're talking about and there are different ways of",
    "start": "1824690",
    "end": "1829970"
  },
  {
    "start": "1827000",
    "end": "1851000"
  },
  {
    "text": "creating these kind of virtual tables you can create them create them manually in the console you can run hive",
    "start": "1829970",
    "end": "1836800"
  },
  {
    "text": "statements or you can call glue API as well if you already have this stuff into",
    "start": "1836800",
    "end": "1841970"
  },
  {
    "text": "a hive mate store you can even import that so different way is very flexible we are not in forcing you to do this a",
    "start": "1841970",
    "end": "1849590"
  },
  {
    "text": "specific approach in there so choose what you're more comfortable another pick parentheses on Amazon redshift so",
    "start": "1849590",
    "end": "1856790"
  },
  {
    "start": "1851000",
    "end": "1880000"
  },
  {
    "text": "it's our data warehousing service it is built to be fast it's built to support",
    "start": "1856790",
    "end": "1862460"
  },
  {
    "text": "open format is built to be secure and expensive but there is a cluster in there okay there are cases maybe you",
    "start": "1862460",
    "end": "1868220"
  },
  {
    "text": "already have a cluster maybe you are you're already using Russian but maybe you want to go to the that exhibit",
    "start": "1868220",
    "end": "1874880"
  },
  {
    "text": "exabyte scale you want to have a flexible structure approach like a data like so how can you go about it I",
    "start": "1874880",
    "end": "1883480"
  },
  {
    "start": "1880000",
    "end": "1916000"
  },
  {
    "text": "the way you go is you use ratchet spectrum so it's still redshift it's very similar to Athena which allows",
    "start": "1883670",
    "end": "1891170"
  },
  {
    "text": "you to declare virtual tables on top on it of s3 is the best of both worlds I would like",
    "start": "1891170",
    "end": "1896900"
  },
  {
    "text": "to say so you can still your your your",
    "start": "1896900",
    "end": "1902450"
  },
  {
    "text": "red ship Buster in there but you want to augment it with data coming from s3 awesome and spectrum if we just do that",
    "start": "1902450",
    "end": "1908929"
  },
  {
    "text": "for you very similar approach all the same benefits of Athena it's just a different engine under the hood",
    "start": "1908929",
    "end": "1914200"
  },
  {
    "text": "different name that's what it looks like for you as a consumer of the data you",
    "start": "1914200",
    "end": "1921380"
  },
  {
    "text": "just push the query into the leader node up there red ship is just one endpoint for you what happens there is that you",
    "start": "1921380",
    "end": "1928970"
  },
  {
    "text": "have the Amazon redshift cluster the blue one but if the system if the leader nodes understands that the data you're",
    "start": "1928970",
    "end": "1935330"
  },
  {
    "text": "querying is honest tree and that the computation can be parallelized it will",
    "start": "1935330",
    "end": "1940730"
  },
  {
    "text": "immediately pass the computation down to the right ship spectrum layer that will scale automatically for you you don't",
    "start": "1940730",
    "end": "1946760"
  },
  {
    "text": "even see those machines at all and then all the aggregation some average all the",
    "start": "1946760",
    "end": "1952340"
  },
  {
    "text": "stuff that can be done in parallel on subsets of the data and then aggregated up will be done completely in parallel",
    "start": "1952340",
    "end": "1958280"
  },
  {
    "text": "and the results will be given to the right ship cluster that will return you the winner results so very scalable",
    "start": "1958280",
    "end": "1964970"
  },
  {
    "text": "approach and the best of both worlds because you can even join data that's",
    "start": "1964970",
    "end": "1970790"
  },
  {
    "text": "red ship with data that's on a string so very powerful this is more or less what",
    "start": "1970790",
    "end": "1976700"
  },
  {
    "text": "it looks like if you want to build data like on s3 again data is there glue helps you with the data discovery data",
    "start": "1976700",
    "end": "1984320"
  },
  {
    "text": "catalog duties crawlers will help you build that data catalog and then you",
    "start": "1984320",
    "end": "1989750"
  },
  {
    "text": "want to have Athena or Amazon Elastic MapReduce or maybe right shipped to build the actual query to extract the",
    "start": "1989750",
    "end": "1996590"
  },
  {
    "text": "information to aggregate data and then we'll push the results into an data exploration tool such as quick side and",
    "start": "1996590",
    "end": "2002830"
  },
  {
    "text": "that's well as what I want to show you right now so",
    "start": "2002830",
    "end": "2007830"
  },
  {
    "text": "let's see if we can mirror our display him real quick",
    "start": "2008500",
    "end": "2017980"
  },
  {
    "text": "how do you do that sorry guys Mira cool ok so this is the AWS console",
    "start": "2020430",
    "end": "2030820"
  },
  {
    "text": "can you see it's very small of course so what I've done is I have defined I",
    "start": "2030820",
    "end": "2038500"
  },
  {
    "text": "wanted to show you what really changes when you have data that is unchanged and",
    "start": "2038500",
    "end": "2045030"
  },
  {
    "text": "data that maybe you have compressed that data with Amazon Kinichi Spyros and",
    "start": "2045030",
    "end": "2051669"
  },
  {
    "text": "maybe you have also converted that data into parque Foreman so okay per case better compression is",
    "start": "2051669",
    "end": "2057970"
  },
  {
    "text": "better because it's fewer data but what really changes so I wanted to show you some real data some metrics in these",
    "start": "2057970",
    "end": "2066520"
  },
  {
    "text": "three different scenarios now so I build an application that gives me an API so",
    "start": "2066520",
    "end": "2072010"
  },
  {
    "text": "with a simple HTTP API I can put data like in JSON format so I've been pushing",
    "start": "2072010",
    "end": "2079629"
  },
  {
    "text": "data in this kind of format here like name and action and value so we have the",
    "start": "2079630",
    "end": "2085270"
  },
  {
    "text": "specific person did an action which can be charged or refill imagine it's some sort of balance of money or credits or",
    "start": "2085270",
    "end": "2093220"
  },
  {
    "text": "whatever it is and a random value so I've been doing this that for different people different names and different",
    "start": "2093220",
    "end": "2099100"
  },
  {
    "text": "actions and I've been doing it three times see three different HTTP invocations here so once to an URL which",
    "start": "2099100",
    "end": "2108220"
  },
  {
    "text": "is the uncompressed URL uncompressed once that's your compressed and was that",
    "start": "2108220",
    "end": "2114190"
  },
  {
    "text": "your partition but it's actually a perky format so there are three completely",
    "start": "2114190",
    "end": "2120940"
  },
  {
    "text": "different stacks three completely different data pipelines I'm just doing it three times because I want to show",
    "start": "2120940",
    "end": "2126370"
  },
  {
    "text": "you the difference when you then go and query that data so what then I did this is Amazon Athena",
    "start": "2126370",
    "end": "2133000"
  },
  {
    "start": "2131000",
    "end": "2194000"
  },
  {
    "text": "very simple UI you can just put a query in there run it you get results",
    "start": "2133000",
    "end": "2138640"
  },
  {
    "text": "immediately so here I have the three tables compressed uncompressed and perky format so I didn't write a single line",
    "start": "2138640",
    "end": "2145600"
  },
  {
    "text": "of code to get from API to datastore on s3 in three different data formats so now",
    "start": "2145600",
    "end": "2153530"
  },
  {
    "text": "let's write some queries I can show you that if I preview table so there is a",
    "start": "2153530",
    "end": "2161000"
  },
  {
    "text": "limit ten in there let's remove the limit ten so the data overall it should",
    "start": "2161000",
    "end": "2168110"
  },
  {
    "text": "be around it's not crazy data it may be a few let's actually count it must be",
    "start": "2168110",
    "end": "2175700"
  },
  {
    "text": "around a few hundred records few thousand it's the twenty three thousand",
    "start": "2175700",
    "end": "2182090"
  },
  {
    "text": "records down here nothing crazy it's a bit small sorry guys nothing crazy but the data looks like",
    "start": "2182090",
    "end": "2190610"
  },
  {
    "text": "that and it's about 1.15 megabytes of",
    "start": "2190610",
    "end": "2197090"
  },
  {
    "start": "2194000",
    "end": "2214000"
  },
  {
    "text": "data it's not crazy scale okay but the tool is the same if you had 10 megabytes",
    "start": "2197090",
    "end": "2202490"
  },
  {
    "text": "10 gigabytes it wouldn't take that much longer because it would be done in parallel right so I want to show you",
    "start": "2202490",
    "end": "2207680"
  },
  {
    "text": "with a small data set today if I remove on so I'm clearing now they",
    "start": "2207680",
    "end": "2213710"
  },
  {
    "text": "compress data it's about 70 kilobytes so we've given this JSON format that I'm",
    "start": "2213710",
    "end": "2220610"
  },
  {
    "start": "2214000",
    "end": "2264000"
  },
  {
    "text": "pushing just by compressing it it becomes like an order of magnitude smaller 70 kilobytes down from one",
    "start": "2220610",
    "end": "2227270"
  },
  {
    "text": "megabyte if I then query the park a format I think it would be a bit larger",
    "start": "2227270",
    "end": "2234590"
  },
  {
    "text": "it's a hundred kilobytes why do you think it's a hundred kilobytes it was 70 kilobytes in the compressed no guys well",
    "start": "2234590",
    "end": "2244100"
  },
  {
    "text": "because parquet is adding some metadata okay I mentioned that a bit at the beginning so the counts and some you",
    "start": "2244100",
    "end": "2250220"
  },
  {
    "text": "know between the different columns that are stored in the columnar format we have some metadata so it is more data in",
    "start": "2250220",
    "end": "2256100"
  },
  {
    "text": "theory but let's do some real query I have some query stored here so the count",
    "start": "2256100",
    "end": "2261890"
  },
  {
    "text": "one very simple example so I want to count the uncompressed one of course it",
    "start": "2261890",
    "end": "2267410"
  },
  {
    "start": "2264000",
    "end": "2280000"
  },
  {
    "text": "will be the same number but I want to show you some metrics about performance and how much data has been scanned so",
    "start": "2267410",
    "end": "2273080"
  },
  {
    "text": "here I'm scanning the whole data set one megabyte 15 if I query the compressed",
    "start": "2273080",
    "end": "2278840"
  },
  {
    "text": "one I'm still scanning the data set because it's canning 70",
    "start": "2278840",
    "end": "2284210"
  },
  {
    "start": "2280000",
    "end": "2300000"
  },
  {
    "text": "kilobytes of data here so the query will be cheaper but maybe not really not that",
    "start": "2284210",
    "end": "2291650"
  },
  {
    "text": "much I'm saving some time but not much if I create a parquet format we go down",
    "start": "2291650",
    "end": "2300859"
  },
  {
    "start": "2300000",
    "end": "2329000"
  },
  {
    "text": "to zero kilobytes can because Athena doesn't need to go and read anything",
    "start": "2300859",
    "end": "2306710"
  },
  {
    "text": "okay the total count is in the park' metadata okay so we went from sun scanning the whole thing down to",
    "start": "2306710",
    "end": "2313220"
  },
  {
    "text": "scanning nothing okay this will will be great and immediate right so that that's",
    "start": "2313220",
    "end": "2319880"
  },
  {
    "text": "already some difference there let me do some other theory maybe add instinct",
    "start": "2319880",
    "end": "2327049"
  },
  {
    "text": "okay let's start easy I'm assuming you all know SQL if you don't know a I'm sorry let's talk out on stage I can",
    "start": "2327049",
    "end": "2334430"
  },
  {
    "start": "2329000",
    "end": "2403000"
  },
  {
    "text": "share more with you so if you go and distinct name for example okay the result will be simple I have a few names",
    "start": "2334430",
    "end": "2340400"
  },
  {
    "text": "John Rachel Jessica George but in the uncompressed version here I'm still scanning the whole data set it's fast",
    "start": "2340400",
    "end": "2347749"
  },
  {
    "text": "but it's expensive okay it's time consuming and money consuming if I scan",
    "start": "2347749",
    "end": "2353119"
  },
  {
    "text": "the compress one it will be again scanning the whole thing 70 kilobytes we can do better",
    "start": "2353119",
    "end": "2358720"
  },
  {
    "text": "guess what if we scan the parquet one",
    "start": "2358720",
    "end": "2363670"
  },
  {
    "text": "remember parquet was a hundred kilobytes total and now we are scanning 13",
    "start": "2364779",
    "end": "2370460"
  },
  {
    "text": "kilobytes it means we are actually scanning only the name table we forget about all sorry the name column we are",
    "start": "2370460",
    "end": "2378109"
  },
  {
    "text": "forgetting about the other two columns so instead of so we are saving a lot of",
    "start": "2378109",
    "end": "2383809"
  },
  {
    "text": "time and a lot of money here if we have instead of one megabyte one gigabyte okay that will be that will start to be",
    "start": "2383809",
    "end": "2391190"
  },
  {
    "text": "kind of an interesting size different interesting sale you will actually save a lot let's go into some more",
    "start": "2391190",
    "end": "2396739"
  },
  {
    "text": "interesting query like averages and no aggregations let's do that so",
    "start": "2396739",
    "end": "2404070"
  },
  {
    "start": "2403000",
    "end": "2442000"
  },
  {
    "text": "this is very simple query select name and average for only their appeals so I",
    "start": "2404070",
    "end": "2409740"
  },
  {
    "text": "also have a wear filter and then I'm grouping by name so my average will be performed on the value field filtering",
    "start": "2409740",
    "end": "2416670"
  },
  {
    "text": "by auction showing the name so I'm kind of using all the columns here okay nice",
    "start": "2416670",
    "end": "2423420"
  },
  {
    "text": "numbers I'm still scanning the whole data set let's change table I'm still scanning",
    "start": "2423420",
    "end": "2429390"
  },
  {
    "text": "the whole data set here 70 kilobytes let's go all the way to the perk a table",
    "start": "2429390",
    "end": "2436920"
  },
  {
    "text": "and here I'm scanning 20 so why because",
    "start": "2436920",
    "end": "2443190"
  },
  {
    "text": "I'm scanning the name I'm computing the average and I'm filtering the action so",
    "start": "2443190",
    "end": "2448800"
  },
  {
    "text": "I believe that per K is doing some also some some sort of sorting within the",
    "start": "2448800",
    "end": "2453990"
  },
  {
    "text": "column or some sort of magic there so that we are not counting only one column but we're not even scanning the whole",
    "start": "2453990",
    "end": "2459570"
  },
  {
    "text": "data set even though we are reading from all the fields so interesting let's do",
    "start": "2459570",
    "end": "2465090"
  },
  {
    "text": "something more interesting more more you know hard to read so can I make it",
    "start": "2465090",
    "end": "2472110"
  },
  {
    "text": "bigger no maybe yes so how many of you have seen queries like that where you have a",
    "start": "2472110",
    "end": "2478800"
  },
  {
    "start": "2475000",
    "end": "2556000"
  },
  {
    "text": "select and maybe another nested select so when you do these kind of queries you're not scanning the database the",
    "start": "2478800",
    "end": "2486360"
  },
  {
    "text": "table only ones you're scanning it twice three times as many times as needed",
    "start": "2486360",
    "end": "2491370"
  },
  {
    "text": "because you need to run all those different queries and then it goes up to the main query and you get the results",
    "start": "2491370",
    "end": "2496800"
  },
  {
    "text": "so what we expect is that even though we have only one megabyte of data in the",
    "start": "2496800",
    "end": "2502320"
  },
  {
    "text": "uncompressed format we are probably going to scan three megabytes and a half",
    "start": "2502320",
    "end": "2507570"
  },
  {
    "text": "because we are scanning the data set three times so we are what is it doing we are counting values for the charges",
    "start": "2507570",
    "end": "2514890"
  },
  {
    "text": "we are counting values for the fields for each person and then grouping by",
    "start": "2514890",
    "end": "2520770"
  },
  {
    "text": "name okay cool so let's see how much we do better how better we can do with the",
    "start": "2520770",
    "end": "2526620"
  },
  {
    "text": "compressed format it's a bit better two megabytes still",
    "start": "2526620",
    "end": "2531760"
  },
  {
    "text": "a lot let's see what we can do with the park' format not much so we're not",
    "start": "2531760",
    "end": "2541150"
  },
  {
    "text": "really doing is it working no it's not working please Ron right that's a good point",
    "start": "2541150",
    "end": "2550630"
  },
  {
    "text": "thank you let's do that so same holds for compressed so we're down 59 kilobytes",
    "start": "2550630",
    "end": "2558490"
  },
  {
    "text": "from a hundred kilobytes so we are just counting less data than the whole data set is that three times as much so the",
    "start": "2558490",
    "end": "2564370"
  },
  {
    "text": "decoder is helping a lot but we're not using partitions so I want to show you",
    "start": "2564370",
    "end": "2569800"
  },
  {
    "text": "another thing that you can do with that ena which is creating partitions with",
    "start": "2569800",
    "end": "2576190"
  },
  {
    "text": "your SQL queries as well so now I RA I have three tables but they're not partition I have no partition in there I",
    "start": "2576190",
    "end": "2583690"
  },
  {
    "start": "2578000",
    "end": "2624000"
  },
  {
    "text": "can now create a table and tell Athena hey go and fetch the data from this",
    "start": "2583690",
    "end": "2590050"
  },
  {
    "text": "other table so I'm selecting value name and action from the perc a table so",
    "start": "2590050",
    "end": "2595420"
  },
  {
    "text": "everything that's already there and I'm creating a new table that we can call partitioned with park' format Parkay",
    "start": "2595420",
    "end": "2603130"
  },
  {
    "text": "compression and the location will be different so it's actually moving the data to another s3 location let's see",
    "start": "2603130",
    "end": "2610210"
  },
  {
    "text": "what happens it should take like a minute like a few seconds so I can do",
    "start": "2610210",
    "end": "2616390"
  },
  {
    "text": "ETL with sequel code as well that's I think we announced this around a month",
    "start": "2616390",
    "end": "2621910"
  },
  {
    "text": "ago so I'm still playing with it okay successful so now we have a new query sorry a new table here shouldn't be that",
    "start": "2621910",
    "end": "2629860"
  },
  {
    "text": "different but I want to show you for example this same query with partitions",
    "start": "2629860",
    "end": "2636250"
  },
  {
    "text": "oh I didn't show you what partitions are created I I asked Athena to create two partition name and action so what",
    "start": "2636250",
    "end": "2644650"
  },
  {
    "text": "happened there is they're on s3 I should now have a new bucket here manually",
    "start": "2644650",
    "end": "2653620"
  },
  {
    "text": "partition so here I have a folder and here I have a data structure that is based on the",
    "start": "2653620",
    "end": "2660270"
  },
  {
    "start": "2655000",
    "end": "2682000"
  },
  {
    "text": "partition that I requested so I have a a level of the folders into us within s3",
    "start": "2660270",
    "end": "2667080"
  },
  {
    "text": "that is named equal value and within that folder in each folder I have an",
    "start": "2667080",
    "end": "2672120"
  },
  {
    "text": "action equal value charge or a field and inside that I have the actual objects",
    "start": "2672120",
    "end": "2678390"
  },
  {
    "text": "and they're already compressed already in Parque format so let's see what changed what changes if we then run this",
    "start": "2678390",
    "end": "2685140"
  },
  {
    "start": "2682000",
    "end": "2700000"
  },
  {
    "text": "query by using the partitions on the",
    "start": "2685140",
    "end": "2690420"
  },
  {
    "text": "partition table what will you expect better hopefully",
    "start": "2690420",
    "end": "2696950"
  },
  {
    "text": "zero so we went from scanning the whole data set three times down to scanning a",
    "start": "2696950",
    "end": "2704820"
  },
  {
    "start": "2700000",
    "end": "2802000"
  },
  {
    "text": "little bit better with parquet down - scanning zero because all the queries that I'm doing are coming from either",
    "start": "2704820",
    "end": "2712740"
  },
  {
    "text": "the Parque metadata or the partitioned metadata so I'm optimized in these",
    "start": "2712740",
    "end": "2718500"
  },
  {
    "text": "queries so that it can 0 kilobytes of data crazy so let me give you some stats",
    "start": "2718500",
    "end": "2724020"
  },
  {
    "text": "let's go back to the presentation look",
    "start": "2724020",
    "end": "2728450"
  },
  {
    "text": "just go away please sorry yes",
    "start": "2731039",
    "end": "2738800"
  },
  {
    "text": "so okay so we went from like first query",
    "start": "2738800",
    "end": "2745550"
  },
  {
    "text": "we went from compression gave us like 94% improvement Parkay gave us 70 percent improvement",
    "start": "2745550",
    "end": "2752380"
  },
  {
    "text": "partitions gave us more or less similar to parquet because that specific query doesn't earn much from from from",
    "start": "2752380",
    "end": "2759740"
  },
  {
    "text": "partitions but overall we went from that much down 95.5 percent ninety nine to",
    "start": "2759740",
    "end": "2766280"
  },
  {
    "text": "five percent other queries that we have seen the count theory went down again",
    "start": "2766280",
    "end": "2771890"
  },
  {
    "text": "ninety four hundred percent with the prague a one hundred percent with partitions so we make these queries free",
    "start": "2771890",
    "end": "2777910"
  },
  {
    "text": "the aalverson improvement cool these complex nested queries we went down",
    "start": "2777910",
    "end": "2784250"
  },
  {
    "text": "ninety-four with the compression 72 with a parquet down to 0 with partition so",
    "start": "2784250",
    "end": "2790130"
  },
  {
    "text": "that's the kind of optimization that you can run on a data like in the cloud without managing servers and just by",
    "start": "2790130",
    "end": "2797870"
  },
  {
    "text": "clicking a few check boxes in your in your managed services mmm I'm a bit out of time I've got to take",
    "start": "2797870",
    "end": "2804830"
  },
  {
    "start": "2802000",
    "end": "2823000"
  },
  {
    "text": "questions offstage and leave time for the next speaker but after you do this you can go and import the result of your",
    "start": "2804830",
    "end": "2813440"
  },
  {
    "text": "of your queries into data exploration tools such as Amazon quick site so there",
    "start": "2813440",
    "end": "2818900"
  },
  {
    "text": "you can go and click on the value field you change the new looks like the some average whatever a kind of aggregation",
    "start": "2818900",
    "end": "2825860"
  },
  {
    "start": "2823000",
    "end": "2845000"
  },
  {
    "text": "you want to do you like the chart or the analysis that you've done you click a button and you're building dashboards",
    "start": "2825860",
    "end": "2832160"
  },
  {
    "text": "based on the screenshots there are actually dynamic and that without update automatically with new",
    "start": "2832160",
    "end": "2837740"
  },
  {
    "text": "data but you kind of screenshot the configuration of your analysis so very powerful check it out we are a bit out",
    "start": "2837740",
    "end": "2844100"
  },
  {
    "text": "of time another few resources for you if you want to generate data automatically",
    "start": "2844100",
    "end": "2850340"
  },
  {
    "start": "2845000",
    "end": "2921000"
  },
  {
    "text": "for your system just testing data or to build a large data set without getting crazy and waiting too much too long you",
    "start": "2850340",
    "end": "2857840"
  },
  {
    "text": "can use the Kinesis data generator library very very fancy so you install it in your account you deploy it into",
    "start": "2857840",
    "end": "2864470"
  },
  {
    "text": "your account configure it what kind of data you want to generate and it will star general data so you can do real-time data",
    "start": "2864470",
    "end": "2870140"
  },
  {
    "text": "analysis and test and play with these tools with a kind of simulating a real time a real production environment all",
    "start": "2870140",
    "end": "2878810"
  },
  {
    "text": "the code that I've used to build this isn't Gaeta so you find me get up calm alex castle",
    "start": "2878810",
    "end": "2884510"
  },
  {
    "text": "bonnie and there there are a few starred repos and surplus data pipeline if you",
    "start": "2884510",
    "end": "2890300"
  },
  {
    "text": "want to read more we have just scratched the surface go and read the big data blog on AWS thank you for your time",
    "start": "2890300",
    "end": "2897410"
  },
  {
    "text": "thank you if we did not scan your badge",
    "start": "2897410",
    "end": "2902600"
  },
  {
    "text": "and you are out of the theater no don't forget to let my colleagues can you because do you get $200 credit and all",
    "start": "2902600",
    "end": "2909470"
  },
  {
    "text": "the presentations and slides from today's session thank you",
    "start": "2909470",
    "end": "2915340"
  }
]