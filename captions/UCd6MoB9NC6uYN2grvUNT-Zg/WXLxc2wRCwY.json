[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "okay good afternoon everybody my name is mark siegel i'm a lean engineer at amazon glacier and today i'm going to be",
    "start": "3439",
    "end": "9599"
  },
  {
    "text": "talking to you about archiving in the cloud now the structure of this talk is going",
    "start": "9599",
    "end": "15759"
  },
  {
    "text": "to be divided into different sections i'm going to spend about 10 to 15 minutes kind of going over the core concepts of the service establishing a",
    "start": "15759",
    "end": "22720"
  },
  {
    "text": "common background but then a significant chunk of this talk is going to go into some nitty-gritty technical details what",
    "start": "22720",
    "end": "28240"
  },
  {
    "text": "i want to talk to you about today is how to build an archival application using grit using glacier in order to ensure",
    "start": "28240",
    "end": "33440"
  },
  {
    "text": "your lowest cost ensuring you're preserving security durability and ease of use",
    "start": "33440",
    "end": "40399"
  },
  {
    "text": "so what do we mean when we're talking about the space of archival data or cold data well most of the time we're developing software thinking about our",
    "start": "43120",
    "end": "49360"
  },
  {
    "start": "45000",
    "end": "45000"
  },
  {
    "text": "applications we're thinking about warm data this is data that's currently in use this is something your users are",
    "start": "49360",
    "end": "54480"
  },
  {
    "text": "currently seeing but there's a whole class of data out there that's very important to you but that doesn't get",
    "start": "54480",
    "end": "59760"
  },
  {
    "text": "accessed very often so i've drawn some examples on the board here you can think of your application logs snapshots of your database research",
    "start": "59760",
    "end": "67600"
  },
  {
    "text": "data healthcare records anything you need to preserve or maintain for financial reasons or for regulatory",
    "start": "67600",
    "end": "73040"
  },
  {
    "text": "reasons now the question is yeah what are you going to do with this data there's a good chance that all of",
    "start": "73040",
    "end": "80000"
  },
  {
    "text": "you or most of you are overpaying to manage and store this data so knowing that",
    "start": "80000",
    "end": "85920"
  },
  {
    "text": "amazon created glacier as a low-cost storage alternative for your cold data now i want to talk about your data in",
    "start": "85920",
    "end": "92079"
  },
  {
    "text": "terms of this iceberg analogy kind of reiterating what i said before you can think of the top part of the",
    "start": "92079",
    "end": "97439"
  },
  {
    "text": "iceberg the visible part that top 10 percent as being your warm data this is the stuff you see often that's the stuff",
    "start": "97439",
    "end": "103360"
  },
  {
    "text": "you deal with but the vast majority of data kind of represented by what's below the ocean is your cold data now it's the overwhelming",
    "start": "103360",
    "end": "110000"
  },
  {
    "text": "majority of that and you're not going to access it very often and what's more there's some projections that say in",
    "start": "110000",
    "end": "116079"
  },
  {
    "text": "about 10 years you're gonna have 50 times more data than you'll have today now what are you going to do with this",
    "start": "116079",
    "end": "121119"
  },
  {
    "text": "data how are you going to store it how are you going to manage it well you want an archival application you want a cold storage application but",
    "start": "121119",
    "end": "127040"
  },
  {
    "text": "let's talk about what you really need out of that kind of application so first and foremost you need something",
    "start": "127040",
    "end": "133520"
  },
  {
    "text": "that's secure data that's stored in archival application could be considered the",
    "start": "133520",
    "end": "138640"
  },
  {
    "text": "crown jewels of your business and having outside access to that or unauthorized access that data can be absolutely",
    "start": "138640",
    "end": "143760"
  },
  {
    "text": "catastrophic so knowing this we designed amazon glacier to be secure from the ground up",
    "start": "143760",
    "end": "150640"
  },
  {
    "text": "first we encrypt all your data at rest by default second data in glacier is immutable",
    "start": "150640",
    "end": "157120"
  },
  {
    "text": "once data is stored in glacier it's impossible to overwrite it with random bits or to somehow corrupt that data",
    "start": "157120",
    "end": "163760"
  },
  {
    "text": "another aspect that you have to worry about with security is let's say you're storing a bunch of stuff on some hard drives or some tapes in your facility",
    "start": "163920",
    "end": "170640"
  },
  {
    "text": "how do you prevent somebody from walking in and walking away with one of those tapes or drives these are all important considerations",
    "start": "170640",
    "end": "176720"
  },
  {
    "text": "another important thing that glacier enforces is integrity so any set of bytes you choose to send us or any set of bytes you choose to",
    "start": "176720",
    "end": "183040"
  },
  {
    "text": "download from us have strong checksum information that's enforced at upload time this prevents tampering or malicious use",
    "start": "183040",
    "end": "189599"
  },
  {
    "text": "or any kind of corruption that occurs in this data in transit so another big concern is durability",
    "start": "189599",
    "end": "197040"
  },
  {
    "text": "you know this is archive data this is archival application data this may be the only copy of the data in the world",
    "start": "197040",
    "end": "202560"
  },
  {
    "text": "and you certainly don't want to lose it so you know we thought back and you know",
    "start": "202560",
    "end": "207599"
  },
  {
    "text": "we kind of learned from our experience in s3 we designed glacier to be as durable as possible",
    "start": "207599",
    "end": "213120"
  },
  {
    "text": "and what this amounts to is we provide the same 11 9's of durability that s3 has",
    "start": "213120",
    "end": "218959"
  },
  {
    "text": "now we've talked about the non-negotiables we talked about security we talked about durability those are two things you absolutely need an archival",
    "start": "222879",
    "end": "229200"
  },
  {
    "text": "application but one thing that's really important is for it to be cost effective",
    "start": "229200",
    "end": "234319"
  },
  {
    "text": "so we we stepped back and we said okay what kind of application we have to build here what are the core requirements you know we didn't set out",
    "start": "234319",
    "end": "240640"
  },
  {
    "text": "to build another s3 that was cheaper s3 is storage for the internet what it allows you to do is you know embed",
    "start": "240640",
    "end": "246239"
  },
  {
    "text": "resources in web pages or provide public access to objects with millisecond level access times and support you know",
    "start": "246239",
    "end": "252159"
  },
  {
    "text": "thousands or tens of thousands of tps per object that's not really a core requirement of a of a cold storage or an",
    "start": "252159",
    "end": "257840"
  },
  {
    "text": "archival application so we stepped back and we said you know what's the minimal or what's the core",
    "start": "257840",
    "end": "263199"
  },
  {
    "text": "set of features we need to build to make glaciers cost effective as possible and as a result the base cost of storage",
    "start": "263199",
    "end": "269440"
  },
  {
    "text": "the cost to store your bites in glacier can be as low as one cent per gigabyte per month",
    "start": "269440",
    "end": "275600"
  },
  {
    "text": "okay so we've talked about kind of what you want out of an archival application let's talk about what it would look like if you had to build one",
    "start": "278800",
    "end": "284720"
  },
  {
    "text": "well fundamentally glacier is a primitive it takes bytes it stores them durably and securely and cost",
    "start": "284720",
    "end": "290400"
  },
  {
    "text": "effectively and it gives you back an identifier to those bytes those identifiers in our system are called archive ids",
    "start": "290400",
    "end": "296639"
  },
  {
    "text": "now any other kind of rich functionality you need to build in your archival application will have to be kind of in that white box over there so let's talk",
    "start": "296639",
    "end": "303360"
  },
  {
    "text": "about some things you might want to put in that box one of the first things you may need or",
    "start": "303360",
    "end": "309120"
  },
  {
    "text": "you definitely will need is a discovery or collection process now this process is responsible for identifying things",
    "start": "309120",
    "end": "315120"
  },
  {
    "text": "that you want to put in glacier so for example you may want to put your application logs in here your database",
    "start": "315120",
    "end": "320320"
  },
  {
    "text": "snapshots your media files etc this may be discovering new files to archive or it might be processing deltas and",
    "start": "320320",
    "end": "326800"
  },
  {
    "text": "figuring out what has changed now this component is also a great place to do any kind of data transformation",
    "start": "326800",
    "end": "332000"
  },
  {
    "text": "you may want to do types of data transformation you may be interested in are aggregation or compression and we'll",
    "start": "332000",
    "end": "337039"
  },
  {
    "text": "talk more about those a little later in the talk another component you really need to",
    "start": "337039",
    "end": "342320"
  },
  {
    "text": "think about when you're building an archival application is an index now i want to take a step back here and talk about s3 again",
    "start": "342320",
    "end": "348639"
  },
  {
    "text": "really if you look at s3 you can break it down and it's offering two different things it's offering basic storage with",
    "start": "348639",
    "end": "353840"
  },
  {
    "text": "very very fast access times but it's also giving you a very a basic index so what's the index like an s3 well the",
    "start": "353840",
    "end": "360400"
  },
  {
    "text": "key is a named object of your choosing along with your bucket name and the value is your bytes now that's a very",
    "start": "360400",
    "end": "366080"
  },
  {
    "text": "simple index and we've talked to people that are using s3 for archival applications and we said you know do you use this index",
    "start": "366080",
    "end": "372240"
  },
  {
    "text": "or how do you guys index your data and it turned out the vast majority of them were not using that directly they",
    "start": "372240",
    "end": "377759"
  },
  {
    "text": "had built their own index and metadata system to provide greater flexibility and functionality than what s3 gives them by",
    "start": "377759",
    "end": "383919"
  },
  {
    "text": "default imagine the kind of stuff you'd store in your index so you want to store a mapping let's say",
    "start": "383919",
    "end": "389360"
  },
  {
    "text": "from archive ids back to original files you may want to store information such as",
    "start": "389360",
    "end": "394560"
  },
  {
    "text": "when the file was created who owns the file what cost center it belongs to or for media applications you may be",
    "start": "394560",
    "end": "400319"
  },
  {
    "text": "storing information such as track length title regional encoding etc really the functionality of the index is",
    "start": "400319",
    "end": "405919"
  },
  {
    "text": "up to you so rather than incur the cost of building a one-size-fits-all index we kind of leave it up to you to decide",
    "start": "405919",
    "end": "411520"
  },
  {
    "text": "what you want in there and what you don't now your options for how you build this index are varied and i have a couple",
    "start": "411520",
    "end": "418080"
  },
  {
    "text": "suggestions up here so amazon dynamodb is a nice natural fit or you can use the various rds offerings",
    "start": "418080",
    "end": "424800"
  },
  {
    "text": "such as mysql oracle microsoft sql or you could do something even simpler using a berkeley database a",
    "start": "424800",
    "end": "431120"
  },
  {
    "text": "local database a file system etc it's really up to you",
    "start": "431120",
    "end": "436919"
  },
  {
    "text": "now i want to touch briefly on s3 glacier integration this was a feature that was launched recently that allows you to put your",
    "start": "436960",
    "end": "442800"
  },
  {
    "text": "objects in s3 into glacier and the way you do this is through life cycle policies you have to set a policy on a",
    "start": "442800",
    "end": "448560"
  },
  {
    "text": "bucket to say that files of greater than a certain age will then be placed into glacier and this is a good use case for",
    "start": "448560",
    "end": "454880"
  },
  {
    "text": "a lot of people it combines the the ease of use with s3 with the cost effective pricing of glacier but i'm",
    "start": "454880",
    "end": "461280"
  },
  {
    "text": "going to be talking more about how to talk to glacier directly in this talk how you'd interact with it",
    "start": "461280",
    "end": "467120"
  },
  {
    "text": "so i'm going to review some core concepts here so let's talk about archives for those",
    "start": "468080",
    "end": "473440"
  },
  {
    "text": "of you that are not familiar with it archives are the fundamental unit of storage in glacier now these can be anywhere between one byte to 40",
    "start": "473440",
    "end": "479759"
  },
  {
    "text": "terabytes in size and there doesn't have to be a one-to-one mapping between the files that you choose to store and the",
    "start": "479759",
    "end": "485120"
  },
  {
    "text": "archives in the system for example you may choose to aggregate a bunch of files together in a zip file before aggregating them before uploading them",
    "start": "485120",
    "end": "491919"
  },
  {
    "text": "it might be a disk image it's up to you there need not be a one-to-one mapping and i'll talk about why it's more cost",
    "start": "491919",
    "end": "497199"
  },
  {
    "text": "effective if you do aggregation later an example archive id by the way is a",
    "start": "497199",
    "end": "502400"
  },
  {
    "text": "bit truncated there but it's a base64 encoded id it's not super human readable",
    "start": "502400",
    "end": "508560"
  },
  {
    "text": "so what we did to improve the situation there was we allow you a time of upload to specify a human readable archive id",
    "start": "508560",
    "end": "514719"
  },
  {
    "text": "sorry a human readable archive description it essentially tags your data with your uh some text of your choosing",
    "start": "514719",
    "end": "523279"
  },
  {
    "text": "so another major concept in glacier is the concept of vaults now we were talking earlier about the",
    "start": "525279",
    "end": "530560"
  },
  {
    "text": "need to build an indexing system with vaults are it's a way to organize your archives in the system and you may",
    "start": "530560",
    "end": "536080"
  },
  {
    "text": "ask yourself why do i need to have vaults if i'm building an indexing system in addition to everything else",
    "start": "536080",
    "end": "542160"
  },
  {
    "text": "the answer is vaults provide a natural human grouping but they also provide a natural ability to specify security",
    "start": "542160",
    "end": "548240"
  },
  {
    "text": "policies so say for example you want to have a bunch of archive ids and you want them only to be accessible by a certain set",
    "start": "548240",
    "end": "554720"
  },
  {
    "text": "of users let's say you have your quarterly statements and you only want alice and bob to be able to access those archives they're not allowed to lead",
    "start": "554720",
    "end": "561120"
  },
  {
    "text": "them either using identity and access management console or api you can attach a policy to those users saying they're",
    "start": "561120",
    "end": "568240"
  },
  {
    "text": "only allowed to perform these actions so vaults become a natural kind of grouping mechanism",
    "start": "568240",
    "end": "573680"
  },
  {
    "text": "so let's at the bottom here i have the uh an arn for the vault and for those of you that aren't super familiar with",
    "start": "573680",
    "end": "579360"
  },
  {
    "text": "arn's it's a global unique identifier that specifies where this resource is",
    "start": "579360",
    "end": "584640"
  },
  {
    "text": "and kind of describes it so you can see that this aeron talks about an object in glacier",
    "start": "584640",
    "end": "590480"
  },
  {
    "text": "in the us east one region for an account id zero one two three four five example account id and it's",
    "start": "590480",
    "end": "596160"
  },
  {
    "text": "talking about a vault name example vault we'll put this together in a second",
    "start": "596160",
    "end": "601519"
  },
  {
    "start": "600000",
    "end": "600000"
  },
  {
    "text": "so we've reviewed some concepts let's talk about actually how to get data into glacier let's take an example use case where you",
    "start": "601519",
    "end": "607839"
  },
  {
    "text": "have a bunch of films and you have films of different encodings from different regions for different formats of screens and you",
    "start": "607839",
    "end": "613760"
  },
  {
    "text": "want to archive them all because you don't need to use them all at one time so the first thing you need to do is create a film a vault we're called films",
    "start": "613760",
    "end": "622640"
  },
  {
    "text": "now this steps optional but i want to talk about why you want to do this we're going to be creating an application to archive our files",
    "start": "622640",
    "end": "629279"
  },
  {
    "text": "and i'm recommending associating a uh an iron policy with this user basically the",
    "start": "629279",
    "end": "636160"
  },
  {
    "text": "user is who your archive application is acting as now this ion policy here says that",
    "start": "636160",
    "end": "641600"
  },
  {
    "text": "this archive application user is only allowed to upload vaults or upload archives to the vault specified now why",
    "start": "641600",
    "end": "648480"
  },
  {
    "text": "would you want to do this well first of all all your data at glacier is not publicly accessible by default",
    "start": "648480",
    "end": "653839"
  },
  {
    "text": "nobody is going to come in and see your data if you don't do this however by locking down our application and saying it can only upload archives we prevent",
    "start": "653839",
    "end": "660240"
  },
  {
    "text": "accidental code bugs or other issues from deleting data or doing anything else malicious it also makes managing",
    "start": "660240",
    "end": "666320"
  },
  {
    "text": "this archive application by revoking its access much easier just a general security best practice",
    "start": "666320",
    "end": "672800"
  },
  {
    "text": "okay so after we've created a vault and attached a policy to the user now we're ready to upload our archives",
    "start": "672800",
    "end": "678560"
  },
  {
    "text": "now using basic http or https rest requests you can upload a stream of bytes to glacier and whenever the http",
    "start": "678560",
    "end": "684480"
  },
  {
    "text": "response code comes back you'll have an archive id at this point the data is stored securely and durably",
    "start": "684480",
    "end": "690640"
  },
  {
    "text": "you want to save that archivity for future retrieval let's talk about getting data out of",
    "start": "690640",
    "end": "696240"
  },
  {
    "text": "glacier this is where we differ a lot from a lot of other services so we went we talked to our customers",
    "start": "696240",
    "end": "701680"
  },
  {
    "text": "and we said what would you be willing to give up for the lowest possible cost of storage and we got a lot of different responses",
    "start": "701680",
    "end": "707920"
  },
  {
    "text": "but one really consistent message was customers were willing to willing to sacrifice time to first access their",
    "start": "707920",
    "end": "713200"
  },
  {
    "start": "711000",
    "end": "711000"
  },
  {
    "text": "data so for example if you want something out of s3 you get it at immediately customers are willing to wait hours or",
    "start": "713200",
    "end": "718800"
  },
  {
    "text": "days so when glacier when you want your data out it's a two-step process",
    "start": "718800",
    "end": "724480"
  },
  {
    "text": "the first thing you do is you tell glacier hey i want this archive id back glacier returns a a job id",
    "start": "724480",
    "end": "731120"
  },
  {
    "text": "now these jobs take between three to five hours to complete and there's a couple ways to discover",
    "start": "731120",
    "end": "736800"
  },
  {
    "text": "when your job is finished the first way is to actually call the service directly and say get me the",
    "start": "736800",
    "end": "742639"
  },
  {
    "text": "status of this job and it returns a json blob that contains information about the job along with a status field it'll tell",
    "start": "742639",
    "end": "748079"
  },
  {
    "text": "you if it's complete or not another way is to use notifications",
    "start": "748079",
    "end": "753839"
  },
  {
    "text": "in our case we use amazon's simple notification service to publish a message to an end point of your choosing",
    "start": "753839",
    "end": "759120"
  },
  {
    "text": "to let you know when your job is finished now between these two options we recommend using notifications over",
    "start": "759120",
    "end": "765279"
  },
  {
    "text": "polling for a couple of different reasons one is polling is a little bit resource intensive on your side gonna be",
    "start": "765279",
    "end": "770880"
  },
  {
    "text": "consuming cpu cycles bandwidth you could be making calls to the service and it's often difficult to choose a correct",
    "start": "770880",
    "end": "776800"
  },
  {
    "text": "polling interval okay so let's say we have our job it's completed we know it's done now it's",
    "start": "776800",
    "end": "783279"
  },
  {
    "text": "time to actually get our bytes so we issue a download output or a get job output request a glacier we",
    "start": "783279",
    "end": "789680"
  },
  {
    "text": "download our bytes we have our archive back we're good to go",
    "start": "789680",
    "end": "794800"
  },
  {
    "text": "okay so for the rest of this talk we're going to get a lot more technical i'm going to be talking about",
    "start": "796480",
    "end": "802639"
  },
  {
    "text": "some very deep nitty gritty stuff on how to actually use glacier at a high scale with high with large objects etc if you",
    "start": "802639",
    "end": "808480"
  },
  {
    "text": "want more introductory information including discussion about pricing please look at the introduction to glacier webinar linked off the glacier",
    "start": "808480",
    "end": "814639"
  },
  {
    "text": "home page",
    "start": "814639",
    "end": "817199"
  },
  {
    "text": "sorry quick water break",
    "start": "821839",
    "end": "825560"
  },
  {
    "text": "all right so we're here to talk about cost and one of the best ways you can reduce costs in a system like glacier is",
    "start": "829839",
    "end": "836160"
  },
  {
    "text": "through aggregation so what do i mean by this when you build a storage system every object you store",
    "start": "836160",
    "end": "842079"
  },
  {
    "text": "occurs a certain amount of metadata to store it to store itself for example in glacier's case every object you store",
    "start": "842079",
    "end": "849360"
  },
  {
    "text": "incurs a 32 kilobyte overhead now what do we use in that well that stores information such as your archive",
    "start": "849360",
    "end": "854800"
  },
  {
    "text": "description extra information used for durability and for replication purposes",
    "start": "854800",
    "end": "860240"
  },
  {
    "text": "and if you're uploading archives and they're a mag 10 megs this overhead of 32 kilobytes is not really a factor in",
    "start": "860240",
    "end": "866959"
  },
  {
    "text": "your total cost but consider the case where you're uploading archives of size 1k",
    "start": "866959",
    "end": "872399"
  },
  {
    "text": "now your overhead for this metadata is 95 of your storage cost and we all have small objects we have to",
    "start": "872399",
    "end": "877760"
  },
  {
    "text": "deal with from time to time so one easy solution to this is aggregation an aggregation saves you",
    "start": "877760",
    "end": "883839"
  },
  {
    "text": "costs in two ways first it amortizes the cost of the metadata over many many small objects",
    "start": "883839",
    "end": "889680"
  },
  {
    "text": "and secondly when you aggregate your objects together before uploading them you'll end up being performing less requests against the service and you'll",
    "start": "889680",
    "end": "896079"
  },
  {
    "text": "end up paying for less requests against the service so we decided we wanted to do",
    "start": "896079",
    "end": "901360"
  },
  {
    "text": "aggregation now the question is what kind of format do we want to use there's a lot of options and the choices",
    "start": "901360",
    "end": "907360"
  },
  {
    "text": "are up to you but there's some pitfalls and considerations to be aware of we launched a feature not too long ago",
    "start": "907360",
    "end": "913040"
  },
  {
    "text": "that allows you to retrieve a range of an archive and you only have to pay for retrieving that range so let's say for example",
    "start": "913040",
    "end": "919360"
  },
  {
    "text": "you've aggregated lots of these 1k objects together and you only want a single archive out of that single file out of that big aggregate archive you",
    "start": "919360",
    "end": "925760"
  },
  {
    "text": "don't want to pay to pull this entire thing down you really want to just be able to plug that file out download it and only pay for that",
    "start": "925760",
    "end": "932079"
  },
  {
    "text": "you want an aggregation format that supports that that makes that easy for you so let's look at a common format like tar now tar essentially",
    "start": "932079",
    "end": "938959"
  },
  {
    "text": "concatenates files together and it's theoretically possible to pluck a file out of a tarball and only pay for that",
    "start": "938959",
    "end": "945519"
  },
  {
    "text": "however one of the common operations you do on top of tar files is compression and compression is going to change the",
    "start": "945519",
    "end": "951199"
  },
  {
    "text": "offsets in that tar file in unpredictable ways and what's more it's difficult to read into the middle of a compression stream",
    "start": "951199",
    "end": "958639"
  },
  {
    "text": "now taking that into account there are some archive formats out there that support compression aggregation and",
    "start": "958639",
    "end": "963759"
  },
  {
    "text": "random file access and two that i've outlined there are zip and 7zip",
    "start": "963759",
    "end": "968880"
  },
  {
    "text": "now these will support the ability for you to pluck an archive out of the middle as well as supporting data integrity",
    "start": "968880",
    "end": "974079"
  },
  {
    "text": "checks they also have an added benefit in that each one of these zip files or 7-zip files has a central directory either at the beginning of the end that",
    "start": "974079",
    "end": "980399"
  },
  {
    "text": "describes kind of what the contents of that file is let's go into a little bit more detail",
    "start": "980399",
    "end": "986240"
  },
  {
    "text": "before i touch on a given example i want to talk a little bit about integrity now when you're downloading a file from",
    "start": "987519",
    "end": "993440"
  },
  {
    "text": "glacier and you downloaded an entire archive we give you the checksum for that data and you can check the bytes that you",
    "start": "993440",
    "end": "999120"
  },
  {
    "text": "downloaded versus that checksum to make sure nothing's been corrupted one thing you're going to know is that when you're dealing with lots of data is",
    "start": "999120",
    "end": "1005199"
  },
  {
    "text": "that it's at some point it's out to get you there countless number of devices out there that could flip your bits randomly",
    "start": "1005199",
    "end": "1011519"
  },
  {
    "text": "and when you're transferring things over a network there's no guarantee that the bytes you ask for are the same bytes that you get",
    "start": "1011519",
    "end": "1016959"
  },
  {
    "text": "so knowing this we return checksums for all bulk data operations but now when you're plucking single files out of",
    "start": "1016959",
    "end": "1022399"
  },
  {
    "text": "archives how do you handle this well you have a couple of options",
    "start": "1022399",
    "end": "1028000"
  },
  {
    "start": "1027000",
    "end": "1027000"
  },
  {
    "text": "on your left i have an approximation of what a zip file looks like layout wise",
    "start": "1028559",
    "end": "1033678"
  },
  {
    "text": "now every file in the zip file is preceded by a header that contains information about the checksum the original file name and some other",
    "start": "1033679",
    "end": "1040000"
  },
  {
    "text": "information at the bottom of the file there's a central index or directory and this contains pointers to those files it",
    "start": "1040000",
    "end": "1045760"
  },
  {
    "text": "stores the offsets of where they're located on the other side of the screen i have what might be an approximation of your",
    "start": "1045760",
    "end": "1052080"
  },
  {
    "text": "local index now in this local index for every file i've stored in my aggregate i have an",
    "start": "1052080",
    "end": "1057360"
  },
  {
    "text": "offset in addition to that i have the checks in the metadata now",
    "start": "1057360",
    "end": "1062720"
  },
  {
    "text": "let's talk about how you'd retrieve plug file number two out of this aggregate archive using the uh the application",
    "start": "1062720",
    "end": "1068640"
  },
  {
    "text": "index so the first thing we do is we go to our index we look up file number two we'd find its offset and then we'd issue",
    "start": "1068640",
    "end": "1074799"
  },
  {
    "text": "a range request to glacier and this starts a job after three to five hours this job would finish",
    "start": "1074799",
    "end": "1080400"
  },
  {
    "text": "we would download our bytes we would verify our bytes against the checksum stored in our index and then",
    "start": "1080400",
    "end": "1085600"
  },
  {
    "text": "once the verification is complete we've successfully restored the file now let's not talk about it from an",
    "start": "1085600",
    "end": "1091840"
  },
  {
    "text": "example where you don't have this stored in your index how would you go about doing this well it's a little more complicated",
    "start": "1091840",
    "end": "1097280"
  },
  {
    "text": "you'd have to do two jobs the first job would be to retrieve the central index and directory at the end of the file",
    "start": "1097280",
    "end": "1102880"
  },
  {
    "text": "once you've downloaded that that's three to five hours later once you've downloaded that you can discover the offset for file number two and repeat",
    "start": "1102880",
    "end": "1108640"
  },
  {
    "text": "the steps i talked about previously now it seems really obvious that you",
    "start": "1108640",
    "end": "1113760"
  },
  {
    "text": "would want to use your application index for this so why the need to store all this extra information in this file to begin with",
    "start": "1113760",
    "end": "1118799"
  },
  {
    "text": "well think about it in cases where you don't have access to your index if your index is unavailable or it's become corrupt or it's somehow otherwise not",
    "start": "1118799",
    "end": "1125120"
  },
  {
    "text": "accessible to you your data here is still self-describing it's always possible to go back and look at your",
    "start": "1125120",
    "end": "1130640"
  },
  {
    "text": "archives and discover everything that's stored in that aggregate and for that reason we recommend doing both",
    "start": "1130640",
    "end": "1137520"
  },
  {
    "text": "let me walk through some sample code showing you how to actually do this with real zip files",
    "start": "1137520",
    "end": "1143559"
  },
  {
    "text": "okay a couple disclaimers about the code samples i'm going to show you guys this is in production code this code was",
    "start": "1175440",
    "end": "1181840"
  },
  {
    "text": "written in order to ease readability so a lot of the error checking that i would normally put in here and a lot of the logging has been removed just to make",
    "start": "1181840",
    "end": "1187679"
  },
  {
    "text": "the code more concise so this run example is the method that",
    "start": "1187679",
    "end": "1193679"
  },
  {
    "text": "we're entering to kind of uh do this entire operation let me tell you what we're going to do",
    "start": "1193679",
    "end": "1198720"
  },
  {
    "text": "we're going to take we're going to take an existing zip file",
    "start": "1198720",
    "end": "1204600"
  },
  {
    "text": "i can try",
    "start": "1206480",
    "end": "1209200"
  },
  {
    "text": "i can't easily make it bigger but this code samples will come out after the presentation",
    "start": "1212240",
    "end": "1218000"
  },
  {
    "text": "so here's what we're going to do we're going to take an existing zip file we're going to look at the essential directory in that zip file we're going",
    "start": "1219280",
    "end": "1224720"
  },
  {
    "text": "to look at all the files stored in there this is a common case if you have an aggregate we're going to build an index that we're going to store and we're",
    "start": "1224720",
    "end": "1230640"
  },
  {
    "text": "going to upload that zip file to glacier the next thing we're going to do is we're going to find a single file out of",
    "start": "1230640",
    "end": "1235760"
  },
  {
    "text": "that zip file that we want out and we're going to construct a range request to glacier to pull that file out download the bytes verify the checksum",
    "start": "1235760",
    "end": "1242799"
  },
  {
    "text": "information and then and successfully basically restore the file so let's walk through what this looks like",
    "start": "1242799",
    "end": "1249840"
  },
  {
    "text": "so first we call a setup method and this doesn't do anything particularly interesting it's just it's initializing credentials and setting the appropriate",
    "start": "1250159",
    "end": "1256240"
  },
  {
    "text": "endpoint now let's talk a little bit about the index we're going to build here i'm using an in-memory index now obviously",
    "start": "1256240",
    "end": "1262400"
  },
  {
    "text": "this is incredibly undurable but it's very readable so we're going to just talk about in context of this so what does an index entry look like in our",
    "start": "1262400",
    "end": "1268559"
  },
  {
    "text": "context well the kind of information you want to store in your index includes stuff like",
    "start": "1268559",
    "end": "1274400"
  },
  {
    "text": "the crc of the file which is the checksum that the zip files contain the offset of that file within the zip",
    "start": "1274400",
    "end": "1280320"
  },
  {
    "text": "file the compressed size now you need to store this because you need to know how many bytes you're actually going to retrieve",
    "start": "1280320",
    "end": "1286159"
  },
  {
    "text": "and the original size this tells you how much expansion you have to do before you're finished",
    "start": "1286159",
    "end": "1291840"
  },
  {
    "text": "so the first thing we do is we build this index using the zip file",
    "start": "1292240",
    "end": "1296880"
  },
  {
    "text": "and we're using a commons compress library now full disclosure a lot of these zip libraries do not expose enough",
    "start": "1301360",
    "end": "1306799"
  },
  {
    "text": "low-level offset information to really build an index so i had to tweak the code a little bit to change member visibility but ultimately the libraries",
    "start": "1306799",
    "end": "1313039"
  },
  {
    "text": "remain unchanged now i loop through everything in the zip file here and for every entry in the zip",
    "start": "1313039",
    "end": "1318880"
  },
  {
    "text": "file i capture its crc its offset its compressed size and its total size and then i store this in an index entry",
    "start": "1318880",
    "end": "1326480"
  },
  {
    "text": "i close the zip file and then i'm finished okay so now it's time to upload this",
    "start": "1326480",
    "end": "1332320"
  },
  {
    "text": "archive let's look at this now we're using the adbus sdk here you can see we upload an archive in",
    "start": "1332320",
    "end": "1338400"
  },
  {
    "text": "about three lines it's super simple we create this archive manager class and we say you know here's our credentials",
    "start": "1338400",
    "end": "1344240"
  },
  {
    "text": "and here's where you should go to and then we say upload this file and we get back an archive id the archive transfer",
    "start": "1344240",
    "end": "1350000"
  },
  {
    "text": "manager handles retries for you it handles chunking it makes it a lot it makes it very convenient to upload",
    "start": "1350000",
    "end": "1355120"
  },
  {
    "text": "files so we get the archive id back and then we do our next step",
    "start": "1355120",
    "end": "1361360"
  },
  {
    "text": "so we have an aggregate file up in glacier now this is the common case now you need to get something back out of it",
    "start": "1361360",
    "end": "1367200"
  },
  {
    "text": "the first thing we do is we look in our index for this particular file we want to retrieve and then we have to do something a",
    "start": "1367200",
    "end": "1373200"
  },
  {
    "text": "little bit tricky here so glacier when you do a range request in glacier we require that you align",
    "start": "1373200",
    "end": "1379520"
  },
  {
    "text": "your requests on megabyte boundaries now i'll talk more about why we do this and why it's important for checksumming",
    "start": "1379520",
    "end": "1384640"
  },
  {
    "text": "later but all this method does is basically adjust the range to make sure it's aligned on a megabyte",
    "start": "1384640",
    "end": "1390880"
  },
  {
    "text": "let's look at actually downloading the file again we're a two-step process here we",
    "start": "1392240",
    "end": "1398400"
  },
  {
    "text": "initiate a job by passing a certain number of parameters to it including the range of bytes we want to retrieve",
    "start": "1398400",
    "end": "1404080"
  },
  {
    "text": "we wait we get a job id and we wait three to five hours in this case i'm just using a sleep but in reality you want to use notifications",
    "start": "1404080",
    "end": "1411360"
  },
  {
    "text": "once the thing is completed we get the output and we download the bytes and copy them to a destination stream",
    "start": "1411360",
    "end": "1416960"
  },
  {
    "text": "and we finished now we've gotten our file but it contains some extra stuff at the beginning and possibly the end",
    "start": "1416960",
    "end": "1422480"
  },
  {
    "text": "so we need to trim it up decompress it and validate it and that's the last step here",
    "start": "1422480",
    "end": "1429320"
  },
  {
    "text": "so the first thing we do is skip past any kind of extra data we had a download in order to get past this megabyte boundary",
    "start": "1433520",
    "end": "1440400"
  },
  {
    "text": "the next thing we have to do is remember i have the zip file up on the board up on the presentation there's this extra header before our",
    "start": "1440400",
    "end": "1447039"
  },
  {
    "text": "data file begins so we just need to skip past that i'm not going to go through the details of that method",
    "start": "1447039",
    "end": "1452400"
  },
  {
    "text": "the next thing we do is we create an inflator input stream now this is just standard java sdk stuff and we start and",
    "start": "1452400",
    "end": "1457919"
  },
  {
    "text": "we create a new crc object now as we're reading the bytes out of this decompressed file we're going to be",
    "start": "1457919",
    "end": "1463279"
  },
  {
    "text": "computing the crc as we go and at the end we're going to compare that against the crc we stored in our index to make",
    "start": "1463279",
    "end": "1468640"
  },
  {
    "text": "sure we retrieved all the bytes correctly so in this loop all i do is read out of",
    "start": "1468640",
    "end": "1473679"
  },
  {
    "text": "the inflator stream i write them to the destination and update the crc and at the very end i",
    "start": "1473679",
    "end": "1479279"
  },
  {
    "text": "return the computed crc the last part of this is so once we've",
    "start": "1479279",
    "end": "1485840"
  },
  {
    "text": "got that crc back the last part of this is we just do a comparison if the crc we got back doesn't match the",
    "start": "1485840",
    "end": "1492080"
  },
  {
    "text": "crc we sold in our index then we've had some exceptional case it hasn't been transferred correctly but otherwise",
    "start": "1492080",
    "end": "1497200"
  },
  {
    "text": "you've successfully plucked an archive out and restored it",
    "start": "1497200",
    "end": "1501840"
  },
  {
    "text": "okay i want to talk about one more concept here and that's vault inventories think about durability again",
    "start": "1508480",
    "end": "1515520"
  },
  {
    "text": "glacier storing all of your data with eleven nines of durability but what about this index that you might be using",
    "start": "1515520",
    "end": "1521200"
  },
  {
    "text": "well if that's on a single drive on an instance somewhere in rds instance or on a local box somewhere you know",
    "start": "1521200",
    "end": "1527440"
  },
  {
    "text": "there's a good chance the durability there is much lower so what happens if you lose your index how can you discover",
    "start": "1527440",
    "end": "1532559"
  },
  {
    "text": "what you've stored in glacier what vault inventories do is they provide a basic mechanism to describe",
    "start": "1532559",
    "end": "1537679"
  },
  {
    "text": "what's contained in a vault now these inventories are not updated in real time they're updated about once a",
    "start": "1537679",
    "end": "1543520"
  },
  {
    "text": "day and your recent activity may not be reflected in here when you think about what you're going to use an inventory for there's really",
    "start": "1543520",
    "end": "1549840"
  },
  {
    "text": "two cases one is reconstructing your index in the case of a loss or two reconciling your index with what glacier",
    "start": "1549840",
    "end": "1556320"
  },
  {
    "text": "actually has let's walk through kind of what these inventories contain so this is a json format it's pretty",
    "start": "1556320",
    "end": "1562080"
  },
  {
    "text": "simple there's a header that has the vault arn again this is the globally unique identifier saying what this fault",
    "start": "1562080",
    "end": "1567679"
  },
  {
    "text": "is where it lives what account id belongs to and it contains an approximate inventory creation date this is when the creation",
    "start": "1567679",
    "end": "1574080"
  },
  {
    "text": "of the inventory happened by glacier's perspective and has a list of archives for each archive we have here we have",
    "start": "1574080",
    "end": "1580240"
  },
  {
    "text": "the id we have the description you use whenever you uploaded the archive that's the human readable part",
    "start": "1580240",
    "end": "1585520"
  },
  {
    "text": "the creation date from glacier's perspective the size and the shock 56 tree hash and the shot 56 tree hash is",
    "start": "1585520",
    "end": "1592080"
  },
  {
    "text": "our checksum okay so we've talked about indexing we",
    "start": "1592080",
    "end": "1598640"
  },
  {
    "start": "1596000",
    "end": "1596000"
  },
  {
    "text": "talked about aggregation let's talk about how you move massive amounts of data in and out of the service",
    "start": "1598640",
    "end": "1603679"
  },
  {
    "text": "now if you're just moving archives let's say have a few megs in and out of glacier then you're not going to encounter a lot of issues it'll just probably just work",
    "start": "1603679",
    "end": "1610080"
  },
  {
    "text": "but if you're trying to upload things of anything over 100 megabytes or if your connection to glacier is poor",
    "start": "1610080",
    "end": "1615200"
  },
  {
    "text": "then you're going to have some issues where things will fail there's a lot of hops between it's also",
    "start": "1615200",
    "end": "1620320"
  },
  {
    "text": "a function of the number of hops between you and glacier at any one of these points a connection can be dropped packet could be lost in a sense it",
    "start": "1620320",
    "end": "1626320"
  },
  {
    "text": "really boils down to internet weather the more data you're sending the longer your connection is going to be open the",
    "start": "1626320",
    "end": "1632240"
  },
  {
    "text": "longer the connection's going to be open the more likely something is to fall fall down and basically interrupt that connection so knowing this we created",
    "start": "1632240",
    "end": "1639039"
  },
  {
    "text": "two kind of techniques to get data in and out of glacier so let's talk first about uploading large amounts of data",
    "start": "1639039",
    "end": "1645840"
  },
  {
    "text": "now if you're familiar with s3 this shouldn't become a surprise to you we have a very similar kind of interface for uploading a lot of data we call it",
    "start": "1645840",
    "end": "1651600"
  },
  {
    "text": "multi-part upload our interface is a little bit different it's a little bit simpler and the way we do checksumming is a little bit different as well so",
    "start": "1651600",
    "end": "1656880"
  },
  {
    "text": "let's walk through that so the first thing you need to do when you want to upload a large chunk of data",
    "start": "1656880",
    "end": "1661919"
  },
  {
    "text": "to glacier is you say okay glacier i'm going to upload a multi-part upload to you and i'm going to use a part size of",
    "start": "1661919",
    "end": "1667120"
  },
  {
    "text": "x now these part sizes are usually multiples or have to be multiples of a megabyte",
    "start": "1667120",
    "end": "1672320"
  },
  {
    "text": "but more specifically they can be one megabyte two megabyte four megabyte eight sixteen thirty two sixty four and",
    "start": "1672320",
    "end": "1678320"
  },
  {
    "text": "it'll become clear why those part sizes matter in a second once you've created that multi-part upload you get an id",
    "start": "1678320",
    "end": "1684240"
  },
  {
    "text": "back which you then can use to refer to that upload the next thing you want to do is upload your parts now you can upload your parts",
    "start": "1684240",
    "end": "1690640"
  },
  {
    "text": "in any order and if you have the cpu and network and i o bandwidth you can do this in parallel and really speed up the",
    "start": "1690640",
    "end": "1696799"
  },
  {
    "text": "performance of your application now since each one of these parts is smaller in size than your total archive if one of them fails the cost of retrying is",
    "start": "1696799",
    "end": "1703360"
  },
  {
    "text": "much smaller now once all the parts have been uploaded you need to tell glacier you're finished",
    "start": "1703360",
    "end": "1709279"
  },
  {
    "text": "so you call complete multi-part upload you need to pass in the checksum for that complete archive and then glacier",
    "start": "1709279",
    "end": "1714720"
  },
  {
    "text": "will give you back an archive id at this point your upload's completed it's durably stored in glacier",
    "start": "1714720",
    "end": "1721440"
  },
  {
    "text": "and it's time to move on so i want to talk a little bit about the way we compute checksums in glacier",
    "start": "1721440",
    "end": "1726799"
  },
  {
    "text": "because it really differs from a lot of other services out there and it kind of makes us unique",
    "start": "1726799",
    "end": "1732158"
  },
  {
    "text": "so let's take an example where you want to upload a multi-part object of six and a half megabytes",
    "start": "1732399",
    "end": "1738159"
  },
  {
    "text": "on your left we have a four megabyte part and on your right we have a two and a half megabyte part so our part size",
    "start": "1738159",
    "end": "1743440"
  },
  {
    "text": "here is going to be four megabytes and every part you upload has to be four megs except for the last part which can",
    "start": "1743440",
    "end": "1748559"
  },
  {
    "text": "be a little smaller so how do we compute a checksum for this well we look at each one megabyte chunk",
    "start": "1748559",
    "end": "1755200"
  },
  {
    "text": "of that four megabyte part and we compute a shot 256 hash just a regular shot 56 hash you can see those bubbles",
    "start": "1755200",
    "end": "1761039"
  },
  {
    "text": "right above those uh purple boxes now in order to compute the hash for the entire object what we do is we take",
    "start": "1761039",
    "end": "1767679"
  },
  {
    "text": "these pairs of shock 256 hashes we concatenate them together and then you shock 56 that again",
    "start": "1767679",
    "end": "1774880"
  },
  {
    "text": "you keep bubbling that up the tree until you reach a root note so you can't combine these anymore now that's the",
    "start": "1774880",
    "end": "1780399"
  },
  {
    "text": "check sum of that entire part and that's kind of labeled that's labeled part one shot 26 tree hash you upload that to glacier",
    "start": "1780399",
    "end": "1786960"
  },
  {
    "text": "you specify the checksum and glacier will check to make sure the data uploads matches the checksum right",
    "start": "1786960",
    "end": "1792080"
  },
  {
    "text": "you have transmission integrity at this point the next thing you need to do is upload part number two again you specify the",
    "start": "1792080",
    "end": "1798320"
  },
  {
    "text": "checksum you compute represented by that part 256 tree hash bubble and glacier says okay again transmission",
    "start": "1798320",
    "end": "1804960"
  },
  {
    "text": "integrity now we need to reassemble that archive we need really integrity of the entire",
    "start": "1804960",
    "end": "1810080"
  },
  {
    "text": "archive to be verified so when we call complete we specify that root hash at the very top of the tree",
    "start": "1810080",
    "end": "1817039"
  },
  {
    "text": "what glacier will do is it'll verify all the bytes have been retrieved successfully and verify that your archive isn't has been reconstructed has",
    "start": "1817039",
    "end": "1823279"
  },
  {
    "text": "been ordered together properly that nothing is missing and that your data is persistently stored",
    "start": "1823279",
    "end": "1828880"
  },
  {
    "text": "now we have a couple options for computing that root hash one is we could read our entire archive again and compute this",
    "start": "1828880",
    "end": "1834399"
  },
  {
    "text": "whole tree all over again the other option is if we kept track of the part hashes all we need to do is concatenate",
    "start": "1834399",
    "end": "1840000"
  },
  {
    "text": "those part hashes run a shot 56 algorithm over that tiny string and then use that as the specified complete",
    "start": "1840000",
    "end": "1845679"
  },
  {
    "text": "multi-part upload parameter for glacier so let's walk through sending a large",
    "start": "1845679",
    "end": "1851679"
  },
  {
    "text": "amount of data to glacier",
    "start": "1851679",
    "end": "1855240"
  },
  {
    "text": "okay here's the method we're interested in again we call setup which initializes",
    "start": "1889039",
    "end": "1894159"
  },
  {
    "text": "our credentials and the endpoint and the first thing we do is we do something very similar to that slideshow where we we initiate a multi-part upload",
    "start": "1894159",
    "end": "1901840"
  },
  {
    "text": "so let's look at this method it's again very simple using the aws",
    "start": "1901840",
    "end": "1906880"
  },
  {
    "text": "provided sdk it's basically a couple of lines where you say i want to initiate a multi-part",
    "start": "1906880",
    "end": "1912000"
  },
  {
    "text": "upload for a given vault for a given description and for a specific a specific part size",
    "start": "1912000",
    "end": "1919120"
  },
  {
    "text": "you make the request and then you get an upload id back so we have the upload id here now",
    "start": "1919120",
    "end": "1927200"
  },
  {
    "text": "the next thing we want to do is upload the actual parts so let's look what that does",
    "start": "1927200",
    "end": "1933799"
  },
  {
    "text": "now we open a couple files here open an input stream to actually start reading the source file then we create a list of all the",
    "start": "1934480",
    "end": "1940559"
  },
  {
    "text": "checksums for these parts",
    "start": "1940559",
    "end": "1943600"
  },
  {
    "text": "now we create a buffer for each part and read each part size in from the file itself",
    "start": "1946480",
    "end": "1952399"
  },
  {
    "text": "we have to calculate the checksum for this part before we send it to glacier so we use a provided utility a tree hash",
    "start": "1952399",
    "end": "1957840"
  },
  {
    "text": "generator to calculate a checksum for that part buffer",
    "start": "1957840",
    "end": "1963039"
  },
  {
    "text": "and we have to convert this to binary for for reasons we'll become clear in a second",
    "start": "1963039",
    "end": "1969039"
  },
  {
    "text": "now the way you specify how these parts are being rearranged in glacier is a little bit different than it is in s3 in",
    "start": "1969039",
    "end": "1974399"
  },
  {
    "text": "s3 parts have a numbering and they just concatenate the numbers they concatenate the parts together based on the numbering the ordering that's provided",
    "start": "1974399",
    "end": "1980399"
  },
  {
    "text": "there with glacier you say specifically this part falls between the range four megabytes and eight megabytes or between",
    "start": "1980399",
    "end": "1987039"
  },
  {
    "text": "one megabytes and two megabytes and what glacier does when you complete your upload is make sure there's no holes or gaps in that so we're calculating the",
    "start": "1987039",
    "end": "1993120"
  },
  {
    "text": "content range based on where we should be now we make a upload multi-part request",
    "start": "1993120",
    "end": "1998720"
  },
  {
    "text": "using all the parameters we just previously calculated we call it and then we advance our pointer",
    "start": "1998720",
    "end": "2004799"
  },
  {
    "text": "now at the end we have a list of all these check sums and we want to compute the root hash for this entire thing",
    "start": "2004799",
    "end": "2010080"
  },
  {
    "text": "so at the end we use a utility method provided by the sdk to calculate the tree hash for all these part checksums",
    "start": "2010080",
    "end": "2016000"
  },
  {
    "text": "and we get the final checksum for the archive",
    "start": "2016000",
    "end": "2019840"
  },
  {
    "text": "at this point we can complete the multi-part upload we specify all the parameters including the checksum and we",
    "start": "2022159",
    "end": "2027200"
  },
  {
    "text": "get the archive id back",
    "start": "2027200",
    "end": "2030919"
  },
  {
    "start": "2037000",
    "end": "2037000"
  },
  {
    "text": "so we talked about uploading large amounts of data to glacier talked about check summing let's talk about how to get large amounts of data out of glacier",
    "start": "2038720",
    "end": "2046399"
  },
  {
    "text": "so again when you're dealing with you know glacier is a two-step process you retrieve your archive and then you download it i'm talking about the point",
    "start": "2046399",
    "end": "2052480"
  },
  {
    "text": "where you're downloading the data now again if you're downloading things of any size over 100 megs you want to be",
    "start": "2052480",
    "end": "2059358"
  },
  {
    "text": "using the range get api if you're familiar with the http spec it kind of it follows that pretty much to the",
    "start": "2059359",
    "end": "2064720"
  },
  {
    "text": "letter so you can ask for a partial partial range of an object for example",
    "start": "2064720",
    "end": "2070720"
  },
  {
    "text": "let's say you have 100 megabyte objects stored in glacier staged in glacier ready for download you can ask for the first 16 megabytes the next 16 megabytes",
    "start": "2070720",
    "end": "2077839"
  },
  {
    "text": "and so on and so forth but once you download all these bytes again you have the issue of verifying the integrity of this data so let's talk a little bit",
    "start": "2077839",
    "end": "2084480"
  },
  {
    "text": "more how you do that so we have another tree similar to the",
    "start": "2084480",
    "end": "2090638"
  },
  {
    "start": "2088000",
    "end": "2088000"
  },
  {
    "text": "one before and i want to talk a little bit about the concept of tree hash alignment versus non-tree hash alignment so if you",
    "start": "2090639",
    "end": "2097440"
  },
  {
    "text": "look at this green box you know it's basically bordering it's boxing out two one megabyte chunks",
    "start": "2097440",
    "end": "2103280"
  },
  {
    "text": "and if you see how those chunks bubble up into that tree it corresponds to a node",
    "start": "2103280",
    "end": "2108400"
  },
  {
    "text": "now anytime you ask for a range of data from glacier and if that range of data corresponds to",
    "start": "2108400",
    "end": "2113520"
  },
  {
    "text": "a node in this tree we give you a checksum on that data this allows you to verify the integrity of the data once you've downloaded it",
    "start": "2113520",
    "end": "2119839"
  },
  {
    "text": "now in contrast if you look at the red square that is also a two megabyte chunk but if you look at the way it bubbles up the",
    "start": "2119839",
    "end": "2125680"
  },
  {
    "text": "tree there's no node that really corresponds to just those two chunks we can't provide a checksum in that case",
    "start": "2125680",
    "end": "2130960"
  },
  {
    "text": "so when you're downloading stuff from a range you really want to use these tree hash aligned offsets so some examples",
    "start": "2130960",
    "end": "2136079"
  },
  {
    "text": "would be you could download every one megabyte you could download every two megabytes you could download every 4 8",
    "start": "2136079",
    "end": "2141760"
  },
  {
    "text": "16 64 etc",
    "start": "2141760",
    "end": "2145839"
  },
  {
    "start": "2147000",
    "end": "2147000"
  },
  {
    "text": "i want to talk about one more use case here we talked about using range retrievals to pluck a single file out of an archive",
    "start": "2151920",
    "end": "2158960"
  },
  {
    "text": "there's another interesting use case here where it can save you a lot of money let's say you want to restore a very",
    "start": "2158960",
    "end": "2164000"
  },
  {
    "text": "large archive the way glacier pricing works is that you're priced based on the bandwidth used for that archive retrieval now that",
    "start": "2164000",
    "end": "2171280"
  },
  {
    "text": "archive retrieval bandwidth is calculated by taking the size of the archive and divide it by four let's do a",
    "start": "2171280",
    "end": "2176480"
  },
  {
    "text": "specific example here let's say you have a 1.2 gigabyte",
    "start": "2176480",
    "end": "2181680"
  },
  {
    "text": "archive and you want to retrieve this with a single four hour job",
    "start": "2181680",
    "end": "2186720"
  },
  {
    "text": "now the way glacier computes the price it'll be charging you basically for a 300 megabyte per hour peak retrieval fee",
    "start": "2186720",
    "end": "2194000"
  },
  {
    "text": "if instead you don't need your data back right away let's say you can wait 25 hours or so to get your data back",
    "start": "2194000",
    "end": "2199599"
  },
  {
    "text": "what you can do is you can use five consecutive jobs to download 256 megabytes each",
    "start": "2199599",
    "end": "2206560"
  },
  {
    "text": "how that would work would you would say glacier please retrieve the first 256 megabytes of this archive once that job",
    "start": "2206560",
    "end": "2212800"
  },
  {
    "text": "is completed you're down able to download the output and store it then you initiate the next job and so on",
    "start": "2212800",
    "end": "2218800"
  },
  {
    "text": "and so forth now this retrieves this reduces your peak retrieval fee from 300 megabytes per hour all the way down to",
    "start": "2218800",
    "end": "2224880"
  },
  {
    "text": "64 megabytes per hour and you'll notice that the number we've chosen here 256 megabytes is one of",
    "start": "2224880",
    "end": "2231440"
  },
  {
    "text": "those sizes that is corresponds to tree hash alignment it's one of those multiples of megabytes that works",
    "start": "2231440",
    "end": "2238560"
  },
  {
    "text": "if you do this correctly well depending on your use case you can retrieve your data from glacier",
    "start": "2238560",
    "end": "2244240"
  },
  {
    "text": "and stay completely within your free retrieval tier if you're willing to spend enough time to retrieve your data",
    "start": "2244240",
    "end": "2250720"
  },
  {
    "start": "2251000",
    "end": "2251000"
  },
  {
    "text": "so this wraps up most of the technical content of this talk i want to give you a couple pointers to where to go next",
    "start": "2251760",
    "end": "2257119"
  },
  {
    "text": "first the aws glacier homepage is a great landing place for everything related to this there we have a lot of",
    "start": "2257119",
    "end": "2262960"
  },
  {
    "text": "discussion on pricing we have a pricing calculator we have pointers to a bunch of other resources including the official documentation where to get the",
    "start": "2262960",
    "end": "2269119"
  },
  {
    "text": "sdk et cetera one of the really great resources though is the aws developer forms so this is other developers using",
    "start": "2269119",
    "end": "2275839"
  },
  {
    "text": "glacier along with a bunch of amazon engineers and they can answer really deep technical questions and help you debug stuff and we monitor these forms",
    "start": "2275839",
    "end": "2282320"
  },
  {
    "text": "regularly and look for customer requests and features and it's a great way to interact with us i'll find a way to distribute the source",
    "start": "2282320",
    "end": "2288560"
  },
  {
    "text": "code that i showed you earlier today for example purposes and i'm going to be heading to the aws booth right after this talk",
    "start": "2288560",
    "end": "2295440"
  },
  {
    "text": "so i really want to thank you guys for coming and watching this presentation about glacier you all know that amazon's a very metrics and data-driven company",
    "start": "2296000",
    "end": "2302160"
  },
  {
    "text": "so i please i'm encouraging you to please fill out your evaluation feedback for me and thanks once again appreciate",
    "start": "2302160",
    "end": "2307520"
  },
  {
    "text": "it",
    "start": "2307520",
    "end": "2310520"
  },
  {
    "text": "you",
    "start": "2314160",
    "end": "2316240"
  }
]