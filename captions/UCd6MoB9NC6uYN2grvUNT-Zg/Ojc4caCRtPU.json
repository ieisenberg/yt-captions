[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "my name is Cecilia dang I'm a software engineer on the ADA is lambda team and",
    "start": "30",
    "end": "5250"
  },
  {
    "text": "I'm here today to talk about building high throughput serverless data processing pipelines and there's a lot",
    "start": "5250",
    "end": "11969"
  },
  {
    "text": "of words I know they're like all storing together and it's kind of buzzy and and it might not be clear what that is about but I will definitely clarify what it",
    "start": "11969",
    "end": "19740"
  },
  {
    "text": "all means and go into it I kind of wanted to start off the show of hands",
    "start": "19740",
    "end": "24869"
  },
  {
    "text": "who's already using Kinesis with lambda we're sort of okay awesome I kind of",
    "start": "24869",
    "end": "30240"
  },
  {
    "text": "want to gauge sort of a fiora T like new your use cases and why you want to do use streams but I will talk a little bit",
    "start": "30240",
    "end": "36329"
  },
  {
    "text": "about like why you would do streams and sort of common architectures that we see",
    "start": "36329",
    "end": "42239"
  },
  {
    "text": "as well so a little bit about me I'm in Canada I went to UBC I started off at EA",
    "start": "42239",
    "end": "49710"
  },
  {
    "start": "44000",
    "end": "120000"
  },
  {
    "text": "and now I'm on a BS lambda for the past three and a half years and the goal as I",
    "start": "49710",
    "end": "59699"
  },
  {
    "text": "mentioned is for high throughput serverless real-time pipeline so what",
    "start": "59699",
    "end": "64948"
  },
  {
    "text": "does that really mean high throughput so it's a pretty nebulous term but generally we consider you know",
    "start": "64949",
    "end": "70710"
  },
  {
    "text": "processing anything in the gigabytes per second as high throughput many services",
    "start": "70710",
    "end": "76290"
  },
  {
    "text": "many 80 bus services have limits where you can tune and options to scale higher but sort of this is what we're targeting",
    "start": "76290",
    "end": "82259"
  },
  {
    "text": "here service here you know it means no",
    "start": "82259",
    "end": "87659"
  },
  {
    "text": "service to manage but you really want to talk about service compute so being able",
    "start": "87659",
    "end": "92670"
  },
  {
    "text": "to sort of abstract away anything besides just having the actual logic of",
    "start": "92670",
    "end": "98430"
  },
  {
    "text": "your code running and then for pipelines this kind of implies sequence and an",
    "start": "98430",
    "end": "104820"
  },
  {
    "text": "order so this is kind of kind of how time works or at least how we observe it so go in at the time definition but here",
    "start": "104820",
    "end": "113610"
  },
  {
    "text": "we're going to talk about sort of the real-time constraints that we live in and why streams is really good for that",
    "start": "113610",
    "end": "120020"
  },
  {
    "start": "120000",
    "end": "190000"
  },
  {
    "text": "so what to expect well why why streams",
    "start": "120020",
    "end": "125909"
  },
  {
    "text": "why do you using why are you using streams what are your use cases and what is it really good for most of this talk",
    "start": "125909",
    "end": "132209"
  },
  {
    "text": "is going to talk about streaming solutions for data processing needs so we'll go over the benefits of",
    "start": "132209",
    "end": "137630"
  },
  {
    "text": "that for sure and then we'll talk about what lambda is I'll do this really quick it seems most people in the room is",
    "start": "137630",
    "end": "143960"
  },
  {
    "text": "pretty familiar with it and then we'll dive into what Kinesis is",
    "start": "143960",
    "end": "149120"
  },
  {
    "text": "how they really do the streaming side of things and a little bit on yeah what",
    "start": "149120",
    "end": "155210"
  },
  {
    "text": "they deliver on that end and then challenges on processing from streams in general and basically how you don't need",
    "start": "155210",
    "end": "162410"
  },
  {
    "text": "to worry about it with a lot of the a diverse manager services that you'll that you have to offer and in",
    "start": "162410",
    "end": "168830"
  },
  {
    "text": "particularly dive into how lambda processes streams that's the team that I work on and in particular I work on the",
    "start": "168830",
    "end": "175340"
  },
  {
    "text": "event sourcing team so I can dive into that and then we'll go into sort of some more example use cases of all these",
    "start": "175340",
    "end": "182900"
  },
  {
    "text": "different offerings and how you can sort of hook them up together and and architectures and yeah so let's start",
    "start": "182900",
    "end": "191780"
  },
  {
    "start": "190000",
    "end": "307000"
  },
  {
    "text": "with the first question why streams what use cases would benefit from streams so",
    "start": "191780",
    "end": "204890"
  },
  {
    "text": "remember the goal we have high throughput service it's managed compute",
    "start": "204890",
    "end": "210200"
  },
  {
    "text": "and real-time so I highly recommend actually this talk srv4 to that goes",
    "start": "210200",
    "end": "217400"
  },
  {
    "text": "into sort of the differences between streaming and MapReduce but here is sort",
    "start": "217400",
    "end": "222410"
  },
  {
    "text": "of my take on it for streams you kind of have a data size constraint generally it's sort of small amounts of data and",
    "start": "222410",
    "end": "229490"
  },
  {
    "text": "you also have a real time constraint you need it to be available right away and you probably don't need it you know a",
    "start": "229490",
    "end": "235370"
  },
  {
    "text": "month from now so you need to process it right away you generally have access when you're processing to only a most",
    "start": "235370",
    "end": "241370"
  },
  {
    "text": "recent cut time window of data so this all kind of ties into the whole time",
    "start": "241370",
    "end": "246710"
  },
  {
    "text": "constraint part of streams versus something like that or MapReduce you don't really have a size constraint you",
    "start": "246710",
    "end": "251810"
  },
  {
    "text": "can have really really large sets of data and there isn't that time constraint you can take your time you",
    "start": "251810",
    "end": "257180"
  },
  {
    "text": "know processing large sets of data and generate a report that type of thing you have access to all your data when you do",
    "start": "257180",
    "end": "263660"
  },
  {
    "text": "this processing as well whereas with streams you only have that window and again it's long running",
    "start": "263660",
    "end": "269120"
  },
  {
    "text": "running so with stream processing you have data that's giant being generated",
    "start": "269120",
    "end": "274850"
  },
  {
    "text": "continuously simultaneously thousands of data sources typically small sizes you",
    "start": "274850",
    "end": "280760"
  },
  {
    "text": "have your social medias your IOT devices like farming equipment stocks click",
    "start": "280760",
    "end": "288530"
  },
  {
    "text": "streams you own your website you want to sort of know how users are using your web service and sports analytics I came",
    "start": "288530",
    "end": "297470"
  },
  {
    "text": "from sports gaming background so I added that in there and all of this needs to be sort of",
    "start": "297470",
    "end": "302990"
  },
  {
    "text": "process sequentially incremental II in sliding time windows so lambda real",
    "start": "302990",
    "end": "311390"
  },
  {
    "start": "307000",
    "end": "341000"
  },
  {
    "text": "quick it's your function it's your code and we give you a programming model so that you",
    "start": "311390",
    "end": "317240"
  },
  {
    "text": "can easily will run it for you flexible model you choose your memory allocated",
    "start": "317240",
    "end": "323090"
  },
  {
    "text": "oh this is all the configurations that comes with the model that we give you really integrated security like all the",
    "start": "323090",
    "end": "329240"
  },
  {
    "text": "bells and whistles that comes with AWS and it runs stateless so this is kind of important as well you don't have a state",
    "start": "329240",
    "end": "336440"
  },
  {
    "text": "we it's a container backed system this",
    "start": "336440",
    "end": "341810"
  },
  {
    "start": "341000",
    "end": "402000"
  },
  {
    "text": "is the interesting part because you defined your logic now what are gonna do with it you need it to be part of the",
    "start": "341810",
    "end": "347900"
  },
  {
    "text": "overall architecture so how do you trigger it and this is where things get really interesting real quick",
    "start": "347900",
    "end": "353810"
  },
  {
    "text": "asynchronous push model so there's three three different varieties of this so one",
    "start": "353810",
    "end": "359600"
  },
  {
    "text": "is the asynchronous push model you have your event sources that use the async invoke API that we provide",
    "start": "359600",
    "end": "365930"
  },
  {
    "text": "this is services like Amazon s3 Amazon SNS they owned the actual event and they",
    "start": "365930",
    "end": "374960"
  },
  {
    "text": "own delivering it to lambda so for Amazon s3 an example could be when you put something into a bucket they own",
    "start": "374960",
    "end": "382250"
  },
  {
    "text": "generating the metadata the little bits of data that tell you what just happened and they own giving it to lambda in and",
    "start": "382250",
    "end": "389600"
  },
  {
    "text": "then we have to do something with it synchronous push model it's something like I do a sigh ot or API gateway or",
    "start": "389600",
    "end": "397160"
  },
  {
    "text": "Alexa the difference here is they wait for the request and then we have the stream fullmetal so",
    "start": "397160",
    "end": "404379"
  },
  {
    "start": "402000",
    "end": "444000"
  },
  {
    "text": "this is the dynamo DB and the Kanishka streams in this case dynamodb owns",
    "start": "404379",
    "end": "410019"
  },
  {
    "text": "generating the data and managing the stream itself and making it available for consumption Amazon Kinesis you own",
    "start": "410019",
    "end": "416379"
  },
  {
    "text": "putting data into the stream but Kinesis makes it highly available and available for consumption lambda is now",
    "start": "416379",
    "end": "423159"
  },
  {
    "text": "responsible for pulling the data from these streams and making sure they get processed so generally this means",
    "start": "423159",
    "end": "429839"
  },
  {
    "text": "grabbing data keeping a memory doing some processing and then sending it to the actual lambda function it sounds",
    "start": "429839",
    "end": "437259"
  },
  {
    "text": "really simple but it's not really so this is sort of what I'll be digging into further today so again just to",
    "start": "437259",
    "end": "446289"
  },
  {
    "start": "444000",
    "end": "468000"
  },
  {
    "text": "summarize lambda can take in all sorts of events from a number of events sources either from notification or for",
    "start": "446289",
    "end": "453009"
  },
  {
    "text": "polling and the key is you can process it however you like compared to other consuming event sources and today we're",
    "start": "453009",
    "end": "461110"
  },
  {
    "text": "going to concentrate on streaming events in particular Amazon Kinesis and with",
    "start": "461110",
    "end": "469659"
  },
  {
    "text": "Kinesis you can now open up to all sorts of real-time streaming data so this includes financial data IOT data",
    "start": "469659",
    "end": "476819"
  },
  {
    "text": "streaming log data it's really a common use case and IOT data and will kind of",
    "start": "476819",
    "end": "483099"
  },
  {
    "text": "also dig into sort of like use cases here so what's Kinesis",
    "start": "483099",
    "end": "490529"
  },
  {
    "start": "488000",
    "end": "529000"
  },
  {
    "text": "well it's George it's real-time storage so that means that the data is stored for a liniment",
    "start": "490529",
    "end": "496899"
  },
  {
    "text": "limited amount of time because you pretty much need it right away and you probably don't even a long time from now",
    "start": "496899",
    "end": "504089"
  },
  {
    "text": "it needs to be made of it available really quickly so this is important because of the real time constraint",
    "start": "504089",
    "end": "509549"
  },
  {
    "text": "typically it's less than a second from the put to get delay it's a managed",
    "start": "509549",
    "end": "515620"
  },
  {
    "text": "service you have a framework it's kind of like the computer model that lambda",
    "start": "515620",
    "end": "521078"
  },
  {
    "text": "has Kinesis has its own set of API s and how they define you should consume it",
    "start": "521079",
    "end": "526209"
  },
  {
    "text": "and best practices there and uses a checkmark checkpoint model in",
    "start": "526209",
    "end": "533230"
  },
  {
    "text": "order to support multiple in concurrent in ordered processing and this is sort",
    "start": "533230",
    "end": "538300"
  },
  {
    "text": "of important because it kind of ties in to sort of you're given a model in which to consume this stream but there are",
    "start": "538300",
    "end": "545649"
  },
  {
    "text": "definitely caveats and tricks on how to do this in a distributed resilient and",
    "start": "545649",
    "end": "551980"
  },
  {
    "text": "scalable way so how do you process a stream in summary you need a pool for",
    "start": "551980",
    "end": "559540"
  },
  {
    "start": "554000",
    "end": "641000"
  },
  {
    "text": "the work you need to call get records and need to grab that data you need to have a checkpoint for progress so you",
    "start": "559540",
    "end": "565240"
  },
  {
    "text": "need a way to store that and update it you can have separate checkpoints for",
    "start": "565240",
    "end": "571120"
  },
  {
    "text": "multiple customers so that's a really cool thing that Kinesis provides versus something like queues and in general",
    "start": "571120",
    "end": "578829"
  },
  {
    "text": "Kinesis provides the KCl library for customers for a way to help you process",
    "start": "578829",
    "end": "586389"
  },
  {
    "text": "the number of chars distribute work and kind of have that fault tolerance built in there it's kind of a similar case for",
    "start": "586389",
    "end": "594699"
  },
  {
    "text": "DynamoDB streams the difference here is that DynamoDB owns producing the events",
    "start": "594699",
    "end": "600250"
  },
  {
    "text": "as well in this case DynamoDB events like quench few update an item in the table or if you put an item in the table",
    "start": "600250",
    "end": "606760"
  },
  {
    "text": "if you make changes otherwise everything in the backend they also they own the",
    "start": "606760",
    "end": "612430"
  },
  {
    "text": "the creation of the shards and it is depending on you at the partition of",
    "start": "612430",
    "end": "618190"
  },
  {
    "text": "your dynamiting to be table but they own distributing the records and across the",
    "start": "618190",
    "end": "624790"
  },
  {
    "text": "partitions so I don't know if I couldn't see you but it's hard so how can you",
    "start": "624790",
    "end": "633160"
  },
  {
    "text": "avoid using KCl and consuming this your cells having to keep track of the checkpoints and having to keep track of",
    "start": "633160",
    "end": "638709"
  },
  {
    "text": "any fault tolerance so we have some options here one of them is Canisius",
    "start": "638709",
    "end": "646360"
  },
  {
    "start": "641000",
    "end": "745000"
  },
  {
    "text": "firehose so what can me does fire hose does is it manages the stream as well so you don't need to worry about any shard",
    "start": "646360",
    "end": "652899"
  },
  {
    "text": "configuration or a partition key or also order so if you need order doesn't do",
    "start": "652899",
    "end": "658510"
  },
  {
    "text": "that for you and it also manages the stream processing so it pulls four records",
    "start": "658510",
    "end": "663670"
  },
  {
    "text": "it dumps it to one of Amazon s3 Amazon redshift or Amazon Elastic search and I",
    "start": "663670",
    "end": "671530"
  },
  {
    "text": "kind of threw this in there it's not really that important right now the compute power is the default of 8k p use",
    "start": "671530",
    "end": "676750"
  },
  {
    "text": "this is I just put that in there for comparison between the other two services analytics and lambda and this",
    "start": "676750",
    "end": "684940"
  },
  {
    "text": "is cool you can choose a lambda function to transform the data here there's some",
    "start": "684940",
    "end": "690370"
  },
  {
    "text": "blue front blue prints available on the lambda console are you guys familiar with blue prints",
    "start": "690370",
    "end": "695730"
  },
  {
    "text": "so when you go on the lambda console and you don't know where to start when you could click create function there is",
    "start": "695730",
    "end": "702550"
  },
  {
    "text": "basically this repository of already already created functions and",
    "start": "702550",
    "end": "708880"
  },
  {
    "text": "configuration like basically like a wizard and you can just click create and it'll create you create for you many",
    "start": "708880",
    "end": "716380"
  },
  {
    "text": "different common scenarios including transformation for Kinesis firehose work",
    "start": "716380",
    "end": "722680"
  },
  {
    "text": "flows from json csv to whatever or Apache log to json csv so that's Kinesis",
    "start": "722680",
    "end": "729400"
  },
  {
    "text": "firehose this is kind of perfect for sort of ETL type jobs so if you had data",
    "start": "729400",
    "end": "735070"
  },
  {
    "text": "coming in needed transform and I just dumped it into storage fire home fire hose manage that for you in a really",
    "start": "735070",
    "end": "741910"
  },
  {
    "text": "great way another classic example is",
    "start": "741910",
    "end": "748030"
  },
  {
    "start": "745000",
    "end": "760000"
  },
  {
    "text": "dumping data like IOT events into s3 and then you can also connect a Amazon",
    "start": "748030",
    "end": "754570"
  },
  {
    "text": "Athena to the s3 bucket and then it produces really nice graphs and stuff so",
    "start": "754570",
    "end": "760960"
  },
  {
    "start": "760000",
    "end": "875000"
  },
  {
    "text": "speaking of in analytics Kinesis also has a service called analytics it's it's",
    "start": "760960",
    "end": "768460"
  },
  {
    "text": "their offering of a sequel language querying on top of real-time analyzed",
    "start": "768460",
    "end": "773530"
  },
  {
    "text": "data i consider this kind of like a filter or aggregation instead of a",
    "start": "773530",
    "end": "778630"
  },
  {
    "text": "transformation that firehose offers with their lambda proof blueprints so camisa",
    "start": "778630",
    "end": "784270"
  },
  {
    "text": "analytics doesn't manage the stream you need to configure the stream itself and then when you hook up analytics to it it",
    "start": "784270",
    "end": "790390"
  },
  {
    "text": "manages the stream processing so it pulls for records it uses the sequel model to continuously map",
    "start": "790390",
    "end": "797010"
  },
  {
    "text": "the record data to their internal concepts of stream tables this is aggregated and then it continuously",
    "start": "797010",
    "end": "803550"
  },
  {
    "text": "queers the internal stream tables for the desired results and this can be your filtering logic and then it outputs the",
    "start": "803550",
    "end": "809820"
  },
  {
    "text": "desired results to additional stream table so further for their aggregation and eventually you can throw it into",
    "start": "809820",
    "end": "816959"
  },
  {
    "text": "another Kinesis stream or firehose and it is a similar compute power of defaults eight kpu so this is this type",
    "start": "816959",
    "end": "829170"
  },
  {
    "text": "of service is really good for real-time queried filtered analysis of data",
    "start": "829170",
    "end": "834350"
  },
  {
    "text": "analytics gives you a sequel model so if you're very familiar and you like the",
    "start": "834350",
    "end": "839370"
  },
  {
    "text": "idea of sequel to use do your filtering it's a great way to sort of take advantage of that you specify the",
    "start": "839370",
    "end": "844889"
  },
  {
    "text": "mapping and you're sorry you stress of the mapping here between your source and",
    "start": "844889",
    "end": "852000"
  },
  {
    "text": "your in application input data and you can also use s3 to sort of enrich the",
    "start": "852000",
    "end": "857399"
  },
  {
    "text": "date the metadata that you want to use and then you specify the query that you want to filter and that's end up stored",
    "start": "857399",
    "end": "864149"
  },
  {
    "text": "in intermediary updating tables and all of this is happening in real time it's just kind of aggregating aggregating and",
    "start": "864149",
    "end": "870360"
  },
  {
    "text": "eventually you output it to Kinesis or firehose so this kind of processing",
    "start": "870360",
    "end": "877850"
  },
  {
    "start": "875000",
    "end": "942000"
  },
  {
    "text": "especially based off of sliding windows can be really good for you know most",
    "start": "877850",
    "end": "882899"
  },
  {
    "text": "recent breakdown or average daily users type of deal constantly changing with new incoming data it's also really good",
    "start": "882899",
    "end": "889620"
  },
  {
    "text": "for insights into latest trends or changes in your system that help alert",
    "start": "889620",
    "end": "896970"
  },
  {
    "text": "you to make relevant decisions faster or combining of various different sources",
    "start": "896970",
    "end": "902760"
  },
  {
    "text": "and kind of coming up with new data and that's useful typical streaming",
    "start": "902760",
    "end": "910230"
  },
  {
    "text": "architecture can consist of streams hooking into analytics hooking back into streams looking into lambda and then",
    "start": "910230",
    "end": "916560"
  },
  {
    "text": "throwing into dynamodb so there's for example social media you can generate",
    "start": "916560",
    "end": "922019"
  },
  {
    "text": "hashtags and do sort of constant aggregation aggregation and Kinesis analytics throw the results into another",
    "start": "922019",
    "end": "928829"
  },
  {
    "text": "Kinesis stream and ada was lambda2 throw it into DynamoDB since it's not available through fire",
    "start": "928829",
    "end": "935709"
  },
  {
    "text": "hose directly lambda ends up being the bridge to anything so lambda this is the other the",
    "start": "935709",
    "end": "948699"
  },
  {
    "start": "942000",
    "end": "1029000"
  },
  {
    "text": "last option and what I'll dive into here lambda doesn't need to also don't configure the stream itself you need to",
    "start": "948699",
    "end": "954660"
  },
  {
    "text": "hook it up to a stream but it manages the stream processing so that postal wreckers we send it for invocation to a",
    "start": "954660",
    "end": "962290"
  },
  {
    "text": "lambda function and the compute power here is the I put it here for comparison is your default limit so your default",
    "start": "962290",
    "end": "971290"
  },
  {
    "text": "concurrent limit a thousand times whatever you've configured your memory for your lambda function and all the associated size CPU you set it up with",
    "start": "971290",
    "end": "979839"
  },
  {
    "text": "an API create event source mapping and the key here is now you can send it for",
    "start": "979839",
    "end": "987850"
  },
  {
    "text": "invocation to a lambda function so really your processing has opened up and",
    "start": "987850",
    "end": "993250"
  },
  {
    "text": "[Music] with lambda you get order so we honor",
    "start": "993250",
    "end": "999480"
  },
  {
    "text": "the basically the contract that Kinesis isset has set out",
    "start": "999480",
    "end": "1005279"
  },
  {
    "text": "you have a soft concurrent limit of a thousand of vacations but it's a soft limit you can ask for that to be increased times a maximum of three",
    "start": "1005279",
    "end": "1012810"
  },
  {
    "text": "gigabytes of memory and associated CPU and it's completely customized model and functionality so you no longer",
    "start": "1012810",
    "end": "1018660"
  },
  {
    "text": "restricted to you know very common but useful use cases like with fire hose or commute analytics your use cases really",
    "start": "1018660",
    "end": "1027058"
  },
  {
    "text": "opens up here another kind of key difference is lambda is available in",
    "start": "1027059",
    "end": "1033918"
  },
  {
    "start": "1029000",
    "end": "1047000"
  },
  {
    "text": "every region we're trying to go out with every new region and currently Amazon",
    "start": "1033919",
    "end": "1040260"
  },
  {
    "text": "Kinesis analytics is in three and fire hoses in six Italy so something else to",
    "start": "1040260",
    "end": "1046980"
  },
  {
    "text": "take into mind so this is kind of what it looks like when lambda processes streams we have a",
    "start": "1046980",
    "end": "1055130"
  },
  {
    "start": "1047000",
    "end": "1136000"
  },
  {
    "text": "service polling data off of a stream keeping track of checkpoints and sending",
    "start": "1055130",
    "end": "1065030"
  },
  {
    "text": "sending data off to lambda for processing as I hinted above this you",
    "start": "1065030",
    "end": "1070550"
  },
  {
    "text": "know the three points aren't as simple as it seems otherwise we probably wouldn't have to manage this for you so",
    "start": "1070550",
    "end": "1077540"
  },
  {
    "text": "we're gonna dive into the details here and give you more insight into how",
    "start": "1077540",
    "end": "1083600"
  },
  {
    "text": "lambda does stream processing to help you appreciate both the awesome work that we do and also get a better sense",
    "start": "1083600",
    "end": "1088970"
  },
  {
    "text": "of best practices when it comes to configuring and monitoring this kind of architecture so configuring the event",
    "start": "1088970",
    "end": "1097610"
  },
  {
    "text": "source you have your batch size this is the max number of records that could",
    "start": "1097610",
    "end": "1104150"
  },
  {
    "text": "possibly be in one payload or one invocation to Kinesis not equivalent to",
    "start": "1104150",
    "end": "1109430"
  },
  {
    "text": "how many records that we're gonna be pulling off we pull off as many records as possible and the starting position",
    "start": "1109430",
    "end": "1114710"
  },
  {
    "text": "just tells you which record that we're going to start processing from so this is only relevant when you first create your event source mapping or when you",
    "start": "1114710",
    "end": "1120170"
  },
  {
    "text": "disable and re-enable it this allows you for example if you made a mistake you",
    "start": "1120170",
    "end": "1125420"
  },
  {
    "text": "need to turn off your function disable it and you do your fixes and then you",
    "start": "1125420",
    "end": "1130880"
  },
  {
    "text": "can rien a bêlit and start back off wherever you wanted to continue processing from okay so what are the",
    "start": "1130880",
    "end": "1139820"
  },
  {
    "text": "steps what's going on here we have a stream of incoming data it's like a sliding window of store data and then we",
    "start": "1139820",
    "end": "1146570"
  },
  {
    "text": "have trim horizon here trim horizon is where Kinesis starts expiring records",
    "start": "1146570",
    "end": "1151940"
  },
  {
    "text": "the default age of records or the TTL I suppose is 24 hours so we have 24 hours",
    "start": "1151940",
    "end": "1158270"
  },
  {
    "text": "to process your data this is a soft limit you can ask for Kinesis to increase this number as well you also",
    "start": "1158270",
    "end": "1166130"
  },
  {
    "start": "1165000",
    "end": "1183000"
  },
  {
    "text": "have your latest record this is the most recent record that you just put into your stream these are the two options",
    "start": "1166130",
    "end": "1172700"
  },
  {
    "text": "that you can tell us - tell lambda to start processing from if you choose latest you're basically saying I don't",
    "start": "1172700",
    "end": "1178700"
  },
  {
    "text": "care about anything that came before me I'm just going to start processing from this point on so lambdas gonna have a check point",
    "start": "1178700",
    "end": "1185340"
  },
  {
    "start": "1183000",
    "end": "1206000"
  },
  {
    "text": "somewhere here between latest and trim horizon and we store this checkpoint in storage you might check the the",
    "start": "1185340",
    "end": "1193549"
  },
  {
    "text": "checkpoints continues and you makes progress and you can have multiple checkpoints so you can have multiple",
    "start": "1193549",
    "end": "1199530"
  },
  {
    "text": "consumers from the stream and they all keep track of their own checkpoints or iterators here's an example payload from",
    "start": "1199530",
    "end": "1208890"
  },
  {
    "start": "1206000",
    "end": "1235000"
  },
  {
    "text": "Kinesis so the data actually is here its basics before encoded but otherwise you",
    "start": "1208890",
    "end": "1215730"
  },
  {
    "text": "get metadata like partition key the sequence number and another metadata the",
    "start": "1215730",
    "end": "1224070"
  },
  {
    "text": "region oh and another key thing to note is that this is a batch of record so you can see",
    "start": "1224070",
    "end": "1231330"
  },
  {
    "text": "it can tell that this is an array here here's an example lambda function that's",
    "start": "1231330",
    "end": "1238380"
  },
  {
    "start": "1235000",
    "end": "1276000"
  },
  {
    "text": "that would be processing from Kinesis you have your event this could be called",
    "start": "1238380",
    "end": "1244620"
  },
  {
    "text": "anything and and you can get your list or array of records here and for each",
    "start": "1244620",
    "end": "1253230"
  },
  {
    "text": "record here I've decided to be you have to basically for decoded in my case I",
    "start": "1253230",
    "end": "1258900"
  },
  {
    "text": "use an example that was in JSON format but doesn't need to be it could just be any text and I decided to upload it to a",
    "start": "1258900",
    "end": "1266549"
  },
  {
    "text": "dynamo DB table and I generally like to use game references so in this case it was a player in a score",
    "start": "1266549",
    "end": "1274309"
  },
  {
    "start": "1276000",
    "end": "1363000"
  },
  {
    "text": "so what's going on really behind the scenes I'm gonna go into the details of",
    "start": "1278400",
    "end": "1284850"
  },
  {
    "text": "what we do per shard and what this and what happens per shard happens can",
    "start": "1284850",
    "end": "1290910"
  },
  {
    "text": "happen in parallel for all the shines so",
    "start": "1290910",
    "end": "1296730"
  },
  {
    "text": "for shard lambda call will call get record and we'll try to get as many records as we can with the max limit if",
    "start": "1296730",
    "end": "1305190"
  },
  {
    "text": "I'm Kinesis which is 10 K records and or 10 megabytes of data or whatever the",
    "start": "1305190",
    "end": "1310230"
  },
  {
    "text": "batch size that you've configured for this event source mapping so we call got records if you don't get any record then",
    "start": "1310230",
    "end": "1316830"
  },
  {
    "text": "we're gonna wait some time right now from Kinesis it's one second otherwise we sub batch in memory so we have all",
    "start": "1316830",
    "end": "1323550"
  },
  {
    "text": "these records in memory and now we've got to honor the your configure batch size the max payload that you can send",
    "start": "1323550",
    "end": "1330390"
  },
  {
    "text": "to lambda and then so we construct this intermediary payload and then we invoke",
    "start": "1330390",
    "end": "1336570"
  },
  {
    "text": "lambda synchronously with this payload and this is key synchronously because we can't move on to the next batch of",
    "start": "1336570",
    "end": "1342780"
  },
  {
    "text": "records until this current batch of records has completed this is in order to honor the Kinesis contract of in",
    "start": "1342780",
    "end": "1352170"
  },
  {
    "text": "ordered processing and then once you invoke lambda lambda itself consent to",
    "start": "1352170",
    "end": "1357570"
  },
  {
    "text": "multiple different destinations or any sort of transportation you like so",
    "start": "1357570",
    "end": "1364400"
  },
  {
    "start": "1363000",
    "end": "1440000"
  },
  {
    "text": "lambda blocks an order processing for each individual shard what this means is",
    "start": "1364400",
    "end": "1369900"
  },
  {
    "text": "that increasing number of shards is your way of increasing the throughput or the concurrency available to your stream",
    "start": "1369900",
    "end": "1376850"
  },
  {
    "text": "this is also kavya you need to be able to deliver data distributed lis across",
    "start": "1376850",
    "end": "1382800"
  },
  {
    "text": "your shards because if you have a hundred shards and you're only writing to one of them it's not going to help you so it's important to pick your",
    "start": "1382800",
    "end": "1389160"
  },
  {
    "text": "partition key there there's lots of best practices on how to produce data to your Kinesis stream my best practice is there",
    "start": "1389160",
    "end": "1396420"
  },
  {
    "text": "that I won't go into today but yeah basically make sure that you're writing to you're taking advantage of all the",
    "start": "1396420",
    "end": "1403440"
  },
  {
    "text": "shards in your stream batch size may impact duration if the lambda function",
    "start": "1403440",
    "end": "1409770"
  },
  {
    "text": "takes longer to process more records so this really depends on if you're sequentially processing or if you're",
    "start": "1409770",
    "end": "1417100"
  },
  {
    "text": "doing things in parallel then you might be compute constraint or memory constrained so you know you might think",
    "start": "1417100",
    "end": "1424540"
  },
  {
    "text": "just increasing the batch size might increase throughput or make things happen faster but if the ultimate",
    "start": "1424540",
    "end": "1430750"
  },
  {
    "text": "duration of your lambda function takes longer then it's not going to really help you that much since we are",
    "start": "1430750",
    "end": "1436750"
  },
  {
    "text": "synchronously blocking per batch we pull",
    "start": "1436750",
    "end": "1443260"
  },
  {
    "text": "and block so you can you'll see per shard it's going to watch out if the put or ingestion rate is greater than the",
    "start": "1443260",
    "end": "1449350"
  },
  {
    "text": "theoretical throughput then your processing is at risk of falling behind and what this means is that your record",
    "start": "1449350",
    "end": "1455559"
  },
  {
    "text": "might be getting closer to trim horizon before we can process it process it and",
    "start": "1455559",
    "end": "1460780"
  },
  {
    "text": "once it hits trim horizons and Kinesis is going to get rid of it so Maxima theoretical throughput here it",
    "start": "1460780",
    "end": "1468010"
  },
  {
    "text": "could be the number of shards times the Kinesis limit on how much we can grab or",
    "start": "1468010",
    "end": "1475470"
  },
  {
    "text": "how much we can grab divided by the lambda function duration so how long it takes for your lambda function to get",
    "start": "1475470",
    "end": "1481600"
  },
  {
    "text": "back to us so we can get to the next record the effect our theoretical throughput would be also dependent on",
    "start": "1481600",
    "end": "1488320"
  },
  {
    "text": "what you've configured for your batch size to emphasize the impact of failures",
    "start": "1488320",
    "end": "1497530"
  },
  {
    "text": "here our retry logic is such that we will retry execution failures and",
    "start": "1497530",
    "end": "1502750"
  },
  {
    "text": "throttles until the record is expired again this is to honor the in ordered processing the ordered contract that we",
    "start": "1502750",
    "end": "1510490"
  },
  {
    "text": "have so any throttles and errors are definitely going to impact the",
    "start": "1510490",
    "end": "1515500"
  },
  {
    "text": "throughput of your processing it's basically going to halt it so the best practice is wholly exponentially back",
    "start": "1515500",
    "end": "1522460"
  },
  {
    "text": "off but also alarm on any throttles or errors that your lambda function might have as well as any Kinesis models",
    "start": "1522460",
    "end": "1529630"
  },
  {
    "text": "because even though you can have multiple consumers from the Kinesis stream an easy thing that consumes from",
    "start": "1529630",
    "end": "1535179"
  },
  {
    "text": "Kinesis will be limited by certain api's on the case itself for example just subscribe streams or other limits so",
    "start": "1535179",
    "end": "1543730"
  },
  {
    "text": "definitely watch out for throttling happening on the Knesset side and to sort of emphasize this the",
    "start": "1543730",
    "end": "1551929"
  },
  {
    "text": "effective theoretical throughput with retries is now the number of chars times batch size divided by not just the",
    "start": "1551929",
    "end": "1557419"
  },
  {
    "text": "function raishin but how many times we have to retry that function so I can go",
    "start": "1557419",
    "end": "1568519"
  },
  {
    "start": "1563000",
    "end": "1575000"
  },
  {
    "text": "into some more examples here of different architectures or I was",
    "start": "1568519",
    "end": "1575840"
  },
  {
    "start": "1575000",
    "end": "1648000"
  },
  {
    "text": "thinking yeah I'll quickly go through this but I was also curious to hear if",
    "start": "1575840",
    "end": "1580849"
  },
  {
    "text": "you guys had your own architectures and any questions there because I thought",
    "start": "1580849",
    "end": "1586220"
  },
  {
    "text": "that was a really cool format from the chop jokes and also if I had time I could go into a quick demo I did I just",
    "start": "1586220",
    "end": "1594619"
  },
  {
    "text": "sort of spun up on getting a Twitter stream set up and and having that put stuff into a Kinesis stream and having a",
    "start": "1594619",
    "end": "1601429"
  },
  {
    "text": "lamb to process it but some examples um you know using streams in lambda is",
    "start": "1601429",
    "end": "1608149"
  },
  {
    "text": "really good for real-time ad serving you have you know audience tracking system",
    "start": "1608149",
    "end": "1614840"
  },
  {
    "text": "ad exchange listeners bidders all sending data to a stream which then can",
    "start": "1614840",
    "end": "1621919"
  },
  {
    "text": "store and expose data for lambda to do whatever business logic that you wanted",
    "start": "1621919",
    "end": "1627470"
  },
  {
    "text": "it into it and then you can throw that into firehose if you don't care about order to really quickly throw that into",
    "start": "1627470",
    "end": "1634460"
  },
  {
    "text": "s3 a redshift and I kind of think of this as sort of the assembly line",
    "start": "1634460",
    "end": "1640669"
  },
  {
    "text": "approach where kind of every steps the way you do another transformation needs and you add more metadata we also have",
    "start": "1640669",
    "end": "1650950"
  },
  {
    "start": "1648000",
    "end": "1679000"
  },
  {
    "text": "kind of the use case for anomaly detection so analytics say we'll take a",
    "start": "1650950",
    "end": "1657109"
  },
  {
    "text": "10 second temblor window of hash tag count from Kinesis streams and we'll get",
    "start": "1657109",
    "end": "1662179"
  },
  {
    "text": "the latest count of hashtags send that into Canisius stream the results to",
    "start": "1662179",
    "end": "1667609"
  },
  {
    "text": "Kinesis stream or lambda can then send it to any data store or for further analysis maybe to s3 which can then be",
    "start": "1667609",
    "end": "1674149"
  },
  {
    "text": "surfaced visually or graphically you can also send it to something like",
    "start": "1674149",
    "end": "1681989"
  },
  {
    "start": "1679000",
    "end": "1700000"
  },
  {
    "text": "SNS for early detection this will help",
    "start": "1681989",
    "end": "1687570"
  },
  {
    "text": "you basically be aware of trends or things that are happening so you can",
    "start": "1687570",
    "end": "1693599"
  },
  {
    "text": "react faster to to your business requirements and I kind of like to think",
    "start": "1693599",
    "end": "1701249"
  },
  {
    "start": "1700000",
    "end": "1712000"
  },
  {
    "text": "of this you know it doesn't always have to be a bad trend it could be a good trend I could be texting something good that's happening so he helps you react",
    "start": "1701249",
    "end": "1707669"
  },
  {
    "text": "faster and take advantage of it for your customers and opportunities also game",
    "start": "1707669",
    "end": "1714119"
  },
  {
    "start": "1712000",
    "end": "1761000"
  },
  {
    "text": "analytics I don't know how common this is I just do that in there because that's what I used for a demo that I use but so I set",
    "start": "1714119",
    "end": "1722399"
  },
  {
    "text": "up something a Kinesis stream that was connected to a lambda which that did",
    "start": "1722399",
    "end": "1728309"
  },
  {
    "text": "some processing through it into dynamodb had a list of say players and and scores",
    "start": "1728309",
    "end": "1733739"
  },
  {
    "text": "you can hook up the Amazon DynamoDB to streams you can turn that on and",
    "start": "1733739",
    "end": "1739649"
  },
  {
    "text": "DynamoDB streams streams all that data to another lambda that does a Gration so",
    "start": "1739649",
    "end": "1745200"
  },
  {
    "text": "say you want leaderboards like you know who are the top players at that given moment and you can surface that through",
    "start": "1745200",
    "end": "1751159"
  },
  {
    "text": "throw that into an s3 bucket and service it through cloud front or a lot of different ways so having real-time",
    "start": "1751159",
    "end": "1763470"
  },
  {
    "start": "1761000",
    "end": "1926000"
  },
  {
    "text": "analyst brings a lot of stake and dimension to a lot of businesses that's why you example it's built on user",
    "start": "1763470",
    "end": "1769590"
  },
  {
    "text": "interactions so have like basically showing users a visual representation or",
    "start": "1769590",
    "end": "1775590"
  },
  {
    "text": "something in in response to what they're doing is really good motivation and captures a progress for example or",
    "start": "1775590",
    "end": "1782940"
  },
  {
    "text": "social competitiveness so kinda wanted to open this up to questions because I",
    "start": "1782940",
    "end": "1789210"
  },
  {
    "text": "feel like then we can dive into the nitty-gritty of if anybody has their use",
    "start": "1789210",
    "end": "1795869"
  },
  {
    "text": "case",
    "start": "1795869",
    "end": "1798049"
  },
  {
    "text": "okay so the question was best practices on throwing things into Kinesis",
    "start": "1813770",
    "end": "1819190"
  },
  {
    "text": "especially through API gateway",
    "start": "1819190",
    "end": "1823509"
  },
  {
    "text": "yeah generally I guess it's the limits are per shard so I guess it depends on",
    "start": "1843809",
    "end": "1849210"
  },
  {
    "text": "you know I guess it's unfortunate if you have bursty traffic and it's unpredictable generally they try to like",
    "start": "1849210",
    "end": "1855629"
  },
  {
    "text": "you know increase shards if you think the throughput is going to be high otherwise you do kind of need your own sort of batching or buffering in front",
    "start": "1855629",
    "end": "1861929"
  },
  {
    "text": "of Kinesis if you don't want to hit those throttles or if you don't want to do your own retry logic that's sort of",
    "start": "1861929",
    "end": "1866940"
  },
  {
    "text": "the advantage of some of the other managed stream services like fire hose",
    "start": "1866940",
    "end": "1872460"
  },
  {
    "text": "because they manage you know increase at the elastic sharding of it if you have your own Kinesis there's i am ii and i'm",
    "start": "1872460",
    "end": "1880859"
  },
  {
    "text": "i think it's there's like you know blogs and best practices I don't I don't know if I have the best advice on like how to",
    "start": "1880859",
    "end": "1887519"
  },
  {
    "text": "produce data to Kinesis but",
    "start": "1887519",
    "end": "1892039"
  },
  {
    "start": "1926000",
    "end": "2026000"
  },
  {
    "text": "okay so the question was um how what are some best practices around error handling and detection I guess and",
    "start": "1926539",
    "end": "1935179"
  },
  {
    "text": "having your when your batch fails I know like something that customers a lot of times ask is you know on on the retry",
    "start": "1935179",
    "end": "1941570"
  },
  {
    "text": "logic having more ability to tune it so that's definitely something we're aware of and and and working on in terms of",
    "start": "1941570",
    "end": "1952869"
  },
  {
    "text": "you know that try not to be redundant that's a little bit tough because you're gonna have to",
    "start": "1952869",
    "end": "1959089"
  },
  {
    "text": "keep state on where you have processed where something has already been processed and you don't want to do it",
    "start": "1959089",
    "end": "1964219"
  },
  {
    "text": "again and keep item potency I guess when it comes to streams and say for",
    "start": "1964219",
    "end": "1969409"
  },
  {
    "text": "asynchronous and somebody else manages the retry logic for you it's hmm",
    "start": "1969409",
    "end": "1975049"
  },
  {
    "text": "sort of just part of the the service where you have to be idempotent you can",
    "start": "1975049",
    "end": "1981409"
  },
  {
    "text": "definitely log where you are in your in the lambda function itself to help you",
    "start": "1981409",
    "end": "1987799"
  },
  {
    "text": "sort of tell where you've done before so if you need to be popping the process you can avoid records that you've",
    "start": "1987799",
    "end": "1993379"
  },
  {
    "text": "already processed or give yourself some flag lamda also has environment",
    "start": "1993379",
    "end": "1999859"
  },
  {
    "text": "variables that you might be able to take advantage of like to sort of pass along metadata if you sort of are aware",
    "start": "1999859",
    "end": "2004929"
  },
  {
    "text": "outside of the system how processing is doing so there's some options they might",
    "start": "2004929",
    "end": "2010719"
  },
  {
    "text": "are looking to",
    "start": "2010719",
    "end": "2013229"
  },
  {
    "text": "is there a way to maintain order within shards so you can sort of define your",
    "start": "2016500",
    "end": "2028450"
  },
  {
    "start": "2026000",
    "end": "2135000"
  },
  {
    "text": "own order I guess I mean it's not going to be the order in which the lambda function itself is executed or when it",
    "start": "2028450",
    "end": "2035890"
  },
  {
    "text": "gets to the container but you could potentially again keep track of metadata outside of the architecture and like if",
    "start": "2035890",
    "end": "2044799"
  },
  {
    "text": "you're a Czech Pony against DynamoDB then you sort of might have expectations of you know where you should be and",
    "start": "2044799",
    "end": "2050500"
  },
  {
    "text": "ignore data or sort of work around it that way again you sort of have to write",
    "start": "2050500",
    "end": "2056618"
  },
  {
    "text": "into your code the ability to be idempotent or to that sort of way but",
    "start": "2056619",
    "end": "2061628"
  },
  {
    "text": "the lambdas themselves won't be able to get the batches in order you'll just",
    "start": "2061629",
    "end": "2066638"
  },
  {
    "text": "have to decide business logic wise what you do with the data when you get it in order to preserve extra order that make",
    "start": "2066639",
    "end": "2073388"
  },
  {
    "text": "sense",
    "start": "2073389",
    "end": "2075598"
  },
  {
    "text": "nice",
    "start": "2081630",
    "end": "2084629"
  },
  {
    "text": "mm-hmm",
    "start": "2092810",
    "end": "2095710"
  },
  {
    "start": "2135000",
    "end": "2209000"
  },
  {
    "text": "okay so the question was around the if I got this right the latency between when you put a record into the stream and",
    "start": "2135100",
    "end": "2141620"
  },
  {
    "text": "when it gets processed so yeah I you",
    "start": "2141620",
    "end": "2146780"
  },
  {
    "text": "know fan out is not going to help there in this case because this is all internal to our polar architecture",
    "start": "2146780",
    "end": "2153310"
  },
  {
    "text": "that's definitely something we are aware of this is kind of like the compromise that you have between you know we're",
    "start": "2153310",
    "end": "2160100"
  },
  {
    "text": "doing this polling for you so when you have no activity you know we've you know in order to be",
    "start": "2160100",
    "end": "2165310"
  },
  {
    "text": "has better utilization we can't just be like console me polling that's sort of one constraint that we have but we are",
    "start": "2165310",
    "end": "2171110"
  },
  {
    "text": "definitely looking at ways to optimize this you know like right now it's one",
    "start": "2171110",
    "end": "2177080"
  },
  {
    "text": "second and you know these there's a lot of like knobs in tunes that we're trying to optimize but definitely something",
    "start": "2177080",
    "end": "2182420"
  },
  {
    "text": "we're aware of getting that latency between when you put the record into the stream and when lambda executes yeah",
    "start": "2182420",
    "end": "2192130"
  },
  {
    "text": "so can you repeat that question oh okay",
    "start": "2202280",
    "end": "2210420"
  },
  {
    "start": "2209000",
    "end": "2273000"
  },
  {
    "text": "the cost comparison between lambda and a dedicated polling a system to pull records from a stream or consume okay I",
    "start": "2210420",
    "end": "2219150"
  },
  {
    "text": "would say the cost is really great because you are not really paying for us to do this for you you're paying for lambda to run the",
    "start": "2219150",
    "end": "2225510"
  },
  {
    "text": "function versus if you have an ec2 instance probably doing the polling you're paying for the ec2 instance it's",
    "start": "2225510",
    "end": "2233730"
  },
  {
    "text": "that sort of answer the question you're not paying for the polling if that was the question yeah okay I think some in",
    "start": "2233730",
    "end": "2241740"
  },
  {
    "text": "the back I can't see",
    "start": "2241740",
    "end": "2245330"
  },
  {
    "start": "2273000",
    "end": "2308000"
  },
  {
    "text": "yeah I was so I got the last part allocated versus use memory that isn't impacted to you like many other eight",
    "start": "2274180",
    "end": "2280750"
  },
  {
    "text": "abyss services you pay for what you use so even if you've configured you know a",
    "start": "2280750",
    "end": "2286270"
  },
  {
    "text": "large amount if if you use a smaller amount if you only use like you know ten",
    "start": "2286270",
    "end": "2292089"
  },
  {
    "text": "seconds in your timeout is three minutes you won't be charged for three minutes or anything like that and then I think your what was your first question again",
    "start": "2292089",
    "end": "2300808"
  },
  {
    "text": "oh okay so I think you're talking about container reuse so you have all these",
    "start": "2307869",
    "end": "2314410"
  },
  {
    "start": "2308000",
    "end": "2346000"
  },
  {
    "text": "partitions really what a shot guarantees for you is that everything in that shard is going to be an ordered that's all it",
    "start": "2314410",
    "end": "2320980"
  },
  {
    "text": "does you know for example if you wanted all of one accounts data to be processed",
    "start": "2320980",
    "end": "2328509"
  },
  {
    "text": "in order you would shard it all to the you would have the partition key or hash it to the same charge it doesn't",
    "start": "2328509",
    "end": "2334450"
  },
  {
    "text": "guarantee that it's all going to go to the same container so as soon as we do the synchronous invoke to lambda all of",
    "start": "2334450",
    "end": "2341259"
  },
  {
    "text": "that is subject to lambdas back-end internal sort of container we use life",
    "start": "2341259",
    "end": "2347109"
  },
  {
    "start": "2346000",
    "end": "2518000"
  },
  {
    "text": "cycle I don't know if are you familiar with the life cycle of okay so so just",
    "start": "2347109",
    "end": "2354069"
  },
  {
    "text": "real quick you know there's the concept of warm and cold containers when it's a new invocation and you haven't had one",
    "start": "2354069",
    "end": "2359740"
  },
  {
    "text": "for a while you don't have a warm one we're gonna spin up a new container and that's going to have its initialization",
    "start": "2359740",
    "end": "2364779"
  },
  {
    "text": "costs if it's a warm container it might have some of the same resources that a previous invocation had so you do save a",
    "start": "2364779",
    "end": "2371890"
  },
  {
    "text": "lot of initialization all of that is sort of like our own proprietary logic",
    "start": "2371890",
    "end": "2377799"
  },
  {
    "text": "and orchestration logic so you don't have insight into that and it's not guaranteed we are trying to optimize that in the back end ourselves as much",
    "start": "2377799",
    "end": "2384519"
  },
  {
    "text": "as we can and like you know every where like we're constantly improving improving that but you I wouldn't there's no guarantee that in the same",
    "start": "2384519",
    "end": "2391839"
  },
  {
    "text": "try that you always go to the same container",
    "start": "2391839",
    "end": "2395249"
  },
  {
    "text": "so in general like yeah we tried to warn if you have really tight latency constraints try not to have and this",
    "start": "2425400",
    "end": "2432160"
  },
  {
    "text": "might be tough but try not to have super sparse data they're super sparse traffic again this is all sort of like to do",
    "start": "2432160",
    "end": "2438040"
  },
  {
    "text": "with how warm things are and how and and like how we're trying to mitigate",
    "start": "2438040",
    "end": "2443920"
  },
  {
    "text": "against utilization so you know if",
    "start": "2443920",
    "end": "2449349"
  },
  {
    "text": "there's a way for you to control like when the records are inserted or have",
    "start": "2449349",
    "end": "2455109"
  },
  {
    "text": "some sort of buffer I guess this kind of goes into like what are best practices for inserting data into the Kinesis stream again yeah it",
    "start": "2455109",
    "end": "2462099"
  },
  {
    "text": "might be useful to you know have some sort of buffer in front of it to try to like have it be more spread even in that",
    "start": "2462099",
    "end": "2467500"
  },
  {
    "text": "case if you're very very late in the sensitive",
    "start": "2467500",
    "end": "2471240"
  },
  {
    "start": "2518000",
    "end": "2601000"
  },
  {
    "text": "hmm no that's that's a fair point yeah fan out if it means that you've reduced",
    "start": "2518460",
    "end": "2523960"
  },
  {
    "text": "the duration of your lambda function will definitely help throughput so yes if you have like you know a fan-out",
    "start": "2523960",
    "end": "2529780"
  },
  {
    "text": "function that's like you know very short and then it hits off other and detergents longer duration that will definitely help weather if we will ever",
    "start": "2529780",
    "end": "2537430"
  },
  {
    "text": "have do that for you I think was the question I'm not going to say no also I can't make promises but we're definitely",
    "start": "2537430",
    "end": "2544030"
  },
  {
    "text": "aware of the issue so yeah something",
    "start": "2544030",
    "end": "2549040"
  },
  {
    "text": "definitely no man's I think somewhere in the front near the question",
    "start": "2549040",
    "end": "2554010"
  },
  {
    "text": "yeah yeah no I think we're aware of the",
    "start": "2575829",
    "end": "2583219"
  },
  {
    "text": "issue and I think it's uh you know yeah just there's a path forward there yeah",
    "start": "2583219",
    "end": "2591640"
  },
  {
    "text": "okay the question is how many consumers can you attach to a single shard before you start getting throttled I think that",
    "start": "2600890",
    "end": "2607860"
  },
  {
    "start": "2601000",
    "end": "2674000"
  },
  {
    "text": "control plane limit for a shard is like five per second I don't know if anybody",
    "start": "2607860",
    "end": "2613320"
  },
  {
    "text": "else remembers so it's not sort of like how many consumers it's how often if you",
    "start": "2613320",
    "end": "2620670"
  },
  {
    "text": "have like you know ten consumers and they only call pull every 20 minutes and",
    "start": "2620670",
    "end": "2626970"
  },
  {
    "text": "like maybe it'll be okay I made those numbers up but if you have like two and they're constantly pulling then they're",
    "start": "2626970",
    "end": "2632310"
  },
  {
    "text": "gonna get muddled",
    "start": "2632310",
    "end": "2634970"
  },
  {
    "text": "yeah so I mean definitely I think there's a lot of we hear a lot of customers asking for this because it's",
    "start": "2648820",
    "end": "2654640"
  },
  {
    "text": "like a really great option to have multiple consumers from the same stream",
    "start": "2654640",
    "end": "2660880"
  },
  {
    "text": "you have like a you know source of truth or a data that lots of people can sort of take advantage of except for this",
    "start": "2660880",
    "end": "2667060"
  },
  {
    "text": "limit so it's definitely on our radar and Kinesis radar there are some our Co",
    "start": "2667060",
    "end": "2675250"
  },
  {
    "start": "2674000",
    "end": "2725000"
  },
  {
    "text": "textures like kind of around it like you can fan out the Kinesis itself like you",
    "start": "2675250",
    "end": "2680760"
  },
  {
    "text": "for example have a Kinesis stream that basically just fans out to multiple other streams and it's just a",
    "start": "2680760",
    "end": "2686950"
  },
  {
    "text": "replication of the data but it makes it available for more consumers to read",
    "start": "2686950",
    "end": "2692200"
  },
  {
    "text": "from so we've definitely had there may be blog posts on that but we have customers doing that but on the other",
    "start": "2692200",
    "end": "2699340"
  },
  {
    "text": "case it is something we're definitely aware of and also trying to improve that experience for sure okay",
    "start": "2699340",
    "end": "2708280"
  },
  {
    "text": "real quick",
    "start": "2708280",
    "end": "2711360"
  },
  {
    "start": "2725000",
    "end": "2751000"
  },
  {
    "text": "lambdas from the on the consumer side so any any retries is owned by whatever the",
    "start": "2726199",
    "end": "2732479"
  },
  {
    "text": "managed service is that's consuming it in this case lambda owns that",
    "start": "2732479",
    "end": "2737689"
  },
  {
    "text": "so a Kinesis doesn't invoke anything Kinesis just makes data available for consumption yeah",
    "start": "2751869",
    "end": "2758300"
  },
  {
    "text": "lambda grabs data does summon memory processing throws it for synchronous invocation if there's any failures or",
    "start": "2758300",
    "end": "2764900"
  },
  {
    "text": "throttles lambda retries",
    "start": "2764900",
    "end": "2768460"
  },
  {
    "text": "yeah that's on empty so if we grabbed you got records and we didn't get any records back we'll wait one second and",
    "start": "2785230",
    "end": "2791890"
  },
  {
    "text": "try again so",
    "start": "2791890",
    "end": "2795390"
  },
  {
    "text": "so I'm not I didn't understand that question",
    "start": "2820800",
    "end": "2825300"
  },
  {
    "text": "right",
    "start": "2832000",
    "end": "2834960"
  },
  {
    "text": "yes that is correct if you have a three shard stream and lambda is consuming from it you wouldn't have more than",
    "start": "2838280",
    "end": "2845270"
  },
  {
    "text": "three concurrent lambda execution was running at the same time no you would",
    "start": "2845270",
    "end": "2855350"
  },
  {
    "text": "never have two lambdas consumed from the same shot yeah okay I think yes yeah yes",
    "start": "2855350",
    "end": "2867849"
  },
  {
    "start": "2877000",
    "end": "2906000"
  },
  {
    "text": "yeah so you get the iterator age so the question was kind of more insight into",
    "start": "2877980",
    "end": "2883570"
  },
  {
    "text": "how the iterator age is computed on the lambda side you get two iterator edge age metrics one from Kinesis one from",
    "start": "2883570",
    "end": "2888850"
  },
  {
    "text": "lambda as far as Kinesis viewers it's the average or they aggregated across",
    "start": "2888850",
    "end": "2894490"
  },
  {
    "text": "all the consumers for that stream so as lambda is concerned it's any stream that's hooked up to that lambda so if",
    "start": "2894490",
    "end": "2900400"
  },
  {
    "text": "you have multiple streams hooked up to that lambda the data might also get kind of weird you so if it's bars I mean the",
    "start": "2900400",
    "end": "2910359"
  },
  {
    "start": "2906000",
    "end": "3061000"
  },
  {
    "text": "way that we calculated is the last record that each shard saw would be one",
    "start": "2910359",
    "end": "2916900"
  },
  {
    "text": "data point and then so when you select you know max or min or average it is using those data points to create it and",
    "start": "2916900",
    "end": "2925450"
  },
  {
    "text": "this some discrepancies that you might see between shards might be you know if",
    "start": "2925450",
    "end": "2930850"
  },
  {
    "text": "you have differences it's like you know a specific customer if you're for example partitioning to a certain shard might be having trouble versus the type",
    "start": "2930850",
    "end": "2937750"
  },
  {
    "text": "of data that's going to another shard and so one particular shot might be actually having more trouble than another based off of the data that's",
    "start": "2937750",
    "end": "2943540"
  },
  {
    "text": "going to it and how lambda is reacting to it if it's getting errors specifically oh yeah so there's any more",
    "start": "2943540",
    "end": "2953350"
  },
  {
    "text": "questions yep",
    "start": "2953350",
    "end": "2960830"
  },
  {
    "text": "[Music]",
    "start": "2968440",
    "end": "2971569"
  },
  {
    "text": "so in this case I had a really simple sort of design here the ADA of is lambda",
    "start": "3006130",
    "end": "3013730"
  },
  {
    "text": "was kind of just dumping into DynamoDB so I wasn't doing pre aggregation at that point I could see I so I think the",
    "start": "3013730",
    "end": "3019370"
  },
  {
    "text": "question was a concern on like if you're doing aggregation at different stages sort of have that manifests and the end",
    "start": "3019370",
    "end": "3024830"
  },
  {
    "text": "result definitely that's a concern you have to really think about what you're trying to convey in my case I kind of",
    "start": "3024830",
    "end": "3032120"
  },
  {
    "text": "just had lambda throwing player and a score player and a score in to dynamodb just a log and and",
    "start": "3032120",
    "end": "3038930"
  },
  {
    "text": "then my second lambda function was the one actually doing all the aggregation",
    "start": "3038930",
    "end": "3044620"
  },
  {
    "text": "yeah you can so you know real some really like really good stuff to batch as a sums because it wouldn't matter at",
    "start": "3061699",
    "end": "3068459"
  },
  {
    "text": "what point in the process you did the sum so if you're just looking for top where max then you can just do sums in",
    "start": "3068459",
    "end": "3074819"
  },
  {
    "text": "the first lambda who went to dynamo do another sum in the second lambda and then you would get the same and resolved",
    "start": "3074819",
    "end": "3082369"
  },
  {
    "text": "so lambda function is your code and you can do whatever you like with it so if you want to okay it just a lambda",
    "start": "3087380",
    "end": "3099449"
  },
  {
    "text": "consume from DynamoDB who is there another",
    "start": "3099449",
    "end": "3103699"
  },
  {
    "text": "I'm not really sure I know there's you know you might want to check out something like a dubious glue for a",
    "start": "3110800",
    "end": "3117530"
  },
  {
    "text": "different ETL workflows or they kind of go more into how you can have a managed",
    "start": "3117530",
    "end": "3124850"
  },
  {
    "text": "scheduled query against your storage so maybe take a look at ATS glue for that",
    "start": "3124850",
    "end": "3131200"
  },
  {
    "text": "and then um sorry",
    "start": "3131200",
    "end": "3136390"
  },
  {
    "start": "3136000",
    "end": "3390000"
  },
  {
    "text": "so real quick I just wanted to show example data so in general",
    "start": "3136720",
    "end": "3147040"
  },
  {
    "text": "I'm gonna try to make this bigger",
    "start": "3161520",
    "end": "3165450"
  },
  {
    "text": "hmm okay I don't know if you can see this but kind of in general when you put",
    "start": "3169270",
    "end": "3179780"
  },
  {
    "text": "a record it just looks like this you throw data in I did a command so I can",
    "start": "3179780",
    "end": "3186170"
  },
  {
    "text": "only look at it from here I set up a Twitter stream this morning where you",
    "start": "3186170",
    "end": "3191630"
  },
  {
    "text": "can basically get a random sample of a random sample sample of tweets this is",
    "start": "3191630",
    "end": "3199490"
  },
  {
    "text": "sort of what it looks like don't look at my secret keys but I mean it just looks",
    "start": "3199490",
    "end": "3211070"
  },
  {
    "text": "like this it's I don't know if you can see that I can't even see it but it's",
    "start": "3211070",
    "end": "3216950"
  },
  {
    "text": "just statuses sample JSON you do a query you get back like a dump of data and I",
    "start": "3216950",
    "end": "3222830"
  },
  {
    "text": "can show you quickly what that looks like",
    "start": "3222830",
    "end": "3226900"
  },
  {
    "text": "so it kind of looks like this this is",
    "start": "3242310",
    "end": "3248590"
  },
  {
    "text": "the response that you get you just get a every line as a jump of random tweets in",
    "start": "3248590",
    "end": "3255010"
  },
  {
    "text": "particular I'm interested in the CSV the column that says text because that that's natural text I'm not responsible",
    "start": "3255010",
    "end": "3262570"
  },
  {
    "text": "for any of this content so I don't know what it actually says and so yeah when",
    "start": "3262570",
    "end": "3269830"
  },
  {
    "text": "you call this API you open up a stream it's constantly dumping data and and I just kind of did this to try it but I",
    "start": "3269830",
    "end": "3277390"
  },
  {
    "text": "you know try to pipe it into something that could send whoops data to into a",
    "start": "3277390",
    "end": "3288760"
  },
  {
    "text": "Kinesis stream so again oil the keys I'm",
    "start": "3288760",
    "end": "3295030"
  },
  {
    "text": "gonna destroy this later but in this case you I just curled this this API is",
    "start": "3295030",
    "end": "3303190"
  },
  {
    "text": "just a get on statuses sample and you just constantly grab a bunch of data and",
    "start": "3303190",
    "end": "3310890"
  },
  {
    "text": "I I'm only I did some janky grapping and",
    "start": "3310890",
    "end": "3316660"
  },
  {
    "text": "stuff to get the just the text that I want and throw it into a stream I called",
    "start": "3316660",
    "end": "3321850"
  },
  {
    "text": "it demo and I can show you sort of like what the end result looks like on the",
    "start": "3321850",
    "end": "3327310"
  },
  {
    "text": "lambda side",
    "start": "3327310",
    "end": "3329850"
  },
  {
    "text": "and this whole screen thing is confusing",
    "start": "3349200",
    "end": "3353670"
  },
  {
    "text": "okay I wouldn't have tried to close this and restart it",
    "start": "3360810",
    "end": "3366980"
  },
  {
    "text": "so here's the console sort of the",
    "start": "3379880",
    "end": "3391770"
  },
  {
    "start": "3390000",
    "end": "3453000"
  },
  {
    "text": "console and you can go into the monitoring tab and take a look at your vacation errors and here we have",
    "start": "3391770",
    "end": "3398790"
  },
  {
    "text": "iterator age where the last record and how old it was you can go to also",
    "start": "3398790",
    "end": "3409740"
  },
  {
    "text": "directly into cloud watch logs and like dig into sort of the metrics over there",
    "start": "3409740",
    "end": "3414840"
  },
  {
    "text": "we can add alarms cloud watch alarms on any errors or throttles that you see you",
    "start": "3414840",
    "end": "3421770"
  },
  {
    "text": "can also go into a cloud watch logs to",
    "start": "3421770",
    "end": "3426810"
  },
  {
    "text": "see any air the actual error outputs so this is what the cloud was so the log",
    "start": "3426810",
    "end": "3433290"
  },
  {
    "text": "stream looks like and it here I'm dumping out the actual payload and and",
    "start": "3433290",
    "end": "3438980"
  },
  {
    "text": "the resulting yeah so",
    "start": "3438980",
    "end": "3447080"
  },
  {
    "text": "so if you have any errors this will be like you can also set different watches on the cloud much logs and definitely",
    "start": "3452960",
    "end": "3459570"
  },
  {
    "start": "3453000",
    "end": "3511000"
  },
  {
    "text": "monitor this for any insight into the details of your stream processing and",
    "start": "3459570",
    "end": "3465180"
  },
  {
    "text": "the result is we're dumping data into dynamodb so this is sort of what it ends",
    "start": "3465180",
    "end": "3471630"
  },
  {
    "text": "up looking like you have a random player ID in the score and I just grab random tweets and threw that in there I don't know if there was like a",
    "start": "3471630",
    "end": "3479580"
  },
  {
    "text": "question specifically on this but a type of monitoring yes",
    "start": "3479580",
    "end": "3490940"
  },
  {
    "start": "3511000",
    "end": "3560000"
  },
  {
    "text": "yep yeah and when you create your lambda function you can go into these triggers",
    "start": "3511560",
    "end": "3520950"
  },
  {
    "text": "to sort of this is on the console where you can visually set up the trigger here I have the Kinesis demo stream but if",
    "start": "3520950",
    "end": "3529560"
  },
  {
    "text": "you wanted to add another one you can do so you can choose from various different",
    "start": "3529560",
    "end": "3539280"
  },
  {
    "text": "sources kinesins again and this is where",
    "start": "3539280",
    "end": "3546630"
  },
  {
    "text": "you set up the configuration like back size or the the role and the starting position and stuff like that yeah",
    "start": "3546630",
    "end": "3562290"
  },
  {
    "start": "3560000",
    "end": "3600000"
  },
  {
    "text": "another important thing to note you know when you set up the stream I mean you'll hit this right away is making sure that",
    "start": "3562290",
    "end": "3568230"
  },
  {
    "text": "your I am role has the right permissions to read from the from your stream and also to describe stream so all of this",
    "start": "3568230",
    "end": "3576450"
  },
  {
    "text": "is in manage roles when you go into I am and you create a role you can look at the manage policies and type in Kinesis",
    "start": "3576450",
    "end": "3582840"
  },
  {
    "text": "and it will pop up with the with a role that's probably best for you use yes",
    "start": "3582840",
    "end": "3592610"
  },
  {
    "text": "[Music]",
    "start": "3594070",
    "end": "3597239"
  },
  {
    "text": "[Applause]",
    "start": "3603370",
    "end": "3606520"
  },
  {
    "text": "[Applause] so it was the question a very large",
    "start": "3611630",
    "end": "3618160"
  },
  {
    "text": "lambda function okay so if you're having",
    "start": "3618160",
    "end": "3627730"
  },
  {
    "text": "trouble with very large lambda zips there is something that might help you",
    "start": "3627730",
    "end": "3633430"
  },
  {
    "text": "watch out for it on Thursday",
    "start": "3633430",
    "end": "3636960"
  },
  {
    "text": "yeah it's definitely you know a constant battle like what do we include what do",
    "start": "3675350",
    "end": "3681110"
  },
  {
    "text": "we support forever maybe you know versus like the flexibility that we give for",
    "start": "3681110",
    "end": "3686390"
  },
  {
    "text": "customers and more the guarantees that we you know we're never gonna break customers depending because all of us",
    "start": "3686390",
    "end": "3692360"
  },
  {
    "text": "these libraries and packages are constantly updating right so yeah but that's it that's an interesting point",
    "start": "3692360",
    "end": "3700060"
  },
  {
    "text": "yeah I think I'm done here or like it",
    "start": "3700060",
    "end": "3705110"
  },
  {
    "text": "the time looks over so thank you that's my Twitter handle I don't use it for",
    "start": "3705110",
    "end": "3711890"
  },
  {
    "text": "that much but mostly complaining on airlines and thank you and don't forget",
    "start": "3711890",
    "end": "3732650"
  },
  {
    "text": "to fill out the survey",
    "start": "3732650",
    "end": "3735640"
  }
]