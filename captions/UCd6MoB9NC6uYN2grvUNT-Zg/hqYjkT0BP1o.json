[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "cuz I I can hear myself so that's good hello everyone how are you my name is",
    "start": "860",
    "end": "6600"
  },
  {
    "text": "Dominik Divakar Unni this is a session for Amazon Elastic inference session ID",
    "start": "6600",
    "end": "12990"
  },
  {
    "text": "is a im3 66 just to make sure you're in the right place I'm Dominic deevak",
    "start": "12990",
    "end": "18990"
  },
  {
    "text": "Rooney I'm a product manager at AWS I've worked on elastic inference I'm joined",
    "start": "18990",
    "end": "24600"
  },
  {
    "text": "here by my colleague sudipta Sengupta and our customers from Autodesk Peter",
    "start": "24600",
    "end": "30420"
  },
  {
    "text": "and Livia and we are here to talk to you about Amazon elastic inference first to",
    "start": "30420",
    "end": "39690"
  },
  {
    "start": "36000",
    "end": "72000"
  },
  {
    "text": "get started I'd like to spend some time analyzing one of the main pain points we face and running deep learning workloads",
    "start": "39690",
    "end": "46550"
  },
  {
    "text": "next I'd like to share what we've brought to the table to help you address some of these issues as you add",
    "start": "46550",
    "end": "52739"
  },
  {
    "text": "intelligence to your applications and scale these workloads and last but not least I'm thrilled to have our customer",
    "start": "52739",
    "end": "59100"
  },
  {
    "text": "Autodesk here to share some of the ways that they're incorporating machine",
    "start": "59100",
    "end": "64228"
  },
  {
    "text": "learning into their applications and how they're using AI to deploy these workloads efficiently and at scale so",
    "start": "64229",
    "end": "72920"
  },
  {
    "start": "72000",
    "end": "145000"
  },
  {
    "text": "first to take a step back and look at why this is important we realize that",
    "start": "74270",
    "end": "81570"
  },
  {
    "text": "today more than at any time in the past it's key for us to keep transforming our",
    "start": "81570",
    "end": "87570"
  },
  {
    "text": "businesses and our customer experience in order to remain competitive if you look at what's happened over the past",
    "start": "87570",
    "end": "94020"
  },
  {
    "text": "few years if you look at how some of these long-standing businesses have been",
    "start": "94020",
    "end": "99270"
  },
  {
    "text": "completely disrupted by new players in a very short time who've used machine learning on top of AWS to do so it's",
    "start": "99270",
    "end": "106860"
  },
  {
    "text": "pretty interesting if you see what Pinterest has done in visual search Airbnb has done in transportation I'm",
    "start": "106860",
    "end": "114689"
  },
  {
    "text": "sorry hotels what lyft has done in transportation and all these companies have done this using machine learning on",
    "start": "114689",
    "end": "121439"
  },
  {
    "text": "top of AWS and have radically changed industries that have either existed for a very long time or that didn't exist",
    "start": "121439",
    "end": "127920"
  },
  {
    "text": "before so today you have to be able to move fast and be very agile",
    "start": "127920",
    "end": "133500"
  },
  {
    "text": "and have the cost structure in this environment to be competitive so our focus is to provide our customers with",
    "start": "133500",
    "end": "141360"
  },
  {
    "text": "the lowest cost infrastructure to enable this transformation now if you look at",
    "start": "141360",
    "end": "149040"
  },
  {
    "start": "145000",
    "end": "214000"
  },
  {
    "text": "the costs associated with running machine learning applications we can divide them into two large categories",
    "start": "149040",
    "end": "155030"
  },
  {
    "text": "ones training training models and then making predictions with them in production known as inference we're",
    "start": "155030",
    "end": "162930"
  },
  {
    "text": "constantly working to make models train faster and cheaper earlier this week we",
    "start": "162930",
    "end": "169050"
  },
  {
    "text": "announced the p3 d n instances with larger hundred gig capacity network to",
    "start": "169050",
    "end": "175860"
  },
  {
    "text": "train models faster and yesterday Andi announced enhancements to tensorflow that improved GPU utilization to enable",
    "start": "175860",
    "end": "183900"
  },
  {
    "text": "training of imagenet from 30 minutes to 14 minutes which is the fastest you can",
    "start": "183900",
    "end": "189570"
  },
  {
    "text": "do today but as many of you know training is a small part of the cost",
    "start": "189570",
    "end": "195030"
  },
  {
    "text": "equation for an application at the speed at which you can train applications",
    "start": "195030",
    "end": "200070"
  },
  {
    "text": "today it's inference that actually takes up the vast majority of the cost of running your application since",
    "start": "200070",
    "end": "206130"
  },
  {
    "text": "applications are running 24/7 you often need the infrastructure to support it also running 24/7 and that can get very",
    "start": "206130",
    "end": "212790"
  },
  {
    "text": "costly so if you look at the",
    "start": "212790",
    "end": "219090"
  },
  {
    "start": "214000",
    "end": "232000"
  },
  {
    "text": "infrastructure that's available to support these applications that support",
    "start": "219090",
    "end": "224700"
  },
  {
    "text": "the applications that make up the 90% of the cost right the applications the inference applications we see a number",
    "start": "224700",
    "end": "230459"
  },
  {
    "text": "of inefficiencies first when you have to use GPUs for inference stand-alone GPU",
    "start": "230459",
    "end": "238890"
  },
  {
    "start": "232000",
    "end": "271000"
  },
  {
    "text": "instances are expensive and they're purpose-built for training and because of that they're oversized for inference",
    "start": "238890",
    "end": "244860"
  },
  {
    "text": "as inference happens to be a very different workload while training jobs",
    "start": "244860",
    "end": "250290"
  },
  {
    "text": "can process hundreds of data samples batch together in parallel very",
    "start": "250290",
    "end": "256709"
  },
  {
    "text": "efficiently most inference workloads only act on a single data sample at a",
    "start": "256709",
    "end": "261720"
  },
  {
    "text": "time and so a large portion of the GPU goes underutilized for inference workloads",
    "start": "261720",
    "end": "267030"
  },
  {
    "text": "especially on line in front for clothes ii models need very different amounts of",
    "start": "267030",
    "end": "274410"
  },
  {
    "start": "271000",
    "end": "358000"
  },
  {
    "text": "GPU CPU and memory depending on the type of application you may need very little",
    "start": "274410",
    "end": "280650"
  },
  {
    "text": "cpu x86 cpu and memory may need higher",
    "start": "280650",
    "end": "286350"
  },
  {
    "text": "amounts of GPU for some language models for instance as we see you need higher amounts of CPU and memory very little",
    "start": "286350",
    "end": "294450"
  },
  {
    "text": "GPU so how do you mix and match if you size resources for the most demanding",
    "start": "294450",
    "end": "302250"
  },
  {
    "text": "resource you pick a GPU instance and size it for that you often end up under utilizing the other resources so we see",
    "start": "302250",
    "end": "308940"
  },
  {
    "text": "these two areas of inefficiency that we want to help tackle for ourselves we want to help tackle for our customers and you when you dive a little bit",
    "start": "308940",
    "end": "317850"
  },
  {
    "text": "closer and look at the the inference workloads and you see how much capaz de",
    "start": "317850",
    "end": "326550"
  },
  {
    "text": "drive out of the GPU you'll see that for smaller back sizes this is an example of",
    "start": "326550",
    "end": "332070"
  },
  {
    "text": "an inception v3 model running on a 3-2 XL instance using a voltage GPU you will",
    "start": "332070",
    "end": "340200"
  },
  {
    "text": "see that for batch size one you can drive about a hundred images per second",
    "start": "340200",
    "end": "346080"
  },
  {
    "text": "out of this GPU and this isn't using concerti this is just standard framework",
    "start": "346080",
    "end": "352430"
  },
  {
    "text": "runtime running on an instance but for larger batch sizes you have for a batch",
    "start": "352430",
    "end": "361260"
  },
  {
    "start": "358000",
    "end": "393000"
  },
  {
    "text": "size of 64 for instance you can generate up to upwards of a thousand samples so",
    "start": "361260",
    "end": "367080"
  },
  {
    "text": "what that means is if you're applying a p3 to excel for inference your underutilizing",
    "start": "367080",
    "end": "372780"
  },
  {
    "text": "about 90% of the GPU for smaller batch sizes so as you deploy your application",
    "start": "372780",
    "end": "379350"
  },
  {
    "text": "at scale if you have 20 instances + / AZ",
    "start": "379350",
    "end": "384560"
  },
  {
    "text": "and your application deployed across a couple regions as you can see the the",
    "start": "384560",
    "end": "390270"
  },
  {
    "text": "underutilization can be expensive and so one of the approaches we see customers",
    "start": "390270",
    "end": "396750"
  },
  {
    "start": "393000",
    "end": "423000"
  },
  {
    "text": "take to solve this problem is to run more concurrent sessions on a pea three to excel and what we found is that",
    "start": "396750",
    "end": "404279"
  },
  {
    "text": "it doesn't easily solve the problem right you'll have to do some very",
    "start": "404279",
    "end": "409909"
  },
  {
    "text": "complicated and unique things in order to get better utilization out of the GPU in fact if you run more concurrent",
    "start": "409909",
    "end": "416729"
  },
  {
    "text": "sessions we've seen the throughput remain flat and they all kind of time share the utilization of the GPU and so",
    "start": "416729",
    "end": "425399"
  },
  {
    "start": "423000",
    "end": "469000"
  },
  {
    "text": "if we and then look from a different angle of which GP you choose do you",
    "start": "425399",
    "end": "431159"
  },
  {
    "text": "choose a P to excel which we launched a couple years ago or a p3 to excel which we long to at last reinvent you'll see",
    "start": "431159",
    "end": "439079"
  },
  {
    "text": "that customers tend to choose the p3 p2 Excel for bat size one inference as it's",
    "start": "439079",
    "end": "446429"
  },
  {
    "text": "a lot more cost effective on a cost per inference basis than a p3 to excel for",
    "start": "446429",
    "end": "452389"
  },
  {
    "text": "bat size 64 the the dynamic flips you can get a lot more out of a p3 to excel",
    "start": "452389",
    "end": "459809"
  },
  {
    "text": "so it's a lot more cost efficient so",
    "start": "459809",
    "end": "465048"
  },
  {
    "text": "pardon me yeah so now once you've chosen",
    "start": "466159",
    "end": "472349"
  },
  {
    "start": "469000",
    "end": "490000"
  },
  {
    "text": "between the lesser of two evils for your application how do you address the",
    "start": "472349",
    "end": "478619"
  },
  {
    "text": "challenge of utilization how do you address the right sizing of resources that's that problem still exists right",
    "start": "478619",
    "end": "486059"
  },
  {
    "text": "so we're happy to tell you that we've we",
    "start": "486059",
    "end": "491789"
  },
  {
    "text": "are we're able to address that problem for you with Amazon Elastic inference having a little problem with it slides",
    "start": "491789",
    "end": "498179"
  },
  {
    "text": "you're apologize Amazon Elastic inference or Amazon e/i for short helps",
    "start": "498179",
    "end": "504209"
  },
  {
    "text": "you lower the cost of running inference by up to 75% by giving you the right amounts of GPU acceleration that's right",
    "start": "504209",
    "end": "512399"
  },
  {
    "text": "size for inference ai helps you attach these quantities of inference",
    "start": "512399",
    "end": "518099"
  },
  {
    "text": "acceleration to any instance type that way if you have a language model that needs a higher amount of memory and ones",
    "start": "518099",
    "end": "525749"
  },
  {
    "text": "that run on an R for instance or the next one instance you can choose that instance types and attach a small amount",
    "start": "525749",
    "end": "532649"
  },
  {
    "text": "of acceleration and right-size resources and save yourselves money Amazon Elastic",
    "start": "532649",
    "end": "542570"
  },
  {
    "start": "540000",
    "end": "567000"
  },
  {
    "text": "inferences available and integrated with ec2 and sage maker so it's convenient to",
    "start": "542570",
    "end": "548149"
  },
  {
    "text": "launch it with any instance on ec2 and on sage maker it provides you a fully managed experience at a low cost",
    "start": "548149",
    "end": "554649"
  },
  {
    "text": "we support out-of-the-box tensorflow models apache MX net and onyx models and",
    "start": "554649",
    "end": "561320"
  },
  {
    "text": "more framework support mo model support is coming soon now II I accelerators are",
    "start": "561320",
    "end": "570770"
  },
  {
    "start": "567000",
    "end": "630000"
  },
  {
    "text": "available in three sizes today with different amounts of throughput and",
    "start": "570770",
    "end": "576170"
  },
  {
    "text": "memory for the medium accelerator you",
    "start": "576170",
    "end": "582260"
  },
  {
    "text": "get one tara operations per second short as tops of single precision FP 32",
    "start": "582260",
    "end": "589459"
  },
  {
    "text": "compute you get a commensurate amount of FP 16 in a much larger quantity and you",
    "start": "589459",
    "end": "597200"
  },
  {
    "text": "get one gig of accelerator memory the price entry point for that is 13 cents",
    "start": "597200",
    "end": "602750"
  },
  {
    "text": "an hour that's significantly cheaper than a GPU instance that starts at 90",
    "start": "602750",
    "end": "608209"
  },
  {
    "text": "cents an hour or three dollars an hour for AP three to excel you have three",
    "start": "608209",
    "end": "615320"
  },
  {
    "text": "different sizes depending on the amount depending on the latency and throughput needs of your application so you can try",
    "start": "615320",
    "end": "621770"
  },
  {
    "text": "one of these accelerators with the instance type that your application requires and and determine if that fits",
    "start": "621770",
    "end": "628880"
  },
  {
    "text": "your needs it's available today in six regions more regions are coming soon",
    "start": "628880",
    "end": "634730"
  },
  {
    "start": "630000",
    "end": "646000"
  },
  {
    "text": "three regions in the United States Northern Virginia Ohio and Oregon and",
    "start": "634730",
    "end": "640730"
  },
  {
    "text": "Dublin Tokyo and South Korea so the big",
    "start": "640730",
    "end": "649760"
  },
  {
    "start": "646000",
    "end": "665000"
  },
  {
    "text": "question with all this is with these smaller amounts of acceleration than a full GPU what does it mean for",
    "start": "649760",
    "end": "656440"
  },
  {
    "text": "performance I'm glad I tell you that in many cases EW I predict inference",
    "start": "656440",
    "end": "662740"
  },
  {
    "text": "performs better than a p2x large instance here are the inferences",
    "start": "662740",
    "end": "667850"
  },
  {
    "text": "per second for three popular computer vision models two of them with image classification one for object detection",
    "start": "667850",
    "end": "675170"
  },
  {
    "text": "and the gray bars that you see are elastic inference accelerators paired",
    "start": "675170",
    "end": "682220"
  },
  {
    "text": "with a very small CPU instance and the costs associated with that and how they",
    "start": "682220",
    "end": "687500"
  },
  {
    "text": "compared to the GPU instances the pink bar is a p2x large at ninety cents and",
    "start": "687500",
    "end": "694040"
  },
  {
    "text": "the orange bar is a p 32x large at three dollars in in all three cases the eye",
    "start": "694040",
    "end": "701870"
  },
  {
    "text": "does better than a p3 to excel and it's way more cost-effective but for your",
    "start": "701870",
    "end": "707089"
  },
  {
    "text": "application your applications needs it may vary so we encourage you to try out",
    "start": "707089",
    "end": "713089"
  },
  {
    "text": "different sizes to see what fits your application needs p3 to excel as you can",
    "start": "713089",
    "end": "718940"
  },
  {
    "text": "see is a lot more performant than any of the EEI sizes so our goal at AWS is to provide you",
    "start": "718940",
    "end": "725329"
  },
  {
    "text": "with a range of options the right tool for the right job as Andy put it",
    "start": "725329",
    "end": "731690"
  },
  {
    "text": "yesterday so you can choose what you need for your model without any code changes and run your applications in",
    "start": "731690",
    "end": "739100"
  },
  {
    "text": "production so how does elastic inference work it it connects to a ec2 instance",
    "start": "739100",
    "end": "750380"
  },
  {
    "start": "740000",
    "end": "803000"
  },
  {
    "text": "you configure your instance to launch with an accelerator and the instance",
    "start": "750380",
    "end": "757779"
  },
  {
    "text": "launches with a attached accelerator behind a VPC endpoint so you first need",
    "start": "757779",
    "end": "765139"
  },
  {
    "text": "to provision a AWS private link VPC endpoint for your V PC and your",
    "start": "765139",
    "end": "771829"
  },
  {
    "text": "availability group you only need to do that at once you don't incur any additional charges for V PC endpoints",
    "start": "771829",
    "end": "777139"
  },
  {
    "text": "and all the accelerators that are launched in that availability zone are behind that V PC endpoint and all your",
    "start": "777139",
    "end": "785029"
  },
  {
    "text": "instances talk to it from using using that connectivity yeah I accelerate errs",
    "start": "785029",
    "end": "794509"
  },
  {
    "text": "are available as an option in launch templates so you can scale it automatically in auto scaling groups",
    "start": "794509",
    "end": "802540"
  },
  {
    "start": "803000",
    "end": "817000"
  },
  {
    "text": "it's it's fairly easy to work with Eon accelerators along with ec2",
    "start": "804889",
    "end": "811050"
  },
  {
    "text": "instances act as the unit of scale and auto scaling groups automatically scale up and down the capacity that you need",
    "start": "811050",
    "end": "818120"
  },
  {
    "start": "817000",
    "end": "860000"
  },
  {
    "text": "it also works with sage maker you can prototype the deployments of your model",
    "start": "818120",
    "end": "825809"
  },
  {
    "text": "wood with notebooks and you can iterate on them and you can test performance in what's known as local mode for stage",
    "start": "825809",
    "end": "832949"
  },
  {
    "text": "maker notebooks if you haven't checked out one of the sage maker classes I encourage you to do that it takes away a lot of the heavy lifting",
    "start": "832949",
    "end": "839129"
  },
  {
    "text": "of running and deploying models and production training prototyping training",
    "start": "839129",
    "end": "844350"
  },
  {
    "text": "and running models in production and then once you've prototype your deployments and in notebook mode in",
    "start": "844350",
    "end": "851459"
  },
  {
    "text": "local mode you can deploy your models to endpoints with low-cost inference acceleration and sage maker manages it",
    "start": "851459",
    "end": "858660"
  },
  {
    "text": "all for you so let's talk about what models are supported and how we go about",
    "start": "858660",
    "end": "865379"
  },
  {
    "start": "860000",
    "end": "876000"
  },
  {
    "text": "supporting them we support tensorflow apache MX net and onyx models at launch",
    "start": "865379",
    "end": "872179"
  },
  {
    "text": "models support for additional model types are coming soon what we provide",
    "start": "872179",
    "end": "878970"
  },
  {
    "start": "876000",
    "end": "944000"
  },
  {
    "text": "you to be able to run these models are Amazon EIN abled tensorflow serving and",
    "start": "878970",
    "end": "885889"
  },
  {
    "text": "Amazon II I enabled Apache MX net for MX and onyx what we've done is we've taken",
    "start": "885889",
    "end": "895199"
  },
  {
    "text": "one of these frameworks or taken intensive for serving for instance and added in the capability to automatically",
    "start": "895199",
    "end": "902189"
  },
  {
    "text": "detect accelerators when one's available so you don't because you don't have a",
    "start": "902189",
    "end": "907439"
  },
  {
    "text": "locally attached CPU you have a network attached accelerator we wanted to build",
    "start": "907439",
    "end": "912899"
  },
  {
    "text": "in the capability the mechanism to automatically detect the presence of one of these things securely allow you to",
    "start": "912899",
    "end": "918509"
  },
  {
    "text": "connect to it through iam policies and distribute the computation of your model",
    "start": "918509",
    "end": "924839"
  },
  {
    "text": "between the local instance and the remote accelerator these packages are available within the AWS deep learning",
    "start": "924839",
    "end": "931679"
  },
  {
    "text": "a.m. eyes they're also available on s3 for for download and they're provided to",
    "start": "931679",
    "end": "938790"
  },
  {
    "text": "you automatically in sage maker when you select an endpoint configured with elastic inference here's an example of",
    "start": "938790",
    "end": "947460"
  },
  {
    "start": "944000",
    "end": "978000"
  },
  {
    "text": "loading and serving models using tensorflow very simple you download the tensorflow",
    "start": "947460",
    "end": "955010"
  },
  {
    "text": "serving binary you provide the base path of your model everything works as it",
    "start": "955010",
    "end": "962190"
  },
  {
    "text": "would with standard tensorflow serving the only difference is here you're you're obtaining our package that we",
    "start": "962190",
    "end": "969810"
  },
  {
    "text": "provide to you either via the deep learning ami or an s3 or automatically through sage maker and you load your model and make inference calls no",
    "start": "969810",
    "end": "976800"
  },
  {
    "text": "changes to your code whatsoever with MX net the only configuration change is",
    "start": "976800",
    "end": "986180"
  },
  {
    "text": "setting the context to MX dot e ia just",
    "start": "986240",
    "end": "992280"
  },
  {
    "text": "as you would MX dot CPU or MX GPU you load the models the same way you make",
    "start": "992280",
    "end": "998310"
  },
  {
    "text": "inference calls the same way nothing else changes for onyx models MX and it",
    "start": "998310",
    "end": "1003710"
  },
  {
    "text": "provides a very convenient import model API nothing changes with that either you",
    "start": "1003710",
    "end": "1010400"
  },
  {
    "text": "just simply set the context as MX study ia to load models and make inference",
    "start": "1010400",
    "end": "1016640"
  },
  {
    "text": "calls so how do you choose the world",
    "start": "1016640",
    "end": "1022730"
  },
  {
    "text": "with elastic inference is slightly different than a local GPU you have a call over the network instead of calls",
    "start": "1022730",
    "end": "1030350"
  },
  {
    "text": "via PCIe to the local GPU so there are",
    "start": "1030350",
    "end": "1035920"
  },
  {
    "text": "considerations and trade-offs you'll need to make the size of your payload your size of your tensor payload from",
    "start": "1035920",
    "end": "1042680"
  },
  {
    "text": "your instance to the accelerator has a bearing on the amount of latency you",
    "start": "1042680",
    "end": "1047810"
  },
  {
    "text": "will incur so the the benchmarks I showed before are for standard image",
    "start": "1047810",
    "end": "1053750"
  },
  {
    "text": "workloads with 224 by 224 image sizes",
    "start": "1053750",
    "end": "1058880"
  },
  {
    "text": "Livio will go into the Autodesk's workload with a much larger set of",
    "start": "1058880",
    "end": "1066410"
  },
  {
    "text": "images that are pumped into the model so you need to try out sizes of",
    "start": "1066410",
    "end": "1071630"
  },
  {
    "text": "accelerators with instances start small determine if that meets the latency needs of your application to right-size",
    "start": "1071630",
    "end": "1078980"
  },
  {
    "text": "this environment you have to calculate the overall latency budget let's say for instance your application needs to",
    "start": "1078980",
    "end": "1084170"
  },
  {
    "text": "respond within 200 milliseconds you have your authentication and other overhead",
    "start": "1084170",
    "end": "1091880"
  },
  {
    "text": "on your application that takes about 100 milliseconds then you essentially have a budget of about 100 milliseconds to work",
    "start": "1091880",
    "end": "1097880"
  },
  {
    "text": "with for your inference call so you try out a combination of an instance and an accelerator you see if that meets your",
    "start": "1097880",
    "end": "1103970"
  },
  {
    "text": "needs your throughput need your latency needs if not you size up you go to a larger size accelerator you choose a",
    "start": "1103970",
    "end": "1110720"
  },
  {
    "text": "different instance type that that may work for you if you are not running your",
    "start": "1110720",
    "end": "1117230"
  },
  {
    "text": "models today in FP 16 I know it's early days a lot of lot of us are are are",
    "start": "1117230",
    "end": "1125000"
  },
  {
    "text": "starting to use FP 16 with the voltage GPUs you highly encourage using FP 16",
    "start": "1125000",
    "end": "1131630"
  },
  {
    "text": "it's a much more cost-effective much more abundant type of compute that's",
    "start": "1131630",
    "end": "1139970"
  },
  {
    "text": "available for you within an accelerator and at this point I'd like to turn it",
    "start": "1139970",
    "end": "1146450"
  },
  {
    "text": "over to autodesk to our customers Peter and Livio to walk us through their use cases and share a little bit about how",
    "start": "1146450",
    "end": "1154940"
  },
  {
    "text": "they use the e aí Thank You Dominic how's everyone doing today good Thursday we're almost done",
    "start": "1154940",
    "end": "1161330"
  },
  {
    "text": "did anyone run the race on Tuesday no hands yeah good one Wow well I ran it -",
    "start": "1161330",
    "end": "1168170"
  },
  {
    "text": "did you run as fast as you could and still didn't win that's yeah same for me fast as I could so I even developed a",
    "start": "1168170",
    "end": "1174380"
  },
  {
    "text": "runners hack so I have my water bottle here just in case so I'll turn out to coffee in your ears my name is Peter Jones I'm head of AI engineering in the",
    "start": "1174380",
    "end": "1181400"
  },
  {
    "text": "Autodesk AI lab and I'm going to talk to you for a minute on how we see AI affecting the future of design so first",
    "start": "1181400",
    "end": "1191450"
  },
  {
    "start": "1190000",
    "end": "1229000"
  },
  {
    "text": "let me tea up some of the problems we see happening today in the world the population is about 7.5 billion and",
    "start": "1191450",
    "end": "1196880"
  },
  {
    "text": "that's projected to be 10 billion by 2050 so the world's growing",
    "start": "1196880",
    "end": "1202370"
  },
  {
    "text": "some good news about that as well the middle class is growing about 400,000 people join the middle class every day",
    "start": "1202370",
    "end": "1208060"
  },
  {
    "text": "but that also presents a challenge for us because those people want more they want more food more water they consume",
    "start": "1208060",
    "end": "1214730"
  },
  {
    "text": "more electricity they want more Goods and so this could become a problem right",
    "start": "1214730",
    "end": "1220250"
  },
  {
    "text": "how do we make and how do we allow us to serve those needs of more that people",
    "start": "1220250",
    "end": "1225890"
  },
  {
    "text": "want without causing negative impact in our environment because remember the",
    "start": "1225890",
    "end": "1232100"
  },
  {
    "text": "more we make the fewer natural resources we have left and today you know we do have waste right 30 percent of the waste",
    "start": "1232100",
    "end": "1239090"
  },
  {
    "text": "in the world is from construction 70% of spare parts for all of our goods are never used so this kind of thing has to",
    "start": "1239090",
    "end": "1246590"
  },
  {
    "text": "change so how do we make all that we you know all that humanity wants and needs without impacting our environment as",
    "start": "1246590",
    "end": "1253880"
  },
  {
    "text": "much how do we how do we reduce the depletion of resources have less waste less carbon emissions so we see this as",
    "start": "1253880",
    "end": "1261440"
  },
  {
    "text": "a capacity issue we have to rethink how we make things and for us since we makes offer for design this is the biggest",
    "start": "1261440",
    "end": "1268400"
  },
  {
    "text": "design opportunity we've ever had so it Autodesk's we make software for people",
    "start": "1268400",
    "end": "1273770"
  },
  {
    "start": "1270000",
    "end": "1308000"
  },
  {
    "text": "that make things we help our customers design and make things from skyscrapers to smart cars from bridges to",
    "start": "1273770",
    "end": "1281060"
  },
  {
    "text": "blockbusters our tools help them design and make what they can envision and we",
    "start": "1281060",
    "end": "1286760"
  },
  {
    "text": "see this as an opportunity to do things better and we see AI and machine intelligence as a key piece of how",
    "start": "1286760",
    "end": "1292520"
  },
  {
    "text": "design will change in the future so I'm going to walk you through just two examples of how we see that changing now",
    "start": "1292520",
    "end": "1299150"
  },
  {
    "text": "and then we'll kind of dive a little deeper into machine learning models and I'll turn it over to Livia who will talk",
    "start": "1299150",
    "end": "1304400"
  },
  {
    "text": "more about how we use the lack of elastic in France for some of our models so first use-cases construction this is",
    "start": "1304400",
    "end": "1312890"
  },
  {
    "start": "1308000",
    "end": "1376000"
  },
  {
    "text": "a family-owned construction business in the Netherlands the van Winans and they specialize in affordable sustainable",
    "start": "1312890",
    "end": "1319490"
  },
  {
    "text": "construction so their goal is to make cost-effective housing with zero net",
    "start": "1319490",
    "end": "1324650"
  },
  {
    "text": "energy footprint because of the solar panels so the first problem they had was",
    "start": "1324650",
    "end": "1329960"
  },
  {
    "text": "they built houses like most houses are built today which is custom built on-site",
    "start": "1329960",
    "end": "1335690"
  },
  {
    "text": "with whatever the configuration is that it was called for by the customer that tends to take time tends to produce",
    "start": "1335690",
    "end": "1341780"
  },
  {
    "text": "waste and tends to reduce profit and raise costs so first thing they did is they deconstructed their house into",
    "start": "1341780",
    "end": "1349340"
  },
  {
    "text": "modular components so now they could make any combination of their house layout which they had over 100 and they",
    "start": "1349340",
    "end": "1355880"
  },
  {
    "text": "could do it on-site in three days so they they really optimized how they assemble this is one pattern we see",
    "start": "1355880",
    "end": "1362540"
  },
  {
    "text": "happening a lot and audit us where construction and manufacturing are merging and so they basically took a",
    "start": "1362540",
    "end": "1367550"
  },
  {
    "text": "manufacturing approach they studied cars and planes and how they were built and said hey we can apply this to houses",
    "start": "1367550",
    "end": "1373420"
  },
  {
    "text": "good good first step right so we worked with them to take it to the next step which is okay now I can build a house in",
    "start": "1373420",
    "end": "1380570"
  },
  {
    "start": "1376000",
    "end": "1445000"
  },
  {
    "text": "any one of many configurations that I know how to do how do I design whole neighborhoods and how do I do that in a",
    "start": "1380570",
    "end": "1386660"
  },
  {
    "text": "way that's optimal for all the different houses in that neighborhood so we used a generative design technique and we used",
    "start": "1386660",
    "end": "1393980"
  },
  {
    "text": "narratives we modeled out eight different goals of design things like the size of the backyard the view from",
    "start": "1393980",
    "end": "1400790"
  },
  {
    "text": "the person inside the house the exposure to the Sun but we also had goals for the business like cost and profit and we fed",
    "start": "1400790",
    "end": "1408110"
  },
  {
    "text": "this to a genetic algorithm and it walked through over 15,000 design alternatives all of which are valid",
    "start": "1408110",
    "end": "1414380"
  },
  {
    "text": "these are not like design possibilities that cannot work they can all work but the designer now thinks at a higher",
    "start": "1414380",
    "end": "1420560"
  },
  {
    "text": "level which is how do I want to balance you know yard size versus solar gain which of those are more important this",
    "start": "1420560",
    "end": "1427730"
  },
  {
    "text": "kind of designing in the large we call it this kind of exploring the design space we see is one critical piece of",
    "start": "1427730",
    "end": "1435020"
  },
  {
    "text": "how design will change in the future because it allows you to sort of balance things like affordability versus",
    "start": "1435020",
    "end": "1440840"
  },
  {
    "text": "livability and sustainability versus profitability here's a different case",
    "start": "1440840",
    "end": "1447530"
  },
  {
    "start": "1445000",
    "end": "1529000"
  },
  {
    "text": "for manufacturing so we worked with General Motors where you see there on the left is their seat belt bracket that",
    "start": "1447530",
    "end": "1453590"
  },
  {
    "text": "they use today that's eight different components and we again applied a",
    "start": "1453590",
    "end": "1459020"
  },
  {
    "text": "genetic algorithm to that and we also used additive manufacturing techniques",
    "start": "1459020",
    "end": "1464090"
  },
  {
    "text": "which are new these days and we were able to come up with a single part in",
    "start": "1464090",
    "end": "1469470"
  },
  {
    "text": "of eight there was 40% lighter and 20% stronger and it still fits the same form",
    "start": "1469470",
    "end": "1474630"
  },
  {
    "text": "factor their original parts it doesn't change anything in terms of the the overall manufacturing process this kind",
    "start": "1474630",
    "end": "1482520"
  },
  {
    "text": "of idea of where you use machine intelligence to explore the design space and come up with an alternative that you",
    "start": "1482520",
    "end": "1488820"
  },
  {
    "text": "may not have considered or thought of if you just used a human to attack the design problem is one of the pieces that",
    "start": "1488820",
    "end": "1494700"
  },
  {
    "text": "we see is the future of design imagine if you asked a design team to explore 15,000 alternative designs well number",
    "start": "1494700",
    "end": "1502140"
  },
  {
    "text": "one they wouldn't even do that many because it would take too long and number two you know how many people have heard about cognitive bias right there's",
    "start": "1502140",
    "end": "1509130"
  },
  {
    "text": "there's 184 known documented biases and how we think and that's very president",
    "start": "1509130",
    "end": "1515520"
  },
  {
    "text": "design because today design is all done by humans we're not trying to replace humans by any means but we think machine",
    "start": "1515520",
    "end": "1521760"
  },
  {
    "text": "intelligence can help them really explore the entire vast options for design so then they can hone in on what",
    "start": "1521760",
    "end": "1527039"
  },
  {
    "text": "makes the most sense for their goals ok let's go a little deeper into machine learning so in the Autodesk AI lab we're",
    "start": "1527039",
    "end": "1534780"
  },
  {
    "start": "1529000",
    "end": "1543000"
  },
  {
    "text": "focused on finding novel and impactful machine learning techniques and algorithms to really push the envelope",
    "start": "1534780",
    "end": "1540780"
  },
  {
    "text": "of how design will change in the future one such kind of machine learning model",
    "start": "1540780",
    "end": "1548309"
  },
  {
    "start": "1543000",
    "end": "1590000"
  },
  {
    "text": "that we like is multi-view convolutional neural networks does anyone use those NB cnn's ok so yeah a couple so in a",
    "start": "1548309",
    "end": "1555450"
  },
  {
    "text": "nutshell you have an object you know you take a picture of it from different angles and you rotate it around so you",
    "start": "1555450",
    "end": "1561570"
  },
  {
    "text": "end up with 80 different pictures of a single object and that's what you see here those are four different samples of",
    "start": "1561570",
    "end": "1567360"
  },
  {
    "text": "four different objects eighty shots of that and you feed that to a network and it understands fairly clearly how to",
    "start": "1567360",
    "end": "1574500"
  },
  {
    "text": "reason about that three-deep piece of geometry it understands it from from multiple angles and and rotations and",
    "start": "1574500",
    "end": "1581429"
  },
  {
    "text": "therefore it's a very stable network to really understand what that geometry is",
    "start": "1581429",
    "end": "1586679"
  },
  {
    "text": "built up and it can classify it and understand its form and function and we do that so that then you can do things",
    "start": "1586679",
    "end": "1593340"
  },
  {
    "text": "like you can really apply an algebra to to manipulating geometric geometric",
    "start": "1593340",
    "end": "1598649"
  },
  {
    "text": "figures so on the top there you have a vase right and you can sort of decide okay in it first of all it",
    "start": "1598649",
    "end": "1604510"
  },
  {
    "text": "understands just from classification that that base has a lip and a base in the center and I can sort of subtract",
    "start": "1604510",
    "end": "1609940"
  },
  {
    "text": "out a center that's too wide not gonna replace it with the center that's more narrow and then it can synthesize a brand new object that meets those",
    "start": "1609940",
    "end": "1616420"
  },
  {
    "text": "constraints one of the things we do with our convolutional neural networks is we also want to run them backwards so we",
    "start": "1616420",
    "end": "1623080"
  },
  {
    "text": "don't want them not just for encoding in classification but we want them for decoding and generation that's very key",
    "start": "1623080",
    "end": "1629650"
  },
  {
    "text": "task because remember part of our problem spaces we're helping designers figure out the design that they want so",
    "start": "1629650",
    "end": "1635620"
  },
  {
    "text": "that means you know we the machine needs to be able to produce content that they may decide they want to use so a key",
    "start": "1635620",
    "end": "1641049"
  },
  {
    "text": "point second thing we found is that for some kind of 3d models geometry and",
    "start": "1641049",
    "end": "1647679"
  },
  {
    "text": "voxels and all the things you typically see in computer vision and in that aspect of research it's not always the",
    "start": "1647679",
    "end": "1654460"
  },
  {
    "text": "best way to represent a 3d model and so we also are heavily involved in point",
    "start": "1654460",
    "end": "1659470"
  },
  {
    "text": "clouds because point clouds so a point cloud you might take the same object and shoot a bunch of random points around it",
    "start": "1659470",
    "end": "1664630"
  },
  {
    "text": "and you might only come up with a couple thousand points so in some ways the data set that represents the model is a lot",
    "start": "1664630",
    "end": "1670480"
  },
  {
    "text": "narrower and smaller than a graphic data set and we found for some kinds of applications point clouds are actually",
    "start": "1670480",
    "end": "1676540"
  },
  {
    "text": "more effective than geometry models for classification and also for them",
    "start": "1676540",
    "end": "1681640"
  },
  {
    "text": "synthesis and and regeneration on the other side of what we think of synthetic output would be now if you think about",
    "start": "1681640",
    "end": "1689710"
  },
  {
    "text": "the software that we make right you saw earlier you saw genetic algorithms which I said is sort of designing in the large",
    "start": "1689710",
    "end": "1695230"
  },
  {
    "text": "it's looking at a lot of different design possibilities but there's other aspects of design that are not sort of in the large they're what I would call",
    "start": "1695230",
    "end": "1701590"
  },
  {
    "text": "them the small which is they're looking at specific parts and they're generating a 3d model about how to represent that",
    "start": "1701590",
    "end": "1707410"
  },
  {
    "text": "part and the machine learning techniques that we're injecting into our products",
    "start": "1707410",
    "end": "1712450"
  },
  {
    "text": "and by the way I can't show you them today I'm sorry a lot of those will be released in the next year in 2019 so",
    "start": "1712450",
    "end": "1717520"
  },
  {
    "text": "you'll have to imagine that apologize for that you can imagine a designer in",
    "start": "1717520",
    "end": "1722620"
  },
  {
    "text": "in something like a manufacturing project you know designing out what a phone looks like and alongside of it you",
    "start": "1722620",
    "end": "1730120"
  },
  {
    "text": "might have a you know a whole bunch of neural networks watching what the designers doing and figuring out where",
    "start": "1730120",
    "end": "1735309"
  },
  {
    "text": "the going and then maybe suggesting hey have you thought about this kind of transition for this case or have you",
    "start": "1735309",
    "end": "1740950"
  },
  {
    "text": "thought about you know finishing your building in this way so that's sort of a suggestive idea and",
    "start": "1740950",
    "end": "1746530"
  },
  {
    "text": "to do that we've got a generate real time you know 3d imagery or 3d models right when the user is interacting with",
    "start": "1746530",
    "end": "1753250"
  },
  {
    "text": "the product and that's why elastic inference was interesting to us because today we run those models on GPUs",
    "start": "1753250",
    "end": "1760530"
  },
  {
    "text": "another kind of example you could see is where a designer's building a building you know does the basic kind of outline",
    "start": "1760530",
    "end": "1767590"
  },
  {
    "text": "and then could say hey you know computer or syria or whatever that thing is named you know please finish out the interior",
    "start": "1767590",
    "end": "1772990"
  },
  {
    "text": "layout of this and generate all the layout for you know desks and chairs and whatnot again a problem that is needs",
    "start": "1772990",
    "end": "1779980"
  },
  {
    "text": "real-time response is looking at both model information and geometry",
    "start": "1779980",
    "end": "1785290"
  },
  {
    "text": "information and then reasoning about it and then also generating results and so as I mentioned we've had success with",
    "start": "1785290",
    "end": "1792370"
  },
  {
    "text": "those models and many of them need GPU power and we've run ups today on GPU instances so when we heard about the",
    "start": "1792370",
    "end": "1797710"
  },
  {
    "text": "elastic inference feature we thought okay that's pretty cool because you know we're not the designer is not every",
    "start": "1797710",
    "end": "1804580"
  },
  {
    "text": "second needing to be prompted with geometry so those instance the the current GPU instances we used today",
    "start": "1804580",
    "end": "1809679"
  },
  {
    "text": "we're not fully utilized so livia will take you deeper into that but before I give it over to him we found that it's",
    "start": "1809679",
    "end": "1816940"
  },
  {
    "text": "quite performant and it also solves a different problem which is we don't have to segment as much our infrastructure",
    "start": "1816940",
    "end": "1823090"
  },
  {
    "text": "for infants on GPU versus inference on CPU you know basically that can be the same box and you can segment things by",
    "start": "1823090",
    "end": "1829059"
  },
  {
    "text": "your docker container or by your libraries and decide how you want to do that so I've been pretty compelling for",
    "start": "1829059",
    "end": "1834429"
  },
  {
    "text": "us we ran some models through tests and live you now we'll take you through that I think we need to push a special button",
    "start": "1834429",
    "end": "1846059"
  },
  {
    "start": "1845000",
    "end": "1944000"
  },
  {
    "text": "okay so thank you so I'll just give a small overview of what I'll talk about here I'll talk about a couple of the",
    "start": "1849840",
    "end": "1856840"
  },
  {
    "text": "example models that we use I'll talk about how to start the elastic inference",
    "start": "1856840",
    "end": "1862450"
  },
  {
    "text": "instance from the command line and from the console take a take you through a couple steps needed that we had to",
    "start": "1862450",
    "end": "1869020"
  },
  {
    "text": "change our model slightly so that we can use it with it and then a small demo of it just running in AWS so first off here",
    "start": "1869020",
    "end": "1877900"
  },
  {
    "text": "and sorry I'm gonna have to stand behind the podium because they have to be attached to the computer so the first",
    "start": "1877900",
    "end": "1883210"
  },
  {
    "text": "model here that we are looking at is this multi view convolutional neural network that Peter had talked about just",
    "start": "1883210",
    "end": "1889480"
  },
  {
    "text": "a little more details in it it's essentially a convolutional neural network a 2d one that will take your",
    "start": "1889480",
    "end": "1895660"
  },
  {
    "text": "images and compress them into smaller and smaller sizes followed by a maximum operation that gets your data into this",
    "start": "1895660",
    "end": "1902880"
  },
  {
    "text": "compressed feature vector that's only a one-dimensional vector and it encodes all the data needed to describe that 3d",
    "start": "1902880",
    "end": "1910360"
  },
  {
    "text": "model so you can either use it use the embedding to compare it to other type of",
    "start": "1910360",
    "end": "1916440"
  },
  {
    "text": "models that you have the other type of vectors and easily cluster them or see similarities and differences or you can",
    "start": "1916440",
    "end": "1922720"
  },
  {
    "text": "use a classifier to them that's been trained to say oh this is a chair this is an airplane or some other object and",
    "start": "1922720",
    "end": "1928690"
  },
  {
    "text": "the one big advantage of working in this way with multi-view is that the 3d object that you're looking at you don't",
    "start": "1928690",
    "end": "1934660"
  },
  {
    "text": "need a 3d convolution to do this by taking the multiple images and processing it this way you're just doing",
    "start": "1934660",
    "end": "1940929"
  },
  {
    "text": "2d convolutions so it's much more efficient another type of model that we use is a",
    "start": "1940929",
    "end": "1947200"
  },
  {
    "start": "1944000",
    "end": "1988000"
  },
  {
    "text": "variational auto encoder it follows a similar principle as the previous one where you have an encoder parts to get",
    "start": "1947200",
    "end": "1954760"
  },
  {
    "text": "your data down into a dense feature vector and a decoder part that then",
    "start": "1954760",
    "end": "1960160"
  },
  {
    "text": "tries to recreate the original object that's where the auto part comes in that it's the input and the output or our",
    "start": "1960160",
    "end": "1966550"
  },
  {
    "text": "attempted to be the same thing and the variational part is that the vectors that you get are essentially sampled",
    "start": "1966550",
    "end": "1973510"
  },
  {
    "text": "from a what do you call it a Gaussian distribution so that you can also easily",
    "start": "1973510",
    "end": "1979660"
  },
  {
    "text": "create new types of data by just sampling that Galus distribution and coming up with new",
    "start": "1979660",
    "end": "1984790"
  },
  {
    "text": "images in that space so how would you",
    "start": "1984790",
    "end": "1990850"
  },
  {
    "start": "1988000",
    "end": "2048000"
  },
  {
    "text": "actually set up your instance with elastic inference well it's pretty much just like setting up an ec2 instance the",
    "start": "1990850",
    "end": "1998650"
  },
  {
    "text": "only yeah so you would just choose the deep learning ami as Dom had mentioned and you would pass the reference to the",
    "start": "1998650",
    "end": "2005070"
  },
  {
    "text": "type of accelerator that you want to use the medium large extra large and there's",
    "start": "2005070",
    "end": "2010080"
  },
  {
    "text": "that one extra step where you only have to define once is to put an end point on your V PC so that your ec2 instance can",
    "start": "2010080",
    "end": "2018179"
  },
  {
    "text": "communicate to the accelerator because as tom said it's over it's not on the machine it's over the network so what",
    "start": "2018179",
    "end": "2024960"
  },
  {
    "text": "this would look like from a command line it would just be your standard UC to run",
    "start": "2024960",
    "end": "2030690"
  },
  {
    "text": "instances where you'd pass in the the image ID that you need the deep learning",
    "start": "2030690",
    "end": "2035910"
  },
  {
    "text": "ami the type of instance you want to use could be any type the one main thing that stands out is that you would pass",
    "start": "2035910",
    "end": "2041640"
  },
  {
    "text": "in the elastic inference accelerator type of either a medium large extra large now that you have your instance",
    "start": "2041640",
    "end": "2050820"
  },
  {
    "start": "2048000",
    "end": "2093000"
  },
  {
    "text": "there how would you use it well we used it with tensorflow because our models are written in tensorflow",
    "start": "2050820",
    "end": "2057000"
  },
  {
    "text": "so we would have to first create the model save it as a saved model to use",
    "start": "2057000",
    "end": "2062429"
  },
  {
    "text": "with serving because again we use their models in tensorflow and they're done with estimators so we had to convert",
    "start": "2062429",
    "end": "2068850"
  },
  {
    "text": "those to save models to use with the tensorflow serving and then in another",
    "start": "2068850",
    "end": "2075169"
  },
  {
    "text": "process you would send the request to that server to predict with whatever test data you want to use and elastic",
    "start": "2075169",
    "end": "2082290"
  },
  {
    "text": "inferencing takes care of seeing which parts can be accelerated and sending the right operations to the to the",
    "start": "2082290",
    "end": "2089310"
  },
  {
    "text": "accelerator so you don't have to worry about that at all so how did we create a",
    "start": "2089310",
    "end": "2094888"
  },
  {
    "start": "2093000",
    "end": "2194000"
  },
  {
    "text": "safe model from a TF estimator format well from our example before of the MVC",
    "start": "2094889",
    "end": "2100890"
  },
  {
    "text": "NN what we expected from our input images as our input would be a bunch of",
    "start": "2100890",
    "end": "2107880"
  },
  {
    "text": "multi-view grayscale images called images and the dimensions of them where",
    "start": "2107880",
    "end": "2113160"
  },
  {
    "text": "were the batch size which in this case would just be one as we're past one piece of data at a time the number",
    "start": "2113160",
    "end": "2119400"
  },
  {
    "text": "of views which could be 2080 the number of multi views that you have the width and height of the picture like 128 by",
    "start": "2119400",
    "end": "2125819"
  },
  {
    "text": "128 or 256 by 256 and the number of color channels which in this case would just be 1 so we loaded in our saved",
    "start": "2125819",
    "end": "2133950"
  },
  {
    "text": "trained classifier sorry our saved estimator we wanted to define",
    "start": "2133950",
    "end": "2139920"
  },
  {
    "text": "what the program expects so that that can pass it into the model so we wanted to define on this image array of 1 1",
    "start": "2139920",
    "end": "2148470"
  },
  {
    "text": "back size 80 views 128 by 128 size and grayscale and that would be what the",
    "start": "2148470",
    "end": "2155279"
  },
  {
    "text": "model takes in we maps we have this map of the name images to that tensor so",
    "start": "2155279",
    "end": "2162180"
  },
  {
    "text": "that when we build a request and say this array this multi-view image array that we have called images should map to",
    "start": "2162180",
    "end": "2169170"
  },
  {
    "text": "this tensor which will then go into the model and you export the save model just",
    "start": "2169170",
    "end": "2174599"
  },
  {
    "text": "pass in the directory and this long looking function that essentially just does what I said that when you pass in",
    "start": "2174599",
    "end": "2180240"
  },
  {
    "text": "something to the images name it would put it into the images tensor that will",
    "start": "2180240",
    "end": "2185910"
  },
  {
    "text": "be passed into the model there's some phones up so I'll give you two seconds",
    "start": "2185910",
    "end": "2191160"
  },
  {
    "text": "for those 1 2 3 so now that we have that",
    "start": "2191160",
    "end": "2198480"
  },
  {
    "start": "2194000",
    "end": "2287000"
  },
  {
    "text": "saved model how would we predict with it so we would as I mentioned before one",
    "start": "2198480",
    "end": "2203520"
  },
  {
    "text": "process will serve the model that we have saved and another process will send",
    "start": "2203520",
    "end": "2208950"
  },
  {
    "text": "the request to it to get the results so the process that serves the model these",
    "start": "2208950",
    "end": "2214710"
  },
  {
    "text": "were in Dom slides 2 but essentially you just call the elastic inference version",
    "start": "2214710",
    "end": "2219779"
  },
  {
    "text": "of tensorflow serving pass in whatever model name you want to call it where the model is and the port that it's",
    "start": "2219779",
    "end": "2225089"
  },
  {
    "text": "listening on on the other side on in the program that's then calling that there's",
    "start": "2225089",
    "end": "2232680"
  },
  {
    "text": "a lot of things here that might look a little more complicated but it's all really just part of the standard example serving code just adjust it to this so",
    "start": "2232680",
    "end": "2239700"
  },
  {
    "text": "the main point here is that we define the port we want to look at a bunch of other things just to create the requests",
    "start": "2239700",
    "end": "2246240"
  },
  {
    "text": "that we're going to send the data in through specify the name the name that we use before and we have",
    "start": "2246240",
    "end": "2254760"
  },
  {
    "text": "a function here called get next input that just takes the next piece of data the next multi-view image array we load",
    "start": "2254760",
    "end": "2261720"
  },
  {
    "text": "that in and that gets put into the request in this way that's the input",
    "start": "2261720",
    "end": "2266880"
  },
  {
    "text": "called images we'll take this input array with this given shape and then it",
    "start": "2266880",
    "end": "2272220"
  },
  {
    "text": "will know to make to turn that into the tensor that will be passed into the model and finally now that your result",
    "start": "2272220",
    "end": "2279630"
  },
  {
    "text": "your request is built you just call predict with it and you get your result back so at this point I just want to",
    "start": "2279630",
    "end": "2289619"
  },
  {
    "text": "show one how this would be done in the console how you would start elastic inference in the console and just do a",
    "start": "2289619",
    "end": "2295980"
  },
  {
    "text": "quick demo of this code that I had showed so from the console if instead of",
    "start": "2295980",
    "end": "2303240"
  },
  {
    "text": "running it from the command line you wanted to start the elastic inference from a console in ec2 as your console",
    "start": "2303240",
    "end": "2314540"
  },
  {
    "text": "you're making an ec2 instance I'll give that Jim but really the provisioning",
    "start": "2314540",
    "end": "2320190"
  },
  {
    "text": "sequence is quite similar let's go back a slide anyways so to fill in the gaps",
    "start": "2320190",
    "end": "2327829"
  },
  {
    "text": "you know when you provision an instance for the console right you go to DC to console you pick your ami all that's the",
    "start": "2327829",
    "end": "2333810"
  },
  {
    "text": "same there's a checkbox that you'll see soon that gives you the choice of the",
    "start": "2333810",
    "end": "2339810"
  },
  {
    "text": "kind of AI estimator that you want and that's pretty much it so imagine that if",
    "start": "2339810",
    "end": "2347190"
  },
  {
    "text": "you will to fill in more details on the copy thank you Peter the launch",
    "start": "2347190",
    "end": "2354660"
  },
  {
    "start": "2353000",
    "end": "2379000"
  },
  {
    "text": "templates view in the ec2 console also lets you do that there's a new box for",
    "start": "2354660",
    "end": "2360500"
  },
  {
    "text": "elastic inference check that select the accelerator type you want and that forms",
    "start": "2360500",
    "end": "2366119"
  },
  {
    "text": "your launch launch template which you can load into your auto scaling group saij maker at the various steps in stage",
    "start": "2366119",
    "end": "2373890"
  },
  {
    "text": "maker where you point out the infrastructure you need elastic inference is also there available as an option",
    "start": "2373890",
    "end": "2380230"
  },
  {
    "start": "2379000",
    "end": "2420000"
  },
  {
    "text": "a few more bits of information it's at launch it's available in CloudFormation",
    "start": "2380230",
    "end": "2385300"
  },
  {
    "text": "so you can launch instances with accelerators in cloud formation and",
    "start": "2385300",
    "end": "2390780"
  },
  {
    "text": "metrics and the health of an accelerator is available in cloud watch so you can",
    "start": "2390780",
    "end": "2396220"
  },
  {
    "text": "get that information there just to shout out to CloudFormation Thank You Dominic",
    "start": "2396220",
    "end": "2401530"
  },
  {
    "text": "for having it live in cloud formation on the release how many you are CloudFormation users Yeah right how many",
    "start": "2401530",
    "end": "2407290"
  },
  {
    "text": "times have you had to not always see or feature first time so this is awesome that day one when it's released you can use it in cloud formation thank you for",
    "start": "2407290",
    "end": "2413710"
  },
  {
    "text": "that appreciate it",
    "start": "2413710",
    "end": "2416640"
  },
  {
    "text": "do you think well the console be happy to take some questions in the meantime",
    "start": "2419160",
    "end": "2425650"
  },
  {
    "start": "2420000",
    "end": "2481000"
  },
  {
    "text": "while we get to sort it out does anyone have yes sir",
    "start": "2425650",
    "end": "2432690"
  },
  {
    "text": "Peter you mind if you tell me where it",
    "start": "2432840",
    "end": "2438220"
  },
  {
    "text": "is I'll play the question for you so the",
    "start": "2438220",
    "end": "2459040"
  },
  {
    "text": "question was from the three that you have kind of small medium large how do you infer that and map that back Dominic",
    "start": "2459040",
    "end": "2465310"
  },
  {
    "text": "to the hardware underneath it to get a better sense of what what hardware is being used yes I at launch you can",
    "start": "2465310",
    "end": "2471580"
  },
  {
    "text": "attach one eeew a instance to every base",
    "start": "2471580",
    "end": "2476859"
  },
  {
    "text": "instance so if we take a cpu instance for example you can attach one yeah as",
    "start": "2476859",
    "end": "2481990"
  },
  {
    "start": "2481000",
    "end": "2773000"
  },
  {
    "text": "you will see in the console and you go through the frameworks tensorflow and a max net storm explained so the framework",
    "start": "2481990",
    "end": "2490270"
  },
  {
    "text": "does the automatic discovery of the attached elastic inference accelerator",
    "start": "2490270",
    "end": "2495369"
  },
  {
    "text": "and it intelligently optimizes your model graph to do the inference across",
    "start": "2495369",
    "end": "2502630"
  },
  {
    "text": "the local CPU instance and the remote accelerator",
    "start": "2502630",
    "end": "2508920"
  },
  {
    "text": "same hardware for the the each each one",
    "start": "2516640",
    "end": "2532430"
  },
  {
    "text": "of those sizes inside the amount of throughput and capacity doubles so a",
    "start": "2532430",
    "end": "2537619"
  },
  {
    "text": "large has twice the amount of capacity as a medium the extra-large has twice the amount of capacity as a large so you",
    "start": "2537619",
    "end": "2543980"
  },
  {
    "text": "have more deep more GPU power and acceleration that's available for you with each of those sizes um think of it",
    "start": "2543980",
    "end": "2555500"
  },
  {
    "text": "as capacity not not as not as chance and again it's going to vary so the the",
    "start": "2555500",
    "end": "2561529"
  },
  {
    "text": "performance numbers that I showed on the on the screen are specific to those models right and you'll see performance",
    "start": "2561529",
    "end": "2567410"
  },
  {
    "text": "vary quite a bit from model to model so you really need to try it out it's not so your models running on an amount of",
    "start": "2567410",
    "end": "2574390"
  },
  {
    "text": "GPU powered acceleration it's not one chip or four chips",
    "start": "2574390",
    "end": "2582160"
  },
  {
    "text": "so so the important point can him yeah note here is what you see as the",
    "start": "2588739",
    "end": "2595379"
  },
  {
    "text": "customer the physical resources that are reserved for your accelerator that",
    "start": "2595379",
    "end": "2602129"
  },
  {
    "text": "includes Tara operations per second tops capacity at two different positions as",
    "start": "2602129",
    "end": "2608009"
  },
  {
    "text": "was mentioned in the table FP 16 FP 32 and also memory and we reserved",
    "start": "2608009",
    "end": "2616589"
  },
  {
    "text": "resources in the accelerator depending on the size of the accelerator that you",
    "start": "2616589",
    "end": "2622079"
  },
  {
    "text": "have provisioned so you don't need to worry about you know how many chips are",
    "start": "2622079",
    "end": "2627329"
  },
  {
    "text": "there or how they are partitioned you know whether they're sharing resources etc because we guarantee the resources",
    "start": "2627329",
    "end": "2634649"
  },
  {
    "text": "that are specified in those three different sizes depending on the size",
    "start": "2634649",
    "end": "2640169"
  },
  {
    "text": "you choose and we can speak further about the question you have since we're the console what are we going why don't",
    "start": "2640169",
    "end": "2646709"
  },
  {
    "text": "we pause there and give poor Olivia a chance to finish disease rockin and ready with this demo about five more minutes then we'll have time for",
    "start": "2646709",
    "end": "2651839"
  },
  {
    "text": "questions afterwards okay so thanks for my word from our sponsor there that was",
    "start": "2651839",
    "end": "2657209"
  },
  {
    "text": "helpful now back to the program so to create the instance you just you would",
    "start": "2657209",
    "end": "2663269"
  },
  {
    "text": "launch a normal instance from the ec2 console and you would choose the deep",
    "start": "2663269",
    "end": "2668969"
  },
  {
    "text": "learning ami either a Linux or Ubuntu whichever one you wish again you can",
    "start": "2668969",
    "end": "2675239"
  },
  {
    "text": "choose any CPU instance that you want to work with in this case we could just keep it as the default and the main",
    "start": "2675239",
    "end": "2682139"
  },
  {
    "text": "thing that stands out here is there is now this elastic inference option that you would select to use elastic",
    "start": "2682139",
    "end": "2687539"
  },
  {
    "text": "inference with your instance so there are two things you'll see that pop up one is you need an IM role that is",
    "start": "2687539",
    "end": "2694259"
  },
  {
    "text": "allowed to connect to that instance there's also protections of whether you can talk to that or not",
    "start": "2694259",
    "end": "2699539"
  },
  {
    "text": "I have them created so that's fine and the other thing is it'll point out that you need that V PC endpoints for elastic",
    "start": "2699539",
    "end": "2707129"
  },
  {
    "text": "inference and so either you can create it or if you use the subnet that already",
    "start": "2707129",
    "end": "2712619"
  },
  {
    "text": "has that created that issue now goes away and from here this is where you",
    "start": "2712619",
    "end": "2717629"
  },
  {
    "text": "would choose the three different types of elastic inference accelerators that available and then you would go and",
    "start": "2717629",
    "end": "2724840"
  },
  {
    "text": "launch you wanna point something out but",
    "start": "2724840",
    "end": "2730690"
  },
  {
    "text": "by the way quick clarification how many of you have heard of the deep learning am i good number they are for those who",
    "start": "2730690",
    "end": "2740110"
  },
  {
    "text": "haven't they contain all the deep learning a.m. eyes are available for Amazon Linux and Ubuntu in the AWS",
    "start": "2740110",
    "end": "2747820"
  },
  {
    "text": "marketplace you can also find it in the Quick Start menu as love you just showed and they contain all the popular deep",
    "start": "2747820",
    "end": "2754510"
  },
  {
    "text": "learning frameworks the hardware acceleration drivers so it's a pre-baked thing that's available for you that you",
    "start": "2754510",
    "end": "2760510"
  },
  {
    "text": "can launch and run your models great so",
    "start": "2760510",
    "end": "2767050"
  },
  {
    "text": "how would this actually run so I have a another issue there it goes well this",
    "start": "2767050",
    "end": "2775840"
  },
  {
    "start": "2773000",
    "end": "2795000"
  },
  {
    "text": "part would be the imagine just a demo where I have a saved model in one",
    "start": "2775840",
    "end": "2780910"
  },
  {
    "text": "process I would run the server and in another process I would just run the client which would do some inference on",
    "start": "2780910",
    "end": "2786970"
  },
  {
    "text": "it we don't have to worry too much about that at this point now I'll actually I can't really show the results so what",
    "start": "2786970",
    "end": "2806230"
  },
  {
    "start": "2795000",
    "end": "2967000"
  },
  {
    "text": "sort of results do we get from this from the MVC NN model there's a picture in a",
    "start": "2806230",
    "end": "2812530"
  },
  {
    "text": "picture I don't know why that's there what we see what I what I shown here is",
    "start": "2812530",
    "end": "2817750"
  },
  {
    "text": "the three different cases one just running with the CPU two running with the GPU and three with the elastic",
    "start": "2817750",
    "end": "2824170"
  },
  {
    "text": "inference now running with the CPU we see that we only pay maybe 20 cents an",
    "start": "2824170",
    "end": "2829180"
  },
  {
    "text": "hour it's pretty cheap but the inference time is quite high whereas with the GP",
    "start": "2829180",
    "end": "2836760"
  },
  {
    "text": "whereas with the GPU the cost is quite high 90 cents an hour but the inference",
    "start": "2836760",
    "end": "2841930"
  },
  {
    "text": "time is very low about 50 milliseconds now the EEI instance here using the same CPU as a CPU instance and with an e I a",
    "start": "2841930",
    "end": "2849760"
  },
  {
    "text": "medium costs about a third of the GPU and it performs it's a",
    "start": "2849760",
    "end": "2855730"
  },
  {
    "text": "twice as slow but as an overall saving there there is the saving in terms of both time and cost now I haven't shown",
    "start": "2855730",
    "end": "2862119"
  },
  {
    "text": "here the using it with medium or large but if you use it with those because of the extra provision throughput and the",
    "start": "2862119",
    "end": "2868720"
  },
  {
    "text": "extra power that time will go down as the cost goes up the other case we",
    "start": "2868720",
    "end": "2875890"
  },
  {
    "text": "looked at the variational auto encoder a similar idea with CPU and GPU that CPU is cheap and slow GPU is more expensive",
    "start": "2875890",
    "end": "2883060"
  },
  {
    "text": "and fast and in this case this model was quite big the whole saved model was",
    "start": "2883060",
    "end": "2888160"
  },
  {
    "text": "about four and a half gigabytes the whole train model the saved model was a little less than that and so for our",
    "start": "2888160",
    "end": "2894280"
  },
  {
    "text": "needs we could only run it on an e I ax large and that's why this eed that the",
    "start": "2894280",
    "end": "2899830"
  },
  {
    "text": "cost point for the e I one is higher because it's using a CPU plus e I a 1x large but what we see is that even",
    "start": "2899830",
    "end": "2906430"
  },
  {
    "text": "though it's more expensive than just a smaller a ia it's both cheaper than the GPU and faster than the GPU so in this",
    "start": "2906430",
    "end": "2913869"
  },
  {
    "text": "case we were able to get a performance increase at a small fraction of the cost",
    "start": "2913869",
    "end": "2919020"
  },
  {
    "text": "and so I'm sorry for all the issues with the presentation but thank you it's all",
    "start": "2919020",
    "end": "2927070"
  },
  {
    "text": "throwback to the Dom now yeah appreciate it thank you so what was kind of shown there in the",
    "start": "2927070",
    "end": "2935440"
  },
  {
    "text": "in the graphs towards the end is that depending on your budget which is both",
    "start": "2935440",
    "end": "2943450"
  },
  {
    "text": "in terms of latency as well as cost you have AWS now provides a number of",
    "start": "2943450",
    "end": "2949570"
  },
  {
    "text": "options three different e aí sizes and a number of GPU sizes as well that you can",
    "start": "2949570",
    "end": "2955000"
  },
  {
    "text": "take advantage of depending on your latency and the the price you want to pay so so you have choice and there's",
    "start": "2955000",
    "end": "2964240"
  },
  {
    "text": "there's you could do that without any code changes so just to wrap up with a quick summary yeah I accelerators are",
    "start": "2964240",
    "end": "2972280"
  },
  {
    "start": "2967000",
    "end": "3031000"
  },
  {
    "text": "available for ec2 and sage maker in a number of sizes they help produce costs of running inference by up to 75% you",
    "start": "2972280",
    "end": "2980500"
  },
  {
    "text": "can call you can configure any instance type you're not limited to any instance particular instance type for attaching",
    "start": "2980500",
    "end": "2987040"
  },
  {
    "text": "an accelerator so you can choose the right tool that works for you it's",
    "start": "2987040",
    "end": "2992380"
  },
  {
    "text": "also available in CloudFormation so you can programmatically launch your",
    "start": "2992380",
    "end": "2998440"
  },
  {
    "text": "instances and template eyes those you can deploy tensorflow mx8 own onyx",
    "start": "2998440",
    "end": "3003990"
  },
  {
    "text": "models with no cape code changes and you can find a lot of the information about",
    "start": "3003990",
    "end": "3009329"
  },
  {
    "text": "how to use the product and some additional resources at our website and",
    "start": "3009329",
    "end": "3014520"
  },
  {
    "text": "there there is also it's also available on AWS forums and within the the",
    "start": "3014520",
    "end": "3021000"
  },
  {
    "text": "documentation you can find a handy email address to reach us in case you have",
    "start": "3021000",
    "end": "3026549"
  },
  {
    "text": "issues or you you'd like some questions to answer so thank you very much for",
    "start": "3026549",
    "end": "3033000"
  },
  {
    "start": "3031000",
    "end": "3047000"
  },
  {
    "text": "taking the time to attend today I hope you had a great reinvent hope you enjoy the replay party later tonight and",
    "start": "3033000",
    "end": "3039290"
  },
  {
    "text": "please do complete your session survey in the mobile app thank you very much",
    "start": "3039290",
    "end": "3045770"
  },
  {
    "text": "[Applause]",
    "start": "3045770",
    "end": "3048949"
  }
]