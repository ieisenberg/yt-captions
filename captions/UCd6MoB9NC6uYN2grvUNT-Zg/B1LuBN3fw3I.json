[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "good afternoon everyone thank you very much for coming here for the AWS summit today and in this session we are gonna",
    "start": "6560",
    "end": "13010"
  },
  {
    "text": "talk about how do you build a service data like on AWS my name is una pelea",
    "start": "13010",
    "end": "18380"
  },
  {
    "text": "I'm a Solutions Architect with the Amazon offers here in Singapore and on a day to day basis I get an opportunity to",
    "start": "18380",
    "end": "25220"
  },
  {
    "text": "work with a lot of customers who are building the data platforms on machine learning and artificial intelligence",
    "start": "25220",
    "end": "30279"
  },
  {
    "text": "applications on AWS I've also been honored by having Daniel Miller who's",
    "start": "30279",
    "end": "37310"
  },
  {
    "text": "the head of cloud infrastructure for spool who'll be coming and sharing his story about how did he build the data",
    "start": "37310",
    "end": "42950"
  },
  {
    "text": "Lake and how is this company benefitting by building a data Lake right so everybody is talking about data lakes",
    "start": "42950",
    "end": "49670"
  },
  {
    "text": "right so it's become like one of the biggest buzz words for the year 2018 let's take a look at what a data Lake is",
    "start": "49670",
    "end": "56180"
  },
  {
    "text": "right so so data Lake is an architecture",
    "start": "56180",
    "end": "62330"
  },
  {
    "start": "60000",
    "end": "60000"
  },
  {
    "text": "pattern that you put together where you'll be able to bring in data from different sources",
    "start": "62330",
    "end": "67549"
  },
  {
    "text": "be it your traditional databases no SQL databases or completely unstructured",
    "start": "67549",
    "end": "73370"
  },
  {
    "text": "data like log files or IOT information bring it to a common bring it to a",
    "start": "73370",
    "end": "78740"
  },
  {
    "text": "center place where you'll be able to store the data and make it available for processing and consumption for multiple",
    "start": "78740",
    "end": "84380"
  },
  {
    "text": "users right so this is what a data Lake is so how does it differ from a data warehousing system right in a date in a",
    "start": "84380",
    "end": "92180"
  },
  {
    "text": "traditional data warehousing system what you look at and what you build is basically a set of databases which will",
    "start": "92180",
    "end": "98210"
  },
  {
    "text": "have which will have the data in them and then a data analyst or a database administrator be our professional will",
    "start": "98210",
    "end": "104270"
  },
  {
    "text": "connect to the database pull the data and then build reports right but it is very custom it is very it is very",
    "start": "104270",
    "end": "110710"
  },
  {
    "text": "specifically built for reporting and bi and stuff right but now the needs of data teams are",
    "start": "110710",
    "end": "117020"
  },
  {
    "text": "increasingly are changing constantly right what does that mean so if you think about 10 years back like there",
    "start": "117020",
    "end": "123799"
  },
  {
    "text": "were hardly any data centers there's the data scientists and the companies right and now if you look at it there are like",
    "start": "123799",
    "end": "129170"
  },
  {
    "text": "data scientists everywhere right and they need access to raw data they need access to data that is clean that is raw",
    "start": "129170",
    "end": "135709"
  },
  {
    "text": "and they can draw inside sort of it right they can innovate you in the data that they have and build",
    "start": "135709",
    "end": "141800"
  },
  {
    "text": "machine learning models artificial intelligence models to go ahead and go",
    "start": "141800",
    "end": "147020"
  },
  {
    "text": "ahead and innovate for their customers right so with that let's take a look at what are the challenges that are being",
    "start": "147020",
    "end": "153320"
  },
  {
    "text": "faced by data teams across the globe right so first of all is the amount of",
    "start": "153320",
    "end": "159710"
  },
  {
    "start": "159000",
    "end": "159000"
  },
  {
    "text": "data that we are being exposed to on a day to day basis is increasing right so 10 years back a terabyte of data was",
    "start": "159710",
    "end": "166190"
  },
  {
    "text": "like huge right now we are talking about petabytes like starting with few petabytes two hexa bytes as well right",
    "start": "166190",
    "end": "172460"
  },
  {
    "text": "and the sources from where this data was coming in is also changing on a day to day basis right so you had traditionally",
    "start": "172460",
    "end": "179300"
  },
  {
    "text": "had only database engines that were giving you data right now you have data coming in from locked files now you have",
    "start": "179300",
    "end": "185030"
  },
  {
    "text": "data coming in from social media io d IOT sensors and stuff like that right so I've had a SmartWatch right that is",
    "start": "185030",
    "end": "191600"
  },
  {
    "text": "sending my information about my my activities what I do what is the heart",
    "start": "191600",
    "end": "196790"
  },
  {
    "text": "rate and everything to the cloud right and that they are analyze it and then they give me recommendations based on I",
    "start": "196790",
    "end": "202040"
  },
  {
    "text": "should be working out more or I should be standing more or whatever right so the amount of data is increasing on a day to day basis right and using one",
    "start": "202040",
    "end": "209360"
  },
  {
    "text": "data store to store all of this data using one mechanism to collect all of this data is not sufficient right and",
    "start": "209360",
    "end": "216020"
  },
  {
    "text": "that's where we need to have a data like architecture that will be building to ingest data from different sources keep",
    "start": "216020",
    "end": "223130"
  },
  {
    "text": "it in a centralized location and then analyze it from there right so second is the who are the",
    "start": "223130",
    "end": "228230"
  },
  {
    "text": "consumers of the data right so I spoke about this earlier the the consumers of the data are also changing on a day to",
    "start": "228230",
    "end": "233960"
  },
  {
    "text": "day basis right so you have you you previously had data analysts and bi professionals who would access the data",
    "start": "233960",
    "end": "240680"
  },
  {
    "text": "build reports do do charting and dashboarding on it right now you have data scientists who need access to the",
    "start": "240680",
    "end": "248120"
  },
  {
    "text": "data and RAW format they process the data and then they make it available for consumption right and then obviously now",
    "start": "248120",
    "end": "254540"
  },
  {
    "text": "every company is in fact every company is interacting with other companies and they want to build integrations right",
    "start": "254540",
    "end": "260480"
  },
  {
    "text": "and for that integrations it's not I mean 10 years back the way a bank 1 bank would be sending data the other bank",
    "start": "260480",
    "end": "266750"
  },
  {
    "text": "would be placing it in an FTP server right and then the other bank would come in and pick it up from there right it doesn't work anymore it's not scalable",
    "start": "266750",
    "end": "273260"
  },
  {
    "text": "right it's not fast enough so what do you do now you create api's right so everybody is talking about API is everybody is",
    "start": "273260",
    "end": "279720"
  },
  {
    "text": "talking about service computing right and how do you do this is by building API building a data like building an API",
    "start": "279720",
    "end": "286800"
  },
  {
    "text": "abstraction layer on top of the data like having your third party integrators connect to the API and fetch the data in",
    "start": "286800",
    "end": "293460"
  },
  {
    "text": "the format that they need right so with that that is the diversified set of users right and obviously the way these",
    "start": "293460",
    "end": "300270"
  },
  {
    "text": "users interact has also change right so data scientists use tools like Zeppelin",
    "start": "300270",
    "end": "305370"
  },
  {
    "text": "notebook or Jupiter notebook to connect to the connect to the data and extract the data and transform it right and",
    "start": "305370",
    "end": "311760"
  },
  {
    "text": "obviously you still have your BI tools like tableau quick side click and stuff",
    "start": "311760",
    "end": "317370"
  },
  {
    "text": "like that right where they'll be connecting they'll be drawing and visualizing the data so when you think",
    "start": "317370",
    "end": "322620"
  },
  {
    "text": "about the data like it should solve these problems but while it's solving these three problems it should also have",
    "start": "322620",
    "end": "329460"
  },
  {
    "text": "some characteristics right so first of all as I said it should be able to collect any kind of data right so you",
    "start": "329460",
    "end": "335850"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "should be able to collect any kind of data that comes in so it should be structured data unstructured data a semi",
    "start": "335850",
    "end": "342300"
  },
  {
    "text": "structured data right so any kind of data it should be able to ingest videos audios and pretty much everything under",
    "start": "342300",
    "end": "349140"
  },
  {
    "text": "the Sun right with that you should be able to give access to the data",
    "start": "349140",
    "end": "354150"
  },
  {
    "text": "scientists or business users of the in the data in a way that they want so that",
    "start": "354150",
    "end": "359340"
  },
  {
    "text": "they can dive deep and the flexibility of access where they should be able to access it in the form that they want",
    "start": "359340",
    "end": "365910"
  },
  {
    "text": "right so for data scientists requires access to raw data like in text format right they should be able to get it if",
    "start": "365910",
    "end": "371250"
  },
  {
    "text": "there is a business is there is a business user who only is interested in aggregated data you should be thinking",
    "start": "371250",
    "end": "377280"
  },
  {
    "text": "about aggregated data like you should be thinking about making that aggregate data available right and last but not",
    "start": "377280",
    "end": "383130"
  },
  {
    "text": "the least if you're spending time building an architecture that is going to last for long with your like together",
    "start": "383130",
    "end": "389520"
  },
  {
    "text": "with your business you want to build something that is future proof right you don't want to build something today that",
    "start": "389520",
    "end": "394830"
  },
  {
    "text": "is going to become obsolete after three years right because otherwise that will end up instead of innovating you will",
    "start": "394830",
    "end": "400320"
  },
  {
    "text": "end up collecting technical depth right you don't want to be in a situation where you have collector technical depth",
    "start": "400320",
    "end": "405570"
  },
  {
    "text": "right so that's where building something that is future proof is important when I say future proof two",
    "start": "405570",
    "end": "411180"
  },
  {
    "text": "things come into my mind right first everything should be secure right otherwise nobody's going to use it or I will not be happy keeping my data in",
    "start": "411180",
    "end": "418290"
  },
  {
    "text": "there because it's not going to be secure right the second is it should not be costing me huge amount of money so it should be",
    "start": "418290",
    "end": "424110"
  },
  {
    "text": "cheaper to run right so I need something that is cheaper to run and I need something that is secure right so these",
    "start": "424110",
    "end": "429840"
  },
  {
    "text": "are the two things that come into mind when I'm thinking about future proofing so throughout the session what we'll be",
    "start": "429840",
    "end": "435060"
  },
  {
    "text": "doing is we'll be taking an example of a customer or like a company that is building for those of you who have come",
    "start": "435060",
    "end": "441750"
  },
  {
    "text": "from Craig session before lunch he spoke about how there's a headphone manufacturing company that is like a",
    "start": "441750",
    "end": "448200"
  },
  {
    "text": "smart headphone manufacturing company that is generating information that is collecting information and sending it to",
    "start": "448200",
    "end": "454740"
  },
  {
    "text": "the cloud for analytics right so we'll be sticking to that example we'll be we'll be looking at emulating some",
    "start": "454740",
    "end": "461010"
  },
  {
    "start": "459000",
    "end": "459000"
  },
  {
    "text": "sensor data we'll send it to the cloud and this is the architecture that we want to build right so we want to build",
    "start": "461010",
    "end": "467220"
  },
  {
    "text": "a data like architecture that will be serving us in the first place why serval",
    "start": "467220",
    "end": "472650"
  },
  {
    "text": "is because we are living in the 21st century and it's 2018 we hate servers right so the second is it should be able",
    "start": "472650",
    "end": "479550"
  },
  {
    "text": "to ingest any kind of data and store information and store unlimited amount",
    "start": "479550",
    "end": "485040"
  },
  {
    "text": "of information right we should be able to figure out what is the data right I'm going to dump the data into the storage",
    "start": "485040",
    "end": "490500"
  },
  {
    "text": "but I want to have the system to be intelligent enough to go ahead and scan that data once the data is scant I want",
    "start": "490500",
    "end": "496710"
  },
  {
    "text": "that to know what is the data that is I want to capture the metadata and then go ahead and analyze that data and",
    "start": "496710",
    "end": "502770"
  },
  {
    "text": "visualize the data after none on analytics right so this is what the journey that I want to take you through",
    "start": "502770",
    "end": "507960"
  },
  {
    "text": "throughout the session right so we're going to use this diagram as the anchor",
    "start": "507960",
    "end": "514380"
  },
  {
    "text": "throughout the session right so on the left hand side what you see like my left hand side also yours so when the left",
    "start": "514380",
    "end": "520050"
  },
  {
    "text": "hand side what you see is a set of devices that will be sending data to the cloud right so obviously I don't have",
    "start": "520050",
    "end": "526460"
  },
  {
    "text": "like hardware devices here I'm going to use a Python script that is going to like our web interface that is going to",
    "start": "526460",
    "end": "532880"
  },
  {
    "text": "plug put the pub push the data token to our injection mechanism and then have",
    "start": "532880",
    "end": "540540"
  },
  {
    "text": "that transform and visualize right so let's take a look at how do you ingest the data into the cloud right it's depending",
    "start": "540540",
    "end": "547950"
  },
  {
    "text": "on what is the type of data that you are dealing with right depending on what is the type of data that you are dealing",
    "start": "547950",
    "end": "554100"
  },
  {
    "start": "552000",
    "end": "552000"
  },
  {
    "text": "with how do you bring in that data it will also deform right so if you have let's say 50 terabytes of data base that",
    "start": "554100",
    "end": "562620"
  },
  {
    "text": "is sitting in your on-premise environment right or some other cloud provider that you want to bring into AWS for analytics right so it won't be it",
    "start": "562620",
    "end": "569730"
  },
  {
    "text": "won't be wise to copy that over the internet because 50 terabytes will take forever to copy that right so we have a",
    "start": "569730",
    "end": "576710"
  },
  {
    "text": "solution called AWS snowball so snowball is a hardware device that is",
    "start": "576710",
    "end": "581910"
  },
  {
    "text": "like knee-length like my knee length and you can that is the demo device outside of the boot so you can go ahead and take",
    "start": "581910",
    "end": "587550"
  },
  {
    "text": "a look at that but it's an offline device you can bring it into your data center plug that plug the device into",
    "start": "587550",
    "end": "592740"
  },
  {
    "text": "your servers move the data and then ship it back to AWS and we will bring that data into the cloud right we will bring",
    "start": "592740",
    "end": "599220"
  },
  {
    "text": "that data into s3 and you'll be able to use it right and similarly there are different ways how you can connect if",
    "start": "599220",
    "end": "605010"
  },
  {
    "text": "you want to connect your on-premise infrastructure to AWS and have a persistent connection throughout the",
    "start": "605010",
    "end": "611790"
  },
  {
    "text": "time you can use AWS Direct Connect so Direct Connect is a dedicated network line that will be pulling from your data",
    "start": "611790",
    "end": "618420"
  },
  {
    "text": "center to our connect our connections and you'll be able to have a dedicated network bandwidth to have that",
    "start": "618420",
    "end": "624990"
  },
  {
    "text": "communication right and then obviously in this case we have we are looking at very fast-moving data right we are",
    "start": "624990",
    "end": "631170"
  },
  {
    "text": "looking at thousands of devices that might be connecting and sending information so we will be using a streaming service so the streaming",
    "start": "631170",
    "end": "637500"
  },
  {
    "start": "637000",
    "end": "637000"
  },
  {
    "text": "service that we'll be using today is called Amazon Kinesis so Amazon Kinesis",
    "start": "637500",
    "end": "643320"
  },
  {
    "text": "is a is a is a real-time is a service that lets you build real time applications on AWS and it lets you",
    "start": "643320",
    "end": "650820"
  },
  {
    "text": "collect terabytes and terabytes of data in an hour at a really fast pace and a really big volume right so Kinesis",
    "start": "650820",
    "end": "658410"
  },
  {
    "text": "streams as I managed is a fully managed streaming service on the other hand that",
    "start": "658410",
    "end": "663450"
  },
  {
    "text": "is Kinesis far hose so far hose is a managed service similar to streams but what it also gives you is it takes the",
    "start": "663450",
    "end": "670200"
  },
  {
    "text": "data and it delivers the data to an endpoint right so you can tell firehose that Farrah's I'm going to dump data into the",
    "start": "670200",
    "end": "676710"
  },
  {
    "text": "pipe can you deliver that data into my data warehouse right it will be able to deliver that they tend to redshift right",
    "start": "676710",
    "end": "682290"
  },
  {
    "text": "so to be able to deliver the data into a specific endpoint that you mentioned during the configuration the third",
    "start": "682290",
    "end": "688620"
  },
  {
    "text": "service that we have as a part of the Kinesis family is Kinesis analytics so",
    "start": "688620",
    "end": "693780"
  },
  {
    "text": "Kenny if you think about kinases as a pipe that is a pipe from which data is traveling Kenny says analytics gives you",
    "start": "693780",
    "end": "700320"
  },
  {
    "text": "a glass window on top of that pipe right so what it gives you is it gives you the ability to run sequel statements inside",
    "start": "700320",
    "end": "707310"
  },
  {
    "text": "on top of the data that is going through the pipe and analyze the data using sequel statements right so you'll be",
    "start": "707310",
    "end": "713880"
  },
  {
    "text": "able to you'll be able to you'll be able to get the data back you'll be able to",
    "start": "713880",
    "end": "719370"
  },
  {
    "text": "get the data back as and when the event happens and you can take an action the next service that we have in the",
    "start": "719370",
    "end": "725460"
  },
  {
    "text": "Canisius streaming in the Kinesis family is video streams so video streams is a service that lets you capture video",
    "start": "725460",
    "end": "732510"
  },
  {
    "text": "frames send it to the cloud and have that analyzed using your machine learning or image recognition models",
    "start": "732510",
    "end": "739200"
  },
  {
    "text": "that you have built right so let's take a look at how will this fit into our architecture so this is how firehose",
    "start": "739200",
    "end": "745680"
  },
  {
    "start": "742000",
    "end": "742000"
  },
  {
    "text": "works you have the data on the left side the data comes in into firehose and you can specify any of these four endpoints",
    "start": "745680",
    "end": "752430"
  },
  {
    "text": "you can have the data sent to s3 which is our highly scalable and durable store",
    "start": "752430",
    "end": "757680"
  },
  {
    "text": "object store you can send that data to redshift which is our data warehousing technology or you can store that data",
    "start": "757680",
    "end": "763710"
  },
  {
    "text": "directly into an Amazon Elastic search cluster and or if you're using Splunk",
    "start": "763710",
    "end": "768960"
  },
  {
    "text": "you can directly have that data delivered into Splunk as well okay so this is how it affects our architecture",
    "start": "768960",
    "end": "776010"
  },
  {
    "text": "we have the data that data goes into fire hose okay the next organic thing to do is now",
    "start": "776010",
    "end": "782310"
  },
  {
    "text": "I've collected the data I need to store the data somewhere so that I can have that analyzed right let's look at let's",
    "start": "782310",
    "end": "789000"
  },
  {
    "text": "take a look at what are the storage options available so in this case we'll be using Amazon s3 Y is 3 because we are",
    "start": "789000",
    "end": "796860"
  },
  {
    "text": "building a big data system and with big data system one of the most important things is to make it future proof we",
    "start": "796860",
    "end": "802740"
  },
  {
    "text": "need to focus on cost and we need to focus on security Amazon s3 gives you unlimited amount of",
    "start": "802740",
    "end": "809590"
  },
  {
    "text": "storage you can store unlimited amount of objects in it and you can have security that is security of data at",
    "start": "809590",
    "end": "816430"
  },
  {
    "text": "rest using encryption and you can have security of data in transit using SSL",
    "start": "816430",
    "end": "822610"
  },
  {
    "text": "right so you you you you have check marked all the required things that you",
    "start": "822610",
    "end": "828220"
  },
  {
    "text": "need in a big data storage system right and that's why I'll be using Amazon s3 so this is how it looks at our it looks",
    "start": "828220",
    "end": "835540"
  },
  {
    "text": "in our architecture so we have collected the data put the data and firehose we are storing the data now in s3 so now",
    "start": "835540",
    "end": "842740"
  },
  {
    "text": "what I'll do is I'll go to my laptop and show you how are we going to generate",
    "start": "842740",
    "end": "847780"
  },
  {
    "text": "the data put it into firehose and store it in s3 so I'm going to run you through a live demo so let's pray to the demo",
    "start": "847780",
    "end": "853870"
  },
  {
    "text": "gods that everything works well that the",
    "start": "853870",
    "end": "863110"
  },
  {
    "text": "guys on the fifth row can you give me a thumbs up if you are able to see the font properly cool perfect thank you",
    "start": "863110",
    "end": "869380"
  },
  {
    "text": "very much so this is my AWS console what",
    "start": "869380",
    "end": "874420"
  },
  {
    "text": "I'm going to do is look for the service called Kinesis and I'll open Kinesis console right and I say get started I'm",
    "start": "874420",
    "end": "884710"
  },
  {
    "text": "navigating to the Canisius console and saying I want to create a new I want to",
    "start": "884710",
    "end": "889750"
  },
  {
    "text": "create a new can assist a fire hose delivery string so a delivery stream I'm saying my delivery stream name should be",
    "start": "889750",
    "end": "898620"
  },
  {
    "text": "summit stream ok and I'm coming down I'm telling it how do I want to store the",
    "start": "898620",
    "end": "904810"
  },
  {
    "text": "data as how do I want to store the data and I say next it's asking me do I want",
    "start": "904810",
    "end": "913390"
  },
  {
    "text": "to make any changes to the changes using transformation so I say next",
    "start": "913390",
    "end": "919920"
  },
  {
    "text": "so just give me a second guys I think I'm having Network issues usually at a",
    "start": "925079",
    "end": "960370"
  },
  {
    "text": "conference that is this big there are some of the other small issues that we face so just bear with me for a few seconds while I fix internet connection",
    "start": "960370",
    "end": "966970"
  },
  {
    "text": "that we have all right looks like be",
    "start": "966970",
    "end": "975490"
  },
  {
    "text": "alive again thank you for your patience so we go ahead and create a Kinesis",
    "start": "975490",
    "end": "982509"
  },
  {
    "text": "Faro's delivery stream we put the stream name as summit stream and i'm telling it",
    "start": "982509",
    "end": "987639"
  },
  {
    "text": "that i'm you're going to put the data directly into the stream right I say next it's asking me do I want to transform the data while it is in the",
    "start": "987639",
    "end": "994449"
  },
  {
    "text": "pipe I said no and I do next remember I told you there are four endpoints that you can put the data and",
    "start": "994449",
    "end": "1000060"
  },
  {
    "text": "s3 put the data from firehose so we'll be choosing s3 in our case",
    "start": "1000060",
    "end": "1005279"
  },
  {
    "text": "right and what is the bucket where I want to store the data so if I go to the",
    "start": "1005279",
    "end": "1011040"
  },
  {
    "text": "bucket I'll choose the bucket name as unique a demo bucket and what is the directory where I want to store this",
    "start": "1011040",
    "end": "1017040"
  },
  {
    "text": "data so I'll be calling it raw data raw right because my data that I'm putting in as a raw data that I'm dealing with",
    "start": "1017040",
    "end": "1023399"
  },
  {
    "text": "the demo so I'll go ahead and click Next it's asking me how frequently do I want",
    "start": "1023399",
    "end": "1030178"
  },
  {
    "text": "to dump the data from firehose to s3 so I'll say either 1 MB or I'll say 60",
    "start": "1030179",
    "end": "1037168"
  },
  {
    "text": "seconds so whichever comes earlier it will dump the data I'm going to use no encryption no compression for the demo",
    "start": "1037169",
    "end": "1043048"
  },
  {
    "text": "and I'll create an iamb role so that my fire hose has the access to go and dump",
    "start": "1043049",
    "end": "1048870"
  },
  {
    "text": "the data into s3 right so I'll allow fire hose to go and dump the return to",
    "start": "1048870",
    "end": "1054450"
  },
  {
    "text": "s3 I'll go and do next and I'll just all the settings that are here right so",
    "start": "1054450",
    "end": "1059700"
  },
  {
    "text": "I said what is the stream name this is the s3 bucket where the data is going to go and the data is going to get dumped",
    "start": "1059700",
    "end": "1065520"
  },
  {
    "text": "into s3 every one MB or one minute whichever comes earlier and we'll do",
    "start": "1065520",
    "end": "1071280"
  },
  {
    "text": "create delivery stream so this should",
    "start": "1071280",
    "end": "1076920"
  },
  {
    "text": "take almost approximately one minute - one minute to spin up in the meantime",
    "start": "1076920",
    "end": "1083010"
  },
  {
    "text": "I'll take you to the AWS s3 console and show what is happening there right so I've already got an s3 bucket that I've",
    "start": "1083010",
    "end": "1089280"
  },
  {
    "text": "created for the purposes of the demo and I've placed some reference data in there",
    "start": "1089280",
    "end": "1094710"
  },
  {
    "text": "but that is not the raw data that we'll be using so under the data folder if you see that",
    "start": "1094710",
    "end": "1103290"
  },
  {
    "text": "is a reference data full term so this data has information on who is the who",
    "start": "1103290",
    "end": "1109500"
  },
  {
    "text": "is what is the music what is the who is the artist for the music and what is the",
    "start": "1109500",
    "end": "1114510"
  },
  {
    "text": "track ID of that particular song and what is that my smart headphone user is listening - right so once we dump the",
    "start": "1114510",
    "end": "1121890"
  },
  {
    "text": "data we should be able to see another folder called raw data in here and that will come in right so looks like my fire",
    "start": "1121890",
    "end": "1128040"
  },
  {
    "text": "should be spun up by now so if i refresh this page it should show active right so it shows active here so let me go to",
    "start": "1128040",
    "end": "1134850"
  },
  {
    "text": "my Kinesis data generator so this is a tool that was built by our Kinesis team for generating dummy data so that you",
    "start": "1134850",
    "end": "1141570"
  },
  {
    "text": "can play around and test Kinesis right so I'm going to log into my Kinesis data",
    "start": "1141570",
    "end": "1147210"
  },
  {
    "text": "generator and generate some raw data or for the demo so we'll be using the",
    "start": "1147210",
    "end": "1154110"
  },
  {
    "text": "region u.s. East one so US East one and the delivery stream is summit stream",
    "start": "1154110",
    "end": "1160710"
  },
  {
    "text": "right I'm going to be a bit mean and send two thousand records per second right and the data that I'm sending here",
    "start": "1160710",
    "end": "1167730"
  },
  {
    "text": "is basically a random UID what is the time stamp on the device what is the temperature of the person who is wearing",
    "start": "1167730",
    "end": "1174780"
  },
  {
    "text": "the device and what is the activity type is the person running walking or",
    "start": "1174780",
    "end": "1179940"
  },
  {
    "text": "whatever right and then I'm sending the data in here so I'll send the data and what will happen now is it will go and",
    "start": "1179940",
    "end": "1186720"
  },
  {
    "text": "send the data into fire hose and fire hose will be able to send the data into",
    "start": "1186720",
    "end": "1192090"
  },
  {
    "text": "and make it available for analytics so let's stop the Kinesis data generator",
    "start": "1192090",
    "end": "1198570"
  },
  {
    "text": "and see if the data has been has the data come too far hos right so we will",
    "start": "1198570",
    "end": "1203789"
  },
  {
    "text": "refresh here we'll just wait make sure that everything is ok come back to s3 and refresh this page so the reader",
    "start": "1203789",
    "end": "1210480"
  },
  {
    "text": "should have come here right so the data is here so we have the raw folder and inside the raw folder I should have data that is 1 MB each if",
    "start": "1210480",
    "end": "1219210"
  },
  {
    "text": "you see this if you see this bit here sorry about the font size guys so if you",
    "start": "1219210",
    "end": "1225090"
  },
  {
    "text": "see this bit here you have the data and you have the date month day right so we",
    "start": "1225090",
    "end": "1232740"
  },
  {
    "text": "have the year month date and the are of the day when the data was data came in right so I've filed it as each file as 1",
    "start": "1232740",
    "end": "1239370"
  },
  {
    "text": "MB so with that we have ingested the data and stored the data the next thing to do is how do we go ahead and catalog",
    "start": "1239370",
    "end": "1245460"
  },
  {
    "text": "the data every team can I have the screen switched piece thank you so now",
    "start": "1245460",
    "end": "1251370"
  },
  {
    "text": "we have now we have ingested the data stored the data into s3 the next thing to do is we need to catalog the data",
    "start": "1251370",
    "end": "1257070"
  },
  {
    "text": "right when I say catalog that data I need something that will go ahead and scan my data so that is where we'll be",
    "start": "1257070",
    "end": "1263130"
  },
  {
    "text": "using AWS glue so AWS glue is a service that you that you use to crawl your data",
    "start": "1263130",
    "end": "1269630"
  },
  {
    "text": "catalog your data and transform your data using serverless ETL that is extract transform and load right so glue",
    "start": "1269630",
    "end": "1276630"
  },
  {
    "text": "crawler what it will do is it will go in and you can point it to an s3 bucket or any database that can talk to talk in",
    "start": "1276630",
    "end": "1283770"
  },
  {
    "text": "JDBC you can point the crawler to that particular data source and it will go ahead scan the data in for the schema so",
    "start": "1283770",
    "end": "1291299"
  },
  {
    "text": "that is the most beautiful thing about glue group products so it will infer the schema tell what is the format of the",
    "start": "1291299",
    "end": "1297480"
  },
  {
    "text": "data and then it will go ahead and create a catalogue of the data right once the catalog is created you will go",
    "start": "1297480",
    "end": "1304529"
  },
  {
    "text": "ahead and write a job to transform that data so that transformation bit is what will be calling ETL and using glue you",
    "start": "1304529",
    "end": "1312390"
  },
  {
    "text": "do it using a service glue is a service technology right so glue is backed by so glue is backed",
    "start": "1312390",
    "end": "1321780"
  },
  {
    "text": "by Apache spark and it gives you a completely managed environment to run your job right so",
    "start": "1321780",
    "end": "1328679"
  },
  {
    "text": "with that let's take a look at how would this affect our demo architecture so with the demo architecture what we'll be",
    "start": "1328679",
    "end": "1335159"
  },
  {
    "start": "1333000",
    "end": "1333000"
  },
  {
    "text": "doing is we'll be creating a crawler that crawler would go ahead and crawl the data in the s3 bucket write it to",
    "start": "1335159",
    "end": "1341700"
  },
  {
    "text": "the catalogue once the data is available in the catalogue you'd be using the glue scripts to run a transformation job and",
    "start": "1341700",
    "end": "1349080"
  },
  {
    "text": "write the process data into the s3 bucket right I hope you like guys the animation I spent 20 minutes building",
    "start": "1349080",
    "end": "1354149"
  },
  {
    "text": "that animation so a team can we have",
    "start": "1354149",
    "end": "1361109"
  },
  {
    "text": "this screen switch please thank you so",
    "start": "1361109",
    "end": "1368519"
  },
  {
    "text": "alright so we are back in the AWS console let me show so we showed I showed you the the data that has come in",
    "start": "1368519",
    "end": "1375479"
  },
  {
    "text": "from a fire hose and into s3 right let's go to the AWS Glu console and create a",
    "start": "1375479",
    "end": "1380849"
  },
  {
    "text": "crawler so this is the AWS glue console",
    "start": "1380849",
    "end": "1388499"
  },
  {
    "text": "so currently there are no tables here because there are no tables here right because I don't have haven't created any",
    "start": "1388499",
    "end": "1395369"
  },
  {
    "text": "crawlers so I'm going to do and so I'm going to do create a crawler and I'm",
    "start": "1395369",
    "end": "1401399"
  },
  {
    "text": "going to call the crawler like summit demo crawler right and I'll say next and",
    "start": "1401399",
    "end": "1407129"
  },
  {
    "text": "I'm telling it what is the data source that I'm interested in right so the data sort of sitting in s3 and which folder",
    "start": "1407129",
    "end": "1413549"
  },
  {
    "text": "is the data sitting in so my data is sitting in the unni summit demo folder and inside the data bucket inside the",
    "start": "1413549",
    "end": "1420869"
  },
  {
    "text": "data folder right and I'll go next click on next it's asking me do I want to add in something else I said no and give",
    "start": "1420869",
    "end": "1428099"
  },
  {
    "text": "access to Glu to go and access my data right and I do next and I want to run it",
    "start": "1428099",
    "end": "1434879"
  },
  {
    "text": "on demand and it is asking me do I want to create a database so obviously I want to create a database and I'll call it",
    "start": "1434879",
    "end": "1441239"
  },
  {
    "text": "summit dB right and I create it and say next once",
    "start": "1441239",
    "end": "1448619"
  },
  {
    "text": "everything is done I'll do finish and I'll click on run the crawler now so this should take approximately 20",
    "start": "1448619",
    "end": "1454739"
  },
  {
    "text": "seconds what it is doing now is it is going to is going to mice 3 bucket looking at the",
    "start": "1454739",
    "end": "1460650"
  },
  {
    "text": "data opening the files figuring out what is the format of the file figuring out what is the format in which my data is",
    "start": "1460650",
    "end": "1466950"
  },
  {
    "text": "set up inside the file that is a schema of the file and come back and update the crawler right so by that time I mean in",
    "start": "1466950",
    "end": "1473160"
  },
  {
    "text": "this meantime I think it should be done now it says it says 27 21 seconds",
    "start": "1473160",
    "end": "1478980"
  },
  {
    "text": "running so we'll go to table and we'll just be like impatient and click keep hitting refresh all right cool so we",
    "start": "1478980",
    "end": "1485520"
  },
  {
    "text": "have the data in here so we have the data that is yeah it has identified that that is a raw table in mice 3 bucket",
    "start": "1485520",
    "end": "1492060"
  },
  {
    "text": "right so the raw table in the s3 bucket is there and with that I'm able to see",
    "start": "1492060",
    "end": "1499710"
  },
  {
    "text": "that there are these there are these fields for that is 3 in that s3 file",
    "start": "1499710",
    "end": "1504810"
  },
  {
    "text": "right if I go back and go to reference data I should see that the reference data has these particular fields so it",
    "start": "1504810",
    "end": "1511890"
  },
  {
    "text": "has a IDE it has sorry this is the raw data sorry so raw data has all my ID times time",
    "start": "1511890",
    "end": "1518970"
  },
  {
    "text": "temperature and stuff right with the reference data if I go and do the reference data it should show me my reference information that is a track ID",
    "start": "1518970",
    "end": "1525480"
  },
  {
    "text": "name and the artist name so let me quickly go ahead and transform this data use a job to do it right so I'll go to",
    "start": "1525480",
    "end": "1531780"
  },
  {
    "text": "create an odd job and I'll say summit demo go ahead and create a go ahead and",
    "start": "1531780",
    "end": "1539610"
  },
  {
    "text": "create a user use the existing role and say next right sorry",
    "start": "1539610",
    "end": "1547170"
  },
  {
    "text": "I'll just use an empty script for the purpose of the demo we'll do next next and finish right so what it will give me",
    "start": "1547170",
    "end": "1554160"
  },
  {
    "text": "is it will give me an editor where I'll be able to write my job let me copy some code and put it in the editor right",
    "start": "1554160",
    "end": "1563510"
  },
  {
    "text": "so I'll go ahead and copy this code into the glue editor and see what it does right so glue editor has a very cool",
    "start": "1573570",
    "end": "1580480"
  },
  {
    "text": "feature where it can actually look at your code and in for what you are trying to do and build a diagram so if I save",
    "start": "1580480",
    "end": "1586210"
  },
  {
    "text": "this and click on generate diagram it will go ahead and create a diagram for me and telling me what I'm doing right",
    "start": "1586210",
    "end": "1592630"
  },
  {
    "text": "let me quickly run you through what I'm doing so I'm taking reference data I am taking the raw data joining both of the",
    "start": "1592630",
    "end": "1599230"
  },
  {
    "text": "data dropping unnecessary fields from it and then storing the data back into s3 right so I'll be going and running this",
    "start": "1599230",
    "end": "1605650"
  },
  {
    "text": "job now but instead of running it from the console I'll be running it using a zeppelin notebook so Zeppelin is a tool",
    "start": "1605650",
    "end": "1611190"
  },
  {
    "text": "from the Apache ecosystem that is used by data scientists to explore and write data science jobs right so I'll be using",
    "start": "1611190",
    "end": "1618370"
  },
  {
    "text": "my development notebook and running the code here let me quickly run you through what I'm going to do here right so I'm",
    "start": "1618370",
    "end": "1624520"
  },
  {
    "text": "inputting the libraries I'm going and telling glue that glue go ahead and read the data that is there in the summit DB",
    "start": "1624520",
    "end": "1631450"
  },
  {
    "text": "database and the name of the table is draw right and I'm printing the printing",
    "start": "1631450",
    "end": "1637000"
  },
  {
    "text": "the schema of that data so this is the schema of the data right now I'm",
    "start": "1637000",
    "end": "1642400"
  },
  {
    "text": "counting how many rows did I ingest so ingest at 30,000 rows now I'm taking the reference data telling glue to go and",
    "start": "1642400",
    "end": "1648820"
  },
  {
    "text": "read the data once everything is done I'm again counting the reference data and it is hundred records right now I'm",
    "start": "1648820",
    "end": "1655720"
  },
  {
    "text": "applying a transformation called join which I'm joining the rod of raw data with the reference data and the new the",
    "start": "1655720",
    "end": "1662950"
  },
  {
    "text": "new schema looks like this and because there are a lot of unnecessary fields here like partition ID and stuff like",
    "start": "1662950",
    "end": "1668800"
  },
  {
    "text": "that I'm going to let go of that and drop the drop those fields once those fields are dropped I'm going to write",
    "start": "1668800",
    "end": "1675280"
  },
  {
    "text": "the data into s3 so if you see this this is what I'm doing and say write the dynamic frame dynamic frame is the data",
    "start": "1675280",
    "end": "1682060"
  },
  {
    "text": "structure in Spock write the dynamic frame into s3 and this is the path where I want to write the dynamic frame right",
    "start": "1682060",
    "end": "1688690"
  },
  {
    "text": "and once everything is done I'm again writing a small bit of Python code here to go ahead and update the catalog so I",
    "start": "1688690",
    "end": "1695470"
  },
  {
    "text": "think by now the catalog should have been updated right so it says that tables have been updated",
    "start": "1695470",
    "end": "1701560"
  },
  {
    "text": "so if I go down now if I go to the glue console again and look at the catalog I",
    "start": "1701560",
    "end": "1707110"
  },
  {
    "text": "should have three tables sitting there right so now have raw data reference",
    "start": "1707110",
    "end": "1712300"
  },
  {
    "text": "data and process data right so we'll go back to the main presentation and I will",
    "start": "1712300",
    "end": "1718240"
  },
  {
    "text": "look at how we are going to analyze this data and also visualize the data right so so far what we have done is we have",
    "start": "1718240",
    "end": "1726450"
  },
  {
    "text": "generated the data brought the data into the cloud using Kinesis from Kinesis the",
    "start": "1726450",
    "end": "1732790"
  },
  {
    "text": "data came into s3 we use the crawlers to crawl the data from crawl the data from",
    "start": "1732790",
    "end": "1738310"
  },
  {
    "text": "s3 update the catalog run an ETL job had that job process the data and dump the",
    "start": "1738310",
    "end": "1743920"
  },
  {
    "text": "data into s3 again right now we need to find some way to run SQL statements on",
    "start": "1743920",
    "end": "1749080"
  },
  {
    "text": "it so that we can analyze the data and visualize it using some visualization tool right so for analysis and for",
    "start": "1749080",
    "end": "1756370"
  },
  {
    "start": "1756000",
    "end": "1756000"
  },
  {
    "text": "analytics in AWS there are multiple options available right so Amazon EMR is",
    "start": "1756370",
    "end": "1761800"
  },
  {
    "text": "a managed Hadoop offering so if you are looking for tools like Apache Hadoop spark hive Pig and tools from the Hadoop",
    "start": "1761800",
    "end": "1768880"
  },
  {
    "text": "ecosystem Apache sorry Amazon EMR allows you to run Hadoop in a managed fashion",
    "start": "1768880",
    "end": "1776050"
  },
  {
    "text": "right and or if you are looking for a simple data warehousing solution that will help you draw reporting I do do",
    "start": "1776050",
    "end": "1784450"
  },
  {
    "text": "reporting draw dashboards you'll be used looking at Amazon redshift so Amazon",
    "start": "1784450",
    "end": "1789550"
  },
  {
    "text": "redshift is a petabyte scale data warehousing service from AWS and which is fully managed right for this demo",
    "start": "1789550",
    "end": "1797290"
  },
  {
    "text": "we'll be using Athena so Athena is that very interesting analytics tool right so",
    "start": "1797290",
    "end": "1802600"
  },
  {
    "start": "1801000",
    "end": "1801000"
  },
  {
    "text": "you have data sitting in s3 how do I go ahead and access that data using SQL without actually launching a database",
    "start": "1802600",
    "end": "1808390"
  },
  {
    "text": "that is what Athena solves so Athena is a interactive query service that gives",
    "start": "1808390",
    "end": "1813460"
  },
  {
    "text": "you access to your data using SQL statements and directly the data that is",
    "start": "1813460",
    "end": "1819250"
  },
  {
    "text": "sitting in s3 right so let's take a look at how it looks how it affects our how",
    "start": "1819250",
    "end": "1824770"
  },
  {
    "text": "it affects our demo so we have the data sitting in s3 we'll be connecting Athena to that s3 bucket using the glue catalog",
    "start": "1824770",
    "end": "1832179"
  },
  {
    "text": "and then we will use quick site to go ahead and realize that so let's go ahead and do",
    "start": "1832179",
    "end": "1837710"
  },
  {
    "text": "the demo and I'll quickly connect to Athena and show you how to do it so this",
    "start": "1837710",
    "end": "1851000"
  },
  {
    "text": "is the AWS console and we will go to Athena right so I'm going to Athena now",
    "start": "1851000",
    "end": "1860350"
  },
  {
    "text": "right so on the left hand side I've you see that what are the databases that Athena can access right so that is",
    "start": "1863170",
    "end": "1869750"
  },
  {
    "text": "summit DB and then there is process data table right if I open the process data table there are a bunch of fields here",
    "start": "1869750",
    "end": "1875510"
  },
  {
    "text": "so I'm interested in the process data table let's take a look at I want to know who was the most heard artist in my",
    "start": "1875510",
    "end": "1882710"
  },
  {
    "text": "user group in my user base right so I say select artist name and I want to",
    "start": "1882710",
    "end": "1889520"
  },
  {
    "text": "count how many times that artists name appeared right from process data table",
    "start": "1889520",
    "end": "1895780"
  },
  {
    "text": "right and then I'll group by sorry I'll",
    "start": "1895780",
    "end": "1901460"
  },
  {
    "text": "just add a variable here and I'll say as count right so I'll say group back sorry",
    "start": "1901460",
    "end": "1907310"
  },
  {
    "text": "group by artist name and we will and we",
    "start": "1907310",
    "end": "1915830"
  },
  {
    "text": "will order by count right and we will do",
    "start": "1915830",
    "end": "1923270"
  },
  {
    "text": "a descending so here we can get the data and here you should be able to see the",
    "start": "1923270",
    "end": "1929540"
  },
  {
    "text": "data that that got that got collected right so here you are seeing that this",
    "start": "1929540",
    "end": "1935600"
  },
  {
    "text": "user particular artist called imagine dragons was heard 1938 times right so",
    "start": "1935600",
    "end": "1941420"
  },
  {
    "text": "similarly you can use this data in front of a table or a quick site or something and you can visualize the data right so",
    "start": "1941420",
    "end": "1948440"
  },
  {
    "text": "with that we are done with the part of the demo what we will do is we'll now we have the honor of having Daniel Mueller",
    "start": "1948440",
    "end": "1954770"
  },
  {
    "text": "who's from spool to come and share his story on how he built a data Lake right",
    "start": "1954770",
    "end": "1959900"
  },
  {
    "text": "so please give a round of applause for Daniel why he is sharing his experience thank you",
    "start": "1959900",
    "end": "1966640"
  },
  {
    "text": "no thanks to me for the intro so my name is Daniel I'm the head of cloud",
    "start": "1968250",
    "end": "1975010"
  },
  {
    "text": "infrastructure for spool spool existing six years we were lucky enough to start",
    "start": "1975010",
    "end": "1981690"
  },
  {
    "text": "our journey directly on 80s so we were cloud using only cloud since ever so who",
    "start": "1981690",
    "end": "1990850"
  },
  {
    "text": "is Paul spool is a leading OTT player we are shipping Indian movies TV shows a",
    "start": "1990850",
    "end": "1998020"
  },
  {
    "text": "live TV to over 50 million of users and that around the globe and on almost any",
    "start": "1998020",
    "end": "2005040"
  },
  {
    "text": "device you can imagine and adding to that challenge we will start soon to provide non-indian content also so why",
    "start": "2005040",
    "end": "2013800"
  },
  {
    "text": "did we build a server less data Lake so we wanted to start to collect more than",
    "start": "2013800",
    "end": "2019410"
  },
  {
    "start": "2014000",
    "end": "2014000"
  },
  {
    "text": "hundred events and that they are coming from the devices they're coming from micro services they're coming from",
    "start": "2019410",
    "end": "2026480"
  },
  {
    "text": "different addressed tools we needed the flexibility of ingestion when I mean flexibility of ingestion mean don't be",
    "start": "2026480",
    "end": "2033690"
  },
  {
    "text": "bound to a schema and we need the flexibility of consumption that means",
    "start": "2033690",
    "end": "2038700"
  },
  {
    "text": "that anyone can consume the data you don't need a doctor rate of whatever a doctor rate of SQL to be able to consume",
    "start": "2038700",
    "end": "2044850"
  },
  {
    "text": "and we didn't want to have to scale storage for the future we want to be",
    "start": "2044850",
    "end": "2051810"
  },
  {
    "text": "able to provide this data to third parties so a bit internal third parties or external third parties and most of",
    "start": "2051810",
    "end": "2058409"
  },
  {
    "text": "all we don't want to manage service so keep it trending hashtag no more service it's like the theme of the day so how",
    "start": "2058410",
    "end": "2069450"
  },
  {
    "text": "did we build that so we started with the",
    "start": "2069450",
    "end": "2075419"
  },
  {
    "text": "ingest point API gateway that will feed the data into kinetic streams so the",
    "start": "2075420",
    "end": "2080730"
  },
  {
    "text": "idea of API gateways to provide a simple HTTP endpoint with our own custom authorizes then this kinases stream will",
    "start": "2080730",
    "end": "2091020"
  },
  {
    "text": "trigger a lambda function that will ingest ADA to s3 directly or to firehose depending on use cases and fire hose",
    "start": "2091020",
    "end": "2097740"
  },
  {
    "text": "will aggregate the data in five minutes 100 MB for 5mb files into what we call our row storage this row",
    "start": "2097740",
    "end": "2105830"
  },
  {
    "text": "storage then again triggers a something that will combine all the data to build our final data leg with the whole",
    "start": "2105830",
    "end": "2112160"
  },
  {
    "text": "fledged data then comes into play the",
    "start": "2112160",
    "end": "2118340"
  },
  {
    "text": "glue cloris they will catalog the data transform it eventually to park here or",
    "start": "2118340",
    "end": "2125390"
  },
  {
    "text": "the data layer could be read directly from a tina or even be shipped to",
    "start": "2125390",
    "end": "2130450"
  },
  {
    "text": "elasticsearch or to Druitt for direct analysis or api integration for third",
    "start": "2130450",
    "end": "2136250"
  },
  {
    "text": "parties the package storage which is a",
    "start": "2136250",
    "end": "2141290"
  },
  {
    "text": "transform data from our data Lake can be fed into a sinner and redshift or",
    "start": "2141290",
    "end": "2146750"
  },
  {
    "text": "redshift spectrum for bigger analysis long-term storage",
    "start": "2146750",
    "end": "2154510"
  },
  {
    "text": "the first kinetic stream also allows us to do real-life data processing through",
    "start": "2154600",
    "end": "2160040"
  },
  {
    "text": "either analytics or the tools and then feed cloud watch alarms the quadrature",
    "start": "2160040",
    "end": "2165440"
  },
  {
    "text": "metrics to get info about things happening on our apps in a real lifetime",
    "start": "2165440",
    "end": "2173380"
  },
  {
    "text": "so that's like really quickly explained it's a bit more complicated than that but the lesson learned about that is to",
    "start": "2174040",
    "end": "2181310"
  },
  {
    "text": "be able to manage all these little lambda functions use a framework don't start to have your scripts left and",
    "start": "2181310",
    "end": "2187010"
  },
  {
    "text": "right bit Sam's over there so a picks up doesn't matter just use a framework keep",
    "start": "2187010",
    "end": "2192500"
  },
  {
    "text": "the raw format data all the time so you can replay you can debug it convert your",
    "start": "2192500",
    "end": "2197840"
  },
  {
    "text": "data to columnar format that allows you to minimize the number of data transfer to you end tools partition your data",
    "start": "2197840",
    "end": "2205940"
  },
  {
    "text": "again that allows you to select only what you need bit partition by date which is the default or even add your",
    "start": "2205940",
    "end": "2211730"
  },
  {
    "text": "own filters and when you do selects",
    "start": "2211730",
    "end": "2217100"
  },
  {
    "text": "don't do select star specify the columns you know the less columns you know the less data you have to transfer create",
    "start": "2217100",
    "end": "2223700"
  },
  {
    "text": "files of about roughly hundred MB files in your buckets that will allow to reduce the number of s3 API calls",
    "start": "2223700",
    "end": "2231530"
  },
  {
    "text": "the data compresses it in a data leg that that will reduce your cost of storage but it doesn't add much on the",
    "start": "2231530",
    "end": "2239059"
  },
  {
    "text": "cost of unprofessional and you slander for all the automation you have a file that you receive the next tree trigger",
    "start": "2239059",
    "end": "2245720"
  },
  {
    "text": "Landa to do something and with that I've done my part and I give it back to you",
    "start": "2245720",
    "end": "2254490"
  },
  {
    "text": "[Applause]",
    "start": "2254490",
    "end": "2259770"
  },
  {
    "text": "thank you very much so with that guys I wanted to summarize the",
    "start": "2262059",
    "end": "2269569"
  },
  {
    "text": "session what we learned today right so it's all about four things think about",
    "start": "2269569",
    "end": "2274760"
  },
  {
    "text": "how you're going to ingest the data think about how you're going to store the data how are you going to catalog",
    "start": "2274760",
    "end": "2280039"
  },
  {
    "text": "and transform the data right once you have figured out all of this then what you create is something called a serving",
    "start": "2280039",
    "end": "2286400"
  },
  {
    "text": "layer right this is the layer where you're where you'll be serving the data out to the different users within your",
    "start": "2286400",
    "end": "2291680"
  },
  {
    "text": "company right so thinking about data lakes storage to make it future-proof",
    "start": "2291680",
    "end": "2296779"
  },
  {
    "text": "think about security think about cost and think about the central storage system that you'll be using that is",
    "start": "2296779",
    "end": "2302869"
  },
  {
    "text": "Amazon s3 and how are you going to build the tools around it to to build your data like right so with that thank you",
    "start": "2302869",
    "end": "2309260"
  },
  {
    "text": "very much I really appreciate your time for coming down today and attending this session and please do fill your",
    "start": "2309260",
    "end": "2314630"
  },
  {
    "text": "feedbacks and your feedbacks are very important for us thank you very much and have a good day",
    "start": "2314630",
    "end": "2320619"
  }
]