[
  {
    "text": "hey everyone this is DL in my previous",
    "start": "199",
    "end": "3080"
  },
  {
    "text": "session I showed you how to deploy",
    "start": "3080",
    "end": "5920"
  },
  {
    "text": "multiple fine-tune Laura adapters for",
    "start": "5920",
    "end": "8800"
  },
  {
    "text": "the single base model on stagemaker in",
    "start": "8800",
    "end": "12440"
  },
  {
    "text": "this session I'm going to walk you",
    "start": "12440",
    "end": "14040"
  },
  {
    "text": "through deploying multiple based",
    "start": "14040",
    "end": "16920"
  },
  {
    "text": "Foundation models with different model",
    "start": "16920",
    "end": "19640"
  },
  {
    "text": "architectures on sagemaker cost",
    "start": "19640",
    "end": "21279"
  },
  {
    "text": "effectively at scale let's get started",
    "start": "21279",
    "end": "24640"
  },
  {
    "text": "so now you have deployed llama 2 model",
    "start": "24640",
    "end": "29439"
  },
  {
    "text": "on St maker using an inference endpoint",
    "start": "29439",
    "end": "31720"
  },
  {
    "text": "on sagemaker successfully in production",
    "start": "31720",
    "end": "34280"
  },
  {
    "text": "now you want to put guard rails to",
    "start": "34280",
    "end": "37360"
  },
  {
    "text": "protect the data privacy and moderate",
    "start": "37360",
    "end": "39920"
  },
  {
    "text": "the content by having toxicity and pii",
    "start": "39920",
    "end": "43200"
  },
  {
    "text": "models hosted in their own endpoints oh",
    "start": "43200",
    "end": "46320"
  },
  {
    "text": "by the way you just forgot to make sure",
    "start": "46320",
    "end": "49399"
  },
  {
    "text": "that your system is not to make sure",
    "start": "49399",
    "end": "52359"
  },
  {
    "text": "that your system is not hallucinating so",
    "start": "52359",
    "end": "54840"
  },
  {
    "text": "you decide to use edding and other type",
    "start": "54840",
    "end": "57480"
  },
  {
    "text": "of search uh models uh so that uh your",
    "start": "57480",
    "end": "60440"
  },
  {
    "text": "application response with facts so you",
    "start": "60440",
    "end": "63840"
  },
  {
    "text": "can see it it easily can lead to",
    "start": "63840",
    "end": "68080"
  },
  {
    "text": "multiple or even hundreds of end points",
    "start": "68080",
    "end": "71479"
  },
  {
    "text": "and you can't scale like this it comes",
    "start": "71479",
    "end": "75240"
  },
  {
    "text": "with an operational burden for you and",
    "start": "75240",
    "end": "78240"
  },
  {
    "text": "more essentially it is costly Affair all",
    "start": "78240",
    "end": "81360"
  },
  {
    "text": "right so how about we combine all these",
    "start": "81360",
    "end": "85240"
  },
  {
    "text": "models into",
    "start": "85240",
    "end": "87119"
  },
  {
    "text": "single single end point and have have",
    "start": "87119",
    "end": "90200"
  },
  {
    "text": "the client invoke the respective model",
    "start": "90200",
    "end": "93560"
  },
  {
    "text": "and handle the orchestration logic but",
    "start": "93560",
    "end": "96399"
  },
  {
    "text": "well what if this models can't fit into",
    "start": "96399",
    "end": "101720"
  },
  {
    "text": "the single machine learning instance how",
    "start": "101720",
    "end": "105159"
  },
  {
    "text": "do I go about it well one of the",
    "start": "105159",
    "end": "107520"
  },
  {
    "text": "possible solutions in solving that",
    "start": "107520",
    "end": "109719"
  },
  {
    "text": "problem if you can't fit all the",
    "start": "109719",
    "end": "112520"
  },
  {
    "text": "foundation models into one machine",
    "start": "112520",
    "end": "114320"
  },
  {
    "text": "learning instance we can always spin up",
    "start": "114320",
    "end": "117159"
  },
  {
    "text": "separate inference endpoint for each",
    "start": "117159",
    "end": "119960"
  },
  {
    "text": "each Foundation model or each Foundation",
    "start": "119960",
    "end": "122360"
  },
  {
    "text": "based model for serving however each",
    "start": "122360",
    "end": "125240"
  },
  {
    "text": "model may have different traffic",
    "start": "125240",
    "end": "127280"
  },
  {
    "text": "patterns and machine learning",
    "start": "127280",
    "end": "128599"
  },
  {
    "text": "accelerator requirements some models may",
    "start": "128599",
    "end": "131200"
  },
  {
    "text": "need two accelerators some may need one",
    "start": "131200",
    "end": "133800"
  },
  {
    "text": "or some may need all eight uh",
    "start": "133800",
    "end": "136680"
  },
  {
    "text": "accelerators in the in the host and then",
    "start": "136680",
    "end": "139120"
  },
  {
    "text": "with this architecture you will end up",
    "start": "139120",
    "end": "140720"
  },
  {
    "text": "having uh inefficient architecture where",
    "start": "140720",
    "end": "143440"
  },
  {
    "text": "the hardware resources are just sitting",
    "start": "143440",
    "end": "145200"
  },
  {
    "text": "idle and these Hardware are costly and",
    "start": "145200",
    "end": "148160"
  },
  {
    "text": "think about the operational over had in",
    "start": "148160",
    "end": "150560"
  },
  {
    "text": "managing these end points when the",
    "start": "150560",
    "end": "152959"
  },
  {
    "text": "number of models grow from tens to",
    "start": "152959",
    "end": "156160"
  },
  {
    "text": "hundreds it'll be a lot of heavy lifting",
    "start": "156160",
    "end": "159000"
  },
  {
    "text": "for you in terms of operational overhead",
    "start": "159000",
    "end": "161760"
  },
  {
    "text": "and",
    "start": "161760",
    "end": "162879"
  },
  {
    "text": "cost so we added a new feature called as",
    "start": "162879",
    "end": "166760"
  },
  {
    "text": "inference",
    "start": "166760",
    "end": "168280"
  },
  {
    "text": "components inference component is a",
    "start": "168280",
    "end": "171200"
  },
  {
    "text": "logical construct that represents a",
    "start": "171200",
    "end": "173879"
  },
  {
    "text": "model deployment and a unit of scale so",
    "start": "173879",
    "end": "177959"
  },
  {
    "text": "that you can set up Autos scaling",
    "start": "177959",
    "end": "179879"
  },
  {
    "text": "policies to scale up and down the number",
    "start": "179879",
    "end": "183080"
  },
  {
    "text": "of copies based on your requirements and",
    "start": "183080",
    "end": "186239"
  },
  {
    "text": "based on your traffic pattern this way",
    "start": "186239",
    "end": "189000"
  },
  {
    "text": "you can deploy one to hundreds of base",
    "start": "189000",
    "end": "192239"
  },
  {
    "text": "model models behind a single sagemaker",
    "start": "192239",
    "end": "196720"
  },
  {
    "text": "inference",
    "start": "196720",
    "end": "198239"
  },
  {
    "text": "endpoint this way you can grow your",
    "start": "198239",
    "end": "200519"
  },
  {
    "text": "endpoint as use your use case evolve now",
    "start": "200519",
    "end": "204840"
  },
  {
    "text": "each model on the Endo can be",
    "start": "204840",
    "end": "208080"
  },
  {
    "text": "independently scaled with a its own",
    "start": "208080",
    "end": "210599"
  },
  {
    "text": "autoscaling policy sagemaker will emit",
    "start": "210599",
    "end": "214040"
  },
  {
    "text": "Matrix in cloudwatch and logs for each",
    "start": "214040",
    "end": "217319"
  },
  {
    "text": "model deployment so that you can use",
    "start": "217319",
    "end": "220319"
  },
  {
    "text": "this information when deciding when you",
    "start": "220319",
    "end": "222680"
  },
  {
    "text": "scale up or down you can also scale down",
    "start": "222680",
    "end": "226680"
  },
  {
    "text": "each model to make room for others uh",
    "start": "226680",
    "end": "228959"
  },
  {
    "text": "other models to be loaded now you can",
    "start": "228959",
    "end": "231400"
  },
  {
    "text": "allocate specific Hardware resources",
    "start": "231400",
    "end": "234519"
  },
  {
    "text": "such as CPUs host memory gpus or even",
    "start": "234519",
    "end": "239319"
  },
  {
    "text": "new on accelerators to each model or",
    "start": "239319",
    "end": "243560"
  },
  {
    "text": "inference component Sage maker will use",
    "start": "243560",
    "end": "246720"
  },
  {
    "text": "this information to smartly place and",
    "start": "246720",
    "end": "250400"
  },
  {
    "text": "pack",
    "start": "250400",
    "end": "251920"
  },
  {
    "text": "models and maximize the",
    "start": "251920",
    "end": "255120"
  },
  {
    "text": "utilization and high",
    "start": "255120",
    "end": "259000"
  },
  {
    "text": "availability we can take a look at the",
    "start": "259400",
    "end": "261759"
  },
  {
    "text": "resources that makes up end and point so",
    "start": "261759",
    "end": "266240"
  },
  {
    "text": "first we have sagemaker end point",
    "start": "266240",
    "end": "270479"
  },
  {
    "text": "this is an abstraction of the",
    "start": "270479",
    "end": "272840"
  },
  {
    "text": "infrastructure that you can send HTTP",
    "start": "272840",
    "end": "275440"
  },
  {
    "text": "restful uh API calls to get the INF",
    "start": "275440",
    "end": "278639"
  },
  {
    "text": "results and sagemaker endpoints are",
    "start": "278639",
    "end": "281600"
  },
  {
    "text": "composed of machine learning instances",
    "start": "281600",
    "end": "284800"
  },
  {
    "text": "that sagemaker manages on our customer",
    "start": "284800",
    "end": "288240"
  },
  {
    "text": "behalf such as hell checks patching",
    "start": "288240",
    "end": "290800"
  },
  {
    "text": "instance replacement and so and so forth",
    "start": "290800",
    "end": "293160"
  },
  {
    "text": "now to enable the features I mentioned",
    "start": "293160",
    "end": "295840"
  },
  {
    "text": "we added a new object influence",
    "start": "295840",
    "end": "298840"
  },
  {
    "text": "component",
    "start": "298840",
    "end": "300160"
  },
  {
    "text": "you can think of it as a model",
    "start": "300160",
    "end": "302280"
  },
  {
    "text": "abstraction which represents deployed",
    "start": "302280",
    "end": "304440"
  },
  {
    "text": "model when an inference component is on",
    "start": "304440",
    "end": "307720"
  },
  {
    "text": "an endpoint that means it's ready to",
    "start": "307720",
    "end": "310400"
  },
  {
    "text": "serve the",
    "start": "310400",
    "end": "313000"
  },
  {
    "text": "traffic uh there the inference",
    "start": "313000",
    "end": "315199"
  },
  {
    "text": "components are made up of five pieces of",
    "start": "315199",
    "end": "318800"
  },
  {
    "text": "information uh the the container URI",
    "start": "318800",
    "end": "322160"
  },
  {
    "text": "that represents your model server and",
    "start": "322160",
    "end": "324160"
  },
  {
    "text": "inference Logic the container can be any",
    "start": "324160",
    "end": "326840"
  },
  {
    "text": "container as long as it is sagemaker",
    "start": "326840",
    "end": "329039"
  },
  {
    "text": "compatible you can also bring your own",
    "start": "329039",
    "end": "331120"
  },
  {
    "text": "container um you can deploy as many",
    "start": "331120",
    "end": "333800"
  },
  {
    "text": "containers as you like the second is the",
    "start": "333800",
    "end": "336880"
  },
  {
    "text": "model weights in",
    "start": "336880",
    "end": "339919"
  },
  {
    "text": "S3 uh how many CPUs or gpus or even",
    "start": "339919",
    "end": "345520"
  },
  {
    "text": "neuron accelerators or or even the CPU",
    "start": "345520",
    "end": "347919"
  },
  {
    "text": "host memory you want to allocate you can",
    "start": "347919",
    "end": "350479"
  },
  {
    "text": "configure that as part of the inference",
    "start": "350479",
    "end": "352560"
  },
  {
    "text": "component um inference component uh",
    "start": "352560",
    "end": "356199"
  },
  {
    "text": "configuration um the number of copies",
    "start": "356199",
    "end": "358440"
  },
  {
    "text": "you want to start with more mean more",
    "start": "358440",
    "end": "361039"
  },
  {
    "text": "copies means it can handle more traffic",
    "start": "361039",
    "end": "363440"
  },
  {
    "text": "and more volume of uh inference requests",
    "start": "363440",
    "end": "366759"
  },
  {
    "text": "and inference component also uh is is a",
    "start": "366759",
    "end": "369160"
  },
  {
    "text": "unit of scale so you can set up Auto",
    "start": "369160",
    "end": "370639"
  },
  {
    "text": "scaling policy as I mentioned earlier to",
    "start": "370639",
    "end": "372720"
  },
  {
    "text": "scale up and down your number of copies",
    "start": "372720",
    "end": "374840"
  },
  {
    "text": "based on your traffic patterns you can",
    "start": "374840",
    "end": "377560"
  },
  {
    "text": "you can use uh different types of cloud",
    "start": "377560",
    "end": "379560"
  },
  {
    "text": "Matrix like um response times model",
    "start": "379560",
    "end": "381560"
  },
  {
    "text": "latency or invocations per per inference",
    "start": "381560",
    "end": "384319"
  },
  {
    "text": "component to scale in scale out um you",
    "start": "384319",
    "end": "387240"
  },
  {
    "text": "can deploy one or more inference",
    "start": "387240",
    "end": "388720"
  },
  {
    "text": "components uh on a single sagemaker end",
    "start": "388720",
    "end": "390919"
  },
  {
    "text": "point which are distributed across",
    "start": "390919",
    "end": "392960"
  },
  {
    "text": "different uh machine learning instances",
    "start": "392960",
    "end": "395039"
  },
  {
    "text": "across multiple uh availability zones um",
    "start": "395039",
    "end": "398160"
  },
  {
    "text": "so let me walk you through the code all",
    "start": "398160",
    "end": "401240"
  },
  {
    "text": "right so I am going to show you how to",
    "start": "401240",
    "end": "404240"
  },
  {
    "text": "deploy code Lama there are two versions",
    "start": "404240",
    "end": "407319"
  },
  {
    "text": "of the code Lama that I'm going to host",
    "start": "407319",
    "end": "409319"
  },
  {
    "text": "in the same sagemaker inference endpoint",
    "start": "409319",
    "end": "412880"
  },
  {
    "text": "using the new feature called as",
    "start": "412880",
    "end": "415080"
  },
  {
    "text": "inference",
    "start": "415080",
    "end": "416120"
  },
  {
    "text": "component using large model inference",
    "start": "416120",
    "end": "418759"
  },
  {
    "text": "container that is sagemaker compatible",
    "start": "418759",
    "end": "420759"
  },
  {
    "text": "we highly recommend you you use large",
    "start": "420759",
    "end": "422800"
  },
  {
    "text": "model inference container in sagemaker",
    "start": "422800",
    "end": "424639"
  },
  {
    "text": "to host your foundation models um so",
    "start": "424639",
    "end": "427400"
  },
  {
    "text": "let's get started so uh what you're",
    "start": "427400",
    "end": "429400"
  },
  {
    "text": "seeing here is I have two Cod Lama",
    "start": "429400",
    "end": "432000"
  },
  {
    "text": "models 34 billion and 13 billion",
    "start": "432000",
    "end": "436280"
  },
  {
    "text": "instruct and we are going to host both",
    "start": "436280",
    "end": "438720"
  },
  {
    "text": "of these models in a single uh single",
    "start": "438720",
    "end": "441960"
  },
  {
    "text": "sagemaker inference endpoint using",
    "start": "441960",
    "end": "444000"
  },
  {
    "text": "inference components using LMI container",
    "start": "444000",
    "end": "446319"
  },
  {
    "text": "with high performance on Sage maker",
    "start": "446319",
    "end": "447759"
  },
  {
    "text": "let's see how so the first thing is you",
    "start": "447759",
    "end": "451680"
  },
  {
    "text": "you create a session of sagemaker using",
    "start": "451680",
    "end": "454080"
  },
  {
    "text": "Bodo three um and you create the",
    "start": "454080",
    "end": "457680"
  },
  {
    "text": "sagemaker endpoint configuration the",
    "start": "457680",
    "end": "461360"
  },
  {
    "text": "endpoint configuration basically in is",
    "start": "461360",
    "end": "463680"
  },
  {
    "text": "consists of the metadata of the endpoint",
    "start": "463680",
    "end": "467360"
  },
  {
    "text": "itself including the instance type the",
    "start": "467360",
    "end": "470560"
  },
  {
    "text": "model um the model uh the download",
    "start": "470560",
    "end": "475039"
  },
  {
    "text": "configuration uh model data download",
    "start": "475039",
    "end": "477560"
  },
  {
    "text": "configuration um container start up uh",
    "start": "477560",
    "end": "480319"
  },
  {
    "text": "timeout or model data download timeout",
    "start": "480319",
    "end": "482960"
  },
  {
    "text": "in seconds which you can see here I've",
    "start": "482960",
    "end": "485599"
  },
  {
    "text": "configured the instance type of for this",
    "start": "485599",
    "end": "487919"
  },
  {
    "text": "endpoint as p4d 24xl which has eight",
    "start": "487919",
    "end": "492639"
  },
  {
    "text": "Nvidia a100 gpus each um each GPU will",
    "start": "492639",
    "end": "497520"
  },
  {
    "text": "come with 40 GB of high bandwidth memory",
    "start": "497520",
    "end": "501319"
  },
  {
    "text": "we have eight such gpus in the",
    "start": "501319",
    "end": "505960"
  },
  {
    "text": "instance so we have I've have set the",
    "start": "505960",
    "end": "508599"
  },
  {
    "text": "instance type to p4d",
    "start": "508599",
    "end": "512000"
  },
  {
    "text": "24xl and I'm setting the model download",
    "start": "512000",
    "end": "515120"
  },
  {
    "text": "timeout in seconds um in such a way so",
    "start": "515120",
    "end": "519479"
  },
  {
    "text": "that it has enough time the container",
    "start": "519479",
    "end": "521320"
  },
  {
    "text": "has enough time to uh download the model",
    "start": "521320",
    "end": "524480"
  },
  {
    "text": "artifacts uh because they're going to be",
    "start": "524480",
    "end": "526519"
  },
  {
    "text": "a little bit on the higher side because",
    "start": "526519",
    "end": "528080"
  },
  {
    "text": "we are talking about 34 billion which is",
    "start": "528080",
    "end": "529600"
  },
  {
    "text": "going to be a pretty uh pretty large",
    "start": "529600",
    "end": "531680"
  },
  {
    "text": "sized model artifacts to download from",
    "start": "531680",
    "end": "533320"
  },
  {
    "text": "S3 on the local container so I'm going",
    "start": "533320",
    "end": "535920"
  },
  {
    "text": "to give a little bit of uh higher",
    "start": "535920",
    "end": "538240"
  },
  {
    "text": "timeouts in seconds which in this case",
    "start": "538240",
    "end": "540200"
  },
  {
    "text": "is 400 seconds um and then you can also",
    "start": "540200",
    "end": "542920"
  },
  {
    "text": "specify the container startup heal check",
    "start": "542920",
    "end": "545040"
  },
  {
    "text": "timeout which stage maker takes into the",
    "start": "545040",
    "end": "546959"
  },
  {
    "text": "consideration to make sure that the",
    "start": "546959",
    "end": "548839"
  },
  {
    "text": "containers it's bringing up they're",
    "start": "548839",
    "end": "550640"
  },
  {
    "text": "healthy and they're ready to go to um",
    "start": "550640",
    "end": "553519"
  },
  {
    "text": "handle the production traffic yeah so",
    "start": "553519",
    "end": "556240"
  },
  {
    "text": "I'm going to set this to 300 uh seconds",
    "start": "556240",
    "end": "558720"
  },
  {
    "text": "just in case uh you can also set higher",
    "start": "558720",
    "end": "561160"
  },
  {
    "text": "or lower values um and then there's a",
    "start": "561160",
    "end": "564120"
  },
  {
    "text": "new uh attribute uh as part of the",
    "start": "564120",
    "end": "566920"
  },
  {
    "text": "endpoint config called as managed in",
    "start": "566920",
    "end": "569399"
  },
  {
    "text": "instant scaling now what that does is um",
    "start": "569399",
    "end": "572760"
  },
  {
    "text": "so remember I talked about that you the",
    "start": "572760",
    "end": "575480"
  },
  {
    "text": "the new scaling unit is no longer",
    "start": "575480",
    "end": "577839"
  },
  {
    "text": "machine learning instance but you can",
    "start": "577839",
    "end": "579440"
  },
  {
    "text": "also have a fine grained um Auto scaling",
    "start": "579440",
    "end": "582839"
  },
  {
    "text": "that you can do at an inference",
    "start": "582839",
    "end": "585040"
  },
  {
    "text": "component level that means the model",
    "start": "585040",
    "end": "587040"
  },
  {
    "text": "level scaling is also available now with",
    "start": "587040",
    "end": "589360"
  },
  {
    "text": "this feature uh you can choose to only",
    "start": "589360",
    "end": "594240"
  },
  {
    "text": "specify the model level scaling policy",
    "start": "594240",
    "end": "598320"
  },
  {
    "text": "without specify explicitly the instance",
    "start": "598320",
    "end": "601160"
  },
  {
    "text": "level autoscaling",
    "start": "601160",
    "end": "602959"
  },
  {
    "text": "policy uh and so that is called as",
    "start": "602959",
    "end": "605360"
  },
  {
    "text": "managed instance scaling where sag maker",
    "start": "605360",
    "end": "608600"
  },
  {
    "text": "will take care of expanding or or or or",
    "start": "608600",
    "end": "614000"
  },
  {
    "text": "reducing the fleet of machine learning",
    "start": "614000",
    "end": "616440"
  },
  {
    "text": "instances based on your model level",
    "start": "616440",
    "end": "619360"
  },
  {
    "text": "autoscaling policies so for example if",
    "start": "619360",
    "end": "621800"
  },
  {
    "text": "you need uh two uh two inference",
    "start": "621800",
    "end": "625279"
  },
  {
    "text": "components or copies of the inference",
    "start": "625279",
    "end": "626839"
  },
  {
    "text": "components as part of your autoscaling",
    "start": "626839",
    "end": "628440"
  },
  {
    "text": "policy which you have configured based",
    "start": "628440",
    "end": "630079"
  },
  {
    "text": "on your models um and if you if",
    "start": "630079",
    "end": "632680"
  },
  {
    "text": "sagemaker finds it doesn't have enough",
    "start": "632680",
    "end": "634640"
  },
  {
    "text": "machine learning instances to uh to",
    "start": "634640",
    "end": "637360"
  },
  {
    "text": "satisfy that request then Sage maker",
    "start": "637360",
    "end": "640839"
  },
  {
    "text": "managed instance scaling policy is going",
    "start": "640839",
    "end": "643600"
  },
  {
    "text": "to take care of expanding the the the",
    "start": "643600",
    "end": "645880"
  },
  {
    "text": "number of instances required in order to",
    "start": "645880",
    "end": "648160"
  },
  {
    "text": "to satisfy that requirement that is what",
    "start": "648160",
    "end": "650320"
  },
  {
    "text": "is called a sagemaker manage instance",
    "start": "650320",
    "end": "653200"
  },
  {
    "text": "scaling where sagemaker will take care",
    "start": "653200",
    "end": "655040"
  },
  {
    "text": "of the ml instance scaling and you",
    "start": "655040",
    "end": "657639"
  },
  {
    "text": "specify only the model levels Autos",
    "start": "657639",
    "end": "659800"
  },
  {
    "text": "scaling policy that's what we're going",
    "start": "659800",
    "end": "661079"
  },
  {
    "text": "to do here so I've enabled this this",
    "start": "661079",
    "end": "664320"
  },
  {
    "text": "feature uh with the status of enable and",
    "start": "664320",
    "end": "667440"
  },
  {
    "text": "then I've uh you can also mention the",
    "start": "667440",
    "end": "670200"
  },
  {
    "text": "minimum um instance count uh which in",
    "start": "670200",
    "end": "673480"
  },
  {
    "text": "this case uh is uh what I've specified",
    "start": "673480",
    "end": "676959"
  },
  {
    "text": "is one and then you can also specify Max",
    "start": "676959",
    "end": "680120"
  },
  {
    "text": "instance count as as you wish you can",
    "start": "680120",
    "end": "682399"
  },
  {
    "text": "also keep this as Dynamic also you can",
    "start": "682399",
    "end": "684320"
  },
  {
    "text": "keep on you know increasing the the",
    "start": "684320",
    "end": "686639"
  },
  {
    "text": "machine learning instance Max count uh",
    "start": "686639",
    "end": "689399"
  },
  {
    "text": "in order so that sagemaker can expand",
    "start": "689399",
    "end": "691880"
  },
  {
    "text": "its Fleet to the max instance gun right",
    "start": "691880",
    "end": "695920"
  },
  {
    "text": "and then there you can also specify the",
    "start": "695920",
    "end": "697760"
  },
  {
    "text": "routing configuration how you want to",
    "start": "697760",
    "end": "700120"
  },
  {
    "text": "want the stage maker to Route the",
    "start": "700120",
    "end": "702200"
  },
  {
    "text": "inference incoming inference request to",
    "start": "702200",
    "end": "704440"
  },
  {
    "text": "the the fleet um here the best practice",
    "start": "704440",
    "end": "707399"
  },
  {
    "text": "is to choose the least outstanding",
    "start": "707399",
    "end": "709760"
  },
  {
    "text": "request policy I highly recommend you",
    "start": "709760",
    "end": "711720"
  },
  {
    "text": "set this uh for in order to get the",
    "start": "711720",
    "end": "714480"
  },
  {
    "text": "higher throughput reduced response time",
    "start": "714480",
    "end": "717720"
  },
  {
    "text": "all right so let's move on we we created",
    "start": "717720",
    "end": "719880"
  },
  {
    "text": "the the uh sagemaker endpoint config uh",
    "start": "719880",
    "end": "723399"
  },
  {
    "text": "we are good to go the fir the next thing",
    "start": "723399",
    "end": "725680"
  },
  {
    "text": "that you do is to create the endpoint",
    "start": "725680",
    "end": "727720"
  },
  {
    "text": "without any inference component please",
    "start": "727720",
    "end": "729839"
  },
  {
    "text": "note here you did not create any",
    "start": "729839",
    "end": "731800"
  },
  {
    "text": "inference components till now you just",
    "start": "731800",
    "end": "734199"
  },
  {
    "text": "created a a a an empty stagemaker",
    "start": "734199",
    "end": "737680"
  },
  {
    "text": "endpoint with all this metadata",
    "start": "737680",
    "end": "740440"
  },
  {
    "text": "information so then we create an",
    "start": "740440",
    "end": "742839"
  },
  {
    "text": "sagemaker endpoint we wait for the",
    "start": "742839",
    "end": "745279"
  },
  {
    "text": "endpoint creation it's successful as you",
    "start": "745279",
    "end": "747199"
  },
  {
    "text": "can see in the the output",
    "start": "747199",
    "end": "749760"
  },
  {
    "text": "and let's move on let's now create two",
    "start": "749760",
    "end": "752399"
  },
  {
    "text": "inference component one for 34 billion",
    "start": "752399",
    "end": "755440"
  },
  {
    "text": "code Lama another one for 13",
    "start": "755440",
    "end": "758839"
  },
  {
    "text": "billion all right so now uh let's let's",
    "start": "758839",
    "end": "763079"
  },
  {
    "text": "come to the for for us to be able to",
    "start": "763079",
    "end": "765399"
  },
  {
    "text": "create an inference component we first",
    "start": "765399",
    "end": "767959"
  },
  {
    "text": "need to create the model object remember",
    "start": "767959",
    "end": "770800"
  },
  {
    "text": "the sagemaker model object in order to",
    "start": "770800",
    "end": "773440"
  },
  {
    "text": "for you to create the the sageer model",
    "start": "773440",
    "end": "775920"
  },
  {
    "text": "object you will need to First um create",
    "start": "775920",
    "end": "780440"
  },
  {
    "text": "uh first U uh get the container URL",
    "start": "780440",
    "end": "784079"
  },
  {
    "text": "because obviously you're going to use",
    "start": "784079",
    "end": "785360"
  },
  {
    "text": "some container to host that model in in",
    "start": "785360",
    "end": "787720"
  },
  {
    "text": "our case to host the code Lama I'm going",
    "start": "787720",
    "end": "790440"
  },
  {
    "text": "to use uh the large model inance",
    "start": "790440",
    "end": "792680"
  },
  {
    "text": "container uh the latest version uh and",
    "start": "792680",
    "end": "796720"
  },
  {
    "text": "the uh the type of the container I'm",
    "start": "796720",
    "end": "798600"
  },
  {
    "text": "using is deep speed um uh deep speed",
    "start": "798600",
    "end": "802040"
  },
  {
    "text": "container off the LMI container and then",
    "start": "802040",
    "end": "804760"
  },
  {
    "text": "I've set the specific environment",
    "start": "804760",
    "end": "807800"
  },
  {
    "text": "variables uh to to configure that",
    "start": "807800",
    "end": "810000"
  },
  {
    "text": "inference endpoint or that configure",
    "start": "810000",
    "end": "811600"
  },
  {
    "text": "that um to configure the the model uh",
    "start": "811600",
    "end": "815760"
  },
  {
    "text": "runtime uh here in this case this",
    "start": "815760",
    "end": "818199"
  },
  {
    "text": "environment variable is related to 34",
    "start": "818199",
    "end": "820360"
  },
  {
    "text": "billion code Lama 34 billion and I've",
    "start": "820360",
    "end": "823480"
  },
  {
    "text": "set the since you can't uh load the",
    "start": "823480",
    "end": "827480"
  },
  {
    "text": "entire 34 billion in FP uh 16 which",
    "start": "827480",
    "end": "831160"
  },
  {
    "text": "requires two bytes per parameter and we",
    "start": "831160",
    "end": "833720"
  },
  {
    "text": "have 34 billion parameters so it at",
    "start": "833720",
    "end": "836440"
  },
  {
    "text": "least needs",
    "start": "836440",
    "end": "837839"
  },
  {
    "text": "68 G gabt of high bandwidth memory of",
    "start": "837839",
    "end": "841000"
  },
  {
    "text": "the GPU device which you can't fit in",
    "start": "841000",
    "end": "843160"
  },
  {
    "text": "into only single device you need",
    "start": "843160",
    "end": "844639"
  },
  {
    "text": "multiple devices in order to to to fit",
    "start": "844639",
    "end": "847199"
  },
  {
    "text": "the 34 billion in fp16 uh fp16 data type",
    "start": "847199",
    "end": "851639"
  },
  {
    "text": "and so what I've said here is tensor",
    "start": "851639",
    "end": "853600"
  },
  {
    "text": "parallel degree to Max meaning whatever",
    "start": "853600",
    "end": "856120"
  },
  {
    "text": "devices I'm going to allocate to this",
    "start": "856120",
    "end": "857720"
  },
  {
    "text": "inference component uh it's going to",
    "start": "857720",
    "end": "860800"
  },
  {
    "text": "it's going to use that as the number of",
    "start": "860800",
    "end": "862199"
  },
  {
    "text": "the shards um and and and go from there",
    "start": "862199",
    "end": "865959"
  },
  {
    "text": "and then I have specified the rolling",
    "start": "865959",
    "end": "867440"
  },
  {
    "text": "batch size in order to maximize",
    "start": "867440",
    "end": "869639"
  },
  {
    "text": "the throughput I highly recommend you",
    "start": "869639",
    "end": "871240"
  },
  {
    "text": "all use the rolling batch or it is also",
    "start": "871240",
    "end": "873160"
  },
  {
    "text": "called as continuous batching highly",
    "start": "873160",
    "end": "875079"
  },
  {
    "text": "recommend that you use that you're going",
    "start": "875079",
    "end": "876920"
  },
  {
    "text": "to use uh specific LMI backend which is",
    "start": "876920",
    "end": "880680"
  },
  {
    "text": "called as LMI disc um or you can also",
    "start": "880680",
    "end": "883519"
  },
  {
    "text": "use VM or you can use tensor RT llm uh",
    "start": "883519",
    "end": "887959"
  },
  {
    "text": "based on your uh model compatibility and",
    "start": "887959",
    "end": "891199"
  },
  {
    "text": "um based on your uh business use cases",
    "start": "891199",
    "end": "893920"
  },
  {
    "text": "here I'm going to use uh LMI",
    "start": "893920",
    "end": "896120"
  },
  {
    "text": "disc um for COD Lama 34 billion all",
    "start": "896120",
    "end": "900360"
  },
  {
    "text": "right so now we have set our",
    "start": "900360",
    "end": "902720"
  },
  {
    "text": "quantization level to",
    "start": "902720",
    "end": "905000"
  },
  {
    "text": "fp16 the bat size to",
    "start": "905000",
    "end": "907600"
  },
  {
    "text": "34 and the tensor parallel degree to the",
    "start": "907600",
    "end": "911639"
  },
  {
    "text": "maximum number of devices that inance",
    "start": "911639",
    "end": "913480"
  },
  {
    "text": "component has has access to all right",
    "start": "913480",
    "end": "916199"
  },
  {
    "text": "let's let's see what do we have for the",
    "start": "916199",
    "end": "919519"
  },
  {
    "text": "code Lama 13 billion now 13 billion uh",
    "start": "919519",
    "end": "922839"
  },
  {
    "text": "if you just take two bytes per parameter",
    "start": "922839",
    "end": "926320"
  },
  {
    "text": "in terms of FP fp16 quantization we just",
    "start": "926320",
    "end": "929279"
  },
  {
    "text": "require 26 gigabytes uh to load one",
    "start": "929279",
    "end": "932759"
  },
  {
    "text": "model copy for 13 billion which you can",
    "start": "932759",
    "end": "934959"
  },
  {
    "text": "easily fit into one device when we",
    "start": "934959",
    "end": "937399"
  },
  {
    "text": "talking about a100 which has already 40",
    "start": "937399",
    "end": "939720"
  },
  {
    "text": "gigabyt of memory available so you can",
    "start": "939720",
    "end": "941360"
  },
  {
    "text": "easily fit in one device um so I'll",
    "start": "941360",
    "end": "944279"
  },
  {
    "text": "still I'll just still set that tensor",
    "start": "944279",
    "end": "946519"
  },
  {
    "text": "parallel degree to Max uh and I'm going",
    "start": "946519",
    "end": "948440"
  },
  {
    "text": "to assign only one device when I",
    "start": "948440",
    "end": "950199"
  },
  {
    "text": "configure the inference component um in",
    "start": "950199",
    "end": "953079"
  },
  {
    "text": "terms of the accelerators um and then I",
    "start": "953079",
    "end": "955519"
  },
  {
    "text": "I just keep all rest of the uh rest of",
    "start": "955519",
    "end": "958399"
  },
  {
    "text": "the uh values as is please note the",
    "start": "958399",
    "end": "961360"
  },
  {
    "text": "optionor modore ID is still set you need",
    "start": "961360",
    "end": "964720"
  },
  {
    "text": "to set it to the the right hugging face",
    "start": "964720",
    "end": "966959"
  },
  {
    "text": "model ID here in this case I've set it",
    "start": "966959",
    "end": "969120"
  },
  {
    "text": "to 13 billion for 34 billion I've set",
    "start": "969120",
    "end": "971079"
  },
  {
    "text": "the the respective value all right so",
    "start": "971079",
    "end": "974440"
  },
  {
    "text": "now we have the the container image that",
    "start": "974440",
    "end": "977959"
  },
  {
    "text": "that I want to use I have set all my",
    "start": "977959",
    "end": "980880"
  },
  {
    "text": "runtime configuration parameters with",
    "start": "980880",
    "end": "983319"
  },
  {
    "text": "all the the fine tunables set for for",
    "start": "983319",
    "end": "986199"
  },
  {
    "text": "getting high performance I'm all set to",
    "start": "986199",
    "end": "988720"
  },
  {
    "text": "create create a model object for 34",
    "start": "988720",
    "end": "990600"
  },
  {
    "text": "billion so sagemaker model",
    "start": "990600",
    "end": "993720"
  },
  {
    "text": "object I reference my container image",
    "start": "993720",
    "end": "997000"
  },
  {
    "text": "that I created the the U that I have",
    "start": "997000",
    "end": "1000040"
  },
  {
    "text": "with the environment variables that I",
    "start": "1000040",
    "end": "1001800"
  },
  {
    "text": "just created the dictionary object and",
    "start": "1001800",
    "end": "1003959"
  },
  {
    "text": "then I create the model object for 34",
    "start": "1003959",
    "end": "1006120"
  },
  {
    "text": "billion cool for 30 million I do the",
    "start": "1006120",
    "end": "1009120"
  },
  {
    "text": "same thing so we have two model objects",
    "start": "1009120",
    "end": "1012600"
  },
  {
    "text": "uh and now we'll create the the",
    "start": "1012600",
    "end": "1015360"
  },
  {
    "text": "inference components for each model",
    "start": "1015360",
    "end": "1017480"
  },
  {
    "text": "object here is an extremely important",
    "start": "1017480",
    "end": "1019959"
  },
  {
    "text": "step so here's a new API call is create",
    "start": "1019959",
    "end": "1022759"
  },
  {
    "text": "inference component I create inference",
    "start": "1022759",
    "end": "1025400"
  },
  {
    "text": "component for 34 billion and what I'm",
    "start": "1025400",
    "end": "1027798"
  },
  {
    "text": "going to do is uh I've set the startup",
    "start": "1027799",
    "end": "1030079"
  },
  {
    "text": "parameters for that uh container with",
    "start": "1030079",
    "end": "1032918"
  },
  {
    "text": "the model data download time out as I",
    "start": "1032919",
    "end": "1035558"
  },
  {
    "text": "explained earlier the same goes for the",
    "start": "1035559",
    "end": "1038038"
  },
  {
    "text": "container startup health check timeout",
    "start": "1038039",
    "end": "1040038"
  },
  {
    "text": "in seconds you can also set it based on",
    "start": "1040039",
    "end": "1042520"
  },
  {
    "text": "the inference component so you can set",
    "start": "1042520",
    "end": "1044240"
  },
  {
    "text": "it different values for different models",
    "start": "1044240",
    "end": "1046160"
  },
  {
    "text": "is what I'm trying to say and then here",
    "start": "1046160",
    "end": "1048360"
  },
  {
    "text": "is a very important part which is",
    "start": "1048360",
    "end": "1050000"
  },
  {
    "text": "assigning a specific compute resources",
    "start": "1050000",
    "end": "1053000"
  },
  {
    "text": "to that model in this case 34 billion uh",
    "start": "1053000",
    "end": "1056280"
  },
  {
    "text": "we have um we have assigned uh four uh",
    "start": "1056280",
    "end": "1062160"
  },
  {
    "text": "four A1 100s uh four devices or four",
    "start": "1062160",
    "end": "1065400"
  },
  {
    "text": "machine learning",
    "start": "1065400",
    "end": "1066640"
  },
  {
    "text": "accelerators um to load one copy of the",
    "start": "1066640",
    "end": "1070600"
  },
  {
    "text": "34",
    "start": "1070600",
    "end": "1071640"
  },
  {
    "text": "billion uh you could also set it to two",
    "start": "1071640",
    "end": "1074039"
  },
  {
    "text": "as well because you can easily fit into",
    "start": "1074039",
    "end": "1075520"
  },
  {
    "text": "two but I've just in case to accommodate",
    "start": "1075520",
    "end": "1078039"
  },
  {
    "text": "the KV C",
    "start": "1078039",
    "end": "1079559"
  },
  {
    "text": "uh as well and to also uh get higher",
    "start": "1079559",
    "end": "1083000"
  },
  {
    "text": "throughput I have set it to four devices",
    "start": "1083000",
    "end": "1085480"
  },
  {
    "text": "so we have eight devices uh in single",
    "start": "1085480",
    "end": "1088640"
  },
  {
    "text": "instance of p4d uh 24 XL out of eight I",
    "start": "1088640",
    "end": "1092720"
  },
  {
    "text": "have assigned four of these devices to",
    "start": "1092720",
    "end": "1095480"
  },
  {
    "text": "34 billion we do still have uh four",
    "start": "1095480",
    "end": "1099799"
  },
  {
    "text": "other devices left idle and what I'm",
    "start": "1099799",
    "end": "1102000"
  },
  {
    "text": "going to do is I'm going to use those",
    "start": "1102000",
    "end": "1105320"
  },
  {
    "text": "rest of the four devices to assign it to",
    "start": "1105320",
    "end": "1108520"
  },
  {
    "text": "the uh 1 13 billion uh model so here is",
    "start": "1108520",
    "end": "1112440"
  },
  {
    "text": "a 13 billion Model A specification and",
    "start": "1112440",
    "end": "1115559"
  },
  {
    "text": "please note here I specified the number",
    "start": "1115559",
    "end": "1118000"
  },
  {
    "text": "of accelerators for 13 million to only",
    "start": "1118000",
    "end": "1120039"
  },
  {
    "text": "one because I can easily load that model",
    "start": "1120039",
    "end": "1122840"
  },
  {
    "text": "um on in in one device without getting",
    "start": "1122840",
    "end": "1126000"
  },
  {
    "text": "CED out of memory uh but then I still",
    "start": "1126000",
    "end": "1128480"
  },
  {
    "text": "have three more devices left what should",
    "start": "1128480",
    "end": "1130240"
  },
  {
    "text": "I do with that well I'm going to",
    "start": "1130240",
    "end": "1132400"
  },
  {
    "text": "increase the copy count and here is an",
    "start": "1132400",
    "end": "1134840"
  },
  {
    "text": "here's a very cool feature about",
    "start": "1134840",
    "end": "1136320"
  },
  {
    "text": "creating the inference compy you can",
    "start": "1136320",
    "end": "1138120"
  },
  {
    "text": "start the inance component with uh with",
    "start": "1138120",
    "end": "1141159"
  },
  {
    "text": "a copy count to make sure that you're",
    "start": "1141159",
    "end": "1143679"
  },
  {
    "text": "ready fully ready to handle the",
    "start": "1143679",
    "end": "1145400"
  },
  {
    "text": "production traffic um and so what here I",
    "start": "1145400",
    "end": "1149200"
  },
  {
    "text": "have specified is I have specified three",
    "start": "1149200",
    "end": "1152280"
  },
  {
    "text": "copies um and actually four copies my",
    "start": "1152280",
    "end": "1155799"
  },
  {
    "text": "bad four copies of 13 billion model so",
    "start": "1155799",
    "end": "1159919"
  },
  {
    "text": "that you can fully utilize those rest of",
    "start": "1159919",
    "end": "1162480"
  },
  {
    "text": "the four devices and so essentially what",
    "start": "1162480",
    "end": "1164679"
  },
  {
    "text": "we have here is one instance can",
    "start": "1164679",
    "end": "1166840"
  },
  {
    "text": "accommodate one copy of of 34 billion",
    "start": "1166840",
    "end": "1170000"
  },
  {
    "text": "and four copies of 13 billion Cod Lama",
    "start": "1170000",
    "end": "1173720"
  },
  {
    "text": "and then you can imagine you can um",
    "start": "1173720",
    "end": "1175400"
  },
  {
    "text": "scale it out based on the inference",
    "start": "1175400",
    "end": "1177400"
  },
  {
    "text": "components based on the traffic if 34",
    "start": "1177400",
    "end": "1179600"
  },
  {
    "text": "billion gets higher traffic um you can",
    "start": "1179600",
    "end": "1182760"
  },
  {
    "text": "adjust the sagemaker will spin up that",
    "start": "1182760",
    "end": "1185120"
  },
  {
    "text": "many copies um or if 13 billion gets",
    "start": "1185120",
    "end": "1188240"
  },
  {
    "text": "higher traffic or lesser traffic it will",
    "start": "1188240",
    "end": "1189919"
  },
  {
    "text": "scale out scale in as per that traffic",
    "start": "1189919",
    "end": "1191960"
  },
  {
    "text": "so this is something very cool feature",
    "start": "1191960",
    "end": "1193679"
  },
  {
    "text": "that you should use all right all right",
    "start": "1193679",
    "end": "1196880"
  },
  {
    "text": "so we have we have got create inference",
    "start": "1196880",
    "end": "1199520"
  },
  {
    "text": "component we have invoked that API for",
    "start": "1199520",
    "end": "1202039"
  },
  {
    "text": "both of the models and that and that API",
    "start": "1202039",
    "end": "1205200"
  },
  {
    "text": "will will will create the and will go",
    "start": "1205200",
    "end": "1207760"
  },
  {
    "text": "ahead and deploy these models um in the",
    "start": "1207760",
    "end": "1210919"
  },
  {
    "text": "endpoint that we we just",
    "start": "1210919",
    "end": "1213200"
  },
  {
    "text": "created all right so that so now uh we",
    "start": "1213200",
    "end": "1216520"
  },
  {
    "text": "wait for the inference component",
    "start": "1216520",
    "end": "1218720"
  },
  {
    "text": "creation to complete both of them is",
    "start": "1218720",
    "end": "1221720"
  },
  {
    "text": "completed as you can see here in the",
    "start": "1221720",
    "end": "1223240"
  },
  {
    "text": "output um they're both in inservice",
    "start": "1223240",
    "end": "1226280"
  },
  {
    "text": "that's great it's time to invoke them",
    "start": "1226280",
    "end": "1228799"
  },
  {
    "text": "point right while invoking the endpoint",
    "start": "1228799",
    "end": "1232159"
  },
  {
    "text": "you can use the invoke endpoint API and",
    "start": "1232159",
    "end": "1235559"
  },
  {
    "text": "there's one additional attribute that",
    "start": "1235559",
    "end": "1237520"
  },
  {
    "text": "you got to specify in order to uh invoke",
    "start": "1237520",
    "end": "1241360"
  },
  {
    "text": "respective inference component or/ model",
    "start": "1241360",
    "end": "1244039"
  },
  {
    "text": "so we have an additional attribute",
    "start": "1244039",
    "end": "1245559"
  },
  {
    "text": "called as inference component name that",
    "start": "1245559",
    "end": "1249600"
  },
  {
    "text": "is very important so you need to set",
    "start": "1249600",
    "end": "1251840"
  },
  {
    "text": "that to the name of the inference",
    "start": "1251840",
    "end": "1253840"
  },
  {
    "text": "component that you want to invoke and",
    "start": "1253840",
    "end": "1255840"
  },
  {
    "text": "sagemaker will route the traffic to the",
    "start": "1255840",
    "end": "1257919"
  },
  {
    "text": "respective container or with respective",
    "start": "1257919",
    "end": "1260840"
  },
  {
    "text": "respective inference component name and",
    "start": "1260840",
    "end": "1263240"
  },
  {
    "text": "invoke the respective container um and",
    "start": "1263240",
    "end": "1265960"
  },
  {
    "text": "it will route it in remember we set the",
    "start": "1265960",
    "end": "1267799"
  },
  {
    "text": "routing strategy as least outstanding",
    "start": "1267799",
    "end": "1269880"
  },
  {
    "text": "request so it will route in that fashion",
    "start": "1269880",
    "end": "1272400"
  },
  {
    "text": "to get you the higher uh through put uh",
    "start": "1272400",
    "end": "1275919"
  },
  {
    "text": "and so uh once you specify that here in",
    "start": "1275919",
    "end": "1278960"
  },
  {
    "text": "this case I specified 34 billion and",
    "start": "1278960",
    "end": "1281640"
  },
  {
    "text": "then in the body section obviously you",
    "start": "1281640",
    "end": "1283520"
  },
  {
    "text": "specify the prompt and parameters and",
    "start": "1283520",
    "end": "1285520"
  },
  {
    "text": "you invoke that um using the invoke",
    "start": "1285520",
    "end": "1288039"
  },
  {
    "text": "endpoint get the output here as you can",
    "start": "1288039",
    "end": "1290000"
  },
  {
    "text": "see here uh we got the output uh for",
    "start": "1290000",
    "end": "1293279"
  },
  {
    "text": "invoking the 34 billion it's it's an",
    "start": "1293279",
    "end": "1295480"
  },
  {
    "text": "output of SQL uh based on the",
    "start": "1295480",
    "end": "1299000"
  },
  {
    "text": "prompt uh the same thing goes for 13",
    "start": "1299000",
    "end": "1301799"
  },
  {
    "text": "billion you um you specify the",
    "start": "1301799",
    "end": "1303960"
  },
  {
    "text": "respective name of the 13 billion",
    "start": "1303960",
    "end": "1305600"
  },
  {
    "text": "inference component name uh by by adding",
    "start": "1305600",
    "end": "1308720"
  },
  {
    "text": "the value by setting the value for",
    "start": "1308720",
    "end": "1310559"
  },
  {
    "text": "inference component name here you get",
    "start": "1310559",
    "end": "1313159"
  },
  {
    "text": "the output here as well I I asked a",
    "start": "1313159",
    "end": "1315600"
  },
  {
    "text": "different question it's not an uh I I I",
    "start": "1315600",
    "end": "1318320"
  },
  {
    "text": "asked a different question for uh for",
    "start": "1318320",
    "end": "1320080"
  },
  {
    "text": "the SQL uh SQL query output did pretty",
    "start": "1320080",
    "end": "1323600"
  },
  {
    "text": "good job here um so that's about it yes",
    "start": "1323600",
    "end": "1327000"
  },
  {
    "text": "so uh I hope this was useful uh we in",
    "start": "1327000",
    "end": "1330960"
  },
  {
    "text": "this session what we saw is how to",
    "start": "1330960",
    "end": "1332760"
  },
  {
    "text": "deploy multiple base Foundation models",
    "start": "1332760",
    "end": "1334840"
  },
  {
    "text": "on single inference endpoint how to",
    "start": "1334840",
    "end": "1336679"
  },
  {
    "text": "scale individual models using inference",
    "start": "1336679",
    "end": "1339400"
  },
  {
    "text": "components and um get the best out of uh",
    "start": "1339400",
    "end": "1342919"
  },
  {
    "text": "your hardware and get the best",
    "start": "1342919",
    "end": "1345200"
  },
  {
    "text": "performance at scale right um don't",
    "start": "1345200",
    "end": "1348159"
  },
  {
    "text": "forget to clean up don't forget to",
    "start": "1348159",
    "end": "1349919"
  },
  {
    "text": "delete your endpoints endpoint config um",
    "start": "1349919",
    "end": "1352919"
  },
  {
    "text": "and the model objects um so that you",
    "start": "1352919",
    "end": "1355440"
  },
  {
    "text": "don't incur cost um if you're just uh",
    "start": "1355440",
    "end": "1358840"
  },
  {
    "text": "learning for uh learning in your",
    "start": "1358840",
    "end": "1360720"
  },
  {
    "text": "accounts don't forget to clean them up",
    "start": "1360720",
    "end": "1363200"
  },
  {
    "text": "all right so I hope this session was",
    "start": "1363200",
    "end": "1364760"
  },
  {
    "text": "useful uh uh so here are some of the",
    "start": "1364760",
    "end": "1367440"
  },
  {
    "text": "resources here's an example notebook uh",
    "start": "1367440",
    "end": "1370039"
  },
  {
    "text": "for you which I went walked you through",
    "start": "1370039",
    "end": "1372720"
  },
  {
    "text": "in today's session um and uh in my next",
    "start": "1372720",
    "end": "1376360"
  },
  {
    "text": "session I'm going to show you how to",
    "start": "1376360",
    "end": "1378520"
  },
  {
    "text": "configure the art scaling policy for the",
    "start": "1378520",
    "end": "1381000"
  },
  {
    "text": "inference component so that you can",
    "start": "1381000",
    "end": "1382440"
  },
  {
    "text": "scale your inference component based on",
    "start": "1382440",
    "end": "1384279"
  },
  {
    "text": "your model and get the best uh optimized",
    "start": "1384279",
    "end": "1388279"
  },
  {
    "text": "cost for uh deploying multiple base",
    "start": "1388279",
    "end": "1391279"
  },
  {
    "text": "Foundation models at scale stay tuned",
    "start": "1391279",
    "end": "1394760"
  },
  {
    "text": "thank you very much",
    "start": "1394760",
    "end": "1398240"
  }
]