[
  {
    "start": "0",
    "end": "211000"
  },
  {
    "text": "my name is Rob Alexander I'm a seattle-based Solutions architect and uh today we're going to dive into EBS",
    "start": "280",
    "end": "7000"
  },
  {
    "text": "elastic Block store uh specifically covering some of the new things we announced today getting into a lot of",
    "start": "7000",
    "end": "12679"
  },
  {
    "text": "the best practices around performance Security Management and uh let's get",
    "start": "12679",
    "end": "19039"
  },
  {
    "text": "into it so I'd like to start with a little",
    "start": "19039",
    "end": "25160"
  },
  {
    "text": "background on the evolution of EBS as a service you know why why did we build this service you know back in 2006 10",
    "start": "25160",
    "end": "32119"
  },
  {
    "text": "years ago ec2 launched and you know ec2 in the beginning the only storage",
    "start": "32119",
    "end": "38000"
  },
  {
    "text": "available to it was local instance stor so hard dis that happened to be attached to the physical host that was hosting",
    "start": "38000",
    "end": "44360"
  },
  {
    "text": "your instance that was the only option and uh you know that was fun and",
    "start": "44360",
    "end": "49879"
  },
  {
    "text": "customers obviously had a few more requirements beyond that you know when the instances would go away uh their",
    "start": "49879",
    "end": "55640"
  },
  {
    "text": "data went away too which was obviously not ideal so we started hearing from customers and feedback that we'd like",
    "start": "55640",
    "end": "61320"
  },
  {
    "text": "something a little bit more persistent that we can uh rely on uh beyond the actual uh instance itself so EBS",
    "start": "61320",
    "end": "69680"
  },
  {
    "text": "launched in 2008 with our first uh volume type we only started with one and that was EBS magnetic so it was a hard",
    "start": "69680",
    "end": "76840"
  },
  {
    "text": "dis drive-based uh platform which uh did exactly that persistent disc store and",
    "start": "76840",
    "end": "82560"
  },
  {
    "text": "allowed customers now to save their data uh isolated from the actual instance and",
    "start": "82560",
    "end": "87600"
  },
  {
    "text": "be able to flexibly move that data around between their various instances uh but then we started to get more",
    "start": "87600",
    "end": "93200"
  },
  {
    "text": "feedback and uh customers said okay that's great we got persistent data now but we'd also like some consistency in",
    "start": "93200",
    "end": "100240"
  },
  {
    "text": "the level of performance and also could you maybe build that on ssds this time so in 2012 we launched provision iops",
    "start": "100240",
    "end": "108520"
  },
  {
    "text": "which was our first SSD based platform and uh provided low latency and particularly the ability to dial in",
    "start": "108520",
    "end": "115240"
  },
  {
    "text": "exactly the amount of performance that you wanted so you could tell us the iops and with three nines of guarantee we",
    "start": "115240",
    "end": "121799"
  },
  {
    "text": "would deliver those iops uh in a consistent fashion so we're starting to build up on",
    "start": "121799",
    "end": "128399"
  },
  {
    "text": "the flexibility and the options of EBS and customers started asking for yeah we really love these ssds we love the",
    "start": "128399",
    "end": "134800"
  },
  {
    "text": "latency uh we love the the random IO performance but you know we've got workloads that are kind of variable we",
    "start": "134800",
    "end": "140920"
  },
  {
    "text": "can't really tell you exactly how much iops we want from a day-to-day basis and so we'd like something a little bit",
    "start": "140920",
    "end": "146959"
  },
  {
    "text": "easier to manage and not have to actually dial that in all the time so in 2014 we launched general purpose two",
    "start": "146959",
    "end": "154000"
  },
  {
    "text": "volumes also SSD based but in this case the bigger the volume the more performance there was a fixed ratio and",
    "start": "154000",
    "end": "160159"
  },
  {
    "text": "all you had to do was tell us how much storage you want and you got a certain amount of performance based out of that",
    "start": "160159",
    "end": "165840"
  },
  {
    "text": "and then along the way we've released lots of features to build on all these different volume types including",
    "start": "165840",
    "end": "171680"
  },
  {
    "text": "encryption for both your data and your boot volumes and also last year we launched larger and faster volumes so",
    "start": "171680",
    "end": "178319"
  },
  {
    "text": "previously the largest volume size size was a terabyte with larger and faster it can now go up to 16 terabytes and the",
    "start": "178319",
    "end": "184959"
  },
  {
    "text": "throughput on those volumes also doubled so significant performance improvements so you can see you know through the",
    "start": "184959",
    "end": "192440"
  },
  {
    "text": "years the evolution of the platform now you can see a lot of flexibility and the offering of EBS and now today with the",
    "start": "192440",
    "end": "199360"
  },
  {
    "text": "launch of the new throughput optimized volume types the the new ST1 and SC1",
    "start": "199360",
    "end": "204840"
  },
  {
    "text": "volumes we've got a real wide range of flexible options to suit your specific workload",
    "start": "204840",
    "end": "211760"
  },
  {
    "start": "211000",
    "end": "211000"
  },
  {
    "text": "so exactly what is EBS you know we talk about it with volumes and volume types but what is it at its heart you know",
    "start": "212159",
    "end": "218760"
  },
  {
    "text": "when you when you launch an EBS volume what does that mean it's it's a container of block storage that is",
    "start": "218760",
    "end": "225879"
  },
  {
    "text": "configurable by you as far as how much block storage is in there and also has performance characteristics that you",
    "start": "225879",
    "end": "231519"
  },
  {
    "text": "Define according to the volume types that we get into those specifically later and then it's attached to an ec2",
    "start": "231519",
    "end": "238200"
  },
  {
    "text": "instance now that attachment happens over the network so this is a network-based block storage",
    "start": "238200",
    "end": "246360"
  },
  {
    "text": "service you're going attach multiple volumes to a single instance uh but a volume itself can only be attached to",
    "start": "248079",
    "end": "254120"
  },
  {
    "text": "one instance at a time so you can see here you can separate out your volume types between boot volumes and data",
    "start": "254120",
    "end": "260120"
  },
  {
    "text": "volumes so you start to see you have real flexibility not only in the way you define your storage there separation of",
    "start": "260120",
    "end": "266440"
  },
  {
    "start": "262000",
    "end": "262000"
  },
  {
    "text": "different storage from different storage types but you have the the ability to separate out your storage from your",
    "start": "266440",
    "end": "273880"
  },
  {
    "text": "compute and your memory so that if you needed to scale up your CPU or the",
    "start": "273880",
    "end": "278960"
  },
  {
    "text": "amount of ram dedicated to your instance you could do that you could launch a bigger instance detach and attach the",
    "start": "278960",
    "end": "284320"
  },
  {
    "text": "volume and you're ready to go or on the opposite side if your storage needs were growing but you actually didn't need to",
    "start": "284320",
    "end": "291039"
  },
  {
    "text": "add more computes so it didn't make sense to you from a cost and performance perspective to launch bigger instance",
    "start": "291039",
    "end": "296479"
  },
  {
    "text": "you could just grow your actual storage amount",
    "start": "296479",
    "end": "301639"
  },
  {
    "text": "so an EBS volume itself is launched into a a single a it's an a specific volume",
    "start": "303240",
    "end": "310400"
  },
  {
    "text": "but it's it's replicated all the availability and durability is taking care of you so every block you write to",
    "start": "310400",
    "end": "316560"
  },
  {
    "text": "an EBS volume is synchronously replicated within that same a and that's",
    "start": "316560",
    "end": "321759"
  },
  {
    "text": "done uh beyond the storage of provision so you only pay for what you provision you're not actually paying for this",
    "start": "321759",
    "end": "327160"
  },
  {
    "text": "synchronous replica of your data and that leads to some uh very interesting",
    "start": "327160",
    "end": "332560"
  },
  {
    "text": "availability and durability statistics I mean uh you know EBS is designed to run",
    "start": "332560",
    "end": "337880"
  },
  {
    "text": "at five nines of availability the reality is we we do much better than that we get nearer to six nines of",
    "start": "337880",
    "end": "343560"
  },
  {
    "text": "availability which uh you know if you consider 69 of availability in a single availability Zone it's a it's a very",
    "start": "343560",
    "end": "349800"
  },
  {
    "text": "impressive uptime statistic on a per volume basis and same with durability so uh durability the annualized failure",
    "start": "349800",
    "end": "356680"
  },
  {
    "text": "rate of a single EBS volume is designed to be between 0.1 and. 2% per year which",
    "start": "356680",
    "end": "363440"
  },
  {
    "text": "is about 20 times higher than your average you know commodity hard disc drive so significant value in both the",
    "start": "363440",
    "end": "370280"
  },
  {
    "text": "durability of the single volume and the availability of those volumes now beyond",
    "start": "370280",
    "end": "375479"
  },
  {
    "text": "that you also have the capability of taking snapshots or point and time copies of your data so that's a snapshot",
    "start": "375479",
    "end": "382160"
  },
  {
    "text": "of the volume that is then copied up into S3 now S3 of course is a regional",
    "start": "382160",
    "end": "387680"
  },
  {
    "text": "service it's not constrained to specific availability Zone and it also has 11 nines of durability so now we're talking",
    "start": "387680",
    "end": "393800"
  },
  {
    "text": "a ma much magnitude greater for the uh preservation of your data and these snapshots what exactly",
    "start": "393800",
    "end": "401400"
  },
  {
    "start": "400000",
    "end": "400000"
  },
  {
    "text": "are they you know how do they actually work so when you take a",
    "start": "401400",
    "end": "406840"
  },
  {
    "text": "snapshot the first one is a full copy of all the states of the change blocks on",
    "start": "407400",
    "end": "412880"
  },
  {
    "text": "your volume so here you see we have blocks a b and c and it's important to know that when",
    "start": "412880",
    "end": "419879"
  },
  {
    "text": "you actually take that snapshot you're not waiting around for that data to transfer as soon as that API call for",
    "start": "419879",
    "end": "425680"
  },
  {
    "text": "create snapshot completes with a success it just comes back and said 200 success you're ready to use that volume again",
    "start": "425680",
    "end": "432319"
  },
  {
    "text": "all the data transfers is actually happening in the background you don't wait for it in order to use your volume",
    "start": "432319",
    "end": "438759"
  },
  {
    "text": "again and so we move on to the second one now the snapshots are incremental so as you take more and more snapshots of",
    "start": "438759",
    "end": "444919"
  },
  {
    "text": "the same volume the only thing that's going to be on subsequent snapshots is then you change data so here we see",
    "start": "444919",
    "end": "450520"
  },
  {
    "text": "we've modified the C block it becomes C1 we take that snapshot so the only thing on that snapshot is the incremental",
    "start": "450520",
    "end": "458280"
  },
  {
    "text": "difference and we move on to the third snapshot of the same volume we've added two new blocks but we've also deleted B",
    "start": "458280",
    "end": "465720"
  },
  {
    "text": "Block and we take that snapshot of the state of that point in time the state of",
    "start": "465720",
    "end": "470800"
  },
  {
    "text": "your data and now we see we got the two new ones and the record that that block has been removed so what happens if we",
    "start": "470800",
    "end": "477319"
  },
  {
    "text": "actually delete these two so the system is designed so that the",
    "start": "477319",
    "end": "483319"
  },
  {
    "text": "latest snapshot is always the only thing necessary to restore your volume to its state when that snapshot was taken so",
    "start": "483319",
    "end": "489800"
  },
  {
    "text": "even though for example the a block is not represented in snapshot number three",
    "start": "489800",
    "end": "494879"
  },
  {
    "text": "it is actually symbolically linked there so that if you use that snapshot then to restore that volume uh to to a new",
    "start": "494879",
    "end": "502319"
  },
  {
    "text": "volume you're going to get the state of that volume at the time the snapshot was taken so you only ever have to keep that",
    "start": "502319",
    "end": "510080"
  },
  {
    "text": "last one and so the data when you create a",
    "start": "510080",
    "end": "516360"
  },
  {
    "text": "new volume from a snapshot starts streaming in in a lazy load fashion so here you see a comes in and C comes in",
    "start": "516360",
    "end": "522560"
  },
  {
    "text": "but if for example you access this volume and you called the block that hadn't been loaded yet that block would",
    "start": "522560",
    "end": "527839"
  },
  {
    "text": "be loaded to the front of the que and move in first so if you happen to hit e and you wanted e that one would move to",
    "start": "527839",
    "end": "533200"
  },
  {
    "text": "the front it would be uh populated on your volume itself and then it the lazy load would move on and start loading the",
    "start": "533200",
    "end": "539279"
  },
  {
    "text": "rest rest of the blocks so you can use snapshots to",
    "start": "539279",
    "end": "545920"
  },
  {
    "text": "launch new volumes in other availability zones you can use snapshots to in resize",
    "start": "545920",
    "end": "551360"
  },
  {
    "text": "the size of your volume so if you decide you want a new Vol you want the same volume but a bigger size you launch a",
    "start": "551360",
    "end": "556440"
  },
  {
    "text": "new snapshot bigger size resize your file system to meet that new volume size with the same data as you had",
    "start": "556440",
    "end": "562600"
  },
  {
    "text": "before and obviously every uh volume create has its own replica you can use Snapshot to copy",
    "start": "562600",
    "end": "570720"
  },
  {
    "text": "across regions so this is a very common component of any aws-based disaster recovery strategy you can copy them",
    "start": "570720",
    "end": "577519"
  },
  {
    "text": "across accounts you can share them across accounts and a good representation of",
    "start": "577519",
    "end": "584240"
  },
  {
    "text": "this is how uh public data sets are actually represented so beyond the snapshots you see where you know if you",
    "start": "584240",
    "end": "591040"
  },
  {
    "text": "go into the Management console you laun ec2 instance if you launch Amazon Linux if",
    "start": "591040",
    "end": "596240"
  },
  {
    "text": "you launch Windows if you launch any of those are all EBS act you know Amazon machine images but",
    "start": "596240",
    "end": "603320"
  },
  {
    "text": "we also have public data sets so you can launch data sets ready to go from a snapshot attach them your instances and",
    "start": "603320",
    "end": "609440"
  },
  {
    "text": "have full access to data sets like genomic data Census Data global weather",
    "start": "609440",
    "end": "614480"
  },
  {
    "text": "Transportation data so they're all available out there for free there's a link there if you want to check out the public data",
    "start": "614480",
    "end": "622000"
  },
  {
    "text": "sets so what happens in in Failure situations so what if the what if the",
    "start": "623880",
    "end": "628920"
  },
  {
    "start": "624000",
    "end": "624000"
  },
  {
    "text": "volume is self fails so that's a very simple EBS based client which is running on the physical host it's hosting your",
    "start": "628920",
    "end": "635240"
  },
  {
    "text": "instance detects that failure makes the switch to the replica of that volume and",
    "start": "635240",
    "end": "640440"
  },
  {
    "text": "as soon as that switch happens that volume now becomes the main volume and it starts replicating its data to a new",
    "start": "640440",
    "end": "646279"
  },
  {
    "text": "replica and this happens in a matter of seconds and the The Experience from your instance would be a few moments of",
    "start": "646279",
    "end": "652560"
  },
  {
    "text": "blocked iio what if you terminate the instance",
    "start": "652560",
    "end": "657800"
  },
  {
    "start": "655000",
    "end": "655000"
  },
  {
    "text": "that the EBS volume is attached to um and this is this is an important",
    "start": "657800",
    "end": "663399"
  },
  {
    "text": "piece of housekeeping I see lots of customers who keep a lot of volumes around because they didn't know about this this flag that is attached to every",
    "start": "663399",
    "end": "670959"
  },
  {
    "text": "EBS volume so the the delete on termination flag which if you just",
    "start": "670959",
    "end": "676720"
  },
  {
    "text": "create a volume outside of launching an instance that flag is by default set to false so if it is set to false and you",
    "start": "676720",
    "end": "684120"
  },
  {
    "text": "terminate the instance that that volume is attached to then the then the volume itself will just endure it will persist",
    "start": "684120",
    "end": "691480"
  },
  {
    "text": "outside of the life of the instance but if the flag is set to True",
    "start": "691480",
    "end": "696519"
  },
  {
    "text": "which is what it set to default if it's a boot volume for example when you when you launch an ec2 instance or if it's",
    "start": "696519",
    "end": "703839"
  },
  {
    "text": "any other instance that is attached to the ec2 instance you create when you launch it those volumes by default will",
    "start": "703839",
    "end": "709880"
  },
  {
    "text": "be marched true and they will be deleted along with the ec2 instance so make sure",
    "start": "709880",
    "end": "716440"
  },
  {
    "text": "that you're keeping Good Housekeeping with these flags and making sure that you're not taking for granted that when you terminate ec2 instances that the",
    "start": "716440",
    "end": "723360"
  },
  {
    "text": "volumes are going with them you want to make sure that flag is set so you don't have a bunch of volumes sitting around that aren't doing anything and not",
    "start": "723360",
    "end": "731120"
  },
  {
    "text": "attached so what if the instance itself fails obviously if the flag is set um",
    "start": "731120",
    "end": "739839"
  },
  {
    "text": "the the volume persists then you can launch a new easy2 instance and attach it to that volume um but there's",
    "start": "739839",
    "end": "745880"
  },
  {
    "text": "actually a unique um feature enabled by EBS which is called ec2 Auto Recovery um",
    "start": "745880",
    "end": "752920"
  },
  {
    "start": "748000",
    "end": "748000"
  },
  {
    "text": "and it enables you to automatically without anything on your behalf any",
    "start": "752920",
    "end": "758240"
  },
  {
    "text": "action to be taken on your behalf recovers that instance on an instance failure uh and that instance comes back",
    "start": "758240",
    "end": "764639"
  },
  {
    "text": "on new hardware with all the characteristics that that former instance had so you know ec2 over the",
    "start": "764639",
    "end": "770399"
  },
  {
    "text": "years has developed this very robust uh system of health checks that cover both",
    "start": "770399",
    "end": "775639"
  },
  {
    "text": "the hardware and software of not only the physical host but your instance itself itself it covers things like",
    "start": "775639",
    "end": "781880"
  },
  {
    "text": "power networking the applications like the hypervisor running on the physical host so all these things are all being",
    "start": "781880",
    "end": "788680"
  },
  {
    "text": "Health checked um in the background and they surface up to you in in two ways so",
    "start": "788680",
    "end": "795600"
  },
  {
    "text": "they're all combined and they have a system level check these are available in cloudwatch so uh there's a metric for",
    "start": "795600",
    "end": "802480"
  },
  {
    "text": "a system level check and an instance level check and uh until this feature the ec2 auto recovery was available",
    "start": "802480",
    "end": "808720"
  },
  {
    "text": "there wasn't much you could do with these uh with these checks they would alarm they would send you an alarm but",
    "start": "808720",
    "end": "814600"
  },
  {
    "text": "as far as the actions you could take you were pretty limited you could just launch new instances to replace uh the",
    "start": "814600",
    "end": "820079"
  },
  {
    "text": "failed ones but with auto",
    "start": "820079",
    "end": "823720"
  },
  {
    "text": "recovery you can actually set actions on the failure of these alarms so for",
    "start": "826320",
    "end": "832320"
  },
  {
    "text": "example if the instant status check fails you can set it to automatically reboot so if your file system gets",
    "start": "832320",
    "end": "838639"
  },
  {
    "text": "corrupted or something goes wrong with your Nick where you can't your virtual Nick on your instance where you can't actually reach the instance that that instance",
    "start": "838639",
    "end": "845800"
  },
  {
    "text": "will then fail that health check and you automatically be rebooted and same with the system level",
    "start": "845800",
    "end": "851279"
  },
  {
    "text": "check so if something goes along with the underlying Hardware on the physical host you can set an action to recover so",
    "start": "851279",
    "end": "857199"
  },
  {
    "text": "it that that instance will be automatically migrated to a new set of hardware and it will retain",
    "start": "857199",
    "end": "864000"
  },
  {
    "text": "all of the characteristics of that instance all the way down to how the volumes are attached what IP addresses",
    "start": "864000",
    "end": "869600"
  },
  {
    "text": "were assigned to it even the instance ID and I note there at the bottom this is limited to to newer generation instance",
    "start": "869600",
    "end": "878000"
  },
  {
    "text": "types which use EBS only storage so obviously this recovery depends on EBS to actually",
    "start": "878000",
    "end": "885040"
  },
  {
    "text": "work so let's get into the actual volume types",
    "start": "885040",
    "end": "890120"
  },
  {
    "text": "themselves so all of ebs's volume types are built on two different Hardware platforms either an SSD BAS B platform",
    "start": "892639",
    "end": "900560"
  },
  {
    "text": "or spinning hard dis drive so and they obviously have very different characteristics you know ssds are very",
    "start": "900560",
    "end": "906079"
  },
  {
    "text": "low latency they're very good for random IO workloads because you know all points on the disc are equally reachable versus",
    "start": "906079",
    "end": "913800"
  },
  {
    "text": "with a hard drive you're up to the mercy of you know rotational physics where you have to get the head exactly in the",
    "start": "913800",
    "end": "919320"
  },
  {
    "text": "right place and if it's a random workload that head is going all over the place and adding to the latency of",
    "start": "919320",
    "end": "924639"
  },
  {
    "text": "actually delivering your data but that said hard dis drives have some very uh interesting qualities when",
    "start": "924639",
    "end": "931399"
  },
  {
    "text": "it comes to getting that head in the right place and delivering sequential data very",
    "start": "931399",
    "end": "938519"
  },
  {
    "text": "fast so under ssds we have the two volume types which I previously",
    "start": "940959",
    "end": "946279"
  },
  {
    "text": "mentioned so first is gp2 or general purpose SSD which is kind of the the",
    "start": "946279",
    "end": "951440"
  },
  {
    "text": "Workhorse of our Fleet it's it's the default boot volume type for any ec2 instance that you launch um and uh we'll",
    "start": "951440",
    "end": "958480"
  },
  {
    "text": "get into the ual performance characteristics in a second and then i1 which is our provisioned iops which is",
    "start": "958480",
    "end": "963519"
  },
  {
    "text": "the one I mentioned that you can actually dial in the specific consistency of iops you want delivered and we give that to you at a Guaranteed",
    "start": "963519",
    "end": "971680"
  },
  {
    "text": "Rate and then under the hard disk platform we have the the two new volume types that were mentioned by uh you know",
    "start": "971680",
    "end": "978040"
  },
  {
    "text": "Matt this morning and these are the ST1 or the throughput optimized HDD and the",
    "start": "978040",
    "end": "983720"
  },
  {
    "text": "SC1 or the cold HDD and these are both throughput optimized so that now the",
    "start": "983720",
    "end": "989160"
  },
  {
    "text": "platform is kind of evolved in these two types of of volumes we have SSD based",
    "start": "989160",
    "end": "994480"
  },
  {
    "text": "which are iops performance-based where you tell us the kind of iops you want and performance is delivered on an iops",
    "start": "994480",
    "end": "1000959"
  },
  {
    "text": "basis or input output per second and then the HDD based the hard disk based platforms which are throughput based so",
    "start": "1000959",
    "end": "1008240"
  },
  {
    "text": "we deliver performance in a megabytes per second fashion and what about the the you know",
    "start": "1008240",
    "end": "1015800"
  },
  {
    "text": "our venerable magnetic uh EBS volume type that it's been around since 2008",
    "start": "1015800",
    "end": "1020839"
  },
  {
    "text": "we'll get to him in a minute uh but first let's talk about the uh the gp2 so",
    "start": "1020839",
    "end": "1026600"
  },
  {
    "text": "gp2 like I said you you tell us how much storage you want and for every gigabyte",
    "start": "1026600",
    "end": "1032959"
  },
  {
    "text": "of storage you provision you get three three iops up to a limit of 10,000 each",
    "start": "1032959",
    "end": "1038038"
  },
  {
    "text": "each volume can support up to 160 megabytes a second of actual throughput",
    "start": "1038039",
    "end": "1043199"
  },
  {
    "text": "and then obviously SSD Bas the millisec single digit millisecond latency is very",
    "start": "1043199",
    "end": "1049000"
  },
  {
    "text": "good and the capacity is anywhere from 1 gig all the way up to 16 terabyt but the interesting thing about uh gp2 is that",
    "start": "1049000",
    "end": "1056080"
  },
  {
    "text": "we introduced a burst model so uh as I described before um customers didn't",
    "start": "1056080",
    "end": "1062640"
  },
  {
    "text": "know exactly how many iops they needed to provision uh but they knew that their workloads were pretty variable and could",
    "start": "1062640",
    "end": "1068280"
  },
  {
    "text": "do very well with a burst model so we came up with this burst model for",
    "start": "1068280",
    "end": "1075120"
  },
  {
    "text": "gp2 which looks like this so so for every gigabyte of data you provision you",
    "start": "1076159",
    "end": "1083600"
  },
  {
    "start": "1077000",
    "end": "1077000"
  },
  {
    "text": "get three Ops and it's constantly accumulating in this birth bucket to the point where with gp2 you",
    "start": "1083600",
    "end": "1091360"
  },
  {
    "text": "can build it up to 5.4 million credits and then you can spend those credits at a rate of up to",
    "start": "1091360",
    "end": "1098360"
  },
  {
    "text": "3,000 iops per second so that means you can burst up to 3,000 iops no matter",
    "start": "1098360",
    "end": "1104120"
  },
  {
    "text": "what the size of your volume at any point in time and you can",
    "start": "1104120",
    "end": "1109440"
  },
  {
    "text": "you always can go back down to your Baseline which is the three I Ops per gigabyte Pro",
    "start": "1109440",
    "end": "1115240"
  },
  {
    "text": "Provisions but the concept of burst is a little misleading because uh you know burst implies that this is not something",
    "start": "1115799",
    "end": "1123240"
  },
  {
    "start": "1117000",
    "end": "1117000"
  },
  {
    "text": "that you depend on for a large amount of time well the fact is even at one",
    "start": "1123240",
    "end": "1129120"
  },
  {
    "text": "gigabyte sorry we're going to show you the provision model first it shows you how fast you can get up to 10,000 so uh",
    "start": "1130360",
    "end": "1138520"
  },
  {
    "text": "the the the the burst itself is only up to one terabyte so everything over a",
    "start": "1138520",
    "end": "1143640"
  },
  {
    "text": "terabyte is obviously up to 3,000 already so you get exactly what that is so here we're just illustrating what the",
    "start": "1143640",
    "end": "1149880"
  },
  {
    "text": "Baseline of iops is as you provisioned versus the burst and then the point at a little",
    "start": "1149880",
    "end": "1156400"
  },
  {
    "text": "over three terabytes where you reach the maximum amount of iops for a gp2",
    "start": "1156400",
    "end": "1162080"
  },
  {
    "text": "volume this is what I wanted so burst itself so even at",
    "start": "1162080",
    "end": "1169720"
  },
  {
    "text": "at one gigabyte you can see you still get about 30 minutes of burst so it's quite a significant amount of time of",
    "start": "1169720",
    "end": "1174840"
  },
  {
    "text": "burst and as you move up the scale of the amount of storage you've allocated to your volume you know at 300 gig you",
    "start": "1174840",
    "end": "1181840"
  },
  {
    "text": "get 43 minutes that's 43 minutes of 3,000 iops solid",
    "start": "1181840",
    "end": "1187760"
  },
  {
    "text": "uh and as you work your way up the curve you can see you know at half a terab you get a full hour of burst so burst is",
    "start": "1187760",
    "end": "1195320"
  },
  {
    "text": "little bit of a misleading term um and obviously is the closer you get to one terab the closer you get to Infinity",
    "start": "1195320",
    "end": "1201080"
  },
  {
    "text": "because you're reaching that limit so what about provision iops so",
    "start": "1201080",
    "end": "1208240"
  },
  {
    "text": "this is our i1 volume you can see the performance characteristics are basically doubled as what we saw in gp2",
    "start": "1208240",
    "end": "1215080"
  },
  {
    "text": "so instead of 10,000 as your max per volume you have 20,000 iops per volume your throughput again is also doubled",
    "start": "1215080",
    "end": "1221280"
  },
  {
    "text": "instead of 160 it's 320 and the capacity is is the same",
    "start": "1221280",
    "end": "1227320"
  },
  {
    "text": "maximum size of 16 terab but uh you know i1 is designed for",
    "start": "1227320",
    "end": "1234880"
  },
  {
    "text": "workloads that are are steady state iops they don't need a burst model you need a",
    "start": "1234880",
    "end": "1239960"
  },
  {
    "text": "guaranteed amount of iops for your workload you know what it takes it runs that day in day out so these are you",
    "start": "1239960",
    "end": "1245760"
  },
  {
    "text": "know workloads like uh you know Mission critical databases transactional databases that have very consistent",
    "start": "1245760",
    "end": "1251840"
  },
  {
    "text": "levels of iops that need to be delivered with a",
    "start": "1251840",
    "end": "1256280"
  },
  {
    "text": "guarantee so unless like gp2 you can scale up very fast to the",
    "start": "1257159",
    "end": "1265000"
  },
  {
    "text": "maximum amount of iops for an io1 volume you know you you get at 66 667 gigabytes",
    "start": "1265000",
    "end": "1273840"
  },
  {
    "text": "you can already provision 20,000 iops to a single",
    "start": "1273840",
    "end": "1278960"
  },
  {
    "start": "1282000",
    "end": "1282000"
  },
  {
    "text": "volume now what about our new ST1 volumes that are throughput provisioned and now we move into a model we're not",
    "start": "1282840",
    "end": "1289679"
  },
  {
    "text": "provisioning the volumes themselves on iops we're provisioning them based on megabytes per second but the interesting",
    "start": "1289679",
    "end": "1295679"
  },
  {
    "text": "thing about these is that we've also taken the burst model to to the new",
    "start": "1295679",
    "end": "1301039"
  },
  {
    "text": "magnetic based hard drive new magnetic based volume so now we have uh a baseline that's determined again the",
    "start": "1301039",
    "end": "1307840"
  },
  {
    "text": "bigger the volume the more performance so on a per terabyte basis you get 40 megabytes of throughput up to a maximum",
    "start": "1307840",
    "end": "1315279"
  },
  {
    "text": "of 500 which is obviously very significant over and above what we were seeing with uh the i1 and the gp2 max",
    "start": "1315279",
    "end": "1323360"
  },
  {
    "text": "throughput per volume um and again we have a burst model where you can burst up to for every terabyte you provision",
    "start": "1323360",
    "end": "1329760"
  },
  {
    "text": "you can burst 250 megabytes up to a maximum of 500 and the capacity is also",
    "start": "1329760",
    "end": "1335360"
  },
  {
    "text": "interesting so the the minimum volume size is 500 gigs so these are not volume types that you're going to be attaching",
    "start": "1335360",
    "end": "1341679"
  },
  {
    "text": "for boot volumes these will be data only volumes",
    "start": "1341679",
    "end": "1346720"
  },
  {
    "text": "so they're obviously ideal and designed for workloads that have sustained sequential workloads so this is anything",
    "start": "1348840",
    "end": "1356039"
  },
  {
    "text": "from you know log processing like Splunk data warehousing like",
    "start": "1356039",
    "end": "1361679"
  },
  {
    "text": "vertica uh streaming data workloads like Kafka so a wide variety of workload",
    "start": "1361679",
    "end": "1368200"
  },
  {
    "text": "types that have sequential throughput requirements and the burst bucket Works",
    "start": "1368200",
    "end": "1374880"
  },
  {
    "start": "1373000",
    "end": "1373000"
  },
  {
    "text": "a little bit different than gp2 so now you're accumulating based on the throughput throughput provisions so 40",
    "start": "1374880",
    "end": "1382200"
  },
  {
    "text": "megabytes per second per terabyte but you can build up to the size of the",
    "start": "1382200",
    "end": "1387559"
  },
  {
    "text": "volume itself so uh it's important to note that all these burst buckets start",
    "start": "1387559",
    "end": "1392760"
  },
  {
    "text": "100% full when you launch the volume so the idea is that the burst bucket size for these volumes is equivalent to what",
    "start": "1392760",
    "end": "1399720"
  },
  {
    "text": "it would take to scan the entire volume at Full Burst rate right so that's why you can build up to the actual size of",
    "start": "1399720",
    "end": "1405880"
  },
  {
    "text": "your volume itself and then you can spend at the actual",
    "start": "1405880",
    "end": "1411200"
  },
  {
    "text": "burst rate so let's do an example of an 8 terabyte volume so at 8 * 40 40",
    "start": "1411200",
    "end": "1419240"
  },
  {
    "start": "1414000",
    "end": "1414000"
  },
  {
    "text": "megabytes per terabyte you're you're accumulating constantly accumulating credits at a 320 megabytes a second rate",
    "start": "1419240",
    "end": "1427039"
  },
  {
    "text": "and you can build that bucket up to eight which is the size of your volume itself and be able to burst an entire",
    "start": "1427039",
    "end": "1432720"
  },
  {
    "text": "scan of your volume at 500 megabytes a second",
    "start": "1432720",
    "end": "1438640"
  },
  {
    "text": "so here we look at those volume sizes and as the volume sizes increase we can see what that does both",
    "start": "1444600",
    "end": "1451000"
  },
  {
    "text": "for the base values and for the burst values and for the example we just got we can just see there what an 8 terabyte",
    "start": "1451000",
    "end": "1457159"
  },
  {
    "text": "volume looks like it crosses a line just above 300 megabytes a second and its",
    "start": "1457159",
    "end": "1462960"
  },
  {
    "text": "burst rate is already at the max at 500",
    "start": "1462960",
    "end": "1468559"
  },
  {
    "text": "now what about SC1 so SC1 the the cold HDD is the same platform as ST1 so it's",
    "start": "1470159",
    "end": "1477679"
  },
  {
    "text": "Bas the same basic platform it's just you trade a little bit of throughput for a really uh really interesting price",
    "start": "1477679",
    "end": "1485000"
  },
  {
    "text": "performance curve so we we'll look at prices in a second um but the the SC1 is",
    "start": "1485000",
    "end": "1490480"
  },
  {
    "text": "for workloads that are also sequential based where you need High throughput where your workload itself is judged on",
    "start": "1490480",
    "end": "1497000"
  },
  {
    "text": "the throughput of uh and you know it it it lowers it from 40",
    "start": "1497000",
    "end": "1503120"
  },
  {
    "text": "to 12 so you get 12 megabytes per terabyte up to a maximum of 192 and the",
    "start": "1503120",
    "end": "1508279"
  },
  {
    "text": "burst is also a little bit lower so you get 80 megabytes per terabyte up to a maximum of",
    "start": "1508279",
    "end": "1515200"
  },
  {
    "text": "250 so this is this is ideal for still sequential workloads but maybe not as",
    "start": "1516840",
    "end": "1522120"
  },
  {
    "text": "hot workloads so workloads that you're not going to be accessing as often for example not doing those full volume",
    "start": "1522120",
    "end": "1527240"
  },
  {
    "text": "scans you know multiple times a day you might do them once a day so these might",
    "start": "1527240",
    "end": "1532320"
  },
  {
    "text": "be workloads like logging or backup or archiving of",
    "start": "1532320",
    "end": "1538278"
  },
  {
    "start": "1539000",
    "end": "1539000"
  },
  {
    "text": "data and so again you look at the burst bucket model you're always accumulating at 12 megabytes per terabyte the bucket",
    "start": "1539279",
    "end": "1546080"
  },
  {
    "text": "size again is related to the size of the volumes the bigger the volume the biger bigger the bucket and you can spend it up to 80",
    "start": "1546080",
    "end": "1553600"
  },
  {
    "text": "megabytes per terabyte",
    "start": "1553600",
    "end": "1557360"
  },
  {
    "text": "and a quick look at the burst and the base for the sc12 so you can see it go up to 250 quite quick at about a 4",
    "start": "1560240",
    "end": "1567440"
  },
  {
    "text": "terabyte size and then the base so the max the",
    "start": "1567440",
    "end": "1572600"
  },
  {
    "text": "max 16t you're getting 192 megabytes a second and then you have your burst up",
    "start": "1572600",
    "end": "1578320"
  },
  {
    "text": "to 250 so here we have a look at all four",
    "start": "1578320",
    "end": "1583799"
  },
  {
    "text": "of the volume types now the io provisioned and the throughput provisioned and and the prices so gp2 is",
    "start": "1583799",
    "end": "1590640"
  },
  {
    "text": "priced per gigabyte that's it you set how much storage you want and it's 10 cents a gig i1 is where you dial in your",
    "start": "1590640",
    "end": "1598399"
  },
  {
    "text": "performance you dial in the exact number of provision iops and we charge you for that and then there's also the charge",
    "start": "1598399",
    "end": "1604640"
  },
  {
    "text": "for the gigabytes themselves for the storage throughput provision model so",
    "start": "1604640",
    "end": "1611240"
  },
  {
    "text": "the ST1 is also priced just like the gp2 you tell us exactly how much gigabytes you want and you only pay for that so",
    "start": "1611240",
    "end": "1617360"
  },
  {
    "text": "you don't pay for any I for the new volumes you just pay for the storage provisioned and then this is a really",
    "start": "1617360",
    "end": "1623840"
  },
  {
    "text": "interesting one so uh you know two and a half cents a gig for the c1s um that",
    "start": "1623840",
    "end": "1630320"
  },
  {
    "text": "that's cheaper than S3 so that opens up some very interesting workload examples of being able to keep data on persistent",
    "start": "1630320",
    "end": "1638320"
  },
  {
    "text": "volumes that can be immediately accessed with very significant throughput um to",
    "start": "1638320",
    "end": "1643360"
  },
  {
    "text": "instances instead of you know maybe shuffling data back and forth from S3 you've got data sets that are sitting",
    "start": "1643360",
    "end": "1648640"
  },
  {
    "text": "there ready to",
    "start": "1648640",
    "end": "1651159"
  },
  {
    "text": "go and I told you we'd cover magnetic so let's get it over with so uh you know",
    "start": "1654640",
    "end": "1660399"
  },
  {
    "text": "magnetic has been around since 2008 um its performance characteristics are you",
    "start": "1660399",
    "end": "1666840"
  },
  {
    "text": "know the use cases are starting to get smaller and smaller um you know now that we have the new uh ST1 and SC1 that are",
    "start": "1666840",
    "end": "1674640"
  },
  {
    "start": "1670000",
    "end": "1670000"
  },
  {
    "text": "ideally uh made for uh hard dis Drive based workloads the magnetic is kind of",
    "start": "1674640",
    "end": "1679880"
  },
  {
    "text": "falling behind so everything about the magnetic is best effort so whether it's",
    "start": "1679880",
    "end": "1685159"
  },
  {
    "text": "iops or burst it's all best effort there's no consistency guarantees um you know if you don't care about how fast",
    "start": "1685159",
    "end": "1691200"
  },
  {
    "text": "your volumes boot because ssds obviously boot a lot faster or if you need something that's uh you know that's",
    "start": "1691200",
    "end": "1698679"
  },
  {
    "text": "small and cheap know maybe magnetic but in general you know we've sent magnetic",
    "start": "1698679",
    "end": "1704360"
  },
  {
    "text": "to the AWS retirement home that we call with with his friends simple DB and",
    "start": "1704360",
    "end": "1711279"
  },
  {
    "text": "things like the M1 small and the M2 and now magnetic so this is a place",
    "start": "1711279",
    "end": "1716840"
  },
  {
    "text": "that we call AWS previous generation and that's kind of where magnetic lives",
    "start": "1716840",
    "end": "1723000"
  },
  {
    "text": "now so let's talk a little bit about some EBS best practices and let's start with",
    "start": "1724960",
    "end": "1733360"
  },
  {
    "start": "1735000",
    "end": "1735000"
  },
  {
    "text": "performance and and the first thing I want is to give give you understanding of how EBS actually is counting the iops",
    "start": "1735519",
    "end": "1742080"
  },
  {
    "text": "you know we've talked a lot about your burst bucket your metrics and how uh how",
    "start": "1742080",
    "end": "1747440"
  },
  {
    "text": "they relate to the iops you're sending towards EBS and how much you're spending but how do how does ebss count these",
    "start": "1747440",
    "end": "1753799"
  },
  {
    "text": "things so let's walk through a few examples and you can see how you might be able to optimize your workload to",
    "start": "1753799",
    "end": "1759240"
  },
  {
    "text": "best take advantage of how the burst buckets work so here we have an example where an",
    "start": "1759240",
    "end": "1764279"
  },
  {
    "text": "application is sending down random iOS so these are four four 16k random iOS",
    "start": "1764279",
    "end": "1770440"
  },
  {
    "text": "there's no way for the operating system to merge these into one it's got to send these down as as unique requests for",
    "start": "1770440",
    "end": "1777640"
  },
  {
    "text": "these blocks so those get sent down to representing",
    "start": "1777640",
    "end": "1783840"
  },
  {
    "text": "each train is uh is a block towards EBS and it has a certain capacity but in",
    "start": "1783840",
    "end": "1790679"
  },
  {
    "text": "this case since it's a random workload it's got to fill each car with only you know one block you know the car can take",
    "start": "1790679",
    "end": "1797159"
  },
  {
    "text": "a lot of blocks in fact for gp2 and i1 it can fill up to",
    "start": "1797159",
    "end": "1802880"
  },
  {
    "text": "256k of data in one of those cars but since this is a random workload it's",
    "start": "1802880",
    "end": "1808000"
  },
  {
    "text": "only the one so you're going to get charged for",
    "start": "1808000",
    "end": "1813360"
  },
  {
    "text": "four iops in that case now what if the application",
    "start": "1813360",
    "end": "1820279"
  },
  {
    "text": "actually sent down you know same 16ks size blocks but they were actually sequential they were able to be merged",
    "start": "1820279",
    "end": "1827159"
  },
  {
    "text": "so EBS sees that see sequential blocks coming in and it's going to put those all up on the same car and count those",
    "start": "1827159",
    "end": "1833919"
  },
  {
    "text": "as one unit so you have a 64k unit and you'll get charged one I",
    "start": "1833919",
    "end": "1840080"
  },
  {
    "text": "instead of",
    "start": "1840080",
    "end": "1842480"
  },
  {
    "start": "1844000",
    "end": "1844000"
  },
  {
    "text": "four now what about ST1 and SC1 you know they don't deal in iops they deal in",
    "start": "1845760",
    "end": "1852159"
  },
  {
    "text": "throughput so in this case we're sending down four 1meg sequential iOS",
    "start": "1852159",
    "end": "1859679"
  },
  {
    "text": "and ST1 and SC1 they count things in one megabyte units so in this case we can",
    "start": "1861519",
    "end": "1867840"
  },
  {
    "text": "fill out an entire car four cars and have four megabytes of throughput",
    "start": "1867840",
    "end": "1874240"
  },
  {
    "text": "counted against us so you re charge four four megabytes a second on your burst",
    "start": "1874240",
    "end": "1880120"
  },
  {
    "text": "bucket however and this is where things get interesting with the new volume types if if you mix and match workloads",
    "start": "1880120",
    "end": "1886919"
  },
  {
    "text": "so if you're sending both sequential and randoms at the same time you know maybe mismatch application or the application",
    "start": "1886919",
    "end": "1892840"
  },
  {
    "text": "has some components that that is sending some random IO down",
    "start": "1892840",
    "end": "1897639"
  },
  {
    "text": "there you know the the car that with the sequential is going to get filled but so",
    "start": "1898639",
    "end": "1904320"
  },
  {
    "text": "is the random so they're going to take up a full car just like before and the problem with that is",
    "start": "1904320",
    "end": "1912120"
  },
  {
    "text": "obviously you're going to get charged the same so you're still going to get charged for the throughput but the amount of data you actually transferred",
    "start": "1912120",
    "end": "1918399"
  },
  {
    "text": "is obviously a lot less so if you think back to our curves if you had a workload",
    "start": "1918399",
    "end": "1923600"
  },
  {
    "text": "that's driving against an ST1 or ST1 volume it's driving random and and",
    "start": "1923600",
    "end": "1929840"
  },
  {
    "text": "sequential workloads you know the burst bucket is going to declin at the same rate as it would if you were sending all",
    "start": "1929840",
    "end": "1936679"
  },
  {
    "text": "sequential IO the difference is going to be the amount of data you actually transfer with those credits",
    "start": "1936679",
    "end": "1944679"
  },
  {
    "text": "so now that we kind of have a little understanding of how uh EBS actually counts the io let's move up one more",
    "start": "1949080",
    "end": "1955519"
  },
  {
    "text": "layer and talk about the actual Network between your instances and EBS because bandwidth really matters when you're",
    "start": "1955519",
    "end": "1962760"
  },
  {
    "text": "dealing with performance and evbs so if you fire up a C3 to extra",
    "start": "1962760",
    "end": "1968279"
  },
  {
    "text": "large and you know launch your favorite per Network performance testing tool say I perf and start testing away at the",
    "start": "1968279",
    "end": "1974960"
  },
  {
    "text": "bandwidth you'll find that the pipe on a C3 2x large is about 125 megabytes a",
    "start": "1974960",
    "end": "1981159"
  },
  {
    "text": "second so that means you've got 125 megabytes a second to work with as far as pushing data to and from your EBS",
    "start": "1981159",
    "end": "1987360"
  },
  {
    "text": "volume the problem is that bandwidth is shared it's not just EBS you're accessing you're accessing other ec2",
    "start": "1987360",
    "end": "1994679"
  },
  {
    "text": "instances you might be accessing S3 this is a shared network bandwidth allocation for all of",
    "start": "1994679",
    "end": "2001799"
  },
  {
    "text": "the things that this instance might be talking to if you fire up a c38 extra large",
    "start": "2001799",
    "end": "2007919"
  },
  {
    "text": "that's obviously a 10 gbit per second instance type you've got the full pipe",
    "start": "2007919",
    "end": "2013600"
  },
  {
    "text": "for that instance of 10 gbits and you get a lot more bandwidth but it's still to all of those things that you're",
    "start": "2013600",
    "end": "2019799"
  },
  {
    "text": "sharing that bandwidth across and that's where EBS optimize",
    "start": "2019799",
    "end": "2025440"
  },
  {
    "text": "comes into play so EBS optimize is an option to give dedicated Network",
    "start": "2025440",
    "end": "2031799"
  },
  {
    "text": "bandwidth to your instance that goes directly to EBS so uh it's enabled by",
    "start": "2031799",
    "end": "2037919"
  },
  {
    "text": "default in many of the new EBS uh EBS only instance types so those are things like the C4 the D2 the M4 those",
    "start": "2037919",
    "end": "2045320"
  },
  {
    "text": "instances only use EBS so they are EBS optimized by default and that's",
    "start": "2045320",
    "end": "2050679"
  },
  {
    "text": "obviously free um and if this is something that's not turned on it's something you can turn on it's very easy",
    "start": "2050679",
    "end": "2056480"
  },
  {
    "text": "to just flip the switch and turn it on you just need to stop and start your instance and then you'll have the extra",
    "start": "2056480",
    "end": "2061638"
  },
  {
    "text": "bandwidth um but as I showed you the c38 extra large before it's not an option on",
    "start": "2061639",
    "end": "2067200"
  },
  {
    "text": "the C3 38 R38 and i28 because they have the full 10 of the physical host",
    "start": "2067200",
    "end": "2073040"
  },
  {
    "text": "underlying them so it's up to you uh how you want to use that full tin and there's a there's a link there",
    "start": "2073040",
    "end": "2079638"
  },
  {
    "text": "for the chart that shows every instance type and the actual bandwidth allocation that you will get for that instance type",
    "start": "2079639",
    "end": "2086040"
  },
  {
    "text": "if you enable ec2 optimized so on a C32 extra large if we",
    "start": "2086040",
    "end": "2094158"
  },
  {
    "text": "turn on EBS optimized we get another DED ated pipe of 1 GB per second so now",
    "start": "2094159",
    "end": "2100000"
  },
  {
    "text": "we've got basically just doubled our bandwidth so we've got 125 megabytes for",
    "start": "2100000",
    "end": "2105040"
  },
  {
    "text": "everything else except EVs and now we have a dedicated gig pipe for EVS",
    "start": "2105040",
    "end": "2110880"
  },
  {
    "start": "2109000",
    "end": "2109000"
  },
  {
    "text": "itself but what if we launch a volume that's you know a 2 terab gp2 volume",
    "start": "2113000",
    "end": "2119560"
  },
  {
    "text": "that's 6,000 iops with a max volume throughput of 160 megabytes a",
    "start": "2119560",
    "end": "2125839"
  },
  {
    "text": "second if we attach that to c3x large the EBS optimized bandwidth for",
    "start": "2125839",
    "end": "2132880"
  },
  {
    "text": "c3x large is only 62.5 megabytes a second and a Max of",
    "start": "2132880",
    "end": "2138760"
  },
  {
    "text": "46k iops so you can see there a huge disparity between what the volume can",
    "start": "2138760",
    "end": "2143880"
  },
  {
    "text": "actually take and what your instance can actually push across the bandwidth that it",
    "start": "2143880",
    "end": "2149880"
  },
  {
    "text": "has so this not a good fit so if we scale up one jump we jump",
    "start": "2150359",
    "end": "2156400"
  },
  {
    "text": "up to a C32 extra lock large now we're up to 125 megabytes a",
    "start": "2156400",
    "end": "2162480"
  },
  {
    "text": "second it's a much better fit for that volume size so if we need to be able to push the full throughput to that volume",
    "start": "2162480",
    "end": "2168440"
  },
  {
    "text": "now we have the bandwidth to be able to do",
    "start": "2168440",
    "end": "2171920"
  },
  {
    "text": "that so let's take a big one so an m410 extra large which has a full 10 gig uh",
    "start": "2176440",
    "end": "2184720"
  },
  {
    "text": "dedicated Network bandwidth and when you enable EBS optimized you get a second pipe of 4 gig so you got quite a lot of",
    "start": "2184720",
    "end": "2191720"
  },
  {
    "text": "bandwidth to deal with and so with this 4 gig so about 500 megabytes a second of of bandwidth to",
    "start": "2191720",
    "end": "2199760"
  },
  {
    "text": "EBS you know we can attach a single 8 terab ST1 volume which as we've talked",
    "start": "2200680",
    "end": "2207200"
  },
  {
    "text": "about can burst up to 500 and has a 320 base so now you've got a good fit for that if you wanted to burst a full",
    "start": "2207200",
    "end": "2213880"
  },
  {
    "text": "throughput you have the dedicated bandwidth to be able to do that uh but what about IO so if we went",
    "start": "2213880",
    "end": "2220760"
  },
  {
    "text": "somewhere around a terabyte of provisioned iops volume and dialed in 16,000 iops we could certainly push a",
    "start": "2220760",
    "end": "2227880"
  },
  {
    "text": "full 16,000 to that uh but we still got some room to go so if we wanted to go over and above that you know even if we",
    "start": "2227880",
    "end": "2234560"
  },
  {
    "text": "maxed that volume out at 20,000 iops we'd still have some good overhead with 32,000 available to us over that",
    "start": "2234560",
    "end": "2241440"
  },
  {
    "text": "link so that's where striping comes into play so",
    "start": "2241440",
    "end": "2248119"
  },
  {
    "text": "here we take two volumes and we raid zero them together stripe them together get Consolidated throughput across both",
    "start": "2248280",
    "end": "2255040"
  },
  {
    "text": "volumes they're combined throughput and so now we can actually push 32,000",
    "start": "2255040",
    "end": "2261800"
  },
  {
    "start": "2264000",
    "end": "2264000"
  },
  {
    "text": "iops so when does it make sense to raid and and and you know this is really a",
    "start": "2265440",
    "end": "2270880"
  },
  {
    "text": "performance decision of when to raid or you know or if your storage",
    "start": "2270880",
    "end": "2276599"
  },
  {
    "text": "requirements actually do exceed on a per instance basis 16 terabytes so if if one single volume is not",
    "start": "2276599",
    "end": "2283000"
  },
  {
    "text": "enough attach another one to be able to go beyond 16t for a single instance same",
    "start": "2283000",
    "end": "2288400"
  },
  {
    "text": "with throughput if your throughput requirements are greater than 500 you have one of these 10 gig or you have this M4 10x large that can do more than",
    "start": "2288400",
    "end": "2295280"
  },
  {
    "text": "500 megabytes a second then you would have to stripe them and same with iops if your boundaries were beyond the",
    "start": "2295280",
    "end": "2302240"
  },
  {
    "text": "performance characteristics of a single volume but what we don't",
    "start": "2302240",
    "end": "2308880"
  },
  {
    "text": "talk about is using raid for redundancy you know uh in the old world of of",
    "start": "2308880",
    "end": "2315319"
  },
  {
    "text": "physical hard drives we did a lot of raid five we did a lot of raid six we did a lot of RAID 10 and uh it it's just",
    "start": "2315319",
    "end": "2321319"
  },
  {
    "text": "not uh necessary in an EBS World um if you take this simple example of of of striping two volumes",
    "start": "2321319",
    "end": "2329200"
  },
  {
    "text": "together in a raate zero configuration so if you send blocks to this stripe",
    "start": "2329560",
    "end": "2334720"
  },
  {
    "text": "they're going down and they're being written to the volumes in a stripe but keep in mind that those replicas are still there so in essence you are",
    "start": "2334720",
    "end": "2341240"
  },
  {
    "text": "actually doing what a RAID 10 does because you've got the same uh number of",
    "start": "2341240",
    "end": "2346400"
  },
  {
    "text": "copies of your data but you're not having to provision double storage and you're also not having to",
    "start": "2346400",
    "end": "2353880"
  },
  {
    "text": "waste half of your bandwidth sending down the duplication of your data so raid one you know it Hal the",
    "start": "2353880",
    "end": "2360680"
  },
  {
    "text": "available EBS bandwidth effectively because you're s sending everything twice and same with raid 56 you lose",
    "start": "2360680",
    "end": "2366760"
  },
  {
    "text": "about 20 or 30% of your usable IO because that IO is being taken up by all those parody sends you're sending down",
    "start": "2366760",
    "end": "2373319"
  },
  {
    "text": "to to",
    "start": "2373319",
    "end": "2376000"
  },
  {
    "start": "2380000",
    "end": "2380000"
  },
  {
    "text": "EBS now what about volume initi initi Jason um this is something that we used",
    "start": "2380520",
    "end": "2386040"
  },
  {
    "text": "to talk about quite a bit in the past when you launch a new volume how do you get it if you want to be able to 100%",
    "start": "2386040",
    "end": "2391520"
  },
  {
    "text": "use that volume right off the bat as fast as possible what do you need to do to get that volume ready and there was",
    "start": "2391520",
    "end": "2396839"
  },
  {
    "text": "some things had to do in the past to be able to to to make that volume ready and it's it's no longer the case so any new",
    "start": "2396839",
    "end": "2403119"
  },
  {
    "text": "EBS volume that you launch requires no work to get it to 100% performance so",
    "start": "2403119",
    "end": "2408160"
  },
  {
    "text": "you don't have to touch the blocks you don't have to do any read through of the blocks new volumes you just attach them and",
    "start": "2408160",
    "end": "2415160"
  },
  {
    "text": "go uh that's not quite the same if you create a new volume based off a snapshot obviously that snapshot data itself is",
    "start": "2415160",
    "end": "2422160"
  },
  {
    "text": "an S3 so that lazy load process I described before that has to happen to get your data on the volume so the lazy",
    "start": "2422160",
    "end": "2429920"
  },
  {
    "text": "load process does start and start streaming your data that volume is immediately usable but it's not going to",
    "start": "2429920",
    "end": "2435800"
  },
  {
    "text": "be 100% performant until all that data is loaded so we do suggest that you do",
    "start": "2435800",
    "end": "2441560"
  },
  {
    "text": "initialize if you if you want to be able to use this volume at 100% performance right off the bat that you do some kind",
    "start": "2441560",
    "end": "2447000"
  },
  {
    "text": "of initialization across it so the best way to do that is just to do a random small block read across the entire",
    "start": "2447000",
    "end": "2452920"
  },
  {
    "text": "volume space that'll queue up all the volume all the blocks on that volume and start coming down at an increased speed",
    "start": "2452920",
    "end": "2459920"
  },
  {
    "text": "than they would just with the lazy load and then as soon as that uh is",
    "start": "2459920",
    "end": "2464960"
  },
  {
    "text": "complete you'll know that all the data is",
    "start": "2464960",
    "end": "2468680"
  },
  {
    "text": "loaded so let's move on to management so taking snapshots themselves um uh there's there's a few",
    "start": "2472280",
    "end": "2481000"
  },
  {
    "start": "2476000",
    "end": "2476000"
  },
  {
    "text": "uh best practices we have around making sure all of the various places in your",
    "start": "2481000",
    "end": "2486880"
  },
  {
    "text": "applic in your operating systems in your OS kernels where data can be cached so you",
    "start": "2486880",
    "end": "2493040"
  },
  {
    "text": "want to make sure that you flushed all this stuff out to dis before you actually take a snapshot if you want to",
    "start": "2493040",
    "end": "2498359"
  },
  {
    "text": "ensure that you're actually getting that entire point in time state of all of the",
    "start": "2498359",
    "end": "2504560"
  },
  {
    "text": "changes you've made to your data so we recommend things like uh with databases flushing databases flushing database CL",
    "start": "2504560",
    "end": "2511800"
  },
  {
    "text": "caches uh on your file systems make sure you're doing a sync or an FS freeze depending upon the file system that you",
    "start": "2511800",
    "end": "2517920"
  },
  {
    "text": "use and then once all that's done issue the the snapshot and again",
    "start": "2517920",
    "end": "2524240"
  },
  {
    "text": "when you issue the snapshot as soon as you get that successful return from the create snapshot API call you're ready to",
    "start": "2524240",
    "end": "2530400"
  },
  {
    "text": "go you don't have to sit around and wait for the actual snapshot itself to complete you know it'll go into a",
    "start": "2530400",
    "end": "2535520"
  },
  {
    "text": "pending State as it's actually transferring data to S3 but you do not have to wait for that to complete before",
    "start": "2535520",
    "end": "2541599"
  },
  {
    "text": "it's available to use",
    "start": "2541599",
    "end": "2545400"
  },
  {
    "start": "2546000",
    "end": "2546000"
  },
  {
    "text": "what about managing snapshots I mean if you've got let's say thousands of volumes and you're taking snapshots of",
    "start": "2547800",
    "end": "2554160"
  },
  {
    "text": "all of these things this can become very cumbersome about maintaining you know what's on the snapshot how long you keep",
    "start": "2554160",
    "end": "2560319"
  },
  {
    "text": "it around and so uh I get give a a quick example of some of the tools that you",
    "start": "2560319",
    "end": "2565880"
  },
  {
    "text": "can use to actually um manage all of these snapshots and be able to implement some kind of overhead and governance",
    "start": "2565880",
    "end": "2572960"
  },
  {
    "text": "around around all of these and we're going to build a simple framework based around a few AWS",
    "start": "2572960",
    "end": "2580559"
  },
  {
    "text": "services including Lambda the Amazon ec2 run command if you're not familiar with this this is a",
    "start": "2580559",
    "end": "2587079"
  },
  {
    "text": "relatively new service that allows you to be able to automate the automate and",
    "start": "2587079",
    "end": "2592400"
  },
  {
    "text": "run commands across your entire fleet at once so it's not the same as like an interactive SS SSH shell across all your",
    "start": "2592400",
    "end": "2599359"
  },
  {
    "text": "instances it's much more controlled than that so it it gets wrapped within IM Im so identity and access management",
    "start": "2599359",
    "end": "2606319"
  },
  {
    "text": "controls uh who can issue commands what those commands can get access to and what they can actually run resource wise",
    "start": "2606319",
    "end": "2613559"
  },
  {
    "text": "and it also is uh integrated with cloud trail so you get to audit exactly what the Run commands do but it allows you to",
    "start": "2613559",
    "end": "2620160"
  },
  {
    "text": "do things like send uh uh a bass shell script to all of your Linux instances",
    "start": "2620160",
    "end": "2626280"
  },
  {
    "text": "all at once and have them run it or same with Windows send a Powershell script to run across all your instances and do",
    "start": "2626280",
    "end": "2632319"
  },
  {
    "text": "maintenance or in this case we're actually going to use it to facilitate the snapshots",
    "start": "2632319",
    "end": "2638480"
  },
  {
    "text": "and there's some more information there if you haven't heard of it and finally robust use of tagging so",
    "start": "2638480",
    "end": "2645280"
  },
  {
    "text": "tagging is critical for maintaining uh you know what these volumes are what these instances are what volumes belong",
    "start": "2645280",
    "end": "2652079"
  },
  {
    "text": "to what instances and expiration dates as we'll see how we use tagging to to drive all of this logic it's all driven",
    "start": "2652079",
    "end": "2658839"
  },
  {
    "text": "by tagging so on our ec2 instances we're",
    "start": "2658839",
    "end": "2665319"
  },
  {
    "text": "going to we're going to apply a couple of tags we're going to try apply first a tag that says you know this this",
    "start": "2665319",
    "end": "2670400"
  },
  {
    "text": "instance is actually worth backing up please back this up and a second tag that actually designates any backups",
    "start": "2670400",
    "end": "2675839"
  },
  {
    "text": "from this instance what its retention date should be so very basic tax then we're going to use Lambda which",
    "start": "2675839",
    "end": "2683240"
  },
  {
    "text": "can you know execute C execute code servicely in the cloud for scheduled",
    "start": "2683240",
    "end": "2688720"
  },
  {
    "text": "events so Lambda has this ability to not only be executed on triggers so it also",
    "start": "2688720",
    "end": "2694240"
  },
  {
    "text": "can be scheduled to execute code on a set schedule so we're going to do a daily snapshot and our Lambda function",
    "start": "2694240",
    "end": "2701520"
  },
  {
    "text": "is going to execute and search across all of our instances for any that are tagged",
    "start": "2701520",
    "end": "2707520"
  },
  {
    "text": "backup it's then going to actually kick off those ec2 run commands through API",
    "start": "2707520",
    "end": "2713040"
  },
  {
    "text": "calls to ec2 run so ec2 run is going to go through our snapshot best practices it's going to Qui us file systems it's",
    "start": "2713040",
    "end": "2719400"
  },
  {
    "text": "going to wait for the feedback and then it's going to issue all the snapshots themselves against the instances",
    "start": "2719400",
    "end": "2726359"
  },
  {
    "text": "and then once all the snapshots start coming in we're going to do the Lambda function is do proper tagging of these",
    "start": "2727880",
    "end": "2733480"
  },
  {
    "text": "volumes themselves and it's going to compute the actual expiration date according to the tag that's on the",
    "start": "2733480",
    "end": "2739520"
  },
  {
    "text": "instances so 30 days it's going to compute all right what day is this that I'm actually issuing the snapshot and",
    "start": "2739520",
    "end": "2745359"
  },
  {
    "text": "slap that label on the snapshot itself you are going to expire on this",
    "start": "2745359",
    "end": "2751480"
  },
  {
    "start": "2753000",
    "end": "2753000"
  },
  {
    "text": "date so the snapshots now have this tag they say expire me on this date that we've",
    "start": "2753680",
    "end": "2760440"
  },
  {
    "text": "computed so we're going have second Lambda function again a centralized lamba function that every day is going",
    "start": "2761319",
    "end": "2766599"
  },
  {
    "text": "to cycle through our inventory search for any snapshots that are tagged to",
    "start": "2766599",
    "end": "2771680"
  },
  {
    "text": "expire on a certain day that day that it's run and any of those that are tagged for that day for expiration will delete them",
    "start": "2771680",
    "end": "2778720"
  },
  {
    "text": "so we've got two Lambda functions one to actually take the snapshots and slap labels on them and two to actually",
    "start": "2778720",
    "end": "2785520"
  },
  {
    "text": "search for things that should be a should be expired so uh that's pretty much it just two Lambda functions that",
    "start": "2785520",
    "end": "2791359"
  },
  {
    "text": "can can control both taking the snapshots across your Fleet and expiring them and just using a few of the AWS uh",
    "start": "2791359",
    "end": "2798160"
  },
  {
    "text": "tools and Frameworks to be able to facilitate",
    "start": "2798160",
    "end": "2802240"
  },
  {
    "text": "that so now a few words on security EBS encryption is something we launched uh last year and it applies to",
    "start": "2805160",
    "end": "2814640"
  },
  {
    "start": "2807000",
    "end": "2807000"
  },
  {
    "text": "uh to any current generation instance that supports uh the AES ni Intel",
    "start": "2814640",
    "end": "2820520"
  },
  {
    "text": "Hardware so that's the encryption Hardware built in underlying physical host it accelerates encryption um that",
    "start": "2820520",
    "end": "2827319"
  },
  {
    "text": "also means that there's no performance impact to actual encryption enabling encryption um and it's supported by all",
    "start": "2827319",
    "end": "2833559"
  },
  {
    "text": "of our EBS volume types uh anything that's um that's any snapshot that's made from",
    "start": "2833559",
    "end": "2840559"
  },
  {
    "text": "the actual volumes that are encrypted is also encrypted and and any volumes you create from those snapshots are also",
    "start": "2840559",
    "end": "2846319"
  },
  {
    "text": "encrypted uh the data at rest is encrypted the communication between the instance and",
    "start": "2846319",
    "end": "2851400"
  },
  {
    "text": "the EVS uh system is encrypted and obviously all all the data on the volume itself is encrypted and finally there is",
    "start": "2851400",
    "end": "2858319"
  },
  {
    "text": "there is no extra up charge for encryption",
    "start": "2858319",
    "end": "2862520"
  },
  {
    "text": "enablement and the best part is it's just",
    "start": "2865800",
    "end": "2870000"
  },
  {
    "text": "checkbox so when you launch a launch a volume there's a simple checkbox to encrypt this volume and that's pretty",
    "start": "2870960",
    "end": "2876559"
  },
  {
    "text": "much there's only one thing I'd like to point out as a best practice and that's where you choose the actual",
    "start": "2876559",
    "end": "2881720"
  },
  {
    "text": "master key so the master key is something that our our key management system or KMS",
    "start": "2881720",
    "end": "2887720"
  },
  {
    "text": "controls and if you do not have a key created yet in advance it's going to",
    "start": "2887720",
    "end": "2893480"
  },
  {
    "text": "automatically create one just for EBS and that's what you see here is the default so as a best practice we",
    "start": "2893480",
    "end": "2898640"
  },
  {
    "text": "recommend in advance you create your own master key that you're going to use for EBS and I'll tell you a few reasons why",
    "start": "2898640",
    "end": "2908520"
  },
  {
    "text": "so this is actually how you create um a master key a unique master key customer",
    "start": "2910720",
    "end": "2917839"
  },
  {
    "text": "master key you go into the encryption key setting which is in the IM am console so all the KMS functions are",
    "start": "2917839",
    "end": "2923000"
  },
  {
    "text": "actually integrated into the IM am Management",
    "start": "2923000",
    "end": "2928520"
  },
  {
    "text": "console you create a new a AWS KMS master key and you're just going to name",
    "start": "2928520",
    "end": "2933559"
  },
  {
    "text": "it whatever you want it in this case I've named it EBS Master because that's what I'm going to use",
    "start": "2933559",
    "end": "2938720"
  },
  {
    "text": "for and why would I do this because if I use the default I can't do any of these",
    "start": "2939559",
    "end": "2945520"
  },
  {
    "text": "things I can't I can't Define a key rotation policy I can't enable ad cloud",
    "start": "2945520",
    "end": "2951359"
  },
  {
    "text": "trail auditing of that key and how it's used I can't control who can use the key",
    "start": "2951359",
    "end": "2956760"
  },
  {
    "text": "and I can't control who can administer the key but if I create my own key and then when I uh encrypt my abs volumes",
    "start": "2956760",
    "end": "2962520"
  },
  {
    "text": "designate that key I can do all of these things so it's it's always recommended as the best practice to use your own key",
    "start": "2962520",
    "end": "2968599"
  },
  {
    "text": "and don't go with the default so you can see here we're back",
    "start": "2968599",
    "end": "2975520"
  },
  {
    "text": "at the you know checkbox encrypt this volume and and I've changed it to this new key that I've created instead of",
    "start": "2975520",
    "end": "2980920"
  },
  {
    "text": "taking the",
    "start": "2980920",
    "end": "2983318"
  },
  {
    "text": "default and how does this actually work so this master key which is which stays",
    "start": "2987359",
    "end": "2993359"
  },
  {
    "text": "always within the key management system uh uses encryption process called",
    "start": "2993359",
    "end": "2998440"
  },
  {
    "text": "envelope encryption to actually do the volume encryption so there's a unique",
    "start": "2998440",
    "end": "3003559"
  },
  {
    "text": "data key that is created for every single EBS volume so each EBS volume has its own data",
    "start": "3003559",
    "end": "3009680"
  },
  {
    "text": "key an EBS calls for the master key to encrypt",
    "start": "3009680",
    "end": "3016200"
  },
  {
    "text": "that key and so that encrypted key which is now double encrypted is stored as metadata with the volume itself",
    "start": "3016200",
    "end": "3025280"
  },
  {
    "text": "so whenever that volume is actually attached to an instance and that instance need to decrypt the data on the",
    "start": "3028359",
    "end": "3034359"
  },
  {
    "text": "volume it calls KMS to decrypt the data key only the data key and that data key",
    "start": "3034359",
    "end": "3039640"
  },
  {
    "text": "is kept in memory to actually do the bulk encryption and decryption of the data on the",
    "start": "3039640",
    "end": "3045680"
  },
  {
    "text": "volume and that's for every volume gets its own data key now this seems you know",
    "start": "3045680",
    "end": "3053240"
  },
  {
    "text": "rather complicated for what we're doing but there's some very unique qualities of envelope encryption that make it very",
    "start": "3053240",
    "end": "3059440"
  },
  {
    "text": "suitable for this kind of bulk data",
    "start": "3059440",
    "end": "3063480"
  },
  {
    "text": "encryption and here are a few of them so obviously it limits your exposure if for whatever reason one of those data Keys",
    "start": "3065240",
    "end": "3071520"
  },
  {
    "text": "is compromised all it's compromised is that single volume and not all your volumes it's it's Unique to that data",
    "start": "3071520",
    "end": "3077760"
  },
  {
    "text": "volume uh it increases the performance significantly because you're not shipping data back and forth between KMS",
    "start": "3077760",
    "end": "3084880"
  },
  {
    "text": "you know massive amounts of this EBS volume data to be encrypted and decrypted the encryption and the",
    "start": "3084880",
    "end": "3089920"
  },
  {
    "text": "decryption is happening at the place that the data is actually being created and it it increases performance",
    "start": "3089920",
    "end": "3095440"
  },
  {
    "text": "significantly and obviously it simplifies Key Management because you can have thousands upon thousands upon thousands of EBS volumes but one master",
    "start": "3095440",
    "end": "3103359"
  },
  {
    "text": "key can can control them and you can also have a lot of flexibility in how you use those master keys so you know",
    "start": "3103359",
    "end": "3109079"
  },
  {
    "text": "you could have a master key per application or po per business it's up to you how you use the master keys but",
    "start": "3109079",
    "end": "3115040"
  },
  {
    "text": "they themselves can be uh controlling any number of actual data Keys per",
    "start": "3115040",
    "end": "3121760"
  },
  {
    "text": "volume and finally uh something we launched just uh this year is the ability to also encrypt your boot volume",
    "start": "3123119",
    "end": "3130119"
  },
  {
    "text": "so it's uh it's a little bit different than the actual check volume process you need to um you need to create an Omi",
    "start": "3130119",
    "end": "3136799"
  },
  {
    "text": "from that so um you actually create an encrypted Omi by copying that an",
    "start": "3136799",
    "end": "3142400"
  },
  {
    "text": "existing Ami so say for example you wanted to create a copy of the Amazon Linux ony you launch copy Ami and on",
    "start": "3142400",
    "end": "3151040"
  },
  {
    "text": "that you would encrypt that Ami and then once you have that encrypted Ami copy you can now use that to launch any",
    "start": "3151040",
    "end": "3157400"
  },
  {
    "text": "number of other uh instances off that encrypted",
    "start": "3157400",
    "end": "3162280"
  },
  {
    "text": "Omi so that's about it so we've talked about all the volume types U about how you can select the instance types",
    "start": "3163480",
    "end": "3170160"
  },
  {
    "text": "themselves to fit your workload how to take snapshots how to tag snapshots how to manage snapshots and finally a little",
    "start": "3170160",
    "end": "3176640"
  },
  {
    "text": "bit about encryption when and how to use it so I really appreciate your time uh thank you for being here uh please",
    "start": "3176640",
    "end": "3182520"
  },
  {
    "text": "please leave feedback uh that's the only way we know what to add what to do",
    "start": "3182520",
    "end": "3187559"
  },
  {
    "text": "better next time and I appreciate your time thank you [Applause]",
    "start": "3187559",
    "end": "3194709"
  }
]