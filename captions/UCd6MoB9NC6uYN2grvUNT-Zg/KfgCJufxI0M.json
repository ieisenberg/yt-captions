[
  {
    "text": "applications today and a lot of them run on yarn so you can run hi bomb tez and spark both on the same sets of nodes and",
    "start": "210",
    "end": "7319"
  },
  {
    "text": "have yarn manage all the containers between them but you know real quick here is a you know what the architecture",
    "start": "7319",
    "end": "13530"
  },
  {
    "text": "looks like runs spark runs the yarn app executors are running yarn containers on yard node managers in EMR we run node",
    "start": "13530",
    "end": "20970"
  },
  {
    "text": "managers on both core and task nodes and you create a cluster the difference is a task I know doesn't have the HDFS data",
    "start": "20970",
    "end": "26820"
  },
  {
    "text": "node so when you scale out or scale and you're not impacting any data in HDFS however most customers are storing the",
    "start": "26820",
    "end": "32820"
  },
  {
    "text": "data in s3 so the shuffling of data and HDFS doesn't really make as much of a difference one common question is where",
    "start": "32820",
    "end": "40260"
  },
  {
    "text": "is the spark UI and spark on yarn you can access it through the resource manager UI if you go to the resource",
    "start": "40260",
    "end": "45989"
  },
  {
    "text": "manager and find your yarn app there's a link for the application tracking UI you can click on that it'll proxy you",
    "start": "45989",
    "end": "51300"
  },
  {
    "text": "through to the spark UI whether it's running in a driver and client mode where it's running on the master node of",
    "start": "51300",
    "end": "56670"
  },
  {
    "text": "your cluster or in cluster mode where it's running in the application master and proxying you through to that node",
    "start": "56670",
    "end": "63809"
  },
  {
    "text": "where it is EMR configures executors using dynamic allocation we've been",
    "start": "63809",
    "end": "69840"
  },
  {
    "text": "doing that for quite some time you know you can still use spark submit and specify the amount of RAM and cores for",
    "start": "69840",
    "end": "75990"
  },
  {
    "text": "an executor and if you had one large multi tenant job or one large job for cluster that would make a lot of sense",
    "start": "75990",
    "end": "81299"
  },
  {
    "text": "however if you have say many applications running at once and you don't want to specify amount of resources for spark the whole dynamic",
    "start": "81299",
    "end": "88710"
  },
  {
    "text": "allocation is very useful so by default we'll set the memory and a number of cores to the specifications that make",
    "start": "88710",
    "end": "97680"
  },
  {
    "text": "most sense for you the nodes in your core group so we'll you know if your core group is a r32 Excel will make the",
    "start": "97680",
    "end": "104130"
  },
  {
    "text": "executor size enough where you can pack as many as possible and on those nodes but we won't actually specify the number",
    "start": "104130",
    "end": "109409"
  },
  {
    "text": "of them to use yarn will do that at runtime so as yarn thinks that you need more executors it'll have more and more",
    "start": "109409",
    "end": "115020"
  },
  {
    "text": "on and we talked about auto scaling you'll see that dynamic allocation is useful to scale out clusters and have",
    "start": "115020",
    "end": "120210"
  },
  {
    "text": "spark autumn yarn automatically add more and more capacity as it goes through however if you were say running one",
    "start": "120210",
    "end": "126689"
  },
  {
    "text": "large application like sages a cluster that was dedicate for one spark streaming application you can easily create one large executor per",
    "start": "126689",
    "end": "133530"
  },
  {
    "text": "node by using the maximize resource allocation option that'll basically calculate what's the largest executor",
    "start": "133530",
    "end": "140490"
  },
  {
    "text": "put on each node and then that job will statically use it so the number of executors will be static for the",
    "start": "140490",
    "end": "145680"
  },
  {
    "text": "duration of that job as well many options to submit spark jobs we've got",
    "start": "145680",
    "end": "151320"
  },
  {
    "text": "notebooks and ID's on the cluster we support the pachi Zeppelin by default",
    "start": "151320",
    "end": "157620"
  },
  {
    "text": "you can with a couple clicks have it installed but if you want you can bootstrap on something like Jupiter or",
    "start": "157620",
    "end": "163800"
  },
  {
    "text": "our studio depending on you know what your data scientists need well your developers like we do have the spark",
    "start": "163800",
    "end": "170340"
  },
  {
    "text": "thrift server as well available it's on the master node if you want to connect by ODBC or JDBC drivers we don't start",
    "start": "170340",
    "end": "177090"
  },
  {
    "text": "it by default but you can run that command and quickly start it up and then connect to spark sequel using those",
    "start": "177090",
    "end": "182370"
  },
  {
    "text": "drivers we have you can choose to install easy on the cluster whose he has spark actions so say you had a",
    "start": "182370",
    "end": "188070"
  },
  {
    "text": "complicated processing pipeline and you wanted to create a dag of spark jobs and have one job wait well another part of",
    "start": "188070",
    "end": "193770"
  },
  {
    "text": "the dag completes and then join them later with easy you can specify these very complex processing pipelines also",
    "start": "193770",
    "end": "200550"
  },
  {
    "text": "you can use the UI and Hugh on the latest EMR to actually draw out that",
    "start": "200550",
    "end": "205560"
  },
  {
    "text": "pipeline and have a nice visual representation of what you're building for musi or you can just use the spark spark shell sparks emit we also have a",
    "start": "205560",
    "end": "212660"
  },
  {
    "text": "jaw a blog post on the AWS big data blog on the talks about how to install a",
    "start": "212660",
    "end": "217770"
  },
  {
    "text": "spark job server as well so you do you can provision really whatever you need using bootstrap actions if it's not",
    "start": "217770",
    "end": "223290"
  },
  {
    "text": "there actually we could cut over real quick to my laptop to show just a couple",
    "start": "223290",
    "end": "231270"
  },
  {
    "text": "of these UIs because I mentioned Zeppelin here and the the data zoo demo will also use that one but you can",
    "start": "231270",
    "end": "237450"
  },
  {
    "text": "easily create notes you can save to get you can save to s3 by configuring the",
    "start": "237450",
    "end": "242520"
  },
  {
    "text": "notebook in the Zeppelin site to save the notebook file in s3 as you can see here you know lots of nice graphs and",
    "start": "242520",
    "end": "249600"
  },
  {
    "text": "very simple visualizations but if you want as I mentioned before this is",
    "start": "249600",
    "end": "254850"
  },
  {
    "text": "running on an EMR cluster I just created you can install our studio and use our studio or things like Jupiter as well",
    "start": "254850",
    "end": "262240"
  },
  {
    "text": "so if this that the tool isn't available by default you can easily install it on",
    "start": "262240",
    "end": "267430"
  },
  {
    "text": "as well however once you kind of have a",
    "start": "267430",
    "end": "276009"
  },
  {
    "text": "production workflow and let's say you want it to not only orchestrate running spark jobs but create the clusters take",
    "start": "276009",
    "end": "281289"
  },
  {
    "text": "the clusters down you have you know coordinating terminating and creating different resources",
    "start": "281289",
    "end": "286990"
  },
  {
    "text": "you probably want whatever scheduling those jobs to run outside the cluster versus say running woozi on a cluster so",
    "start": "286990",
    "end": "293470"
  },
  {
    "text": "there's a couple options here are some popular ones you can actually use the Amazon EMR step API when you create a",
    "start": "293470",
    "end": "299110"
  },
  {
    "text": "cluster you can submit a step which is unit of work it could be a hive job or a spark job and actually have EMR tooling",
    "start": "299110",
    "end": "306550"
  },
  {
    "text": "around it to say when this job completes terminate the clusters it's a very easy way to just fire off transient clusters",
    "start": "306550",
    "end": "312009"
  },
  {
    "text": "shut them down or you can have a long-running cluster and submit steps to it over the lifetime of the cluster you can also use a lambda to submit steps if",
    "start": "312009",
    "end": "319240"
  },
  {
    "text": "you say have a job that needs to kick off certain times a day and actually the interesting blog post that Avril post",
    "start": "319240",
    "end": "325509"
  },
  {
    "text": "about using lambda to fire off spark clusters you can use that also like bucket preconditions that sort of thing",
    "start": "325509",
    "end": "331870"
  },
  {
    "text": "land is very useful and a lightweight way to kick off these jobs we're gonna use something like data pipeline which",
    "start": "331870",
    "end": "337330"
  },
  {
    "text": "is an AWS service that can orchestrate these workflows or air flow Luigi or other schedulers that you can run on ec2",
    "start": "337330",
    "end": "344849"
  },
  {
    "text": "you know one thing that we also see from just how people are leveraging spark native us is really leveraging all the",
    "start": "344849",
    "end": "350650"
  },
  {
    "text": "different storage layers where you might have certain types of customer data stored in dynamo DB with raw maybe",
    "start": "350650",
    "end": "356469"
  },
  {
    "text": "clickstream in s3 and being able to use spark to join all these disparate data sets in your analytics Emaar actually",
    "start": "356469",
    "end": "364030"
  },
  {
    "text": "just open source to our high of dynamodb connector it's available you can see it on AWS labs and it also works with spark",
    "start": "364030",
    "end": "372610"
  },
  {
    "text": "as well so you can write spark jobs against DynamoDB you can access data and",
    "start": "372610",
    "end": "377800"
  },
  {
    "text": "RDS or elastic surgeries in the spark elasticsearch connector you can access data in Amazon redshift using the spark",
    "start": "377800",
    "end": "385030"
  },
  {
    "text": "redshift connector spark streaming with Apache kafka Amazon qisas and finally accessing data and s3",
    "start": "385030",
    "end": "390849"
  },
  {
    "text": "using EMR FS which is our our own s3 connector that allows you to perform the access data in s3 with spark you know",
    "start": "390849",
    "end": "399490"
  },
  {
    "text": "that's one key theme of most SPARC architecture as we see on AWS is decoupling your storage and your compute",
    "start": "399490",
    "end": "405580"
  },
  {
    "text": "s3 is designed for eleven nines of durability extremely scalable low-cost and is a great place to store huge data",
    "start": "405580",
    "end": "411999"
  },
  {
    "text": "sets in a very very durable and available way also it's available across a Z's it's the regional endpoint so if",
    "start": "411999",
    "end": "418599"
  },
  {
    "text": "you want to say fire per cluster another AZ you don't need to copy any data from AZ to AZ as Three's except accessible",
    "start": "418599",
    "end": "425199"
  },
  {
    "text": "and all of them one quick tip though is that for folks running something like spark sequel you can store your hive",
    "start": "425199",
    "end": "431469"
  },
  {
    "text": "tables in something like Amazon Aurora or RDS so if you want to shut your cluster down because your data is",
    "start": "431469",
    "end": "436719"
  },
  {
    "text": "durable in s3 you don't need a cluster up all the time you don't need to recreate all of your tables and you",
    "start": "436719",
    "end": "441729"
  },
  {
    "text": "recreate your cluster and you can configure that using values in hi site",
    "start": "441729",
    "end": "446909"
  },
  {
    "text": "just a couple tips for us 3 you know you want to try and avoid key names in LexA",
    "start": "446909",
    "end": "452349"
  },
  {
    "text": "graphical order if you're doing very very large scans it'll increase your throughput and list performance also",
    "start": "452349",
    "end": "458050"
  },
  {
    "text": "utilizing compression and splittable compression if it's appropriate depending on your file size will really",
    "start": "458050",
    "end": "464169"
  },
  {
    "text": "reduce the bandwidth from s3 to the cluster nodes and ec2 and using columnar",
    "start": "464169",
    "end": "469300"
  },
  {
    "text": "formats like parquet and as many of you know spark has really great integration with parquet to easily create data",
    "start": "469300",
    "end": "474339"
  },
  {
    "text": "frames from parquet files so that's you know besides just usability performance",
    "start": "474339",
    "end": "479559"
  },
  {
    "text": "also makes a big difference if you're just filtering down and selecting columns just a quick overview of spark",
    "start": "479559",
    "end": "486459"
  },
  {
    "text": "security really just focusing on encryption here EMR if you have say HIPAA workload or requirements around",
    "start": "486459",
    "end": "492819"
  },
  {
    "text": "encryption it's pretty easy to encrypt all the different data layers involved in your cluster and that doesn't just",
    "start": "492819",
    "end": "498459"
  },
  {
    "text": "include the disks on your cluster it also includes s3 and internode transit so with EMR security configurations is",
    "start": "498459",
    "end": "505749"
  },
  {
    "text": "actually just a representation in the console but it's you know you can create it from the API you can create these security configurations specify either a",
    "start": "505749",
    "end": "512560"
  },
  {
    "text": "TBS KMS keys or encryption material providers to get keys out of say an HSM or some other custom provide a custom",
    "start": "512560",
    "end": "518888"
  },
  {
    "text": "key vendor that you have you write you know you save it you tell a cluster to use it and",
    "start": "518889",
    "end": "524270"
  },
  {
    "text": "Ammar will encrypt all the local disks EMR FS will use either client-side or server-side s3 encryption depending on",
    "start": "524270",
    "end": "529940"
  },
  {
    "text": "what you've specified and an encrypt spark in transit encryption as well one",
    "start": "529940",
    "end": "537680"
  },
  {
    "text": "feature that we recently released two weeks ago is auto scaling so when you're considering performance for your spark",
    "start": "537680",
    "end": "543020"
  },
  {
    "text": "jobs and because dynamic allocation is enabled by default you can set an auto",
    "start": "543020",
    "end": "548690"
  },
  {
    "text": "scaling policy with EMR where your mark provides a bunch of yarn metrics and one",
    "start": "548690",
    "end": "554120"
  },
  {
    "text": "useful one to say yarn memory percentage used you could say with the EMR when you create your cluster you can add a policy",
    "start": "554120",
    "end": "559790"
  },
  {
    "text": "that says when yarn is using 80% of ram add on to more nodes and in the",
    "start": "559790",
    "end": "565580"
  },
  {
    "text": "background EMR will configure cloud watch alarms use application auto scaling and also provide events in our",
    "start": "565580",
    "end": "571040"
  },
  {
    "text": "console to show when your cluster scaling up and down to scale out your cluster when it needs more nodes when a",
    "start": "571040",
    "end": "577250"
  },
  {
    "text": "new node manager comes up your spark application of dynamic allocations enabled if you if you kept the default",
    "start": "577250",
    "end": "582560"
  },
  {
    "text": "well then if you know add more executors as needed to the new capacity that's",
    "start": "582560",
    "end": "588230"
  },
  {
    "text": "come up one interesting thing also is that for scale down we've implemented two features one will scale down at the",
    "start": "588230",
    "end": "595610"
  },
  {
    "text": "hourly boundary by default so even if you say your yarn memory threshold goes",
    "start": "595610",
    "end": "600740"
  },
  {
    "text": "below with what your alarm was set for we won't actually take any nodes away because you paid for them for now what",
    "start": "600740",
    "end": "606650"
  },
  {
    "text": "if another job comes before the hours up so we'll actually wait and keep track and only take nodes out and terminate",
    "start": "606650",
    "end": "613100"
  },
  {
    "text": "them you know when it's nearing that hourly boundary but another thing is you can configure it to wait for task",
    "start": "613100",
    "end": "619520"
  },
  {
    "text": "completion so say we won't terminate any node until all running containers on that node are finished running and what",
    "start": "619520",
    "end": "626360"
  },
  {
    "text": "we'll do in that case is we'll blacklist that node so no more containers can be placed on it and drain the existing work",
    "start": "626360",
    "end": "631790"
  },
  {
    "text": "so more of a cost saving auto scaling and then a kind of a work preserving auto scaling mode and finally to close",
    "start": "631790",
    "end": "639160"
  },
  {
    "text": "we have an upcoming feature for advanced spa provisioning many customers leverage spot to save up to 80% on their computer",
    "start": "639160",
    "end": "646250"
  },
  {
    "text": "compute costs so it's a very effective way to lower your costs for your cluster",
    "start": "646250",
    "end": "651310"
  },
  {
    "text": "this new feature will allow you today when you more cluster you specify an innocent group with an instance type and a bid",
    "start": "651310",
    "end": "657130"
  },
  {
    "text": "price with this new functionality you'll be able to specify a list of instances",
    "start": "657130",
    "end": "662170"
  },
  {
    "text": "with different prices in different configurations and also different availability zones that you can launch your cluster in and then one CMR creates",
    "start": "662170",
    "end": "669490"
  },
  {
    "text": "a cluster we look at all the different AZ's figure out what makes what AZ is the most capacity available and then",
    "start": "669490",
    "end": "674680"
  },
  {
    "text": "from the instance types what's the best value based on your inputs and we'll create a cluster but maybe even multiple",
    "start": "674680",
    "end": "680920"
  },
  {
    "text": "instance types within that group to use I'm also the support spot block so if",
    "start": "680920",
    "end": "687910"
  },
  {
    "text": "you have a job that say you know we'll finish in a four hour duration and you don't want any spot interruptions you",
    "start": "687910",
    "end": "693279"
  },
  {
    "text": "can specify a spot block and will utilize spa block capacity for those instance types now hand the mic over to",
    "start": "693279",
    "end": "701199"
  },
  {
    "text": "your kesa from data's ooh and he'll run over how they use spark for ETL and data science thank you John hey you guys how are you",
    "start": "701199",
    "end": "707800"
  },
  {
    "text": "doing good yeah I'm having a great conference so far are you guys having a",
    "start": "707800",
    "end": "713110"
  },
  {
    "text": "great conference I can't wait for Amazon to announce the new exciting thing and at the keynote so",
    "start": "713110",
    "end": "718750"
  },
  {
    "text": "waiting eagerly alright so thanks for being here and thanks to Amazon for",
    "start": "718750",
    "end": "725920"
  },
  {
    "text": "hosting this session we wanted to share some of our learning experiences as we embarked on this journey to a dot there",
    "start": "725920",
    "end": "732130"
  },
  {
    "text": "is a spark at data Zoo and my name is akia and i'm joined by my",
    "start": "732130",
    "end": "738220"
  },
  {
    "text": "colleagues dong and and socket we're going to talk through our journey of",
    "start": "738220",
    "end": "743439"
  },
  {
    "text": "spark a quick spill on Delos so what we expect from this session a quick trail I",
    "start": "743439",
    "end": "748480"
  },
  {
    "text": "promise to keep it short and why spark what motivated us to actually use park and and what was it like before right",
    "start": "748480",
    "end": "756300"
  },
  {
    "text": "and then the rest of the talk is split into two tracks one is more engineering",
    "start": "756300",
    "end": "762399"
  },
  {
    "text": "oriented data processing kind of task oriented workflows and then there are",
    "start": "762399",
    "end": "769360"
  },
  {
    "text": "science workflows so we'll look at both of them and embedded in each section is",
    "start": "769360",
    "end": "775209"
  },
  {
    "text": "actually a demo and their so the demo I know the standard jitters might apply so",
    "start": "775209",
    "end": "780519"
  },
  {
    "text": "let's kudos to the engineers and and let's wish them good luck we'll see how",
    "start": "780519",
    "end": "785740"
  },
  {
    "text": "this works of that said let's get started so who is that a zoo",
    "start": "785740",
    "end": "791260"
  },
  {
    "text": "well data zoo is a petabyte scale marketing platform we were spun out of MIT labs we operated internet scale the",
    "start": "791260",
    "end": "799300"
  },
  {
    "text": "numbers speak for themselves and we know what in terms of what we do is we help",
    "start": "799300",
    "end": "806500"
  },
  {
    "text": "the brand's engage with the consumers along their journey our brands is our",
    "start": "806500",
    "end": "813040"
  },
  {
    "text": "customers basically the advertisers we had in the ad tech space as you can tell we try to more tactically speaking we",
    "start": "813040",
    "end": "819010"
  },
  {
    "text": "try to maximize the return on investment for our advertisers so how we might do that I'm going to cover that in the next",
    "start": "819010",
    "end": "825460"
  },
  {
    "text": "couple of slides quick run through the numbers we are we have a real-time",
    "start": "825460",
    "end": "831010"
  },
  {
    "text": "engine capability which process is about two million transactions a second so this talk is about an hour long talk so",
    "start": "831010",
    "end": "838930"
  },
  {
    "text": "during this talk our system would process about 7.2 billion transactions as I'm standing here our systems collect",
    "start": "838930",
    "end": "847029"
  },
  {
    "text": "about 180 terabytes of logs every day and we analyze about two petabytes of",
    "start": "847029",
    "end": "854800"
  },
  {
    "text": "data on a daily basis and we operate out",
    "start": "854800",
    "end": "859930"
  },
  {
    "text": "of 13 different regions and data centers it's a 24 by 7 operations they make a",
    "start": "859930",
    "end": "865030"
  },
  {
    "text": "big metal point from this slide really is we collect and store and analyze",
    "start": "865030",
    "end": "870839"
  },
  {
    "text": "large volumes of data like most of you and what we need to do our job well is a",
    "start": "870839",
    "end": "878680"
  },
  {
    "text": "reliable secure an elastic platform that can handle iterative and interactive",
    "start": "878680",
    "end": "885190"
  },
  {
    "text": "workloads right and of course we need a lot heavy dose of automation without",
    "start": "885190",
    "end": "891070"
  },
  {
    "text": "automation you can run this scale of you know jobs right so real-time bidding so",
    "start": "891070",
    "end": "897190"
  },
  {
    "text": "I said we help our brands engage with their customers so how do we do that well we have do that with our real-time",
    "start": "897190",
    "end": "903010"
  },
  {
    "text": "capabilities so I'm going to I'm not going to talk through each block in this picture but here is the we're going to",
    "start": "903010",
    "end": "909459"
  },
  {
    "text": "give you a more concrete example so it's more tangible let's say you are Martin you're looking for a car you might go on",
    "start": "909459",
    "end": "915400"
  },
  {
    "text": "the some websites you might put in some parameters you put in the car manufacturer star",
    "start": "915400",
    "end": "921670"
  },
  {
    "text": "price you might do some side-by-side comparisons you might look for deals in the internet in your neighborhood as",
    "start": "921670",
    "end": "928779"
  },
  {
    "text": "you're doing that you're dropping hints to the medium of and the websites that",
    "start": "928779",
    "end": "934839"
  },
  {
    "text": "you are perhaps searching on you know might have some ad slots on the side that get auctioned off in real time and",
    "start": "934839",
    "end": "941940"
  },
  {
    "text": "real time engines like ours we bid for those opportunities we are not the only",
    "start": "941940",
    "end": "947230"
  },
  {
    "text": "one there's we have competition and and the real-time auctioning of ad slots at",
    "start": "947230",
    "end": "953230"
  },
  {
    "text": "an impression level is called real-time bidding it's basically that that's it I am Not sure the one thing that I want",
    "start": "953230",
    "end": "960010"
  },
  {
    "text": "you to note from this picture is that as we are bidding out there in the internet",
    "start": "960010",
    "end": "965070"
  },
  {
    "text": "we are sending love if you are generating lots of events and these events are hey I bid for this user at",
    "start": "965070",
    "end": "973570"
  },
  {
    "text": "this price and I want the option or I bid for this user at this price I didn't",
    "start": "973570",
    "end": "979150"
  },
  {
    "text": "win the option we need to collect this data so we know to adjust these bidding strategies in",
    "start": "979150",
    "end": "986950"
  },
  {
    "text": "real time right that's where data science comes in so there is a lot of",
    "start": "986950",
    "end": "992500"
  },
  {
    "text": "logging going on and and the real-time systems fees are the lots of data that's 180 terabytes come from and we have our",
    "start": "992500",
    "end": "1000450"
  },
  {
    "text": "data pipeline runs through these logs on a daily basis and looking for the you",
    "start": "1000450",
    "end": "1005940"
  },
  {
    "text": "know tidbits of information and extracting nuggets of wisdom so we can feed it back into real time engine so it",
    "start": "1005940",
    "end": "1011520"
  },
  {
    "text": "can you know change its bidding strategies on the fly all right so why",
    "start": "1011520",
    "end": "1018990"
  },
  {
    "text": "spark one spark was the before spark I'm",
    "start": "1018990",
    "end": "1025949"
  },
  {
    "text": "going to show you a slide how it looked like we were a heavy MapReduce job heavy",
    "start": "1025949",
    "end": "1032010"
  },
  {
    "text": "MapReduce platform as well as we had you know we still have a massively parallel",
    "start": "1032010",
    "end": "1039300"
  },
  {
    "text": "databases MPP databases right but to do our ETL so we have EDL's that are going",
    "start": "1039300",
    "end": "1046589"
  },
  {
    "text": "on in MapReduce and we have ETL s going on in our MVP databases but we at the",
    "start": "1046589",
    "end": "1053850"
  },
  {
    "text": "scale of our data it wasn't number one cost-effective and number two wasn't meeting the sls so we",
    "start": "1053850",
    "end": "1060330"
  },
  {
    "text": "had we were under a time crunch to process this volume of data so and and",
    "start": "1060330",
    "end": "1065940"
  },
  {
    "text": "and most of you are familiar with the big data space you know all the technology offerings so in a MapReduce",
    "start": "1065940",
    "end": "1071580"
  },
  {
    "text": "hive and all the other things the big appeal of SPARC is really its performance its speed right its gets its",
    "start": "1071580",
    "end": "1079710"
  },
  {
    "text": "performance from a couple of things couple of ways right one is in-memory data structures and computations and",
    "start": "1079710",
    "end": "1085309"
  },
  {
    "text": "number two like John mentioned the dag they directed a cyclical graph if you",
    "start": "1085309",
    "end": "1091410"
  },
  {
    "text": "look at a traditional database execution plan it looks like a three execution plan instead of consider instructions in",
    "start": "1091410",
    "end": "1098040"
  },
  {
    "text": "a fund-raiser tree that's that's a directed acyclic graph right and then it",
    "start": "1098040",
    "end": "1105059"
  },
  {
    "text": "knows to avoid the shuffle when the dead has already been partitioned the weight is needed so overall SPARC is very fast I'm not",
    "start": "1105059",
    "end": "1112799"
  },
  {
    "text": "going to give you a lot of this this topic has been talked about several times the contrast that with the",
    "start": "1112799",
    "end": "1118440"
  },
  {
    "text": "MapReduce where there's a lot of disk i/o going on within the mapper between the mapper and reducer and when it kicks",
    "start": "1118440",
    "end": "1124890"
  },
  {
    "text": "off a shuffle phase there's a lot of data transfer over the internet or over the N network so a lot of this guy",
    "start": "1124890",
    "end": "1131460"
  },
  {
    "text": "you're going on a lot of you know bytes moving around not participating in the compute function so that's where we",
    "start": "1131460",
    "end": "1139200"
  },
  {
    "text": "started to scratch our head and figure out how do we do this and then the other",
    "start": "1139200",
    "end": "1145200"
  },
  {
    "text": "thing is you know it speaks your language your Java Scala perhaps for your data engineering tasks Python and",
    "start": "1145200",
    "end": "1152250"
  },
  {
    "text": "are more for the analysis if that's what you choose but the best part is the following they all interoperate well",
    "start": "1152250",
    "end": "1159600"
  },
  {
    "text": "together unlike its predecessors right so you can you can construct your data",
    "start": "1159600",
    "end": "1165330"
  },
  {
    "text": "sets or data frames in one language leave it in memory let the other one pick up from there it's very well-integrated streaming use",
    "start": "1165330",
    "end": "1172950"
  },
  {
    "text": "cases of course you know it comes built in with the streaming support which is great SPARC is certainly appealing no",
    "start": "1172950",
    "end": "1179640"
  },
  {
    "text": "questions right but spark on EMR makes it compelling here is why Yama's on EMR is a very",
    "start": "1179640",
    "end": "1188220"
  },
  {
    "text": "mature elastic and secure platform you can spin",
    "start": "1188220",
    "end": "1193649"
  },
  {
    "text": "it up and down as much as you knee and John talked about sparred instances and",
    "start": "1193649",
    "end": "1200279"
  },
  {
    "text": "auto scaling features and then you know it's a decoupled storage and compute you",
    "start": "1200279",
    "end": "1205559"
  },
  {
    "text": "only pull in the data that you need for your job right unlike traditional databases where you have to load the whole thing into into its database",
    "start": "1205559",
    "end": "1212639"
  },
  {
    "text": "structures in EMR you just bring in the data you need for your analysis so for",
    "start": "1212639",
    "end": "1219029"
  },
  {
    "text": "these reasons and then now the throw in these part instances it makes it much more appealing in terms of the TCO the",
    "start": "1219029",
    "end": "1225299"
  },
  {
    "text": "total cost of ownership right so for these reasons spark on your mark you can't ignore any more so this is how",
    "start": "1225299",
    "end": "1234239"
  },
  {
    "text": "our architect should look like before before you know we embarked on on spark",
    "start": "1234239",
    "end": "1240079"
  },
  {
    "text": "so I talked about real-time bidding generating lots of events and we bring all those events through kinases",
    "start": "1240079",
    "end": "1246449"
  },
  {
    "text": "endpoint that hasn't changed that still is the same and we store all the data in",
    "start": "1246449",
    "end": "1251969"
  },
  {
    "text": "s3 that's our archive and then we have these three big MapReduce jobs each one",
    "start": "1251969",
    "end": "1257249"
  },
  {
    "text": "runs sequentially one after the other reading and writing out of s3 right now",
    "start": "1257249",
    "end": "1263579"
  },
  {
    "text": "you can tell there's a lot of disk i/o going on right so this is where it",
    "start": "1263579",
    "end": "1269549"
  },
  {
    "text": "wasn't as efficient as we wanted it to be and then of course we have the data",
    "start": "1269549",
    "end": "1275729"
  },
  {
    "text": "analytics see that that little goofy that's bump bouncing up and down that's our analytics engine that's data",
    "start": "1275729",
    "end": "1282149"
  },
  {
    "text": "science producing signals back into the real time engine and socket is going to cover a little bit of that coming up one",
    "start": "1282149",
    "end": "1289109"
  },
  {
    "text": "thing I want to mention before I leave this slide is the ecosystem of Amazon services s3 EMR they're all great but",
    "start": "1289109",
    "end": "1298019"
  },
  {
    "text": "the other things that actually glue everything together like the I AMS the VP sees the direct connect the dynamo DB",
    "start": "1298019",
    "end": "1305749"
  },
  {
    "text": "they are right on to run any production workloads as well I just want to call that out all right so we're and that's",
    "start": "1305749",
    "end": "1315629"
  },
  {
    "text": "the picture we had before right that's transitioning out so we thought long and hard how do we do this",
    "start": "1315629",
    "end": "1320750"
  },
  {
    "text": "so data science team actually started exploring new ways to write their",
    "start": "1320750",
    "end": "1326120"
  },
  {
    "text": "algorithms and then they ran into a spark and then they followed by engineering team I have the pleasure of",
    "start": "1326120",
    "end": "1333590"
  },
  {
    "text": "working with both teams so you know we bounce ideas off of each other and the data science team started working with",
    "start": "1333590",
    "end": "1339890"
  },
  {
    "text": "spark we said hey you know maybe we should give it a try traditional ETL you think of a database right and from",
    "start": "1339890",
    "end": "1346730"
  },
  {
    "text": "Attica and whatever tools work for you classic and database workloads but then",
    "start": "1346730",
    "end": "1352820"
  },
  {
    "text": "we said you know why don't we try spark so we embarked on this journey nothing changed on the left-hand side it still",
    "start": "1352820",
    "end": "1358460"
  },
  {
    "text": "kinases nothing changed on the right-hand side it is still as three but how we process the data changed we",
    "start": "1358460",
    "end": "1366260"
  },
  {
    "text": "restructured our work flows into spark jobs one iterates on the other right",
    "start": "1366260",
    "end": "1374179"
  },
  {
    "text": "it's an iterative workflow all the data is stored in memory we had read over and over again until we get the final result",
    "start": "1374179",
    "end": "1380929"
  },
  {
    "text": "dump it in s3 and and it starts all over again right but then at the time that we",
    "start": "1380929",
    "end": "1388190"
  },
  {
    "text": "are building this spark by plane we also thought about hey we have multiple teams here right we have engineering teams",
    "start": "1388190",
    "end": "1394460"
  },
  {
    "text": "multiple of them we have you know data science team how do we make this sport",
    "start": "1394460",
    "end": "1399470"
  },
  {
    "text": "more reusable across the board so what we did was we created this another we refactored the codebase such",
    "start": "1399470",
    "end": "1406460"
  },
  {
    "text": "that the foundation or the the common back-end is factored out of that",
    "start": "1406460",
    "end": "1411710"
  },
  {
    "text": "application logic so we have one system that note that bit it handles all the",
    "start": "1411710",
    "end": "1419090"
  },
  {
    "text": "infrastructure level things things like logging monitoring alerting",
    "start": "1419090",
    "end": "1424159"
  },
  {
    "text": "you know auto scaling and things like that you know integration with lambda John mentioned this one so we use that",
    "start": "1424159",
    "end": "1429470"
  },
  {
    "text": "as well you can fire up a cluster automatically using lambda so we kind of refactored all of those things and",
    "start": "1429470",
    "end": "1435860"
  },
  {
    "text": "pushed it one layer down for maximum code reuse so that's another team doing",
    "start": "1435860",
    "end": "1441260"
  },
  {
    "text": "it and then we support all those libraries on top of that engine so the",
    "start": "1441260",
    "end": "1446299"
  },
  {
    "text": "the foundation doesn't care which library use you can use Park sequel you can use spark and my lib you can use",
    "start": "1446299",
    "end": "1452570"
  },
  {
    "text": "anything you want and by the way that's why we expect the innovation to continue that's why I have dot dot dot",
    "start": "1452570",
    "end": "1457670"
  },
  {
    "text": "right well the year Mar and the comment back-end stays the same we expect a lot",
    "start": "1457670",
    "end": "1464000"
  },
  {
    "text": "of innovation to come in that year so that's basically the picture now let's look at how that guts that simplified",
    "start": "1464000",
    "end": "1470330"
  },
  {
    "text": "our lives right now you can compare and contrast what happened to that picture",
    "start": "1470330",
    "end": "1476780"
  },
  {
    "text": "with big three MapReduce jobs running lot of processing lot of disk right",
    "start": "1476780",
    "end": "1483020"
  },
  {
    "text": "that's simplified very efficient in terms of compute in terms of in terms of",
    "start": "1483020",
    "end": "1489740"
  },
  {
    "text": "processing times a much simplified architecture and we have even refracted the code so there's just one code base",
    "start": "1489740",
    "end": "1495620"
  },
  {
    "text": "running all different applications with that said now let me invite upward dog",
    "start": "1495620",
    "end": "1500690"
  },
  {
    "text": "and he's going to walk us to the ETL part of the workload thank you sir so",
    "start": "1500690",
    "end": "1513650"
  },
  {
    "text": "I'm going to talk to you about the ETL pipeline desu you're looking at any typical pipeline this is also one of our",
    "start": "1513650",
    "end": "1520460"
  },
  {
    "text": "most critical pipeline on the top left side you are looking at event data that",
    "start": "1520460",
    "end": "1525860"
  },
  {
    "text": "is billions of records per day there are our impressions attributions events on",
    "start": "1525860",
    "end": "1531050"
  },
  {
    "text": "the bottom left you are looking at the reference data those are campaign configurations",
    "start": "1531050",
    "end": "1536060"
  },
  {
    "text": "audience retargeting such data can be also large some of those configurations",
    "start": "1536060",
    "end": "1541790"
  },
  {
    "text": "you can easily be half a billion records so that now if you're either on the right side that's the product of ETL",
    "start": "1541790",
    "end": "1549500"
  },
  {
    "text": "process so you're looking at application logs exceptional datasets but most importantly our reporting datasets in",
    "start": "1549500",
    "end": "1556490"
  },
  {
    "text": "the middle with is our eto engine and I'm going to describe to you our ever",
    "start": "1556490",
    "end": "1561950"
  },
  {
    "text": "journey in finding a better uto engine about four or five years ago we adopted an EP database we run MPP database um in",
    "start": "1561950",
    "end": "1570740"
  },
  {
    "text": "our Colo it served as well how are we quickly run into some issues we have hardware",
    "start": "1570740",
    "end": "1576950"
  },
  {
    "text": "stability issues at any point of time we cannot be sure a single cluster can support all of our production load we",
    "start": "1576950",
    "end": "1584210"
  },
  {
    "text": "ended up provisioning multiple clusters that compounded our production",
    "start": "1584210",
    "end": "1589440"
  },
  {
    "text": "we have to DBAs constantly watching other two clusters a lot of heavy maintenance we have unpredictable work",
    "start": "1589440",
    "end": "1598380"
  },
  {
    "text": "time we don't know we how long each pipelines are going to finish and it's",
    "start": "1598380",
    "end": "1603720"
  },
  {
    "text": "not elastic it take us months to plan and execute a cluster expansion so that",
    "start": "1603720",
    "end": "1609000"
  },
  {
    "text": "is not working well the next step we looked at it running the MPP at the club",
    "start": "1609000",
    "end": "1615900"
  },
  {
    "text": "environment we couldn't get the emotinal working at a time in club in AWS some",
    "start": "1615900",
    "end": "1623340"
  },
  {
    "text": "networking issues so we end up with a single node MPP running at the base and",
    "start": "1623340",
    "end": "1629040"
  },
  {
    "text": "with the largest iTune instance we can get at a time it solved a lot of",
    "start": "1629040",
    "end": "1634260"
  },
  {
    "text": "problems it's stable we have predictable execution time but we still have two",
    "start": "1634260",
    "end": "1639780"
  },
  {
    "text": "problems we found out that it still affects the capacity so if we project",
    "start": "1639780",
    "end": "1645570"
  },
  {
    "text": "business is going to grow three times it's going to exhaust other capacities that IT instance node care support and",
    "start": "1645570",
    "end": "1652280"
  },
  {
    "text": "we also found our application stack is written for the hosted solution and does not take advantage of all the other",
    "start": "1652280",
    "end": "1658830"
  },
  {
    "text": "cloud can offer as the case I mentioned that our data science teams that you're looking to spark and we have also a lot",
    "start": "1658830",
    "end": "1666090"
  },
  {
    "text": "of in-house expertise in EMR so we decided to look at this part of running Anya mark we changed our pattern",
    "start": "1666090",
    "end": "1674340"
  },
  {
    "text": "when we adopt a spark Anjum our previously we always have a persistent cluster MPP database cluster running",
    "start": "1674340",
    "end": "1682110"
  },
  {
    "text": "either in hosted solution Iranian cloud on the spark 4-year Mar we actually",
    "start": "1682110",
    "end": "1688230"
  },
  {
    "text": "launching transient clusters we only start clusters when we accumulate a sufficient batch data and then we run",
    "start": "1688230",
    "end": "1695670"
  },
  {
    "text": "through the cluster and the way it's finished we terminate a cluster that is significant for two reasons that we no",
    "start": "1695670",
    "end": "1702570"
  },
  {
    "text": "longer have stability maintenance issues any problems coming up we just dispose",
    "start": "1702570",
    "end": "1707610"
  },
  {
    "text": "the cluster and launch a new one and then has significant cost signal cost savings for us because we're not running",
    "start": "1707610",
    "end": "1714030"
  },
  {
    "text": "a permanent trust abroad time so in this",
    "start": "1714030",
    "end": "1719460"
  },
  {
    "text": "slide are going to show you some of the cost performance benchmarks you're looking at the three scenarios",
    "start": "1719460",
    "end": "1725420"
  },
  {
    "text": "the MP Pyrenean Colo you can see it's a large installation for the eight notes",
    "start": "1725420",
    "end": "1731610"
  },
  {
    "text": "and we have multiple of them then we have the single node MTP when you",
    "start": "1731610",
    "end": "1736680"
  },
  {
    "text": "depress and then we have spark rainier year mark when your different cluster",
    "start": "1736680",
    "end": "1742020"
  },
  {
    "text": "size and the instance types I'll let you chew on these numbers but I'll just quickly pointing out some key findings",
    "start": "1742020",
    "end": "1748980"
  },
  {
    "text": "here if you look at the execution time then the execution time on MTP database",
    "start": "1748980",
    "end": "1755160"
  },
  {
    "text": "is hugely variable we just cannot know exactly when to finish but if you look",
    "start": "1755160",
    "end": "1761010"
  },
  {
    "text": "at the spark it is very very consistent it's almost on top it's always within a",
    "start": "1761010",
    "end": "1766440"
  },
  {
    "text": "minute it's a very very stable environment and then if you look at a monthly cost on top of the 75% a drop we",
    "start": "1766440",
    "end": "1774780"
  },
  {
    "text": "can get from moving MTP from hosted solution into a the best when we moved",
    "start": "1774780",
    "end": "1780180"
  },
  {
    "text": "to spark we're actually getting another 80 to 90 percent savings such significant savings is coming from the",
    "start": "1780180",
    "end": "1786420"
  },
  {
    "text": "fact that imagine previously that we are using the transient cluster but also we are using a much smaller cluster",
    "start": "1786420",
    "end": "1792840"
  },
  {
    "text": "compared to MTP and all these numbers calculated for spot are actually",
    "start": "1792840",
    "end": "1799110"
  },
  {
    "text": "argument instances so if we when we switch to spot we can spot instances we",
    "start": "1799110",
    "end": "1804330"
  },
  {
    "text": "can get additional savings I mean this slide I'm showing some scalability tests",
    "start": "1804330",
    "end": "1810110"
  },
  {
    "text": "on the left side we are looking at execution time and the cost for a",
    "start": "1810110",
    "end": "1815640"
  },
  {
    "text": "different sized cluster this is for the same data set but we are creating",
    "start": "1815640",
    "end": "1821130"
  },
  {
    "text": "different sizes of cluster from two core notes up to six core nodes we are looking at a range of possibilities we",
    "start": "1821130",
    "end": "1829020"
  },
  {
    "text": "get a larger drop in the execution time on the lower end and the diminishing returns on a higher end so this give us",
    "start": "1829020",
    "end": "1836460"
  },
  {
    "text": "the possibility that you look at a different types of the pipeline and a pick us we spot if you have a very tight",
    "start": "1836460",
    "end": "1844140"
  },
  {
    "text": "SOA requirement you actually want to run a larger cluster to ensure you meet that SOA Wow if you have the pipeline that",
    "start": "1844140",
    "end": "1850500"
  },
  {
    "text": "don't have such a type of an SLA requirement you may want to use a cost-effective cluster on the right side",
    "start": "1850500",
    "end": "1857580"
  },
  {
    "text": "well we are looking at is a study of what if our data volume becomes 10 times",
    "start": "1857580",
    "end": "1864120"
  },
  {
    "text": "so we stage the data set in increments around 1 times 2 times all the way to",
    "start": "1864120",
    "end": "1870480"
  },
  {
    "text": "time to 10 times and with increment each increment of the data set we are a six",
    "start": "1870480",
    "end": "1876120"
  },
  {
    "text": "core nodes to the cluster so at an ideal case should on the blue line is that you",
    "start": "1876120",
    "end": "1882960"
  },
  {
    "text": "would see a flat line of the execution time who actually found out some what",
    "start": "1882960",
    "end": "1888180"
  },
  {
    "text": "non linear scalability the P point here is actually it is relatively very very",
    "start": "1888180",
    "end": "1894540"
  },
  {
    "text": "easy to set this up llamar we can run these experiments in a couple of hours",
    "start": "1894540",
    "end": "1900360"
  },
  {
    "text": "and we can say confidently to our business that what if our business grow",
    "start": "1900360",
    "end": "1905640"
  },
  {
    "text": "10 times what are the possible problems we will face and what kind of engineering efforts we need to put in",
    "start": "1905640",
    "end": "1911940"
  },
  {
    "text": "there to reach that scalability so I'm going to share some tips so sparks equal",
    "start": "1911940",
    "end": "1919560"
  },
  {
    "text": "you want to use the broadcast hash joint if you're not familiar with it it is",
    "start": "1919560",
    "end": "1925500"
  },
  {
    "text": "basically you have these very small dimension tables you want to replicate",
    "start": "1925500",
    "end": "1930900"
  },
  {
    "text": "them across the executors and for the large fact tables you were to partition them and the spread evenly across the",
    "start": "1930900",
    "end": "1936960"
  },
  {
    "text": "executors this was not available and typically the base it is somewhat hard",
    "start": "1936960",
    "end": "1943380"
  },
  {
    "text": "to enable it our spark 1.6 and it's very easy in 2.0 so if you are still running",
    "start": "1943380",
    "end": "1948720"
  },
  {
    "text": "1.6 it may be time to consider upgrading we look at our UDF as compared to some",
    "start": "1948720",
    "end": "1956730"
  },
  {
    "text": "of the traditional databases were used when you switch from sequel engine to a procedure execution language engine you",
    "start": "1956730",
    "end": "1963930"
  },
  {
    "text": "get some performance penalties we find that spark has does not incur the penalty in fact in some cases you get a",
    "start": "1963930",
    "end": "1970140"
  },
  {
    "text": "better performance but we primarily use UDF for readability better modularity",
    "start": "1970140",
    "end": "1976170"
  },
  {
    "text": "and testability which I was showing the demo we are able to execute very complex",
    "start": "1976170",
    "end": "1982890"
  },
  {
    "text": "equals our spark we're actually taking a 250 lines of support for",
    "start": "1982890",
    "end": "1988260"
  },
  {
    "text": "directly from our MEP database and just a droplet into spark with the minor",
    "start": "1988260",
    "end": "1993510"
  },
  {
    "text": "changes to the function names it just runs so this is great while we also find out that the spark",
    "start": "1993510",
    "end": "2001310"
  },
  {
    "text": "has a limit of the most optimized code with spark 2.0 the hostage Cove Jane",
    "start": "2001310",
    "end": "2007880"
  },
  {
    "text": "we actually found there's a 64 K maximum limit where exceed a limiter you don't",
    "start": "2007880",
    "end": "2013190"
  },
  {
    "text": "get optimized code what we do is use some or child part and database techniques just materialize some section",
    "start": "2013190",
    "end": "2021110"
  },
  {
    "text": "of the code into into files we write out as park' and the red backing you know",
    "start": "2021110",
    "end": "2027410"
  },
  {
    "text": "the way you were to collapse the dag and might become so much simpler query so then you get all the optimized code",
    "start": "2027410",
    "end": "2033640"
  },
  {
    "text": "always watch out for your bottlenecks we found that you want to really scrutinize",
    "start": "2033640",
    "end": "2039950"
  },
  {
    "text": "your actions on the spark driver that is not paralyzed so any actions you do",
    "start": "2039950",
    "end": "2045560"
  },
  {
    "text": "there is could be a bottleneck sometimes the bottleneck is outside the spark we",
    "start": "2045560",
    "end": "2050810"
  },
  {
    "text": "have seen JDBC connections sometimes even iam RFS having not",
    "start": "2050810",
    "end": "2057950"
  },
  {
    "text": "sufficient capacity provision for the DynamoDB so big lessons learned",
    "start": "2057950",
    "end": "2066820"
  },
  {
    "text": "forklifting from hosted solution to cloud sometimes works we have mixed",
    "start": "2066820",
    "end": "2072860"
  },
  {
    "text": "results in our case we actually have to rewrite our entire application stack the",
    "start": "2072860",
    "end": "2079128"
  },
  {
    "text": "rewrite give us about 80% of code reductions will the Scala and we also",
    "start": "2079129",
    "end": "2086300"
  },
  {
    "text": "increase the testing coverage so you know okay so a I have to rewrite the estab you want to rethink about ETL",
    "start": "2086300",
    "end": "2093320"
  },
  {
    "text": "processing as a measuring the spark Sepoy is rock-solid and the 2.0 supports",
    "start": "2093320",
    "end": "2098990"
  },
  {
    "text": "most of the 2003 sequel standard with some of the advanced features we found",
    "start": "2098990",
    "end": "2104990"
  },
  {
    "text": "out that most of our ethiop at line we don't need MapReduce anymore MapReduce becomes mostly a niche case you want to",
    "start": "2104990",
    "end": "2113660"
  },
  {
    "text": "do the extreme streamline your processing you know MPP database what we did",
    "start": "2113660",
    "end": "2119250"
  },
  {
    "text": "we were to mark the staging tables with the audience and a copy those exceptions",
    "start": "2119250",
    "end": "2124980"
  },
  {
    "text": "into exception table and delete them for a staging table that is again a lot of",
    "start": "2124980",
    "end": "2130080"
  },
  {
    "text": "i/o back and forth so in the spark what we did is as you read data into the",
    "start": "2130080",
    "end": "2136349"
  },
  {
    "text": "memory we were to mark those records with select statement so that will collect all the exceptions into a single",
    "start": "2136349",
    "end": "2142890"
  },
  {
    "text": "column and a way write out data when that column is now there you would write out to the reporting data set and that",
    "start": "2142890",
    "end": "2150119"
  },
  {
    "text": "the column means none now you would arrive to the exception data set you want to rethink about your memory",
    "start": "2150119",
    "end": "2155580"
  },
  {
    "text": "processing SPARC is great in memory engine but you want to look after all",
    "start": "2155580",
    "end": "2160800"
  },
  {
    "text": "the incoming data footprints particularly the dimension table machine that our some of our dimension tables",
    "start": "2160800",
    "end": "2167310"
  },
  {
    "text": "are now attribute and we actually instead of what MPP database did where",
    "start": "2167310",
    "end": "2174060"
  },
  {
    "text": "we have the table staged for all pipelines with in the ethiopian air spot",
    "start": "2174060",
    "end": "2179880"
  },
  {
    "text": "that into your pipeline we actually filtered the dimension in both horizontally and vertically so that we",
    "start": "2179880",
    "end": "2186480"
  },
  {
    "text": "are only getting the data we need control your unnecessary joints use your",
    "start": "2186480",
    "end": "2191640"
  },
  {
    "text": "joints effectively use it to validate foreign keys as well as project columns for downstream processing and last but",
    "start": "2191640",
    "end": "2201150"
  },
  {
    "text": "not least EMR is our excellent comment backend we took EMR and I just plug in",
    "start": "2201150",
    "end": "2208560"
  },
  {
    "text": "the default configurations with the maximize resource allocation and we just produced sufficient performance for us",
    "start": "2208560",
    "end": "2215460"
  },
  {
    "text": "we didn't do any formats to near acquired the only thing we looked at is",
    "start": "2215460",
    "end": "2220800"
  },
  {
    "text": "making sure a broadcast has joins working to us as we also mentioned previously iam archivists very",
    "start": "2220800",
    "end": "2228140"
  },
  {
    "text": "consistent performance so with that please wish me luck I cannot start to",
    "start": "2228140",
    "end": "2234000"
  },
  {
    "text": "fear a demo",
    "start": "2234000",
    "end": "2237020"
  },
  {
    "text": "all right I think we're sorry as a first",
    "start": "2240440",
    "end": "2247160"
  },
  {
    "text": "hiccup I lost there the VPN connection",
    "start": "2247160",
    "end": "2253849"
  },
  {
    "text": "so what you are looking at is a sapling notebook if for those of you who are not familiar with Zeppelin it is a web-based",
    "start": "2263060",
    "end": "2269240"
  },
  {
    "text": "interface where you can connect directly to spark and then when your interactive data analysis we started a EMR cluster",
    "start": "2269240",
    "end": "2277910"
  },
  {
    "text": "with spark and the Zeppelin are installed so I'm going to kick this off as a talk so in this demo you are",
    "start": "2277910",
    "end": "2287360"
  },
  {
    "text": "looking at we first week and I'll load up the schema we're going to load up some of the fact tables and the",
    "start": "2287360",
    "end": "2294710"
  },
  {
    "text": "dimensions we have a big impression table that is about 12 million records",
    "start": "2294710",
    "end": "2299990"
  },
  {
    "text": "and then you could get a 17 different dimension tables in the next section we",
    "start": "2299990",
    "end": "2308240"
  },
  {
    "text": "are showcasing the spark joint you have a impression table that is joining to",
    "start": "2308240",
    "end": "2314090"
  },
  {
    "text": "all the 17 different dimensions meanwhile during these dimensions we",
    "start": "2314090",
    "end": "2319820"
  },
  {
    "text": "also want to make sure you are doing these exception checking's along the way",
    "start": "2319820",
    "end": "2325660"
  },
  {
    "text": "so that for all the foreign keys validations you are looking at you will",
    "start": "2325660",
    "end": "2331190"
  },
  {
    "text": "join to the dimensions and if you don't find your corresponding records in dimension table you'll just mark them as",
    "start": "2331190",
    "end": "2337550"
  },
  {
    "text": "foreign key violations and then you just collect them claps them into your single",
    "start": "2337550",
    "end": "2342710"
  },
  {
    "text": "column what we are seeing here is a",
    "start": "2342710",
    "end": "2347930"
  },
  {
    "text": "physical plan it's an execution plan for the 17 way joints you can see all the",
    "start": "2347930",
    "end": "2355070"
  },
  {
    "text": "broadcast hash joints just happens 17 times and then we found three exceptions",
    "start": "2355070",
    "end": "2362090"
  },
  {
    "text": "out of the air out of the 12 million records the next section I'm going to",
    "start": "2362090",
    "end": "2371750"
  },
  {
    "text": "show is the UDF before jumping into that I'm gonna show you whether we have before what we had before is a very",
    "start": "2371750",
    "end": "2379430"
  },
  {
    "text": "complex equal execution we have doing some very complex spend calculations",
    "start": "2379430",
    "end": "2385610"
  },
  {
    "text": "what you see here is a iterative query blocks each query blocks calculate",
    "start": "2385610",
    "end": "2392000"
  },
  {
    "text": "certain columns that then feed into your next query blocks as can see here the second curve blocks it",
    "start": "2392000",
    "end": "2399010"
  },
  {
    "text": "takes seeing some of the columns that's calculating the first credit blocks calculating some additional columns and",
    "start": "2399010",
    "end": "2405730"
  },
  {
    "text": "then those columns they're fed into the next block this big giant section",
    "start": "2405730",
    "end": "2413260"
  },
  {
    "text": "repeats several times this is not unit okay we cannot test unit test this",
    "start": "2413260",
    "end": "2418690"
  },
  {
    "text": "section and we end up building a very complex integration test foil will spark",
    "start": "2418690",
    "end": "2424540"
  },
  {
    "text": "we are able to rewrite it as a scholar UDF first of all we take advantage of",
    "start": "2424540",
    "end": "2429940"
  },
  {
    "text": "the versatile map data type we can use the key value data store so that you",
    "start": "2429940",
    "end": "2435910"
  },
  {
    "text": "have the spent component name as the key and the spell number as the value in",
    "start": "2435910",
    "end": "2442780"
  },
  {
    "text": "addition we can rewrite all these sequel sections spark and Scala and you can see",
    "start": "2442780",
    "end": "2448750"
  },
  {
    "text": "that it's actually a direct translation of a sequel into Scala code with this we",
    "start": "2448750",
    "end": "2455170"
  },
  {
    "text": "can run we would register the UDF and we would have run the query by invoking the",
    "start": "2455170",
    "end": "2463320"
  },
  {
    "text": "UDF section I'm not going to show you all the 12 million records but the top",
    "start": "2463320",
    "end": "2470260"
  },
  {
    "text": "20 records here you can see the map of a key and a value data set are generated",
    "start": "2470260",
    "end": "2477460"
  },
  {
    "text": "from this UDF so with that I'm gonna hand off to my colleague circuit to talk",
    "start": "2477460",
    "end": "2483040"
  },
  {
    "text": "about data size at the data Zoo all right okay so I am socket and I am",
    "start": "2483040",
    "end": "2493540"
  },
  {
    "text": "the senior principal data scientist at a zoo and what my team works on is basically training machine learning",
    "start": "2493540",
    "end": "2500200"
  },
  {
    "text": "algorithms that goes and buys online ads on the Internet so basically when we basically saw ads",
    "start": "2500200",
    "end": "2508600"
  },
  {
    "text": "for almost like thousands of advertisers and what our algorithm says are doing is trying to find when a user comes in",
    "start": "2508600",
    "end": "2514390"
  },
  {
    "text": "tries to find which advertisers ads fits this users are most and what's the probability that this user is going to",
    "start": "2514390",
    "end": "2520360"
  },
  {
    "text": "click on the ad or show a positive intent for the ad and to do this we use a lot of data as we have it seen so let",
    "start": "2520360",
    "end": "2527590"
  },
  {
    "text": "me just explain how the process ivenna will be so and add on the intellect whenever you see a banner ad",
    "start": "2527590",
    "end": "2533350"
  },
  {
    "text": "or a video or ad on youtube and so on we create a log line in our database and those log lines are called as",
    "start": "2533350",
    "end": "2539920"
  },
  {
    "text": "impressions now whenever a user reacts positively to any of our impressions",
    "start": "2539920",
    "end": "2545500"
  },
  {
    "text": "like maybe click on the ad or maybe go to the advertisers web page and buy something or check something out we call",
    "start": "2545500",
    "end": "2551650"
  },
  {
    "text": "that as a positive signal on the internet and we call it as attributions so using spark what we do is we are",
    "start": "2551650",
    "end": "2559210"
  },
  {
    "text": "running or you join between all the impressions that we are serving and all the positive signals we are getting on",
    "start": "2559210",
    "end": "2565270"
  },
  {
    "text": "the Internet and this generates a data set where each impression either is a",
    "start": "2565270",
    "end": "2570430"
  },
  {
    "text": "positive or a negative that can be used in a machine learning classifier now we give this data to our training models we",
    "start": "2570430",
    "end": "2577420"
  },
  {
    "text": "create classifiers out of them the classifiers what they do is whenever user comes online and C goes to goes to",
    "start": "2577420",
    "end": "2583270"
  },
  {
    "text": "a web page it tries to predict what's the probability that this user will have a positive signal positive signal when I",
    "start": "2583270",
    "end": "2589690"
  },
  {
    "text": "show an ad to him or her once we create the models we evaluate the models before",
    "start": "2589690",
    "end": "2594940"
  },
  {
    "text": "pushing it in production so we try to see if the models actually are able to predict conversions or not and once if",
    "start": "2594940",
    "end": "2601210"
  },
  {
    "text": "the models are good we will push it to s3 at which time the real-time bidding system will pick up the models and use",
    "start": "2601210",
    "end": "2607090"
  },
  {
    "text": "them for buying more ads that generates more data and again we will train our classifier so I think using the data so",
    "start": "2607090",
    "end": "2614410"
  },
  {
    "text": "I just want it to appreciate why this problem is difficult and I'm pretty sure all of you work with a lot of data so",
    "start": "2614410",
    "end": "2621010"
  },
  {
    "text": "what we have here is every day we have to use two petabytes of data and train",
    "start": "2621010",
    "end": "2627250"
  },
  {
    "text": "machine learning algorithms from it because all the ads that we are serving on the internet we are collecting all",
    "start": "2627250",
    "end": "2632440"
  },
  {
    "text": "this data and we are training classifies from it right so it's a lot of data and we are training thousands of classifiers",
    "start": "2632440",
    "end": "2638590"
  },
  {
    "text": "every day for all the different advertisers that we are serving ads to moreover the real-time bidding system is",
    "start": "2638590",
    "end": "2644650"
  },
  {
    "text": "making almost 2 million decisions every second every time a person goes on the web page we are making a decision to",
    "start": "2644650",
    "end": "2650200"
  },
  {
    "text": "show which ad to show to that person and how much do we want to pay for that ad so we are making 2 million decisions",
    "start": "2650200",
    "end": "2655870"
  },
  {
    "text": "every second on that person so the system has to be really fast in training",
    "start": "2655870",
    "end": "2660970"
  },
  {
    "text": "as well as doing the evaluation secondly we wanted to create a unattended system so we are not running",
    "start": "2660970",
    "end": "2667539"
  },
  {
    "text": "these jobs everyday what we wanted to do was the system automatically just collects the data from the internet from",
    "start": "2667539",
    "end": "2672819"
  },
  {
    "text": "the ads that were served yesterday creates the new models again goes out by his new ads is it using those models",
    "start": "2672819",
    "end": "2679059"
  },
  {
    "text": "again creates these again gets the data creates models and keeps on going so we",
    "start": "2679059",
    "end": "2684160"
  },
  {
    "text": "wanted to create an unattended system so we don't have to maintain it we just have to keep on pushing new code and it",
    "start": "2684160",
    "end": "2689319"
  },
  {
    "text": "just works and thirdly we are in the industry of online ad market so there",
    "start": "2689319",
    "end": "2694809"
  },
  {
    "text": "are a lot of competitors who are trying to build on these ads it's like a auction where everyone is trying to bid",
    "start": "2694809",
    "end": "2700210"
  },
  {
    "text": "on the ad the person who bids the highest who gets to win the ad right so we want to have state-of-the-art",
    "start": "2700210",
    "end": "2706480"
  },
  {
    "text": "algorithms because if our competitor has a better algorithm than us they will be able to buy more efficient ads than us",
    "start": "2706480",
    "end": "2712299"
  },
  {
    "text": "so it's we have to keep on adapting to the customers requirements really quickly so what I'm going to show in the",
    "start": "2712299",
    "end": "2720190"
  },
  {
    "text": "next few slides is why we are so excited about SPARC at data Zoo for the data science tasks so we ran around some",
    "start": "2720190",
    "end": "2728890"
  },
  {
    "text": "benchmarks using spark we use three algorithms from the ML Lib library and the first thing we saw was spark trains",
    "start": "2728890",
    "end": "2737470"
  },
  {
    "text": "in linear time it is not a lot of overhead when we are training models on spark they are really well tuned and this is very important",
    "start": "2737470",
    "end": "2744220"
  },
  {
    "text": "for us because we are showing we have thousands of advertisers we serve - right there are some advertisers that",
    "start": "2744220",
    "end": "2749380"
  },
  {
    "text": "are spending hundreds of dollars per day there are some that are spending thousands some that that are spending ten tens of thousands of dollars per day",
    "start": "2749380",
    "end": "2755230"
  },
  {
    "text": "so the largest advertiser generates the most data to train in the classifier and that's the bottleneck in our system and",
    "start": "2755230",
    "end": "2761079"
  },
  {
    "text": "we have to train our classifiers in the linear fashion because as we get more data and as the business scales we just",
    "start": "2761079",
    "end": "2768160"
  },
  {
    "text": "want to warn our algorithms to scale with that secondly as I said that we are making almost two million decisions",
    "start": "2768160",
    "end": "2774309"
  },
  {
    "text": "every second so we want two things in our models firstly we want our models to",
    "start": "2774309",
    "end": "2779920"
  },
  {
    "text": "have a very small memory footprint so that they can stay in a very fast memory and secondly the evaluation time or the",
    "start": "2779920",
    "end": "2787539"
  },
  {
    "text": "bidding time has to be really small so what we found was sparks performance was",
    "start": "2787539",
    "end": "2792940"
  },
  {
    "text": "really good out of the box now you can see that it read as we Maurice did well in these tests but we have spent",
    "start": "2792940",
    "end": "2799030"
  },
  {
    "text": "like seven years on optimizing these algorithms over and over again to make them as efficient as possible and Spock",
    "start": "2799030",
    "end": "2805510"
  },
  {
    "text": "this performance out here is out of the box and we found that it was feasible for us to use the spark models directly",
    "start": "2805510",
    "end": "2811540"
  },
  {
    "text": "out of the box which is pretty cool so just wanted to go over the big picture",
    "start": "2811540",
    "end": "2816700"
  },
  {
    "text": "of what we found when we are using spark now first of all now we have a homogeneous platform where our ETL",
    "start": "2816700",
    "end": "2822910"
  },
  {
    "text": "processes are using spark and our machine learning processes are using sparks so all the data flows in memory",
    "start": "2822910",
    "end": "2828850"
  },
  {
    "text": "and we are draining models out of it secondly a spark book this works really",
    "start": "2828850",
    "end": "2834040"
  },
  {
    "text": "well on EMR out of the box which is really important for us because most of our platform is using EMR and spark",
    "start": "2834040",
    "end": "2840670"
  },
  {
    "text": "really works well on that we needed very little code to create peace class files",
    "start": "2840670",
    "end": "2845890"
  },
  {
    "text": "like we need and I'll show demo for that but it was really easy to implement the SPARC architecture on the existing",
    "start": "2845890",
    "end": "2852850"
  },
  {
    "text": "platform and lastly SPARC did provide a stability to actually create an unattended system that just works well",
    "start": "2852850",
    "end": "2859860"
  },
  {
    "text": "having said that I am going to go to the demo",
    "start": "2859860",
    "end": "2864300"
  },
  {
    "text": "just going to run this because it takes a while I remember I said that we have",
    "start": "2871569",
    "end": "2885499"
  },
  {
    "text": "all the impressions that we saw and then we also get positive signals on the Internet now to combine all the data data",
    "start": "2885499",
    "end": "2892130"
  },
  {
    "text": "together which is called an attribution job you can see that we can do the entire thing in one SQL statement out",
    "start": "2892130",
    "end": "2897739"
  },
  {
    "text": "here on spark which is very cool once we create the data set now we have a data",
    "start": "2897739",
    "end": "2904429"
  },
  {
    "text": "set that we can use in machine learning what we are going to do is we are going to divide the data into two parts the",
    "start": "2904429",
    "end": "2909739"
  },
  {
    "text": "training data which is 70% of the data and the testing data which is 30% of the data now this is really interesting so",
    "start": "2909739",
    "end": "2918019"
  },
  {
    "text": "what happens is once we have the test set we are going to create a spark pipeline to our trainer models what I",
    "start": "2918019",
    "end": "2924679"
  },
  {
    "text": "mean here is I'm going to define different stages like the four stages to get the data the second stage is to",
    "start": "2924679",
    "end": "2930469"
  },
  {
    "text": "pre-process the features so you can use like feature selection algorithms on it you can use a top k transforms and stuff",
    "start": "2930469",
    "end": "2937219"
  },
  {
    "text": "like that in this phase and you pre process all your features that you are going to use in the third stage we are",
    "start": "2937219",
    "end": "2942859"
  },
  {
    "text": "going to assemble all the features back together to create a data set and in the fourth stage you are going to select a",
    "start": "2942859",
    "end": "2949159"
  },
  {
    "text": "classifier now as a data scientist I want to test using many different classifiers and using this pipeline",
    "start": "2949159",
    "end": "2955759"
  },
  {
    "text": "architecture I can just replace any classifier here and create different pipelines for different campaigns that",
    "start": "2955759",
    "end": "2960979"
  },
  {
    "text": "we are working with and finally I put this stages into a pipeline now here the",
    "start": "2960979",
    "end": "2968630"
  },
  {
    "text": "trainings for the models are going on this is live so we created a train data set before right so what I'm going to do",
    "start": "2968630",
    "end": "2974809"
  },
  {
    "text": "is I'm going to convert the training data set into Sparky readable format which is called as string in ml data set",
    "start": "2974809",
    "end": "2980419"
  },
  {
    "text": "and all I'm going to say is I'm going to fit this data into the pipeline so now",
    "start": "2980419",
    "end": "2985489"
  },
  {
    "text": "the train data set actually gets all the data together goes through the pre-processing phase where I'm going to",
    "start": "2985489",
    "end": "2991639"
  },
  {
    "text": "run feature selection algorithms and key transforms and stuff like that on it then I'm going to reassemble all the",
    "start": "2991639",
    "end": "2996799"
  },
  {
    "text": "features together and run classifiers on it and all that can be done in this one step",
    "start": "2996799",
    "end": "3002640"
  },
  {
    "text": "now the training is going on so once the model is trained we also want to",
    "start": "3002640",
    "end": "3008349"
  },
  {
    "text": "evaluate if the model is good or not because if the model is not able to predict if a person is going to have positive signal or not it's useless to",
    "start": "3008349",
    "end": "3015099"
  },
  {
    "text": "me right so we are going to run an evaluation phase where I am going to take the 30 percent of the data created",
    "start": "3015099",
    "end": "3021369"
  },
  {
    "text": "into a spark readable format and then pass it through the trained model so for the 30 percent of the data it's going to",
    "start": "3021369",
    "end": "3028150"
  },
  {
    "text": "go and try to predict probability of conversion for each of the rows and I also know the ground truth for them so I",
    "start": "3028150",
    "end": "3033519"
  },
  {
    "text": "know if my predictions are correct or not so once the predictions are done okay so",
    "start": "3033519",
    "end": "3040779"
  },
  {
    "text": "the training job is finished and now the prediction job is running once the predictions are done I'm going to find a",
    "start": "3040779",
    "end": "3047739"
  },
  {
    "text": "UC which is a measure of accuracy for the classifiers so if it tends close to hundred percent that means my that my",
    "start": "3047739",
    "end": "3053829"
  },
  {
    "text": "classifier is good at predicting of conversion if it tends close to fifty percent it means that it's a random",
    "start": "3053829",
    "end": "3059829"
  },
  {
    "text": "classifier so the training is going on right now the testing is going on like",
    "start": "3059829",
    "end": "3065949"
  },
  {
    "text": "right now and once this job is done I expect to see what accuracy of my classifier is it should be done in few",
    "start": "3065949",
    "end": "3072219"
  },
  {
    "text": "seconds so the big picture for this demo is you can see that just using this",
    "start": "3072219",
    "end": "3077949"
  },
  {
    "text": "small amount of code what I was running here was the data that was passed through Dongs ETL so it was 20 million",
    "start": "3077949",
    "end": "3084999"
  },
  {
    "text": "records that passed through a ETL process during the demo that went through the attribution phase I created",
    "start": "3084999",
    "end": "3091179"
  },
  {
    "text": "a pipeline for training the data set the whole training 20 million rows actually went through a training model created a",
    "start": "3091179",
    "end": "3097089"
  },
  {
    "text": "model and now you can see that it's going through the classification phase and any minute now it should be done and",
    "start": "3097089",
    "end": "3102549"
  },
  {
    "text": "give me our CI which is right here and not bad I guess so that concludes our",
    "start": "3102549",
    "end": "3108189"
  },
  {
    "text": "talk do we have time for Q&A",
    "start": "3108189",
    "end": "3111869"
  }
]