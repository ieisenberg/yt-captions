[
  {
    "text": "good afternoon welcome to my talk today I'll talk about the concept of tensors",
    "start": "60",
    "end": "7890"
  },
  {
    "text": "as an algorithmic concept how we can incorporate it in a range of machine",
    "start": "7890",
    "end": "14219"
  },
  {
    "text": "learning algorithms in particular I'll focus on the concept of topic modeling",
    "start": "14219",
    "end": "19470"
  },
  {
    "text": "as well as deep learning and show how you can utilize these algorithms on the",
    "start": "19470",
    "end": "25769"
  },
  {
    "text": "newly launched Amazon sage maker framework so machine learning is now",
    "start": "25769",
    "end": "33480"
  },
  {
    "text": "pervasive it's in so many domains the most popular one is perhaps I in the",
    "start": "33480",
    "end": "40320"
  },
  {
    "text": "field of computer vision where you want to understand what's in the images right",
    "start": "40320",
    "end": "46230"
  },
  {
    "text": "so if you go to the recognition API you can get different tags on the various",
    "start": "46230",
    "end": "53219"
  },
  {
    "text": "objects in an image similarly if you want to understand the content in text",
    "start": "53219",
    "end": "59430"
  },
  {
    "text": "documents we have now the newly launched comprehend service that talks about",
    "start": "59430",
    "end": "65040"
  },
  {
    "text": "understanding text and one of the services available as of today is topic",
    "start": "65040",
    "end": "71939"
  },
  {
    "text": "detection so what I have here as an example is a New York Times article so",
    "start": "71939",
    "end": "78540"
  },
  {
    "text": "it's talking about high-tech industry there's talking about politics right and there is a detailed article",
    "start": "78540",
    "end": "85740"
  },
  {
    "text": "and what you would like out of this are what are the particular topics in this",
    "start": "85740",
    "end": "91979"
  },
  {
    "text": "document and so you expect the output to be something like this you know this",
    "start": "91979",
    "end": "98070"
  },
  {
    "text": "document is talking about government so you I see particular words that are",
    "start": "98070",
    "end": "103979"
  },
  {
    "text": "representative of the government topic like Congress federal government Washington and so on you also see",
    "start": "103979",
    "end": "112680"
  },
  {
    "text": "information technology being represented in this document through words such as",
    "start": "112680",
    "end": "117979"
  },
  {
    "text": "online internet and so on politics you",
    "start": "117979",
    "end": "123270"
  },
  {
    "text": "can think of as an odd topic because there's reference to Democrats and Republicans so you expect an output that",
    "start": "123270",
    "end": "131910"
  },
  {
    "text": "is rich in detail on what each document talks about as well as what are the representative",
    "start": "131910",
    "end": "138750"
  },
  {
    "text": "words of the various topics covered in this document and so I'll cover today on",
    "start": "138750",
    "end": "145350"
  },
  {
    "text": "how we can solve this task efficiently and how you can access this algorithm on",
    "start": "145350",
    "end": "151440"
  },
  {
    "text": "the Amazon sage maker framework so when it comes to solving various machine",
    "start": "151440",
    "end": "158370"
  },
  {
    "text": "learning tasks we need a Trinity right so we first need the algorithms but it's",
    "start": "158370",
    "end": "164940"
  },
  {
    "text": "also important to have data and compute infrastructure without any one of this",
    "start": "164940",
    "end": "171020"
  },
  {
    "text": "ingredients machine learning would fail and ADA Bleus has been at the forefront",
    "start": "171020",
    "end": "177420"
  },
  {
    "text": "of providing solutions on all these three aspects you can get data annotated",
    "start": "177420",
    "end": "184830"
  },
  {
    "text": "through Amazon Mechanical Turk it's one of the oldest services and when it comes",
    "start": "184830",
    "end": "191220"
  },
  {
    "text": "to compute infrastructure we give you a range of options right so you can also",
    "start": "191220",
    "end": "197370"
  },
  {
    "text": "see different tiered framework of accessing AWS at the lowest level are",
    "start": "197370",
    "end": "207080"
  },
  {
    "text": "the GPUs and cpu instances as well as the new IOT framework or Greengrass we",
    "start": "207080",
    "end": "214110"
  },
  {
    "text": "launched and the mobile frameworks so that's the infrastructure level and with",
    "start": "214110",
    "end": "219630"
  },
  {
    "text": "this depending on your task depending on what the cost requirements are you can",
    "start": "219630",
    "end": "226500"
  },
  {
    "text": "either go with the latest GPU instances like the p3 instances as well as the CPU",
    "start": "226500",
    "end": "233580"
  },
  {
    "text": "instances like the sefa instances which have a good trade-off between cost and",
    "start": "233580",
    "end": "239190"
  },
  {
    "text": "speed so about the infrastructure are the frameworks so we need efficient",
    "start": "239190",
    "end": "245610"
  },
  {
    "text": "frameworks for writing our algorithms and especially with deep learning we",
    "start": "245610",
    "end": "251400"
  },
  {
    "text": "have a very complex neural network architectures that need to be written and AWS offers a whole choice of popular",
    "start": "251400",
    "end": "260760"
  },
  {
    "text": "open source deep learning frameworks including a TMX net and glue on which we are",
    "start": "260760",
    "end": "267620"
  },
  {
    "text": "actively supporting and developing on and you can access all these frameworks",
    "start": "267620",
    "end": "273020"
  },
  {
    "text": "with the AWS deep learning army so a level above the frameworks are the",
    "start": "273020",
    "end": "279110"
  },
  {
    "text": "platform services you saw the announcement today on AWS deep lines",
    "start": "279110",
    "end": "284630"
  },
  {
    "text": "that's the platform for cameras and Amazon sage maker which is the platform",
    "start": "284630",
    "end": "290690"
  },
  {
    "text": "for machine learning so I'll focus on how sage maker makes it easy and",
    "start": "290690",
    "end": "295730"
  },
  {
    "text": "convenient to launch various machine learning jobs a level above the",
    "start": "295730",
    "end": "301040"
  },
  {
    "text": "platforms are the managed application services so here you know machine",
    "start": "301040",
    "end": "307580"
  },
  {
    "text": "learning is already done for you so you can directly access the result of the",
    "start": "307580",
    "end": "312890"
  },
  {
    "text": "machine learning algorithms and there are domain-specific application services",
    "start": "312890",
    "end": "318080"
  },
  {
    "text": "there is recognition for computer vision for speech there is poly which is",
    "start": "318080",
    "end": "323960"
  },
  {
    "text": "text-to-speech and transcribe which is speech to text and with languages we",
    "start": "323960",
    "end": "330200"
  },
  {
    "text": "have the newly launched service comprehend which does text analytics",
    "start": "330200",
    "end": "335770"
  },
  {
    "text": "translation as well as legs so as you can see there have been a lot of",
    "start": "335770",
    "end": "342260"
  },
  {
    "text": "exciting announcements today and AWS offers a range of solutions for machine",
    "start": "342260",
    "end": "349190"
  },
  {
    "text": "learning so as I mentioned comprehend is",
    "start": "349190",
    "end": "354320"
  },
  {
    "text": "a managed service full text and you can get a range of text analytics including",
    "start": "354320",
    "end": "360760"
  },
  {
    "text": "named entity recognition sentiment analysis keyphrase extraction and topic",
    "start": "360760",
    "end": "367640"
  },
  {
    "text": "modeling and I will show how you can utilize topic modeling and what are some",
    "start": "367640",
    "end": "374289"
  },
  {
    "text": "algorithmic intuitions behind how to solve topic modeling so as I said sage",
    "start": "374289",
    "end": "383330"
  },
  {
    "text": "maker is several s framework for machine learning you can now access it through",
    "start": "383330",
    "end": "389690"
  },
  {
    "text": "the AWS console and you'll this is the screen shot where you'll see that you",
    "start": "389690",
    "end": "395659"
  },
  {
    "text": "can create notebook instances you can view various training jobs that you've",
    "start": "395659",
    "end": "401300"
  },
  {
    "text": "launched and you can have all your pre trained models stored in one location",
    "start": "401300",
    "end": "407599"
  },
  {
    "text": "and you can launch endpoints to do inference based on those trained models",
    "start": "407599",
    "end": "413360"
  },
  {
    "text": "and so it offers this n to add a framework for machine learning so it's",
    "start": "413360",
    "end": "421069"
  },
  {
    "text": "arguably the quickest and the easiest way to do machine learning in AWS to",
    "start": "421069",
    "end": "426800"
  },
  {
    "text": "take it from conception to production and it's flexible and you pay by the",
    "start": "426800",
    "end": "434479"
  },
  {
    "text": "second so this is very easy to get started on now machine learning and",
    "start": "434479",
    "end": "442060"
  },
  {
    "text": "within Sage Maker we have a range of first party algorithms so these are",
    "start": "442060",
    "end": "448250"
  },
  {
    "text": "algorithms we've extensively developed and tested and made sure that they meet",
    "start": "448250",
    "end": "454580"
  },
  {
    "text": "strong performance benchmarks they are much faster than existing open source",
    "start": "454580",
    "end": "460940"
  },
  {
    "text": "frameworks there and hence they are cheaper to run on AWS so you have the",
    "start": "460940",
    "end": "467389"
  },
  {
    "text": "classic algorithms such as linear classification and regression as well as",
    "start": "467389",
    "end": "472729"
  },
  {
    "text": "XG boost and factorization machines so we have unsupervised learning k-means is",
    "start": "472729",
    "end": "480380"
  },
  {
    "text": "the popular algorithm for clustering PCA or principal component analysis does",
    "start": "480380",
    "end": "486289"
  },
  {
    "text": "dimensionality reduction we also have image classification with convolutional",
    "start": "486289",
    "end": "494210"
  },
  {
    "text": "neural network models so where is neural network architectures from the MX net",
    "start": "494210",
    "end": "499460"
  },
  {
    "text": "framework as well as other frameworks are available and ready to use on Sage",
    "start": "499460",
    "end": "505039"
  },
  {
    "text": "maker and lastly I'll be covering more in detail topic modeling how we can",
    "start": "505039",
    "end": "511789"
  },
  {
    "text": "utilize Lda which stands for latent Dersch layer location model as well as",
    "start": "511789",
    "end": "517130"
  },
  {
    "text": "NTM the neural topic modeling framework within Sage maker and so the benefit of",
    "start": "517130",
    "end": "524660"
  },
  {
    "text": "having these first party algorithms is that they are highly efficient and ready",
    "start": "524660",
    "end": "530449"
  },
  {
    "text": "to use in production so this is the page for LTE I will you",
    "start": "530449",
    "end": "541280"
  },
  {
    "text": "know give the example in detail I just wanted to show the screenshot so let me",
    "start": "541280",
    "end": "548300"
  },
  {
    "text": "now talk about what LD a topic models you know what the inspiration is and how",
    "start": "548300",
    "end": "555890"
  },
  {
    "text": "its applicable for document categorization so as I showed in the",
    "start": "555890",
    "end": "563180"
  },
  {
    "text": "example before the task is given any document we should be able to",
    "start": "563180",
    "end": "568190"
  },
  {
    "text": "automatically infer what the topics are right so the input to the algorithm is",
    "start": "568190",
    "end": "574310"
  },
  {
    "text": "the document in particular it's like what are the words that are occurring in this document and given those statistics",
    "start": "574310",
    "end": "582830"
  },
  {
    "text": "can I automatically infer what are the set of topics so I should have output",
    "start": "582830",
    "end": "588830"
  },
  {
    "text": "that looks like this right list what are the topics in this document with what",
    "start": "588830",
    "end": "594290"
  },
  {
    "text": "proportion these topics occur in this document and what are the words that represent these topics strongly so",
    "start": "594290",
    "end": "602360"
  },
  {
    "text": "that's the kind of result I'm expecting from a topic model so what's the",
    "start": "602360",
    "end": "609530"
  },
  {
    "text": "challenge with topic model so topic models are what are known as unsupervised learning algorithms because",
    "start": "609530",
    "end": "617480"
  },
  {
    "text": "they don't use label later so the input to the topic modeling algorithm will",
    "start": "617480",
    "end": "623450"
  },
  {
    "text": "have no labels right will not have a document with all these labels shown like this and that's because it's very",
    "start": "623450",
    "end": "630850"
  },
  {
    "text": "manually expensive to annotate an entire long document right in fact lots of such",
    "start": "630850",
    "end": "638090"
  },
  {
    "text": "documents to get such annotations and hence all the algorithm sees is an input",
    "start": "638090",
    "end": "646490"
  },
  {
    "text": "that consists of documents but having no labels and so in this case how do we",
    "start": "646490",
    "end": "654170"
  },
  {
    "text": "automatically infer topics in the document as you can see another",
    "start": "654170",
    "end": "659750"
  },
  {
    "text": "challenge is that there are multiple topics in the document and this adds an",
    "start": "659750",
    "end": "665420"
  },
  {
    "text": "addition Challenge so as we are aware when it",
    "start": "665420",
    "end": "670730"
  },
  {
    "text": "comes to machine learning algorithms there are two broad categories there is the framework of unsupervised learning",
    "start": "670730",
    "end": "677260"
  },
  {
    "text": "like the topic modeling where the input has no labels right and the output is we",
    "start": "677260",
    "end": "684200"
  },
  {
    "text": "want to come up with a model or an explanation of what the data is about",
    "start": "684200",
    "end": "690070"
  },
  {
    "text": "the more popular framework is the supervised learning where the input",
    "start": "690070",
    "end": "695620"
  },
  {
    "text": "samples have labels and unsupervised learning is considered one of the",
    "start": "695620",
    "end": "701540"
  },
  {
    "text": "hardest challenges in machine learning and this is what makes even document",
    "start": "701540",
    "end": "707930"
  },
  {
    "text": "categorization a challenging task relatively so the most popular form of",
    "start": "707930",
    "end": "719060"
  },
  {
    "text": "for supervised learning is clustering so in this case as shown in the picture",
    "start": "719060",
    "end": "724790"
  },
  {
    "text": "what you would like to do is classify each of your input data point into your",
    "start": "724790",
    "end": "730610"
  },
  {
    "text": "unique cluster right so in the context of topic modeling you consider each",
    "start": "730610",
    "end": "738290"
  },
  {
    "text": "document and you want to classify it into single topic but as we saw earlier",
    "start": "738290",
    "end": "744860"
  },
  {
    "text": "this is going to be very limiting because most text documents don't have a",
    "start": "744860",
    "end": "750080"
  },
  {
    "text": "single topic there can have multiple topics so you have these topics that can",
    "start": "750080",
    "end": "758089"
  },
  {
    "text": "incorporate words that are overlapping and documents that can incorporate",
    "start": "758089",
    "end": "763100"
  },
  {
    "text": "multiple topics and that's why a simple clustering model using the k-means",
    "start": "763100",
    "end": "768800"
  },
  {
    "text": "algorithm is usually not effective when you want to classify documents when you",
    "start": "768800",
    "end": "774980"
  },
  {
    "text": "want to understand what are the topics in a document so we need to go beyond clustering and that's where the LDA or",
    "start": "774980",
    "end": "783589"
  },
  {
    "text": "the late and additionally allocation model comes into picture so what it",
    "start": "783589",
    "end": "789650"
  },
  {
    "text": "models the document as is a mixture of topics so on the right here the picture",
    "start": "789650",
    "end": "795920"
  },
  {
    "text": "you see you're seeing this history with different proportions of topics so",
    "start": "795920",
    "end": "803689"
  },
  {
    "text": "the yellow bar represents the topic justice for this document and you know",
    "start": "803689",
    "end": "808970"
  },
  {
    "text": "as as you can see there is a stronger continuity of the topic justice in this",
    "start": "808970",
    "end": "816649"
  },
  {
    "text": "document so that's a having a higher proportion so there is a proportions of",
    "start": "816649",
    "end": "822949"
  },
  {
    "text": "different topics covered in this document and that's modeled by the variable at the top and now to model how",
    "start": "822949",
    "end": "831499"
  },
  {
    "text": "words occur in the document what you do is you draw topics according to that proportion and then draw individual",
    "start": "831499",
    "end": "839449"
  },
  {
    "text": "words according to this matrix that I've shown which models the distribution of",
    "start": "839449",
    "end": "846920"
  },
  {
    "text": "words under each topic so intuitively what's happening here is an explanation",
    "start": "846920",
    "end": "853730"
  },
  {
    "text": "of how the frequency of words is the result of multiple topics occurring in",
    "start": "853730",
    "end": "860509"
  },
  {
    "text": "the document so this is able to incorporate multiple topics as well as",
    "start": "860509",
    "end": "867499"
  },
  {
    "text": "the property that a word could occur in different topics but the frequency of",
    "start": "867499",
    "end": "873019"
  },
  {
    "text": "occurrence would be different and hence this is a much richer model compared to",
    "start": "873019",
    "end": "879019"
  },
  {
    "text": "clustering and so we have enabled Lda in",
    "start": "879019",
    "end": "884449"
  },
  {
    "text": "Sage maker and there are two separate stages right so we take the corpus of",
    "start": "884449",
    "end": "891769"
  },
  {
    "text": "documents that's our input training data and we want to learn the LDA model and",
    "start": "891769",
    "end": "897670"
  },
  {
    "text": "so learning means that i'm learning this topic word matrix so each column here",
    "start": "897670",
    "end": "903499"
  },
  {
    "text": "represents the frequency of words under the topic so suppose the document had",
    "start": "903499",
    "end": "909589"
  },
  {
    "text": "only the topic how would the frequency of words look like right but because the",
    "start": "909589",
    "end": "915799"
  },
  {
    "text": "documents have multiple topics it would be a mixture of these columns in the matrix so that's the learning stage we",
    "start": "915799",
    "end": "924769"
  },
  {
    "text": "are learning this matrix that models how words would occur under various topics",
    "start": "924769",
    "end": "932420"
  },
  {
    "text": "and then the next stage is in France where we take each of the documents and",
    "start": "932420",
    "end": "938440"
  },
  {
    "text": "say what topics are occurring in this document like the example that I showed",
    "start": "938440",
    "end": "944180"
  },
  {
    "text": "before and so we will proceed in both these stages so now I'll show this with",
    "start": "944180",
    "end": "953420"
  },
  {
    "text": "a notebook demo you can access this file from github it's now available and in",
    "start": "953420",
    "end": "959720"
  },
  {
    "text": "addition you will see other notebooks as well with respect to the first party",
    "start": "959720",
    "end": "965570"
  },
  {
    "text": "algorithms that I described earlier in the slide so before I jump into the",
    "start": "965570",
    "end": "971540"
  },
  {
    "text": "notebook demo what I'll show here is a synthetic example it's a visual example",
    "start": "971540",
    "end": "976970"
  },
  {
    "text": "that makes it easier to see how the LDA algorithm works because you know when it",
    "start": "976970",
    "end": "983959"
  },
  {
    "text": "comes to real text data it's harder to understand the working of the algorithm",
    "start": "983959",
    "end": "989570"
  },
  {
    "text": "so we'll have the synthetic data generated for this notebook demo so what",
    "start": "989570",
    "end": "996949"
  },
  {
    "text": "I'm showing here is a pictorial representation of the word frequency",
    "start": "996949",
    "end": "1003160"
  },
  {
    "text": "under each topic so each of these squares here represents one topic so",
    "start": "1003160",
    "end": "1010329"
  },
  {
    "text": "under the first topic the set of words that occur very frequently are the ones",
    "start": "1010329",
    "end": "1016449"
  },
  {
    "text": "that are shown here so each of this small square here represents a particular word in the vocabulary so",
    "start": "1016449",
    "end": "1024339"
  },
  {
    "text": "think of this word maybe Apple here this word maybe orange this word maybe banana",
    "start": "1024339",
    "end": "1029668"
  },
  {
    "text": "right just to have an intuition and so this first topic could represent fruits",
    "start": "1029669",
    "end": "1036730"
  },
  {
    "text": "and that's you know so the mostly the words that occur in fruits are these",
    "start": "1036730",
    "end": "1042548"
  },
  {
    "text": "apple orange and banana and so on whereas now we moved to a different",
    "start": "1042549",
    "end": "1049210"
  },
  {
    "text": "topic say the topic here the words that occur in the document if that was the",
    "start": "1049210",
    "end": "1055299"
  },
  {
    "text": "only topic would differ right and so for instance this word could this topic",
    "start": "1055299",
    "end": "1062530"
  },
  {
    "text": "could mean software and so you would still have the occur of word Apple but then the other words",
    "start": "1062530",
    "end": "1068620"
  },
  {
    "text": "that occur with it would be different and that could be software iPhone or any",
    "start": "1068620",
    "end": "1075280"
  },
  {
    "text": "of the other words right so intuitively what I'm showing here is with each topic",
    "start": "1075280",
    "end": "1081010"
  },
  {
    "text": "the words that occur in the document will change and what I'm showing is the",
    "start": "1081010",
    "end": "1086740"
  },
  {
    "text": "heat map of the frequency of the word occurrence for each of these topics so",
    "start": "1086740",
    "end": "1093100"
  },
  {
    "text": "we'll use this a distribution to generate our documents so the way the",
    "start": "1093100",
    "end": "1099880"
  },
  {
    "text": "documents are generated is will have a mixture of topics so in this example",
    "start": "1099880",
    "end": "1105490"
  },
  {
    "text": "here the topics that are mostly occurring on this topic 7 and 8 and if",
    "start": "1105490",
    "end": "1113350"
  },
  {
    "text": "you go back and notice what the 7 and 8 represents it's mostly these horizontal",
    "start": "1113350",
    "end": "1118930"
  },
  {
    "text": "bars here and because these are the topics in this particular document the",
    "start": "1118930",
    "end": "1126070"
  },
  {
    "text": "frequency of occurrence of words is mostly those two horizontal bars so I",
    "start": "1126070",
    "end": "1133360"
  },
  {
    "text": "hope pictorially you can see how when there are multiple topics in a document",
    "start": "1133360",
    "end": "1139420"
  },
  {
    "text": "you have this overlap of frequency of word occurrences according to this topic",
    "start": "1139420",
    "end": "1146110"
  },
  {
    "text": "world probability distribution and so this pictorially conveys the data",
    "start": "1146110",
    "end": "1153250"
  },
  {
    "text": "generation according to this Lda topic model and similarly now you can generate",
    "start": "1153250",
    "end": "1160210"
  },
  {
    "text": "different documents that contain different topics so each here set of",
    "start": "1160210",
    "end": "1166810"
  },
  {
    "text": "topics you see has a different distribution so this is mostly the dominant topics in this document in a",
    "start": "1166810",
    "end": "1173770"
  },
  {
    "text": "different document the topics can be different so this is how we generated",
    "start": "1173770",
    "end": "1180220"
  },
  {
    "text": "the data and what I'll show now is how to run the sage maker Lda model with",
    "start": "1180220",
    "end": "1189990"
  },
  {
    "text": "this data generated and get",
    "start": "1189990",
    "end": "1195510"
  },
  {
    "text": "and run through and see what we'll get so this is the sage maker console you",
    "start": "1196620",
    "end": "1203399"
  },
  {
    "text": "can create notebook instances here I will go to the notebook that's already",
    "start": "1203399",
    "end": "1210480"
  },
  {
    "text": "executed in the interest of time because it does take some time to spin up a new",
    "start": "1210480",
    "end": "1216090"
  },
  {
    "text": "notebook instance and this is also",
    "start": "1216090",
    "end": "1221279"
  },
  {
    "text": "available as I said in the slide before in the github repository and this is",
    "start": "1221279",
    "end": "1227220"
  },
  {
    "text": "actually in the scientific details so this is the detailed notebook if you want all the details of how the LDA",
    "start": "1227220",
    "end": "1234960"
  },
  {
    "text": "model works and how to run it in sage maker so the first initial aspect is",
    "start": "1234960",
    "end": "1243210"
  },
  {
    "text": "importing various packages including MX nets numpy and sage maker of course and",
    "start": "1243210",
    "end": "1251899"
  },
  {
    "text": "as well as some serializers for the data and yeah so here you will say what the",
    "start": "1251899",
    "end": "1262080"
  },
  {
    "text": "s3 bucket you want it to be where to where you store your train model so the",
    "start": "1262080",
    "end": "1270389"
  },
  {
    "text": "LTA model is defined in detail here I will skip that because I just described",
    "start": "1270389",
    "end": "1276539"
  },
  {
    "text": "how we were generating the synthetic data for this example and so this is",
    "start": "1276539",
    "end": "1282419"
  },
  {
    "text": "again representing pictorially what I described earlier so we'll skip the data",
    "start": "1282419",
    "end": "1287639"
  },
  {
    "text": "generation part because I already described that so when it comes to",
    "start": "1287639",
    "end": "1295409"
  },
  {
    "text": "training I will describe how tensor algorithms can learn the LTA model but",
    "start": "1295409",
    "end": "1301440"
  },
  {
    "text": "let's now see how this is done on Sage maker so as we said we want to load our",
    "start": "1301440",
    "end": "1309119"
  },
  {
    "text": "training data into s3 and so you can you",
    "start": "1309119",
    "end": "1314190"
  },
  {
    "text": "know should prescribe where the training data lives and also we are converting",
    "start": "1314190",
    "end": "1321480"
  },
  {
    "text": "the generated synthetic data into the product of record i/o format and that's",
    "start": "1321480",
    "end": "1326519"
  },
  {
    "text": "where the serializers are used so the sage maker algorithms run in",
    "start": "1326519",
    "end": "1333360"
  },
  {
    "text": "docker containers and so this is the specification of the LDA containers to",
    "start": "1333360",
    "end": "1339600"
  },
  {
    "text": "run the training job so the LDA algorithm has some hyper parameters you",
    "start": "1339600",
    "end": "1347400"
  },
  {
    "text": "want to specify as you can expect you want to say how many topics you want in",
    "start": "1347400",
    "end": "1354360"
  },
  {
    "text": "your document corpus right like what is the expected number of topics and again you can change this to different",
    "start": "1354360",
    "end": "1360900"
  },
  {
    "text": "quantities and see how the results change the other is the size of the",
    "start": "1360900",
    "end": "1366030"
  },
  {
    "text": "vocabulary like how big is your vocabulary size the number of training",
    "start": "1366030",
    "end": "1371430"
  },
  {
    "text": "documents so this alpha not is an optional parameter so in the beginning I",
    "start": "1371430",
    "end": "1376860"
  },
  {
    "text": "would suggest not to think about it too much because if you have an idea of how",
    "start": "1376860",
    "end": "1382260"
  },
  {
    "text": "makestar your documents are your documents mostly about a single topic or do they have lots of topics together you",
    "start": "1382260",
    "end": "1389880"
  },
  {
    "text": "can set and play with this parameter as well and the other settings are what",
    "start": "1389880",
    "end": "1396990"
  },
  {
    "text": "kind of instance you want to run your sage maker so the currently LD a you",
    "start": "1396990",
    "end": "1403710"
  },
  {
    "text": "know is only on CPU instances and we recommend c4 and also ultimately c5 when",
    "start": "1403710",
    "end": "1410940"
  },
  {
    "text": "that becomes available because these are compute heavy jobs and so now we can",
    "start": "1410940",
    "end": "1418560"
  },
  {
    "text": "create a sage maker client and create the training job and so you know",
    "start": "1418560",
    "end": "1423570"
  },
  {
    "text": "everything we are gathering all what we specified here we are specifying the output part of where the train model",
    "start": "1423570",
    "end": "1431130"
  },
  {
    "text": "should be stored there is the instance count is one and what instance type and",
    "start": "1431130",
    "end": "1436140"
  },
  {
    "text": "all the hyper parameters are set here and so when you do LD a dot fit you run",
    "start": "1436140",
    "end": "1442380"
  },
  {
    "text": "the training job so there is a very highly verbose output so you know I will",
    "start": "1442380",
    "end": "1449550"
  },
  {
    "text": "skip that and when the training job is successful it says the job is complete",
    "start": "1449550",
    "end": "1455160"
  },
  {
    "text": "and so now the trained model is stored in the s3 bucket that we specified",
    "start": "1455160",
    "end": "1463350"
  },
  {
    "text": "so we can now download it from the s3 bucket the model that we obtained from",
    "start": "1463350",
    "end": "1468840"
  },
  {
    "text": "training and so there is a code snippet to now analyze what the model has done",
    "start": "1468840",
    "end": "1476640"
  },
  {
    "text": "right what the training has done so if you recall how we generated data with",
    "start": "1476640",
    "end": "1482670"
  },
  {
    "text": "the synthetic example and so we can go back and verify how accurate are the",
    "start": "1482670",
    "end": "1489710"
  },
  {
    "text": "parameters that are estimated by the algorithm and so the because this is",
    "start": "1489710",
    "end": "1497250"
  },
  {
    "text": "unsupervised learning the topics that are estimated by the algorithm need not be in the same order as the one that we",
    "start": "1497250",
    "end": "1505740"
  },
  {
    "text": "generated it with and so there is some code snippet that permutes it to see how",
    "start": "1505740",
    "end": "1513090"
  },
  {
    "text": "what is the closest match and you can see here from the figure that the topic",
    "start": "1513090",
    "end": "1521190"
  },
  {
    "text": "word probability with which the words documents were generated and the one",
    "start": "1521190",
    "end": "1527520"
  },
  {
    "text": "that's estimated by the sage maker algorithm are nearly identical so we can",
    "start": "1527520",
    "end": "1532830"
  },
  {
    "text": "also say how much the error was in estimating these parameters so this is a",
    "start": "1532830",
    "end": "1538380"
  },
  {
    "text": "sanity check to make sure that at least in the case of synthetic data we have",
    "start": "1538380",
    "end": "1543480"
  },
  {
    "text": "the right model that's recovered by the algorithm so the next step as I said",
    "start": "1543480",
    "end": "1551130"
  },
  {
    "text": "will is inference right once you have a model of how documents are generated",
    "start": "1551130",
    "end": "1559010"
  },
  {
    "text": "with respect to various frequency of words and the topics you want to do",
    "start": "1559010",
    "end": "1565530"
  },
  {
    "text": "inference on all the documents so you want to say what are the topics in each document and so this is where we have",
    "start": "1565530",
    "end": "1575310"
  },
  {
    "text": "this now function called LD a door deploy that will deploy the LDA",
    "start": "1575310",
    "end": "1580590"
  },
  {
    "text": "inference and again we can specify what instance type and so we can pass",
    "start": "1580590",
    "end": "1589950"
  },
  {
    "text": "different kinds of data formats to the inference endpoint so these are formats",
    "start": "1589950",
    "end": "1595920"
  },
  {
    "text": "in in terms of how we encode the frequencies of words in each document",
    "start": "1595920",
    "end": "1603179"
  },
  {
    "text": "and so here we are gonna use the CSV formatted data although you have other",
    "start": "1603179",
    "end": "1609840"
  },
  {
    "text": "options like JSON and record i/o and so it's just doing that conversion and so",
    "start": "1609840",
    "end": "1616770"
  },
  {
    "text": "now if you do Lda in France dot predict and this is the set of documents on",
    "start": "1616770",
    "end": "1622230"
  },
  {
    "text": "which you want to do the topic detection you will get this result again it's verbose but it is you know",
    "start": "1622230",
    "end": "1630510"
  },
  {
    "text": "formatted according to this right so it's saying like under each document",
    "start": "1630510",
    "end": "1635820"
  },
  {
    "text": "what is the mixture of topics right so you can get the entire proportion of topics in each document so after some",
    "start": "1635820",
    "end": "1645740"
  },
  {
    "text": "you know simple plotting code you can see here again a sanity check that the",
    "start": "1645740",
    "end": "1653790"
  },
  {
    "text": "blue was the original topics with which the document was generated and the",
    "start": "1653790",
    "end": "1660120"
  },
  {
    "text": "orange is the one that's estimated by the algorithm so that's orange is the result of the inference algorithm and",
    "start": "1660120",
    "end": "1667169"
  },
  {
    "text": "you can see here that it's pretty close to the with respect to the topics which",
    "start": "1667169",
    "end": "1674910"
  },
  {
    "text": "were generating the document and so this notebooks gives you a sanity check that",
    "start": "1674910",
    "end": "1680850"
  },
  {
    "text": "if you generated data synthetically according to an Lda model this algorithm",
    "start": "1680850",
    "end": "1686549"
  },
  {
    "text": "recovers both the the topic word probability distribution as well as in",
    "start": "1686549",
    "end": "1694080"
  },
  {
    "text": "first topics on each document pretty accurately so though notebook has much",
    "start": "1694080",
    "end": "1700350"
  },
  {
    "text": "more detailed analysis on what the errors are on different documents like",
    "start": "1700350",
    "end": "1705960"
  },
  {
    "text": "you can have analysis of which documents had good accuracy of inference and which",
    "start": "1705960",
    "end": "1712440"
  },
  {
    "text": "ones did not and finally of course you should close your endpoint to finish the",
    "start": "1712440",
    "end": "1718559"
  },
  {
    "text": "execution there is also a brief discussion on how we go from synthetic",
    "start": "1718559",
    "end": "1726090"
  },
  {
    "text": "data to text data right because what I showed was a toy example and now if you want to run this",
    "start": "1726090",
    "end": "1733059"
  },
  {
    "text": "on text data you want to do some additional pre-processing and you know",
    "start": "1733059",
    "end": "1739899"
  },
  {
    "text": "that's described here as well as there are a number of papers that describe how to do it so now I'll come back to the",
    "start": "1739899",
    "end": "1751719"
  },
  {
    "text": "slides and ask you know I showed like how to run the algorithm and sage maker",
    "start": "1751719",
    "end": "1756729"
  },
  {
    "text": "and also gave intuition of how the LDA model incorporates mixtures of topics",
    "start": "1756729",
    "end": "1765399"
  },
  {
    "text": "into different documents so now let's do performance analysis on real text data",
    "start": "1765399",
    "end": "1772259"
  },
  {
    "text": "right because that's where we want this to be applicable of course as a side",
    "start": "1772259",
    "end": "1779109"
  },
  {
    "text": "comment Lda model can also be applicable in other domains especially in genetic",
    "start": "1779109",
    "end": "1786009"
  },
  {
    "text": "analysis it's a pretty popular model and with sage maker you can use it across",
    "start": "1786009",
    "end": "1791349"
  },
  {
    "text": "different domains as well but here we've analyzed now performance with respect to",
    "start": "1791349",
    "end": "1797069"
  },
  {
    "text": "text data and let's see let's do some deep dive so first a Lucian Bock one",
    "start": "1797069",
    "end": "1805329"
  },
  {
    "text": "qualitative analysis see what are the recovered topics in various document",
    "start": "1805329",
    "end": "1811749"
  },
  {
    "text": "corpuses and what human interpretability does it have you know to ask can we",
    "start": "1811749",
    "end": "1818259"
  },
  {
    "text": "think of these as reasonable topics or you know are they not so we took the New",
    "start": "1818259",
    "end": "1825759"
  },
  {
    "text": "York Times corpus this is a popular open source corpus of about three hundred",
    "start": "1825759",
    "end": "1831789"
  },
  {
    "text": "thousand documents and the top topics that were recovered from this corpus are",
    "start": "1831789",
    "end": "1838209"
  },
  {
    "text": "shown here so what I'm showing is the word cloud corresponding to that topic",
    "start": "1838209",
    "end": "1843249"
  },
  {
    "text": "so these are the top representative words in that topic so here you see one",
    "start": "1843249",
    "end": "1850569"
  },
  {
    "text": "topic that covered sports right the dominant words are game team play and so",
    "start": "1850569",
    "end": "1857859"
  },
  {
    "text": "on there's another word there's another topic that covers business",
    "start": "1857859",
    "end": "1862960"
  },
  {
    "text": "and so that's represented by company % million stock market and so on",
    "start": "1862960",
    "end": "1871180"
  },
  {
    "text": "and this new york times corpus I think was between the 90s and 2000's so you",
    "start": "1871180",
    "end": "1879490"
  },
  {
    "text": "can see the politics they're covered Bush and Al Gore and so those were the",
    "start": "1879490",
    "end": "1885550"
  },
  {
    "text": "dominant words and of course their lifestyle is another topic and the",
    "start": "1885550",
    "end": "1891220"
  },
  {
    "text": "representative words you can see so as you can see there is good interpretability from these topics we",
    "start": "1891220",
    "end": "1898000"
  },
  {
    "text": "can think of these as being reasonable to model documents another corpus that",
    "start": "1898000",
    "end": "1907060"
  },
  {
    "text": "we utilized is the PubMed corpus the total corpus is about 8 million",
    "start": "1907060",
    "end": "1912760"
  },
  {
    "text": "documents and again when we asked what are the top topics that we recover here",
    "start": "1912760",
    "end": "1919260"
  },
  {
    "text": "they have you know these various categories one is cancer and of course",
    "start": "1919260",
    "end": "1927430"
  },
  {
    "text": "the most dominant word as you expect is tumor there is the public health",
    "start": "1927430",
    "end": "1934260"
  },
  {
    "text": "framework so their children's system and data and population are some",
    "start": "1934260",
    "end": "1940300"
  },
  {
    "text": "presentative words and we also have clinical trials rat is the most common",
    "start": "1940300",
    "end": "1948240"
  },
  {
    "text": "animal that's used in clinical trials so it's natural that shows up pretty",
    "start": "1948240",
    "end": "1954010"
  },
  {
    "text": "significantly and blood seems to be a popular category with concentration",
    "start": "1954010",
    "end": "1961630"
  },
  {
    "text": "plasma you know serum being the representative words",
    "start": "1961630",
    "end": "1966820"
  },
  {
    "text": "so this PubMed is the NIH repository for biology articles so as you can expect",
    "start": "1966820",
    "end": "1974790"
  },
  {
    "text": "the topics can be you know very much interpreted in that context so we can",
    "start": "1974790",
    "end": "1983620"
  },
  {
    "text": "also now go back and look at individual documents and ask the topics that are",
    "start": "1983620",
    "end": "1989440"
  },
  {
    "text": "recovered there are they reasonable so this is the document that I showed in the beginning",
    "start": "1989440",
    "end": "1995389"
  },
  {
    "text": "and as we saw in this document having government information technology and",
    "start": "1995389",
    "end": "2001960"
  },
  {
    "text": "politics are very reasonable topics and the words that represent these topics",
    "start": "2001960",
    "end": "2008850"
  },
  {
    "text": "you know have a high human interpret ability and show another example this is",
    "start": "2008850",
    "end": "2016600"
  },
  {
    "text": "again a document from the 1990 99 and so",
    "start": "2016600",
    "end": "2026470"
  },
  {
    "text": "this is talking about the Super Bowl so if I a human were to read just the title",
    "start": "2026470",
    "end": "2032500"
  },
  {
    "text": "you would expect sports to be a category right but you see the algorithm did not",
    "start": "2032500",
    "end": "2038139"
  },
  {
    "text": "recover sports as a category because if you'll read through the entire article there's almost no discussion of the",
    "start": "2038139",
    "end": "2045370"
  },
  {
    "text": "actual Super Bowl right so it's more an analysis of the business so there's",
    "start": "2045370",
    "end": "2051820"
  },
  {
    "text": "talking about company there is a consulting company there is commercial there's a chief executive being quoted",
    "start": "2051820",
    "end": "2058210"
  },
  {
    "text": "and it's also talking about the information technology like internet.com",
    "start": "2058210",
    "end": "2065608"
  },
  {
    "text": "websites so there is no discussion of sports in the document and so the topic",
    "start": "2065609",
    "end": "2073990"
  },
  {
    "text": "model is very useful in the analysis as opposed to the human just reading the",
    "start": "2073990",
    "end": "2079300"
  },
  {
    "text": "title and trying to infer what's in the document and so that's where we need automated algorithms that can analyze",
    "start": "2079300",
    "end": "2086819"
  },
  {
    "text": "entire long documents and try to come up with topic detection as opposed to",
    "start": "2086819",
    "end": "2093460"
  },
  {
    "text": "manual labeling that only looks at keywords or you know very high-level how",
    "start": "2093460",
    "end": "2099069"
  },
  {
    "text": "our title to categorize and so we can see this as for one of the useful",
    "start": "2099069",
    "end": "2104890"
  },
  {
    "text": "features of topic models so let me now",
    "start": "2104890",
    "end": "2111130"
  },
  {
    "text": "show some quantitative performance benchmarks ask how fast are these",
    "start": "2111130",
    "end": "2118060"
  },
  {
    "text": "algorithms how do they compare with other open source frameworks and how can",
    "start": "2118060",
    "end": "2123579"
  },
  {
    "text": "we measure the quality of topics that are recovered",
    "start": "2123579",
    "end": "2129240"
  },
  {
    "text": "so the sage maker ld8 turns out to be much faster than mallet that's the",
    "start": "2129390",
    "end": "2136210"
  },
  {
    "text": "popular open-source framework for topic modeling we ran both of them on the sage",
    "start": "2136210",
    "end": "2141730"
  },
  {
    "text": "maker platform so we can docker eyes mallet as well and run it on sage maker along with our algorithm and so what you",
    "start": "2141730",
    "end": "2151089"
  },
  {
    "text": "can see here on the new york times corpus with three hundred thousand documents it was up to 22 times faster",
    "start": "2151089",
    "end": "2159780"
  },
  {
    "text": "compared to mallet in terms of the training time like how fast it took to",
    "start": "2159780",
    "end": "2166510"
  },
  {
    "text": "train the LDA model and on the pubmed corpus it was about 12x faster on",
    "start": "2166510",
    "end": "2174250"
  },
  {
    "text": "average so the pubmed corpus only has abstracts so they're very short articles",
    "start": "2174250",
    "end": "2180549"
  },
  {
    "text": "as opposed to New York Times with very long articles and so our sage maker Lda",
    "start": "2180549",
    "end": "2186549"
  },
  {
    "text": "will have advantage as the documents get longer because that's where it can more",
    "start": "2186549",
    "end": "2193030"
  },
  {
    "text": "efficiently aggregate information compared to mallet that doesn't have the",
    "start": "2193030",
    "end": "2198849"
  },
  {
    "text": "deficiency so once you have a faster",
    "start": "2198849",
    "end": "2204760"
  },
  {
    "text": "algorithm this also translates to lower cost or need of us I've just changed now",
    "start": "2204760",
    "end": "2211900"
  },
  {
    "text": "the y-axis from time to cost on the c48 x-large instance and you can see that",
    "start": "2211900",
    "end": "2219270"
  },
  {
    "text": "it's translates proportionally and so indeed like there is a very strong",
    "start": "2219270",
    "end": "2227500"
  },
  {
    "text": "motivation for us to keep developing faster and more efficient algorithms to",
    "start": "2227500",
    "end": "2233319"
  },
  {
    "text": "transfer the benefit to our customers so",
    "start": "2233319",
    "end": "2238480"
  },
  {
    "text": "as I said in sage maker the training and the inference are decoupled so if you",
    "start": "2238480",
    "end": "2243849"
  },
  {
    "text": "only want to train your model you can do that separately and you can use a pre trained model to run your inference and",
    "start": "2243849",
    "end": "2250780"
  },
  {
    "text": "get topics on individual documents so if you compare only the inference times you",
    "start": "2250780",
    "end": "2256660"
  },
  {
    "text": "can see that it is still competitive compared to mallet up to 13 X faster on New York Times and 3.5",
    "start": "2256660",
    "end": "2264640"
  },
  {
    "text": "X faster on PubMed and since mallet does",
    "start": "2264640",
    "end": "2270550"
  },
  {
    "text": "training an inference together like you know there is no way to decouple one from the other in mallet we also wanted",
    "start": "2270550",
    "end": "2277810"
  },
  {
    "text": "to compare the joint I'm so where you do both training and inference together and",
    "start": "2277810",
    "end": "2282820"
  },
  {
    "text": "even there you see that ours is faster in New York Times it's up to 7 X faster",
    "start": "2282820",
    "end": "2289540"
  },
  {
    "text": "and on PubMed up to 2.5 X faster and so you can see that there is an advantage",
    "start": "2289540",
    "end": "2297880"
  },
  {
    "text": "in terms of running times both with training inference or joint training and",
    "start": "2297880",
    "end": "2303370"
  },
  {
    "text": "inference with sage maker Lda so this is great we have a fast algorithm but is",
    "start": "2303370",
    "end": "2310870"
  },
  {
    "text": "this a good algorithm right does this give good quality topics so I showed you",
    "start": "2310870",
    "end": "2316510"
  },
  {
    "text": "earlier qualitative results showing that the topics that Willy covered are interpretable but we also want a",
    "start": "2316510",
    "end": "2323620"
  },
  {
    "text": "quantifiable measure and this is actually challenging for unsupervised",
    "start": "2323620",
    "end": "2329170"
  },
  {
    "text": "learning because it's not clear what is the right metric to measure and with",
    "start": "2329170",
    "end": "2334870"
  },
  {
    "text": "respect to topic models researchers have argued that the score known as pairwise",
    "start": "2334870",
    "end": "2340900"
  },
  {
    "text": "mutual information is very reasonable one and it closely correlates with how",
    "start": "2340900",
    "end": "2347230"
  },
  {
    "text": "humans church has good quality topics and we see here either we beat or match",
    "start": "2347230",
    "end": "2353530"
  },
  {
    "text": "mallet in terms of topic quality as well so in summary we have faster algorithm",
    "start": "2353530",
    "end": "2361360"
  },
  {
    "text": "at the same time have competitive topic quality so we have best of both the",
    "start": "2361360",
    "end": "2366490"
  },
  {
    "text": "worlds with this age maker Lda algorithm",
    "start": "2366490",
    "end": "2371400"
  },
  {
    "text": "so as I said Don sage maker we have a variety of options for first party",
    "start": "2372960",
    "end": "2380140"
  },
  {
    "text": "algorithms and another one is the neural topic modeling so I won't get into the",
    "start": "2380140",
    "end": "2385360"
  },
  {
    "text": "details here from in this case you have a neural network to",
    "start": "2385360",
    "end": "2391119"
  },
  {
    "text": "operate your topic model in general the training here tends to be much slower than the LDA topic model but inference",
    "start": "2391119",
    "end": "2399369"
  },
  {
    "text": "tends to be faster because it's just passing through the neural network so",
    "start": "2399369",
    "end": "2404410"
  },
  {
    "text": "depending on what your trade-offs are you can try either one of them and see which one best fits your needs",
    "start": "2404410",
    "end": "2412950"
  },
  {
    "text": "so now I want to take some time and do a science deep dive and give some",
    "start": "2413970",
    "end": "2420630"
  },
  {
    "text": "intuitions into what algorithms have gone into building the sage maker Lda",
    "start": "2420630",
    "end": "2426700"
  },
  {
    "text": "algorithm and that incorporates sensors",
    "start": "2426700",
    "end": "2432450"
  },
  {
    "text": "so this is a cartoon diagram of what tensors are right so you know tensors",
    "start": "2434130",
    "end": "2441279"
  },
  {
    "text": "represent higher dimensional objects so you know the zero dimensional object is",
    "start": "2441279",
    "end": "2446349"
  },
  {
    "text": "the scale R which is a single point vector is one dimensional if you have a",
    "start": "2446349",
    "end": "2451930"
  },
  {
    "text": "matrix that's two-dimensional and now our tensor is 3 or higher dimensional",
    "start": "2451930",
    "end": "2458529"
  },
  {
    "text": "object and so how can we incorporate tensors into learning the LDA algorithm",
    "start": "2458529",
    "end": "2468420"
  },
  {
    "text": "so if you recall the LDA algorithm incorporated this topic word matrix",
    "start": "2468420",
    "end": "2475569"
  },
  {
    "text": "right so you wanted to learn the distribution of words under each topic",
    "start": "2475569",
    "end": "2480970"
  },
  {
    "text": "and so the intuition is we will now",
    "start": "2480970",
    "end": "2486190"
  },
  {
    "text": "utilize co-occurrence of words to learn this topic word matrix so intuitively if",
    "start": "2486190",
    "end": "2494160"
  },
  {
    "text": "multiple words Corker together that a higher amount of information to",
    "start": "2494160",
    "end": "2501339"
  },
  {
    "text": "represent a topic and we utilize Triplets of words so that becomes a",
    "start": "2501339",
    "end": "2507940"
  },
  {
    "text": "third order tensor so you collect every triplet of words and you ask how many",
    "start": "2507940",
    "end": "2513489"
  },
  {
    "text": "times did this occur together in my document corpus and so using those",
    "start": "2513489",
    "end": "2519430"
  },
  {
    "text": "statistics you can do a decomposition and you can recover the columns of this topic word matrix I",
    "start": "2519430",
    "end": "2527010"
  },
  {
    "text": "won't get into the technical details there are research papers that I have",
    "start": "2527010",
    "end": "2532450"
  },
  {
    "text": "published in this regard but the intuition is you what it does is it",
    "start": "2532450",
    "end": "2538540"
  },
  {
    "text": "extends matrix decomposition methods to tensors so I think all of you're",
    "start": "2538540",
    "end": "2544690"
  },
  {
    "text": "familiar with linear algebra operations in particular singular value decomposition so what you do there is",
    "start": "2544690",
    "end": "2551740"
  },
  {
    "text": "you take a matrix and you decompose it into its low rank components and we can",
    "start": "2551740",
    "end": "2558490"
  },
  {
    "text": "now extend the same two tensors so you can now take third order or higher order",
    "start": "2558490",
    "end": "2563530"
  },
  {
    "text": "tensors and find decomposition into low rank components and in the context of",
    "start": "2563530",
    "end": "2570430"
  },
  {
    "text": "topic model if we take the co-occurrence tensor so we are taking every triplet of",
    "start": "2570430",
    "end": "2576640"
  },
  {
    "text": "words and calculating how many times it occurred together in the document corpus",
    "start": "2576640",
    "end": "2582750"
  },
  {
    "text": "and if you do that decomposition each lower rank component each rank one",
    "start": "2582750",
    "end": "2588550"
  },
  {
    "text": "component corresponds to one of the topics so the intuition is you can",
    "start": "2588550",
    "end": "2594040"
  },
  {
    "text": "recover topics by doing this tensor decomposition so why should we do this",
    "start": "2594040",
    "end": "2600370"
  },
  {
    "text": "tensor decomposition right I mean tensors seem much more much harder to",
    "start": "2600370",
    "end": "2606790"
  },
  {
    "text": "understand compared to matrices so why should we go to tensors so it turns out one is due to statistical reasons so you",
    "start": "2606790",
    "end": "2615430"
  },
  {
    "text": "need tensors as you incorporate higher order relationships in your data so if",
    "start": "2615430",
    "end": "2621100"
  },
  {
    "text": "you only wanted to record pairwise co-occurrences in our document corpus we",
    "start": "2621100",
    "end": "2626110"
  },
  {
    "text": "could have used a matrix but if you want to record co-occurrences among triplets",
    "start": "2626110",
    "end": "2631180"
  },
  {
    "text": "that's where we need tensors and so we can incorporate these higher-order",
    "start": "2631180",
    "end": "2636450"
  },
  {
    "text": "relationships through tensors and the second reason is you need to go to these",
    "start": "2636450",
    "end": "2643390"
  },
  {
    "text": "higher-order relationships if you only use pairwise co-current statistics that",
    "start": "2643390",
    "end": "2649090"
  },
  {
    "text": "does not reveal what the topics are so this has to do with some deep",
    "start": "2649090",
    "end": "2654460"
  },
  {
    "text": "mathematical reason on how matrix decomposition differs from the",
    "start": "2654460",
    "end": "2660070"
  },
  {
    "text": "tensor sorry about that so that when you decompose a matrix the algebraic",
    "start": "2660070",
    "end": "2665350"
  },
  {
    "text": "properties are very different from when you decompose a tensor and there have",
    "start": "2665350",
    "end": "2670900"
  },
  {
    "text": "been extensive works published in that regard but intuitively you need to these",
    "start": "2670900",
    "end": "2677350"
  },
  {
    "text": "higher-order statistics without it you cannot discover what the underlying topics are so that's the statistical",
    "start": "2677350",
    "end": "2684910"
  },
  {
    "text": "reason the other one is computational so as we solve from our performance",
    "start": "2684910",
    "end": "2690550"
  },
  {
    "text": "benchmarks these lead to much faster algorithms and the underlying reason is",
    "start": "2690550",
    "end": "2696460"
  },
  {
    "text": "because tensor algebra is easily parallelizable like linear algebra so",
    "start": "2696460",
    "end": "2702880"
  },
  {
    "text": "I've been doing extensive research on how to incorporate faster and",
    "start": "2702880",
    "end": "2709000"
  },
  {
    "text": "parallelizable operations when it comes to manipulating tensors and you can",
    "start": "2709000",
    "end": "2715690"
  },
  {
    "text": "build new primitives you can extend blast kernels to make them highly efficient so you know whenever the",
    "start": "2715690",
    "end": "2723940"
  },
  {
    "text": "foundation is linear algebra we can build on top very efficient algorithms",
    "start": "2723940",
    "end": "2729880"
  },
  {
    "text": "and so that's what makes the use of tensor algorithms in sage maker LD a",
    "start": "2729880",
    "end": "2736360"
  },
  {
    "text": "much faster than other algorithms and we also saw that this led to a decoupling",
    "start": "2736360",
    "end": "2743200"
  },
  {
    "text": "between training and inference so this is flexible compared to other LD",
    "start": "2743200",
    "end": "2748240"
  },
  {
    "text": "algorithms where training and inference are coupled together and if you're",
    "start": "2748240",
    "end": "2753940"
  },
  {
    "text": "interested in the details of the theory on you know why tensors and how transfer decompositions help discover the topics",
    "start": "2753940",
    "end": "2761710"
  },
  {
    "text": "you can refer to my paper in fact in theory it's guaranteed to recover the",
    "start": "2761710",
    "end": "2766720"
  },
  {
    "text": "correct model so so that's kind of the overall set of intuitions on how tensors",
    "start": "2766720",
    "end": "2777220"
  },
  {
    "text": "are helpful in giving us very fast algorithms for Lda",
    "start": "2777220",
    "end": "2783130"
  },
  {
    "text": "and at the same time having good topic quality",
    "start": "2783130",
    "end": "2788340"
  },
  {
    "text": "so now I want to spend a bit of time also asking how we can utilize the",
    "start": "2789510",
    "end": "2795660"
  },
  {
    "text": "concept of tensors in other frameworks including deep learning so far what we",
    "start": "2795660",
    "end": "2801990"
  },
  {
    "text": "saw was how tensors are useful in incorporating the LDA model and it",
    "start": "2801990",
    "end": "2809310"
  },
  {
    "text": "incorporates higher-order statistics there and helps us efficiently recover",
    "start": "2809310",
    "end": "2814590"
  },
  {
    "text": "the topics so what other contexts are useful to incorporate tensors and indeed",
    "start": "2814590",
    "end": "2823230"
  },
  {
    "text": "since deep learning has shown very impressive performance across multiple domains our natural thought is can we",
    "start": "2823230",
    "end": "2831570"
  },
  {
    "text": "incorporate tensor operations into our deep learning architectures so if you",
    "start": "2831570",
    "end": "2837300"
  },
  {
    "text": "think about most deep learning architectures those involve in ear algebra operations right they involve",
    "start": "2837300",
    "end": "2843720"
  },
  {
    "text": "matrix operations and so now how can we turn them into tensor operations how can",
    "start": "2843720",
    "end": "2850680"
  },
  {
    "text": "we exploit multiple dimensions together in these architectures and so this is",
    "start": "2850680",
    "end": "2856950"
  },
  {
    "text": "what I want to spend the remaining time talking about so this is a very popular",
    "start": "2856950",
    "end": "2864530"
  },
  {
    "text": "architecture for computer vision where you input the image you send it through",
    "start": "2864530",
    "end": "2871230"
  },
  {
    "text": "various convolutional layers right so with the convolutional layers what",
    "start": "2871230",
    "end": "2876810"
  },
  {
    "text": "you're doing is you're sending it through various filters so you're increasing the number of channels as you",
    "start": "2876810",
    "end": "2883890"
  },
  {
    "text": "go from one layer to the next and the spatial pooling means that you are reducing the dimensionality of your",
    "start": "2883890",
    "end": "2891390"
  },
  {
    "text": "input but on the other hand once you get to the fully connected layers you're",
    "start": "2891390",
    "end": "2896700"
  },
  {
    "text": "vectorizing near the activations and sending it through a matrix product",
    "start": "2896700",
    "end": "2902460"
  },
  {
    "text": "operation and applying some non-linearity and in the very end coming",
    "start": "2902460",
    "end": "2907470"
  },
  {
    "text": "up with the output of what object categories are present in these images",
    "start": "2907470",
    "end": "2913160"
  },
  {
    "text": "so this is the standard deep learning network and so what we did was to ask",
    "start": "2913160",
    "end": "2919590"
  },
  {
    "text": "you know can we incorporate the tensile structure the architecture so meaning we",
    "start": "2919590",
    "end": "2925599"
  },
  {
    "text": "incorporate the 3d structure all the way to the end and one way to do it is to",
    "start": "2925599",
    "end": "2933070"
  },
  {
    "text": "incorporate what are known as tensor contractions so we don't destroy this three-dimensional information even as",
    "start": "2933070",
    "end": "2939849"
  },
  {
    "text": "we're sending it through the fully connected layers and even in the last layer we do a chancer regression so",
    "start": "2939849",
    "end": "2946839"
  },
  {
    "text": "without going into details what this accomplishes is to retain the 3d",
    "start": "2946839",
    "end": "2952030"
  },
  {
    "text": "information throughout the network right so now we can ask okay how is this",
    "start": "2952030",
    "end": "2957849"
  },
  {
    "text": "beneficial what advantage does this give me it turns out that this can lead to",
    "start": "2957849",
    "end": "2964089"
  },
  {
    "text": "much more compact networks if you count the number of parameters in the fully",
    "start": "2964089",
    "end": "2969130"
  },
  {
    "text": "connected layers you can reduce the number of parameters by a huge amount without reducing the accuracy so you can",
    "start": "2969130",
    "end": "2977890"
  },
  {
    "text": "have faster more compact networks by incorporating tensors into the deep",
    "start": "2977890",
    "end": "2983140"
  },
  {
    "text": "learning architectures so this was one use of how tensors can now retain the",
    "start": "2983140",
    "end": "2990720"
  },
  {
    "text": "three-dimensional information that would have been lost if in the if you did",
    "start": "2990720",
    "end": "2995950"
  },
  {
    "text": "matrix operations like in the traditional architectures so another",
    "start": "2995950",
    "end": "3002040"
  },
  {
    "text": "aspect where we apply tensors was for sequence modeling so these are popular",
    "start": "3002040",
    "end": "3008190"
  },
  {
    "text": "now in a variety of domains the one we considered was forecasting meaning you",
    "start": "3008190",
    "end": "3014580"
  },
  {
    "text": "have a bunch of input time series points and you want to forecast you want to",
    "start": "3014580",
    "end": "3019920"
  },
  {
    "text": "predict what would the next time points look like and this is the standard",
    "start": "3019920",
    "end": "3025200"
  },
  {
    "text": "recurrent neural network architecture and so what we incorporated was now",
    "start": "3025200",
    "end": "3030990"
  },
  {
    "text": "tensors into the hidden layer of these architectures I won't go into the",
    "start": "3030990",
    "end": "3036720"
  },
  {
    "text": "details but these involve water known as tensor train decompositions and intuitively what they're doing is",
    "start": "3036720",
    "end": "3043609"
  },
  {
    "text": "incorporating higher-order statistics of the past for predicting the future and",
    "start": "3043609",
    "end": "3050490"
  },
  {
    "text": "now we can ask how are they useful how are they better than and 'red RNN architectures and what we",
    "start": "3050490",
    "end": "3059520"
  },
  {
    "text": "found is it is much more effective for long term forecasting so in the traffic",
    "start": "3059520",
    "end": "3065760"
  },
  {
    "text": "data set what we want to predict is how the traffic changes in the next hour or",
    "start": "3065760",
    "end": "3070830"
  },
  {
    "text": "next two hours next four hours and so on so especially if you want a predict way",
    "start": "3070830",
    "end": "3076560"
  },
  {
    "text": "into the future it turns out that the tensile STM that incorporates tensors",
    "start": "3076560",
    "end": "3082050"
  },
  {
    "text": "has much lower error compared to the standard architectures and within the",
    "start": "3082050",
    "end": "3088200"
  },
  {
    "text": "same with climate data set and found the same so tensors can more effectively",
    "start": "3088200",
    "end": "3093420"
  },
  {
    "text": "incorporate higher order information from the past and help us do better long",
    "start": "3093420",
    "end": "3099270"
  },
  {
    "text": "term forecasting so tensors can also incorporate multiple modalities so if",
    "start": "3099270",
    "end": "3106920"
  },
  {
    "text": "your input has different kinds of data right so in this example you have both",
    "start": "3106920",
    "end": "3113580"
  },
  {
    "text": "the image and the text as input we can now encode them more effectively using",
    "start": "3113580",
    "end": "3119520"
  },
  {
    "text": "tensors so in this example here I've shown the task of visual question and",
    "start": "3119520",
    "end": "3124560"
  },
  {
    "text": "answering where you have an image you have a question so the question here is",
    "start": "3124560",
    "end": "3130200"
  },
  {
    "text": "what is the mustache made of you expect the answer to be banana and now the",
    "start": "3130200",
    "end": "3135630"
  },
  {
    "text": "thing is can the deep learning system automatically come up with this answer",
    "start": "3135630",
    "end": "3140820"
  },
  {
    "text": "and the magic sauce is this pooling",
    "start": "3140820",
    "end": "3146160"
  },
  {
    "text": "right so how do we combine both the text and image features together to come up",
    "start": "3146160",
    "end": "3151740"
  },
  {
    "text": "with an efficient way to answer the question and so this is where we are",
    "start": "3151740",
    "end": "3158190"
  },
  {
    "text": "developing frameworks that are more water known as tensor sketches that can incorporate the three-dimensional",
    "start": "3158190",
    "end": "3164880"
  },
  {
    "text": "information that's present in image features as well as the text information",
    "start": "3164880",
    "end": "3169970"
  },
  {
    "text": "that's extracted through LST m or some of the sequence models and combine them",
    "start": "3169970",
    "end": "3176670"
  },
  {
    "text": "very effectively I'll skip the details here but intuitively tensors are natural",
    "start": "3176670",
    "end": "3184100"
  },
  {
    "text": "structures to combine different modalities together because those can just add",
    "start": "3184100",
    "end": "3190349"
  },
  {
    "text": "additional dimensions very seamlessly so",
    "start": "3190349",
    "end": "3195839"
  },
  {
    "text": "overall you know through this I wanted to give up an idea of different",
    "start": "3195839",
    "end": "3200849"
  },
  {
    "text": "operations and the use of tensors and different kinds of neural network",
    "start": "3200849",
    "end": "3207299"
  },
  {
    "text": "architectures and we've been building them into a framework called sensibly",
    "start": "3207299",
    "end": "3212489"
  },
  {
    "text": "it's an open source framework that was developed by John Joseph you also",
    "start": "3212489",
    "end": "3217789"
  },
  {
    "text": "interned with our group and it's now has an active developer community and this",
    "start": "3217789",
    "end": "3225630"
  },
  {
    "text": "has multiple backends so you started with numpy but now for scalability you",
    "start": "3225630",
    "end": "3232170"
  },
  {
    "text": "can use MX net or PI torch and the benefit is you can combine it with deep",
    "start": "3232170",
    "end": "3237299"
  },
  {
    "text": "learning so it is very easy to define the neural network architectures that I",
    "start": "3237299",
    "end": "3242880"
  },
  {
    "text": "described using tense early because you can describe tensor operations that are",
    "start": "3242880",
    "end": "3248700"
  },
  {
    "text": "built into this framework and you can describe your usual deep learning operations using MX net or PI torch and",
    "start": "3248700",
    "end": "3256799"
  },
  {
    "text": "put the two together and train n to end and so I encourage you to check out",
    "start": "3256799",
    "end": "3263160"
  },
  {
    "text": "there are also example notebooks available in the repository that you can run and develop further so I want to",
    "start": "3263160",
    "end": "3274049"
  },
  {
    "text": "conclude I you know had several themes in this talk the first one was on aw Sh",
    "start": "3274049",
    "end": "3282150"
  },
  {
    "text": "maker we are very excited with the launch we believe that this will be the",
    "start": "3282150",
    "end": "3287880"
  },
  {
    "text": "quickest and the easiest way for data scientist to do machine learning on AWS",
    "start": "3287880",
    "end": "3294720"
  },
  {
    "text": "it's a serverless framework it's easy to deploy I showed you notebooks that where",
    "start": "3294720",
    "end": "3301769"
  },
  {
    "text": "you can easily launch various machine learning jobs and also we have an",
    "start": "3301769",
    "end": "3307259"
  },
  {
    "text": "extensive array of first party algorithms that we've extensively developed and tested and benchmarked and",
    "start": "3307259",
    "end": "3315319"
  },
  {
    "text": "the explanations of those algorithms are present in the notebooks",
    "start": "3315319",
    "end": "3321359"
  },
  {
    "text": "so in particular I talked about the topic modeling so topic modeling is the",
    "start": "3321359",
    "end": "3326849"
  },
  {
    "text": "unsupervised framework for automatically categorizing documents you know tagging",
    "start": "3326849",
    "end": "3333059"
  },
  {
    "text": "each documents with a set of topics as well as tagging each topic with",
    "start": "3333059",
    "end": "3338339"
  },
  {
    "text": "representative words you know for each topic and we saw that the LDA algorithm",
    "start": "3338339",
    "end": "3347609"
  },
  {
    "text": "in Sage Maker it's much faster and hence cheaper compared to other open source",
    "start": "3347609",
    "end": "3353099"
  },
  {
    "text": "frameworks like mallet and SPARC and at the same time we also saw that there is",
    "start": "3353099",
    "end": "3359369"
  },
  {
    "text": "good topic quality both from a human interpretability angle as well as through the power pairwise mutual",
    "start": "3359369",
    "end": "3366150"
  },
  {
    "text": "information measure and so lastly I also did a deep dive on how tensors are",
    "start": "3366150",
    "end": "3373349"
  },
  {
    "text": "applicable for learning the LDA model intuitively if you want to look at",
    "start": "3373349",
    "end": "3379859"
  },
  {
    "text": "higher order relationships of words co-occurring in documents you want to",
    "start": "3379859",
    "end": "3385499"
  },
  {
    "text": "use tensors and the statistic we used was the third order co-occurrence that",
    "start": "3385499",
    "end": "3392670"
  },
  {
    "text": "is how do triplets of words Corcoran documents and decomposing that tensor",
    "start": "3392670",
    "end": "3398009"
  },
  {
    "text": "leads to a fast algorithm for Lda and lastly kind the last section I",
    "start": "3398009",
    "end": "3405420"
  },
  {
    "text": "showed some quick examples of how you can utilize tensors in various deep",
    "start": "3405420",
    "end": "3411630"
  },
  {
    "text": "learning contexts you can use it to retain the 3d structure in your",
    "start": "3411630",
    "end": "3417709"
  },
  {
    "text": "convolutional models you can use it for long term forecasting in sequence models",
    "start": "3417709",
    "end": "3423690"
  },
  {
    "text": "and you can use it for incorporating multiple modalities such as in visual",
    "start": "3423690",
    "end": "3429509"
  },
  {
    "text": "question and answering and there's also the tensor lis open source package that",
    "start": "3429509",
    "end": "3435119"
  },
  {
    "text": "allows you to easily express these tensor functions and combine them with deep deep neural network training so if",
    "start": "3435119",
    "end": "3444660"
  },
  {
    "text": "I want to stop my talk here and thank you",
    "start": "3444660",
    "end": "3449028"
  },
  {
    "text": "you",
    "start": "3454410",
    "end": "3456470"
  }
]