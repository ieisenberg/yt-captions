[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "In this video, you'll see how you can build a big data\nanalytics pipeline using modern data architecture.",
    "start": "160",
    "end": "5290"
  },
  {
    "text": "With this data architecture, you can populate the data lake using\ndata buckets stored in Amazon Simple Storage Service (Amazon S3),",
    "start": "5822",
    "end": "12480"
  },
  {
    "text": "query data across multiple data stores from Amazon Athena, and build\nbusiness intelligence (BI)visualizations to develop insights using Amazon QuickSight.",
    "start": "12480",
    "end": "20883"
  },
  {
    "start": "21000",
    "end": "120000"
  },
  {
    "text": "Let's begin by going to AWS \nGlue to populate our data lake.",
    "start": "21934",
    "end": "25054"
  },
  {
    "text": "We'll add a crawler to extract the data from one \nof our S3 buckets and load it into the data lake,",
    "start": "26856",
    "end": "31336"
  },
  {
    "text": "which will also be stored in Amazon S3.",
    "start": "31336",
    "end": "33440"
  },
  {
    "text": "Let's give our crawler a name.",
    "start": "34487",
    "end": "35847"
  },
  {
    "text": "We'll keep the default options to \ncrawl all folders in our data store.",
    "start": "39405",
    "end": "42587"
  },
  {
    "text": "Here we'll choose the path for the \nS3 bucket that we want to populate.",
    "start": "44943",
    "end": "47948"
  },
  {
    "text": "Let's select an appropriate AWS Identity and \nAccess Management (AWS IAM) role for the crawler.",
    "start": "54637",
    "end": "60237"
  },
  {
    "text": "As you can see, there's many schedule options \nfor crawlers.",
    "start": "64354",
    "end": "67239"
  },
  {
    "text": "We'll have ours run on demand.",
    "start": "67239",
    "end": "68771"
  },
  {
    "text": "We'll create a new database \nto store the crawler's output.",
    "start": "72936",
    "end": "75495"
  },
  {
    "text": "Let's review our crawler \nspecifications and complete the setup.",
    "start": "81708",
    "end": "84466"
  },
  {
    "text": "Once it's ready, let's run our crawler.",
    "start": "86115",
    "end": "87875"
  },
  {
    "text": "As you can see, it successfully crawled our S3 \ndata bucket and added 15 tables to our database.",
    "start": "91534",
    "end": "96469"
  },
  {
    "text": "Let's view those tables.",
    "start": "98487",
    "end": "99673"
  },
  {
    "text": "Here we have the option to edit \nthe tables and their schema.",
    "start": "102000",
    "end": "104400"
  },
  {
    "text": "Let's adjust the column names for our \nperson table, which defaulted to numerals.",
    "start": "106659",
    "end": "110411"
  },
  {
    "text": "Let's move on and access the AWS Glue Studio.",
    "start": "115843",
    "end": "118699"
  },
  {
    "text": "Here we'll set up extract, \ntransform, and load (ETL) jobs",
    "start": "121165",
    "end": "124685"
  },
  {
    "text": "to convert the database tables into Apache Parquet file format so that\nwe can query them later on using Amazon Athena in our data pipeline.",
    "start": "124685",
    "end": "131457"
  },
  {
    "text": "Let's retain the default job options.",
    "start": "133826",
    "end": "135652"
  },
  {
    "text": "In the data source node, we'll select the \ndatabase we just created for our crawler data.",
    "start": "138000",
    "end": "142181"
  },
  {
    "text": "We'll need a different ETL job for each table \nin our database. Let’s start with this table.",
    "start": "143127",
    "end": "147287"
  },
  {
    "text": "In the transform node, we can \nadjust the data types as necessary.",
    "start": "151544",
    "end": "154921"
  },
  {
    "text": "In the data target node, we’ll select the Parquet format\nand specify the S3 bucket to store the output.",
    "start": "157645",
    "end": "162845"
  },
  {
    "text": "We'll use the same bucket \nthat contains the source data.",
    "start": "167277",
    "end": "169558"
  },
  {
    "text": "On the Job details tab, let's name \nthe job and choose its IAM role.",
    "start": "178226",
    "end": "181805"
  },
  {
    "text": "We'll disable job bookmarks so that the job \nprocesses our entire dataset every time.",
    "start": "189506",
    "end": "194226"
  },
  {
    "text": "Let's save and run the job.",
    "start": "195235",
    "end": "196515"
  },
  {
    "text": "From the Monitoring tab, we can confirm \nthat the job run was successful.",
    "start": "201496",
    "end": "204939"
  },
  {
    "text": "The job run succeeded.",
    "start": "206066",
    "end": "207265"
  },
  {
    "text": "Next, let’s visit our S3 bucket.",
    "start": "208393",
    "end": "210390"
  },
  {
    "text": "As you can see, we have a new \nfolder with our Parquet files.",
    "start": "214835",
    "end": "217604"
  },
  {
    "text": "Now let’s return to AWS Glue Studio.",
    "start": "220915",
    "end": "223178"
  },
  {
    "start": "225000",
    "end": "305000"
  },
  {
    "text": "We now need to create jobs for the other tables in \nour database to convert them into Parquet format.",
    "start": "225638",
    "end": "230007"
  },
  {
    "text": "For the purposes of this video, \nwe’ll do that behind the scenes.",
    "start": "230477",
    "end": "233221"
  },
  {
    "text": "Let’s move on to the next step,",
    "start": "235489",
    "end": "237249"
  },
  {
    "text": "which is to build a crawler to extract and \nsave our Parquet files in the data lake.",
    "start": "237249",
    "end": "241019"
  },
  {
    "text": "We'll create and name the crawler.",
    "start": "243000",
    "end": "244520"
  },
  {
    "text": "We'll choose many of the same specifications as for\nthe previously created crawler that mines our S3 data bucket.",
    "start": "250077",
    "end": "255357"
  },
  {
    "text": "However, this time we'll choose the \npath for the Parquet files only.",
    "start": "259645",
    "end": "262768"
  },
  {
    "text": "Let's also add a prefix so it is easy to recognize\nthese are Parquet files when reading the database tables.",
    "start": "281875",
    "end": "286981"
  },
  {
    "text": "Let's complete and run the crawler.",
    "start": "287885",
    "end": "289360"
  },
  {
    "text": "Once the crawler has completed its run,",
    "start": "298195",
    "end": "300195"
  },
  {
    "text": "we'll see the new folders with a \nParquet prefix in our database.",
    "start": "300195",
    "end": "303120"
  },
  {
    "start": "305000",
    "end": "405000"
  },
  {
    "text": "Now that we've populated our data lake,",
    "start": "306136",
    "end": "308056"
  },
  {
    "text": "the next step in building a data analytics \npipeline is to query our data sources.",
    "start": "308056",
    "end": "312073"
  },
  {
    "text": "Amazon Athena is an interactive query service that makes it\neasy to analyze data from multiple sources using standard SQL.",
    "start": "314350",
    "end": "320614"
  },
  {
    "text": "We'll execute queries via data source\nconnectors that run in AWS Lambda.",
    "start": "321000",
    "end": "325030"
  },
  {
    "text": "We'll create a data source \nconnection to Amazon DynamoDB.",
    "start": "325906",
    "end": "328873"
  },
  {
    "text": "Let's name our data source.",
    "start": "332421",
    "end": "333684"
  },
  {
    "text": "We'll create a new Lambda function \nfor this data source connection.",
    "start": "338066",
    "end": "340706"
  },
  {
    "text": "Let's follow the link to complete \nour function in AWS Lambda.",
    "start": "343057",
    "end": "346177"
  },
  {
    "text": "We’ll provide a name for the spill bucket \nwhere the function will store output data.",
    "start": "348000",
    "end": "351329"
  },
  {
    "text": "Let's also choose a name for the Athena catalog.  ",
    "start": "352226",
    "end": "354456"
  },
  {
    "text": "This will automatically become \nthe name for this Lambda function.",
    "start": "354727",
    "end": "357287"
  },
  {
    "text": "We'll retain all other default \nsettings and deploy the function.",
    "start": "358230",
    "end": "360950"
  },
  {
    "text": "From the AWS Lambda Deployments tab, \nwe can view the function’s status.",
    "start": "366407",
    "end": "370181"
  },
  {
    "text": "Now that the Lambda function \nis successfully deployed,",
    "start": "371798",
    "end": "374118"
  },
  {
    "text": "let’s return to the Athena data source \nconnection page and choose it from the list.",
    "start": "374118",
    "end": "377753"
  },
  {
    "text": "Now let’s connect the data source.",
    "start": "381917",
    "end": "383524"
  },
  {
    "text": "Our data source connection \nwas created successfully.",
    "start": "386317",
    "end": "388587"
  },
  {
    "text": "Let's go to the query editor \nto test the connection.",
    "start": "389089",
    "end": "391409"
  },
  {
    "text": "We'll query our DynamoDB database \nand a Parquet table in our data lake.",
    "start": "392417",
    "end": "395936"
  },
  {
    "text": "The query produced accurate results,",
    "start": "398946",
    "end": "400786"
  },
  {
    "text": "confirming the data source connector \nis linked to both databases.",
    "start": "400786",
    "end": "403649"
  },
  {
    "start": "405000",
    "end": "499000"
  },
  {
    "text": "Our data pipeline is now fully set up.",
    "start": "406209",
    "end": "408164"
  },
  {
    "text": "Next, we'll visualize information in the data lake using the\ninteractive business BI dashboards available in Amazon QuickSight.",
    "start": "409235",
    "end": "415488"
  },
  {
    "text": "Let's create a new analysis.",
    "start": "416522",
    "end": "417882"
  },
  {
    "text": "We'll use Athena, which hosts our data source \nconnections, as the source for our dataset.",
    "start": "420678",
    "end": "424918"
  },
  {
    "text": "Next, we’ll select the table with \nthe data we want to visualize.",
    "start": "428400",
    "end": "431464"
  },
  {
    "text": "We'll keep the default setting to import data to the Super-fast,  \nParallel, In-memory, Calculation Engine (SPICE).",
    "start": "432689",
    "end": "438606"
  },
  {
    "text": "Once the dataset is ready, we can create \nvisualizations from a range of visual types.",
    "start": "440918",
    "end": "445238"
  },
  {
    "text": "Let's visualize the ticket price sum as currency.",
    "start": "447447",
    "end": "450007"
  },
  {
    "text": "Let's add another visual, this \ntime using a vertical bar graph.",
    "start": "454205",
    "end": "457353"
  },
  {
    "text": "It's easy to choose what data \nto represent on the graph.",
    "start": "460226",
    "end": "462870"
  },
  {
    "text": "We can easily add another \ndimension, such as seat level data.",
    "start": "468417",
    "end": "471137"
  },
  {
    "text": "We can also change the visual type.",
    "start": "472939",
    "end": "474658"
  },
  {
    "text": "QuickSight Insights interprets the data added to visuals\nand automatically generates a summary of findings in plain language.",
    "start": "479687",
    "end": "485621"
  },
  {
    "text": "You've just seen how you can build a big data \nanalytics pipeline using modern data architecture.",
    "start": "486709",
    "end": "491050"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "492000",
    "end": "495127"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "495430",
    "end": "497360"
  }
]