[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "and again I'd like to welcome you today to our webcast introducing Amazon redshift with Ben Butler who is our",
    "start": "0",
    "end": "6750"
  },
  {
    "text": "presenter today then I'll hand it off to you Thank You Aurora hello everyone my",
    "start": "6750",
    "end": "11969"
  },
  {
    "text": "name is Ben Butler and on the worldwide public sector solutions architect and if",
    "start": "11969",
    "end": "17160"
  },
  {
    "text": "you have further questions later on after the webinar you can contact me at Butler be at amazon com when we",
    "start": "17160",
    "end": "24119"
  },
  {
    "text": "appreciate your time and interest in learning about red chef today we will introduce redshift and talk about why we",
    "start": "24119",
    "end": "29880"
  },
  {
    "text": "built it how we achieve the performance that we're going to talk about and its ease of use we'll start with the brief",
    "start": "29880",
    "end": "35880"
  },
  {
    "text": "overview of Amazon Web Services and database services in particular will then look at the reasons we create a",
    "start": "35880",
    "end": "42120"
  },
  {
    "text": "redshift and describe the services so this is a high-level overview of the",
    "start": "42120",
    "end": "48390"
  },
  {
    "text": "portfolio of web services that we offer built on top of our global infrastructure the compute storage and",
    "start": "48390",
    "end": "54840"
  },
  {
    "text": "data services are the core of our offering we then surround these services with the range of supporting components",
    "start": "54840",
    "end": "61109"
  },
  {
    "text": "like management tools network services and application augmentation services all of this is hosted with our global",
    "start": "61109",
    "end": "68250"
  },
  {
    "text": "data center footprint which encompasses nine independently operated clouds which we call AWS regions that allows you to",
    "start": "68250",
    "end": "75330"
  },
  {
    "text": "consume services without having to build out facilities or equipment there's a lot of equipment powering our web",
    "start": "75330",
    "end": "81119"
  },
  {
    "text": "services and in fact every day in WS adds enough server capacity to power amazoncom in 2003 when it was a 5",
    "start": "81119",
    "end": "89340"
  },
  {
    "text": "billion dollar enterprise and so let's do a quick review of our manage database",
    "start": "89340",
    "end": "96119"
  },
  {
    "text": "services we're just going to focus on these and then we'll go into redshift database services is a foundational",
    "start": "96119",
    "end": "102210"
  },
  {
    "start": "100000",
    "end": "212000"
  },
  {
    "text": "service of AWS database service customers are offered a broad choice for",
    "start": "102210",
    "end": "107369"
  },
  {
    "text": "their database platforms in daily enabling them to deploy the best data store for their youth case all database",
    "start": "107369",
    "end": "114360"
  },
  {
    "text": "services are fully managed which means they provide automated management of administrative function this enables",
    "start": "114360",
    "end": "121380"
  },
  {
    "text": "developers and administrators to focus on application functionality and performance rather than patching and",
    "start": "121380",
    "end": "127890"
  },
  {
    "text": "backup maintenance RDS is a fully managed sequel database service for oltp",
    "start": "127890",
    "end": "134190"
  },
  {
    "text": "workloads and it runs with 23 database engines Mike sequel Oracle and sequel",
    "start": "134190",
    "end": "139500"
  },
  {
    "text": "server and a high-performance durable configuration that can be provisioned in minutes and in a multi availability zone",
    "start": "139500",
    "end": "146700"
  },
  {
    "text": "or known as multi ad replication is a feature of this service designed for",
    "start": "146700",
    "end": "151830"
  },
  {
    "text": "production instances multi AZ provides a one-click disaster recovery to remote data center or availability zone for",
    "start": "151830",
    "end": "158819"
  },
  {
    "text": "fast failover and maximum data protection elastic cash is a fully managed memcache d compliant in-memory",
    "start": "158819",
    "end": "165959"
  },
  {
    "text": "caching service that allows you to offload rira queries a full of in-memory",
    "start": "165959",
    "end": "173519"
  },
  {
    "text": "clustered with with ec2 instances and then that we have dynamo DB which is a",
    "start": "173519",
    "end": "179819"
  },
  {
    "text": "fully managed new sequel service running on solid state hard drives for massively scalable high throughput low latency",
    "start": "179819",
    "end": "186930"
  },
  {
    "text": "workloads and then we'll talk about redshift which is a fully managed fast",
    "start": "186930",
    "end": "192540"
  },
  {
    "text": "and powerful petabyte scale database where health service which will be the focus with today's webinar and it's a",
    "start": "192540",
    "end": "199109"
  },
  {
    "text": "warehouse that's visited specifically enhanced for the cloud and was developed after many customers requested a fully",
    "start": "199109",
    "end": "205709"
  },
  {
    "text": "managed high scale option for analytical workloads so for anyone who had",
    "start": "205709",
    "end": "214829"
  },
  {
    "start": "212000",
    "end": "252000"
  },
  {
    "text": "experience with traditional where data where health services knows that it's really expensive and really complicated",
    "start": "214829",
    "end": "220859"
  },
  {
    "text": "to manage you see that on that piece of paper there how many things you have to go and figure out what to pay for you",
    "start": "220859",
    "end": "227069"
  },
  {
    "text": "have to get the licenses you might have to have multiple like need the number of cores you need to",
    "start": "227069",
    "end": "232709"
  },
  {
    "text": "multiply it gets expensive pretty quickly and yet as Gartner reported points out that enterprises averaged",
    "start": "232709",
    "end": "239819"
  },
  {
    "text": "three to four DBA per data warehouse that they have to hire for each data",
    "start": "239819",
    "end": "245670"
  },
  {
    "text": "warehouse that you have so it's really expensive and really complex for tuning and administration so customers have",
    "start": "245670",
    "end": "253019"
  },
  {
    "start": "252000",
    "end": "281000"
  },
  {
    "text": "told us for years that they're unhappy with these types of solutions first of all large companies feel that they're",
    "start": "253019",
    "end": "258389"
  },
  {
    "text": "paying too much and that these data warehouses are difficult to manage and too hard to scale and for smaller",
    "start": "258389",
    "end": "263610"
  },
  {
    "text": "companies although has never been easier than today or less expensive to collect",
    "start": "263610",
    "end": "268680"
  },
  {
    "text": "and analyze data because of the cloud they can't afford these expensive data warehouse solutions the store this data",
    "start": "268680",
    "end": "274830"
  },
  {
    "text": "long-term and they end up having to throw out a lot of data so customers",
    "start": "274830",
    "end": "282360"
  },
  {
    "start": "281000",
    "end": "357000"
  },
  {
    "text": "have asked us to look into this space and we thought to ourselves we would it take to do data warehousing the Amazon",
    "start": "282360",
    "end": "288960"
  },
  {
    "text": "Web Services way it would have to be easy to get started self-service be able",
    "start": "288960",
    "end": "295320"
  },
  {
    "text": "to scale up massively we wouldn't want anyone to pay up front and hate and have",
    "start": "295320",
    "end": "300900"
  },
  {
    "text": "a pay-as-you-go model this cost model is also almost as important as having a low",
    "start": "300900",
    "end": "305910"
  },
  {
    "text": "price and on-demand utility style pricing is always popular is always a",
    "start": "305910",
    "end": "311280"
  },
  {
    "text": "popular request and it has to have really fast performance at a really low price of ease of use so these",
    "start": "311280",
    "end": "317190"
  },
  {
    "text": "requirements were generally in response to price performance as a value that customers wanted and felt we're not",
    "start": "317190",
    "end": "322889"
  },
  {
    "text": "getting from other options so we have this service that we need to be able to have it open and flexible and work with",
    "start": "322889",
    "end": "329849"
  },
  {
    "text": "popular business intelligent elegent tools that people are using because there's a data warehouse ecosystem",
    "start": "329849",
    "end": "335729"
  },
  {
    "text": "already in place that's healthy with a lot of technology vendors and especially those who focus on data movement and",
    "start": "335729",
    "end": "341610"
  },
  {
    "text": "business intelligence and so we wanted to create a data warehouse in the same style as our RDS service and be able to",
    "start": "341610",
    "end": "347610"
  },
  {
    "text": "provide an open open framework through sequel ODBC and jdbc connections to be",
    "start": "347610",
    "end": "353159"
  },
  {
    "text": "able to build out redshift so",
    "start": "353159",
    "end": "358580"
  },
  {
    "start": "357000",
    "end": "404000"
  },
  {
    "text": "introducing our Amazon redshift is doing data warehousing the Amazon way it's",
    "start": "358580",
    "end": "363719"
  },
  {
    "text": "easily and rapidly available to analyze petabyte scales of data at one-tenth the",
    "start": "363719",
    "end": "369870"
  },
  {
    "text": "cost of traditional data warehouses and automated deployment and administration",
    "start": "369870",
    "end": "375240"
  },
  {
    "text": "and compatible with popular business tools redshift refers to the astronomical phenomenon which was used",
    "start": "375240",
    "end": "382469"
  },
  {
    "text": "by Edwin Hubble to determine the universe was expanding amazon redshift is a fast and easy to use fully managed",
    "start": "382469",
    "end": "389189"
  },
  {
    "text": "petabyte scale data warehouse service in the AWS cloud and is designed to help customers to make sense of an",
    "start": "389189",
    "end": "395939"
  },
  {
    "text": "ever-expanding universe of data and to function as a central component and a big data strategy so most data never",
    "start": "395939",
    "end": "407069"
  },
  {
    "start": "404000",
    "end": "473000"
  },
  {
    "text": "makes it to the warehouse and there's this large data analysis gap enterprise",
    "start": "407069",
    "end": "412259"
  },
  {
    "text": "data is growing over fifty percent yearly as applications and can automate data and also with the advent of cloud",
    "start": "412259",
    "end": "418319"
  },
  {
    "text": "computing be able to create and analyze data is growing an exponential rate much what more so than the speed or the size",
    "start": "418319",
    "end": "426539"
  },
  {
    "text": "of data warehousing it's only growing at a ten percent yearly clip and so most data is left on the floor so various",
    "start": "426539",
    "end": "434819"
  },
  {
    "text": "analysts have attempted to quantify this gap generated by the application and data that makes it way into the",
    "start": "434819",
    "end": "440639"
  },
  {
    "text": "analytical environment the general trend is that this gap is large and growing and people just can't capture and",
    "start": "440639",
    "end": "446520"
  },
  {
    "text": "process the following the data that they're generating so we're specifically designing redshift to eliminate two of",
    "start": "446520",
    "end": "453389"
  },
  {
    "text": "those major barriers to broad adoptions of analytic effort and cost operational",
    "start": "453389",
    "end": "459990"
  },
  {
    "text": "data because it doesn't find his way off to the floor it's not being used and",
    "start": "459990",
    "end": "465710"
  },
  {
    "text": "we're trying to flip that script and be able to have just enable that so we set out to build",
    "start": "465710",
    "end": "475250"
  },
  {
    "text": "this fast powerful petabyte scale data warehouse that's a lot faster our gold ten times faster a lot cheaper 10 times",
    "start": "475250",
    "end": "483290"
  },
  {
    "text": "cheaper really used easy to use and it is our fastest growing AWS service it",
    "start": "483290",
    "end": "489950"
  },
  {
    "text": "was conceived with the core set of objectives of being the faster cheaper and easier use than any other data",
    "start": "489950",
    "end": "495740"
  },
  {
    "text": "warehouse alternative to achieve these goals every aspect of the service has",
    "start": "495740",
    "end": "500810"
  },
  {
    "text": "been optimized for data warehouse workload with extensive use of automation io efficiency is especially",
    "start": "500810",
    "end": "507230"
  },
  {
    "text": "important for performance and eliminating unnecessary io has been an area of great focus for the redshift",
    "start": "507230",
    "end": "512990"
  },
  {
    "text": "team reduce the goal is to reduce its undifferentiated heavy lifting so in a",
    "start": "512990",
    "end": "519320"
  },
  {
    "text": "data warehouse we want the customers to increase the differential offered by",
    "start": "519320",
    "end": "524480"
  },
  {
    "text": "focusing on the uniqueness of their application rather than spending time on back up on trying to scale patch work on",
    "start": "524480",
    "end": "533450"
  },
  {
    "text": "the operating system or other hardware administration we want to lead that month to Amazon to work with with",
    "start": "533450",
    "end": "539480"
  },
  {
    "text": "redshift so Apple Amazon joining the",
    "start": "539480",
    "end": "545840"
  },
  {
    "start": "542000",
    "end": "580000"
  },
  {
    "text": "fray here with data warehousing how are we increasing performance that's not already being able to done already done",
    "start": "545840",
    "end": "551870"
  },
  {
    "text": "before and we're focusing really on Io so in Moore's law memory capacity and",
    "start": "551870",
    "end": "558920"
  },
  {
    "text": "cpu performance double about every two years but there is a lag with hard disk",
    "start": "558920",
    "end": "565010"
  },
  {
    "text": "and they have mechanical constraints and so performance doubles only about every 10 years so because of that the",
    "start": "565010",
    "end": "571640"
  },
  {
    "text": "input/output operations to disk they dominate data warehouse performance",
    "start": "571640",
    "end": "576740"
  },
  {
    "text": "today in terms of bottleneck so we attack this in several different ways",
    "start": "576740",
    "end": "584000"
  },
  {
    "start": "580000",
    "end": "632000"
  },
  {
    "text": "when which will go through about how do we dramatically reduce this i/o first of",
    "start": "584000",
    "end": "589340"
  },
  {
    "text": "all a significant factor is ren chip high performances in the use of columnar",
    "start": "589340",
    "end": "595760"
  },
  {
    "text": "storage which is optimized for scan operation operations because one of those important factors",
    "start": "595760",
    "end": "602200"
  },
  {
    "text": "and bottlenecks and columnar technology helps reducing i/o need and and only",
    "start": "602200",
    "end": "607900"
  },
  {
    "text": "concentrate on the i/o that is considered that you need for your",
    "start": "607900",
    "end": "613270"
  },
  {
    "text": "queries instead of scanning every single item some queries when you scan you",
    "start": "613270",
    "end": "618400"
  },
  {
    "text": "throw away three-fourths of the data that you're able to scan because you only need some section of it for your actual query and so also what we do in",
    "start": "618400",
    "end": "627339"
  },
  {
    "text": "the column or storage is that we're able to do data compression and being able to",
    "start": "627339",
    "end": "634240"
  },
  {
    "start": "632000",
    "end": "702000"
  },
  {
    "text": "do data compression of we have we ought",
    "start": "634240",
    "end": "640120"
  },
  {
    "text": "we augment how each of those fields are put into redshift and we look at the",
    "start": "640120",
    "end": "646000"
  },
  {
    "text": "data and analyze it and then determine the most optimal compression mechanism to be able to place on those roads and",
    "start": "646000",
    "end": "653680"
  },
  {
    "text": "by doing that of we can get typical data workloads to be two to four times",
    "start": "653680",
    "end": "659650"
  },
  {
    "text": "smaller than their traditional relational database and indexes are not required in red shift because the",
    "start": "659650",
    "end": "666010"
  },
  {
    "text": "commoner storage is already physically optimized for scan operations this relative savings can be even greater",
    "start": "666010",
    "end": "673060"
  },
  {
    "text": "sometimes four to eight times in data compression so afford terabyte traditional data warehouse service of",
    "start": "673060",
    "end": "680470"
  },
  {
    "text": "storage can be one terabyte in red chip so you get these compression benefits",
    "start": "680470",
    "end": "686520"
  },
  {
    "text": "and similar data of all these columns that are stored together and take",
    "start": "686520",
    "end": "692950"
  },
  {
    "text": "advantage and to use this compression and use different types of compression to enable efficiencies and then we use",
    "start": "692950",
    "end": "703510"
  },
  {
    "start": "702000",
    "end": "741000"
  },
  {
    "text": "zone maps so zone maps allow you to keep track of allows us to keep track of the",
    "start": "703510",
    "end": "709900"
  },
  {
    "text": "minimum maximum value of each block that stored on disk for the columns and so",
    "start": "709900",
    "end": "715660"
  },
  {
    "text": "when you run queries you can skip over these blocks that don't contain the data needed for a given query",
    "start": "715660",
    "end": "721030"
  },
  {
    "text": "so when I mentioned earlier where instead of having to scan the whole database to get all the result sets and",
    "start": "721030",
    "end": "726490"
  },
  {
    "text": "then filter off of it we can skip entire blocks that don't matter for that",
    "start": "726490",
    "end": "731650"
  },
  {
    "text": "particular query and so that enables us to minimize unnecessary i/o and",
    "start": "731650",
    "end": "741810"
  },
  {
    "start": "741000",
    "end": "797000"
  },
  {
    "text": "furthermore we also use direct attached storage and large block sizes so this",
    "start": "741810",
    "end": "748330"
  },
  {
    "text": "direct attached storage and I'll get into how is architected with the types of storage that we offer on the side but",
    "start": "748330",
    "end": "754210"
  },
  {
    "text": "it's directly attached to the redshift servers to not using the network for local storage instead using local",
    "start": "754210",
    "end": "761530"
  },
  {
    "text": "storage and using large block sizes of one megabyte and so we have high performance I owed rise and the CPU",
    "start": "761530",
    "end": "768070"
  },
  {
    "text": "memory all local in each of the compute nodes and we take care of all the local management of resources and so when we",
    "start": "768070",
    "end": "774730"
  },
  {
    "text": "do a read or force can we get as much relevant data as possible with large",
    "start": "774730",
    "end": "780070"
  },
  {
    "text": "block sizes and and this enables it to get the most performance out of the system and then lastly Amazon manages",
    "start": "780070",
    "end": "789340"
  },
  {
    "text": "the durability of the data that's being stored on these local drives for you",
    "start": "789340",
    "end": "795630"
  },
  {
    "start": "797000",
    "end": "900000"
  },
  {
    "text": "so let's take a look at the Amazon redshift architecture so you would connect to the cluster using a JDBC or",
    "start": "797860",
    "end": "805790"
  },
  {
    "text": "odbc client I can drive our connection to a leader node so this leader node is",
    "start": "805790",
    "end": "812089"
  },
  {
    "text": "the sequel endpoint and it's the only endpoint into the redshift cluster this",
    "start": "812089",
    "end": "817790"
  },
  {
    "text": "leader node stores metadata it coordinates the query execution what it",
    "start": "817790",
    "end": "823579"
  },
  {
    "text": "also does is that it distributes job of to the compute nodes that are running so",
    "start": "823579",
    "end": "830959"
  },
  {
    "text": "you can have one one or more compute nodes and then the leader node distributes tasks to this compute nodes",
    "start": "830959",
    "end": "839029"
  },
  {
    "text": "and two the compute nodes they have local columnar storage and they execute all the queries in parallel and I'll get",
    "start": "839029",
    "end": "845870"
  },
  {
    "text": "into a little bit about Amazon architecture and doing the massively parallel programming architecture and",
    "start": "845870",
    "end": "853550"
  },
  {
    "text": "and this allows you each of the compute nodes to work on these instances",
    "start": "853550",
    "end": "859060"
  },
  {
    "text": "independently and in parallel and then you can load this data from amazon s3",
    "start": "859060",
    "end": "864740"
  },
  {
    "text": "our simple storage service or also be able to load this for a dynamo dB using",
    "start": "864740",
    "end": "872630"
  },
  {
    "text": "a copy data table to the command and then there's also a single node version for our small extra large instance type",
    "start": "872630",
    "end": "879829"
  },
  {
    "text": "where you can use this where the leader note in the compute node are co-located and enables you the ability to test the",
    "start": "879829",
    "end": "887149"
  },
  {
    "text": "service out and to get started but then you can scale and we'll talk about being able to scale from the two terabytes to",
    "start": "887149",
    "end": "893690"
  },
  {
    "text": "the one point six petabytes so Amazon",
    "start": "893690",
    "end": "902240"
  },
  {
    "start": "900000",
    "end": "1010000"
  },
  {
    "text": "all red shift runs on optimized hardware and so this server infrastructure was",
    "start": "902240",
    "end": "908449"
  },
  {
    "text": "custom-built and was made into a new ec2 instance type and this instant type has",
    "start": "908449",
    "end": "914000"
  },
  {
    "text": "been made herbal as 82 inches called HS 18",
    "start": "914000",
    "end": "919850"
  },
  {
    "text": "extra-large which has 128 gigs of ram 16 cores 24 spindles and and with the",
    "start": "919850",
    "end": "929510"
  },
  {
    "text": "redshift you get 16 terabytes of compressed user storage and then again if that's a factor of four you could",
    "start": "929510",
    "end": "937459"
  },
  {
    "text": "actually have potentially a 64 terabyte storage on on that node and compressing",
    "start": "937459",
    "end": "942680"
  },
  {
    "text": "to 16 terabytes and then you also get a 2 gigabyte stand rate so we've taken",
    "start": "942680",
    "end": "948290"
  },
  {
    "text": "this instance type and then we've also taken 18 of it and so we call that the",
    "start": "948290",
    "end": "954440"
  },
  {
    "text": "HS 18 extra-large which has 16 gigs of ram two cores the three spindles and two",
    "start": "954440",
    "end": "962000"
  },
  {
    "text": "terabytes of compressed customer storage and so this a smaller instance type HS",
    "start": "962000",
    "end": "969649"
  },
  {
    "text": "one extra-large is exactly 18 the size of the eight extra large and all the dimension and it's so it's linear as",
    "start": "969649",
    "end": "976550"
  },
  {
    "text": "well as the pricing so the extra-large is 18 the cost of the eight extra large",
    "start": "976550",
    "end": "982700"
  },
  {
    "text": "and then you can start small and grow big and grow either two terabytes or 16",
    "start": "982700",
    "end": "989240"
  },
  {
    "text": "terabytes at a time but it is when I talk about the cluster of the clusters",
    "start": "989240",
    "end": "994250"
  },
  {
    "text": "have to be homogenous meaning that the cluster has to have all of the extra",
    "start": "994250",
    "end": "999770"
  },
  {
    "text": "larges or all of the eight extra larges but you can switch between the two and we'll go through that when we talk about",
    "start": "999770",
    "end": "1005860"
  },
  {
    "text": "resizing and so a redshift paralyzes and",
    "start": "1005860",
    "end": "1014230"
  },
  {
    "text": "distributes everything is a clustered system and it's using a massively",
    "start": "1014230",
    "end": "1020110"
  },
  {
    "text": "parallel programming system the leader node is your endpoint to interact with the redshift cluster you can't remotely",
    "start": "1020110",
    "end": "1027010"
  },
  {
    "text": "log into it like SSH but what you do is you have a sequel endpoint that you get to with either a jade",
    "start": "1027010",
    "end": "1033490"
  },
  {
    "text": "DC or an ODBC driver and so this leader node then turns your sequel statement so",
    "start": "1033490",
    "end": "1041438"
  },
  {
    "text": "you submit sequel statements to the leader node and it turns those into an explain plan and op the tube for the",
    "start": "1041439",
    "end": "1048550"
  },
  {
    "text": "optimizer and the optimizer generates a query plan that uses statistics stored in the system tables on how the data is",
    "start": "1048550",
    "end": "1055870"
  },
  {
    "text": "physically laid out on all the computers and then the execution engine turns that",
    "start": "1055870",
    "end": "1061870"
  },
  {
    "text": "plan into compiled C++ code and then that code gets distributed to each of",
    "start": "1061870",
    "end": "1067660"
  },
  {
    "text": "those compute nodes in parallel so there is a small penalty while that compilation is in progress but what",
    "start": "1067660",
    "end": "1073750"
  },
  {
    "text": "happens is when you have a subsequent query that that same type and you're just changing the parameters all those",
    "start": "1073750",
    "end": "1080710"
  },
  {
    "text": "compiled code is already distributed throughout the system so running that query again you won't get that same",
    "start": "1080710",
    "end": "1087250"
  },
  {
    "text": "impact as you would if you query it the first time and so there's also a multiple threads on each of the compute",
    "start": "1087250",
    "end": "1093580"
  },
  {
    "text": "nodes in terms of slices and this enables us to divide and conquer the",
    "start": "1093580",
    "end": "1098800"
  },
  {
    "text": "workloads so this is a scale-out architecture and you can add more compute nodes up to our limit of 100 for",
    "start": "1098800",
    "end": "1105460"
  },
  {
    "text": "the large one and 32 for the small ones and so not only do you get more storage by adding more nodes but you're also",
    "start": "1105460",
    "end": "1112270"
  },
  {
    "text": "getting more compute and more memory of for that cluster",
    "start": "1112270",
    "end": "1117929"
  },
  {
    "start": "1121000",
    "end": "1287000"
  },
  {
    "text": "we also focus on load so we want to make loads parallel as well so we don't add",
    "start": "1121090",
    "end": "1127460"
  },
  {
    "text": "time loading a cluster if you have two nodes or if you have 20 nodes since they're all working at the same time",
    "start": "1127460",
    "end": "1132710"
  },
  {
    "text": "data can be loaded in the same time frame and also can be loaded by single",
    "start": "1132710",
    "end": "1139040"
  },
  {
    "text": "copy command either from the necessary just bucket or a dynamo DB table you can",
    "start": "1139040",
    "end": "1144890"
  },
  {
    "text": "use sequel insert statements as well but those are not done in parallel they're actually done through the leader node so",
    "start": "1144890",
    "end": "1151220"
  },
  {
    "text": "depending on your data requirements in your velocity of load you would want to",
    "start": "1151220",
    "end": "1156380"
  },
  {
    "text": "consider using the copy man to command instead of the best practice and so when",
    "start": "1156380",
    "end": "1162050"
  },
  {
    "text": "you load this into each compute node it's done in parallel and then each computing that has a process called a",
    "start": "1162050",
    "end": "1168080"
  },
  {
    "text": "flight which we talked about and so the extra-large has two slices and the eight extra large has 16 so when you're doing",
    "start": "1168080",
    "end": "1175730"
  },
  {
    "text": "a low you should map the number of data files loading to your cluster to the number of available slices in the",
    "start": "1175730",
    "end": "1182090"
  },
  {
    "text": "cluster so that each slice so each computing process is taking a piece of",
    "start": "1182090",
    "end": "1187520"
  },
  {
    "text": "the data loading it in parallel and then all of the nodes are connected by 10 gigabit ethernet fiber are the ethernet",
    "start": "1187520",
    "end": "1194270"
  },
  {
    "text": "network and so the data needs to be distributed between the nodes it will happen automatically and expeditiously",
    "start": "1194270",
    "end": "1200300"
  },
  {
    "text": "and one part of the columnar system is that whenever you need data that you",
    "start": "1200300",
    "end": "1205310"
  },
  {
    "text": "that needs to be joined another best practices you want to store the data on",
    "start": "1205310",
    "end": "1210830"
  },
  {
    "text": "the same node that you're joining so that the jones joins our node local and they'll perform better than having joins",
    "start": "1210830",
    "end": "1217490"
  },
  {
    "text": "that distribute across the network of monks and multiple compute nodes and so",
    "start": "1217490",
    "end": "1223220"
  },
  {
    "text": "because of that one of the options that you have when you create tables is your choice of a distribution key this is how",
    "start": "1223220",
    "end": "1230210"
  },
  {
    "text": "you physically layout the data so for example if you have a customer ID for an account and the customers need to add",
    "start": "1230210",
    "end": "1236540"
  },
  {
    "text": "and then the customer also has an account transitions and transaction you may want",
    "start": "1236540",
    "end": "1242760"
  },
  {
    "text": "to distribute your data by that customer ID so you have to do joins on ads and",
    "start": "1242760",
    "end": "1248280"
  },
  {
    "text": "transactions then all of those joins is no local and you'll get better performance in terms of reduced query",
    "start": "1248280",
    "end": "1254160"
  },
  {
    "text": "time and all of this is done behind the scenes so what you have what you do is you pick the number of nodes define your",
    "start": "1254160",
    "end": "1260669"
  },
  {
    "text": "tables and your distribution and sort keys and then load your data and as I said before you can load it with the",
    "start": "1260669",
    "end": "1266700"
  },
  {
    "text": "sequel insert command but it's better to do it through the copy command so there's a data processing pipeline where",
    "start": "1266700",
    "end": "1273980"
  },
  {
    "text": "you format the data to put it either into dynamo or s3 and then do the massively parallel copy into red chips",
    "start": "1273980",
    "end": "1284299"
  },
  {
    "start": "1287000",
    "end": "1458000"
  },
  {
    "text": "we also invested a lot of time in a continuous backup and easy restore",
    "start": "1287270",
    "end": "1292530"
  },
  {
    "text": "process backups are done at the node level they're done in parallel our automatic and incremental so anytime",
    "start": "1292530",
    "end": "1300150"
  },
  {
    "text": "data changes in the cluster will kick off a backup process to asynchronously copy data to s3 they're not only reusing",
    "start": "1300150",
    "end": "1307950"
  },
  {
    "text": "s3 as a loading point we're also using it as a way to back up the whole cluster and so we only copy that data to s3 for",
    "start": "1307950",
    "end": "1317700"
  },
  {
    "text": "data that's changed so if nothing's changing your cluster you won't see any additional snaps two shots in s3 only",
    "start": "1317700",
    "end": "1324390"
  },
  {
    "text": "when you do incremental and so there's also a system snapshot period and that's",
    "start": "1324390",
    "end": "1329550"
  },
  {
    "text": "configurable the default is one day but you can set it up to be up to 35 days and then we'll just age out the data",
    "start": "1329550",
    "end": "1336870"
  },
  {
    "text": "that that time windows slides past your your your day your time and so you can",
    "start": "1336870",
    "end": "1343380"
  },
  {
    "text": "also create user snapshots than any time so a snapshot basically gives you a",
    "start": "1343380",
    "end": "1348630"
  },
  {
    "text": "black bloc copy of your entire cluster and so you can freeze that cluster state and enables you to create an exact copy",
    "start": "1348630",
    "end": "1355920"
  },
  {
    "text": "of it and so you can trigger that snapshot manually either with the console or API call or you can automate",
    "start": "1355920",
    "end": "1362760"
  },
  {
    "text": "it using a script that does that and that frozen cluster state will be retained until you explicitly delete it",
    "start": "1362760",
    "end": "1369059"
  },
  {
    "text": "so if you ever do eat a cluster you'll also get a an option to do a final",
    "start": "1369059",
    "end": "1374460"
  },
  {
    "text": "snapshot of it so when you delete the cluster you you release all the compute nodes and the leader nodes of that",
    "start": "1374460",
    "end": "1381150"
  },
  {
    "text": "cluster so you're not incurring charges but if you want to reload that cluster to have the exact same state before you",
    "start": "1381150",
    "end": "1388020"
  },
  {
    "text": "release it or terminated it then that's why you want to use those manual or",
    "start": "1388020",
    "end": "1393390"
  },
  {
    "text": "final snapshot another feature that we offer is that we have large instant",
    "start": "1393390",
    "end": "1400050"
  },
  {
    "text": "types with 16 terabytes of storage and so if you're restoring from a cluster",
    "start": "1400050",
    "end": "1406140"
  },
  {
    "text": "you don't want to have to wait for all the 16 terabytes load before you can start doing your queries so we spent a",
    "start": "1406140",
    "end": "1412890"
  },
  {
    "text": "lot of time and optimizing and doing a streaming door which is also coming from s3 so",
    "start": "1412890",
    "end": "1419119"
  },
  {
    "text": "what we have what we do is as you're making requests we're paging data from",
    "start": "1419119",
    "end": "1424249"
  },
  {
    "text": "s3 to the data that's needed and this is also all done automatic and behind the Z",
    "start": "1424249",
    "end": "1429619"
  },
  {
    "text": "as part of this managed service and so what we've seen depending on time",
    "start": "1429619",
    "end": "1434809"
  },
  {
    "text": "related data that we for query once you start getting three to five percent of",
    "start": "1434809",
    "end": "1440149"
  },
  {
    "text": "that data streaming and restored from s3 you for most of our customers workloads",
    "start": "1440149",
    "end": "1445749"
  },
  {
    "text": "that starting to work of using the streaming restore and not having to wait",
    "start": "1445749",
    "end": "1450889"
  },
  {
    "text": "for the full node of each of the compute nodes to load up with data and now we'll",
    "start": "1450889",
    "end": "1460039"
  },
  {
    "start": "1458000",
    "end": "1614000"
  },
  {
    "text": "also talk about resize at redshift make scaling up and scaling down very easy",
    "start": "1460039",
    "end": "1465860"
  },
  {
    "text": "the resize experience and redshift has been very popular with anyone who's been through a scale up exercise with the",
    "start": "1465860",
    "end": "1472580"
  },
  {
    "text": "traditional data where health technology planning and preparation using traditional technology often takes weeks",
    "start": "1472580",
    "end": "1479570"
  },
  {
    "text": "and when it time to execute that effort is equivalent to a full data store migration but behind the scenes of what",
    "start": "1479570",
    "end": "1486590"
  },
  {
    "text": "we're doing a bread shift what we do is we place the what we call save the",
    "start": "1486590",
    "end": "1492080"
  },
  {
    "text": "original cluster in a read-only mode and then what we do in parallel is operate a",
    "start": "1492080",
    "end": "1499429"
  },
  {
    "text": "new cluster a newly requested cluster to the target size and so it's important to",
    "start": "1499429",
    "end": "1505369"
  },
  {
    "text": "note when you want to downsize your cluster you want to make sure your new target cluster is large enough to hold",
    "start": "1505369",
    "end": "1512509"
  },
  {
    "text": "with the size of your original cluster so you may have several nodes at sixty",
    "start": "1512509",
    "end": "1517940"
  },
  {
    "text": "four terabytes you want to go down to 32 terabytes you just want to make sure your original cluster is less than",
    "start": "1517940",
    "end": "1524179"
  },
  {
    "text": "thirty two terabytes in size to be able to scale down but the data is again copied in parallel from one computer to",
    "start": "1524179",
    "end": "1531799"
  },
  {
    "text": "the net to the other compute node from current cluster to the new one and that rate of transfer is limited by the",
    "start": "1531799",
    "end": "1538350"
  },
  {
    "text": "smaller of the two clusters so that depends if you're you're up sizing or downsizing a cluster and and so clients",
    "start": "1538350",
    "end": "1545870"
  },
  {
    "text": "continue to communicate with the leader node of the original cluster while the new cluster is being built out and then",
    "start": "1545870",
    "end": "1553020"
  },
  {
    "text": "moving the data over and so the leader node of the of the original cluster is",
    "start": "1553020",
    "end": "1558030"
  },
  {
    "text": "available for read queries but the rights are suspended during this resize process and so we're pausing the rights",
    "start": "1558030",
    "end": "1564990"
  },
  {
    "text": "we're creating that new cluster we're copying everything over to the new cluster and when that process is",
    "start": "1564990",
    "end": "1570030"
  },
  {
    "text": "finished we do then do a dns switch and your applications don't have to change",
    "start": "1570030",
    "end": "1575340"
  },
  {
    "text": "and you're able to move from from your old cluster to your new cluster",
    "start": "1575340",
    "end": "1581670"
  },
  {
    "text": "seamlessly and by the way you don't pay for the new cluster until that dns which",
    "start": "1581670",
    "end": "1588450"
  },
  {
    "text": "has happened and then we decommission this source cluster and get that out of the way and this is also a simple",
    "start": "1588450",
    "end": "1595380"
  },
  {
    "text": "operation via the AWS console or using the api so this enables you to resize",
    "start": "1595380",
    "end": "1603990"
  },
  {
    "text": "and scale up and scale down your cluster with the only requirement making sure that the new cluster can contain the new",
    "start": "1603990",
    "end": "1610680"
  },
  {
    "text": "data so talking about scalability so",
    "start": "1610680",
    "end": "1617130"
  },
  {
    "start": "1614000",
    "end": "1741000"
  },
  {
    "text": "from the scalability perspective we work hard to make it easy to scale up as your",
    "start": "1617130",
    "end": "1622770"
  },
  {
    "text": "knees required already talked about the resize you can start with that single extra-large node and it has two",
    "start": "1622770",
    "end": "1629070"
  },
  {
    "text": "terabytes of compressed storage on it and then you can cluster a maximum of 32 of these extra large nodes at a time so",
    "start": "1629070",
    "end": "1637050"
  },
  {
    "text": "that gives you 60 4 terabytes of total storage if you do all 32 and again you",
    "start": "1637050",
    "end": "1642330"
  },
  {
    "text": "get with the smaller node size you get the option of having a basically a single node cluster where the lead",
    "start": "1642330",
    "end": "1648770"
  },
  {
    "text": "node and the compute node is are co-located on that infant pipe but then",
    "start": "1648770",
    "end": "1654110"
  },
  {
    "text": "should you want to scale up you can scale up to the eight extra large in civ type which then gives you 16 terabytes",
    "start": "1654110",
    "end": "1661250"
  },
  {
    "text": "for that node and then you can Network a hundred of these and so with this the",
    "start": "1661250",
    "end": "1667610"
  },
  {
    "text": "smallest cluster you can have is two so that gives you the smaller size of a 32 terabyte cluster using the eight extra",
    "start": "1667610",
    "end": "1673430"
  },
  {
    "text": "large and you can have a hundred of those which gives you a 1.6 petabytes in",
    "start": "1673430",
    "end": "1679490"
  },
  {
    "text": "this is the petabyte scale of storage now we've already received a request from our current customers are using",
    "start": "1679490",
    "end": "1685520"
  },
  {
    "text": "redshift to increase this limit and so we're looking into that at the time",
    "start": "1685520",
    "end": "1692210"
  },
  {
    "text": "we're trying to take those requirements in and see what we can do to make it larger but it's also important to note",
    "start": "1692210",
    "end": "1699280"
  },
  {
    "text": "from a price perspective or a performance perspective there's no difference of running the eight extra",
    "start": "1699280",
    "end": "1705350"
  },
  {
    "text": "larges to one extra large it really comes down out what you want you're",
    "start": "1705350",
    "end": "1710720"
  },
  {
    "text": "scaling increment and your total size of your cluster to be because the eight extra large is exactly 18 of the large",
    "start": "1710720",
    "end": "1716870"
  },
  {
    "text": "is 18 of the extra-large instance and one exactly one eighth of the cost so",
    "start": "1716870",
    "end": "1722960"
  },
  {
    "text": "we're also making it easier to determine what your cluster will cost you and then again each of these clusters that you",
    "start": "1722960",
    "end": "1729350"
  },
  {
    "text": "create have to be homogeneous of the instant site so you can have a cluster of eight extra larges and extra larges",
    "start": "1729350",
    "end": "1736610"
  },
  {
    "text": "but not a mix of the two and so talking",
    "start": "1736610",
    "end": "1743450"
  },
  {
    "start": "1741000",
    "end": "1949000"
  },
  {
    "text": "about price it were we're priced to let you analyze all of your data so we were",
    "start": "1743450",
    "end": "1748670"
  },
  {
    "text": "built we've built redshift to focus on the performance and the price and the ease of use and we want to make it easy",
    "start": "1748670",
    "end": "1754730"
  },
  {
    "text": "and cost-effective for our customers to analyze their data and want to make even having to throw your data away",
    "start": "1754730",
    "end": "1760430"
  },
  {
    "text": "because it was too expensive for the data warehouse and so pricing starts on",
    "start": "1760430",
    "end": "1765590"
  },
  {
    "text": "our extra large instance with two terabytes of storage at less than a dollar an hour so 85 cents to get",
    "start": "1765590",
    "end": "1772970"
  },
  {
    "text": "started for the on demand rate and then to if I don't have it on the chart but the eight extra large is exactly eight",
    "start": "1772970",
    "end": "1779450"
  },
  {
    "text": "times that to 680 per hour we also have reservation models if you're familiar",
    "start": "1779450",
    "end": "1785090"
  },
  {
    "text": "with ec2 um not only is it pays you go but the more you use and the more you",
    "start": "1785090",
    "end": "1791000"
  },
  {
    "text": "know you're going to use the more you can save and so how that works is you can prepay some upfront fees and get a",
    "start": "1791000",
    "end": "1797990"
  },
  {
    "text": "lower hourly rate because reservations give AWS insight into capacity needs and",
    "start": "1797990",
    "end": "1804080"
  },
  {
    "text": "so by reducing this uncertainty of the capacity needs it reduces our costs which we provide back those cost savings",
    "start": "1804080",
    "end": "1811400"
  },
  {
    "text": "to our customers in the form of reduced hourly rates for the reservation model",
    "start": "1811400",
    "end": "1816650"
  },
  {
    "text": "and so we have to type you can reserve instances for one or three years and so",
    "start": "1816650",
    "end": "1824530"
  },
  {
    "text": "you can see it's a little over thirty seven hundred dollars to do on demand /",
    "start": "1824530",
    "end": "1830210"
  },
  {
    "text": "terabytes if you have that two terabyte on that that's the price per terabyte but then you say forty percent if you do",
    "start": "1830210",
    "end": "1837440"
  },
  {
    "text": "a one-year reservation because the hourly rate goes from 85 cents to fifty cents and so you have a lower effective",
    "start": "1837440",
    "end": "1844610"
  },
  {
    "text": "of hourly price which is basically taking the upfront payment adding up the",
    "start": "1844610",
    "end": "1849980"
  },
  {
    "text": "new price per hour times number of hours to get the 2190 which is about the forty",
    "start": "1849980",
    "end": "1855200"
  },
  {
    "text": "percent price reduction and so we if you do the three year instead of a one year then you can get it down to under 23",
    "start": "1855200",
    "end": "1862730"
  },
  {
    "text": "cents an hour to run your a petabyte scale cluster at a per terabyte cost and",
    "start": "1862730",
    "end": "1868520"
  },
  {
    "text": "so they get you down to under a thousand dollars per terabyte per year and if you",
    "start": "1868520",
    "end": "1874309"
  },
  {
    "text": "look at the cost of typical data warehouses cost can run between 19 and 25,000 per terabyte per year so this is",
    "start": "1874309",
    "end": "1883010"
  },
  {
    "text": "a significant change in the pricing in terms of cost but also in the pricing",
    "start": "1883010",
    "end": "1888380"
  },
  {
    "text": "model of utility model page and go computing so this pricing model has already achieved",
    "start": "1888380",
    "end": "1894460"
  },
  {
    "text": "two main benefits for our customers one a dramatic reduction in cost and two",
    "start": "1894460",
    "end": "1901799"
  },
  {
    "text": "customers are now using data warehouses for more use cases than before and we'll go over some of the popular use cases",
    "start": "1901799",
    "end": "1908379"
  },
  {
    "text": "later on the presentation and so there's no charge for the leader knows that's",
    "start": "1908379",
    "end": "1913629"
  },
  {
    "text": "also another benefit so that's another compute instance we have that it's just",
    "start": "1913629",
    "end": "1919179"
  },
  {
    "text": "built into the hourly rate of your cluster and so if you want to calculate the cost of your cluster you just",
    "start": "1919179",
    "end": "1926409"
  },
  {
    "text": "calculate the hourly rate times the number of nodes um it's as simple as that there is no data transfer in and",
    "start": "1926409",
    "end": "1934029"
  },
  {
    "text": "out of redshift as a cost all those standards s3 and bpc transfer rates of",
    "start": "1934029",
    "end": "1939700"
  },
  {
    "text": "apply so you can look on our website for those pricing but it's so different than what you would do for normal s3 so",
    "start": "1939700",
    "end": "1950350"
  },
  {
    "start": "1949000",
    "end": "2034000"
  },
  {
    "text": "having to spend time installing hardware and software doing patching backups and scaling and tuning prevents our",
    "start": "1950350",
    "end": "1956769"
  },
  {
    "text": "customers from focusing on their applications or data and the insights and decision that they could drive from",
    "start": "1956769",
    "end": "1963009"
  },
  {
    "text": "being able to analyze that data so we want to reduce that time of installing",
    "start": "1963009",
    "end": "1969129"
  },
  {
    "text": "the hardware and software and and focus it more on the customers benefit and so",
    "start": "1969129",
    "end": "1974259"
  },
  {
    "text": "because of that we designed the right ship to be fast and easy to provision it takes about 15 minutes to provision and",
    "start": "1974259",
    "end": "1980409"
  },
  {
    "text": "I'm going to do a quick little demo and showing how that's done just a launch of a nominal wretched cluster and then",
    "start": "1980409",
    "end": "1988720"
  },
  {
    "text": "that's much faster right what I'm going to show you on screen is much faster than waiting weeks to provision",
    "start": "1988720",
    "end": "1994960"
  },
  {
    "text": "traditional data warehouse clusters okay and then we can monitor query performance to N and then you can spend",
    "start": "1994960",
    "end": "2002999"
  },
  {
    "text": "a lot of time developing the metrics and capo kissing on those and then um we",
    "start": "2002999",
    "end": "2009360"
  },
  {
    "text": "handled the disk failures the performance and the durability as part of the manage sir that's built into the hourly rate we're",
    "start": "2009360",
    "end": "2016409"
  },
  {
    "text": "doing the back up to f3 that's continuous and incremental and automatic and we have these api to command line",
    "start": "2016409",
    "end": "2023159"
  },
  {
    "text": "tools to control this so you can get this as a fully programmable data warehouse from your perspective so",
    "start": "2023159",
    "end": "2035220"
  },
  {
    "start": "2034000",
    "end": "2074000"
  },
  {
    "text": "provisioning a data warehouse in minutes and so there's four quick steps we're",
    "start": "2035220",
    "end": "2041450"
  },
  {
    "text": "I'll do a quick launch in just a second I'll share my screen but we have the cut use put in the cluster details the node",
    "start": "2041450",
    "end": "2048929"
  },
  {
    "text": "configuration additional configuration review it and then launch the cluster and then it's pretty much point and",
    "start": "2048929",
    "end": "2054690"
  },
  {
    "text": "click and and then again with the single extra large you can have one node and",
    "start": "2054690",
    "end": "2059929"
  },
  {
    "text": "then we also fully support VPC in identity and access management I am so",
    "start": "2059929",
    "end": "2066450"
  },
  {
    "text": "you can control who is allowed to describe or create their clusters so I",
    "start": "2066450",
    "end": "2075530"
  },
  {
    "start": "2074000",
    "end": "2257000"
  },
  {
    "text": "hopefully can see my screen they'll let me know if you can I'm on my console",
    "start": "2075530",
    "end": "2081179"
  },
  {
    "text": "right now in my account and I'm going to the redshift management console I've already launched a couple clusters which",
    "start": "2081179",
    "end": "2087480"
  },
  {
    "text": "will connect to in a little bit but I just want to go through the process of launching one and showing you how really",
    "start": "2087480",
    "end": "2092490"
  },
  {
    "text": "easy it is so you launch the cluster you give a cluster identifier I'll just call",
    "start": "2092490",
    "end": "2097530"
  },
  {
    "text": "it demo cluster and then i'll call it and give it a database name because this",
    "start": "2097530",
    "end": "2102599"
  },
  {
    "text": "is a first database you can add multiple databases at a later time but during the home during the initial consult you just",
    "start": "2102599",
    "end": "2111119"
  },
  {
    "text": "give it a database name then the database port is 5439 but you can also",
    "start": "2111119",
    "end": "2116550"
  },
  {
    "text": "change that and then you put a username and a password",
    "start": "2116550",
    "end": "2129859"
  },
  {
    "text": "hopefully I got that right let me let me just do a simple one here you get and",
    "start": "2131920",
    "end": "2146059"
  },
  {
    "text": "put in a password and then here you can pick the node type and so you have the",
    "start": "2146059",
    "end": "2151609"
  },
  {
    "text": "HS extra large HS one extra large and the eight extra large I'll just pick a nice extra large and then that defaults",
    "start": "2151609",
    "end": "2158779"
  },
  {
    "text": "to a multi-manager environment of if you do the HS one then you can have that",
    "start": "2158779",
    "end": "2163880"
  },
  {
    "text": "choice of a single note so i will just i'll do a multi-node i'll leave it as an",
    "start": "2163880",
    "end": "2170720"
  },
  {
    "text": "extra-large actually and i'll just pick two notes so you have to start out to maximum 16 here of you can increase this",
    "start": "2170720",
    "end": "2179029"
  },
  {
    "text": "there's a limit request of you can also so you can do 16 here and when you do",
    "start": "2179029",
    "end": "2184069"
  },
  {
    "text": "the 100 of you can also increase the limit there and so hit continue we'll",
    "start": "2184069",
    "end": "2189529"
  },
  {
    "text": "just leave these by default you can also launch it in a VPC you've got to create a cluster subnet group and that's in the",
    "start": "2189529",
    "end": "2196130"
  },
  {
    "text": "documentation how to set that up pretty easy and then you pick a security group just like an ec2 of where that's being",
    "start": "2196130",
    "end": "2203900"
  },
  {
    "text": "available you can also say which of the availability zones in your region that you want to deploy in i'll leave it no",
    "start": "2203900",
    "end": "2211130"
  },
  {
    "text": "preference for now and hit continue and then you review and so you're going to",
    "start": "2211130",
    "end": "2217760"
  },
  {
    "text": "start recurring charges it's giving you some examples here of your database",
    "start": "2217760",
    "end": "2224119"
  },
  {
    "text": "configuration your configuration information your security acts and access and then you hit launch cluster",
    "start": "2224119",
    "end": "2231520"
  },
  {
    "text": "and then you can view your cluster in your dashboard and it's in a creating status and this takes about 15 minutes",
    "start": "2231520",
    "end": "2238940"
  },
  {
    "text": "to a provision and then start loading data and so I already have a couple bear",
    "start": "2238940",
    "end": "2244640"
  },
  {
    "text": "i will show a connection of that a little bit but I just want to we'll get back to the",
    "start": "2244640",
    "end": "2250390"
  },
  {
    "text": "presentation so after your provision it",
    "start": "2250390",
    "end": "2260080"
  },
  {
    "start": "2257000",
    "end": "2370000"
  },
  {
    "text": "you start connecting to it and running queries against it you can monitor query performance which you know the common",
    "start": "2260080",
    "end": "2267070"
  },
  {
    "text": "use of your clusters to look at the performance and and we spent a lot of effort in developing our console to",
    "start": "2267070",
    "end": "2273430"
  },
  {
    "text": "provide you the information when your query it so that helped our customers get a sense of what resources are being",
    "start": "2273430",
    "end": "2278710"
  },
  {
    "text": "consumed in this example this is a time chart the horizontal bars of the queries",
    "start": "2278710",
    "end": "2283750"
  },
  {
    "text": "that have run on the cluster if you hover over them it shades the resource that you can see in that light blue that",
    "start": "2283750",
    "end": "2291670"
  },
  {
    "text": "what are the resources have been utilized when that query has been executed and so that pink is the leader",
    "start": "2291670",
    "end": "2297370"
  },
  {
    "text": "node and you see that the CPU utilization is very low because it's just doing the coordinating and that's",
    "start": "2297370",
    "end": "2303160"
  },
  {
    "text": "pretty good you'd be concerned if that was pegged high and then when it's running the query so you can see over",
    "start": "2303160",
    "end": "2308350"
  },
  {
    "text": "time we're running a lot of queries over a short period of time and so you see those two spikes of a two node cluster",
    "start": "2308350",
    "end": "2314860"
  },
  {
    "text": "of compute one and compute to you see that they're launching out nearly in parallel so that's giving you a good",
    "start": "2314860",
    "end": "2321790"
  },
  {
    "text": "indication that it's distributed pretty evenly if you were only seeing one line spike up that may be because of a",
    "start": "2321790",
    "end": "2328390"
  },
  {
    "text": "co-located join or that your data is distributed on it not in a non even",
    "start": "2328390",
    "end": "2335080"
  },
  {
    "text": "fashion so it gives you an insight as to what you need to do if you click on the",
    "start": "2335080",
    "end": "2341830"
  },
  {
    "text": "right there's a sequel statement so it shows you when you click the queries it shows you the sequel commands that you",
    "start": "2341830",
    "end": "2348310"
  },
  {
    "text": "operate it for each of those queries in this as a single statement and then how the leader node has converted that into",
    "start": "2348310",
    "end": "2354550"
  },
  {
    "text": "an explain plan that goes in detail of how it's creating other steps to run",
    "start": "2354550",
    "end": "2361900"
  },
  {
    "text": "that query and so you can look at those and then also see the metrics as well",
    "start": "2361900",
    "end": "2368700"
  },
  {
    "text": "and then the point-and-click resize and so this is very easy as well and",
    "start": "2369030",
    "end": "2378070"
  },
  {
    "start": "2370000",
    "end": "2456000"
  },
  {
    "text": "I'll just go ahead and do that I'm going to go back to my consult I'll hit share",
    "start": "2378070",
    "end": "2385060"
  },
  {
    "text": "my screen I've got this example cluster um it is only got one node and I'm just",
    "start": "2385060",
    "end": "2391360"
  },
  {
    "text": "going to click resize um I will say multi-node and will click three nodes",
    "start": "2391360",
    "end": "2397630"
  },
  {
    "text": "and hit Reese I oh I hit my quota so I've got that soft limit as well let's",
    "start": "2397630",
    "end": "2404140"
  },
  {
    "text": "see if I can just see if I can set this",
    "start": "2404140",
    "end": "2409690"
  },
  {
    "text": "to maybe two okay I'm able to set that too I'm hitting mind because I'm running",
    "start": "2409690",
    "end": "2414850"
  },
  {
    "text": "multiple clusters and different accounts okay so let's go back I'll go back to my",
    "start": "2414850",
    "end": "2420640"
  },
  {
    "text": "presentation here and so point-and-click resize is very easy and you know allows",
    "start": "2420640",
    "end": "2426640"
  },
  {
    "text": "you to increase resources so instead of having you that full scale data",
    "start": "2426640",
    "end": "2432280"
  },
  {
    "text": "migration effort it's more of a point quick or command line tool to be able to",
    "start": "2432280",
    "end": "2438010"
  },
  {
    "text": "do that and again like I said when you do the resize it distributes that cluster in a parallel a new cluster",
    "start": "2438010",
    "end": "2446010"
  },
  {
    "text": "while that while that's being done you do the reads the rights are suspended and then it fails over so we also have",
    "start": "2446010",
    "end": "2457540"
  },
  {
    "start": "2456000",
    "end": "2613000"
  },
  {
    "text": "built-in security and we took advantage of the security features that Amazon already offers when we launch redshift",
    "start": "2457540",
    "end": "2463990"
  },
  {
    "text": "so we have SSL to secure the data that's in transit you will be able to not enable this on the client side and",
    "start": "2463990",
    "end": "2470830"
  },
  {
    "text": "connect to make sure all data flows are encrypted so you use in SSL to have encrypted data connection between",
    "start": "2470830",
    "end": "2476410"
  },
  {
    "text": "redshifts s3 and DynamoDB as well and so ssl can enable you on your end and you",
    "start": "2476410",
    "end": "2482950"
  },
  {
    "text": "can choose that at any time that you want and then as for data encryption at rest should you choose it it's hardware",
    "start": "2482950",
    "end": "2490180"
  },
  {
    "text": "accelerated AES 256-bit encryption we do this at a very low level in the sub system so that every block that's ever",
    "start": "2490180",
    "end": "2497290"
  },
  {
    "text": "written to disk or loaded on or with temporary data or just loaded down",
    "start": "2497290",
    "end": "2502320"
  },
  {
    "text": "everything is encrypted at rest and we are doing continuous back up right as part of the backup and so at the block",
    "start": "2502320",
    "end": "2509880"
  },
  {
    "text": "level every block that's backed up to s3 is also encrypted and then we also use a multi-layer team mechanism where each",
    "start": "2509880",
    "end": "2517200"
  },
  {
    "text": "block has the key to encrypt each block individually then there's a cluster key",
    "start": "2517200",
    "end": "2522540"
  },
  {
    "text": "that encrypts those block keys then there's yet another team to encrypt a cluster key and that's kept off cluster",
    "start": "2522540",
    "end": "2529050"
  },
  {
    "text": "and so we keep innovating in that space and look forward to that but we have a",
    "start": "2529050",
    "end": "2535560"
  },
  {
    "text": "very strong multi-layer key mechanism and so that option to encrypt is done at",
    "start": "2535560",
    "end": "2541710"
  },
  {
    "text": "creation time or resize time and there is a ten to fifteen percent performance impact by our testing and so we don't",
    "start": "2541710",
    "end": "2548130"
  },
  {
    "text": "enable that by default we let our customers make that decision so on each",
    "start": "2548130",
    "end": "2553680"
  },
  {
    "text": "redshift instance is also provision solely for the customer that created it and each of those compute nodes are",
    "start": "2553680",
    "end": "2560010"
  },
  {
    "text": "created in an internal VPC and so a VPC is amazon's virtual private cloud and",
    "start": "2560010",
    "end": "2565860"
  },
  {
    "text": "it's a way to logically isolate a section of the AWS cloud and set up your own network space and access your cloud",
    "start": "2565860",
    "end": "2572550"
  },
  {
    "text": "resources privately and so nothing can access these compute nodes directly you",
    "start": "2572550",
    "end": "2578070"
  },
  {
    "text": "can only interface a cluster through the leader node and even the data nodes of",
    "start": "2578070",
    "end": "2583170"
  },
  {
    "text": "the compute nodes when you put data in them if they come it's a pull request",
    "start": "2583170",
    "end": "2588840"
  },
  {
    "text": "from the compute node um nothing's ever pushed on to those compute nodes they're",
    "start": "2588840",
    "end": "2595290"
  },
  {
    "text": "pulled from s3 or dynamo or from the leader nodes during a sequel insert",
    "start": "2595290",
    "end": "2600600"
  },
  {
    "text": "statement and then you can also set up your own PPC doing your cluster subnet and then control access however you like",
    "start": "2600600",
    "end": "2608040"
  },
  {
    "text": "for access to that collector and so",
    "start": "2608040",
    "end": "2614370"
  },
  {
    "start": "2613000",
    "end": "2699000"
  },
  {
    "text": "another part of the operation is to take care of your data we use local attached storage it's ephemeral and we use it for",
    "start": "2614370",
    "end": "2621510"
  },
  {
    "text": "cluster performance but as with ec2 when you have multiple terabytes on data on",
    "start": "2621510",
    "end": "2627060"
  },
  {
    "text": "disk having only a firm storage is not a good thing in terms of durability the redshift takes care of",
    "start": "2627060",
    "end": "2632830"
  },
  {
    "text": "that durability management of your data it's a theme of all of our AWS manage resources so any time you load data on",
    "start": "2632830",
    "end": "2639220"
  },
  {
    "text": "the cluster will secretly replicate on multiple computers within that cluster so if you go too easy to and you look at",
    "start": "2639220",
    "end": "2646120"
  },
  {
    "text": "the eight extra large you actually get 48 terabytes of storage because it's two x 24 terabyte spindles but you only",
    "start": "2646120",
    "end": "2652690"
  },
  {
    "text": "receive 16 terabytes and red shifts because we're using the other two terabytes for redundant copies and we're",
    "start": "2652690",
    "end": "2658570"
  },
  {
    "text": "doing that all across the availability zones within region when you do a",
    "start": "2658570",
    "end": "2663760"
  },
  {
    "text": "snapshot we do a backup of that cluster and so s 3 is designed for 11 9 the",
    "start": "2663760",
    "end": "2670030"
  },
  {
    "text": "durability it has a lot of scale to it",
    "start": "2670030",
    "end": "2675520"
  },
  {
    "text": "and enables us to automatically and continuously and incrementally store that and so we're continuously",
    "start": "2675520",
    "end": "2682270"
  },
  {
    "text": "monitoring and automating the recovery of any node failures pulling it from s3",
    "start": "2682270",
    "end": "2687400"
  },
  {
    "text": "or from the other compute nodes as we see fit we reboot nodes and that",
    "start": "2687400",
    "end": "2692700"
  },
  {
    "text": "transparent to the sequel client and so",
    "start": "2692700",
    "end": "2698490"
  },
  {
    "text": "that brings us to integrating with multiple data sources so we really",
    "start": "2698490",
    "end": "2704140"
  },
  {
    "start": "2699000",
    "end": "2756000"
  },
  {
    "text": "design redshift to be the central data warehouse and central database",
    "start": "2704140",
    "end": "2709830"
  },
  {
    "text": "repository or of this hug with all of the different ec2 services so we see",
    "start": "2709830",
    "end": "2715090"
  },
  {
    "text": "customers who use traditional oltp databases either with RDS or running their own databases on ec2 or on-premise",
    "start": "2715090",
    "end": "2722920"
  },
  {
    "text": "and we have customers running no sequel data stores on ec2 or DynamoDB especially for real-time data capture",
    "start": "2722920",
    "end": "2729850"
  },
  {
    "text": "and or using EMR so using MapReduce or elastic MapReduce our Hadoop offering on",
    "start": "2729850",
    "end": "2736180"
  },
  {
    "text": "ec2 to take unstructured data put it into structured columnar format put it",
    "start": "2736180",
    "end": "2741700"
  },
  {
    "text": "in parallel to s3 which then we can import into redshift the redshift is at the hub of being able to collect data",
    "start": "2741700",
    "end": "2749100"
  },
  {
    "text": "for longer life cycle than previously been able to do before",
    "start": "2749100",
    "end": "2754349"
  },
  {
    "text": "and so we provide a lot of loading options upload s3 do an import/export",
    "start": "2755080",
    "end": "2760760"
  },
  {
    "start": "2756000",
    "end": "2794000"
  },
  {
    "text": "where you ship hard drives to us do a Direct Connect which is a one gig or 10",
    "start": "2760760",
    "end": "2767390"
  },
  {
    "text": "gig private fiber pipe to a to the cloud and you can do those in increments so",
    "start": "2767390",
    "end": "2773480"
  },
  {
    "text": "you can truncate those and be able to do love even larger scales of end them you",
    "start": "2773480",
    "end": "2779540"
  },
  {
    "text": "can work with partners data integration or system integrators that you see here on the slide that have optimized and",
    "start": "2779540",
    "end": "2785810"
  },
  {
    "text": "being able to move lots of data into the cloud and being able to import it into redshift and so redshift works with your",
    "start": "2785810",
    "end": "2796640"
  },
  {
    "text": "existing and analysts tools um so with the a JDBC odbc connectivity using",
    "start": "2796640",
    "end": "2802370"
  },
  {
    "text": "postgres drivers so it's not a Postgres database engine it's our own proprietary",
    "start": "2802370",
    "end": "2808160"
  },
  {
    "text": "engine but we use the protocol the wire protocol from postgres in order to",
    "start": "2808160",
    "end": "2813260"
  },
  {
    "text": "interact with it and and then we have a lot of business intelligence tools that",
    "start": "2813260",
    "end": "2818390"
  },
  {
    "text": "if they can use odbc and jdbc connections and use sequel to make requests into databases then they should",
    "start": "2818390",
    "end": "2826220"
  },
  {
    "text": "be able to work with redshift and so we have had a lot of partners certified their own applications and they've done",
    "start": "2826220",
    "end": "2832790"
  },
  {
    "text": "so also on our marketplace to be able to connect to red shift and so we have a",
    "start": "2832790",
    "end": "2838070"
  },
  {
    "start": "2837000",
    "end": "2863000"
  },
  {
    "text": "marketplace of it's basically an app store of different services that can run on our cloud and we also have a certain",
    "start": "2838070",
    "end": "2846620"
  },
  {
    "text": "section for redshift so you can see some of the BI tools such as jasper solve MicroStrategy and in tunity that have",
    "start": "2846620",
    "end": "2853490"
  },
  {
    "text": "platforms that enable you to discover redshift clusters in your account and be able to set up the connections to them",
    "start": "2853490",
    "end": "2862030"
  },
  {
    "text": "so we had a we've had several customers will work on creating a lot of public",
    "start": "2862030",
    "end": "2868550"
  },
  {
    "start": "2863000",
    "end": "2962000"
  },
  {
    "text": "case studies but Nokia um as well as air B&B they've done presentation",
    "start": "2868550",
    "end": "2874340"
  },
  {
    "text": "where they've shown how they found benefits in red shift but I will talk",
    "start": "2874340",
    "end": "2880190"
  },
  {
    "text": "about what we've done with Amazon so our retail amazon.com business is pretty big",
    "start": "2880190",
    "end": "2885610"
  },
  {
    "text": "pretty big business and it has a pretty big old guard traditional data warehouse",
    "start": "2885610",
    "end": "2890900"
  },
  {
    "text": "solution and we've been working with their database team internally for",
    "start": "2890900",
    "end": "2896540"
  },
  {
    "text": "several months with testing and using of Amazon redshift and so their data",
    "start": "2896540",
    "end": "2902120"
  },
  {
    "text": "warehouse which cost several millions of dollars per year to run has 32 notes four point two terabytes of RAM and one",
    "start": "2902120",
    "end": "2908720"
  },
  {
    "text": "point six petabytes of disk and so again this is costing millions of dollars to run and so what we did was we took two",
    "start": "2908720",
    "end": "2915860"
  },
  {
    "text": "billion records and six of the most complex queries that they usually run on this data warehouse and did a",
    "start": "2915860",
    "end": "2922430"
  },
  {
    "text": "benchmarking on those and we do there on a redshift cluster 216 terabyte notes they had extra large and roughly around",
    "start": "2922430",
    "end": "2930080"
  },
  {
    "text": "32,000 per year to run that with reserved instances and we got it b12 to",
    "start": "2930080",
    "end": "2935660"
  },
  {
    "text": "a hundred fifty times performance and being able to do that and so the DB a team of Amazon that concept it could",
    "start": "2935660",
    "end": "2942200"
  },
  {
    "text": "probably tweak their database cluster to kind of match those redshift of improvements but they would have to take",
    "start": "2942200",
    "end": "2949130"
  },
  {
    "text": "a couple months to do so again going back to the performance and tuning undifferentiated heavy lifting that you",
    "start": "2949130",
    "end": "2955910"
  },
  {
    "text": "would have to do to get this kind of performance which you can get by the hour and so I'm just going to quickly go",
    "start": "2955910",
    "end": "2962270"
  },
  {
    "start": "2962000",
    "end": "2997000"
  },
  {
    "text": "through these use cases just because I'd like to show you a little bit about connecting to the wretched cluster and",
    "start": "2962270",
    "end": "2967880"
  },
  {
    "text": "so we got this reporting warehouse use case where you have a database that's of or ERP system pumping data into a",
    "start": "2967880",
    "end": "2975230"
  },
  {
    "text": "relational database which then you can off lift into a redshift cluster and then again that's the end point of being",
    "start": "2975230",
    "end": "2982910"
  },
  {
    "text": "able to do reporting and bi so you can use our DBMS for the oh you know online",
    "start": "2982910",
    "end": "2988480"
  },
  {
    "text": "transactional processing but then use a redshift for the online analytical processing for your reporting and bi",
    "start": "2988480",
    "end": "2994910"
  },
  {
    "text": "tool and then on-premises integration because of VPC and also using our data",
    "start": "2994910",
    "end": "3001630"
  },
  {
    "text": "integration partners you can create secure tunnel amazon cloud and also be able to use red",
    "start": "3001630",
    "end": "3007260"
  },
  {
    "text": "shift as it was a private resource and then there's live archiving for",
    "start": "3007260",
    "end": "3013800"
  },
  {
    "start": "3012000",
    "end": "3054000"
  },
  {
    "text": "structured big data so DynamoDB is a massively scale out unlimited size provision throughputs of reads and",
    "start": "3013800",
    "end": "3020880"
  },
  {
    "text": "writes of if you 12 hundreds of thousands of reads and writes requests requests per second if you wanted to and",
    "start": "3020880",
    "end": "3026700"
  },
  {
    "text": "so you can use dynamodb at this structured document no sequel store",
    "start": "3026700",
    "end": "3032430"
  },
  {
    "text": "which then you can then provide as a source to import into red shift and then",
    "start": "3032430",
    "end": "3037920"
  },
  {
    "text": "again you can use red shift as that hub to collect all that time data so you can use dynamodb to handle hot data that's",
    "start": "3037920",
    "end": "3045090"
  },
  {
    "text": "coming in very fast and then using redshift to analyze that on offline so",
    "start": "3045090",
    "end": "3051810"
  },
  {
    "text": "speak and then you can use a entity and transaction loading for big data using",
    "start": "3051810",
    "end": "3059850"
  },
  {
    "start": "3054000",
    "end": "3116000"
  },
  {
    "text": "EMR again you can have EMR process of unstructured data that's being stored in",
    "start": "3059850",
    "end": "3065640"
  },
  {
    "text": "f3 formatting that to fit into the profile that you need for red shift",
    "start": "3065640",
    "end": "3070970"
  },
  {
    "text": "which is basically any bike delimited type of file txt file in utf-8 format so",
    "start": "3070970",
    "end": "3078210"
  },
  {
    "text": "you can have EMR run that and then redshift be able to consume that in from",
    "start": "3078210",
    "end": "3084480"
  },
  {
    "text": "s3 and so by having this longer history you can ensure a better insight and",
    "start": "3084480",
    "end": "3091020"
  },
  {
    "text": "airbnb said they'd use the EMR to run some hive interactive queries and they",
    "start": "3091020",
    "end": "3096630"
  },
  {
    "text": "found almost 20 to 40 times better performance by having those similar",
    "start": "3096630",
    "end": "3101670"
  },
  {
    "text": "queries being stored in red shift so you may use EMR in tandem with redshift depends on your use case but if you're",
    "start": "3101670",
    "end": "3109260"
  },
  {
    "text": "doing a lot of analytical processing redshift is definitely going to give you a better performance so there's a here's",
    "start": "3109260",
    "end": "3118590"
  },
  {
    "start": "3116000",
    "end": "3158000"
  },
  {
    "text": "a series of resources again if you have a more detailed question we'll have a",
    "start": "3118590",
    "end": "3123600"
  },
  {
    "text": "little Q&A here but you can email me and we can also get in touch with the",
    "start": "3123600",
    "end": "3128640"
  },
  {
    "text": "database team the redshift team and so you can get started by looking at redshift at amazon AWS amazon com flash",
    "start": "3128640",
    "end": "3136260"
  },
  {
    "text": "red shift you can also look at our marketplace we also have documentation",
    "start": "3136260",
    "end": "3141359"
  },
  {
    "text": "and user guide and a getting started guide so you can walk through some steps of launching a cluster and getting",
    "start": "3141359",
    "end": "3147270"
  },
  {
    "text": "started and then we also have best practices when you want to start loading designing tables and loading large large",
    "start": "3147270",
    "end": "3154319"
  },
  {
    "text": "data sets and so uh you know we started",
    "start": "3154319",
    "end": "3161760"
  },
  {
    "start": "3158000",
    "end": "3198000"
  },
  {
    "text": "with that brief overview in summary of Amazon redshift enables you to have this",
    "start": "3161760",
    "end": "3167910"
  },
  {
    "text": "petabyte scale page ago no upfront costs fast performance easy to use point click",
    "start": "3167910",
    "end": "3174420"
  },
  {
    "text": "or API calls to have large clustered database solution and so I'd like to end",
    "start": "3174420",
    "end": "3180599"
  },
  {
    "text": "this off oh there's also a link at the bottom AWS amazon com / resources /",
    "start": "3180599",
    "end": "3186540"
  },
  {
    "text": "database services / webinars to be able to see these recorded webinars on ours",
    "start": "3186540",
    "end": "3193770"
  },
  {
    "text": "are different services so I'm going to",
    "start": "3193770",
    "end": "3199740"
  },
  {
    "start": "3198000",
    "end": "3599000"
  },
  {
    "text": "connect to a cluster really quick I'm using a tool called sequel oh I'm sorry let me share my desktop sorry so I've",
    "start": "3199740",
    "end": "3207480"
  },
  {
    "text": "got my I got a workbench tool and so when you when you go to red shift and let me go back to it really quick you're",
    "start": "3207480",
    "end": "3215970"
  },
  {
    "text": "getting a JDBC URL and you're connecting to it as user name and password you got",
    "start": "3215970",
    "end": "3221490"
  },
  {
    "text": "your pork and so you've got this endpoint as to be able to connect to and so I'm considering this this is a sequel",
    "start": "3221490",
    "end": "3228780"
  },
  {
    "text": "client I've already got one configured using the post-crash driver the URL I",
    "start": "3228780",
    "end": "3234000"
  },
  {
    "text": "got a username and password I'm going to hit OK to connect first step you would",
    "start": "3234000",
    "end": "3239460"
  },
  {
    "text": "do is to create a table right and then you can copy the table so I just have",
    "start": "3239460",
    "end": "3246119"
  },
  {
    "text": "I'm not going to stay long on the sequel man's but I have a great table sequel statement and then what we have here is",
    "start": "3246119",
    "end": "3253730"
  },
  {
    "text": "a copy table name so this way it would work if you copy the table name from",
    "start": "3253730",
    "end": "3260280"
  },
  {
    "text": "your s3 or dynamodb endpoint you set what your credentials are your acts of",
    "start": "3260280",
    "end": "3265410"
  },
  {
    "text": "key and secret key and then say what the delimiter is so if it's a comma separated value file you'd have a common",
    "start": "3265410",
    "end": "3271680"
  },
  {
    "text": "in civil delimiter and so you would load that and once you load it you can see we have a database Explorer I've got a",
    "start": "3271680",
    "end": "3280560"
  },
  {
    "text": "table here that has that table that I created I've also got some notional data that I pulled from census and i'll hit",
    "start": "3280560",
    "end": "3287840"
  },
  {
    "text": "so I've got population by raised by state and county in the United States",
    "start": "3287840",
    "end": "3293580"
  },
  {
    "text": "from the 2010 census and so this gives me a sample of 25 of the records there's",
    "start": "3293580",
    "end": "3299370"
  },
  {
    "text": "over 3,000 in this particular data set that you're able to do and so once you have that in you can verify that you've",
    "start": "3299370",
    "end": "3305070"
  },
  {
    "text": "got it and then you can run a couple queries I'll just hit play here to run",
    "start": "3305070",
    "end": "3310260"
  },
  {
    "text": "this one and if it's 3,000 records or a billion records i did a demo and New",
    "start": "3310260",
    "end": "3316380"
  },
  {
    "text": "York and San Francisco summit where we did it over three hundred and forty million records and then 1.6 billion",
    "start": "3316380",
    "end": "3323220"
  },
  {
    "text": "records to the same concept applies here i'm running a query of population by",
    "start": "3323220",
    "end": "3329100"
  },
  {
    "text": "state and then i'll run one here by county and so it's already compiled and",
    "start": "3329100",
    "end": "3334980"
  },
  {
    "text": "put this through so every time I'm executing this it you're seeing sub 10",
    "start": "3334980",
    "end": "3340170"
  },
  {
    "text": "milliseconds our response time on uncertain of these queries and so let's",
    "start": "3340170",
    "end": "3345540"
  },
  {
    "text": "go back before I end it will do a couple questions I want to go back to the",
    "start": "3345540",
    "end": "3351980"
  },
  {
    "text": "cluster here and so i'll hit refresh and it's still creating that demo 1 and so i",
    "start": "3351980",
    "end": "3359550"
  },
  {
    "text": "look at the example cluster i've increased it from one node 22 notes so that gives hopefully that gives you a",
    "start": "3359550",
    "end": "3364980"
  },
  {
    "text": "little bit of an idea of amazon right Justin how to operate it with its four and its benefit and so what I'll do is",
    "start": "3364980",
    "end": "3372049"
  },
  {
    "text": "I'll I'll stop this and then leave it on the on the last slide here and we'll",
    "start": "3372049",
    "end": "3380710"
  },
  {
    "text": "answer some question so let's say here",
    "start": "3380710",
    "end": "3386660"
  },
  {
    "text": "so do people you read ship operational data as an operational data store so",
    "start": "3386660",
    "end": "3394150"
  },
  {
    "text": "mostly most workloads there you can use you can use in a relational database or",
    "start": "3394150",
    "end": "3401119"
  },
  {
    "text": "an RDS but it depends on for online",
    "start": "3401119",
    "end": "3406549"
  },
  {
    "text": "transactional processing type systems or what you're calling operational to what I'm assuming you really want to use a",
    "start": "3406549",
    "end": "3413839"
  },
  {
    "text": "normal database for that the data warehouse is more for analytics and for reporting and interactive queries to be",
    "start": "3413839",
    "end": "3421729"
  },
  {
    "text": "able to go through that okay and so another great question is from what",
    "start": "3421729",
    "end": "3429410"
  },
  {
    "text": "happens what happens if a leader node goes down so that's a great question and",
    "start": "3429410",
    "end": "3434660"
  },
  {
    "text": "so what happens is we actually launched another league leader node in parallel",
    "start": "3434660",
    "end": "3439849"
  },
  {
    "text": "we would suspend reads and writes and the access to the cluster but another",
    "start": "3439849",
    "end": "3445099"
  },
  {
    "text": "leader node will come up and take its place and then do a DNS switch over into",
    "start": "3445099",
    "end": "3451039"
  },
  {
    "text": "that leader node and resume if the leader node fails you're not going to lose the entire cluster ok",
    "start": "3451039",
    "end": "3460050"
  },
  {
    "text": "we'll take I think we've got time for like another question or two ok let me",
    "start": "3460050",
    "end": "3468630"
  },
  {
    "text": "just take a quick look Syria is it",
    "start": "3468630",
    "end": "3474780"
  },
  {
    "text": "possible to collect x.x a possible export collected performance metrics for",
    "start": "3474780",
    "end": "3480980"
  },
  {
    "text": "further analysis yes you can you can do that you can you can see it in the",
    "start": "3480980",
    "end": "3486840"
  },
  {
    "text": "console but there's also system tables within redshift that you can also do queries and be able to collect that and",
    "start": "3486840",
    "end": "3494160"
  },
  {
    "text": "then if you need it to be able to keep that offline you can then just create a",
    "start": "3494160",
    "end": "3500850"
  },
  {
    "text": "table to hold all that performance metrics and then also there are some integrations with cloud watch which you",
    "start": "3500850",
    "end": "3506040"
  },
  {
    "text": "can then pull as well and is it possible",
    "start": "3506040",
    "end": "3511140"
  },
  {
    "text": "to transition from a oh oh one large",
    "start": "3511140",
    "end": "3517260"
  },
  {
    "text": "cluster to a eight extra large cluster automatically in terms of automatically",
    "start": "3517260",
    "end": "3525840"
  },
  {
    "text": "because everything is programmatic approach a mattock it's not a built-in service and to redshift 22 autoscale",
    "start": "3525840",
    "end": "3532470"
  },
  {
    "text": "this cluster size that would still be up to you to be able to do it you can automate it with some code and ec2 and",
    "start": "3532470",
    "end": "3539880"
  },
  {
    "text": "using a command line tools or api's to be able to increase the size of the",
    "start": "3539880",
    "end": "3544950"
  },
  {
    "text": "cluster you may have different performance metrics like even though my total size is 32 terabytes but because i",
    "start": "3544950",
    "end": "3552600"
  },
  {
    "text": "want to massively have a parallel architecture I'll actually have more of nodes even though the total storage of",
    "start": "3552600",
    "end": "3558630"
  },
  {
    "text": "my new cluster is a lot more capable and I'm not storing that same amount of data",
    "start": "3558630",
    "end": "3563960"
  },
  {
    "text": "you may do it for performance reasons where you can then stripe across multiple slices on",
    "start": "3563960",
    "end": "3570089"
  },
  {
    "text": "computed and I think we have time for one more question let me see if I can",
    "start": "3570089",
    "end": "3576069"
  },
  {
    "text": "take a look can I move the entire",
    "start": "3576069",
    "end": "3588549"
  },
  {
    "text": "database to red shift in order to work with it and then turn down the sequel",
    "start": "3588549",
    "end": "3594039"
  },
  {
    "text": "server you you can move an entire database into red shift and work and",
    "start": "3594039",
    "end": "3599589"
  },
  {
    "text": "work with it and not use sequel server but again that's going up down the lines of your youth case if you're using",
    "start": "3599589",
    "end": "3605649"
  },
  {
    "text": "sequel server as your warehouse of data then yes you can export that data into",
    "start": "3605649",
    "end": "3611829"
  },
  {
    "text": "flat files into tables you would put them into s3 format them make sure that",
    "start": "3611829",
    "end": "3617380"
  },
  {
    "text": "you're getting all the fields in our documentation we say what what all the",
    "start": "3617380",
    "end": "3622929"
  },
  {
    "text": "postcards feels that we support and then the data types that we support in so as",
    "start": "3622929",
    "end": "3628869"
  },
  {
    "text": "long as you format that correctly and you put it as files and what you would want to do especially with the entire",
    "start": "3628869",
    "end": "3634869"
  },
  {
    "text": "database of large you want to chunk those data files through that depending on your cluster size you can parallel",
    "start": "3634869",
    "end": "3642159"
  },
  {
    "text": "import all of those files in so there's a transformation step that you would have to do is export that database into",
    "start": "3642159",
    "end": "3648219"
  },
  {
    "text": "flat files store them into s3 and then with s3 to a parallel copy using the",
    "start": "3648219",
    "end": "3653949"
  },
  {
    "text": "copy command and then once that's all done you can test it run sample queries that you do in sequel server you could",
    "start": "3653949",
    "end": "3660880"
  },
  {
    "text": "do that against your redshift cluster and make sure everything's good and then you could turn down sequel server if you",
    "start": "3660880",
    "end": "3666069"
  },
  {
    "text": "wanted to that if you're using it as the analytical use case if you're still",
    "start": "3666069",
    "end": "3671409"
  },
  {
    "text": "doing transactional processing like financial records redshift is not the solution for that um you would want to",
    "start": "3671409",
    "end": "3678489"
  },
  {
    "text": "use a sequel server and so what we'll do is we'll answer the rest of the",
    "start": "3678489",
    "end": "3684159"
  },
  {
    "text": "questions post webinar I really appreciate your time and thank you for the great question and will respond back",
    "start": "3684159",
    "end": "3691269"
  },
  {
    "text": "to the people who ask questions in a follow-up that concludes it for me or or back to you",
    "start": "3691269",
    "end": "3698480"
  },
  {
    "text": "and with that I'll just do a final thank you I'd like to thank you been for the great presentation today and thanks",
    "start": "3698480",
    "end": "3704310"
  },
  {
    "text": "everyone for attending but I",
    "start": "3704310",
    "end": "3707600"
  }
]