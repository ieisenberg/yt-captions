[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "all right good afternoon everyone welcome back from the lab and from lunch hello everyone on twitch my name again",
    "start": "1550",
    "end": "9840"
  },
  {
    "text": "is Ben Willett I'm a Solutions Architect here with AWS and with me today is",
    "start": "9840",
    "end": "15389"
  },
  {
    "text": "Sudhir Gupta a partner Solutions Architect tell you a little bit about his background and I'm the solutions",
    "start": "15389",
    "end": "24420"
  },
  {
    "text": "architect in EWS specialized in Amazon redshift so normally I am responsible",
    "start": "24420",
    "end": "30060"
  },
  {
    "text": "for working with is we and SI partners any any opportunity related to the Amazon redshift like building technical",
    "start": "30060",
    "end": "37530"
  },
  {
    "text": "competencies with respect to the Amazon redshift or Title II and like validating",
    "start": "37530",
    "end": "43020"
  },
  {
    "text": "certifying the eysie product which integrates with redshift and so on overall I have more than 13 years of",
    "start": "43020",
    "end": "51600"
  },
  {
    "text": "experience in database and Analytics domain II when I worked as a senior",
    "start": "51600",
    "end": "57000"
  },
  {
    "text": "database engineer for amazon.com and I even I lifted you know I mean giant",
    "start": "57000",
    "end": "62460"
  },
  {
    "text": "migrations 50 petabytes from Oracle to the redshift and literally awesome",
    "start": "62460",
    "end": "68700"
  },
  {
    "text": "thanks to dear so I'm gonna kick off the presentation Sudhir is gonna come up and deliver the middle and then I'll come",
    "start": "68700",
    "end": "74760"
  },
  {
    "text": "back at the end and wrap things up thanks in here alright terrific so what",
    "start": "74760",
    "end": "81119"
  },
  {
    "text": "we're gonna dive into a little bit of redshift we've covered some of this so I'm gonna go fairly quickly but for the",
    "start": "81119",
    "end": "87390"
  },
  {
    "start": "82000",
    "end": "82000"
  },
  {
    "text": "twitch folks I want to make sure that we have a good understanding of what redshift is and a little bit on the",
    "start": "87390",
    "end": "93000"
  },
  {
    "text": "redshift internal side of things so again redshift is a an SI sequel",
    "start": "93000",
    "end": "99090"
  },
  {
    "text": "compliant database with full acid properties support so all those good",
    "start": "99090",
    "end": "104880"
  },
  {
    "text": "OLTP and all those good transaction processing skills you learned in school are all implemented here this is a fully",
    "start": "104880",
    "end": "112860"
  },
  {
    "text": "managed data warehouse provided by AWS so for example you will spin up a",
    "start": "112860",
    "end": "119130"
  },
  {
    "text": "redshift cluster we will take care of the patching and the maintenance of that cluster we'll do automatic backups for",
    "start": "119130",
    "end": "126270"
  },
  {
    "text": "you will monitor the nodes for health and we do something pretty slick we're",
    "start": "126270",
    "end": "131760"
  },
  {
    "text": "behind the scenes will actually have separate copies of your redshift database spread on different nodes to",
    "start": "131760",
    "end": "138960"
  },
  {
    "text": "become more fault tolerant if a node becomes unavailable we're able to rehydrate a new node based on extra data",
    "start": "138960",
    "end": "146250"
  },
  {
    "text": "stored around the cluster so good fault tolerance there as you'll see the back",
    "start": "146250",
    "end": "151650"
  },
  {
    "text": "end is massively parallel shared nothing OLAP architecture and we talked a lot this morning about columnar datastore",
    "start": "151650",
    "end": "158870"
  },
  {
    "text": "redshift really shines in large-scale aggregate reads so if you're doing",
    "start": "158870",
    "end": "164670"
  },
  {
    "text": "things like select average sale price select maximum sales price or show me",
    "start": "164670",
    "end": "171750"
  },
  {
    "text": "the thing that sold the most on Christmas Day what was the number one toy on Christmas Day redshift is going",
    "start": "171750",
    "end": "179070"
  },
  {
    "text": "to handle those large-scale aggregate reads extremely well again engineered",
    "start": "179070",
    "end": "184590"
  },
  {
    "text": "for performance sudeer we'll talk about some real-world scenarios as well as his experience",
    "start": "184590",
    "end": "190710"
  },
  {
    "text": "working on amazon.com but it's designed to scale but for one tenth of the cost",
    "start": "190710",
    "end": "196740"
  },
  {
    "text": "of traditional data warehouse solutions and again security is absolutely",
    "start": "196740",
    "end": "202950"
  },
  {
    "text": "priority number one at AWS and we've got end-to-end encryption on your red ship",
    "start": "202950",
    "end": "208170"
  },
  {
    "text": "communication between the client the server and back and then we've got",
    "start": "208170",
    "end": "213930"
  },
  {
    "text": "encryption at rest in the database engine and then a number of certifications including sauk one two",
    "start": "213930",
    "end": "219930"
  },
  {
    "text": "and three HIPAA for healthcare applications FedRAMP for government",
    "start": "219930",
    "end": "226290"
  },
  {
    "text": "applications and many many more all these certifications are available on our website I encourage you to take a",
    "start": "226290",
    "end": "233430"
  },
  {
    "text": "look at them and then both Forrester and Gartner have positioned us as leaders in",
    "start": "233430",
    "end": "238590"
  },
  {
    "text": "their quadrants so more customers use Amazon redshift for their data work data",
    "start": "238590",
    "end": "245130"
  },
  {
    "text": "warehouse workloads than anyone else we've got a bunch of logos up here on the screen everybody from Johnson &",
    "start": "245130",
    "end": "251340"
  },
  {
    "text": "Johnson and healthcare liberty mutual for insurance we've got Phillips we've got Amgen and",
    "start": "251340",
    "end": "257579"
  },
  {
    "text": "the healthcare life sciences division if you don't see your vertical up here",
    "start": "257580",
    "end": "262729"
  },
  {
    "text": "definitely go to the red shift page search for reg",
    "start": "262729",
    "end": "268620"
  },
  {
    "text": "case studies or customer success stories and I'm sure you'll find a use case up",
    "start": "268620",
    "end": "273780"
  },
  {
    "text": "there that matches your environment that you can learn from so what does red ship",
    "start": "273780",
    "end": "281279"
  },
  {
    "start": "280000",
    "end": "280000"
  },
  {
    "text": "look like under the hood so so far we've been talking about redshift as a sequel data warehouse where you've got a JDBC",
    "start": "281279",
    "end": "288509"
  },
  {
    "text": "connection coming in or an ODBC connection coming in either from a bi tool like quick site bi tool like",
    "start": "288509",
    "end": "296460"
  },
  {
    "text": "tableau or maybe it's a custom application you've written to use JDBC",
    "start": "296460",
    "end": "301590"
  },
  {
    "text": "or ODBC as the interface that's gonna hit a Postgres compatible engine in",
    "start": "301590",
    "end": "307650"
  },
  {
    "text": "redshift and so anything that you can do up against a postscript date of post",
    "start": "307650",
    "end": "314430"
  },
  {
    "text": "Gress database will work here but behind the scenes behind that Postgres",
    "start": "314430",
    "end": "320580"
  },
  {
    "text": "compatible interface we've done some pretty amazing engineering in terms of combining a massively parallel",
    "start": "320580",
    "end": "327599"
  },
  {
    "text": "processing back-end and you'll see how the redshift cluster can be spread out",
    "start": "327599",
    "end": "332789"
  },
  {
    "text": "across 100 and up to 128 nodes we've got an OLAP engine in there for again doing",
    "start": "332789",
    "end": "339479"
  },
  {
    "text": "these large-scale aggregate queries and then it's a columnar datastore and again",
    "start": "339479",
    "end": "346110"
  },
  {
    "text": "instead of doing row wise operations when you're doing large-scale aggregate",
    "start": "346110",
    "end": "351479"
  },
  {
    "text": "reads columnar is really the way to scale for maximum performance so if you",
    "start": "351479",
    "end": "357719"
  },
  {
    "text": "put that Postgres compatible sequel engine in front and then you've got a",
    "start": "357719",
    "end": "363060"
  },
  {
    "text": "massively parallel engine behind it with an OLAP server and a columnar datastore",
    "start": "363060",
    "end": "368490"
  },
  {
    "text": "that is what we built with Amazon redshift and we've integrated that with",
    "start": "368490",
    "end": "374819"
  },
  {
    "text": "all of the AWS goodness that you know and love so runs in a V PC uses I am for",
    "start": "374819",
    "end": "381899"
  },
  {
    "text": "Identity and Access Management who can control the cluster who can start who can stop who can expand the cluster will",
    "start": "381899",
    "end": "389819"
  },
  {
    "text": "talk about s3 storage using redshift spectrum and then key management system",
    "start": "389819",
    "end": "396419"
  },
  {
    "text": "again if you want to have more control over the encryption you can let redshift worry",
    "start": "396419",
    "end": "401550"
  },
  {
    "text": "that it or you can manage keys yourself with kms and then cloud watch right very",
    "start": "401550",
    "end": "408120"
  },
  {
    "text": "important redshift is a massively powerful data warehouse but you still",
    "start": "408120",
    "end": "416010"
  },
  {
    "text": "want to keep an eye on it you want to understand what your traffic patterns look like you want to monitor your CPU",
    "start": "416010",
    "end": "421560"
  },
  {
    "text": "you want to monitor your disk and cloud watch is going to give you all of that",
    "start": "421560",
    "end": "426810"
  },
  {
    "text": "information and then you can use things like cloud watch alerts and events to send notifications to make sure that the",
    "start": "426810",
    "end": "434220"
  },
  {
    "text": "cluster is performing as you'd expect so",
    "start": "434220",
    "end": "440820"
  },
  {
    "start": "438000",
    "end": "438000"
  },
  {
    "text": "the architecture we talked a little bit about this but again shared nothing architecture so you've got this thing",
    "start": "440820",
    "end": "447390"
  },
  {
    "text": "called a leader node this is sort of the brains of the cluster this is going to be your JDBC endpoint you'll get a DNS",
    "start": "447390",
    "end": "454470"
  },
  {
    "text": "name for this leader node that's where your queries land the query processor on",
    "start": "454470",
    "end": "460200"
  },
  {
    "text": "the leader node will actually look at its metadata it'll understand how the data is spread out across the cluster",
    "start": "460200",
    "end": "467300"
  },
  {
    "text": "it'll formulate a query execution plan and then it'll actually break that query",
    "start": "467300",
    "end": "473400"
  },
  {
    "text": "execution plan up to C into C++ executables that will be deployed on",
    "start": "473400",
    "end": "479700"
  },
  {
    "text": "each compute node in the cluster to process the data locally so the leader",
    "start": "479700",
    "end": "485520"
  },
  {
    "text": "node has got the metadata catalog it's got your sequel query it understands how",
    "start": "485520",
    "end": "491370"
  },
  {
    "text": "to generate the query execution plan appropriate for each node in the cluster it generates that executable pushes it",
    "start": "491370",
    "end": "500280"
  },
  {
    "text": "down to the compute nodes and that code runs locally on the computer note so let's take an example we've got a 26",
    "start": "500280",
    "end": "508260"
  },
  {
    "text": "node cluster and we've got our data spread out alphabetically A through Z across that back-end and the leader node",
    "start": "508260",
    "end": "516270"
  },
  {
    "text": "is going to send a query down to one node and say get me your ROS another node get me your ROS another node get me",
    "start": "516270",
    "end": "523830"
  },
  {
    "text": "your ROS all those queries run parallel on the compute nodes they take their",
    "start": "523830",
    "end": "529710"
  },
  {
    "text": "result sets they send them back to the leader node the leader node aggregates them",
    "start": "529710",
    "end": "535080"
  },
  {
    "text": "and hands them down to the client this is how redshift delivers that massive",
    "start": "535080",
    "end": "540320"
  },
  {
    "text": "performance again there's a interconnect",
    "start": "540320",
    "end": "546630"
  },
  {
    "text": "between the leader node and those back-end compute nodes this is a 10 gig Ethernet network it's a private network",
    "start": "546630",
    "end": "554370"
  },
  {
    "text": "just for that redshift cluster there's no other traffic running on this 10 gig",
    "start": "554370",
    "end": "559650"
  },
  {
    "text": "network so you've got super low latency connections between the leader node and between the compute nodes themselves so",
    "start": "559650",
    "end": "567510"
  },
  {
    "text": "if we need to move rows back and forth between the compute nodes and the leader we can do it very very quickly over that",
    "start": "567510",
    "end": "574320"
  },
  {
    "text": "network and then the connection to s3 for a couple of purposes one we talked",
    "start": "574320",
    "end": "580980"
  },
  {
    "text": "about redshift spectrum and expanding expanding your cluster to petabytes and exabytes scale but also for real-time",
    "start": "580980",
    "end": "589290"
  },
  {
    "text": "streaming backups so redshift is monitoring your traffic patterns and in",
    "start": "589290",
    "end": "594390"
  },
  {
    "text": "the background it's snapshotting your data out to s3 automatically for you all",
    "start": "594390",
    "end": "603540"
  },
  {
    "text": "right here you can see the redshift spectrum fleet so again if you're",
    "start": "603540",
    "end": "609030"
  },
  {
    "text": "comfortable and all of your data fits on your redshift cluster maybe you've got a 10 node cluster maybe you've got a",
    "start": "609030",
    "end": "615630"
  },
  {
    "text": "hundred node cluster your data is there your queries are performing well awesome",
    "start": "615630",
    "end": "621260"
  },
  {
    "text": "if you have a situation where you need you know we talked about keeping 2017",
    "start": "621260",
    "end": "628230"
  },
  {
    "text": "and 2018 and 2019 in the cluster and then going out to s3 for historical data",
    "start": "628230",
    "end": "634230"
  },
  {
    "text": "you can literally keep exabytes of data in s3 for 2.3 cents a gigabyte even",
    "start": "634230",
    "end": "642330"
  },
  {
    "text": "lower for some of the others infrequent access or single AZ storage classes so",
    "start": "642330",
    "end": "648990"
  },
  {
    "text": "you can now afford to keep all of that historical data around and query it through redshift when you need it and",
    "start": "648990",
    "end": "655920"
  },
  {
    "text": "only pay for that when you access it so super powerful feature with redshift",
    "start": "655920",
    "end": "662720"
  },
  {
    "text": "spectrums ability to bring in all that s3 data",
    "start": "662720",
    "end": "668270"
  },
  {
    "text": "you are one slide ahead of me so the question was how does redshift know what's in the cluster and what's on s3",
    "start": "674700",
    "end": "682180"
  },
  {
    "text": "and I'm about to get to that so again you can think of spectrum as an Amazon",
    "start": "682180",
    "end": "689350"
  },
  {
    "text": "redshift feature that allows you to query data in s3 without loading it into the cluster potentially exabytes of data",
    "start": "689350",
    "end": "697630"
  },
  {
    "text": "from your data Lake and again we've got a fleet of spectrum nodes behind the",
    "start": "697630",
    "end": "702850"
  },
  {
    "text": "scenes that you guys don't need to manage you throw a query at it and we will bring up enough spectrum nodes to",
    "start": "702850",
    "end": "709810"
  },
  {
    "text": "manage that s3 data in a very performant way and again it's pay per query you're",
    "start": "709810",
    "end": "715510"
  },
  {
    "text": "only paying $5 per terabyte of scan data and again we talked about using that",
    "start": "715510",
    "end": "721870"
  },
  {
    "text": "very efficient Parkay file format in s3 to limit the amount of data scan for",
    "start": "721870",
    "end": "727990"
  },
  {
    "text": "very inexpensive queries so here's the answer to the question so you're all",
    "start": "727990",
    "end": "735280"
  },
  {
    "start": "731000",
    "end": "731000"
  },
  {
    "text": "used to writing DDL and in sequel write write a create table statement you give",
    "start": "735280",
    "end": "740830"
  },
  {
    "text": "it your column list you run the query the difference here is that we're gonna use this external keyword right we're",
    "start": "740830",
    "end": "748210"
  },
  {
    "text": "gonna go into redshift and we're gonna say create external schema give it a schema name and then we're gonna go in",
    "start": "748210",
    "end": "754660"
  },
  {
    "text": "and say create external table give it a table name that's the signal to store",
    "start": "754660",
    "end": "761590"
  },
  {
    "text": "this metadata in that glue catalog that we talked about that hive compatible",
    "start": "761590",
    "end": "767470"
  },
  {
    "text": "meta store that's gonna map our DDL back to the underlying as three resources and",
    "start": "767470",
    "end": "773740"
  },
  {
    "text": "that's the clue that we don't want to bring that data into the redshift cluster we just want to have this",
    "start": "773740",
    "end": "780190"
  },
  {
    "text": "external table definition that's really just a mapping layer between the sequel",
    "start": "780190",
    "end": "785950"
  },
  {
    "text": "DDL and the underlying s3 bucket and prefix structure and the underlying file",
    "start": "785950",
    "end": "792700"
  },
  {
    "text": "structure in those s3 buckets so once we've defined the external schema and",
    "start": "792700",
    "end": "798550"
  },
  {
    "text": "the external table we can then query it and we can use both redshift data and",
    "start": "798550",
    "end": "805840"
  },
  {
    "text": "spectrum data in the same sequel statement just by including that schema name ahead of the table name so",
    "start": "805840",
    "end": "813720"
  },
  {
    "text": "you could think of it as select columns from internal redshift table dot table",
    "start": "813720",
    "end": "821400"
  },
  {
    "text": "name and external table dot table name you could think of doing a union across",
    "start": "821400",
    "end": "829100"
  },
  {
    "text": "2019 data in the cluster and 2018 data in s3 you could think of joining tables",
    "start": "829100",
    "end": "837210"
  },
  {
    "text": "where I keep certain tables in redshift and I do a join against tables stored in",
    "start": "837210",
    "end": "843660"
  },
  {
    "text": "spectrum so you've got a lot of power and a lot of flexibility there just by using these external keywords there's a",
    "start": "843660",
    "end": "852690"
  },
  {
    "text": "couple of different instance types you could think of these these are just ec2 instances under the hood right but they",
    "start": "852690",
    "end": "858600"
  },
  {
    "start": "853000",
    "end": "853000"
  },
  {
    "text": "are optimized for redshift there's a DC to family and a ds2 family the DC twos",
    "start": "858600",
    "end": "865380"
  },
  {
    "text": "are more compute optimized they've got nvme SSDs so smaller high-performance",
    "start": "865380",
    "end": "873540"
  },
  {
    "text": "disks and then you've got DS 2's which are magnetic spin spinning disks but",
    "start": "873540",
    "end": "878910"
  },
  {
    "text": "much larger capacities we go from 160 gigs on the DC to large up to 2",
    "start": "878910",
    "end": "885510"
  },
  {
    "text": "terabytes on the DS to extra-large so depending on whether your workload is",
    "start": "885510",
    "end": "890820"
  },
  {
    "text": "compute intensive or storage intensive you can pick the DC 2 versus of GS 2 I",
    "start": "890820",
    "end": "897270"
  },
  {
    "text": "get a lot of customers who ask me how to decide and again the the answer I give a",
    "start": "897270",
    "end": "902910"
  },
  {
    "text": "lot of the time is build a POC and try it out so if you were to spin up a 2 node cluster on DC to larges would cost",
    "start": "902910",
    "end": "911040"
  },
  {
    "text": "you 50 cents an hour throw some queries at it see what the performance is maybe you decide you want",
    "start": "911040",
    "end": "917670"
  },
  {
    "text": "4 nodes or 8 nodes or maybe you decide you know my application is a little bit",
    "start": "917670",
    "end": "923250"
  },
  {
    "text": "more data intensive than compute intensive I'm gonna spin up a two node ds2 extra-large cluster and benchmark",
    "start": "923250",
    "end": "930960"
  },
  {
    "text": "against that and so again the key is with this flexibility to very quickly",
    "start": "930960",
    "end": "937740"
  },
  {
    "text": "spin up a cluster throw some queries at get some benchmarks experiment fail fast",
    "start": "937740",
    "end": "943649"
  },
  {
    "text": "and build the cluster that meets your workloads requirements so with that I'm",
    "start": "943649",
    "end": "950429"
  },
  {
    "text": "going to turn it over to Sudhir and he's going to get into some interesting details and use cases thanks to you so",
    "start": "950429",
    "end": "964319"
  },
  {
    "text": "here I'm going to talk about the cluster design considerations so whenever you",
    "start": "964319",
    "end": "969509"
  },
  {
    "start": "969000",
    "end": "969000"
  },
  {
    "text": "are designing a redshift cluster you always have to think about the five attributes you can remember those",
    "start": "969509",
    "end": "975269"
  },
  {
    "text": "attribute with the acronym set DW s for shorting you for encoding key for table",
    "start": "975269",
    "end": "981839"
  },
  {
    "text": "maintenance these data distribution and W is a workload management which is also known as w LM deal I mean these are the",
    "start": "981839",
    "end": "990540"
  },
  {
    "text": "five real attributes you always have to think whenever you are designing and if you are able to take care of these five",
    "start": "990540",
    "end": "996239"
  },
  {
    "text": "things appropriately and optimized to be you are good to go with the Amazon redshift as the been said Amazon",
    "start": "996239",
    "end": "1006919"
  },
  {
    "start": "1003000",
    "end": "1003000"
  },
  {
    "text": "redshift he uses that columnar storage in the behind the same so why we are doing the kilometer stories let's see",
    "start": "1006919",
    "end": "1013939"
  },
  {
    "text": "the difference between row versus the columnar restore so you will know why what is the reason behind the columnar",
    "start": "1013939",
    "end": "1020119"
  },
  {
    "text": "storage so in case of roast or all the attributes of the rows are stored",
    "start": "1020119",
    "end": "1027288"
  },
  {
    "text": "together in a block in the in the formal physical file in the backend but when",
    "start": "1027289",
    "end": "1032659"
  },
  {
    "text": "you have the columnar storage what happens each value all the values of",
    "start": "1032659",
    "end": "1037819"
  },
  {
    "text": "particular columns are stored in a one particular block so it means each columns are going to be storing",
    "start": "1037819",
    "end": "1042949"
  },
  {
    "text": "different blocks and as when you run the analytic workload what happens normally",
    "start": "1042949",
    "end": "1048889"
  },
  {
    "text": "you don't scan all the columns you normally run the queries on one or two columns so with the columnar storage",
    "start": "1048889",
    "end": "1055789"
  },
  {
    "text": "what we do we just scan the relevant columns the blocks which are relevant to those columns you are just skipping all",
    "start": "1055789",
    "end": "1061940"
  },
  {
    "text": "agitator so here in example if you see if you just want to scan all the social",
    "start": "1061940",
    "end": "1068029"
  },
  {
    "text": "security number of the all SSN I mean all the residents with the column nary storage you have to",
    "start": "1068029",
    "end": "1073880"
  },
  {
    "text": "the entire table but when it comes to the columnar storage you just scan you",
    "start": "1073880",
    "end": "1078980"
  },
  {
    "text": "know the blocks related to that particular column so it reduces the i/o so basically improve the performance of",
    "start": "1078980",
    "end": "1085340"
  },
  {
    "text": "your delivery and thats the reason why the redshift a design to choose the columnar storage because it is optimized",
    "start": "1085340",
    "end": "1092570"
  },
  {
    "text": "for the analytic workload now let's talk",
    "start": "1092570",
    "end": "1099500"
  },
  {
    "start": "1097000",
    "end": "1097000"
  },
  {
    "text": "about the first attribute which is an encoding encoding is also known as a compression to a table so there are",
    "start": "1099500",
    "end": "1106460"
  },
  {
    "text": "dozens of encoding algorithms are available like the byte bytes or run and",
    "start": "1106460",
    "end": "1112400"
  },
  {
    "text": "mostly and then L coz standard there were plenty the trade ship documentation",
    "start": "1112400",
    "end": "1118310"
  },
  {
    "text": "talks about all these in details so here mainly I'm trying to focus this is really important and why it is important",
    "start": "1118310",
    "end": "1125330"
  },
  {
    "text": "because it reduces the storage requirement and at the same time it gives the best performance customer can",
    "start": "1125330",
    "end": "1132320"
  },
  {
    "text": "get up to the 5x improvement in the performance even there is some overhead",
    "start": "1132320",
    "end": "1138020"
  },
  {
    "text": "associated associated when you do the encoding right because you are doing some and compression and uncompressing",
    "start": "1138020",
    "end": "1143930"
  },
  {
    "text": "when you are scanning it but it's still the overhead versus the gain I mean the",
    "start": "1143930",
    "end": "1149660"
  },
  {
    "text": "gain is very far too much with the compression so I highly encourage you to",
    "start": "1149660",
    "end": "1155150"
  },
  {
    "text": "set up the proper encoding for all your tables you might not see the major gain",
    "start": "1155150",
    "end": "1161210"
  },
  {
    "text": "with the small table but for the larger table you will definitely notice the improvement in the performance when the",
    "start": "1161210",
    "end": "1168800"
  },
  {
    "text": "table is empty and you run the first copy command you will see the I mean redshift decides the right encoding",
    "start": "1168800",
    "end": "1176540"
  },
  {
    "text": "automatically and intelligently but if you have the data already loaded and you",
    "start": "1176540",
    "end": "1181820"
  },
  {
    "text": "really want to validate whether the right encoding is assigned or not what you can do it you just run the analyzed",
    "start": "1181820",
    "end": "1188180"
  },
  {
    "text": "compression followed by a table name and then it will give the recommendations and just take those recommendations and",
    "start": "1188180",
    "end": "1194240"
  },
  {
    "text": "you can you can recreate the table to get the best and quality encoding",
    "start": "1194240",
    "end": "1199770"
  },
  {
    "text": "there is no view which is called the peasy table deaf from where you can",
    "start": "1199770",
    "end": "1205240"
  },
  {
    "text": "verify what is that encoding options is assigned to each column of your table",
    "start": "1205240",
    "end": "1212550"
  },
  {
    "start": "1212000",
    "end": "1212000"
  },
  {
    "text": "next attribute is a data distribution so why we define the data distribution",
    "start": "1212550",
    "end": "1218020"
  },
  {
    "text": "because it determines how the data is going to be spread across all the nodes",
    "start": "1218020",
    "end": "1224170"
  },
  {
    "text": "of your cluster and just remember this is a shared nothing architecture and this is the reason why the data",
    "start": "1224170",
    "end": "1231340"
  },
  {
    "text": "distribution matters a lot because whatever is stored in slice one it's not necessary I mean it is not storing other",
    "start": "1231340",
    "end": "1237640"
  },
  {
    "text": "slices unless you have chosen the all distribution so with certain",
    "start": "1237640",
    "end": "1242680"
  },
  {
    "text": "distribution type like the key and even it really important how you have spread",
    "start": "1242680",
    "end": "1248200"
  },
  {
    "text": "the data across all the slices so there are four options and when to choose with",
    "start": "1248200",
    "end": "1256420"
  },
  {
    "text": "all this distribution it depends on the some factors like if you have a table",
    "start": "1256420",
    "end": "1262030"
  },
  {
    "text": "very small table and you are not doing frequent modifications into that",
    "start": "1262030",
    "end": "1268600"
  },
  {
    "text": "particular table I would suggest go with the altai all distribution and normally the dimension tables are the good",
    "start": "1268600",
    "end": "1275380"
  },
  {
    "text": "candidate for the all distribution when you have a significantly large table and",
    "start": "1275380",
    "end": "1281650"
  },
  {
    "text": "you know this is a finely join on particular column with some other dimension tables then you can think of",
    "start": "1281650",
    "end": "1287680"
  },
  {
    "text": "using the key distribution when you are really not sure whether key are all then",
    "start": "1287680",
    "end": "1293500"
  },
  {
    "text": "last choice is a even distribution but best fit for the even distribution is and normally the stand alone table which",
    "start": "1293500",
    "end": "1299980"
  },
  {
    "text": "you are not normally joining it and butts in some cases but if you really do",
    "start": "1299980",
    "end": "1307570"
  },
  {
    "text": "not not able to determine what is the key and all at a table is significantly large in that case also you can you",
    "start": "1307570",
    "end": "1313600"
  },
  {
    "text": "choose that even the last option which is Auto this is a recently introduced a",
    "start": "1313600",
    "end": "1319870"
  },
  {
    "text": "feature what happens in this case when you define a distribution a style a as a",
    "start": "1319870",
    "end": "1325120"
  },
  {
    "text": "Auto and you create a table it starts with all distribution in the",
    "start": "1325120",
    "end": "1330340"
  },
  {
    "text": "beginning and as table grows ultimate it will be converted to the even distribution now let's see the example",
    "start": "1330340",
    "end": "1339990"
  },
  {
    "start": "1338000",
    "end": "1338000"
  },
  {
    "text": "rate different distribution styles so when here in this example this is a deep",
    "start": "1339990",
    "end": "1347250"
  },
  {
    "text": "dive table with three columns ordinance ID location and date now let's see how",
    "start": "1347250",
    "end": "1352529"
  },
  {
    "text": "the data is going to be stored into the node at all slices with the different",
    "start": "1352529",
    "end": "1359909"
  },
  {
    "start": "1359000",
    "end": "1359000"
  },
  {
    "text": "distribution is time so first is even so when you have even the data is going to be stored in the",
    "start": "1359909",
    "end": "1366299"
  },
  {
    "text": "round robin fashion it's all I mean it's no logic just round robin the first record when you insert the first rows it",
    "start": "1366299",
    "end": "1373169"
  },
  {
    "text": "is going to be just considered in the slice 0 of the node 1 when you insert the second record it goes to the next",
    "start": "1373169",
    "end": "1378899"
  },
  {
    "text": "slice 3rd and 4th like this way so with the even distribution definitely you",
    "start": "1378899",
    "end": "1384240"
  },
  {
    "text": "engage all the nodes to store the data and all the slices but it is sometimes",
    "start": "1384240",
    "end": "1390809"
  },
  {
    "text": "it is not very optimum because the goal for defining the data distribution is to",
    "start": "1390809",
    "end": "1396929"
  },
  {
    "text": "optimize your joint performance in because as it is a shared nothing",
    "start": "1396929",
    "end": "1402179"
  },
  {
    "text": "architecture and if you don't define the right data distribution and the data is spread across different slices it might",
    "start": "1402179",
    "end": "1409230"
  },
  {
    "text": "be possible that the dynamic distribution will happen when you scan the query because still you have to join",
    "start": "1409230",
    "end": "1415710"
  },
  {
    "text": "the data with some other slice I mean which is stored in different slice okay",
    "start": "1415710",
    "end": "1422519"
  },
  {
    "text": "slice you can think as a virtual processor so you have the one compute",
    "start": "1422519",
    "end": "1428429"
  },
  {
    "text": "node and each compute node is further partition into the slices so slice means",
    "start": "1428429",
    "end": "1433649"
  },
  {
    "text": "you get some small portion of the memory and compute from that particular node but slices is independent you know they",
    "start": "1433649",
    "end": "1441870"
  },
  {
    "text": "can run all in parallel and they store each slices to the different information",
    "start": "1441870",
    "end": "1448518"
  },
  {
    "text": "virtual processor not exactly the physical core but they are the virtual",
    "start": "1448669",
    "end": "1453720"
  },
  {
    "text": "core you can think like that way",
    "start": "1453720",
    "end": "1457669"
  },
  {
    "text": "yeah the question is how many how to determine how many slices so as the been",
    "start": "1459990",
    "end": "1466810"
  },
  {
    "text": "said there are two choices right either you can go with the DA's category or dc-dc to category and the number of",
    "start": "1466810",
    "end": "1473860"
  },
  {
    "text": "slices are pre-configured like if you chose a DC too large you are going to get the two slices in each node when you",
    "start": "1473860",
    "end": "1480700"
  },
  {
    "text": "are creating a cluster DC to a texture you get the 16 slices in each class each node so it is a predefined the only",
    "start": "1480700",
    "end": "1489370"
  },
  {
    "text": "thing you get option which type of ec2 instance you want behind the scene and we will get the number of slices based",
    "start": "1489370",
    "end": "1495850"
  },
  {
    "text": "on that next is a key distribution so in",
    "start": "1495850",
    "end": "1501550"
  },
  {
    "text": "the given example suppose you with the key distribution the first thing you have to determine which key you are",
    "start": "1501550",
    "end": "1507130"
  },
  {
    "text": "going to use for the data distribution and consider you have chosen the I mean",
    "start": "1507130",
    "end": "1512980"
  },
  {
    "text": "location column as a data distribution and the same record right the previous example if you insert the first record",
    "start": "1512980",
    "end": "1519370"
  },
  {
    "text": "and consider the sand F SFO entry it fall the hash key of the SFO comes into",
    "start": "1519370",
    "end": "1526150"
  },
  {
    "text": "slice of the node one it is stored here the next entry is JFK it may fall into",
    "start": "1526150",
    "end": "1531340"
  },
  {
    "text": "the different slices when you enter the third record it's again SFO so hash key is again going to come into the same",
    "start": "1531340",
    "end": "1537640"
  },
  {
    "text": "slices where it was stored earlier same for the JFK so what you notice here if",
    "start": "1537640",
    "end": "1544540"
  },
  {
    "text": "you are not defining the right data distribution key you will see the",
    "start": "1544540",
    "end": "1550570"
  },
  {
    "text": "skewness in the data and in this example just consider you have loaded only four",
    "start": "1550570",
    "end": "1555700"
  },
  {
    "text": "rows and all four rows are stored only the note 1 note2 doesn't have any data and when you query it only you are going",
    "start": "1555700",
    "end": "1563980"
  },
  {
    "text": "to hang is a node 1 because no 2 doesn't have any data so this is not good key",
    "start": "1563980",
    "end": "1569950"
  },
  {
    "text": "candidate to store your table yes",
    "start": "1569950",
    "end": "1576389"
  },
  {
    "text": "even distributing you should always think about first try to define as a key",
    "start": "1577460",
    "end": "1583080"
  },
  {
    "text": "distribution not even because even it is going to distribute evenly but during the scan time you will see the dynamic",
    "start": "1583080",
    "end": "1592169"
  },
  {
    "text": "redistribution of the data from one slice to a different node or one slice to different slice so to avoid the",
    "start": "1592169",
    "end": "1598950"
  },
  {
    "text": "network traffic during the scan better to choose a right distribution key but",
    "start": "1598950",
    "end": "1604740"
  },
  {
    "text": "suppose if you have the stand alone table you know right one vraja' fact table you are not joining with anything",
    "start": "1604740",
    "end": "1610320"
  },
  {
    "text": "then it's good to choose an even distribution is the first place if you",
    "start": "1610320",
    "end": "1615600"
  },
  {
    "text": "know that your table is not going to join with anything then better to spread it across all the slices so that",
    "start": "1615600",
    "end": "1621269"
  },
  {
    "text": "whenever you is can you give the equal amount of work to all the nodes of your cluster the same example if you choose",
    "start": "1621269",
    "end": "1630269"
  },
  {
    "text": "the audience ID as a distribution column so because you have the different",
    "start": "1630269",
    "end": "1637009"
  },
  {
    "start": "1632000",
    "end": "1632000"
  },
  {
    "text": "audience ID for all the records you can see the data is stored in the different slices so even with their so what is a",
    "start": "1637009",
    "end": "1645210"
  },
  {
    "text": "takeaway from here if you define the right distribution key you can see the",
    "start": "1645210",
    "end": "1650460"
  },
  {
    "text": "even distribution across all the slices and you will see the good performance when you do the joining with other",
    "start": "1650460",
    "end": "1655980"
  },
  {
    "text": "tables yeah composite keys no so in data",
    "start": "1655980",
    "end": "1663450"
  },
  {
    "text": "distribution you can define only one column you don't get options to define the multiple column is a data",
    "start": "1663450",
    "end": "1669090"
  },
  {
    "text": "distribution",
    "start": "1669090",
    "end": "1671388"
  },
  {
    "text": "so the question is can the column be a composite so if you are storing the data",
    "start": "1674730",
    "end": "1680440"
  },
  {
    "text": "you know I mean I mean the join of two columns yes then you can define it ultimately it will be treated as a one",
    "start": "1680440",
    "end": "1685570"
  },
  {
    "text": "column but the way what you are trying to store it the two values from two",
    "start": "1685570",
    "end": "1690669"
  },
  {
    "text": "different columns yes you can do that way exactly",
    "start": "1690669",
    "end": "1700320"
  },
  {
    "text": "yes exactly thanks for pointing it and now the oldest situation with the all",
    "start": "1711690",
    "end": "1719200"
  },
  {
    "text": "distribution the entire table is going to be stored in the slice zero of all",
    "start": "1719200",
    "end": "1725650"
  },
  {
    "start": "1722000",
    "end": "1722000"
  },
  {
    "text": "the nodes so the same record see in the first record it's stored in the both the nodes second record third record fourth",
    "start": "1725650",
    "end": "1733539"
  },
  {
    "text": "record but only the first place so this is very good in case of when you don't have the appropriate a column which is",
    "start": "1733539",
    "end": "1740950"
  },
  {
    "text": "going to be used very often with the joins and the table is not very large because if the table is significantly",
    "start": "1740950",
    "end": "1747820"
  },
  {
    "text": "just considered terabytes then you are storing the two copies right in the cluster so you are at the same time you",
    "start": "1747820",
    "end": "1754000"
  },
  {
    "text": "are consuming more space into that cluster but the benefit is other side",
    "start": "1754000",
    "end": "1759270"
  },
  {
    "text": "even the same data is not co-located in the same slices you are not doing the",
    "start": "1759270",
    "end": "1764530"
  },
  {
    "text": "dynamic distribution during the scan the data is physically available into that node so if the data is co-located into",
    "start": "1764530",
    "end": "1771100"
  },
  {
    "text": "the same node it is definitely going to deliver the first result now let's talk",
    "start": "1771100",
    "end": "1778809"
  },
  {
    "text": "about the zone maps so zone maps are the in-memory metadata information for each",
    "start": "1778809",
    "end": "1785230"
  },
  {
    "text": "block it contains the minimum and the maximum value which is stored into that",
    "start": "1785230",
    "end": "1790270"
  },
  {
    "text": "particular block so why it is important because just after scanning the method",
    "start": "1790270",
    "end": "1796720"
  },
  {
    "text": "at this zone map information the richardville query optimizer will know okay my data rainfalls for this block or",
    "start": "1796720",
    "end": "1803770"
  },
  {
    "text": "not if it doesn't well it is not going to scan so it reduces the number of i/os",
    "start": "1803770",
    "end": "1809140"
  },
  {
    "text": "you are going to perform to scan the table now let us see the third attribute",
    "start": "1809140",
    "end": "1818140"
  },
  {
    "start": "1815000",
    "end": "1815000"
  },
  {
    "text": "which is sort keys short Keys define the physical order of your data into the",
    "start": "1818140",
    "end": "1824049"
  },
  {
    "text": "disk how the data is actually going to be storing - that is and you get two",
    "start": "1824049",
    "end": "1829659"
  },
  {
    "text": "options yeah there are two types of short Keys you can define when is a compound sort keys and the second is",
    "start": "1829659",
    "end": "1835630"
  },
  {
    "text": "interleaved short Keys in the short Keys cases you can define the multiple",
    "start": "1835630",
    "end": "1841179"
  },
  {
    "text": "columns it's not necessary to have one one column in one short case and if we",
    "start": "1841179",
    "end": "1847570"
  },
  {
    "text": "you can think the short keys as an index but it's not actually in an index but",
    "start": "1847570",
    "end": "1853690"
  },
  {
    "text": "you the way it functions you can think here and this is a kind of index for a table and if when does you have the sort",
    "start": "1853690",
    "end": "1864250"
  },
  {
    "text": "keys define and because zone maps conformation is already stored you can see with the help of the rights I mean",
    "start": "1864250",
    "end": "1871720"
  },
  {
    "text": "sort keys you reduce the number of i/o calls take the example of this so you",
    "start": "1871720",
    "end": "1878530"
  },
  {
    "start": "1877000",
    "end": "1877000"
  },
  {
    "text": "have the deep dive table same table when the table is not sorted you can see the same record maybe just 9th of June to",
    "start": "1878530",
    "end": "1885720"
  },
  {
    "text": "2017 it might be spread across you know just multiple blocks here three blocks",
    "start": "1885720",
    "end": "1892570"
  },
  {
    "text": "but if the data is sorted all the tech can fit in you know the same block or",
    "start": "1892570",
    "end": "1897580"
  },
  {
    "text": "maybe the subsequent number of the blocks you can see the number of i/os immediately reduced with the same data",
    "start": "1897580",
    "end": "1903940"
  },
  {
    "text": "sets just after having the right sort keys so you eliminate a lot of iOS so",
    "start": "1903940",
    "end": "1913600"
  },
  {
    "start": "1912000",
    "end": "1912000"
  },
  {
    "text": "let's start with the compound sort case this is a default option if you don't put anything like any specific key I",
    "start": "1913600",
    "end": "1920860"
  },
  {
    "text": "like the compound or interleaved by default it is a compound sort case in",
    "start": "1920860",
    "end": "1925960"
  },
  {
    "text": "the sort keys as I said you can choose a multiple columns but the column order",
    "start": "1925960",
    "end": "1931210"
  },
  {
    "text": "matters because if you are coming from some different or DBMS background people",
    "start": "1931210",
    "end": "1937030"
  },
  {
    "text": "might think ok you have the created an index on three columns and let's take the example of Oracle right and you have",
    "start": "1937030",
    "end": "1944200"
  },
  {
    "text": "you are not using the first column still you take the advantages of the you know the indexes using the skips can skip the",
    "start": "1944200",
    "end": "1951190"
  },
  {
    "text": "scan operations but if you have defined a sort keys on just considered column",
    "start": "1951190",
    "end": "1958179"
  },
  {
    "text": "ABC and you are not running and when you",
    "start": "1958179",
    "end": "1963190"
  },
  {
    "text": "are running that query and in the where clause if you are not using the column a the sort you are not taking advantage of",
    "start": "1963190",
    "end": "1969549"
  },
  {
    "text": "the sort case so in order to take the advantage all those columns should be coming as a where",
    "start": "1969549",
    "end": "1975309"
  },
  {
    "text": "I mean as a predicate in the where class if it is not then you are not taking the",
    "start": "1975309",
    "end": "1980980"
  },
  {
    "text": "advantage so always think which columns you are a frenemy used in the where clause and try to create short kissing",
    "start": "1980980",
    "end": "1987759"
  },
  {
    "text": "those columns and another thing the column order matters so suppose you know",
    "start": "1987759",
    "end": "1995289"
  },
  {
    "text": "that column a B and all three see all three you are going to use it very of enemy so in that case I will say put the",
    "start": "1995289",
    "end": "2004620"
  },
  {
    "text": "column which is a low which has a low cardinality as a first column because",
    "start": "2004620",
    "end": "2009960"
  },
  {
    "text": "ultimately you know the way you traverse to scan the block if you will have prune",
    "start": "2009960",
    "end": "2014999"
  },
  {
    "text": "most of the number of blocks in the first scan itself you are not scanning more number of",
    "start": "2014999",
    "end": "2020850"
  },
  {
    "text": "blocks because that is the next step in that's the reason I said this is a kind of index it's not really the physical",
    "start": "2020850",
    "end": "2027480"
  },
  {
    "text": "structure it is another metadata stored in the block itself so the second option",
    "start": "2027480",
    "end": "2037440"
  },
  {
    "start": "2035000",
    "end": "2035000"
  },
  {
    "text": "is interleaved sort case there are some use cases right it is possible that you",
    "start": "2037440",
    "end": "2044759"
  },
  {
    "text": "have created a sort keys in three columns a B and C and sometimes you are accessing you are doing the query and",
    "start": "2044759",
    "end": "2052740"
  },
  {
    "text": "put the where clause on just the column a sometimes column B and some some column C and might be some combination",
    "start": "2052740",
    "end": "2058250"
  },
  {
    "text": "but still you want to take the advantage of sort case in that case this is a one",
    "start": "2058250",
    "end": "2064349"
  },
  {
    "text": "I mean interleaved sort keys can help you if you define when you are creating",
    "start": "2064349",
    "end": "2070200"
  },
  {
    "text": "a sort keys big you are defining the sort keys and you make mention it as I like the interleaved sort keys then",
    "start": "2070200",
    "end": "2077000"
  },
  {
    "text": "retrieved create gives the equal weight to all columns or whatever you have",
    "start": "2077000",
    "end": "2082440"
  },
  {
    "text": "chosen in the in the as a form of short case so irrespective whether those by",
    "start": "2082440",
    "end": "2088290"
  },
  {
    "text": "one particular column is used or not still you are going to take the advantage of sort case I know this looks",
    "start": "2088290",
    "end": "2096358"
  },
  {
    "text": "okay this is a one solution right by default why not to jump on the internet sort case I will suggest don't",
    "start": "2096359",
    "end": "2103930"
  },
  {
    "text": "your table design with an interleaved sort keys this is a last option majora",
    "start": "2103930",
    "end": "2109450"
  },
  {
    "text": "based on the what we have seen from the customer 99% of the use cases just fit",
    "start": "2109450",
    "end": "2115000"
  },
  {
    "text": "for the compound short case and the second thing interleaved sort keys there isn't a some overhead associated where",
    "start": "2115000",
    "end": "2121390"
  },
  {
    "text": "because of the complex structure it is based on z-index it is really complex and in order to maintain the z-index",
    "start": "2121390",
    "end": "2127450"
  },
  {
    "text": "when you do some DML it adds some overhead and when we run the vacuum",
    "start": "2127450",
    "end": "2133530"
  },
  {
    "text": "because that is another process I think in the later of the sections Ben is going to talk about that so in order to",
    "start": "2133530",
    "end": "2140860"
  },
  {
    "text": "just to delete the clean the deleted blocks that is a vacuum is a process",
    "start": "2140860",
    "end": "2146550"
  },
  {
    "text": "vacuum runs longer so whenever you are thinking about having the interleaved",
    "start": "2146550",
    "end": "2153580"
  },
  {
    "text": "sort keys I will suggest open a case with Amazon redshift I mean support case and then take their",
    "start": "2153580",
    "end": "2159880"
  },
  {
    "text": "guidance as well and use it very cautiously because yeah I know one place it gives a advantage but other place",
    "start": "2159880",
    "end": "2166690"
  },
  {
    "text": "there is some disadvantage also associated but it doesn't mean that you",
    "start": "2166690",
    "end": "2171730"
  },
  {
    "text": "cannot use this some people are you really using it and taking the advantages now let's talk about the four",
    "start": "2171730",
    "end": "2180220"
  },
  {
    "text": "component which is a wlm Amazon redshift provide a sophisticated tool to handle",
    "start": "2180220",
    "end": "2186820"
  },
  {
    "text": "your workload which is also known as a wnm so through that wlm primarily you",
    "start": "2186820",
    "end": "2192760"
  },
  {
    "text": "control the concurrency and the memory allocation whenever you create a",
    "start": "2192760",
    "end": "2198190"
  },
  {
    "text": "redshift cluster default parameter group WL MQ I mean parameter group will come",
    "start": "2198190",
    "end": "2204280"
  },
  {
    "text": "which will have one super user queue and when the user queue with five slots when",
    "start": "2204280",
    "end": "2211510"
  },
  {
    "text": "I say the five slot it means only five queries can run concurrently at the same",
    "start": "2211510",
    "end": "2217090"
  },
  {
    "text": "time onto the cluster which is normally you know I mean sometimes you might think you have because if you have not",
    "start": "2217090",
    "end": "2223000"
  },
  {
    "text": "created your own WL MQ and the default is coming with five connections and if you are running more queries even though",
    "start": "2223000",
    "end": "2229480"
  },
  {
    "text": "cause cluster can have potential you might say oh the cluster is running very less query at a time it is possible",
    "start": "2229480",
    "end": "2236420"
  },
  {
    "text": "and major disadvantage with having just default user qyz because if you will",
    "start": "2236420",
    "end": "2242750"
  },
  {
    "text": "have some mixed workload right some long careers are running some short queries are running and if too many long queries",
    "start": "2242750",
    "end": "2248180"
  },
  {
    "text": "are entered into the cluster the long as most returning queries will be just waiting waiting waiting and ultimately",
    "start": "2248180",
    "end": "2254380"
  },
  {
    "text": "it will give the unpredictable performance away the user might think hey my query was running in two seconds",
    "start": "2254380",
    "end": "2260180"
  },
  {
    "text": "why it's just because it is waiting but from end-user perspective that it has not written the results so you might see",
    "start": "2260180",
    "end": "2267230"
  },
  {
    "text": "the unpredictable performance so how can you avoid that simply create a multiple",
    "start": "2267230",
    "end": "2272569"
  },
  {
    "text": "wlm cures simple example you can have the two WL m q1 for short user and one",
    "start": "2272569",
    "end": "2279289"
  },
  {
    "text": "for the long-running users and there isn't a ways you can die avert your queries to specific designated queue so",
    "start": "2279289",
    "end": "2287809"
  },
  {
    "text": "like that you can use the user group or you can use a query rule to define okay",
    "start": "2287809",
    "end": "2293599"
  },
  {
    "text": "these queries are the candidate for short queries and these are the queries candidate for the long group yeah thanks",
    "start": "2293599",
    "end": "2304730"
  },
  {
    "text": "the question is is there any cost associated to the wlm the answer is no it comes with the redshift you don't",
    "start": "2304730",
    "end": "2311599"
  },
  {
    "text": "have to pay you can create even three queues for queues in reality people normally create two to three w LM q1 for",
    "start": "2311599",
    "end": "2320059"
  },
  {
    "text": "ETL purpose where they are really doing the heavy intensive jobs and there they",
    "start": "2320059",
    "end": "2325190"
  },
  {
    "text": "are located more memory with less number of concurrent sessions then another queue people things for the dashboard",
    "start": "2325190",
    "end": "2330859"
  },
  {
    "text": "where you allocate the less number of memory but you can handle the more concurrency and third one further and a",
    "start": "2330859",
    "end": "2336950"
  },
  {
    "text": "half purpose so yes so yes so I think I",
    "start": "2336950",
    "end": "2345609"
  },
  {
    "text": "don't have that here right you can see I mean the memory you can define the",
    "start": "2345609",
    "end": "2351319"
  },
  {
    "text": "memory and the number of slots you can really define it by own each W lmq can",
    "start": "2351319",
    "end": "2357799"
  },
  {
    "text": "have the different memory percentage with different number of concurrent slots",
    "start": "2357799",
    "end": "2363880"
  },
  {
    "start": "2364000",
    "end": "2364000"
  },
  {
    "text": "over the period of the time the Richard engineering team has added a lot many",
    "start": "2365070",
    "end": "2370420"
  },
  {
    "text": "features with the wlm based on the feedback we are constantly hearing from the customers so one is a sqa the first",
    "start": "2370420",
    "end": "2378970"
  },
  {
    "text": "great feature sqa is a short query acceleration what happens Amazon redshift he uses some machine running",
    "start": "2378970",
    "end": "2385900"
  },
  {
    "text": "machine algorithm to determine okay these are very short queues which can be",
    "start": "2385900",
    "end": "2391240"
  },
  {
    "text": "finished less than 10 second and 20 second why to put in some different WL MQ and keep fitting it until it rain",
    "start": "2391240",
    "end": "2398410"
  },
  {
    "text": "isn't so what it will do it will run the algorithm it's constantly running it's not like that it will run on that time",
    "start": "2398410",
    "end": "2404470"
  },
  {
    "text": "so we run the machine learning all the times behind for your cluster and as we",
    "start": "2404470",
    "end": "2409690"
  },
  {
    "text": "detect hey this is the short queries and you have enabled the sqa option for your cluster it will take your jobs and run",
    "start": "2409690",
    "end": "2416770"
  },
  {
    "text": "into the dedicated space and return the results so it will not take the route of",
    "start": "2416770",
    "end": "2423460"
  },
  {
    "text": "the standard wmq you have defined this is really great and you can with the SQL",
    "start": "2423460",
    "end": "2429100"
  },
  {
    "text": "you get option you can define ok 0 second to 20 seconds or you can just leave it on everything to the redshift",
    "start": "2429100",
    "end": "2435130"
  },
  {
    "text": "itself so if redshift things okay this is my three second four second query let's run it through the square or",
    "start": "2435130",
    "end": "2441250"
  },
  {
    "text": "not second is concurrency scaling this is a recently introduced I think last week",
    "start": "2441250",
    "end": "2448150"
  },
  {
    "text": "only we announced now it is in GA it's a really cool feature and after the",
    "start": "2448150",
    "end": "2455310"
  },
  {
    "text": "concurrency is scaling now you can scale your I mean scale that compute and",
    "start": "2455310",
    "end": "2462070"
  },
  {
    "text": "storage separately so what does it means that suppose your cluster you have",
    "start": "2462070",
    "end": "2467560"
  },
  {
    "text": "created a cluster maybe 10 node connect restore or five northwestern and you are able to run certain number of the jobs but at some given point of time high",
    "start": "2467560",
    "end": "2474700"
  },
  {
    "text": "traffic may come and then your queue is will be piled up how you are going to handle it if you have enabled the",
    "start": "2474700",
    "end": "2481240"
  },
  {
    "text": "concurrency scaling then redshift will spurn a new cluster transient rates of",
    "start": "2481240",
    "end": "2488500"
  },
  {
    "text": "cluster behind the same and it will route the queries to your transient",
    "start": "2488500",
    "end": "2493900"
  },
  {
    "text": "cluster deliver the result and as the query goes down the transient cluster will be deleted",
    "start": "2493900",
    "end": "2499720"
  },
  {
    "text": "automatically you don't have to do anything it will happen automatically behind the scene so with the concurrency",
    "start": "2499720",
    "end": "2506320"
  },
  {
    "text": "scaling you can achieve the max I mean the consistent throughput from your cluster even when the load is high on",
    "start": "2506320",
    "end": "2513040"
  },
  {
    "text": "your cluster and definitely there is some cost associated right people will think hey because you are creating",
    "start": "2513040",
    "end": "2519190"
  },
  {
    "text": "another cluster the good thing is most of the customers are able to use a concurrency scaling with without being",
    "start": "2519190",
    "end": "2526690"
  },
  {
    "text": "anything because we give one hour credit every day if you have the register every",
    "start": "2526690",
    "end": "2534550"
  },
  {
    "text": "day you get one hour free but if you are crossing the one hour I mean more than one hour then you will have to pay the",
    "start": "2534550",
    "end": "2540070"
  },
  {
    "text": "on-demand price for that the last thing is a dynamic wlm so this is another",
    "start": "2540070",
    "end": "2547270"
  },
  {
    "text": "common feedback we hear from customer you know I mean visas right create multiple queues and determine okay what",
    "start": "2547270",
    "end": "2553420"
  },
  {
    "text": "is the number of slots you want to configure it and sometimes due to many",
    "start": "2553420",
    "end": "2558460"
  },
  {
    "text": "reasons right people are not able to guess it what is the right number of slots for my wlm queue and what happens",
    "start": "2558460",
    "end": "2566470"
  },
  {
    "text": "because suppose you have the wlm cute and you define 50% memory and you are running two sessions it means that you",
    "start": "2566470",
    "end": "2573220"
  },
  {
    "text": "are giving 25% of the memory to each session even though you are able to use",
    "start": "2573220",
    "end": "2578650"
  },
  {
    "text": "it or not use it maybe you are just running very small query with the dynamic wlm the good thing is Amazon",
    "start": "2578650",
    "end": "2585610"
  },
  {
    "text": "redshift ran our own algorithms to determine what is the best slot count",
    "start": "2585610",
    "end": "2591520"
  },
  {
    "text": "for your wlm so it will based on your traffic based on your work load it will",
    "start": "2591520",
    "end": "2599640"
  },
  {
    "text": "decide how many concurrent queries tauren in your cluster so in this way no",
    "start": "2599640",
    "end": "2604990"
  },
  {
    "text": "need to set up how many slots for your wlm it will be handled by redshift itself really great features this is in",
    "start": "2604990",
    "end": "2612550"
  },
  {
    "text": "beta not in GA but you can still enroll and you can request for access if you want to do some pocs",
    "start": "2612550",
    "end": "2620310"
  },
  {
    "start": "2619000",
    "end": "2619000"
  },
  {
    "text": "wlm best practices in redshift wlm",
    "start": "2620590",
    "end": "2626470"
  },
  {
    "text": "number of concurrent slots are limited to the 50 but I will suggest don't",
    "start": "2626470",
    "end": "2633360"
  },
  {
    "text": "configure your parameter group for more than 15 connections because redshift is",
    "start": "2633360",
    "end": "2639190"
  },
  {
    "text": "designed to give the best report it's not the ultimate rejection systems right where you want the immediate response",
    "start": "2639190",
    "end": "2645310"
  },
  {
    "text": "you get the customer will have requirement like this I want to run thousand queries in per seconds or maybe",
    "start": "2645310",
    "end": "2651160"
  },
  {
    "text": "10 minutes so it doesn't matter each queries you know a thousand queries",
    "start": "2651160",
    "end": "2656650"
  },
  {
    "text": "running immediately you have some window okay one hour or 10 minutes on these many jobs if you define the less number",
    "start": "2656650",
    "end": "2664540"
  },
  {
    "text": "of session it means you are getting giving them more resources to your your job so your jobs can finish very fast",
    "start": "2664540",
    "end": "2672070"
  },
  {
    "text": "and it is really optimized to deliver the best report I will say don't go",
    "start": "2672070",
    "end": "2678460"
  },
  {
    "text": "beyond 15 maybe if you have just dashboard kind of queries you can think",
    "start": "2678460",
    "end": "2683470"
  },
  {
    "text": "of 20 connections but I mean 20 wlm snorts but normally less than 15 our",
    "start": "2683470",
    "end": "2689820"
  },
  {
    "text": "second thing is yes you can create up to eight custom u-w-l mq",
    "start": "2690000",
    "end": "2695170"
  },
  {
    "text": "but I will recommend you to limit to three and anyway you have some",
    "start": "2695170",
    "end": "2701380"
  },
  {
    "text": "additional feature like the concurrency scaling sqa any way you are handling some of the traffic there or - so limit",
    "start": "2701380",
    "end": "2708820"
  },
  {
    "text": "your number of custom queues to three second thing third thing for different",
    "start": "2708820",
    "end": "2716470"
  },
  {
    "text": "type of workload use a different W lmq so that the same patterns the users can",
    "start": "2716470",
    "end": "2722800"
  },
  {
    "text": "get the same performance like I explained right sort users to run in swati query group and the long-running",
    "start": "2722800",
    "end": "2729520"
  },
  {
    "text": "user divert into the long-running queue the four thing is keep the 5% memory",
    "start": "2729520",
    "end": "2738340"
  },
  {
    "text": "unallocated so as I said right you can define the memory for each W lmq so",
    "start": "2738340",
    "end": "2743350"
  },
  {
    "text": "let's say you have a 3w lmq option is defined just I am giving a random option",
    "start": "2743350",
    "end": "2748870"
  },
  {
    "text": "30 40 30 and you are giving 100% but there will be some situation you",
    "start": "2748870",
    "end": "2754369"
  },
  {
    "text": "know maybe you define 30% but you might need some ridden with more memory and if you reserved some memory unallocated",
    "start": "2754369",
    "end": "2760940"
  },
  {
    "text": "like 30 30 35 you reserved 5% okay that 5% can be located to anywhere based on",
    "start": "2760940",
    "end": "2768500"
  },
  {
    "text": "the need so it is recommended to leave the 5% memory unallocated and then",
    "start": "2768500",
    "end": "2776210"
  },
  {
    "text": "definitely enable the s Q and the Q M are rules because it really helps I think I am NOT talking about that PMR",
    "start": "2776210",
    "end": "2782630"
  },
  {
    "text": "rules the kurma rules are I can say they are the performance based some matrices",
    "start": "2782630",
    "end": "2788600"
  },
  {
    "text": "and you can enforce some rules like if you can create a some rule if your query",
    "start": "2788600",
    "end": "2794330"
  },
  {
    "text": "scans more than 10,000 blocks okay hop to the next Q or about the query or just",
    "start": "2794330",
    "end": "2801040"
  },
  {
    "text": "lock the query and you can do the further troubleshooting you can play I",
    "start": "2801040",
    "end": "2807320"
  },
  {
    "text": "mean the data most of the parameters of there WI limbs can be changed dynamically I mean you can play around",
    "start": "2807320",
    "end": "2812750"
  },
  {
    "text": "it like the number of connections memory a percentage allocation so like night time you might think okay I am running",
    "start": "2812750",
    "end": "2819050"
  },
  {
    "text": "heavy ETL jobs ETL wmq maybe give the 60% of the memory in the day time your",
    "start": "2819050",
    "end": "2825850"
  },
  {
    "text": "reporting is happening mainly so in the wmq responsible for e2 reporting give",
    "start": "2825850",
    "end": "2832100"
  },
  {
    "text": "them more memory you can do this flexibility and but someone has to do it right so either build your own",
    "start": "2832100",
    "end": "2837859"
  },
  {
    "text": "algorithms or own logic put it in Python or ec2 and run it or you can take the",
    "start": "2837859",
    "end": "2842930"
  },
  {
    "text": "advantage of some github codes available in I mean provided by the AWS just",
    "start": "2842930",
    "end": "2848690"
  },
  {
    "text": "deploy it and you get some configuration options and even you can kick off through the lambda also at this time",
    "start": "2848690",
    "end": "2854930"
  },
  {
    "text": "okay make this wlm like this and the different time you can change the WLAN parameters now",
    "start": "2854930",
    "end": "2862570"
  },
  {
    "start": "2861000",
    "end": "2861000"
  },
  {
    "text": "[Music]",
    "start": "2862570",
    "end": "2865699"
  },
  {
    "text": "all right thanks a lot Sudhir okay so just a couple more modules here on",
    "start": "2868670",
    "end": "2874430"
  },
  {
    "text": "redshift so at this point we understand a lot about the internals and the",
    "start": "2874430",
    "end": "2881370"
  },
  {
    "text": "performance tuning of redshift let's do a deep dive on getting data into redshift so there's a number of options",
    "start": "2881370",
    "end": "2888480"
  },
  {
    "text": "for loading data into redshift there's definitely some best practices here that",
    "start": "2888480",
    "end": "2894450"
  },
  {
    "text": "we want to encourage folks to adhere to so it is possible to do row level",
    "start": "2894450",
    "end": "2900020"
  },
  {
    "text": "inserts into redshift but this is not the most high performance basically we",
    "start": "2900020",
    "end": "2907020"
  },
  {
    "text": "have taken the copy command and we've optimized the redshift copy command to",
    "start": "2907020",
    "end": "2912990"
  },
  {
    "text": "work with Amazon s3 and there's actually a very small bug in this slide",
    "start": "2912990",
    "end": "2919740"
  },
  {
    "text": "unfortunately it was given to me as a bitmap and I couldn't enough time to recreate the entire image but basically",
    "start": "2919740",
    "end": "2926880"
  },
  {
    "text": "what we're saying here is that using the redshift copy command we can break our",
    "start": "2926880",
    "end": "2933240"
  },
  {
    "text": "input files into smaller files right so if let's say we've got a 1 gigabyte file",
    "start": "2933240",
    "end": "2940530"
  },
  {
    "text": "we might cut that up into 10 100 megabyte files or if we've got a you",
    "start": "2940530",
    "end": "2946380"
  },
  {
    "text": "know a large data set we might use glue or we might use the Linux split command",
    "start": "2946380",
    "end": "2951510"
  },
  {
    "text": "and we want to crack that file into smaller pieces and load them in parallel and so the redshift copy command is",
    "start": "2951510",
    "end": "2958590"
  },
  {
    "text": "going to allow us to do that there's also some operators like unload and create table as that are going to be",
    "start": "2958590",
    "end": "2965940"
  },
  {
    "text": "much more high-performance than just doing row level inserts so we definitely encourage folks to take a look at the",
    "start": "2965940",
    "end": "2973770"
  },
  {
    "text": "copy command when they think about loading data so what does that look like specifically so again you go to your",
    "start": "2973770",
    "end": "2980400"
  },
  {
    "start": "2976000",
    "end": "2976000"
  },
  {
    "text": "sequel command line you're going to say copy table from location the location is",
    "start": "2980400",
    "end": "2985590"
  },
  {
    "text": "going to be an s3 bucket or folder you're gonna pass in some credentials",
    "start": "2985590",
    "end": "2990600"
  },
  {
    "text": "there's actually two methods on the slide I definitely don't encourage method",
    "start": "2990600",
    "end": "2996750"
  },
  {
    "text": "number one so it is possible to pasen your secret key pair to the copy",
    "start": "2996750",
    "end": "3001920"
  },
  {
    "text": "command this is absolutely not a best practice why because you're gonna take this load script and you're gonna check",
    "start": "3001920",
    "end": "3007589"
  },
  {
    "text": "it into github and that's that's gonna get you somewhere you don't want to go so the best practices are to use this I",
    "start": "3007589",
    "end": "3015959"
  },
  {
    "text": "am roll and specify an arn and that role will have permissions to do a load into",
    "start": "3015959",
    "end": "3022469"
  },
  {
    "text": "your redshift cluster so definitely take the time to learn about setting that I am roll-up and specifying that Arn in",
    "start": "3022469",
    "end": "3030359"
  },
  {
    "text": "the copy command and then you're gonna specify the region that you're targeting for the load there's a bunch of options",
    "start": "3030359",
    "end": "3038420"
  },
  {
    "text": "for the copy command we talked about park' files a lot so in December we",
    "start": "3038420",
    "end": "3044430"
  },
  {
    "text": "announced parquet support for the copy command so in addition to CSV Jason Avro",
    "start": "3044430",
    "end": "3051869"
  },
  {
    "text": "files we also now support parquet files there's a number of compression and encryption options again we want you to",
    "start": "3051869",
    "end": "3058890"
  },
  {
    "text": "really have high performance copies so that you can do these frequently to keep",
    "start": "3058890",
    "end": "3065489"
  },
  {
    "text": "your data loaded as frequently as possible right so we don't want the case",
    "start": "3065489",
    "end": "3071729"
  },
  {
    "text": "where you know we're loading data once a month we'd love to see you guys be able to do it every week every day every hour",
    "start": "3071729",
    "end": "3078209"
  },
  {
    "text": "every 15 minutes we want this to be as close to a real time operation as we can",
    "start": "3078209",
    "end": "3083910"
  },
  {
    "text": "to make that data available to your user and so the more performance tuning you",
    "start": "3083910",
    "end": "3089160"
  },
  {
    "text": "use on the copy command the fresher your data is going to be in that data warehouse and again there's some mild",
    "start": "3089160",
    "end": "3095999"
  },
  {
    "text": "transformation options like you can drop columns and stuff this is not meant to be an ETL tool this is just really to",
    "start": "3095999",
    "end": "3103709"
  },
  {
    "text": "get that ETL cleaned up data into redshift as quickly as possible but you",
    "start": "3103709",
    "end": "3110940"
  },
  {
    "text": "heard Sudhir talk about nodes and slices and the redshift copy command let's say",
    "start": "3110940",
    "end": "3116789"
  },
  {
    "text": "we've got a cluster and it's got a ten node cluster we want to have at least ten input files so that we can load them",
    "start": "3116789",
    "end": "3124440"
  },
  {
    "text": "all in parallel right so if we've got one big input file we're gonna hit one",
    "start": "3124440",
    "end": "3130259"
  },
  {
    "start": "3130000",
    "end": "3130000"
  },
  {
    "text": "node in the cluster we're gonna dump that entire file into that one node and the other nine nodes",
    "start": "3130259",
    "end": "3136670"
  },
  {
    "text": "in our cluster are gonna sit idle right so what we want to try to do is again",
    "start": "3136670",
    "end": "3142160"
  },
  {
    "text": "either using the Linux split command or ideally AWS glue to ETL it into smaller",
    "start": "3142160",
    "end": "3149210"
  },
  {
    "text": "files that we can then move in parallel for maximum efficiency so basically have",
    "start": "3149210",
    "end": "3155840"
  },
  {
    "text": "as many input files as you have slices so in DC to large we've got two slices",
    "start": "3155840",
    "end": "3163460"
  },
  {
    "text": "in ten node cluster that's 20 slices you want to have 20 input files or 40 input",
    "start": "3163460",
    "end": "3170090"
  },
  {
    "text": "files or you know some multiple of 20 so that your load can happen as quickly as",
    "start": "3170090",
    "end": "3176420"
  },
  {
    "text": "possible and then again unloading so there's cases where you want to unload",
    "start": "3176420",
    "end": "3184400"
  },
  {
    "text": "the entire database and store it in s3 maybe you want to recreate a test",
    "start": "3184400",
    "end": "3190100"
  },
  {
    "text": "environment so you're gonna dump out production you're gonna fuzz that data to get rid of any PII and then you're",
    "start": "3190100",
    "end": "3198170"
  },
  {
    "text": "gonna put that data into a test environment you'd use the unload command for that and again supports a number of",
    "start": "3198170",
    "end": "3205190"
  },
  {
    "text": "file formats and number of different options and integrates really well with s3 so we talked to customers all the",
    "start": "3205190",
    "end": "3214460"
  },
  {
    "text": "time they loved the performance of redshift it's the right tool for their large-scale data warehouse workloads but",
    "start": "3214460",
    "end": "3222500"
  },
  {
    "text": "they said it was a little bit of a pain in the neck in terms of the care and feeding of the cluster and they asked us",
    "start": "3222500",
    "end": "3228680"
  },
  {
    "text": "what can we do to make operating the redshift cluster a little bit easier and so we recently introduced the ability to",
    "start": "3228680",
    "end": "3235970"
  },
  {
    "text": "do an auto analyze and an auto vacuum on the cluster and basically what that means is instead of you having to have a",
    "start": "3235970",
    "end": "3244100"
  },
  {
    "text": "specific step in your workflow where you vacuum to reclaim the space occupied by",
    "start": "3244100",
    "end": "3250670"
  },
  {
    "text": "those dead rows we're gonna do that in the background and instead of you having",
    "start": "3250670",
    "end": "3255950"
  },
  {
    "text": "to have a specific maintenance task to update your indexes and your metadata",
    "start": "3255950",
    "end": "3261650"
  },
  {
    "text": "we're gonna run Auto analyze in the background and we're gonna keep all that meted updated for you so again these are steps",
    "start": "3261650",
    "end": "3269869"
  },
  {
    "text": "that people traditionally had to build in we've copied some data into the",
    "start": "3269869",
    "end": "3275059"
  },
  {
    "text": "cluster and now I have to run analyze to update all my indexes now we're gonna do",
    "start": "3275059",
    "end": "3280130"
  },
  {
    "text": "that behind the scenes for you yes so",
    "start": "3280130",
    "end": "3292369"
  },
  {
    "text": "there is some intelligent so the question was when does it run so there is some intelligence built into Auto analyze an auto vacuum about when it",
    "start": "3292369",
    "end": "3299480"
  },
  {
    "text": "runs and as Sudhir said there's a lot of self-awareness and the redshift cluster",
    "start": "3299480",
    "end": "3304789"
  },
  {
    "text": "in terms of its performance and self-healing and optimization and so this will look for dips in your workload",
    "start": "3304789",
    "end": "3312170"
  },
  {
    "text": "to kick in because there is a performance penalty these are necessary",
    "start": "3312170",
    "end": "3317450"
  },
  {
    "text": "operations and I have customers that are afraid to run analyze and afraid to run",
    "start": "3317450",
    "end": "3322609"
  },
  {
    "text": "vacuum because it's gonna slow down their cluster and this leads to disaster because your index has become out of",
    "start": "3322609",
    "end": "3328880"
  },
  {
    "text": "date and your queries start to slow down and your cluster starts to fill up",
    "start": "3328880",
    "end": "3333920"
  },
  {
    "text": "because you've got all those dead rows in there so you absolutely want to do maintenance on your cluster it's not",
    "start": "3333920",
    "end": "3340160"
  },
  {
    "text": "something that can be kicked down the road you can't kick this can down the road and to make that a little bit",
    "start": "3340160",
    "end": "3345859"
  },
  {
    "text": "easier for customers we've implemented these two functions Auto analyze an auto vacuum that will make some intelligent",
    "start": "3345859",
    "end": "3353059"
  },
  {
    "text": "decisions about when to run and run off and to keep those metadata catalogs in",
    "start": "3353059",
    "end": "3359210"
  },
  {
    "text": "that space utilization optimized and",
    "start": "3359210",
    "end": "3365900"
  },
  {
    "text": "then resizing so there's a new option called elastic resize and this is going",
    "start": "3365900",
    "end": "3371329"
  },
  {
    "start": "3366000",
    "end": "3366000"
  },
  {
    "text": "to give you the ability to add nodes of the same type to your cluster so let's",
    "start": "3371329",
    "end": "3377480"
  },
  {
    "text": "say we've got a four node DC to large cluster with I don't know what is that",
    "start": "3377480",
    "end": "3383390"
  },
  {
    "text": "six hundred and forty gigs of storage and we want to add another four nodes to",
    "start": "3383390",
    "end": "3388490"
  },
  {
    "text": "it traditionally we would put the cluster in read-only mode and we would copy the data to a new cluster and this",
    "start": "3388490",
    "end": "3395599"
  },
  {
    "text": "would take hours this is a lengthy process with the last degrees we're able to add nodes at the same time",
    "start": "3395599",
    "end": "3402530"
  },
  {
    "text": "very very quickly in a period of minutes now the caveat there is if you want to",
    "start": "3402530",
    "end": "3411200"
  },
  {
    "text": "switch from let's say DC to dense compute nodes to ds2 dense storage nodes",
    "start": "3411200",
    "end": "3418090"
  },
  {
    "text": "you're gonna have to use the traditional classic resizing method but again this",
    "start": "3418090",
    "end": "3424910"
  },
  {
    "text": "really makes it easy to provision your cluster for exactly the number of nodes",
    "start": "3424910",
    "end": "3429920"
  },
  {
    "text": "you need for your workload and then grow it as required so we encourage folks to",
    "start": "3429920",
    "end": "3435790"
  },
  {
    "text": "create a cluster with a certain amount of headroom and then when their data",
    "start": "3435790",
    "end": "3440900"
  },
  {
    "text": "starts to fill up and maybe they hit 70 percent they can allocate four more nodes and then they get to 70 percent",
    "start": "3440900",
    "end": "3447200"
  },
  {
    "text": "and they can allocate four more or whatever the number is that make sense for your environment and that way you're",
    "start": "3447200",
    "end": "3453320"
  },
  {
    "text": "not over provisioning with a bunch of idle capacity",
    "start": "3453320",
    "end": "3458710"
  },
  {
    "text": "yeah so redshift cluster is gonna run in a specific region I would have to check",
    "start": "3464910",
    "end": "3471339"
  },
  {
    "text": "the AWS regions page to tell you every region that's available in yeah exactly",
    "start": "3471339",
    "end": "3478960"
  },
  {
    "text": "yep cool and then there are a lot of resources online so if you go to",
    "start": "3478960",
    "end": "3485920"
  },
  {
    "start": "3482000",
    "end": "3482000"
  },
  {
    "text": "aws.amazon.com slash redshift you can get information on data warehousing",
    "start": "3485920",
    "end": "3491920"
  },
  {
    "text": "there's an e-book up there we've got blog posts customer success stories again this is how we learn we go",
    "start": "3491920",
    "end": "3499480"
  },
  {
    "text": "visit with customers we understand what they're doing and we take those best practices build a proof of concept fail",
    "start": "3499480",
    "end": "3507250"
  },
  {
    "text": "fast right spin up a to know DC to cluster for 50 cents an hour it's 320",
    "start": "3507250",
    "end": "3513790"
  },
  {
    "text": "gigs of storage throw some queries at it write some benchmarks and get a sense of",
    "start": "3513790",
    "end": "3519369"
  },
  {
    "text": "like hey you know what this is perfect for my workload or you know what I really need dense storage instead of",
    "start": "3519369",
    "end": "3525760"
  },
  {
    "text": "dense compute tear down the cluster only pay for what you use and it really gives",
    "start": "3525760",
    "end": "3531460"
  },
  {
    "text": "you the freedom to experiment you know I talked about the old days of like getting purchase orders signed and then",
    "start": "3531460",
    "end": "3537790"
  },
  {
    "text": "ordering Hardware and waiting it for to arrive and then getting it racked and getting the software installed you had",
    "start": "3537790",
    "end": "3545680"
  },
  {
    "text": "to guess right it was very difficult to go back to the CFO and say I didn't really want dense compute I wanted dense",
    "start": "3545680",
    "end": "3552849"
  },
  {
    "text": "storage and I need another sand and that would not you know get you where you",
    "start": "3552849",
    "end": "3558760"
  },
  {
    "text": "wanted to go whereas with red shift we can provision customers and clusters and minutes do a",
    "start": "3558760",
    "end": "3566470"
  },
  {
    "text": "POC on them and tear them down we can grow them and shrink them elastically as our workload dictates it gives us a lot",
    "start": "3566470",
    "end": "3573819"
  },
  {
    "text": "of freedom to experiment and to build out new workloads and new POCs so that's",
    "start": "3573819",
    "end": "3580390"
  },
  {
    "text": "everything I have today thank you so much for listening and thanks to everybody on Twitch I'll see you next",
    "start": "3580390",
    "end": "3586510"
  },
  {
    "text": "time",
    "start": "3586510",
    "end": "3588900"
  }
]