[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "good morning good morning good morning the last day of reinvent who went to deadmau5 last night was your hands who",
    "start": "4040",
    "end": "10860"
  },
  {
    "text": "thought it was pronounced dead now before last night yeah some people in here lying I can tell already",
    "start": "10860",
    "end": "16949"
  },
  {
    "text": "my name is Craig Karl I'm a Solutions Architect with Amazon Web Services we're gonna talk today about NFS and Sif's",
    "start": "16949",
    "end": "22830"
  },
  {
    "text": "network file system and common Internet file system what are they they're",
    "start": "22830",
    "end": "27930"
  },
  {
    "text": "protocols used to implement shared access to files there on the wire",
    "start": "27930",
    "end": "33059"
  },
  {
    "start": "30000",
    "end": "30000"
  },
  {
    "text": "protocols they leverage TCP back in the day NFS leveraged UF sorry UDP another TCP",
    "start": "33059",
    "end": "41550"
  },
  {
    "text": "protocols they're different from block and from object store so object store uses restful puts and gets",
    "start": "41550",
    "end": "47899"
  },
  {
    "text": "NFS and sips use F opens F close F writes current versions nfsv4 and SMB v3",
    "start": "47899",
    "end": "55800"
  },
  {
    "text": "but in my experience we're not seeing a lot of nfsv4 or SMB v3 yet so is anybody",
    "start": "55800",
    "end": "62969"
  },
  {
    "text": "here running NFS version 3 in production today and what about version 4 ok",
    "start": "62969",
    "end": "71189"
  },
  {
    "text": "alright so a good mix and who's running sips who uses Windows not too many all",
    "start": "71189",
    "end": "77220"
  },
  {
    "text": "right some I don't yeah alright some so you know do you need an NFS or sips file",
    "start": "77220",
    "end": "85710"
  },
  {
    "text": "system that shared you know we have s3 wherever possible we say two people use",
    "start": "85710",
    "end": "91500"
  },
  {
    "text": "s3 it's bigger it's more robust it's easier to make highly available if your",
    "start": "91500",
    "end": "98119"
  },
  {
    "text": "application can be modified to use s3 use it wherever possible we like loosely",
    "start": "98119",
    "end": "105630"
  },
  {
    "text": "coupled stateless infrastructures in the cloud and it's hard to build a loosely",
    "start": "105630",
    "end": "111090"
  },
  {
    "text": "coupled stateless infrastructure if you're using NFS and sips but there are",
    "start": "111090",
    "end": "116280"
  },
  {
    "text": "some places you need it if you're running a bunch of Oracle if you're running s AP if you're running legacy",
    "start": "116280",
    "end": "122189"
  },
  {
    "text": "applications even some new stuff WordPress really really really wants an NFS or sips file system in there",
    "start": "122189",
    "end": "127890"
  },
  {
    "text": "somewhere shared or clustered file databases right",
    "start": "127890",
    "end": "133260"
  },
  {
    "text": "my C we'll post great sequel they all run really well using NFS as a backing store or you want multi instance read and",
    "start": "133260",
    "end": "140670"
  },
  {
    "text": "write access to the same data set and you want to modify files in situ so with",
    "start": "140670",
    "end": "146700"
  },
  {
    "text": "s3 you need to get a file modify it put a file if you want to just modify a file",
    "start": "146700",
    "end": "153150"
  },
  {
    "text": "in place that's a good place for NFS or sips so we're going to talk today about",
    "start": "153150",
    "end": "158280"
  },
  {
    "text": "how you can build your own NFS and sifts infrastructures then we're going to talk",
    "start": "158280",
    "end": "163470"
  },
  {
    "text": "about some of our partners who deliver that stuff either as a prepackaged software solution or as a service where",
    "start": "163470",
    "end": "171180"
  },
  {
    "text": "we're going to move pretty quick today we've got lots of architectures to look at we've got some code samples to look at everyone in this room will get an",
    "start": "171180",
    "end": "178739"
  },
  {
    "start": "178000",
    "end": "178000"
  },
  {
    "text": "email on Wednesday with a link to this video recording a link to the presentation a link to a github repo",
    "start": "178739",
    "end": "186269"
  },
  {
    "text": "with all of the code samples and all of the snippets plus footnotes and links",
    "start": "186269",
    "end": "192480"
  },
  {
    "text": "and references to all of the documentation so you'll all get that on Wednesday of next week so if we blow",
    "start": "192480",
    "end": "198840"
  },
  {
    "text": "pious something quickly you can get it on Wednesday plus I'll take Q&A at the end so if you're going to build your own",
    "start": "198840",
    "end": "205319"
  },
  {
    "text": "NFS or sift store what do you have to think about one availability right a",
    "start": "205319",
    "end": "211699"
  },
  {
    "text": "single availability zone in an Amazon region could fail at any time so if you",
    "start": "211699",
    "end": "217199"
  },
  {
    "text": "put your solution in a single AZ you have an availability challenge so we're",
    "start": "217199",
    "end": "222480"
  },
  {
    "text": "going to talk about if you build it in a single AZ what's your recovery process or how do you build it so it spans",
    "start": "222480",
    "end": "228239"
  },
  {
    "text": "availability zones talk about the durability of data we're gonna get into the backing store we're going to talk",
    "start": "228239",
    "end": "234780"
  },
  {
    "text": "about the performance characteristics we worry about this on three dimensions primarily first the internet or public",
    "start": "234780",
    "end": "242220"
  },
  {
    "text": "facing interface that that where your clients are talking to your date to your NFS server the network that we're",
    "start": "242220",
    "end": "250560"
  },
  {
    "text": "connected to EBS if we're using EBS as a backing store we need to worry about the performance of that network connection",
    "start": "250560",
    "end": "256409"
  },
  {
    "text": "and then we need to worry about the actual performance of the disk the actual EBS volume so we're going to get",
    "start": "256409",
    "end": "261599"
  },
  {
    "text": "into a discussion of EBS and P I ops and EBS on HPC machines",
    "start": "261599",
    "end": "267220"
  },
  {
    "text": "consistency if you think you put food Tex down on disk will you get food text",
    "start": "267220",
    "end": "273550"
  },
  {
    "text": "back when you go to read it and we'll talk about that as well so let's start by talking about the stores that are",
    "start": "273550",
    "end": "280630"
  },
  {
    "text": "going to back your Mount EBS is durable in its very nature it has an annual",
    "start": "280630",
    "end": "287770"
  },
  {
    "start": "282000",
    "end": "282000"
  },
  {
    "text": "failure rate of 0.1 to 0.5% per volume",
    "start": "287770",
    "end": "293640"
  },
  {
    "text": "which is roughly comparable to a raid 5 array at your Drobo at home or in your",
    "start": "293640",
    "end": "300160"
  },
  {
    "text": "office right roughly equivalent to that array ephemeral storage are the discs",
    "start": "300160",
    "end": "306010"
  },
  {
    "text": "that are available in the local instance ok they're nice because we don't charge",
    "start": "306010",
    "end": "311680"
  },
  {
    "text": "you for them there they're there no additional charge on top of the instance charge but they're not protected unless",
    "start": "311680",
    "end": "317650"
  },
  {
    "text": "you protect them yourselves you can't snapshot them off to s3 the way you can with EBS",
    "start": "317650",
    "end": "322840"
  },
  {
    "text": "so you have a real serious durability problem with ephemeral but they're fast they're wicked fast right our new i28",
    "start": "322840",
    "end": "331030"
  },
  {
    "text": "extra large instances have got about 5.7 terabytes of SSDs at 350,000 for KI ops",
    "start": "331030",
    "end": "338980"
  },
  {
    "text": "per second I mean they're just fast and we're going to talk about how we can leverage that instant size and still",
    "start": "338980",
    "end": "347950"
  },
  {
    "text": "build a high-performance consistent durable NFS store and then s3 it's",
    "start": "347950",
    "end": "355870"
  },
  {
    "text": "really really attractive to say I'm going to put an NFS and sift store on top of s3 but there needs to be some",
    "start": "355870",
    "end": "362590"
  },
  {
    "text": "real intelligence in that layer because s3 doesn't have locking it doesn't have",
    "start": "362590",
    "end": "367990"
  },
  {
    "text": "partial file rights it doesn't have partial foot as partial file reads it doesn't have partial file rights so",
    "start": "367990",
    "end": "373750"
  },
  {
    "text": "there are some open source projects that do it that have some real serious data",
    "start": "373750",
    "end": "379030"
  },
  {
    "text": "loss potential and we're going to talk about a partner solution to partner solutions that have solved some of these",
    "start": "379030",
    "end": "384970"
  },
  {
    "text": "issues that I think are viable solutions for delivering NFS and sifts on top of",
    "start": "384970",
    "end": "391000"
  },
  {
    "text": "s3 so let's look at the most basic and what we're gonna do is we're going to look at the most basic we're going to",
    "start": "391000",
    "end": "396550"
  },
  {
    "start": "396000",
    "end": "396000"
  },
  {
    "text": "work our way up to more complicated building and building and building on top of things so this is a single EBS back instance so",
    "start": "396550",
    "end": "404290"
  },
  {
    "text": "I have one instance running in one availability zone I want to be able to grow it to a reasonable size so I",
    "start": "404290",
    "end": "410230"
  },
  {
    "text": "attached a bunch of EBS volumes to it okay the nice things about EBS volumes is is",
    "start": "410230",
    "end": "415690"
  },
  {
    "text": "one EBS volume is capable of X I ops per second if I take six of them or eight of",
    "start": "415690",
    "end": "422800"
  },
  {
    "text": "them and strike them together it's capable of x times eight performance so not only by taking eight EBS volumes and",
    "start": "422800",
    "end": "430450"
  },
  {
    "text": "putting them against one machine I get eight times the performance of a single volume and eight times this the the",
    "start": "430450",
    "end": "437560"
  },
  {
    "text": "space of a single volume the sweet spot is eight EBS volumes on regular",
    "start": "437560",
    "end": "444190"
  },
  {
    "text": "instances right at eight the network bandwidth the EBS becomes the bottleneck",
    "start": "444190",
    "end": "449650"
  },
  {
    "text": "so that's the sweet spot I then need to take these eight volumes I need to put them together into a raid array and",
    "start": "449650",
    "end": "457300"
  },
  {
    "text": "we're going to use MD Adnan to do that and we're actually going to look at the code that does this plus a really neat",
    "start": "457300",
    "end": "463150"
  },
  {
    "text": "open source project that makes it much much easier I then want to snapshot that",
    "start": "463150",
    "end": "470320"
  },
  {
    "text": "array of volumes to s3 if I snapshot just one of those volumes it doesn't do me any good I need to issue an AWS ec2",
    "start": "470320",
    "end": "477220"
  },
  {
    "text": "create snapshot against all eight and I need to do it at the same time so there's an open source project called",
    "start": "477220",
    "end": "483370"
  },
  {
    "text": "ec2 consistent snapshot maintained by boon - and that's what's going to let us",
    "start": "483370",
    "end": "489669"
  },
  {
    "text": "take a snapshot of the array okay we're going to look deeper at the performance",
    "start": "489669",
    "end": "495280"
  },
  {
    "text": "considerations of this and all of the other instances or all of the other architectures so we talked about the",
    "start": "495280",
    "end": "503710"
  },
  {
    "text": "public facing internet connection on the machine can be a bottleneck low",
    "start": "503710",
    "end": "510640"
  },
  {
    "text": "performance I don't want to see anybody building this on a t1 micro right that",
    "start": "510640",
    "end": "516490"
  },
  {
    "text": "machine is six cents an hour for a reason it doesn't have a whole lot of network oomph to it it's fantastic if",
    "start": "516490",
    "end": "523990"
  },
  {
    "start": "520000",
    "end": "520000"
  },
  {
    "text": "you run a fleet of 600 of those to serve up your website they are cost-effective they're incredibly useful this use case",
    "start": "523990",
    "end": "531839"
  },
  {
    "text": "No No so we're going to try and avoid the ones that say low performance they give",
    "start": "531839",
    "end": "537760"
  },
  {
    "text": "you a sub gigabit of network connectivity out to your clients so let's avoid those moderate performance",
    "start": "537760",
    "end": "544210"
  },
  {
    "text": "this gets you about a gigabit of performance from these instances out to all of your clients so these are a good",
    "start": "544210",
    "end": "550810"
  },
  {
    "text": "choice let's say you've got a whole bunch of t1 micro web servers they want",
    "start": "550810",
    "end": "556510"
  },
  {
    "text": "to access a shared NFS store you could take 30 40 50 t1 micros and point them",
    "start": "556510",
    "end": "561910"
  },
  {
    "text": "in a single and one extra large because the m1 extra large has got plenty of network bandwidth okay and then of",
    "start": "561910",
    "end": "568480"
  },
  {
    "text": "course we've got our HPC instances that have 10 gigabits of network bandwidth so I could take 10 m1 extra larges point",
    "start": "568480",
    "end": "577000"
  },
  {
    "text": "them at Assisi one for extra-large and saturate all 10 of the network",
    "start": "577000",
    "end": "582070"
  },
  {
    "text": "connections on my clients ok so you're going to think about how many megabits",
    "start": "582070",
    "end": "587950"
  },
  {
    "text": "per second do I want to do on the public interface when you're choosing a server",
    "start": "587950",
    "end": "594400"
  },
  {
    "text": "and the really nice thing is if you say you know today I just need an m1 extra large you create it six months later you",
    "start": "594400",
    "end": "600730"
  },
  {
    "text": "say you know my bandwidth is a bottleneck you do an ec2 modify instance attributes API call you change your m1",
    "start": "600730",
    "end": "607750"
  },
  {
    "text": "extra-large to a cc to eight extra large you reboot and you've gone from one gigabit of bandwidth to 10 gigabits of",
    "start": "607750",
    "end": "614050"
  },
  {
    "text": "bandwidth the wonder of the cloud all right let's talk about the EBS facing",
    "start": "614050",
    "end": "619420"
  },
  {
    "text": "interface this is very important so there's an analogy of EBS - I scuzzy in",
    "start": "619420",
    "end": "625770"
  },
  {
    "start": "623000",
    "end": "623000"
  },
  {
    "text": "this when we're talking about network bandwidth so you're building a nice cozy network on premise and you have to say",
    "start": "625770",
    "end": "632560"
  },
  {
    "text": "am I going to buy one gigabit interfaces for EBS am I going to buy 10 gigabit interfaces for EBS or sorry for my I",
    "start": "632560",
    "end": "640000"
  },
  {
    "text": "scuzzy the same question comes up here we call it EBS optimized and ten gigabit instances",
    "start": "640000",
    "end": "646600"
  },
  {
    "text": "so without EBS optimized its best effort we will do everything we can to get you",
    "start": "646600",
    "end": "653080"
  },
  {
    "text": "really good EBS band width right but there are no commitments and there's no",
    "start": "653080",
    "end": "658839"
  },
  {
    "text": "guarantees if you do EBS optimized we offer EBS optimized on a bunch of",
    "start": "658839",
    "end": "665230"
  },
  {
    "text": "different instance types on em enlarges m22 extra larges and m3 extra larges it's 500 megabits from your",
    "start": "665230",
    "end": "672459"
  },
  {
    "text": "instance to the EBS fleet which means the fastest you'll be able to read and write from those eight disks that we've",
    "start": "672459",
    "end": "678519"
  },
  {
    "text": "attached is 500 megabits probably not what you want so you're going to step up",
    "start": "678519",
    "end": "683589"
  },
  {
    "text": "to EBS optimized on m1 extra-large or some of those other instance types that commits to you a gigabit with an SLA",
    "start": "683589",
    "end": "690959"
  },
  {
    "text": "okay you're going to decide how much performance you need we can stripe",
    "start": "690959",
    "end": "696370"
  },
  {
    "text": "together eight 4000 P IAP svali ohms and we can build one what is that 30 mm I",
    "start": "696370",
    "end": "704259"
  },
  {
    "text": "ups volume no not that many my mouth--i sucks anyway we can we can build one",
    "start": "704259",
    "end": "710769"
  },
  {
    "text": "very fast volume or we can take eight standard volumes and build a volume that's fairly fast but a little bit",
    "start": "710769",
    "end": "716079"
  },
  {
    "text": "variable okay on our HPC instance is that 10 gig interface we talked about it's not just",
    "start": "716079",
    "end": "723160"
  },
  {
    "text": "public facing it's also 2 EBS so if you have 5 megabits or five gigabits of",
    "start": "723160",
    "end": "728709"
  },
  {
    "text": "traffic going out to your clients you have five gigabits free to go to EBS so that could be very very fast right we",
    "start": "728709",
    "end": "736269"
  },
  {
    "text": "announced a couple of weeks ago that on our 10 gigabit machines there's enough bandwidth between an HPC instance and",
    "start": "736269",
    "end": "742689"
  },
  {
    "text": "the EBS fleet to move 800 megabytes per second of traffic so if you need a very",
    "start": "742689",
    "end": "748480"
  },
  {
    "text": "high performance NFS mount this is one way to do it so let's look at the code",
    "start": "748480",
    "end": "754029"
  },
  {
    "text": "this is it by the way you spin up at Amazon Linux instance and this is how you get it up and running as a NFS mount",
    "start": "754029",
    "end": "760839"
  },
  {
    "text": "point the first line we install NFS and Samba Samba is how we get sips working",
    "start": "760839",
    "end": "765850"
  },
  {
    "text": "the second line we use a great piece of open-source software this thing's fantastic normally to",
    "start": "765850",
    "end": "771819"
  },
  {
    "text": "create a raid array we create our eight volumes we mount our eight volumes to",
    "start": "771819",
    "end": "776889"
  },
  {
    "text": "our machine we do an MD admin create and we create a raid array then we format",
    "start": "776889",
    "end": "783279"
  },
  {
    "text": "and mount that array or you can go to github download raid former the link to",
    "start": "783279",
    "end": "788889"
  },
  {
    "text": "raid formers down at the bottom of the screen and pass in a single command line that creates your volumes that mounts",
    "start": "788889",
    "end": "794800"
  },
  {
    "text": "your volumes that creates the MD admin array that formats it and mounts it",
    "start": "794800",
    "end": "800470"
  },
  {
    "text": "all at once so I put this script in my user data so when the machine launches it automatically attaches a bunch of",
    "start": "800470",
    "end": "806890"
  },
  {
    "text": "volumes formats them and makes a raid array out of them I then edit my Etsy exports in my Etsy",
    "start": "806890",
    "end": "812140"
  },
  {
    "text": "Samba files and I start the to services so it's very easy to get up and running but again you have an availability",
    "start": "812140",
    "end": "818920"
  },
  {
    "text": "problem with a single instance and a single availability zone your recovery",
    "start": "818920",
    "end": "824110"
  },
  {
    "text": "process is this that availability zone fails you go to another availability",
    "start": "824110",
    "end": "829540"
  },
  {
    "text": "zone you create volumes out of your snapshots because you've been snapshotting all of these volumes on a",
    "start": "829540",
    "end": "835480"
  },
  {
    "text": "regular basis you spin up an instance in your new availability zone you attach the volumes to the instance you rebuild",
    "start": "835480",
    "end": "843100"
  },
  {
    "text": "the raid array and then you install NFS and start the machine up or start the",
    "start": "843100",
    "end": "850090"
  },
  {
    "text": "service up so it's not pretty right but it's easy and it's inexpensive and if you don't have anything critical it will",
    "start": "850090",
    "end": "856330"
  },
  {
    "text": "work so let's talk about another solution all right let's build upon that let's",
    "start": "856330",
    "end": "862870"
  },
  {
    "text": "say that EBS isn't fast enough EBS has higher latency and there are",
    "start": "862870",
    "end": "868480"
  },
  {
    "text": "certain circumstances when EBS won't service the workload so what we can do is we can leverage the ephemeral drives",
    "start": "868480",
    "end": "875770"
  },
  {
    "text": "as cache right and this is a good architecture so what I can do here is I",
    "start": "875770",
    "end": "881980"
  },
  {
    "text": "can take that H or that new i2 with about six terabytes of incredibly fast",
    "start": "881980",
    "end": "887440"
  },
  {
    "text": "disk I create a raid array of that incredibly fast disk and then I use an",
    "start": "887440",
    "end": "892480"
  },
  {
    "text": "open source project called dr BD to asynchronously replicate the blocks from",
    "start": "892480",
    "end": "898900"
  },
  {
    "text": "the ephemeral to EBS EBS is already much more durable than the ephemeral and I",
    "start": "898900",
    "end": "906040"
  },
  {
    "text": "can snapshot that EBS array to s3 so now I'm incredibly durable ok this is a solid good",
    "start": "906040",
    "end": "913900"
  },
  {
    "text": "architecture you still have the same recovery path but you've just four zero additional cost over the other version",
    "start": "913900",
    "end": "921240"
  },
  {
    "text": "increased your performance 10 or 15 fold ok but still a tough recovery path an",
    "start": "921240",
    "end": "927970"
  },
  {
    "text": "we're going to talk about that I've done some testing if I really really",
    "start": "927970",
    "end": "933250"
  },
  {
    "text": "aggressively run to the ephemeral discs I cannot get EBS more than about eight seconds behind so",
    "start": "933250",
    "end": "941470"
  },
  {
    "text": "that's when it comes to durability if you lost the instance if the instance failed in my testing you will lose about",
    "start": "941470",
    "end": "950290"
  },
  {
    "text": "eight seconds of data that hasn't made it from the instance to the ephemeral store and that's been my experience you",
    "start": "950290",
    "end": "957399"
  },
  {
    "text": "can monitor that and it's incredibly important to monitor that the command to monitor is at the bottom of the screen",
    "start": "957399",
    "end": "963040"
  },
  {
    "text": "and that will tell you how far out of sync your EBS volume is with your ephemeral volume okay any questions on",
    "start": "963040",
    "end": "971529"
  },
  {
    "text": "this we covered a couple of real important topics we're going to dive into a little bit deeper any questions on this so far all right so let's look",
    "start": "971529",
    "end": "980260"
  },
  {
    "text": "at that dr BD configuration it's fairly straightforward it's a very mature product it's not difficult to do here we",
    "start": "980260",
    "end": "987490"
  },
  {
    "text": "create the global common comm file for dr DB we set the protocol the protocol a",
    "start": "987490",
    "end": "992589"
  },
  {
    "text": "which is a synchronous replication and then we configure the actual resources",
    "start": "992589",
    "end": "999220"
  },
  {
    "text": "here i'm replicating on one machine to another machine so my online are the",
    "start": "999220",
    "end": "1006149"
  },
  {
    "text": "same I'm replicating from dr BD 0 to b dr BD 1 md 0 to md 1 on the same machine",
    "start": "1006149",
    "end": "1015079"
  },
  {
    "text": "right with with with asynchronous replication let's kick it up a notch",
    "start": "1015079",
    "end": "1021420"
  },
  {
    "text": "let's solve the recovery problem right this is a good architecture okay i have",
    "start": "1021420",
    "end": "1028500"
  },
  {
    "text": "one machine and availability zone a i have another machine and availability Zone B on both I've attached a whole",
    "start": "1028500",
    "end": "1034620"
  },
  {
    "text": "bunch of disks I've created an MD admin array so we've taken that original single instance with EBS and we've",
    "start": "1034620",
    "end": "1040650"
  },
  {
    "text": "duplicated it in another region we're using dr BD now the same dr BD config",
    "start": "1040650",
    "end": "1047428"
  },
  {
    "text": "that we use before only now we're replicating not from the same machine to the same machine but from one machine to",
    "start": "1047429",
    "end": "1053700"
  },
  {
    "text": "another machine the config is exactly the same except for I change the device",
    "start": "1053700",
    "end": "1061410"
  },
  {
    "text": "name right here that would become a machine in a ZB right so there's not a significant change",
    "start": "1061410",
    "end": "1069019"
  },
  {
    "text": "we use Linux pacemaker which is a standard open-source clustering",
    "start": "1069019",
    "end": "1076080"
  },
  {
    "text": "heartbeat technology and the secret sauce here is how do we get it to move IP addresses for us we've got this",
    "start": "1076080",
    "end": "1083070"
  },
  {
    "text": "exported on 1000:1 and we want to continue to keep it exported on 10001",
    "start": "1083070",
    "end": "1088470"
  },
  {
    "text": "when we move to the other availability zone linux pacemaker normally uses a",
    "start": "1088470",
    "end": "1093809"
  },
  {
    "text": "gratuitous ARP a layer two ARP to tell the switch it's attached to hey 1000:1",
    "start": "1093809",
    "end": "1100049"
  },
  {
    "text": "has gone from this port to this port if you send gratuitous ARP on the Amazon",
    "start": "1100049",
    "end": "1106740"
  },
  {
    "text": "network we put it in the bit bucket right it goes to dev no we ignore it so that isn't going to work so the secret",
    "start": "1106740",
    "end": "1113519"
  },
  {
    "text": "here is getting pacemaker to use our api's to move that IP address BAM it's",
    "start": "1113519",
    "end": "1120659"
  },
  {
    "text": "not that hard alright pacemaker uses a script so right now if you installed",
    "start": "1120659",
    "end": "1126720"
  },
  {
    "start": "1124000",
    "end": "1124000"
  },
  {
    "text": "pacemaker would have a script that did it go to at isarc we replace that script with one thing that uses our API that",
    "start": "1126720",
    "end": "1133619"
  },
  {
    "text": "does an AWS assign private IP address and now pacemaker can move IP addresses",
    "start": "1133619",
    "end": "1139799"
  },
  {
    "text": "around members of a cluster write a full",
    "start": "1139799",
    "end": "1144899"
  },
  {
    "text": "all full configs write everything you need for all of these is at the end of the deck and you'll have access to it on",
    "start": "1144899",
    "end": "1150570"
  },
  {
    "text": "Wednesday okay so this is um a solid config this is",
    "start": "1150570",
    "end": "1160320"
  },
  {
    "text": "almost what I do so the next thing we're going to talk about is what I do there okay let's say EBS isn't fast enough",
    "start": "1160320",
    "end": "1168379"
  },
  {
    "text": "we're building and we're building and we're building on top of things you've obviously figured out by now you could",
    "start": "1168379",
    "end": "1173820"
  },
  {
    "text": "just cluster ephemeral to ephemeral now you're durable if an ephemeral drive on",
    "start": "1173820",
    "end": "1180570"
  },
  {
    "text": "a machine fails that's fine you just move over to the other AZ so you'd have to take multiple failures for this to",
    "start": "1180570",
    "end": "1188460"
  },
  {
    "text": "break and it's very very fast it's 350,000 I ops per second fast if",
    "start": "1188460",
    "end": "1195059"
  },
  {
    "text": "you do it on an AI - so incredibly fast NFS but you still have a durability",
    "start": "1195059",
    "end": "1200279"
  },
  {
    "text": "problem the holy grail of durability at AWS is",
    "start": "1200279",
    "end": "1205619"
  },
  {
    "text": "s3 so I really really want to get this data at s3 so let's complicate things",
    "start": "1205619",
    "end": "1211499"
  },
  {
    "text": "just a little bit more this is what I run when I'm building NFS clusters okay",
    "start": "1211499",
    "end": "1218909"
  },
  {
    "text": "I take the ephemeral i mirror it to the ephemeral synchronously so when it's",
    "start": "1218909",
    "end": "1224519"
  },
  {
    "text": "committed to the machine and aza the client does not get a commit until",
    "start": "1224519",
    "end": "1230879"
  },
  {
    "text": "it's committed on a ZB so I'm incredibly durable and I have consistency between",
    "start": "1230879",
    "end": "1237119"
  },
  {
    "text": "my two instances and then I use asynchronous replication to get it down",
    "start": "1237119",
    "end": "1242519"
  },
  {
    "text": "to EBS so this is incredibly fast",
    "start": "1242519",
    "end": "1247580"
  },
  {
    "text": "incredibly durable and a little bit expensive but it's a good solid config",
    "start": "1247580",
    "end": "1254009"
  },
  {
    "text": "and if you're going to go build it yourself and we're going to give you some alternatives if you're looking at this and go and this is a lot of work",
    "start": "1254009",
    "end": "1260129"
  },
  {
    "text": "we're going to give you some alternatives if you're going to go build it yourself and it needs to be fast and",
    "start": "1260129",
    "end": "1265440"
  },
  {
    "text": "it needs to be mission-critical this is the architecture to use and I've given you all of the pieces you're going to",
    "start": "1265440",
    "end": "1271590"
  },
  {
    "text": "use raid former to build the raid arrays you're going to use dr BD synchronously",
    "start": "1271590",
    "end": "1277169"
  },
  {
    "text": "between the two instances and then asynchronously on to the EBS volumes okay solid configuration you're limited",
    "start": "1277169",
    "end": "1284820"
  },
  {
    "text": "here though I would never recommend you attached more than about 16 terabytes of EBS to most instance types you're not",
    "start": "1284820",
    "end": "1293129"
  },
  {
    "text": "going to want to go above that at that point you start saturating the network connection to EBS there's an exception",
    "start": "1293129",
    "end": "1299159"
  },
  {
    "text": "up at the high end of the instances CC to CR ones a chai ones you can get away",
    "start": "1299159",
    "end": "1304619"
  },
  {
    "text": "with about 25 volumes but you're not going to want to go bigger than that so you say Craig what if I need a petabyte",
    "start": "1304619",
    "end": "1311299"
  },
  {
    "text": "what if I need two petabytes what if I need three petabytes of storage so the",
    "start": "1311299",
    "end": "1317730"
  },
  {
    "text": "last company I worked at was Gluster before they got acquired by Red Hat where we built these sorts of architectures and Glosser has some",
    "start": "1317730",
    "end": "1325230"
  },
  {
    "text": "advantages and Gluster has some disadvantages advantage you can build",
    "start": "1325230",
    "end": "1330629"
  },
  {
    "text": "great big clusters I've built multiple to petabyte and test mount points out of Gluster in AWS",
    "start": "1330629",
    "end": "1338340"
  },
  {
    "text": "so you can build big big ones is anybody here familiar with Gloucester does anybody use it today ok so lots of",
    "start": "1338340",
    "end": "1344760"
  },
  {
    "text": "people use it today if you don't get Gluster if you've never looked at it before it's magic you take the storage off of one or five",
    "start": "1344760",
    "end": "1353130"
  },
  {
    "text": "or ten or 50 instances you make a single mount call and all of a sudden all of",
    "start": "1353130",
    "end": "1360120"
  },
  {
    "text": "that storage appears as a single drive a single NFS Sif's or Gluster FS mount",
    "start": "1360120",
    "end": "1365550"
  },
  {
    "text": "point it's magic it really is incredible stuff we have some challenges with with",
    "start": "1365550",
    "end": "1372810"
  },
  {
    "text": "file sizes under about 128 KB it gets very very slow with file sizes under 32",
    "start": "1372810",
    "end": "1380580"
  },
  {
    "text": "KB 5 megabytes per second 6 megabytes per second with 2k 4k files 2 megabytes",
    "start": "1380580",
    "end": "1389070"
  },
  {
    "text": "per second distributed file systems and small files do not work well together",
    "start": "1389070",
    "end": "1395270"
  },
  {
    "text": "right it's a challenge ok to get true high availability true redundancy from",
    "start": "1395270",
    "end": "1403440"
  },
  {
    "text": "Gluster you need to use their client their client is a 64-bit Linux client so if you're using Linux you can use their",
    "start": "1403440",
    "end": "1410550"
  },
  {
    "text": "client it works just like NFS the operating system thinks it's NFS it looks like NFS only it adds some",
    "start": "1410550",
    "end": "1416730"
  },
  {
    "text": "durability and it does it in a couple of pretty cool ways but if you want to",
    "start": "1416730",
    "end": "1422040"
  },
  {
    "text": "attach Windows to this it'll work every one of these 10 machines you export samba and you do a loopback mount to",
    "start": "1422040",
    "end": "1429210"
  },
  {
    "text": "Gluster but that means if that machine that that Windows client happens to be connected on fails it has to reconnect to another",
    "start": "1429210",
    "end": "1437670"
  },
  {
    "text": "machine so you end up doing some fancy round robin DNS stuff and you do the",
    "start": "1437670",
    "end": "1443310"
  },
  {
    "text": "same thing with NFS right so if you can use the Gluster client and your file",
    "start": "1443310",
    "end": "1450150"
  },
  {
    "text": "sizes are not very small right if they're larger than 128 kb then Gluster",
    "start": "1450150",
    "end": "1455250"
  },
  {
    "text": "is a great option it's a great option it's not for beginners right I'd say you",
    "start": "1455250",
    "end": "1460470"
  },
  {
    "text": "need to be medium to advanced Linux administrator to set up and maintain Gluster a lot of the tools that we used",
    "start": "1460470",
    "end": "1467280"
  },
  {
    "text": "that we've talked about already you can use you could attach eight drives to every one of your Gluster instances you",
    "start": "1467280",
    "end": "1473050"
  },
  {
    "text": "can use raid former to do that and then you install bluster everywhere and actually setting up the server is really",
    "start": "1473050",
    "end": "1479020"
  },
  {
    "text": "really easy on any one of those machines you do a Gluster volume create so first",
    "start": "1479020",
    "end": "1485380"
  },
  {
    "start": "1483000",
    "end": "1483000"
  },
  {
    "text": "you a cluster peer you make them all part of a cluster and then you do a Gluster volume create I'm doing replica",
    "start": "1485380",
    "end": "1491470"
  },
  {
    "text": "- and what replica two means is it's going to take the first server line 1000",
    "start": "1491470",
    "end": "1497860"
  },
  {
    "text": "one and it's going to replicate all of the data on that machine to 10011",
    "start": "1497860",
    "end": "1503220"
  },
  {
    "text": "then it's going to take everything on 1000 - and replicate it to 1001 - so you",
    "start": "1503220",
    "end": "1508870"
  },
  {
    "text": "get to control where things are replicated so you can say everything in a z1 gets replicated to a z2 okay",
    "start": "1508870",
    "end": "1516870"
  },
  {
    "text": "cluster is a great option it's open source maintained and run by Red Hat on",
    "start": "1517450",
    "end": "1522790"
  },
  {
    "text": "the client you use the standard mount command and when you install the client it adds a protocol so just like you",
    "start": "1522790",
    "end": "1528910"
  },
  {
    "text": "would do mount - T NFS you do mount - T Gluster FS any one of the Gluster",
    "start": "1528910",
    "end": "1534940"
  },
  {
    "text": "servers and if that server died it doesn't matter what the client does is it goes to that that server and it says",
    "start": "1534940",
    "end": "1541060"
  },
  {
    "text": "give me a list of every machine participating in this cluster and then the client load balances connections",
    "start": "1541060",
    "end": "1547690"
  },
  {
    "text": "across all of those machines so you've got a really even distribution of reads",
    "start": "1547690",
    "end": "1554020"
  },
  {
    "text": "and writes right because we're replicating the client knows that the file is trying to read from is on two",
    "start": "1554020",
    "end": "1560980"
  },
  {
    "text": "machines so every couple of seconds it says what machine am I getting faster responses from okay I'm going to do all",
    "start": "1560980",
    "end": "1567700"
  },
  {
    "text": "my reads from there for the next five or six seconds and then it'll check again so it's really intelligent it's a really",
    "start": "1567700",
    "end": "1573520"
  },
  {
    "text": "nice piece of software we run into challenges with smaller files okay and the work involved in maintaining",
    "start": "1573520",
    "end": "1582040"
  },
  {
    "text": "Gloster okay Red Hat also sells a paid supported version Red Hat storage the",
    "start": "1582040",
    "end": "1589330"
  },
  {
    "text": "Red Hat storage people is there anybody from reddit storage in the room Oh couple we're going to try and make it they have a booth in the main conference",
    "start": "1589330",
    "end": "1595360"
  },
  {
    "text": "hall I'll give you the booth number if you would like Gluster that somebody else sets up and maintains for you they",
    "start": "1595360",
    "end": "1600700"
  },
  {
    "text": "will be happy to do that let's talk about Windows we have some windows peoples in the in the room",
    "start": "1600700",
    "end": "1606090"
  },
  {
    "text": "before Windows Server 2012 I didn't have a really good answer for you here right",
    "start": "1606090",
    "end": "1611679"
  },
  {
    "text": "with Server 2012 I have a really good answer DFS got very very nice so we can do a",
    "start": "1611679",
    "end": "1618940"
  },
  {
    "text": "standard windows instance attach a bunch of EBS to it and exports if you have",
    "start": "1618940",
    "end": "1625809"
  },
  {
    "text": "your durability issues you have your availability issues how do we make that more redundant we use DFS and DFS got",
    "start": "1625809",
    "end": "1632950"
  },
  {
    "text": "really really nice in 2012 and so that's good we've got one downside so I can",
    "start": "1632950",
    "end": "1639970"
  },
  {
    "start": "1635000",
    "end": "1635000"
  },
  {
    "text": "take two machines running Windows I attached 8 terabytes of disks to both of",
    "start": "1639970",
    "end": "1645460"
  },
  {
    "text": "them because I need 8 terabytes of storage I can click through a couple of windows it's real quick and I can set up",
    "start": "1645460",
    "end": "1650770"
  },
  {
    "text": "a DFS cluster and then the Samba client will write to both machines for me",
    "start": "1650770",
    "end": "1657700"
  },
  {
    "text": "synchronously and it'll read from both machines from me depending on the performance so it's a really good",
    "start": "1657700",
    "end": "1664990"
  },
  {
    "text": "solution you can do it with a ephemeral disk you can do it with EBS you've got a",
    "start": "1664990",
    "end": "1670150"
  },
  {
    "text": "whole bunch of options problem is the clients have to be SMB v3 which means",
    "start": "1670150",
    "end": "1675429"
  },
  {
    "text": "your clients need to be Server 2012 as well ok I don't have a good answer for",
    "start": "1675429",
    "end": "1682240"
  },
  {
    "text": "you if your clients are Server 2008 that's a hard of high-availability",
    "start": "1682240",
    "end": "1688320"
  },
  {
    "text": "problem to solve and i don't have a really good answer but if your server 2012 everywhere then DFS on Server 2012",
    "start": "1688320",
    "end": "1697210"
  },
  {
    "text": "is a fantastic option so you're saying",
    "start": "1697210",
    "end": "1702240"
  },
  {
    "text": "wow that's an awful lot of work doesn't Amazon make it easier for me to concentrate on my web development",
    "start": "1702240",
    "end": "1708760"
  },
  {
    "text": "doesn't it make it easier for me to concentrate on my analytics and not build infrastructure yes we do and so we",
    "start": "1708760",
    "end": "1715630"
  },
  {
    "text": "go to partners and we say to partners make this easier for our customers and they do so like I said Red Hat storage",
    "start": "1715630",
    "end": "1722470"
  },
  {
    "start": "1720000",
    "end": "1720000"
  },
  {
    "text": "is here on site booth 400 if you are sitting here needing I need a petabyte",
    "start": "1722470",
    "end": "1728409"
  },
  {
    "text": "of NFS go talk to these people they will sell you support and professional services to set that up and to help you",
    "start": "1728409",
    "end": "1735250"
  },
  {
    "text": "maintain it okay imagine addicts is also here booth",
    "start": "1735250",
    "end": "1741779"
  },
  {
    "text": "1006 so these are a lot of the folks that came from AFS they built a they",
    "start": "1741779",
    "end": "1750100"
  },
  {
    "text": "built a compelling solution it's highly available across availability zones it uses s3 on the backend so you've got",
    "start": "1750100",
    "end": "1756730"
  },
  {
    "text": "tremendous durability and it's got deduplication and some strong encryption built into it so it's a compelling",
    "start": "1756730",
    "end": "1763419"
  },
  {
    "text": "solution as well they're here in the building is there anybody here from a genetics in the room there are so we've got a couple of people from a genetics",
    "start": "1763419",
    "end": "1769389"
  },
  {
    "text": "in the room track them down at the end of the session and they can help you",
    "start": "1769389",
    "end": "1775139"
  },
  {
    "text": "softness softness is available in the AWS marketplace it implements and I'm",
    "start": "1775139",
    "end": "1781960"
  },
  {
    "text": "going to fly through some slides here real quick their software implements",
    "start": "1781960",
    "end": "1788139"
  },
  {
    "text": "this for you top to bottom you can run to soft NAS devices it replicates",
    "start": "1788139",
    "end": "1794909"
  },
  {
    "text": "between the two using a femoral and/or EBS and then it caches and stores the",
    "start": "1794909",
    "end": "1800919"
  },
  {
    "text": "the really durable stuff instead of EBS down to s3 so soft Ness has a great",
    "start": "1800919",
    "end": "1806649"
  },
  {
    "text": "solution it's a gooey top to bottom so for those of you that are like you know",
    "start": "1806649",
    "end": "1812019"
  },
  {
    "text": "I just I just want something that that looks like an AZ device that runs at my office I've got two controllers and a",
    "start": "1812019",
    "end": "1818169"
  },
  {
    "text": "bunch of discs and it just works softness is an option okay",
    "start": "1818169",
    "end": "1824669"
  },
  {
    "text": "Zadar abouthe 711 I'm going to invite gnome shandar up on stage the door is my",
    "start": "1824669",
    "end": "1830320"
  },
  {
    "text": "favorite solution for this problem okay and that's why that's why we're inviting",
    "start": "1830320",
    "end": "1835749"
  },
  {
    "text": "gnome on stage gnome delivers virtual private storage arrays as a service so",
    "start": "1835749",
    "end": "1841779"
  },
  {
    "text": "he delivers to you NFS and Sif's not as something you build yourself not as something you maintain yourself but as a",
    "start": "1841779",
    "end": "1848019"
  },
  {
    "text": "service Thank You gnome thanks Craig clicker",
    "start": "1848019",
    "end": "1854320"
  },
  {
    "text": "all right thank you if happy fried everybody I'll very happy to be here",
    "start": "1854320",
    "end": "1860169"
  },
  {
    "text": "I'll try to match Craig's energy level but it's an impossible task I can tell",
    "start": "1860169",
    "end": "1865659"
  },
  {
    "text": "you a little bit about zidar storage I want to leave time for questions for especially for Craig so let's let's get",
    "start": "1865659",
    "end": "1871750"
  },
  {
    "text": "into it what is the Dhara we built a private storage as a service offering for AWS customers that's built",
    "start": "1871750",
    "end": "1878830"
  },
  {
    "start": "1877000",
    "end": "1877000"
  },
  {
    "text": "hourly and doesn't require any configuration or dr BD or md admin or",
    "start": "1878830",
    "end": "1885100"
  },
  {
    "text": "anything like that in your part it mounts directly to your virtual machines and provide the standard NFS or sips or",
    "start": "1885100",
    "end": "1891370"
  },
  {
    "text": "ice cozy interfaces I'll talk a little bit about the architecture but a key",
    "start": "1891370",
    "end": "1896529"
  },
  {
    "text": "point to know is a very very low latency because it's co-located with Amazon's",
    "start": "1896529",
    "end": "1901629"
  },
  {
    "text": "clouds but without running on AWS is own infrastructure the footprint is global",
    "start": "1901629",
    "end": "1907450"
  },
  {
    "text": "it's in four different regions US east u.s. west the Tokyo region calls a called asia-pacific northeast in Europe",
    "start": "1907450",
    "end": "1915220"
  },
  {
    "text": "and Dublin the quality of service is ridiculously high this is not an",
    "start": "1915220",
    "end": "1920230"
  },
  {
    "text": "exaggeration I'll talk a little bit about that and it's built with no single point of failure",
    "start": "1920230",
    "end": "1925299"
  },
  {
    "text": "it's fully redundant and and thanks to the multi region capability it's also",
    "start": "1925299",
    "end": "1930360"
  },
  {
    "text": "resilient against disasters this is this",
    "start": "1930360",
    "end": "1935799"
  },
  {
    "text": "is where it sits and a little bit about how it works it sits in for colocation facilities that are the Direct Connect",
    "start": "1935799",
    "end": "1942129"
  },
  {
    "start": "1937000",
    "end": "1937000"
  },
  {
    "text": "facilities that AWS has and you can see them listed here and we're we're adding",
    "start": "1942129",
    "end": "1948100"
  },
  {
    "text": "more all the time because there's a lot of demand for this service and we use Direct Connect and a lot of",
    "start": "1948100",
    "end": "1953139"
  },
  {
    "text": "it it's between 20 and 40 gigabits of dedicated bandwidth in every single site and that is elastic as well we can add",
    "start": "1953139",
    "end": "1959409"
  },
  {
    "text": "additional additional bandwidth as needed for a total of 100 gigabits of dedicated bandwidth going in and out of",
    "start": "1959409",
    "end": "1965590"
  },
  {
    "text": "AWS in the storage because it's it's a direct connect is visible to all AZ's in",
    "start": "1965590",
    "end": "1970690"
  },
  {
    "text": "that region at the same time so I'll talk about sharing as well but now you",
    "start": "1970690",
    "end": "1975970"
  },
  {
    "text": "can share the storage to multiple VMs not only within an Daisy but actually across all of the ACS in that region and",
    "start": "1975970",
    "end": "1983789"
  },
  {
    "text": "we offer secure remote replication among those regions I'll touch I'll touch on that point in a second the",
    "start": "1983789",
    "end": "1990639"
  },
  {
    "text": "replication sorry the provisioning is super super easy this is the live production screen that you would use to",
    "start": "1990639",
    "end": "1997149"
  },
  {
    "start": "1991000",
    "end": "1991000"
  },
  {
    "text": "provision this for yourself you would you would obviously specify which region you want the storage in because we need",
    "start": "1997149",
    "end": "2003570"
  },
  {
    "text": "to know where to create your instance where it's close to your compute once you specify the region you make two",
    "start": "2003570",
    "end": "2010019"
  },
  {
    "text": "selections one is the size of your zidar engine think of this adhara engine as a virtual controller and the bigger it is",
    "start": "2010019",
    "end": "2016110"
  },
  {
    "text": "the the higher your peak performance is going to be and that is fully elastic you can grow your engine or shrink your",
    "start": "2016110",
    "end": "2022590"
  },
  {
    "text": "engine on demand non-disruptive li based on what your application needs and then you select the drives and these are",
    "start": "2022590",
    "end": "2029490"
  },
  {
    "text": "actual drives these are spindles they can be SSDs that can be rotating drives that can be any mix of the above and",
    "start": "2029490",
    "end": "2035220"
  },
  {
    "text": "they're yours and yours alone no other customer is touching those drives and other customers affecting the",
    "start": "2035220",
    "end": "2040409"
  },
  {
    "text": "performance of those drives and that is elastic as well you can pick more drives later you can give drives back you can",
    "start": "2040409",
    "end": "2047250"
  },
  {
    "text": "you can wipe those drives clean if you don't you don't trust us to do that that is all all part of this and from the",
    "start": "2047250",
    "end": "2054960"
  },
  {
    "text": "moment you hit submit at the bottom of the screen 90 seconds later you're fully redundant high availability and a",
    "start": "2054960",
    "end": "2061800"
  },
  {
    "text": "festive saw rice cozy storages is ready and running so why why our customers use",
    "start": "2061800",
    "end": "2068368"
  },
  {
    "text": "this or there are many reasons and I'll just touch on some highlights every single volume is has an SSD read/write",
    "start": "2068369",
    "end": "2074250"
  },
  {
    "text": "cache which does a really really nice job especially in random more clothes and even nicer on random write workloads",
    "start": "2074250",
    "end": "2079980"
  },
  {
    "text": "I mentioned the volumes can be shared I also mention that the volumes can be huge there's actually no no real limit",
    "start": "2079980",
    "end": "2086550"
  },
  {
    "text": "on the size of the volume so we have customers using 100 plus terabyte volumes snapshots are instant and",
    "start": "2086550",
    "end": "2093179"
  },
  {
    "text": "immediately available mention the protocols and the remote replication gives you as little as five minutes of",
    "start": "2093179",
    "end": "2099329"
  },
  {
    "text": "RPO recovery point objective we also offer data at rest encryption as well as",
    "start": "2099329",
    "end": "2105660"
  },
  {
    "text": "data in flight encryption a nice twist on the data at rest encryption is that you control the encryption decryption",
    "start": "2105660",
    "end": "2111630"
  },
  {
    "text": "key we do not AWS does not so you you need to manage that key for yourself",
    "start": "2111630",
    "end": "2117300"
  },
  {
    "text": "good news is that means nobody else can touch your data bad news is don't ever lose the key if you lose the key you're",
    "start": "2117300",
    "end": "2124230"
  },
  {
    "text": "done you're hosed and we actually put up a warning screen before you enable encryption saying literally five",
    "start": "2124230",
    "end": "2130320"
  },
  {
    "text": "different ways you'll lose the data irretrievably it'll become inaccessible it'll be lost it will be gone it'll be",
    "start": "2130320",
    "end": "2136020"
  },
  {
    "text": "dead there's no recovery and then old snapshots are instantly cloneable and",
    "start": "2136020",
    "end": "2142500"
  },
  {
    "text": "the clone is an instant zero capacity read/write volume that is an identical copy of the snapshot it can do that as",
    "start": "2142500",
    "end": "2149490"
  },
  {
    "text": "many times as you need it's really nice for test and dev for example and no limit on the number of volumes just",
    "start": "2149490",
    "end": "2154710"
  },
  {
    "text": "create as many as you need and each volume can be ice Kazi or NFS or sips your choice you can mix them one really",
    "start": "2154710",
    "end": "2162750"
  },
  {
    "text": "nice use case is dr or business continuity so using the snapshots you can you can have these automatically",
    "start": "2162750",
    "end": "2169260"
  },
  {
    "start": "2168000",
    "end": "2168000"
  },
  {
    "text": "scheduled say every five minutes in case there's a database corruption something like that you can go back to a previously working version and and",
    "start": "2169260",
    "end": "2176070"
  },
  {
    "text": "immediately be up and running again and then you in a relaxed fashion figure out what went wrong enroll in transactions",
    "start": "2176070",
    "end": "2182730"
  },
  {
    "text": "as necessary remote replication uses the very same snapshots to create state the same state in one of the other regions",
    "start": "2182730",
    "end": "2189450"
  },
  {
    "text": "or even on-premise if you want to work with us that way and recovery as I said",
    "start": "2189450",
    "end": "2195030"
  },
  {
    "text": "as as little as a five minute RPO in the event that something something went wrong let's say with the software or",
    "start": "2195030",
    "end": "2200910"
  },
  {
    "text": "even even with the facility a customer example a company called IG it's the",
    "start": "2200910",
    "end": "2207060"
  },
  {
    "text": "largest internet portal in Brazil you can think of it as the Yahoo of Brazil they're attaching 170 NFS clients to our",
    "start": "2207060",
    "end": "2215820"
  },
  {
    "text": "virtual private storage array to a single virtual private storage array obviously moving a lot of data terabytes",
    "start": "2215820",
    "end": "2221100"
  },
  {
    "text": "and terabytes per day and you can see just a short list of the volumes that they have here it's um actually a much",
    "start": "2221100",
    "end": "2227190"
  },
  {
    "text": "much longer screen I think that takes me back to Craig and who's going to",
    "start": "2227190",
    "end": "2233610"
  },
  {
    "text": "summarize this for us thank you thank",
    "start": "2233610",
    "end": "2243360"
  },
  {
    "text": "you very much gnome so knows being a little bit modest he didn't talk about their newest feature right they now can",
    "start": "2243360",
    "end": "2251160"
  },
  {
    "text": "as anybody here use n F sorry net app on Prem this net upon progress okay you can now",
    "start": "2251160",
    "end": "2256810"
  },
  {
    "text": "snap mirror to Zadora and you can snap mirror back from Zadora you can use on",
    "start": "2256810",
    "end": "2263170"
  },
  {
    "text": "tap on Zadora so if you have on-premise workloads and you want to snap mirror",
    "start": "2263170",
    "end": "2268390"
  },
  {
    "text": "them to AWS talk to no no man hell pew with that we covered a lot today and",
    "start": "2268390",
    "end": "2275740"
  },
  {
    "text": "we're going to take some time for QA we talked about single instances and how they're inexpensive but you have an",
    "start": "2275740",
    "end": "2282790"
  },
  {
    "text": "availability and a durability problem we talked about ephemeral drives that are super fast but you have an availability",
    "start": "2282790",
    "end": "2290200"
  },
  {
    "text": "problem we talked about clustering things together which solves your availability and your durability problem",
    "start": "2290200",
    "end": "2296140"
  },
  {
    "text": "but there's a cost so we talked mostly about trade-offs and options and and the ideas behind some of",
    "start": "2296140",
    "end": "2303790"
  },
  {
    "text": "these code and CloudFormation templates to build everything you see will be sent",
    "start": "2303790",
    "end": "2309700"
  },
  {
    "text": "to you on Wednesday so you can you can find we can give you easier ways to build this we have folks here at the",
    "start": "2309700",
    "end": "2317140"
  },
  {
    "text": "conference from xodar imagine attic softness and Red Hat storage they can help you with this problem as well so",
    "start": "2317140",
    "end": "2324070"
  },
  {
    "text": "questions please",
    "start": "2324070",
    "end": "2327420"
  }
]