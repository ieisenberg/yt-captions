[
  {
    "start": "0",
    "end": "153000"
  },
  {
    "text": "alright well thanks for joining everyone I'm Alex wood I'm a software developer",
    "start": "500",
    "end": "6569"
  },
  {
    "text": "with the ADA boost SDKs and tools team I work on the AWS sdk for Ruby and a",
    "start": "6569",
    "end": "12929"
  },
  {
    "text": "couple other things that will come up over the course of this talk so why are",
    "start": "12929",
    "end": "19949"
  },
  {
    "text": "we here we're here because our data is valuable for many different reasons and",
    "start": "19949",
    "end": "27359"
  },
  {
    "text": "I've talked to many of you over the course of the conference so far and as your businesses have been scaling up as",
    "start": "27359",
    "end": "33690"
  },
  {
    "text": "your number of users are scaling up you have a lot of data and there's a lot of",
    "start": "33690",
    "end": "39629"
  },
  {
    "text": "value in there that you want to get out now this data comes in many shapes many",
    "start": "39629",
    "end": "47160"
  },
  {
    "text": "file formats comes in many sizes from small files that you grep through to",
    "start": "47160",
    "end": "54440"
  },
  {
    "text": "petabytes of log data stored across s3 files and many sources sometimes you",
    "start": "54440",
    "end": "61739"
  },
  {
    "text": "have those logs in s3 sometimes they're on local instances sometimes you're",
    "start": "61739",
    "end": "67500"
  },
  {
    "text": "needing to copy from databases or from any number of other sources and in many",
    "start": "67500",
    "end": "75659"
  },
  {
    "text": "cases this starts out as a reactive process something went wrong or our",
    "start": "75659",
    "end": "83460"
  },
  {
    "text": "conversions just drop you made a bunch less money less here last month for example why did this happen we have no",
    "start": "83460",
    "end": "91140"
  },
  {
    "text": "idea you have a bunch of logs and the answers in there somewhere and you craft a purpose-built job and figure it out or",
    "start": "91140",
    "end": "97880"
  },
  {
    "text": "your system goes down where was the error that caused the problem and what",
    "start": "97880",
    "end": "104130"
  },
  {
    "text": "you start to find over time is these purpose-built reactive jobs don't scale",
    "start": "104130",
    "end": "109290"
  },
  {
    "text": "very well with your time so how can we get proactive how can we build the",
    "start": "109290",
    "end": "115770"
  },
  {
    "text": "solution before we know exactly what the problem is for as many cases as we can",
    "start": "115770",
    "end": "122030"
  },
  {
    "text": "so a little secret this talk is not for you this talk is actually for me about",
    "start": "122030",
    "end": "127770"
  },
  {
    "text": "two years ago where I had to face this kind of problem and made a lot of mistakes along the way or",
    "start": "127770",
    "end": "134989"
  },
  {
    "text": "had a lot of difficulties that didn't have to happen and my hope is that as",
    "start": "134989",
    "end": "141030"
  },
  {
    "text": "you go and face these kinds of problems in your own working lives that this can make it a little bit easier for you and",
    "start": "141030",
    "end": "147900"
  },
  {
    "text": "you can get going a little bit faster than I could so what are we going to go",
    "start": "147900",
    "end": "153959"
  },
  {
    "start": "153000",
    "end": "153000"
  },
  {
    "text": "over well we're going to build today a log processing system we're going to go",
    "start": "153959",
    "end": "160709"
  },
  {
    "text": "all the way from logs in s3 to a redshift database where you can generate",
    "start": "160709",
    "end": "167280"
  },
  {
    "text": "regular reports and have the ability for people to make ad hoc requests and to",
    "start": "167280",
    "end": "173700"
  },
  {
    "text": "get there we're going to talk about how you can build log processing jobs we're",
    "start": "173700",
    "end": "178709"
  },
  {
    "text": "going to talk about how you can automate it so that your time can scale we're",
    "start": "178709",
    "end": "183720"
  },
  {
    "text": "going to talk about how you can get this information into amazon redshift we're",
    "start": "183720",
    "end": "188819"
  },
  {
    "text": "going to talk about how you build reports on top of Amazon redshift which you know even then my talk is slightly",
    "start": "188819",
    "end": "195269"
  },
  {
    "text": "out of date because we've already released a new service to help you do this but it gives you a good starting",
    "start": "195269",
    "end": "201329"
  },
  {
    "text": "point and finally we'll get into some of the lessons I've learned as you start to",
    "start": "201329",
    "end": "207090"
  },
  {
    "text": "build a system at scale and maybe go through a few roadblocks that you might",
    "start": "207090",
    "end": "213180"
  },
  {
    "text": "encounter as you go through the same thing so one thing I'll keep in I'll",
    "start": "213180",
    "end": "218400"
  },
  {
    "text": "point out is we are going to go into the code level so we're going to be doing a little bit of live coding and walking",
    "start": "218400",
    "end": "224609"
  },
  {
    "text": "through a little bit of code and all of the code that we're going to go through",
    "start": "224609",
    "end": "230810"
  },
  {
    "text": "is available on github right now and",
    "start": "230810",
    "end": "236450"
  },
  {
    "text": "after the session is done we'll have the links in the slides and I'll tweet it out afterwards but you can go through",
    "start": "236450",
    "end": "243239"
  },
  {
    "text": "this and play around with this and this is all you need everything you need to",
    "start": "243239",
    "end": "248340"
  },
  {
    "text": "do everything we're going to do in this talk is in this repo so what I'm trying to make what I'm trying to promise is no",
    "start": "248340",
    "end": "254760"
  },
  {
    "text": "black boxes we're not going to say magic a plus magic be and then it works we're",
    "start": "254760",
    "end": "259799"
  },
  {
    "text": "going to go through every detail that you need",
    "start": "259799",
    "end": "263539"
  },
  {
    "text": "so where are we starting from this is a weblog that roughly speaking is in a",
    "start": "265620",
    "end": "272259"
  },
  {
    "text": "standard web log format you have IP addresses user information user agents",
    "start": "272259",
    "end": "278139"
  },
  {
    "text": "made quite a bit smaller to fit in a slide and information about the kind of",
    "start": "278139",
    "end": "283270"
  },
  {
    "text": "requests that they're making so this is going to be the example data we're going to build on top of and we're going to go",
    "start": "283270",
    "end": "289509"
  },
  {
    "text": "from web logs like this to output that can be digested by human beings such as",
    "start": "289509",
    "end": "296770"
  },
  {
    "start": "292000",
    "end": "292000"
  },
  {
    "text": "regular reports on what's going on inside the system or the ability to do",
    "start": "296770",
    "end": "302949"
  },
  {
    "text": "ad-hoc queries to answer the questions that your product managers your CIOs",
    "start": "302949",
    "end": "308139"
  },
  {
    "text": "might have that you as the builder of the system didn't know they would be asking so what goes into the system to",
    "start": "308139",
    "end": "317590"
  },
  {
    "start": "315000",
    "end": "315000"
  },
  {
    "text": "start off with we have logs in Amazon s3 this is not the only input source that",
    "start": "317590",
    "end": "323889"
  },
  {
    "text": "you could use but it's the most common pattern that I've seen and we're going",
    "start": "323889",
    "end": "330099"
  },
  {
    "text": "to start with if you have these logs what can we do with them we're going to talk about processing these logs using",
    "start": "330099",
    "end": "337000"
  },
  {
    "text": "Amazon Elastic MapReduce how can we take these logs and transform them and filter",
    "start": "337000",
    "end": "342310"
  },
  {
    "text": "them into something we can use we're going to again ingest them into amazon",
    "start": "342310",
    "end": "347800"
  },
  {
    "text": "redshift and talk about how we can build reports from that so let's start by",
    "start": "347800",
    "end": "356650"
  },
  {
    "text": "going through how do we write a log processing job so I have gone to other",
    "start": "356650",
    "end": "363940"
  },
  {
    "text": "conferences in this such as in the Ruby community where you know big data is",
    "start": "363940",
    "end": "369099"
  },
  {
    "text": "also a very hot topic and the thing I've heard about processing data through",
    "start": "369099",
    "end": "375370"
  },
  {
    "text": "tools like mapreduce is it feels hard to get started but I don't think that it",
    "start": "375370",
    "end": "380650"
  },
  {
    "text": "has to be and so we're going to build a streaming Amazon Elastic MapReduce job",
    "start": "380650",
    "end": "386620"
  },
  {
    "text": "in Ruby and run it so again what's our",
    "start": "386620",
    "end": "392440"
  },
  {
    "text": "starting point our starting point is logs in s3 and in this case we have them formatted",
    "start": "392440",
    "end": "401380"
  },
  {
    "text": "roughly like this to understand why that is useful one common pattern as you",
    "start": "401380",
    "end": "409600"
  },
  {
    "start": "406000",
    "end": "406000"
  },
  {
    "text": "store files in s3 is not only two separate objects across buckets but two separate across prefixes so why is this",
    "start": "409600",
    "end": "417250"
  },
  {
    "text": "handy well one EMR mechanic that's available when you're building your jobs",
    "start": "417250",
    "end": "422680"
  },
  {
    "text": "is to specify an s3 prefix and it will pull all files within that prefix so if",
    "start": "422680",
    "end": "429190"
  },
  {
    "text": "we have date separated logs simply saying the input is anything in this",
    "start": "429190",
    "end": "434320"
  },
  {
    "text": "bucket with this prefix will only include the files that we actually want",
    "start": "434320",
    "end": "439750"
  },
  {
    "text": "to be processing so we're not processing the same data over and over again so to",
    "start": "439750",
    "end": "447430"
  },
  {
    "text": "go back into it in this example of generated decent amount of logs so we're",
    "start": "447430",
    "end": "454690"
  },
  {
    "text": "talking a few gigabytes every day for the example scale and I've used prefixes",
    "start": "454690",
    "end": "460210"
  },
  {
    "text": "to separate each day's logs into a different virtual folder and that's",
    "start": "460210",
    "end": "465250"
  },
  {
    "text": "going to be our starting point",
    "start": "465250",
    "end": "468240"
  },
  {
    "text": "so from there we're going to build a system to process those logs using",
    "start": "474920",
    "end": "481620"
  },
  {
    "text": "Amazon Elastic MapReduce so to start off with what is Amazon Elastic MapReduce so",
    "start": "481620",
    "end": "490980"
  },
  {
    "start": "484000",
    "end": "484000"
  },
  {
    "text": "for the sake of the kind of jobs we're building the main concepts we want to think about is that it's a cluster of machines with the master instance that",
    "start": "490980",
    "end": "499230"
  },
  {
    "text": "does tasks such as job tracking and worker mappers and reducers and Hadoop",
    "start": "499230",
    "end": "505200"
  },
  {
    "text": "Alon top of this provide orchestration to split apart your input files to send into the mappers sort your map our",
    "start": "505200",
    "end": "512610"
  },
  {
    "text": "output output and provide that into your reducers and then once your reducers",
    "start": "512610",
    "end": "517919"
  },
  {
    "text": "have output put them into output files that can be ingested or used any other way you'd like so along the same",
    "start": "517920",
    "end": "526530"
  },
  {
    "text": "concepts a cluster is can made up of usually one master node and an indeterminate number of slave nodes",
    "start": "526530",
    "end": "533660"
  },
  {
    "text": "depending on the size of your workload and the type of the workload and we'll get a bit more later into how you would",
    "start": "533660",
    "end": "540210"
  },
  {
    "text": "select these and how you would design your cluster so let's talk about",
    "start": "540210",
    "end": "545370"
  },
  {
    "start": "544000",
    "end": "544000"
  },
  {
    "text": "streaming jobs Amazon Elastic MapReduce has a built in streaming jar you don't",
    "start": "545370",
    "end": "552390"
  },
  {
    "text": "need to provide it it's available for you and you're going to bring your own mapper which is an executable file that",
    "start": "552390",
    "end": "559740"
  },
  {
    "text": "can take input from standard in and right to center doubt and you're going to bring your own reducer along the same",
    "start": "559740",
    "end": "564960"
  },
  {
    "text": "lines and we're going to walk through that here in this presentation again Hadoop does all the orchestration around",
    "start": "564960",
    "end": "571770"
  },
  {
    "text": "this we just focus on making sure our mapper is correct and our reducer is correct and we can trust the system to",
    "start": "571770",
    "end": "578010"
  },
  {
    "text": "do its job so how's the mapper working",
    "start": "578010",
    "end": "583830"
  },
  {
    "text": "again we're taking input line by line from standard in in the Ruby sense the",
    "start": "583830",
    "end": "589920"
  },
  {
    "start": "586000",
    "end": "586000"
  },
  {
    "text": "Arg f variable is a stand-in for this and will work for our code now some of",
    "start": "589920",
    "end": "597240"
  },
  {
    "text": "you might have a bit of a raised eyebrow the fact that we're using Ruby to write a MapReduce job in one sense it's",
    "start": "597240",
    "end": "603480"
  },
  {
    "text": "because I can make the code very concise and easy to show in a presentation it's also a great",
    "start": "603480",
    "end": "608880"
  },
  {
    "text": "choice for prototyping a more complicated MapReduce operation before",
    "start": "608880",
    "end": "615000"
  },
  {
    "text": "you go and write it in lower level languages to try to squeeze every bit of performance out of them and again the",
    "start": "615000",
    "end": "622290"
  },
  {
    "text": "output goes the standard out in a streaming job so the bottom line what we're doing is filtering values and",
    "start": "622290",
    "end": "630800"
  },
  {
    "text": "let's take a look at what that looks like in code so what I have here is a",
    "start": "630800",
    "end": "642120"
  },
  {
    "text": "working mapper code again it takes line",
    "start": "642120",
    "end": "647190"
  },
  {
    "text": "by line input from standard in which in this case is each line of that sample log file that we showed before we're",
    "start": "647190",
    "end": "655170"
  },
  {
    "text": "going to split it by spaces and if it's not a full sized value like if there is",
    "start": "655170",
    "end": "661770"
  },
  {
    "text": "like a just a new line we're going to skip past that and not try to process it and from there we're filtering out the",
    "start": "661770",
    "end": "668340"
  },
  {
    "text": "particular values inside the sample log that we care about so we're going to",
    "start": "668340",
    "end": "674100"
  },
  {
    "text": "skip the IP address so why might we do that well one important concept when",
    "start": "674100",
    "end": "679350"
  },
  {
    "text": "you're building a data processing job is you're probably wanting to reduce the",
    "start": "679350",
    "end": "684390"
  },
  {
    "text": "size of the log file that goes into it or you're trying to create more",
    "start": "684390",
    "end": "689540"
  },
  {
    "text": "duplicate values that you can tally up with a count instead of simply storing",
    "start": "689540",
    "end": "695280"
  },
  {
    "text": "141 everything inside your logs so for example we're going to drop time of day",
    "start": "695280",
    "end": "700440"
  },
  {
    "text": "on the floor you might choose in your own applications to not do that or to roll up to an hourly level a minute",
    "start": "700440",
    "end": "707610"
  },
  {
    "text": "level or even a weekly level it depends on your application and what it questions you're trying to answer so",
    "start": "707610",
    "end": "715230"
  },
  {
    "text": "with this code is doing here and it is more heavily annotated in the open source repository we've taken out the",
    "start": "715230",
    "end": "721260"
  },
  {
    "text": "comments so it can help fit in this screen but we're taking out from that",
    "start": "721260",
    "end": "726600"
  },
  {
    "text": "string the different values we care about and then we're turning it into pipe delimited output and then putting",
    "start": "726600",
    "end": "734520"
  },
  {
    "text": "after a tab a one for the number of times we've seen that particular log entry in the mapper the fact that we",
    "start": "734520",
    "end": "741570"
  },
  {
    "text": "only have one tab now is kind of important because by default Hadoop is going to say everything before the first tab is what",
    "start": "741570",
    "end": "748199"
  },
  {
    "text": "I'm sorting so if we tab delimited the entire output it would just sort our output by IP addresses or user names and",
    "start": "748199",
    "end": "755339"
  },
  {
    "text": "it's not that useful so we want to make sure that Hadoop knows how to sort this",
    "start": "755339",
    "end": "761628"
  },
  {
    "text": "another helpful thing is you can actually test this so we have the sample",
    "start": "762379",
    "end": "768959"
  },
  {
    "text": "log from the earlier slide and we can run this Ruby code locally to get a good",
    "start": "768959",
    "end": "774749"
  },
  {
    "text": "idea of how well this is going to work so if I cat the sample log and run that",
    "start": "774749",
    "end": "781170"
  },
  {
    "text": "map or script you'll see that it transforms by reducing the log file to a",
    "start": "781170",
    "end": "787470"
  },
  {
    "text": "smaller size pipe delimiting it and this is something that can be passed into",
    "start": "787470",
    "end": "793379"
  },
  {
    "text": "Hadoop sorting and later reduce this is also very helpful for building unit",
    "start": "793379",
    "end": "799889"
  },
  {
    "text": "tests or integration tests of your mapping and reducing code if you can test it locally without having to build",
    "start": "799889",
    "end": "805470"
  },
  {
    "text": "a cluster and this allows you to do that",
    "start": "805470",
    "end": "809120"
  },
  {
    "start": "813000",
    "end": "813000"
  },
  {
    "text": "next let's talk about reducers so again",
    "start": "813139",
    "end": "818490"
  },
  {
    "text": "we're getting input from our mappers that's sorted by Hadoop by the time it gets to the reducer and that's important",
    "start": "818490",
    "end": "824819"
  },
  {
    "text": "for how we're going to write it again we're getting that map our output line",
    "start": "824819",
    "end": "829949"
  },
  {
    "text": "by line through standard in and we're going to transform that output mainly by",
    "start": "829949",
    "end": "837660"
  },
  {
    "text": "counting the duplicate keys and reducing it and this gives us a lot of our file storage savings that we're going to",
    "start": "837660",
    "end": "844920"
  },
  {
    "text": "realize when we move this in Amazon redshift and again the same concept with",
    "start": "844920",
    "end": "850980"
  },
  {
    "text": "the mapper we do output to standard out",
    "start": "850980",
    "end": "855199"
  },
  {
    "text": "so let's take a look at the reducer and there's a little bit more code here but the concept is pretty simple if you",
    "start": "858529",
    "end": "866670"
  },
  {
    "text": "remember from the testing output we had and we say it's sorted by Hadoop you're",
    "start": "866670",
    "end": "873899"
  },
  {
    "text": "going to see that there are some duplicate entries in this log essentially the same roughly roughly the",
    "start": "873899",
    "end": "882120"
  },
  {
    "text": "same operation happens many different times and we're going to reduce those into a single row that's going to go",
    "start": "882120",
    "end": "888990"
  },
  {
    "text": "into redshift and that's what this code is going to accomplish so what it does is if we see the same key multiple times",
    "start": "888990",
    "end": "895649"
  },
  {
    "text": "in a row we build a count and when the key changes we output our result so can",
    "start": "895649",
    "end": "902759"
  },
  {
    "text": "we get items line by line we're tab delimiting them now so that we can",
    "start": "902759",
    "end": "908819"
  },
  {
    "text": "ingest them into redshift more easily if we have every road limited or column delimited by the same value we're going",
    "start": "908819",
    "end": "915540"
  },
  {
    "text": "to have an easy time writing the ingestion code as you'll see later and then we simply roll up the duplicates",
    "start": "915540",
    "end": "923490"
  },
  {
    "text": "and output to standard out and that's all the reducer needs to do so we can",
    "start": "923490",
    "end": "930899"
  },
  {
    "text": "under we can grasp this we can understand this writing a mapper from",
    "start": "930899",
    "end": "935940"
  },
  {
    "text": "standard into standard out is not hard writing a reducer from standard in two standard out is not hard when the",
    "start": "935940",
    "end": "942569"
  },
  {
    "text": "orchestration around that is handled you know now we're cooking with gas we're",
    "start": "942569",
    "end": "947639"
  },
  {
    "text": "almost there so just to summarize how we",
    "start": "947639",
    "end": "953610"
  },
  {
    "start": "953000",
    "end": "953000"
  },
  {
    "text": "built this log processing job streaming mappers and reducers are executable scripts it could be a shell script it",
    "start": "953610",
    "end": "961199"
  },
  {
    "text": "can be a Python script it can be written in go you can write it in anything that",
    "start": "961199",
    "end": "966569"
  },
  {
    "text": "you like as long as on the dupe machines you can give it an executable command to",
    "start": "966569",
    "end": "972329"
  },
  {
    "text": "run Hadoop manages the orchestration around the streaming all you are",
    "start": "972329",
    "end": "977970"
  },
  {
    "text": "bringing is a notion of where the input is where the output is going and how",
    "start": "977970",
    "end": "983250"
  },
  {
    "text": "we're going to do the map and reduce operations which you can test locally the input comes through standard end",
    "start": "983250",
    "end": "990750"
  },
  {
    "text": "the output goes to standard out it's the golden rule every time you're writing these streaming tools and again it's",
    "start": "990750",
    "end": "999180"
  },
  {
    "text": "very important to remember that you can test this locally and this is very useful when you want to keep your",
    "start": "999180",
    "end": "1006050"
  },
  {
    "text": "development cycle tight and you're writing more complex mapper and reducer scripts so to show you what this looks",
    "start": "1006050",
    "end": "1012710"
  },
  {
    "text": "like we remember the sample log we were looking at before if we take this all",
    "start": "1012710",
    "end": "1019370"
  },
  {
    "text": "the way to the end we have duplicate entries they've been reduced together",
    "start": "1019370",
    "end": "1025180"
  },
  {
    "text": "it's tab delimited and we're down to only the fields that we care about in other words we're ready to put this into",
    "start": "1025180",
    "end": "1032270"
  },
  {
    "text": "a database and do something a little more interesting than reading now tab delimited still a human not",
    "start": "1032270",
    "end": "1038329"
  },
  {
    "text": "understandable file but before we get",
    "start": "1038329",
    "end": "1044900"
  },
  {
    "text": "there how are we going to orchestrate these jobs automation is important and",
    "start": "1044900",
    "end": "1050510"
  },
  {
    "text": "this is one of the biggest lessons that I've learned in actually building a system like this so we're going to use",
    "start": "1050510",
    "end": "1057440"
  },
  {
    "text": "the AWS sdk for Ruby to automate the creation of these jobs so that we can batch and scale that work the core",
    "start": "1057440",
    "end": "1066140"
  },
  {
    "start": "1064000",
    "end": "1064000"
  },
  {
    "text": "concepts around a streaming step that we need to know to do this automation our mapper and reducer source files a notion",
    "start": "1066140",
    "end": "1075230"
  },
  {
    "text": "of where our input files exist a notion of where they're going and what we're",
    "start": "1075230",
    "end": "1084470"
  },
  {
    "text": "going to do is we're going to live code the EMR API call that we need to make this happen so we've built our mapper as",
    "start": "1084470",
    "end": "1091820"
  },
  {
    "text": "we've seen we've built our reducer as we've seen we've put them up in s3 and",
    "start": "1091820",
    "end": "1097580"
  },
  {
    "text": "now we actually want to run it and see it work",
    "start": "1097580",
    "end": "1101500"
  },
  {
    "text": "so what I've got here is the beginnings of the EMR API call the name is",
    "start": "1104780",
    "end": "1112160"
  },
  {
    "text": "obviously whatever you'd want your cluster to be called the ami version were actually now also on ami version 4",
    "start": "1112160",
    "end": "1119830"
  },
  {
    "text": "so this is not going to often change between different clusters in a system",
    "start": "1119830",
    "end": "1125450"
  },
  {
    "text": "that you create the instance groups that's going to be a little bit more",
    "start": "1125450",
    "end": "1131180"
  },
  {
    "text": "interesting that's a bit more involved and that's going to be one of the things that we're live coding the ec2 key name",
    "start": "1131180",
    "end": "1137480"
  },
  {
    "text": "is important because if anything is going wrong in our cluster and this can",
    "start": "1137480",
    "end": "1143060"
  },
  {
    "text": "start to happen in scale we're going to want to be able to get on there and figure out what's going on inside of our cluster in real time so by providing an",
    "start": "1143060",
    "end": "1150560"
  },
  {
    "text": "ec2 key name we're going to be able to do that finally we have the steps in",
    "start": "1150560",
    "end": "1156020"
  },
  {
    "text": "this example we're only going to have one step to keep it simple we're going to process logs and give it an",
    "start": "1156020",
    "end": "1163310"
  },
  {
    "text": "appropriate name we're going to",
    "start": "1163310",
    "end": "1169400"
  },
  {
    "text": "terminate the flow if the step fails because there's no more work and we don't want to have a non-working cluster",
    "start": "1169400",
    "end": "1176000"
  },
  {
    "text": "operating and again remember how we said that the Hadoop streaming jar was built",
    "start": "1176000",
    "end": "1182210"
  },
  {
    "text": "in we can just point to a path on the instance and it'll be there and this",
    "start": "1182210",
    "end": "1187640"
  },
  {
    "text": "will be the streaming job that you need to run the code we've just written the arguments are what's interesting and",
    "start": "1187640",
    "end": "1193820"
  },
  {
    "text": "that's the next thing we'll dive into finally with EMR you want to make sure that you're including job flow and",
    "start": "1193820",
    "end": "1200090"
  },
  {
    "text": "service roles these are defaults that are generated the first time you create neum our job and if your mappers and",
    "start": "1200090",
    "end": "1207890"
  },
  {
    "text": "reducers only our processing line by line input and sending them out this is",
    "start": "1207890",
    "end": "1213590"
  },
  {
    "text": "all you need if your mappers or reducers are for example calling out to different",
    "start": "1213590",
    "end": "1218780"
  },
  {
    "text": "AWS services you may need to craft your own I am roles to do that and any of you",
    "start": "1218780",
    "end": "1224540"
  },
  {
    "text": "who are running into a situation like that also feel free to talk to me after it's a little more involved in what we're going to get to in this talk but",
    "start": "1224540",
    "end": "1231080"
  },
  {
    "text": "like many things it's pretty approachable once you get down to it",
    "start": "1231080",
    "end": "1235960"
  },
  {
    "text": "so first we're going to look at instance configuration which is just answering the question how many workers do we need",
    "start": "1238830",
    "end": "1245320"
  },
  {
    "start": "1239000",
    "end": "1239000"
  },
  {
    "text": "and how big do they have to be to handle the job that we're giving them it's",
    "start": "1245320",
    "end": "1251230"
  },
  {
    "text": "important to note that master and worker nodes are not doing the same thing and so you might need to have different",
    "start": "1251230",
    "end": "1257290"
  },
  {
    "text": "instance types appropriate to what they're doing to dive a little bit more",
    "start": "1257290",
    "end": "1263260"
  },
  {
    "text": "into that is we remember from the earlier graph the master node is generally doing job tracking and",
    "start": "1263260",
    "end": "1269020"
  },
  {
    "text": "orchestration which as a job gets very very big might be a very memory intensive task well your worker nodes",
    "start": "1269020",
    "end": "1277299"
  },
  {
    "text": "could be doing tasks that are less memory intensive and are more CPU or network bound in which case the same",
    "start": "1277299",
    "end": "1283929"
  },
  {
    "text": "instance type would not be appropriate for both instance types here and let's",
    "start": "1283929",
    "end": "1290440"
  },
  {
    "text": "actually live code that and see what it looks like",
    "start": "1290440",
    "end": "1294330"
  },
  {
    "text": "so right here the instance groups are an",
    "start": "1301649",
    "end": "1307899"
  },
  {
    "text": "array and it's going to be an array of hashes that describe the type of",
    "start": "1307899",
    "end": "1312969"
  },
  {
    "text": "instances we're creating and all of the descriptions of the arguments were using",
    "start": "1312969",
    "end": "1317979"
  },
  {
    "text": "are available in the EMR documentation and the 80 s SDK for Ruby documentation",
    "start": "1317979",
    "end": "1323289"
  },
  {
    "text": "really any SDKs documentation you're going to use to build this kind of system so we're going to have a master",
    "start": "1323289",
    "end": "1331269"
  },
  {
    "text": "group and a worker group",
    "start": "1331269",
    "end": "1335489"
  },
  {
    "text": "now the on-demand parameter obviously just means we are taking on demand instances as you build out a system and",
    "start": "1343500",
    "end": "1349950"
  },
  {
    "text": "you have an idea of how you're using it you may choose to get your instances from reserved instances that you've",
    "start": "1349950",
    "end": "1355350"
  },
  {
    "text": "purchased or you may get your worker nodes from the spot market if it's okay if the occasional job fails but for",
    "start": "1355350",
    "end": "1362820"
  },
  {
    "text": "simplicity we're using on demand here",
    "start": "1362820",
    "end": "1366620"
  },
  {
    "text": "obviously the instance role is a innumerable value of specific different",
    "start": "1370760",
    "end": "1377730"
  },
  {
    "text": "types of instance roles that are specified by Hadoop we're going to",
    "start": "1377730",
    "end": "1385110"
  },
  {
    "text": "choose to use em one large instances a little bit arbitrary for this but it's just going to be a value that's going to",
    "start": "1385110",
    "end": "1391350"
  },
  {
    "text": "work and then finally we need our worker",
    "start": "1391350",
    "end": "1400140"
  },
  {
    "text": "nodes again many of the same concepts it's on demand the core innumerable is",
    "start": "1400140",
    "end": "1411720"
  },
  {
    "text": "how Hadoop describes instance workers",
    "start": "1411720",
    "end": "1415940"
  },
  {
    "text": "and more arbitrarily we're going to choose a instance count of five and",
    "start": "1420300",
    "end": "1425760"
  },
  {
    "text": "again as you start to look at how your jobs are running in real life you will",
    "start": "1425760",
    "end": "1431490"
  },
  {
    "text": "start to kind of tweak that number up if you have cpu-bound jobs or maybe if you",
    "start": "1431490",
    "end": "1436950"
  },
  {
    "text": "have a job that's not using much CPU memory or networking at all your instance is way too big you could have a",
    "start": "1436950",
    "end": "1442050"
  },
  {
    "text": "smaller instance and perhaps more of them parallelism is definitely a virtue especially as you scale so those are the",
    "start": "1442050",
    "end": "1450990"
  },
  {
    "text": "instance groups and let's go straight to",
    "start": "1450990",
    "end": "1458190"
  },
  {
    "text": "understanding the arguments that are going into the streaming step again the first concept is where are we getting",
    "start": "1458190",
    "end": "1465870"
  },
  {
    "text": "our source files from so these have been",
    "start": "1465870",
    "end": "1470970"
  },
  {
    "text": "uploaded s3 so we have our mapper and reducer available and we just need to",
    "start": "1470970",
    "end": "1476370"
  },
  {
    "text": "point Hadoop to where those are",
    "start": "1476370",
    "end": "1480590"
  },
  {
    "text": "now you might notice that we just did something cool that probably doesn't come with your garden-variety Hadoop",
    "start": "1517010",
    "end": "1523070"
  },
  {
    "text": "distribution and that is that these files treated s3 like it's just part of your local file system and that is",
    "start": "1523070",
    "end": "1529670"
  },
  {
    "text": "functionality that Amazon Elastic MapReduce provides for you and we're going to be using this very liberally to",
    "start": "1529670",
    "end": "1535610"
  },
  {
    "text": "make our system as simple as possible so",
    "start": "1535610",
    "end": "1543050"
  },
  {
    "text": "next we specify where our input is coming from so again we described",
    "start": "1543050",
    "end": "1553340"
  },
  {
    "text": "earlier the way that we've been storing our logs and so we can simply say this prefix is what we're going to build our",
    "start": "1553340",
    "end": "1560150"
  },
  {
    "text": "job on top of and then finally as we've",
    "start": "1560150",
    "end": "1567290"
  },
  {
    "text": "discussed we're going to say where the files are going",
    "start": "1567290",
    "end": "1573250"
  },
  {
    "text": "and the last things we need our justice tell Hadoop what command it needs to run to run the mapper and what command it",
    "start": "1579860",
    "end": "1590240"
  },
  {
    "text": "needs to run the reducer and this is where your arbitrary executables go so",
    "start": "1590240",
    "end": "1595490"
  },
  {
    "text": "it seems like a lot when you get started but the EMR API is not very difficult to tame and once you've done that you get a",
    "start": "1595490",
    "end": "1602150"
  },
  {
    "text": "lot of time savings and so now if we",
    "start": "1602150",
    "end": "1607460"
  },
  {
    "text": "actually run this",
    "start": "1607460",
    "end": "1610390"
  },
  {
    "text": "we have a job flow created that's going to have everything that we've specified",
    "start": "1624509",
    "end": "1630259"
  },
  {
    "text": "it's going to know what jobs to run where to pull input from and where to",
    "start": "1630259",
    "end": "1635759"
  },
  {
    "text": "pull output from so for now we're going to get rid of this because we're going to get into a little bit of a more",
    "start": "1635759",
    "end": "1641249"
  },
  {
    "text": "interesting example later but this is just to show you that the EMR API is approachable you can write this and once",
    "start": "1641249",
    "end": "1649799"
  },
  {
    "text": "you've done that you gain a lot of automation benefits and let's talk a bit more about that so really quickly raise",
    "start": "1649799",
    "end": "1658739"
  },
  {
    "start": "1656000",
    "end": "1656000"
  },
  {
    "text": "your hand if you've ever created an EMR job before so quite a few of you and how",
    "start": "1658739",
    "end": "1664229"
  },
  {
    "text": "many of you did that through the console so actually let's have you than I",
    "start": "1664229",
    "end": "1669269"
  },
  {
    "text": "expected and it's the easiest way to get started with it you click through a few",
    "start": "1669269",
    "end": "1676199"
  },
  {
    "text": "menus it guides you through what you need to do like we've just done and you have a job up and running using the AWS",
    "start": "1676199",
    "end": "1684539"
  },
  {
    "text": "sdk it's very similar we build those options as we've described and run a single API call to get it going so what",
    "start": "1684539",
    "end": "1693299"
  },
  {
    "text": "happens when you're running a hundred jobs a day well using the console is",
    "start": "1693299",
    "end": "1698940"
  },
  {
    "text": "suddenly a little bit less appealing you could do it but it's going to take up a lot of your time but with the AWS sdk",
    "start": "1698940",
    "end": "1706349"
  },
  {
    "text": "for Ruby or any of our SDKs it's still just one command so how is that possible",
    "start": "1706349",
    "end": "1714559"
  },
  {
    "text": "it's possible through batching so once",
    "start": "1714559",
    "end": "1720959"
  },
  {
    "text": "we have batched our jobs you get an end state in production something like this you have different sized clusters that",
    "start": "1720959",
    "end": "1728449"
  },
  {
    "text": "could be running multiple steps from one to dozens to hundreds it all depends on",
    "start": "1728449",
    "end": "1734249"
  },
  {
    "text": "what kind of processing you're doing and you can run this with a single API call",
    "start": "1734249",
    "end": "1740789"
  },
  {
    "text": "per cluster and as you start to need to run dozens of clusters a day and run",
    "start": "1740789",
    "end": "1745979"
  },
  {
    "text": "hundreds of steps every day it's just a script that you can kick off with a cron every morning so let's take a look at",
    "start": "1745979",
    "end": "1754529"
  },
  {
    "text": "what batching looks like",
    "start": "1754529",
    "end": "1758179"
  },
  {
    "text": "so we've changed our log processor a little bit from what we just coded and",
    "start": "1761950",
    "end": "1768610"
  },
  {
    "text": "everything that we've done here is annotated in the github repo that is going to be linked to at the end of the",
    "start": "1768610",
    "end": "1774350"
  },
  {
    "text": "presentation again but the important thing to note here is we're just separating out what separates steps from",
    "start": "1774350",
    "end": "1782960"
  },
  {
    "text": "each other from what is global to the cluster and as it turns out describing",
    "start": "1782960",
    "end": "1789680"
  },
  {
    "text": "these steps is not really that difficult you have different names optionally but",
    "start": "1789680",
    "end": "1796340"
  },
  {
    "text": "recommended you have different input prefixes because you're not processing the same processing the same data",
    "start": "1796340",
    "end": "1802760"
  },
  {
    "text": "multiple times and you have different output destinations in most cases everything else is basically the same so",
    "start": "1802760",
    "end": "1811190"
  },
  {
    "text": "once you've written this once the amount of code you need to describe the different steps that you're going to run",
    "start": "1811190",
    "end": "1817010"
  },
  {
    "text": "is not a lot and we can get this up and running quite quickly so what this does",
    "start": "1817010",
    "end": "1826100"
  },
  {
    "text": "here is it runs a job that takes our shared instance options which are the",
    "start": "1826100",
    "end": "1831770"
  },
  {
    "text": "same as what we wrote before the different options that describe unique steps and is going to create a job from",
    "start": "1831770",
    "end": "1838910"
  },
  {
    "text": "those and the short version of that code is here again I highly encourage you to",
    "start": "1838910",
    "end": "1845330"
  },
  {
    "text": "look at the annotated repo when we're done it's a great starting point as you start to prototype these kinds of",
    "start": "1845330",
    "end": "1851210"
  },
  {
    "text": "applications",
    "start": "1851210",
    "end": "1853360"
  },
  {
    "text": "but let's run our batching example so",
    "start": "1857940",
    "end": "1864010"
  },
  {
    "text": "again it's one quick API call but we've made a job that's quite a bit more substantial in that process so it's",
    "start": "1864010",
    "end": "1873400"
  },
  {
    "text": "going to provision the instances that we've described but now we have multiple",
    "start": "1873400",
    "end": "1878860"
  },
  {
    "text": "steps that are each unique to the input and output sources that they're",
    "start": "1878860",
    "end": "1884920"
  },
  {
    "text": "processing and you'll also notice a little other staff has snuck in here to",
    "start": "1884920",
    "end": "1890200"
  },
  {
    "text": "set up debugging this one is really important in production and if you look",
    "start": "1890200",
    "end": "1896770"
  },
  {
    "text": "in the log processing code that we wrote we actually snuck this step in to be the",
    "start": "1896770",
    "end": "1903130"
  },
  {
    "text": "first step that every cluster runs and what having this step does for you",
    "start": "1903130",
    "end": "1910710"
  },
  {
    "text": "described right here is all of the logs that are generated by your tasks your",
    "start": "1914700",
    "end": "1921280"
  },
  {
    "text": "mappers reducers and the orchestration code are available to you so a job fails you can go dig into the logs and figure",
    "start": "1921280",
    "end": "1928360"
  },
  {
    "text": "out what happened otherwise your logs die with the cluster so this will put",
    "start": "1928360",
    "end": "1933580"
  },
  {
    "text": "them into an s3 bucket that you specify",
    "start": "1933580",
    "end": "1937649"
  },
  {
    "text": "and if we take a look here you can just see that every single job flow we create",
    "start": "1939900",
    "end": "1944980"
  },
  {
    "text": "has a very large amount of logs that we can go through so this is going to take",
    "start": "1944980",
    "end": "1952810"
  },
  {
    "text": "a while to run and so we're not going to worry about this for now but we have run",
    "start": "1952810",
    "end": "1960070"
  },
  {
    "text": "this earlier so we can show you it looks like at the end so it provisionally",
    "start": "1960070",
    "end": "1967180"
  },
  {
    "text": "instances it goes through each step and then we have processed log output that",
    "start": "1967180",
    "end": "1973690"
  },
  {
    "text": "is ready to go into Amazon redshift it's going to run the mappers and reducers using the same pattern that we saw",
    "start": "1973690",
    "end": "1980620"
  },
  {
    "text": "locally and we're ready to move on",
    "start": "1980620",
    "end": "1985620"
  },
  {
    "text": "so again to wrap up using the AWS SDKs in the EMR API allows us to do",
    "start": "1988040",
    "end": "1993900"
  },
  {
    "text": "automation at scale and getting started with it is simple you can build out this",
    "start": "1993900",
    "end": "2000590"
  },
  {
    "text": "prototype in less than an hour and then you're ready to use that as a starting",
    "start": "2000590",
    "end": "2007280"
  },
  {
    "text": "point as your system gets more complex and n greats more of the logic that you need to get things done to do that",
    "start": "2007280",
    "end": "2015260"
  },
  {
    "text": "you're going to separate common configuration from job specific you will start to build out a control plane that",
    "start": "2015260",
    "end": "2022010"
  },
  {
    "text": "describes the jobs that you need and then we're ready to ingest the data into",
    "start": "2022010",
    "end": "2027050"
  },
  {
    "text": "amazon redshift so again amazon redshift",
    "start": "2027050",
    "end": "2033140"
  },
  {
    "start": "2030000",
    "end": "2030000"
  },
  {
    "text": "is a scalable data warehouse and like",
    "start": "2033140",
    "end": "2039470"
  },
  {
    "text": "the log processing system we've designed it's ready to deal with the petabyte scale the most important thing for",
    "start": "2039470",
    "end": "2045650"
  },
  {
    "text": "writing this code is hidden right at the top of that graphic and that's that it",
    "start": "2045650",
    "end": "2051080"
  },
  {
    "text": "has JDBC and odbc adapters in other words it's a Postgres database for",
    "start": "2051080",
    "end": "2057139"
  },
  {
    "text": "almost all client purposes that you have and this is really handy so ingestion is",
    "start": "2057140",
    "end": "2064220"
  },
  {
    "start": "2063000",
    "end": "2063000"
  },
  {
    "text": "as simple as the sequel copy command if we provide a one-to-one mapping with",
    "start": "2064220",
    "end": "2070220"
  },
  {
    "text": "table columns separated by a delimiter which is what we've done a single command is going to load things into the",
    "start": "2070220",
    "end": "2077149"
  },
  {
    "text": "database exactly the way we expected so in this case we've created a table that",
    "start": "2077150",
    "end": "2084770"
  },
  {
    "text": "not coincidentally has a one-to-one mapping with the values that were in our",
    "start": "2084770",
    "end": "2089870"
  },
  {
    "text": "log system obviously if the order was different your reducer would be shuffling around the order before you",
    "start": "2089870",
    "end": "2095990"
  },
  {
    "text": "print out there's one thing that's interesting in here that also helps with",
    "start": "2095990",
    "end": "2101630"
  },
  {
    "text": "performance I'd like to draw your attention to and that is the distribution in sort keys a little bit",
    "start": "2101630",
    "end": "2108050"
  },
  {
    "text": "of the magic is in here and it's very worth checking out the Amazon redshift documentation to learn more about it but",
    "start": "2108050",
    "end": "2115190"
  },
  {
    "text": "it enables some common data warehouse queries to want run quickly what you'll notice when you look",
    "start": "2115190",
    "end": "2120980"
  },
  {
    "text": "at a table like this is you're probably not running reports for all of time you're probably running it for the last",
    "start": "2120980",
    "end": "2127369"
  },
  {
    "text": "week or the last month so by having a sort key around that the way that",
    "start": "2127369",
    "end": "2132650"
  },
  {
    "text": "redshift runs queries will very quickly trim out much of the data that you don't need to look at and will give you very",
    "start": "2132650",
    "end": "2139220"
  },
  {
    "text": "performant queries even as your total data crosses the petabyte threshold and",
    "start": "2139220",
    "end": "2145180"
  },
  {
    "text": "again copying is very simple redshift like EMR can alias s3 paths we can",
    "start": "2145180",
    "end": "2154130"
  },
  {
    "text": "provide custom delimiter 'he's and simply run a command to get it in so now",
    "start": "2154130",
    "end": "2163550"
  },
  {
    "text": "let's see what that looks like so I have an ec2 instance here that has",
    "start": "2163550",
    "end": "2168859"
  },
  {
    "text": "permissions to access my redshift database and we have a very simple piece",
    "start": "2168859",
    "end": "2177530"
  },
  {
    "text": "of code that I'll actually show here it'll be a little bit more clear that",
    "start": "2177530",
    "end": "2185630"
  },
  {
    "text": "connects to a database using the Ruby PG gem so again it's just postgres and we",
    "start": "2185630",
    "end": "2193640"
  },
  {
    "text": "have the operations needed to create our table and ingest data into it in a",
    "start": "2193640",
    "end": "2199910"
  },
  {
    "text": "scalable automatable manner so in this case I'm going to create a interactive",
    "start": "2199910",
    "end": "2207109"
  },
  {
    "text": "shell here",
    "start": "2207109",
    "end": "2209588"
  },
  {
    "text": "oh and I'm doing IRB wrong I think it's",
    "start": "2215490",
    "end": "2222030"
  },
  {
    "text": "a rule of live coding you have to get at least one small thing wrong at some point while you're going so we can",
    "start": "2222030",
    "end": "2234180"
  },
  {
    "text": "create a table and again it's just postgres and following the description",
    "start": "2234180",
    "end": "2243390"
  },
  {
    "text": "we have here we can ingest data from the",
    "start": "2243390",
    "end": "2249570"
  },
  {
    "text": "bucket that we've been storing processed data in and if we go back to our output",
    "start": "2249570",
    "end": "2259710"
  },
  {
    "text": "we can follow along this prefix to ingest all of this",
    "start": "2259710",
    "end": "2264710"
  },
  {
    "text": "and then finally we're going to run this with an IM role an interesting thing to",
    "start": "2274640",
    "end": "2279980"
  },
  {
    "text": "notice about this code is that you do need to specify your credentials for redshift to be able to access your data",
    "start": "2279980",
    "end": "2286010"
  },
  {
    "text": "from s3 and you should never hard code this please if you take one thing away",
    "start": "2286010",
    "end": "2291799"
  },
  {
    "text": "from this talk don't let it be about mappers and reducers don't let it be about databases let it be never ever",
    "start": "2291799",
    "end": "2297829"
  },
  {
    "text": "ever hardcode your credentials and so what we're going to do is we're going to take it from an IM role on the ec2",
    "start": "2297829",
    "end": "2304309"
  },
  {
    "text": "instance we pull the credentials from you see to metadata and provide that to red shift and it will work",
    "start": "2304309",
    "end": "2311619"
  },
  {
    "text": "and it's not really cooperating with me right now sing me one second",
    "start": "2329960",
    "end": "2335140"
  },
  {
    "text": "okay so running this ingest data process is quite quick even where if you look at",
    "start": "2339680",
    "end": "2347670"
  },
  {
    "text": "the output files we've generated we have solidly over a gigabyte of output data",
    "start": "2347670",
    "end": "2355349"
  },
  {
    "text": "which is helpfully parallelized for you by EMR for the easiest loading into redshift running a load query let's go",
    "start": "2355349",
    "end": "2367529"
  },
  {
    "text": "back to that load that we just did",
    "start": "2367529",
    "end": "2370789"
  },
  {
    "text": "running that load query only takes about a minute and a half so it is definitely not going to be the slowest thing in",
    "start": "2375410",
    "end": "2382619"
  },
  {
    "text": "your data pipeline so we can see running",
    "start": "2382619",
    "end": "2389099"
  },
  {
    "text": "that command to copy into the table takes just about one minute and six",
    "start": "2389099",
    "end": "2395910"
  },
  {
    "text": "seconds for over a gigabyte of data and that's helpful",
    "start": "2395910",
    "end": "2401359"
  },
  {
    "text": "so again amazon redshift the interface is just like sequel you can alias with",
    "start": "2405579",
    "end": "2412220"
  },
  {
    "text": "an s3 source just like you do with the mr it's ready to load from the EMR",
    "start": "2412220",
    "end": "2417529"
  },
  {
    "text": "output structure and now we can start to talk about how we generate reports once",
    "start": "2417529",
    "end": "2423170"
  },
  {
    "text": "it's in red shift now again this is kind of cool because when I was writing this",
    "start": "2423170",
    "end": "2429410"
  },
  {
    "text": "talk I was using the tools that were available to me and now we have a new one which is the business intelligence",
    "start": "2429410",
    "end": "2436849"
  },
  {
    "text": "service that was talked about at the keynote I believe two days ago this one",
    "start": "2436849",
    "end": "2442869"
  },
  {
    "text": "there's there so many I'm losing track very quickly of how many new services that we're launching but this is",
    "start": "2442869",
    "end": "2450319"
  },
  {
    "text": "actually going to be really helpful and again any other tool that you have that",
    "start": "2450319",
    "end": "2455509"
  },
  {
    "text": "can build reports off of postgres databases is ready to go with this system so you can go from a simple count",
    "start": "2455509",
    "end": "2464239"
  },
  {
    "start": "2461000",
    "end": "2461000"
  },
  {
    "text": "query of how many users you have you can have date ranged queries about how many",
    "start": "2464239",
    "end": "2469969"
  },
  {
    "start": "2467000",
    "end": "2467000"
  },
  {
    "text": "requests you're getting to your website or maybe you didn't anticipate that your",
    "start": "2469969",
    "end": "2476359"
  },
  {
    "text": "CIO really cares about this ad campaign he's been running and he wants to know how much activity is coming from your",
    "start": "2476359",
    "end": "2483019"
  },
  {
    "text": "newest registered users but all of that can be done through redshift through a",
    "start": "2483019",
    "end": "2489529"
  },
  {
    "text": "sequel query database and it operates at the scale that you need and again going",
    "start": "2489529",
    "end": "2494900"
  },
  {
    "start": "2494000",
    "end": "2494000"
  },
  {
    "text": "back to the beginning this supports planned reports so you're saying how many requests am I getting every day and",
    "start": "2494900",
    "end": "2500839"
  },
  {
    "text": "you're emailing it to your CTO every morning you can do that it supports ad",
    "start": "2500839",
    "end": "2506509"
  },
  {
    "text": "hoc queries through any postgres adapter that you can bring so we have",
    "start": "2506509",
    "end": "2513789"
  },
  {
    "text": "programmatic reporting with sequel query logic that's not redshifts specific you",
    "start": "2513789",
    "end": "2520309"
  },
  {
    "text": "don't have to learn any new redshift query language to get things done this was probably the fastest part of",
    "start": "2520309",
    "end": "2525739"
  },
  {
    "text": "building a system like this for myself a new sequel I'd never used the data warehouse before but writing report",
    "start": "2525739",
    "end": "2532729"
  },
  {
    "text": "queries is easy i wrote the queries the way i would do it for any other sequel database and they just work",
    "start": "2532729",
    "end": "2537990"
  },
  {
    "text": "colombier storage optimizes for common data warehouse queries breaking apart",
    "start": "2537990",
    "end": "2543070"
  },
  {
    "text": "your queries by end dates will give you a huge performance benefit you can use",
    "start": "2543070",
    "end": "2548830"
  },
  {
    "text": "s3 to store reports which is very handy you can have the output from your",
    "start": "2548830",
    "end": "2553930"
  },
  {
    "text": "reports go into s3 where it's available to almost any other tool that you have and you can take advantage of some",
    "start": "2553930",
    "end": "2560380"
  },
  {
    "text": "interesting postgres sequel features like window functions and common table expressions so let's take a quick look",
    "start": "2560380",
    "end": "2569740"
  },
  {
    "text": "at some of that so in the repo that we've linked I built a very simple",
    "start": "2569740",
    "end": "2577840"
  },
  {
    "text": "reporting tool that again just connects to it like any other Postgres database and runs a few example interesting",
    "start": "2577840",
    "end": "2586690"
  },
  {
    "text": "queries on the data that we've processed to see what we can learn from it and so",
    "start": "2586690",
    "end": "2594400"
  },
  {
    "text": "let's take a look at that",
    "start": "2594400",
    "end": "2597119"
  },
  {
    "text": "okay and that part seems to be missing from my instance incidentally but the idea is if you look at the queries that",
    "start": "2636339",
    "end": "2642880"
  },
  {
    "text": "we've provided here you can get very quick turnaround on how many users you have how many requests you have each day",
    "start": "2642880",
    "end": "2650140"
  },
  {
    "text": "or what were your top product pages if you have a ecommerce store for example",
    "start": "2650140",
    "end": "2656349"
  },
  {
    "text": "and you can build these queries quite easily you can turn them into CSVs and",
    "start": "2656349",
    "end": "2661779"
  },
  {
    "text": "send out an excel sheet every morning to anyone technical or non-technical that",
    "start": "2661779",
    "end": "2666940"
  },
  {
    "text": "you need to communicate this with so",
    "start": "2666940",
    "end": "2672160"
  },
  {
    "text": "let's dig into some of the finer points of this as we start to go into production so this is a nice toy that",
    "start": "2672160",
    "end": "2679150"
  },
  {
    "text": "we've built it operates on a few gigabytes of logs you might have more than this and you think okay this is",
    "start": "2679150",
    "end": "2684789"
  },
  {
    "text": "great i can get it to work but can this actually scale can this scale to the needs that I have so before I get into",
    "start": "2684789",
    "end": "2694719"
  },
  {
    "text": "this story I always feel like reminding mainly myself of exactly how big a petabyte is pedda places a lot of data",
    "start": "2694719",
    "end": "2702579"
  },
  {
    "text": "1,000 terabytes or one quadrillion bites and I actually had to look that number",
    "start": "2702579",
    "end": "2707739"
  },
  {
    "text": "up cuz I didn't know what it was I just knew it had a lot of zeros well one day",
    "start": "2707739",
    "end": "2713349"
  },
  {
    "text": "I had a problem or I had five quadrillion problems and that's a large",
    "start": "2713349",
    "end": "2719799"
  },
  {
    "start": "2716000",
    "end": "2716000"
  },
  {
    "text": "amount of the data that we had was wrong we had a incomplete input source the",
    "start": "2719799",
    "end": "2725529"
  },
  {
    "text": "system was running fine but if you have bad garbage data in garbage data out and",
    "start": "2725529",
    "end": "2730599"
  },
  {
    "text": "I had to replace 5 petabytes of data in like any time that you have an in",
    "start": "2730599",
    "end": "2735849"
  },
  {
    "text": "accuracy and something that people read they needed it last week so I needed to",
    "start": "2735849",
    "end": "2742569"
  },
  {
    "text": "fill this up really really fast so what",
    "start": "2742569",
    "end": "2747640"
  },
  {
    "text": "do you do when you live in a world with AWS well you spin up 1,200 machines you",
    "start": "2747640",
    "end": "2753910"
  },
  {
    "text": "turn through that 5 petabytes in about 36 hours and the new data is there and",
    "start": "2753910",
    "end": "2759249"
  },
  {
    "text": "everybody is happy and once i was done i went i grabbed all those 1,200 machines",
    "start": "2759249",
    "end": "2765430"
  },
  {
    "text": "and i threw them in a dumpster out back and refused to pay for any more of their time it seems crazy but that's what you",
    "start": "2765430",
    "end": "2775070"
  },
  {
    "text": "can do if you have a giant amount of burst capacity about above what you use",
    "start": "2775070",
    "end": "2780500"
  },
  {
    "text": "every day you can get that done you can use on demand instances you can you spot",
    "start": "2780500",
    "end": "2785540"
  },
  {
    "text": "instances you go through your load and then you drop it all on the floor and you don't pay for it again and I want",
    "start": "2785540",
    "end": "2792770"
  },
  {
    "text": "you to close your eyes for a second and imagine five years ago or ten years ago",
    "start": "2792770",
    "end": "2798490"
  },
  {
    "text": "what is happening to you if you have this problem to solve you can open your",
    "start": "2798490",
    "end": "2804680"
  },
  {
    "text": "eyes I don't want to give you nightmares I don't want to have that on my conscience so what did we learn from",
    "start": "2804680",
    "end": "2811430"
  },
  {
    "start": "2810000",
    "end": "2810000"
  },
  {
    "text": "doing this it wasn't necessarily easy well one is I've alluded to a few times",
    "start": "2811430",
    "end": "2817340"
  },
  {
    "text": "Master instant selection matters I was lazy and I was using the same instance",
    "start": "2817340",
    "end": "2822770"
  },
  {
    "text": "type for my masters and workers and as soon as I tried to spin this up the first time it was way too much data and",
    "start": "2822770",
    "end": "2829520"
  },
  {
    "text": "the master just completely fell failed the jobtracker couldn't handle how much",
    "start": "2829520",
    "end": "2835400"
  },
  {
    "text": "data was coming in the workers were fine but the job would never complete and we",
    "start": "2835400",
    "end": "2841280"
  },
  {
    "text": "had to use a different instance type that had a lot more memory to be able to support the size of our load so you want",
    "start": "2841280",
    "end": "2847670"
  },
  {
    "text": "to pay attention to what kind of master instances are you selecting for the kind of jobs you're running the memory on",
    "start": "2847670",
    "end": "2854570"
  },
  {
    "text": "your workers matters how many mappers do you need to run for how much of the workload is based on mapping how many",
    "start": "2854570",
    "end": "2861230"
  },
  {
    "text": "reducers do you need to run how much memory do they need to get their job done as little as needed so you can",
    "start": "2861230",
    "end": "2867590"
  },
  {
    "text": "parallel eyes as much as possible but enough so that mappers aren't persistently failing and your job never",
    "start": "2867590",
    "end": "2872960"
  },
  {
    "text": "completes the final thing we learned is that elasticity is awesome the fact that",
    "start": "2872960",
    "end": "2879800"
  },
  {
    "text": "we could spin this up so quickly and spin it down just as quickly enabled us",
    "start": "2879800",
    "end": "2884840"
  },
  {
    "text": "to solve a really scary problem in a couple of days and not have to deal with",
    "start": "2884840",
    "end": "2890780"
  },
  {
    "text": "the persistent budget of building out a new data center just to solve a one-time problem so the next thing I let last",
    "start": "2890780",
    "end": "2900020"
  },
  {
    "start": "2897000",
    "end": "2897000"
  },
  {
    "text": "thing I'd like to talk about is a few more or less you're going to learn as you bring this into production and as I've hinted to",
    "start": "2900020",
    "end": "2908089"
  },
  {
    "text": "again repeated manual tasks are evil they start out innocently enough but",
    "start": "2908089",
    "end": "2914930"
  },
  {
    "text": "then you're spending all of your time looking through output making sure everything's there looking through your",
    "start": "2914930",
    "end": "2921380"
  },
  {
    "text": "database making sure things loaded the way is they expected anything that you can automate please automate it don't do",
    "start": "2921380",
    "end": "2930380"
  },
  {
    "text": "it for me do it for yourself having multiple sources of truth is important",
    "start": "2930380",
    "end": "2936109"
  },
  {
    "text": "we didn't get 25 petabytes of bad data",
    "start": "2936109",
    "end": "2942190"
  },
  {
    "text": "overnight we got there because we weren't checking with other potential",
    "start": "2942190",
    "end": "2948109"
  },
  {
    "text": "data sources to see are we actually loading everything that we need so if",
    "start": "2948109",
    "end": "2953660"
  },
  {
    "text": "you have multiple sources of truth try to check against them see if your numbers are different sometimes they're",
    "start": "2953660",
    "end": "2960079"
  },
  {
    "text": "wrong sometimes you're wrong but if you have these multiple sources of truth you're not going to be wrong for nearly",
    "start": "2960079",
    "end": "2966770"
  },
  {
    "text": "as long understand the storage ramifications of your table design if",
    "start": "2966770",
    "end": "2973329"
  },
  {
    "text": "you want to keep track of IP addresses if that's important to you you're going",
    "start": "2973329",
    "end": "2978589"
  },
  {
    "text": "to need to store potentially a lot more data if users or changing IP addresses a lot so our IP address is important can",
    "start": "2978589",
    "end": "2985970"
  },
  {
    "text": "you drop them on the floor do you need to have down to the second granularity about what people are doing with your",
    "start": "2985970",
    "end": "2992569"
  },
  {
    "text": "system you can do that redshift can handle it EMR can handle it but you're",
    "start": "2992569",
    "end": "2997700"
  },
  {
    "text": "going to be storing a lot more data so if you want to keep your ops people happy be judicious about what data",
    "start": "2997700",
    "end": "3005680"
  },
  {
    "text": "you're saving automate your validation again notice that we've been storing our",
    "start": "3005680",
    "end": "3012460"
  },
  {
    "text": "data in s3 it's as simple as a list objects query ness three with the prefix",
    "start": "3012460",
    "end": "3018430"
  },
  {
    "text": "I expect to have output to say did my job actually finish or if it did finish",
    "start": "3018430",
    "end": "3025359"
  },
  {
    "text": "did it actually produce what it said it was going to produce you can automate these things and just kick off a new job",
    "start": "3025359",
    "end": "3030549"
  },
  {
    "text": "if you had a job failure they happen from time to time and so for validation again it's the",
    "start": "3030549",
    "end": "3040720"
  },
  {
    "text": "same concept of checking through the examples making sure that you know if I",
    "start": "3040720",
    "end": "3048700"
  },
  {
    "text": "expected 10-8 to have data right now run that query and create a new job using",
    "start": "3048700",
    "end": "3054460"
  },
  {
    "text": "the control plane we described without you ever seeing it periodically in the",
    "start": "3054460",
    "end": "3060640"
  },
  {
    "text": "system i maintain we do hundreds of load jobs every day almost all of them",
    "start": "3060640",
    "end": "3066460"
  },
  {
    "text": "succeed one or two a day don't a couple instances die the job doesn't complete",
    "start": "3066460",
    "end": "3072309"
  },
  {
    "text": "successfully I never really noticed because the next morning a validation job runs and statistically they almost",
    "start": "3072309",
    "end": "3078940"
  },
  {
    "text": "never failed twice and when they do you just run validation again so I've taken away that manual work from my life and",
    "start": "3078940",
    "end": "3087309"
  },
  {
    "text": "I'm a lot happier for it and remember",
    "start": "3087309",
    "end": "3092380"
  },
  {
    "start": "3091000",
    "end": "3091000"
  },
  {
    "text": "you don't have to do this all yourself this is not a show-and-tell this is a",
    "start": "3092380",
    "end": "3098859"
  },
  {
    "text": "prototype this is a guide post that says if you don't know how to do MapReduce",
    "start": "3098859",
    "end": "3104170"
  },
  {
    "text": "going in you can start with this and you can build a real system following these patterns but there are a lot of related",
    "start": "3104170",
    "end": "3111819"
  },
  {
    "text": "services that help you not only the new bi service but AWS data pipeline can",
    "start": "3111819",
    "end": "3117009"
  },
  {
    "text": "help you do this orchestration and now you'll understand what it's doing for you behind the scenes amazon machine",
    "start": "3117009",
    "end": "3123369"
  },
  {
    "text": "learning can give you a lot of really helpful insights on that data you've spent all that work putting into",
    "start": "3123369",
    "end": "3129160"
  },
  {
    "text": "redshift Kinesis can be another source of input maybe you're streaming data in",
    "start": "3129160",
    "end": "3134980"
  },
  {
    "text": "real time and to read shift and Kinesis can do that and now you know the concepts behind it you can send out",
    "start": "3134980",
    "end": "3141309"
  },
  {
    "text": "those reports that you've put into s3 using Amazon simple email service or Amazon simple notification service to",
    "start": "3141309",
    "end": "3146980"
  },
  {
    "text": "even build an API around it make API calls every time a report is done if you",
    "start": "3146980",
    "end": "3152319"
  },
  {
    "text": "need to and also one thing I've noticed as I walk around the huge booth area is",
    "start": "3152319",
    "end": "3159009"
  },
  {
    "text": "there are numerous AWS partners that are more than happy to help you answer some",
    "start": "3159009",
    "end": "3165130"
  },
  {
    "text": "of your data problems so check out the AWS marketplace anything that's going to work on top of",
    "start": "3165130",
    "end": "3170830"
  },
  {
    "text": "postgres is going to work so to wrap up",
    "start": "3170830",
    "end": "3176190"
  },
  {
    "text": "we know how to build an Amazon Elastic MapReduce job we know how to automate",
    "start": "3176190",
    "end": "3182290"
  },
  {
    "text": "this cluster creation using the AWS sdk we can format those results and ingest",
    "start": "3182290",
    "end": "3189010"
  },
  {
    "text": "them automatically int amazon redshift we can create useful reports using any",
    "start": "3189010",
    "end": "3195760"
  },
  {
    "text": "tool that can be built on top of postgres and we can start to think about how are we going to scale the system and",
    "start": "3195760",
    "end": "3203110"
  },
  {
    "text": "how are we going to run it in production and that's really why we're here and that's what matters so there are",
    "start": "3203110",
    "end": "3213790"
  },
  {
    "start": "3213000",
    "end": "3213000"
  },
  {
    "text": "resources and again I will be tweeting these out and the slide should be available and the talk will be again",
    "start": "3213790",
    "end": "3219310"
  },
  {
    "text": "available on youtube if you'd like to review check out the sample code play around with it there's a lot of really",
    "start": "3219310",
    "end": "3226390"
  },
  {
    "text": "helpful insights in the elastic MapReduce documentation and the redshift documentation and everything you need to",
    "start": "3226390",
    "end": "3232990"
  },
  {
    "text": "build these api calls and more is in the documentation for the AWS SDKs you can",
    "start": "3232990",
    "end": "3239230"
  },
  {
    "text": "find me on Twitter I'd love to hear about what you're building and what questions you still have so thank you",
    "start": "3239230",
    "end": "3245320"
  },
  {
    "text": "very much for your time",
    "start": "3245320",
    "end": "3248010"
  }
]