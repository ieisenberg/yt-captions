[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "good afternoon we'll start on time thank you everybody for being here my name is",
    "start": "1580",
    "end": "7319"
  },
  {
    "text": "Abby shakes and I'm the principal product manager responsible for a couple of products in the Big Data space",
    "start": "7319",
    "end": "13380"
  },
  {
    "text": "amazonia Mar and Amazon and peanut to be specific and I have an immense",
    "start": "13380",
    "end": "18510"
  },
  {
    "text": "opportunity here to welcome two customers who are going to talk about a use case who are going to talk about how",
    "start": "18510",
    "end": "25109"
  },
  {
    "text": "they have built a data platform on top of a table Louis using EMR so these two",
    "start": "25109",
    "end": "30949"
  },
  {
    "text": "companies are very diverse in their cultures they come from very different businesses but the reason we kind of",
    "start": "30949",
    "end": "36840"
  },
  {
    "text": "brought them together well they have one common theme that kind of binds them together and they will talk about how",
    "start": "36840",
    "end": "42840"
  },
  {
    "text": "they're built these platforms the common theme is they are all going from a team",
    "start": "42840",
    "end": "48690"
  },
  {
    "text": "that does data analytics for their businesses to a team that is building a data platform for all their lines of",
    "start": "48690",
    "end": "55649"
  },
  {
    "text": "businesses so I'm delighted to welcome salesforce.com and Vanguard both of a",
    "start": "55649",
    "end": "63840"
  },
  {
    "text": "premier customers on to the stage let's start with salesforce.com that's Rupa",
    "start": "63840",
    "end": "70049"
  },
  {
    "text": "and Sidharth from Salesforce thank you shake so so my name is Rupa Gupta and I",
    "start": "70049",
    "end": "83400"
  },
  {
    "start": "80000",
    "end": "130000"
  },
  {
    "text": "I became part of Salesforce through the acquisition of Crocs it happened couple of years ago and I think",
    "start": "83400",
    "end": "89880"
  },
  {
    "text": "we started around in 2010 and the interesting thing is you know we have",
    "start": "89880",
    "end": "95280"
  },
  {
    "text": "grown with Amazon you know at that time AWS was also I mean the reinvent conferences were not this big you could",
    "start": "95280",
    "end": "101970"
  },
  {
    "text": "go into one casino and that that was it but in the last you know we have grown",
    "start": "101970",
    "end": "107670"
  },
  {
    "text": "to the Amazon we have learned with Amazon we have pushed we have pushed our be shake and the team and all and so that is something but what we excited",
    "start": "107670",
    "end": "115380"
  },
  {
    "text": "about is we have built our entire technology now now in Amazon and I am very excited to share some of these so",
    "start": "115380",
    "end": "120659"
  },
  {
    "text": "with me we have said I'm Siddharth Sharma and I'll eat later Yvette manages the handshake between",
    "start": "120659",
    "end": "126390"
  },
  {
    "text": "Amazon Web Services and Salesforce DMK platform whoo so I'll start with the",
    "start": "126390",
    "end": "133290"
  },
  {
    "start": "130000",
    "end": "145000"
  },
  {
    "text": "forward-looking statement which is you know just to make sure our legal team is happy whatever you whatever you buy or",
    "start": "133290",
    "end": "140459"
  },
  {
    "text": "whatever you visit the stock or the product just make sure it's based on what we have so before we start on the technology I",
    "start": "140459",
    "end": "148109"
  },
  {
    "start": "145000",
    "end": "202000"
  },
  {
    "text": "just wanted to give a quick you know what is a DMP what does diem really mean so think of DMP we are part of marketing",
    "start": "148109",
    "end": "155489"
  },
  {
    "text": "cloud so it's a it's a critical piece of the marketing you know tech stack and what we do is we collect data from all",
    "start": "155489",
    "end": "163139"
  },
  {
    "text": "the different sources that brands or publishers have so whether it's your browsers whether it's your app whether",
    "start": "163139",
    "end": "170159"
  },
  {
    "text": "it's you know CRM system whether it's offline files all that we collect the data and this is both you're known as",
    "start": "170159",
    "end": "176280"
  },
  {
    "text": "well as your unknown users then we apply we unify the data we apply and we clean the data and we apply an identity layer",
    "start": "176280",
    "end": "183000"
  },
  {
    "text": "on top of it and finally we allow our customers to create segments so these are the",
    "start": "183000",
    "end": "188579"
  },
  {
    "text": "segments that our customers are interested in they can be used for targeting they can be used for giving",
    "start": "188579",
    "end": "194430"
  },
  {
    "text": "different experience to their clients and every market in the world is trying to do that they're trying to find the right audience now if you look at our",
    "start": "194430",
    "end": "203729"
  },
  {
    "start": "202000",
    "end": "229000"
  },
  {
    "text": "DLP it is it it is at internet scale if you look at this is like a you know 60 second view of how much data that we",
    "start": "203729",
    "end": "210090"
  },
  {
    "text": "collect and just as a comparison if you think about it Twitter does about 350,000 you know tweets per minute and",
    "start": "210090",
    "end": "216479"
  },
  {
    "text": "we have about 200,000 QPS so we collect data from all over the world and we have",
    "start": "216479",
    "end": "221849"
  },
  {
    "text": "grown you know when we will start up we were very small we had a few clients but we have grown with Amazon to be at this",
    "start": "221849",
    "end": "227400"
  },
  {
    "text": "scale this is just you know some of the brands that are that are part of our you",
    "start": "227400",
    "end": "233819"
  },
  {
    "start": "229000",
    "end": "243000"
  },
  {
    "text": "know our you know client base and we have more than 40 40 petabytes of data that we process that we analyze that we",
    "start": "233819",
    "end": "240239"
  },
  {
    "text": "filter on a regular basis this is the so",
    "start": "240239",
    "end": "245549"
  },
  {
    "start": "243000",
    "end": "377000"
  },
  {
    "text": "now we'll get into a little bit of tech so so I would love to show you the kind of architecture that we have built this",
    "start": "245549",
    "end": "251250"
  },
  {
    "text": "is an overall architecture of how we have build the DMP and then we will get deeper into one aspect of it so in terms",
    "start": "251250",
    "end": "258389"
  },
  {
    "text": "of data collection you know as we you know as I said you can get it from your",
    "start": "258389",
    "end": "263710"
  },
  {
    "text": "ages from your browsers from your apps from CRM system all that data gets collected we process it and we keep it",
    "start": "263710",
    "end": "271270"
  },
  {
    "text": "all in all in s3 there is also what we do is all this data that comes in it",
    "start": "271270",
    "end": "276699"
  },
  {
    "text": "goes through a real-time pipe real-time pipeline we use Kafka for that and what we do is we use spark streaming to take",
    "start": "276699",
    "end": "283270"
  },
  {
    "text": "this real-time stream we clean it we assign meaning to it we do segmentation on that and then in",
    "start": "283270",
    "end": "289090"
  },
  {
    "text": "real-time we can send this data to our partners if you look at the center part",
    "start": "289090",
    "end": "295150"
  },
  {
    "text": "of it they there we have we support a lot of data signs and batch workloads so",
    "start": "295150",
    "end": "300190"
  },
  {
    "text": "a very common problem in in online media is let's say for a brand you have a set",
    "start": "300190",
    "end": "305590"
  },
  {
    "text": "of converters and you have that segment you know these are the users users you have converted now what brands are",
    "start": "305590",
    "end": "311320"
  },
  {
    "text": "looking for is can you find the users that are similar to that so how do you do that so we have we have built a",
    "start": "311320",
    "end": "317050"
  },
  {
    "text": "machine learning model around look-alikes so all those workloads we use EMR for that too we use spark as well as",
    "start": "317050",
    "end": "324039"
  },
  {
    "text": "MapReduce to process the data we look at the raw data sets and we figure out who are the users who have similar",
    "start": "324039",
    "end": "329979"
  },
  {
    "text": "characteristics and we produce look-alikes for them and finally if you think about on-demand segmentation our",
    "start": "329979",
    "end": "336610"
  },
  {
    "text": "users are going in all the data that is being collected they are building segments with it they are building rule-based segments they're",
    "start": "336610",
    "end": "342460"
  },
  {
    "text": "also they are relying on our machine learning techniques to come up with Einstein segmentation so that's what is",
    "start": "342460",
    "end": "348400"
  },
  {
    "text": "being used to understand who their audiences are and finally after this processing we allow all these insights",
    "start": "348400",
    "end": "355659"
  },
  {
    "text": "to be available to our customers we allow all this data to be back on the page or back on the app so that they can",
    "start": "355659",
    "end": "362349"
  },
  {
    "text": "do targeting for those users and then we also use you know you have external API",
    "start": "362349",
    "end": "368050"
  },
  {
    "text": "as we are connected with hundreds of partners where all this data can be sent so that our customers can find these",
    "start": "368050",
    "end": "373960"
  },
  {
    "text": "users elsewhere now I will go a little",
    "start": "373960",
    "end": "379780"
  },
  {
    "start": "377000",
    "end": "431000"
  },
  {
    "text": "bit deep into the on image segmentation and to do evoke the the standard use case if you see on the on your on the",
    "start": "379780",
    "end": "386830"
  },
  {
    "text": "left side this is a this is the Salesforce tower a picture of it which is not personalized anyone goes in this",
    "start": "386830",
    "end": "392320"
  },
  {
    "text": "see the picture of the sales for star Sirius DMP allows you to take that information and the user",
    "start": "392320",
    "end": "398890"
  },
  {
    "text": "information and it gives you personalized experience so on the right side you will see it's a very",
    "start": "398890",
    "end": "404020"
  },
  {
    "text": "personalized experience for you this is what brands want to do they want to know what are my users interested in",
    "start": "404020",
    "end": "410650"
  },
  {
    "text": "what are they looking for and then they give you an experience that converts you into customers and along with that what",
    "start": "410650",
    "end": "417820"
  },
  {
    "text": "we do is we give you a lot of information we we allow you to create rule-based segments we allow you to",
    "start": "417820",
    "end": "422830"
  },
  {
    "text": "create Einstein segmentation and a combination of that along with data is what is used to help our customers going",
    "start": "422830",
    "end": "431320"
  },
  {
    "text": "a little bit deeper into it all the data that is collected it goes into s3 all",
    "start": "431320",
    "end": "438130"
  },
  {
    "text": "the rules we have the DMP console where you go in you create your rules it all",
    "start": "438130",
    "end": "443920"
  },
  {
    "text": "goes into RDS and then we use EMR to connect the data and the rules that have",
    "start": "443920",
    "end": "449830"
  },
  {
    "text": "been created now an interesting story is where we were really small we actually started with hive because that seemed to",
    "start": "449830",
    "end": "455470"
  },
  {
    "text": "be the most obvious way to do it which is you know we have this data we this was all unstructured data we took this",
    "start": "455470",
    "end": "461710"
  },
  {
    "text": "data and we we converted into a semi structured data and then we wrote high queries on that the interesting thing",
    "start": "461710",
    "end": "468460"
  },
  {
    "text": "was to scale that was really hard because our clients started you know building thousands and thousands of",
    "start": "468460",
    "end": "474640"
  },
  {
    "text": "segments and you can't really run the high price on the thousand segments because you are doing multiple parts of",
    "start": "474640",
    "end": "479830"
  },
  {
    "text": "the same data and for us data you want minimum passes on the data because the",
    "start": "479830",
    "end": "484840"
  },
  {
    "text": "data size are really really large so we actually converted to use MapReduce and SPARC to look at the data look at the",
    "start": "484840",
    "end": "490630"
  },
  {
    "text": "rules and then make sure we do one pass of the data and the rules and then assign users to these different rules",
    "start": "490630",
    "end": "497350"
  },
  {
    "text": "and different segments and on top of that what we do is we build Einstein segmentation on that because we do we",
    "start": "497350",
    "end": "504520"
  },
  {
    "text": "look at your data again we figure out what are the patterns in the data all that information comes in into the DMP",
    "start": "504520",
    "end": "510220"
  },
  {
    "text": "and we create segments using that once you've created segments we'll give you insights into it will give you",
    "start": "510220",
    "end": "515830"
  },
  {
    "text": "demographic information will give you more details about the segments but the most important thing that we do is we",
    "start": "515830",
    "end": "522510"
  },
  {
    "text": "convert it to a user level segmentation what that means is the segments that I",
    "start": "522510",
    "end": "527560"
  },
  {
    "text": "belong to is very different than the segment that Sid will belong to or the segmented British will belong to and it is giving once you have these",
    "start": "527560",
    "end": "534639"
  },
  {
    "text": "segments it gives a very personalized experience for the end user so this is",
    "start": "534639",
    "end": "541629"
  },
  {
    "start": "539000",
    "end": "596000"
  },
  {
    "text": "we are think of our scale you know we are we allow our customers to create",
    "start": "541629",
    "end": "547660"
  },
  {
    "text": "clusters just by going in the console and creating segments so a scale is really large",
    "start": "547660",
    "end": "553060"
  },
  {
    "text": "we have about you know three thousand Americans just running you know you know it's a mix of MapReduce and spark and V",
    "start": "553060",
    "end": "560230"
  },
  {
    "text": "you spot instances a lot I think that has been a big savior for us you know being a start-up at one time we were you",
    "start": "560230",
    "end": "567550"
  },
  {
    "text": "know cost was very very important to us and as our clients were using more and more of our product and as we got",
    "start": "567550",
    "end": "572769"
  },
  {
    "text": "clients all across the world spot instances is something that came to our you know it saved us and we have been",
    "start": "572769",
    "end": "578860"
  },
  {
    "text": "using spot instances since if you guys haven't used spot instances I would highly recommend it you do have to",
    "start": "578860",
    "end": "585610"
  },
  {
    "text": "architect your system in such a way that these spot instances can be taken away so you need to make sure you can have",
    "start": "585610",
    "end": "592269"
  },
  {
    "text": "retry there are checkpoints and all that is available for you the other thing",
    "start": "592269",
    "end": "597370"
  },
  {
    "text": "about spot instances is earlier we used to use instance groups but there the challenge was you have to find write a Z",
    "start": "597370",
    "end": "604089"
  },
  {
    "text": "you have to find the right place to get the minimum price for the right instance type and all that so it was a little bit",
    "start": "604089",
    "end": "609579"
  },
  {
    "text": "cumbersome so we are actually very excited to use the instance fleets that we are using now where we give AWS a set",
    "start": "609579",
    "end": "616930"
  },
  {
    "text": "of options that we want and a SS helps us in finding the right mix of these",
    "start": "616930",
    "end": "623079"
  },
  {
    "text": "options to be used to create a cluster and to find I think spotter advisor is",
    "start": "623079",
    "end": "631089"
  },
  {
    "start": "627000",
    "end": "648000"
  },
  {
    "text": "also another very useful tool that we have which allows us to understand what is the probability of some of these",
    "start": "631089",
    "end": "637779"
  },
  {
    "text": "instances either going away or you know they will be taken away and what which has low probability so you can build your clusters or you can build the",
    "start": "637779",
    "end": "644589"
  },
  {
    "text": "instance fleet in the right way so with that I'll pass it on to Sid to talk",
    "start": "644589",
    "end": "650800"
  },
  {
    "start": "648000",
    "end": "668000"
  },
  {
    "text": "about the event-driven you know the architecture that here thank you for sharing what we do how we do and the",
    "start": "650800",
    "end": "657430"
  },
  {
    "text": "scale at which we run our workloads I will now walk you to a simple example of how we effectively you",
    "start": "657430",
    "end": "663520"
  },
  {
    "text": "instance fleet using even driven service architecture a job that triggered either",
    "start": "663520",
    "end": "669830"
  },
  {
    "start": "668000",
    "end": "799000"
  },
  {
    "text": "on-demand or periodically using Amazon CloudWatch Quran rules both these triggers are put a message to a",
    "start": "669830",
    "end": "676310"
  },
  {
    "text": "sqs q SQ SQ tries to decouple the producer from the consumer this sq sq is",
    "start": "676310",
    "end": "684770"
  },
  {
    "text": "is such a subscribed by Allah but by a lambda function lambda function encapsulate the business logic of first",
    "start": "684770",
    "end": "693110"
  },
  {
    "text": "passing on a job name and and fetching the job definition from the metadata repository as part of the job definition",
    "start": "693110",
    "end": "700310"
  },
  {
    "text": "- metadata a developers must have defined the minimum processing or you know though a minimum processing",
    "start": "700310",
    "end": "707420"
  },
  {
    "text": "requirement for a job looking at the processing requirement it gets back an",
    "start": "707420",
    "end": "712580"
  },
  {
    "text": "instance fleet mapping from DynamoDB table it then it then uses this instance",
    "start": "712580",
    "end": "719420"
  },
  {
    "text": "feed mapping looks at other metadata like EBS volumes and then creates a MapReduce cluster",
    "start": "719420",
    "end": "728290"
  },
  {
    "text": "this cluster uses s3 as the input and output data source if for some reason a",
    "start": "728290",
    "end": "737240"
  },
  {
    "text": "gust of fields we have a cloud watch alarm which will send out a message to",
    "start": "737240",
    "end": "743540"
  },
  {
    "text": "an SNS topic SNS topic your acts as the pub/sub to a broadcast failure events to",
    "start": "743540",
    "end": "751100"
  },
  {
    "text": "multiple subscribers one of those subscriber is an ACS email which is sent",
    "start": "751100",
    "end": "756770"
  },
  {
    "text": "out to devs another another important subscriber is a failure retry handle",
    "start": "756770",
    "end": "763640"
  },
  {
    "text": "which is backed by a lambda function again which will retry the job as as",
    "start": "763640",
    "end": "771710"
  },
  {
    "text": "part of the pups of mechanism you can add more subscribers you can you know you can look at the failure events a",
    "start": "771710",
    "end": "777320"
  },
  {
    "text": "bailout of payload and and send out an actionable alert to your instant",
    "start": "777320",
    "end": "782600"
  },
  {
    "text": "messaging tool and then we also have a",
    "start": "782600",
    "end": "787790"
  },
  {
    "text": "service which constantly looks at all the spot advisor tips and updates rate",
    "start": "787790",
    "end": "793430"
  },
  {
    "text": "and updates rating since fleet mapping in table with this let me share some",
    "start": "793430",
    "end": "802180"
  },
  {
    "start": "799000",
    "end": "924000"
  },
  {
    "text": "learnings from experience running big data processing at scale the most",
    "start": "802180",
    "end": "809140"
  },
  {
    "text": "important requirement to use spot instances or to run a femoral EMR clusters is to separate out compute and",
    "start": "809140",
    "end": "816250"
  },
  {
    "text": "storage if you separate out compute from storage multiple clusters can access the",
    "start": "816250",
    "end": "821470"
  },
  {
    "text": "s3 data at the same time with this you can also spin up and take teardown EMR",
    "start": "821470",
    "end": "827080"
  },
  {
    "text": "clusters without moving your data set this gives birth to a pattern called as",
    "start": "827080",
    "end": "832210"
  },
  {
    "text": "a job scope stateless workloads the",
    "start": "832210",
    "end": "838360"
  },
  {
    "text": "second important requirement is to not be dependent on a particular ec2 instance type a developer should test",
    "start": "838360",
    "end": "844870"
  },
  {
    "text": "benchmark and define the processing requirements in a generic terms and then the and then the framework should take",
    "start": "844870",
    "end": "851620"
  },
  {
    "text": "care of building the instance fleet mapping this is something controversial",
    "start": "851620",
    "end": "860140"
  },
  {
    "text": "but from her do point of view what we have observe in our testing lab is doing",
    "start": "860140",
    "end": "865450"
  },
  {
    "text": "more with less helps in helps in reducing the network chattiness and the",
    "start": "865450",
    "end": "870580"
  },
  {
    "text": "interruption likelihood master node is",
    "start": "870580",
    "end": "877360"
  },
  {
    "text": "just the orchestrator and performs no computation of its own hence you can run master node on cheap on-demand instance",
    "start": "877360",
    "end": "883750"
  },
  {
    "text": "types to a say one cost if your job",
    "start": "883750",
    "end": "890020"
  },
  {
    "text": "needs more storage for intermediate processing it is advisable to use a",
    "start": "890020",
    "end": "896860"
  },
  {
    "text": "cheaper GP to EBS volumes over expensive instance types which comes with more instance tools last but not least in",
    "start": "896860",
    "end": "908440"
  },
  {
    "text": "order to effectively use spot instances it is important that the system is built",
    "start": "908440",
    "end": "913510"
  },
  {
    "text": "a ground up keeping fault tolerance in mind your jobs have to be idempotent",
    "start": "913510",
    "end": "918820"
  },
  {
    "text": "they should have checkpoints and they should be self-healing",
    "start": "918820",
    "end": "923460"
  },
  {
    "start": "924000",
    "end": "1040000"
  },
  {
    "text": "moving on to storage learnings or you should use s3 as your data leak it with",
    "start": "924350",
    "end": "930900"
  },
  {
    "text": "s3 you get infinity elasticity you don't have to do any upfront capacity planning",
    "start": "930900",
    "end": "937380"
  },
  {
    "text": "and you only pay for what you use and if you use s3 as your data link you should",
    "start": "937380",
    "end": "942900"
  },
  {
    "text": "ensure that you store data in different log formats a formats like Avro and Parque help you take take advantage of",
    "start": "942900",
    "end": "950940"
  },
  {
    "text": "data locality for a reduced storage and",
    "start": "950940",
    "end": "957180"
  },
  {
    "text": "faster access to data you should also compress data in s3 buckets and you know",
    "start": "957180",
    "end": "963480"
  },
  {
    "text": "there are there are there are few spirit able algorithms like l0 snappy which go",
    "start": "963480",
    "end": "968790"
  },
  {
    "text": "well with Hadoop and they help you run your jobs faster for better coordinate",
    "start": "968790",
    "end": "977220"
  },
  {
    "text": "security and compliance we store data in buckets across multiple accounts and",
    "start": "977220",
    "end": "982670"
  },
  {
    "text": "using I am assume rolls data from different buckets living in different",
    "start": "982670",
    "end": "989610"
  },
  {
    "text": "accounts can be accessed in the same job for massive cost savings you should look",
    "start": "989610",
    "end": "999480"
  },
  {
    "text": "at applying expiration and a transition policies to your s3 buckets we have",
    "start": "999480",
    "end": "1005000"
  },
  {
    "text": "noticed around 20% reductions in cost after we started applying these policies",
    "start": "1005000",
    "end": "1011020"
  },
  {
    "text": "here are a few optimization tips which us which are specific to here Mar FS which is the s3 implementation of HDFS",
    "start": "1012550",
    "end": "1020140"
  },
  {
    "text": "in optimization tips like reducing the you know which is like the bill which is",
    "start": "1020140",
    "end": "1026030"
  },
  {
    "text": "like reducing the replication factor increasing the disk utilization increasing the file mac split size will",
    "start": "1026030",
    "end": "1032540"
  },
  {
    "text": "help you will help you squeeze out more juice out of your cluster moving on to",
    "start": "1032540",
    "end": "1041120"
  },
  {
    "start": "1040000",
    "end": "1192000"
  },
  {
    "text": "monitoring and alerting we use tags heavily on all amazon resources right so",
    "start": "1041120",
    "end": "1048800"
  },
  {
    "text": "we have a fixed set of taxonomy that we use on each resource that we use in Amazon Web service this helps you to",
    "start": "1048800",
    "end": "1055730"
  },
  {
    "text": "associate and unify accost failures for a team feature",
    "start": "1055730",
    "end": "1061340"
  },
  {
    "text": "or a given cluster at this high scale a",
    "start": "1061340",
    "end": "1069769"
  },
  {
    "text": "few bad jobs or fewer latina queries can cause a spike in your bills right so to",
    "start": "1069769",
    "end": "1076039"
  },
  {
    "text": "mitigate we have set cloud watch alarms which look at the threshold for a given",
    "start": "1076039",
    "end": "1083090"
  },
  {
    "text": "service and if there is a sudden spike in it we we get notified Amy you know we",
    "start": "1083090",
    "end": "1088609"
  },
  {
    "text": "get notified immediately that way you can keep a control on your cost it is",
    "start": "1088609",
    "end": "1096889"
  },
  {
    "text": "very important to observe a filter and notify failures but the most important",
    "start": "1096889",
    "end": "1103639"
  },
  {
    "text": "thing is to build actionable alerts right so you need alerts which on which you which which you can act immediately",
    "start": "1103639",
    "end": "1109519"
  },
  {
    "text": "right ascending alerts via email is an old-school way of doing it right you",
    "start": "1109519",
    "end": "1115220"
  },
  {
    "text": "should send alerts via your instant messaging tool wherein you can act immediately right so of so we have",
    "start": "1115220",
    "end": "1120470"
  },
  {
    "text": "integrated note our failure notification with an instant tool where in as soon as",
    "start": "1120470",
    "end": "1126230"
  },
  {
    "text": "a tester fails and an alert gets sent to the right team and that person can",
    "start": "1126230",
    "end": "1131989"
  },
  {
    "text": "immediately retry that job from that tool itself this is something that",
    "start": "1131989",
    "end": "1138889"
  },
  {
    "text": "Amazon Web service does not provide natively and in order to help our support team with job investigation and",
    "start": "1138889",
    "end": "1144619"
  },
  {
    "text": "deep and I noticed so what we did was we we have built a own custom solution of ingesting all the MapReduce cluster jobs",
    "start": "1144619",
    "end": "1152470"
  },
  {
    "text": "you know from one s3 bucket into n another s3 bucket the way we do it is we",
    "start": "1152470",
    "end": "1158239"
  },
  {
    "text": "have s3 even notifications on the source bucket so as soon as a log gets copied to the source bucket and s3 even",
    "start": "1158239",
    "end": "1164690"
  },
  {
    "text": "notification gets fired there is a lambda function which is which is which is looking at that s3 even notification",
    "start": "1164690",
    "end": "1170749"
  },
  {
    "text": "it looks at that event there are some rags file pattern you know wherein we add raw files which we don't need and we",
    "start": "1170749",
    "end": "1177139"
  },
  {
    "text": "then ingest it to our log aggregation tool you can use your own custom log",
    "start": "1177139",
    "end": "1183230"
  },
  {
    "text": "aggregation tool you know based on elk stack or you can use any third party aggregation tool",
    "start": "1183230",
    "end": "1190210"
  },
  {
    "start": "1192000",
    "end": "1302000"
  },
  {
    "text": "over that I thank you for listening to a talk and I would now like to call you no",
    "start": "1193580",
    "end": "1198840"
  },
  {
    "text": "meditations day good afternoon everybody",
    "start": "1198840",
    "end": "1210840"
  },
  {
    "text": "first let's start with an introduction Rita Shaw I'm a program manager are we",
    "start": "1210840",
    "end": "1226769"
  },
  {
    "text": "good okay we are good okay I'm Rita Shaw of Senior Program Manager",
    "start": "1226769",
    "end": "1232099"
  },
  {
    "text": "for chief technology officer at why not I rather came that engineers all these",
    "start": "1232099",
    "end": "1239580"
  },
  {
    "text": "services specifically in the analytics AI machine learning space for everybody",
    "start": "1239580",
    "end": "1246720"
  },
  {
    "text": "at Vanguard to use so that everybody does not have to go through the same gyration over and over again so my team",
    "start": "1246720",
    "end": "1254279"
  },
  {
    "text": "is like an engineering team cutting across all IT and business units before",
    "start": "1254279",
    "end": "1260639"
  },
  {
    "text": "we go further to questions how many of you know Vanguard or have dealt with",
    "start": "1260639",
    "end": "1266759"
  },
  {
    "text": "Vanguard quick show of hands looks like",
    "start": "1266759",
    "end": "1272220"
  },
  {
    "text": "50% second question how many of you have",
    "start": "1272220",
    "end": "1277229"
  },
  {
    "text": "started your analytics in AWS journey or are new and about to start or how many",
    "start": "1277229",
    "end": "1284940"
  },
  {
    "text": "of you are already there to some level",
    "start": "1284940",
    "end": "1289460"
  },
  {
    "text": "looks like 40% are new looks like those who raised hands were new",
    "start": "1291080",
    "end": "1296789"
  },
  {
    "text": "I'm guessing and remaining may be already there so let's start let's start",
    "start": "1296789",
    "end": "1304889"
  },
  {
    "start": "1302000",
    "end": "1333000"
  },
  {
    "text": "with first Vanguard right background often got we are one of the largest investment management companies in u.s.",
    "start": "1304889",
    "end": "1313220"
  },
  {
    "text": "headquartered in Malvern Philadelphia started operations in 1975 and we have",
    "start": "1313220",
    "end": "1320700"
  },
  {
    "text": "multiple lines of buzz we do institutional business retail",
    "start": "1320700",
    "end": "1326820"
  },
  {
    "text": "advisory business international and others agenda for today we want to start",
    "start": "1326820",
    "end": "1337270"
  },
  {
    "start": "1333000",
    "end": "1575000"
  },
  {
    "text": "with what I call State of the Union from a perspective of Vanguard right where",
    "start": "1337270",
    "end": "1343210"
  },
  {
    "text": "were we in 2017 where are we in 2018 is why I call it State of the Union what",
    "start": "1343210",
    "end": "1350740"
  },
  {
    "text": "did we learn over those two years some metrics for 2017 2018 and those metrics",
    "start": "1350740",
    "end": "1360430"
  },
  {
    "text": "lead to why are we changing some of our directions as a reason of change and",
    "start": "1360430",
    "end": "1366420"
  },
  {
    "text": "then what are we looking at 2018 2019 when I say 2018 it's like last month of",
    "start": "1366420",
    "end": "1373960"
  },
  {
    "text": "2018 going into 2019 so that's the high level idea that I'm gonna walk everybody",
    "start": "1373960",
    "end": "1380170"
  },
  {
    "text": "through State of Union I like to present this in a way where we",
    "start": "1380170",
    "end": "1387550"
  },
  {
    "text": "can start at 50,000 feet level and start driving deeper into how data leak and",
    "start": "1387550",
    "end": "1395560"
  },
  {
    "text": "data platforms work within Vanguard so first a very high level of data leak",
    "start": "1395560",
    "end": "1403020"
  },
  {
    "text": "like you can see typically most companies bring data from on their on",
    "start": "1403020",
    "end": "1409330"
  },
  {
    "text": "Prem systems or third-party vendors that they work with into AWS and we started",
    "start": "1409330",
    "end": "1416380"
  },
  {
    "text": "with our data Lake being in s3 so s3 is our data Lake all the data comes there",
    "start": "1416380",
    "end": "1422430"
  },
  {
    "text": "but what we do is we bring data and organize it in s3 based on data domains",
    "start": "1422430",
    "end": "1430200"
  },
  {
    "text": "that own that data so that ongoing maintenance of that data is easier and",
    "start": "1430200",
    "end": "1436410"
  },
  {
    "text": "then we have business streams that take that data and derive it for their",
    "start": "1436410",
    "end": "1443050"
  },
  {
    "text": "analytics purposes transform it into however format they want and",
    "start": "1443050",
    "end": "1449800"
  },
  {
    "text": "at the end they then have their EMR clusters pointing to this lake and do",
    "start": "1449800",
    "end": "1458920"
  },
  {
    "text": "their analytics right so at a very high level that's the theme the EMR clusters",
    "start": "1458920",
    "end": "1464520"
  },
  {
    "text": "used to bring data transform the data or for running analytics we run all of them",
    "start": "1464520",
    "end": "1471850"
  },
  {
    "text": "as ephemeral clusters so they come up do their work and they go away right so",
    "start": "1471850",
    "end": "1478630"
  },
  {
    "text": "they typically don't run 24 by 7 they run for a finite period of time that is",
    "start": "1478630",
    "end": "1485590"
  },
  {
    "text": "configured by the cluster owners and IT teams so now let's go a little bit",
    "start": "1485590",
    "end": "1493870"
  },
  {
    "text": "deeper right and this set of slides focus on internals of the data leak",
    "start": "1493870",
    "end": "1501070"
  },
  {
    "text": "right so once we bring data into data like s3 what we do is we put it into a",
    "start": "1501070",
    "end": "1508150"
  },
  {
    "text": "layer what we call is a rolly or data is typically brought in as is from the",
    "start": "1508150",
    "end": "1513610"
  },
  {
    "text": "source system typically never changed it sits there in the raw format then we",
    "start": "1513610",
    "end": "1521200"
  },
  {
    "text": "have set of clusters that spin up clean the data put it in a clean data area",
    "start": "1521200",
    "end": "1528910"
  },
  {
    "text": "maybe it's a temporary area for some time but there are clusters that clean",
    "start": "1528910",
    "end": "1534160"
  },
  {
    "text": "the data and then either the same set of clusters or a new set of clusters take",
    "start": "1534160",
    "end": "1540010"
  },
  {
    "text": "data from the clean bucket and put it into a set of buckets we call ready for",
    "start": "1540010",
    "end": "1545440"
  },
  {
    "text": "analytics this this is where most teams would be running their analytics from",
    "start": "1545440",
    "end": "1552570"
  },
  {
    "text": "from their EMR clusters or any other future services we have also seen data",
    "start": "1552570",
    "end": "1559210"
  },
  {
    "text": "scientists who don't want to go to that ready for analytics data but go to the raw data we do allow data scientists to",
    "start": "1559210",
    "end": "1567100"
  },
  {
    "text": "go there but typically most of the folks are using the ready for analytics data",
    "start": "1567100",
    "end": "1572440"
  },
  {
    "text": "at times now we dive deeper further into",
    "start": "1572440",
    "end": "1578950"
  },
  {
    "start": "1575000",
    "end": "1975000"
  },
  {
    "text": "how do we bring data in to the lake right into different",
    "start": "1578950",
    "end": "1584120"
  },
  {
    "text": "different areas or s3 buckets that I presented so first first thing what it",
    "start": "1584120",
    "end": "1591600"
  },
  {
    "text": "shows is all the code is deployed through an automated bamboo build",
    "start": "1591600",
    "end": "1597480"
  },
  {
    "text": "process into AWS in an s3 bucket it has",
    "start": "1597480",
    "end": "1603330"
  },
  {
    "text": "all the cloud formation templates JSON yeah Mel and symbol configurations all",
    "start": "1603330",
    "end": "1608640"
  },
  {
    "text": "of that is placed in a specific region in a specific deployment bucket and then",
    "start": "1608640",
    "end": "1615750"
  },
  {
    "text": "we have control M calling a load",
    "start": "1615750",
    "end": "1621270"
  },
  {
    "text": "balancer and we have an job nor that we utilize to spin up either an ec2",
    "start": "1621270",
    "end": "1628200"
  },
  {
    "text": "instance or any other artifact that we need to spin up moving on from there",
    "start": "1628200",
    "end": "1636960"
  },
  {
    "text": "this process shows if there's a vendor file we want to bring that into AWS we",
    "start": "1636960",
    "end": "1644580"
  },
  {
    "text": "use this ec2 node we'll have the DTS team that manages vendor feeds push the",
    "start": "1644580",
    "end": "1652200"
  },
  {
    "text": "data into AWS into s3 buckets that we have one key important point for",
    "start": "1652200",
    "end": "1659309"
  },
  {
    "text": "Vanguard all s3 buckets are encrypted and all data in transit is encrypted and",
    "start": "1659309",
    "end": "1665330"
  },
  {
    "text": "when we encrypt data in in s3 we use our own managed keys AWS calls it as kms cmk",
    "start": "1665330",
    "end": "1674250"
  },
  {
    "text": "so winegar manages their own keys etc for the bucket the another pattern to",
    "start": "1674250",
    "end": "1682830"
  },
  {
    "text": "bring in data is we may have data sitting on on-premise servers in form of",
    "start": "1682830",
    "end": "1690179"
  },
  {
    "text": "files and we FTP it the way that process works says control and spins up an",
    "start": "1690179",
    "end": "1696630"
  },
  {
    "text": "ephemeral ec2 instance and then that ec2 instance pulls data from the on-prem",
    "start": "1696630",
    "end": "1702779"
  },
  {
    "text": "servers and puts it on to s3 buckets the",
    "start": "1702779",
    "end": "1709649"
  },
  {
    "text": "next pattern is typical data stores right you have lot of",
    "start": "1709649",
    "end": "1715400"
  },
  {
    "text": "relational databases out there we spin up an ephemeral EMR cluster you scoop to",
    "start": "1715400",
    "end": "1723870"
  },
  {
    "text": "bring data in from these databases and store the data in s3 and we build a hive",
    "start": "1723870",
    "end": "1732290"
  },
  {
    "text": "meta store or hive tables on top of the data that is brought in also we use data",
    "start": "1732290",
    "end": "1741360"
  },
  {
    "text": "replication tools like at unity and in future we are looking at DMS as an way",
    "start": "1741360",
    "end": "1747540"
  },
  {
    "text": "to argument and we replicate that data into a RDS Postgres database and from",
    "start": "1747540",
    "end": "1756690"
  },
  {
    "text": "there EMR clusters sorry I clicked quickly EMR",
    "start": "1756690",
    "end": "1762420"
  },
  {
    "text": "clusters pull that data from pause grass scoop it out and then put it back into",
    "start": "1762420",
    "end": "1768300"
  },
  {
    "text": "s3 so this gives like a high broad overview of how data pipelines and",
    "start": "1768300",
    "end": "1776870"
  },
  {
    "text": "transformations work at one our next set of slides are gonna be towards how do we",
    "start": "1776870",
    "end": "1785760"
  },
  {
    "text": "do analytics at Vanguard once the data is in the s3 bucket wherever we need it how do we do it",
    "start": "1785760",
    "end": "1792890"
  },
  {
    "text": "the first section is anybody who needs a cluster for doing any analytics their",
    "start": "1792890",
    "end": "1800250"
  },
  {
    "text": "code their cluster is deployed through an automated process into an s3 bucket",
    "start": "1800250",
    "end": "1806670"
  },
  {
    "text": "all their cloud formation etcetera next we use similarly we use control an to",
    "start": "1806670",
    "end": "1814830"
  },
  {
    "text": "spin up the cluster because we go we have an ephemeral nature of all our",
    "start": "1814830",
    "end": "1820620"
  },
  {
    "text": "clusters we have control I'm spinning it up and then once it spins up it connects",
    "start": "1820620",
    "end": "1827280"
  },
  {
    "text": "to appropriate s3 buckets we have a centralized hive meta store to provide",
    "start": "1827280",
    "end": "1833160"
  },
  {
    "text": "all the metadata about tables and data out there is how we set it up for doing",
    "start": "1833160",
    "end": "1840570"
  },
  {
    "text": "any analytics right of the cluster we also enable you as a browser-based",
    "start": "1840570",
    "end": "1846420"
  },
  {
    "text": "interface with a hive presto as capabilities to",
    "start": "1846420",
    "end": "1852500"
  },
  {
    "text": "perform analytics next we also support",
    "start": "1852500",
    "end": "1857900"
  },
  {
    "text": "at Vanguard sorry yeah I went through",
    "start": "1857900",
    "end": "1864440"
  },
  {
    "text": "this the next piece is what we support is we support other visualization tools",
    "start": "1864440",
    "end": "1870710"
  },
  {
    "text": "that don't natively reside within the EMR cluster so we enable tableau that",
    "start": "1870710",
    "end": "1877549"
  },
  {
    "text": "would connect onto EMR cluster using presto and and then the data scientist",
    "start": "1877549",
    "end": "1885740"
  },
  {
    "text": "and or analysts can build their visualization on it and then publish it",
    "start": "1885740",
    "end": "1891919"
  },
  {
    "text": "to the tableau server environment for consumption going deeper into how is an",
    "start": "1891919",
    "end": "1902660"
  },
  {
    "text": "EMR cluster belt at Vanguard so we have like I said my team engineers or",
    "start": "1902660",
    "end": "1910270"
  },
  {
    "text": "reusable artifacts for everybody to spin up their cluster so my team builds cloud",
    "start": "1910270",
    "end": "1918080"
  },
  {
    "text": "formation templates ansible playbooks we maintain our lambda functions JSON",
    "start": "1918080",
    "end": "1925309"
  },
  {
    "text": "parameters etc and we store it in a sandbox bitbucket repository and then we",
    "start": "1925309",
    "end": "1934730"
  },
  {
    "text": "push this sandbox bitbucket repository into AWS into s3 the highlight there is",
    "start": "1934730",
    "end": "1944720"
  },
  {
    "text": "we we just build it test it put it out there that doesn't mean that everybody",
    "start": "1944720",
    "end": "1950270"
  },
  {
    "text": "at Vanguard can start using it the next step of it is we have all the IT teams",
    "start": "1950270",
    "end": "1957100"
  },
  {
    "text": "replicate what we have done into their own big bucket repository and use their",
    "start": "1957100",
    "end": "1963710"
  },
  {
    "text": "bamboo build processes to deploy they are clusters in their accounts on their",
    "start": "1963710",
    "end": "1972020"
  },
  {
    "text": "timelines lessons",
    "start": "1972020",
    "end": "1977790"
  },
  {
    "start": "1975000",
    "end": "2472000"
  },
  {
    "text": "before we go into lessons learned right we had four major what I said",
    "start": "1977790",
    "end": "1984340"
  },
  {
    "text": "non-functional requirements first one was authentication right all services",
    "start": "1984340",
    "end": "1991060"
  },
  {
    "text": "within EMR or any other AWS services has to integrate with Active Directory or",
    "start": "1991060",
    "end": "1997260"
  },
  {
    "text": "radiant logic or was a fundamental requirement for authorization",
    "start": "1997260",
    "end": "2005870"
  },
  {
    "text": "perspective we have requirements where we do not allow anybody to manually",
    "start": "2005870",
    "end": "2012120"
  },
  {
    "text": "provide authorizations through AWS console or any other means it has to be",
    "start": "2012120",
    "end": "2018960"
  },
  {
    "text": "fully automated scriptable and the playable asset it cannot be user",
    "start": "2018960",
    "end": "2025950"
  },
  {
    "text": "clicking through the screens and providing access and it goes into using",
    "start": "2025950",
    "end": "2033330"
  },
  {
    "text": "I am as the identity management mechanism and the last piece under",
    "start": "2033330",
    "end": "2039210"
  },
  {
    "text": "authorization was we we have a model of least privileged access what it means is",
    "start": "2039210",
    "end": "2046580"
  },
  {
    "text": "anybody who needs access to data you give them access to only data that they",
    "start": "2046580",
    "end": "2052080"
  },
  {
    "text": "really need and not to everything out there which puts a certain kind of",
    "start": "2052080",
    "end": "2058409"
  },
  {
    "text": "restrictions on how you enable these services at AWS third pillar which is",
    "start": "2058410",
    "end": "2065730"
  },
  {
    "text": "very important also is auditing we always want to know what access can be",
    "start": "2065730",
    "end": "2071940"
  },
  {
    "text": "provided or provision what access is provision and what access that was",
    "start": "2071940",
    "end": "2079740"
  },
  {
    "text": "provision is being used by anybody so those three pillars within auditing are",
    "start": "2079740",
    "end": "2086100"
  },
  {
    "text": "very important for anda to have a clear line of sight on who is the user",
    "start": "2086100",
    "end": "2091520"
  },
  {
    "text": "accessing what data and running what queries are running what jobs against",
    "start": "2091520",
    "end": "2097650"
  },
  {
    "text": "what bina at all point in time last one I already mentioned it we have we are",
    "start": "2097650",
    "end": "2106920"
  },
  {
    "text": "using our Brandon management management caption keys encryption at rest is a",
    "start": "2106920",
    "end": "2113540"
  },
  {
    "text": "mandatory requirement and encryption in flight for everything is mandatory",
    "start": "2113540",
    "end": "2119390"
  },
  {
    "text": "requirement so those 4 non-functional",
    "start": "2119390",
    "end": "2126130"
  },
  {
    "text": "requirements are some of the reasons why we experience some challenges and I",
    "start": "2126130",
    "end": "2132140"
  },
  {
    "text": "convert challenges as lessons learned typically and I've organized it into",
    "start": "2132140",
    "end": "2137960"
  },
  {
    "text": "three areas but there are other challenges we can talk offline first one",
    "start": "2137960",
    "end": "2143330"
  },
  {
    "text": "is high rent presto in our two-year journey we figured out when we enabled",
    "start": "2143330",
    "end": "2151040"
  },
  {
    "text": "presto in 2017 it would do anonymous binds into Active Directory which is not",
    "start": "2151040",
    "end": "2156950"
  },
  {
    "text": "allowed at Wagner so we worked very closely with AWS and AW has provided",
    "start": "2156950",
    "end": "2162560"
  },
  {
    "text": "effects and they committed it back to the open-source community so that presto",
    "start": "2162560",
    "end": "2168530"
  },
  {
    "text": "does no longer does anonymous binds to Active Directory there were auditing",
    "start": "2168530",
    "end": "2173990"
  },
  {
    "text": "gaps based on the requirements that were not hard and AWS provided us a solution",
    "start": "2173990",
    "end": "2182450"
  },
  {
    "text": "and we partnered on that solution and it's now available in 512 version of EMR",
    "start": "2182450",
    "end": "2189410"
  },
  {
    "text": "for everybody to use and last authorizations is a challenge Vanguard",
    "start": "2189410",
    "end": "2198140"
  },
  {
    "text": "has strict requirements like I said only give access to data that you need to so",
    "start": "2198140",
    "end": "2203570"
  },
  {
    "text": "at times we need column level access and that's very significantly challenging at",
    "start": "2203570",
    "end": "2209960"
  },
  {
    "text": "Vanguard but also for authorization we have to automate everything so we wrote",
    "start": "2209960",
    "end": "2215630"
  },
  {
    "text": "our own custom lambda function and other mechanisms to automatically authorize",
    "start": "2215630",
    "end": "2222560"
  },
  {
    "text": "hive and presto queries through high sequel authorization next we enabled",
    "start": "2222560",
    "end": "2232400"
  },
  {
    "text": "Jupiter SPARC and we were one of the first few clients working with AWS on",
    "start": "2232400",
    "end": "2239900"
  },
  {
    "text": "enabling it what we have found as levy which is a core component of spark and",
    "start": "2239900",
    "end": "2247400"
  },
  {
    "text": "Jupiter integration has no authentication and does not support SSL",
    "start": "2247400",
    "end": "2252950"
  },
  {
    "text": "so we had to put core around it to make sure all communication between a",
    "start": "2252950",
    "end": "2259010"
  },
  {
    "text": "notebook and Livi is secured is what we had to do by default version 514 that we",
    "start": "2259010",
    "end": "2269150"
  },
  {
    "text": "are using right now and production does not integrate with a custom repo seamlessly the seamless part of it is if",
    "start": "2269150",
    "end": "2278870"
  },
  {
    "text": "you need a Python package distributed on all the nodes of the cluster that",
    "start": "2278870",
    "end": "2285080"
  },
  {
    "text": "capability is not available in 514 so we had to build our own custom mechanism to",
    "start": "2285080",
    "end": "2290720"
  },
  {
    "text": "take packages and distribute it on all nodes of the cluster I believe that is",
    "start": "2290720",
    "end": "2296870"
  },
  {
    "text": "resolved in 519 or a bow but that's something Abbey shake or be shaken",
    "start": "2296870",
    "end": "2302720"
  },
  {
    "text": "answer next one SPARC hive integration we have seen a",
    "start": "2302720",
    "end": "2309440"
  },
  {
    "text": "steady increase in demand of having to write SPARC jobs and reuse hive meta",
    "start": "2309440",
    "end": "2317420"
  },
  {
    "text": "store so that you don't have to worry about the structure of the data that integration is not enabled at Vanguard",
    "start": "2317420",
    "end": "2324800"
  },
  {
    "text": "because we have found gaps in that implementation where it bypasses sequel",
    "start": "2324800",
    "end": "2331010"
  },
  {
    "text": "authorization meaning a person using spark core could drop metadata about",
    "start": "2331010",
    "end": "2339170"
  },
  {
    "text": "that table from the high - store so we haven't enabled that we are partnering",
    "start": "2339170",
    "end": "2344690"
  },
  {
    "text": "with AWS to resolve that and move forward with it",
    "start": "2344690",
    "end": "2350020"
  },
  {
    "text": "auditing or shows up on this area - we had challenges in that space EMR we work",
    "start": "2350020",
    "end": "2357140"
  },
  {
    "text": "with EMR team to enable EMR FS level auditing so that we can know at every",
    "start": "2357140",
    "end": "2362870"
  },
  {
    "text": "point in time who the user is using spark or Python to access data and then",
    "start": "2362870",
    "end": "2369920"
  },
  {
    "text": "we enabled or maybe Linux container executors and AWS execution role or",
    "start": "2369920",
    "end": "2377119"
  },
  {
    "text": "environment for Python as we found a issue where the user information was",
    "start": "2377119",
    "end": "2383599"
  },
  {
    "text": "never being transmitted on to the Livi side so we have to enable certain",
    "start": "2383599",
    "end": "2389450"
  },
  {
    "text": "capabilities next one is you and Uzi we",
    "start": "2389450",
    "end": "2396440"
  },
  {
    "text": "have challenges in that space Hugh doesn't support file browse capability",
    "start": "2396440",
    "end": "2401930"
  },
  {
    "text": "for encrypted buckets you cannot upload file and put it in so we ended up",
    "start": "2401930",
    "end": "2408530"
  },
  {
    "text": "opening up console and allow our users to upload and download files through the",
    "start": "2408530",
    "end": "2415220"
  },
  {
    "text": "AWS console hive allows execution of",
    "start": "2415220",
    "end": "2421430"
  },
  {
    "text": "oozy actions as yon user which we did not like because it get it gives the",
    "start": "2421430",
    "end": "2427910"
  },
  {
    "text": "user lot more access than required which we had to fix",
    "start": "2427910",
    "end": "2435099"
  },
  {
    "text": "susie-wusie local actions can also be",
    "start": "2435099",
    "end": "2440390"
  },
  {
    "text": "bypassed which we had to fix for and then you see hive server 2 does not",
    "start": "2440390",
    "end": "2447380"
  },
  {
    "text": "support what we call service IDs we didn't want you see jobs written by data",
    "start": "2447380",
    "end": "2455150"
  },
  {
    "text": "scientists to have their own personal user IDs stored in it so we want them to use service IDs but whose in and",
    "start": "2455150",
    "end": "2461930"
  },
  {
    "text": "supported so we had to re-engineer b-line and customize it to work for our",
    "start": "2461930",
    "end": "2467930"
  },
  {
    "text": "requirements let's go to some metrics of",
    "start": "2467930",
    "end": "2474859"
  },
  {
    "start": "2472000",
    "end": "2566000"
  },
  {
    "text": "and reasons to change write current state in production we have 150 plus IT",
    "start": "2474859",
    "end": "2483950"
  },
  {
    "text": "engineer's data engineer scientists using EMR clusters today we have 250",
    "start": "2483950",
    "end": "2492079"
  },
  {
    "text": "plus clusters that get spun up and brought down across different",
    "start": "2492079",
    "end": "2497359"
  },
  {
    "text": "environments we have around 100 plus data stores or data domains brought n2s",
    "start": "2497359",
    "end": "2504339"
  },
  {
    "text": "3d and we have almost a petabyte of data enabled in Denali these are important",
    "start": "2504339",
    "end": "2514109"
  },
  {
    "text": "metrics because these are some of the driving forces of reason to change right",
    "start": "2514109",
    "end": "2520940"
  },
  {
    "text": "because of all this we have seen increased demands for analytics faster",
    "start": "2520940",
    "end": "2526800"
  },
  {
    "text": "enablement of compute has become a requirement operational resiliency as",
    "start": "2526800",
    "end": "2533150"
  },
  {
    "text": "more and more analytics gets done is becoming important and then last but not",
    "start": "2533150",
    "end": "2540240"
  },
  {
    "text": "least right most important is self management of compute we hear loud and",
    "start": "2540240",
    "end": "2546119"
  },
  {
    "text": "clear from our clients business and IT saying they want to manage the clusters",
    "start": "2546119",
    "end": "2551700"
  },
  {
    "text": "themselves turn it on turn it off change the size of the cluster etc without",
    "start": "2551700",
    "end": "2558150"
  },
  {
    "text": "having to go through some deployment process that could delay their cluster availability",
    "start": "2558150",
    "end": "2565790"
  },
  {
    "text": "that brings us to 2018-2019 where are we",
    "start": "2565790",
    "end": "2570869"
  },
  {
    "start": "2566000",
    "end": "2731000"
  },
  {
    "text": "going as a future state as part of that",
    "start": "2570869",
    "end": "2576720"
  },
  {
    "text": "effort enough for share how are we tackling self-service or management of",
    "start": "2576720",
    "end": "2583290"
  },
  {
    "text": "clusters this set of slides are towards usage of Service Catalog as a mechanism",
    "start": "2583290",
    "end": "2592260"
  },
  {
    "text": "to launch clusters going forward first",
    "start": "2592260",
    "end": "2598080"
  },
  {
    "text": "what happens is each line of business team in service catalog using a",
    "start": "2598080",
    "end": "2603900"
  },
  {
    "text": "controlled process built something called a portfolio within which you can",
    "start": "2603900",
    "end": "2610109"
  },
  {
    "text": "have your EMR clusters next is CTO team",
    "start": "2610109",
    "end": "2616220"
  },
  {
    "text": "provides a base set of templates and artifacts that get deployed all the way",
    "start": "2616220",
    "end": "2623460"
  },
  {
    "text": "to production for every client and that becomes the foundational configuration",
    "start": "2623460",
    "end": "2629609"
  },
  {
    "text": "of an EMR cluster after which si teams",
    "start": "2629609",
    "end": "2634800"
  },
  {
    "text": "can come in and customize it example they can change their LDAP",
    "start": "2634800",
    "end": "2641250"
  },
  {
    "text": "connection string or hive meta store connection strings if required they come",
    "start": "2641250",
    "end": "2646920"
  },
  {
    "text": "in and customize the cluster little bit after the cluster is customized to work",
    "start": "2646920",
    "end": "2655190"
  },
  {
    "text": "business users directly go to service catalog and spin up the cluster as part",
    "start": "2655190",
    "end": "2661950"
  },
  {
    "text": "of their spin up process they can provide parameters like instance types number of nodes they want whether they",
    "start": "2661950",
    "end": "2669930"
  },
  {
    "text": "want to use spot instance or not etc so they can customize certain attributes of",
    "start": "2669930",
    "end": "2676230"
  },
  {
    "text": "their EMR cluster going forward which gives them that self-management capability to spin up the cluster when",
    "start": "2676230",
    "end": "2684089"
  },
  {
    "text": "they want shut it down when they want move to a newer version when they want",
    "start": "2684089",
    "end": "2689099"
  },
  {
    "text": "or keep using the older version benefits",
    "start": "2689099",
    "end": "2695069"
  },
  {
    "text": "of this is now we have a very simplified and faster deployment process it also",
    "start": "2695069",
    "end": "2703020"
  },
  {
    "text": "gives increased operational resiliency because nobody is copying templates and",
    "start": "2703020",
    "end": "2709289"
  },
  {
    "text": "making clones of what my team provides they are reusing what we deploy out there provides flexibility to business",
    "start": "2709289",
    "end": "2717329"
  },
  {
    "text": "to manage clusters on their own and it becomes very easier to integrate this",
    "start": "2717329",
    "end": "2724680"
  },
  {
    "text": "entire process into standard itll products and processes where are we",
    "start": "2724680",
    "end": "2732450"
  },
  {
    "start": "2731000",
    "end": "2827000"
  },
  {
    "text": "going in future our main focus going forward and future as operational",
    "start": "2732450",
    "end": "2738510"
  },
  {
    "text": "resiliency and also how to get from one region to multiple regions and have full",
    "start": "2738510",
    "end": "2745920"
  },
  {
    "text": "high availability all the time first power that is around it is all around",
    "start": "2745920",
    "end": "2753569"
  },
  {
    "text": "EMR cluster and how do we take service catalog and put up front end to it",
    "start": "2753569",
    "end": "2760890"
  },
  {
    "text": "when God's looking at using ServiceNow as a front end and we will be using AWS",
    "start": "2760890",
    "end": "2767970"
  },
  {
    "text": "Service Catalog connector to launch as for products in in AWS next one rs3 how",
    "start": "2767970",
    "end": "2779680"
  },
  {
    "text": "do we make it more resilient we want to start working on enabling versioning",
    "start": "2779680",
    "end": "2784990"
  },
  {
    "text": "which we haven't done yet and which is a prerequisite for replicating it across",
    "start": "2784990",
    "end": "2790810"
  },
  {
    "text": "regions so that's where we are adding resiliency there and then adding",
    "start": "2790810",
    "end": "2797140"
  },
  {
    "text": "resiliency for the IT teams to use glue blue catalog for building their",
    "start": "2797140",
    "end": "2803910"
  },
  {
    "text": "workflows data ingestion processes providing them an additional tool set in",
    "start": "2803910",
    "end": "2810010"
  },
  {
    "text": "their toolbox outside of this we are expanding into other services like",
    "start": "2810010",
    "end": "2815770"
  },
  {
    "text": "Athena we are in process of enabling sage maker in production in 2018 early",
    "start": "2815770",
    "end": "2824230"
  },
  {
    "text": "2019 thank you and I'll open it up for",
    "start": "2824230",
    "end": "2829960"
  },
  {
    "start": "2827000",
    "end": "3321000"
  },
  {
    "text": "questions we have two roughly twelve and a half minutes we can take as many",
    "start": "2829960",
    "end": "2835720"
  },
  {
    "text": "questions let's start from here",
    "start": "2835720",
    "end": "2849450"
  },
  {
    "text": "okay who's the question fault okay",
    "start": "2859070",
    "end": "2865530"
  },
  {
    "text": "so the question is the - do you have a pre-production environment for your developers yes we we have clear",
    "start": "2865530",
    "end": "2871470"
  },
  {
    "text": "environments in Vanguard we call it as accounts we have an engineering account where is like a development account then",
    "start": "2871470",
    "end": "2879630"
  },
  {
    "text": "we have a test account where they elevate their cluster but it's all not controlled where they cannot make",
    "start": "2879630",
    "end": "2885420"
  },
  {
    "text": "changes to it outside of the CD process and then we have the production account",
    "start": "2885420",
    "end": "2890940"
  },
  {
    "text": "so we have two other accounts for each cluster or each team so the question is",
    "start": "2890940",
    "end": "2914760"
  },
  {
    "text": "how do you how do test accounts share production data no they don't write",
    "start": "2914760",
    "end": "2919890"
  },
  {
    "text": "Vanguard has a stringent rule of not having any Prada in any lower account so",
    "start": "2919890",
    "end": "2925859"
  },
  {
    "text": "what we do is when you do engineering we connect to engineering databases and",
    "start": "2925859",
    "end": "2931050"
  },
  {
    "text": "pull that data tested then we have a separate set of unstow on-premise data",
    "start": "2931050",
    "end": "2936630"
  },
  {
    "text": "stores that we call it as test databases test a test account pull data from test",
    "start": "2936630",
    "end": "2943320"
  },
  {
    "text": "and test against it and then all of that goes to product so I think is the question is is there a when you test on",
    "start": "2943320",
    "end": "2950760"
  },
  {
    "text": "the on the dev environment is the data still in s3 does it come from everything",
    "start": "2950760",
    "end": "2956400"
  },
  {
    "text": "everything is in s3 it's a separate set of buckets localized to that account so",
    "start": "2956400",
    "end": "2962570"
  },
  {
    "text": "engineering past prod everything has separate as three buckets there",
    "start": "2962570",
    "end": "2970220"
  },
  {
    "text": "yeah so we have seen those kind of issues pop up where you elevate your",
    "start": "2979840",
    "end": "2985400"
  },
  {
    "text": "logic or application with the cluster to production and it doesn't work as desired we try to handle it by a",
    "start": "2985400",
    "end": "2995200"
  },
  {
    "text": "convention called we call it a pre prod timeframe where any clusters running in",
    "start": "2995200",
    "end": "3001240"
  },
  {
    "text": "during that cycle that data is not available to business or we tell business saying that data may not be",
    "start": "3001240",
    "end": "3007930"
  },
  {
    "text": "clean yet for analytics so that's how we are handle it at times so the question",
    "start": "3007930",
    "end": "3013720"
  },
  {
    "text": "was how do you prevent regressions if you're not testing for the entire data set okay that question comes on very",
    "start": "3013720",
    "end": "3027010"
  },
  {
    "text": "often when not intentionally chose not to use Apache arranger reason because",
    "start": "3027010",
    "end": "3032680"
  },
  {
    "text": "when we started our journey it was available for hive it didn't have much",
    "start": "3032680",
    "end": "3039780"
  },
  {
    "text": "availability from an authorization perspective for spar and presto wasn't",
    "start": "3039780",
    "end": "3045820"
  },
  {
    "text": "available either plus we didn't want to limit ourselves for future meaning when",
    "start": "3045820",
    "end": "3051040"
  },
  {
    "text": "we enable Athena what do we do right it doesn't use Ranger or when we use",
    "start": "3051040",
    "end": "3056530"
  },
  {
    "text": "redshift it doesn't use a ranger so we intentionally decided not to use Ranger",
    "start": "3056530",
    "end": "3061630"
  },
  {
    "text": "and if required brute force separate tii from non PII if required and authorized",
    "start": "3061630",
    "end": "3069760"
  },
  {
    "text": "people only for right data that they need to other get access to so you're also a ranger or at least from from my",
    "start": "3069760",
    "end": "3077620"
  },
  {
    "text": "knowledge of Ranger still doesn't enforce policies on s/3 s/3 enforces policies in the data and s3 so if you",
    "start": "3077620",
    "end": "3084070"
  },
  {
    "text": "have Ranger the ranger agent actually works on enforcing policies in HDFS and that works fundamentally fine if you've",
    "start": "3084070",
    "end": "3090730"
  },
  {
    "text": "seen and the announcement that came out today around AWS lake formation one of",
    "start": "3090730",
    "end": "3096100"
  },
  {
    "text": "the things that is going to do is enforce policies on data that is sitting in s3 across EMR",
    "start": "3096100",
    "end": "3102320"
  },
  {
    "text": "a red shift and blue sari the scheduling",
    "start": "3102320",
    "end": "3119900"
  },
  {
    "text": "of the um ourselves so actually we all our jobs are actually scheduled through data pipeline so we have what your build",
    "start": "3119900",
    "end": "3125810"
  },
  {
    "text": "is to build a framework or a you know like a you know a DSL on top of the data",
    "start": "3125810",
    "end": "3131660"
  },
  {
    "text": "pipeline and that is what is used for scheduling so we have done very interesting ways to say when some jobs",
    "start": "3131660",
    "end": "3137000"
  },
  {
    "text": "can be done in parallel some cannot be done in parallel and all that so we have done that through the data pipeline",
    "start": "3137000",
    "end": "3142600"
  },
  {
    "text": "questionnaire for me yeah yeah all data",
    "start": "3142600",
    "end": "3152840"
  },
  {
    "text": "in s3 is encrypted using kms customer",
    "start": "3152840",
    "end": "3158510"
  },
  {
    "text": "manage keys everywhere including and if we use RDS as an example Postgres that",
    "start": "3158510",
    "end": "3165740"
  },
  {
    "text": "data is also encrypted if we also do the same actually we also encrypted data and",
    "start": "3165740",
    "end": "3170810"
  },
  {
    "text": "a bunch of these tools like EMR and Athena and glue they all allow you to",
    "start": "3170810",
    "end": "3176120"
  },
  {
    "text": "use the encrypted keys from the same Kallstrom there was a question there right batteries",
    "start": "3176120",
    "end": "3183460"
  },
  {
    "text": "[Music]",
    "start": "3190180",
    "end": "3193369"
  },
  {
    "text": "yeah so we actually so so the question is do we have standard you know do we",
    "start": "3195230",
    "end": "3200910"
  },
  {
    "text": "spin up spin down or do we have like standalone clusters or not so in our use case as users create as our customers",
    "start": "3200910",
    "end": "3206400"
  },
  {
    "text": "create segments it's a very on-demand workflow so we use we basically spin up",
    "start": "3206400",
    "end": "3212010"
  },
  {
    "text": "and spin down our EMR clusters all across so today if you go and create few segments immediately after that a",
    "start": "3212010",
    "end": "3218790"
  },
  {
    "text": "cluster will be spun up based on that so think of it as that will basically trigger a cluster creation it will spin",
    "start": "3218790",
    "end": "3225300"
  },
  {
    "text": "up it will look at the data at look at the rules it will process it and will bring it down that's right that's right",
    "start": "3225300",
    "end": "3233940"
  },
  {
    "text": "that's right that's right",
    "start": "3233940",
    "end": "3237020"
  },
  {
    "text": "for Vanguard we we do it at a bucket level so no we don't actually we do it",
    "start": "3247650",
    "end": "3257860"
  },
  {
    "text": "at the bucket level actually a three doesn't recognize that there is a field because the the entity in s3 is",
    "start": "3257860",
    "end": "3265630"
  },
  {
    "text": "essentially a bucket and an object so it can only encrypt buckets and objects right so it doesn't understand concepts",
    "start": "3265630",
    "end": "3271210"
  },
  {
    "text": "of s3 by itself doesn't understand concepts of tables or concepts of rows or columns or records right it",
    "start": "3271210",
    "end": "3278230"
  },
  {
    "text": "understands buckets and objects yeah so they",
    "start": "3278230",
    "end": "3284190"
  },
  {
    "text": "sure true when you do encrypt objects the entire object is encrypted I think what",
    "start": "3294819",
    "end": "3301670"
  },
  {
    "text": "you're referring to is can I selectively encrypt a certain field yeah any more",
    "start": "3301670",
    "end": "3308900"
  },
  {
    "text": "questions well thank you for your time and thank you gentlemen for thank you",
    "start": "3308900",
    "end": "3315640"
  },
  {
    "text": "[Applause]",
    "start": "3315640",
    "end": "3321170"
  }
]