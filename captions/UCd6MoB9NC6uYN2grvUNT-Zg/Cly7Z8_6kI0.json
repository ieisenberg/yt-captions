[
  {
    "start": "0",
    "end": "34000"
  },
  {
    "text": "In this video, you’ll see how to set up \nan Amazon Aurora MySQL zero-ETL",
    "start": "0",
    "end": "5382"
  },
  {
    "text": "integration with Amazon Redshift \nthat includes data filtering.",
    "start": "5382",
    "end": "9557"
  },
  {
    "text": "With this integration, you get an easy and \nreliable solution for running near real-time",
    "start": "10617",
    "end": "15371"
  },
  {
    "text": "analytics and machine learning on \npetabytes of transactional data, with",
    "start": "15371",
    "end": "19146"
  },
  {
    "text": "no complex ETL (extract, transform, \nand load) infrastructure to maintain.",
    "start": "19146",
    "end": "22372"
  },
  {
    "text": "You can also define the replication \nscope using data filtering.",
    "start": "22834",
    "end": "26248"
  },
  {
    "text": "This saves costs when performing \nanalytics on fewer tables and makes it",
    "start": "26531",
    "end": "30145"
  },
  {
    "text": "possible to exclude tables with \nsensitive personal information.",
    "start": "30145",
    "end": "33823"
  },
  {
    "start": "34000",
    "end": "63000"
  },
  {
    "text": "A zero-ETL integration simplifies setting\n up and maintaining an analytics data",
    "start": "35017",
    "end": "40023"
  },
  {
    "text": "pipeline between a source--in this case,\n Amazon Aurora MySQL--and a target",
    "start": "40023",
    "end": "45576"
  },
  {
    "text": "system--in this case, Amazon Redshift.",
    "start": "45576",
    "end": "48619"
  },
  {
    "text": "You can use both operational and analytics \ndatabases for your respective workloads.",
    "start": "49052",
    "end": "53907"
  },
  {
    "text": "Once your operational data is in Amazon \nRedshift, you can leverage a number of",
    "start": "55027",
    "end": "59521"
  },
  {
    "text": "powerful analytics tools and features.",
    "start": "59521",
    "end": "62058"
  },
  {
    "start": "63000",
    "end": "90000"
  },
  {
    "text": "To create an integration, we \nneed a source and a target.",
    "start": "64250",
    "end": "67759"
  },
  {
    "text": "For demonstration purposes, these \nresources have already been configured.",
    "start": "68132",
    "end": "72232"
  },
  {
    "text": "To get started, let’s navigate to the \nAmazon Relational Database Service",
    "start": "73441",
    "end": "77556"
  },
  {
    "text": "console and look at our source.",
    "start": "77556",
    "end": "79808"
  },
  {
    "text": "Our source is an Aurora MySQL \ndatabase cluster named zeroetl-demo.",
    "start": "81000",
    "end": "85979"
  },
  {
    "text": "Next, let’s navigate to the \nZero-ETL integrations page.",
    "start": "86247",
    "end": "90120"
  },
  {
    "start": "90000",
    "end": "129000"
  },
  {
    "text": "This page guides you through the steps\n required to create a zero-ETL integration,",
    "start": "91269",
    "end": "96573"
  },
  {
    "text": "which include preparing the source \ndatabase, preparing the target Redshift",
    "start": "96573",
    "end": "100825"
  },
  {
    "text": "data warehouse, and \ncreating the integration itself.",
    "start": "100825",
    "end": "104187"
  },
  {
    "text": "Now let’s create an integration.",
    "start": "105441",
    "end": "108000"
  },
  {
    "text": "First, we’ll give the integration a name.",
    "start": "109656",
    "end": "111767"
  },
  {
    "text": "Next, we’ll select the source.",
    "start": "115199",
    "end": "116835"
  },
  {
    "text": "We can browse to available RDS databases.",
    "start": "117103",
    "end": "119733"
  },
  {
    "text": "RDS automatically filters to \nsupported engine versions.",
    "start": "120853",
    "end": "124104"
  },
  {
    "text": "We’ll keep zeroetl-demo as the source.",
    "start": "124567",
    "end": "127304"
  },
  {
    "start": "129000",
    "end": "162000"
  },
  {
    "text": "RDS validates the parameter \nsettings for the database cluster.",
    "start": "129945",
    "end": "133377"
  },
  {
    "text": "In this case, the parameter \nvalues need to be fixed.",
    "start": "133541",
    "end": "136358"
  },
  {
    "text": "We’ll have RDS fix them for us.",
    "start": "136835",
    "end": "138877"
  },
  {
    "text": "Now, we can also perform data filtering\n by including or excluding databases",
    "start": "141936",
    "end": "146518"
  },
  {
    "text": "and database tables based on a comma \nseparated list of filter expressions.",
    "start": "146518",
    "end": "150740"
  },
  {
    "text": "For this example, we’ll include all tables\n from sales and marketing databases",
    "start": "151113",
    "end": "155627"
  },
  {
    "text": "and exclude all tables from the hr database.",
    "start": "155627",
    "end": "158660"
  },
  {
    "start": "162000",
    "end": "193000"
  },
  {
    "text": "RDS will create a new DB cluster \nparameter group based on the",
    "start": "163031",
    "end": "166837"
  },
  {
    "text": "parameter group that is currently \nassociated with the cluster, then it will",
    "start": "166837",
    "end": "170571"
  },
  {
    "text": "make changes to the aurora_enhanced_binlog, \nbinlog_replication_globaldb, binlog_row_image,",
    "start": "170571",
    "end": "177619"
  },
  {
    "text": "binlog_backup, binlog_format, and \nbinlog_row_metadata parameters.",
    "start": "177619",
    "end": "183062"
  },
  {
    "text": "To allow RDS to make the parameter changes \nand reboot our database, we’ll enter confirm.",
    "start": "183912",
    "end": "188883"
  },
  {
    "text": "Let’s reboot and continue.",
    "start": "191315",
    "end": "192741"
  },
  {
    "text": "RDS is now fixing the parameter values.",
    "start": "193889",
    "end": "196639"
  },
  {
    "text": "We can view more details from the \ninformation panel at the top of the page.",
    "start": "196893",
    "end": "201000"
  },
  {
    "text": "RDS has created a parameter group, \nmodified it to apply the correct parameters,",
    "start": "201776",
    "end": "206939"
  },
  {
    "text": "and associated the source DB \ncluster with the parameter group.",
    "start": "206939",
    "end": "210721"
  },
  {
    "text": "It is currently rebooting the source database.",
    "start": "211079",
    "end": "213853"
  },
  {
    "text": "Next, we’ll select the target\n Redshift data warehouse.",
    "start": "215838",
    "end": "219183"
  },
  {
    "text": "We can choose a Redshift data \nwarehouse in our current AWS",
    "start": "219617",
    "end": "222540"
  },
  {
    "text": "account or in a different account.",
    "start": "222540",
    "end": "224321"
  },
  {
    "text": "To specify a different account, we’d simply \nenter the Redshift data warehouse ARN.",
    "start": "225426",
    "end": "230808"
  },
  {
    "text": "In this case, we’ll choose an existing \nRedshift data warehouse in our current account.",
    "start": "231300",
    "end": "236012"
  },
  {
    "start": "240000",
    "end": "266000"
  },
  {
    "text": "Our Redshift data warehouse does \nnot have the correct resource policy",
    "start": "241159",
    "end": "244674"
  },
  {
    "text": "to support zero-ETL integration.",
    "start": "244674",
    "end": "247111"
  },
  {
    "text": "We’ll have RDS fix this for us.",
    "start": "247469",
    "end": "249629"
  },
  {
    "text": "RDS will update the target data \nwarehouse resource policy with the",
    "start": "252762",
    "end": "256512"
  },
  {
    "text": "authorized principal and authorized \nintegration source, and will enable case",
    "start": "256512",
    "end": "260782"
  },
  {
    "text": "sensitivity for the target data warehouse.",
    "start": "260783",
    "end": "263238"
  },
  {
    "text": "We’ll select continue to apply these changes.",
    "start": "263493",
    "end": "266280"
  },
  {
    "start": "266000",
    "end": "287000"
  },
  {
    "text": "Optionally, we can add tags and \ncustomize the encryption settings.",
    "start": "267295",
    "end": "270904"
  },
  {
    "text": "We’ll skip this step.",
    "start": "271605",
    "end": "272803"
  },
  {
    "text": "Let’s review the settings \nand create the integration.",
    "start": "274370",
    "end": "276979"
  },
  {
    "text": "It can take up to 30 minutes \nto create the integration.",
    "start": "280097",
    "end": "282939"
  },
  {
    "text": "Let’s fast forward to when it \nhas been successfully created.",
    "start": "283238",
    "end": "286327"
  },
  {
    "start": "287000",
    "end": "313000"
  },
  {
    "text": "The last step is to create a database in \nRedshift that is linked to the integration.",
    "start": "288415",
    "end": "292714"
  },
  {
    "text": "To do this, we’ll navigate to the \nAmazon Redshift Serverless console.",
    "start": "293266",
    "end": "297403"
  },
  {
    "text": "Next, we’ll go to our namespace and open \nthe details for the integration we created.",
    "start": "298567",
    "end": "303210"
  },
  {
    "text": "We’ll now create a database\n from the integration.",
    "start": "306985",
    "end": "309362"
  },
  {
    "start": "313000",
    "end": "345000"
  },
  {
    "text": "The database has been created.",
    "start": "314300",
    "end": "315913"
  },
  {
    "text": "Now data written to our source database \ncluster will be replicated to our Redshift database.",
    "start": "316196",
    "end": "321421"
  },
  {
    "text": "We can view integration metrics \nat the bottom of this page.",
    "start": "322242",
    "end": "325373"
  },
  {
    "text": "These metrics will be populated \nas soon as data starts flowing.",
    "start": "326030",
    "end": "329272"
  },
  {
    "text": "This information will also be \navailable in Amazon CloudWatch.",
    "start": "330108",
    "end": "333671"
  },
  {
    "text": "In addition, we can view statistics for \ntables that are currently active or have errors,",
    "start": "334388",
    "end": "339655"
  },
  {
    "text": "and leverage all of the other powerful\n tools Amazon Redshift has to offer.",
    "start": "339655",
    "end": "344226"
  },
  {
    "start": "345000",
    "end": "370000"
  },
  {
    "text": "You’ve just seen how to set up an Amazon \nAurora MySQL zero-ETL integration with",
    "start": "345957",
    "end": "350861"
  },
  {
    "text": "Amazon Redshift that eliminates the \nneed to manually build ETL pipelines",
    "start": "350861",
    "end": "356057"
  },
  {
    "text": "and how to perform data filtering to save \nreplication costs and protect sensitive data.",
    "start": "356057",
    "end": "361260"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "362096",
    "end": "366000"
  },
  {
    "text": "Thanks for watching. Now it's your turn to try.",
    "start": "366519",
    "end": "369146"
  }
]