[
  {
    "text": "my name is Robert Kissel a Solutions Architect for AWS I've been with abs for",
    "start": "60",
    "end": "5640"
  },
  {
    "text": "about three and a half years and during that timeframe I have actually had the pleasure to work with FINRA the entire",
    "start": "5640",
    "end": "11190"
  },
  {
    "text": "span of their cloud journey and my technology focus is a Big Data machine",
    "start": "11190",
    "end": "17220"
  },
  {
    "text": "learning and how those two have a natural and sometimes unnatural",
    "start": "17220",
    "end": "23810"
  },
  {
    "text": "symbiotic relationship today I'm here with John hitching hamm Senior Director",
    "start": "23810",
    "end": "29010"
  },
  {
    "text": "for engineering at FINRA and we're here to discuss the benefits of the data Lake",
    "start": "29010",
    "end": "34430"
  },
  {
    "text": "how fin or performs trade analytics and market surveillance on AWS so let's talk",
    "start": "34430",
    "end": "46469"
  },
  {
    "text": "about the the pillars of a data Lake so the first pillar is scale so storing",
    "start": "46469",
    "end": "53070"
  },
  {
    "text": "and analyzing all of your data centrally ingesting your data quickly without",
    "start": "53070",
    "end": "60149"
  },
  {
    "text": "predefined schemas and being able to separate your storage and your compute",
    "start": "60149",
    "end": "67400"
  },
  {
    "text": "being able to scale both of those components separately the second pillar",
    "start": "67400",
    "end": "74040"
  },
  {
    "text": "is cost pay for only what you need and pay for only what you use and utilizing",
    "start": "74040",
    "end": "81720"
  },
  {
    "text": "diverse services and being able to optimize your costs so being able to",
    "start": "81720",
    "end": "88130"
  },
  {
    "text": "understand your utilization and performance and being able to adjust your services your features your",
    "start": "88430",
    "end": "95340"
  },
  {
    "text": "applications next is security so when we",
    "start": "95340",
    "end": "101009"
  },
  {
    "text": "have encryption at every step whether that is the encryption that you bring to",
    "start": "101009",
    "end": "106369"
  },
  {
    "text": "AWS or using utilizing one of the AWS services having explicit control of all",
    "start": "106369",
    "end": "113520"
  },
  {
    "text": "ingress and egress whether that is through security groups network ackles",
    "start": "113520",
    "end": "119040"
  },
  {
    "text": "or access and Identity Management and",
    "start": "119040",
    "end": "124290"
  },
  {
    "text": "then having compliance and governance at the data access layer using the AWS",
    "start": "124290",
    "end": "132120"
  },
  {
    "text": "native services and then the final is agility big data",
    "start": "132120",
    "end": "138690"
  },
  {
    "text": "doesn't mean that it has to be all batch processing this could be data that could be streaming in from IOT devices or from",
    "start": "138690",
    "end": "145440"
  },
  {
    "text": "advertisements being able to mix and match your on cloud and in premise",
    "start": "145440",
    "end": "150540"
  },
  {
    "text": "applications and your resources so a",
    "start": "150540",
    "end": "159570"
  },
  {
    "text": "moment ago I just know mentioned that using something like Amazon s3 is your",
    "start": "159570",
    "end": "164640"
  },
  {
    "text": "centralized storage excuse me you know being Amazon s3 it's scalable secure and",
    "start": "164640",
    "end": "171780"
  },
  {
    "text": "cost-effective multi-tiered storage service but getting your data to s3",
    "start": "171780",
    "end": "178760"
  },
  {
    "text": "becomes the little bit of the challenge so that can be done if you have massive",
    "start": "178760",
    "end": "183990"
  },
  {
    "text": "amounts of data can be moved by a snowball within days or snowmobile if",
    "start": "183990",
    "end": "189870"
  },
  {
    "text": "you have massive massive amounts of data through things like streaming with",
    "start": "189870",
    "end": "196050"
  },
  {
    "text": "Kinesis being able to utilize Direct Connect connections to move your data or",
    "start": "196050",
    "end": "203700"
  },
  {
    "text": "and/or using the database migration services to migrate data stores and then",
    "start": "203700",
    "end": "213300"
  },
  {
    "text": "what are you going to do with that data so you're processing in your analytics can come in many many forms it can come",
    "start": "213300",
    "end": "220950"
  },
  {
    "text": "from your own custom applications running on premise or running in Amazon",
    "start": "220950",
    "end": "225959"
  },
  {
    "text": "ec2 it could be utilizing one of the managed services like EMR could be using",
    "start": "225959",
    "end": "234230"
  },
  {
    "text": "excuse me quick site or athina for those",
    "start": "234230",
    "end": "239780"
  },
  {
    "text": "business intelligence and or queries and",
    "start": "239780",
    "end": "246860"
  },
  {
    "text": "then being able to utilize dynamodb to be able to kind of catalog your data",
    "start": "246860",
    "end": "253290"
  },
  {
    "text": "being able to create a meta store so that you can access your data in using",
    "start": "253290",
    "end": "262169"
  },
  {
    "text": "utilizing fast queries and or last a search for your enterprise search functions we have",
    "start": "262169",
    "end": "272419"
  },
  {
    "text": "kognito we have API gateway to be able to gain authentication and authorization",
    "start": "272419",
    "end": "280740"
  },
  {
    "text": "access to your data and then being able",
    "start": "280740",
    "end": "285930"
  },
  {
    "text": "to protect and secure data so utilizing kms Amazon kms for your encryption",
    "start": "285930",
    "end": "294470"
  },
  {
    "text": "utilizing your own encryption modules",
    "start": "294470",
    "end": "299960"
  },
  {
    "text": "and Identity and Access Management so",
    "start": "299960",
    "end": "305310"
  },
  {
    "text": "now I want to turn the mic over to John hinchik am to go through FINRA's",
    "start": "305310",
    "end": "310340"
  },
  {
    "text": "implementation of their data Lake and their analytics and surveillance right",
    "start": "310340",
    "end": "317509"
  },
  {
    "text": "hi everybody I'm John hitching ham from FINRA engineering and today I'm going to",
    "start": "319130",
    "end": "324840"
  },
  {
    "text": "share with you how FINRA performs trade surveillance and analytics on its multi",
    "start": "324840",
    "end": "329910"
  },
  {
    "text": "petabyte data Lake in AWS so first what",
    "start": "329910",
    "end": "335039"
  },
  {
    "text": "is FINRA FINRA is the Financial Industry Regulatory Authority a not-for-profit organization authorized by Congress to",
    "start": "335039",
    "end": "342900"
  },
  {
    "text": "protect America's investors and it does this through oversight of over 3,500",
    "start": "342900",
    "end": "348780"
  },
  {
    "text": "broker-dealer firms and over 600,000 broker-dealer individuals in the",
    "start": "348780",
    "end": "354479"
  },
  {
    "text": "industry professionals in the US it also ensures the integrity of America's markets monitoring over 99% of the",
    "start": "354479",
    "end": "363030"
  },
  {
    "text": "equity and 70% of the options activity in the US looking for fraud insider trading market manipulation and abuse",
    "start": "363030",
    "end": "372590"
  },
  {
    "text": "how did swim to do this our market regulation division pulls in data from",
    "start": "372590",
    "end": "378360"
  },
  {
    "text": "outside entities broker dealers exchanges augmented with third-party reference data we bring it in we",
    "start": "378360",
    "end": "386250"
  },
  {
    "text": "validate it we went through a series of processing transforms to normalize the data to feed automated detection models",
    "start": "386250",
    "end": "393660"
  },
  {
    "text": "those automated detection models will look for anomalies in the data when they find them trade anomalies event",
    "start": "393660",
    "end": "399979"
  },
  {
    "text": "anomalies kick out an alert to a case management system where analysts will",
    "start": "399979",
    "end": "405229"
  },
  {
    "text": "then go and use a series of interactive analytic tools to go explore around the anomaly to see whether there's an issue",
    "start": "405229",
    "end": "411860"
  },
  {
    "text": "there that requires regulatory follow-up or not in parallel we have a team of",
    "start": "411860",
    "end": "417919"
  },
  {
    "text": "data scientists they're always working to develop new detection models using a variety of tools and then those get",
    "start": "417919",
    "end": "424159"
  },
  {
    "text": "deployed and built into the the process as far as the detection models go this",
    "start": "424159",
    "end": "429800"
  },
  {
    "text": "entire operation truly is Big Data 275 billion records per day over 20",
    "start": "429800",
    "end": "435800"
  },
  {
    "text": "petabytes of data now up to 25 petabytes of data actually supporting major",
    "start": "435800",
    "end": "442129"
  },
  {
    "text": "exchange clients we've had pieces of the system running in production for over three years the entire portfolio",
    "start": "442129",
    "end": "447949"
  },
  {
    "text": "supporting are supporting our market regulation division has been running for a year and a half in production in AWS",
    "start": "447949",
    "end": "456400"
  },
  {
    "text": "so how do we run it implementation wise the architecture we use is what we we",
    "start": "456879",
    "end": "462919"
  },
  {
    "text": "call internally our our managed data leak architecture and the key to this is",
    "start": "462919",
    "end": "467930"
  },
  {
    "text": "we've taken the silos or puddles of storage query compute start compute and",
    "start": "467930",
    "end": "475460"
  },
  {
    "text": "catalog that existed in our on-premises environment and we've remap those to services in AWS so instead of clusters",
    "start": "475460",
    "end": "483560"
  },
  {
    "text": "of you know Hadoop on-premises instead of data warehouse appliances databases",
    "start": "483560",
    "end": "489349"
  },
  {
    "text": "we have a series of services in AWS catalog for for catalog we use a series",
    "start": "489349",
    "end": "494870"
  },
  {
    "text": "of open source catalogs for storage we use s3 we also use some glacier and for",
    "start": "494870",
    "end": "502279"
  },
  {
    "text": "query compute we use a variety of Amazon analytics for AWS analytic services and",
    "start": "502279",
    "end": "507379"
  },
  {
    "text": "compute services primarily EMR but also lambda and also redshift and what this",
    "start": "507379",
    "end": "513979"
  },
  {
    "text": "lets us do is by mapping to a series of services we can now scale out without limits both in terms of processing",
    "start": "513979",
    "end": "520820"
  },
  {
    "text": "capacity and storage capacity so that when there's disruptions or unplanned activity it becomes a non-event to deal",
    "start": "520820",
    "end": "526970"
  },
  {
    "text": "with it we've also broken down the data silos in organization so that all the data is now",
    "start": "526970",
    "end": "532189"
  },
  {
    "text": "available and accessible through a centralized catalog and basically a single storage medium where I can all go",
    "start": "532189",
    "end": "539149"
  },
  {
    "text": "be accessed all can be accessed so key",
    "start": "539149",
    "end": "546230"
  },
  {
    "text": "to the managed data lake is the data catalog and when we started our journey",
    "start": "546230",
    "end": "551990"
  },
  {
    "text": "to the cloud five years ago now we wanted to we had a set of unique",
    "start": "551990",
    "end": "557810"
  },
  {
    "text": "requirements that we need to have satisfied by our catalog for one we wanted to be able to manage storage",
    "start": "557810",
    "end": "564050"
  },
  {
    "text": "managed objects in the catalog at the business level not just the physical level that way we could have different",
    "start": "564050",
    "end": "571100"
  },
  {
    "text": "underlying storage mechanisms underneath we could reference data whether it be stored in relational database or whether",
    "start": "571100",
    "end": "577250"
  },
  {
    "text": "it be stored on s3 we also had business requirement to be able to track data",
    "start": "577250",
    "end": "583519"
  },
  {
    "text": "versioning in data lineage as part of our business process workflow we also",
    "start": "583519",
    "end": "589519"
  },
  {
    "text": "wanted the ability to capture metadata again not just technical metadata like schema information and location",
    "start": "589519",
    "end": "595550"
  },
  {
    "text": "information of partitioning information but also descriptive business metadata about the objects that we were we were",
    "start": "595550",
    "end": "601129"
  },
  {
    "text": "cataloging also important was the ability to be able to trigger events when new data",
    "start": "601129",
    "end": "607699"
  },
  {
    "text": "arrives and gets registered with a catalog we all wanted to be able to do eventing and triggering off of that and",
    "start": "607699",
    "end": "612889"
  },
  {
    "text": "a variety of other requirements so as I said we started looking five years ago when we started our journey to the cloud",
    "start": "612889",
    "end": "618860"
  },
  {
    "text": "at catalogs that were available commercially or open source we didn't find any that met our requirements so we",
    "start": "618860",
    "end": "625579"
  },
  {
    "text": "developed our own internally we call the data management today it's available",
    "start": "625579",
    "end": "630740"
  },
  {
    "text": "open source as the Hurd project HT Rd available on github and this is the",
    "start": "630740",
    "end": "636290"
  },
  {
    "text": "catalog that we use today is the core of our managed data Lake in AWS so as an",
    "start": "636290",
    "end": "643639"
  },
  {
    "text": "example I talked about kind of some of the requirements that we had one that I mentioned was data versioning data",
    "start": "643639",
    "end": "649550"
  },
  {
    "text": "lineage and why is this important to us we get those data sets from those outs from those external providers we do that",
    "start": "649550",
    "end": "656180"
  },
  {
    "text": "series of transforms and process on it to have a output data set that we do regulatory analytics on",
    "start": "656180",
    "end": "661500"
  },
  {
    "text": "occasionally we'll get corrections actually you know frequently we can get corrections from those upstream",
    "start": "661500",
    "end": "667269"
  },
  {
    "text": "providers where we get a new amended data set and we need to be able to track both the there was the original version",
    "start": "667269",
    "end": "673089"
  },
  {
    "text": "of the data along with the amended data that the provider provides so we also be",
    "start": "673089",
    "end": "678970"
  },
  {
    "text": "able to need to track through down dough to the outputs that resulted from that data because there's a case that the",
    "start": "678970",
    "end": "684399"
  },
  {
    "text": "data set that we use to run regulatory analytic on might need to be rerun or updated because it might impact the",
    "start": "684399",
    "end": "690579"
  },
  {
    "text": "regulatory decision based on the new data that comes in so by tracking all this information the catalog would make",
    "start": "690579",
    "end": "696160"
  },
  {
    "text": "it easy to do impact analysis and to go out and determine whether or not we need to rerun any set of analytics in our in",
    "start": "696160",
    "end": "703000"
  },
  {
    "text": "our portfolio another example you know",
    "start": "703000",
    "end": "708399"
  },
  {
    "text": "the things that we're doing today to build and extend the catalog records management we're building a records",
    "start": "708399",
    "end": "714070"
  },
  {
    "text": "management capability on top of the storage level api's so we can track",
    "start": "714070",
    "end": "719079"
  },
  {
    "text": "through a centralized tool the records that we manage in the cloud instead of having to use a you know separate",
    "start": "719079",
    "end": "726550"
  },
  {
    "text": "out-of-band process with with manual activities as part of that another",
    "start": "726550",
    "end": "733180"
  },
  {
    "text": "example of what we're doing is user interface on top of the catalog so that our analyst community or data science",
    "start": "733180",
    "end": "740020"
  },
  {
    "text": "community can go and browse search and explore the data sets that are available out there as I said we're capturing",
    "start": "740020",
    "end": "747160"
  },
  {
    "text": "business metadata in addition the technical metadata so they can search for business really do business related",
    "start": "747160",
    "end": "752380"
  },
  {
    "text": "searches find the data sets of interest and then they can go through and grab the technical metadata that they need in",
    "start": "752380",
    "end": "758020"
  },
  {
    "text": "order to go out and figure out where to access and query and start using the data this is something that's available",
    "start": "758020",
    "end": "764440"
  },
  {
    "text": "today in our internal version of the software looking to open source it either late you know later this year if",
    "start": "764440",
    "end": "770560"
  },
  {
    "text": "not early next court earlier first quarter of next year so again that the",
    "start": "770560",
    "end": "777130"
  },
  {
    "text": "data catalog is kind of key to what we do we do have watched the emergence of new catalogs are out there which is the",
    "start": "777130",
    "end": "783430"
  },
  {
    "text": "glue catalog at this point our business requirements the only thing that's out there that meets them is the herd",
    "start": "783430",
    "end": "788620"
  },
  {
    "text": "catalog so we continue to build them step on it so with the catalog and you",
    "start": "788620",
    "end": "796800"
  },
  {
    "text": "know as through is a storage layer that becomes the hub of our entire processing pipeline so data comes in from external",
    "start": "796800",
    "end": "804960"
  },
  {
    "text": "providers either lands directly on s3 or it can come in through other feeds like an SFTP feed we have four legacy",
    "start": "804960",
    "end": "811560"
  },
  {
    "text": "interfaces data comes in it gets registered in the catalog after landing on s3 and then we can do a series of process",
    "start": "811560",
    "end": "819120"
  },
  {
    "text": "things against it we can do validation we can do ETL we can run those detection models and in each of those cases the",
    "start": "819120",
    "end": "825690"
  },
  {
    "text": "process starts by kicking off a workflow process that reads from the catalog finds the metadata that's necessary and",
    "start": "825690",
    "end": "832350"
  },
  {
    "text": "then go pulls data off of s3 or in some cases we can create many cases actually being queried directly against s3 and do",
    "start": "832350",
    "end": "839520"
  },
  {
    "text": "the processing so we can go up and go through that validation process I talked about that ETL transform process the",
    "start": "839520",
    "end": "846300"
  },
  {
    "text": "model detection process in each case the inputs read from the catalog and s3 and write back to there as a result we can",
    "start": "846300",
    "end": "853800"
  },
  {
    "text": "even run an interactive analytics against the data for a lot of our use case we can keep the data on s3 and",
    "start": "853800",
    "end": "859830"
  },
  {
    "text": "using open source query technologies like prosthetic spark we can query directly against the data on s3 without",
    "start": "859830",
    "end": "865380"
  },
  {
    "text": "having to do ETL out of there in some limited cases we do ETL for performant",
    "start": "865380",
    "end": "870570"
  },
  {
    "text": "query for certain types of query but for a majority of our use cases we can directly query against the data on s3",
    "start": "870570",
    "end": "876510"
  },
  {
    "text": "which is one of the big advantages of the data Lake architecture the other thing about this is by isolating these",
    "start": "876510",
    "end": "883020"
  },
  {
    "text": "different functional planes we're able to both scale out if there's a performance there's increased load that",
    "start": "883020",
    "end": "889080"
  },
  {
    "text": "might impact our ETL processing there's no impact on our automated detection routines or detection models or our",
    "start": "889080",
    "end": "896220"
  },
  {
    "text": "validation additionally makes it really easy to swap in additional different toolsets of the different technologies",
    "start": "896220",
    "end": "902280"
  },
  {
    "text": "evolved in the different areas we can still integrate them in to one hole so",
    "start": "902280",
    "end": "909870"
  },
  {
    "text": "kind of going through these areas in a little bit more detail I'm just going to briefly mention our ETL processing so we",
    "start": "909870",
    "end": "916020"
  },
  {
    "text": "have something we call the ETL framework which is a tool that lets our developers write ETL we primarily do it in in",
    "start": "916020",
    "end": "922680"
  },
  {
    "text": "sequel the language right now for our ETL and then they can go ahead and deploy that as part of a pipeline that can get",
    "start": "922680",
    "end": "929920"
  },
  {
    "text": "triggered based on a schedule or get trigger based on events coming out of our data catalog so new data arrives",
    "start": "929920",
    "end": "936639"
  },
  {
    "text": "gets registered can trigger off one or more ETL events associated with that it's also able to read interact with the",
    "start": "936639",
    "end": "944050"
  },
  {
    "text": "metadata in the catalog to go find out on s3 and instantiate the DDL necessary",
    "start": "944050",
    "end": "949149"
  },
  {
    "text": "to go out and do the processing against all that data I think people that watch",
    "start": "949149",
    "end": "955540"
  },
  {
    "text": "Amazon know there's a there's a new product that's available called glue you know we're currently evaluating it you",
    "start": "955540",
    "end": "962230"
  },
  {
    "text": "know it's still missing some features that we're looking for but you know it's along the same lines of the processing that we do at the capabilities it would",
    "start": "962230",
    "end": "969220"
  },
  {
    "text": "exist in a service like that so physically from an implementation",
    "start": "969220",
    "end": "975880"
  },
  {
    "text": "perspective how does the ETL work we have a separate EMR cluster elastic MapReduce cluster for each process that",
    "start": "975880",
    "end": "984009"
  },
  {
    "text": "we run so when we need to trigger an ETL we go and we spin up an EMR cluster as",
    "start": "984009",
    "end": "989800"
  },
  {
    "text": "part of the the bootstrapping process or the startup process of that cluster it will go out it'll read interrogate our",
    "start": "989800",
    "end": "996310"
  },
  {
    "text": "catalog find the information it needs to do its processing go out read that data from s3 again in many cases we can just",
    "start": "996310",
    "end": "1003810"
  },
  {
    "text": "query directly against s3 it'll go through its workflow and then will you know terminate the cluster at the end of",
    "start": "1003810",
    "end": "1009510"
  },
  {
    "text": "that that workflow so a big advantage of that is that we can get extreme parallel",
    "start": "1009510",
    "end": "1015060"
  },
  {
    "text": "processing across across our work sets you know with a complete isolation between the different workloads so if",
    "start": "1015060",
    "end": "1022019"
  },
  {
    "text": "there's increased demand one particular of ETL processing and doesn't impact any of our other ETL processing we're able",
    "start": "1022019",
    "end": "1029339"
  },
  {
    "text": "to do that to live you know all these clusters spinning up and terminating on a regular basis we're able to do that",
    "start": "1029339",
    "end": "1035069"
  },
  {
    "text": "because of spot pricing you know we can pay pennies on the dollar saving anywhere between 90 you know 70 to 90",
    "start": "1035069",
    "end": "1042120"
  },
  {
    "text": "percent on the dollar through the use of our spot pricing pretty much the majority of our workloads that we run our batch workloads all run on spot",
    "start": "1042120",
    "end": "1050299"
  },
  {
    "text": "we've also had increased savings from the recent introduction of per second billing because a lot of our workloads",
    "start": "1050299",
    "end": "1055770"
  },
  {
    "text": "also will spend and they'll run for for less than an hour so we're getting additional savings now from from that too as I mentioned a",
    "start": "1055770",
    "end": "1064660"
  },
  {
    "text": "lot of her ETL or the majority of are details written in hive started written in sequel we've been using hives go",
    "start": "1064660",
    "end": "1069730"
  },
  {
    "text": "language when we originally started going to the cloud many years ago we're now moving to using spark for better",
    "start": "1069730",
    "end": "1075790"
  },
  {
    "text": "performance and some of the language features that the spark platform supports as part of that as we go",
    "start": "1075790",
    "end": "1081040"
  },
  {
    "text": "forward on the portfolio so it really is",
    "start": "1081040",
    "end": "1087100"
  },
  {
    "text": "truly dynamic processing by having all these different workloads a separate EMR clusters we can spin up to tens of",
    "start": "1087100",
    "end": "1094299"
  },
  {
    "text": "thousands of ec2 nodes a day and EMR at peak and then we can go back down to just a few hundred when the need goes",
    "start": "1094299",
    "end": "1100690"
  },
  {
    "text": "away based on processing that lets us respond to events like you know market",
    "start": "1100690",
    "end": "1106360"
  },
  {
    "text": "fluctuations and the market data we need to process where fluctuations can increase you know volume can increase",
    "start": "1106360",
    "end": "1112299"
  },
  {
    "text": "300% day over day it also lets us respond on an unanticipated business",
    "start": "1112299",
    "end": "1118030"
  },
  {
    "text": "demand also we can react quickly to it so again it's a truly dynamic environment servers coming up servers",
    "start": "1118030",
    "end": "1124000"
  },
  {
    "text": "going down on a regular basis adjusting just to the demand and capacity that we need in a given point in time so moving",
    "start": "1124000",
    "end": "1135340"
  },
  {
    "text": "from on moving on from ETL processing to interactive analytics and this is actually and something that's very",
    "start": "1135340",
    "end": "1140679"
  },
  {
    "text": "powerful on the data Lake given all the data that's out there on s3 using the",
    "start": "1140679",
    "end": "1148809"
  },
  {
    "text": "right architecture as possible to go do interactive query against all that data",
    "start": "1148809",
    "end": "1153840"
  },
  {
    "text": "so the way that we do that typically is we have we run presto which is an",
    "start": "1153840",
    "end": "1160450"
  },
  {
    "text": "open-source sequel query engine we run it on top of EMR and technologies like",
    "start": "1160450",
    "end": "1165940"
  },
  {
    "text": "presto and also like spark have the ability to define data files on s3 is",
    "start": "1165940",
    "end": "1171549"
  },
  {
    "text": "something called external tables for all the database people out there and that lets you directly query against the data",
    "start": "1171549",
    "end": "1177880"
  },
  {
    "text": "while keeping it out on s3 you don't have to do an ETL process to load it up into an area where you can go and query against it we're able to configure do",
    "start": "1177880",
    "end": "1186220"
  },
  {
    "text": "that by populating something called the hive meta story which is a catalog an open-source",
    "start": "1186220",
    "end": "1191240"
  },
  {
    "text": "catalog for defining all the data objects the tools like presto and spark can use and we do that by populating",
    "start": "1191240",
    "end": "1198710"
  },
  {
    "text": "events from our herd data catalog so it was new data sets get registered we replicate the the information over to a",
    "start": "1198710",
    "end": "1204830"
  },
  {
    "text": "meta story catalog that can easily integrate in with presto and spark out-of-the-box",
    "start": "1204830",
    "end": "1210190"
  },
  {
    "text": "so given this now our analysts are data scientists they can use thick clients",
    "start": "1210190",
    "end": "1215780"
  },
  {
    "text": "any sort of JDBC or ODBC client and they can go ahead and now do exploratory ad hoc queries against the data that's out",
    "start": "1215780",
    "end": "1222620"
  },
  {
    "text": "on s3 they can do it securely we're able to integrate in with our corporate LDAP for authentication we're able to use the",
    "start": "1222620",
    "end": "1229910"
  },
  {
    "text": "hive meta store for authorization control again integrated in from information from our Active Directory",
    "start": "1229910",
    "end": "1235490"
  },
  {
    "text": "and we're able to encrypt the entire all the channels going backwards and forwards from the data out on s3 using",
    "start": "1235490",
    "end": "1242930"
  },
  {
    "text": "SSC and kms a server-side encryption and key management system encryption all the",
    "start": "1242930",
    "end": "1248000"
  },
  {
    "text": "way throughout to the out to the client and so again this really becomes powerful because you know in our case",
    "start": "1248000",
    "end": "1254510"
  },
  {
    "text": "one of our production stores we have four petabytes of data out there in the cloud it's all available now and",
    "start": "1254510",
    "end": "1261470"
  },
  {
    "text": "analysts can start querying it within seconds and get results back in seconds and that holds true for four petabytes",
    "start": "1261470",
    "end": "1267530"
  },
  {
    "text": "that would also hold true for 10 petabytes 100 petabytes as the data lake grows your ability to go in and access",
    "start": "1267530",
    "end": "1273920"
  },
  {
    "text": "any portion of it and start querying on it instantly without having you an ETL process still remain so that's really a",
    "start": "1273920",
    "end": "1279710"
  },
  {
    "text": "powerful thing from from the data Lake architecture for interactive analytics",
    "start": "1279710",
    "end": "1285669"
  },
  {
    "text": "and there's a lot of different things in this table here just to kind of go through one is capturing over on the the",
    "start": "1287170",
    "end": "1295310"
  },
  {
    "text": "right hand column we store all our data sets in sort of a canonical text format",
    "start": "1295310",
    "end": "1300610"
  },
  {
    "text": "where we keep the data in a format we can use any sort of tool to come and",
    "start": "1300610",
    "end": "1305720"
  },
  {
    "text": "query against because pretty much everything speaks text but we also make a copy and a perform a query format or C",
    "start": "1305720",
    "end": "1311870"
  },
  {
    "text": "or ork format for use by our query tools like presto and lexpark and as you can",
    "start": "1311870",
    "end": "1317810"
  },
  {
    "text": "see you know if you move to a format like Oh our C or work you can get extremely fast performance for a lot of",
    "start": "1317810",
    "end": "1324530"
  },
  {
    "text": "queries we were able to run queries against billions of rows of data you know 2 billion rows of data on s3 and",
    "start": "1324530",
    "end": "1330470"
  },
  {
    "text": "get answers back in seconds also you",
    "start": "1330470",
    "end": "1335660"
  },
  {
    "text": "know we working with presto for about two years now and the performance has improved a lot over",
    "start": "1335660",
    "end": "1341000"
  },
  {
    "text": "those two years it's good in our case we have a lot of queries that are just filtering type queries but even a lot of",
    "start": "1341000",
    "end": "1346820"
  },
  {
    "text": "the queries that we had before around joins and also some windowing functions where we had to move it to another",
    "start": "1346820",
    "end": "1352400"
  },
  {
    "text": "technology like a redshift to get performant Koryo processing we can now directly just use pressed Orion against",
    "start": "1352400",
    "end": "1358220"
  },
  {
    "text": "s3 for our use cases and get performant query and again because all the data is",
    "start": "1358220",
    "end": "1367790"
  },
  {
    "text": "on s3 it's not just a question of you know having to have one application that",
    "start": "1367790",
    "end": "1374300"
  },
  {
    "text": "gets used to go out and query this if we want to we can go spin up additional clusters all running against the same data set on s3 whether that's for",
    "start": "1374300",
    "end": "1381650"
  },
  {
    "text": "workload isolation whether that's for additional capacity and so we have the ability if user demand exceeds",
    "start": "1381650",
    "end": "1388100"
  },
  {
    "text": "expectations on our primary cluster we can just go spin up another we have more capacity for our users to use in fact",
    "start": "1388100",
    "end": "1394910"
  },
  {
    "text": "what we're doing today is we're now not just using this for exploratory ad-hoc queries for users but we're actually",
    "start": "1394910",
    "end": "1401510"
  },
  {
    "text": "building applications on top of this approach so we have applications or using presto an EMR wearing sequel",
    "start": "1401510",
    "end": "1407690"
  },
  {
    "text": "against that as a back-end keeping the data out on s3 in fact it's a whole it's",
    "start": "1407690",
    "end": "1414950"
  },
  {
    "text": "a whole set of applications that we've built to do this everything from applications to let user",
    "start": "1414950",
    "end": "1420560"
  },
  {
    "text": "self provision data to do exploration on datasets to visualize depth of market at a given",
    "start": "1420560",
    "end": "1427480"
  },
  {
    "text": "point in time and also we've even integrated in with our case management system so in in the majority of these",
    "start": "1427480",
    "end": "1434170"
  },
  {
    "text": "cases were in a situation where we don't have separate analytic databases for these the datasets that sit down and s3",
    "start": "1434170",
    "end": "1441450"
  },
  {
    "text": "we leverage presto on the front as a query technology that we can go ahead and query on the backend I said we",
    "start": "1441450",
    "end": "1448810"
  },
  {
    "text": "leveraged presto for most of them that's not true in all the cases we still use redshift as part of our diver",
    "start": "1448810",
    "end": "1454770"
  },
  {
    "text": "application which lets people provision and explore data Mart's for some certain query performance scenarios we also use",
    "start": "1454770",
    "end": "1463530"
  },
  {
    "text": "HBase technology for something called our fast order lifecycle application",
    "start": "1463530",
    "end": "1469150"
  },
  {
    "text": "which lets users go ahead and view the trade lifecycle of any given trade or",
    "start": "1469150",
    "end": "1474160"
  },
  {
    "text": "set of trades in the market so this is really big data it's 700 terabytes of data in the application we keep it",
    "start": "1474160",
    "end": "1481960"
  },
  {
    "text": "although on s3 because we run it on EMR EMR has the ability to run HBase and",
    "start": "1481960",
    "end": "1488230"
  },
  {
    "text": "keep the data sets on s3 which lets us continue to use the data like architecture for that for significant",
    "start": "1488230",
    "end": "1493960"
  },
  {
    "text": "cost savings and operational benefits over the way we did it before which is to run all that data locally on a",
    "start": "1493960",
    "end": "1499960"
  },
  {
    "text": "cluster so again a lot of operational costs and management benefits from running the data leak approach and it's",
    "start": "1499960",
    "end": "1506050"
  },
  {
    "text": "not just one application it's not just ad hoc query it's a whole suite of business applications that we're",
    "start": "1506050",
    "end": "1511540"
  },
  {
    "text": "building on top of this this design",
    "start": "1511540",
    "end": "1516150"
  },
  {
    "text": "and it's not just user applications data science is I mentioned before we have a",
    "start": "1519320",
    "end": "1524690"
  },
  {
    "text": "whole team of data scientists that work on developing new models and new automated detection routines and a",
    "start": "1524690",
    "end": "1531290"
  },
  {
    "text": "variety of other work the data science work going on at FINRA and again the data lake lends itself to that too the",
    "start": "1531290",
    "end": "1537740"
  },
  {
    "text": "data sets are out there they're all available on s3 they're integrated in and searchable and find it searchable",
    "start": "1537740",
    "end": "1543980"
  },
  {
    "text": "through our catalog so the data science community can go out and use their catalog to go find the data sets once",
    "start": "1543980",
    "end": "1550550"
  },
  {
    "text": "they've found the data sets they can on their own through self-service provision an environment where they can go and",
    "start": "1550550",
    "end": "1555980"
  },
  {
    "text": "start doing processing now and get working very quickly against those data sets without having to wait for",
    "start": "1555980",
    "end": "1561980"
  },
  {
    "text": "technology staff to go find the data retrieve the data provision and",
    "start": "1561980",
    "end": "1568070"
  },
  {
    "text": "environment for them to go to do their work which really accelerates the the pace of data science that we've been doing we have a couple different",
    "start": "1568070",
    "end": "1575300"
  },
  {
    "text": "platforms that we use on the data Lake for data science you know just at a basic level that ad hoc query capability",
    "start": "1575300",
    "end": "1582080"
  },
  {
    "text": "on presto can be used for data profiling to some degree of data engineering we",
    "start": "1582080",
    "end": "1588050"
  },
  {
    "text": "also at the bottom of the down towards the there's a application called the UDS",
    "start": "1588050",
    "end": "1594530"
  },
  {
    "text": "P Universal data science platform and what that is is that's an ec2 box",
    "start": "1594530",
    "end": "1599600"
  },
  {
    "text": "pre-configured with a set of diet data science tools so there are data science community through a web application can",
    "start": "1599600",
    "end": "1606290"
  },
  {
    "text": "go out and provision an environment with all their software stack on them it can be everything from a small machine you",
    "start": "1606290",
    "end": "1612560"
  },
  {
    "text": "know all the way up to a really big r-class machine be very easy to make it an X 1 class machine where they can go",
    "start": "1612560",
    "end": "1618650"
  },
  {
    "text": "out and they can do single machine processing for their data sets and this is really good for data sets that go up",
    "start": "1618650",
    "end": "1624590"
  },
  {
    "text": "into the you know tens of millions of in the few millions of rows of data we also",
    "start": "1624590",
    "end": "1631610"
  },
  {
    "text": "use SPARC running on the data breaks platform with interactive notebooks to",
    "start": "1631610",
    "end": "1637280"
  },
  {
    "text": "do processing using machine learning libraries to process bigger data sets so",
    "start": "1637280",
    "end": "1642620"
  },
  {
    "text": "this is when you're up into the range of billions of billions data and we do that processing but all these tools can be brought to bear it's",
    "start": "1642620",
    "end": "1649050"
  },
  {
    "text": "very easy to bring new tools to bear again all working against the same data sets in the same catalogue on the back",
    "start": "1649050",
    "end": "1655470"
  },
  {
    "text": "end so I mentioned how data you know the",
    "start": "1655470",
    "end": "1664530"
  },
  {
    "text": "data scientists they developed that's the detection models spend a little bit of time talking about the detection",
    "start": "1664530",
    "end": "1671070"
  },
  {
    "text": "models and kind of how we deploy and use them an example is our cross market",
    "start": "1671070",
    "end": "1676830"
  },
  {
    "text": "surveillance model that's something fairly unique that we do because we're in a position to bring in data from the",
    "start": "1676830",
    "end": "1682380"
  },
  {
    "text": "different exchanges and also through our own order reporting system a trade order reporting system called out so we bring",
    "start": "1682380",
    "end": "1688710"
  },
  {
    "text": "in data from those sources from trade reporting facilities from the IHG audit",
    "start": "1688710",
    "end": "1694410"
  },
  {
    "text": "trail and we went through a normalization process so that we can now trace trade activity across all these",
    "start": "1694410",
    "end": "1700170"
  },
  {
    "text": "different market centers and the firms that they're out there and this becomes",
    "start": "1700170",
    "end": "1705900"
  },
  {
    "text": "the basis that we use to drive our automated surveillance models or detection models additionally given that",
    "start": "1705900",
    "end": "1712650"
  },
  {
    "text": "data set as I mentioned before there's interactive exploration of the data using tools like that diver tool for",
    "start": "1712650",
    "end": "1718650"
  },
  {
    "text": "people the self-serve explore the data sets and also that depth of market tool those can leverage the same data sets on",
    "start": "1718650",
    "end": "1724410"
  },
  {
    "text": "the back end and s3 to go do their analysis implementation wise the",
    "start": "1724410",
    "end": "1733280"
  },
  {
    "text": "processing of our detection models is very much like our ETL every detection",
    "start": "1733280",
    "end": "1738510"
  },
  {
    "text": "model runs as its own EMR cluster it gets created it does its processing and then it terminates loading data from s3",
    "start": "1738510",
    "end": "1745830"
  },
  {
    "text": "and registering it back in the catalog and writing it to s3 when it's done or it can also go through and you know",
    "start": "1745830",
    "end": "1753090"
  },
  {
    "text": "again process directly against s3 in a lot of cases having to avoid that that expensive data load process",
    "start": "1753090",
    "end": "1759770"
  },
  {
    "text": "parallelization is still there we because we have this degree of parallelization we can run our workloads as may you know whatever workloads we",
    "start": "1760160",
    "end": "1767400"
  },
  {
    "text": "have we can just keep scaling out with no clusters to go and handle the processing that comes in handy a lot because we'll run into situations where",
    "start": "1767400",
    "end": "1773700"
  },
  {
    "text": "I talked about where we get those upstream data Corrections and we need to do reprocessing now in addition to running you know the",
    "start": "1773700",
    "end": "1779440"
  },
  {
    "text": "previous day's processing if we need to possibly process something from several weeks ago we can go ahead and we can do",
    "start": "1779440",
    "end": "1786399"
  },
  {
    "text": "the processing against that in parallel with no impact of on the SLA is of the current processing so in a case before",
    "start": "1786399",
    "end": "1794109"
  },
  {
    "text": "finding the capacity to be able to do that and respond to the the business request to do the reprocessing could",
    "start": "1794109",
    "end": "1800589"
  },
  {
    "text": "take several weeks to a month now we can do it in less than a day from a",
    "start": "1800589",
    "end": "1807279"
  },
  {
    "text": "technology standpoint similar to the ETL processing when we started out we were",
    "start": "1807279",
    "end": "1813070"
  },
  {
    "text": "using high of sequel on hive as the technology for doing the processing now",
    "start": "1813070",
    "end": "1820329"
  },
  {
    "text": "we're moving over to using spark where we're porting over our detection",
    "start": "1820329",
    "end": "1826179"
  },
  {
    "text": "routines from high over to spark because you know just running basically the same sequel on you know the same the same EMR",
    "start": "1826179",
    "end": "1833139"
  },
  {
    "text": "sized clusters we can see anywhere from a 60 to 90 percent speed-up in run time which is you know since we only run the",
    "start": "1833139",
    "end": "1840339"
  },
  {
    "text": "cluster as long as we need to that directly impacts the bottom line and improves our cost posture by by",
    "start": "1840339",
    "end": "1846309"
  },
  {
    "text": "switching to the faster technology but",
    "start": "1846309",
    "end": "1853179"
  },
  {
    "text": "for our detection models it's not just a question moving to spark just for increased performance when we started",
    "start": "1853179",
    "end": "1858909"
  },
  {
    "text": "you know back before we went to the cloud we had rules based detection models we were running them against a",
    "start": "1858909",
    "end": "1865269"
  },
  {
    "text": "variety of data sets in data warehouse appliances in Hadoop clusters on-premises and a big challenge we had",
    "start": "1865269",
    "end": "1872229"
  },
  {
    "text": "there was just getting the data to the right environment where we could actually do the processing and finding it moving it getting to the point we",
    "start": "1872229",
    "end": "1878859"
  },
  {
    "text": "could do processing dealing with our capacity challenges that was a real struggle that we had before before we",
    "start": "1878859",
    "end": "1885219"
  },
  {
    "text": "went to the cloud as part of the journey to the cloud we addressed those challenges with our centralized catalog",
    "start": "1885219",
    "end": "1891249"
  },
  {
    "text": "with used the data Lake we no longer were moving data we had no wouldn't have issues with finding data and we could",
    "start": "1891249",
    "end": "1897219"
  },
  {
    "text": "bring the capacity that we needed to bear against the data whenever we needed it but from a technology standpoint you",
    "start": "1897219",
    "end": "1904209"
  },
  {
    "text": "know we're still we're still doing this process where the detection models were written in sequel based off of",
    "start": "1904209",
    "end": "1910059"
  },
  {
    "text": "requirement that were develop ideas scientists who may have been using other tools and technologies to come up with a model and",
    "start": "1910059",
    "end": "1915690"
  },
  {
    "text": "then we had essentially mapped that to sort of a rules-based you know program implementation that would run in sequel",
    "start": "1915690",
    "end": "1922350"
  },
  {
    "text": "in the cloud what we're doing now is we're moving from a rules based detection regime to a machine learning",
    "start": "1922350",
    "end": "1930360"
  },
  {
    "text": "approach to our detection models where we're able instead of you know building models by data scientists that could",
    "start": "1930360",
    "end": "1937620"
  },
  {
    "text": "have to get recoded on a different technology and to set of requirements by developers and engineers we're able to",
    "start": "1937620",
    "end": "1945330"
  },
  {
    "text": "integrate that in into into one process so the benefits being you know we will smooth that that development pipeline",
    "start": "1945330",
    "end": "1951450"
  },
  {
    "text": "for machine learning for for detection model development and then we have",
    "start": "1951450",
    "end": "1956630"
  },
  {
    "text": "excuse me then we've also gone through and we have you know streamlined that process and",
    "start": "1956630",
    "end": "1963810"
  },
  {
    "text": "we've also made it possible so that we can detect and find events with those models we might not have found with a",
    "start": "1963810",
    "end": "1970110"
  },
  {
    "text": "rules-based approach",
    "start": "1970110",
    "end": "1972710"
  },
  {
    "text": "and the framework that we do it on is something we call our dynamic surveillance platform what that is",
    "start": "1977730",
    "end": "1984779"
  },
  {
    "text": "that's we we leverage as I mentioned before spark and the machine learning libraries in spark and now we run that",
    "start": "1984779",
    "end": "1992669"
  },
  {
    "text": "currently in spark on top of the the data bricks environment but we've integrated it in with our data catalog",
    "start": "1992669",
    "end": "1998309"
  },
  {
    "text": "or her data catalog and using that in the DNS 3 makes it very easy for data",
    "start": "1998309",
    "end": "2003559"
  },
  {
    "text": "scientists to again get working start developing finding the data sets that they need in our of interest bringing",
    "start": "2003559",
    "end": "2009830"
  },
  {
    "text": "them in profiling them doing their feature engineering and then doing their model development and testing you know",
    "start": "2009830",
    "end": "2016460"
  },
  {
    "text": "on a platform but we also augment those data scientists people with engineers",
    "start": "2016460",
    "end": "2022010"
  },
  {
    "text": "who can also help at the same time make sure we're building the models out for resiliency and performance and by doing",
    "start": "2022010",
    "end": "2028070"
  },
  {
    "text": "that we can now with the same technology stack promote that model as a notebook to our production environment and then",
    "start": "2028070",
    "end": "2036139"
  },
  {
    "text": "schedule it and run it you know as a batch mode they're a headless mode their response to events for our surveillance",
    "start": "2036139",
    "end": "2041929"
  },
  {
    "text": "so we've eliminated this kind of hard handoff between the data science community and their language in their",
    "start": "2041929",
    "end": "2047809"
  },
  {
    "text": "tool set and the production operate and the production you know model tool set",
    "start": "2047809",
    "end": "2053300"
  },
  {
    "text": "so that it's possible to get a seamless pipeline to go across the stacks so I've",
    "start": "2053300",
    "end": "2063138"
  },
  {
    "text": "talked about the the data Lake and the different components of an ETL you know automated model detection models",
    "start": "2063139",
    "end": "2068800"
  },
  {
    "text": "interactive analytics one it's been a little bit time talking about the kind",
    "start": "2068800",
    "end": "2074300"
  },
  {
    "text": "of the foundation that underpins our our data lake and we call it a cloud management platform or cloud cloud",
    "start": "2074300",
    "end": "2080628"
  },
  {
    "text": "management framework and it's the the set of environments processes and tools",
    "start": "2080629",
    "end": "2085638"
  },
  {
    "text": "that lets us build and operate secure and compliant applications on the cloud",
    "start": "2085639",
    "end": "2091099"
  },
  {
    "text": "and you know I want to talk about two things that are really important to us as a regulator security and compliance",
    "start": "2091099",
    "end": "2098260"
  },
  {
    "text": "so for security Amazon has a shared security model where they're responsible for the underlying infrastructure and",
    "start": "2098260",
    "end": "2106250"
  },
  {
    "text": "the security the services we finra is the customer responsible for config to meet our security security",
    "start": "2106250",
    "end": "2114000"
  },
  {
    "text": "requirements and you know as part of our dream of the cloud we made a pretty significant effort investment in time to",
    "start": "2114000",
    "end": "2120780"
  },
  {
    "text": "understand how to configure these tools properly to meet our objectives and so you know we have we've achieved it were",
    "start": "2120780",
    "end": "2127230"
  },
  {
    "text": "able to achieve isolation of the different environments and a variety of different ways through VP C's to isolate",
    "start": "2127230",
    "end": "2133890"
  },
  {
    "text": "the network's security groups due to a host level isolation we're able to",
    "start": "2133890",
    "end": "2139200"
  },
  {
    "text": "isolate within our accounts the the capabilities the individual services through the use of IM roles associated",
    "start": "2139200",
    "end": "2145860"
  },
  {
    "text": "with each application we have encryption of all the data at rest and in transit and we have really leveraged amazon's",
    "start": "2145860",
    "end": "2153480"
  },
  {
    "text": "key management system or kms that you know we found integrates very well with a lot of the amazon services so that",
    "start": "2153480",
    "end": "2160950"
  },
  {
    "text": "it's very easy to achieve a breadth of encryption that's this much greater and easier to implement than we could ever",
    "start": "2160950",
    "end": "2166530"
  },
  {
    "text": "do before on prem for authentication we're able to integrate in I am with our",
    "start": "2166530",
    "end": "2174210"
  },
  {
    "text": "internal corporate Active Directory so we don't have to manage individual keys for the different users that go in we're",
    "start": "2174210",
    "end": "2181290"
  },
  {
    "text": "able to manage all our users still using our native Active Directory tools and our corporate Active Directory and we",
    "start": "2181290",
    "end": "2187260"
  },
  {
    "text": "can federate the logon then to access AWS services not just the console with the different services that our",
    "start": "2187260",
    "end": "2193080"
  },
  {
    "text": "development community needs to go needs to go on access so it makes a lot easier",
    "start": "2193080",
    "end": "2198120"
  },
  {
    "text": "from a management perspective and then from a monitoring perspective for our",
    "start": "2198120",
    "end": "2203160"
  },
  {
    "text": "security analysts to do their seam reporting we record all the infrastructure events change events we",
    "start": "2203160",
    "end": "2208980"
  },
  {
    "text": "record it and we have Splunk that we use to go ahead to build dashboards and alerts off of that that can drive that",
    "start": "2208980",
    "end": "2215040"
  },
  {
    "text": "you know bring an analyst security analysts to come in and go take a look but there's an event that requires exploration there's security and then",
    "start": "2215040",
    "end": "2225570"
  },
  {
    "text": "there's compliance so we have a team of system architects and security staff",
    "start": "2225570",
    "end": "2232080"
  },
  {
    "text": "that define the policies in our data center before in the on-premise world",
    "start": "2232080",
    "end": "2237360"
  },
  {
    "text": "those policies would typically be paper documents that administrators that would then have",
    "start": "2237360",
    "end": "2242579"
  },
  {
    "text": "to go follow to make sure that we were actually you know doing what we said we should be doing and the cloud is",
    "start": "2242579",
    "end": "2248970"
  },
  {
    "text": "providing us an opportunity to transform that to you know enforce this all through automation and you know",
    "start": "2248970",
    "end": "2256019"
  },
  {
    "text": "programmatic met approaches so we've developed a series of tools that lets",
    "start": "2256019",
    "end": "2261779"
  },
  {
    "text": "our development community basically ensure if they're gonna go out and build an application stack that application stack is going to built to our",
    "start": "2261779",
    "end": "2268220"
  },
  {
    "text": "requirements or compliance requirements we also have tools that make sure for security groups they're properly",
    "start": "2268220",
    "end": "2274019"
  },
  {
    "text": "leveraging the security groups that are defined and approved by our security team in the building and design of their",
    "start": "2274019",
    "end": "2281069"
  },
  {
    "text": "application and same thing for there's a similar tool for I am groups too or I am",
    "start": "2281069",
    "end": "2286170"
  },
  {
    "text": "roles and then a variety of other other applications that we built to this whole suite of tools is available to our",
    "start": "2286170",
    "end": "2292380"
  },
  {
    "text": "developer community who can then use this in a self-service basis to go out and very quickly get up and running and",
    "start": "2292380",
    "end": "2298170"
  },
  {
    "text": "building applications in a compliant way in the cloud we give those development",
    "start": "2298170",
    "end": "2304230"
  },
  {
    "text": "teams full access to an approved menu of services in our development environment",
    "start": "2304230",
    "end": "2310069"
  },
  {
    "text": "from there they can go and build their application but if they promote up to higher level higher level environments",
    "start": "2310069",
    "end": "2316259"
  },
  {
    "text": "test or QC or production those all have to be done via automation so you know",
    "start": "2316259",
    "end": "2322259"
  },
  {
    "text": "there's no there's no code or applications to go to production unless",
    "start": "2322259",
    "end": "2327390"
  },
  {
    "text": "it gets there through an automated process an automated deployment pipeline and you know there's a couple different",
    "start": "2327390",
    "end": "2335099"
  },
  {
    "text": "advantages from that one is obviously through automation everything's going to be consistent as this going out and",
    "start": "2335099",
    "end": "2340470"
  },
  {
    "text": "getting configured and deployed the other from a compliance perspective that's attractive is that we can really reduce the number of people that have",
    "start": "2340470",
    "end": "2347849"
  },
  {
    "text": "access to that production environment whereas before in our own data center we might have dozens of people system",
    "start": "2347849",
    "end": "2353789"
  },
  {
    "text": "administrators database administrators that would need to have access to that environment now because we use automation we're able to limit that to a",
    "start": "2353789",
    "end": "2360299"
  },
  {
    "text": "few a small team of release management staff and operation staff occasionally",
    "start": "2360299",
    "end": "2367289"
  },
  {
    "text": "we do have need you know sometimes there's things that you can't go ahead and invest I should add the other key to making",
    "start": "2367289",
    "end": "2373269"
  },
  {
    "text": "that work is we take all the log data and event data and we put that off in two locations where we can go access it",
    "start": "2373269",
    "end": "2380169"
  },
  {
    "text": "from Splunk so we'll put it off to s 3 we'll put it off to another source we can go investigate and explore it",
    "start": "2380169",
    "end": "2385179"
  },
  {
    "text": "through Splunk we've also built in a lot of self-healing out capabilities to our applications so in a lot of cases if",
    "start": "2385179",
    "end": "2391390"
  },
  {
    "text": "there's a Fault in production the easiest solution is to just you know kill the instance and restart it rather",
    "start": "2391390",
    "end": "2396939"
  },
  {
    "text": "than go ahead and try to troubleshoot like an active live instances out there in some cases we do need to put people",
    "start": "2396939",
    "end": "2402699"
  },
  {
    "text": "in to production to go get on a host to go look at something again we build a series of tools that lets them do that",
    "start": "2402699",
    "end": "2408519"
  },
  {
    "text": "in a temporary basis with a full audit log so we have a great much greater degree of traceability to the exact",
    "start": "2408519",
    "end": "2414669"
  },
  {
    "text": "events and actions or going in production as I said you know we kind of",
    "start": "2414669",
    "end": "2419679"
  },
  {
    "text": "omit these events to it to a series of logs they're out there that are searchable and Splore bolt through Splunk between a series of Splunk",
    "start": "2419679",
    "end": "2425739"
  },
  {
    "text": "reports and compliance reports we've developed ourselves we're able to do a couple different things we're able to",
    "start": "2425739",
    "end": "2432130"
  },
  {
    "text": "build scorecards to show back to our compliance staff the the security staff",
    "start": "2432130",
    "end": "2437559"
  },
  {
    "text": "the enterprise architects and how the applications are doing and that's all some information that we can share with",
    "start": "2437559",
    "end": "2442569"
  },
  {
    "text": "our auditors when they come in to audit our application portfolio and we've been",
    "start": "2442569",
    "end": "2449319"
  },
  {
    "text": "in the cloud now for four three years as I said in a production environment with multiple applications therefore for a",
    "start": "2449319",
    "end": "2456249"
  },
  {
    "text": "year and a half so we've we've experienced a lot of audits in the cloud going going to the cloud it requires",
    "start": "2456249",
    "end": "2461349"
  },
  {
    "text": "sometimes we found a bit of a conversation change with the auditors an example is in the on-premise audits that",
    "start": "2461349",
    "end": "2467559"
  },
  {
    "text": "existed before there was a particular control that we had to maintain a server inventory for each application that was",
    "start": "2467559",
    "end": "2472929"
  },
  {
    "text": "reviewed and approved by the the project manager for that particular application well you know we go in and we have the",
    "start": "2472929",
    "end": "2479169"
  },
  {
    "text": "audit in the cloud we don't have anything like that because the servers are dynamically being created and destroyed you know tens of thousands a",
    "start": "2479169",
    "end": "2486159"
  },
  {
    "text": "day right so what does that really mean that required a conversation with the auditors to say well what's the what's",
    "start": "2486159",
    "end": "2491349"
  },
  {
    "text": "the real risk here we understand the control what's the risk so we go back to the risk of them which is obviously that",
    "start": "2491349",
    "end": "2497019"
  },
  {
    "text": "if you don't have a process to track your server assets where your data is you might actually lose you know inadvertently lose that",
    "start": "2497019",
    "end": "2502750"
  },
  {
    "text": "risk of losing that data through not properly managing that process so the conversation with the auditors is that",
    "start": "2502750",
    "end": "2508450"
  },
  {
    "text": "well in our architecture the data is on s3 we have a series of controls and mechanisms in place to protect control",
    "start": "2508450",
    "end": "2514810"
  },
  {
    "text": "access to data there we need to rewrite the control to reflect you know how the how the risk would manifest itself in",
    "start": "2514810",
    "end": "2520960"
  },
  {
    "text": "the cloud and we're able to have that conversation now because we've been through several rounds of that with our auditors so again just another example",
    "start": "2520960",
    "end": "2527170"
  },
  {
    "text": "of experience that we've been able to build by working up in the cloud for for several years in a production capacity",
    "start": "2527170",
    "end": "2534450"
  },
  {
    "text": "so kind of going back to the data lake overall and benefits of it that we've",
    "start": "2536820",
    "end": "2542320"
  },
  {
    "text": "seen the data like architecture has really led us go ahead and get a handle",
    "start": "2542320",
    "end": "2549160"
  },
  {
    "text": "on our data through data management we can now you know find and explore data and make and find out where it is we can",
    "start": "2549160",
    "end": "2556450"
  },
  {
    "text": "explore it we can make it accessible all throughout its its data lifecycle because of the scaling capacities the",
    "start": "2556450",
    "end": "2563260"
  },
  {
    "text": "scaling ability of the cloud we're now able to respond to things like market fluctuations or unexpected business",
    "start": "2563260",
    "end": "2570130"
  },
  {
    "text": "demand it's really it's a non-event now which used to require a lot of people to go out and respond you know kind of",
    "start": "2570130",
    "end": "2576310"
  },
  {
    "text": "emergency mode the systems are all now designed just automatically scale and deal with that for reporting",
    "start": "2576310",
    "end": "2583270"
  },
  {
    "text": "investigation we can now respond much quicker to regulatory reports reporting",
    "start": "2583270",
    "end": "2588280"
  },
  {
    "text": "requests because we have a less much greater capability amount of data that we can keep online as part of our as",
    "start": "2588280",
    "end": "2596020"
  },
  {
    "text": "part of our architecture and we can add the analysts can go ahead and now and ask bigger questions of the data because",
    "start": "2596020",
    "end": "2602740"
  },
  {
    "text": "of the increased capacity of the architecture so before where we could only they could only deal with",
    "start": "2602740",
    "end": "2608380"
  },
  {
    "text": "potentially a hundred thousand rows of data to do their exploration and interaction they can now deal with tens",
    "start": "2608380",
    "end": "2615550"
  },
  {
    "text": "of millions of rows of data or even 100 million rows of data for the data science community the existence of that",
    "start": "2615550",
    "end": "2622450"
  },
  {
    "text": "catalog and the ability to self provision environments has accelerated data science in the organization making",
    "start": "2622450",
    "end": "2627640"
  },
  {
    "text": "it much easier to do and as I said we've gone through we've collapsed all these",
    "start": "2627640",
    "end": "2632830"
  },
  {
    "text": "different silos of information into a single the data that makes it easy to go out and access and find done this all at a",
    "start": "2632830",
    "end": "2641569"
  },
  {
    "text": "run rate that's you know total cost of ownership is 30% less than our on-premise environment before while",
    "start": "2641569",
    "end": "2649190"
  },
  {
    "text": "meeting the security and regulatory requirements that we have as a regulator",
    "start": "2649190",
    "end": "2654999"
  },
  {
    "text": "security compliance requirements that we have I don't know if the we also have",
    "start": "2654999",
    "end": "2663410"
  },
  {
    "text": "here at FINRA is part of AWS this is one of five presentations the total so",
    "start": "2663410",
    "end": "2668449"
  },
  {
    "text": "there's four others that will be going on at reinvent this week just as an example of an indication of the",
    "start": "2668449",
    "end": "2674630"
  },
  {
    "text": "experience that you know FINRA is an organization has built up across a wide variety of people with building and",
    "start": "2674630",
    "end": "2680809"
  },
  {
    "text": "operating and production capacity production workloads at scale in the cloud so you know for my portion that",
    "start": "2680809",
    "end": "2687170"
  },
  {
    "text": "would just leave it at the fact that if you're in the financial services area or",
    "start": "2687170",
    "end": "2692239"
  },
  {
    "text": "in the regulatory area and you know you're starting off on your journey to the cloud or you're in the process of",
    "start": "2692239",
    "end": "2698119"
  },
  {
    "text": "your journey of the cloud and you're looking to accelerate and make it go faster we've built up a wealth of",
    "start": "2698119",
    "end": "2703219"
  },
  {
    "text": "experience that we're happy to share with others in the industry others in the community to see if you know we can",
    "start": "2703219",
    "end": "2709130"
  },
  {
    "text": "help you know provide a way to learn from us and see if there's ways that you can accelerate your journey to the cloud",
    "start": "2709130",
    "end": "2714799"
  },
  {
    "text": "and with that I will turn it back over to Robert so we have a couple of mics",
    "start": "2714799",
    "end": "2721519"
  },
  {
    "text": "set up so if anyone has any questions please feel free to head to one of the mics",
    "start": "2721519",
    "end": "2728440"
  },
  {
    "text": "can you actually go to that one of the mics so there yeah because I'm recording it in the back thanks oh it's a question",
    "start": "2733240",
    "end": "2745220"
  },
  {
    "text": "um one of your pillars I noticed wasn't data resiliency resiliency from what I",
    "start": "2745220",
    "end": "2753650"
  },
  {
    "text": "might call an availability or disaster recovery point of view now I'll focus",
    "start": "2753650",
    "end": "2759530"
  },
  {
    "text": "the question a little bit more I mean given the benefit a little bit of time s3 can provide us availability and data",
    "start": "2759530",
    "end": "2766850"
  },
  {
    "text": "resiliency I guess my question would be in the in the ingress portions where",
    "start": "2766850",
    "end": "2772100"
  },
  {
    "text": "you're getting those feeds from the different sources where you're getting feeds from did you meet any particular challenge in sort of safe storing and",
    "start": "2772100",
    "end": "2779810"
  },
  {
    "text": "creating availability and resiliency of data at the ingress point at the ingress",
    "start": "2779810",
    "end": "2786020"
  },
  {
    "text": "point you mean before it goes as three or what we might call you know typically",
    "start": "2786020",
    "end": "2792340"
  },
  {
    "text": "bringing data into a system you you safe store it things like synchronous storage",
    "start": "2792340",
    "end": "2802760"
  },
  {
    "text": "between different data data centers right right I'm so so in that ingress point trying to get it so you can save",
    "start": "2802760",
    "end": "2809150"
  },
  {
    "text": "store your data that's coming in from from your data sources yeah so so again in a lot of our a lot of our use cases",
    "start": "2809150",
    "end": "2815630"
  },
  {
    "text": "were able to write data data directly to s3 and so by writing it to s3 you know",
    "start": "2815630",
    "end": "2821990"
  },
  {
    "text": "from the client and you persist it to s3 as the service and then as three and that's again one of the huge advantages",
    "start": "2821990",
    "end": "2827890"
  },
  {
    "text": "under the covers its handling that automatic replication across multiple data centers within a region so you're",
    "start": "2827890",
    "end": "2834890"
  },
  {
    "text": "getting that high availability that high resiliency redundancy as soon as you're writing the bytes to the service",
    "start": "2834890",
    "end": "2840440"
  },
  {
    "text": "basically that's there for some of our legacy feeds you know we still bring them in through other technologies like SFTP and there we just have to run a",
    "start": "2840440",
    "end": "2847100"
  },
  {
    "text": "farm of servers basically but again it's the same thing it's essentially a stream of data coming in temporarily out on",
    "start": "2847100",
    "end": "2853250"
  },
  {
    "text": "that infrastructure with local disk very quickly rushing to persist it back to s3 where we get that resilient the multi",
    "start": "2853250",
    "end": "2859460"
  },
  {
    "text": "datacenter resiliency built into it we don't have to go through all that engineering work we're able to just basically write the client",
    "start": "2859460",
    "end": "2865400"
  },
  {
    "text": "and the rest of his taken care of after that on a regional basis but what about",
    "start": "2865400",
    "end": "2874580"
  },
  {
    "text": "across regional basis for our yeah so it's not cross region it's within a",
    "start": "2874580",
    "end": "2879830"
  },
  {
    "text": "region but there's multiple data centers within a region and there's isolation between those data centers that we've",
    "start": "2879830",
    "end": "2885740"
  },
  {
    "text": "gone through and you know for our requirements you know the availability that we can get through that the resiliency is far greater than we could",
    "start": "2885740",
    "end": "2892580"
  },
  {
    "text": "get even with different data centers that we were self managing whether they be geographically close to geographically distributed so yeah for",
    "start": "2892580",
    "end": "2904070"
  },
  {
    "text": "your success they're sort of describing what you've achieved in terms of you know before and after have you developed",
    "start": "2904070",
    "end": "2910670"
  },
  {
    "text": "any metrics around surveillance instant incidents and looking at things like",
    "start": "2910670",
    "end": "2916490"
  },
  {
    "text": "both scope of market like do you feel like you're getting a higher percentage of incidents that should be looked at",
    "start": "2916490",
    "end": "2922790"
  },
  {
    "text": "and then the other one is from a cost saver spective do you measure cost per incident and has that gone down as well",
    "start": "2922790",
    "end": "2929620"
  },
  {
    "text": "yeah I haven't I personally haven't gone through to do the metrics on you know",
    "start": "2929620",
    "end": "2934850"
  },
  {
    "text": "cost per incident again at the portfolio level across the whole market regulation area the cost to operate the plan as a",
    "start": "2934850",
    "end": "2940760"
  },
  {
    "text": "whole is you know like I said roughly thirty percent lower there on prom I mean I think you know we could go ahead",
    "start": "2940760",
    "end": "2945890"
  },
  {
    "text": "and potentially do that work at some point if we wanted to to kind of break it down at per incident basis but that's",
    "start": "2945890",
    "end": "2951380"
  },
  {
    "text": "not work that I've I've done personally from a breadth and richness perspective",
    "start": "2951380",
    "end": "2957800"
  },
  {
    "text": "as I said I think the thing that we've been really focusing on is kind of improving time to market and improving",
    "start": "2957800",
    "end": "2964190"
  },
  {
    "text": "you know resiliency the ability to quickly respond quickly to those reprocessing events and things like that",
    "start": "2964190",
    "end": "2969380"
  },
  {
    "text": "from the infrastructure side what we're really engaged in right now is that kind of evolving the capabilities of the",
    "start": "2969380",
    "end": "2975080"
  },
  {
    "text": "detection portfolio itself through things like machine learning and that's a process that's that's really in full",
    "start": "2975080",
    "end": "2981260"
  },
  {
    "text": "focus right now with you know additional ramp up plan for for next year right so",
    "start": "2981260",
    "end": "2986270"
  },
  {
    "text": "that so you think that overall is gonna achieve a greater confidence that you're",
    "start": "2986270",
    "end": "2992150"
  },
  {
    "text": "detecting all of the events you should be detecting and identifying them as significant events yeah I mean I think",
    "start": "2992150",
    "end": "2998780"
  },
  {
    "text": "the idea with the machine learning portfolio besides the efficiencies that I talked about is that it also makes it",
    "start": "2998780",
    "end": "3003790"
  },
  {
    "text": "you know you're gonna have events that you may not be able to detect with sort of a rules-based regime that you can basically get greater you know more",
    "start": "3003790",
    "end": "3010870"
  },
  {
    "text": "events are gonna come to your attention then you could get that you might not you know a priori be looking for just",
    "start": "3010870",
    "end": "3016390"
  },
  {
    "text": "you know based on picking up requirements so right okay thanks yeah all right over here so the data sets",
    "start": "3016390",
    "end": "3023440"
  },
  {
    "text": "that you maintain there what is your compliance that you need to mean keep them ongoing I mean you're growing to",
    "start": "3023440",
    "end": "3028750"
  },
  {
    "text": "twenty five petabytes is that is that the 3-year maintenance of it or how do",
    "start": "3028750",
    "end": "3035230"
  },
  {
    "text": "you do manage cleanup of legacy data set yeah that's a good question so we have a",
    "start": "3035230",
    "end": "3041290"
  },
  {
    "text": "data retention policy a lot of our data is you know we keep it for seven years and then we would destroy it at that",
    "start": "3041290",
    "end": "3047590"
  },
  {
    "text": "point for going to the cloud we haven't been in the cloud long enough that we have lots of data sets that meet that to",
    "start": "3047590",
    "end": "3054010"
  },
  {
    "text": "meet that mark so that's why we're now putting in the process like the records management piece and things like that so",
    "start": "3054010",
    "end": "3059500"
  },
  {
    "text": "that when we get to those events we need to start destroying records we've got an automated processing that can do that so",
    "start": "3059500",
    "end": "3065740"
  },
  {
    "text": "so yeah but the idea is you know we need to comply with our records policy so when we reach those thresholds the data would have to be destroyed yeah I think",
    "start": "3065740",
    "end": "3076120"
  },
  {
    "text": "you were waiting longer do you have much",
    "start": "3076120",
    "end": "3081550"
  },
  {
    "text": "PII or other sensitive data in your data sets and do you have to tackle problems",
    "start": "3081550",
    "end": "3087430"
  },
  {
    "text": "around one-way hashing and otherwise redact in that data before it reaches the clients yeah we do the mid so of the",
    "start": "3087430",
    "end": "3097540"
  },
  {
    "text": "data that we have in the cloud the vast majority of it would be we have different you know tiers or classification we call it confidential",
    "start": "3097540",
    "end": "3103840"
  },
  {
    "text": "but requires encryption of the data we have some small amount of PII data in",
    "start": "3103840",
    "end": "3109660"
  },
  {
    "text": "the cloud that requires at a minimum of the encryption layer that's where we were required to use kms encryption we",
    "start": "3109660",
    "end": "3115420"
  },
  {
    "text": "can leverage it for ease of use in other areas too but for the actual data that the pulling it back and rendering it to",
    "start": "3115420",
    "end": "3122710"
  },
  {
    "text": "a user in an application the client we also do redaction of the applique the data sets as they come back so",
    "start": "3122710",
    "end": "3128980"
  },
  {
    "text": "that's something that's responsible right now the client application following a set of you know processes",
    "start": "3128980",
    "end": "3134260"
  },
  {
    "text": "interviews is responsible for building that into the application that's all handled in the EMR is it the redaction",
    "start": "3134260",
    "end": "3142090"
  },
  {
    "text": "of the information no II more in our case generally would become sort of as in the case of the interactive example",
    "start": "3142090",
    "end": "3148330"
  },
  {
    "text": "would be a query layer so it's kind of like the database back-end we would tend to redact that information typically and",
    "start": "3148330",
    "end": "3153520"
  },
  {
    "text": "the you know once it gets received before it gets rendered to the to the user through through the client",
    "start": "3153520",
    "end": "3158770"
  },
  {
    "text": "interface okay one of the different formats of data sets that you have on",
    "start": "3158770",
    "end": "3165670"
  },
  {
    "text": "your s3 especially for using presto and creating your data files in yes three different formats in terms of like",
    "start": "3165670",
    "end": "3172510"
  },
  {
    "text": "physical storage format or yeah the majority of our like I said we keep a",
    "start": "3172510",
    "end": "3178750"
  },
  {
    "text": "text file format copy of all of our data that we have in the cloud and then in",
    "start": "3178750",
    "end": "3184450"
  },
  {
    "text": "the cases where we're doing a performant query format typically we do that through Oh RC or file format for common",
    "start": "3184450",
    "end": "3190900"
  },
  {
    "text": "air compressed those are pretty much the two formats that we use we probably use a little parquet you know as part of our",
    "start": "3190900",
    "end": "3198970"
  },
  {
    "text": "spark work but that's not something that we formally you know do at the enterprise level that's just for you",
    "start": "3198970",
    "end": "3205060"
  },
  {
    "text": "know users creating their own output tables and they just chose to use parquet for a temporary tables and",
    "start": "3205060",
    "end": "3210130"
  },
  {
    "text": "things like that okay then how do you find access control of the data within a data set access control within the data",
    "start": "3210130",
    "end": "3218290"
  },
  {
    "text": "center the data set data within a data set how do you find access control how",
    "start": "3218290",
    "end": "3223450"
  },
  {
    "text": "do we define access control so we basically again two different scenarios one for for batch applications",
    "start": "3223450",
    "end": "3229740"
  },
  {
    "text": "they'll use the catalog to determine whether or not they can have access to the data and then for interactive",
    "start": "3229740",
    "end": "3235660"
  },
  {
    "text": "applications that's typically controlled at the application layer to define whether or not the user should be able",
    "start": "3235660",
    "end": "3240970"
  },
  {
    "text": "to have access and do anything with with the data all right thanks yeah yes my",
    "start": "3240970",
    "end": "3248800"
  },
  {
    "text": "question was related to your three-year journey into into the cloud from",
    "start": "3248800",
    "end": "3253810"
  },
  {
    "text": "premises environments you didn't replace everything wholesale all at once",
    "start": "3253810",
    "end": "3259670"
  },
  {
    "text": "you're in state is a very elegant solution and all of it seems to live in AWS but as you increment it into AWS did",
    "start": "3259670",
    "end": "3267799"
  },
  {
    "text": "you find products that just did not function with a latency difference",
    "start": "3267799",
    "end": "3274039"
  },
  {
    "text": "between you know maybe this data was there and the application was there now the data is out right at ws but the",
    "start": "3274039",
    "end": "3280490"
  },
  {
    "text": "applications still here did you did you tackle any problems like that during your migration yeah as part of our",
    "start": "3280490",
    "end": "3287619"
  },
  {
    "text": "market regulation portfolio migration we didn't really experience like",
    "start": "3287619",
    "end": "3293440"
  },
  {
    "text": "interactive kind of scenarios like that because we're the data said the big data sets themselves tended to be you know in",
    "start": "3293440",
    "end": "3300200"
  },
  {
    "text": "batches or micro batches so we built the whole mechanism we call it the data bridge to basically keep the data sets",
    "start": "3300200",
    "end": "3305299"
  },
  {
    "text": "in sync between our on-premises Hadoop clusters data warehouse appliances and the data sets in the cloud and that",
    "start": "3305299",
    "end": "3311839"
  },
  {
    "text": "replication you know we had no issues with that but throughout the duration though we did have cases where we had",
    "start": "3311839",
    "end": "3317150"
  },
  {
    "text": "client applications that were running and meaning to talk to you know databases be a redshift or or BIA you",
    "start": "3317150",
    "end": "3323839"
  },
  {
    "text": "know Siebel clusters up in the cloud and you know so we have a Direct Connect connection through with division capacity and you know we didn't really",
    "start": "3323839",
    "end": "3330829"
  },
  {
    "text": "see anything significant in terms of latency perspective then we could dial at the performance to be what we needed so how long did you have to operate with",
    "start": "3330829",
    "end": "3338630"
  },
  {
    "text": "replication coming back to your on-prem environment for the entire data set we basically so the really the one of the",
    "start": "3338630",
    "end": "3344480"
  },
  {
    "text": "the data catalog and that data bridge were some of the first things we set up as part of the migration so we ran that",
    "start": "3344480",
    "end": "3350599"
  },
  {
    "text": "through the the whole lifecycle of the the migration really you know until there were there weren't any",
    "start": "3350599",
    "end": "3356210"
  },
  {
    "text": "dependencies on on on-prem infrastructure for that thank you yeah",
    "start": "3356210",
    "end": "3363038"
  },
  {
    "text": "last question yeah I'm curious about your use of kms are you using the bring",
    "start": "3363430",
    "end": "3370039"
  },
  {
    "text": "your own key or are using Amazon managed master key and also I'm also curious",
    "start": "3370039",
    "end": "3376640"
  },
  {
    "text": "about your data classification that you mentioned is there certain types of data that you've determined you can never put",
    "start": "3376640",
    "end": "3382190"
  },
  {
    "text": "into Amazon no so did I did briefly because I know I was told we had to stop",
    "start": "3382190",
    "end": "3387559"
  },
  {
    "text": "in five minutes to go but just briefly on the last one no I mean we can we have personal",
    "start": "3387559",
    "end": "3392599"
  },
  {
    "text": "confidential information up there in the cloud encrypted through kms basically you know we generate the key and then",
    "start": "3392599",
    "end": "3398390"
  },
  {
    "text": "the only the infrastructure is the piece that's this managed by by AWS as part of our approach so yeah and I I think we",
    "start": "3398390",
    "end": "3407630"
  },
  {
    "text": "have to wrap the questions right yep so we'll be out here actually in the main hall all the way down at the end",
    "start": "3407630",
    "end": "3413869"
  },
  {
    "text": "for any anyone else who has any questions I want to thank everyone for coming when you have a moment please",
    "start": "3413869",
    "end": "3420290"
  },
  {
    "text": "fill out server for the session thank you Hey [Applause]",
    "start": "3420290",
    "end": "3427219"
  }
]