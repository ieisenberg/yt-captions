[
  {
    "text": "hello everyone welcome to the ECF deep dive my name is Brent Langston and I'm a",
    "start": "0",
    "end": "7649"
  },
  {
    "text": "developer advocate with the Container Services team and that's the team that brings you ECS eks Fargate ECR app mesh",
    "start": "7649",
    "end": "20600"
  },
  {
    "text": "and some other services so I want to talk you talk to you today specifically",
    "start": "20600",
    "end": "27660"
  },
  {
    "text": "about ECS so we're going to take a look",
    "start": "27660",
    "end": "36719"
  },
  {
    "text": "at ECS we're gonna go over kind of some some history but then we're also going",
    "start": "36719",
    "end": "42030"
  },
  {
    "text": "to talk about some specific advanced features of ECS now this particular talk",
    "start": "42030",
    "end": "49770"
  },
  {
    "text": "is labeled con 402 meaning it's a 400 level talk it's advanced we're talking",
    "start": "49770",
    "end": "57719"
  },
  {
    "text": "about advanced features but I don't want you to think that it's complicated one of the design principles that we",
    "start": "57719",
    "end": "64140"
  },
  {
    "text": "have with ECS is we try and make things as simple as possible so we're going to",
    "start": "64140",
    "end": "69900"
  },
  {
    "text": "take some of these advanced features some of these advanced topics and I'm going to show you hopefully show you how",
    "start": "69900",
    "end": "75750"
  },
  {
    "text": "simple they actually become when you're doing it on ECS there are some other",
    "start": "75750",
    "end": "81479"
  },
  {
    "text": "related breakouts that you should be aware of we just had con 201 which was an",
    "start": "81479",
    "end": "87600"
  },
  {
    "text": "overview of using containers on e CS and then we're going to have next up con 303",
    "start": "87600",
    "end": "94130"
  },
  {
    "text": "mastering eks and then con 404 getting your code all the way to a running",
    "start": "94130",
    "end": "100079"
  },
  {
    "text": "container so let's talk a little bit about some history so that we understand",
    "start": "100079",
    "end": "106430"
  },
  {
    "text": "why we are where we are today some background for you about me I actually",
    "start": "106430",
    "end": "114259"
  },
  {
    "text": "helped build tumblr and Spotify and then",
    "start": "114259",
    "end": "119579"
  },
  {
    "text": "went on to be one of the founding engineers and architects at Oscar health and then Ari architected and was the",
    "start": "119579",
    "end": "127500"
  },
  {
    "text": "platform developed a platform director for cloud passage",
    "start": "127500",
    "end": "132950"
  },
  {
    "text": "so I've been running microservices in production at scale since really before",
    "start": "132950",
    "end": "139790"
  },
  {
    "text": "docker was around or was popular so I definitely understand some of the pain",
    "start": "139790",
    "end": "146840"
  },
  {
    "text": "points of operating at that kind of scale with thousands of services so to",
    "start": "146840",
    "end": "154970"
  },
  {
    "text": "understand why things work the way they do today it's handy to to sort of have a",
    "start": "154970",
    "end": "161030"
  },
  {
    "text": "bit of history and a bit of context and so we look back to running services on",
    "start": "161030",
    "end": "166790"
  },
  {
    "text": "instances and the common pattern back then was one service one instance and if",
    "start": "166790",
    "end": "174530"
  },
  {
    "text": "you needed to scale that service up you add instances and of course adding instances was slow it took if you if you",
    "start": "174530",
    "end": "181220"
  },
  {
    "text": "had great automation it took several minutes configs had to be edited you",
    "start": "181220",
    "end": "189350"
  },
  {
    "text": "know these new instances had to be sort of accounted for by the rest of the of the platform and so on and so forth so",
    "start": "189350",
    "end": "197450"
  },
  {
    "text": "it was it was painful but manageable the",
    "start": "197450",
    "end": "203030"
  },
  {
    "text": "tooling that existed around doing that was also tended to be language specific",
    "start": "203030",
    "end": "209959"
  },
  {
    "text": "when it came to for example deploying a service you would write tooling that would deploy a specific language so you",
    "start": "209959",
    "end": "217640"
  },
  {
    "text": "would find organizations that would try and standardize on one or maybe a handful of languages because they were",
    "start": "217640",
    "end": "224840"
  },
  {
    "text": "investing in writing the tooling and writing the automation for those languages well docker came along and docker solved",
    "start": "224840",
    "end": "231920"
  },
  {
    "text": "some of these problems for us one of the problems that it solved is it standardized the way we distribute the",
    "start": "231920",
    "end": "239030"
  },
  {
    "text": "way we package our code and what goes into that packaging we can package the",
    "start": "239030",
    "end": "244220"
  },
  {
    "text": "code plus all of its dependencies into one deployable artifact and the deploy",
    "start": "244220",
    "end": "249709"
  },
  {
    "text": "mechanism is now standardized as well so we don't have to limit our service teams",
    "start": "249709",
    "end": "256130"
  },
  {
    "text": "anymore about what languages they're allowed to use they can be free to use any language",
    "start": "256130",
    "end": "262909"
  },
  {
    "text": "or whatever makes the most sense for that service team so docker really helped with that",
    "start": "262909",
    "end": "269920"
  },
  {
    "text": "another thing that it helped with though was it actually allowed us to put more",
    "start": "269920",
    "end": "277480"
  },
  {
    "text": "than one service on to an instance so we could now run services alongside each other a lot easier than we could before",
    "start": "277480",
    "end": "284820"
  },
  {
    "text": "so we were able to get more density for our services so containers have now",
    "start": "284820",
    "end": "293410"
  },
  {
    "text": "become the standard way to ship and run our application in the cloud but let's",
    "start": "293410",
    "end": "300160"
  },
  {
    "text": "look at for example the mechanism that we had once people started using containers there was a person an",
    "start": "300160",
    "end": "307900"
  },
  {
    "text": "operator who would deploy a container or set of containers out onto an instance",
    "start": "307900",
    "end": "313060"
  },
  {
    "text": "and that often you know at the most basic level meant they were SS aging into an instance docker pull docker run",
    "start": "313060",
    "end": "319510"
  },
  {
    "text": "etc or they might use some kind of automation to do that for them but they",
    "start": "319510",
    "end": "324910"
  },
  {
    "text": "were tying the container to the instance still or they were tying maybe a set of",
    "start": "324910",
    "end": "330760"
  },
  {
    "text": "containers to an instance so if they needed to scale a service or a",
    "start": "330760",
    "end": "336490"
  },
  {
    "text": "collection of containers they would have to still add instances right so once we",
    "start": "336490",
    "end": "343690"
  },
  {
    "text": "started going all in with containers and putting all of our logic and our",
    "start": "343690",
    "end": "350080"
  },
  {
    "text": "resources and our application into containers what was left on the",
    "start": "350080",
    "end": "355270"
  },
  {
    "text": "operating system very little basically nothing so the operating system itself",
    "start": "355270",
    "end": "361720"
  },
  {
    "text": "became dumb not dumb but less important",
    "start": "361720",
    "end": "366730"
  },
  {
    "text": "so we didn't have to care as much right that could just become a pool of resources so this is going to be a",
    "start": "366730",
    "end": "374680"
  },
  {
    "text": "powder reoccurring theme by the way at AWS we're always looking at what are",
    "start": "374680",
    "end": "380350"
  },
  {
    "text": "what are our customers doing and we're trying to understand how can we make things better for them we do that by",
    "start": "380350",
    "end": "387610"
  },
  {
    "text": "listening to feedback from them by hearing what they're telling us that they have problem wise but then we're",
    "start": "387610",
    "end": "394240"
  },
  {
    "text": "also just observing what they're doing and trying to see that's something that is happening you",
    "start": "394240",
    "end": "400280"
  },
  {
    "text": "know commonly over a lot of our customers and can we fix it can we make it better so what we did is we built",
    "start": "400280",
    "end": "408259"
  },
  {
    "text": "ECS and ECS is this container Orchestrator and what we decided to do",
    "start": "408259",
    "end": "415910"
  },
  {
    "text": "was take a different approach to managing software and managing processes",
    "start": "415910",
    "end": "421030"
  },
  {
    "text": "instead of trying to figure out how do I run three containers on instance a and",
    "start": "421030",
    "end": "428750"
  },
  {
    "text": "five containers on instance B all I should have to do is have you describe",
    "start": "428750",
    "end": "435050"
  },
  {
    "text": "what you want describe the end State so launch ten copies of my container",
    "start": "435050",
    "end": "441289"
  },
  {
    "text": "distributed across three availability zones and connect them to this load balancer just send me that description",
    "start": "441289",
    "end": "447530"
  },
  {
    "text": "and I'll go out and make it happen on your fleet of servers so that's what ECS does you describe the end state that",
    "start": "447530",
    "end": "454130"
  },
  {
    "text": "you're looking for and the orchestrator figures out how to make it happen on just a pool of resources so nodes no",
    "start": "454130",
    "end": "462440"
  },
  {
    "text": "longer are individual they're no longer important they are just a fleet of",
    "start": "462440",
    "end": "468320"
  },
  {
    "text": "resources so ECS has been around for several years let's take a look at what",
    "start": "468320",
    "end": "474949"
  },
  {
    "text": "we've done with it in 2018 the velocity of enhancement is actually pretty",
    "start": "474949",
    "end": "480440"
  },
  {
    "text": "impressive we've increased launch times we've added a GPU pinning feature we've",
    "start": "480440",
    "end": "485990"
  },
  {
    "text": "added app mesh preview we've updated the CLI support for docker we've added SSM",
    "start": "485990",
    "end": "494060"
  },
  {
    "text": "parameter support ECS agent signing service discovery and target tracking",
    "start": "494060",
    "end": "500719"
  },
  {
    "text": "and auto scaling so we've done a lot and this isn't even a comprehensive list",
    "start": "500719",
    "end": "505759"
  },
  {
    "text": "this is just sort of what I could remember thinking back over the year what I want to do is talk to you about",
    "start": "505759",
    "end": "512979"
  },
  {
    "text": "these four because I think these four are really handy and really good problem",
    "start": "512979",
    "end": "518870"
  },
  {
    "text": "solvers for existing you know problems that people are fighting with so daemon",
    "start": "518870",
    "end": "526190"
  },
  {
    "text": "scheduling again make this easier for us",
    "start": "526190",
    "end": "531950"
  },
  {
    "text": "prior to us launching this feature demon scheduling was something that people",
    "start": "531950",
    "end": "537710"
  },
  {
    "text": "were trying to figure out how to do they were they were working with what they had but there they were working you know",
    "start": "537710",
    "end": "544880"
  },
  {
    "text": "they were coming up with workarounds for this feature they were looking at what's the size of my auto scale group and then",
    "start": "544880",
    "end": "551750"
  },
  {
    "text": "they were deploying a task or a service that was sized so that there would be",
    "start": "551750",
    "end": "557900"
  },
  {
    "text": "one deployed four to every host for",
    "start": "557900",
    "end": "563480"
  },
  {
    "text": "every host in their Auto skill group so daemon scheduling was something that we said oh look this is this is an easy",
    "start": "563480",
    "end": "570800"
  },
  {
    "text": "solution for us we can come out with this feature and solve a lot of problems so this specifically deploys a task to",
    "start": "570800",
    "end": "579440"
  },
  {
    "text": "each cluster instance so instead of having to figure out how many instances",
    "start": "579440",
    "end": "585050"
  },
  {
    "text": "do I have at any one given time which with auto scaling is not trivially easy",
    "start": "585050",
    "end": "590570"
  },
  {
    "text": "you might only have one auto scaling group if you have if you have a very simple platform but a lot of people",
    "start": "590570",
    "end": "598070"
  },
  {
    "text": "actually have three or four or five or ten auto scaling groups and they would have to do math and figure out how to",
    "start": "598070",
    "end": "604910"
  },
  {
    "text": "add all those together to come up with the right number to deploy we can take care of doing that for you and oh by the",
    "start": "604910",
    "end": "610850"
  },
  {
    "text": "way if you add another node group we'll roll that into the calculation as well so for you to do daemon scheduling now",
    "start": "610850",
    "end": "619040"
  },
  {
    "text": "all you have to do is scheduling strategy daemon it's very simple so we",
    "start": "619040",
    "end": "624860"
  },
  {
    "text": "took something that was complex that customers were struggling with and we",
    "start": "624860",
    "end": "630170"
  },
  {
    "text": "made it very simple and then once you've launched that task this is the feedback",
    "start": "630170",
    "end": "635450"
  },
  {
    "text": "that you get back scheduling strategy daemon by the way I've highlighted the important parts scheduling strategy",
    "start": "635450",
    "end": "642440"
  },
  {
    "text": "daemon like we saw before but then deployment configuration you see maximum percent and minimum healthy percent this",
    "start": "642440",
    "end": "650300"
  },
  {
    "text": "is how we control the way this service rolls out you probably familiar with the",
    "start": "650300",
    "end": "655370"
  },
  {
    "text": "term Bluegreen deploy blue green means roll out all new containers before we",
    "start": "655370",
    "end": "662000"
  },
  {
    "text": "kill off the old one but for the case you only want one running for any given",
    "start": "662000",
    "end": "668170"
  },
  {
    "text": "instance you wouldn't want to do a Bluegreen deploy because then there'd be a period of time where you have two of",
    "start": "668170",
    "end": "673930"
  },
  {
    "text": "those containers running at the same time on the same instance so this is how we do a red-black deploy where we kill",
    "start": "673930",
    "end": "681009"
  },
  {
    "text": "the container and then we bring up its replacement so minimum healthy percent",
    "start": "681009",
    "end": "686259"
  },
  {
    "text": "zero means we're allowed to go below a hundred percent and then maximum percent",
    "start": "686259",
    "end": "692680"
  },
  {
    "text": "means we're not allowed to go over a hundred percent you can see more I",
    "start": "692680",
    "end": "699779"
  },
  {
    "text": "highlighted the important parts of the documentation docks Amazon ECS developer",
    "start": "699779",
    "end": "705339"
  },
  {
    "text": "guide ECS services that's a good thing to have you know ready to click on it",
    "start": "705339",
    "end": "710860"
  },
  {
    "text": "for lots of different features the next one target tracking auto scaling so this",
    "start": "710860",
    "end": "717339"
  },
  {
    "text": "is another thing that we were hearing auto scaling is a feature that we added",
    "start": "717339",
    "end": "722430"
  },
  {
    "text": "to containers so just like before you could scale instances you can also scale",
    "start": "722430",
    "end": "729490"
  },
  {
    "text": "containers directly using auto scale groups but figuring out the right",
    "start": "729490",
    "end": "734829"
  },
  {
    "text": "thresholds you know the minimum threshold and the maximum threshold can sometimes be complicated well really if",
    "start": "734829",
    "end": "742329"
  },
  {
    "text": "you stop and think about it there's there's one approach that makes to me it makes the most sense so this is",
    "start": "742329",
    "end": "748329"
  },
  {
    "text": "the approach I tend to take with my micro services instead of trying to",
    "start": "748329",
    "end": "755079"
  },
  {
    "text": "figure out upper and lower thresholds what I do is I benchmark the service and I find out this service can handle X",
    "start": "755079",
    "end": "762670"
  },
  {
    "text": "number of ops per second and then I turn around and I back off from that number a",
    "start": "762670",
    "end": "768220"
  },
  {
    "text": "little bit and I set that as a target tracking service so for my example I",
    "start": "768220",
    "end": "774160"
  },
  {
    "text": "actually have a Ruby application and it can handle 1400 ops per second that was",
    "start": "774160",
    "end": "780130"
  },
  {
    "text": "the benchmark so I'm going to set this to target track a thousand operations",
    "start": "780130",
    "end": "785379"
  },
  {
    "text": "per second and then as I get requests that come in I'll actually scale up so",
    "start": "785379",
    "end": "792970"
  },
  {
    "text": "that each target is trying to maintain around a thousand operations per second",
    "start": "792970",
    "end": "799089"
  },
  {
    "text": "and then as requests die off I'll scale back down so that again I'm still",
    "start": "799089",
    "end": "805809"
  },
  {
    "text": "maintaining a thousand operations per second per target the way you set it up very very simple you can use the AWS CLI",
    "start": "805809",
    "end": "814739"
  },
  {
    "text": "register scalable target and notice the scalable dimension is desired count so",
    "start": "814739",
    "end": "822519"
  },
  {
    "text": "I'm setting the scalable dimension to the number of tasks that are going to be running and the resource ID in this case",
    "start": "822519",
    "end": "829689"
  },
  {
    "text": "is that is my task the ECS demo front-end and I'm also setting minimums",
    "start": "829689",
    "end": "834999"
  },
  {
    "text": "and maximum so minimum three maximum twenty five because I don't want to necessarily you know scale to infinity",
    "start": "834999",
    "end": "842799"
  },
  {
    "text": "because I might not be able to pay for that and I don't want to scale down to zero all the time I want to always",
    "start": "842799",
    "end": "848769"
  },
  {
    "text": "maintain some kind of presence and then you create a scaling policy and the",
    "start": "848769",
    "end": "855039"
  },
  {
    "text": "scaling policy you can see I have a JSON file that's configuring it and I'm giving it a name track ops per target",
    "start": "855039",
    "end": "862179"
  },
  {
    "text": "and the target value I mentioned was a thousand so I'm setting the scaling policy to track a thousand operations",
    "start": "862179",
    "end": "869379"
  },
  {
    "text": "per second now where does that metric come from well this particular service is tied to an al be an al be includes",
    "start": "869379",
    "end": "877949"
  },
  {
    "text": "some metrics built into it one of them is requests per target so I'm actually",
    "start": "877949",
    "end": "884169"
  },
  {
    "text": "just reading those metrics directly from cloud watch from the alb metrics so when",
    "start": "884169",
    "end": "892329"
  },
  {
    "text": "I do that and by the way this is a screenshot from when I ran Apache bench",
    "start": "892329",
    "end": "898149"
  },
  {
    "text": "against my service the dotted red line is a thousand operations per second and",
    "start": "898149",
    "end": "904749"
  },
  {
    "text": "so you can see that I started sending it some traffic and this is kind of an abnormal pattern a more realistic",
    "start": "904749",
    "end": "911949"
  },
  {
    "text": "pattern would have been a much gentler curve but in this particular case it was",
    "start": "911949",
    "end": "917079"
  },
  {
    "text": "a spike because I started I started hitting it with all of my traffic all at once but you see that we spiked up and",
    "start": "917079",
    "end": "925029"
  },
  {
    "text": "as soon as we cross that red line the default is for three measurement",
    "start": "925029",
    "end": "931450"
  },
  {
    "text": "periods which are one minute so for three minutes across that red line we start to scale out and we also scale",
    "start": "931450",
    "end": "938020"
  },
  {
    "text": "based on the amount over that line we've gone so if we're double our line if",
    "start": "938020",
    "end": "944260"
  },
  {
    "text": "we're running at 3,000 ops per second we're gonna add two additional targets so we're gonna go from whatever we're at",
    "start": "944260",
    "end": "951040"
  },
  {
    "text": "now we're gonna double it or triple it or whatever that math works out to be",
    "start": "951040",
    "end": "956190"
  },
  {
    "text": "once we've been running for a while notice how the graph tapers back down that's not because we're getting less",
    "start": "956190",
    "end": "962830"
  },
  {
    "text": "traffic remember this is traffic per target so because it's per target the",
    "start": "962830",
    "end": "969100"
  },
  {
    "text": "total count can still be going up but the per target count is going down until",
    "start": "969100",
    "end": "974350"
  },
  {
    "text": "it's finally below that red line again and that's when we're considered healthy again and we've scaled fully to our",
    "start": "974350",
    "end": "981670"
  },
  {
    "text": "correct capacity so when I got up on stage I actually launched this on my",
    "start": "981670",
    "end": "988420"
  },
  {
    "text": "laptop so let's see let's see what's happened so I launched this when I got",
    "start": "988420",
    "end": "998140"
  },
  {
    "text": "up here and notice you can see the scaling has kind of had an effect where",
    "start": "998140",
    "end": "1003170"
  },
  {
    "text": "I've been able to take some traffic we out we added a target I'm sure as why",
    "start": "1003170",
    "end": "1009300"
  },
  {
    "text": "this dipped down substantially and then because of that Apache bench was able to",
    "start": "1009300",
    "end": "1014640"
  },
  {
    "text": "probably send it even more transactions than it was before so we went up a little bit and then tapered back down a",
    "start": "1014640",
    "end": "1021060"
  },
  {
    "text": "little bit so we'll kind of seesaw until we get that back down below this line but in only a few minutes I'm scaling",
    "start": "1021060",
    "end": "1029250"
  },
  {
    "text": "out and if I look I started out at two running tasks minimum instead of three",
    "start": "1029250",
    "end": "1036329"
  },
  {
    "text": "like on the slide and I'm now up to six desired and five running sorry I realize",
    "start": "1036330",
    "end": "1043770"
  },
  {
    "text": "this is probably too small for you to see so six desired and five running and",
    "start": "1043770",
    "end": "1052050"
  },
  {
    "text": "if we look at the actual tasks now all",
    "start": "1052050",
    "end": "1058890"
  },
  {
    "text": "six are running so it's very simple at this point to figure out how to scale out how to scale",
    "start": "1058890",
    "end": "1065550"
  },
  {
    "text": "in I'm gonna go ahead and kill this traffic I don't know if we'll see the scale in part because there's a there's",
    "start": "1065550",
    "end": "1072690"
  },
  {
    "text": "a pretty conservative cooldown we don't start actually scaling back in the dafuq",
    "start": "1072690",
    "end": "1077910"
  },
  {
    "text": "this is all customizable but the default is to wait 15 minutes before we start scaling back in but think about what",
    "start": "1077910",
    "end": "1086190"
  },
  {
    "text": "this does for you from a budgeting standpoint and from an operational standpoint I just had to set two",
    "start": "1086190",
    "end": "1092070"
  },
  {
    "text": "parameters and all of a sudden I was scaling my service and I don't have to",
    "start": "1092070",
    "end": "1097470"
  },
  {
    "text": "create or manage any kind of time-series database or metric service or anything",
    "start": "1097470",
    "end": "1105780"
  },
  {
    "text": "like that I don't have to figure out how to scale a metric service or I don't have to send requests to constantly be",
    "start": "1105780",
    "end": "1112410"
  },
  {
    "text": "pulling or reading from that metric service it's just all built in for me so",
    "start": "1112410",
    "end": "1122790"
  },
  {
    "text": "this is a sample from the events that occur so if you wanted to like watch for",
    "start": "1122790",
    "end": "1129150"
  },
  {
    "text": "a log of events or something like that in the ECS event log you can see that as",
    "start": "1129150",
    "end": "1135420"
  },
  {
    "text": "we scale out you'll see for example successfully set desired count to 7 and then we started another task we",
    "start": "1135420",
    "end": "1143790"
  },
  {
    "text": "registered it with the target the alb target and then it reaches steady state",
    "start": "1143790",
    "end": "1148800"
  },
  {
    "text": "which means we start sending it traffic",
    "start": "1148800",
    "end": "1153260"
  },
  {
    "text": "it's not specific to far gate it's an ECS function the question by the way from the audience was is that specific",
    "start": "1154790",
    "end": "1161340"
  },
  {
    "text": "to far gate I'm running these on far gate but this could just as easily happen with easiest ec2 based ECS tasks",
    "start": "1161340",
    "end": "1170910"
  },
  {
    "text": "so it's not specific to far gate if you want to check out the demo I've actually put it on github so you can run it",
    "start": "1170910",
    "end": "1178200"
  },
  {
    "text": "yourself and you can see how easy it is to set up auto scaling for your service",
    "start": "1178200",
    "end": "1184430"
  },
  {
    "text": "service discovery is another thing that customers were struggling with and they",
    "start": "1184430",
    "end": "1189630"
  },
  {
    "text": "were trying to figure out you know what's the best way to do service discovery when I was at Spotify we actually this",
    "start": "1189630",
    "end": "1195790"
  },
  {
    "text": "was again before containers we actually did service discovery using DNS and this",
    "start": "1195790",
    "end": "1201309"
  },
  {
    "text": "was before console or any of the other DNS based service discoveries so it was",
    "start": "1201309",
    "end": "1207130"
  },
  {
    "text": "it was cool it kept things simple but it wasn't oriented for containers so again",
    "start": "1207130",
    "end": "1213280"
  },
  {
    "text": "it was oriented for instances an instance gets one IP and at that one IP",
    "start": "1213280",
    "end": "1218770"
  },
  {
    "text": "we expect to be able to reach this one service on this one port so what if we",
    "start": "1218770",
    "end": "1225580"
  },
  {
    "text": "could have that kind of simplicity again but with a container well with ECS we",
    "start": "1225580",
    "end": "1231370"
  },
  {
    "text": "have the option of attaching an en I directly to the task so that all the",
    "start": "1231370",
    "end": "1237670"
  },
  {
    "text": "containers in that task are at an individual IP address and have their own",
    "start": "1237670",
    "end": "1242679"
  },
  {
    "text": "port range that you can run services on so we can have that kind of simplicity so then the next step is register it",
    "start": "1242679",
    "end": "1249850"
  },
  {
    "text": "with a service so we use cloud map for that and cloud map actually can we talk",
    "start": "1249850",
    "end": "1256240"
  },
  {
    "text": "ECS the control plane talks to cloud map and as tasks come up and go down we",
    "start": "1256240",
    "end": "1263710"
  },
  {
    "text": "register and D register them with route 53 using cloud map so we also by the way",
    "start": "1263710",
    "end": "1272020"
  },
  {
    "text": "have an HTTP UI or HTTP interface for that as well but I find the DNS",
    "start": "1272020",
    "end": "1278290"
  },
  {
    "text": "interface to be more than adequate so this is kind of what that architecture looks like but again you don't have to",
    "start": "1278290",
    "end": "1285100"
  },
  {
    "text": "build this and you don't have to run this you simply check a box and you get it for free so as a as traffic comes in",
    "start": "1285100",
    "end": "1294670"
  },
  {
    "text": "to your load balancer it needs to be routed to your containers so your front-end is probably tied directly to",
    "start": "1294670",
    "end": "1301929"
  },
  {
    "text": "your load balancer but how does your how does your front-end find its back-end services that's a great use case for",
    "start": "1301929",
    "end": "1308559"
  },
  {
    "text": "service discovery so instead of trying to keep track of where all those",
    "start": "1308559",
    "end": "1313900"
  },
  {
    "text": "back-end services are running or instead of running another load balancer for them internally we can actually just",
    "start": "1313900",
    "end": "1320980"
  },
  {
    "text": "give them a DNS name and then we can just start talking to that DNS name",
    "start": "1320980",
    "end": "1326570"
  },
  {
    "text": "so this is how we do that so I'm bringing up a back-end service ECS demo",
    "start": "1326570",
    "end": "1332130"
  },
  {
    "text": "nodejs and all I'm doing it when I'm bringing it up as I'm saying create a",
    "start": "1332130",
    "end": "1337950"
  },
  {
    "text": "private DNS namespace so it's private only to the VPC it's not accessible from",
    "start": "1337950",
    "end": "1343590"
  },
  {
    "text": "outside the VPC and I'm just giving it a top-level domain of service so it I'll",
    "start": "1343590",
    "end": "1348690"
  },
  {
    "text": "end up with a name ECS demo nodejs dot service and then",
    "start": "1348690",
    "end": "1355200"
  },
  {
    "text": "enable service discovery that's the command-line way of checking that box that I want to have the service",
    "start": "1355200",
    "end": "1361200"
  },
  {
    "text": "discovery feature so then my front-end when I'm defining this is a docker",
    "start": "1361200",
    "end": "1366450"
  },
  {
    "text": "compose defining how to talk to the backend I'm just passing an environment variable the environment variable key",
    "start": "1366450",
    "end": "1373140"
  },
  {
    "text": "values key value pair nodejs URL equals and I'm and I'm telling it I'm to direct",
    "start": "1373140",
    "end": "1380520"
  },
  {
    "text": "traffic to the DNS name ECS demo nodejs dot service so there's a",
    "start": "1380520",
    "end": "1388020"
  },
  {
    "text": "demo for this as well this is actually by the way ECS workshop comm is a workshop that i",
    "start": "1388020",
    "end": "1395970"
  },
  {
    "text": "debuted over a year a little over a year ago here in Tel Aviv so I'm I'm happy to be back to talk and",
    "start": "1395970",
    "end": "1403920"
  },
  {
    "text": "talking about it again definitely check it out if you haven't already but let me",
    "start": "1403920",
    "end": "1408960"
  },
  {
    "text": "show you this demo where we have the front end and back end services running",
    "start": "1408960",
    "end": "1415940"
  },
  {
    "text": "well let's see here okay so don't worry",
    "start": "1421290",
    "end": "1428670"
  },
  {
    "text": "about the text on the right I know you probably can't read it but it's really for me to make sure that I don't forget",
    "start": "1428670",
    "end": "1434220"
  },
  {
    "text": "what to show you but I have a front-end talking and then I have note the ECS",
    "start": "1434220",
    "end": "1441480"
  },
  {
    "text": "demo nodejs a back-end API running again in Fargate and a VPC and if i just do a",
    "start": "1441480",
    "end": "1448590"
  },
  {
    "text": "simple dig for ECS demo nodejs service I get back",
    "start": "1448590",
    "end": "1453860"
  },
  {
    "text": "three IP addresses I have three instances of those running three tasks",
    "start": "1453860",
    "end": "1459720"
  },
  {
    "text": "running so just like this particular workspace any service that's in that V",
    "start": "1459720",
    "end": "1466950"
  },
  {
    "text": "PC can do a simple DNS query and know how to go talk to that API let's",
    "start": "1466950",
    "end": "1472260"
  },
  {
    "text": "actually watch that let me figure out how to zoom out my screen again",
    "start": "1472260",
    "end": "1480110"
  },
  {
    "text": "there we go okay so now we can see in the upper left corner we can see all of",
    "start": "1509930",
    "end": "1516510"
  },
  {
    "text": "the IP addresses that were that my task currently has and again the front end is",
    "start": "1516510",
    "end": "1522270"
  },
  {
    "text": "free to connect to any or all of those if you're familiar with the workshop",
    "start": "1522270",
    "end": "1527460"
  },
  {
    "text": "then you've probably seen this before but what we end up deploying in the workshop is this so this is I wanted a",
    "start": "1527460",
    "end": "1535530"
  },
  {
    "text": "set of applications that's so simple that you won't ever get hung up on what",
    "start": "1535530",
    "end": "1540600"
  },
  {
    "text": "they're actually doing and instead we can focus on how they connect to each other and how you set them up so what",
    "start": "1540600",
    "end": "1547740"
  },
  {
    "text": "we're working with what I'm about to show you and work with is the Green Line the nodejs line and you can see that",
    "start": "1547740",
    "end": "1554160"
  },
  {
    "text": "currently we're hitting we have three copies running and they're distributed",
    "start": "1554160",
    "end": "1559350"
  },
  {
    "text": "across availability zones so one in 1a availability Zone B and availability",
    "start": "1559350",
    "end": "1566309"
  },
  {
    "text": "zone C let's kill one and see what happens to our service to our front end",
    "start": "1566309",
    "end": "1572880"
  },
  {
    "text": "service first let's get a set of running",
    "start": "1572880",
    "end": "1578929"
  },
  {
    "text": "let's see the running tasks so there's",
    "start": "1578929",
    "end": "1583950"
  },
  {
    "text": "our tasks that are running and I'm gonna pick out let's kill the one that's in Zone A",
    "start": "1583950",
    "end": "1590510"
  },
  {
    "text": "oops",
    "start": "1600410",
    "end": "1603100"
  },
  {
    "text": "recopy this so I sent the kill command",
    "start": "1606800",
    "end": "1616250"
  },
  {
    "text": "now that's going to take a few seconds to propagate from the orchestrator down",
    "start": "1616250",
    "end": "1622070"
  },
  {
    "text": "to far gate where it's actually running but what we should vote there it happens so now we're down to and again we have",
    "start": "1622070",
    "end": "1630070"
  },
  {
    "text": "multiple authoritative servers so that also needs to propagate to all of our servers but you see we're down to two",
    "start": "1630070",
    "end": "1637180"
  },
  {
    "text": "let's go back here and our green green",
    "start": "1637180",
    "end": "1642200"
  },
  {
    "text": "one we're really only hitting B and C",
    "start": "1642200",
    "end": "1647540"
  },
  {
    "text": "right now right now the service itself that that API endpoint that ap that IP",
    "start": "1647540",
    "end": "1654380"
  },
  {
    "text": "is still alive for a few more seconds because we take it out of service before",
    "start": "1654380",
    "end": "1659780"
  },
  {
    "text": "we actually kill the task but once it's been out of service for a few seconds",
    "start": "1659780",
    "end": "1664940"
  },
  {
    "text": "and we've killed the task let's come back over here we'll see the replacement",
    "start": "1664940",
    "end": "1671120"
  },
  {
    "text": "task pop in and a few seconds and there",
    "start": "1671120",
    "end": "1680030"
  },
  {
    "text": "it is the 115 is the replacement task and we're already hitting a Z a is",
    "start": "1680030",
    "end": "1690040"
  },
  {
    "text": "starting to take traffic again so what did I have to set up to have service",
    "start": "1690040",
    "end": "1696170"
  },
  {
    "text": "discovery for my for my cluster for my services nothing I had to check a box so",
    "start": "1696170",
    "end": "1701810"
  },
  {
    "text": "we try to keep things simple we take complicated topics instead of having you",
    "start": "1701810",
    "end": "1707870"
  },
  {
    "text": "run a control plane for this instead of having you figure out how to upgrade a service every time there's a new version",
    "start": "1707870",
    "end": "1713660"
  },
  {
    "text": "that comes out we take care of it for you and we run it for you",
    "start": "1713660",
    "end": "1719320"
  },
  {
    "text": "okay the last thing I want to talk about is GPU pinning and this is probably the",
    "start": "1724320",
    "end": "1729790"
  },
  {
    "text": "most recent big feature that we rolled out so what is GPU pinning well before",
    "start": "1729790",
    "end": "1738610"
  },
  {
    "text": "we had this feature customers that wanted to run machine learning training",
    "start": "1738610",
    "end": "1743800"
  },
  {
    "text": "on a cluster they would have to figure out weird workarounds to launch GPU",
    "start": "1743800",
    "end": "1750670"
  },
  {
    "text": "based tasks onto their cluster and have them hit the appropriate type of",
    "start": "1750670",
    "end": "1756430"
  },
  {
    "text": "instance so the type of instance that's good for jeep for that kind of training",
    "start": "1756430",
    "end": "1761950"
  },
  {
    "text": "are p-series instances so we you could potentially like set a constraint only",
    "start": "1761950",
    "end": "1767500"
  },
  {
    "text": "run this on a p-series instance but you wouldn't necessarily under have a good",
    "start": "1767500",
    "end": "1772690"
  },
  {
    "text": "understanding of how many GPUs are available on that instance and if I launch a task on that instance and it",
    "start": "1772690",
    "end": "1778300"
  },
  {
    "text": "takes up all the GPUs then that instance is no longer available to schedule future GPU based tasks so we built GPUs",
    "start": "1778300",
    "end": "1788860"
  },
  {
    "text": "directly into the scheduler so now they're a first-class citizen and you can request them and you can say I need",
    "start": "1788860",
    "end": "1795670"
  },
  {
    "text": "four of them or I need eight of them or I need one of them and we'll find that out on the cluster and schedule your",
    "start": "1795670",
    "end": "1801160"
  },
  {
    "text": "tasks or will tell you sorry there aren't any available and we won't schedule your tasks and the way you do",
    "start": "1801160",
    "end": "1808660"
  },
  {
    "text": "that in the task definition you just simply say you have a resource requirement of GPU and the count so you",
    "start": "1808660",
    "end": "1817210"
  },
  {
    "text": "can say GPU one for me running this tensorflow container and then when you",
    "start": "1817210",
    "end": "1824110"
  },
  {
    "text": "well heck let's just do it so I've done that in a task definition and registered",
    "start": "1824110",
    "end": "1833410"
  },
  {
    "text": "that task definition so if I run in fact",
    "start": "1833410",
    "end": "1839560"
  },
  {
    "text": "what I've this is by the way the the one demo that's not on Fargate and it's because I needed a cluster that has GPUs",
    "start": "1839560",
    "end": "1847210"
  },
  {
    "text": "so I wanted a cluster that I could launch some training training on and",
    "start": "1847210",
    "end": "1854590"
  },
  {
    "text": "have those tasks run using a GPU so I have a cluster that has to I don't",
    "start": "1854590",
    "end": "1862000"
  },
  {
    "text": "remember regular instances and then to p3 16x larges so that's where I hope",
    "start": "1862000",
    "end": "1867429"
  },
  {
    "text": "that these tasks are going to get skin it's where I know these tasks are going to get scheduled on so each of those",
    "start": "1867429",
    "end": "1873460"
  },
  {
    "text": "those two instances has eight GPUs so I have a task definition here that's",
    "start": "1873460",
    "end": "1878529"
  },
  {
    "text": "requesting eight GPUs and it's going to launch a tensor flow container that's going to make use of those eight GPUs so",
    "start": "1878529",
    "end": "1887500"
  },
  {
    "text": "you can see in the output it tells me the GPUs that are assigned and now if I run that task again I'm scheduled",
    "start": "1887500",
    "end": "1894370"
  },
  {
    "text": "another eight GPUs if I try and do it a third time I get back errors saying hey",
    "start": "1894370",
    "end": "1900520"
  },
  {
    "text": "wait the resource GPU can't be met so this is a good way for me to fail fast",
    "start": "1900520",
    "end": "1906940"
  },
  {
    "text": "and understand that I can't run this task because I'm out of resources so I could either then choose to scale up or",
    "start": "1906940",
    "end": "1913510"
  },
  {
    "text": "wait and run that task once the first two are done if we take a look at my",
    "start": "1913510",
    "end": "1923919"
  },
  {
    "text": "cluster",
    "start": "1923919",
    "end": "1926429"
  },
  {
    "text": "we see my two tensorflow container tasks and they're pending right now so we'll",
    "start": "1934790",
    "end": "1941700"
  },
  {
    "text": "wait for them to it their bit that's a big container it's like a gig container to download and run but they're running",
    "start": "1941700",
    "end": "1948170"
  },
  {
    "text": "so we'll go and look and see if we have any log data yet",
    "start": "1948170",
    "end": "1954170"
  },
  {
    "text": "so these are running a tensorflow benchmark actually and it takes I think",
    "start": "1954380",
    "end": "1959910"
  },
  {
    "text": "it ends up running over the course of about sixty seconds but it takes a while to warm up and get get the data ready to",
    "start": "1959910",
    "end": "1967650"
  },
  {
    "text": "run so here it is coming up and you can see that we in the task definition",
    "start": "1967650",
    "end": "1975560"
  },
  {
    "text": "requested eight GPUs and somewhere in",
    "start": "1975560",
    "end": "1980970"
  },
  {
    "text": "here in the logs will actually have a record of the GPUs that were assigned I",
    "start": "1980970",
    "end": "1987150"
  },
  {
    "text": "think this line right here yeah you can't see it but zero through seven are the GPUs that were there we go zero",
    "start": "1987150",
    "end": "1995160"
  },
  {
    "text": "through seven the GPUs that were assigned so these are not only assigned",
    "start": "1995160",
    "end": "2000350"
  },
  {
    "text": "to this task but now they're not available for scheduling other tasks so we've pinned them to this task and they",
    "start": "2000350",
    "end": "2007340"
  },
  {
    "text": "won't be usable until this task is completed",
    "start": "2007340",
    "end": "2012490"
  },
  {
    "text": "so again the output is directly in the output from when we launched the task and you can see this you can run this",
    "start": "2017650",
    "end": "2026240"
  },
  {
    "text": "demo yourself from this link if you want so let's take let's review real quick",
    "start": "2026240",
    "end": "2032180"
  },
  {
    "text": "ECS is an Orchestrator that tries to make orchestrating micro-services easy",
    "start": "2032180",
    "end": "2039560"
  },
  {
    "text": "and we're constantly adding new features but we want to keep your operational",
    "start": "2039560",
    "end": "2044780"
  },
  {
    "text": "responsibilities simple so we want to add features without adding more work for you and then for examples daemon",
    "start": "2044780",
    "end": "2052460"
  },
  {
    "text": "scheduling launched a container out onto every instance it doesn't matter how many you have target tracking auto",
    "start": "2052460",
    "end": "2059030"
  },
  {
    "text": "scaling just keep the auto scaling set to the operations per second that you're",
    "start": "2059030",
    "end": "2064700"
  },
  {
    "text": "looking for you can also track based on CPU utilization or memory utilization if that makes more sense and then service",
    "start": "2064700",
    "end": "2072050"
  },
  {
    "text": "discovery check a box get your eye peas registered with route 53 and then have",
    "start": "2072050",
    "end": "2079370"
  },
  {
    "text": "that be available inside the V PC to everything else that it might make use of and then GPU pinning allocate GPUs",
    "start": "2079370",
    "end": "2086780"
  },
  {
    "text": "just like you do memory and CPU so that you know when you've run out of them and",
    "start": "2086780",
    "end": "2092030"
  },
  {
    "text": "you can make a decision about what to do about that",
    "start": "2092030",
    "end": "2096370"
  },
  {
    "text": "these were my see more links my demo links if you want to take a picture of",
    "start": "2097040",
    "end": "2103580"
  },
  {
    "text": "that or have that available I think you'll also get a copy of the slides after this and that's it so I'm gonna be",
    "start": "2103580",
    "end": "2110480"
  },
  {
    "text": "at the back if you have any questions but I really appreciate you guys coming out and listening to the talk about ECS",
    "start": "2110480",
    "end": "2119890"
  },
  {
    "text": "you",
    "start": "2120120",
    "end": "2122180"
  }
]