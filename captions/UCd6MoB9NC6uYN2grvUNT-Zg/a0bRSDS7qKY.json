[
  {
    "start": "0",
    "end": "100000"
  },
  {
    "text": "clear okay um welcome to today's session uh it's it's about advanced data",
    "start": "160",
    "end": "6160"
  },
  {
    "text": "migration for rds so as people are streaming in i guess uh",
    "start": "6160",
    "end": "12160"
  },
  {
    "text": "just a show of hands how many of you here already use rds wow that's my people all right and",
    "start": "12160",
    "end": "21199"
  },
  {
    "text": "how many of you have like in-flight projects for migration things that you're working on right now",
    "start": "21199",
    "end": "27599"
  },
  {
    "text": "almost all of you very nice so very appropriate topic for everybody i guess how many my sequel",
    "start": "27599",
    "end": "35840"
  },
  {
    "text": "okay oracle and sql server okay so some of you at",
    "start": "36160",
    "end": "42800"
  },
  {
    "text": "least have multiple projects going on then i guess and um how many dbas here",
    "start": "42800",
    "end": "50480"
  },
  {
    "text": "okay so the others i'm guessing are ops people infrastructure management okay very nice so we have",
    "start": "50480",
    "end": "59120"
  },
  {
    "text": "a lot of content to cover today and i'm not sure how much we'll uh get through and leave enough time for",
    "start": "59120",
    "end": "65198"
  },
  {
    "text": "q a so just a a heads up here if we don't have enough time and we can't cover your",
    "start": "65199",
    "end": "70880"
  },
  {
    "text": "questions right here then i'd advise you we have a booth called database services in in the in",
    "start": "70880",
    "end": "76960"
  },
  {
    "text": "the reinvent central area so please head over there and we'll have our presenters be available there's a whole bunch of",
    "start": "76960",
    "end": "83520"
  },
  {
    "text": "more people there and they should be able to answer all your questions around rds and data migration and so on",
    "start": "83520",
    "end": "90159"
  },
  {
    "text": "so heading into",
    "start": "90159",
    "end": "93600"
  },
  {
    "text": "the slides here in in the next hour what we are covering is a bit of an overview you're already",
    "start": "95360",
    "end": "101439"
  },
  {
    "start": "100000",
    "end": "100000"
  },
  {
    "text": "using rds so you're aware of it but i'll just cover what's new in rds in the past couple months talk about",
    "start": "101439",
    "end": "109520"
  },
  {
    "text": "the types of data migration that we're thinking of that we want to support that",
    "start": "109520",
    "end": "114560"
  },
  {
    "text": "we know customers are using and and we'd like to enable for our customers and users",
    "start": "114560",
    "end": "120479"
  },
  {
    "text": "talk about some general considerations around what goes into a migration project and like probably",
    "start": "120479",
    "end": "126399"
  },
  {
    "text": "three quarters of you here have indicated you are doing migration so you're already thinking of this with just",
    "start": "126399",
    "end": "131680"
  },
  {
    "text": "laying it out on a slide here and then so i'm the first presenter we've got two more presenters after me one will cover",
    "start": "131680",
    "end": "137760"
  },
  {
    "text": "uh abdul he'll cover advanced data migration techniques for oracle and then we'll have bernie coming to cover",
    "start": "137760",
    "end": "144000"
  },
  {
    "text": "my sequel as well so what's new what's recent",
    "start": "144000",
    "end": "151120"
  },
  {
    "text": "for folks who use oracle you should know we launched support for oracle transparent data encryption we do tde",
    "start": "151120",
    "end": "156879"
  },
  {
    "start": "154000",
    "end": "154000"
  },
  {
    "text": "with amazon managed keys at the moment but that's out that's available if you have",
    "start": "156879",
    "end": "162239"
  },
  {
    "text": "requirements around encryption for oracle that's available we launched mysql56 so you can launch",
    "start": "162239",
    "end": "168879"
  },
  {
    "text": "new instances of five six and then we made it available on our largest instance which is the cr18xl",
    "start": "168879",
    "end": "175599"
  },
  {
    "text": "so that's 32 vcpus that's over 240 gigabytes of memory a very powerful machine and then you",
    "start": "175599",
    "end": "182480"
  },
  {
    "text": "know combine that with uh with provision iops three terabytes of storage 30 000 iops",
    "start": "182480",
    "end": "188319"
  },
  {
    "text": "it should we believe meet the bulk the majority of the use cases out there for production for high availability for",
    "start": "188319",
    "end": "194480"
  },
  {
    "text": "mission critical type workloads and essentially that's where we're going bigger instances larger instances",
    "start": "194480",
    "end": "200080"
  },
  {
    "text": "support for cross regions on on the last bullet there you'll see cross region snapshot copy",
    "start": "200080",
    "end": "205280"
  },
  {
    "text": "um so exciting things coming there as well but this is what's already available um support for oracle stats pack so if",
    "start": "205280",
    "end": "211280"
  },
  {
    "text": "you have your awr snapshots that you want to take and run reporting off of that you can do that",
    "start": "211280",
    "end": "216720"
  },
  {
    "text": "the one thing that this slide does not capture because it had to be embargoed before we released it was sql",
    "start": "216720",
    "end": "223040"
  },
  {
    "text": "server td so besides oracle will now support encryption for sql server as well",
    "start": "223040",
    "end": "228080"
  },
  {
    "text": "so that's just recapping what's available what's new and then stay tuned we've got a lot of exciting",
    "start": "228080",
    "end": "233280"
  },
  {
    "text": "announcements coming up in tomorrow's keynote as well so going to types of data migration now",
    "start": "233280",
    "end": "238480"
  },
  {
    "start": "235000",
    "end": "235000"
  },
  {
    "text": "obviously a lot of people here are doing it i'm just kind of reiterating what's already known to",
    "start": "238480",
    "end": "244159"
  },
  {
    "text": "folks here it's it's not new ground that we're covering here but think of a migration project as is almost akin to you moving from one",
    "start": "244159",
    "end": "252959"
  },
  {
    "text": "place to another right that there's a lot of things that you need to consider is it a one-time migration do you just",
    "start": "252959",
    "end": "258079"
  },
  {
    "text": "move house from one place to another are you constantly traveling back and forth so there are interesting corollaries there",
    "start": "258079",
    "end": "265280"
  },
  {
    "text": "but a one-time migration could be the first step you do in in a longer project which",
    "start": "265280",
    "end": "271520"
  },
  {
    "text": "actually eventually ends up doing replication or it could be the end all right it could be that you",
    "start": "271520",
    "end": "276720"
  },
  {
    "text": "just migrate from wherever your production is running right now into the cloud so that's one type of migration you copy",
    "start": "276720",
    "end": "283199"
  },
  {
    "text": "your entire database and and possibly the applications as well into the cloud itself",
    "start": "283199",
    "end": "288320"
  },
  {
    "text": "and use cases like i said would be my great production to to rds it could be that you want to build pocs or pilots um",
    "start": "288320",
    "end": "294800"
  },
  {
    "text": "it could be possibly the first step or it could be the first step after which you will do",
    "start": "294800",
    "end": "300560"
  },
  {
    "text": "incremental migrations right so that's one one time migration the second would be periodic where",
    "start": "300560",
    "end": "306639"
  },
  {
    "text": "you did this one time but then that's not enough that's just one snapshot that you copied over to to",
    "start": "306639",
    "end": "312960"
  },
  {
    "text": "the cloud and then after that you want to bring in um incremental uh changes into the cloud and and that",
    "start": "312960",
    "end": "319840"
  },
  {
    "text": "could be because you want to do reporting or bi type activities on rds maybe it could be because you are",
    "start": "319840",
    "end": "326479"
  },
  {
    "start": "320000",
    "end": "320000"
  },
  {
    "text": "running production somewhere but you want to do dev test in rds and so you want to bring in delta type changes",
    "start": "326479",
    "end": "332560"
  },
  {
    "text": "and and the reason i'm going through these kind of use cases is because i want to make sure everybody understands our",
    "start": "332560",
    "end": "338000"
  },
  {
    "text": "intention with rds is to enable all these use cases now there are some which work better right now and some that",
    "start": "338000",
    "end": "343440"
  },
  {
    "text": "don't but all those features if they're not right now available they're definitely coming and",
    "start": "343440",
    "end": "350960"
  },
  {
    "text": "then obviously the last one which uh is possibly the most technically challenging sometimes but",
    "start": "350960",
    "end": "357520"
  },
  {
    "text": "also something that people need a lot use a lot is ongoing replication so you might have done a one-time",
    "start": "357520",
    "end": "363440"
  },
  {
    "text": "migration up front but then you want to keep that rds database in sync with something else",
    "start": "363440",
    "end": "368800"
  },
  {
    "text": "it could be an ec2 instance it could be your on-prem database it could be a database in another region",
    "start": "368800",
    "end": "374479"
  },
  {
    "text": "altogether and then obviously support this for everything for oracle mysql sql server",
    "start": "374479",
    "end": "379759"
  },
  {
    "text": "and like i said we don't support all of these right now but then we'll cover more about what's available with mysql in in",
    "start": "379759",
    "end": "385520"
  },
  {
    "text": "the third section today with bernie but the way you do ongoing replication would be you somehow",
    "start": "385520",
    "end": "392080"
  },
  {
    "text": "get the delta changes up right uh you either do log shipping or some kind of transaction replication",
    "start": "392080",
    "end": "398000"
  },
  {
    "text": "and the use cases you're trying to support with that are some kind of bi reporting query floating off loading",
    "start": "398000",
    "end": "403280"
  },
  {
    "text": "type thing or you are possibly doing um a migration project",
    "start": "403280",
    "end": "408479"
  },
  {
    "text": "but where you're sensitive to downtime you've got production running on ec2 or on-prem or in a colo and as you are",
    "start": "408479",
    "end": "415919"
  },
  {
    "text": "migrating you don't want to take extensive downtime so you want the first stage to be that you",
    "start": "415919",
    "end": "421199"
  },
  {
    "text": "brought in a snapshot you want an ongoing replication going on and then when you're comfortable",
    "start": "421199",
    "end": "426639"
  },
  {
    "text": "at that point you switch over your applications to point to rds and with minimum downtime you can achieve that migration",
    "start": "426639",
    "end": "434400"
  },
  {
    "text": "so having gone over that what do you need to think about what do you need to consider um these are again just interesting",
    "start": "434400",
    "end": "440160"
  },
  {
    "text": "questions and like i said you're probably thinking of them already but just to go over them and the reason",
    "start": "440160",
    "end": "446160"
  },
  {
    "text": "you want to think about these more deeply is because you might now hit into constraints you might hit into",
    "start": "446160",
    "end": "451759"
  },
  {
    "text": "into areas where you have to figure out if if this migration project is feasible or",
    "start": "451759",
    "end": "457280"
  },
  {
    "text": "do you need to change something around it so the first one obviously is number of databases size of databases current restraint",
    "start": "457280",
    "end": "464080"
  },
  {
    "text": "constraints around rds are three terabytes of storage um for oracle and mysql and one terabyte",
    "start": "464080",
    "end": "470160"
  },
  {
    "text": "for sql server so essentially if you're moving one instance from one place to another it's the data's got to fit that that's the",
    "start": "470160",
    "end": "476479"
  },
  {
    "text": "basic limitation that you need to address other things would be data types character sets do you have",
    "start": "476479",
    "end": "482319"
  },
  {
    "text": "unstructured data are you carrying over blobs how do you exactly plan to move that data what's the",
    "start": "482319",
    "end": "487520"
  },
  {
    "text": "physical mechanism right are you exporting a dump file is it flat files are you putting them into s3",
    "start": "487520",
    "end": "493440"
  },
  {
    "text": "are you shipping us a device there's an aws export import service so you can give us a physical device like a flash drive",
    "start": "493440",
    "end": "500960"
  },
  {
    "text": "mail it in and we'll restore it for you on s3 or on ebs volume so are you doing those are you moving",
    "start": "500960",
    "end": "507360"
  },
  {
    "text": "data using some kind of ftp sftp or uh bernie will cover this",
    "start": "507360",
    "end": "512800"
  },
  {
    "text": "uh use some tsunami udp type or any other file transfer mechanism right so that itself needs to be thought through",
    "start": "512800",
    "end": "520479"
  },
  {
    "text": "what else is involved it's typically never a database by itself right nobody needs a database",
    "start": "520479",
    "end": "526080"
  },
  {
    "text": "just for the sake of the database you need it because there are applications that are consuming it there are other databases that are linked up to it",
    "start": "526080",
    "end": "533279"
  },
  {
    "text": "there are reporting systems so what else is involved in that project and and what's going to happen once this",
    "start": "533279",
    "end": "539200"
  },
  {
    "text": "database moves over you got to think about that if there are links there you got to think of that and then um",
    "start": "539200",
    "end": "545519"
  },
  {
    "text": "any replication that you're doing or any ftp mechanism you're doing you need to think about the the networking implications do you need",
    "start": "545519",
    "end": "552560"
  },
  {
    "text": "to punch holes through a firewall are proxy ports needed how are you going to",
    "start": "552560",
    "end": "559360"
  },
  {
    "text": "measure the bandwidth and the latency requirements around this can you physically actually move that much data",
    "start": "559680",
    "end": "565040"
  },
  {
    "text": "in the available time and with all this obviously there is there's the constraints around",
    "start": "565040",
    "end": "570800"
  },
  {
    "text": "availability around downtime what is going on on if it was a production environment",
    "start": "570800",
    "end": "576640"
  },
  {
    "text": "what's going on to the production environment while you are in flight while you're moving your data so all of these",
    "start": "576640",
    "end": "582160"
  },
  {
    "text": "depending on the answers you give to these questions that would decide which mechanism you pick for data migration",
    "start": "582160",
    "end": "588240"
  },
  {
    "text": "and um how do you go about actually executing that project right and so this",
    "start": "588240",
    "end": "593839"
  },
  {
    "text": "is kind of just an overview and then obviously the second and third uh the other speakers will cover more in",
    "start": "593839",
    "end": "599200"
  },
  {
    "text": "depth about actually how to do it with oracle and and mysql so in general though as",
    "start": "599200",
    "end": "605279"
  },
  {
    "text": "you are migrating into rds there are a few best practices that you want to go over",
    "start": "605279",
    "end": "610399"
  },
  {
    "text": "before you migrate and then a few things that you want to do after you migrate and these are",
    "start": "610399",
    "end": "616160"
  },
  {
    "text": "applicable no matter which mechanism you use no matter which engine which database you're using",
    "start": "616160",
    "end": "621279"
  },
  {
    "text": "so the first thing is if it's a big data load that you're doing chances are you don't want applications",
    "start": "621279",
    "end": "628000"
  },
  {
    "text": "constantly accessing that data now obviously if you have a use case that allows that and it's possible for you to handle that that's fine",
    "start": "628000",
    "end": "634640"
  },
  {
    "text": "but the best practice would be don't have applications accessing your database while you're doing major uploads into the database",
    "start": "634640",
    "end": "641360"
  },
  {
    "text": "right the other would be take a consistent snapshot before you begin this project just in case something goes wrong and",
    "start": "641360",
    "end": "647040"
  },
  {
    "text": "you need to roll back to a prior snapshot you want to disable backups on that rds",
    "start": "647040",
    "end": "652079"
  },
  {
    "text": "instance because backups typically involve running bin logs or running archive logs and",
    "start": "652079",
    "end": "657680"
  },
  {
    "text": "other kinds of activities going on and they're just slowing down your bulk data load so disable backups before you do a major",
    "start": "657680",
    "end": "664240"
  },
  {
    "text": "data upload or migration you want to use single ac instances now this is typically we advise customers",
    "start": "664240",
    "end": "670640"
  },
  {
    "text": "that if you're running production or any kind of mission critical application you use multi-az",
    "start": "670640",
    "end": "676160"
  },
  {
    "text": "but this is a special case where you're loading a lot of data and the standby is a synchronous replicated instance so",
    "start": "676160",
    "end": "682000"
  },
  {
    "text": "it's only going to slow you down so for the duration of a bulk data upload convert it into a single ac",
    "start": "682000",
    "end": "688800"
  },
  {
    "text": "instance do your data load and then if you want you can go back to multi-ac and pick the right instance type for an",
    "start": "688800",
    "end": "695839"
  },
  {
    "text": "upload now this might be different from what you need for running production or running dev test right",
    "start": "695839",
    "end": "701200"
  },
  {
    "text": "you want to pick the beefiest instance that you want to use you want to possibly use provision diops",
    "start": "701200",
    "end": "707360"
  },
  {
    "text": "if you can you want to take a look at where the instance placement is going to be",
    "start": "707360",
    "end": "713519"
  },
  {
    "text": "if there are staging machines involved then you want to keep the ec2 staging instance in the same availability zone",
    "start": "713519",
    "end": "719279"
  },
  {
    "text": "where the database is running so basic stuff but again make a checklist and make sure you're going through all these",
    "start": "719279",
    "end": "724880"
  },
  {
    "text": "it's like a migration project if you want another analogy it's like a plane taking off right even though everybody",
    "start": "724880",
    "end": "729920"
  },
  {
    "text": "knows these are the steps you need to go through sometimes people forget so having a checklist is always a great idea",
    "start": "729920",
    "end": "736880"
  },
  {
    "text": "and then security configuration for migration can be different from what you need for running",
    "start": "736880",
    "end": "742079"
  },
  {
    "text": "regular production so you might have to open up more uh security rules you might have to",
    "start": "742079",
    "end": "748720"
  },
  {
    "text": "allow access to some other machines for staging or for database links or for uh for your on-site database or",
    "start": "748720",
    "end": "755120"
  },
  {
    "text": "something so the configuration might have to change before you do a migration now obviously the moment you're done",
    "start": "755120",
    "end": "760480"
  },
  {
    "text": "with a migration project you want to turn back your backups you want to go back to multi-ac um",
    "start": "760480",
    "end": "766399"
  },
  {
    "text": "if you were running my sql then at this point you want to start up your replicas because there was no point running replicas",
    "start": "766399",
    "end": "772240"
  },
  {
    "text": "during a major load uh tighten down that security again you might have opened up holes and ports in in your security",
    "start": "772240",
    "end": "778560"
  },
  {
    "text": "group you want to go back and enclose them you might want to enable notifications",
    "start": "778560",
    "end": "783600"
  },
  {
    "text": "using cloudwatch at this point because you might want to get notified if your cpu spikes above 80",
    "start": "783600",
    "end": "788800"
  },
  {
    "text": "or you're within 90 of your allocated storage so all good practices that you want to",
    "start": "788800",
    "end": "794240"
  },
  {
    "text": "go back and take a look at that instance after you're done with the migration because there might be things that you need to",
    "start": "794240",
    "end": "799360"
  },
  {
    "text": "to put back in place which you might have disabled for migration so that's essentially all i have to",
    "start": "799360",
    "end": "805120"
  },
  {
    "text": "cover and then i'll invite abdul to come up and and talk about the oracle migration steps",
    "start": "805120",
    "end": "815839"
  },
  {
    "text": "should be now all right i think pretty much everybody could hear me even without the mics",
    "start": "830720",
    "end": "838000"
  },
  {
    "text": "very excited to see a lot of familiar faces that are you know so excited about all the stuff",
    "start": "839120",
    "end": "844880"
  },
  {
    "text": "that's happening here and how did you like the three new service announcement we had for today",
    "start": "844880",
    "end": "852240"
  },
  {
    "text": "all right so let's uh look into how we can better use the one we",
    "start": "852720",
    "end": "858399"
  },
  {
    "text": "you know already have and most of you guys like and already using um can",
    "start": "858399",
    "end": "866800"
  },
  {
    "text": "can i have a show of hands of how many of you guys have tried migrating more than half a giga 500",
    "start": "866800",
    "end": "873199"
  },
  {
    "text": "gigabytes of data half a terabyte of data wow quite a few people so one of the",
    "start": "873199",
    "end": "878720"
  },
  {
    "text": "things that we heard from a lot of people is especially after we moved from a one",
    "start": "878720",
    "end": "884000"
  },
  {
    "text": "terabyte limit for rds to three terabyte limit now people want to move more data it's not",
    "start": "884000",
    "end": "890240"
  },
  {
    "text": "really small databases anymore and most of the people want to move this data with minimal",
    "start": "890240",
    "end": "896880"
  },
  {
    "text": "downtime and most people want to do it on under 24 hours in one day or over a weekend",
    "start": "896880",
    "end": "904000"
  },
  {
    "text": "and we agree that's the best way to do it you know when you you want to have minimal interruptions to your business",
    "start": "904000",
    "end": "910079"
  },
  {
    "text": "and you want to do it when you can bring down your production so that you can move your data so what i'm going to do here is",
    "start": "910079",
    "end": "917199"
  },
  {
    "text": "basically show you how we can do it uh it's kind of like a cooking demonstration because the whole",
    "start": "917199",
    "end": "924399"
  },
  {
    "text": "migration process takes a little while i wouldn't be really able to do the demo here",
    "start": "924399",
    "end": "929600"
  },
  {
    "text": "so i put together a really nice process around how to move bulk data from",
    "start": "929600",
    "end": "936320"
  },
  {
    "text": "your on-prem or your own data center to rds instance most of the components of",
    "start": "936320",
    "end": "942240"
  },
  {
    "text": "this process is something that you already know some of them you probably wouldn't",
    "start": "942240",
    "end": "947519"
  },
  {
    "text": "but many of you might not have really lined things up this way till this point to do it",
    "start": "947519",
    "end": "952880"
  },
  {
    "text": "this way one of the things that i have seen with many of our customers",
    "start": "952880",
    "end": "958240"
  },
  {
    "text": "um coming from where they are doing things on their own servers their own data",
    "start": "958240",
    "end": "964480"
  },
  {
    "text": "centers on-prem sometimes people don't really remember",
    "start": "964480",
    "end": "970079"
  },
  {
    "text": "you could do things differently on aws aws is quite a bit about not having",
    "start": "970079",
    "end": "975759"
  },
  {
    "text": "constraints right so it's okay to spin up 15 instances at a time if you have to right if you have 15",
    "start": "975759",
    "end": "983759"
  },
  {
    "text": "really huge data files that you're trying to compress you don't have to do it one by one you have to do that in parallel and you",
    "start": "983759",
    "end": "991279"
  },
  {
    "text": "have to bring them together into a database so we are actually going to leverage a lot of parallelism",
    "start": "991279",
    "end": "997040"
  },
  {
    "text": "and and multi-threading so how about a 500 gig database so that's what we are",
    "start": "997040",
    "end": "1003040"
  },
  {
    "text": "going to do so we'll move 500 gig database from on-prem server to rds",
    "start": "1003040",
    "end": "1010160"
  },
  {
    "text": "and when i was doing it i was actually doing this on my desktop which is a i7 with",
    "start": "1010160",
    "end": "1015279"
  },
  {
    "text": "four cores so you guys should be able to do this much better right so this is my database",
    "start": "1015279",
    "end": "1022839"
  },
  {
    "text": "i brought up a database pumped up a bunch of data we have 502 gigabytes of",
    "start": "1022839",
    "end": "1028400"
  },
  {
    "text": "data in there we'll take this data from the data oracle database running on my desktop",
    "start": "1028400",
    "end": "1034880"
  },
  {
    "text": "all the way into rds and see how quickly how efficiently we can do this so you",
    "start": "1034880",
    "end": "1040400"
  },
  {
    "text": "can replicate the same process the reason i picked 500 gigabytes is that seems to be kind of like the sweet",
    "start": "1040400",
    "end": "1046240"
  },
  {
    "text": "spot many people have more so if we can do this in an acceptable amount of time",
    "start": "1046240",
    "end": "1052000"
  },
  {
    "text": "you can probably extrapolate and figure out how much longer it's going to take you for two terabytes a day right and then the other thing is",
    "start": "1052000",
    "end": "1058480"
  },
  {
    "text": "shaquille touched up on this a little bit uh moving data is kind of like moving",
    "start": "1058480",
    "end": "1063679"
  },
  {
    "text": "homes right so one of the things that you do when you move homes is you throw away the stuff that you",
    "start": "1063679",
    "end": "1069760"
  },
  {
    "text": "don't need right and you probably donate a bunch of stuff i'm not recommending you donate",
    "start": "1069760",
    "end": "1075919"
  },
  {
    "text": "your data but avoid the data that you don't need to to move",
    "start": "1075919",
    "end": "1082880"
  },
  {
    "text": "a lot of times depending on how well tuned your database is for a particular purpose",
    "start": "1083760",
    "end": "1089200"
  },
  {
    "text": "like databases that are more for reporting that kind of purposes which is kind of like where you where you have",
    "start": "1089200",
    "end": "1095919"
  },
  {
    "text": "fewer inserts and updates and more selects you'll see a lot of indexes and i have seen databases",
    "start": "1095919",
    "end": "1103360"
  },
  {
    "text": "in which the index size the the amount of indexes is so big that it's like half the size of the",
    "start": "1103360",
    "end": "1108400"
  },
  {
    "text": "database you don't really need to move indexes you can bring the data over to rds and you can rebuild the indexes",
    "start": "1108400",
    "end": "1115200"
  },
  {
    "text": "right it all depends on how exactly you want to do it so so consider all these things avoid data",
    "start": "1115200",
    "end": "1120640"
  },
  {
    "text": "that you don't need to move a word indexes that doesn't need to be moved and also try to figure out",
    "start": "1120640",
    "end": "1127280"
  },
  {
    "text": "if if it's okay to move the data in parts in chunks many times depending on",
    "start": "1127280",
    "end": "1134080"
  },
  {
    "text": "what you are using your database for people use the same database for multiple applications so each",
    "start": "1134080",
    "end": "1139520"
  },
  {
    "text": "application uses its own schema so you don't need to move the entire database maybe you are moving your",
    "start": "1139520",
    "end": "1145120"
  },
  {
    "text": "application one by one so you can move the schemas one by one right so figure out what is the best way to",
    "start": "1145120",
    "end": "1150240"
  },
  {
    "text": "move data so if you let's say i have a one terabyte database if you apply most of the things that i",
    "start": "1150240",
    "end": "1155679"
  },
  {
    "text": "mentioned so far it might come down to our 500 gig database right",
    "start": "1155679",
    "end": "1160720"
  },
  {
    "text": "so let's see wha the process of migration how exactly we are going to do",
    "start": "1160720",
    "end": "1166400"
  },
  {
    "start": "1165000",
    "end": "1165000"
  },
  {
    "text": "it what you are seeing there is actually um the the on-prem",
    "start": "1166400",
    "end": "1171760"
  },
  {
    "text": "your own data center you have the oracle database sitting there so we are setting up our vpc uh within",
    "start": "1171760",
    "end": "1179200"
  },
  {
    "text": "the vpc we have an rdb rds oracle instance running and like",
    "start": "1179200",
    "end": "1186000"
  },
  {
    "text": "shakil was a little shy to tell you to pick the largest instance possible he said the optimal instance",
    "start": "1186000",
    "end": "1192480"
  },
  {
    "text": "i'll tell you to pick the largest instance because you are going to do this only for a little while till the migration is",
    "start": "1192480",
    "end": "1199039"
  },
  {
    "text": "done right six hours eight hours 24 hours whatever it is so the your cost for that size of instance",
    "start": "1199039",
    "end": "1206080"
  },
  {
    "text": "is only for that period of time then you can cut down to whatever your your actual size is",
    "start": "1206080",
    "end": "1212480"
  },
  {
    "text": "so in addition to our rds instance we'll set up an ec2 instance",
    "start": "1212480",
    "end": "1217840"
  },
  {
    "text": "that this is going to be our bridge instance or staging instance we are going to get your data from your existing database into this",
    "start": "1217840",
    "end": "1224720"
  },
  {
    "text": "ec2 instance you'll see in a little bit why we want to do it that way and then from there to",
    "start": "1224720",
    "end": "1230240"
  },
  {
    "text": "the rds instance the the main reason i'm doing it this way is you don't have access directly to the box where ids is",
    "start": "1230240",
    "end": "1237919"
  },
  {
    "text": "running right you don't have operating system level access so you cannot get date files over there",
    "start": "1237919",
    "end": "1243360"
  },
  {
    "text": "so some of the some of the methods we have on our documents on how to",
    "start": "1243360",
    "end": "1248720"
  },
  {
    "text": "migrate data to ideas is using database links right using import export",
    "start": "1248720",
    "end": "1256159"
  },
  {
    "text": "or using data bump export in the network mode we are actually going to do in a file mode so we need to move files into the audio",
    "start": "1256159",
    "end": "1263760"
  },
  {
    "text": "instance unfortunately we cannot do that at the operating system level so you are going to use this bridge",
    "start": "1263760",
    "end": "1269840"
  },
  {
    "text": "instance in between to get you there so the first part of doing this whole thing is actually doing",
    "start": "1269840",
    "end": "1276080"
  },
  {
    "text": "the data pump export in your on-prem date instance so we did that our database our",
    "start": "1276080",
    "end": "1282080"
  },
  {
    "text": "our storage is full with the files that we need we move those files we find out the best",
    "start": "1282080",
    "end": "1287280"
  },
  {
    "text": "way to move those files to the ec2 instance then from the ec2 instance we move those",
    "start": "1287280",
    "end": "1292720"
  },
  {
    "text": "files to the rds instance then import that into the rds so this is the first part of the whole process",
    "start": "1292720",
    "end": "1299440"
  },
  {
    "text": "we use oracle data pump export data pump export is a very very powerful",
    "start": "1299440",
    "end": "1305919"
  },
  {
    "text": "tool it lets you pick and choose the kind of data that you want in this case you can see i i'm i said",
    "start": "1305919",
    "end": "1312480"
  },
  {
    "text": "full a sql y means i'm exporting the entire database you can select you know a particular schema or a",
    "start": "1312480",
    "end": "1320000"
  },
  {
    "text": "bunch of tables or you can even use a query to cut down the amount of data that you want to bring right",
    "start": "1320000",
    "end": "1326400"
  },
  {
    "text": "a couple of other key things you need to pay attention to there is the parallelism i'm using this in a",
    "start": "1326400",
    "end": "1331600"
  },
  {
    "text": "multi-threaded fashion we're saying parallel is equal to eight so eight processes is going to work on this",
    "start": "1331600",
    "end": "1336720"
  },
  {
    "text": "together and compression very very very important you you compress your data before you get into",
    "start": "1336720",
    "end": "1343280"
  },
  {
    "text": "the file so i'm saying compress the data so when i'm done with all this i'll",
    "start": "1343280",
    "end": "1349039"
  },
  {
    "text": "my my total database the data size that i'm getting out of the database is going to much smaller than",
    "start": "1349039",
    "end": "1354400"
  },
  {
    "start": "1353000",
    "end": "1353000"
  },
  {
    "text": "the actual data so this is a screenshot of me kicking off the process you can see actually started the process",
    "start": "1354400",
    "end": "1362000"
  },
  {
    "text": "at 15 18 51 as the time the process started and a letter run multi-threaded i have",
    "start": "1362000",
    "end": "1370000"
  },
  {
    "text": "eight threads going creating three files at a time because you know you probably",
    "start": "1370000",
    "end": "1375120"
  },
  {
    "text": "noticed in the picture i actually had three disks on my desktop so because i had three disks i have",
    "start": "1375120",
    "end": "1381440"
  },
  {
    "text": "three independent i o subsystems i want to leverage that maybe your server has seven disks",
    "start": "1381440",
    "end": "1386880"
  },
  {
    "text": "then you do seven files at the same time so it's all about really utilizing what you already have",
    "start": "1386880",
    "end": "1393280"
  },
  {
    "text": "so i'm creating three files at a time using eight threads and i got started at 15 18",
    "start": "1393280",
    "end": "1400240"
  },
  {
    "text": "and 1736 i'm done exporting our 500 gigs of data",
    "start": "1400240",
    "end": "1405360"
  },
  {
    "start": "1404000",
    "end": "1404000"
  },
  {
    "text": "so that is 2 hours and 18 minutes so that's all it took me to export 500",
    "start": "1405360",
    "end": "1411520"
  },
  {
    "text": "gigs of data so i got that 500 gigs of data and i need to move this",
    "start": "1411520",
    "end": "1419200"
  },
  {
    "text": "from my my on-prem database over to the ec2 instance right",
    "start": "1419600",
    "end": "1427200"
  },
  {
    "start": "1426000",
    "end": "1426000"
  },
  {
    "text": "that's my 175 gigs of data from the 18 files that we have so my 500 gigs of database from export",
    "start": "1427200",
    "end": "1435360"
  },
  {
    "text": "and the compression came down to 18 files uh 100 totaling 175 gigs",
    "start": "1435360",
    "end": "1441760"
  },
  {
    "text": "so now i move this 175 gigs to my ec2 instance what i did was i used a udp-based tool",
    "start": "1441760",
    "end": "1449120"
  },
  {
    "start": "1444000",
    "end": "1444000"
  },
  {
    "text": "called tsunami this is again you know optimizing that part of your process",
    "start": "1449120",
    "end": "1455039"
  },
  {
    "text": "you don't want to use ftp or sftp or something because it's going to use a very tiny",
    "start": "1455039",
    "end": "1460880"
  },
  {
    "text": "part of your network bandwidth so what we want to do is we want to saturate our network bandwidth we want to",
    "start": "1460880",
    "end": "1467039"
  },
  {
    "text": "make use of every piece of bandwidth we have available so that we can move whole chunks of data",
    "start": "1467039",
    "end": "1472159"
  },
  {
    "text": "in parallel because we want to get it there as soon as possible right we want to get it done within our 24-hour window",
    "start": "1472159",
    "end": "1478720"
  },
  {
    "text": "that we originally talked about so this tsunami is an open source tool",
    "start": "1478720",
    "end": "1484000"
  },
  {
    "text": "you can just google it or use the url that's in that slide to go get it",
    "start": "1484000",
    "end": "1490000"
  },
  {
    "text": "there are many other similar commercially available tools like aspara or there are van optimizers",
    "start": "1490000",
    "end": "1496159"
  },
  {
    "text": "like river but white water you can use any one of these i just choose to use tsunami so this is how you install and",
    "start": "1496159",
    "end": "1503520"
  },
  {
    "text": "compile tsunami once that is done make sure you have the right port open to get the data out very very important",
    "start": "1503520",
    "end": "1511200"
  },
  {
    "text": "so once that is done the next step is to go to our install the tsunami on",
    "start": "1511200",
    "end": "1517120"
  },
  {
    "start": "1515000",
    "end": "1515000"
  },
  {
    "text": "both the source server where i'm getting the data out from as well as in my target server my target",
    "start": "1517120",
    "end": "1523520"
  },
  {
    "text": "server in this case is the ec2 instance right so what kind of ec2 instance would you pick who will pick the smallest ec2",
    "start": "1523520",
    "end": "1529919"
  },
  {
    "text": "instance show of hands who would pick the largest ec2 instance",
    "start": "1529919",
    "end": "1535760"
  },
  {
    "text": "yep that's what we want because we are going to use this you know only for a few hours right and this tsunami is quite",
    "start": "1535760",
    "end": "1542640"
  },
  {
    "text": "demanding i think that's why it's called tsunami right so you want to you want to use the largest possible instance",
    "start": "1542640",
    "end": "1549679"
  },
  {
    "text": "with multiple ebs volume so you know again independent i o subsystem",
    "start": "1549679",
    "end": "1556240"
  },
  {
    "text": "so you can write multiple files at the same time so you want to install tsunami on the",
    "start": "1556240",
    "end": "1561440"
  },
  {
    "text": "source side which is on your on-prem database instant as well as we want to install the same",
    "start": "1561440",
    "end": "1566559"
  },
  {
    "text": "thing in the ec2 instance so now one acts as a server and the other acts as a client so we are going to get the",
    "start": "1566559",
    "end": "1572960"
  },
  {
    "text": "data from the source the files from the source all the way into the destination which is our ec2",
    "start": "1572960",
    "end": "1579760"
  },
  {
    "text": "instance now you remember we saw 18 files that were",
    "start": "1579760",
    "end": "1585200"
  },
  {
    "text": "kind of like spit up from our original database right from our source oracle database",
    "start": "1585200",
    "end": "1591200"
  },
  {
    "text": "and we were doing three files at a time i told you i i picked three files at a time because i had three independent",
    "start": "1591200",
    "end": "1596559"
  },
  {
    "text": "hard disks so i i didn't wait for all the 18 files to come out",
    "start": "1596559",
    "end": "1601840"
  },
  {
    "text": "it's not like you know it's like you don't have to wait for all the cakes to be baked before you start eating",
    "start": "1601840",
    "end": "1607120"
  },
  {
    "text": "right as soon as the first three files came out i started moving it using tsunami",
    "start": "1607120",
    "end": "1612559"
  },
  {
    "text": "so again parallelism right you can do multiple things in parallel you don't have to wait for the whole process one step to be done to start the",
    "start": "1612559",
    "end": "1619679"
  },
  {
    "text": "next one so the first three came in i'm moving into ec2 the next three comes",
    "start": "1619679",
    "end": "1624960"
  },
  {
    "text": "moving into ec2 moving this 175 gigs of data using udp using tsunami took me 2 hours",
    "start": "1624960",
    "end": "1632880"
  },
  {
    "text": "and 23 minutes isn't that cool when i do the same thing",
    "start": "1632880",
    "end": "1638399"
  },
  {
    "text": "using sftp it took me it takes me about 38 hours so that's a good amount of difference",
    "start": "1638399",
    "end": "1644880"
  },
  {
    "text": "right so i got the 175 gigs now in my ec to",
    "start": "1644880",
    "end": "1651200"
  },
  {
    "text": "my large ec2 instance right here is something that uh that you need to keep in mind if you",
    "start": "1651200",
    "end": "1659039"
  },
  {
    "start": "1658000",
    "end": "1658000"
  },
  {
    "text": "do not know i don't know if you have enough documentation on this already we have a directory this",
    "start": "1659039",
    "end": "1665279"
  },
  {
    "text": "is called the oracle directory object in the rds instance called data",
    "start": "1665279",
    "end": "1670799"
  },
  {
    "text": "underscore pump underscore dir so this is a directory that you can",
    "start": "1670799",
    "end": "1676320"
  },
  {
    "text": "access outside of rds instance though you don't have access directly at the operating system level",
    "start": "1676320",
    "end": "1683200"
  },
  {
    "text": "this is a directory where you can write and read files from in the rds instance so this is what we are going to",
    "start": "1683200",
    "end": "1689279"
  },
  {
    "text": "really make use of so what i'm going to talk about is migrating data to rds",
    "start": "1689279",
    "end": "1694880"
  },
  {
    "text": "right so so what we'll be looking into is moving these files into the rds",
    "start": "1694880",
    "end": "1701039"
  },
  {
    "text": "instance you can actually make use of this directory to also get data out of your rds instance i'm not going to talk about",
    "start": "1701039",
    "end": "1707600"
  },
  {
    "text": "it but just keep in mind so so this is a very good front of yours",
    "start": "1707600",
    "end": "1713600"
  },
  {
    "text": "i'm going to use this pulse script um it's a very simple pole script oh by the way actually uh next wednesday",
    "start": "1713919",
    "end": "1720799"
  },
  {
    "start": "1718000",
    "end": "1718000"
  },
  {
    "text": "you will you'll get all the decks of all these sessions that you have attended as pdf files so all this is going to be",
    "start": "1720799",
    "end": "1727520"
  },
  {
    "text": "available to you if you want to make use of it so this is basically a single perl script",
    "start": "1727520",
    "end": "1732880"
  },
  {
    "text": "all that it does is it it reads the file that we move to the ec2 instance and",
    "start": "1732880",
    "end": "1739120"
  },
  {
    "text": "writes it out to the which directory that i talked about",
    "start": "1739120",
    "end": "1744480"
  },
  {
    "text": "yes the data file out directory right it gets into the data file out directory but keep this is the the what paul does",
    "start": "1744480",
    "end": "1752640"
  },
  {
    "text": "is it acts we are actually using uh oracle utl file package",
    "start": "1752640",
    "end": "1759840"
  },
  {
    "text": "to do the operation because what we are doing is not moving the files at the operating system level we are using this using oracle technology so",
    "start": "1759840",
    "end": "1767279"
  },
  {
    "text": "there's a utl file package that oracle provides we are using that to move this data into the oracle",
    "start": "1767279",
    "end": "1773919"
  },
  {
    "text": "directory object",
    "start": "1773919",
    "end": "1780000"
  },
  {
    "text": "that's the rest of the code so i'm just opening up reading the files one chunk at a time and moving it to the",
    "start": "1780000",
    "end": "1786240"
  },
  {
    "text": "rds file now if i'm doing this one file at a time",
    "start": "1786240",
    "end": "1791679"
  },
  {
    "text": "it's again going to take me a while right because this is a single threaded process this is my own script i'm actually doing",
    "start": "1791679",
    "end": "1798720"
  },
  {
    "text": "a little bit of chunk at a time and moving you know in a serial fashion so this is going to take a while if i do",
    "start": "1798720",
    "end": "1804480"
  },
  {
    "text": "it one after the other so the best way for you to do this would be spinning up multiple of these scripts so",
    "start": "1804480",
    "end": "1811679"
  },
  {
    "text": "you can run it in the background run that script with the name of the file that we want to move",
    "start": "1811679",
    "end": "1817520"
  },
  {
    "text": "and again you don't have to wait for tsunami to bring in all the files over here right so what we the way i did it was i",
    "start": "1817520",
    "end": "1825840"
  },
  {
    "text": "started the export of the database using data pump export i i'm going to eventually get eight file",
    "start": "1825840",
    "end": "1832399"
  },
  {
    "text": "18 files but as soon as i got three files i kicked off tsunami to move those three files to the ec2",
    "start": "1832399",
    "end": "1838159"
  },
  {
    "text": "instance as soon as those three files came to the ec2 instance i started off my script to move those",
    "start": "1838159",
    "end": "1845279"
  },
  {
    "text": "files into the rds instance right at the same time there is still data export happening from the database",
    "start": "1845279",
    "end": "1852000"
  },
  {
    "text": "the same time there's more file coming through tsunami so now we have all these three files",
    "start": "1852000",
    "end": "1857360"
  },
  {
    "text": "moving in parallel so how long it took me it took me three",
    "start": "1857360",
    "end": "1863039"
  },
  {
    "text": "hours and 19 minutes to get this from the ec2 instance into the rds instance",
    "start": "1863039",
    "end": "1870320"
  },
  {
    "text": "keep in mind this is the total time it took me for that particular process that doesn't mean it is the time that is",
    "start": "1870320",
    "end": "1876559"
  },
  {
    "text": "taken from the previous after the previous process because we are doing it in parallel right so now our our files are into the",
    "start": "1876559",
    "end": "1883440"
  },
  {
    "text": "in the rds instance once the files are in the rds instance now i need to use the same process the opposite of the",
    "start": "1883440",
    "end": "1890320"
  },
  {
    "text": "process that i used in the first place to get the data out from the oracle database",
    "start": "1890320",
    "end": "1895679"
  },
  {
    "text": "so i use oracle data pump export to export the data out of the oracle database in the first place",
    "start": "1895679",
    "end": "1901279"
  },
  {
    "text": "now i move that data to our target database the next thing i want to do is use the opposite of it the import",
    "start": "1901279",
    "end": "1908159"
  },
  {
    "text": "process to get the data into the rds oracle database right so what i'm going to do is",
    "start": "1908159",
    "end": "1913519"
  },
  {
    "text": "write a little bit of a pl sql script which uses the oracle's technology",
    "start": "1913519",
    "end": "1919039"
  },
  {
    "text": "oracle data pump so just like utl file oracle provides a package called dbms",
    "start": "1919039",
    "end": "1924799"
  },
  {
    "start": "1924000",
    "end": "1924000"
  },
  {
    "text": "data pump you can call different functions in this package to do",
    "start": "1924799",
    "end": "1930000"
  },
  {
    "text": "the same thing that you can do from a shell script or from the data pump export utility on command line",
    "start": "1930000",
    "end": "1937200"
  },
  {
    "text": "so you can use this pl sql package to achieve the same thing so i'm going to run this pl sql package",
    "start": "1937200",
    "end": "1942960"
  },
  {
    "text": "this pl sql package is going to work on the files that i imported this is going in just like we did first",
    "start": "1942960",
    "end": "1950159"
  },
  {
    "text": "we did a parallel import from a we use eight threads we can do the same thing here as well we",
    "start": "1950159",
    "end": "1956080"
  },
  {
    "text": "can use have a multi-threaded import so once i kick off kick this off i'm going to get all the data",
    "start": "1956080",
    "end": "1962320"
  },
  {
    "text": "into the rds instance so that took me 3 hours and 49 minutes",
    "start": "1962320",
    "end": "1968559"
  },
  {
    "start": "1968000",
    "end": "1968000"
  },
  {
    "text": "how much did it take me in total because i was doing these things in parallel it took me 6 hours and 42 minutes to",
    "start": "1968559",
    "end": "1975440"
  },
  {
    "text": "move 500 gigs of data from my on-prem oracle database into rds",
    "start": "1975440",
    "end": "1980799"
  },
  {
    "text": "database who thinks that's cool all right so now so that's 500 gigs of data right",
    "start": "1980799",
    "end": "1988159"
  },
  {
    "text": "so if you have two terabytes of data to move it's probably going to take you four times this time maybe",
    "start": "1988159",
    "end": "1993679"
  },
  {
    "text": "maybe less so even if you try to move four terabytes of data once you chop off the stuff that you don't need",
    "start": "1993679",
    "end": "2000080"
  },
  {
    "text": "you can still get it all done within 24 hours just make sure you follow the same",
    "start": "2000080",
    "end": "2005120"
  },
  {
    "text": "methodology you do things in parallel you use multi-threaded processes",
    "start": "2005120",
    "end": "2010399"
  },
  {
    "text": "so one of the things that i in that import export process i use the compression to get compressed data",
    "start": "2010399",
    "end": "2017039"
  },
  {
    "start": "2012000",
    "end": "2012000"
  },
  {
    "text": "that's how we got 500 gigs down to 175 gigs",
    "start": "2017039",
    "end": "2022159"
  },
  {
    "text": "you can use compression that way only if you are using oracle database enterprise version",
    "start": "2022159",
    "end": "2029200"
  },
  {
    "text": "since a lot of our customers who use rds use oracle database standard version",
    "start": "2029200",
    "end": "2034559"
  },
  {
    "text": "i think many of you might want to might be using standard version currently and want to",
    "start": "2034559",
    "end": "2041519"
  },
  {
    "text": "make use of the export import process available there so if you're using standard pro version you won't be able to do compress",
    "start": "2041519",
    "end": "2048638"
  },
  {
    "text": "e equal to y you could do it you know the engine will ignore it so you it won't get compressed so in that",
    "start": "2048639",
    "end": "2054878"
  },
  {
    "text": "case you are going to have to get the files out first then you'll have to compress it yourself",
    "start": "2054879",
    "end": "2060560"
  },
  {
    "text": "right you'll need to zip it so do not move the files using udp or",
    "start": "2060560",
    "end": "2066560"
  },
  {
    "text": "whatever tool of your choice without compressing the files it's very important to compress the files that's one of the main part of",
    "start": "2066560",
    "end": "2072960"
  },
  {
    "text": "you know cutting down the amount of time right again use a zip utility to come",
    "start": "2072960",
    "end": "2079358"
  },
  {
    "text": "compress it but i would suggest you do not use a regular zip or gzip because those are single threaded",
    "start": "2079359",
    "end": "2086320"
  },
  {
    "text": "process so once you once you get out of this room there are two things i want to put in",
    "start": "2086320",
    "end": "2091358"
  },
  {
    "text": "you know keep in your head one is parallelism and compression we want to achieve both of these things together",
    "start": "2091359",
    "end": "2097599"
  },
  {
    "text": "to to stay within our timeline right so to if you don't have enterprise edition",
    "start": "2097599",
    "end": "2102720"
  },
  {
    "text": "to compress the file you want to use any utility that that's multi-threaded so there are a bunch of",
    "start": "2102720",
    "end": "2107920"
  },
  {
    "text": "other uh zipping utilities available like bzip2 or 7-zip or pixy so one",
    "start": "2107920",
    "end": "2115119"
  },
  {
    "text": "use one of these again do not wait for all the files to come out as soon as the first chunk of files",
    "start": "2115119",
    "end": "2120240"
  },
  {
    "text": "comes out you start compressor right so you're running multiple compression processes",
    "start": "2120240",
    "end": "2125359"
  },
  {
    "text": "in parallel so you can cut down the total amount of time now once you get this data over to ec2",
    "start": "2125359",
    "end": "2131599"
  },
  {
    "text": "now you have to decompress it right you have to uncompress it before you can move the data to",
    "start": "2131599",
    "end": "2137760"
  },
  {
    "text": "the rds instance right so that process you need to do a little different than",
    "start": "2137760",
    "end": "2142800"
  },
  {
    "text": "what i showed you in my in our case since the files are compressed by",
    "start": "2142800",
    "end": "2148240"
  },
  {
    "text": "the export process itself the data pump itself when you import it using data pump data",
    "start": "2148240",
    "end": "2153920"
  },
  {
    "text": "pump knows that this is compressed data and it knows how to uncompress it before it writes into the",
    "start": "2153920",
    "end": "2159200"
  },
  {
    "text": "database right when you are actually not using data pump compression",
    "start": "2159200",
    "end": "2164720"
  },
  {
    "text": "you are going to do it yourself and you are going to uncompress it before you get it over to rds",
    "start": "2164720",
    "end": "2170800"
  },
  {
    "text": "right now let's say you got this 18 files into ec2 again if you are going to uncompress it",
    "start": "2170800",
    "end": "2177280"
  },
  {
    "text": "on ec2 it's going to take you a while but since you can use you can spin up multiple ec2 instance",
    "start": "2177280",
    "end": "2184079"
  },
  {
    "text": "i i would suggest you do it that way so if you once you have all the files compressed in the source",
    "start": "2184079",
    "end": "2189599"
  },
  {
    "text": "instead of using one ec2 instance like i used you use 18 ec2 instances and you run",
    "start": "2189599",
    "end": "2195760"
  },
  {
    "text": "tsunami there egc2 instance picks one file and all of them uncompress them in parallel and all of them are pushing",
    "start": "2195760",
    "end": "2202560"
  },
  {
    "text": "into rds in parallel so parallelism and compression the two very key uh part of this whole",
    "start": "2202560",
    "end": "2210480"
  },
  {
    "text": "process just to summarize everything that i said so far so reduce the data set to the",
    "start": "2210480",
    "end": "2216240"
  },
  {
    "text": "optimal size compression and parallel processing and independent i o so when it comes to you know pick the",
    "start": "2216240",
    "end": "2223280"
  },
  {
    "text": "number of files based on the number of disks you have on on the ec2 side also pick",
    "start": "2223280",
    "end": "2229440"
  },
  {
    "text": "multiple ebs volumes and make sure you choose pi ops evs volumes nothing else pi ops",
    "start": "2229440",
    "end": "2236720"
  },
  {
    "text": "you could also pick uh your ec2 instance with ssds like high io instance right so",
    "start": "2236720",
    "end": "2243440"
  },
  {
    "text": "depending on the size of your file and you can make use of the the local disk and for data upload you",
    "start": "2243440",
    "end": "2250000"
  },
  {
    "text": "know i would i would strongly suggest tsunami but if you find something better you know you could use that as well and let",
    "start": "2250000",
    "end": "2255599"
  },
  {
    "start": "2251000",
    "end": "2251000"
  },
  {
    "text": "me know and it comes to um data upload again",
    "start": "2255599",
    "end": "2262160"
  },
  {
    "start": "2257000",
    "end": "2257000"
  },
  {
    "text": "pick the largest possible rds instance you can always you know scale it down once the process",
    "start": "2262160",
    "end": "2268000"
  },
  {
    "text": "is done shaquille covered a lot of you know pre and post migration steps",
    "start": "2268000",
    "end": "2273280"
  },
  {
    "text": "there are some other things that you need to do before migration like turning off the backup and make it into single ac and whatnot so",
    "start": "2273280",
    "end": "2280960"
  },
  {
    "text": "once you're done with that one one of the next thing to do with go scale up the instance once you get the largest",
    "start": "2280960",
    "end": "2287119"
  },
  {
    "text": "type of instance you start your migration when you're done with your migration scale it down",
    "start": "2287119",
    "end": "2292720"
  },
  {
    "text": "so now we are done with a one-time load of data right a lot of times one time is not enough because by the",
    "start": "2292720",
    "end": "2298640"
  },
  {
    "start": "2296000",
    "end": "2296000"
  },
  {
    "text": "time you are done with the data upload some data might have changed so you want to do some periodic update",
    "start": "2298640",
    "end": "2304000"
  },
  {
    "text": "as well so for this you can use our all well documented processes using",
    "start": "2304000",
    "end": "2309200"
  },
  {
    "text": "data above network mode or materialize views or even your custom triggers so now you are we are talking about",
    "start": "2309200",
    "end": "2315200"
  },
  {
    "text": "moving a little bit of data you you moved 500 gigs of data after i moved 500 gigs of data the",
    "start": "2315200",
    "end": "2320480"
  },
  {
    "text": "change data is probably like you know 20 megabytes or 200 megabytes compared",
    "start": "2320480",
    "end": "2325520"
  },
  {
    "text": "to the data that we moved this is pretty small so you can either use the data pump network mode operation",
    "start": "2325520",
    "end": "2331520"
  },
  {
    "text": "or you can use custom triggers to move the data or you can use something like golden gate so golden gate is again",
    "start": "2331520",
    "end": "2338560"
  },
  {
    "text": "something a lot of people ask us about for real-time replication even when you",
    "start": "2338560",
    "end": "2344160"
  },
  {
    "text": "people want to use rds as the reporting instance or or a dr instance or a geo-redundant",
    "start": "2344160",
    "end": "2351599"
  },
  {
    "text": "instance something like that so so we want to we want to keep the data in your production instance in sync with the rds instance right the",
    "start": "2351599",
    "end": "2358800"
  },
  {
    "text": "best way to do that is using some tool that'll move data in real time",
    "start": "2358800",
    "end": "2364400"
  },
  {
    "text": "but even moving data in real time first you want to do an initial load of the existing data right",
    "start": "2364400",
    "end": "2370079"
  },
  {
    "text": "you cannot really move the existing two terabytes of data in real time so take a little bit of downtime like we",
    "start": "2370079",
    "end": "2377280"
  },
  {
    "text": "what we mentioned so you can export all the data up and bring it to rds now your existing data is an rds then you",
    "start": "2377280",
    "end": "2384320"
  },
  {
    "text": "can use something like global golden gate to continually move data into rds",
    "start": "2384320",
    "end": "2390720"
  },
  {
    "text": "so this opens up a whole new set of capabilities for you now you can",
    "start": "2390720",
    "end": "2397200"
  },
  {
    "text": "have multiple databases for doing different things you can have your production database where you are",
    "start": "2397200",
    "end": "2402560"
  },
  {
    "text": "doing all your old tp operations you'll have a replicated database using goldengate where you could be doing your",
    "start": "2402560",
    "end": "2408560"
  },
  {
    "text": "reporting and analytics and whatnot if you're if you're everything that i",
    "start": "2408560",
    "end": "2414960"
  },
  {
    "text": "mentioned so far again keep in mind this is for large data sets right because there's a bunch of things that you need",
    "start": "2414960",
    "end": "2420640"
  },
  {
    "text": "to do to get the data over here so if your database is pretty small you don't need to do any of this",
    "start": "2420640",
    "end": "2425920"
  },
  {
    "start": "2421000",
    "end": "2421000"
  },
  {
    "text": "so what is small it's kind of like a very relative right it's kind of like uh the the the largest",
    "start": "2425920",
    "end": "2432319"
  },
  {
    "text": "pan size that was available in gulliver's island was probably small for gulliver right so",
    "start": "2432319",
    "end": "2438880"
  },
  {
    "text": "depending on the the amount of data that you have the whether you have data direct connect or or the latency of your",
    "start": "2438880",
    "end": "2446000"
  },
  {
    "text": "network the bandwidth you have the small size means different things for different people",
    "start": "2446000",
    "end": "2451440"
  },
  {
    "text": "so i would say anything between like 10 megabytes to you know a couple of hundred megabytes of data you don't need to use",
    "start": "2451440",
    "end": "2458640"
  },
  {
    "text": "the process that i outline so you can use any one of these you can use the oracle import export or data from network mode or sql",
    "start": "2458640",
    "end": "2466000"
  },
  {
    "text": "order or materialize fuse materialized views is a very very efficient way of getting data",
    "start": "2466000",
    "end": "2472000"
  },
  {
    "text": "synchronized and data moved from small data sets moved from your existing database to rds",
    "start": "2472000",
    "end": "2477920"
  },
  {
    "text": "and with that i'd like to invite my colleague bernie to take you through the same process for my sequel and it's more exciting there",
    "start": "2477920",
    "end": "2486640"
  },
  {
    "text": "can you guys hear me yeah so um abdul covered um the migrating",
    "start": "2491920",
    "end": "2498720"
  },
  {
    "text": "approach approaches for oracle so in the next 15 minutes what i'll be doing is i'll be",
    "start": "2498720",
    "end": "2504560"
  },
  {
    "text": "walking you through the migration approaches for my sequel with really you know less downtime you",
    "start": "2504560",
    "end": "2511040"
  },
  {
    "text": "know minimal downtime is what we're going to talk about you know which i'm going to cover i mean let's say you have an application with the",
    "start": "2511040",
    "end": "2516960"
  },
  {
    "text": "database of the my sequel database on premise so we try to you know",
    "start": "2516960",
    "end": "2522160"
  },
  {
    "text": "the use case that i used and then and the demo that i had which we're going to be doing some screenshots here um",
    "start": "2522160",
    "end": "2529040"
  },
  {
    "text": "is something you know so we take a mysql dump of your existing mysql on your",
    "start": "2529040",
    "end": "2534720"
  },
  {
    "text": "on-premise database using mysql dump to a staging location and then the key thing is having your",
    "start": "2534720",
    "end": "2541440"
  },
  {
    "text": "network set up you know the security configurations make sure this 44026 the ports open on",
    "start": "2541440",
    "end": "2548000"
  },
  {
    "text": "your firewall so that based on the protocols that you use you punch those holes to migrate this",
    "start": "2548000",
    "end": "2554240"
  },
  {
    "text": "and then create you know choose a region of your choice and then have an ec2 staging instance",
    "start": "2554240",
    "end": "2561200"
  },
  {
    "text": "because we're gonna we're gonna follow through the same approach of using an exist you know using a",
    "start": "2561200",
    "end": "2566640"
  },
  {
    "text": "staging ec2 instance to migrate the my sequel dumb from on premise into that ec2 instance",
    "start": "2566640",
    "end": "2573599"
  },
  {
    "text": "as as abdul mentioned you know make sure you have an ec2 instance that that's big enough",
    "start": "2573599",
    "end": "2579599"
  },
  {
    "text": "and then have you know try to choose between two things one is an instance that has femoral storage",
    "start": "2579599",
    "end": "2585040"
  },
  {
    "text": "which has the ssd drives is one option the second option is an ec2 instance with pi ops ebs volumes",
    "start": "2585040",
    "end": "2592560"
  },
  {
    "text": "so those are the two in this case i use the pi ops on a an ac2 instance this will be your",
    "start": "2592560",
    "end": "2599440"
  },
  {
    "text": "staging server and then create an rds my sequel database",
    "start": "2599440",
    "end": "2606640"
  },
  {
    "text": "and then move your staging the mysql dump the backup of your on-premise mysql",
    "start": "2606640",
    "end": "2612480"
  },
  {
    "text": "into the staging server right we'll walk through a couple of things that i tried and then show you you know which one",
    "start": "2612480",
    "end": "2618720"
  },
  {
    "text": "that i did in this case it's fc if you know scp or tsunami any of that",
    "start": "2618720",
    "end": "2624400"
  },
  {
    "text": "and then once the the transfer of the backup is done we'll basically use the staging server",
    "start": "2624400",
    "end": "2631599"
  },
  {
    "text": "to import the data into mysql the mysql database and the rds",
    "start": "2631599",
    "end": "2636720"
  },
  {
    "text": "once we do that i mean once the load is done we make the on-premise database as the",
    "start": "2636720",
    "end": "2642400"
  },
  {
    "text": "master database and then configure the mysql rds",
    "start": "2642400",
    "end": "2648079"
  },
  {
    "start": "2647000",
    "end": "2647000"
  },
  {
    "text": "instance has a slave instance and then we enable an application right so that there's a delta between the time",
    "start": "2648079",
    "end": "2655280"
  },
  {
    "text": "between you started your backup and then so that you know you capture i'll walk you through the process of you",
    "start": "2655280",
    "end": "2661440"
  },
  {
    "text": "know the file there's a sql log bin file and the file number that you need to document and once that application is",
    "start": "2661440",
    "end": "2667760"
  },
  {
    "text": "done you know you migrate your application and then promote the slave to a master",
    "start": "2667760",
    "end": "2672960"
  },
  {
    "text": "database so that you can have your application in a migrator to the cloud",
    "start": "2672960",
    "end": "2678240"
  },
  {
    "text": "okay so importing so um the master sql database here",
    "start": "2678240",
    "end": "2685359"
  },
  {
    "text": "is on premise the changes are basically logged into events in the sql",
    "start": "2685359",
    "end": "2690480"
  },
  {
    "text": "binary log file and the slave database retrieves these events when you enable",
    "start": "2690480",
    "end": "2695760"
  },
  {
    "text": "replication between the mash from the slave and it will replay these events to bring the database up to",
    "start": "2695760",
    "end": "2702160"
  },
  {
    "text": "the the latest right so um once we do that importing uh and then",
    "start": "2702160",
    "end": "2708960"
  },
  {
    "text": "migrating this backup from on premise to amazon so we went through multiple",
    "start": "2708960",
    "end": "2714720"
  },
  {
    "text": "ways i mean i tried all the following so the the first step the first one is use ftp or scp",
    "start": "2714720",
    "end": "2721040"
  },
  {
    "text": "on the wire that can be slow that can be really slow based on your pipe",
    "start": "2721040",
    "end": "2726400"
  },
  {
    "text": "and the second option we tried was the sftp and scp with the direct connect in place",
    "start": "2726400",
    "end": "2732560"
  },
  {
    "text": "you know direct connect is a private connection that you have between the enterprise endpoint",
    "start": "2732560",
    "end": "2738319"
  },
  {
    "text": "and the amazon endpoint and it gives you that point-to-point connection you know an",
    "start": "2738319",
    "end": "2743599"
  },
  {
    "text": "expected throughput and the consistent latency right we tried that and then the next option we",
    "start": "2743599",
    "end": "2749119"
  },
  {
    "text": "tried was the tsunami udp which is the utp-based protocol that it uses and the next one is aspara was something",
    "start": "2749119",
    "end": "2755920"
  },
  {
    "text": "that we thought we could use it but i did not try that the the other option we tried is the storage gateway",
    "start": "2755920",
    "end": "2762560"
  },
  {
    "text": "you can deploy a storage gateway on premise and then map your backup disk that you",
    "start": "2762560",
    "end": "2769119"
  },
  {
    "text": "backed up the mysql dump as the initiator and then the s3 bucket",
    "start": "2769119",
    "end": "2775200"
  },
  {
    "text": "in in amazon as a target so that once the backup is done you take a snapshot into s3 use that",
    "start": "2775200",
    "end": "2782160"
  },
  {
    "text": "snapshot to mount as ebs volume and then mount that to your ec2 instance so that you can start",
    "start": "2782160",
    "end": "2788240"
  },
  {
    "text": "the the my sequel dump the last one is import and export where you can ship drives to amazon you know we will import",
    "start": "2788240",
    "end": "2795920"
  },
  {
    "text": "your data into an s3 bucket or an eps volume and then to improve performance you know",
    "start": "2795920",
    "end": "2802079"
  },
  {
    "text": "you can always choose to use some of the van acceleration from riverbed or citrix",
    "start": "2802079",
    "end": "2807839"
  },
  {
    "text": "or archive any of this so that you can accelerate the migration so here is a",
    "start": "2807839",
    "end": "2814160"
  },
  {
    "text": "database called bench which is about 150 gig that we tried to migrate",
    "start": "2814160",
    "end": "2819760"
  },
  {
    "start": "2817000",
    "end": "2817000"
  },
  {
    "text": "i just want to run through a sql command to show the size of the database and then we run the mysql dump where you",
    "start": "2819760",
    "end": "2827280"
  },
  {
    "start": "2824000",
    "end": "2824000"
  },
  {
    "text": "take this and then create your mysql dump on the on-premise database",
    "start": "2827280",
    "end": "2833760"
  },
  {
    "text": "and and then you know went into amazon web services and then created a my",
    "start": "2833760",
    "end": "2840079"
  },
  {
    "start": "2834000",
    "end": "2834000"
  },
  {
    "text": "sequel an rds mysql using console or cli in this case i use a cli to create the",
    "start": "2840079",
    "end": "2846640"
  },
  {
    "text": "rds instance and then created the staging database i",
    "start": "2846640",
    "end": "2851680"
  },
  {
    "text": "mean the staging server i use the cli to do that once you're done creating the the mysql database",
    "start": "2851680",
    "end": "2859760"
  },
  {
    "text": "the rds mysql and the ec2 staging you go to the master and create a master",
    "start": "2859760",
    "end": "2865920"
  },
  {
    "text": "user and grant access in order to select that application in the master database",
    "start": "2865920",
    "end": "2871440"
  },
  {
    "text": "on premise right once you do that you basically",
    "start": "2871440",
    "end": "2876640"
  },
  {
    "text": "um you know update your hc you know my con my dot config this is where you start",
    "start": "2876640",
    "end": "2882559"
  },
  {
    "text": "you know you enable replication on your master database the server id and the bin log and the",
    "start": "2882559",
    "end": "2888160"
  },
  {
    "text": "replay and these are the things that you you update your file you know this enables bin logging you know which creates the file",
    "start": "2888160",
    "end": "2895440"
  },
  {
    "text": "you know the files that are recorded and the changes that have been occurred on the master database so this is how",
    "start": "2895440",
    "end": "2901520"
  },
  {
    "text": "this is what you do on the master database so once you do that you restart your database",
    "start": "2901520",
    "end": "2907040"
  },
  {
    "text": "and then log into the master database and then run the show my show match to",
    "start": "2907040",
    "end": "2913839"
  },
  {
    "start": "2908000",
    "end": "2908000"
  },
  {
    "text": "this is where you're going to document the file the mybind.mysql.band.0023",
    "start": "2913839",
    "end": "2921200"
  },
  {
    "text": "and the position of that you know this is the position of your sql the bin logs so document these two parameters that",
    "start": "2921200",
    "end": "2927359"
  },
  {
    "text": "are shown on the screen and once you have that you know you can do multiple ways you know you can target",
    "start": "2927359",
    "end": "2933359"
  },
  {
    "text": "the file you can compress it and then ship it to ec2 instance in case what the best timing that i got on this",
    "start": "2933359",
    "end": "2940720"
  },
  {
    "text": "150 gig my sequel dump was using the udp database",
    "start": "2940720",
    "end": "2945760"
  },
  {
    "start": "2941000",
    "end": "2941000"
  },
  {
    "text": "udp protocol using tsunami it was about two and a half hours i was able to move that to the staging",
    "start": "2945760",
    "end": "2953200"
  },
  {
    "text": "server and then i mean on the staging server make sure",
    "start": "2953200",
    "end": "2958240"
  },
  {
    "text": "the security group is kind of you know you you update the ec2 security group so",
    "start": "2958240",
    "end": "2963920"
  },
  {
    "text": "that you have the ports that open the protocols and ports open in order for that transfer to happen and",
    "start": "2963920",
    "end": "2970079"
  },
  {
    "text": "then on the ec2 instance make sure you enter the file and then",
    "start": "2970079",
    "end": "2975280"
  },
  {
    "text": "on the rds database log in and create the bench database and that's the first step once",
    "start": "2975280",
    "end": "2982800"
  },
  {
    "text": "you create that you take all the you know i mean this is we're doing it this from the staging server right",
    "start": "2982800",
    "end": "2989119"
  },
  {
    "text": "so from the staging server use the load data file data local files to import these uh",
    "start": "2989119",
    "end": "2996079"
  },
  {
    "text": "the data files that we have migrated from on-premise so once you do that you configure the",
    "start": "2996079",
    "end": "3002559"
  },
  {
    "text": "mysql rds as a slave database and because",
    "start": "3002559",
    "end": "3007680"
  },
  {
    "text": "we can i mean on a on-premise slave to enable it we can always edit the my.config file",
    "start": "3007680",
    "end": "3013920"
  },
  {
    "text": "and make that as a slave because rds will not give you access we don't have access to",
    "start": "3013920",
    "end": "3019920"
  },
  {
    "text": "the operating system we have a stored procedure called mysql.rds underscore set external master",
    "start": "3019920",
    "end": "3028240"
  },
  {
    "text": "so when you use this procedure you can see you know we're setting an external master database as the",
    "start": "3028240",
    "end": "3035200"
  },
  {
    "text": "master in the slave database so here you give the name of the master server the port at which is running and the",
    "start": "3035200",
    "end": "3042160"
  },
  {
    "start": "3036000",
    "end": "3036000"
  },
  {
    "text": "replication user that we created on the master database and the password and the key thing is the file that we",
    "start": "3042160",
    "end": "3048960"
  },
  {
    "text": "documented and then the you know and then the other parameters so that with this",
    "start": "3048960",
    "end": "3054720"
  },
  {
    "text": "you can set the on-premise mysql database as a master",
    "start": "3054720",
    "end": "3059760"
  },
  {
    "text": "on the slave and rds once you do that you start the application",
    "start": "3059760",
    "end": "3064960"
  },
  {
    "text": "on your rds and then once you start the audio so this is a screenshot you see",
    "start": "3064960",
    "end": "3071280"
  },
  {
    "text": "the you know it starts with application um the the rds uh this is the amazon console where you",
    "start": "3071280",
    "end": "3077839"
  },
  {
    "start": "3072000",
    "end": "3072000"
  },
  {
    "text": "have the rds console up the console will show that it's replicating and once the changes the",
    "start": "3077839",
    "end": "3084079"
  },
  {
    "text": "deltas have been moved over into the slave database",
    "start": "3084079",
    "end": "3089119"
  },
  {
    "start": "3089000",
    "end": "3089000"
  },
  {
    "text": "you basically switch the rds database you know so i walk you through the steps",
    "start": "3089440",
    "end": "3094960"
  },
  {
    "text": "here you can stop the application service that's pointing to the master database and then once all the changes have been",
    "start": "3094960",
    "end": "3102640"
  },
  {
    "text": "applied to the rds in mysql you can stop the application by running the stored",
    "start": "3102640",
    "end": "3107920"
  },
  {
    "text": "procedure sequel dot rds stopped application at this point you're basically stopping the application",
    "start": "3107920",
    "end": "3113520"
  },
  {
    "text": "and then once the migration is done you kind of reset the external master",
    "start": "3113520",
    "end": "3119599"
  },
  {
    "text": "and then promote the slave as you mastered as a regular mysql database right",
    "start": "3119599",
    "end": "3126319"
  },
  {
    "text": "so i mean please feel free to provide any feedback on the session",
    "start": "3128079",
    "end": "3133520"
  },
  {
    "text": "the slides will be available as abdul said in on wednesday",
    "start": "3133520",
    "end": "3140079"
  },
  {
    "text": "and feedback can be provided online as well as on the on your app",
    "start": "3140079",
    "end": "3146400"
  },
  {
    "text": "thank you",
    "start": "3147520",
    "end": "3153839"
  },
  {
    "text": "you",
    "start": "3155359",
    "end": "3157440"
  }
]