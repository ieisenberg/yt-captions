[
  {
    "text": "hey folks uh Emily here today you are going to learn how to build a retrieval augmented generation system on Amazon",
    "start": "520",
    "end": "6799"
  },
  {
    "text": "sagemaker let's get started all right uh so what what on Earth is retrieval",
    "start": "6799",
    "end": "11880"
  },
  {
    "text": "augmented generation so let's break down these three words number one retrieval",
    "start": "11880",
    "end": "17359"
  },
  {
    "text": "the word retrieval of course stands for going and grabbing a document it's commonly associated with search so a",
    "start": "17359",
    "end": "24880"
  },
  {
    "text": "search system can also be called a retrieval system the idea is that you have number of documents stored in",
    "start": "24880",
    "end": "32360"
  },
  {
    "text": "something like a database and then your retrieval process goes and tries to find the right document usually you're trying",
    "start": "32360",
    "end": "39239"
  },
  {
    "text": "to find the right document based on something your customer is asking so a customer asks a question and then the",
    "start": "39239",
    "end": "45840"
  },
  {
    "text": "retriever is going to go and find that question second word for you",
    "start": "45840",
    "end": "51800"
  },
  {
    "text": "augmented and so what we're augmenting is we're taking that document so the you",
    "start": "51800",
    "end": "57440"
  },
  {
    "text": "the customer asks a question we go when we look up a document from a database",
    "start": "57440",
    "end": "62519"
  },
  {
    "text": "and we're going to use that document to augment the generation from the large",
    "start": "62519",
    "end": "68560"
  },
  {
    "text": "language model and so that's retrieval augmented generation we're looking up a document based on a question that comes",
    "start": "68560",
    "end": "76040"
  },
  {
    "text": "in from our customer and then we take that document we're going to send the whole document to the language model and",
    "start": "76040",
    "end": "82360"
  },
  {
    "text": "we're going to ask the customer's question at the bottom and that's what you see in this visual here essentially",
    "start": "82360",
    "end": "87960"
  },
  {
    "text": "we're prompting the language model with our entire document we're sending in the",
    "start": "87960",
    "end": "93600"
  },
  {
    "text": "whole document as large as we can depending on the Contex size of the model we're going to add a couple things",
    "start": "93600",
    "end": "98920"
  },
  {
    "text": "so let's break this down so first we're adding in same as last time the the guidance to the model this is like the",
    "start": "98920",
    "end": "105640"
  },
  {
    "text": "high level setting with the model that tells it you know you are a assistant um",
    "start": "105640",
    "end": "111960"
  },
  {
    "text": "gives it high level information about how you want it to respond so you provide the guidance and then the second",
    "start": "111960",
    "end": "118920"
  },
  {
    "text": "is the context so the context is is that document right we go and we retrieve or",
    "start": "118920",
    "end": "126119"
  },
  {
    "text": "we look up or we search for a document in the database once we have that document we are going to put that",
    "start": "126119",
    "end": "132720"
  },
  {
    "text": "directly into the string and so here it's it's just really a regex insertion",
    "start": "132720",
    "end": "138040"
  },
  {
    "text": "we're just inserting that entire document into this gigantic string here so that's the context and then the third",
    "start": "138040",
    "end": "145560"
  },
  {
    "text": "is the instruction and that instruction comes directly from the customer right so the customer shows up they ask a",
    "start": "145560",
    "end": "151760"
  },
  {
    "text": "question we go and we look up a document based on that question and then we take",
    "start": "151760",
    "end": "158400"
  },
  {
    "text": "the document we insert it into the prompt then we're also going to insert the customer's question and so that's",
    "start": "158400",
    "end": "166080"
  },
  {
    "text": "this third component that you see here this question is we're inserting both of those into the prompts so we're sending",
    "start": "166080",
    "end": "173920"
  },
  {
    "text": "all of these to the language model and getting a very informed response back let's see what this looks like um a bit",
    "start": "173920",
    "end": "180319"
  },
  {
    "text": "more broadly so again your customer is asking a question and",
    "start": "180319",
    "end": "185879"
  },
  {
    "text": "they're going to be asking a question through some sort of web application this might be a streamlet app might be a",
    "start": "185879",
    "end": "191400"
  },
  {
    "text": "gradio app it can be a node.js app it can be a react app any type of application you are developing your",
    "start": "191400",
    "end": "197519"
  },
  {
    "text": "customer is going to ask a question they ask this question and then uh you're",
    "start": "197519",
    "end": "203159"
  },
  {
    "text": "going to route that question through some type of lookup service there are",
    "start": "203159",
    "end": "208519"
  },
  {
    "text": "many many different options for looking up questions um in this case this is a",
    "start": "208519",
    "end": "214439"
  },
  {
    "text": "uh Vector database uh created with Amazon open search Service uh and so",
    "start": "214439",
    "end": "220319"
  },
  {
    "text": "that's one candidate option for using uh Vector stores the reason why we care",
    "start": "220319",
    "end": "225480"
  },
  {
    "text": "about vectors here is because when you go and ingest documents into this Vector",
    "start": "225480",
    "end": "231480"
  },
  {
    "text": "store there's this offline process that you do ahead of time so you start with your documents for example in your S3",
    "start": "231480",
    "end": "238400"
  },
  {
    "text": "bucket you're going to create embeddings with those documents and so you need",
    "start": "238400",
    "end": "245000"
  },
  {
    "text": "another model to actually generate embeddings for this so you might use which is what we're going to do in the",
    "start": "245000",
    "end": "250920"
  },
  {
    "text": "demo you might use a stag maker endpoint that's hosting a model to generate",
    "start": "250920",
    "end": "256440"
  },
  {
    "text": "embeddings actually it's still a large language model believe it or not the first 75% of that model is the same as",
    "start": "256440",
    "end": "264280"
  },
  {
    "text": "every other llm but the differ is that the output instead of outputting text",
    "start": "264280",
    "end": "270400"
  },
  {
    "text": "it's outputting a vector actually so that that llm the uh embedding model is",
    "start": "270400",
    "end": "277440"
  },
  {
    "text": "taking a document and it's creating what's called a dense representation of that document that's also called an",
    "start": "277440",
    "end": "283680"
  },
  {
    "text": "embedding so we create this embedding which is sort of a dense uh",
    "start": "283680",
    "end": "288919"
  },
  {
    "text": "representation of the syntactical meaning of what's in that document and we're going to get these embeddings for",
    "start": "288919",
    "end": "294639"
  },
  {
    "text": "the entire database so our whole document store essentially we're going to convert all of those documents into",
    "start": "294639",
    "end": "301880"
  },
  {
    "text": "these lower level um representations which is the embedding and then the embeddings are what make it into the",
    "start": "301880",
    "end": "309680"
  },
  {
    "text": "database so then the database or the vector store I mean it's called a vector",
    "start": "309680",
    "end": "315039"
  },
  {
    "text": "store because it's hosting it's holding vectors it's holding the embeddings remember Vector is just a really long",
    "start": "315039",
    "end": "320440"
  },
  {
    "text": "list of numbers uh and so this open search database essentially is holding",
    "start": "320440",
    "end": "325479"
  },
  {
    "text": "all of those um Den uh embeddings or representations for you and then there are lots of",
    "start": "325479",
    "end": "332680"
  },
  {
    "text": "different ways to handle this lookup which is where your customer asks a question and then you want to go and",
    "start": "332680",
    "end": "339720"
  },
  {
    "text": "look up the right document based on the customer's question most of the time what you're going to do is you're going",
    "start": "339720",
    "end": "344960"
  },
  {
    "text": "to take the customer's question and you're going to route that question through an embedding endpoint for",
    "start": "344960",
    "end": "351120"
  },
  {
    "text": "example so you're going to take that question convert that into an embedding and then do like an embedding similarity",
    "start": "351120",
    "end": "358319"
  },
  {
    "text": "lookup basically that that's going to do like an approximate K nearest neighbors and just extract the you know handful of",
    "start": "358319",
    "end": "366080"
  },
  {
    "text": "vectors that are most similar um to the customer's question now this will vary",
    "start": "366080",
    "end": "372400"
  },
  {
    "text": "based on the uh Vector store that you're working with um some of them let you handle more of this some let you handle",
    "start": "372400",
    "end": "378840"
  },
  {
    "text": "less uh so you have a lot of different options for how to dial in that retrieval step again where you're just",
    "start": "378840",
    "end": "384440"
  },
  {
    "text": "trying to find the right document using embeddings coming out of llms to do that",
    "start": "384440",
    "end": "390039"
  },
  {
    "text": "once you actually have that document that's the hard part actually the hard",
    "start": "390039",
    "end": "395560"
  },
  {
    "text": "part is just grabbing the document once you have the document then you're doing the same um sort of prompt concatenation",
    "start": "395560",
    "end": "403840"
  },
  {
    "text": "um and then invoking the endpoint as you've always been doing so you're going to grab that document um put the",
    "start": "403840",
    "end": "409440"
  },
  {
    "text": "document into the prompt along with the customer's question and then just send it straight to the llm to have it read",
    "start": "409440",
    "end": "417440"
  },
  {
    "text": "the document ask the question and then answer the question appropriately and so this is one way uh",
    "start": "417440",
    "end": "424400"
  },
  {
    "text": "that you can build a retrieval stack uh using sagemaker now in the demo we're going to",
    "start": "424400",
    "end": "430599"
  },
  {
    "text": "one up this um so in the demo I'm going to show you a multimodal retrieval augmented generation stack so multimodal",
    "start": "430599",
    "end": "437160"
  },
  {
    "text": "here of course refers to multiple modalities or modality is like image and",
    "start": "437160",
    "end": "442800"
  },
  {
    "text": "text for example so two different modalities and what we're going to do is we're going to build a multimodal",
    "start": "442800",
    "end": "448960"
  },
  {
    "text": "retrieval just in your sagemaker notebook so something really small um that works",
    "start": "448960",
    "end": "454240"
  },
  {
    "text": "with uh Amazon science papers so I downloaded um a handful of some of the latest um papers on Amazon science and",
    "start": "454240",
    "end": "461720"
  },
  {
    "text": "then I'm going to uh use Lang chain and stagemaker endpoints to build a local um",
    "start": "461720",
    "end": "469000"
  },
  {
    "text": "multimodal retrieval component for that uh it's going to run chroma DB on a sage",
    "start": "469000",
    "end": "474680"
  },
  {
    "text": "maker Studio notebook um and then uh we're going to be able to look up",
    "start": "474680",
    "end": "479879"
  },
  {
    "text": "documents for this based on the content of the paper both in the language and in",
    "start": "479879",
    "end": "486599"
  },
  {
    "text": "the summary of the images that was generated by lava where lava is another",
    "start": "486599",
    "end": "492759"
  },
  {
    "text": "um model that does large language and vision analysis combined and so that's",
    "start": "492759",
    "end": "498879"
  },
  {
    "text": "what we're going to run through right now so let's check it out all right uh so just as usual we're",
    "start": "498879",
    "end": "506639"
  },
  {
    "text": "going to start in Sag maker studio um um actually when I uh went to do this I",
    "start": "506639",
    "end": "513440"
  },
  {
    "text": "needed to use Studio Classic as it turned out um so if you didn't know this in sagemaker studio um you can run a",
    "start": "513440",
    "end": "520159"
  },
  {
    "text": "number of applications uh including the new Jupiter lab and also sagemaker Studio Classic and so it turned out that",
    "start": "520159",
    "end": "526720"
  },
  {
    "text": "I needed to I needed to run Studio Classic for this one so that's what I'll be using I also created a number of",
    "start": "526720",
    "end": "533279"
  },
  {
    "text": "endpoints so in the deployment section uh we can look at these endpoints that I have running so I have two models that",
    "start": "533279",
    "end": "540800"
  },
  {
    "text": "are hosted on sagemaker endpoints uh one of them is a llama 270b",
    "start": "540800",
    "end": "546399"
  },
  {
    "text": "chat this is the large model that will be answering the questions of the",
    "start": "546399",
    "end": "551440"
  },
  {
    "text": "customers based on that document um that comes in through the retrieval stack and",
    "start": "551440",
    "end": "557000"
  },
  {
    "text": "then the second model is the embedding um this is a gptj 6 billion uh parameter",
    "start": "557000",
    "end": "562480"
  },
  {
    "text": "model that uh takes language and then converts that into a dense representation of that which is the",
    "start": "562480",
    "end": "567839"
  },
  {
    "text": "embedding and then of course use sagemaker jump start uh to deploy those in clicking it was very fast it was",
    "start": "567839",
    "end": "574720"
  },
  {
    "text": "delightful very reliable those endpoints were up and running in minutes so the notebook I'm using um",
    "start": "574720",
    "end": "582720"
  },
  {
    "text": "this is heavily reliant on um The Lang chain cookbook uh so the Lang chain uh",
    "start": "582720",
    "end": "588640"
  },
  {
    "text": "open source repo has a cookbook with many many different solutions um I took a look at one of the solutions which was",
    "start": "588640",
    "end": "594760"
  },
  {
    "text": "multimotor retrieval and then essentially this notebook is a modification of that to work nicely with",
    "start": "594760",
    "end": "601240"
  },
  {
    "text": "the SAG maker components um so CJ thank you for thank you for the the source",
    "start": "601240",
    "end": "606680"
  },
  {
    "text": "code um I hope you enjoy the sagemaker demo and so feel free to follow along",
    "start": "606680",
    "end": "612440"
  },
  {
    "text": "with me again this is available publicly um so if you'd like to download this",
    "start": "612440",
    "end": "617959"
  },
  {
    "text": "notebook and then uh step along with me you are welcome to do that let's check",
    "start": "617959",
    "end": "623640"
  },
  {
    "text": "it out so again as promised here I am in sagemaker Studio and then um um there are quite a",
    "start": "623640",
    "end": "631079"
  },
  {
    "text": "few instances that you are going to need to run this let's see what they are so",
    "start": "631079",
    "end": "637279"
  },
  {
    "text": "in the notebook I I tried to list them up front so you know so there are really three uh GPU machines that you're going",
    "start": "637279",
    "end": "644760"
  },
  {
    "text": "to use here the first one is for the notebook so the model that's going to work with images is called lava uh and",
    "start": "644760",
    "end": "652639"
  },
  {
    "text": "this lava model is actually running on Studio locally actually it's it's parked",
    "start": "652639",
    "end": "659120"
  },
  {
    "text": "on on a local g52 EXL machine um so the first thing I did when I opened up",
    "start": "659120",
    "end": "665200"
  },
  {
    "text": "sagemaker Studio Classic was uh go in and U make sure that I'm running on the",
    "start": "665200",
    "end": "671720"
  },
  {
    "text": "right machine so I actually turned on this g5. 2XL and then that's where my",
    "start": "671720",
    "end": "676839"
  },
  {
    "text": "notebook is parked and then just as I mentioned previously then I turned on two extra end points and so one was for",
    "start": "676839",
    "end": "684760"
  },
  {
    "text": "the large language model that's going to be chatting with me and that's the",
    "start": "684760",
    "end": "689800"
  },
  {
    "text": "uh G5 actually was what I used that was a g5. 48 XL and then the third one was",
    "start": "689800",
    "end": "696160"
  },
  {
    "text": "the g512 Excel that's hosting that embedding model and then remember I'm going to use that embedding model to",
    "start": "696160",
    "end": "702800"
  },
  {
    "text": "create these dense representations or embeddings of the documents and then",
    "start": "702800",
    "end": "707920"
  },
  {
    "text": "those documents are what go into my chroma DB uh Vector store there are a lot of requirements",
    "start": "707920",
    "end": "715720"
  },
  {
    "text": "for this notebook so plan on spending a lot of time installing and upgrading pip",
    "start": "715720",
    "end": "721560"
  },
  {
    "text": "getting all the requirements downloaded um when I ran into issues with different versions I tried to just pin it so for",
    "start": "721560",
    "end": "729120"
  },
  {
    "text": "lava um make sure using Transformers at least 4391 um boto 3 you want to hit at least",
    "start": "729120",
    "end": "739120"
  },
  {
    "text": "13423 and then make sure you definitely have this uh open CV python headless",
    "start": "739199",
    "end": "745199"
  },
  {
    "text": "that gives you some of the lower libraries that we're going to use to parse the PDF",
    "start": "745199",
    "end": "750320"
  },
  {
    "text": "and then lots of others um SQL Alchemy flask SQL so get those",
    "start": "750320",
    "end": "755800"
  },
  {
    "text": "installed update aptg get install all of the rest of these aptg get packages",
    "start": "755800",
    "end": "762279"
  },
  {
    "text": "these other packages are what are going to let us parse the PDFs actually so this notebook is going to take PDF files",
    "start": "762279",
    "end": "769760"
  },
  {
    "text": "as an argument and then we're going to parse those into text tables and images",
    "start": "769760",
    "end": "775519"
  },
  {
    "text": "and so these lower level libraries are what let us do that processing so once you have those",
    "start": "775519",
    "end": "781000"
  },
  {
    "text": "downloaded go ahead and restart uh your Jupiter kernel just so you know that you",
    "start": "781000",
    "end": "786279"
  },
  {
    "text": "definitely have the new packages and that um they they actually did install",
    "start": "786279",
    "end": "792160"
  },
  {
    "text": "and then two check so your Transformers version uh you want to make sure you have at least again that",
    "start": "792160",
    "end": "798079"
  },
  {
    "text": "4391 and then make sure you can import this uh function in particular so the",
    "start": "798079",
    "end": "804959"
  },
  {
    "text": "library we're going to use to parse the PDFs is called unstructured so it's unru structured um this open source python",
    "start": "804959",
    "end": "811240"
  },
  {
    "text": "package and then uh we're going to use this partition PDF function this can be",
    "start": "811240",
    "end": "817320"
  },
  {
    "text": "finicky so really don't go on to step two still until you've actually gotten",
    "start": "817320",
    "end": "822959"
  },
  {
    "text": "successful Imports on those uh Parts in step one okay so once you're done with",
    "start": "822959",
    "end": "828320"
  },
  {
    "text": "that then we're going to download lava again to this notebook and we're going to test it locally to make sure it's",
    "start": "828320",
    "end": "833639"
  },
  {
    "text": "it's working uh so again if unless you have the recent version of the Transformers SDK that import will not",
    "start": "833639",
    "end": "840800"
  },
  {
    "text": "work it will fail so just update it make sure you have the right one uh once you",
    "start": "840800",
    "end": "845959"
  },
  {
    "text": "do then this was the uh lava version I use so v1.6 and then um so lava has both vision",
    "start": "845959",
    "end": "854399"
  },
  {
    "text": "and language components and I've seen different options that switch out the language back end so this is a a mraw 7B",
    "start": "854399",
    "end": "862440"
  },
  {
    "text": "uh language back end so that's here and then um just like normal you need the",
    "start": "862440",
    "end": "867680"
  },
  {
    "text": "processor which is basically the tokenizer um except it's processing bytes and images and then great so we",
    "start": "867680",
    "end": "875720"
  },
  {
    "text": "set that that's going to load and then uh to test lava go ahead and download an",
    "start": "875720",
    "end": "881959"
  },
  {
    "text": "image um I got an image from the lava paper which I thought was fun um so how",
    "start": "881959",
    "end": "889040"
  },
  {
    "text": "andl thank you uh for the jpeg here I got this directly from your GitHub page",
    "start": "889040",
    "end": "895680"
  },
  {
    "text": "and then this is um what we call a radar plot so we looked at this radar plot in",
    "start": "895680",
    "end": "902839"
  },
  {
    "text": "the video in this series on evaluating language models um honestly I think radar plots are a really helpful way to",
    "start": "902839",
    "end": "911079"
  },
  {
    "text": "visualize many different benchmarks that are all sort of normalized in similar",
    "start": "911079",
    "end": "916600"
  },
  {
    "text": "ranges so if they're all normalized in say like 0 to one or or 1 to 100 um and",
    "start": "916600",
    "end": "923079"
  },
  {
    "text": "then you want to know basically how models are per how different models are performing on all of these dimensions",
    "start": "923079",
    "end": "929519"
  },
  {
    "text": "then the radar plot is a really nice way of just representing that information and so so here's a radar plot for",
    "start": "929519",
    "end": "938279"
  },
  {
    "text": "different um models that are doing visual question answering so the lava model is right here 1.5 and you can see",
    "start": "938279",
    "end": "946399"
  },
  {
    "text": "clearly that has the most surface area so that's hitting the best performance on the benchmarks as compared to these",
    "start": "946399",
    "end": "952319"
  },
  {
    "text": "three other models let me Zoom back in so you can see the rest of the code all right uh and so go ahead and get this",
    "start": "952319",
    "end": "960079"
  },
  {
    "text": "image and then once you have it locally uh we're going to read it into this image uh package and then so the prompt",
    "start": "960079",
    "end": "969160"
  },
  {
    "text": "that you're going to send to the model it is quite opinionated it needs to see this this instruction um square bracket",
    "start": "969160",
    "end": "976959"
  },
  {
    "text": "so it needs to know what the instruction is and then it also um for the prompting",
    "start": "976959",
    "end": "982360"
  },
  {
    "text": "to work you need to have these uh tags here for image and then the um the the",
    "start": "982360",
    "end": "989120"
  },
  {
    "text": "syntax below is going to insert the image into that line actually and then",
    "start": "989120",
    "end": "994560"
  },
  {
    "text": "both both of those go to the lava model when I was testing this I I could see that lava was good at at understanding",
    "start": "994560",
    "end": "1002920"
  },
  {
    "text": "sort of like really basic questions or really very introductory questions so something like what is shown in this",
    "start": "1002920",
    "end": "1008959"
  },
  {
    "text": "image or what is the meaning of the image but once I started to ask for specifics about the about um say like",
    "start": "1008959",
    "end": "1017639"
  },
  {
    "text": "which model cover the most surface area in this plot Which models are shown in the legend Which models has the least",
    "start": "1017639",
    "end": "1024520"
  },
  {
    "text": "surface area in the plot I mean it was absolutely not accurate and so I just thought that was an interesting note uh",
    "start": "1024520",
    "end": "1030760"
  },
  {
    "text": "so we we take the processor and we send the prompt the image to the processor",
    "start": "1030760",
    "end": "1037079"
  },
  {
    "text": "we're going to drop that to Cuda so that's on the GPU and then the model",
    "start": "1037079",
    "end": "1042160"
  },
  {
    "text": "actually is already on the GPU because of bits and bites and then that's going to generate a response using the inputs",
    "start": "1042160",
    "end": "1048558"
  },
  {
    "text": "and then we get this so here's what lava thinks is in the image image is a radar",
    "start": "1048559",
    "end": "1053840"
  },
  {
    "text": "plot multi-dimensional plot displays values for multiple quantitative",
    "start": "1053840",
    "end": "1060160"
  },
  {
    "text": "variables all right and then it it gets some of the axies so such as mmm vet mmm",
    "start": "1060160",
    "end": "1066720"
  },
  {
    "text": "bench let's just double check that it was Triple M ah it was only a double m",
    "start": "1066720",
    "end": "1073120"
  },
  {
    "text": "so so lava was uh was hypoth was uh hallucinating an extra M here but I mean",
    "start": "1073120",
    "end": "1079559"
  },
  {
    "text": "it was pretty close all right great so we downloaded lava to this notebook and",
    "start": "1079559",
    "end": "1085880"
  },
  {
    "text": "then we tested it to make sure it's it's mostly working the next thing we're going to do in step three is we're going",
    "start": "1085880",
    "end": "1091760"
  },
  {
    "text": "to ingest these local PDF files using this unstructured Library so again that",
    "start": "1091760",
    "end": "1098520"
  },
  {
    "text": "unstructured library is going to use this partition PDF function and then from line chain I'm importing this",
    "start": "1098520",
    "end": "1105000"
  },
  {
    "text": "character text splitter uh that's going to work nicely with both of these",
    "start": "1105000",
    "end": "1110240"
  },
  {
    "text": "and so this partition PDF takes the file name uh Max",
    "start": "1110240",
    "end": "1116559"
  },
  {
    "text": "characters and a chunking strategy and then uh essentially this um",
    "start": "1116559",
    "end": "1124159"
  },
  {
    "text": "helps with the process and so in the file path uh point to the papers and",
    "start": "1124159",
    "end": "1130080"
  },
  {
    "text": "then within the papers um we're going to add in the PDF and I think I am doing",
    "start": "1130080",
    "end": "1135400"
  },
  {
    "text": "one PDF actually looks like so let's let's look at this PDF",
    "start": "1135400",
    "end": "1140880"
  },
  {
    "text": "here so that's out over here in the papers directory yeah and so here is",
    "start": "1140880",
    "end": "1147159"
  },
  {
    "text": "this meaning PDF um that again I got from Amazon science uh where it's looking for the",
    "start": "1147159",
    "end": "1153799"
  },
  {
    "text": "meaning representation from trajectories and autor regressive language models and",
    "start": "1153799",
    "end": "1159559"
  },
  {
    "text": "so that's a paper from back in November and then uh just to so to give",
    "start": "1159559",
    "end": "1166039"
  },
  {
    "text": "you a sense of what this unstructured library is going to do so it",
    "start": "1166039",
    "end": "1171480"
  },
  {
    "text": "will grab the text and it will sort of um it's it's going to take the PDF and",
    "start": "1171480",
    "end": "1176679"
  },
  {
    "text": "it's going to break it into three parts one part is just the raw text a second",
    "start": "1176679",
    "end": "1181799"
  },
  {
    "text": "part is a table and then the third part are the images and so this is an image",
    "start": "1181799",
    "end": "1188679"
  },
  {
    "text": "that this unstructured Library will actually write as a figure um it's going to write a new file in my working",
    "start": "1188679",
    "end": "1196480"
  },
  {
    "text": "directory with these images and I'll show you that in a second and so we get",
    "start": "1196480",
    "end": "1201520"
  },
  {
    "text": "the body of the text out of the unstructured and we also get the tables",
    "start": "1201520",
    "end": "1208200"
  },
  {
    "text": "uh I don't think unstructured is going to do a good job of like extracting the equations in a meaningful way or",
    "start": "1208200",
    "end": "1214840"
  },
  {
    "text": "extracting the algorithms in a meaningful way so that's future work for someone to pick up um but nonetheless",
    "start": "1214840",
    "end": "1222280"
  },
  {
    "text": "yeah this table for example uh will be well extracted by that unstructured",
    "start": "1222280",
    "end": "1227520"
  },
  {
    "text": "Library so nonetheless that's what we have so we get the raw PDFs we categorize",
    "start": "1227520",
    "end": "1232660"
  },
  {
    "text": "[Music] them run the text splitter sing the chunk size so um for the chunk size make",
    "start": "1232660",
    "end": "1239880"
  },
  {
    "text": "sure that is small enough to fit in the context window of your model so if",
    "start": "1239880",
    "end": "1245720"
  },
  {
    "text": "you're using the Llama 2 um then you want the chunk size to be definitely under 4,000 and I just did 2,000 to make",
    "start": "1245720",
    "end": "1253039"
  },
  {
    "text": "the the um example here easier run okay and so then we'll have the",
    "start": "1253039",
    "end": "1259039"
  },
  {
    "text": "tokens and so here is just an example of the first text that was",
    "start": "1259039",
    "end": "1264320"
  },
  {
    "text": "split and so yeah as promised here we go meaning representations with the authors and",
    "start": "1264320",
    "end": "1270760"
  },
  {
    "text": "then they discuss it so so definitely we extracted that text uh logically step four is we're going to",
    "start": "1270760",
    "end": "1278200"
  },
  {
    "text": "set up Lang chain with our sagemaker endpoints and we're going to generate text summary so we're going to go",
    "start": "1278200",
    "end": "1284559"
  },
  {
    "text": "summarize we're going to use the chat model that we hosted to summarize um the",
    "start": "1284559",
    "end": "1291159"
  },
  {
    "text": "text and so uh we've got all the Imports so again uh from Lang chain we're",
    "start": "1291159",
    "end": "1296799"
  },
  {
    "text": "importing the sagemaker endpoint then we have this chat prompt template and then the content Handler so we looked at this",
    "start": "1296799",
    "end": "1304400"
  },
  {
    "text": "last time um your content Handler remember is another uh class just to to",
    "start": "1304400",
    "end": "1310159"
  },
  {
    "text": "lightly modify to point to your stage maker end points endpoints from Lang chain and if you're working on a llama 2",
    "start": "1310159",
    "end": "1317000"
  },
  {
    "text": "model then just make sure that your output looks like exactly like this so",
    "start": "1317000",
    "end": "1323320"
  },
  {
    "text": "uh first uh index zero and then uh generated text as a",
    "start": "1323320",
    "end": "1329400"
  },
  {
    "text": "key all right and then so we'll point to um Bodo 3 again using the stage maker",
    "start": "1329400",
    "end": "1336600"
  },
  {
    "text": "runtime client we'll start with this template and then we're going to take",
    "start": "1336600",
    "end": "1342360"
  },
  {
    "text": "that template and then load that into Lang chain so we set that into the chat",
    "start": "1342360",
    "end": "1347640"
  },
  {
    "text": "prom templates and then we have the content Handler from that uh class we defined above and",
    "start": "1347640",
    "end": "1354480"
  },
  {
    "text": "then here we're pointing to the uh sag maker model so a couple things to call",
    "start": "1354480",
    "end": "1359679"
  },
  {
    "text": "out here the endpoint name this is obvious that's the name of the endpoint",
    "start": "1359679",
    "end": "1365360"
  },
  {
    "text": "the inference component name might be a little bit hard for you to find so let me go show you where that is so if",
    "start": "1365360",
    "end": "1372760"
  },
  {
    "text": "you're in sagemaker studio let me zoom in here so you're in sagemaker studio let me collapse this menu so I'm going",
    "start": "1372760",
    "end": "1380240"
  },
  {
    "text": "to look at that text generation endpoint so you can see this is my model I'm",
    "start": "1380240",
    "end": "1386240"
  },
  {
    "text": "going to go to test inference and then in test inference I'm",
    "start": "1386240",
    "end": "1391440"
  },
  {
    "text": "going to click test the sample request and it should come up with a base uh",
    "start": "1391440",
    "end": "1397240"
  },
  {
    "text": "right here I think last time we did peanut butter so let's do chocolate chip",
    "start": "1397240",
    "end": "1403720"
  },
  {
    "text": "cookies and let's capitalize this because that's just what I feel like doing right now",
    "start": "1403720",
    "end": "1409960"
  },
  {
    "text": "all right we're going to send the request okay and so that is the payload",
    "start": "1409960",
    "end": "1416840"
  },
  {
    "text": "object that's going to hit this llama model on your behalf you can see the parameters that are defined right here",
    "start": "1416840",
    "end": "1423559"
  },
  {
    "text": "over on the right hand side it's it's handling this right it was a little too large",
    "start": "1423559",
    "end": "1430440"
  },
  {
    "text": "noted and here is what llama chat thinks about our recipe for chocolate chip",
    "start": "1430440",
    "end": "1435640"
  },
  {
    "text": "cookies four cups flour baking soda teaspoon unsalted butter room",
    "start": "1435640",
    "end": "1441559"
  },
  {
    "text": "temperature wasn't expecting that white granulated sugar brown sugar chocolate",
    "start": "1441559",
    "end": "1449480"
  },
  {
    "text": "chips yeah that looks Fair okay so we got a we got a",
    "start": "1449559",
    "end": "1455200"
  },
  {
    "text": "provisional recipe for chocolate chip cookies uh the reason why we're looking at this is I want you to look at this",
    "start": "1455200",
    "end": "1462240"
  },
  {
    "text": "request carrot right here so this request drop down shows you this Quest",
    "start": "1462240",
    "end": "1468720"
  },
  {
    "text": "body but it also has extra things that we need so the endpoint name and then lo",
    "start": "1468720",
    "end": "1475320"
  },
  {
    "text": "and behold the inference component name so if you are getting errors in your",
    "start": "1475320",
    "end": "1482640"
  },
  {
    "text": "line chain and stagemaker connector like I did until I figured this out that you need to add an inference component just",
    "start": "1482640",
    "end": "1490000"
  },
  {
    "text": "go in here copy this hoof back over to your notebook and",
    "start": "1490000",
    "end": "1496200"
  },
  {
    "text": "then just add that to the endpoint keyword arguments uh so your stagemaker",
    "start": "1496200",
    "end": "1502799"
  },
  {
    "text": "endpoint object can take a few arguments such as the name of the endpoint your B3",
    "start": "1502799",
    "end": "1508320"
  },
  {
    "text": "client model keyword arguments and then endpoint keyword arguments when you're",
    "start": "1508320",
    "end": "1513679"
  },
  {
    "text": "working with LL models you have to add this accept Ula and then uh you should",
    "start": "1513679",
    "end": "1519360"
  },
  {
    "text": "also add this inference component name once you've and then the content Handler",
    "start": "1519360",
    "end": "1525120"
  },
  {
    "text": "once you've done that then we're going to build this mini chain here that's just a summarization chain and",
    "start": "1525120",
    "end": "1531520"
  },
  {
    "text": "that takes this uh element function and then the prompt the model and then the",
    "start": "1531520",
    "end": "1537320"
  },
  {
    "text": "output parser and I find it very healthy to make sure that your components are",
    "start": "1537320",
    "end": "1543520"
  },
  {
    "text": "working nicely um so you can also just invoke the model so you can hit this",
    "start": "1543520",
    "end": "1549200"
  },
  {
    "text": "model just grab the name and then say model. invoke let's see if this is on",
    "start": "1549200",
    "end": "1554880"
  },
  {
    "text": "our side right now",
    "start": "1554880",
    "end": "1558559"
  },
  {
    "text": "great okay so our model is is definitely with us today all right so now uh we're going to",
    "start": "1561600",
    "end": "1569000"
  },
  {
    "text": "generate text summaries and",
    "start": "1569000",
    "end": "1574080"
  },
  {
    "text": "so what's interesting is that so Lang chain actually does batching for you by",
    "start": "1574159",
    "end": "1579640"
  },
  {
    "text": "default which I think is is pretty interesting actually so you can build this chain right here right we we built",
    "start": "1579640",
    "end": "1586880"
  },
  {
    "text": "the summarized chain and then it's actually going to let us batch the",
    "start": "1586880",
    "end": "1592279"
  },
  {
    "text": "responses just by calling badge here which I think is really nice so",
    "start": "1592279",
    "end": "1598200"
  },
  {
    "text": "presumably that I don't actually know but I think what this is doing is using",
    "start": "1598200",
    "end": "1603840"
  },
  {
    "text": "more cores on our machine uh to send a maximum of five",
    "start": "1603840",
    "end": "1610080"
  },
  {
    "text": "requests um for a single point in time although I mean it has to be doing this",
    "start": "1610080",
    "end": "1615840"
  },
  {
    "text": "in a way that's not overloading my model and points so maybe there's some intelligent timing on that batch I'm not",
    "start": "1615840",
    "end": "1622840"
  },
  {
    "text": "sure but in any case I appreciate that that it has this batch by default that's very nice and so then to get summaries",
    "start": "1622840",
    "end": "1631440"
  },
  {
    "text": "from our language model for both the text and the tables this is what we're going to do we run this function to send",
    "start": "1631440",
    "end": "1639080"
  },
  {
    "text": "all of that in and it just does it for us which is really nice and then again",
    "start": "1639080",
    "end": "1644200"
  },
  {
    "text": "we'll just check the first text summary I mean these numbers are odd but the rest of it looks fine so it's so so",
    "start": "1644200",
    "end": "1653279"
  },
  {
    "text": "this is a figure and then it's summarizing uh essentially the text",
    "start": "1653279",
    "end": "1659320"
  },
  {
    "text": "that's probably below this figure where they're sampling 10 to 20 trajectories for each input and then Computing the",
    "start": "1659320",
    "end": "1665799"
  },
  {
    "text": "likelihood score for each of these pairs so that's our first text summary let's",
    "start": "1665799",
    "end": "1671000"
  },
  {
    "text": "assume that the rest of them are okay now that we have text summaries for",
    "start": "1671000",
    "end": "1676279"
  },
  {
    "text": "both the text and the tables we're going to use our local lava model to generate",
    "start": "1676279",
    "end": "1683159"
  },
  {
    "text": "summaries for the images in natural language so that's what lava is going to do lava is going to look at the pictures",
    "start": "1683159",
    "end": "1691039"
  },
  {
    "text": "and it's going to convert the picture into a natural Language summary and then the summaries are what we're going to",
    "start": "1691039",
    "end": "1697399"
  },
  {
    "text": "use as our index which I think is very clever so great we're going to Loop through those JPEG files generated by",
    "start": "1697399",
    "end": "1704120"
  },
  {
    "text": "unstructured let's go see where those are so back in our working directory",
    "start": "1704120",
    "end": "1709880"
  },
  {
    "text": "which is right out here the unstructured library is going to create this new folder called",
    "start": "1709880",
    "end": "1717039"
  },
  {
    "text": "figures and so figures is just as it sounds all of the figures from the image",
    "start": "1717039",
    "end": "1724919"
  },
  {
    "text": "so we saw this one input sentences trajectories comparisons let's look at this one",
    "start": "1724919",
    "end": "1732360"
  },
  {
    "text": "another graph okay great so again once you get all of those image packag is installed",
    "start": "1732360",
    "end": "1739880"
  },
  {
    "text": "then this is what unstructured will do that Library all right",
    "start": "1739880",
    "end": "1745000"
  },
  {
    "text": "great so we have those figures we're going to walk through that uh list we're",
    "start": "1745000",
    "end": "1752200"
  },
  {
    "text": "going to walk through the files in that directory we're going to send each to this local lava model and have it",
    "start": "1752200",
    "end": "1757360"
  },
  {
    "text": "generate a summary for us of what it sees in that image so that's right here",
    "start": "1757360",
    "end": "1762519"
  },
  {
    "text": "so uh this I needed to write U myself because of course the line chain uh solution is pointing to just one model",
    "start": "1762519",
    "end": "1769039"
  },
  {
    "text": "that does both language and image and I decided to kind of split it up and do lots of different models and so this one",
    "start": "1769039",
    "end": "1775200"
  },
  {
    "text": "I I definitely need it to write myself so this is just an image summarized function that which actually takes that",
    "start": "1775200",
    "end": "1781200"
  },
  {
    "text": "whole lava model that we downloaded earlier and then because it's all just sitting on my notebook it just takes the",
    "start": "1781200",
    "end": "1787399"
  },
  {
    "text": "path for the image and then the prompt so we're just going to open that image run it through that processor along with",
    "start": "1787399",
    "end": "1794120"
  },
  {
    "text": "the prompt generate the output and and then uh decode the output",
    "start": "1794120",
    "end": "1800440"
  },
  {
    "text": "and then remember the decoding uh takes basically the probability distribution that comes out of the lava model and",
    "start": "1800440",
    "end": "1807039"
  },
  {
    "text": "then converts that back into language and so this is our message the encoding um into base 64 you",
    "start": "1807039",
    "end": "1814799"
  },
  {
    "text": "don't need that for lava um but you do need that to build the um database",
    "start": "1814799",
    "end": "1820799"
  },
  {
    "text": "actually the local chroma DB that's going to actually index the images uh",
    "start": "1820799",
    "end": "1825960"
  },
  {
    "text": "and so that's where that's useful okay and so next just as promised we're",
    "start": "1825960",
    "end": "1831919"
  },
  {
    "text": "going to walk through uh this local path here look at all the jpeg images pick",
    "start": "1831919",
    "end": "1837000"
  },
  {
    "text": "them all up encode them add them to the encoded list and then uh summarize the",
    "start": "1837000",
    "end": "1843679"
  },
  {
    "text": "image yeah so then run that image summarized function and we're going to pass in the lava model the path and the",
    "start": "1843679",
    "end": "1849039"
  },
  {
    "text": "PRT all right great we return this run it and then once this has finished",
    "start": "1849039",
    "end": "1855279"
  },
  {
    "text": "running um here is just a another understand check so that's that first uh",
    "start": "1855279",
    "end": "1860880"
  },
  {
    "text": "object in the image summaries list and I just removed this instruct an",
    "start": "1860880",
    "end": "1866600"
  },
  {
    "text": "instruction syntax because it's it's a little challenging on the eyes okay",
    "start": "1866600",
    "end": "1872399"
  },
  {
    "text": "great and so here's the summary image displays two bar graphs each",
    "start": "1872399",
    "end": "1877600"
  },
  {
    "text": "representing different data sets left is labeled number trajectories right graph is called",
    "start": "1877600",
    "end": "1884600"
  },
  {
    "text": "temperature the graphs are informative and appear to be part of a scientific or technical presentation nice job lava",
    "start": "1884600",
    "end": "1891039"
  },
  {
    "text": "nailed it okay so now we have summaries of the",
    "start": "1891039",
    "end": "1896480"
  },
  {
    "text": "text we have summaries of the figures we have summaries of the images let's put all of these together into one vector",
    "start": "1896480",
    "end": "1903120"
  },
  {
    "text": "store all right so now let's try to spell multi",
    "start": "1903120",
    "end": "1908960"
  },
  {
    "text": "correctly all right so we're going to create a multi Vector retrieval store and again that's storing the text",
    "start": "1909240",
    "end": "1916360"
  },
  {
    "text": "tables and images in this local doc store and then we're going to store the text summaries and images in the vector",
    "start": "1916360",
    "end": "1923799"
  },
  {
    "text": "store and so this sort of dual storage that's going on here lets us um first",
    "start": "1923799",
    "end": "1930360"
  },
  {
    "text": "look up the vectors in this Vector store and then from the vectors we're going to point to the",
    "start": "1930360",
    "end": "1937720"
  },
  {
    "text": "documents all right and so to do this again uh we need an embedding model and",
    "start": "1938240",
    "end": "1943559"
  },
  {
    "text": "so um as I mentioned earlier I went into Sage Baker studio and opened up up the embedding model and deployed this uh",
    "start": "1943559",
    "end": "1950600"
  },
  {
    "text": "onto an end point and then same as for the chat model um you just need the",
    "start": "1950600",
    "end": "1955960"
  },
  {
    "text": "endpoint name no inference component name for the endpoints though so that was a lot easier okay uh so we're going to import",
    "start": "1955960",
    "end": "1963080"
  },
  {
    "text": "from Lang chain this multiv Vector retrieval uh point to",
    "start": "1963080",
    "end": "1968360"
  },
  {
    "text": "chroma and then uh so there is a different object actually for endpoint",
    "start": "1968360",
    "end": "1973639"
  },
  {
    "text": "embeddings which is this one so sagemaker endpoint embeddings and then from the stagemaker endpoints",
    "start": "1973639",
    "end": "1980559"
  },
  {
    "text": "object and embeddings content Handler so get those two Imports and then uh this took a little",
    "start": "1980559",
    "end": "1988960"
  },
  {
    "text": "bit of munching just to make sure that um I had the inputs basically correct",
    "start": "1988960",
    "end": "1995240"
  },
  {
    "text": "and that it was going directly to Json and I just had to modify the key",
    "start": "1995240",
    "end": "2000440"
  },
  {
    "text": "basically to actually hit the right key so that's the input and then the outputs",
    "start": "2000440",
    "end": "2006600"
  },
  {
    "text": "um same as last time read that load it into Json and then this one I also had to",
    "start": "2006600",
    "end": "2013240"
  },
  {
    "text": "modify a little bit so the function um the chain is expecting that this",
    "start": "2013240",
    "end": "2019440"
  },
  {
    "text": "function is going to Output vectors um and so just make sure that you output the vectors from this function directly",
    "start": "2019440",
    "end": "2026200"
  },
  {
    "text": "and in this case that was model predictions and then this embedding key",
    "start": "2026200",
    "end": "2031440"
  },
  {
    "text": "uh hashes to the actual vectors and so that's what I return all right great so with that will",
    "start": "2031440",
    "end": "2038080"
  },
  {
    "text": "rerun the uh stage maker runtime client and then as promised just pass in the",
    "start": "2038080",
    "end": "2043600"
  },
  {
    "text": "endpoint name a client and then that uh embedding content",
    "start": "2043600",
    "end": "2048878"
  },
  {
    "text": "Handler so I should probably have named this embedding content Handler um because it might conflict with the",
    "start": "2048879",
    "end": "2055240"
  },
  {
    "text": "earlier one that's also called content Handler so consider actually changing the names for",
    "start": "2055240",
    "end": "2061040"
  },
  {
    "text": "those all right now we're going to create this multiv Vector retriever so this is going to take an in",
    "start": "2061040",
    "end": "2067839"
  },
  {
    "text": "memory store this doc ID multiv Vector retriever takes the vector store and the",
    "start": "2067839",
    "end": "2073800"
  },
  {
    "text": "ID key and the document store so again two storage objects one is the uh",
    "start": "2073800",
    "end": "2080079"
  },
  {
    "text": "basically the the vector store and then the vector store points to the documents",
    "start": "2080079",
    "end": "2085960"
  },
  {
    "text": "so we're going to use the vector store to find the right document and then we go and retrieve the document itself from",
    "start": "2085960",
    "end": "2091440"
  },
  {
    "text": "the doc store so then we're going to injust the documents using this function that",
    "start": "2091440",
    "end": "2098359"
  },
  {
    "text": "worked beautifully thank you writers uh and then add",
    "start": "2098359",
    "end": "2104320"
  },
  {
    "text": "those then return the Retriever and then now we have this chroma Vector store that works and that",
    "start": "2104320",
    "end": "2113160"
  },
  {
    "text": "has a name and the embeddings and then we create the",
    "start": "2113160",
    "end": "2118520"
  },
  {
    "text": "Retriever and then this retriever has the vector store the summaries the text",
    "start": "2118520",
    "end": "2124040"
  },
  {
    "text": "for all three of our different parts and then just just to check that the vector store is indeed a valid",
    "start": "2124040",
    "end": "2131000"
  },
  {
    "text": "chroma and then the multiv vector image is also a valid",
    "start": "2131000",
    "end": "2136400"
  },
  {
    "text": "object all right now that we created those two Vector stores we're going to",
    "start": "2136400",
    "end": "2142960"
  },
  {
    "text": "build those into the chain so that we can actually retrieve documents and then",
    "start": "2142960",
    "end": "2149000"
  },
  {
    "text": "send them to the language models and get response back all in the same session so we're going to put it all together",
    "start": "2149000",
    "end": "2155040"
  },
  {
    "text": "here so again this this needs the the um uh B 64 image encoding to retrieve the",
    "start": "2155040",
    "end": "2162160"
  },
  {
    "text": "images and actually the chain is set up to also take images um but I just decided to only take um",
    "start": "2162160",
    "end": "2170880"
  },
  {
    "text": "text so part of that is for uh sending in images as the core for the retrieval",
    "start": "2171280",
    "end": "2178560"
  },
  {
    "text": "here all right lots of again helpful functions that worked very nicely I was",
    "start": "2178560",
    "end": "2184400"
  },
  {
    "text": "very happy okay great and then the T so so I changed this uh so CJ built this to uh",
    "start": "2184400",
    "end": "2191800"
  },
  {
    "text": "be a financial adviser actually that will um look at documents about",
    "start": "2191800",
    "end": "2198400"
  },
  {
    "text": "companies and give Financial advice for those uh investment opportunities I",
    "start": "2198400",
    "end": "2204520"
  },
  {
    "text": "modify this to be an AI scientist uh so this AI is an AI scientist uh task with",
    "start": "2204520",
    "end": "2212599"
  },
  {
    "text": "giving examples on AI research projects um they will be given a mix of text tables and images usually charts or",
    "start": "2212599",
    "end": "2218880"
  },
  {
    "text": "graphs and then use this to provide AI research advice related to the user question and so the the text again will",
    "start": "2218880",
    "end": "2226720"
  },
  {
    "text": "take in the question and then it's going to look at the tables okay so now we'll build this",
    "start": "2226720",
    "end": "2236000"
  },
  {
    "text": "chain I was feeling a little bit lazy so I just pasted the stage Baker endpoint",
    "start": "2236000",
    "end": "2241400"
  },
  {
    "text": "right here into this multimodal chain you could pass it as an argument but it's a long notebook so I just just",
    "start": "2241400",
    "end": "2247640"
  },
  {
    "text": "repasted it so that's that uh chat model that we pointed to in the first",
    "start": "2247640",
    "end": "2253079"
  },
  {
    "text": "case and then we're going to set up the the rag pipeline here so this is the whole chain that we're going to use to",
    "start": "2253079",
    "end": "2260119"
  },
  {
    "text": "ask questions um to this whole component here so we have the context with the",
    "start": "2260119",
    "end": "2265560"
  },
  {
    "text": "retriever which uses this runnable Lambda and then the question uh which is",
    "start": "2265560",
    "end": "2271359"
  },
  {
    "text": "this pass through and so the retriever of course goes and retrieves the documents based on the vector and then",
    "start": "2271359",
    "end": "2279240"
  },
  {
    "text": "the quest of course comes through from the customer all right so that's the chain",
    "start": "2279240",
    "end": "2284599"
  },
  {
    "text": "and that's sort of like setting uh key value pairs for the whole chain and then",
    "start": "2284599",
    "end": "2290319"
  },
  {
    "text": "we invoke this runnable Lambda on the image prompt function which then hits the model uh",
    "start": "2290319",
    "end": "2298480"
  },
  {
    "text": "and then we output it and it worked incredibly well once I had things set up",
    "start": "2298480",
    "end": "2305240"
  },
  {
    "text": "it it ran like a Char okay so now we're going to check the Retriever and I asked my question give me papers that are",
    "start": "2305240",
    "end": "2311280"
  },
  {
    "text": "related to large language models and I got four documents and here was an",
    "start": "2311280",
    "end": "2317800"
  },
  {
    "text": "image and then I asked for papers that are related to multimodal retrieval and",
    "start": "2317800",
    "end": "2322960"
  },
  {
    "text": "I got four and here was the first one all right so I hope you enjoyed this",
    "start": "2322960",
    "end": "2329640"
  },
  {
    "text": "multimodal retrieval demo let me get you some resources all right so two resources for",
    "start": "2329640",
    "end": "2335880"
  },
  {
    "text": "you so the first one um of course is the documentation and so that shows how to customize a foundation model by",
    "start": "2335880",
    "end": "2342520"
  },
  {
    "text": "developing a retrieval augmented generation pipeline um using uh sagemaker jumart Foundation models and",
    "start": "2342520",
    "end": "2350040"
  },
  {
    "text": "then the second link um is an example notebook so that example notebook",
    "start": "2350040",
    "end": "2355359"
  },
  {
    "text": "actually stands up rag with Amazon open search and then sagemaker um so feel free to check that one out and then um",
    "start": "2355359",
    "end": "2363400"
  },
  {
    "text": "of course if you want to see the multimodal project um just check out the link in the video directly and so that",
    "start": "2363400",
    "end": "2370680"
  },
  {
    "text": "my friends is a wrap I hope you enjoyed this video about multimodal retrial augmented generation with sage maker I",
    "start": "2370680",
    "end": "2377440"
  },
  {
    "text": "will catch you next time bye",
    "start": "2377440",
    "end": "2381480"
  }
]