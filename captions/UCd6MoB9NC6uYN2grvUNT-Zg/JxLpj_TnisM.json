[
  {
    "text": "hi everybody thank you for coming my name is Rahul Pathak I'm the senior product manager for Amazon redshift",
    "start": "0",
    "end": "6080"
  },
  {
    "text": "appreciate your time today you'll learn a little bit about why we built redshift how we achieve some of the performance",
    "start": "6080",
    "end": "12570"
  },
  {
    "text": "that we talk about an ease of use and then you also hear from one of our early",
    "start": "12570",
    "end": "18090"
  },
  {
    "text": "customers Airbnb we've got Henry and Flo here to talk about their experiences with redshift and how it fit into their",
    "start": "18090",
    "end": "24449"
  },
  {
    "text": "infrastructure so redshift exists because our customers asked for us to",
    "start": "24449",
    "end": "30660"
  },
  {
    "text": "build them a way to do data warehousing that didn't require tons of upfront investment that made it easy for them to",
    "start": "30660",
    "end": "37680"
  },
  {
    "text": "provision data warehouses and we also wanted to deliver something that was fast cheap and easy to use",
    "start": "37680",
    "end": "43950"
  },
  {
    "text": "and finally we wanted it to be compatible with the tool sets that people use today and that means sequel",
    "start": "43950",
    "end": "50579"
  },
  {
    "text": "and JDBC and ODBC so I'll talk about all of those in just a second and what we",
    "start": "50579",
    "end": "56309"
  },
  {
    "text": "set out to build was a fast petabyte scale data warehouse service and our our",
    "start": "56309",
    "end": "61559"
  },
  {
    "text": "goals were really to build something that was 10 times faster than traditional databases for analytic",
    "start": "61559",
    "end": "66840"
  },
  {
    "text": "queries and at least the 10th the cost of traditional data warehousing solutions and we also wanted it to be",
    "start": "66840",
    "end": "72960"
  },
  {
    "text": "really easy to use and we're off to a good start we're excited about the",
    "start": "72960",
    "end": "78330"
  },
  {
    "text": "response that we've seen in the market I think you heard Andy at the keynote talk about redshift being the fastest growing",
    "start": "78330",
    "end": "83820"
  },
  {
    "text": "AWS service and while we're excited about our beginning we also know we have a ton of work to do in terms of",
    "start": "83820",
    "end": "90500"
  },
  {
    "text": "improvements we'd like to make and as always I think we'd like to make things faster and easier to use and improve",
    "start": "90500",
    "end": "96720"
  },
  {
    "text": "along all the dimensions that matter to our customers and so let's talk about",
    "start": "96720",
    "end": "102420"
  },
  {
    "text": "performance so at data warehousing it's scale IO really becomes one of the important factors and what we're trying",
    "start": "102420",
    "end": "109560"
  },
  {
    "text": "to do with redshift is use columnar technology to reduce the amount of i/o that we need to do so if you look at",
    "start": "109560",
    "end": "115560"
  },
  {
    "text": "this table in a traditional row base store if you're trying to get the average of the amounts you're gonna have",
    "start": "115560",
    "end": "121469"
  },
  {
    "text": "to read every single item in your table do the full scan and then throw away three-quarters of the data just to get",
    "start": "121469",
    "end": "126960"
  },
  {
    "text": "the column that you care about so you end up doing more i/o than you need whereas with a column based system",
    "start": "126960",
    "end": "133800"
  },
  {
    "text": "columnar data stored sequentially on disk you can go to the start of the column that you want and then you'll get",
    "start": "133800",
    "end": "140100"
  },
  {
    "text": "just the read that you're looking for and you end up doing only the i/o that you want you just read the data that you",
    "start": "140100",
    "end": "146430"
  },
  {
    "text": "need to answer the queries that you're working on the other benefit with columnar technology is you get",
    "start": "146430",
    "end": "153030"
  },
  {
    "text": "compression benefits so with like data stored sorry about the advancing you",
    "start": "153030",
    "end": "158730"
  },
  {
    "text": "would like data stored on disk sequentially you're able to take advantage of the fact that there's",
    "start": "158730",
    "end": "164580"
  },
  {
    "text": "similar data next to each other and get compression as you can see we use in",
    "start": "164580",
    "end": "169980"
  },
  {
    "text": "multiple different types of compression algorithms based on the datasets that are being loaded and red shift is smart",
    "start": "169980",
    "end": "175650"
  },
  {
    "text": "enough to analyze and sample your data when you first load an empty table and to pick the compression scheme that's",
    "start": "175650",
    "end": "181320"
  },
  {
    "text": "most suited for that data so in real-world datasets our customers see anywhere from 4 X - 8 X savings over",
    "start": "181320",
    "end": "188880"
  },
  {
    "text": "there in tag database and so what that means is for terabyte database coming from a traditional data source becomes a",
    "start": "188880",
    "end": "195900"
  },
  {
    "text": "one terabyte data warehouse in redshift and that's real-world data some columns you can imagine if you had a column just",
    "start": "195900",
    "end": "202080"
  },
  {
    "text": "containing gender with two values that would compress far better and so this is",
    "start": "202080",
    "end": "207990"
  },
  {
    "text": "just taken as a whole over their data set and the big benefit for customers with this is one it costs less so more",
    "start": "207990",
    "end": "214830"
  },
  {
    "text": "of your data will fit into a given size redshift instance and the second thing is when we do end up doing reads we get",
    "start": "214830",
    "end": "221580"
  },
  {
    "text": "more data back from the system so we're more efficient it leads to better performance another technique for",
    "start": "221580",
    "end": "228300"
  },
  {
    "text": "performance and i/o efficiency that we use in redshift extensively is zone maps and the idea here is to keep track of",
    "start": "228300",
    "end": "235320"
  },
  {
    "text": "the min and Max values in every block that's on disk and so when you have sorted columns zone maps allow you to",
    "start": "235320",
    "end": "243960"
  },
  {
    "text": "essentially focus on the data that you need so if we know that a data data isn't in a given block to answer the",
    "start": "243960",
    "end": "249750"
  },
  {
    "text": "query that we're trying to run we can skip over it and this is why sort Keys",
    "start": "249750",
    "end": "255510"
  },
  {
    "text": "come into play and we'll talk about data distribution on the system in a little bit but it allows you to focus on i/o",
    "start": "255510",
    "end": "260670"
  },
  {
    "text": "that you need to do and then when we decide that we have to do I owe and actually need to go to disk to do reads",
    "start": "260670",
    "end": "267090"
  },
  {
    "text": "we want them to be as high-performance as possible and so we use direct-attached storage and I'll talk about our instance",
    "start": "267090",
    "end": "274080"
  },
  {
    "text": "types but they're optimized for high performance IO and because we maximize",
    "start": "274080",
    "end": "280110"
  },
  {
    "text": "the amount of performance you get because the drives and the CPU and the memory systems are all local we also",
    "start": "280110",
    "end": "285540"
  },
  {
    "text": "have to manage durability in an ec2 environment and we take care of that for you as well so you get the benefits of performance and then as a managed",
    "start": "285540",
    "end": "292050"
  },
  {
    "text": "service we take care of management and then with large block sizes and with",
    "start": "292050",
    "end": "297090"
  },
  {
    "text": "compression when we do end up doing a read we get as much relevant data as possible so overall a lot of efforts",
    "start": "297090",
    "end": "303060"
  },
  {
    "text": "been put in to try and figure out how to get the most performance out of the system so from an architecture",
    "start": "303060",
    "end": "310320"
  },
  {
    "text": "perspective redshift is designed to be run as a clustered data warehouse although we do have a single node",
    "start": "310320",
    "end": "316320"
  },
  {
    "text": "variant available that gives you a low cost easy entry point as you can see",
    "start": "316320",
    "end": "321870"
  },
  {
    "text": "you've got JDBC and ODBC to connect your sequel clients we use the Postgres wire",
    "start": "321870",
    "end": "326970"
  },
  {
    "text": "protocol you can download drivers from the Postgres website so really any system that speaks Postgres over JDBC or",
    "start": "326970",
    "end": "333660"
  },
  {
    "text": "ODBC you can use to talk to redshift there's a leader node which is your sequel endpoint that stores system",
    "start": "333660",
    "end": "339900"
  },
  {
    "text": "metadata and it also coordinates query execution and then the compute nodes is",
    "start": "339900",
    "end": "345360"
  },
  {
    "text": "where all the data stored so those systems have the drives and there's a CPU and RAM that operates on your",
    "start": "345360",
    "end": "351600"
  },
  {
    "text": "queries and those are loaded and backed up in parallel we'll go into each of those operations as well and the whole",
    "start": "351600",
    "end": "358710"
  },
  {
    "text": "idea here is you issue queries at the leader node those are then distributed and executed in parallel across the",
    "start": "358710",
    "end": "364260"
  },
  {
    "text": "cluster maximizing the resource utilization to try and get you back results as quickly as possible so our",
    "start": "364260",
    "end": "372690"
  },
  {
    "text": "hardware platform is one that's optimized for data processing it's actually the it's also available on ec2",
    "start": "372690",
    "end": "378150"
  },
  {
    "text": "it's the high storage instance it was announced at reinvent in November and",
    "start": "378150",
    "end": "383640"
  },
  {
    "text": "it's a pretty beefy box it's got 128 gigs of RAM 16 cores 24 spinning disks",
    "start": "383640",
    "end": "390240"
  },
  {
    "text": "on it 24 spindles for parallel i/o and 16 terabytes of compressed user storage",
    "start": "390240",
    "end": "395490"
  },
  {
    "text": "and so it for a 4x compression you can imagine a 64 terabyte abase will fit",
    "start": "395490",
    "end": "401040"
  },
  {
    "text": "onto these notes and you get over two gigabytes per second scanned rate between the drives in the CPU system so",
    "start": "401040",
    "end": "409260"
  },
  {
    "text": "that's our eight extra large that's also available on ec2 if you want to run your own systems there and in redshift",
    "start": "409260",
    "end": "416370"
  },
  {
    "text": "uniquely we make it available in a 1/8 size which is the single XL and that's 1/8 on all dimensions so 16 gigs of ram",
    "start": "416370",
    "end": "424229"
  },
  {
    "text": "2 cores and 2 terabytes of storage again this is optimized for high",
    "start": "424229",
    "end": "429360"
  },
  {
    "text": "performance on data processing it lives in our HPC which is our high performance computing network so non-blocking non",
    "start": "429360",
    "end": "436530"
  },
  {
    "text": "oversubscribed network in that system and it's really the underpinning of redshift from a hardware perspective so",
    "start": "436530",
    "end": "443310"
  },
  {
    "text": "the other part of performance so another aim with performance is redshift is a",
    "start": "443310",
    "end": "450180"
  },
  {
    "text": "clustered system so we try and do as much work in parallel as we can and so this is a look at what happens in the",
    "start": "450180",
    "end": "456330"
  },
  {
    "text": "leader node when you issue a query we actually will parse your sequel to any logical optimizations that we can well",
    "start": "456330",
    "end": "463560"
  },
  {
    "text": "then we'll take advantage of the system's knowledge of how data is laid out and statistics on the tables that",
    "start": "463560",
    "end": "469080"
  },
  {
    "text": "you have to build a query plan that then goes to an execution engine where we",
    "start": "469080",
    "end": "474210"
  },
  {
    "text": "actually generate and compile C++ code and then push it out and execute it on each of the compute nodes in parallel",
    "start": "474210",
    "end": "480389"
  },
  {
    "text": "and that compute that compiled code is actually stored in the system and reused",
    "start": "480389",
    "end": "485849"
  },
  {
    "text": "so even though parameters change we don't have to incur the compilation penalty more than once and there",
    "start": "485849",
    "end": "492330"
  },
  {
    "text": "multiple threads on the compute nodes executing this compute this compiled code in parallel again a key part of how",
    "start": "492330",
    "end": "498750"
  },
  {
    "text": "we're trying to get performance we want to divide and conquer the workloads and as you've seen in the architecture it's a scale out system so you can add more",
    "start": "498750",
    "end": "505620"
  },
  {
    "text": "nodes both to get storage but also to get performance because you'll get more memory more more CPU cores acting on a",
    "start": "505620",
    "end": "513180"
  },
  {
    "text": "smaller amount of data if your data size is fixed and your node size goes up and",
    "start": "513180",
    "end": "518719"
  },
  {
    "text": "in addition to querying we've also focused on mating making data loads parallel and the reason is we don't want",
    "start": "518719",
    "end": "524850"
  },
  {
    "text": "you to have to spend extra time loading a cluster when you have two nodes or 20 nodes and so data loading at the moment",
    "start": "524850",
    "end": "531180"
  },
  {
    "text": "can be done via a single sequel copy command it points to an s3 or a dynamodb table and it then loads",
    "start": "531180",
    "end": "538110"
  },
  {
    "text": "each compute node in parallel simultaneously and on the compute nodes",
    "start": "538110",
    "end": "543360"
  },
  {
    "text": "for each core we have a process called a slice and so on a single Excel node you",
    "start": "543360",
    "end": "548790"
  },
  {
    "text": "have two slices and on the 8xl you have sixteen and you should map the number of input files that you're loading to the",
    "start": "548790",
    "end": "554699"
  },
  {
    "text": "number of slices because we load each of those slices is loading data in parallel the nodes are also all connected by a",
    "start": "554699",
    "end": "561810"
  },
  {
    "text": "ten gig e and so if data needs to be redistributed from one node to another that'll happen automatically one part of",
    "start": "561810",
    "end": "569190"
  },
  {
    "text": "columnar systems is that whenever you have data that needs to be joined you want to have it live on the same node as",
    "start": "569190",
    "end": "575399"
  },
  {
    "text": "much as possible because joins that are local to the node will perform better that joins that have to go over to network and so one of the options that",
    "start": "575399",
    "end": "582449"
  },
  {
    "text": "you have when creating tables is a choice of a distribution key and this is how you physically layout data and so if",
    "start": "582449",
    "end": "588449"
  },
  {
    "text": "you can think about it for example if you have a customer ID and a customer sees ads and a customer also has",
    "start": "588449",
    "end": "594269"
  },
  {
    "text": "accounts and makes transactions you might want to distribute on customer ID so all data for that customer lives on",
    "start": "594269",
    "end": "600120"
  },
  {
    "text": "any on the same node and therefore when you run joins for that customer all this data is node local and you will get",
    "start": "600120",
    "end": "605579"
  },
  {
    "text": "better performance out of the system overall the other thing that happens here is that this is all done behind the scenes",
    "start": "605579",
    "end": "611790"
  },
  {
    "text": "so all you have to do is pick the number of nodes to find your table and then load your data you can also do sequel",
    "start": "611790",
    "end": "617339"
  },
  {
    "text": "loads using inserts and updates via JDBC and ODBC that does not perform as well",
    "start": "617339",
    "end": "623010"
  },
  {
    "text": "as the parallel loading mechanism so really it's a question of looking at your data velocity and figuring out",
    "start": "623010",
    "end": "628320"
  },
  {
    "text": "which mechanism is best we do recommend the s3 or the dynamodb pathway and we'll",
    "start": "628320",
    "end": "633720"
  },
  {
    "text": "be adding more of those over time with DynamoDB you use a copy command as well you specify a table and you can tell it",
    "start": "633720",
    "end": "641010"
  },
  {
    "text": "how much of your provision throughput you want to use and it will then load that data into redshift in parallel",
    "start": "641010",
    "end": "646260"
  },
  {
    "text": "again we also have invested a lot of effort in continuous backup and easy",
    "start": "646260",
    "end": "653819"
  },
  {
    "text": "restores and so backups are also done at the node level they're done in parallel they are automatic and they're encrypt",
    "start": "653819",
    "end": "659910"
  },
  {
    "text": "and they are incremental and so anytime data changes in a cluster we will kick off a backup process that will",
    "start": "659910",
    "end": "666449"
  },
  {
    "text": "asynchronously copy data to s3 and just copy what's needed so if your data doesn't change you won't see any",
    "start": "666449",
    "end": "672330"
  },
  {
    "text": "new blocks in s3 and that system snapshot period is configurable so it defaults to one day",
    "start": "672330",
    "end": "677910"
  },
  {
    "text": "you can set it to up to 35 days and we'll just age out old data as that window slides the other thing that you",
    "start": "677910",
    "end": "685770"
  },
  {
    "text": "can do is take a user snapshot at any point in time so say for example you've just loaded data and you want to freeze",
    "start": "685770",
    "end": "690960"
  },
  {
    "text": "cluster State you can trigger a snapshot via the console or an API call and that",
    "start": "690960",
    "end": "696060"
  },
  {
    "text": "will give you the state of that cluster that'll be retained until you explicitly delete it so you have a lot of control",
    "start": "696060",
    "end": "701460"
  },
  {
    "text": "over how your clusters backups are stored and at any point you can restore",
    "start": "701460",
    "end": "707040"
  },
  {
    "text": "a new cluster from any of those snapshots and we will set it up reload the data onto it and make it available",
    "start": "707040",
    "end": "712110"
  },
  {
    "text": "to you for querying one feature that we spend a lot of time on to try and",
    "start": "712110",
    "end": "717590"
  },
  {
    "text": "minimize the time it takes for your cluster to start performing normally when we restore it is a feature we call",
    "start": "717590",
    "end": "724140"
  },
  {
    "text": "streaming restore so if you think about it we have nodes that can store up to 16 terabytes of data on each one and you",
    "start": "724140",
    "end": "730860"
  },
  {
    "text": "don't want have to wait for 16 T to be loaded on each node when you do a restore and so what streaming restore as",
    "start": "730860",
    "end": "737490"
  },
  {
    "text": "soon as the node is available and connected to your cluster which takes minutes we begin bringing back data that",
    "start": "737490",
    "end": "743040"
  },
  {
    "text": "was most recently used first and allowing you to execute queries and then the system will page in data from s3 as",
    "start": "743040",
    "end": "749610"
  },
  {
    "text": "needed and what we found for customers is that because they typically have some time locality your access patterns and",
    "start": "749610",
    "end": "755640"
  },
  {
    "text": "their data once just three to five percent of the data is back query performance is normal for most queries",
    "start": "755640",
    "end": "761040"
  },
  {
    "text": "that they would run again all automatic done done behind the scenes",
    "start": "761040",
    "end": "766910"
  },
  {
    "text": "so resize is another operation that's really easy to do in the cloud you can",
    "start": "766910",
    "end": "772710"
  },
  {
    "text": "pick the number of nodes in your cluster we let you start with one scale up to up",
    "start": "772710",
    "end": "777930"
  },
  {
    "text": "to a hundred of the larger ones we'll talk about the details of that in a second and the resized operation what we",
    "start": "777930",
    "end": "783150"
  },
  {
    "text": "actually do is provision a brand new cluster in the background and we do a parallel node to node copy from each",
    "start": "783150",
    "end": "789240"
  },
  {
    "text": "node we'll move data from your source cluster to your target cluster and your original cluster is available for reads",
    "start": "789240",
    "end": "795540"
  },
  {
    "text": "while this is happening and if you think about it what this means is the time it takes to resize is really limited by the",
    "start": "795540",
    "end": "802020"
  },
  {
    "text": "amount of data per node on the smaller cluster so if you have three nodes with four",
    "start": "802020",
    "end": "807970"
  },
  {
    "text": "terabytes each on them a 12t system and you're moving the same amount of data to a four node system you'll have four",
    "start": "807970",
    "end": "814600"
  },
  {
    "text": "terabytes per node on the store start on the source cluster and that'll be the rate limiting step of the resize",
    "start": "814600",
    "end": "820120"
  },
  {
    "text": "operation naturally we only charge you for your source cluster until your",
    "start": "820120",
    "end": "825790"
  },
  {
    "text": "target is active and your sequel endpoints been cut over and once the resize is complete we'll automatically",
    "start": "825790",
    "end": "832030"
  },
  {
    "text": "use DNS to switch your sequel endpoint over to the new cluster so you won't have to do any application configuration",
    "start": "832030",
    "end": "838030"
  },
  {
    "text": "changes so from a scalability perspective what we try to do with",
    "start": "838030",
    "end": "844840"
  },
  {
    "text": "redshift was to make it very easy to really scale up as your needs required",
    "start": "844840",
    "end": "850330"
  },
  {
    "text": "and so you can start with a single Excel node that has two terabytes of customer available compressed storage on it and",
    "start": "850330",
    "end": "856950"
  },
  {
    "text": "those can be run in a clustered model up to 32 of those at a time and that gives",
    "start": "856950",
    "end": "862660"
  },
  {
    "text": "you 64 terabytes of storage at any point you can switch over to the eight XL size",
    "start": "862660",
    "end": "867820"
  },
  {
    "text": "that requires a cluster or a minimum size of two nodes so that's 32 T and you",
    "start": "867820",
    "end": "873910"
  },
  {
    "text": "can chain a hundred of those together to get to 1.6 petabytes we've already started hearing requests for that limit",
    "start": "873910",
    "end": "880690"
  },
  {
    "text": "to be raised from some customers so we will look at that and the there's really",
    "start": "880690",
    "end": "886120"
  },
  {
    "text": "no difference between running 8 single X cells or 1/8 X cell it really comes down",
    "start": "886120",
    "end": "891250"
  },
  {
    "text": "to your scaling increment they're exactly one eighth of the larger systems on each size and along with ease of",
    "start": "891250",
    "end": "900880"
  },
  {
    "text": "provisioning and scalability one of our goals with redshift was to make it really easy and cost-effective to",
    "start": "900880",
    "end": "906880"
  },
  {
    "text": "analyze all of your data so what we're seeing is customers who throw away a ton of their data either to fit in to size",
    "start": "906880",
    "end": "913090"
  },
  {
    "text": "limits of cheaper editions of software that they already have or because their existing systems just can't handle the",
    "start": "913090",
    "end": "919450"
  },
  {
    "text": "scale and what we wanted to do with redshift was to make it economical to analyze much more of your data so you",
    "start": "919450",
    "end": "925120"
  },
  {
    "text": "can focus on really what are the right questions what data do I need to answer it and not how much is it going to cost",
    "start": "925120",
    "end": "930520"
  },
  {
    "text": "me to analyze this and in keeping with our general pricing philosophy at AWS",
    "start": "930520",
    "end": "936370"
  },
  {
    "text": "you can pay by the hour on-demand so the single node pricing that you're seeing here which is 1/8 the pricing of the 8",
    "start": "936370",
    "end": "943540"
  },
  {
    "text": "XL is 85 cents an hour for a single 2 terabyte node and that works out to an",
    "start": "943540",
    "end": "949120"
  },
  {
    "text": "annual cost of about thirty seven hundred dollars per terabyte per year and then we have our reservation models",
    "start": "949120",
    "end": "955410"
  },
  {
    "text": "and this is where you can prepay a slightly larger upfront payment in return for lower ongoing fees and to",
    "start": "955410",
    "end": "962290"
  },
  {
    "text": "give us some visibility into capacity needs and in return with a one-year reservation you get a 40% discount and",
    "start": "962290",
    "end": "968529"
  },
  {
    "text": "then with a 3-year reservation you get a 73% discount which comes down to under",
    "start": "968529",
    "end": "974920"
  },
  {
    "text": "$1,000 per terabyte per year and that's traditional data warehousing systems are",
    "start": "974920",
    "end": "980140"
  },
  {
    "text": "at the 20k per terabyte per year range and higher so this is a significant significant change in the pricing model",
    "start": "980140",
    "end": "986950"
  },
  {
    "text": "and we're seeing some of that translate into a adoption but also be customers",
    "start": "986950",
    "end": "992680"
  },
  {
    "text": "finding and well a lot more use cases for data warehousing than they did before the other part of this is we",
    "start": "992680",
    "end": "998950"
  },
  {
    "text": "don't charge you for the leader node that's something that we absorb and pricing models very simple you can scale",
    "start": "998950",
    "end": "1004920"
  },
  {
    "text": "up scale down it's the number of nodes that are running per hour that determines your costs so we talked about",
    "start": "1004920",
    "end": "1012959"
  },
  {
    "text": "performance and price now ease of use was another thing that we wanted to spend a lot of time on traditionally",
    "start": "1012959",
    "end": "1019650"
  },
  {
    "text": "data warehousing requires lots of DBA is it requires complex node setup you have to understand the networking of the",
    "start": "1019650",
    "end": "1025470"
  },
  {
    "text": "system you have to deal with backups and all of those things just take time and prevent you from focusing on your",
    "start": "1025470",
    "end": "1032069"
  },
  {
    "text": "application and your data and the insights you want to drive from that and so what we've done with redshift is",
    "start": "1032069",
    "end": "1037438"
  },
  {
    "text": "we've designed a system that is easy to use easy to provision and we've also tried to simplify day-to-day operations",
    "start": "1037439",
    "end": "1043620"
  },
  {
    "text": "like security backups resizing and provisioning is a fifteen-minute",
    "start": "1043620",
    "end": "1049650"
  },
  {
    "text": "operation and we're always working to make that faster although last week in a customer meeting they started laughing",
    "start": "1049650",
    "end": "1055290"
  },
  {
    "text": "when I told them that it was 15 minutes because they were used to looking at weeks from a traditional data warehousing model and all you have to do",
    "start": "1055290",
    "end": "1062970"
  },
  {
    "text": "is choose the size of nodes and the number of nodes you want in your system we support V PC out of the gate so you",
    "start": "1062970",
    "end": "1068820"
  },
  {
    "text": "can make those selections and you can launch it and as with all of our services at AWS they're also api's",
    "start": "1068820",
    "end": "1075390"
  },
  {
    "text": "and command-line tools to enable you to do this and you have full control of your resources with things like Identity",
    "start": "1075390",
    "end": "1081960"
  },
  {
    "text": "and Access Management you can decide which users can create clusters provision clusters do any of those",
    "start": "1081960",
    "end": "1087210"
  },
  {
    "text": "things so it's fully programmable from that perspective now the common thing",
    "start": "1087210",
    "end": "1092910"
  },
  {
    "text": "that people want to do is look at query performance and so we spent a lot of effort investing in our console to",
    "start": "1092910",
    "end": "1098640"
  },
  {
    "text": "provide provide you with information on queries when they're running to give you a sense for what resources they consumed",
    "start": "1098640",
    "end": "1105300"
  },
  {
    "text": "so what you're looking at here is a time chart the horizontal bars are queries that ran on the system you can hover",
    "start": "1105300",
    "end": "1112200"
  },
  {
    "text": "over them and then it'll shade the resource utilization on the nodes in the cluster as when that query was being run",
    "start": "1112200",
    "end": "1118440"
  },
  {
    "text": "you can see what we're looking at here that green one at the bottom left and it's shaded on the nodes the two compute",
    "start": "1118440",
    "end": "1126030"
  },
  {
    "text": "nodes in this system have pretty even distribution of data which is what you want to see you don't want to see any",
    "start": "1126030",
    "end": "1131640"
  },
  {
    "text": "hot spotting on a particular node you'll notice utilization is pretty similar and you'll see the leader node is doing",
    "start": "1131640",
    "end": "1136710"
  },
  {
    "text": "close to nothing because it's just coordinating so this is data that's well distributed you'd want to see this sort",
    "start": "1136710",
    "end": "1142230"
  },
  {
    "text": "of pattern when you're executing queries the other thing that you get is you get the sequel for the query on the",
    "start": "1142230",
    "end": "1147420"
  },
  {
    "text": "right-hand side and if you click on the query you can actually get the explained plans you know exactly what the system",
    "start": "1147420",
    "end": "1152490"
  },
  {
    "text": "did and can troubleshoot from there and all this data is also available to you through access to system tables directly",
    "start": "1152490",
    "end": "1159450"
  },
  {
    "text": "from sequel from your sequel client of choice resize we've talked about the",
    "start": "1159450",
    "end": "1165750"
  },
  {
    "text": "mechanics of it it's a very simple operation with redshift all you're doing is picking the number of nodes and the",
    "start": "1165750",
    "end": "1172500"
  },
  {
    "text": "type of nodes and then clicking the button also available via API security",
    "start": "1172500",
    "end": "1179160"
  },
  {
    "text": "is also built-in so we spend a lot of time making sure that redshift had really was able to take advantage of",
    "start": "1179160",
    "end": "1185310"
  },
  {
    "text": "everything that we offered at AWS from a security perspective at launch and so we",
    "start": "1185310",
    "end": "1190530"
  },
  {
    "text": "have SSL to secure data that's in transit you'll enable this on the client side and then can connect to ensure that",
    "start": "1190530",
    "end": "1197340"
  },
  {
    "text": "all data flows are encrypted we also use SSL for any data that we move to and from s3 so there's",
    "start": "1197340",
    "end": "1203909"
  },
  {
    "text": "unencrypted data encrypted data connections between s3 DynamoDB and redshift as well SSL is an option that",
    "start": "1203909",
    "end": "1211589"
  },
  {
    "text": "you can enable on your end and it's a choice that you can do it any time we",
    "start": "1211589",
    "end": "1216690"
  },
  {
    "text": "also have encryption of data at rest so this is hardware-accelerated aes 256 we",
    "start": "1216690",
    "end": "1222929"
  },
  {
    "text": "do this at a very low level in the subsystem so every block that's ever written to disk whether it's data that you've stored",
    "start": "1222929",
    "end": "1228989"
  },
  {
    "text": "when you loaded it but also temporary data that spills over everything is encrypted and then because we're",
    "start": "1228989",
    "end": "1234719"
  },
  {
    "text": "continuously backing up at a block level every block that's put into s3 is also encrypted when you enable this option",
    "start": "1234719",
    "end": "1240769"
  },
  {
    "text": "and we actually use a multi layer key mechanism where each block at its gets",
    "start": "1240769",
    "end": "1246179"
  },
  {
    "text": "its unique key that's generated we use that to encrypt you to block individually there's then a cluster key",
    "start": "1246179",
    "end": "1251369"
  },
  {
    "text": "which encrypts all of those keys and yet another key which encrypts that and that's stored in encrypted fashion off",
    "start": "1251369",
    "end": "1257369"
  },
  {
    "text": "cluster so multiple layers on that front will continue to innovate on that side and you know that's an option that you",
    "start": "1257369",
    "end": "1265259"
  },
  {
    "text": "can enable at any point either at close to creation time or by modifying your cluster it does have a performance",
    "start": "1265259",
    "end": "1272519"
  },
  {
    "text": "impact we're in our testing we see about 10 to 15% and that's why it's not enabled by default it's really up to our",
    "start": "1272519",
    "end": "1278190"
  },
  {
    "text": "customers to make the decision of what they'd like to use each red shift",
    "start": "1278190",
    "end": "1283739"
  },
  {
    "text": "instance is provisioned solely for the customer that created it all of those compute nodes exist inside their own",
    "start": "1283739",
    "end": "1289319"
  },
  {
    "text": "private VPC our V PC is the Amazon virtual private cloud so it lets you",
    "start": "1289319",
    "end": "1294799"
  },
  {
    "text": "logically isolate a section of the AWS cloud and set up your own networking rules and access rules and when we",
    "start": "1294799",
    "end": "1301979"
  },
  {
    "text": "provision red stuff clusters we actually put the compute nodes in the leader node into their own private VPC so nothing",
    "start": "1301979",
    "end": "1307799"
  },
  {
    "text": "can access the compute nodes directly even data loading operations are pull operations so we actually pull data in",
    "start": "1307799",
    "end": "1314449"
  },
  {
    "text": "nothing's pushed onto them and you can also set up your own V PC and so run",
    "start": "1314449",
    "end": "1321119"
  },
  {
    "text": "redshift inside of that and really lock down the system as much as you'd like to and design it for your architecture and",
    "start": "1321119",
    "end": "1326609"
  },
  {
    "text": "infrastructure so from a the other part of operations is really taking care of",
    "start": "1326609",
    "end": "1333989"
  },
  {
    "text": "your data and we use local attached storage as I mentioned",
    "start": "1333989",
    "end": "1339010"
  },
  {
    "text": "for performance but on ec2 local storage is ephemeral and when you're dealing with multiple terabytes of data per node",
    "start": "1339010",
    "end": "1345820"
  },
  {
    "text": "you don't want ephemeral is not a good thing so we've spent a lot of time investing on durability to make sure",
    "start": "1345820",
    "end": "1352450"
  },
  {
    "text": "that you don't need to worry about the durability of your data we'll take care of that for you and that's generally a theme with all of our managed services",
    "start": "1352450",
    "end": "1358660"
  },
  {
    "text": "at AWS so anytime you load data into a cluster we will synchronously replicate",
    "start": "1358660",
    "end": "1364600"
  },
  {
    "text": "it onto other drives within that cluster and so if a write is successful multiple copies of that data exists within the",
    "start": "1364600",
    "end": "1370960"
  },
  {
    "text": "cluster what this allows us to do is to tolerate Drive failures transparently and you know if you think about it if",
    "start": "1370960",
    "end": "1377320"
  },
  {
    "text": "you're running a hundred node system with 24 drives per node that's 2,400 spindles going all at once and so just",
    "start": "1377320",
    "end": "1385299"
  },
  {
    "text": "given probabilities you're bound to see Drive failures at some point in time and we wanted the system to be able to",
    "start": "1385299",
    "end": "1390549"
  },
  {
    "text": "handle those gracefully so you have multiple copies and as in the cluster itself on different nodes to minimize",
    "start": "1390549",
    "end": "1396880"
  },
  {
    "text": "the blast radius we then backup to s3 continuously automatically incrementally an s3 is is",
    "start": "1396880",
    "end": "1404320"
  },
  {
    "text": "a highly durable system designed for 11 nines of durability and so with all of these multiple copies will make sure",
    "start": "1404320",
    "end": "1410049"
  },
  {
    "text": "that your data is available when you need it and we're continuously monitoring everything so every node is",
    "start": "1410049",
    "end": "1415419"
  },
  {
    "text": "continuously monitored all components of the system we will recover reboot replace nodes as needed to make sure",
    "start": "1415419",
    "end": "1422350"
  },
  {
    "text": "that your cluster is available and the streaming restore applies even in the",
    "start": "1422350",
    "end": "1427360"
  },
  {
    "text": "case of a single node so if a node needs to be replaced as soon as we can connect it back up you'll be able to resume",
    "start": "1427360",
    "end": "1433030"
  },
  {
    "text": "querying and go from there another another important point to mention here",
    "start": "1433030",
    "end": "1439090"
  },
  {
    "text": "is that we use s3 for both data loading and for snapshots s3 is a regional service so with AWS within a given",
    "start": "1439090",
    "end": "1446500"
  },
  {
    "text": "region we have what we call availability zones which are multiple isolated data centers with their own power and",
    "start": "1446500",
    "end": "1452530"
  },
  {
    "text": "networking cooling and with s3 you can restore your snapshots into any other",
    "start": "1452530",
    "end": "1458980"
  },
  {
    "text": "AEE our availability zone within a given region so that gives you options in case",
    "start": "1458980",
    "end": "1464140"
  },
  {
    "text": "there is an outage of some kind or an event you can bring up a new cluster using exactly the same data that you had",
    "start": "1464140",
    "end": "1470140"
  },
  {
    "text": "before in a different so we've really designed redshift to be",
    "start": "1470140",
    "end": "1476919"
  },
  {
    "text": "a central data warehouse to be used with as a central place to analyze really all",
    "start": "1476919",
    "end": "1482620"
  },
  {
    "text": "of the data that's generated within an organization and we see customers who use traditional OLTP stores they have",
    "start": "1482620",
    "end": "1489220"
  },
  {
    "text": "RDS maybe they're running their own databases either on ec2 or on-premise we",
    "start": "1489220",
    "end": "1495309"
  },
  {
    "text": "have customers running no sequel data stores on ec2 or using DynamoDB for real-time data capture we also see",
    "start": "1495309",
    "end": "1502179"
  },
  {
    "text": "MapReduce and EMR used in conjunction with redshift a lot it's great for",
    "start": "1502179",
    "end": "1507249"
  },
  {
    "text": "processing unstructured data turning it into columnar delimited data that can then be easily imported into redshift",
    "start": "1507249",
    "end": "1512499"
  },
  {
    "text": "and so because redshift exists as really as a hub for a lot of these data flows we've designed it to talk to multiple",
    "start": "1512499",
    "end": "1519100"
  },
  {
    "text": "data systems there's that single sequel integration with dynamodb and s3 and",
    "start": "1519100",
    "end": "1524820"
  },
  {
    "text": "through partners and tools and really data movement on s3 you can bring data in from any other system as well we",
    "start": "1524820",
    "end": "1533860"
  },
  {
    "text": "provide multiple options for loading data and so we have numerous partners",
    "start": "1533860",
    "end": "1539440"
  },
  {
    "text": "and ways to upload data in parallel to s3 f3 can really help you if you're",
    "start": "1539440",
    "end": "1545649"
  },
  {
    "text": "running multiple uploads saturate your outbound pipe so it'll take data as fast as you can throw at it we have AWS import/export which is when",
    "start": "1545649",
    "end": "1555610"
  },
  {
    "text": "you essentially ship us drives we'll load them onto s3 send them back to you and it's surprising how high bandwidth",
    "start": "1555610",
    "end": "1560919"
  },
  {
    "text": "FedEx can be if you're moving petabytes of data around and we have Direct",
    "start": "1560919",
    "end": "1566379"
  },
  {
    "text": "Connect which allows you to set up a dedicated one gig or ten gig II connection as many as you need between your hosting provider and and the AWS",
    "start": "1566379",
    "end": "1573820"
  },
  {
    "text": "cloud and that's a dedicated connection that you can provision by the hour we",
    "start": "1573820",
    "end": "1579279"
  },
  {
    "text": "also have a number of partners that provide data integration capability as well as systems integrators and boutique",
    "start": "1579279",
    "end": "1585460"
  },
  {
    "text": "consulting firms that can help you design an overall data movement and flow strategy from an analytic perspective we",
    "start": "1585460",
    "end": "1594850"
  },
  {
    "text": "have really designed redshift to work with whatever tools that you're using today and so we use JDBC and ODBC",
    "start": "1594850",
    "end": "1602529"
  },
  {
    "text": "interfaces Postgres drivers you can see if we have a range of partners here that have certified",
    "start": "1602529",
    "end": "1608950"
  },
  {
    "text": "redshift for use with their systems they've done their testing and validated and built integrations onto redshift and",
    "start": "1608950",
    "end": "1615779"
  },
  {
    "text": "we also have some partner updates for folks that are not on here that I'll talk about in just a second and a lot of",
    "start": "1615779",
    "end": "1623830"
  },
  {
    "text": "our partners are also available on the AWS marketplace so you've heard about this this is essentially an app store",
    "start": "1623830",
    "end": "1629289"
  },
  {
    "text": "for AWS software I makes it really easy to launch applications and have them run",
    "start": "1629289",
    "end": "1635380"
  },
  {
    "text": "on AWS and talk to your AWS resources one of our partners on here jaspersoft",
    "start": "1635380",
    "end": "1640480"
  },
  {
    "text": "has done a ton of work in making it easy for their system to actually auto discover your redshift and RDS resources",
    "start": "1640480",
    "end": "1646779"
  },
  {
    "text": "so really simplifying even more the integration steps there and also a",
    "start": "1646779",
    "end": "1653380"
  },
  {
    "text": "couple of other updates from partners burst is available on the marketplace as well and one of our newest partners size",
    "start": "1653380",
    "end": "1659770"
  },
  {
    "text": "sense has done a ton of work integrating redshift into their overall system we actually got permission to share that",
    "start": "1659770",
    "end": "1665669"
  },
  {
    "text": "Merc is using that platform which provides real easy way to do high-performance analytics on pretty",
    "start": "1665669",
    "end": "1672429"
  },
  {
    "text": "simple systems they have an elastic cube technology which is pretty interesting and I'll have some you can talk to Bruno who's here in the",
    "start": "1672429",
    "end": "1679720"
  },
  {
    "text": "back if you're curious about that and now I'll turn it over to Henry and Flo",
    "start": "1679720",
    "end": "1685210"
  },
  {
    "text": "from Airbnb thanks guys",
    "start": "1685210",
    "end": "1688799"
  },
  {
    "text": "good afternoon everyone and the we're glad to share a little bit story about",
    "start": "1698710",
    "end": "1704590"
  },
  {
    "text": "how we end up choosing red ship at Airbnb so let's take a look at our data",
    "start": "1704590",
    "end": "1712120"
  },
  {
    "text": "infrastructure at Airbnb like any internet skill company we have multiple",
    "start": "1712120",
    "end": "1719320"
  },
  {
    "text": "data sources you know we have data from key value store you know dynamodb search",
    "start": "1719320",
    "end": "1726760"
  },
  {
    "text": "and also we do picture checking to make sure you know our page are loading fast",
    "start": "1726760",
    "end": "1732130"
  },
  {
    "text": "enough and also we have our own logging services or the web request will end up",
    "start": "1732130",
    "end": "1738700"
  },
  {
    "text": "like sending requests to our own logging services all that thing we are storing",
    "start": "1738700",
    "end": "1744280"
  },
  {
    "text": "them in Amazon s3 right now and traditionally we use Hadoop family",
    "start": "1744280",
    "end": "1751320"
  },
  {
    "text": "including big hive cast log and cascading to process them and then dump",
    "start": "1751320",
    "end": "1759910"
  },
  {
    "text": "them into a relational database for a team of analysts to answer deeper",
    "start": "1759910",
    "end": "1766240"
  },
  {
    "text": "questions and so we like big data tools like Hadoop it's really great at",
    "start": "1766240",
    "end": "1772150"
  },
  {
    "text": "crunching terabytes and petabytes but they're not very interactive and our",
    "start": "1772150",
    "end": "1777790"
  },
  {
    "text": "solution as I just showing guests like we used to do to do nightly patches to",
    "start": "1777790",
    "end": "1783550"
  },
  {
    "text": "process and then we dump them into like a RTS for interactive queries and there",
    "start": "1783550",
    "end": "1790420"
  },
  {
    "text": "are two problems a the size of our data",
    "start": "1790420",
    "end": "1795430"
  },
  {
    "text": "is keep growing and B our team is growing as well so the above solution",
    "start": "1795430",
    "end": "1802030"
  },
  {
    "text": "will only work if if those two doesn't exist so as we grow we need to look at",
    "start": "1802030",
    "end": "1809710"
  },
  {
    "text": "other options to solve this issue and we look at a few alternatives out there and",
    "start": "1809710",
    "end": "1816600"
  },
  {
    "text": "but am I often I think the initial evasion result from redshift was the",
    "start": "1816600",
    "end": "1822730"
  },
  {
    "text": "most promising and the most of the word adapt red ship is",
    "start": "1822730",
    "end": "1828220"
  },
  {
    "text": "among schema migration and also design your data processing ETL and also make",
    "start": "1828220",
    "end": "1837190"
  },
  {
    "text": "sure your current data will play really really well ways within the limitation",
    "start": "1837190",
    "end": "1842860"
  },
  {
    "text": "of redshift and we don't believe we don't use it as a replacement for how to",
    "start": "1842860",
    "end": "1848860"
  },
  {
    "text": "do but rather it's a complementary to of 200 and so we're using to mostly replace",
    "start": "1848860",
    "end": "1858549"
  },
  {
    "text": "hive queries we have more than a cup of",
    "start": "1858549",
    "end": "1863679"
  },
  {
    "text": "terabytes of data generated every day and we have like a dozen of analysts",
    "start": "1863679",
    "end": "1869620"
  },
  {
    "text": "they for this limitations I mentioned before they have constantly go back to",
    "start": "1869620",
    "end": "1876309"
  },
  {
    "text": "to hive and which is really really slow and you know slow down their productivity a lot and so we did",
    "start": "1876309",
    "end": "1884710"
  },
  {
    "text": "migration to rest ship at beginning of this year we have lots of JSON data like",
    "start": "1884710",
    "end": "1891909"
  },
  {
    "text": "many other web companies out there so the first step is you need to figure out a way to transform those into CSV or CSV",
    "start": "1891909",
    "end": "1900460"
  },
  {
    "text": "format and also the other thing you need to deal with is right now there's only",
    "start": "1900460",
    "end": "1906610"
  },
  {
    "text": "11 primitive types supported by this system so you probably need to figure",
    "start": "1906610",
    "end": "1914980"
  },
  {
    "text": "out a way to work around the limitation and you know load your data into redshift and as Rahul mentioned before",
    "start": "1914980",
    "end": "1923140"
  },
  {
    "text": "the distribution key and sort key is very very important when you design your schema because that will affect how your",
    "start": "1923140",
    "end": "1930520"
  },
  {
    "text": "data is distributed and when you Ryan join and the performance will be improve",
    "start": "1930520",
    "end": "1936159"
  },
  {
    "text": "a lot if you choose the right ones so a few limitations here is you know like",
    "start": "1936159",
    "end": "1943900"
  },
  {
    "text": "stream stream split it's not supported also a percentile or rejects are not",
    "start": "1943900",
    "end": "1950320"
  },
  {
    "text": "supported as of now and the other thing we found is like it would be nice if",
    "start": "1950320",
    "end": "1956490"
  },
  {
    "text": "materialized views will be supported because we tend to around big joints",
    "start": "1956490",
    "end": "1962270"
  },
  {
    "text": "again again so how we're doing this we",
    "start": "1962270",
    "end": "1968660"
  },
  {
    "text": "load the data all the data is the s3 already and also or dynamodb so it's",
    "start": "1968660",
    "end": "1973880"
  },
  {
    "text": "really really easy it's basically just one command the only thing we did there is to figure out each one of the data",
    "start": "1973880",
    "end": "1982640"
  },
  {
    "text": "sets what are the curries to fix and also what are the you know now values",
    "start": "1982640",
    "end": "1989360"
  },
  {
    "text": "and specialties around the table so you need to pay attention around Apple a little bit and then we did a little bit",
    "start": "1989360",
    "end": "1996830"
  },
  {
    "text": "parallel loading originally we're just using one process to load it and we were loading more than like one terabyte data",
    "start": "1996830",
    "end": "2003580"
  },
  {
    "text": "it was taking a few hours so we found like parallel loading helps a lot and the other thing we run into constantly",
    "start": "2003580",
    "end": "2011590"
  },
  {
    "text": "was the UTF support and the thing is",
    "start": "2011590",
    "end": "2017200"
  },
  {
    "text": "like it supports up to three characters lengths of UTF characters and for",
    "start": "2017200",
    "end": "2023500"
  },
  {
    "text": "whatever reason the data was noisy so we had to do a little bit pre-processing there to strip them out",
    "start": "2023500",
    "end": "2030100"
  },
  {
    "text": "and the other thing you need to pay attention to is that now very handling right now it can only actually specify",
    "start": "2030100",
    "end": "2037660"
  },
  {
    "text": "one type of null value out there so as",
    "start": "2037660",
    "end": "2043300"
  },
  {
    "text": "you can see the other stab will be how to wire wrap chip into our existing",
    "start": "2043300",
    "end": "2051010"
  },
  {
    "text": "Ethier pipeline and it's a problem has been boggling us for a long time and Flo",
    "start": "2051010",
    "end": "2060300"
  },
  {
    "text": "who is our team lead build a tool to help us to wire everything together you",
    "start": "2060300",
    "end": "2066310"
  },
  {
    "text": "might have a bunch of Hadoop jobs you might have some scripts to pre-processing them and then you know we",
    "start": "2066310",
    "end": "2073179"
  },
  {
    "text": "won't glue everything together to make a whole pipeline so without further ado let me introduce Flo to talk about the",
    "start": "2073180",
    "end": "2080530"
  },
  {
    "text": "tool call Chronos thanks Henry hi so how many of you guys have actually",
    "start": "2080530",
    "end": "2086139"
  },
  {
    "text": "worked on an ETL pipeline before a fair number right so",
    "start": "2086140",
    "end": "2091810"
  },
  {
    "text": "how many of you guys have used Chronos whereas sorry kron4 that in the past a",
    "start": "2091810",
    "end": "2097020"
  },
  {
    "text": "couple right so that's that's how the state of the world was when I joined Airbnb we used cron jobs and about",
    "start": "2097020",
    "end": "2104260"
  },
  {
    "text": "10,000 lines of bash in order to tie a number of jobs together so Airbnb as a",
    "start": "2104260",
    "end": "2110140"
  },
  {
    "text": "as a variety of data sources we we read data from databases such as my sequel we",
    "start": "2110140",
    "end": "2116350"
  },
  {
    "text": "read from what other databases do we have we read some from post press some from some from some flat files we get",
    "start": "2116350",
    "end": "2123310"
  },
  {
    "text": "data out of Redis and as scenaries mentioned we use Hadoop in order to do",
    "start": "2123310",
    "end": "2128920"
  },
  {
    "text": "some of the ETL work and join some of these datasets together and then",
    "start": "2128920",
    "end": "2133950"
  },
  {
    "text": "repopulate some my sequel databases so that was kind of the state of the art a",
    "start": "2133950",
    "end": "2140740"
  },
  {
    "text": "state of the world when I joined Airbnb and this is an example of a job flow at Airbnb this computes the hosting",
    "start": "2140740",
    "end": "2148480"
  },
  {
    "text": "viability like basically how good is a hosting and uses various data sources various intermittent steps there are a",
    "start": "2148480",
    "end": "2155320"
  },
  {
    "text": "number of high if jobs a number of pick jobs there are just some set and arc transformations and the the end result",
    "start": "2155320",
    "end": "2162460"
  },
  {
    "text": "is basically data loaded into my sequel well now it's actually been replaced data is now loaded into redshift but in",
    "start": "2162460",
    "end": "2169840"
  },
  {
    "text": "order to tie all of these things together we wrote a tool called Chronos and Chronos is open source you can download it from github if you go to",
    "start": "2169840",
    "end": "2175590"
  },
  {
    "text": "github.com slash Airbnb left Kronos you'll find it and the reason for for",
    "start": "2175590",
    "end": "2181360"
  },
  {
    "text": "writing Kronos was that when you're in the cloud you experience a lot more variants in your network requests for",
    "start": "2181360",
    "end": "2187540"
  },
  {
    "text": "example in latency you also see oftentimes api's that don't work the first time so if you imagine a",
    "start": "2187540",
    "end": "2193570"
  },
  {
    "text": "multi-step pipeline like this one you're oftentimes run into the issue that one of the jobs breaks and you're stuck your",
    "start": "2193570",
    "end": "2201520"
  },
  {
    "text": "jobs don't progress the next day the analysts come to work and none of them can do actually the the work and so then",
    "start": "2201520",
    "end": "2208390"
  },
  {
    "text": "you spent or I spend a ton of time debugging that so I think in up until November I would come to work on Monday",
    "start": "2208390",
    "end": "2214510"
  },
  {
    "text": "Tuesday Wednesday and spend the first two to four hours debugging this pipeline and we have many more pipelines",
    "start": "2214510",
    "end": "2220990"
  },
  {
    "text": "like it and basically that in that involved going into",
    "start": "2220990",
    "end": "2226150"
  },
  {
    "text": "the log files trying to figure out which step actually broke and then rerunning those steps manually so and that was the",
    "start": "2226150",
    "end": "2233680"
  },
  {
    "text": "motivation to build this to a Cronus and what Kronos basically does is it's a tool that's written on top of a cluster",
    "start": "2233680",
    "end": "2241860"
  },
  {
    "text": "kernel and the cluster kernel is called mesos and Kronos looks Cronus looks like",
    "start": "2241860",
    "end": "2251470"
  },
  {
    "text": "this this is the UI for Cronus so what it lets you do is it lets you define that lets you define job job",
    "start": "2251470",
    "end": "2258160"
  },
  {
    "text": "dependencies and you can create jobs that are time-based or jobs that are dependencies of other jobs and you can",
    "start": "2258160",
    "end": "2265780"
  },
  {
    "text": "basically add repeats so it can repeat daily it can repeat hourly just like you are you used to it from crown but it",
    "start": "2265780",
    "end": "2272740"
  },
  {
    "text": "lets you do much more than that Kronos is a fault tolerant highly available system it has it has many many",
    "start": "2272740",
    "end": "2279160"
  },
  {
    "text": "masters and many workers and it's written in Scala and as I said it's",
    "start": "2279160",
    "end": "2284860"
  },
  {
    "text": "written on top of this cluster cluster kernel which is called mesas really really interesting project and what what",
    "start": "2284860",
    "end": "2292450"
  },
  {
    "text": "that allowed us to do running writing in on the top of mesas is we didn't have to write a single line of network code and",
    "start": "2292450",
    "end": "2299290"
  },
  {
    "text": "that's really that's really big I think because as I said it runs on hundreds of nodes I mean which we tested it on ec2",
    "start": "2299290",
    "end": "2306220"
  },
  {
    "text": "and hundreds of nodes and it works very well there but yet no single line of",
    "start": "2306220",
    "end": "2311350"
  },
  {
    "text": "network code so there's no socket open in Cronus the mesas abstraction is",
    "start": "2311350",
    "end": "2318100"
  },
  {
    "text": "resource offer and the workers all give you resource offers or they give Kronos",
    "start": "2318100",
    "end": "2323290"
  },
  {
    "text": "a resource offer and then Cronus can say hey I have a job to run and it runs it on one of the workers we also build retries in to Cronus so you can actually",
    "start": "2323290",
    "end": "2330370"
  },
  {
    "text": "say you see that here on the on the on the right hand side you see with these",
    "start": "2330370",
    "end": "2335530"
  },
  {
    "text": "two sample jobs that there are two retries so if one of the api is is down it will be retried also if one of the",
    "start": "2335530",
    "end": "2341980"
  },
  {
    "text": "slaves goes away another and you start another one it will automatically failover if you have two slaves running",
    "start": "2341980",
    "end": "2348700"
  },
  {
    "text": "and one of them goes away the other one will take over automatically so that's all handled by Kronos and this is an",
    "start": "2348700",
    "end": "2356830"
  },
  {
    "text": "example for a job that has dependencies like the job that you saw earlier it's also in there but we didn't depict it because",
    "start": "2356830",
    "end": "2362920"
  },
  {
    "text": "it looks a little messy but the nice thing about this for example is that you see right away whether or not a job",
    "start": "2362920",
    "end": "2368890"
  },
  {
    "text": "succeeded or it failed because it it will be either red or it will be green",
    "start": "2368890",
    "end": "2374080"
  },
  {
    "text": "if it succeeded and so when we when we push Kronos into production I basically",
    "start": "2374080",
    "end": "2380260"
  },
  {
    "text": "had no longer any work to do so it came to work and was like wow I can actually start writing code again so that was",
    "start": "2380260",
    "end": "2385660"
  },
  {
    "text": "really good for us yeah if you guys are interested just check it out on github and more than happy to answer any",
    "start": "2385660",
    "end": "2391840"
  },
  {
    "text": "questions",
    "start": "2391840",
    "end": "2393870"
  },
  {
    "text": "happy to take questions for really any of us did you want to talk about your performance results here's the initial",
    "start": "2404460",
    "end": "2419650"
  },
  {
    "text": "result we got when we first evaluate rest shift so we were comparing recive",
    "start": "2419650",
    "end": "2426670"
  },
  {
    "text": "cluster with 16 based basic notes to a",
    "start": "2426670",
    "end": "2431860"
  },
  {
    "text": "EMR hive cluster weighs 44 notes and the cost is listed there and as you can see",
    "start": "2431860",
    "end": "2440910"
  },
  {
    "text": "the EMR cluster actually has a better end up type than the Amazon redshift",
    "start": "2440910",
    "end": "2448090"
  },
  {
    "text": "here sorry yes yeah and then so we try",
    "start": "2448090",
    "end": "2456400"
  },
  {
    "text": "it with the simple query you know just do a group by over a little bit more",
    "start": "2456400",
    "end": "2462010"
  },
  {
    "text": "than one terabyte of data and here's a result so high up to 28 minutes but you",
    "start": "2462010",
    "end": "2469030"
  },
  {
    "text": "know right ship lasted six minute and this is really really important to us",
    "start": "2469030",
    "end": "2474430"
  },
  {
    "text": "because when the analysts write their queries oftentimes it takes few",
    "start": "2474430",
    "end": "2479650"
  },
  {
    "text": "iterations for them to get the right version and as you can imagine if you know it took analyst three iteration",
    "start": "2479650",
    "end": "2486160"
  },
  {
    "text": "together query right it's gonna take more than an hour resisting redshift is like half less than half an hour and",
    "start": "2486160",
    "end": "2491950"
  },
  {
    "text": "then we tried another complex curry which is a",
    "start": "2491950",
    "end": "2497290"
  },
  {
    "text": "triple join and you know with some group by right there and the data set is",
    "start": "2497290",
    "end": "2505060"
  },
  {
    "text": "a much small here it's probably a half terabyte ish and it worked really really",
    "start": "2505060",
    "end": "2511780"
  },
  {
    "text": "well it's almost like 20 times faster in this case",
    "start": "2511780",
    "end": "2517470"
  },
  {
    "text": "so our conclusion for now is compared to the traditional data basis we use its",
    "start": "2517470",
    "end": "2525910"
  },
  {
    "text": "really really responsive and can handle range queries and aggregations against",
    "start": "2525910",
    "end": "2532240"
  },
  {
    "text": "really real large data set very very well and it's very very easy to adopt if",
    "start": "2532240",
    "end": "2537640"
  },
  {
    "text": "you you have people who has like some sequel background and it's very cost",
    "start": "2537640",
    "end": "2542770"
  },
  {
    "text": "effective as Rahul mentioned before but at the same time you probably need to",
    "start": "2542770",
    "end": "2548260"
  },
  {
    "text": "work with limitations as of now but if you can figure out a way to walk around",
    "start": "2548260",
    "end": "2554560"
  },
  {
    "text": "the issue the benefit is huge and that's all thank you guys thanks a lot guys",
    "start": "2554560",
    "end": "2567070"
  },
  {
    "text": "happy to take questions if we have them go ahead so red shift red shift is a",
    "start": "2567070",
    "end": "2584920"
  },
  {
    "text": "data warehousing engine so it's a system it's the it's a columnar sequel base",
    "start": "2584920",
    "end": "2590290"
  },
  {
    "text": "data warehouse you know if you're running analytics you can use a bi layer on top of it or we have customers that",
    "start": "2590290",
    "end": "2596590"
  },
  {
    "text": "use our or other packages but really it's designed to return queries on large",
    "start": "2596590",
    "end": "2602440"
  },
  {
    "text": "data sets as quickly as possible so the",
    "start": "2602440",
    "end": "2609250"
  },
  {
    "text": "question is who's the competitor I mean really at AWS we're focused on our customers and letting customers choose",
    "start": "2609250",
    "end": "2615400"
  },
  {
    "text": "the tool that's best for them so you know there are many relational data warehouses out there they're all great",
    "start": "2615400",
    "end": "2620590"
  },
  {
    "text": "systems and customers will choose the ones that matter the most I think what we focused on here is performance price",
    "start": "2620590",
    "end": "2626860"
  },
  {
    "text": "and ease of use go ahead I actually don't",
    "start": "2626860",
    "end": "2638740"
  },
  {
    "text": "know the specific answer to that so I'd have to check our documentation I mean I think we support a fairly complete set of sequel and windowing functions I",
    "start": "2638740",
    "end": "2646150"
  },
  {
    "text": "think as Henry mentioned some things like regex and text functions aren't on where they need to be but you know that",
    "start": "2646150",
    "end": "2652630"
  },
  {
    "text": "sort of feedback always helps us get better but if we have it it'll be documented and if not and we need it",
    "start": "2652630",
    "end": "2657640"
  },
  {
    "text": "then I'd like to hear about it so we can figure out how to improve",
    "start": "2657640",
    "end": "2662670"
  },
  {
    "text": "as so when we say usage-based pricing in the redshift context it's to do with the",
    "start": "2674120",
    "end": "2679530"
  },
  {
    "text": "number of nodes that you have active an active means dedicated to you so if you run a four node cluster from our",
    "start": "2679530",
    "end": "2686190"
  },
  {
    "text": "perspective you're using four nodes and the size of those nodes will determine your hourly charges it doesn't matter",
    "start": "2686190",
    "end": "2693450"
  },
  {
    "text": "how much you put on it or how much data you use or the load or queries you run it's really just hourly cost per node",
    "start": "2693450",
    "end": "2699510"
  },
  {
    "text": "size per node and number of nodes and if you have questions for the Airbnb team",
    "start": "2699510",
    "end": "2705720"
  },
  {
    "text": "feel free as well they're happy to take them too sir in the blue jacket so the",
    "start": "2705720",
    "end": "2719850"
  },
  {
    "text": "question is is there any plans for supporting MDX so we don't so we don't come in on our roadmap specifically but",
    "start": "2719850",
    "end": "2725730"
  },
  {
    "text": "we are always receptive to customer feedback and that's how we make sure that we evolve in the right direction so",
    "start": "2725730",
    "end": "2731730"
  },
  {
    "text": "if that's a use case that can't be worked around other ways that's good to know and right behind yes so in terms of",
    "start": "2731730",
    "end": "2746370"
  },
  {
    "text": "looking at the evolution of the service both we have a pretty active forum and where we'll post our announcements as",
    "start": "2746370",
    "end": "2751560"
  },
  {
    "text": "well as the AWS blog where any major feature releases will be covered oh",
    "start": "2751560",
    "end": "2758030"
  },
  {
    "text": "sorry thank you go ahead",
    "start": "2760940",
    "end": "2764750"
  },
  {
    "text": "we didn't bet we didn't benchmarking our our high shops against raw mapreduce I",
    "start": "2785609",
    "end": "2791380"
  },
  {
    "text": "mean that's the the that has been done other people have done that and for most",
    "start": "2791380",
    "end": "2796450"
  },
  {
    "text": "queries that should be fairly natural negligible so yeah it wasn't really what",
    "start": "2796450",
    "end": "2804250"
  },
  {
    "text": "I'm looking to replace you know a traditional have their jobs more of like",
    "start": "2804250",
    "end": "2809710"
  },
  {
    "text": "a interactive query so that's why with benchmark against hive",
    "start": "2809710",
    "end": "2814799"
  },
  {
    "text": "so we you know we let our customers make those kinds of decisions and really",
    "start": "2824290",
    "end": "2829670"
  },
  {
    "text": "because benchmarking and databases is so dependent on query and setup and dataset",
    "start": "2829670",
    "end": "2834740"
  },
  {
    "text": "size you know from a technology perspective Vertica as a columnar NPP scale out system and so is redshift",
    "start": "2834740",
    "end": "2841160"
  },
  {
    "text": "so that's something that customers do and we allow them to make the best choices the hardware platform for",
    "start": "2841160",
    "end": "2846470"
  },
  {
    "text": "redshift is available on ec2 so that's something that a customer would do so",
    "start": "2846470",
    "end": "2890810"
  },
  {
    "text": "you know I think as I mentioned in the prior question Vertica is also relational columnar systems so I would",
    "start": "2890810",
    "end": "2896720"
  },
  {
    "text": "expect workload characteristics to be similar so it's really up to you from a",
    "start": "2896720",
    "end": "2902000"
  },
  {
    "text": "perspective of you know the price performance and manageability dimensions on which you want to run and so from a",
    "start": "2902000",
    "end": "2908750"
  },
  {
    "text": "if you're running data warehouses on ec2 you'll be managing your own durability you'll be managing your own hardware",
    "start": "2908750",
    "end": "2914570"
  },
  {
    "text": "setup your own node configurations on redshift will manage that for you so it really comes down to fit an appetite and",
    "start": "2914570",
    "end": "2920660"
  },
  {
    "text": "where you want to spend your time and money",
    "start": "2920660",
    "end": "2923920"
  },
  {
    "text": "yep so the question is how does redshift deal with data distribution across multiple nodes when you resize and so",
    "start": "2951720",
    "end": "2959089"
  },
  {
    "text": "redshift is an MPP system so we actually do physically redistribute your data when you add more nodes to the system",
    "start": "2959089",
    "end": "2965130"
  },
  {
    "text": "and that's what's happening in the background when we're parallel copying data to the new cluster so we'll you",
    "start": "2965130",
    "end": "2971250"
  },
  {
    "text": "know pay attention to the table schemas and then distribute data as appropriate so you'll end up with less data on each",
    "start": "2971250",
    "end": "2977130"
  },
  {
    "text": "node in your larger cluster you can also resize down if that makes sense for you at the time but essentially we'll pick",
    "start": "2977130",
    "end": "2983130"
  },
  {
    "text": "the optimal distribution based on your schema for the number of nodes that you've selected in your target go ahead",
    "start": "2983130",
    "end": "2990210"
  },
  {
    "text": "sir so we manage all of that for you so",
    "start": "2990210",
    "end": "2997910"
  },
  {
    "text": "there's no direct way for a customer to access that I'm sorry I didn't hear that",
    "start": "2997910",
    "end": "3010329"
  },
  {
    "text": "so the question is where where are the BI tools hosted so really that's that's",
    "start": "3010329",
    "end": "3015530"
  },
  {
    "text": "a function of your infrastructure and the setup you want we have customers that run bi on Prem and connect over the",
    "start": "3015530",
    "end": "3022069"
  },
  {
    "text": "Internet to redshift we also have customers that run their bi tools within the AWS cloud it's really just choice",
    "start": "3022069",
    "end": "3030819"
  },
  {
    "text": "yes so it's ODBC JDBC Postgres drivers you",
    "start": "3034329",
    "end": "3039410"
  },
  {
    "text": "know we have our the partners that I listed have done work to centrally certify and customize where appropriate",
    "start": "3039410",
    "end": "3045260"
  },
  {
    "text": "so that they're generating optimal sequel for the characteristics of the redshift system but really anything you use that speaks JDBC ODBC can talk about",
    "start": "3045260",
    "end": "3052640"
  },
  {
    "text": "maybe we should we'll talk after because I think I want to let a few other people just behind",
    "start": "3052640",
    "end": "3058190"
  },
  {
    "text": "you and then I'll come back",
    "start": "3058190",
    "end": "3061060"
  },
  {
    "text": "so for redshift we try to keep it very simple so it's really the cost is driven",
    "start": "3073760",
    "end": "3078770"
  },
  {
    "text": "by the night the node size and the number of nodes we don't charge for data transfer into redshift or out of",
    "start": "3078770",
    "end": "3086450"
  },
  {
    "text": "redshift if you're loading data into s3 any s3 related costs for your initial",
    "start": "3086450",
    "end": "3092450"
  },
  {
    "text": "load will be charged by s3 we don't charge for backup storage up to the amount of active data warehouse storage",
    "start": "3092450",
    "end": "3099290"
  },
  {
    "text": "so that means that if you have a four terabyte cluster you'll get four terabytes of free backup storage",
    "start": "3099290",
    "end": "3104319"
  },
  {
    "text": "associated with that running redshift cluster so we've try to keep it as simple as possible there are some AWS",
    "start": "3104319",
    "end": "3110420"
  },
  {
    "text": "wide charges that apply and things like V PC and those are all spelled out on our pricing pages go ahead sir",
    "start": "3110420",
    "end": "3120099"
  },
  {
    "text": "so the question is how to deal with essentially time-series tables and data",
    "start": "3131650",
    "end": "3136880"
  },
  {
    "text": "aging so we actually recommend that for customers that have a defined retention window to actually chunk their data for",
    "start": "3136880",
    "end": "3142640"
  },
  {
    "text": "example keep a table per month and then build a view on top of that that scans the entire working set and that way you",
    "start": "3142640",
    "end": "3148550"
  },
  {
    "text": "can we have an unload capability which is the opposite of copy you can essentially drop a table onto s3 and",
    "start": "3148550",
    "end": "3154430"
  },
  {
    "text": "archive that and then load in new data as you go so that capability is built-in it does take you know it's not a setting",
    "start": "3154430",
    "end": "3160340"
  },
  {
    "text": "that you define the snapshots aged out automatically but anything within the data model you would control",
    "start": "3160340",
    "end": "3167560"
  },
  {
    "text": "in case you didn't hear the question was how long the transition time was and who was it a few weeks he's three weeks",
    "start": "3187940",
    "end": "3192950"
  },
  {
    "text": "three weeks I think we have time for probably one more good",
    "start": "3192950",
    "end": "3200470"
  },
  {
    "text": "so the question is do we still use - Hadoop now that we run redshift the",
    "start": "3214450",
    "end": "3219980"
  },
  {
    "text": "answer is yes we still use hi if we don't use it use it as much for interactive queries anymore we still",
    "start": "3219980",
    "end": "3225800"
  },
  {
    "text": "have some ETL like it's not really ETL but it's like some processing jobs that",
    "start": "3225800",
    "end": "3230990"
  },
  {
    "text": "run on hive and that data itself is then loaded into redshift I think there was",
    "start": "3230990",
    "end": "3238160"
  },
  {
    "text": "another question over there the question",
    "start": "3238160",
    "end": "3245300"
  },
  {
    "text": "is is chrome running on AWS yes it is yes",
    "start": "3245300",
    "end": "3250270"
  },
  {
    "text": "so the question is it's an example for taking your own application and embedding it with this with the system",
    "start": "3258310",
    "end": "3264680"
  },
  {
    "text": "yes so Kronos is a batch scheduler so it can schedule any jobs it schedules red shift",
    "start": "3264680",
    "end": "3270290"
  },
  {
    "text": "jobs wire the red shift API or command-line tools alright everybody",
    "start": "3270290",
    "end": "3275870"
  },
  {
    "text": "thank you very much my email is up there happy to answer any questions one on one please say your time",
    "start": "3275870",
    "end": "3283240"
  }
]