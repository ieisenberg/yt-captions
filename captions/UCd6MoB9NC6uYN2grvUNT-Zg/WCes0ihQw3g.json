[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "hello and welcome to today's webinar streaming data analytics with Amazon Kinesis fire hose and red shift on your",
    "start": "3879",
    "end": "11880"
  },
  {
    "text": "screen you will see a Q&A pod please submit your questions for our presenter",
    "start": "11880",
    "end": "17560"
  },
  {
    "text": "and moderators at any time and we will address them as they come",
    "start": "17560",
    "end": "22599"
  },
  {
    "text": "in at the conclusion of today's presentation there will be several polling questions preceding the live Q&A",
    "start": "22599",
    "end": "30599"
  },
  {
    "text": "a please provide us your feedback when you see the polling questions appear on",
    "start": "30599",
    "end": "35760"
  },
  {
    "text": "your screen today's webinar is being recorded in the next few days we will send you a",
    "start": "35760",
    "end": "42600"
  },
  {
    "text": "follow-up email with the on demand links to the presentation deck and the webinar",
    "start": "42600",
    "end": "50520"
  },
  {
    "text": "recording and again welcome to today's webinar streaming data analytics with Amazon kesa SP hose and red",
    "start": "50520",
    "end": "58160"
  },
  {
    "text": "shift our presenter today is Ray Zoo Ry is a senior product manager for Amazon",
    "start": "58160",
    "end": "65239"
  },
  {
    "text": "Kinesis prior to joining AWS Ray had five years of experience in",
    "start": "65240",
    "end": "71119"
  },
  {
    "text": "the business intelligence software and telecommunication",
    "start": "71119",
    "end": "76200"
  },
  {
    "text": "Industries Ray holds a bachelor's degree in electronic and information",
    "start": "76200",
    "end": "82439"
  },
  {
    "text": "engineering and a master's degree in Business Administration also with us we have Alan",
    "start": "82439",
    "end": "90240"
  },
  {
    "text": "mcginness and Tony Gibbs they will be your moderators who will be engaging",
    "start": "90240",
    "end": "95880"
  },
  {
    "text": "with you to answer your questions throughout the presentation Ray welcome the floor is",
    "start": "95880",
    "end": "101560"
  },
  {
    "text": "now yours thank you Heather and good morning everyone and um welcome to today's",
    "start": "101560",
    "end": "108119"
  },
  {
    "text": "webinar session um the topic for today is streaming data analytics with Amazon",
    "start": "108119",
    "end": "114000"
  },
  {
    "text": "Kinesis fire hose and R shift so in today's session I'm going to walk you",
    "start": "114000",
    "end": "120479"
  },
  {
    "text": "through an example of using pesa's file host and red shift to analyze Maryland's",
    "start": "120479",
    "end": "127960"
  },
  {
    "text": "traffic violation data in real time and the purpose of this session is for you",
    "start": "127960",
    "end": "133879"
  },
  {
    "text": "to get familiar with the setup and the data flow for using kesa fire hose and rare shift for realtime streaming data",
    "start": "133879",
    "end": "141560"
  },
  {
    "text": "analytics and as Heather mentioned at this session you'll have access to to",
    "start": "141560",
    "end": "146879"
  },
  {
    "text": "the recording of this session and also the deex and I think you can use the contents within the deck as a stepbystep",
    "start": "146879",
    "end": "154760"
  },
  {
    "text": "guideline for you to start trying out these two services with your own data and also the sample data we'll use in",
    "start": "154760",
    "end": "161440"
  },
  {
    "text": "this session so to start this session we'll",
    "start": "161440",
    "end": "169239"
  },
  {
    "start": "167000",
    "end": "167000"
  },
  {
    "text": "very quickly spend probably 30 to 60 seconds to give a quick introduction to",
    "start": "169239",
    "end": "174400"
  },
  {
    "text": "the Kinesis fire host service and the r shift service and then the main part of",
    "start": "174400",
    "end": "179560"
  },
  {
    "text": "this session we're going to walk through the fstep setup um for streaming data",
    "start": "179560",
    "end": "185319"
  },
  {
    "text": "pipeline from kesa fire host to rare shift and then using a visualization",
    "start": "185319",
    "end": "190560"
  },
  {
    "text": "tool to run a query and visualize the data from rare shift and again in this",
    "start": "190560",
    "end": "196640"
  },
  {
    "text": "session the sample data I'm using it's the Maryland traffic violation data and",
    "start": "196640",
    "end": "202000"
  },
  {
    "text": "the data set is publicly accessible so that after this session you can try it out yourself and probably do more",
    "start": "202000",
    "end": "208920"
  },
  {
    "text": "interesting videos of uh on the rare shift",
    "start": "208920",
    "end": "213519"
  },
  {
    "start": "215000",
    "end": "215000"
  },
  {
    "text": "query so Kinesis fire hose is one of the streaming data services at the Amazon",
    "start": "215680",
    "end": "221760"
  },
  {
    "text": "Kinesis platform Kinesis firose automatically loads streaming data into",
    "start": "221760",
    "end": "228120"
  },
  {
    "text": "Amazon S3 rare shift and elastic search Services you do not need to build any",
    "start": "228120",
    "end": "234480"
  },
  {
    "text": "application or manage any infrastructure uh for using fire hose",
    "start": "234480",
    "end": "240200"
  },
  {
    "text": "all you need to do is to spend probably 60 seconds to set up a fire host every stream through the console which will be",
    "start": "240200",
    "end": "247159"
  },
  {
    "text": "a streaming data pipeline pointing to your destination resource and afterwards",
    "start": "247159",
    "end": "252760"
  },
  {
    "text": "you configure data producers to continuously push data into your fire host delivery stream then everything",
    "start": "252760",
    "end": "259720"
  },
  {
    "text": "else will be managed behind the scene by the fir host service and for today's",
    "start": "259720",
    "end": "264759"
  },
  {
    "text": "session we're going to focus on fire host to rare shift",
    "start": "264759",
    "end": "270919"
  },
  {
    "text": "Amazon rare shift it's AWS paby scale data warehouse where you can run",
    "start": "272440",
    "end": "277880"
  },
  {
    "text": "standard SQL against for complex data an",
    "start": "277880",
    "end": "282680"
  },
  {
    "text": "analysis so before we dive into the five steps for setting up this streaming data",
    "start": "285360",
    "end": "290639"
  },
  {
    "text": "Pipeline and analytical process we'll give a quick overview on the data flow",
    "start": "290639",
    "end": "295840"
  },
  {
    "text": "when we use fire host to stream data into rare shift",
    "start": "295840",
    "end": "301360"
  },
  {
    "text": "so on the very left hand side you'll configure data producers and the concept",
    "start": "301360",
    "end": "307199"
  },
  {
    "text": "of data producer are the entities that are generating data where you want to capture and analyze so a data producer",
    "start": "307199",
    "end": "315000"
  },
  {
    "text": "could be a web application server could be ECT instance could be mobile devices",
    "start": "315000",
    "end": "321479"
  },
  {
    "text": "or could be iot devices and you can configure hundreds or even thousands of devices to continuously and concurrently",
    "start": "321479",
    "end": "329720"
  },
  {
    "text": "push data into the F host service now sitting in the middle it is",
    "start": "329720",
    "end": "335360"
  },
  {
    "text": "the F host service and that's the core service we're going to use here today",
    "start": "335360",
    "end": "340400"
  },
  {
    "text": "for the streaming data ingestion the file host service will capture these data from your data producers in a",
    "start": "340400",
    "end": "347319"
  },
  {
    "text": "continuous streaming manner then on the right hand side the fost service will",
    "start": "347319",
    "end": "352919"
  },
  {
    "text": "automatically deliver the data into the destination that is specified and today",
    "start": "352919",
    "end": "358720"
  },
  {
    "text": "the service support reports Amazon S3 rare shift and elas elastic search",
    "start": "358720",
    "end": "364080"
  },
  {
    "text": "Service as the destination and once the data lands into",
    "start": "364080",
    "end": "369560"
  },
  {
    "text": "these destinations in a continuous manner you can use your favorite analytical tools to query and analyze or",
    "start": "369560",
    "end": "377120"
  },
  {
    "text": "visualize the data from these data stores on a continuous and realtime",
    "start": "377120",
    "end": "383880"
  },
  {
    "text": "manner okay let's dive into the stepbystep setup um so first first",
    "start": "384520",
    "end": "389960"
  },
  {
    "text": "before um we start the streaming data pipeline we're going to set up a r of",
    "start": "389960",
    "end": "395000"
  },
  {
    "text": "cluster with a database and a table in it and we're are going to stream the",
    "start": "395000",
    "end": "400080"
  },
  {
    "text": "data into this particular rare shift table through the F host",
    "start": "400080",
    "end": "405840"
  },
  {
    "text": "service I'm going to the r shift console and uh create a new cluster here I'm",
    "start": "405880",
    "end": "412680"
  },
  {
    "text": "naming my cluster as race streaming cluster also create a database called",
    "start": "412680",
    "end": "417879"
  },
  {
    "text": "streaming DB for the port I'm using the default R shift Port of",
    "start": "417879",
    "end": "425160"
  },
  {
    "text": "5439 I'm also going to configure a master user and password for admin",
    "start": "425160",
    "end": "430800"
  },
  {
    "text": "purpose to the red shift cluster Next Step I'm going to pick a",
    "start": "430800",
    "end": "437039"
  },
  {
    "text": "note type here I'm using dc1 Lodge which is the smallest node on rare shift for",
    "start": "437039",
    "end": "443080"
  },
  {
    "text": "this demo purpose I believe if you are eligible for R's free tier program this",
    "start": "443080",
    "end": "449120"
  },
  {
    "text": "type of instense will be free for two month so for for those of you who want to try out um the whole setup after this",
    "start": "449120",
    "end": "456720"
  },
  {
    "text": "session you can check out see if you're qualified for the rro free T and you'll be free of charge for two months for the",
    "start": "456720",
    "end": "463240"
  },
  {
    "text": "exercise of this this this example and I'm also going to use a single note again for this demo purpose that will be",
    "start": "463240",
    "end": "470599"
  },
  {
    "text": "powerful enough um to handle that data",
    "start": "470599",
    "end": "474800"
  },
  {
    "text": "load all right next step is going through all the different configurations here I'm keeping pretty",
    "start": "476120",
    "end": "482479"
  },
  {
    "text": "much everything as default other than one particular setting for the netw",
    "start": "482479",
    "end": "488360"
  },
  {
    "text": "configuration here you're required to make the cluster publicly accessible so",
    "start": "488360",
    "end": "494120"
  },
  {
    "text": "that the fire host service can get connected into your R shift cluster and load data into that however that doesn't",
    "start": "494120",
    "end": "501680"
  },
  {
    "text": "necessarily mean that you're opening up the cluster to the public because later on in the VPC security group setting we",
    "start": "501680",
    "end": "509560"
  },
  {
    "text": "going to restrict access only from the F host IP address range but to make it",
    "start": "509560",
    "end": "515399"
  },
  {
    "text": "work you do need to make your Rift cluster publicly accessible and this is",
    "start": "515399",
    "end": "520640"
  },
  {
    "text": "a key setting that you'll have to",
    "start": "520640",
    "end": "524760"
  },
  {
    "text": "do afterwards I'll review everything to make sure the configurations are all right then click on launch the cluster",
    "start": "526000",
    "end": "533040"
  },
  {
    "text": "then a few minutes later my Rift cluster will be up and running with the database",
    "start": "533040",
    "end": "538200"
  },
  {
    "text": "created in it",
    "start": "538200",
    "end": "541440"
  },
  {
    "text": "now we are going to the configuration page of our cluster to modify the",
    "start": "543760",
    "end": "549000"
  },
  {
    "text": "default V VPC security group setting so that it will allow the inbound access",
    "start": "549000",
    "end": "554600"
  },
  {
    "text": "from the fileo service IP",
    "start": "554600",
    "end": "558680"
  },
  {
    "text": "range so we're going to pick Rift as the inbound and access rule through TCP",
    "start": "561160",
    "end": "566720"
  },
  {
    "text": "protocol with a default Port we specified in the Foster configuration and then we'll add the F host service IP",
    "start": "566720",
    "end": "573920"
  },
  {
    "text": "range so depending on the region where you create a f host resource we'll use a",
    "start": "573920",
    "end": "580320"
  },
  {
    "text": "different IP address range for the service to access the r of cluster so",
    "start": "580320",
    "end": "585720"
  },
  {
    "text": "listed on this slide we have three different IP range for three different regions the fost service is currently in",
    "start": "585720",
    "end": "593480"
  },
  {
    "text": "so for this example I'm using the fire hoold delivery stream in Us East Northern Virginia Reg reg so I'm using",
    "start": "593480",
    "end": "600760"
  },
  {
    "text": "the corresponding IP address here so that the file host service is able to",
    "start": "600760",
    "end": "605959"
  },
  {
    "text": "access your R shift cluster for data load",
    "start": "605959",
    "end": "610760"
  },
  {
    "text": "purpose okay now we've created Rift cluster with the database in it with all",
    "start": "611720",
    "end": "617680"
  },
  {
    "text": "the configuration from the networking perspective set up for the fold dat stream to access now we are going to",
    "start": "617680",
    "end": "624640"
  },
  {
    "text": "connect to the Rashi database and create a particular table there for us to stream the data int here I'm using this",
    "start": "624640",
    "end": "632839"
  },
  {
    "text": "um aginity workbench tool to connect to RI cluster you can use any obbc or jdbc",
    "start": "632839",
    "end": "639760"
  },
  {
    "text": "connection tools as you",
    "start": "639760",
    "end": "643200"
  },
  {
    "text": "prefer and in this example I'm going to use the the this following SQL statement",
    "start": "645279",
    "end": "652240"
  },
  {
    "text": "to create a table for us to load the streaming data into rare shift and we'll have to make",
    "start": "652240",
    "end": "659399"
  },
  {
    "text": "sure the table structure we create has to match the sample data like number of",
    "start": "659399",
    "end": "666680"
  },
  {
    "text": "data fields needs to match a number of data columns the data type for each of the field value within the data record",
    "start": "666680",
    "end": "673639"
  },
  {
    "text": "needs to match the column data type for the rare shift table so this suo I've",
    "start": "673639",
    "end": "679560"
  },
  {
    "text": "modified to make it compatible with the data sample we're going to use today you can see in the traffic violation data",
    "start": "679560",
    "end": "687600"
  },
  {
    "text": "set we have um a very Rich set of data for example time of the stop time of the",
    "start": "687600",
    "end": "694519"
  },
  {
    "text": "incident the gender of the driver the geolocation of the incident and also a",
    "start": "694519",
    "end": "701440"
  },
  {
    "text": "color and maker of the C and and very rich information",
    "start": "701440",
    "end": "707079"
  },
  {
    "text": "Etc so after I hit this particular create create table",
    "start": "707079",
    "end": "712639"
  },
  {
    "text": "SQL this particular table is being created and I've named it a traffic",
    "start": "712639",
    "end": "717720"
  },
  {
    "text": "violation and you can see if I run a query against this table it's empty and",
    "start": "717720",
    "end": "723760"
  },
  {
    "text": "there's no data set in it now we've pretty much set up",
    "start": "723760",
    "end": "730240"
  },
  {
    "start": "728000",
    "end": "728000"
  },
  {
    "text": "everything on the rift side we've set up the destination and it's ready for taking the data from the F host service",
    "start": "730240",
    "end": "737680"
  },
  {
    "text": "and later on for us to query and analyze the data front now step two we're going",
    "start": "737680",
    "end": "744040"
  },
  {
    "text": "to set up the file host delivery stream and the file host delivery stream will",
    "start": "744040",
    "end": "749320"
  },
  {
    "text": "basically collect the data from the data producers on one side and on the other side automatically and continuously",
    "start": "749320",
    "end": "757160"
  },
  {
    "text": "deliver these data into the rare shift table we just set",
    "start": "757160",
    "end": "762519"
  },
  {
    "text": "up I'm going to the file host console and I'll choose Amazon rare",
    "start": "763120",
    "end": "769199"
  },
  {
    "text": "shift as a destination I want to use for this particular delivery stream and I'll name my delivery stream as Ray rash",
    "start": "769199",
    "end": "776480"
  },
  {
    "text": "shift stream and here we have this configuration called intermediate S3",
    "start": "776480",
    "end": "783160"
  },
  {
    "text": "bucket so for file host to rare shift the service will actually first load all",
    "start": "783160",
    "end": "790639"
  },
  {
    "text": "the data into a particular S3 bucket that you own and then continuously issue",
    "start": "790639",
    "end": "796720"
  },
  {
    "text": "R shift copy command to load the data from your S3 bucket to your rare shift",
    "start": "796720",
    "end": "803000"
  },
  {
    "text": "cluster and there are two reasons for this particular design and setup number",
    "start": "803000",
    "end": "808519"
  },
  {
    "text": "one loading data from S3 to rare shift is still the most efficient way for",
    "start": "808519",
    "end": "814839"
  },
  {
    "text": "loading data into rare shift cluster this way we make sure the data",
    "start": "814839",
    "end": "820240"
  },
  {
    "text": "load doesn't use much of the resource on the r shift side so that they can be",
    "start": "820240",
    "end": "825440"
  },
  {
    "text": "preserved for query and analytical purpose so here I'm using fho test",
    "start": "825440",
    "end": "832440"
  },
  {
    "text": "bucket it's one of the bucket that I own for the fire host service to First streaming the data into and here",
    "start": "832440",
    "end": "839920"
  },
  {
    "text": "optionally you can also specify an S3 prefix and this file host service will",
    "start": "839920",
    "end": "845279"
  },
  {
    "text": "attach this particular prefix for all the objects it delivers to the S3",
    "start": "845279",
    "end": "851279"
  },
  {
    "text": "bucket this is particularly useful for cases where you want to use the same bucket for multiple purposes so that",
    "start": "851279",
    "end": "859000"
  },
  {
    "text": "your objects can be grouped logically using this prefix and for this particular exercise I'm not going to",
    "start": "859000",
    "end": "866000"
  },
  {
    "text": "specify particular prefix as the bucket will only be used for this demonstration",
    "start": "866000",
    "end": "871360"
  },
  {
    "text": "and example then we're going to the rare shift cluster",
    "start": "871360",
    "end": "877680"
  },
  {
    "text": "configuration in the cluster name I'm putting race streaming which was a cluster we just created from the rift",
    "start": "877680",
    "end": "884639"
  },
  {
    "text": "console and so does the database name streaming DB and the table here I'm",
    "start": "884639",
    "end": "890480"
  },
  {
    "text": "going to use will be traffic violation that's the table we just created through",
    "start": "890480",
    "end": "895600"
  },
  {
    "text": "the um odbc and jdbc connection tool R shift table column I'm going to leave",
    "start": "895600",
    "end": "902639"
  },
  {
    "text": "it blank here because when I create the table I've ensured that the number of",
    "start": "902639",
    "end": "908120"
  },
  {
    "text": "columns within that particular table exactly matches the number of data fields within the data",
    "start": "908120",
    "end": "914759"
  },
  {
    "text": "sample in case you have a mismatch you can actually specify the columns here so",
    "start": "914759",
    "end": "920920"
  },
  {
    "text": "that the service will know what are the columns you want to the service to load",
    "start": "920920",
    "end": "926320"
  },
  {
    "text": "the data into for this particular rare shift table then I'm going to specify a RI",
    "start": "926320",
    "end": "932759"
  },
  {
    "text": "username and password here I'm using the the master username and a password we",
    "start": "932759",
    "end": "938279"
  },
  {
    "text": "just created from the r shift cluster for practical purpose you don't need to and you probably do not want to use a",
    "start": "938279",
    "end": "945120"
  },
  {
    "text": "Master username here you can create a separate user just for the F host service to load data into your R shift",
    "start": "945120",
    "end": "952800"
  },
  {
    "text": "cluster and this particular user needs to have insert permission on the r",
    "start": "952800",
    "end": "959680"
  },
  {
    "text": "site so that the F host service can use the credential of this user to load data",
    "start": "959680",
    "end": "966040"
  },
  {
    "text": "and insert data into this particular rare shift table on the rare shift copy option I'm",
    "start": "966040",
    "end": "974440"
  },
  {
    "text": "going to use Json Auto because the sample data we're going to use today is",
    "start": "974440",
    "end": "980279"
  },
  {
    "text": "in Json format and this option will ensure the rare shift cluster will go",
    "start": "980279",
    "end": "985560"
  },
  {
    "text": "ahead and Par the Json objects and and then pass each of the data field to each",
    "start": "985560",
    "end": "992199"
  },
  {
    "text": "of the column within that table we also have a retry duration",
    "start": "992199",
    "end": "998440"
  },
  {
    "text": "configuration for the F host delivery stream and the retry duration can be",
    "start": "998440",
    "end": "1004199"
  },
  {
    "text": "anywhere from 0 second to two hours when you configure 0 second that",
    "start": "1004199",
    "end": "1010399"
  },
  {
    "text": "means upon data dely failure to R shift the file host service will not retry and",
    "start": "1010399",
    "end": "1016759"
  },
  {
    "text": "skip to the next batch of data directly so this is a",
    "start": "1016759",
    "end": "1023480"
  },
  {
    "text": "configuration that you can tailor to your particular use case for example for",
    "start": "1023480",
    "end": "1029400"
  },
  {
    "text": "use cases where you care about data recency that you always want of the most",
    "start": "1029400",
    "end": "1034880"
  },
  {
    "text": "fresh data to get loaded into your red shift cluster then you want to configure",
    "start": "1034880",
    "end": "1040000"
  },
  {
    "text": "low retry duration so that you want you can make sure that a fire host always",
    "start": "1040000",
    "end": "1045199"
  },
  {
    "text": "skip to the most recent data if the connection or the data load is",
    "start": "1045199",
    "end": "1051360"
  },
  {
    "text": "failed on the other side if your case cares more about data completeness then",
    "start": "1051360",
    "end": "1059160"
  },
  {
    "text": "you can use a longer retry duration for example the maximum retry duration",
    "start": "1059160",
    "end": "1064559"
  },
  {
    "text": "window of 2 hours is normally enough to cover the r of maintenance window so in",
    "start": "1064559",
    "end": "1071559"
  },
  {
    "text": "cases where your RI cluster is under maintenance F host will keep rwi and",
    "start": "1071559",
    "end": "1077240"
  },
  {
    "text": "unhold the data delivery until the r shift cluster is back",
    "start": "1077240",
    "end": "1082760"
  },
  {
    "text": "online so for this particular exercise I'm going to use 30 seconds it's a f",
    "start": "1082760",
    "end": "1089039"
  },
  {
    "text": "balance between data recency and the completeness 30 seconds is enough to",
    "start": "1089039",
    "end": "1095440"
  },
  {
    "text": "cover intermittent like network issues connection issues but also it's not too",
    "start": "1095440",
    "end": "1101679"
  },
  {
    "text": "long so that I can make sure my data will get delivered after 30 seconds if",
    "start": "1101679",
    "end": "1107200"
  },
  {
    "text": "there's any failure for the load into the ri cluster at the bottom of this page this",
    "start": "1107200",
    "end": "1115559"
  },
  {
    "text": "is a particular useful feature for initial setup purpose we display the",
    "start": "1115559",
    "end": "1120960"
  },
  {
    "text": "copy command from the firehost console so as I previously mentioned for f host to rash shift the",
    "start": "1120960",
    "end": "1128640"
  },
  {
    "text": "service will first deliver the data into your S3 bucket and then issue a copy",
    "start": "1128640",
    "end": "1134840"
  },
  {
    "text": "command on a continuous manner to load the data from the s three bucket into",
    "start": "1134840",
    "end": "1140200"
  },
  {
    "text": "the red shift cluster So based on the configuration we put in here for this fire host Del",
    "start": "1140200",
    "end": "1147400"
  },
  {
    "text": "stream the service will generate a copy command to match the configurations we",
    "start": "1147400",
    "end": "1153039"
  },
  {
    "text": "put in here and the copy command displayed here is the exact copy command",
    "start": "1153039",
    "end": "1158799"
  },
  {
    "text": "the service is going to use to copy the data from S3 to rare",
    "start": "1158799",
    "end": "1164919"
  },
  {
    "text": "shift so in the initial setup phase it'll be very helpful for us to test out",
    "start": "1164919",
    "end": "1172320"
  },
  {
    "text": "this copy command by using the jdbc or odbc connection tool to Red shift",
    "start": "1172320",
    "end": "1178360"
  },
  {
    "text": "cluster directly so this way we can ensure that all the configuration is",
    "start": "1178360",
    "end": "1183919"
  },
  {
    "text": "correctly set up and put in and if there's any copy failure from the r shift cluster we can come back and",
    "start": "1183919",
    "end": "1191039"
  },
  {
    "text": "modify the configuration before we set up the delivery stream",
    "start": "1191039",
    "end": "1197559"
  },
  {
    "text": "then we'll click on next going to the configuration page the first option here",
    "start": "1199440",
    "end": "1205400"
  },
  {
    "text": "is S3 buffer so before file host deliver the data into the S3 bucket it can",
    "start": "1205400",
    "end": "1213080"
  },
  {
    "text": "buffer the data records into a certain size or for a certain period of time",
    "start": "1213080",
    "end": "1218840"
  },
  {
    "text": "you'll configure your buffering options around buffer Sizer interval the size",
    "start": "1218840",
    "end": "1224000"
  },
  {
    "text": "can be anywhere from 1 Megabyte to 128 megabytes and the buffer interval can be",
    "start": "1224000",
    "end": "1230720"
  },
  {
    "text": "anywhere from 60 seconds to 15 minutes and whatever condition gets",
    "start": "1230720",
    "end": "1237280"
  },
  {
    "text": "satisfied first will trigger the data delivery and flush into your S3",
    "start": "1237280",
    "end": "1243919"
  },
  {
    "text": "bucket so F host S3 buffering feature actually ensures efficient data load",
    "start": "1243919",
    "end": "1249520"
  },
  {
    "text": "into S3 bucket so that number one we don't end up with too many s3p API cost",
    "start": "1249520",
    "end": "1257919"
  },
  {
    "text": "and that will incur a put API call cost on the S3 side number two we ensure that",
    "start": "1257919",
    "end": "1264480"
  },
  {
    "text": "the objects within S3 is not too small so that it'll be more efficient for the",
    "start": "1264480",
    "end": "1270000"
  },
  {
    "text": "downst streaming processing and in this case it'll be more efficient for us to load these objects into the rare shift",
    "start": "1270000",
    "end": "1278679"
  },
  {
    "text": "cluster then we'll have S3 compression and encryption option we can configure the F host",
    "start": "1278840",
    "end": "1286640"
  },
  {
    "text": "service to compress our Data before we sended it to the S3 bucket and the",
    "start": "1286640",
    "end": "1292840"
  },
  {
    "text": "compression format supported here will be gzip and that will be the supported data format for Rift cluster to load the",
    "start": "1292840",
    "end": "1300279"
  },
  {
    "text": "data from S3 and one thing to point out is if we do configure gzip as a compression",
    "start": "1300279",
    "end": "1307640"
  },
  {
    "text": "format here we'll need to add the gzip copy option in the copy configuration on",
    "start": "1307640",
    "end": "1314320"
  },
  {
    "text": "the previous page so that when the F host service issue the copy command to rare shift rare shift knows that the",
    "start": "1314320",
    "end": "1321919"
  },
  {
    "text": "objects in S3 is gzipped and so that it can uncompress accordingly and here for this particular",
    "start": "1321919",
    "end": "1329159"
  },
  {
    "text": "session and exercise I'm going to configure it as uncompress the data to",
    "start": "1329159",
    "end": "1335320"
  },
  {
    "text": "S3 optionally we can also specify km's key for the service to encrypt the data",
    "start": "1335320",
    "end": "1343080"
  },
  {
    "text": "and if when km's key is configured here for encryption upon data delivery to S3",
    "start": "1343080",
    "end": "1349679"
  },
  {
    "text": "the fire host service will trigger the S3's server side encryption to use the",
    "start": "1349679",
    "end": "1355440"
  },
  {
    "text": "particular cams ke to encrypt the data on the S3 side so that this way your",
    "start": "1355440",
    "end": "1362240"
  },
  {
    "text": "data will be security stored on S3 then we'll have an error logging",
    "start": "1362240",
    "end": "1368440"
  },
  {
    "text": "feature so in case when the fire host is not able to deliver the data either to",
    "start": "1368440",
    "end": "1374480"
  },
  {
    "text": "your S3 bucket or to your RF cluster it'll log an error message in your",
    "start": "1374480",
    "end": "1381080"
  },
  {
    "text": "cloudwatch log group and you can view these error messages either from the cloudwatch console or from the fire host",
    "start": "1381080",
    "end": "1388520"
  },
  {
    "text": "console directly which will demonstrate a little bit later on and this feature",
    "start": "1388520",
    "end": "1393919"
  },
  {
    "text": "is very useful for troubleshooting purposes so in case you don't see the data being show up in your RI cluster",
    "start": "1393919",
    "end": "1402600"
  },
  {
    "text": "you can come back and check the error message see what has gone wrong and you can troubleshoot an fix the issue",
    "start": "1402600",
    "end": "1410720"
  },
  {
    "text": "accordingly then lastly there's this IM roll for security control purpose this",
    "start": "1410720",
    "end": "1418440"
  },
  {
    "text": "IM roll you will create and allow the file host service to access the resource",
    "start": "1418440",
    "end": "1424679"
  },
  {
    "text": "Rec configured in this file host Zary stream for example the particular S3",
    "start": "1424679",
    "end": "1430200"
  },
  {
    "text": "bucket we use here the r of cluster we use here and at the km ke we specify if",
    "start": "1430200",
    "end": "1436760"
  },
  {
    "text": "we do enable encryption this is to ensure that only fost service",
    "start": "1436760",
    "end": "1442440"
  },
  {
    "text": "can access these resources you specify here with your explicit permission",
    "start": "1442440",
    "end": "1447960"
  },
  {
    "text": "granted and also the fold service will only have access to the specific bucket",
    "start": "1447960",
    "end": "1454400"
  },
  {
    "text": "key and cluster we specify here okay at these two step then we'll",
    "start": "1454400",
    "end": "1462960"
  },
  {
    "text": "review all the configuration and settings we put in for the F host stream",
    "start": "1462960",
    "end": "1468399"
  },
  {
    "text": "then we'll hit the creative stream then a few minutes later the stream will be up and running and ready for us to pump",
    "start": "1468399",
    "end": "1475200"
  },
  {
    "text": "the data through so as you can see it probably takes about 60 seconds for you",
    "start": "1475200",
    "end": "1481159"
  },
  {
    "text": "to go through the whole configuration and set up for the f h z stream and literally after these 60 seconds we've",
    "start": "1481159",
    "end": "1489600"
  },
  {
    "text": "already had a streaming data pipeline that'll continuous load data into my rare shift cluster behind the",
    "start": "1489600",
    "end": "1496919"
  },
  {
    "text": "scene there's no no infrastructure management that I need from my",
    "start": "1496919",
    "end": "1502559"
  },
  {
    "text": "side excuse me and I don't need to provision any server hosts storage space",
    "start": "1502559",
    "end": "1509320"
  },
  {
    "text": "or things like that all I need to do is this 60 seconds configuration through the console and all the resource",
    "start": "1509320",
    "end": "1516360"
  },
  {
    "text": "management is handled behind the scene by the fire host",
    "start": "1516360",
    "end": "1521799"
  },
  {
    "text": "service okay now I've had my R of cluster set up my fire host deliver",
    "start": "1521919",
    "end": "1527200"
  },
  {
    "start": "1523000",
    "end": "1523000"
  },
  {
    "text": "stream set up so I've pretty much already set up a pipeline and the data storage on the rare shift side now step",
    "start": "1527200",
    "end": "1535000"
  },
  {
    "text": "three it's ready for me to send the data into the file host delivery",
    "start": "1535000",
    "end": "1541480"
  },
  {
    "text": "Stream So the sample data again as I mentioned here we're going to use is",
    "start": "1542240",
    "end": "1547480"
  },
  {
    "text": "Maryland's traffic violation data these data ented on the cellular device by the",
    "start": "1547480",
    "end": "1554200"
  },
  {
    "text": "police officer whenever there's a traffic violation incident",
    "start": "1554200",
    "end": "1559279"
  },
  {
    "text": "so this data set is is publicly accessible I've put the UR here so that",
    "start": "1559279",
    "end": "1564919"
  },
  {
    "text": "for those of you who want to kick the tire and try it out later on yourself you can go ahead and get the data set",
    "start": "1564919",
    "end": "1572840"
  },
  {
    "text": "online and in this demonstration I'm going to directly using the this data",
    "start": "1572840",
    "end": "1578159"
  },
  {
    "text": "set and pass them and send it to the F host service in a real world use case",
    "start": "1578159",
    "end": "1583600"
  },
  {
    "text": "you can imagine that actually this data record will be streamed from the police",
    "start": "1583600",
    "end": "1588880"
  },
  {
    "text": "officer cellular devices directly into the F host service so as soon as a",
    "start": "1588880",
    "end": "1595520"
  },
  {
    "text": "traffic violation is recorded on the device it'll be directly streamed into",
    "start": "1595520",
    "end": "1600840"
  },
  {
    "text": "the F host service and later on being able to access to the r of cluster",
    "start": "1600840",
    "end": "1606080"
  },
  {
    "text": "within seconds or minutes latency so this is a sample record on the screen as I mentioned it has pretty",
    "start": "1606080",
    "end": "1613720"
  },
  {
    "text": "rich information here we have the location of this incident gender race of",
    "start": "1613720",
    "end": "1619480"
  },
  {
    "text": "the driver geographic location of the driver and also the information about",
    "start": "1619480",
    "end": "1625919"
  },
  {
    "text": "the vehicle from maker to color and",
    "start": "1625919",
    "end": "1630639"
  },
  {
    "text": "Etc and in this exercise I'm going to use a couple of lines of of code to pass",
    "start": "1631919",
    "end": "1638720"
  },
  {
    "text": "the data sample and then send them into the fost deliv stream the file host",
    "start": "1638720",
    "end": "1644840"
  },
  {
    "text": "service offers two apis to put data into the service one is called put record and",
    "start": "1644840",
    "end": "1652559"
  },
  {
    "text": "for this set of API we'll put each of a single record into one particular API call and the",
    "start": "1652559",
    "end": "1660480"
  },
  {
    "text": "second API is called put record badge and as indicated by the name of the API",
    "start": "1660480",
    "end": "1666480"
  },
  {
    "text": "call we can put multiple data records into a sing Single API call and this way",
    "start": "1666480",
    "end": "1673159"
  },
  {
    "text": "you can achieve higher efficiency and um higher throughput on the data producer",
    "start": "1673159",
    "end": "1679519"
  },
  {
    "text": "and for this this example I'm just going to use the single put record API call",
    "start": "1679519",
    "end": "1685200"
  },
  {
    "text": "this piece of a code will pass the data from the sample data",
    "start": "1685200",
    "end": "1690799"
  },
  {
    "text": "and grab each of the line as a Jon object record and then send it into the F host",
    "start": "1690799",
    "end": "1697559"
  },
  {
    "text": "service now I've highlighted one particular line which is to add a new",
    "start": "1697559",
    "end": "1702880"
  },
  {
    "text": "line character behind each of the data record the reason is because one once",
    "start": "1702880",
    "end": "1708320"
  },
  {
    "text": "each of the data record lands into the fost service it'll buffer and concatenate all the records and then",
    "start": "1708320",
    "end": "1716200"
  },
  {
    "text": "flush that into the S3 bucket so by adding a delimiter and in this case the",
    "start": "1716200",
    "end": "1721919"
  },
  {
    "text": "new line character behind each of the record before we send the data to the file host service it'll ensure that",
    "start": "1721919",
    "end": "1730080"
  },
  {
    "text": "after the service concatenated all the data records we can still distinguish this each single record in the S3 object",
    "start": "1730080",
    "end": "1740039"
  },
  {
    "start": "1742000",
    "end": "1742000"
  },
  {
    "text": "so after I execute this piece of code and start paing and send the data records from the sample data into the",
    "start": "1742760",
    "end": "1750159"
  },
  {
    "text": "fost service then the fost service will start delivering the data into my Rift",
    "start": "1750159",
    "end": "1756399"
  },
  {
    "text": "cluster and as I keep sending these data records to the file host service it'll",
    "start": "1756399",
    "end": "1762159"
  },
  {
    "text": "continuously gets the data loaded into the rift table that I just created",
    "start": "1762159",
    "end": "1769159"
  },
  {
    "text": "so step four we'll wait for about a minute or two depending on the buffering",
    "start": "1769159",
    "end": "1774440"
  },
  {
    "text": "option we configuring the fosty stream then the data will start showing up in",
    "start": "1774440",
    "end": "1779760"
  },
  {
    "text": "my red shift table so now if I run a query from the",
    "start": "1779760",
    "end": "1785240"
  },
  {
    "text": "table I can see now the data is starting getting loaded into my red shift table",
    "start": "1785240",
    "end": "1791039"
  },
  {
    "text": "with all the columns that I pre-created they'll match the data record field that I'll be inserted from the data sample",
    "start": "1791039",
    "end": "1798720"
  },
  {
    "text": "and I can keep refreshing this query from time to time because the new data will be continuously loaded into my R",
    "start": "1798720",
    "end": "1805279"
  },
  {
    "text": "shift table as they get streamed into the F host delivery",
    "start": "1805279",
    "end": "1810840"
  },
  {
    "text": "stream and we can also do some visualization against the data set once",
    "start": "1811519",
    "end": "1816559"
  },
  {
    "text": "they land in the ri cluster here I'm going to use um Amazon quick site it's",
    "start": "1816559",
    "end": "1822480"
  },
  {
    "text": "the business intelligence tool on the cloud provided by AWS this service is in",
    "start": "1822480",
    "end": "1828720"
  },
  {
    "text": "preview right now and I think you can sign up through the website if you want to try it out so from the service",
    "start": "1828720",
    "end": "1835640"
  },
  {
    "text": "console for quick site I'm going to pick rare shift as my data source and putting",
    "start": "1835640",
    "end": "1841279"
  },
  {
    "text": "my rare shift cluster name database name username and a password for quick side",
    "start": "1841279",
    "end": "1846919"
  },
  {
    "text": "to connect into my rare shift table and it's very easy to set up and",
    "start": "1846919",
    "end": "1854840"
  },
  {
    "text": "afterwards I'm creating a dashboard to an ize the number of instance by hour",
    "start": "1854840",
    "end": "1861320"
  },
  {
    "text": "throughout the day and also break down by gender and of course it's a very rich data set you can come up with a lot more",
    "start": "1861320",
    "end": "1868360"
  },
  {
    "text": "interesting analysis and visualizations and on this spot chart graph it basically shows the number of",
    "start": "1868360",
    "end": "1876080"
  },
  {
    "text": "traffic violation incidents throughout the whole day by hour and this Statistics is from the whole Year's data",
    "start": "1876080",
    "end": "1883480"
  },
  {
    "text": "in 2014 and the blue bar are the incidents by male drivers and the orange bar are",
    "start": "1883480",
    "end": "1890960"
  },
  {
    "text": "the incidents from female drivers and there are a couple of pretty interesting patterns that you can see during morning",
    "start": "1890960",
    "end": "1898639"
  },
  {
    "text": "hours actually there's a pretty large gap by traffic violations between male and female drivers that's probably",
    "start": "1898639",
    "end": "1905799"
  },
  {
    "text": "because male still is still a larger population in the working environment",
    "start": "1905799",
    "end": "1910840"
  },
  {
    "text": "and uh in the morning there are more male working population committ to",
    "start": "1910840",
    "end": "1915919"
  },
  {
    "text": "work and around noon you can see actually the overall traffic violation",
    "start": "1915919",
    "end": "1921039"
  },
  {
    "text": "starts going down probably because the drivers have less activity around around",
    "start": "1921039",
    "end": "1927159"
  },
  {
    "text": "noon or maybe all the police officers at lunch break so now you know when to",
    "start": "1927159",
    "end": "1932360"
  },
  {
    "text": "speeding Maryland and I'm just kidding don't speed um then around afternoon around",
    "start": "1932360",
    "end": "1939399"
  },
  {
    "text": "300 p.m. to 5:00 p.m. is you'll notice actually the gap for traffic violations between female and male the Gap gets",
    "start": "1939399",
    "end": "1947440"
  },
  {
    "text": "smaller and smaller and that's probably because in the afternoon the female driver's activity starts getting picked",
    "start": "1947440",
    "end": "1953919"
  },
  {
    "text": "up so this kind of a very simple graph that I created and with a live dashboard",
    "start": "1953919",
    "end": "1959639"
  },
  {
    "text": "you can imagine as the police officers start booking these incidents on their",
    "start": "1959639",
    "end": "1965240"
  },
  {
    "text": "cellular devices they gets pushed into the fire host delivery stream and from",
    "start": "1965240",
    "end": "1970760"
  },
  {
    "text": "the fire host service it'll continuously load this data into the red shift table",
    "start": "1970760",
    "end": "1976120"
  },
  {
    "text": "and this live dashboard will constantly refreshing you can imagine that throughout the whole day I can see these",
    "start": "1976120",
    "end": "1982559"
  },
  {
    "text": "SP starting going up and getting populated on my dashboard and after this session um",
    "start": "1982559",
    "end": "1989880"
  },
  {
    "text": "there are actually some other interesting realation we can create for example we can create a um a geographic",
    "start": "1989880",
    "end": "1996080"
  },
  {
    "text": "map graph to with a heat map setup to indicate the number of traffic violations over time for each of the",
    "start": "1996080",
    "end": "2003399"
  },
  {
    "text": "district in the Maryland state and I'll leave this exercise to you guys you can use your favorite business intelligence",
    "start": "2003399",
    "end": "2010600"
  },
  {
    "text": "tools that are familiar with to do that so these are the four steps set up",
    "start": "2010600",
    "end": "2018519"
  },
  {
    "start": "2016000",
    "end": "2016000"
  },
  {
    "text": "for us to collect the data from the cellular devices by the police officer and of course here we are using the",
    "start": "2018519",
    "end": "2025080"
  },
  {
    "text": "sample data for simulation purpose or the way to pipe the data into the F host",
    "start": "2025080",
    "end": "2030240"
  },
  {
    "text": "datab stream and then from the fold stream the service will automatically",
    "start": "2030240",
    "end": "2035519"
  },
  {
    "text": "populate the data into my my r cluster and table then using a business",
    "start": "2035519",
    "end": "2041000"
  },
  {
    "text": "intelligence tool to visualize the data then step five um how do we monitor",
    "start": "2041000",
    "end": "2048679"
  },
  {
    "text": "the streaming data pipeline as you can see the five host service has tried to",
    "start": "2048679",
    "end": "2054158"
  },
  {
    "text": "minimize the effort from the user side and try to manage as many things as possible behind the scene so that we",
    "start": "2054159",
    "end": "2061118"
  },
  {
    "text": "don't need to worry about many of the setup and maintenance and management but at the same time the service also offers",
    "start": "2061119",
    "end": "2068358"
  },
  {
    "text": "a pretty rich monitoring feature set so that we can clearly understand what's happening behind the",
    "start": "2068359",
    "end": "2076398"
  },
  {
    "text": "scene so first the fire host service will offer a set of cloud watch metrics",
    "start": "2076839",
    "end": "2082520"
  },
  {
    "text": "for us to monitor the data flow we can for example monitor the data from data",
    "start": "2082520",
    "end": "2088960"
  },
  {
    "text": "volume perspective by bytes or by number of Records we can monitor the data flow from the data producer to the F host",
    "start": "2088960",
    "end": "2096320"
  },
  {
    "text": "service and from the host service to the S3 bucket and then from S3 bucket to the",
    "start": "2096320",
    "end": "2102040"
  },
  {
    "text": "ri cluster we can also monitor the data refreshness so that we know when the",
    "start": "2102040",
    "end": "2108720"
  },
  {
    "text": "data is being delivered by the fost service how much time it has passed",
    "start": "2108720",
    "end": "2114079"
  },
  {
    "text": "since the data is being injested into the service and we can also monitor the",
    "start": "2114079",
    "end": "2119599"
  },
  {
    "text": "data delivery success to S3 bucket or Rift cluster so that we can set up alarm",
    "start": "2119599",
    "end": "2126839"
  },
  {
    "text": "or alert on the Cloud watch side so that we'll get an email or text message or",
    "start": "2126839",
    "end": "2131880"
  },
  {
    "text": "trigger Lambda downstreaming workflow when the data delivery",
    "start": "2131880",
    "end": "2137160"
  },
  {
    "text": "fails so other than the cloudwatch Matrix which tells you if things are working well and which component might",
    "start": "2137160",
    "end": "2144119"
  },
  {
    "text": "not be working well at this point we also have this cloudwatch logging",
    "start": "2144119",
    "end": "2149800"
  },
  {
    "text": "feature I mentioned previously which tells you what has gone wrun and it's",
    "start": "2149800",
    "end": "2155040"
  },
  {
    "text": "it'll be very rich information for troubleshooting purpose so in this case",
    "start": "2155040",
    "end": "2160520"
  },
  {
    "text": "I actually after I created a delivery stream I intentionally change the",
    "start": "2160520",
    "end": "2166839"
  },
  {
    "text": "username and password in my R shift cluster and using different password and",
    "start": "2166839",
    "end": "2172359"
  },
  {
    "text": "of course the fileo service is still using the O password and the data delivery will be failed because the",
    "start": "2172359",
    "end": "2178440"
  },
  {
    "text": "service now not able to access the r of cluster excuse me and now you can see",
    "start": "2178440",
    "end": "2186640"
  },
  {
    "text": "actually one whenever the F host is not able to deliver the data into the r shift cluster it'll book the error",
    "start": "2186640",
    "end": "2192880"
  },
  {
    "text": "message and show up here so when these kind of issues happen I can clearly see",
    "start": "2192880",
    "end": "2198920"
  },
  {
    "text": "oh the username and the password is not correct so that I know I need to go to",
    "start": "2198920",
    "end": "2204200"
  },
  {
    "text": "the F host console update my delivery stream with a new valid username and",
    "start": "2204200",
    "end": "2209800"
  },
  {
    "text": "password then the F host service will recover and start continuously deliver",
    "start": "2209800",
    "end": "2215079"
  },
  {
    "text": "the data to my R table",
    "start": "2215079",
    "end": "2219520"
  },
  {
    "text": "so these are the kind of uh five steps we've gone through today so again I",
    "start": "2221480",
    "end": "2227240"
  },
  {
    "text": "think as you can see it's really fairly easy to get started with streaming data",
    "start": "2227240",
    "end": "2233040"
  },
  {
    "text": "so for those of you who are using Rift today or other data warehouse Solutions chances are you are probably",
    "start": "2233040",
    "end": "2239760"
  },
  {
    "text": "loading your data into these places on a daily basis or maybe hourly basis now",
    "start": "2239760",
    "end": "2245400"
  },
  {
    "text": "with the fire host service on the table really you can shorten that period of time to minutes or even seconds and you",
    "start": "2245400",
    "end": "2252760"
  },
  {
    "text": "can see from this session it really doesn't require any expertise or",
    "start": "2252760",
    "end": "2257839"
  },
  {
    "text": "knowledge on the streaming data sector all you need to do is create the delivery stream from the file host",
    "start": "2257839",
    "end": "2263800"
  },
  {
    "text": "console then you can start experiment and using the streaming data pipeline to fill and uh populate the data into your",
    "start": "2263800",
    "end": "2271880"
  },
  {
    "text": "RI cluster so again I think after this session you'll have access to the slides",
    "start": "2271880",
    "end": "2277280"
  },
  {
    "text": "and recording of this session and you also have access to the data sample that",
    "start": "2277280",
    "end": "2282400"
  },
  {
    "text": "I'm I use today in this session so I encourage you guys to go ahead and try",
    "start": "2282400",
    "end": "2287800"
  },
  {
    "text": "it out on your site yourself so that you can get a sense of how easy it is to",
    "start": "2287800",
    "end": "2293079"
  },
  {
    "text": "getting started with streaming data attic on AWS okay and this is the part of the",
    "start": "2293079",
    "end": "2300440"
  },
  {
    "text": "session we'll have today and now we'll have some time and open up to",
    "start": "2300440",
    "end": "2305920"
  },
  {
    "text": "questions so the first first question that I get on the screen is where do failure data points end",
    "start": "2305920",
    "end": "2312680"
  },
  {
    "text": "up for example if I have the wrong number of columns in my R shift table",
    "start": "2312680",
    "end": "2318160"
  },
  {
    "text": "that's a really good question that's one of the reason why we have a intermediate",
    "start": "2318160",
    "end": "2323240"
  },
  {
    "text": "S3 bucket so in case fire host is not able to load the data from S3 to your RI",
    "start": "2323240",
    "end": "2330880"
  },
  {
    "text": "cluster fire host will log an error manafest file and send that into your S3",
    "start": "2330880",
    "end": "2336960"
  },
  {
    "text": "bucket under the error prefix so whenever there's a data failure to R",
    "start": "2336960",
    "end": "2342280"
  },
  {
    "text": "shift you can go to your S3 Bucket look for the error prefix and uh the objects",
    "start": "2342280",
    "end": "2348319"
  },
  {
    "text": "that are failed to be delivered into the rift cluster will be logged as a manifest file and you can actually use",
    "start": "2348319",
    "end": "2356599"
  },
  {
    "text": "the odbc or jdpc connection tool to rare shift to do back manual backfill",
    "start": "2356599",
    "end": "2361960"
  },
  {
    "text": "afterwards so number one all of your data is security stored in your S3 bucket no matter if your Rift data load",
    "start": "2361960",
    "end": "2369680"
  },
  {
    "text": "is successful or not number two when the data load from S3 to R shift fails the",
    "start": "2369680",
    "end": "2375680"
  },
  {
    "text": "fire host service will log an error manifest file to the S3 bucket to tell you what are the objects that will fail",
    "start": "2375680",
    "end": "2382960"
  },
  {
    "text": "to load and on the ri side you can actually manually backfill with the error manifest",
    "start": "2382960",
    "end": "2390640"
  },
  {
    "text": "file the next question is what is the use of the S3 prefix so as I mentioned",
    "start": "2390640",
    "end": "2397880"
  },
  {
    "text": "um for all the objects F host deliver to your bucket first it'll automatically",
    "start": "2397880",
    "end": "2404480"
  },
  {
    "text": "append a prefix in time format so that you know your objects will be be be um",
    "start": "2404480",
    "end": "2411960"
  },
  {
    "text": "be grouped in the time stamp and partition that way and at the same time we often see customers use uh the same",
    "start": "2411960",
    "end": "2419880"
  },
  {
    "text": "bucket for multiple purposes for example if I have two delivery streams one of",
    "start": "2419880",
    "end": "2426400"
  },
  {
    "text": "them used for collecting my web server lock data and the other",
    "start": "2426400",
    "end": "2431920"
  },
  {
    "text": "one I'm going to use them to collect my um clickstream data from from mobile",
    "start": "2431920",
    "end": "2437680"
  },
  {
    "text": "devices and I want to store these data in the same S3 bucket so the prefix",
    "start": "2437680",
    "end": "2443680"
  },
  {
    "text": "feature will be very handy um for fire host service to add this particular prefix before the object is being",
    "start": "2443680",
    "end": "2450599"
  },
  {
    "text": "delivered to the S3 bucket so that I know when I log into the S3 bucket these",
    "start": "2450599",
    "end": "2456680"
  },
  {
    "text": "data will be grouped logically and I know under which prefix or what kind of data",
    "start": "2456680",
    "end": "2463400"
  },
  {
    "text": "sets the next question is is eventual data delivery guaranteed irrespective of",
    "start": "2463400",
    "end": "2470280"
  },
  {
    "text": "retry setting or we need to custom code to handle this so file host will retry",
    "start": "2470280",
    "end": "2476880"
  },
  {
    "text": "the data delivery for a period of time that you can configure anywhere from 0 second to 2 hours and upon the Rite",
    "start": "2476880",
    "end": "2485240"
  },
  {
    "text": "duration if the delivery still f FS the service will skip to the next batch of",
    "start": "2485240",
    "end": "2490520"
  },
  {
    "text": "data and give up on that batch and the failed data delivery information again",
    "start": "2490520",
    "end": "2496000"
  },
  {
    "text": "will be logged into the S3 bucket as an error manage Fest file to indicate what",
    "start": "2496000",
    "end": "2502000"
  },
  {
    "text": "are what are the objects that were failed to deliver and the service current doesn't",
    "start": "2502000",
    "end": "2507839"
  },
  {
    "text": "handle automatic data backfill which means for the data that will fail to be",
    "start": "2507839",
    "end": "2512960"
  },
  {
    "text": "delivered you'll need to do the manual backfill yourself and and often we see",
    "start": "2512960",
    "end": "2518160"
  },
  {
    "text": "customers implementation to be for example we can set up an S3 event to",
    "start": "2518160",
    "end": "2523440"
  },
  {
    "text": "monitor the error manifest file and once the error manage Fest file is being",
    "start": "2523440",
    "end": "2528880"
  },
  {
    "text": "delivered to the S3 bucket it can trigger a Lambda function or a downstreaming workflow to retry the copy",
    "start": "2528880",
    "end": "2536079"
  },
  {
    "text": "from S3 bucket to Red shift cluster for back fill purpose and you can also run a",
    "start": "2536079",
    "end": "2541680"
  },
  {
    "text": "daily job or an hourly job to scan the error manifest file and do the data load",
    "start": "2541680",
    "end": "2546800"
  },
  {
    "text": "Auto aut atically the next question is the retry",
    "start": "2546800",
    "end": "2552880"
  },
  {
    "text": "at S3 bucket stage or for copying to rare shift that's a very good question",
    "start": "2552880",
    "end": "2558800"
  },
  {
    "text": "and the retry is for the copying to rare shift for data delivery to S3 bucket",
    "start": "2558800",
    "end": "2565760"
  },
  {
    "text": "we'll keep retrying until the S3 bucket is back online the reason is because",
    "start": "2565760",
    "end": "2572079"
  },
  {
    "text": "this way we ensure that there's always no data loss so the file poost will store the",
    "start": "2572079",
    "end": "2578319"
  },
  {
    "text": "data on the service side for up to 24 hours until the these data will be",
    "start": "2578319",
    "end": "2583359"
  },
  {
    "text": "deleted from the Del delivery stream so whenever the S3 bucket is not a",
    "start": "2583359",
    "end": "2588599"
  },
  {
    "text": "available or the fire host service is not able to send the data to the bucket for whatever reason the service will",
    "start": "2588599",
    "end": "2594880"
  },
  {
    "text": "keep retrying and not give up until 24 hours later the data will be deleted",
    "start": "2594880",
    "end": "2599960"
  },
  {
    "text": "from the delivery stream the next question on the screen is is there a size limitation",
    "start": "2599960",
    "end": "2607640"
  },
  {
    "text": "on incoming data into fire hose so there are two set of limitations",
    "start": "2607640",
    "end": "2614760"
  },
  {
    "text": "I think that you can think of number one is the limitation for each of the record size and for each of the data record we",
    "start": "2614760",
    "end": "2622680"
  },
  {
    "text": "support up to 1,000 kilobytes and that should be sufficient for vast majority",
    "start": "2622680",
    "end": "2628680"
  },
  {
    "text": "of the streaming data use cases the second limitation is on the",
    "start": "2628680",
    "end": "2634920"
  },
  {
    "text": "throughput side each of the F host regre by default can support 5 megabytes per",
    "start": "2634920",
    "end": "2642079"
  },
  {
    "text": "second incoming data volume and that limit is soft and can be easily raised",
    "start": "2642079",
    "end": "2647640"
  },
  {
    "text": "but you can just file a ticket with us through the limit increase form and tell us the through put that you desire and",
    "start": "2647640",
    "end": "2655079"
  },
  {
    "text": "we'll go ahead and raise that for you and from pricing perspective we actually don't charge you for the for the",
    "start": "2655079",
    "end": "2661079"
  },
  {
    "text": "bandwidth or throughput of your delivery stream we only charge for the data volume that gets inest ingested through",
    "start": "2661079",
    "end": "2667280"
  },
  {
    "text": "the F host service if the question is about scale",
    "start": "2667280",
    "end": "2673079"
  },
  {
    "text": "like how much data volume can the service handle the service scills extremely well we can handle data load",
    "start": "2673079",
    "end": "2680200"
  },
  {
    "text": "from millions of devices we can handle TPS all the way to a couple million TPS",
    "start": "2680200",
    "end": "2685520"
  },
  {
    "text": "per second and also a couple of gigabytes per second through put so",
    "start": "2685520",
    "end": "2691040"
  },
  {
    "text": "again the service scales extremely well the next question",
    "start": "2691040",
    "end": "2698040"
  },
  {
    "text": "where do you define the source for f host how do you define source for streaming data so the data source really",
    "start": "2698040",
    "end": "2705280"
  },
  {
    "text": "could be anything for example an eect instance and web server a mobile device",
    "start": "2705280",
    "end": "2711559"
  },
  {
    "text": "and iot device and the data source is currently not defined from the fost",
    "start": "2711559",
    "end": "2716920"
  },
  {
    "text": "console so from the fost side you set up a streaming data Pipeline and point that",
    "start": "2716920",
    "end": "2722520"
  },
  {
    "text": "into a destination and for getting data into the service you'll use one of the two",
    "start": "2722520",
    "end": "2728960"
  },
  {
    "text": "put API calls we provide on the F host service site put record or put record",
    "start": "2728960",
    "end": "2734119"
  },
  {
    "text": "badge you'll configure data producer to use these two API calls to push the data",
    "start": "2734119",
    "end": "2741079"
  },
  {
    "text": "into the F host service so you'll run an SDK and and a couple line simple code on",
    "start": "2741079",
    "end": "2747839"
  },
  {
    "text": "the data producer side for example on your ECT instance to push the data",
    "start": "2747839",
    "end": "2752920"
  },
  {
    "text": "continuously into the F host service the next question is there a Java",
    "start": "2752920",
    "end": "2760640"
  },
  {
    "text": "package or something provided by AWS firehost that can be used by the producer so other than the two put API",
    "start": "2760640",
    "end": "2768160"
  },
  {
    "text": "calls that I just mentioned we do offer in Java application we call it Kinesis",
    "start": "2768160",
    "end": "2774760"
  },
  {
    "text": "agent and you can install that on your data producer if it's a Unix",
    "start": "2774760",
    "end": "2780559"
  },
  {
    "text": "instance what Kinesis agent does is it'll monitor a set of local files on",
    "start": "2780559",
    "end": "2787119"
  },
  {
    "text": "the instance or servers and as new data gets written into these files the agent",
    "start": "2787119",
    "end": "2793680"
  },
  {
    "text": "will automatically pick up these new data records pause it and send it into the file host service directly and",
    "start": "2793680",
    "end": "2801319"
  },
  {
    "text": "automatically the agent will also handle file rotation checkpointing if there's a",
    "start": "2801319",
    "end": "2806960"
  },
  {
    "text": "failure on the instance and it can also lock Matrix Cloud wash for to monitor",
    "start": "2806960",
    "end": "2812520"
  },
  {
    "text": "the performance of the agent an example for using the agent",
    "start": "2812520",
    "end": "2817680"
  },
  {
    "text": "which will be particularly useful is for streaming log analytics for example if you have a set",
    "start": "2817680",
    "end": "2825040"
  },
  {
    "text": "of web application servers you can install the agent on your web application",
    "start": "2825040",
    "end": "2830720"
  },
  {
    "text": "server and as the new log line gets generated on the local log file the",
    "start": "2830720",
    "end": "2836640"
  },
  {
    "text": "Kinesis agent can pick up these new data record the agent also has a",
    "start": "2836640",
    "end": "2841960"
  },
  {
    "text": "pre-processing functionality you can configure and convert these log lines into Json format and do filtering and",
    "start": "2841960",
    "end": "2850280"
  },
  {
    "text": "Etc then pass that Json format log line into the fost service automatically then",
    "start": "2850280",
    "end": "2857640"
  },
  {
    "text": "from the fost service you can load that log data in a streaming manner into rare",
    "start": "2857640",
    "end": "2863400"
  },
  {
    "text": "shift or elastic",
    "start": "2863400",
    "end": "2866279"
  },
  {
    "text": "search the next question is how long does data stay in S3 three and is there",
    "start": "2872160",
    "end": "2877760"
  },
  {
    "text": "any automatic archal and Etc so file host delivers the data into",
    "start": "2877760",
    "end": "2884599"
  },
  {
    "text": "S3 bucket that is owned by you and the F host service does not automatically",
    "start": "2884599",
    "end": "2890640"
  },
  {
    "text": "delete objects in the S3 bucket which means if you don't choose to delete the data in the S3 bucket it'll always be",
    "start": "2890640",
    "end": "2898680"
  },
  {
    "text": "there and on the S3 side you can set up life cycle policy to either delete these",
    "start": "2898680",
    "end": "2905480"
  },
  {
    "text": "S3 objects from time to time when the data has been loaded into red shift cluster or you can choose to Archive",
    "start": "2905480",
    "end": "2912280"
  },
  {
    "text": "this S3 data into Glacier for archival purpose and with a much lower cost for",
    "start": "2912280",
    "end": "2918000"
  },
  {
    "text": "storing these data the next question can we use kesa fire hose as a realtime",
    "start": "2918000",
    "end": "2924000"
  },
  {
    "text": "streaming analysis platform for custom anomaly detection and alerts and the",
    "start": "2924000",
    "end": "2929319"
  },
  {
    "text": "answer is yes so other than kesis file host at the Amazon kesa platform we also",
    "start": "2929319",
    "end": "2936240"
  },
  {
    "text": "offer a service called kesis analytics it allows you to run standard",
    "start": "2936240",
    "end": "2941920"
  },
  {
    "text": "SQL against the data stream and then to trigger an alert or downst streaming",
    "start": "2941920",
    "end": "2947799"
  },
  {
    "text": "workflow so a very typical setup that you can use you can stream your data",
    "start": "2947799",
    "end": "2954799"
  },
  {
    "text": "into the F host delivery stream and the F host delivery stream will archive this",
    "start": "2954799",
    "end": "2960480"
  },
  {
    "text": "data into S3 in case you want to keep a copy of this data or for other complex",
    "start": "2960480",
    "end": "2967319"
  },
  {
    "text": "batch oriented an analytics down the stream for example through a Hadoop job against these three objects at the same",
    "start": "2967319",
    "end": "2974599"
  },
  {
    "text": "time you can start an Kinesis analytics application which will run standard SQL",
    "start": "2974599",
    "end": "2981400"
  },
  {
    "text": "continuously against the file host deliv stream for example you can do a count of",
    "start": "2981400",
    "end": "2987839"
  },
  {
    "text": "error during the past 5 10 seconds it will be a sliding window algorithm on the Kinesis",
    "start": "2987839",
    "end": "2995359"
  },
  {
    "text": "analytics side and for example as soon as the number of Errors bleach a certain",
    "start": "2995359",
    "end": "3001319"
  },
  {
    "text": "threshold for the past 5 Seconds it it can trigger an alarm or alert or trigger",
    "start": "3001319",
    "end": "3007040"
  },
  {
    "text": "downstreaming workflow on the Kinesis analytic side and you can use that for",
    "start": "3007040",
    "end": "3012119"
  },
  {
    "text": "the anomaly detection it's very similar to the land architecture people have",
    "start": "3012119",
    "end": "3018160"
  },
  {
    "text": "been talking about where you'll have a fire hold D stream on one hand it'll",
    "start": "3018160",
    "end": "3023359"
  },
  {
    "text": "archive the data to S3 for batch oriented workflow on the other hand we'll have a Kinesis",
    "start": "3023359",
    "end": "3029559"
  },
  {
    "text": "analytics application ta into the F host delivery stream for Real Time anomaly",
    "start": "3029559",
    "end": "3037079"
  },
  {
    "text": "detection I think these are the questions we have for today and we're about um to run out of time I think uh",
    "start": "3037480",
    "end": "3045400"
  },
  {
    "text": "this will concludes our session today thanks everyone for attending this session and I hope you have a good day",
    "start": "3045400",
    "end": "3053720"
  }
]