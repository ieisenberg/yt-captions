[
  {
    "text": "hi everyone my name is Eva say I manag the big data services team at",
    "start": "1280",
    "end": "7480"
  },
  {
    "text": "Netflix today I'm very happy to be here and along with Kurt Brown we're going to talk about how we use Amazon S3 as our",
    "start": "7480",
    "end": "15000"
  },
  {
    "text": "centralized data Hub and through that lens we're going to talk about the big data processing um engines that we use",
    "start": "15000",
    "end": "22119"
  },
  {
    "text": "the big data ecosystem that we built the architecture we choose and the tools and services that we built",
    "start": "22119",
    "end": "30240"
  },
  {
    "text": "so let's start by saying that asri is in the center of our big data universe and",
    "start": "31080",
    "end": "36760"
  },
  {
    "text": "we will use that to weave story about our architecture when we first started using",
    "start": "36760",
    "end": "43360"
  },
  {
    "text": "as3 is a great intuition that we have it is intuitive because it is a cloud",
    "start": "43360",
    "end": "49719"
  },
  {
    "text": "native service it is free engineering although it's not free of course um it",
    "start": "49719",
    "end": "55600"
  },
  {
    "text": "is practically infinitely scalable in terms of storage it is 11 durable and",
    "start": "55600",
    "end": "60719"
  },
  {
    "text": "49's available it also allow us to decouple compute and storage and which",
    "start": "60719",
    "end": "67479"
  },
  {
    "text": "is a great architectural choice and intuition we have and I'll cover more about that",
    "start": "67479",
    "end": "73439"
  },
  {
    "text": "later it is also very counterintuitive there are actually engineers and Netflix questioning why we",
    "start": "73439",
    "end": "79799"
  },
  {
    "text": "choose to do that it is counterintuitive because SRI is eventually",
    "start": "79799",
    "end": "84920"
  },
  {
    "text": "consistent and also there is performance impact S3 nonetheless is a um",
    "start": "84920",
    "end": "91360"
  },
  {
    "text": "cloud-based service that is remote from your cluster so there is performance impact and I'm going to talk a little",
    "start": "91360",
    "end": "97759"
  },
  {
    "text": "bit more about how we do the tradeoffs um in the later",
    "start": "97759",
    "end": "103079"
  },
  {
    "text": "slides so our current data Hub scales is we currently have 60 petabyte data with",
    "start": "103079",
    "end": "108759"
  },
  {
    "text": "1.5 billion objects and more um in terms of data velocity on a",
    "start": "108759",
    "end": "116159"
  },
  {
    "text": "daily basis we ingest about a ter 100 terabyte of data we read about 3.5 terabyte daily in our",
    "start": "116159",
    "end": "123119"
  },
  {
    "text": "EPL processing then we write back 500 terabytes and we also expire about 400",
    "start": "123119",
    "end": "130720"
  },
  {
    "text": "terabyte a day so as you can see there's a lot of data that's turning through our data hub on a daily",
    "start": "130720",
    "end": "137200"
  },
  {
    "text": "basis there are two main data ingestion pipeline in our data platform the first",
    "start": "137200",
    "end": "142760"
  },
  {
    "text": "one is the event data pipeline these are business events these are generated when you search for a title or when you click",
    "start": "142760",
    "end": "149440"
  },
  {
    "text": "play on a title we collect around 500 billions of",
    "start": "149440",
    "end": "154480"
  },
  {
    "text": "them per day and it has a 5 minut SLA to arrive in our data",
    "start": "154480",
    "end": "160959"
  },
  {
    "text": "Hub this is a rough architecture of our event data pipeline Netflix is hosted on",
    "start": "161080",
    "end": "167080"
  },
  {
    "text": "three different regions in each region there is a CFA data pipeline Cloud application is going to send the event",
    "start": "167080",
    "end": "173560"
  },
  {
    "text": "to the Kafka data Pipeline and then the Kafka data pipeline cluster is going to write these events in files",
    "start": "173560",
    "end": "180760"
  },
  {
    "text": "into the regional S3 buckets after it has done that it's also going to issue a",
    "start": "180760",
    "end": "186640"
  },
  {
    "text": "um message on the sqs Q to put the key name on",
    "start": "186640",
    "end": "191680"
  },
  {
    "text": "there Downstream is the Ursa system which is the event ingestion pipeline",
    "start": "191680",
    "end": "196760"
  },
  {
    "text": "that the Big Data platform built so what it does is basically consume the sqs events across all three region and read",
    "start": "196760",
    "end": "203640"
  },
  {
    "text": "theile directly from Sri in this architecture it allows urso to read the",
    "start": "203640",
    "end": "208920"
  },
  {
    "text": "file much sooner in a more near realtime fashion because Ursa is reading around six million files a day across all the",
    "start": "208920",
    "end": "215799"
  },
  {
    "text": "different regions um it also allow Ursa not leading to list SRI continuously to",
    "start": "215799",
    "end": "221760"
  },
  {
    "text": "discover new files and also not need to handle eventual consistency issue so",
    "start": "221760",
    "end": "227439"
  },
  {
    "text": "basically at this point what URS does is to group and merge these events based on",
    "start": "227439",
    "end": "232560"
  },
  {
    "text": "Kafka topic and then publish it onto the data Hub so when an Iran um started from",
    "start": "232560",
    "end": "238079"
  },
  {
    "text": "the cloud application to the point that it arrived to data Hub it's around 5",
    "start": "238079",
    "end": "243840"
  },
  {
    "text": "minutes so there is a second data pipeline that's a dimension data pipeline um some of you might know at",
    "start": "243840",
    "end": "250760"
  },
  {
    "text": "Netflix we use Cassandra as our online data store um we store stateful data in",
    "start": "250760",
    "end": "256040"
  },
  {
    "text": "Cassandra these are data like subscriber data we have over a 100 um Cassandra",
    "start": "256040",
    "end": "261720"
  },
  {
    "text": "cluster for one for each data set and we extract from about 40 of them as",
    "start": "261720",
    "end": "267680"
  },
  {
    "text": "Dimension data and put it in the data house um we do da extracts and for the more",
    "start": "267680",
    "end": "274240"
  },
  {
    "text": "time sensitive cluster or information we do every 4 hours or 6 hours data",
    "start": "274240",
    "end": "280520"
  },
  {
    "text": "extract again this is a high level architecture of what it looks like um as",
    "start": "280520",
    "end": "285800"
  },
  {
    "text": "I mentioned um there is one cassendra cluster for each data set is replicated across three different regions the",
    "start": "285800",
    "end": "292479"
  },
  {
    "text": "cassendra cluster is going to write incremental backup file as well as snapshot in SS table format onto the",
    "start": "292479",
    "end": "300120"
  },
  {
    "text": "regional S3 bucket Downstream is the ages uh system that we built um it is a",
    "start": "300120",
    "end": "307039"
  },
  {
    "text": "bulk data extraction pipeline it is also open source on Netflix OSS and what it",
    "start": "307039",
    "end": "312680"
  },
  {
    "text": "does is basically process these SS table from a preferred region generate the and",
    "start": "312680",
    "end": "318080"
  },
  {
    "text": "do transform generate a dimension table and put it into the data Hub in case the backup tables uh the",
    "start": "318080",
    "end": "324560"
  },
  {
    "text": "backup SS table is not available in the preferred region a this can also consume",
    "start": "324560",
    "end": "330280"
  },
  {
    "text": "from an alternate region and do the data",
    "start": "330280",
    "end": "334919"
  },
  {
    "text": "extract so in this architecture you can see that agus is actually listing these",
    "start": "335800",
    "end": "341520"
  },
  {
    "text": "files on S3 and the reason why we do it is because agus is doing daily extract",
    "start": "341520",
    "end": "347280"
  },
  {
    "text": "um and also there's not as many files as the event pipeline however we are working on the second version of agus",
    "start": "347280",
    "end": "353919"
  },
  {
    "text": "right now to do the event uh publish notification framework just like URS and",
    "start": "353919",
    "end": "359120"
  },
  {
    "text": "the reason is we want to decouple the um the Upstream um SS table generation from",
    "start": "359120",
    "end": "364639"
  },
  {
    "text": "the downstream consumption uh model so let's take a look at our",
    "start": "364639",
    "end": "370880"
  },
  {
    "text": "transform and data processing architecture so we have two rationale when we do the um architectural Choice",
    "start": "370880",
    "end": "377000"
  },
  {
    "text": "once the first one is that data is gold we can afford to lose a clusters or",
    "start": "377000",
    "end": "383160"
  },
  {
    "text": "clusters um but we cannot afford to lose any data the second ration now is that there",
    "start": "383160",
    "end": "390560"
  },
  {
    "text": "is no one siiz fit all in the big data processing World some engine is good for",
    "start": "390560",
    "end": "396280"
  },
  {
    "text": "low latency some are good for high data throughput so we want to have an",
    "start": "396280",
    "end": "402280"
  },
  {
    "text": "architecture that allow all these different engines to interoperate on the same set of data and let our users",
    "start": "402280",
    "end": "408919"
  },
  {
    "text": "choose the right engine based on the SLA and the constraints that they have in",
    "start": "408919",
    "end": "414280"
  },
  {
    "text": "mind so here's a rough high level architecture of our data",
    "start": "414280",
    "end": "420759"
  },
  {
    "text": "processing engines so Hive and pig are for ad hoc and ETL processing two years",
    "start": "420759",
    "end": "428520"
  },
  {
    "text": "ago we introduced Presto in our architecture it is for interactive at hog querying so Presto today already",
    "start": "428520",
    "end": "436800"
  },
  {
    "text": "largely overtake the hi use case at the beginning of this year we introduced spark and we're hoping that",
    "start": "436800",
    "end": "443879"
  },
  {
    "text": "spark will be eventually the ETL engine of choice and overtake some of the pck use cases and all of the pick use cases",
    "start": "443879",
    "end": "451800"
  },
  {
    "text": "eventually and Spark also support machine learning algorithm so it would allow our platform to actually include",
    "start": "451800",
    "end": "459199"
  },
  {
    "text": "additional use cases currently hi pick spark are all running on top of Hadoop",
    "start": "459199",
    "end": "464720"
  },
  {
    "text": "yarn clusters we are running Amazon EMR clusters prestor is directly running on top of",
    "start": "464720",
    "end": "471919"
  },
  {
    "text": "easy2 um we have a couple of really large hadyan clusters together they are",
    "start": "472400",
    "end": "478360"
  },
  {
    "text": "around 3500 d24 XL for Presto we also have a couple of clusters and together",
    "start": "478360",
    "end": "485159"
  },
  {
    "text": "we have around 250 to 400 R34 XL it is a",
    "start": "485159",
    "end": "490240"
  },
  {
    "text": "range because um we do expand and shrink the cluster uh the Presto cluster based",
    "start": "490240",
    "end": "495759"
  },
  {
    "text": "on availability of spare capacity in the Netflix accounts environment so as you",
    "start": "495759",
    "end": "502039"
  },
  {
    "text": "can see in this architecture um all these engine can interoperate with the same data set in the data Hub so it's",
    "start": "502039",
    "end": "508280"
  },
  {
    "text": "very flexible but this architecture also makes sense from the scalability",
    "start": "508280",
    "end": "513800"
  },
  {
    "text": "perspective so if we need to store 60 paby of data in d24 XL which is a very",
    "start": "513800",
    "end": "519560"
  },
  {
    "text": "beefy storage machine already we need around 2500 of them and if we want to achieve three-way",
    "start": "519560",
    "end": "527360"
  },
  {
    "text": "replication to have residency in just one zone we would need to double the",
    "start": "527360",
    "end": "533000"
  },
  {
    "text": "cluster size the Hado cluster size we have so ultimately this architecture",
    "start": "533000",
    "end": "538640"
  },
  {
    "text": "also allow us to scale compute and storage independently at their own",
    "start": "538640",
    "end": "545240"
  },
  {
    "text": "pace so at this point let's address the elephant in the room so how did we",
    "start": "545920",
    "end": "552200"
  },
  {
    "text": "handle the tradeoffs of eventual consistency and the performance impact",
    "start": "552200",
    "end": "557920"
  },
  {
    "text": "and I like to share some of the thoughts that we have around the performance um that we're",
    "start": "557920",
    "end": "563959"
  },
  {
    "text": "seeing in terms of eventual consistency updates is one of the eventual consist",
    "start": "563959",
    "end": "569440"
  },
  {
    "text": "an operation in S3 um we work around it by completely spping it so whenever we",
    "start": "569440",
    "end": "575720"
  },
  {
    "text": "need to update data we create a new key meaning a new file name and then delete",
    "start": "575720",
    "end": "581079"
  },
  {
    "text": "the old one by doing that we're only creating new puts in S3 which is a",
    "start": "581079",
    "end": "586279"
  },
  {
    "text": "strongly consistent operation list is another very important operation for us um it is also a",
    "start": "586279",
    "end": "594200"
  },
  {
    "text": "eventual consistent operation what we need to know is when we do a listing we",
    "start": "594200",
    "end": "599519"
  },
  {
    "text": "include all the files that we need and also not including files that we deleted",
    "start": "599519",
    "end": "605519"
  },
  {
    "text": "right because listing could list file that has been deleted so what we did is two years ago we implemented samper",
    "start": "605519",
    "end": "612120"
  },
  {
    "text": "which is a system that we open source in Netflix OSS what it allow us to do is to",
    "start": "612120",
    "end": "617399"
  },
  {
    "text": "keep track of the files or Manifesto files for the prefix that are generated",
    "start": "617399",
    "end": "623399"
  },
  {
    "text": "by Upstream job so that Downstream job know exactly what to look for and expect",
    "start": "623399",
    "end": "629600"
  },
  {
    "text": "and we store that manifest in Dynamo DB um subsequently Amazon EMR also",
    "start": "629600",
    "end": "636079"
  },
  {
    "text": "implements something very similar called EMR FS so in case any of you are using",
    "start": "636079",
    "end": "641680"
  },
  {
    "text": "um Amazon EMR already um listing consistency should not be an",
    "start": "641680",
    "end": "648800"
  },
  {
    "text": "issue and then I'm going to talk about performance um before I talk about",
    "start": "649680",
    "end": "654720"
  },
  {
    "text": "performance I want to mention that we're using paret file format um and and it is",
    "start": "654720",
    "end": "660320"
  },
  {
    "text": "paret file format is a aachi open source project it's interoperable with all the engines that we choose um we have",
    "start": "660320",
    "end": "667000"
  },
  {
    "text": "engineers in the team to help improve paret efficiency and performance so that",
    "start": "667000",
    "end": "673240"
  },
  {
    "text": "we could close the gap of um S3 performance versus reading or or hdfs",
    "start": "673240",
    "end": "680000"
  },
  {
    "text": "the performance of hdfs on local disk and all the performance number or",
    "start": "680000",
    "end": "687800"
  },
  {
    "text": "performance thoughts that I'm going to is based on the fact that we are using paret file format so one of the main",
    "start": "687800",
    "end": "694360"
  },
  {
    "text": "benefits for paret is uh reperformance both column projections",
    "start": "694360",
    "end": "700680"
  },
  {
    "text": "and predicate push down um means to reduce the amount of data that we need to pull over the wire when we read from",
    "start": "700680",
    "end": "707600"
  },
  {
    "text": "M3 paret is a columnal file format so it allow column projection if you are",
    "start": "707600",
    "end": "713079"
  },
  {
    "text": "selecting one out of 50 column it only read one column over the wire similarly",
    "start": "713079",
    "end": "718320"
  },
  {
    "text": "paret also group uh columns into row groups at the end of the row groups it would have statistics so in case you are",
    "start": "718320",
    "end": "725639"
  },
  {
    "text": "selecting data B based on some predicate condition and it would and if it cannot",
    "start": "725639",
    "end": "730959"
  },
  {
    "text": "find these predicate values in the road group section based on the statistics what it would do is also skipping",
    "start": "730959",
    "end": "737440"
  },
  {
    "text": "complet completely skipping the road group so these are tactics that it would",
    "start": "737440",
    "end": "742519"
  },
  {
    "text": "help us to actually reduce the amount of data that we read over the wire from Mastery it also supports theorized read",
    "start": "742519",
    "end": "749720"
  },
  {
    "text": "what it means is when it read a whole column it could read it in batch instead of reading it row by row so it reduce",
    "start": "749720",
    "end": "756399"
  },
  {
    "text": "the number of seats and round trips to go to as3 as well so even given the performance uh",
    "start": "756399",
    "end": "764240"
  },
  {
    "text": "even given the fact that we are using paret We are continuing improving it we're still seeing performance impact um",
    "start": "764240",
    "end": "771000"
  },
  {
    "text": "so I'm going to share some of our thoughts around that as well as some of the things that we're thinking to",
    "start": "771000",
    "end": "777000"
  },
  {
    "text": "continue to improve on to narrow that Gap in terms of read performance of",
    "start": "777000",
    "end": "783440"
  },
  {
    "text": "course it comes from throughput and latency impact right um for simple queries let's say you're doing a count",
    "start": "783440",
    "end": "790360"
  },
  {
    "text": "star round trip going to asri dominates latency dominates that the time that you",
    "start": "790360",
    "end": "795959"
  },
  {
    "text": "spend on the query because in paret file all you need to do is to read the footer and get the count um in those cases qu",
    "start": "795959",
    "end": "803399"
  },
  {
    "text": "run fast however the performance impact in terms of percentage is high on the",
    "start": "803399",
    "end": "809160"
  },
  {
    "text": "the other hand if you're running an ETL job that you're selecting 40 columns out of 45 columns you're pulling a lot of",
    "start": "809160",
    "end": "815279"
  },
  {
    "text": "data over the wire from as3 um the the performance difference is huge however",
    "start": "815279",
    "end": "821800"
  },
  {
    "text": "your job is also going to take a lot longer to run because you need to deserialize the data you're likely doing a lot of complex computation so at the",
    "start": "821800",
    "end": "829240"
  },
  {
    "text": "end of the day the performance impact in terms of percentage is actually",
    "start": "829240",
    "end": "834880"
  },
  {
    "text": "lower and one of the Improvement that we can still improve read performance is to improve",
    "start": "835639",
    "end": "841920"
  },
  {
    "text": "the I manager in paret what we want to do is to do parallel read on column",
    "start": "841920",
    "end": "847240"
  },
  {
    "text": "chunks as well as um reading adjacent columns together to reduce the number of",
    "start": "847240",
    "end": "852360"
  },
  {
    "text": "seeks so these are things that we are thinking about doing to improve in paret file format in order to close the",
    "start": "852360",
    "end": "860440"
  },
  {
    "text": "gap in terms of right performance if you could think about big data processing job what it does is it has a lot of",
    "start": "860440",
    "end": "866639"
  },
  {
    "text": "parallel task all processing different dat sets they're writing output into local files when the master of the or",
    "start": "866639",
    "end": "874320"
  },
  {
    "text": "the client know that all the tasks have finished and done is going to do a commit during the commit logic what it",
    "start": "874320",
    "end": "881480"
  },
  {
    "text": "does is each of the parallel tasks is going to do a rename to put the file into the final location there output",
    "start": "881480",
    "end": "887959"
  },
  {
    "text": "files in the final location but as does not support rename so what happened",
    "start": "887959",
    "end": "893519"
  },
  {
    "text": "um so what happened is in these big data processing engine they write to local dis before uploading to S3 at the very",
    "start": "893519",
    "end": "900519"
  },
  {
    "text": "end and introducing an extra hop and latency so what one thing very obvious",
    "start": "900519",
    "end": "906399"
  },
  {
    "text": "that we could do is to improve the commit Logic for these big data engine to do direct writing onto S3 and we'll",
    "start": "906399",
    "end": "913519"
  },
  {
    "text": "Leverage The semantics of how multiart upload works on as3 in which if the jobs",
    "start": "913519",
    "end": "919440"
  },
  {
    "text": "actually failed at the very end um we could actually abort multiart upload across all the parallel task so no files",
    "start": "919440",
    "end": "927680"
  },
  {
    "text": "will be written on S3 and is still consistent um in terms of listing the",
    "start": "927680",
    "end": "935360"
  },
  {
    "text": "biggest panalty come from when the job startup time in a big data processing job um if you could think about in the",
    "start": "935360",
    "end": "941880"
  },
  {
    "text": "big data processing job at the job startup time what we do is do a listing to find out all the files we need to do",
    "start": "941880",
    "end": "948240"
  },
  {
    "text": "read as input right and the client is going to list all the file look at the size of all the files and do a split",
    "start": "948240",
    "end": "955199"
  },
  {
    "text": "calculation to do that is actually make a lot of round trip to asre to list out",
    "start": "955199",
    "end": "960399"
  },
  {
    "text": "all the files that it needs to read and usually it need to do one list one listing per partition and in big ETL job",
    "start": "960399",
    "end": "967600"
  },
  {
    "text": "in our environment we could do be doing thousands of them so job starter time is",
    "start": "967600",
    "end": "972680"
  },
  {
    "text": "usually a bottleneck for us uh because of the listing latency so what we could do and this is",
    "start": "972680",
    "end": "980040"
  },
  {
    "text": "something that we're thinking right now is instead of tracking prefixes for these partitions in the metadata we're",
    "start": "980040",
    "end": "985600"
  },
  {
    "text": "tracking the actual files if we do that we could completely skip the necessity to do any listing so these are some of",
    "start": "985600",
    "end": "992399"
  },
  {
    "text": "the things that we are thinking about to continue to improve to close the gap to",
    "start": "992399",
    "end": "997880"
  },
  {
    "text": "summarize for big ETL jobs in our environment is very CPU bound and",
    "start": "997880",
    "end": "1003920"
  },
  {
    "text": "performance when you compare using S3 versus using hdfs on local dis converges",
    "start": "1003920",
    "end": "1009959"
  },
  {
    "text": "as the complexity and the data volume increase for interactive queries by the",
    "start": "1009959",
    "end": "1016120"
  },
  {
    "text": "virtue of the fact that interactive query you read a lot less data so",
    "start": "1016120",
    "end": "1022360"
  },
  {
    "text": "um mostly it is the latency that is dominating so the percentage of impact",
    "start": "1022360",
    "end": "1027880"
  },
  {
    "text": "is higher but these jobs run really fast in our environment the median execution",
    "start": "1027880",
    "end": "1033600"
  },
  {
    "text": "time for interactive prestal query is 3 seconds so we are not talking about a",
    "start": "1033600",
    "end": "1039280"
  },
  {
    "text": "lot of weight time so at the end of the day all things consider all the work we have done we",
    "start": "1039280",
    "end": "1045640"
  },
  {
    "text": "still think the benefits of using asry outweight the cost and that's why we're here today to talk",
    "start": "1045640",
    "end": "1051640"
  },
  {
    "text": "about our experience so let's talk about how we",
    "start": "1051640",
    "end": "1056919"
  },
  {
    "text": "manage um the four different engines that we have different versions of them",
    "start": "1056919",
    "end": "1062280"
  },
  {
    "text": "are running across let's say 10 different customer how do we manage that combination and what are the most common",
    "start": "1062280",
    "end": "1068559"
  },
  {
    "text": "problems or or questions from our users um they want to know where they can run",
    "start": "1068559",
    "end": "1074080"
  },
  {
    "text": "the job should they run it run the client on the laptop if so what version of the tool tool should they install um",
    "start": "1074080",
    "end": "1082039"
  },
  {
    "text": "which custom should they pick to run their high priority job or the backfield job um they just they don't care where",
    "start": "1082039",
    "end": "1088720"
  },
  {
    "text": "you run the job they just want to see all the jobs in one place from administrative perspective",
    "start": "1088720",
    "end": "1094400"
  },
  {
    "text": "like people in the Big Data team they want to be able to know exactly what",
    "start": "1094400",
    "end": "1099799"
  },
  {
    "text": "version of what to is installed in which cluster and they also want to have the",
    "start": "1099799",
    "end": "1105080"
  },
  {
    "text": "freedom to be able to swap out cluster and do deployment and upgrade without user",
    "start": "1105080",
    "end": "1111880"
  },
  {
    "text": "impact so with that we implemented and designed and implemented Genie um which",
    "start": "1111880",
    "end": "1117400"
  },
  {
    "text": "is a job and cluster Management Service um it solves all of the above problem it",
    "start": "1117400",
    "end": "1124240"
  },
  {
    "text": "allow user to discover the cluster to run the job and also act as a Gateway",
    "start": "1124240",
    "end": "1129360"
  },
  {
    "text": "for them to run the job on so that they don't need to run it on their own machine and also it's a One-Stop shop",
    "start": "1129360",
    "end": "1134919"
  },
  {
    "text": "for them to see all the jobs that they've run um from ad min trated perspective it is obviously a",
    "start": "1134919",
    "end": "1140600"
  },
  {
    "text": "configuration management tool it manages all the different tools and versions across all the Clusters it also allow us",
    "start": "1140600",
    "end": "1147200"
  },
  {
    "text": "to do very easy deployment so at Netflix whenever we need to upgrade a cluster what we do is we spin up a new cluster",
    "start": "1147200",
    "end": "1155520"
  },
  {
    "text": "register it with Genie mark it as up so that users when they submit job it would go to the new cluster then we Mark the",
    "start": "1155520",
    "end": "1162919"
  },
  {
    "text": "old cluster is out of service job will continue to run and we'll wait for it to finish when it's done we'll shut down",
    "start": "1162919",
    "end": "1168919"
  },
  {
    "text": "the old",
    "start": "1168919",
    "end": "1171279"
  },
  {
    "text": "cluster so everyone is going through Genie to get access to the cluster and",
    "start": "1174000",
    "end": "1179679"
  },
  {
    "text": "Genie also use S3 we use S3 to uh store the archive job output files for 90 days",
    "start": "1179679",
    "end": "1186480"
  },
  {
    "text": "so that people can go back and see also it is the place where we put all the job",
    "start": "1186480",
    "end": "1192280"
  },
  {
    "text": "executables tools or engines cluster configurations so that it can be shared",
    "start": "1192280",
    "end": "1198039"
  },
  {
    "text": "across all the clusters Genie is definitely open source",
    "start": "1198039",
    "end": "1203240"
  },
  {
    "text": "on Netflix OSS it's been open source for two three years now um we just released",
    "start": "1203240",
    "end": "1209080"
  },
  {
    "text": "3.0 internally at Netflix and it's running in production for two months now so if you have a similar problem like we",
    "start": "1209080",
    "end": "1217760"
  },
  {
    "text": "do have a complex system architecture I highly encourage you to take a look at Genie we are going to publicly um",
    "start": "1217760",
    "end": "1224960"
  },
  {
    "text": "announce 3.0 um towards end of this year or beginning off next year and the only",
    "start": "1224960",
    "end": "1230600"
  },
  {
    "text": "thing that's holding us up right now is our favorite part of engineering which is documentation other than that you can",
    "start": "1230600",
    "end": "1238120"
  },
  {
    "text": "use it so we've been talking a lot about the object size how many objects we have an",
    "start": "1238120",
    "end": "1245720"
  },
  {
    "text": "a um how do we manage them what is the brain behind it so for all the data that",
    "start": "1245720",
    "end": "1252360"
  },
  {
    "text": "we store on as3 we use hi metast store to store the metadata for the data on S3",
    "start": "1252360",
    "end": "1258360"
  },
  {
    "text": "data Hub it works well it's into operates with all the four different engines that",
    "start": "1258360",
    "end": "1264039"
  },
  {
    "text": "we choose but we didn't stop there we actually built a centralized metadata",
    "start": "1264039",
    "end": "1269360"
  },
  {
    "text": "service called medicat so that all the four different engines can interoperate",
    "start": "1269360",
    "end": "1274640"
  },
  {
    "text": "with medicat and proxy the request back into hi meta store so what is medicat it",
    "start": "1274640",
    "end": "1281279"
  },
  {
    "text": "is a Federated metadata service it is a proxy across all the data sources we",
    "start": "1281279",
    "end": "1286440"
  },
  {
    "text": "have and the keyword is proxy medicad does not actually store the metadata for the different data sources",
    "start": "1286440",
    "end": "1293320"
  },
  {
    "text": "it's just a proxy back so medicat obviously proxy to proxy the U request",
    "start": "1293320",
    "end": "1299679"
  },
  {
    "text": "to hi meta store there are three additional data stores that we use so it proxy the request fact to R shift",
    "start": "1299679",
    "end": "1305919"
  },
  {
    "text": "teradata and Amazon RDS as well so why did we build medicat it",
    "start": "1305919",
    "end": "1313880"
  },
  {
    "text": "allow us to provide a common set of API for all the tools and applications that we buil in our platform um Kurt is going",
    "start": "1313880",
    "end": "1321720"
  },
  {
    "text": "to cover more about the applications and tools that we built in the Big Data ecosystems that we have um it's also",
    "start": "1321720",
    "end": "1329039"
  },
  {
    "text": "support Thrift API obviously so that all the four different big data processing engine can interoperate with",
    "start": "1329039",
    "end": "1336080"
  },
  {
    "text": "it it allow us to do metadata Discovery across all the different data",
    "start": "1336080",
    "end": "1341440"
  },
  {
    "text": "sources um the single most important reason why we do it is also because um",
    "start": "1341440",
    "end": "1347200"
  },
  {
    "text": "it allow us to put additional business context across all the data sources for example we can put life cycle policy",
    "start": "1347200",
    "end": "1353960"
  },
  {
    "text": "across all of them we can t the different table put additional metadata we can put in userdefined custom metrics",
    "start": "1353960",
    "end": "1361480"
  },
  {
    "text": "across um different data sources and medicad currently is um",
    "start": "1361480",
    "end": "1368480"
  },
  {
    "text": "checked in into Netflix Open Source GitHub project we do have planed to open source it next year but I don't have a",
    "start": "1368480",
    "end": "1375640"
  },
  {
    "text": "specific date at this point so how do we manage the data life cycle",
    "start": "1375640",
    "end": "1380919"
  },
  {
    "text": "we we expire 400 terabytes right how do we do that unfortunately we cannot leverage the S3 life cycle policy",
    "start": "1380919",
    "end": "1388320"
  },
  {
    "text": "because we have more complex business rules so we end up building a set of gen genitor tools much like the jator monkey",
    "start": "1388320",
    "end": "1395279"
  },
  {
    "text": "we built to clean out ec2 instances and EBS volumes so we delete dangling data after",
    "start": "1395279",
    "end": "1402159"
  },
  {
    "text": "60 day what is dangling data dangling data are data in the data Hub that are not referenced by The Hive meta store",
    "start": "1402159",
    "end": "1408559"
  },
  {
    "text": "these are dangling data um we delete it after 60 days and there is a special",
    "start": "1408559",
    "end": "1413960"
  },
  {
    "text": "case which is when we do data updates we create new keys and delete the old data that I mentioned earlier in those cases",
    "start": "1413960",
    "end": "1421039"
  },
  {
    "text": "we delete it a lot sooner we delete after 30 days and then of course the other twool basically delete the um",
    "start": "1421039",
    "end": "1428880"
  },
  {
    "text": "table petitions based on the table TTL",
    "start": "1428880",
    "end": "1433400"
  },
  {
    "text": "policy but our delet patents is pretty spiky if you look at this chart on One Fine Day we're deleting 20 million",
    "start": "1435240",
    "end": "1441760"
  },
  {
    "text": "objects and actually it didn't happen on that day it happened in the span of probably 10 minutes or 5 minutes when",
    "start": "1441760",
    "end": "1448919"
  },
  {
    "text": "the genitor TOs is executed when a genitor to is executed maybe somebody changed the TTL yesterday to reduce it",
    "start": "1448919",
    "end": "1455720"
  },
  {
    "text": "or somebody decided to drop a table so we need to handle scaling as3",
    "start": "1455720",
    "end": "1462240"
  },
  {
    "text": "deletes and what we did is we buil a deletion service um it is a",
    "start": "1462240",
    "end": "1469360"
  },
  {
    "text": "centralized service to handle errors coming back with from as3 when we do a lot of deletes we receive 503 from S3",
    "start": "1469360",
    "end": "1477360"
  },
  {
    "text": "asking us to back off and so we do exponential back off in the service it",
    "start": "1477360",
    "end": "1482600"
  },
  {
    "text": "also has a very cool feature called cool down period so whenever users want to um",
    "start": "1482600",
    "end": "1488799"
  },
  {
    "text": "delete um partitions what we do is we delete the metadata first let the data stay in the uh data hub for a few more",
    "start": "1488799",
    "end": "1497600"
  },
  {
    "text": "days um in case they regret it they just need to recover the metadata and don't need to recover the",
    "start": "1497600",
    "end": "1504399"
  },
  {
    "text": "data and of course having one centralized service allow us to store history and statistics and also allow us",
    "start": "1504399",
    "end": "1511480"
  },
  {
    "text": "to have easy recovery in case we do need to recover the data back from the as3 data",
    "start": "1511480",
    "end": "1518440"
  },
  {
    "text": "Hub so with that I'm going to hand the Baton over to Kurt to",
    "start": "1518440",
    "end": "1524120"
  },
  {
    "text": "continue thank you very much or the clicker in this case is the Baton um so I'm going to continue our journey",
    "start": "1524120",
    "end": "1532039"
  },
  {
    "text": "into our big data ecosystem through the lens of S3 I'm also going to throw in some tech philosophy to hopefully keep",
    "start": "1532039",
    "end": "1537760"
  },
  {
    "text": "you guys awake during this process but I'm going to start with a very glamorous topic of backup strategy to to really",
    "start": "1537760",
    "end": "1543679"
  },
  {
    "text": "keep you riveted so for starters like how do you back up 60 pedabytes of data",
    "start": "1543679",
    "end": "1549240"
  },
  {
    "text": "that's pretty challenging uh fortunately core S3 provides a lot of the capabilities out of the box so we use",
    "start": "1549240",
    "end": "1556440"
  },
  {
    "text": "version buckets for just about all of data that will do soft deletes so if somebody regrets later on like oops I",
    "start": "1556440",
    "end": "1563279"
  },
  {
    "text": "made a mistake after the few days of the cool down period that Eva mentioned we can go back to about 20 days for most of",
    "start": "1563279",
    "end": "1569480"
  },
  {
    "text": "our data um we have we've created a tool that lets us easily recover the data from S3 and it's a little bit more",
    "start": "1569480",
    "end": "1575480"
  },
  {
    "text": "manual to actually get the hive metadata back and the medic medicat metadata back we have to do a restore of those",
    "start": "1575480",
    "end": "1580840"
  },
  {
    "text": "databases and it seems like well why don't we just build a tool for that and we probably will at some point but just",
    "start": "1580840",
    "end": "1586559"
  },
  {
    "text": "the effort versus the value is not there there so I guess that's philosophy number one like you know don't don't",
    "start": "1586559",
    "end": "1592039"
  },
  {
    "text": "over engineer until you need it um as far as UH 60 pedabytes the",
    "start": "1592039",
    "end": "1597600"
  },
  {
    "text": "beauty of S3 is it's implicitly handles the scale not only for the active data but also for the the soft deleted data",
    "start": "1597600",
    "end": "1605240"
  },
  {
    "text": "and I used to manage um a lot of stuff in a data center a lot of like MPP huge",
    "start": "1605240",
    "end": "1610399"
  },
  {
    "text": "databases huge meaning like hundreds of terabytes and um it was just so painful",
    "start": "1610399",
    "end": "1615720"
  },
  {
    "text": "backups strategy for that I mean you have through group of considerations you have pre-provisioning you got to get",
    "start": "1615720",
    "end": "1621240"
  },
  {
    "text": "that right you have like flaky tapes and you don't have any of this with S3 you have 11 NES of dur durability just",
    "start": "1621240",
    "end": "1627760"
  },
  {
    "text": "waiting for you so it takes care of it's just simple you don't even have to think about it but you know in and of itself",
    "start": "1627760",
    "end": "1634480"
  },
  {
    "text": "we don't want to just keep data in S3 so what about our really really important data well interestingly a sort of",
    "start": "1634480",
    "end": "1641360"
  },
  {
    "text": "secondary effect of the fact that we use other data stores and other data processing engines we also have some of",
    "start": "1641360",
    "end": "1647120"
  },
  {
    "text": "our really important data and other data stores for fast analysis so two examples are red shift and Druid which I'll cover",
    "start": "1647120",
    "end": "1653720"
  },
  {
    "text": "later on we also sh don't tell anyone have a heterogeneous Cloud platform",
    "start": "1653720",
    "end": "1658760"
  },
  {
    "text": "despite being in an AWS conference so we do put a little bit of data into the Google Cloud so this is our really",
    "start": "1658760",
    "end": "1665799"
  },
  {
    "text": "important customer type data it's encrypted but if we really had a disaster like a huge huge disaster which",
    "start": "1665799",
    "end": "1671840"
  },
  {
    "text": "would never happened at Amazon then we'd have some way to get it back um and we've also been dabbling",
    "start": "1671840",
    "end": "1678000"
  },
  {
    "text": "with cross Regional replication and working with the S3 team at at Amazon quite a bit to try to figure out how can",
    "start": "1678000",
    "end": "1683799"
  },
  {
    "text": "we really turn this into a fullscale disaster recovery solution and we are using CR just a little bit on my team",
    "start": "1683799",
    "end": "1690039"
  },
  {
    "text": "right now but um not currently for the the massive 60 pedabytes",
    "start": "1690039",
    "end": "1695720"
  },
  {
    "text": "on the data accessibility front so our general philosophy is really simple is",
    "start": "1695720",
    "end": "1701279"
  },
  {
    "text": "that we just like to keep everything open in the data warehouse which unfortunately probably for a lot of you",
    "start": "1701279",
    "end": "1706360"
  },
  {
    "text": "is not an option you might have to get down to like row levels and columns and this person in finance can see this and",
    "start": "1706360",
    "end": "1711519"
  },
  {
    "text": "this other team can't we don't have any of that everybody can see everything in the data warehouse and the reason we do",
    "start": "1711519",
    "end": "1717919"
  },
  {
    "text": "that is that this data is gold that Eva talked about before like we want to get that gold out in everyone's hands not",
    "start": "1717919",
    "end": "1724480"
  },
  {
    "text": "spend all this time with security policies well how can we do that well the way we are able to have an open open",
    "start": "1724480",
    "end": "1731799"
  },
  {
    "text": "window I guess is to have a closed door I don't know if that's the best analogy um but what we do is we just keep",
    "start": "1731799",
    "end": "1737640"
  },
  {
    "text": "personal identifiable information out of our data warehouse there is some analytic value in terms of someone's",
    "start": "1737640",
    "end": "1743159"
  },
  {
    "text": "address but when we weigh the benefits of an open system versus this limited set of data with limited use cases we'd",
    "start": "1743159",
    "end": "1750279"
  },
  {
    "text": "rather just keep it open and keep that data out altoe we do have some Provisions though some automated checks",
    "start": "1750279",
    "end": "1756320"
  },
  {
    "text": "and even some manual checks to make sure that nothing seeps into the data warehouse over time through that massive pipeline that even talked",
    "start": "1756320",
    "end": "1763679"
  },
  {
    "text": "about on the data track tracking front we have a multivaried approach to how we track our data um we have a",
    "start": "1763679",
    "end": "1770679"
  },
  {
    "text": "lot of automated ways to say who is doing what in the system but another philosophical principle is often we just",
    "start": "1770679",
    "end": "1777640"
  },
  {
    "text": "count on people to tell us who they are which sounds again like you know a little strange can you count on people",
    "start": "1777640",
    "end": "1783640"
  },
  {
    "text": "to like properly tagged their data if someone runs a genie job like who are they and in that Genie job they can",
    "start": "1783640",
    "end": "1790440"
  },
  {
    "text": "specify the right user that that we can then tie into reports that have proper business context if it's like user X I",
    "start": "1790440",
    "end": "1797320"
  },
  {
    "text": "don't know what that means me if it's the marketing data analytics team that's running this job I know who's you know",
    "start": "1797320",
    "end": "1803279"
  },
  {
    "text": "neck to to strangle when the system is is going haywire um and where do we put the data",
    "start": "1803279",
    "end": "1809399"
  },
  {
    "text": "so um whenever you whenever we run these data processing jobs and we put data into S3 uh the user agent we fill in all",
    "start": "1809399",
    "end": "1815919"
  },
  {
    "text": "sorts of Rich information so that we can track down who's actually writing this data so we have things like the job ID",
    "start": "1815919",
    "end": "1820960"
  },
  {
    "text": "and the cluster ID and the task IDs and the user ID all of that is stored in the user agent for easy processing and we've",
    "start": "1820960",
    "end": "1828320"
  },
  {
    "text": "written some spark jobs that go through the S3 access logs they figure out who's using what figure out um if nobody's",
    "start": "1828320",
    "end": "1835320"
  },
  {
    "text": "using it we might have an opportunity to get rid of it or we've even got Amazon contacting Us in some cases saying can",
    "start": "1835320",
    "end": "1841360"
  },
  {
    "text": "you please stop hammering S3 and in that case we will go into Presto typically",
    "start": "1841360",
    "end": "1846720"
  },
  {
    "text": "and query the table that has these X3 S3 access logs and then go to the developer and be like I know this is infinitely",
    "start": "1846720",
    "end": "1853440"
  },
  {
    "text": "scalable but can you please you know ease up a little bit um we also have a metrics pipeline so",
    "start": "1853440",
    "end": "1860440"
  },
  {
    "text": "for our jobs as they're running they're collecting all sorts of metrics um like the job history files so we take these",
    "start": "1860440",
    "end": "1866880"
  },
  {
    "text": "history files and we put them down the same data pipeline that Eva mentioned before so instead of like Reinventing a",
    "start": "1866880",
    "end": "1872080"
  },
  {
    "text": "wheel of how do we process all of our metadata within the data warehouse we use the same infrastructure we've already built to feed the data warehouse",
    "start": "1872080",
    "end": "1878760"
  },
  {
    "text": "in the first place and one of the things that we send down this metrics pipeline is um execution plans for our various um",
    "start": "1878760",
    "end": "1886279"
  },
  {
    "text": "jobs that are running and the reason reason we do that is then Charlotte is a system that we built and maybe we'll",
    "start": "1886279",
    "end": "1891480"
  },
  {
    "text": "open source it at some point that does dependency analysis and lineage across our data so it'll take these execution",
    "start": "1891480",
    "end": "1898000"
  },
  {
    "text": "plans and figure out what data is this job actually affecting and then in addition to the data we put down this",
    "start": "1898000",
    "end": "1903919"
  },
  {
    "text": "pipeline we also have parsers that look at uh teradata queries and redshift queries And Hive queries right now and",
    "start": "1903919",
    "end": "1910080"
  },
  {
    "text": "figure out what are they affecting and by having that information we can do really powerful things and I'll talk about that later",
    "start": "1910080",
    "end": "1916559"
  },
  {
    "text": "on so are we going to get it all right you know are people going to correctly identify themselves are we going to have",
    "start": "1916559",
    "end": "1923000"
  },
  {
    "text": "a perfect parer that's going to get every job dependency there's no way and we don't even try to a certain degree",
    "start": "1923000",
    "end": "1928960"
  },
  {
    "text": "like we do the best we can and then let it run and then you know example of",
    "start": "1928960",
    "end": "1934159"
  },
  {
    "text": "something for efficiency we might say we're going to delete a bunch of data because nobody is using this anymore and",
    "start": "1934159",
    "end": "1939639"
  },
  {
    "text": "we might be wrong and the first thing we'll do is we'll send out a spreadsheet typically or Google sheet and we'll say",
    "start": "1939639",
    "end": "1945279"
  },
  {
    "text": "here's all the stuff that we're about to delete let us know know if you want to keep it and then people scramble around",
    "start": "1945279",
    "end": "1950960"
  },
  {
    "text": "and look at the Google sheet and they like uh and then sometimes they' be like uh duh like I mean this is the most important data at Netflix of course",
    "start": "1950960",
    "end": "1957639"
  },
  {
    "text": "everything's the most important data at Netflix and then we'll go through and we like why did we miss this how did Charlotte Miss This in the processing",
    "start": "1957639",
    "end": "1963720"
  },
  {
    "text": "and then we just fix the rules engine it covers that whole class of problem in the future and then it just gets better",
    "start": "1963720",
    "end": "1969000"
  },
  {
    "text": "and better over time um as far as data cost again back",
    "start": "1969000",
    "end": "1974639"
  },
  {
    "text": "to what Eva said about data being gold it's not the top prior priority is cutting costs you can see that big",
    "start": "1974639",
    "end": "1980440"
  },
  {
    "text": "dollar sign going into like a medium dollar sign instead of a really small dollar sign and again you might not have that luxury at your company and I I'm",
    "start": "1980440",
    "end": "1986360"
  },
  {
    "text": "sorry for you if that's the case um but what for us it's like we really want to",
    "start": "1986360",
    "end": "1992519"
  },
  {
    "text": "like let people make the most out of this information and interestingly another philosophical principle is that",
    "start": "1992519",
    "end": "1999039"
  },
  {
    "text": "our first calculation that we provided to people on how expensive is this data table it included this soft deleted data",
    "start": "1999039",
    "end": "2007360"
  },
  {
    "text": "now Tech technically that is correct that is the better measure of what it is the problem with it though is that some",
    "start": "2007360",
    "end": "2013279"
  },
  {
    "text": "of our good citizens would do a bunch of cleanup and they look at the report and see how much space am I using and",
    "start": "2013279",
    "end": "2019240"
  },
  {
    "text": "nothing changed because it hadn't ttld out of the system so we were like well let's actually change it to something",
    "start": "2019240",
    "end": "2025559"
  },
  {
    "text": "that's actionable and makes more sense to a user even though it's wrong so you know I guess philosophical principle um",
    "start": "2025559",
    "end": "2032279"
  },
  {
    "text": "you know better to be comprehensible than correct in some cases we might add back a secondary at some point that",
    "start": "2032279",
    "end": "2038639"
  },
  {
    "text": "covers both of those things and we did do a side analysis just to see is this Soft elited Data like is that just",
    "start": "2038639",
    "end": "2044080"
  },
  {
    "text": "overwhelming and it's a small portion over our overall data warehouse and how do we surface the data",
    "start": "2044080",
    "end": "2050560"
  },
  {
    "text": "costs well we use Tableau reports which Engineers don't look at um but managers do sometimes so that's one way you get",
    "start": "2050560",
    "end": "2057320"
  },
  {
    "text": "the the occasional like hey are you do you really mean to spend $200,000 on this table um so a manager might talk to",
    "start": "2057320",
    "end": "2063280"
  },
  {
    "text": "some people on their team just make sure it's sane but more realistically what it what you need something more Tactical for an engineer because they have their",
    "start": "2063280",
    "end": "2069280"
  },
  {
    "text": "day job of of developing jobs and doing great things so you need it something front and center so we created something",
    "start": "2069280",
    "end": "2075839"
  },
  {
    "text": "called the data doctor and this is sort of like an inbox of things that you might and you should care about as a",
    "start": "2075839",
    "end": "2081320"
  },
  {
    "text": "developer and an example that I'm showing here is all the tables that are in the top 5% most expensive show up in",
    "start": "2081320",
    "end": "2088000"
  },
  {
    "text": "their sort of inbox and they it also has suggestions on here's things you might do to reduce the size use parket remove",
    "start": "2088000",
    "end": "2095118"
  },
  {
    "text": "unnecessary columns reduce the lifetime setting and this is just one of many many different rules that you can set up",
    "start": "2095119",
    "end": "2101720"
  },
  {
    "text": "others are like highly skewed jobs or maybe if your job is really long running compared to historical trends like all",
    "start": "2101720",
    "end": "2108040"
  },
  {
    "text": "this shows up in the developer inbox and another philosophical principle here is well that's great the first time and",
    "start": "2108040",
    "end": "2114160"
  },
  {
    "text": "it's really annoying if you keep seeing that every single day especially if you're like yeah it's expensive because",
    "start": "2114160",
    "end": "2119280"
  },
  {
    "text": "it's a lot of data and it's really important so we have this snooze option um and that second Tab and in that you",
    "start": "2119280",
    "end": "2124800"
  },
  {
    "text": "can snooze it for a week a month three months or you can just Whit list it all together and you you're required to give",
    "start": "2124800",
    "end": "2130599"
  },
  {
    "text": "a reason if you Whit list it forever saying it's expensive because I need it and I've compressed it and I've reduced",
    "start": "2130599",
    "end": "2135680"
  },
  {
    "text": "columns and I've set the ttls and just leave me alone and you won't see it again um ttls that Eva had mentioned um",
    "start": "2135680",
    "end": "2144359"
  },
  {
    "text": "are another way that we get rid of cost over time so um some data some data is",
    "start": "2144359",
    "end": "2149520"
  },
  {
    "text": "90 days TTL some is seven it really depends on you know do you need that Atomic data for a long period of",
    "start": "2149520",
    "end": "2155839"
  },
  {
    "text": "time and then some things that were we're considering doing to reduce costs one is we have a lot of Rich job cost",
    "start": "2155839",
    "end": "2162040"
  },
  {
    "text": "information and we have a lot of data cost information but we haven't really tied it together very well and it really",
    "start": "2162040",
    "end": "2167079"
  },
  {
    "text": "is a holistic picture it's not just what is the data cost it's also what is the processing cost of that data and then",
    "start": "2167079",
    "end": "2172760"
  },
  {
    "text": "also we're not heavily leveraging Sia or Glacier right now it's being used in spotty ways throughout Netflix um",
    "start": "2172760",
    "end": "2180280"
  },
  {
    "text": "there's increasing appetite to use it but it takes a lot of analysis and for the most part we want people to run",
    "start": "2180280",
    "end": "2185560"
  },
  {
    "text": "quickly but it's it's an opportunity for uh another since this is uh Big Data",
    "start": "2185560",
    "end": "2191760"
  },
  {
    "text": "through the S3 lens I want to talk about some other Technologies we have and then ways that S3 is playing this best",
    "start": "2191760",
    "end": "2197200"
  },
  {
    "text": "supporting actor role for us um so for red shift like how does red",
    "start": "2197200",
    "end": "2203440"
  },
  {
    "text": "shift really benefit for from S3 so for starters like why do we use red shift if we have Presto and we have Hive it's",
    "start": "2203440",
    "end": "2209839"
  },
  {
    "text": "really good for fast interaction on subsets of data so we push sub we let a",
    "start": "2209839",
    "end": "2215319"
  },
  {
    "text": "user push a subset of information into red shift and they want to do fast interactive analysis they could do it or",
    "start": "2215319",
    "end": "2220400"
  },
  {
    "text": "connect reporting tools on top of it some of our Engineers use it some don't some just want to use Presto they want",
    "start": "2220400",
    "end": "2226599"
  },
  {
    "text": "to use Hive they want to use spark SQL and they don't have to like bother with a secondary copy of the data other",
    "start": "2226599",
    "end": "2231839"
  },
  {
    "text": "people if they do want to use it we set up a process where you can just put a tag in medicat that we said before and",
    "start": "2231839",
    "end": "2238480"
  },
  {
    "text": "every time new data comes into our S3 data Hub it automatically gets replicated into red shift and the beauty",
    "start": "2238480",
    "end": "2245280"
  },
  {
    "text": "of red shift within the Amazon ecosystem is this really fast to load I surprisingly so so the throughputs",
    "start": "2245280",
    "end": "2251319"
  },
  {
    "text": "really good and you can also send in compressed data directly into red shift and we're and we're just doing a pend",
    "start": "2251319",
    "end": "2257560"
  },
  {
    "text": "only anyway so getting data in there it's it's a no-brainer um another cool ways that red",
    "start": "2257560",
    "end": "2263160"
  },
  {
    "text": "shift this is like a godsend compared to like a data center um MPP database for",
    "start": "2263160",
    "end": "2268400"
  },
  {
    "text": "backups restores and expansion so backups it's continually doing snapshots to S3 so if you lose your red shift",
    "start": "2268400",
    "end": "2274280"
  },
  {
    "text": "cluster you can regenerate it the restore process is is another it's pretty amazing how it works you just",
    "start": "2274280",
    "end": "2280079"
  },
  {
    "text": "spin up a new cluster and then you can start using it right away and it will fill in the data that it needs and if",
    "start": "2280079",
    "end": "2286119"
  },
  {
    "text": "you actually do a query and the data is not there it Pages the data in from S3 and then expansion um there's there's a",
    "start": "2286119",
    "end": "2292560"
  },
  {
    "text": "couple different ways you you can expand your system without using S3 but you have to put it into a readon mode and",
    "start": "2292560",
    "end": "2297839"
  },
  {
    "text": "you add new nodes and you know you can still query it um we've tried some other novel ways of using S3 one way we've",
    "start": "2297839",
    "end": "2304520"
  },
  {
    "text": "done it is that we have our main cluster here we start up another red shift cluster we um we put the main cluster in",
    "start": "2304520",
    "end": "2313040"
  },
  {
    "text": "readon mode except for ETL so we can keep the most of the data up to date in",
    "start": "2313040",
    "end": "2318200"
  },
  {
    "text": "this core red shift cluster that's where people go in the meantime we spin up another cluster we let it hydrate we",
    "start": "2318200",
    "end": "2324359"
  },
  {
    "text": "resize it we play all the etail on top of it and then at a certain point we just flip the people over there so",
    "start": "2324359",
    "end": "2329440"
  },
  {
    "text": "they've gotten a a relatively current system throughout this whole expansion",
    "start": "2329440",
    "end": "2335400"
  },
  {
    "text": "process uh Druid I don't know if many people are using the system actually out of curiosity how many people in the room",
    "start": "2335400",
    "end": "2340560"
  },
  {
    "text": "are using Druid in your companies it's very small set um so we",
    "start": "2340560",
    "end": "2346400"
  },
  {
    "text": "didn't have it in our ecosystem for quite some time but we started finding that some cases the outof the-box",
    "start": "2346400",
    "end": "2352280"
  },
  {
    "text": "visualization tools weren't cutting it they couldn't handle like billions of rows that you want to slice and dice on the fly so Druid is really good for",
    "start": "2352280",
    "end": "2359920"
  },
  {
    "text": "interactive response time at scale so we started some teams started creating custom visualization type apps like",
    "start": "2359920",
    "end": "2365920"
  },
  {
    "text": "really rich business applications on lots of data and they were using Druid as a backend and the way he uses S3",
    "start": "2365920",
    "end": "2373200"
  },
  {
    "text": "Druid uses S3 is it's a source of Truth like everything else the data lands in S3 you might do some ETL processing but",
    "start": "2373200",
    "end": "2379400"
  },
  {
    "text": "then it uses it as like a secondary source of Truth so our indexing jobs that get data into Druid the Deep",
    "start": "2379400",
    "end": "2385319"
  },
  {
    "text": "storage that underlines Druid is also an S3 so the sort of post-processed Druid ready files that get loaded into",
    "start": "2385319",
    "end": "2391800"
  },
  {
    "text": "historical nodes and Druid so again it's a pretty good story for having S3",
    "start": "2391800",
    "end": "2397480"
  },
  {
    "text": "uh Tableau yet again S3 is the source of Truth like just about everything at Netflix in the Big Data space um however",
    "start": "2397480",
    "end": "2405359"
  },
  {
    "text": "it's mostly extracts that we're using for Tableau so we're not directly connecting to S3 and we we're definitely",
    "start": "2405359",
    "end": "2411160"
  },
  {
    "text": "not using the obbc um driver for most things that that's a total disaster um if you have any questions on that you",
    "start": "2411160",
    "end": "2416760"
  },
  {
    "text": "can talk to me later um but we've we've worked out a programmatic way instead of um filling the extracts for Tableau um",
    "start": "2416760",
    "end": "2424960"
  },
  {
    "text": "and the one way that we are using S3 interestingly this is the one cross Regional replication case that we're",
    "start": "2424960",
    "end": "2431200"
  },
  {
    "text": "using on my team which is that when we back up Tableau itself we also cross",
    "start": "2431200",
    "end": "2436560"
  },
  {
    "text": "regionally replicate Tau at least our setup of it now another",
    "start": "2436560",
    "end": "2442000"
  },
  {
    "text": "philosophical principle that has been hugely beneficial to us is that we've created this thing called the Big Data",
    "start": "2442000",
    "end": "2448000"
  },
  {
    "text": "portal like this is almost like an alphabet soup of so many technologies that could have been thrown at you imagine you work at Netflix and you got",
    "start": "2448000",
    "end": "2454400"
  },
  {
    "text": "to use these every single day so we honed in on strategy of saying let's just have a One-Stop shop where you can",
    "start": "2454400",
    "end": "2460560"
  },
  {
    "text": "go to do big data stuff and the sort of initial carrot was really simple it was",
    "start": "2460560",
    "end": "2465960"
  },
  {
    "text": "a query pane where you could enter the query and some of our most technical power developers are like this is a total waste of time this portal like I",
    "start": "2465960",
    "end": "2472760"
  },
  {
    "text": "know how to use all my special fancy tools and I don't you know keep my let me keep my Unix editors whenever I want",
    "start": "2472760",
    "end": "2479079"
  },
  {
    "text": "them but over time like everybody uses this now because what has happened is we",
    "start": "2479079",
    "end": "2484440"
  },
  {
    "text": "just it's it's a place where people go so we can invest it and because we're investing it it's a better place to go",
    "start": "2484440",
    "end": "2489680"
  },
  {
    "text": "so we invest in it more and it's just gotten better and better over time so you can see in a drop down if your eyesight's good Presto is the current",
    "start": "2489680",
    "end": "2496160"
  },
  {
    "text": "engine that you can running you can change that to spark SQL or pig or Hive or or red shift or Terra dat whatever",
    "start": "2496160",
    "end": "2502920"
  },
  {
    "text": "you want in one little pain and you can query any of these data sources um you can switch to a test database you can",
    "start": "2502920",
    "end": "2509359"
  },
  {
    "text": "save your queries it has autoc completion and then on the left you see things like the data doctor alerts so",
    "start": "2509359",
    "end": "2514839"
  },
  {
    "text": "that's the inbox that I was showing before um there's also a whole bunch of other options but you know two specific",
    "start": "2514839",
    "end": "2520520"
  },
  {
    "text": "ones I'll call out that interact with S3 are the schema search and the S3 browser so the if I search for a",
    "start": "2520520",
    "end": "2527960"
  },
  {
    "text": "particular table in this case geoc country D because it's not something it's something I can actually share",
    "start": "2527960",
    "end": "2533040"
  },
  {
    "text": "There's No Business Special Value to our country table at Netflix um and this is",
    "start": "2533040",
    "end": "2538599"
  },
  {
    "text": "sort of it just sort of shows like a lot of things we've been talking about in a really simple way so one is it's",
    "start": "2538599",
    "end": "2543960"
  },
  {
    "text": "recommending for you that maybe you want to put this in paret format for efficiency it also has things that we've",
    "start": "2543960",
    "end": "2549559"
  },
  {
    "text": "layered on top of our ecosystem that are not even supported in Hive things like default values that when pig is writing",
    "start": "2549559",
    "end": "2556480"
  },
  {
    "text": "data it can use these default values or foreign keys and primary keys that you can do audit checks on um in this",
    "start": "2556480",
    "end": "2563160"
  },
  {
    "text": "details tab that I'm not showing right now it has the ttls that you could set I want 90 days or seven days it's just one",
    "start": "2563160",
    "end": "2568839"
  },
  {
    "text": "place that's really obvious where to go uh the Charlotte data that I'd mentioned before you can see this is",
    "start": "2568839",
    "end": "2575680"
  },
  {
    "text": "another tab this is the the data dependencies so you can see this particular table was loaded by this pig",
    "start": "2575680",
    "end": "2581680"
  },
  {
    "text": "job geoc country D Pig and then here's all the uses of that data so if we're",
    "start": "2581680",
    "end": "2586839"
  },
  {
    "text": "trying to figure out like hey I need to change this table and I want to notify all the users you can see everyone who's",
    "start": "2586839",
    "end": "2592240"
  },
  {
    "text": "using it or if you're like I want to retire this because I don't think anyone's using it you can see if anyone's actually using it or probably",
    "start": "2592240",
    "end": "2598240"
  },
  {
    "text": "the other way around it could be that you are using it in a lot of places but nobody's loading the table and that's a",
    "start": "2598240",
    "end": "2604359"
  },
  {
    "text": "pretty scary state which happens occasionally and then another um really powerful",
    "start": "2604359",
    "end": "2609640"
  },
  {
    "text": "concept that has worked well for us is that underlying the portal and our whole data platform we've created a lot of uh",
    "start": "2609640",
    "end": "2615960"
  },
  {
    "text": "python libraries that do anything you want to do on the data platform so if you want to move data for example",
    "start": "2615960",
    "end": "2622200"
  },
  {
    "text": "between um Hive and red shift it's just these two python statements you import",
    "start": "2622200",
    "end": "2628599"
  },
  {
    "text": "the the module and then go and then underneath the scenes this creates a load ready file on S3 of course because",
    "start": "2628599",
    "end": "2635839"
  },
  {
    "text": "this is an S3 presentation um it deals with like date type conversions between the different",
    "start": "2635839",
    "end": "2641359"
  },
  {
    "text": "systems um it does gzip compress like everything just sort of happens for you underneath the",
    "start": "2641359",
    "end": "2646720"
  },
  {
    "text": "hood and then taking it up a level um as they're sort of coming to the the final stretch here is that here's another sort",
    "start": "2646720",
    "end": "2653599"
  },
  {
    "text": "of picture that resmar what EV and I have talked about from a different perspective you can sort of see S3 is",
    "start": "2653599",
    "end": "2659599"
  },
  {
    "text": "just in the middle of everything we do the pipelines coming in the processing engines the additional data stores and",
    "start": "2659599",
    "end": "2665880"
  },
  {
    "text": "the reporting tools like 3 is just front and center so what are some things that are",
    "start": "2665880",
    "end": "2671160"
  },
  {
    "text": "on the horizon for us um on the S3 front one is we've toyed with the idea of adding caching for a long time we may",
    "start": "2671160",
    "end": "2678079"
  },
  {
    "text": "not ever do it um back to the performance implications that that Eva talked about really right now like red",
    "start": "2678079",
    "end": "2684359"
  },
  {
    "text": "shift you know would probably be our caching layer or Druid would be it and then most jobs don't really need it but",
    "start": "2684359",
    "end": "2689440"
  },
  {
    "text": "if we really wanted like Presta to be that much faster we might consider adding a caching layer in the mix um",
    "start": "2689440",
    "end": "2695240"
  },
  {
    "text": "storage efficiency is something we definitely looking at pretty heavily so this is things like should we use Saia or Glacier should we use Charlotte and",
    "start": "2695240",
    "end": "2702440"
  },
  {
    "text": "the S3 access logs to figure out who's actually using the data um we recently added brole compression to parquet so",
    "start": "2702440",
    "end": "2709240"
  },
  {
    "text": "that's like 20% more efficient for for that that and then um something that we",
    "start": "2709240",
    "end": "2715000"
  },
  {
    "text": "might do next year this is a little more grandiose with 60 pedabytes of data is create a data analyzer that goes and",
    "start": "2715000",
    "end": "2720640"
  },
  {
    "text": "looks for lots of small files in S3 and combines them into larger files which is the much more efficient way of going for",
    "start": "2720640",
    "end": "2726559"
  },
  {
    "text": "Big Data processing and then we're contining to partner with the S3 team and Amazon for",
    "start": "2726559",
    "end": "2731839"
  },
  {
    "text": "our sake and hopefully for the community sake for example the cross Regional replication for disaster",
    "start": "2731839",
    "end": "2737200"
  },
  {
    "text": "recovery and then I'll leave you with super short sweet simple takeaways um",
    "start": "2737200",
    "end": "2742520"
  },
  {
    "text": "Amazon as an S3 data Hub leads to happiness so do that um don't try to do",
    "start": "2742520",
    "end": "2748720"
  },
  {
    "text": "it all at once though or you'll you'll lose your mind so everything we we started in around 2010 with Java M ruce",
    "start": "2748720",
    "end": "2756119"
  },
  {
    "text": "and hive on S3 so that was that was basically our world and then all this stuff over the",
    "start": "2756119",
    "end": "2762280"
  },
  {
    "text": "years has sort of layered on top of it based on business needs and those business needs really do",
    "start": "2762280",
    "end": "2768319"
  },
  {
    "text": "take an ecosystem like we could not pick one tool if you have small data and you can fit it in red shift and write some Python scripts that might be fine but as",
    "start": "2768319",
    "end": "2775119"
  },
  {
    "text": "it starts growing and growing and growing they you just make that trade-off is what is the effort versus what is the",
    "start": "2775119",
    "end": "2780520"
  },
  {
    "text": "value so with that uh I want to thank you and say we have office hours",
    "start": "2780520",
    "end": "2786280"
  },
  {
    "text": "tomorrow at at 2:00 we have a Netflix booth in the Expo Center so if you want to dive deeper into it like how to get",
    "start": "2786280",
    "end": "2792680"
  },
  {
    "text": "data into Tableau um without using odbc driver or if this sounds so awesome that you really want to work at Netflix then",
    "start": "2792680",
    "end": "2798960"
  },
  {
    "text": "happy to talk about that um and please complete your evaluations so that you",
    "start": "2798960",
    "end": "2804680"
  },
  {
    "text": "can say hopefully that this was great and if not you can tell us that as well and if you have any questions then fire away",
    "start": "2804680",
    "end": "2810760"
  },
  {
    "text": "[Applause]",
    "start": "2810760",
    "end": "2814559"
  }
]