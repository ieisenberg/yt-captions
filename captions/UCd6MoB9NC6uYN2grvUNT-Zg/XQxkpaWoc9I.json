[
  {
    "text": "We have over 245 million mobile customers,",
    "start": "0",
    "end": "3795"
  },
  {
    "text": "over 25 million landline customers,",
    "start": "3795",
    "end": "6047"
  },
  {
    "text": "and over 21 million broadline customers.",
    "start": "6047",
    "end": "9551"
  },
  {
    "text": "My team recently is starting\nto generate and train",
    "start": "9551",
    "end": "13388"
  },
  {
    "text": "their own large language\nmodel, not from scratch,",
    "start": "13388",
    "end": "15681"
  },
  {
    "text": "but using openly available models,",
    "start": "15682",
    "end": "18143"
  },
  {
    "text": "with the purpose of\ngetting a customized model",
    "start": "18143",
    "end": "21312"
  },
  {
    "text": "or fine-tuned to our domain.",
    "start": "21312",
    "end": "22981"
  },
  {
    "text": "We use AWS to provide the infrastructure",
    "start": "22981",
    "end": "25400"
  },
  {
    "text": "to securely and robustly\nscale our experiments.",
    "start": "25400",
    "end": "30739"
  },
  {
    "text": "Specifically for the use\ncase of generative AI,",
    "start": "30739",
    "end": "33783"
  },
  {
    "text": "we found that the customized chips",
    "start": "33783",
    "end": "36828"
  },
  {
    "text": "at the Inferentia2 and Trainium",
    "start": "36828",
    "end": "39705"
  },
  {
    "text": "provide a really good performance",
    "start": "39706",
    "end": "42584"
  },
  {
    "text": "and really good cost benefit\nto actually achieve our goals.",
    "start": "42584",
    "end": "48131"
  },
  {
    "text": "My experience with customized chips",
    "start": "48131",
    "end": "49758"
  },
  {
    "text": "is that sometimes it will take\nmonths, weeks, or even years",
    "start": "49758",
    "end": "52968"
  },
  {
    "text": "to actually take advantage\nof those benefits",
    "start": "52969",
    "end": "55555"
  },
  {
    "text": "on that customized chipset.",
    "start": "55555",
    "end": "57640"
  },
  {
    "text": "My experience with Inferentia2,",
    "start": "57640",
    "end": "59017"
  },
  {
    "text": "that process was much smoother\nthan I actually anticipated.",
    "start": "59017",
    "end": "62853"
  },
  {
    "text": "We found out that using\nthe Inferentia2 chip,",
    "start": "62854",
    "end": "65023"
  },
  {
    "text": "we actually yield 25% better\nnon-functional performance.",
    "start": "65023",
    "end": "70695"
  },
  {
    "text": "Part of that, of course, it's all\ndue to the Inferentia2 team",
    "start": "70695",
    "end": "74365"
  },
  {
    "text": "that was there from day one\nto support us and to actually",
    "start": "74365",
    "end": "78494"
  },
  {
    "text": "push some of the changes that\nwe needed and releases faster.",
    "start": "78495",
    "end": "81456"
  },
  {
    "text": "I think one of the best\nthings about working with AWS",
    "start": "81456",
    "end": "84793"
  },
  {
    "text": "is that our colleagues can\nactually improve their workflow,",
    "start": "84793",
    "end": "89798"
  },
  {
    "text": "having access to better information faster",
    "start": "89798",
    "end": "92174"
  },
  {
    "text": "retrieved by generative AI, and in turn,",
    "start": "92175",
    "end": "95720"
  },
  {
    "text": "solve our customers' problems faster.",
    "start": "95720",
    "end": "97889"
  }
]