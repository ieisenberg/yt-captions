[
  {
    "text": "Machine learning inference",
    "start": "1290",
    "end": "2730"
  },
  {
    "text": "requires scalable,",
    "start": "2730",
    "end": "3930"
  },
  {
    "text": "high performance compute resources",
    "start": "3930",
    "end": "6150"
  },
  {
    "text": "and can account for",
    "start": "6150",
    "end": "6983"
  },
  {
    "text": "over 80% of ML costs.",
    "start": "6983",
    "end": "9510"
  },
  {
    "text": "Because as traffic fluctuates",
    "start": "9510",
    "end": "11550"
  },
  {
    "text": "and reaches new highs,",
    "start": "11550",
    "end": "12870"
  },
  {
    "text": "you'll need a massive fleet",
    "start": "12870",
    "end": "14280"
  },
  {
    "text": "of servers to maintain",
    "start": "14280",
    "end": "15540"
  },
  {
    "text": "throughput and latency.",
    "start": "15540",
    "end": "17430"
  },
  {
    "text": "And these servers can get expensive.",
    "start": "17430",
    "end": "19650"
  },
  {
    "text": "Without an optimized solution,",
    "start": "19650",
    "end": "21360"
  },
  {
    "text": "inference costs and infrastructure",
    "start": "21360",
    "end": "23760"
  },
  {
    "text": "can become unwieldy.",
    "start": "23760",
    "end": "25770"
  },
  {
    "text": "And to further complicate things,",
    "start": "25770",
    "end": "27450"
  },
  {
    "text": "not every ML problem is the same.",
    "start": "27450",
    "end": "29850"
  },
  {
    "text": "Different use cases pose",
    "start": "29850",
    "end": "31440"
  },
  {
    "text": "various performance requirements.",
    "start": "31440",
    "end": "33540"
  },
  {
    "text": "When it comes to inference,",
    "start": "33540",
    "end": "35250"
  },
  {
    "text": "one size definitely doesn't fit all.",
    "start": "35250",
    "end": "38010"
  },
  {
    "text": "No matter your use case.",
    "start": "38010",
    "end": "39449"
  },
  {
    "text": "Amazon SageMaker provides a choice",
    "start": "39450",
    "end": "41580"
  },
  {
    "text": "of inference solutions",
    "start": "41580",
    "end": "42900"
  },
  {
    "text": "to deliver low latency",
    "start": "42900",
    "end": "44567"
  },
  {
    "text": "high throughput predictions.",
    "start": "44567",
    "end": "46380"
  },
  {
    "text": "Real time inference delivers",
    "start": "46380",
    "end": "48090"
  },
  {
    "text": "low latency in milliseconds.",
    "start": "48090",
    "end": "50220"
  },
  {
    "text": "And serverless inference",
    "start": "50220",
    "end": "51570"
  },
  {
    "text": "is a cost effective option",
    "start": "51570",
    "end": "53130"
  },
  {
    "text": "for intermittent",
    "start": "53130",
    "end": "54030"
  },
  {
    "text": "or infrequent traffic patterns.",
    "start": "54030",
    "end": "56520"
  },
  {
    "text": "Asynchronous inference handles",
    "start": "56520",
    "end": "58410"
  },
  {
    "text": "large payloads",
    "start": "58410",
    "end": "59400"
  },
  {
    "text": "or long processing times with ease.",
    "start": "59400",
    "end": "62220"
  },
  {
    "text": "Batch Transform runs predictions",
    "start": "62220",
    "end": "64440"
  },
  {
    "text": "on batches of data.",
    "start": "64440",
    "end": "66390"
  },
  {
    "text": "You can deploy thousands of models",
    "start": "66390",
    "end": "68250"
  },
  {
    "text": "and many containers to a single",
    "start": "68250",
    "end": "70020"
  },
  {
    "text": "endpoint using multi-model",
    "start": "70020",
    "end": "71700"
  },
  {
    "text": "and multi-container endpoints,",
    "start": "71700",
    "end": "73530"
  },
  {
    "text": "reducing costs.",
    "start": "73530",
    "end": "75090"
  },
  {
    "text": "And Inference Recommender",
    "start": "75090",
    "end": "76590"
  },
  {
    "text": "advises you on choosing",
    "start": "76590",
    "end": "77897"
  },
  {
    "text": "the best available",
    "start": "77898",
    "end": "78900"
  },
  {
    "text": "compute instance and configuration",
    "start": "78900",
    "end": "81090"
  },
  {
    "text": "for your ML models to optimize",
    "start": "81090",
    "end": "83189"
  },
  {
    "text": "inference performance and cost.",
    "start": "83190",
    "end": "85890"
  },
  {
    "text": "Use serverless inference",
    "start": "85890",
    "end": "87420"
  },
  {
    "text": "or scaling policies to",
    "start": "87420",
    "end": "88950"
  },
  {
    "text": "automatically handle underlying",
    "start": "88950",
    "end": "90689"
  },
  {
    "text": "compute resources to manage",
    "start": "90690",
    "end": "92490"
  },
  {
    "text": "fluctuations in inference requests.",
    "start": "92490",
    "end": "95189"
  },
  {
    "text": "You can also reduce operational burden,",
    "start": "95190",
    "end": "97410"
  },
  {
    "text": "while accelerating time to value,",
    "start": "97410",
    "end": "99450"
  },
  {
    "text": "using the fully managed ML Ops ready",
    "start": "99450",
    "end": "101939"
  },
  {
    "text": "model deployment features.",
    "start": "101940",
    "end": "104130"
  },
  {
    "text": "SageMaker offers over 70 instance types",
    "start": "104130",
    "end": "106860"
  },
  {
    "text": "and varying levels",
    "start": "106860",
    "end": "108068"
  },
  {
    "text": "of compute and memory,",
    "start": "108068",
    "end": "109860"
  },
  {
    "text": "so you have the flexibility to pick",
    "start": "109860",
    "end": "111960"
  },
  {
    "text": "the instance that's",
    "start": "111960",
    "end": "112793"
  },
  {
    "text": "best suited for your needs.",
    "start": "112793",
    "end": "115020"
  },
  {
    "text": "Run your machine learning inference",
    "start": "115020",
    "end": "116909"
  },
  {
    "text": "on Amazon SageMaker,",
    "start": "116910",
    "end": "118350"
  },
  {
    "text": "no matter your use case.",
    "start": "118350",
    "end": "120183"
  }
]