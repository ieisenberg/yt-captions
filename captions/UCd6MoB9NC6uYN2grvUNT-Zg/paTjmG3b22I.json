[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "my name is charity majors i am the tech lead for infrastructure and operations at pars",
    "start": "2240",
    "end": "9200"
  },
  {
    "text": "i've been at parks for a while basically since uh we first opened it up to developers in beta",
    "start": "9200",
    "end": "14799"
  },
  {
    "text": "and i'm here to tell you how we built a mobile backend as a service entirely on aws so",
    "start": "14799",
    "end": "21680"
  },
  {
    "text": "how many of you here are familiar with parse or have used parse nice and uh how many of you are mobile",
    "start": "21680",
    "end": "28840"
  },
  {
    "text": "developers a couple how many of you are more like infrastructurey folks",
    "start": "28840",
    "end": "35280"
  },
  {
    "text": "okay cool well this is really a talk for you guys and if you're if you're like a mobile",
    "start": "35280",
    "end": "41280"
  },
  {
    "text": "developer and you're curious about parse like there's stuff in here that you might find interesting about our stack",
    "start": "41280",
    "end": "47200"
  },
  {
    "text": "but like this is really a story for the infrastructure nerds about how we built a robust and scalable",
    "start": "47200",
    "end": "53520"
  },
  {
    "text": "platform on top of aws so what is parse parse is a platform for",
    "start": "53520",
    "end": "60719"
  },
  {
    "text": "mobile developers we support we support ios android and um windows devices uh to the extent that",
    "start": "60719",
    "end": "68479"
  },
  {
    "text": "there are windows device developers out there we do support them we have a rest api and native sdks",
    "start": "68479",
    "end": "76000"
  },
  {
    "text": "for a bunch of different environments and languages we have a lot of different products and",
    "start": "76000",
    "end": "81439"
  },
  {
    "start": "80000",
    "end": "80000"
  },
  {
    "text": "features in our platform so like a very partial list would be client analytics cloud code file storage web hosting",
    "start": "81439",
    "end": "89439"
  },
  {
    "text": "push notifications et cetera so that's like half the pitch right we have great features it will make your",
    "start": "89439",
    "end": "94720"
  },
  {
    "text": "life easy and the other half of the pitch is we really do everything for you on the back",
    "start": "94720",
    "end": "100960"
  },
  {
    "text": "end we handle the databases we generate your schemas we handle all of your indexing and all",
    "start": "100960",
    "end": "106799"
  },
  {
    "text": "of your performance issues your scaling your data modeling your storage your user management and",
    "start": "106799",
    "end": "112320"
  },
  {
    "text": "third-party integration um we do it all so basically if you're an app developer",
    "start": "112320",
    "end": "119200"
  },
  {
    "text": "parse is like magic all of these really sticky scaling problems the things you need to hire an ops team for the",
    "start": "119200",
    "end": "126399"
  },
  {
    "text": "things that go boom in in the middle of the night like that's our bread and butter",
    "start": "126399",
    "end": "131520"
  },
  {
    "text": "if your app gets featured in the itunes store in the middle of the night and you wake up and you literally have a",
    "start": "131520",
    "end": "137040"
  },
  {
    "text": "hundred times more traffic as when you went to bed like that's a really great thing to have happen to you",
    "start": "137040",
    "end": "142560"
  },
  {
    "text": "but you can't really plan for that um so we deal with this every day so app",
    "start": "142560",
    "end": "148640"
  },
  {
    "text": "developers don't have to and this frees you mobile developers up to do you know whatever it is that makes",
    "start": "148640",
    "end": "154400"
  },
  {
    "text": "your app really awesome and different um you know your user experience your ui",
    "start": "154400",
    "end": "161840"
  },
  {
    "text": "fancy features i don't even really understand what it is that mobile developers do but like that's what you get to do well",
    "start": "161840",
    "end": "166879"
  },
  {
    "text": "i do what i do so you get to work on that i get to work on genetic generic like back-end",
    "start": "166879",
    "end": "172319"
  },
  {
    "text": "problems that i find interesting so parse is completely hosted in aws",
    "start": "172319",
    "end": "179280"
  },
  {
    "text": "we have never touched bare metal our trajectory over time so far has to be been to integrate",
    "start": "179280",
    "end": "185120"
  },
  {
    "text": "more closely with aws's higher level services not less closely uh and yes we were recently acquired by",
    "start": "185120",
    "end": "192800"
  },
  {
    "text": "facebook in may and no we have we get asked this a lot are you planning to move off of aws",
    "start": "192800",
    "end": "199519"
  },
  {
    "text": "we don't have any current plans to move off of aws uh you know the other big facebook",
    "start": "199519",
    "end": "204959"
  },
  {
    "text": "acquisition instagram is currently in the middle of trying to migrate onto facebook zinfra",
    "start": "204959",
    "end": "210080"
  },
  {
    "text": "so i mean maybe someday they're arguments for arguments against but like we love aws",
    "start": "210080",
    "end": "216400"
  },
  {
    "text": "it's really not on our radar at the moment one of the reasons aws is so good for us",
    "start": "216400",
    "end": "223120"
  },
  {
    "text": "is that parse is growing insanely fast by pretty much any metric we've been growing by about 500 percent",
    "start": "223120",
    "end": "230560"
  },
  {
    "text": "per year for the past two years so like here's our number of apps",
    "start": "230560",
    "end": "235840"
  },
  {
    "text": "developers using parse we're up to 180 000 now and you can see how that curve is",
    "start": "235840",
    "end": "241760"
  },
  {
    "text": "increasing ever since we got bought here's a number of ec2",
    "start": "241760",
    "end": "247200"
  },
  {
    "text": "compute units over the past year the number of 200 requests",
    "start": "247200",
    "end": "253599"
  },
  {
    "text": "the number of connected established android device connections at any given time is now up to about 25 million",
    "start": "253599",
    "end": "259359"
  },
  {
    "text": "devices phoning home so you know graphs graphs whatever this is a sample",
    "start": "259359",
    "end": "265520"
  },
  {
    "text": "but like in addition to getting just plain larger and running more traffic price is getting rapidly more complex",
    "start": "265520",
    "end": "272560"
  },
  {
    "text": "like every few weeks we come out with some big new feature or backing capability and this is a copy",
    "start": "272560",
    "end": "277680"
  },
  {
    "text": "of our adorable little infrastructure diagram from about a year ago we've got like every host drawn out",
    "start": "277680",
    "end": "285520"
  },
  {
    "text": "uh and here is the current diagram or honestly this was current as of about",
    "start": "285520",
    "end": "291280"
  },
  {
    "text": "a month ago when i drew it up and there's already like five or six things that should be on it so given how quickly",
    "start": "291280",
    "end": "298479"
  },
  {
    "text": "we're scaling elastic provisioning is totally key for us if you would have asked me a year ago",
    "start": "298479",
    "end": "303840"
  },
  {
    "text": "guess how many servers you're going to need a year from now and guess what services you're going to have deployed and guess which databases you're going",
    "start": "303840",
    "end": "309039"
  },
  {
    "text": "to be running i i can't tell you what i would have guessed but i can tell you i wouldn't have been right",
    "start": "309039",
    "end": "315199"
  },
  {
    "text": "and if you ask me the same thing now about what the hell are we going to be doing a year from now like i can spitball some but it's",
    "start": "315199",
    "end": "322880"
  },
  {
    "text": "i don't know so like i said ops kind of a key pillar of our service",
    "start": "322880",
    "end": "329039"
  },
  {
    "text": "we are a platform we have 180 000 apps that are relying on us to stay up",
    "start": "329039",
    "end": "334639"
  },
  {
    "text": "we can't just like take the site down to run a migration in some ways honestly parks is kind of",
    "start": "334639",
    "end": "340479"
  },
  {
    "text": "like the ops problem from help because of all these shared resource and multi-tenancy issues and because we're letting random people",
    "start": "340479",
    "end": "346960"
  },
  {
    "text": "off the internet run queries in our databases and arbitrary code and our stack so",
    "start": "346960",
    "end": "352240"
  },
  {
    "text": "it would be super easy to let things just drift into a state where we're firefighting all the time we have a pretty small team",
    "start": "352240",
    "end": "359280"
  },
  {
    "text": "we only have three systems engineers and we're all full stack generalists who do everything from all the dba work",
    "start": "359280",
    "end": "366560"
  },
  {
    "text": "to writing code being on call et cetera we like to take",
    "start": "366560",
    "end": "371680"
  },
  {
    "text": "vacations we also like to go home at 6 pm so it's super important to us",
    "start": "371680",
    "end": "378319"
  },
  {
    "text": "culturally to do things right we really we feel like we're doing a good job if we're spending about 20 of our time",
    "start": "378319",
    "end": "385039"
  },
  {
    "text": "reacting to problems and 80 of the time you know looking around",
    "start": "385039",
    "end": "390160"
  },
  {
    "text": "and deciding what would be cool to build next what would make our lives easier a month or two from now",
    "start": "390160",
    "end": "397120"
  },
  {
    "text": "uh we haven't always been this way a year ago we were really suffering we spent anywhere from 50 to 100 of our",
    "start": "397280",
    "end": "404319"
  },
  {
    "text": "time just reacting to incidents we were working really long nights getting woken up all the time",
    "start": "404319",
    "end": "410080"
  },
  {
    "text": "we were all pretty worn out and we're like god damn it this really blows so over the past year we've really",
    "start": "410080",
    "end": "416479"
  },
  {
    "start": "412000",
    "end": "412000"
  },
  {
    "text": "consciously iterated towards an infrastructure that cares about services not about",
    "start": "416479",
    "end": "421520"
  },
  {
    "text": "individual nodes it can survive most minor or you know some major incidents and it",
    "start": "421520",
    "end": "427759"
  },
  {
    "text": "degrades gracefully instead of being all up or all down and to do this what we've done is we've shifted a lot of that manual work into",
    "start": "427759",
    "end": "434160"
  },
  {
    "text": "aws and let it do the work for us our infrastructure is at least five times as large and complicated as",
    "start": "434160",
    "end": "441280"
  },
  {
    "text": "it was a year ago but we get paged about a quarter as much and many weeks we never even get woken",
    "start": "441280",
    "end": "448160"
  },
  {
    "text": "up at all so i'm going to do two things in this talk",
    "start": "448160",
    "end": "453440"
  },
  {
    "text": "first i'm going to walk you through the whole backend architecture of parse i'm going to talk you through the api",
    "start": "453440",
    "end": "459360"
  },
  {
    "text": "path the push infrastructure the rest of our architecture diagram so",
    "start": "459360",
    "end": "464400"
  },
  {
    "text": "you can see what kinds of services we built uh what kinds of backends we deal with what kinds of databases we use",
    "start": "464400",
    "end": "470879"
  },
  {
    "text": "i'm going to talk some about the technical discussions involved in building and growing parse",
    "start": "470879",
    "end": "478080"
  },
  {
    "text": "and after that i'm going to tell you the story of how we grew up from an architecture that was very fragile",
    "start": "478080",
    "end": "483199"
  },
  {
    "text": "and took a lot of moving parts and manual labor to a much more scalable and resilient",
    "start": "483199",
    "end": "489120"
  },
  {
    "text": "service there are a couple things that i hope you will come away with at the end of this talk",
    "start": "489120",
    "end": "494800"
  },
  {
    "text": "uh number number one you're gonna think that auto scaling groups are amazing",
    "start": "494800",
    "end": "500479"
  },
  {
    "text": "in my opinion you can either just say  it i'm gonna hire a couple extra ops monkeys and they're going to do",
    "start": "500479",
    "end": "506160"
  },
  {
    "text": "nothing but push buttons all day or you can really commit to auto-scaling",
    "start": "506160",
    "end": "511199"
  },
  {
    "text": "groups also i want you to think about why your automation should be reusable",
    "start": "511199",
    "end": "517360"
  },
  {
    "text": "it's not enough to like everybody says we'll just automate everything you know well it's not enough to just write scripts to do the same stuff that you",
    "start": "517360",
    "end": "523440"
  },
  {
    "text": "were doing before if you have a really complex infrastructure and you're constantly adding new services",
    "start": "523440",
    "end": "529360"
  },
  {
    "text": "you need to be able to reuse the work that you've already done to automate other services",
    "start": "529360",
    "end": "535279"
  },
  {
    "text": "and the last thing that i want you to come away with is an appreciation for how important it",
    "start": "535279",
    "end": "540399"
  },
  {
    "text": "is to choose your source of truth carefully we have had three different sources of",
    "start": "540399",
    "end": "546480"
  },
  {
    "text": "truth over over the course of the last year get chef and zookeeper and there is not necessarily a right or",
    "start": "546480",
    "end": "553279"
  },
  {
    "text": "wrong answer here and the right answer is kind of zookeeper but depending on your size",
    "start": "553279",
    "end": "558640"
  },
  {
    "text": "there's not necessarily a right or wrong answer here but you should definitely understand how each of them comes with its own set of compromises",
    "start": "558640",
    "end": "565040"
  },
  {
    "text": "and tradeoffs so let's talk about the parse stack",
    "start": "565040",
    "end": "572480"
  },
  {
    "text": "again this is our only mildly out of date architecture diagram before we really dig into it i want to",
    "start": "572560",
    "end": "579760"
  },
  {
    "text": "just run through some top-level architecture goals some design choices that we made kind of",
    "start": "579760",
    "end": "585360"
  },
  {
    "text": "from the beginning number one we use chef we",
    "start": "585360",
    "end": "591600"
  },
  {
    "text": "love chef as a team i would actually say we are not generally what you",
    "start": "591600",
    "end": "596959"
  },
  {
    "text": "would call dogmatic or religious about tools be because you know whatever they all kind",
    "start": "596959",
    "end": "602959"
  },
  {
    "text": "of stuck in their own special way but we really value the chef community ethos",
    "start": "602959",
    "end": "608480"
  },
  {
    "text": "there's this really strong focus on collaboratively building shared and reusable cookbooks that work",
    "start": "608480",
    "end": "614240"
  },
  {
    "text": "across lots of environments it's great because it means that the community tools are robust",
    "start": "614240",
    "end": "620079"
  },
  {
    "text": "lots of people have run them and you have a whole tribe of open source folks who are working on improving them at all times i really",
    "start": "620079",
    "end": "626880"
  },
  {
    "text": "think that the chef community is its single biggest asset we also believe in route 53 and using",
    "start": "626880",
    "end": "633839"
  },
  {
    "text": "real host names now i personally don't ever want to manage dns again",
    "start": "633839",
    "end": "641440"
  },
  {
    "text": "not because it's hard because it's boring like how many dns systems have we all",
    "start": "641440",
    "end": "646800"
  },
  {
    "text": "managed over the course of our careers it's boring it's a solved problem i don't want to do it anymore just give me an api and let",
    "start": "646800",
    "end": "653040"
  },
  {
    "text": "me be done with it i also i don't really want to get paid or emailed about instance i",
    "start": "653040",
    "end": "658640"
  },
  {
    "text": "zero five blah blah blah whatever i wanna know what it does i wanna know what i'm being alerted about if",
    "start": "658640",
    "end": "664480"
  },
  {
    "text": "i'm being alerted about something so i want a human readable host name",
    "start": "664480",
    "end": "669360"
  },
  {
    "text": "obviously because we're aws and it's a no-brainer we believe in",
    "start": "669519",
    "end": "674720"
  },
  {
    "text": "distributing across availability zones and doing automatic failover for every backend possible",
    "start": "674720",
    "end": "681040"
  },
  {
    "text": "we are currently only in one region and we have no plans to do cross-region",
    "start": "681040",
    "end": "686640"
  },
  {
    "text": "replication for reasons i can go into later whatever if you want but cross-availability zones",
    "start": "686640",
    "end": "692959"
  },
  {
    "start": "690000",
    "end": "690000"
  },
  {
    "text": "duh should totally do that and we also believe in having a single source of truth about our systems at all",
    "start": "692959",
    "end": "700240"
  },
  {
    "text": "times a couple of ec2 design choices we found it's best to standardize on a few",
    "start": "700839",
    "end": "706880"
  },
  {
    "text": "instance types we use primarily m1.large m1.xl and m 2.4 xl",
    "start": "706880",
    "end": "713440"
  },
  {
    "text": "since we use chef we can't actually use any instance type that doesn't have at least two core because chef will always peg one core",
    "start": "713440",
    "end": "720480"
  },
  {
    "text": "every time that it runs limiting the number of instance types makes it easier for us to deal with reserved instances",
    "start": "720480",
    "end": "726240"
  },
  {
    "text": "although i am super excited about that new feature that lets you actually convert reserved",
    "start": "726240",
    "end": "731760"
  },
  {
    "text": "instances in the same family you guys heard about that so sweet",
    "start": "731760",
    "end": "737040"
  },
  {
    "text": "um we prefer kind of conceptually to use lots of smaller disposable instances if",
    "start": "737040",
    "end": "742160"
  },
  {
    "text": "you call it m1.large is small i guess you kind of can these days instead of fewer numbers of large instances",
    "start": "742160",
    "end": "748639"
  },
  {
    "text": "we are also devoted users of security groups we are still on ec2 classic nothing",
    "start": "748639",
    "end": "754560"
  },
  {
    "text": "talks directly to the internet from from the internet to these two instances traffic is only allowed to ingress",
    "start": "754560",
    "end": "760959"
  },
  {
    "text": "through the elbs each host role gets its own security group with minimal ports opened",
    "start": "760959",
    "end": "766160"
  },
  {
    "text": "and we also have a nadio's check that periodically verifies that the running security group config",
    "start": "766160",
    "end": "771360"
  },
  {
    "text": "actually matches what's checked in to get so starting at the top of our api path",
    "start": "771360",
    "end": "778959"
  },
  {
    "text": "we've got an elb handles all inbound traffic hands off to nginx which serves some",
    "start": "778959",
    "end": "784160"
  },
  {
    "start": "781000",
    "end": "781000"
  },
  {
    "text": "static files and routes to the ruby rails servers and ruby app servers which are all running unicorn which is",
    "start": "784160",
    "end": "789600"
  },
  {
    "text": "this pure ruby http server um i'm not going to rag on ruby it's",
    "start": "789600",
    "end": "795120"
  },
  {
    "text": "treated us really very well through our baby startup years but we are really at a point where we need real threads",
    "start": "795120",
    "end": "801440"
  },
  {
    "text": "and real asynchronous behavior because basically if a single backend gets slow",
    "start": "801440",
    "end": "806480"
  },
  {
    "text": "right every single unicorn thread is going to fill up with requests to that back end in no time flat and that's really our single biggest",
    "start": "806480",
    "end": "812800"
  },
  {
    "text": "threat to reliability at this point so we're rewriting the api server from the ground up starting with the drivers that talk to",
    "start": "812800",
    "end": "818639"
  },
  {
    "text": "the databases so currently the way it exists it's kind of in this limbo state where",
    "start": "818639",
    "end": "823760"
  },
  {
    "text": "some requests get proxied from the ruby api servers to the go api service to the back end",
    "start": "823760",
    "end": "830079"
  },
  {
    "text": "we also pipe some logging events into a go service that proxies logs over into facebook's scribe ecosystem for like aggregation",
    "start": "830079",
    "end": "837920"
  },
  {
    "text": "query time analysis real-time ddos prevention performance analysis that sort of thing",
    "start": "837920",
    "end": "844880"
  },
  {
    "text": "we have another service written in go this provides the parse hosting product it's effectively like a fancy wrapper",
    "start": "844880",
    "end": "851120"
  },
  {
    "text": "for cloud code and s3 which together lets you serve a very fully featured website off of parse we have a couple alas i",
    "start": "851120",
    "end": "858880"
  },
  {
    "text": "don't know if you know those or not but like most register registrars won't actually let you see name the apex domain",
    "start": "858880",
    "end": "864800"
  },
  {
    "text": "so we set up a couple of elastic ips and people can give their apex domain and a record the points of the elastic ips",
    "start": "864800",
    "end": "870880"
  },
  {
    "text": "and then those will just like redirect your request to the price hosting servers so it's kind of a cute hack",
    "start": "870880",
    "end": "878480"
  },
  {
    "text": "um cloud code cloud code is server-side javascript and a v8 virtual",
    "start": "878560",
    "end": "884480"
  },
  {
    "text": "machine and this is a pretty cool feature this is the kind of thing that is really hard to do right",
    "start": "884480",
    "end": "889680"
  },
  {
    "text": "because you're letting people run run them random stuff on your servers right so it's inherently kind of",
    "start": "889680",
    "end": "894800"
  },
  {
    "text": "dangerous with cloud code you can upload snippets of javascript and then call those snippets with an api request",
    "start": "894800",
    "end": "900959"
  },
  {
    "text": "including like triggers to execute snippets like before save after a save sort of thing we have lots of",
    "start": "900959",
    "end": "907519"
  },
  {
    "text": "third-party modules here for like stripe twilio mail gun sendgrid etc",
    "start": "907519",
    "end": "913519"
  },
  {
    "text": "and we just added long running jobs so like normal cloud codes snippets have to run in under 15 seconds",
    "start": "913519",
    "end": "919519"
  },
  {
    "text": "long-running jobs can run continuously so that's like a parallel bank of stuff",
    "start": "919519",
    "end": "924720"
  },
  {
    "text": "and then there's our whole push ecosystem we handle push notifications for ios",
    "start": "924720",
    "end": "930079"
  },
  {
    "text": "android windows we send literally like billions of pushes per month an individual app",
    "start": "930079",
    "end": "935360"
  },
  {
    "text": "will sometimes sound like 10 million pushes a day we run around 700 pushes per second",
    "start": "935360",
    "end": "940399"
  },
  {
    "text": "steady state and it spikes about 15x to like 10k per second our push",
    "start": "940399",
    "end": "946720"
  },
  {
    "text": "infrastructure is rescue on redis rescue just a ruby cueing library",
    "start": "946720",
    "end": "953279"
  },
  {
    "text": "we're also planning on substantially rewriting this whole stacking go the way push basically works is the",
    "start": "953279",
    "end": "959839"
  },
  {
    "text": "expansion queue takes in total push jobs from the api server explodes them out into ios android and",
    "start": "959839",
    "end": "965279"
  },
  {
    "text": "windows device jobs and place them on those cues so like there's a certain amount of added complexity there for",
    "start": "965279",
    "end": "971839"
  },
  {
    "text": "uh different qos requirements fast queues slow cues isolated jail and whatnot but like that's the mean",
    "start": "971839",
    "end": "977040"
  },
  {
    "text": "potatoes of it we also have this interesting beast called ptns",
    "start": "977040",
    "end": "982240"
  },
  {
    "text": "which is the parse push notification service so it's like apple's apns but for android devices",
    "start": "982240",
    "end": "989120"
  },
  {
    "text": "we'll be augmenting this with gcm soon but gcm has not been around for long and not all devices use",
    "start": "989120",
    "end": "995120"
  },
  {
    "text": "gcm so we still have to do this what the service does is it literally holds open a socket",
    "start": "995120",
    "end": "1000880"
  },
  {
    "text": "to every android device device that is phoning home for push notifications",
    "start": "1000880",
    "end": "1005920"
  },
  {
    "text": "uh we currently have about 25 million simultaneous android device connections and 200 million android outboxes",
    "start": "1005920",
    "end": "1014399"
  },
  {
    "text": "we have to run this on a lot of servers unfortunately the ruby event machine library can only handle about 250 000",
    "start": "1014959",
    "end": "1020880"
  },
  {
    "text": "connections on an m1.large this is not even like a kernel limitation or anything this is just",
    "start": "1020880",
    "end": "1026798"
  },
  {
    "text": "the ruby library's limitations but we actually benchmarked this exact same service using go",
    "start": "1026799",
    "end": "1032000"
  },
  {
    "text": "and we can hold open up to 1.5 million connections on a single m1.large with",
    "start": "1032000",
    "end": "1037199"
  },
  {
    "text": "minor kernel tuning so we're pretty excited about this we also use pdns for ppms and this is",
    "start": "1037199",
    "end": "1043438"
  },
  {
    "text": "the only place that we do serve our own dns we used to route 53 just round robin all of these ips",
    "start": "1043439",
    "end": "1049679"
  },
  {
    "text": "but we literally outgrew the udp packet size when we got over like 25 hosts and if the udp reply fails the clients",
    "start": "1049679",
    "end": "1056720"
  },
  {
    "text": "are supposed to retry using tcp but not all clients do this correctly including a lot in russia and thailand",
    "start": "1056720",
    "end": "1064080"
  },
  {
    "text": "like for some reason like all of our customers in russia and thailand were just like suddenly unable to resolve push.com",
    "start": "1064080",
    "end": "1070400"
  },
  {
    "text": "so instead we delegate just that single a record to our pdf server and it just returns a randomized subset",
    "start": "1070400",
    "end": "1078640"
  },
  {
    "text": "so that's like the highlight reel of our services well let's talk about some back-ends we have this running joke of parse that",
    "start": "1078799",
    "end": "1085440"
  },
  {
    "text": "we run all the databases we actually only run mongodb",
    "start": "1085440",
    "end": "1092080"
  },
  {
    "text": "mysql cassandra redis and hive so no big deal",
    "start": "1092080",
    "end": "1098960"
  },
  {
    "text": "usually when you talk to people about best practices they're like yeah you should really choose when databases stick to it",
    "start": "1098960",
    "end": "1104799"
  },
  {
    "text": "and we'd love to but we do a lot of different things with data and there's just no one-size-fits-all",
    "start": "1104799",
    "end": "1110080"
  },
  {
    "text": "solution for all the things we want to do and honestly like you guys know this dealing with databases and aws",
    "start": "1110080",
    "end": "1117360"
  },
  {
    "text": "is still kind of hard it's not like you know service oriented architecture",
    "start": "1117360",
    "end": "1122480"
  },
  {
    "text": "every node should be disposable that's great for your services that's absolutely true",
    "start": "1122480",
    "end": "1129120"
  },
  {
    "text": "but the whole cloud philosophy of elasticity and disposability is a lot harder to do when you actually care about your data",
    "start": "1129120",
    "end": "1136240"
  },
  {
    "text": "so what this means for us is that instrumenting your databases and your backups and your restores and",
    "start": "1136240",
    "end": "1142960"
  },
  {
    "text": "your provisioning and everything is really more important than ever so let's talk about mongodb is our",
    "start": "1142960",
    "end": "1151120"
  },
  {
    "text": "workout workhorse we don't have the largest mango deployment in the world but we",
    "start": "1151120",
    "end": "1157840"
  },
  {
    "start": "1152000",
    "end": "1152000"
  },
  {
    "text": "definitely have one of the most complicated we have about 15 replica sets three to",
    "start": "1157840",
    "end": "1163280"
  },
  {
    "text": "five nodes for a replica set two to four terabytes rebel set most of these rebel sets are storing just",
    "start": "1163280",
    "end": "1168320"
  },
  {
    "text": "structured application data for our mobile apps uh we also use for some other stuff but that's kind of",
    "start": "1168320",
    "end": "1173840"
  },
  {
    "text": "a relevant because the use cases are not as interesting but we have because we have 180 000 apps",
    "start": "1173840",
    "end": "1179919"
  },
  {
    "text": "we have over 180 000 schemas and user data collections alone so",
    "start": "1179919",
    "end": "1185600"
  },
  {
    "text": "that's over a million collections which if you talk to the tangent engineers it's really not the way manga",
    "start": "1185600",
    "end": "1191360"
  },
  {
    "text": "was designed to be used uh but then again there really isn't a database that was designed to do this",
    "start": "1191360",
    "end": "1197039"
  },
  {
    "text": "so we did some pretty cool stuff with  to allow us to manage all this data we do some intelligent auto indexing of",
    "start": "1197039",
    "end": "1203120"
  },
  {
    "text": "the keys based on the entropy of the values we compute and generate compound indexes",
    "start": "1203120",
    "end": "1209039"
  },
  {
    "text": "by analyzing like real api query traffic we implemented our own app level",
    "start": "1209039",
    "end": "1215280"
  },
  {
    "text": "sharding because the built-in sharding would not work for us for additionally complicated reasons",
    "start": "1215280",
    "end": "1221600"
  },
  {
    "text": "which i can discuss later if you're interested we also use arbiters to provide",
    "start": "1221600",
    "end": "1226880"
  },
  {
    "text": "stability like i said we use in a lot of edge casey ways so we have run into scenarios where all",
    "start": "1226880",
    "end": "1232720"
  },
  {
    "text": "the secondaries will die at once after that happen once or twice like we're going to use arbiters to manage",
    "start": "1232720",
    "end": "1238320"
  },
  {
    "text": "our votes so that this doesn't happen anymore in terms of resources we use primarily m",
    "start": "1238320",
    "end": "1244480"
  },
  {
    "text": "2.4 xls with striped provision iops volumes so like the main constraint for",
    "start": "1244480",
    "end": "1249679"
  },
  {
    "text": "scaling wise is that the working set must fit into memory so we just go for some like big memory instances",
    "start": "1249679",
    "end": "1254880"
  },
  {
    "text": "when we were looking forward to trying out some of the new instance types it's pretty exciting our lives were pretty miserable back",
    "start": "1254880",
    "end": "1260799"
  },
  {
    "text": "when we were running on class evs but provisioned iops has been rock solid for us we love it",
    "start": "1260799",
    "end": "1266080"
  },
  {
    "text": "like pi apps really made databases feasible on aws in my opinion i don't know if you saw ilia's keynote",
    "start": "1266080",
    "end": "1272159"
  },
  {
    "text": "this morning where he showed that awesome graph of like what our latency looks like with all the",
    "start": "1272159",
    "end": "1277679"
  },
  {
    "text": "spikes and it's like using classic ebs it was terrible and then the same graph after we moved to pi",
    "start": "1277679",
    "end": "1283520"
  },
  {
    "text": "ups like average latency just dropped in half spikes all smoothed out they've been amazing",
    "start": "1283520",
    "end": "1289600"
  },
  {
    "text": "and and again like going back to the subject of instrumentation we've done a lot of work on the manga",
    "start": "1289600",
    "end": "1294640"
  },
  {
    "text": "and aws chef cookbooks uh we wrote some stuff for the aws cookbook to let you provision and",
    "start": "1294640",
    "end": "1299679"
  },
  {
    "text": "assemble ebs raid volumes either from scratch or from snapshot we implemented pi ups we implemented ebs",
    "start": "1299679",
    "end": "1305760"
  },
  {
    "text": "optimized and on the side we have fully automated snapshotting",
    "start": "1305760",
    "end": "1311039"
  },
  {
    "text": "provisioning so basically we can bring up a new node from snapshot in five minutes and that has saved us i",
    "start": "1311039",
    "end": "1318320"
  },
  {
    "text": "don't even know how many hundreds of hours over the past two years",
    "start": "1318320",
    "end": "1323279"
  },
  {
    "text": "we use memcache obviously this is not really a database but i thought i'd throw it in here because it's an example of a design choice that i would not make",
    "start": "1323360",
    "end": "1330960"
  },
  {
    "text": "i would definitely use elastic next time we also use redis like i said in the",
    "start": "1330960",
    "end": "1337919"
  },
  {
    "text": "push infrastructure it's mostly just as a back end for cueing using rescue",
    "start": "1337919",
    "end": "1344080"
  },
  {
    "start": "1338000",
    "end": "1338000"
  },
  {
    "text": "uh redness is like somewhat limited in its applications but it's really been amazing for us it's like it does one",
    "start": "1344080",
    "end": "1350799"
  },
  {
    "text": "thing and it does it so well only like two or three times a year do we even really even have to think",
    "start": "1350799",
    "end": "1356559"
  },
  {
    "text": "about our reddish stuff so usually when we run up against that single thread cpu",
    "start": "1356559",
    "end": "1361840"
  },
  {
    "text": "constraint and then we have to think about how to logically separate the workload um yeah brettis we are not using a",
    "start": "1361840",
    "end": "1369760"
  },
  {
    "text": "elastic ash radius but i am very intrigued by redis and my sequel are obviously our",
    "start": "1369760",
    "end": "1375200"
  },
  {
    "text": "two bed goods that don't have a solid automatic failover scheme so we're still working on that another really interesting option here",
    "start": "1375200",
    "end": "1381679"
  },
  {
    "text": "is twin proxy or nutcracker have you guys used that uh it's really cool it's like it'll sit",
    "start": "1381679",
    "end": "1386720"
  },
  {
    "text": "in front of a whole bunch of reticences and do this proxying for you it helps us like gracefully failing and",
    "start": "1386720",
    "end": "1392640"
  },
  {
    "text": "stuff unfortunately we can't use it because we depend on rescue scheduler which uses the multicommand but maybe you can",
    "start": "1392640",
    "end": "1400640"
  },
  {
    "text": "and then there's my sequel we do use my sql we would love to get rid of it unfortunately we can't get rid of it until we get rid of rails because",
    "start": "1400640",
    "end": "1406960"
  },
  {
    "text": "active record just freaks out if my sequel goes away for two seconds",
    "start": "1406960",
    "end": "1412480"
  },
  {
    "text": "uh we've waffled back and forth on whether to go to rds or not the fact that there's no chain",
    "start": "1412799",
    "end": "1418000"
  },
  {
    "text": "replication is honestly kind of killer it's also kind of a black box if you",
    "start": "1418000",
    "end": "1423760"
  },
  {
    "text": "can't get a mysql shell it's hard to see what's going on and like our whole team has been running",
    "start": "1423760",
    "end": "1429600"
  },
  {
    "text": "my sequel for god knows how many years and it's it's kind of challenging to want to give up that control",
    "start": "1429600",
    "end": "1435120"
  },
  {
    "text": "so the current plan is basically we get we get rid of rails we get completely on go then we move my sequel to rds so we get",
    "start": "1435120",
    "end": "1442159"
  },
  {
    "text": "auto failover because once you don't have record active record our stack is going to be a lot more",
    "start": "1442159",
    "end": "1447279"
  },
  {
    "text": "tolerant to service blips",
    "start": "1447279",
    "end": "1451760"
  },
  {
    "text": "not that that ever happens lastly we also run a 12 node cassandra",
    "start": "1452320",
    "end": "1459039"
  },
  {
    "text": "ring and this powers the parse developer analytics process the columnar store super fast and",
    "start": "1459039",
    "end": "1465360"
  },
  {
    "text": "efficient for rights and increments pretty slow for reads so like it's the exact polar opposite of",
    "start": "1465360",
    "end": "1470880"
  },
  {
    "text": "but this is like exactly what you want for an analytics product we use ephemeral storage here it's great it's free we also like for",
    "start": "1470880",
    "end": "1478720"
  },
  {
    "text": "instrumenting cassandra we took a slightly different tack instead of going the the chef route we used netflix's prium",
    "start": "1478720",
    "end": "1485360"
  },
  {
    "text": "which runs cassandra inside of an auto scaling group and it manages some of the more painful things about cassandra like",
    "start": "1485360",
    "end": "1492240"
  },
  {
    "text": "initial token assignments backups and incremental backups to s3 et",
    "start": "1492240",
    "end": "1497360"
  },
  {
    "text": "cetera premium not super easy to set up not super well documented but it's totally worth it if you have an",
    "start": "1497360",
    "end": "1504000"
  },
  {
    "text": "underperforming node like you look at your gang google you're like you know read legacy is really really slow on this one no you just kill it",
    "start": "1504000",
    "end": "1511120"
  },
  {
    "text": "prim restores all the data and adds cassandra back to the ring we actually did a full rolling upgrade",
    "start": "1511120",
    "end": "1518240"
  },
  {
    "start": "1518000",
    "end": "1518000"
  },
  {
    "text": "of all 12 nodes from m1.xl to m 2.4 xl in just a day we just like killed one one after the",
    "start": "1518240",
    "end": "1524720"
  },
  {
    "text": "other it was awesome so that's the parse architecture now you",
    "start": "1524720",
    "end": "1530799"
  },
  {
    "text": "know what our services look like and all the various backends so this",
    "start": "1530799",
    "end": "1535840"
  },
  {
    "text": "brings me to the third section of my talk and this is my favorite part this is the story of how we went from an",
    "start": "1535840",
    "end": "1541039"
  },
  {
    "text": "infrastructure that you was managed in kind of the old-fashioned way the traditional way",
    "start": "1541039",
    "end": "1546080"
  },
  {
    "text": "slowly and painfully with and with hand-managed lists of servers",
    "start": "1546080",
    "end": "1552000"
  },
  {
    "text": "to the infrastructure where nodes are pretty disposable self-registering self-disposing when",
    "start": "1552000",
    "end": "1557679"
  },
  {
    "text": "they're sick and i'm going to illustrate this process just by showing step by step how we drop the length of time it takes",
    "start": "1557679",
    "end": "1564480"
  },
  {
    "text": "to scale up any individual service from like two and a half hours to like five minutes",
    "start": "1564480",
    "end": "1571840"
  },
  {
    "text": "so when we started off as little baby startup parts we were not building primarily for",
    "start": "1572799",
    "end": "1579039"
  },
  {
    "start": "1575000",
    "end": "1575000"
  },
  {
    "text": "scalability or reusability or the best possible operations best practices",
    "start": "1579039",
    "end": "1584559"
  },
  {
    "text": "because that would be dumb right you're a startup most startups fail one of the worst things you can do is",
    "start": "1584559",
    "end": "1590559"
  },
  {
    "text": "waste your time over engineering every little thing over engineering",
    "start": "1590559",
    "end": "1595600"
  },
  {
    "text": "at this time is just it's a waste of your time it's not going to get you where you want to go you don't actually know at this point if you're ever going",
    "start": "1595600",
    "end": "1600799"
  },
  {
    "text": "to have any scaling problems so our first generation infrastructure",
    "start": "1600799",
    "end": "1606640"
  },
  {
    "text": "is like a lot of ruby and rails we use chef to build base amis capistrano to deploy code i'm not going",
    "start": "1606640",
    "end": "1614559"
  },
  {
    "text": "to say cap is like the greatest or cleanest deployment tool in the world but it fits really really well with the",
    "start": "1614559",
    "end": "1622240"
  },
  {
    "text": "ruby world and for registering services and lists of node we just uh",
    "start": "1622240",
    "end": "1627520"
  },
  {
    "text": "use yaml files checked in to get so this is a world where we have like you know 20 to",
    "start": "1627520",
    "end": "1632559"
  },
  {
    "text": "50 total instances and maybe once a month we need to add or remove something",
    "start": "1632559",
    "end": "1638080"
  },
  {
    "text": "and it's a pain in the ass so like th these are all the steps involved for",
    "start": "1638080",
    "end": "1643919"
  },
  {
    "text": "just bringing up 20 new hosts uh for any one service under a primitive infrastructure like this",
    "start": "1643919",
    "end": "1649679"
  },
  {
    "text": "you run 20 knife ec2 commands to bootstrap the new hosts and you can parallelize this with some stupid shell tricks but it still takes a",
    "start": "1649679",
    "end": "1656559"
  },
  {
    "text": "while and you have to manually figure out and assign the ami name the role the host names distribute the instances",
    "start": "1656559",
    "end": "1662720"
  },
  {
    "text": "across availability zones etc when you add all these new host names to",
    "start": "1662720",
    "end": "1668240"
  },
  {
    "text": "cap the cap deploy file using the crazy ec2 domain names you can deploy from externally because we're still using",
    "start": "1668240",
    "end": "1674159"
  },
  {
    "text": "etsy hosts at this point then you add all the new host names to the ammo files put the email files to",
    "start": "1674159",
    "end": "1679520"
  },
  {
    "text": "github you wait for the nodes to come up you do a cold deploy which involves pulling down the entire github repo on all your nodes",
    "start": "1679520",
    "end": "1686159"
  },
  {
    "text": "which takes an hour two hours depending on how big your repo is and also github will throttle you if you",
    "start": "1686159",
    "end": "1691520"
  },
  {
    "text": "have a lot of nodes um run shaft client pick up the changes to etsy hosts",
    "start": "1691520",
    "end": "1697360"
  },
  {
    "text": "and then finally you like construct a mental list of all the services that need to talk to these new nodes and you",
    "start": "1697360",
    "end": "1702640"
  },
  {
    "text": "do a full deploy and restart all it's exhausting just like listing listing all the things you have to do",
    "start": "1702640",
    "end": "1708159"
  },
  {
    "text": "and all in all this process takes like an hour and a half two and a half hours",
    "start": "1708159",
    "end": "1713279"
  },
  {
    "text": "depending on the size of your repository and that sucks",
    "start": "1713279",
    "end": "1718480"
  },
  {
    "text": "especially since at least half the time if you're bringing up 20 new servers it's probably you need them now because",
    "start": "1718640",
    "end": "1723679"
  },
  {
    "text": "you're probably out of capacity so and besides like the super glaring",
    "start": "1723679",
    "end": "1729840"
  },
  {
    "start": "1728000",
    "end": "1728000"
  },
  {
    "text": "obvious issue of how much time it takes there are some other more subtle landmines here",
    "start": "1729840",
    "end": "1736559"
  },
  {
    "text": "someone has to babysit this process it would be bad enough if you're just kicking off the process with a single command then you walk away",
    "start": "1736559",
    "end": "1743200"
  },
  {
    "text": "you do other stuff you wait for it to finish and you come back but no you're interacting here with several different systems that can't easily communicate",
    "start": "1743200",
    "end": "1749840"
  },
  {
    "text": "with each other so you have to sit in the middle and wait for thing one to finish so you can thank two and so on and this is a",
    "start": "1749840",
    "end": "1756240"
  },
  {
    "text": "terrible use of an engineer's time another problem you're maintaining lists",
    "start": "1756240",
    "end": "1761840"
  },
  {
    "text": "of machines by hand the same lists in multiple places",
    "start": "1761840",
    "end": "1767279"
  },
  {
    "text": "this lends itself to typos and this lends itself to lists getting out of sync with each other",
    "start": "1767279",
    "end": "1774080"
  },
  {
    "text": "you're using the long non-human readable ec2 host names you can't even really tell at a glance",
    "start": "1774080",
    "end": "1779200"
  },
  {
    "text": "what you've added or what you've removed or what you've deployed to it's also a big mistake to require a",
    "start": "1779200",
    "end": "1785360"
  },
  {
    "text": "full code deploy just to add you know to change the list of hosts you're trying to talk to",
    "start": "1785360",
    "end": "1791520"
  },
  {
    "text": "i mean deployment best practices you want to make as few changes as as possible at once so that you can",
    "start": "1791520",
    "end": "1797039"
  },
  {
    "text": "easily isolate any problem if you're adding a few hosts you want to just add the hosts",
    "start": "1797039",
    "end": "1802880"
  },
  {
    "text": "if you're deploying a feature just deploy the feature you know and if you're doing a mysql migration for god's sake",
    "start": "1802880",
    "end": "1809360"
  },
  {
    "text": "don't do anything but that migration it also this this terrible process it",
    "start": "1809360",
    "end": "1815120"
  },
  {
    "text": "also requires humans to remember things and make decisions like which set of services needs to know",
    "start": "1815120",
    "end": "1821679"
  },
  {
    "text": "about these new hosts this in general is a terrible mistake because humans are really bad at this",
    "start": "1821679",
    "end": "1828159"
  },
  {
    "text": "so this old original system breaks all these best practices it's fine if you're young you have a",
    "start": "1828159",
    "end": "1833520"
  },
  {
    "text": "small set of hosts you're not doing this very often whatever but if you're doing well and you're a little bit lucky",
    "start": "1833520",
    "end": "1839840"
  },
  {
    "text": "you grow out of this stage pretty fast and this is basically where we found ourselves at pars about",
    "start": "1839840",
    "end": "1845360"
  },
  {
    "text": "about a year ago a little more so the second generation of our",
    "start": "1845360",
    "end": "1850559"
  },
  {
    "text": "infrastructure consisted primarily of moving our source of truth about the universe out of github and",
    "start": "1850559",
    "end": "1857440"
  },
  {
    "start": "1853000",
    "end": "1853000"
  },
  {
    "text": "into chef so instead of maintaining lists of file lists of hosts by hand and yaml files",
    "start": "1857440",
    "end": "1863519"
  },
  {
    "text": "we generate these yaml files from chef using roles so that they get updated automatically",
    "start": "1863519",
    "end": "1869200"
  },
  {
    "text": "every time that chef runs we do the same thing for h8 proxy configs you know anything with lists of hosts",
    "start": "1869200",
    "end": "1874559"
  },
  {
    "text": "chef generates a list excludes any hosts that we have specifically excluded and so that we do have a way of removing",
    "start": "1874559",
    "end": "1880960"
  },
  {
    "text": "nodes from service we started registering the hostname with route 53 in chef so",
    "start": "1880960",
    "end": "1886720"
  },
  {
    "text": "every time chef runs it registers the hostname and a special internal domain",
    "start": "1886720",
    "end": "1892320"
  },
  {
    "text": "so this infrastructure is it's a big step in the right direction we don't have to do full deploys to add",
    "start": "1892320",
    "end": "1898960"
  },
  {
    "text": "services we're generating yml files and we only have one set of files to maintain by hand which is the capistrano",
    "start": "1898960",
    "end": "1904840"
  },
  {
    "text": "files so to bootstrap new hosts you know you run the knife vc2 commands",
    "start": "1904840",
    "end": "1910480"
  },
  {
    "start": "1907000",
    "end": "1907000"
  },
  {
    "text": "you add the hostname to the cap deploy file you generate the yaml files and then you just restart the new services",
    "start": "1910480",
    "end": "1916880"
  },
  {
    "text": "and you know this this is better we've reduced the complexity in the list of steps but it still takes",
    "start": "1916880",
    "end": "1922000"
  },
  {
    "text": "a while but we're growing pretty fast and we really hate doing things by hand so this",
    "start": "1922000",
    "end": "1927519"
  },
  {
    "text": "is this is the point where we like sat down like all right what are our goals here",
    "start": "1927519",
    "end": "1933440"
  },
  {
    "text": "and this is what we came up with for our goals first of all we needed to be able to scale up any",
    "start": "1933440",
    "end": "1939279"
  },
  {
    "text": "single class of services in five minutes five to 10 10 minutes tops",
    "start": "1939279",
    "end": "1944720"
  },
  {
    "text": "this is our criteria now for some people 10 minutes is gonna be too long you know they have super bursty traffic",
    "start": "1944720",
    "end": "1950559"
  },
  {
    "text": "whatever they can't wait that long and it is possible to optimize this process down to two or",
    "start": "1950559",
    "end": "1955919"
  },
  {
    "text": "three minutes but that's not really where our priority lies at this point with 180 000 apps our",
    "start": "1955919",
    "end": "1961440"
  },
  {
    "text": "traffic is not that spiky and we maintain a certain amount of over-provision overhead to absorb unexpected things we don't want it to",
    "start": "1961440",
    "end": "1968720"
  },
  {
    "text": "take 30 minutes to scale up a service because that's annoying and it interferes with our workflow but the 5",
    "start": "1968720",
    "end": "1974159"
  },
  {
    "text": "to 10 minute range totally fine next scaling up should be one command we",
    "start": "1974159",
    "end": "1981679"
  },
  {
    "text": "really wanted to take human judgment out of the loop it's so easy even for good engineers you know to mess up just a series of",
    "start": "1981679",
    "end": "1988080"
  },
  {
    "text": "commands and judgment calls it's pretty hard to up one command also we never want to ever ever ever",
    "start": "1988080",
    "end": "1996320"
  },
  {
    "text": "have to maintain another list of hosts ever",
    "start": "1996320",
    "end": "2001278"
  },
  {
    "text": "we want to automatically detect when new nodes have been added or automatically remove down nodes from",
    "start": "2001519",
    "end": "2007600"
  },
  {
    "text": "service and we needed to be able to deploy fast including deploying from master",
    "start": "2007600",
    "end": "2013360"
  },
  {
    "text": "so this is kind of a key constraint for us we're still pretty startupy sometimes we need to get it fixed out",
    "start": "2013360",
    "end": "2018720"
  },
  {
    "text": "fast we don't have time to build a new ami we don't even always have time to like",
    "start": "2018720",
    "end": "2024159"
  },
  {
    "text": "run tests so there's obviously a big trade-off here if you build an ami for every",
    "start": "2024159",
    "end": "2029440"
  },
  {
    "text": "deploy you can scale up incredibly fast and you can get your nodes into service in just a minute or two",
    "start": "2029440",
    "end": "2035440"
  },
  {
    "text": "but the process of building that ami and preparing for the deploy can take a couple hours so we decided this is a trade-off we are",
    "start": "2035440",
    "end": "2041600"
  },
  {
    "text": "totally willing to make we would rather be able to deploy fast and deploy from master",
    "start": "2041600",
    "end": "2047279"
  },
  {
    "text": "so we'll eat the cost of doing a few extra minutes of configuration every time we bring up a bank of new nodes",
    "start": "2047279",
    "end": "2052878"
  },
  {
    "text": "and finally we knew we were going to be moving from the ruby world to the go world so we decided to design a new deploy",
    "start": "2052879",
    "end": "2059679"
  },
  {
    "text": "process from scratch something that really makes sense for go now go has statically linked binaries",
    "start": "2059679",
    "end": "2065040"
  },
  {
    "text": "that are really small and ridiculously easy to distribute so it makes no sense to pull down the",
    "start": "2065040",
    "end": "2071040"
  },
  {
    "text": "entire github repo onto every machine it also seemed like building an ami every time",
    "start": "2071040",
    "end": "2076560"
  },
  {
    "text": "is really overkill when there's just one binary to deploy so those are our goals we knew we wanted",
    "start": "2076560",
    "end": "2083358"
  },
  {
    "text": "to use auto scaling groups i came to reinvent last year and attempted attended a couple of the netflix talks they were talking about",
    "start": "2083359",
    "end": "2089118"
  },
  {
    "text": "their auto scaling groups and i was just like oh my god i want that so bad so we decided to use",
    "start": "2089119",
    "end": "2096158"
  },
  {
    "text": "a generic ami image and then all the role specific stuff actually still gets laid down by chef",
    "start": "2096159",
    "end": "2102640"
  },
  {
    "text": "when it's bootstrapped each asg is named after a chef roll so it can infer how to bootstrap itself and like",
    "start": "2102640",
    "end": "2110160"
  },
  {
    "text": "what to put in at chefclient.rb from the role name we already use",
    "start": "2110160",
    "end": "2115200"
  },
  {
    "text": "jenkins for continuous integration and we love it so we decided to just have jenkins generate a tarble art artifact",
    "start": "2115200",
    "end": "2121520"
  },
  {
    "start": "2119000",
    "end": "2119000"
  },
  {
    "text": "after every successful build and upload that to s3 it just gets billed with it just gets tagged with the build number and dumped",
    "start": "2121520",
    "end": "2128160"
  },
  {
    "text": "in a bucket named after its service then we wrote two pretty awesome utilities is yeah my",
    "start": "2128160",
    "end": "2135839"
  },
  {
    "text": "co-worker dynamite uh wrote them um and we are i think working",
    "start": "2135839",
    "end": "2141200"
  },
  {
    "text": "on open sourcing them yeah yeah okay the first one is called auto bootstrap",
    "start": "2141200",
    "end": "2146560"
  },
  {
    "text": "it's a script that you run out of in it on the first boot it guesses the chef roll from the asg name it generates the client.ib",
    "start": "2146560",
    "end": "2153760"
  },
  {
    "start": "2149000",
    "end": "2149000"
  },
  {
    "text": "and an initial run list it sets the host name by pulling zookeeper for the base name",
    "start": "2153760",
    "end": "2159599"
  },
  {
    "text": "and then it appends the next available integer and then it registers itself with route53 and it grabs a lock from",
    "start": "2159599",
    "end": "2165280"
  },
  {
    "text": "zookeeper while it does this so the dns registration is atomic and you're not going to collide with our host names that are trying to register",
    "start": "2165280",
    "end": "2170720"
  },
  {
    "text": "at the same time then it bootstraps runs chef configures itself",
    "start": "2170720",
    "end": "2176480"
  },
  {
    "text": "and that's that's where the extra four to six minutes goes it's that initial chef run that does all all of the system configuration installs",
    "start": "2176480",
    "end": "2183680"
  },
  {
    "text": "a bunch of packages blah blah blah and when bootstrap is done running chef it runs the auto deploy scripts",
    "start": "2183680",
    "end": "2189760"
  },
  {
    "text": "and the auto deploy script just pulls down the latest build artifact from s3 unpacks it copies the config files and",
    "start": "2189760",
    "end": "2196560"
  },
  {
    "text": "starts a service so we have now basically taken care of how to scale up services really fast",
    "start": "2196560",
    "end": "2203520"
  },
  {
    "text": "and how to deploy code really fast so what about service registration and discovery",
    "start": "2203520",
    "end": "2211119"
  },
  {
    "text": "well this is where zookeeper comes in zookeeper is this distributed coordinated",
    "start": "2212079",
    "end": "2217119"
  },
  {
    "start": "2214000",
    "end": "2214000"
  },
  {
    "text": "system that a bunch of other people you guys probably don't know what zookeeper is it's not always super trivial to set up",
    "start": "2217119",
    "end": "2223119"
  },
  {
    "text": "um it can be a little confusing and opaque and the quality of the client libraries varies wildly but",
    "start": "2223119",
    "end": "2231280"
  },
  {
    "text": "it is pretty much the only way well it's one of the only ways to get a consistent source of truth that accurately reflects the state of",
    "start": "2231280",
    "end": "2238480"
  },
  {
    "text": "your system in real time at any given point and like one of the cool things you can do",
    "start": "2238480",
    "end": "2243920"
  },
  {
    "text": "you have clients come up they register in a femoral node with zookeeper um and then you can just query the list",
    "start": "2243920",
    "end": "2250400"
  },
  {
    "text": "of those ephemeral nodes it gives you the list of all the active clients it also does some things like distributed locking allocation of unique",
    "start": "2250400",
    "end": "2257680"
  },
  {
    "text": "ids and so forth we really love zooeber",
    "start": "2257680",
    "end": "2262960"
  },
  {
    "text": "wherever possible we bake it into the service itself so the service starts up registers itself as zookeeper",
    "start": "2262960",
    "end": "2268880"
  },
  {
    "text": "and if the client dies the femoral node goes away and it de-registers automatically in the places where we can't bake it",
    "start": "2268880",
    "end": "2274800"
  },
  {
    "text": "into the client we've written some scripts um to perform these utilities we have a zookeeper node registry service",
    "start": "2274800",
    "end": "2281760"
  },
  {
    "text": "that will detect what a local service is up and establish the federal node zookeeper",
    "start": "2281760",
    "end": "2287760"
  },
  {
    "text": "and we also have scripts that generate yaml files from the zookeeper node information and a watcher script that will kick the",
    "start": "2287760",
    "end": "2293680"
  },
  {
    "text": "service if the yml file has changed we also use it for lock coordination so that like only n",
    "start": "2293680",
    "end": "2299440"
  },
  {
    "text": "indexing jobs or badge increments can be run on a given shard at any given time",
    "start": "2299440",
    "end": "2304880"
  },
  {
    "text": "um i should also point out we migrated from yaml files to zookeeper very carefully very slowly and",
    "start": "2304880",
    "end": "2312880"
  },
  {
    "text": "incrementally and the first thing we did was instead of generating the ammo files from chef we generate them from both chef and",
    "start": "2312880",
    "end": "2319359"
  },
  {
    "text": "zookeeper and then we perform a bunch of sanity checks and after a while when we were confident",
    "start": "2319359",
    "end": "2324400"
  },
  {
    "text": "that the zookeeper information was reliable we switched to using the ones generated by zookeeper we also wrote some code",
    "start": "2324400",
    "end": "2331520"
  },
  {
    "text": "to integrate the zookeeper and the asgas with capistrano so instead of having to maintain that",
    "start": "2331520",
    "end": "2336800"
  },
  {
    "text": "list of file names in the deploy file we just say with an environment variable that we would like it to query either zookeeper for the list of nodes or the",
    "start": "2336800",
    "end": "2343440"
  },
  {
    "text": "auto scaling group for the list of nodes and that was our last hand maintained",
    "start": "2343440",
    "end": "2348960"
  },
  {
    "text": "thing oh he's kind of stuck at slides alright so to sum up the third and",
    "start": "2348960",
    "end": "2355040"
  },
  {
    "start": "2353000",
    "end": "2353000"
  },
  {
    "text": "current generation of our infrastructure has these characteristics we have some go services",
    "start": "2355040",
    "end": "2360079"
  },
  {
    "text": "we have some ruby services we still use chef to build the ami and maintain state we",
    "start": "2360079",
    "end": "2366400"
  },
  {
    "text": "have one asg per role we use capistrano plus zookeeper plus jenkins plus s3",
    "start": "2366400",
    "end": "2372960"
  },
  {
    "text": "to build and deploy code and our single source of truth for the state of the world is zookeeper",
    "start": "2372960",
    "end": "2379119"
  },
  {
    "text": "so we have no list of hosts to maintain by hand instances get added and removed from service automatically",
    "start": "2379119",
    "end": "2386800"
  },
  {
    "text": "so let's go back to our example to bring up 20 new nodes all we do is adjust the",
    "start": "2386800",
    "end": "2392800"
  },
  {
    "text": "size of the auto scaling group then we sit back have a drink",
    "start": "2392800",
    "end": "2398320"
  },
  {
    "text": "well machines do all the work like five to ten minutes later our instances are in service running the",
    "start": "2398320",
    "end": "2403599"
  },
  {
    "text": "latest code",
    "start": "2403599",
    "end": "2406240"
  },
  {
    "text": "yay so are we exactly where we want to be no we'd still like to set up some cloud",
    "start": "2408839",
    "end": "2414880"
  },
  {
    "text": "watch triggers uh unlike available app server threads so that we don't even have to type in",
    "start": "2414880",
    "end": "2420079"
  },
  {
    "text": "the single asg command to scale up and this will be like lazy ops nirvana right it's gonna be amazing we haven't done it",
    "start": "2420079",
    "end": "2427920"
  },
  {
    "text": "yet because um honestly asgs have not been particularly useful for us for dealing with bursts",
    "start": "2427920",
    "end": "2433200"
  },
  {
    "text": "our bursts tend to come and go in two to three minutes which is not enough time for asps to really",
    "start": "2433200",
    "end": "2439119"
  },
  {
    "text": "respond and fulfill those requests so we mainly use asg's to respond to lengthier trends and we rely",
    "start": "2439119",
    "end": "2445599"
  },
  {
    "text": "on a certain amount of slack capacity and application burst limits to protect the api for the short term bursts",
    "start": "2445599",
    "end": "2451280"
  },
  {
    "text": "we also don't really have any like significant periodicity to our traffic on like either a daily or you know time",
    "start": "2451280",
    "end": "2458079"
  },
  {
    "text": "of day week to week whatever so it hasn't been a high priority we also need to do some more tooling",
    "start": "2458079",
    "end": "2464079"
  },
  {
    "text": "around downsizing the asgs it's a single command to scale up uh but it's like half a dozen",
    "start": "2464079",
    "end": "2470240"
  },
  {
    "text": "commands is scaled down so we don't do that very often it's also possible",
    "start": "2470240",
    "end": "2475359"
  },
  {
    "text": "we may decide to optimize our initial chef run we we really haven't even like tried",
    "start": "2475359",
    "end": "2480560"
  },
  {
    "text": "or cared about it we could build amis for each um for each role that you know install",
    "start": "2480560",
    "end": "2487040"
  },
  {
    "text": "the packages and all this stuff but it just hasn't really been a priority to us yet right now like we've clearly we've built a system that",
    "start": "2487040",
    "end": "2492960"
  },
  {
    "text": "addresses our current needs and constraints very well but in the future we may have slightly different needs and constraints so",
    "start": "2492960",
    "end": "2499520"
  },
  {
    "text": "other remaining issues we've we're stuck with capistrano for as long as we have ruby code um once we're mostly on go we will",
    "start": "2499520",
    "end": "2506079"
  },
  {
    "text": "probably trigger deploys by just updating a value in zookeeper and then we'll have a canary deploy off that and that",
    "start": "2506079",
    "end": "2511359"
  },
  {
    "text": "succeeds all the other hosts will auto deploy we're not super big fans of capistrano",
    "start": "2511359",
    "end": "2517760"
  },
  {
    "text": "and i'm not gonna say any more about that um also we we badly need to get rid of the",
    "start": "2519200",
    "end": "2526000"
  },
  {
    "text": "mysql and redis single points of failure we don't have automatic failure for either back end",
    "start": "2526000",
    "end": "2531280"
  },
  {
    "text": "and it like hasn't been used yet but it will eventually so hopefully we'll have a solution by that time and our next like big project is",
    "start": "2531280",
    "end": "2539040"
  },
  {
    "text": "migrating from ec2 classic into vpcs asgs are really going to help with this",
    "start": "2539040",
    "end": "2544160"
  },
  {
    "text": "and then we'll be able to use internal eobs that you know the asg can like auto register with the internal elb so we can get aj",
    "start": "2544160",
    "end": "2550560"
  },
  {
    "text": "proxy out of the mix and it's going to be amazing so final thoughts should know what your",
    "start": "2550560",
    "end": "2557599"
  },
  {
    "text": "source of truth is make sure you only have one you either have a single source of truth or you",
    "start": "2557599",
    "end": "2564079"
  },
  {
    "text": "have multiple sources of lies the more real time your source of truth is the faster your response time can be",
    "start": "2564079",
    "end": "2570560"
  },
  {
    "text": "the faster and more automatic your response time can be and you see is that our source of truth",
    "start": "2570560",
    "end": "2575680"
  },
  {
    "text": "moves from get to chef to zookeeper everything just gets easier and more responsive",
    "start": "2575680",
    "end": "2581680"
  },
  {
    "text": "and like i said auto scaling groups are amazing it's a bit of work up front but it's super worth it",
    "start": "2581680",
    "end": "2587040"
  },
  {
    "text": "you reach a point around 100 servers maybe where you really don't want to have to think about nodes and you shouldn't have",
    "start": "2587040",
    "end": "2593359"
  },
  {
    "text": "to think about nodes you should only have to think about healthy services and if you're doing it right it should be just as easy to",
    "start": "2593359",
    "end": "2599040"
  },
  {
    "text": "manage 5000 nodes as 50 nodes well services service nodes",
    "start": "2599040",
    "end": "2604480"
  },
  {
    "text": "not database nodes necessarily but we actually spend far less time managing a thousand nodes than",
    "start": "2604480",
    "end": "2610000"
  },
  {
    "text": "we did managing 100 nodes so thank you auto scaling groups chef and zookeeper",
    "start": "2610000",
    "end": "2616960"
  },
  {
    "text": "and uh that's about it i think we have a 15 minutes left if there's any questions",
    "start": "2616960",
    "end": "2628400"
  }
]