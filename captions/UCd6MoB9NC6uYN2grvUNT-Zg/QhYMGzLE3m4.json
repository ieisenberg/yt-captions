[
  {
    "start": "0",
    "end": "247000"
  },
  {
    "text": "thanks for being here it's really exciting to kick off the Chicago summit I was here at the the the summit last",
    "start": "0",
    "end": "5940"
  },
  {
    "text": "year and it's really exciting to see how much it's grown and really how much this this area in general if I look across",
    "start": "5940",
    "end": "11880"
  },
  {
    "text": "the schedule how many big data related sessions there are when when I joined AWS about three and a half four years",
    "start": "11880",
    "end": "17940"
  },
  {
    "text": "ago it was in some people's minds but it was still early days and now you have companies across the spectrum and so",
    "start": "17940",
    "end": "24810"
  },
  {
    "text": "many different market verticals and horizontals actually using it in their businesses every day it's not just",
    "start": "24810",
    "end": "30420"
  },
  {
    "text": "something that a CTO told you to look into it's now a core part of many businesses and many businesses that have",
    "start": "30420",
    "end": "36360"
  },
  {
    "text": "been quite surprising quite honestly at the beginning we saw a strong uptake from a lot of health care life sciences",
    "start": "36360",
    "end": "42629"
  },
  {
    "text": "and financial services companies and the kind of people you'd expect to have a lot of big data and now we're seeing it",
    "start": "42629",
    "end": "49079"
  },
  {
    "text": "across the board you know there was this big uptick in advertising and marketing and now we have pretty much everybody",
    "start": "49079",
    "end": "55230"
  },
  {
    "text": "you know from the gaming to design an engineering to health care life sciences to financial services to immediate",
    "start": "55230",
    "end": "61559"
  },
  {
    "text": "entertainment the ability to analyze a lot of data at scale and to do it quickly is becoming increasingly core to",
    "start": "61559",
    "end": "68159"
  },
  {
    "text": "all of our businesses and just to life you know we all have a lot of personal data to and the ability to mine that data is actually interesting in itself",
    "start": "68159",
    "end": "75920"
  },
  {
    "text": "so what we're going to do today is is talk about getting started talk about",
    "start": "75920",
    "end": "81000"
  },
  {
    "text": "building your first big data application this may or may not be the first time that you have built a big data",
    "start": "81000",
    "end": "86100"
  },
  {
    "text": "application it might be for some of you so we'll touch on some basics but we'll also go a little deep and my intention",
    "start": "86100",
    "end": "91350"
  },
  {
    "text": "here today I've done variations of this talk for a few years now and what I realized is that never underestimate",
    "start": "91350",
    "end": "97470"
  },
  {
    "text": "people people can go even if they've never done big data or even if they're not coders because I should go pretty deep pretty fast once they get excited",
    "start": "97470",
    "end": "103439"
  },
  {
    "text": "about it so I'm I'm here today to try and get you excited about big data so",
    "start": "103439",
    "end": "108600"
  },
  {
    "text": "before I go any further my name is Manish and I lead the solutions architecture team don't worry about the last name just call me Matt I'd leave",
    "start": "108600",
    "end": "115380"
  },
  {
    "text": "the solutions architecture team for AWS as software partners I've done a lot of big data stuff a lot of media entertainment stuff over the years than",
    "start": "115380",
    "end": "121140"
  },
  {
    "text": "with AWS for a while and I'm very excited to do a live demo with you early",
    "start": "121140",
    "end": "126570"
  },
  {
    "text": "in the morning it's really a bad idea ask anybody especially solutions architects they all say the same thing",
    "start": "126570",
    "end": "132060"
  },
  {
    "text": "make sure you record it this is not a recording today we're going to do a live demo we're gonna build an application together in real",
    "start": "132060",
    "end": "137790"
  },
  {
    "text": "time the whole point of today is to give you something that you can take home and",
    "start": "137790",
    "end": "142860"
  },
  {
    "text": "try again you can do this whole application that we build today for just a couple bucks if that",
    "start": "142860",
    "end": "148650"
  },
  {
    "text": "on your own time and your own AWS account and in fact you could do an enterprise-grade version of this for",
    "start": "148650",
    "end": "154350"
  },
  {
    "text": "much much greater scale today we're gonna deal with about I think it's about 32,000 lines of logs or something that's",
    "start": "154350",
    "end": "159360"
  },
  {
    "text": "all we have time for in 45 minutes in terms of loading but you could easily extend this architecture to be",
    "start": "159360",
    "end": "165330"
  },
  {
    "text": "three-hundred thousand three million potentially 3 billion rows of anything and that's what makes this whole thing",
    "start": "165330",
    "end": "171450"
  },
  {
    "text": "kind of exciting is that this this presentation is designed to scale and it's designed to be take-home so it says",
    "start": "171450",
    "end": "178710"
  },
  {
    "text": "it comes with a take home lab the take home lab is actually the slides everything that is on the slides is all",
    "start": "178710",
    "end": "184230"
  },
  {
    "text": "you need to build your application today so there's going to be a little bit of code on the screen there's going to be some Scala on the screen but even if you",
    "start": "184230",
    "end": "192240"
  },
  {
    "text": "don't necessarily want to care about Scala or Java that that's okay you don't",
    "start": "192240",
    "end": "197310"
  },
  {
    "text": "need to understand much if you understand how to write a basic regular expression a simple transformation you",
    "start": "197310",
    "end": "202530"
  },
  {
    "text": "can do everything that we can do today you can launch clusters and one of my",
    "start": "202530",
    "end": "207720"
  },
  {
    "text": "most the most exciting thing for me here is thinking back four or five years ago like I said it came from immediate",
    "start": "207720",
    "end": "214050"
  },
  {
    "text": "entertainment background we had a lot of transcoding clusters and even firing up clusters of 1020 nodes it was it was a",
    "start": "214050",
    "end": "219570"
  },
  {
    "text": "big deal it required a lot of people let alone Hadoop clusters that we're doing MapReduce and big data processing we're",
    "start": "219570",
    "end": "225840"
  },
  {
    "text": "gonna fire up data warehouses that are enterprise-grade we're gonna fire up Hadoop clusters in just a command or two",
    "start": "225840",
    "end": "230850"
  },
  {
    "text": "and we're not faking it that's how easy it is and hopefully that's your takeaway from today if I do my job right the",
    "start": "230850",
    "end": "237480"
  },
  {
    "text": "takeaway is that yes I can do this too I can extend this for my own uses and that really in 45 minutes I can build",
    "start": "237480",
    "end": "242640"
  },
  {
    "text": "something useful that can help me solve a business problem so I always like to",
    "start": "242640",
    "end": "248820"
  },
  {
    "start": "247000",
    "end": "300000"
  },
  {
    "text": "level set what I'm talking about big data you know what what is Big Data it's an overused term that's for sure",
    "start": "248820",
    "end": "253890"
  },
  {
    "text": "your big data is not my big data big data is not necessarily size it can be",
    "start": "253890",
    "end": "259859"
  },
  {
    "text": "complexity it can mean any number of things for me big data all it means is that it takes a lot of creativity to",
    "start": "259859",
    "end": "266190"
  },
  {
    "text": "unlock the value inherent in that data and creativity can be in the form of code it can be in the form of tools but",
    "start": "266190",
    "end": "272550"
  },
  {
    "text": "the interesting about Big Data and as things have evolved certainly since I joined AWS is that the tools change but",
    "start": "272550",
    "end": "278610"
  },
  {
    "text": "the patterns remain eerily similar one thing I have seen recently is that streaming data is now a much bigger deal",
    "start": "278610",
    "end": "284280"
  },
  {
    "text": "much more important real time reaction so we're going to talk a bit about real time today but fundamentally this is",
    "start": "284280",
    "end": "290970"
  },
  {
    "text": "what you have to do you know you have to store data in ever-increasing amounts and you have to derive value from that",
    "start": "290970",
    "end": "296610"
  },
  {
    "text": "data in some way so you have to collect it such that you can store it and then you have to be able to process it",
    "start": "296610",
    "end": "302930"
  },
  {
    "start": "300000",
    "end": "627000"
  },
  {
    "text": "sometimes you have to normalize it like if it has to go into a relational database that's highly structured maybe",
    "start": "302930",
    "end": "308130"
  },
  {
    "text": "you don't have to normalize it you need some way to analyze it ultimately to extract that data and if you've ever",
    "start": "308130",
    "end": "314430"
  },
  {
    "text": "taken you know a MapReduce output and showing it to your CEO you'll know that that doesn't go over too well they want",
    "start": "314430",
    "end": "319770"
  },
  {
    "text": "to see a nice pretty graph they want to see something on tableau they want to see a nice visualization that that",
    "start": "319770",
    "end": "324990"
  },
  {
    "text": "allows someone to reach an insight about what you're trying to show them very quickly so visualization is actually very important and it's an art to take",
    "start": "324990",
    "end": "332070"
  },
  {
    "text": "the output of that big data processing and to make it not just not simplify it but to create value to give an insight",
    "start": "332070",
    "end": "338940"
  },
  {
    "text": "in a visual way is actually really hard so visualization is a key part and we have quick sight now for that we have a",
    "start": "338940",
    "end": "344790"
  },
  {
    "text": "rich partner ecosystem for data visualization as well so we're going to talk about Amazon s3 which is our object",
    "start": "344790",
    "end": "350760"
  },
  {
    "text": "storage service that's where we're going to store all the data s3 is an amazing place to store data because of a few",
    "start": "350760",
    "end": "356430"
  },
  {
    "text": "reasons it's it's really cheap you can store as much as you want and it has a lot of different tiers so depending on",
    "start": "356430",
    "end": "362790"
  },
  {
    "text": "how quickly you need your data you can store it at different tiers if you don't need it that quick you can store it in",
    "start": "362790",
    "end": "367800"
  },
  {
    "text": "si a if you need it once a year or maybe never you can store it in glacier if you need it right away for a very fast very",
    "start": "367800",
    "end": "374340"
  },
  {
    "text": "scalable big data processing like today you can store it in the standard to your of s3 and have access to that data at",
    "start": "374340",
    "end": "380310"
  },
  {
    "text": "scale for many different applications simultaneously it's a very very flexible very very affordable place to store",
    "start": "380310",
    "end": "385860"
  },
  {
    "text": "really anything today we're gonna store lines of logs but it could be really anything that's bits you could just stick an s3 collection is is something",
    "start": "385860",
    "end": "394560"
  },
  {
    "text": "that was historically hard you know it's easy to say yeah distort and s3 but the question we used to get a few years ago",
    "start": "394560",
    "end": "399570"
  },
  {
    "text": "was like you said that Matt but now I have to rattle this software and deploy as my whole fleet and push it up to s3 and",
    "start": "399570",
    "end": "404970"
  },
  {
    "text": "keep track of whether things made it there or not so that's what we came up with Kinesis and more recently Kinesis fire hose and",
    "start": "404970",
    "end": "411240"
  },
  {
    "text": "the Kinesis agent and the ecosystem around Kinesis for anyone who's ever grappled with Kafka and setting that up",
    "start": "411240",
    "end": "417810"
  },
  {
    "text": "you'll know that it's not trivial if to usually hire someone to do that and that person is expensive so Kinesis makes the",
    "start": "417810",
    "end": "425009"
  },
  {
    "text": "collection and the storage or buffering of data certainly streaming data such that you can then persist it into s3",
    "start": "425009",
    "end": "430970"
  },
  {
    "text": "really easy then of course processing a lot of people will ask well what's the",
    "start": "430970",
    "end": "437610"
  },
  {
    "text": "difference between elastic MapReduce or EMR and redshift well you know why would I need one or the other can't you do sort of data warehousing",
    "start": "437610",
    "end": "443580"
  },
  {
    "text": "now an EMR why do I need redshift buy BMR can't you do data transformation on redshift then why do I need EMR I've",
    "start": "443580",
    "end": "450030"
  },
  {
    "text": "really always seen them as really really complementary in a redshift is your enterprise grade data warehouse that",
    "start": "450030",
    "end": "455069"
  },
  {
    "text": "allows you to do really serious sort of hardcore analysis it works really well with an ecosystem of BI tools of",
    "start": "455069",
    "end": "461879"
  },
  {
    "text": "visualization tools it's sequel anyone who's ever worked on any kind of a sequel database a relational database",
    "start": "461879",
    "end": "468300"
  },
  {
    "text": "a data warehouse sort of immediately inherently understands how to use redshift so you can use existing",
    "start": "468300",
    "end": "473909"
  },
  {
    "text": "resources in your business or existing knowledge that you may have and it scales really really well it's",
    "start": "473909",
    "end": "478949"
  },
  {
    "text": "rock-solid EMR is almost like an operating system for Big Data it gives you a little bit",
    "start": "478949",
    "end": "484050"
  },
  {
    "text": "more flexibility the data doesn't have to be a structure and going in redshift works in tables it works in sequel you",
    "start": "484050",
    "end": "490229"
  },
  {
    "text": "have to know ahead of time what your data is going to look like the thing with streaming data and big data is what",
    "start": "490229",
    "end": "495270"
  },
  {
    "text": "if there's glitches in the data or what if you're collecting data from a lot of different sources and that data is in a",
    "start": "495270",
    "end": "500279"
  },
  {
    "text": "ton of different formats what if your data changes over time what if you acquire some company and their data collection is storing in a totally",
    "start": "500279",
    "end": "507539"
  },
  {
    "text": "different format or what if people are people and they just make mistakes and you need to normalize the data or modify",
    "start": "507539",
    "end": "513029"
  },
  {
    "text": "the data or just handle a variation of data formats that's where EMR comes in really handy and when I said it's kind",
    "start": "513029",
    "end": "519060"
  },
  {
    "text": "of like an operating system for Big Data I say that because inside of EMR you can run a number of different processing",
    "start": "519060",
    "end": "525420"
  },
  {
    "text": "engines so redshift has its redshift processing engine it kind of looks and feels a little bit like Postgres but",
    "start": "525420",
    "end": "531449"
  },
  {
    "text": "it's our own thing whereas with EMR you and run MapReduce you can run spark you can run a number of applications within",
    "start": "531449",
    "end": "537900"
  },
  {
    "text": "EMR and do it at scale and so as trends change and we'll talk about this in a little bit you can adjust and without",
    "start": "537900",
    "end": "544200"
  },
  {
    "text": "having to change the common platform that is running your cluster so EMR think of it as a cluster management tool that sort of a place like Hadoop and EMR",
    "start": "544200",
    "end": "552240"
  },
  {
    "text": "are an operating system that run a number of big data applications that will be suited for individual needs and then redshift is kind of the the final",
    "start": "552240",
    "end": "559110"
  },
  {
    "text": "destination to do your sort of deep dive analysis in this case today we're going to use quick sites to analyze data and",
    "start": "559110",
    "end": "564390"
  },
  {
    "text": "redshift we're gonna use spark running on EMR on an EMR cluster to process that",
    "start": "564390",
    "end": "570060"
  },
  {
    "text": "data to do some data normalization we're gonna use Kinesis and Kinesis firehose to take data from logs to bundle it up",
    "start": "570060",
    "end": "577560"
  },
  {
    "text": "such that it's efficiently stored in s3 and and is also more efficient to iterate over I'll talk about that in a",
    "start": "577560",
    "end": "583500"
  },
  {
    "text": "little bit and we're gonna use s3 is kind of the thing to tie the room together it's the single place to store",
    "start": "583500",
    "end": "589950"
  },
  {
    "text": "all the data it's a great place to sit in the middle of any sort of ecosystem of big data tools I didn't really touch",
    "start": "589950",
    "end": "597060"
  },
  {
    "text": "on that but for me that might be the single best reason to use s3 is that so many tools these days whether it's spark",
    "start": "597060",
    "end": "602790"
  },
  {
    "text": "or redshift loading from loading from s3 or Kinesis being able to persist s3 with",
    "start": "602790",
    "end": "609630"
  },
  {
    "text": "firehose and a number of ecosystem tools around the world they can all work with s3 so it's a great sort of single source",
    "start": "609630",
    "end": "616440"
  },
  {
    "text": "of truth the center of your data world onto which you can bolt a number of tools for different use cases so what",
    "start": "616440",
    "end": "623820"
  },
  {
    "text": "actual problem are we gonna solve today well really kind of simple one we're",
    "start": "623820",
    "end": "629550"
  },
  {
    "start": "627000",
    "end": "686000"
  },
  {
    "text": "gonna mimic web logs a fleet of web servers many of us have mobile applications or web servers so we're",
    "start": "629550",
    "end": "635490"
  },
  {
    "text": "going to sort of imitate having a server or a bunch of servers all pushing apache",
    "start": "635490",
    "end": "641550"
  },
  {
    "text": "HTTP d web logs up to s3 or in this case up to Kinesis persisting that into s3",
    "start": "641550",
    "end": "647750"
  },
  {
    "text": "processing those logs because we don't need all the information in the logs I don't have any of you come from an advertising background and if you've",
    "start": "647750",
    "end": "653880"
  },
  {
    "text": "ever seen an Omniture log for example it has like it's an insane amount of columns like 99% of which you don't need",
    "start": "653880",
    "end": "660030"
  },
  {
    "text": "so rather than create all those columns and then have to deal with all the overhead you can use the tool like EMR",
    "start": "660030",
    "end": "665250"
  },
  {
    "text": "to parse out the data that you want on the way in and then we're gonna put that data into redshift and we're gonna",
    "start": "665250",
    "end": "671600"
  },
  {
    "text": "analyze it using quick site now quick site is not it generally released yet well I don't know if we'll be at the",
    "start": "671600",
    "end": "677210"
  },
  {
    "text": "keynote or not we'll see but you'll get a peek into what quick site looks like and how easy it makes data visualization",
    "start": "677210",
    "end": "685180"
  },
  {
    "text": "so let's do it I said I was gonna do live demo I wasn't lying again everything in the slides is",
    "start": "685930",
    "end": "691940"
  },
  {
    "text": "all you need to build a big data application and what we're going to use today is the command line tool now I could have been clicking around the",
    "start": "691940",
    "end": "698270"
  },
  {
    "text": "management console but hey that's my guess it looks prettier but it's not",
    "start": "698270",
    "end": "703520"
  },
  {
    "text": "really how you work once you get rolling with AWS the command line tool the AWS CLI is a very efficient way to interact",
    "start": "703520",
    "end": "710630"
  },
  {
    "text": "just spin-up and spin-down resources it's really kind of a thin overlay on top of our API so I'm going to use the",
    "start": "710630",
    "end": "716390"
  },
  {
    "text": "command line tool to do things like spin up EMR clusters to start your redshift clusters even to work with I am so the",
    "start": "716390",
    "end": "723650"
  },
  {
    "text": "commands you're seeing today are all part of this one tool the AWS CLI you can download it now if you want and that's what we're gonna use to do pretty",
    "start": "723650",
    "end": "730040"
  },
  {
    "text": "much everything I'll go into the console to kind of prove to you at different points that I am actually doing a live demo and there's data flowing but that's",
    "start": "730040",
    "end": "736190"
  },
  {
    "text": "really all I'm doing you could do this entire demo using the command line except for the visualization part at the end so first thing we need to do is",
    "start": "736190",
    "end": "746180"
  },
  {
    "start": "744000",
    "end": "887000"
  },
  {
    "text": "create an Amazon s3 bucket and you'll notice that I put all of the steps for the command line so you don't have to do",
    "start": "746180",
    "end": "751850"
  },
  {
    "text": "copypasta from PowerPoint we need to create an s3 bucket to hold our data",
    "start": "751850",
    "end": "756920"
  },
  {
    "text": "so again s3 is an object storage service so it can store pretty much anything and this is the one step I did do ahead of",
    "start": "756920",
    "end": "762110"
  },
  {
    "text": "time because I was worried that someone would take my bucket name in the time that I was getting set out but either",
    "start": "762110",
    "end": "767690"
  },
  {
    "text": "way to create an s3 bucket it's a really simple one line command or you can go into the console and create a bucket you can think of a bucket kind of like a",
    "start": "767690",
    "end": "774220"
  },
  {
    "text": "folder a folder that can hold objects and objects are essentially files so s3",
    "start": "774220",
    "end": "780890"
  },
  {
    "text": "is a place to hold files or objects in a big bucket and one common misperception",
    "start": "780890",
    "end": "786410"
  },
  {
    "text": "with s3 is that there's sort of subfolders that's actually just a UI trick when you put a slash in your",
    "start": "786410",
    "end": "792320"
  },
  {
    "text": "object name on the UI it shows up as a folder so if you have like matt slash big slash data it'll show up as like",
    "start": "792320",
    "end": "798980"
  },
  {
    "text": "cascading folders down and fact that's one object name called Matt slash dated slash you'll see that we",
    "start": "798980",
    "end": "806420"
  },
  {
    "text": "also parsed by folder just to make it easier to understand but in fact those are just single objects hanging off of your s3 bucket so we're gonna create a",
    "start": "806420",
    "end": "812930"
  },
  {
    "text": "bucket to hold objects in this case the files are going to be logs I think with",
    "start": "812930",
    "end": "818150"
  },
  {
    "text": "s3 to keep in mind is that having a ton of little files is not necessarily a great idea for a lot of reasons if you",
    "start": "818150",
    "end": "824150"
  },
  {
    "text": "look carefully the pricing you you pay for the API auctions it's a tiny tiny amount of money but if you're doing billions and billions of gets that can",
    "start": "824150",
    "end": "830870"
  },
  {
    "text": "add up also if you're loading data in the redshift and we'll talk this about this later it's not efficient from for",
    "start": "830870",
    "end": "836870"
  },
  {
    "text": "any database to do a million writes into the database what's much more efficient for databases like redshift that",
    "start": "836870",
    "end": "843170"
  },
  {
    "text": "supported is to aggregate those logs was like log lines in our case into little",
    "start": "843170",
    "end": "848870"
  },
  {
    "text": "bunches and have to essentially batch it into EMR or interested with EMR same",
    "start": "848870",
    "end": "854780"
  },
  {
    "text": "thing if you're using Hadoop MapReduce or SPARC it's much more efficient to process chunks of data than it is to",
    "start": "854780",
    "end": "860900"
  },
  {
    "text": "process individual log lines you can think of any distributed system if you have to spin up a new process to process",
    "start": "860900",
    "end": "866540"
  },
  {
    "text": "one Apache log line it's not really efficient use of resources there's overhead associated with doing things so",
    "start": "866540",
    "end": "872240"
  },
  {
    "text": "it's better to batch things up so what we're gonna do today is actually take those log lines batch them together and",
    "start": "872240",
    "end": "877910"
  },
  {
    "text": "a little 1 megabyte or so chunks it's actually a little bit smaller and we have those chunks sitting as gzip tour",
    "start": "877910",
    "end": "883610"
  },
  {
    "text": "compress files in s3 so I said we're gonna use firehose so what's firehose",
    "start": "883610",
    "end": "890830"
  },
  {
    "start": "887000",
    "end": "1019000"
  },
  {
    "text": "you can really think of Kinesis like a queue or a buffer if you have a bunch of",
    "start": "890830",
    "end": "896320"
  },
  {
    "text": "servers writing web logs it's fine to write them all to s3 but it's it's much",
    "start": "896320",
    "end": "903890"
  },
  {
    "text": "better to be able to batch them for example into s3 and to compress them having an intermediary having a to",
    "start": "903890",
    "end": "910310"
  },
  {
    "text": "decouple yourself from writing directly from s3 gives you a lot of flexibility you can have a number of different",
    "start": "910310",
    "end": "916130"
  },
  {
    "text": "sources streaming at s3 and also we have tools associate with Kinesis called fire hose and fire hose what it does is it",
    "start": "916130",
    "end": "921830"
  },
  {
    "text": "takes care of that hard part takes care of that batching so it'll send it up to Kinesis and then it'll batch things for",
    "start": "921830",
    "end": "927470"
  },
  {
    "text": "you compress it in the format you want on a time interval that you want so the file size that you want and then go and",
    "start": "927470",
    "end": "932540"
  },
  {
    "text": "persist into s3 so think of fire hose is kind of a tool that associated with Kinesis that allows you to push into this buffer this",
    "start": "932540",
    "end": "939070"
  },
  {
    "text": "intermediary in front of s3 do some things to your data and a Kinesis and",
    "start": "939070",
    "end": "944350"
  },
  {
    "text": "then persistent in its final resting place in s3 and the format that you need it in this case we're targeting about a one megabyte gzip chunks and fire hose",
    "start": "944350",
    "end": "951280"
  },
  {
    "text": "takes care of that so Canisius allows you to decouple your application or your things spitting out",
    "start": "951280",
    "end": "956410"
  },
  {
    "text": "data from from the place where you actually want to persist it another reason you might use Kinesis is maybe",
    "start": "956410",
    "end": "961630"
  },
  {
    "text": "you want to persist data in a number of places maybe you have different applications that require different different types of data stores maybe you",
    "start": "961630",
    "end": "968830"
  },
  {
    "text": "want to persistent and s3 maybe you want to pull data somewhere else so putting data in that intermediary decouples you",
    "start": "968830",
    "end": "973990"
  },
  {
    "text": "from the place where you're ultimately persisting it and if you you're gonna hear people say d couple a lot at this",
    "start": "973990",
    "end": "979090"
  },
  {
    "text": "summit it's really important to always think of how can I separate the components of my application to give me",
    "start": "979090",
    "end": "984580"
  },
  {
    "text": "greater flexibility so the reason why we have I am here up on the screen is because we need to give firehose",
    "start": "984580",
    "end": "991450"
  },
  {
    "text": "permission to write to that bucket so we're gonna be writing to Kinesis fire",
    "start": "991450",
    "end": "996520"
  },
  {
    "text": "hose and then a fire hose needs to have permission within the account remember we lock everything down services can't",
    "start": "996520",
    "end": "1002010"
  },
  {
    "text": "just write to each other arbitrarily you have to explicitly opt-in to that so we're gonna say hey fire hose you're",
    "start": "1002010",
    "end": "1007170"
  },
  {
    "text": "allowed to write into this as three buckets so when I push stuff into Kinesis fire hose it can then persistent",
    "start": "1007170",
    "end": "1013170"
  },
  {
    "text": "and write objects into s3 so we have to make that explicit so that's what these two steps are if this looks like JSON it",
    "start": "1013170",
    "end": "1022020"
  },
  {
    "text": "is that's an I am policy for an I am role and a role whether it's for ec2 or",
    "start": "1022020",
    "end": "1027689"
  },
  {
    "text": "for fire hoses just in a set of permissions that a service or an instance assumes so in this case fire",
    "start": "1027690",
    "end": "1034380"
  },
  {
    "text": "hose is assuming permission to write to s3 this is only something have to do once you can give it access to all of s3",
    "start": "1034380",
    "end": "1042000"
  },
  {
    "text": "buckets if you're testing within your account but in a production application you want to be a little more granular you don't want like your logs stand up",
    "start": "1042000",
    "end": "1049770"
  },
  {
    "text": "in an s3 bucket with production data that it shouldn't touch for example you want to segregate your data so this is why roles are powerful it allows you to",
    "start": "1049770",
    "end": "1056460"
  },
  {
    "text": "be very explicit about what can write to what specific buckets or even what prefix is what subfolders if you will",
    "start": "1056460",
    "end": "1062640"
  },
  {
    "text": "within those buckets so policy is a really powerful way of segregating your data and adding data security to your stream so let's do",
    "start": "1062640",
    "end": "1070529"
  },
  {
    "start": "1069000",
    "end": "1120000"
  },
  {
    "text": "it let's create a roll",
    "start": "1070529",
    "end": "1073880"
  },
  {
    "text": "so can you guys see that at the back if you can't can you raise your hand okay",
    "start": "1079630",
    "end": "1085260"
  },
  {
    "text": "so we're gonna be at a console hopefully you can see that okay it's not really important that you see the details of",
    "start": "1085980",
    "end": "1091480"
  },
  {
    "text": "the console it's more just to show you that I'm actually doing a live demo in",
    "start": "1091480",
    "end": "1097600"
  },
  {
    "text": "this case I'm going to use the CLI to called I am and to create a new role",
    "start": "1097600",
    "end": "1105010"
  },
  {
    "text": "called fire hose demo and to assume the policy that I have in a document on my laptop that you just saw on the screen",
    "start": "1105010",
    "end": "1110020"
  },
  {
    "text": "just a JSON file that's it and it spits out just a confirmation with an",
    "start": "1110020",
    "end": "1116590"
  },
  {
    "text": "identifier unique identifier for that role so next I'm going to add a policy",
    "start": "1116590",
    "end": "1122320"
  },
  {
    "start": "1120000",
    "end": "1167000"
  },
  {
    "text": "and remember now that we've given fire hose a role we need to give it a policy you are allowed to write to this bucket",
    "start": "1122320",
    "end": "1127600"
  },
  {
    "text": "so I'm gonna give it a policy that's gonna write to a bucket called I think AWS Mya in Chicago I think that's the",
    "start": "1127600",
    "end": "1133360"
  },
  {
    "text": "bucket that I created so it's going to give explicit policy or permission rather to Kinesis fire hose to write",
    "start": "1133360",
    "end": "1139179"
  },
  {
    "text": "into that bucket",
    "start": "1139179",
    "end": "1141658"
  },
  {
    "text": "that's it all right so now we're all set so Kinesis firehose which again think of it as this intermediary this buffer or",
    "start": "1149360",
    "end": "1155120"
  },
  {
    "text": "this cue where we can push our log data our streaming data and then it takes care of that batching and that",
    "start": "1155120",
    "end": "1160310"
  },
  {
    "text": "compression before persisting it in it into s3 this will make more sense in a second so now we're going to create the",
    "start": "1160310",
    "end": "1168950"
  },
  {
    "start": "1167000",
    "end": "1256000"
  },
  {
    "text": "firehose stream so we use the term stream for a firehose and this is a place where we're gonna send our Apache",
    "start": "1168950",
    "end": "1174710"
  },
  {
    "text": "web blog data and you could have a fleet of a thousand servers across multiple availability zones across multiple data",
    "start": "1174710",
    "end": "1180170"
  },
  {
    "text": "centers and they can all push data into your Kinesis stream and when you push it into firehose it gives you that added",
    "start": "1180170",
    "end": "1185420"
  },
  {
    "text": "flexibility that added tooling to do things like aggregation and compression etc so we're gonna create a stream we're",
    "start": "1185420",
    "end": "1191210"
  },
  {
    "text": "gonna create a location where we can push this data so if you look at this",
    "start": "1191210",
    "end": "1201560"
  },
  {
    "text": "command oops create delivery stream in this case I'm gonna push it into this",
    "start": "1201560",
    "end": "1209090"
  },
  {
    "text": "bucket called Mei and demo Chicago that's the calling out to the role this is the permission that it's gonna inherit so that it can write into this",
    "start": "1209090",
    "end": "1215900"
  },
  {
    "text": "bucket and this is the important part this is the part that I want you guys to get well two things three things really first of all when it writes to s3 it's",
    "start": "1215900",
    "end": "1222380"
  },
  {
    "text": "gonna put a prefix that says firehose slash so it'll look like a folder just to keep things separate in case there's",
    "start": "1222380",
    "end": "1227930"
  },
  {
    "text": "other things in the bucket it's gonna buffer things every 60 seconds so as I'm pushing tons of stuff to Kinesis it's",
    "start": "1227930",
    "end": "1233930"
  },
  {
    "text": "going to wait 60 seconds bash up all those things in the 60 seconds compress it using gzip and then every 60 seconds",
    "start": "1233930",
    "end": "1241130"
  },
  {
    "text": "push a little chunk into s3 so that we can more efficiently work with that data out of s3 later well I'll already use",
    "start": "1241130",
    "end": "1252530"
  },
  {
    "text": "this next we're gonna process the data the",
    "start": "1252530",
    "end": "1260230"
  },
  {
    "start": "1256000",
    "end": "1444000"
  },
  {
    "text": "well I guess we'll push in the data at first but this actually takes some time so let's do this first let's create a cluster so this is where things get",
    "start": "1260230",
    "end": "1265690"
  },
  {
    "text": "exciting especially if you've never done Big Data we're gonna create a Hadoop cluster so just returning the EMR for a second",
    "start": "1265690",
    "end": "1272290"
  },
  {
    "text": "EMR is essentially like a cluster management tool that runs Hadoop and Hadoop you can kind of think it in",
    "start": "1272290",
    "end": "1278590"
  },
  {
    "text": "combination with the EMR like an operating system for big data applications in this case we're going to install a few applications on our EMR",
    "start": "1278590",
    "end": "1286480"
  },
  {
    "text": "cluster we're gonna install something called hive which is an oldie but a goodie it's a it's something that allows",
    "start": "1286480",
    "end": "1292060"
  },
  {
    "text": "you to do process and view and interact with your data using a sequel interface and in fact when you write sequel it",
    "start": "1292060",
    "end": "1297850"
  },
  {
    "text": "takes that sequel and translates into MapReduce jobs or in this case into spark jobs we're gonna install spark",
    "start": "1297850",
    "end": "1304980"
  },
  {
    "text": "moster you will have heard the term MapReduce and some of you may know the term spark others may know the term",
    "start": "1304980",
    "end": "1311980"
  },
  {
    "text": "flink this is a fast-changing space I've been giving the big data bootcamp for a few years now and the tools that we use",
    "start": "1311980",
    "end": "1318040"
  },
  {
    "text": "in the bootcamp change every six months to twelve months but the patterns stay the same and quite frankly the fundamentals stay the same",
    "start": "1318040",
    "end": "1323950"
  },
  {
    "text": "what spark is is really an execution engine a processing engine it's you can think of it like a next generation of",
    "start": "1323950",
    "end": "1329920"
  },
  {
    "text": "MapReduce MapReduce is amazing it opened up batch processing workloads to a whole",
    "start": "1329920",
    "end": "1335080"
  },
  {
    "text": "world of people in sort of a commoditized way but it was slow and sparkies is a bunch of techniques like",
    "start": "1335080",
    "end": "1341830"
  },
  {
    "text": "in-memory processing and a bunch of other techniques to run faster I'll talk more about SPARC in a second and lastly",
    "start": "1341830",
    "end": "1347920"
  },
  {
    "text": "we're gonna use something kind of cool that's pretty new on your EMR called Zeppelin and Zeppelin is a web interface if every used EMR and you're not that",
    "start": "1347920",
    "end": "1354760"
  },
  {
    "text": "comfortable with Linux or the command line or just coding in general it's a bit of a start learning curve at the beginning so what Zeppelin does is make",
    "start": "1354760",
    "end": "1360940"
  },
  {
    "text": "that a lot easier you have a really nice web UI that allows you to execute sparc commands hive commands sequel and really",
    "start": "1360940",
    "end": "1367450"
  },
  {
    "text": "interact with your data on your cluster but that having a like SSH into it and do a bunch of crazy stuff or look at output on a console which is not easy",
    "start": "1367450",
    "end": "1374260"
  },
  {
    "text": "sometimes to interpret especially when you get a big fat error and you have to sort of scroll up in a console not the best user experience so Zeppelin makes",
    "start": "1374260",
    "end": "1381190"
  },
  {
    "text": "that easy so we're gonna install three applications on our EMR cluster and spin that up right now",
    "start": "1381190",
    "end": "1386280"
  },
  {
    "text": "this is a two node just a small two node cluster of an m3 extra-large and we're running",
    "start": "1386280",
    "end": "1391840"
  },
  {
    "text": "the latest EMR distribution okay so",
    "start": "1391840",
    "end": "1404349"
  },
  {
    "text": "there's the cluster ID now that one command is going to spin up an earmark cluster I could have put to zero and had",
    "start": "1404349",
    "end": "1410469"
  },
  {
    "text": "a 20 node Hadoop cluster and what's really cool about this is that if you look at our console and go to EMR you'll",
    "start": "1410469",
    "end": "1422109"
  },
  {
    "text": "see that it's already starting and in about two minutes you'll have a new cluster up and running we'll come back",
    "start": "1422109",
    "end": "1427989"
  },
  {
    "text": "to it to show you that it really is two minutes but this is wild if any of you ever worked on premises in a data center and had to rack and stack and set up a",
    "start": "1427989",
    "end": "1434320"
  },
  {
    "text": "Hadoop cluster you'll know that it didn't take two minutes and for me when I joined AWS this was certainly a big eye-opener but let's just get ahead of",
    "start": "1434320",
    "end": "1441249"
  },
  {
    "text": "ourselves and come back to that so we're not wasting too much time so now that we've set up a enterprise to do cluster",
    "start": "1441249",
    "end": "1446889"
  },
  {
    "start": "1444000",
    "end": "1562000"
  },
  {
    "text": "for data processing we're gonna do step two let's set up enterprise grade data warehouse easy right well it is I don't",
    "start": "1446889",
    "end": "1454210"
  },
  {
    "text": "know how many of you have on premises tried to set up a data warehouse but again I did want to my old job and it's",
    "start": "1454210",
    "end": "1461799"
  },
  {
    "text": "hard it's hard hard to get right you have to do a lot of tuning there's always gonna be optimizations and tuning",
    "start": "1461799",
    "end": "1467169"
  },
  {
    "text": "after the facts for your queries and your schemas and your tables you know that's actually what we pay the guys in the white lab coats for but the cluster",
    "start": "1467169",
    "end": "1474070"
  },
  {
    "text": "part the infrastructure part AWS takes care of that for you so in this simple command we're gonna spin up a data",
    "start": "1474070",
    "end": "1479589"
  },
  {
    "text": "warehouse a redshift cluster in this case we're doing a small single node cluster but again this could be a multi",
    "start": "1479589",
    "end": "1485049"
  },
  {
    "text": "node very large data warehouse capable of storing petabytes of data if you wanted to what we specify here is just",
    "start": "1485049",
    "end": "1493779"
  },
  {
    "text": "the type of node the type of there's different types of redshift nodes that you can read up on whether you need more relative CPU to storage or if you're",
    "start": "1493779",
    "end": "1501129"
  },
  {
    "text": "more concerned about storage it's just a classic sizing exercise based on the kind of data you're processing this case",
    "start": "1501129",
    "end": "1506349"
  },
  {
    "text": "we're going with a simple DC one type and a single node so let's spin that up",
    "start": "1506349",
    "end": "1514830"
  },
  {
    "text": "okay let's go back to the console for a second see how our Hadoop clusters doing",
    "start": "1521440",
    "end": "1528980"
  },
  {
    "text": "so this is still provisioning its what it's doing is firing up ec2 instances",
    "start": "1528980",
    "end": "1534150"
  },
  {
    "text": "it's provisioning those two instances make this a little bigger for you guys and it's going through the steps and",
    "start": "1534150",
    "end": "1540870"
  },
  {
    "text": "what steps are is it's installing those applications it's been stalling spark it's installing high if it's installing Zeppelin if I go over here to our",
    "start": "1540870",
    "end": "1550260"
  },
  {
    "text": "redshift console you'll see that my cluster is starting to fire up it's",
    "start": "1550260",
    "end": "1555510"
  },
  {
    "text": "creating and if we come back to this in a minute or two we'll have a red ship cluster up and running okay so now that",
    "start": "1555510",
    "end": "1563520"
  },
  {
    "start": "1562000",
    "end": "1606000"
  },
  {
    "text": "we have our thing to process the data and we have firehose set up so we can",
    "start": "1563520",
    "end": "1569580"
  },
  {
    "text": "collect the data and we have redshift set up so we can analyze the data we actually need some data to play with",
    "start": "1569580",
    "end": "1575550"
  },
  {
    "text": "right so this is the one part of the demo that's kind of fake in the sense that I wasn't gonna have a fleet of web",
    "start": "1575550",
    "end": "1581940"
  },
  {
    "text": "servers pushing up web logs but I did want to show you something real I didn't want to take an existing data set so what we'll do is I'm going to take a",
    "start": "1581940",
    "end": "1588510"
  },
  {
    "text": "very big web log and push it in real time line by line sort of imitating a fleet of web servers so in this case",
    "start": "1588510",
    "end": "1594480"
  },
  {
    "text": "it's just 32,000 lines which sounds like a lot but it's also not that much but we're gonna push all of those 32,000",
    "start": "1594480",
    "end": "1600300"
  },
  {
    "text": "this demo up into Kinesis firehose and then eventually into s3 so you can push",
    "start": "1600300",
    "end": "1607440"
  },
  {
    "start": "1606000",
    "end": "1733000"
  },
  {
    "text": "data from logs in a number of ways there's the Kinesis agent which is",
    "start": "1607440",
    "end": "1612480"
  },
  {
    "text": "probably the best way to do it but in this case because I'm using a flat file the fastest way for me and for you guys",
    "start": "1612480",
    "end": "1618090"
  },
  {
    "text": "at home was just to use a little Python script so those whatever it is like 10",
    "start": "1618090",
    "end": "1624240"
  },
  {
    "text": "lines of Python it uses boto which is the same Python framework that our CLI is built on and it's you know even if",
    "start": "1624240",
    "end": "1631530"
  },
  {
    "text": "you don't speak Python this is pretty easy to understand it opens up a file called weblog and read-only and it puts",
    "start": "1631530",
    "end": "1637200"
  },
  {
    "text": "records into firehose line by line until the file is done so this is effectively imitating a fleet of web servers that",
    "start": "1637200",
    "end": "1642990"
  },
  {
    "text": "are taking web traffic and and it's putting those log lines those Apache logs and I whipped by it let me just go",
    "start": "1642990",
    "end": "1648900"
  },
  {
    "text": "back to this this is what the Apache logs look like so there's a bunch of data in there the I public IP address of",
    "start": "1648900",
    "end": "1655140"
  },
  {
    "text": "the host the time what they're trying to get like a webpage or a fav icon or whatever what",
    "start": "1655140",
    "end": "1661150"
  },
  {
    "text": "the code was if it was a 200 okay or a 404 not okay the user agent they were",
    "start": "1661150",
    "end": "1666670"
  },
  {
    "text": "using so if you're in advertising or even if you're anyone and you run a business there's a lot of data in here",
    "start": "1666670",
    "end": "1672130"
  },
  {
    "text": "that is interesting and useful to you potentially you can tie the IP address to the geographic location of where your",
    "start": "1672130",
    "end": "1678640"
  },
  {
    "text": "visitors are coming from maybe you get a ton of visitors from Brazil and so maybe you want to localize your mobile app in",
    "start": "1678640",
    "end": "1683679"
  },
  {
    "text": "Portuguese maybe you have a ton of users using Mozilla like I don't know four or something and it's breaking these are",
    "start": "1683679",
    "end": "1690940"
  },
  {
    "text": "things you need to know to run your business and if you have a million or a billion lines of logs those types of",
    "start": "1690940",
    "end": "1696730"
  },
  {
    "text": "insights are not easy you can't just use you know that summer intern that you got who wrote that Python script for you",
    "start": "1696730",
    "end": "1702160"
  },
  {
    "text": "it's a parser your web logs that doesn't scale we've all learned this the hard way there comes a point when you have",
    "start": "1702160",
    "end": "1707590"
  },
  {
    "text": "too much data that is easy to process and so in this case if I wanted to run across a billion lines of logs and to",
    "start": "1707590",
    "end": "1715750"
  },
  {
    "text": "find out what is the most popular user agents or how much traffic is taken up by 404 so I have a bunch of dead pages",
    "start": "1715750",
    "end": "1721540"
  },
  {
    "text": "in my mobile app or my website that's the kind of stuff that EMR can help you do really easily and that's the kind of",
    "start": "1721540",
    "end": "1726850"
  },
  {
    "text": "insights that will actually look at today so those are some simple but important business problems that we will",
    "start": "1726850",
    "end": "1732040"
  },
  {
    "text": "solve in real time today so let's push them push some logs up again you can download the log I put it up on my s3",
    "start": "1732040",
    "end": "1739419"
  },
  {
    "start": "1733000",
    "end": "1852000"
  },
  {
    "text": "bucket there so you can do this whole thing okay so I'll just fire up on",
    "start": "1739419",
    "end": "1749049"
  },
  {
    "text": "Python here",
    "start": "1749049",
    "end": "1752279"
  },
  {
    "text": "well okay one second",
    "start": "1758570",
    "end": "1764400"
  },
  {
    "text": "oh I serve it that live demo I also have",
    "start": "1764400",
    "end": "1776970"
  },
  {
    "text": "to do another one second there wouldn't be live demo without the credential problem I just want to put my",
    "start": "1776970",
    "end": "1788850"
  },
  {
    "text": "credentials up on the screen that's the hidden thing I did that last time it's",
    "start": "1788850",
    "end": "1794640"
  },
  {
    "text": "amazing how fast people will figure that out all right let's try this again",
    "start": "1794640",
    "end": "1801080"
  },
  {
    "text": "there we go so by the way what I had to do there is go into my credentials file and I had changed it to from default to",
    "start": "1804570",
    "end": "1811590"
  },
  {
    "text": "something else for my 8 OAS seal a cadet credentials in in preparation for the demo anyway what you're seeing here is",
    "start": "1811590",
    "end": "1816929"
  },
  {
    "text": "the is the important part and now you know this is a real demo I'm pushing log lines and this is this is real big data",
    "start": "1816929",
    "end": "1823529"
  },
  {
    "text": "this is an imitation of a fleet of web servers or applications servers of some kind and this is an imitation of people",
    "start": "1823529",
    "end": "1829919"
  },
  {
    "text": "hitting your app what you're seeing here all this stuff flying by is 32,000 lines of people of users actually usually it's",
    "start": "1829919",
    "end": "1836700"
  },
  {
    "text": "not people it's usually bots but bots and people hitting my application that's what this is okay so let's just hide",
    "start": "1836700",
    "end": "1842490"
  },
  {
    "text": "this for a while because that's gonna make you all dizzy this early but",
    "start": "1842490",
    "end": "1849299"
  },
  {
    "text": "another one up there instead and we'll have to wait about 60 seconds now and",
    "start": "1849299",
    "end": "1856830"
  },
  {
    "start": "1852000",
    "end": "1878000"
  },
  {
    "text": "while we're waiting let's talk about the next thing we why do we have to wait 60 seconds because if you were paying attention what I'm doing is sending it",
    "start": "1856830",
    "end": "1862200"
  },
  {
    "text": "in a Kinesis firehose and when I created the stream for firehose I set up a buffer time of 60 seconds so it's gonna",
    "start": "1862200",
    "end": "1868200"
  },
  {
    "text": "collect those lines for 60 seconds and then it'll batch them together gzip them and push them into s3 so let's",
    "start": "1868200",
    "end": "1874980"
  },
  {
    "text": "just hang out for 60 seconds to talk about spark so spark is amazing because",
    "start": "1874980",
    "end": "1881250"
  },
  {
    "start": "1878000",
    "end": "2009000"
  },
  {
    "text": "it's fast it's a general-purpose framework to do a lot of stuff most people use it for batch data processing",
    "start": "1881250",
    "end": "1887879"
  },
  {
    "text": "but there's also spark streaming if you've attended our big data bootcamp you know we we do a lot of that there's a graph database graphics there's a",
    "start": "1887879",
    "end": "1894450"
  },
  {
    "text": "machine learning it's very powerful and evolving framework and you install spark",
    "start": "1894450",
    "end": "1899669"
  },
  {
    "text": "on a cluster of some kind and you need something to manage that cluster and to distribute resources so if you have data",
    "start": "1899669",
    "end": "1906509"
  },
  {
    "text": "and you need something to tell spark where to go to find that data and how to work and that's what for example Hadoop",
    "start": "1906509",
    "end": "1911879"
  },
  {
    "text": "is Hadoop is you can run spark on Hadoop and we're running a dupe on EMR you can",
    "start": "1911879",
    "end": "1917009"
  },
  {
    "text": "also run the spark stand-alone cluster you can run spark on maize oh so you can run spark on any kind of cluster management tool I'm just choosing to use",
    "start": "1917009",
    "end": "1923639"
  },
  {
    "text": "EMR and Hadoop because it's easy and it's integrated in the NWS platform and",
    "start": "1923639",
    "end": "1928710"
  },
  {
    "text": "it's my sort of platform of choice but you have choice here spark is think of it like an application that runs on a distribution and I'm running it on the",
    "start": "1928710",
    "end": "1934700"
  },
  {
    "text": "EMR distribution of Hadoop so why is spark fast because it takes",
    "start": "1934700",
    "end": "1940520"
  },
  {
    "text": "better use of memory like I said it does a bunch of processing we don't have time to get into all the details of spark if you're interested come find me after but",
    "start": "1940520",
    "end": "1947300"
  },
  {
    "text": "long story short I used to give a demo with like two years ago where I did a data processing would take like 25 minutes he'd sit there and it was super",
    "start": "1947300",
    "end": "1953870"
  },
  {
    "text": "boring because MapReduce was designed to be scalable but slow SPARC can do that same thing in fact we do a version of it",
    "start": "1953870",
    "end": "1959690"
  },
  {
    "text": "today in about 5 seconds so it's a lot a lot faster orders of magnitude faster now the great thing about SPARC is that",
    "start": "1959690",
    "end": "1965150"
  },
  {
    "text": "who knows if it's gonna stick around everyone thought MapReduce was hot like five years ago and everyone's talking about SPARC and just when you think",
    "start": "1965150",
    "end": "1970610"
  },
  {
    "text": "you're grabbing your head around SPARC and now there's flink and everyone's talking about flink now it's the same in the streaming world everyone was talking",
    "start": "1970610",
    "end": "1975890"
  },
  {
    "text": "about storm and then Twitter who created storm turned around and said ah no we're done with storm we have Heron now it was",
    "start": "1975890",
    "end": "1981320"
  },
  {
    "text": "like oh man so just when you learn one thing you got to move to another but if I'm trying to reinforce anything today it's that the patterns don't change",
    "start": "1981320",
    "end": "1987770"
  },
  {
    "text": "sometimes the language is changed a little bit like Java - Scala or Python but end of the day what you're trying to",
    "start": "1987770",
    "end": "1993470"
  },
  {
    "text": "do remains fundamentally the same which is why I like working with higher-level frameworks a lot of the time like hive",
    "start": "1993470",
    "end": "1998930"
  },
  {
    "text": "or spark sequel it makes things kind of easy and you let someone else take care of the heavy lifting so you can get your",
    "start": "1998930",
    "end": "2004660"
  },
  {
    "text": "job done which we often forget in this world we have a job to do so spark",
    "start": "2004660",
    "end": "2010210"
  },
  {
    "start": "2009000",
    "end": "2067000"
  },
  {
    "text": "sequel makes it easy to run spark jobs I mentioned hive before there's also my",
    "start": "2010210",
    "end": "2016870"
  },
  {
    "text": "favorite one is uh is Pig which is another way to interact with data another sort of framework and language",
    "start": "2016870",
    "end": "2022090"
  },
  {
    "text": "and pig on spark is called pork the really great naming but spark sequel",
    "start": "2022090",
    "end": "2028030"
  },
  {
    "text": "essentially you can write sequel select star from Matt where big data equals fun and it gives you results we most of us",
    "start": "2028030",
    "end": "2035140"
  },
  {
    "text": "here know how to use sequel many of us here don't know how to write the equivalent in Scala right so spark",
    "start": "2035140",
    "end": "2040150"
  },
  {
    "text": "sequel prevents you from ever having to learn Scala which for most of us is a good thing and it takes care it",
    "start": "2040150",
    "end": "2046660"
  },
  {
    "text": "translates that sequel into a spark job and goes and runs that spark job just like in the old days like a year ago",
    "start": "2046660",
    "end": "2052240"
  },
  {
    "text": "when you wrote a hive sequel statement it would translate that into a MapReduce job and go run it so that's what spark",
    "start": "2052240",
    "end": "2058540"
  },
  {
    "text": "sequel is and that's what we're gonna use today where are you gonna do a bit of both we're going to do spark in Scala",
    "start": "2058540",
    "end": "2063879"
  },
  {
    "text": "and then I'm going to show you some sequel statements as well and lastly Zeppelin",
    "start": "2063880",
    "end": "2069840"
  },
  {
    "start": "2067000",
    "end": "2107000"
  },
  {
    "text": "this is me killing time while the data loads up Zeppelin is the web UI that makes all of this a little prettier a",
    "start": "2069840",
    "end": "2076409"
  },
  {
    "text": "little easier to use and also so you can save it there's nothing worse than running a bunch of commands and in a screen and SSH shell and then realizing",
    "start": "2076410",
    "end": "2084240"
  },
  {
    "text": "you screwed up way at the top then you forgot to set your line buffer in your terminal you can't scroll up anymore I mean these are real word problems for",
    "start": "2084240",
    "end": "2090240"
  },
  {
    "text": "big data people Zeppelin you can actually write notebook so you can save and export and import and in fact I give",
    "start": "2090240",
    "end": "2095580"
  },
  {
    "text": "you a notebook to import into Zeppelin so you don't have to copy/paste all this stuff so you can share code more easily see the results debug it makes working",
    "start": "2095580",
    "end": "2102900"
  },
  {
    "text": "with things like spark a lot easier okay",
    "start": "2102900",
    "end": "2108180"
  },
  {
    "start": "2107000",
    "end": "2145000"
  },
  {
    "text": "so let's see if we have anything I'm",
    "start": "2108180",
    "end": "2114810"
  },
  {
    "text": "gonna have to adjust my",
    "start": "2114810",
    "end": "2118190"
  },
  {
    "text": "I said to change my profile that I'm using now okay so let's check it out",
    "start": "2126730",
    "end": "2136420"
  },
  {
    "text": "where was step was I on so if all was working which I hope so we should have makes a little bigger",
    "start": "2136420",
    "end": "2145020"
  },
  {
    "start": "2145000",
    "end": "2175000"
  },
  {
    "text": "yeah so I'll show you so you believe me it's real but remember that bucket had",
    "start": "2145050",
    "end": "2150700"
  },
  {
    "text": "created I should have showed you the beginning that was empty so it's not a sleight of hand but I promise it was empty the MEA",
    "start": "2150700",
    "end": "2156040"
  },
  {
    "text": "and demo Chicago now has remember I said the prefix to fire hose and my fire hose and it now has these clusters now in",
    "start": "2156040",
    "end": "2162250"
  },
  {
    "text": "this case the 60 seconds didn't add up to a megabyte so instead of waiting until it's a mega it just waited 60",
    "start": "2162250",
    "end": "2168070"
  },
  {
    "text": "seconds but we have batches of log lines so if I switch back to that other thing",
    "start": "2168070",
    "end": "2173590"
  },
  {
    "text": "that was making us all dizzy you'll see that I'm still pushing we're question there and the number of files in that bucket will grow as the minutes pass so",
    "start": "2173590",
    "end": "2180700"
  },
  {
    "start": "2175000",
    "end": "2194000"
  },
  {
    "text": "we now have streaming data streaming from our cluster of fake web servers through Kinesis firehose into s3 we have",
    "start": "2180700",
    "end": "2186880"
  },
  {
    "text": "started to collect big data we've solved problem number one with big data now",
    "start": "2186880",
    "end": "2195130"
  },
  {
    "start": "2194000",
    "end": "2222000"
  },
  {
    "text": "what I did do to be fair is I did spin up a in a separate account I spent up EMR and redshift so that had",
    "start": "2195130",
    "end": "2201609"
  },
  {
    "text": "the full data set if we have time to do this on the if all 32,000 lines get up in time we'll work off that the real",
    "start": "2201609",
    "end": "2206680"
  },
  {
    "text": "time one but I also have a backup so you can see a little more data and the visualizations look a little prettier",
    "start": "2206680",
    "end": "2211770"
  },
  {
    "text": "but what we have to do now is connect to Zeppelin and connect to our SSH sorry to our EMR cluster and we connect to EMR",
    "start": "2211770",
    "end": "2218230"
  },
  {
    "text": "using SSH so in this case I'm gonna",
    "start": "2218230",
    "end": "2224050"
  },
  {
    "start": "2222000",
    "end": "2266000"
  },
  {
    "text": "connect to my I actually know let's connect to the one we just created let's do let's let's live dangerously so here",
    "start": "2224050",
    "end": "2230440"
  },
  {
    "text": "we are with the cluster it's up okay so remember we spun off that EMR cluster when it's in waiting state that means",
    "start": "2230440",
    "end": "2236050"
  },
  {
    "text": "it's available and now it gives us a address which is right here it's a copy-paste is I'm sorry it'll",
    "start": "2236050",
    "end": "2244780"
  },
  {
    "text": "make this easy across the screens this is why I use the CLI okay so just copy",
    "start": "2244780",
    "end": "2252160"
  },
  {
    "text": "it in here so I'm going to connect to that cluster",
    "start": "2252160",
    "end": "2257609"
  },
  {
    "start": "2266000",
    "end": "2346000"
  },
  {
    "text": "now what I've done here let me just copy again so you can see it I'm just a",
    "start": "2266430",
    "end": "2271440"
  },
  {
    "text": "little inception okay if you look at this command sometimes a little messy let me make it bigger just this is one",
    "start": "2271440",
    "end": "2279599"
  },
  {
    "text": "concept I wanna make sure you understand is poor forwarding if you're using there's all these applications running on your EMR cluster right and a lot of",
    "start": "2279599",
    "end": "2287130"
  },
  {
    "text": "these applications like spark like MapReduce and in this case like Zeppelin have web interfaces but EMR remember is",
    "start": "2287130",
    "end": "2293789"
  },
  {
    "text": "running in my AWS account and those ports are not necessarily open to the world by design I don't want anyone",
    "start": "2293789",
    "end": "2300450"
  },
  {
    "text": "accessing my EMR cluster right so we can use something called SSH port forwarding to effectively create kind of like a",
    "start": "2300450",
    "end": "2306329"
  },
  {
    "text": "poor-man's tunnel and that allows the in this case the zeppelin port which is eight eight nine zero that's the port of the web",
    "start": "2306329",
    "end": "2312539"
  },
  {
    "text": "interface to be exposed on my local laptop a localhost i just arbitrarily picked 18 890 because",
    "start": "2312539",
    "end": "2319799"
  },
  {
    "text": "amazon doesn't block that port in my laptop but you can pick any port and so using that dash l allows you to forward",
    "start": "2319799",
    "end": "2325859"
  },
  {
    "text": "local locally exposed interfaces and ports from like the machine that you're connecting to and expose them on",
    "start": "2325859",
    "end": "2332160"
  },
  {
    "text": "localhost on your machine so it's a great way without having to set up fancy VPNs or anything like that to access",
    "start": "2332160",
    "end": "2337559"
  },
  {
    "text": "stuff so i have connected to this cluster and forward in that port so now if i have some luck i should be able to",
    "start": "2337559",
    "end": "2347609"
  },
  {
    "start": "2346000",
    "end": "2389000"
  },
  {
    "text": "see zeppelin and i'm actually looking at the zeppelin interface on my EMR cluster this is a new zeppelin so i guess i'll",
    "start": "2347609",
    "end": "2357779"
  },
  {
    "text": "import it rather than do a bunch of copy pasting that's not fun for everybody",
    "start": "2357779",
    "end": "2362778"
  },
  {
    "text": "can't see just call it chicago now i have exported the zeppelin notebook just",
    "start": "2362900",
    "end": "2368130"
  },
  {
    "text": "so you don't don't watch me copy/paste a ton but it could have just created new",
    "start": "2368130",
    "end": "2374069"
  },
  {
    "text": "notes so here we are this is this is Epling and the first command that we're",
    "start": "2374069",
    "end": "2383880"
  },
  {
    "text": "looking at it's going to start interacting and processing our data so",
    "start": "2383880",
    "end": "2390420"
  },
  {
    "start": "2389000",
    "end": "2530000"
  },
  {
    "text": "just a reminder where we are we're pushing data into Kinesis firehose it's getting persistent into s3 and now we're",
    "start": "2390420",
    "end": "2396630"
  },
  {
    "text": "going to use EMR with spark to interact directly with that three and we didn't really talk about",
    "start": "2396630",
    "end": "2401850"
  },
  {
    "text": "yet that yet but this is where it gets really exciting because with redshift and with most databases you need to load",
    "start": "2401850",
    "end": "2407190"
  },
  {
    "text": "the data into that database first to interact with it but if you're using Hadoop and with a framework that",
    "start": "2407190",
    "end": "2412770"
  },
  {
    "text": "supports s3 as a as a data source like you know MapReduce on EMR or SPARC you",
    "start": "2412770",
    "end": "2419610"
  },
  {
    "text": "can actually interact with the data manipulate the data directly on s3 you don't need HDFS you don't need to load",
    "start": "2419610",
    "end": "2425010"
  },
  {
    "text": "it into some into some intermediary so it's a great fast way you can spin up an EMR cluster that can go away when you're",
    "start": "2425010",
    "end": "2431490"
  },
  {
    "text": "done during the analysis and your data stays safe on s3 you can transform that data in place on",
    "start": "2431490",
    "end": "2436530"
  },
  {
    "text": "s3 wipe out the cluster now if you think about this this is enormous ly powerful because you can have petabytes of data",
    "start": "2436530",
    "end": "2442560"
  },
  {
    "text": "on s3 and when you need to analyze it you can spin up a cluster an EMR cluster do whatever you got to do exploratory",
    "start": "2442560",
    "end": "2448620"
  },
  {
    "text": "analytics data processing changing it and then wipe that cluster out and you only pay for the time that you had that",
    "start": "2448620",
    "end": "2453810"
  },
  {
    "text": "cluster up and the data stored in s3 is super cheap now contrast this to a on-premises data warehouse you're paying",
    "start": "2453810",
    "end": "2461220"
  },
  {
    "text": "like sometimes millions of bucks to have the compute and the storage coupled together running on-premises and you",
    "start": "2461220",
    "end": "2466560"
  },
  {
    "text": "have to keep them there it's not like you can get throw out the racks of servers when you when you don't need to do the analysis that's not true",
    "start": "2466560",
    "end": "2472230"
  },
  {
    "text": "but with EMR when you have all this data and your CTO wakes up really angry one day and says you know give this to me by",
    "start": "2472230",
    "end": "2477900"
  },
  {
    "text": "Monday morning you can just spin up that cluster hit that data shut down the cluster and still meet your budget so by",
    "start": "2477900",
    "end": "2483570"
  },
  {
    "text": "decoupling the storage keeping the storage in s3 from the compute in this state in this case EMR it gives you that",
    "start": "2483570",
    "end": "2489420"
  },
  {
    "text": "that costs that value for money that flexibility so we're gonna use a little Scala I wanted to use real Scala to show",
    "start": "2489420",
    "end": "2496440"
  },
  {
    "text": "that it's actually an easy language and anyone can pick it up and we're gonna do something very simple we're gonna load",
    "start": "2496440",
    "end": "2502290"
  },
  {
    "text": "in or what not load in we're gonna create it an object from s3 the object",
    "start": "2502290",
    "end": "2507990"
  },
  {
    "text": "in SPARC terminology is called an RDD it's going to point to that location that we're currently pushing data to in",
    "start": "2507990",
    "end": "2513720"
  },
  {
    "text": "s3 it's not going to pull the data in it that's actually gonna act on the data there we're gonna count the amount of",
    "start": "2513720",
    "end": "2519450"
  },
  {
    "text": "lines there and we're going to print one of the lines and then we're going to do",
    "start": "2519450",
    "end": "2524760"
  },
  {
    "text": "some basic data manipulation in this case we're gonna split the log up so",
    "start": "2524760",
    "end": "2530550"
  },
  {
    "start": "2530000",
    "end": "2645000"
  },
  {
    "text": "let's check it out now I loaded it so it already sort of did that ahead of time but did you shift enter to run this and this goes off and",
    "start": "2530550",
    "end": "2537859"
  },
  {
    "text": "runs the spark job and it's gonna do a",
    "start": "2537859",
    "end": "2543079"
  },
  {
    "text": "count it looks like all of the logs are",
    "start": "2543079",
    "end": "2549710"
  },
  {
    "text": "already in there let's just check while we're waiting here no well close enough",
    "start": "2549710",
    "end": "2556270"
  },
  {
    "text": "so now it's doing its running and it's actually iterating over those gzip files in s3 and in this case we're counting",
    "start": "2558069",
    "end": "2565280"
  },
  {
    "text": "all the lines and seeing what the total count is now if you were to do this in MapReduce we'd be sitting here for another 20 minutes",
    "start": "2565280",
    "end": "2571309"
  },
  {
    "text": "but it with spark even on a single single node access tonight whoa okay",
    "start": "2571309",
    "end": "2576890"
  },
  {
    "text": "sorry oh because I have the wrong bucket that's what I get for copy pasting",
    "start": "2576890",
    "end": "2582790"
  },
  {
    "text": "Chicago is where I put it right try to",
    "start": "2582790",
    "end": "2589550"
  },
  {
    "text": "keep a human let this run",
    "start": "2589550",
    "end": "2596619"
  },
  {
    "text": "their view always seems to be working out okay cool so so far we have loaded in 15,000 lines that makes sense there's",
    "start": "2601119",
    "end": "2608170"
  },
  {
    "text": "about 39,000 total and if we look at this I asked it to show me the first line of s3 just we can see what's",
    "start": "2608170",
    "end": "2614589"
  },
  {
    "text": "actually the first line of the first log inside that gzip file and then to actually do some parsing and this is the",
    "start": "2614589",
    "end": "2620230"
  },
  {
    "text": "result of the parsing when things are parsed out nothing fancy I just wanted to show you that we can access the data in s3 and do account if",
    "start": "2620230",
    "end": "2626920"
  },
  {
    "text": "you think of this in just a few seconds there after I fixed the name of the bucket we counted 15,000 lines of logs",
    "start": "2626920",
    "end": "2632920"
  },
  {
    "text": "in s3 using a single node or to know DMR cluster in just a few seconds without loading the data into a database there's",
    "start": "2632920",
    "end": "2638529"
  },
  {
    "text": "no index there's no database here it was actually reaching into the files on s3 and doing a line count and that's pretty powerful so let's do something real so",
    "start": "2638529",
    "end": "2647910"
  },
  {
    "text": "rumor has it that not all logs are clean and that there's a lot of junk that gets introduced into logs",
    "start": "2647910",
    "end": "2653529"
  },
  {
    "text": "what more often than not what happens is log formats are all over the place some of them are JSON some of them are XML",
    "start": "2653529",
    "end": "2658779"
  },
  {
    "text": "some of them are tab delimited some of them are CSV some of them are just Bob who you hired and just threw some stuff",
    "start": "2658779",
    "end": "2663880"
  },
  {
    "text": "in the bucket that he shouldn't have and if you try to load all that stuff into a database your database will choke",
    "start": "2663880",
    "end": "2669700"
  },
  {
    "text": "because you're doing your through your relational database because your relational database will say well hold on I have column a which is host column",
    "start": "2669700",
    "end": "2676359"
  },
  {
    "text": "B which is refer column C which is whatever like oh yeah HTTP status code and Bob had Bob loves noodles and they",
    "start": "2676359",
    "end": "2683980"
  },
  {
    "text": "don't fit into the column and so you need a way to normalize to clean your data and to potentially pitch out some",
    "start": "2683980",
    "end": "2690190"
  },
  {
    "text": "of the data like Bob's data that isn't good in this case I want to do some data manipulation maybe to drop some of the",
    "start": "2690190",
    "end": "2695920"
  },
  {
    "text": "data I don't need remember I talked about Omniture logs and they have like 90 columns in some cases I only need",
    "start": "2695920",
    "end": "2700930"
  },
  {
    "text": "like 10 columns of data to derive the value that I want out of those logs so I want to drop the rest of the data maybe",
    "start": "2700930",
    "end": "2706480"
  },
  {
    "text": "I need to pick some of the columns out so you could hire that summer intern to write that Python script to do a regular",
    "start": "2706480",
    "end": "2711970"
  },
  {
    "text": "expression or maybe in Perl or something if that but that won't scale what instead you can do is run EMR and use",
    "start": "2711970",
    "end": "2717549"
  },
  {
    "text": "something like spark and in this case Scala which is really good at doing essentially replace all is in text",
    "start": "2717549",
    "end": "2724660"
  },
  {
    "text": "replacements and field manipulation so in this case I'm doing something simple I'm taking out commas and it you know it",
    "start": "2724660",
    "end": "2730630"
  },
  {
    "text": "looks like spaghetti code on the screen here but if you look carefully it's really not that okay I'm just doing basic string manipulation and if you've ever written",
    "start": "2730630",
    "end": "2736960"
  },
  {
    "text": "a regular expression in your editor of choice this should make sense so let's",
    "start": "2736960",
    "end": "2742960"
  },
  {
    "start": "2742000",
    "end": "2794000"
  },
  {
    "text": "do that sorry that's a little small make it bigger so it's the same code I have",
    "start": "2742960",
    "end": "2749859"
  },
  {
    "text": "there and what's cool about this is that a it just finished already and be I'm",
    "start": "2749859",
    "end": "2756130"
  },
  {
    "text": "I'm setting up the transformations such that it'll go then and apply that transformation to the files in place on",
    "start": "2756130",
    "end": "2761589"
  },
  {
    "text": "s3 so it's kind of loaded the stuff that needs to do in memory you can do a bunch of operations in memory and then you can",
    "start": "2761589",
    "end": "2766720"
  },
  {
    "text": "tell it to write back and treat s3 like a database so it works very very quickly again if I was to do this with MapReduce",
    "start": "2766720",
    "end": "2772349"
  },
  {
    "text": "it would actually have to go off and do it it would take like 20 minutes with SPARC it's really fast so now I'm going",
    "start": "2772349",
    "end": "2779020"
  },
  {
    "text": "to create now that I've done my data transformation I did something really simple you would probably have to do a lot more or maybe not maybe you just",
    "start": "2779020",
    "end": "2785680"
  },
  {
    "text": "need to remove every third comma from your text file you could write that in two lines of scala now I'm going to",
    "start": "2785680",
    "end": "2791320"
  },
  {
    "text": "create a table which in spark parlance is called a data frame and then I'm",
    "start": "2791320",
    "end": "2797680"
  },
  {
    "start": "2794000",
    "end": "2836000"
  },
  {
    "text": "gonna actually apply the transformation so I'm just doing basic text manipulation here let's do that I don't",
    "start": "2797680",
    "end": "2805599"
  },
  {
    "text": "expect you to all read this code you can read it on your spare time at home but I just I wanted to make this slide deck complete so that you can so even if it",
    "start": "2805599",
    "end": "2811660"
  },
  {
    "text": "doesn't make the nicest show when you when you take it home you have all the code you need in the slides so again",
    "start": "2811660",
    "end": "2818440"
  },
  {
    "text": "this is just basic string manipulation that we're doing to normalize our data and why are we normalizing our data because we're gonna put this data into",
    "start": "2818440",
    "end": "2824260"
  },
  {
    "text": "redshift and redshift is a structured database it's a data warehouse so I need to make sure that my data Maps up to the",
    "start": "2824260",
    "end": "2830349"
  },
  {
    "text": "columns of my table in redshift and that's what I'm doing okay so let's go",
    "start": "2830349",
    "end": "2837430"
  },
  {
    "start": "2836000",
    "end": "2964000"
  },
  {
    "text": "back and actually do this so here we go let's create the table and we'll apply",
    "start": "2837430",
    "end": "2845829"
  },
  {
    "text": "the transformations pretty fast well I",
    "start": "2845829",
    "end": "2851829"
  },
  {
    "text": "don't need to drop it cuz there's a new cluster now this is this is when we apply the transformations what that",
    "start": "2851829",
    "end": "2857380"
  },
  {
    "text": "actually means is that we're gonna take all the transformations that we've done in this inspark in memory and we're gonna write them back into s3 so we're",
    "start": "2857380",
    "end": "2864670"
  },
  {
    "text": "gonna take the data and essentially funnel it through spark back in s3 and in this case we're doing back into a",
    "start": "2864670",
    "end": "2869890"
  },
  {
    "text": "separate folder if you will or prefix into s3 so we do that by creating an",
    "start": "2869890",
    "end": "2875800"
  },
  {
    "text": "external table now those of you who are paying attention will notice that I've done something here I switched to sequel",
    "start": "2875800",
    "end": "2882359"
  },
  {
    "text": "oops oh the buckets wrong thank you",
    "start": "2882359",
    "end": "2887430"
  },
  {
    "text": "someone has pay attention Chicago that's",
    "start": "2889740",
    "end": "2895390"
  },
  {
    "text": "where we are there we go so I'm using",
    "start": "2895390",
    "end": "2900970"
  },
  {
    "text": "sequel now so now we're using spark sequel to interact with the data I could have done did the data transformation",
    "start": "2900970",
    "end": "2906010"
  },
  {
    "text": "and the earlier steps in sparks equal but sequel is not a great language for doing data transformation it's",
    "start": "2906010",
    "end": "2911500"
  },
  {
    "text": "inefficient it's if you needed to do the same data or text transformation that I did in those previous step so we're in a",
    "start": "2911500",
    "end": "2917559"
  },
  {
    "text": "way way longer in sequel so even if you're comfortable to sequel it actually helps to learn another language like",
    "start": "2917559",
    "end": "2922630"
  },
  {
    "text": "Scala or Java Scala is much more condensed than Java to to learn to do",
    "start": "2922630",
    "end": "2929200"
  },
  {
    "text": "the data transformation so there is created now we're just going to set some options with hive and all this is doing is just telling hive to split the files",
    "start": "2929200",
    "end": "2936549"
  },
  {
    "text": "up and to compress them when it writes back to s3 this is the kind of thing you google and copy paste so if you if",
    "start": "2936549",
    "end": "2946299"
  },
  {
    "text": "you're reading quickly here I'm just telling it to you gzip when it writes out and to not get too stressed about",
    "start": "2946299",
    "end": "2952420"
  },
  {
    "text": "the data and now we're gonna write it",
    "start": "2952420",
    "end": "2958349"
  },
  {
    "text": "okay so this will run and let's look back to the console for a second so if I",
    "start": "2958829",
    "end": "2966220"
  },
  {
    "text": "go into s3 here we have firehose that we created let me make its bigger so you",
    "start": "2966220",
    "end": "2972309"
  },
  {
    "text": "can see it here we have firehose and this is where the data that we were writing through firehose from our fake",
    "start": "2972309",
    "end": "2977410"
  },
  {
    "text": "web logs was going and you'll see that all the packages there now there's probably all 39,000 lines now and oh",
    "start": "2977410",
    "end": "2984069"
  },
  {
    "text": "this is actually Chicago this is the she once or actually go to my real one there",
    "start": "2984069",
    "end": "2990180"
  },
  {
    "text": "that's a we're using the one we're doing in real time rather than use the the pre-baked one just have some fun in the",
    "start": "2990180",
    "end": "2996190"
  },
  {
    "text": "morning so here's Chicago this is the one we actually did today firehose 2016",
    "start": "2996190",
    "end": "3003290"
  },
  {
    "text": "that's today and here's the little bundles if we go back to the Chicago",
    "start": "3007010",
    "end": "3013650"
  },
  {
    "text": "access locks processed this is what we're writing right now from Zeppelin and from spark it's the middle of",
    "start": "3013650",
    "end": "3019740"
  },
  {
    "text": "writing them and these little gzip files are the transform data so it's taking those transformations those text manipulations that we did to normalize",
    "start": "3019740",
    "end": "3026730"
  },
  {
    "text": "the data such that I'll match up with the way the schema the table that we have in redshift and now we're going to",
    "start": "3026730",
    "end": "3032130"
  },
  {
    "text": "do is take these files and load them in a redshift now that we have normalized the data we've cleansed the data we've processed the data it's ready to get",
    "start": "3032130",
    "end": "3038040"
  },
  {
    "text": "loaded into our database we're the",
    "start": "3038040",
    "end": "3043410"
  },
  {
    "text": "homestretch this is the steps I just took it to right before we actually",
    "start": "3043410",
    "end": "3050790"
  },
  {
    "start": "3049000",
    "end": "3145000"
  },
  {
    "text": "loaded in though I wanted to show you that you can actually query the data in spark as well and the line between data",
    "start": "3050790",
    "end": "3056040"
  },
  {
    "text": "warehouses insert the spark world and redshift is actually increasingly blurry you can do a lot of data warehouse stuff",
    "start": "3056040",
    "end": "3061680"
  },
  {
    "text": "in spark and hive in flink that you can do in redshift it's just the redshift is",
    "start": "3061680",
    "end": "3067050"
  },
  {
    "text": "is more like mature it was supports a ritual richer sequel syntax using sequel",
    "start": "3067050",
    "end": "3073980"
  },
  {
    "text": "for true like you know that data warehouse guy that you hired in 1979 and like he has his crazy sequel statements",
    "start": "3073980",
    "end": "3079890"
  },
  {
    "text": "that he's written that do these amazing insights for your business they will likely not work on spark sequel today",
    "start": "3079890",
    "end": "3085170"
  },
  {
    "text": "they will work on redshift redshift is really designed for the enterprise-grade sequel but if you're doing exploratory",
    "start": "3085170",
    "end": "3090570"
  },
  {
    "text": "analytics spark and spark sequel is amazing and what's exploratory analytics this where you don't know what you have",
    "start": "3090570",
    "end": "3095940"
  },
  {
    "text": "yet what if you inherit a giant data source or a giant data set and you don't know what the structure is you can't",
    "start": "3095940",
    "end": "3101520"
  },
  {
    "text": "don't have a luxury I'm just loading it into a redshift because you don't know the schema you don't know that columns are so doing exploratory analytics on",
    "start": "3101520",
    "end": "3108180"
  },
  {
    "text": "high round spark sequel is amazingly powerful and soon will release Kinesis analytics and you'll be able to do",
    "start": "3108180",
    "end": "3114200"
  },
  {
    "text": "analytics sequel based analytics on your data in your Kinesis stream as well but",
    "start": "3114200",
    "end": "3119310"
  },
  {
    "text": "in the meantime we can do this we can use spark to do basic things like how much data do have what does it actually look like what tables do I need to",
    "start": "3119310",
    "end": "3125280"
  },
  {
    "text": "create in my database what transformations do I have to do to normalize this data and that type of exploratory data where it doesn't matter",
    "start": "3125280",
    "end": "3130890"
  },
  {
    "text": "what format the data in it is in because spark doesn't care it's not structured is enormous ly powerful and not only",
    "start": "3130890",
    "end": "3137369"
  },
  {
    "text": "that but it can do it on top of any data source an s3 so someone hands you an s3 bucket you can use sparks equal to kind",
    "start": "3137369",
    "end": "3142499"
  },
  {
    "text": "of look into it check it out so let's do that so they did oh yeah so",
    "start": "3142499",
    "end": "3151710"
  },
  {
    "start": "3145000",
    "end": "3297000"
  },
  {
    "text": "it's loaded it's now sitting in s3 so let's do a count I don't know how many because we're using the real the real",
    "start": "3151710",
    "end": "3156720"
  },
  {
    "text": "demo here I don't know how many lines actually made it in before you ran this oh well let's see in a second it's running there's 39 thousand total if we",
    "start": "3156720",
    "end": "3163769"
  },
  {
    "text": "do this in the full data set let's see how many made it in is that little guy still running it's still going so not",
    "start": "3163769",
    "end": "3169499"
  },
  {
    "text": "all 39,000 I'll be there but we still not bad using the McCormick Place Wi-Fi we managed to get 15,000 logged lines in",
    "start": "3169499",
    "end": "3175650"
  },
  {
    "text": "in the time I was talking here so that's what's actually an s3 that's actually what we pushed and let's let's do a",
    "start": "3175650",
    "end": "3180990"
  },
  {
    "text": "little exploratory analytics let's do something more let's actually see the data see how fast that was so I just ran",
    "start": "3180990",
    "end": "3188190"
  },
  {
    "text": "that query and it showed me the the lines in the in the data frame or in the table so this is the actual data now",
    "start": "3188190",
    "end": "3194700"
  },
  {
    "text": "parsed into columns let's do something more real let's actually do an analytic type thing",
    "start": "3194700",
    "end": "3200359"
  },
  {
    "text": "sorry so in this case I'm actually doing",
    "start": "3200359",
    "end": "3207690"
  },
  {
    "text": "some kind of a real query let me make that a little bigger for you oops",
    "start": "3207690",
    "end": "3213259"
  },
  {
    "text": "so select request path take account from access logs where response code is 404",
    "start": "3217560",
    "end": "3223830"
  },
  {
    "text": "and give me the top three if you speak sequel that's what I just asked so this is a really simple thing if I run a",
    "start": "3223830",
    "end": "3229350"
  },
  {
    "text": "website and it has a billion pages because I'm I'm in a retail company that you may know some of those pages you",
    "start": "3229350",
    "end": "3235710"
  },
  {
    "text": "know may have dropped off the radar and I want to find out what is throwing 303's well it turns out that I have missing five icons on one hundred",
    "start": "3235710",
    "end": "3242040"
  },
  {
    "text": "thirty-four pages so right there I have an insight and this is admittedly a bit of a trivial one but that's something",
    "start": "3242040",
    "end": "3249060"
  },
  {
    "text": "you're getting misses on faith there's there's fav IKOS missing you know the little logos for websites on a bunch of",
    "start": "3249060",
    "end": "3254580"
  },
  {
    "text": "my pages and that's not a good user experience maybe so it's a little because I've zoomed in there it's all",
    "start": "3254580",
    "end": "3260010"
  },
  {
    "text": "squished up but what this is showing you is that Zeppelin and combined with spark steeple you can actually do basic",
    "start": "3260010",
    "end": "3265260"
  },
  {
    "text": "visualizations in Zeppelin and you can do sequel exploratory analytics without even firing up tableau or a quick site",
    "start": "3265260",
    "end": "3270930"
  },
  {
    "text": "or whatever maybe you can answer that business question that your CTO is pressuring you without ever leaving Zeppelin without ever loading at a",
    "start": "3270930",
    "end": "3276930"
  },
  {
    "text": "redshift without ever going to Aurora it's pretty powerful but most business",
    "start": "3276930",
    "end": "3282060"
  },
  {
    "text": "problems are not solved by Matz select statement like this this is a pretty simple statement once you get into the",
    "start": "3282060",
    "end": "3287070"
  },
  {
    "text": "heavier stuff you need to move to that data warehouse so let's do that so last but not least we showed you those in s3",
    "start": "3287070",
    "end": "3295020"
  },
  {
    "text": "we're gonna analyze the data so with that cluster remember that cluster way back when we spun up let's",
    "start": "3295020",
    "end": "3300750"
  },
  {
    "start": "3297000",
    "end": "3599000"
  },
  {
    "text": "check it out it's up it's available there it is let me grab the the the name sorry the of",
    "start": "3300750",
    "end": "3309930"
  },
  {
    "text": "the cluster make sure that I have I'll sorry let me get the real one we spun up",
    "start": "3309930",
    "end": "3316430"
  },
  {
    "text": "well that's it okay so it's publicly available I should be able to connect to it I'll do another window here",
    "start": "3316430",
    "end": "3328640"
  },
  {
    "text": "you can hear a pin drop in here okay",
    "start": "3338690",
    "end": "3344349"
  },
  {
    "text": "here we go so I'm going to use again because you know you can probably tell I like the command line now you can use any tool",
    "start": "3344349",
    "end": "3350150"
  },
  {
    "text": "some people use I really like a JUnit II workbench for a redshift if you like a",
    "start": "3350150",
    "end": "3355249"
  },
  {
    "text": "GUI or sequel what's called sequel workbench for Mac is okay pretty much anything that can talk Postgres but",
    "start": "3355249",
    "end": "3360650"
  },
  {
    "text": "ideally can work with the custom redshift drivers connect connect to redshift in this case I'm just using the",
    "start": "3360650",
    "end": "3365749"
  },
  {
    "text": "Postgres command-line client piece equal",
    "start": "3365749",
    "end": "3369369"
  },
  {
    "text": "there should be eight if this doesn't get act for some reason I'll just use the one I have backup I've lost track",
    "start": "3371829",
    "end": "3377150"
  },
  {
    "text": "now which is which might be blocking me",
    "start": "3377150",
    "end": "3383539"
  },
  {
    "text": "on the Wi-Fi one second",
    "start": "3383539",
    "end": "3387339"
  },
  {
    "text": "either on hostname sorry",
    "start": "3399099",
    "end": "3402720"
  },
  {
    "text": "or it's not connecting hope the Wi-Fi is not blocking me today",
    "start": "3410510",
    "end": "3415299"
  },
  {
    "text": "let's stop the other guy that's taking up all my shoot",
    "start": "3422490",
    "end": "3431520"
  },
  {
    "text": "hmm okay so guys it could be having a",
    "start": "3434700",
    "end": "3439950"
  },
  {
    "text": "firewall issue",
    "start": "3439950",
    "end": "3442578"
  },
  {
    "text": "let's try one more time and if not we'll just go straight a quick sight",
    "start": "3465160",
    "end": "3469529"
  },
  {
    "text": "okay well while we're waiting for that my apologies let's go to quick site",
    "start": "3475310",
    "end": "3481130"
  },
  {
    "text": "there was much to show you on the command line anyway I was just gonna do a sequel statement but what I wanted to",
    "start": "3482180",
    "end": "3487590"
  },
  {
    "text": "show you was that connecting the redshift is easy and that you can use the Postgres client and just issue sequel statements against it so this is",
    "start": "3487590",
    "end": "3493890"
  },
  {
    "text": "quick site rather simple UI and it's designed for analysis for doing data",
    "start": "3493890",
    "end": "3499950"
  },
  {
    "text": "visualization and what quick site does is you can either analyze directly the",
    "start": "3499950",
    "end": "3505170"
  },
  {
    "text": "data sources in locations like redshift or you can load it into its high speed in memory data storage called spice so",
    "start": "3505170",
    "end": "3513060"
  },
  {
    "text": "depending on what types of visualizations you're doing you can tell stories where you put together a number of different visualizations if you need",
    "start": "3513060",
    "end": "3518670"
  },
  {
    "text": "those visually it'll be really high speed and changing in real time makes sense to load that data in this spice but for a quick site for our purposes",
    "start": "3518670",
    "end": "3526200"
  },
  {
    "text": "just using it as a database tool to look at redshift it's perfect so in this case",
    "start": "3526200",
    "end": "3532320"
  },
  {
    "text": "I've set up a similar visualization for redshift this is talking to redshift and",
    "start": "3532320",
    "end": "3538470"
  },
  {
    "text": "it's doing something similar to what I did before where I've set up a query that essentially asks for all access",
    "start": "3538470",
    "end": "3546510"
  },
  {
    "text": "logs the request path it's similar to the one we ran in Zeppelin before I've put in a filter where the requests",
    "start": "3546510",
    "end": "3551910"
  },
  {
    "text": "equals four four four hundred four we could change this just to do two hundred",
    "start": "3551910",
    "end": "3557640"
  },
  {
    "text": "to see what is the top pages where are we getting the most HTTP okay's and there we go let's make this we can",
    "start": "3557640",
    "end": "3563850"
  },
  {
    "text": "actually see it so Oh",
    "start": "3563850",
    "end": "3568860"
  },
  {
    "text": "Alexa it's not the Alexa that we all know and love that I have in my living room but it's the it's the web ranking",
    "start": "3568860",
    "end": "3574350"
  },
  {
    "text": "tool but it's a it's a widget that's getting hit a lot so we see 800,000 hits the response code 200 so we're getting",
    "start": "3574350",
    "end": "3580680"
  },
  {
    "text": "an HTTP okay for 200 on Alexa for our widgets RSS is also really popular now",
    "start": "3580680",
    "end": "3587160"
  },
  {
    "text": "this is an insight it turns out that we have 500 to thousand people using our RSS feed or at least five hundred to",
    "start": "3587160",
    "end": "3593280"
  },
  {
    "text": "thousand hits in the time that these weblog is represented so that's a decent business insight we should keep the RSS",
    "start": "3593280",
    "end": "3598560"
  },
  {
    "text": "up so these are pretty simple graphs but you can use quick cite I just wanted to show it to you quickly to build any",
    "start": "3598560",
    "end": "3605190"
  },
  {
    "text": "number of data visualizations against the tables that you have in redshift you can load into spice or hit directly in",
    "start": "3605190",
    "end": "3611119"
  },
  {
    "text": "redshift it's a quick and easy way to build database and to create stories for your business",
    "start": "3611119",
    "end": "3616520"
  },
  {
    "text": "the Louden to see the insights in your data oh it was just slow ok let's do",
    "start": "3616520",
    "end": "3629599"
  },
  {
    "text": "some redshift we'll just backtrack for a second so here we go almost done here so I'm gonna create a table just like I",
    "start": "3629599",
    "end": "3635839"
  },
  {
    "text": "wouldn't sequel oh let's drop it so",
    "start": "3635839",
    "end": "3648710"
  },
  {
    "text": "there I created a table and this table corresponds to the fields that remember when we did the data transformation using spark so if I had selected or used",
    "start": "3648710",
    "end": "3656480"
  },
  {
    "text": "the the text manipulation to pull out the fields I wanted this is what it's corresponding to redshift is a well-structured data warehouse I didn't",
    "start": "3656480",
    "end": "3663319"
  },
  {
    "text": "really optimize my column sizes don't judge me but we have a distribution key which distributes the data across the",
    "start": "3663319",
    "end": "3668869"
  },
  {
    "text": "cluster in this case it's a single node cluster so it doesn't matter that much and we're using request time as the sort key we don't have time to get into the",
    "start": "3668869",
    "end": "3675109"
  },
  {
    "text": "nuances of optimizing a data warehouse but just take this take away choose your",
    "start": "3675109",
    "end": "3680569"
  },
  {
    "text": "distribution and store keys it's important to choose them well because if",
    "start": "3680569",
    "end": "3685640"
  },
  {
    "text": "you choose them properly your data gets distributed across the data storage in your cluster in such a way that your queries will run faster likewise with",
    "start": "3685640",
    "end": "3692210"
  },
  {
    "text": "sort keys so read up on distribution keys in store keys it'll save you tons of seconds waiting for queries to come",
    "start": "3692210",
    "end": "3697819"
  },
  {
    "text": "back ok so now let's actually do something let's load data from s3 so",
    "start": "3697819",
    "end": "3704240"
  },
  {
    "text": "remember we use spark 2 now if you copy",
    "start": "3704240",
    "end": "3710630"
  },
  {
    "text": "that access key don't worry I get rid of it right after the presentation now that was not a joke that was how fast",
    "start": "3710630",
    "end": "3717589"
  },
  {
    "text": "redshift loaded 15,000 rows from our table into our database so what we've",
    "start": "3717589",
    "end": "3725030"
  },
  {
    "text": "done is we've taken the the transform data that we transform a spark and we've loaded it into redshift and now we",
    "start": "3725030",
    "end": "3730220"
  },
  {
    "text": "should be able to query that data directly in redshift so let's do just",
    "start": "3730220",
    "end": "3738109"
  },
  {
    "text": "some simple queries just to show you that it's working so there we go here's a count of",
    "start": "3738109",
    "end": "3745490"
  },
  {
    "text": "different response codes we're getting a lot of 200s on a lot of people more people visiting some insights here more",
    "start": "3745490",
    "end": "3751700"
  },
  {
    "text": "people visiting our site on July 21st than July 22nd and a lot of 404s and we probably have to clean up and some other",
    "start": "3751700",
    "end": "3757490"
  },
  {
    "text": "weird errors that we probably have to look into across our app so there we have some business insights that our coders can go fix let's just do one or",
    "start": "3757490",
    "end": "3763880"
  },
  {
    "text": "two others just to check it out this is the one I was running out of spark so we",
    "start": "3763880",
    "end": "3771290"
  },
  {
    "text": "have 290 in the the new data set that I have a 404 and last but not least give",
    "start": "3771290",
    "end": "3777740"
  },
  {
    "text": "me some business insights from that what is causing all those four fours the",
    "start": "3777740",
    "end": "3783920"
  },
  {
    "text": "favicon so if we go and fix that one will reduce our 404 our error count effectively for web requests",
    "start": "3783920",
    "end": "3789680"
  },
  {
    "text": "dramatically by just fixing that one fatal favicon or adding it to our to our top-level tree so what I wanted to show",
    "start": "3789680",
    "end": "3795710"
  },
  {
    "text": "you before I was waiting for a red shift in the Wi-Fi to connect was that this is great but if you pull up a terminal and show",
    "start": "3795710",
    "end": "3802760"
  },
  {
    "text": "that to your CEO that they're gonna not smile you want to show them a nice graph you want to show them insights and",
    "start": "3802760",
    "end": "3808100"
  },
  {
    "text": "that's where quick side comes in so we'll end with this again",
    "start": "3808100",
    "end": "3813040"
  },
  {
    "text": "so quick site takes what I just showed you in that black and white console and turns it into something pretty and you",
    "start": "3813160",
    "end": "3819530"
  },
  {
    "text": "can add visuals you can have multiple graphs and you can take that data that we've extracted that we've stored that",
    "start": "3819530",
    "end": "3825140"
  },
  {
    "text": "we've collected that we've transformed and turned it into a real business insight for whatever you are doing and",
    "start": "3825140",
    "end": "3830990"
  },
  {
    "text": "that's what I want to show you today in real time with just a couple glitches we built an actual Big Data application that could scale to three billion rows I",
    "start": "3830990",
    "end": "3837170"
  },
  {
    "text": "encourage you to download the slides I also encourage you to use the mobile app to review this session your feedback is",
    "start": "3837170",
    "end": "3842270"
  },
  {
    "text": "is very important and we also have a lot of time before the keynote so if you have any questions feel free to come up and ask me afterwards happy to answer",
    "start": "3842270",
    "end": "3848600"
  },
  {
    "text": "any questions you may have thanks for your time today and have fun at the summit you",
    "start": "3848600",
    "end": "3855190"
  }
]