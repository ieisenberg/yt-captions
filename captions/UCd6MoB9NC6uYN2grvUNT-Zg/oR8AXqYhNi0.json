[
  {
    "text": "okay are we ready okay thank you for",
    "start": "3859",
    "end": "9660"
  },
  {
    "text": "appearing with us on this delightful container track today this is the last session shouldn't be shouldn't be too",
    "start": "9660",
    "end": "17100"
  },
  {
    "text": "long just a reminder that immediately following this is the happy hour so",
    "start": "17100",
    "end": "22740"
  },
  {
    "text": "it'll be from 4:45 to 6 this session conveniently gets out right at 4:45 and",
    "start": "22740",
    "end": "28949"
  },
  {
    "text": "so be first or BB Ellis so I guess just push and shove out the door like usual",
    "start": "28949",
    "end": "35489"
  },
  {
    "text": "um so this is advanced container scheduling and resource management it is",
    "start": "35489",
    "end": "41250"
  },
  {
    "text": "not as boring as it sounds although I successfully scared away enough people for everyone to have a chair this time",
    "start": "41250",
    "end": "46289"
  },
  {
    "text": "so everyone is welcome it'll be a little",
    "start": "46289",
    "end": "52739"
  },
  {
    "text": "bit more of an assortment of things though so we'll talk about resource scheduling and some different ways that you can do that some ways that you can",
    "start": "52739",
    "end": "58680"
  },
  {
    "text": "get more out of scheduling your containers themselves and we'll talk about some of the things that let us do",
    "start": "58680",
    "end": "64049"
  },
  {
    "text": "that and then we'll look into some of the internals on how we actually handle the the scheduling part and how you can",
    "start": "64049",
    "end": "70950"
  },
  {
    "text": "customize that if you so choose so first things first what is container",
    "start": "70950",
    "end": "76350"
  },
  {
    "text": "scheduling and why should you care about it so scheduling is just how your containers are placed and run so that",
    "start": "76350",
    "end": "81630"
  },
  {
    "text": "might be something that how did you schedule that container in response select to an event or based on a cron",
    "start": "81630",
    "end": "87630"
  },
  {
    "text": "expression or part of a have a long-running service and it could also",
    "start": "87630",
    "end": "92970"
  },
  {
    "text": "mean something like task placement which we'll dig into so just like with kind of the the overarching managing of containers with",
    "start": "92970",
    "end": "99479"
  },
  {
    "text": "Fargate managing one container is really easy and managing many containers is hard so there's many pieces all with",
    "start": "99479",
    "end": "106439"
  },
  {
    "text": "very unique resource requirements a lot of them can run in response to different events or different tasks so it's really",
    "start": "106439",
    "end": "114390"
  },
  {
    "text": "an area that although you do not need to customize you can get a significant amount of resource usage improvements",
    "start": "114390",
    "end": "120899"
  },
  {
    "text": "and performance improvements out of customizing a little bit so as always the default is totally fine so for",
    "start": "120899",
    "end": "128879"
  },
  {
    "text": "everything that we're going to talk about today there is a default so for task scheduling there is a default",
    "start": "128879",
    "end": "133920"
  },
  {
    "text": "for placement sorry there's a default for the resource usage there is defaults so if you're not ready to dive into any",
    "start": "133920",
    "end": "141239"
  },
  {
    "text": "of this that is totally fine trust the default and you can add some of this as you as you get along and so starting a",
    "start": "141239",
    "end": "148620"
  },
  {
    "text": "little bit with the scheduling part couple different components that we'll look into so the scheduling engine the placement engine and extensions first",
    "start": "148620",
    "end": "156780"
  },
  {
    "text": "one up is types of scheduler so the scheduling engine so a couple different ways and this is more commonly known for",
    "start": "156780",
    "end": "162510"
  },
  {
    "text": "a lot of people as as what kind of job it is so something that's managed by a",
    "start": "162510",
    "end": "167580"
  },
  {
    "text": "service is like a service job so a nice TS or Fargate it would be like messaging",
    "start": "167580",
    "end": "173400"
  },
  {
    "text": "service or something and it controls okay I want a minimum of four tasks a desired count of six and a maximum count",
    "start": "173400",
    "end": "180090"
  },
  {
    "text": "of 45 so that might be an example of a service scheduler the service is responsible for starting restocked",
    "start": "180090",
    "end": "186510"
  },
  {
    "text": "starting restarting and stopping those tasks you can run batch jobs so there's",
    "start": "186510",
    "end": "191940"
  },
  {
    "text": "some really cool architectures out there that do batch processing yes so to take multiple items off of queue or something",
    "start": "191940",
    "end": "198090"
  },
  {
    "text": "like that process them all in parallel and dump the results somewhere else you can also have event driven so some",
    "start": "198090",
    "end": "203790"
  },
  {
    "text": "questions earlier about how to run cron basically in a container this is the a",
    "start": "203790",
    "end": "210090"
  },
  {
    "text": "closer equivalent of that so a more container native way to do it so you can do it in response to a cron expression",
    "start": "210090",
    "end": "215870"
  },
  {
    "text": "or you can do an event-driven one so that might be something like like a cloud watch alarm and you can also do",
    "start": "215870",
    "end": "222000"
  },
  {
    "text": "daemon processes and so an example of that might be like a logging agent or a",
    "start": "222000",
    "end": "227820"
  },
  {
    "text": "monitoring agent something that you in a past life might have run on the ec2 host itself so you run one of them it hangs",
    "start": "227820",
    "end": "235920"
  },
  {
    "text": "out collecting data from another container or another process placement",
    "start": "235920",
    "end": "241739"
  },
  {
    "text": "engine so there is a default behavior here which I'll get into in a second but the task placement engine basically",
    "start": "241739",
    "end": "248150"
  },
  {
    "text": "tells you how you can place your tasks among your pool of resources so you can",
    "start": "248150",
    "end": "253829"
  },
  {
    "text": "filter on custom things so on a certain amia or an availability zone or a custom",
    "start": "253829",
    "end": "260250"
  },
  {
    "text": "one so if you label your Theor your instances with them with a the key value",
    "start": "260250",
    "end": "265919"
  },
  {
    "text": "pair so like there's in this case stack is prod you can place based on those characteristics and you go a couple different ways here",
    "start": "265919",
    "end": "274650"
  },
  {
    "text": "so the first one is cluster constraints which we're going to dig into so a cluster constraint is does it have the",
    "start": "274650",
    "end": "281490"
  },
  {
    "text": "hard number of resources that it needs to support that task so does it have CPU memory and in some cases does it have",
    "start": "281490",
    "end": "288629"
  },
  {
    "text": "port port is a hard constraint but we'll talk in - we'll talk in a second about",
    "start": "288629",
    "end": "294150"
  },
  {
    "text": "how some of the load balancer types port is no longer a hard constraint but for the kind of classic way of looking at",
    "start": "294150",
    "end": "300509"
  },
  {
    "text": "things you have kind of three big ones and there are more resource constraints other than that but port memory and CPU",
    "start": "300509",
    "end": "306199"
  },
  {
    "text": "then you have customs constraints so these are the ones that we just looked at so does it meet my strategy so like a",
    "start": "306199",
    "end": "313680"
  },
  {
    "text": "bin packing or a spread or distinct instances does it meet my strategy so something like filtering an ami ID and",
    "start": "313680",
    "end": "323550"
  },
  {
    "text": "then finally select the final instance for placement so some supportive",
    "start": "323550",
    "end": "328740"
  },
  {
    "text": "placement strategies so this is what we are talking about a little bit earlier and people are making obviously jokes on",
    "start": "328740",
    "end": "334710"
  },
  {
    "text": "Twitter about the room being been packed earlier Bing pecking means fitting as",
    "start": "334710",
    "end": "340319"
  },
  {
    "text": "many containers as possible in the smallest amount of space so that I use if I have two available instances I will",
    "start": "340319",
    "end": "347639"
  },
  {
    "text": "fill one instance as close as possible to full before allocating my tasks to a",
    "start": "347639",
    "end": "353130"
  },
  {
    "text": "new instance so this is really good if you want to well one example is really good for like a development environment",
    "start": "353130",
    "end": "358889"
  },
  {
    "text": "where you want to put the least amount of resources towards running your development environment so you want to pack in as many tasks as possible before",
    "start": "358889",
    "end": "365460"
  },
  {
    "text": "scaling up a pool people do this in production too so I use segmented as an",
    "start": "365460",
    "end": "370889"
  },
  {
    "text": "example this morning where they use bin packing in production to run as many",
    "start": "370889",
    "end": "376380"
  },
  {
    "text": "events as possible I'll get into it in a bit but you can use these strategies in",
    "start": "376380",
    "end": "382560"
  },
  {
    "text": "conjunction with constraints also so you could for example bin pack while distributing between availability zones",
    "start": "382560",
    "end": "389659"
  },
  {
    "text": "you also have spread which will distribute your tasks as evenly as possible between the instances that will",
    "start": "389659",
    "end": "396180"
  },
  {
    "text": "support the tasks you have affinity which means go and like groups so if I have task a and task B and they",
    "start": "396180",
    "end": "403680"
  },
  {
    "text": "belong together the the placement engine will always put task a and task B",
    "start": "403680",
    "end": "408900"
  },
  {
    "text": "together and will keep them in pairs like that distinct instance try to keep",
    "start": "408900",
    "end": "415620"
  },
  {
    "text": "one copy of that task on every instance that's running in your cluster so if I",
    "start": "415620",
    "end": "420930"
  },
  {
    "text": "have one on two of the instances that are running and I have one that's empty fill that empty one before moving on to",
    "start": "420930",
    "end": "428250"
  },
  {
    "text": "another instance you can chain these though so if I wanted to spread across",
    "start": "428250",
    "end": "433860"
  },
  {
    "text": "zone and then bin pack within that I can add multiple strategies so I could do",
    "start": "433860",
    "end": "439610"
  },
  {
    "text": "distinct instance within spread so spread as much as possible but make sure",
    "start": "439610",
    "end": "444900"
  },
  {
    "text": "that there are distinct instances I could do it by affinity I think the most common one though is probably bin",
    "start": "444900",
    "end": "451470"
  },
  {
    "text": "packing while also spreading by availability zone so still keep high availability but use my infrastructure",
    "start": "451470",
    "end": "457830"
  },
  {
    "text": "as kind of tightly as possible beyond that so who's responsible for all of",
    "start": "457830",
    "end": "463800"
  },
  {
    "text": "this because sometimes it's you and sometimes it's not so container manager is in charge of basically a resource",
    "start": "463800",
    "end": "470400"
  },
  {
    "text": "management so how many resources do you have available how do I knew it a resource has changed when a service",
    "start": "470400",
    "end": "477090"
  },
  {
    "text": "requests resources how do i allocate that and it's in charge of making sure that all those numbers match up so if I",
    "start": "477090",
    "end": "484580"
  },
  {
    "text": "if you didn't have a container manager you'd run into an issue where a service requested resources got resources so",
    "start": "484580",
    "end": "492419"
  },
  {
    "text": "then the you were missing resources from the instance and then another service tried to place a task there and thought",
    "start": "492419",
    "end": "500039"
  },
  {
    "text": "that it had enough but since there would be no updates it wouldn't be able to place a task there but it wouldn't know until it tried and failed so a container",
    "start": "500039",
    "end": "506699"
  },
  {
    "text": "manager is in charge of keeping all of that straight so how many resources are left in the instances where there are",
    "start": "506699",
    "end": "511860"
  },
  {
    "text": "resources left and then keeping track of accepting or denying requests or resources resource constraints so I said",
    "start": "511860",
    "end": "520770"
  },
  {
    "text": "CPU and memory and ports you obviously have a couple more things like network bandwidth and I ops and disk space so",
    "start": "520770",
    "end": "527579"
  },
  {
    "text": "these are all resource constraints meaning with some exceptions you cannot",
    "start": "527579",
    "end": "533370"
  },
  {
    "text": "get more of them without changing your instance type so if I need 1024 of CPU",
    "start": "533370",
    "end": "539580"
  },
  {
    "text": "to deploy my task bless you to deploy my task and the box that I'm trying to",
    "start": "539580",
    "end": "546000"
  },
  {
    "text": "deploy to does not have 1024 of CPU I cannot deploy there that is a resource",
    "start": "546000",
    "end": "551460"
  },
  {
    "text": "constraint and I cannot make more CPU in that box without either redistributing the tasks that are on there or trying to",
    "start": "551460",
    "end": "558480"
  },
  {
    "text": "deploy to a different instance so they're hard constraints or semi hard constraints they're the first things",
    "start": "558480",
    "end": "565560"
  },
  {
    "text": "that need to happen on your kind of your order of operation so it's a lot like middle school math class where you you",
    "start": "565560",
    "end": "570810"
  },
  {
    "text": "kind of you have to go in the order and you have to evaluate them in order so the first one for placing your tasks is",
    "start": "570810",
    "end": "576330"
  },
  {
    "text": "always going to be those resource constraints so does this server physically have enough resources to",
    "start": "576330",
    "end": "582540"
  },
  {
    "text": "support my service so what enforces this",
    "start": "582540",
    "end": "587670"
  },
  {
    "text": "so in most cases this is going to be the ECS agent when they're container manager right so on that instance I'm running",
    "start": "587670",
    "end": "595140"
  },
  {
    "text": "docker I have my tasks I have the container that belongs to that task and it's the ECS agent that's responsible",
    "start": "595140",
    "end": "601860"
  },
  {
    "text": "for all this communication so if you've ever gone in and looked on one of your boxes running an e CS the e CS optimized",
    "start": "601860",
    "end": "609450"
  },
  {
    "text": "a my sets up that agent for you but it's responsible for doing all that communication so communicating back to",
    "start": "609450",
    "end": "616080"
  },
  {
    "text": "the the state manager accepting the resource requests and then whether the task has started or stopped so whether",
    "start": "616080",
    "end": "622860"
  },
  {
    "text": "it's been able to fulfill the request and we'll look at a timeline of how a request goes through those different managers a quick digression though to",
    "start": "622860",
    "end": "631020"
  },
  {
    "text": "talk about load balancers because they have an impact on how you can schedule and use your resources so what's a load",
    "start": "631020",
    "end": "639780"
  },
  {
    "text": "balancer high level they distribute your traffic between targets so that might be between IP addresses or containers or",
    "start": "639780",
    "end": "647280"
  },
  {
    "text": "tasks or ec2 instances so different types of these started off with lb",
    "start": "647280",
    "end": "653130"
  },
  {
    "text": "classic so that's the original it balances traffic between only ec2 instances that's it right now it's",
    "start": "653130",
    "end": "661050"
  },
  {
    "text": "pretty much designed still for ec2 classic so if you're starting up new services or you're working with ECI",
    "start": "661050",
    "end": "667430"
  },
  {
    "text": "the strong suggestion as you go with application load balancer our network load balancer had a couple of really",
    "start": "667430",
    "end": "673490"
  },
  {
    "text": "interesting questions on these after the State of the Union about how do you know",
    "start": "673490",
    "end": "679550"
  },
  {
    "text": "which one is is good for you and I included a link for when you're looking at the sides the the load balancing page",
    "start": "679550",
    "end": "685190"
  },
  {
    "text": "is very thorough and it will give you a whole list of the different use cases and when when it's good for one I put in",
    "start": "685190",
    "end": "691490"
  },
  {
    "text": "a couple of tips application load bouncer I think is extra great for micro",
    "start": "691490",
    "end": "696830"
  },
  {
    "text": "services because I can route based based on path or content so rather than having",
    "start": "696830",
    "end": "702080"
  },
  {
    "text": "one load balancer per service I can have one load balancer with multiple target groups that will distribute the traffic",
    "start": "702080",
    "end": "708050"
  },
  {
    "text": "so it lets me use my top-level resources a little bit more effectively also because instead of having to have 25",
    "start": "708050",
    "end": "714920"
  },
  {
    "text": "load balancers for 25 services I can have one load bouncer with those 25 target groups that all route based on",
    "start": "714920",
    "end": "721850"
  },
  {
    "text": "path so a path in this case might be something like slash web or slash API or slash friends / v1 so I can route based",
    "start": "721850",
    "end": "729650"
  },
  {
    "text": "on that way through the load bouncer only VPC network load balancer the",
    "start": "729650",
    "end": "736700"
  },
  {
    "text": "beginning for me on this one is you get a lot of questions still about pre-warming load balancers and that's",
    "start": "736700",
    "end": "742670"
  },
  {
    "text": "something that you don't have to think about for Fargate but it's a pretty traditional question for elastic load",
    "start": "742670",
    "end": "749120"
  },
  {
    "text": "balancers that there used to be some cases that you'd have to contact support and ask them to pre-warm your load",
    "start": "749120",
    "end": "754460"
  },
  {
    "text": "balancer when you were switching your traffic over network load bouncer is designed to be really high throughput",
    "start": "754460",
    "end": "760370"
  },
  {
    "text": "really low latency I can't spell throughput by the way so if for some reason that's not how you spell it then",
    "start": "760370",
    "end": "766000"
  },
  {
    "text": "you know what I mean spelling is hard and so where I think this is really good",
    "start": "766000",
    "end": "773120"
  },
  {
    "text": "though is for people that have a really erratic use case so if you're one of the people that has no traffic and then",
    "start": "773120",
    "end": "778370"
  },
  {
    "text": "jumps up and the number of requests per second but then it goes back down again and then jumps back up goes up a little",
    "start": "778370",
    "end": "784070"
  },
  {
    "text": "bit goes down a little bit goes back up so that's a was before would have been a",
    "start": "784070",
    "end": "789140"
  },
  {
    "text": "really hard traffic pattern for a load bouncer so if you were switching your load balancer you'd have to have it pre",
    "start": "789140",
    "end": "794990"
  },
  {
    "text": "warned because it wouldn't be able to keep up with the sudden influx of requests Network load balancer is great for that",
    "start": "794990",
    "end": "801550"
  },
  {
    "text": "you can also use it to assign an elastic IP per subnet if you're looking to",
    "start": "801550",
    "end": "807020"
  },
  {
    "text": "switch your load balance or type I very much recommend looking at that that breakdown page because it will give you",
    "start": "807020",
    "end": "812240"
  },
  {
    "text": "more information that I'm gonna do what does it have to do with scheduling now so first that's what's in charge of",
    "start": "812240",
    "end": "819770"
  },
  {
    "text": "actually distributing your requests so your load balancer is playing a pretty significant role and in your scheduling",
    "start": "819770",
    "end": "825380"
  },
  {
    "text": "to start with you can also do I think little-known tip for for speeding up",
    "start": "825380",
    "end": "831320"
  },
  {
    "text": "your deployments with ECS is that a lot of these things can still be controlled",
    "start": "831320",
    "end": "836690"
  },
  {
    "text": "that the load balancer level so for example connection draining connection",
    "start": "836690",
    "end": "841790"
  },
  {
    "text": "draining is the time period that it takes for the load balancer to gracefully start removing letting",
    "start": "841790",
    "end": "848480"
  },
  {
    "text": "allowing connections to wrap up before draining them and then replacing them somewhere else so an example of this",
    "start": "848480",
    "end": "855770"
  },
  {
    "text": "would be an e CS deployment that when you add a new revision of your task the load balancer will very very very slowly",
    "start": "855770",
    "end": "862520"
  },
  {
    "text": "drain connections off the previous revision if it passes health checks and then will roll those over to the next",
    "start": "862520",
    "end": "868010"
  },
  {
    "text": "revision Athene is an example of connection draining and that's sign that",
    "start": "868010",
    "end": "873260"
  },
  {
    "text": "you can speed up by tweaking this connection draining timeout so by default it is quite generous but you",
    "start": "873260",
    "end": "880430"
  },
  {
    "text": "don't need that much time you can you can you can shorten it and you can drain connections off a little bit faster if",
    "start": "880430",
    "end": "886070"
  },
  {
    "text": "you don't mind that every once in a while someone might get briefly disconnected and then reconnected not",
    "start": "886070",
    "end": "892040"
  },
  {
    "text": "good if you're doing something like WebSockets obviously but that's why you have a WebSocket is to connect to the",
    "start": "892040",
    "end": "897680"
  },
  {
    "text": "WebSocket so highly recommend looking into those load balancer settings because since they're not accessible",
    "start": "897680",
    "end": "904190"
  },
  {
    "text": "through the ECS console they're often overlooked with how to speed up your deployment another cool one is the the",
    "start": "904190",
    "end": "911390"
  },
  {
    "text": "auto scaling group cooldown period is often very generous and a cool-down period it's the time that it takes to",
    "start": "911390",
    "end": "917650"
  },
  {
    "text": "sit and do nothing before it really taught us killing role so in some cases",
    "start": "917650",
    "end": "922730"
  },
  {
    "text": "you might Auto scale and add one instance in response to changes in cluster resources and then wait in",
    "start": "922730",
    "end": "929780"
  },
  {
    "text": "seconds so 300 or something but or it will re-evaluate the rule again but for a lot of people that doesn't",
    "start": "929780",
    "end": "935690"
  },
  {
    "text": "help them then add more resources more quickly if their pattern was changing more dramatically so you can tweak",
    "start": "935690",
    "end": "941990"
  },
  {
    "text": "things like the like the cooldown period - - to allocate your resources a little",
    "start": "941990",
    "end": "947540"
  },
  {
    "text": "bit more efficiently and a little bit more quickly secondly dynamic port allocation so this is kind of the",
    "start": "947540",
    "end": "954949"
  },
  {
    "text": "original way that you were able to do that so previously with ELB classic you had to bind always a container port to",
    "start": "954949",
    "end": "961279"
  },
  {
    "text": "the host port so port 80 on the container was mapped to 80 81 and a host or something like that but guess how",
    "start": "961279",
    "end": "968750"
  },
  {
    "text": "many copies of port 8081 are on each server there's only one so it is very much a hard constraint and",
    "start": "968750",
    "end": "975380"
  },
  {
    "text": "that was really limiting for a lot of people because in order to run and especially with with monoliths right it's because in order to run multiple",
    "start": "975380",
    "end": "981769"
  },
  {
    "text": "copies of your application and scale up you needed a whole nother server and if you're using dynamic port allocation you",
    "start": "981769",
    "end": "987860"
  },
  {
    "text": "can bind it to port 0 and the load balancer will take care of allocating a port for that service to run on so",
    "start": "987860",
    "end": "993170"
  },
  {
    "text": "theoretically a service that used to only be able to run one copy per host can now run many copies per host with",
    "start": "993170",
    "end": "999620"
  },
  {
    "text": "port allocation happening at the load balancer level we talked about a BW sbpc earlier hole do you see Paul with this",
    "start": "999620",
    "end": "1008140"
  },
  {
    "text": "stand microphone my assistant Paul okay",
    "start": "1008140",
    "end": "1015220"
  },
  {
    "text": "I will repeat your question then yes",
    "start": "1015220",
    "end": "1020009"
  },
  {
    "text": "so HTTP and HTTPS can happen through alb",
    "start": "1041850",
    "end": "1048000"
  },
  {
    "text": "how do you do oh sorry I'm repeating the question which I didn't do a good job of so repeating the question is how do I",
    "start": "1048000",
    "end": "1055030"
  },
  {
    "text": "manage things like SSL with the with the load balancer when I'm doing dynamic port allocation in a lot of cases SSL",
    "start": "1055030",
    "end": "1062380"
  },
  {
    "text": "terminates at the load balancer so your your SSL connection is from the the",
    "start": "1062380",
    "end": "1067630"
  },
  {
    "text": "client to the load balancer and then the connection from the load balancer to the container is not always SSL in some",
    "start": "1067630",
    "end": "1074169"
  },
  {
    "text": "cases it is if you're doing WebSockets or something like like your if you're",
    "start": "1074169",
    "end": "1080679"
  },
  {
    "text": "trying to do like socket connections everyone has a different potentially extraordinarily hacky way of doing it I",
    "start": "1080679",
    "end": "1088059"
  },
  {
    "text": "used to also do it a very happy way so don't ask me on twitch but I'll tell you afterwards so different people have",
    "start": "1088059",
    "end": "1095950"
  },
  {
    "text": "different solutions to that one in a lot of cases I've just terminated it terminated it at the load balancer if my",
    "start": "1095950",
    "end": "1103510"
  },
  {
    "text": "lovely assistant ever rejoins us perhaps he has a good answer as well I don't",
    "start": "1103510",
    "end": "1108789"
  },
  {
    "text": "know where he went though he totally abandoned me after I did questions for all of his talks so he's not gonna get a",
    "start": "1108789",
    "end": "1115030"
  },
  {
    "text": "beer if I find them okay so dynamic port allocation a big",
    "start": "1115030",
    "end": "1123970"
  },
  {
    "text": "win I think that doesn't sound that exciting but what it does that you do is use your actual cluster resources in a",
    "start": "1123970",
    "end": "1130750"
  },
  {
    "text": "way that's significantly more efficient so in a lot of cases you can use way",
    "start": "1130750",
    "end": "1135789"
  },
  {
    "text": "less instances than you could before so if your cluster is full like C for triple excels like adding adding adding",
    "start": "1135789",
    "end": "1144400"
  },
  {
    "text": "one less of those is a big deal so use your resources just a little bit more effectively use what you have I'm gonna",
    "start": "1144400",
    "end": "1151990"
  },
  {
    "text": "talk a little bit about the importance of images we have beers nopal but we do have beers",
    "start": "1151990",
    "end": "1159519"
  },
  {
    "text": "so if Paul comes back tell them you can't have a beer so the importance of images here so I think this is one",
    "start": "1159519",
    "end": "1165850"
  },
  {
    "text": "that's it's also a hot topic or a lot of people have I get a lot of emails about",
    "start": "1165850",
    "end": "1172359"
  },
  {
    "text": "people that are having issues with this a huge way to eat up all of the disk",
    "start": "1172359",
    "end": "1177789"
  },
  {
    "text": "space not that I'm speaking from experience and get paged is to not",
    "start": "1177789",
    "end": "1182859"
  },
  {
    "text": "control the size of your doctor images at all that's how you can especially if you have auto scaling rules so just keep",
    "start": "1182859",
    "end": "1188499"
  },
  {
    "text": "adding hosts you're clustering you know like well when I left yesterday there was only ten and now there's now there's",
    "start": "1188499",
    "end": "1193659"
  },
  {
    "text": "15 and also I'm getting paged saying then you're running out of disk space so don't be like me definitely watch the",
    "start": "1193659",
    "end": "1200889"
  },
  {
    "text": "size of your doctor images and they add up really quickly in general and this is",
    "start": "1200889",
    "end": "1206049"
  },
  {
    "text": "only sort of true with docker 1.1 someone asked a question and you weren't",
    "start": "1206049",
    "end": "1212139"
  },
  {
    "text": "here yeah you're not allowed to have one [Laughter]",
    "start": "1212139",
    "end": "1220889"
  },
  {
    "text": "in general it's a little bit different with docker 1.1 but the more layers you have and the larger those layers are the",
    "start": "1220889",
    "end": "1227529"
  },
  {
    "text": "larger your final image would be so obviously that eats up disk space a couple ways that you can make this a",
    "start": "1227529",
    "end": "1233320"
  },
  {
    "text": "little bit better on yourself the first one which is only on this intro side because it didn't fit in another slide",
    "start": "1233320",
    "end": "1238799"
  },
  {
    "text": "you don't always need the recommended packages so an example from apt-get no install recommends every time I use that",
    "start": "1238799",
    "end": "1245619"
  },
  {
    "text": "example someone says but in my package manager it's this I know that there are other package managers I only included",
    "start": "1245619",
    "end": "1251919"
  },
  {
    "text": "one on this slide please insert in your head the appropriate no install",
    "start": "1251919",
    "end": "1258580"
  },
  {
    "text": "recommends command for your favorite package manager so how can you reduce image size so something that we didn't",
    "start": "1258580",
    "end": "1265539"
  },
  {
    "text": "talk about earlier or that maybe we should have as is using shared base images so I get a lot of questions on well how can I use up less space for my",
    "start": "1265539",
    "end": "1273759"
  },
  {
    "text": "images it's the Simpsons quote it says shrink ray there's always something else to buy and so users shared base images",
    "start": "1273759",
    "end": "1281619"
  },
  {
    "text": "were possible so especially if you have five services that are all written in nodejs maybe you want to share a node.js basin",
    "start": "1281619",
    "end": "1287860"
  },
  {
    "text": "with the common packages and then only your app code is different on top of that limit the data that you write",
    "start": "1287860",
    "end": "1294429"
  },
  {
    "text": "during the actual execution of your container to that to that right layer so containers are basically a stack so it's",
    "start": "1294429",
    "end": "1300640"
  },
  {
    "text": "the the base OS and the read-only that forms the base of the container then",
    "start": "1300640",
    "end": "1305890"
  },
  {
    "text": "there's a really thin read/write layer on top of that so limit the data that you write during that execution phase so",
    "start": "1305890",
    "end": "1311530"
  },
  {
    "text": "something like adding a large file you can chain your run statements so",
    "start": "1311530",
    "end": "1316990"
  },
  {
    "text": "something that I would have done when I first started out with docker would ordering every line and I'm sure it",
    "start": "1316990",
    "end": "1322059"
  },
  {
    "text": "looked beautiful I'm sure I commented everything but it's like apt-get update comment apt-get install and then I",
    "start": "1322059",
    "end": "1329169"
  },
  {
    "text": "installed a bunch of stuff then I cleaned up then I did something else and although that looks beautiful and",
    "start": "1329169",
    "end": "1334210"
  },
  {
    "text": "readable it is not super effective for caching purposes so change your run statements there's lots of blog posts",
    "start": "1334210",
    "end": "1340570"
  },
  {
    "text": "about this if you're looking to learn more but in general also just prevent cache misses at build for as long as",
    "start": "1340570",
    "end": "1347500"
  },
  {
    "text": "possible so let's not cache for a little bit darker has all kinds of exceptions",
    "start": "1347500",
    "end": "1352900"
  },
  {
    "text": "by the way to how they use the cache I put a link in a couple slides from now",
    "start": "1352900",
    "end": "1358410"
  },
  {
    "text": "to their documentation on the exact order of operations they use for their cache but at a high level run ad and",
    "start": "1358410",
    "end": "1366490"
  },
  {
    "text": "copy all add layers starting in docker 1.1 other instructions do not do that if",
    "start": "1366490",
    "end": "1371890"
  },
  {
    "text": "you were on a different version of docker other commands also add layers to the final image size really really",
    "start": "1371890",
    "end": "1379179"
  },
  {
    "text": "really quick overview of how the cache works starting from your current layer doctor looks at all subsequent layers to",
    "start": "1379179",
    "end": "1385450"
  },
  {
    "text": "see if they use the exact same instruction and I'll explain that if they match the exact same instruction",
    "start": "1385450",
    "end": "1391210"
  },
  {
    "text": "the cache is used if you're adding or copying files it checks a checksum other",
    "start": "1391210",
    "end": "1396640"
  },
  {
    "text": "than add and copy the contents is never checked so like after get update that",
    "start": "1396640",
    "end": "1401890"
  },
  {
    "text": "string has not changed so it will use the cache so sometimes this is my",
    "start": "1401890",
    "end": "1407169"
  },
  {
    "text": "footnote so what happens if your command string is always the same but you need",
    "start": "1407169",
    "end": "1412809"
  },
  {
    "text": "to rerun the command so a good example of that is pulling from a git repository the repository address of your registry",
    "start": "1412809",
    "end": "1419590"
  },
  {
    "text": "will never change so a lot of people do something by either passing the flag to ignore the",
    "start": "1419590",
    "end": "1425770"
  },
  {
    "text": "cache entirely or you can cache break by adding something like a like a different string every time so for a lot of people",
    "start": "1425770",
    "end": "1432010"
  },
  {
    "text": "that's a timestamp so that they've changed the you're effectively changing the line every time so that it's forced",
    "start": "1432010",
    "end": "1438250"
  },
  {
    "text": "to rerun the command even though the string that makes up the command has not actually changed clean up as you go",
    "start": "1438250",
    "end": "1444190"
  },
  {
    "text": "obviously if you install a big file delete the charred version in the same line anything that you do within the",
    "start": "1444190",
    "end": "1451090"
  },
  {
    "text": "same line butts in the same instruction does not commit to the final image size only the result so if I install unzip",
    "start": "1451090",
    "end": "1458740"
  },
  {
    "text": "use and then delete the charred version that tarred version of the file will",
    "start": "1458740",
    "end": "1464770"
  },
  {
    "text": "never be added to my final image size just the contents of that file that I used for something else",
    "start": "1464770",
    "end": "1471000"
  },
  {
    "text": "do built-ins also so every single package manager has something about",
    "start": "1471000",
    "end": "1476380"
  },
  {
    "text": "either removing library files or a purge or something like that clean up after",
    "start": "1476380",
    "end": "1481450"
  },
  {
    "text": "yourself the same thing with your local machine err on that system so you can prune images or you can put in the",
    "start": "1481450",
    "end": "1486970"
  },
  {
    "text": "system which will get rid of dangling volumes so with containers comes a little bit of a mess so clean up after",
    "start": "1486970",
    "end": "1492970"
  },
  {
    "text": "yourself there is a garbage collection as part of most orchestration tools so",
    "start": "1492970",
    "end": "1498580"
  },
  {
    "text": "there's one in East yes for kubernetes a lot of people say that the built-in does not quite get them the amount of",
    "start": "1498580",
    "end": "1505480"
  },
  {
    "text": "cleanliness that they would like so there are third-party ones out there one",
    "start": "1505480",
    "end": "1510610"
  },
  {
    "text": "that I've used and like that's open source is Spotify GC which is a collector container they'll basically",
    "start": "1510610",
    "end": "1515950"
  },
  {
    "text": "run on your hosts to clean up more aggresively unused images something else",
    "start": "1515950",
    "end": "1521920"
  },
  {
    "text": "that I've done to save images in the past to save space in the past also is if I'm running a log collection",
    "start": "1521920",
    "end": "1528190"
  },
  {
    "text": "container anyway I very aggressively log rotated things away because disk space",
    "start": "1528190",
    "end": "1534880"
  },
  {
    "text": "as exciting as it sounds disk races are resource too so you can have the most advanced task placement",
    "start": "1534880",
    "end": "1542140"
  },
  {
    "text": "policies and strategies and you can write all these complicated things but if you run out of disk space you still",
    "start": "1542140",
    "end": "1547240"
  },
  {
    "text": "have a problem so be responsible with your docker images and with your layers I promised a little bit of a timeline on",
    "start": "1547240",
    "end": "1554640"
  },
  {
    "text": "how the the internal scheduler actually works so when you launch an instance and",
    "start": "1554640",
    "end": "1559710"
  },
  {
    "text": "it registers itself with the ECS cluster it reports its total amount of resources so it calls it as total resources so",
    "start": "1559710",
    "end": "1567300"
  },
  {
    "text": "it'll report every single thing unless you tell it not to and that gets added to the whole pool so that way the agent",
    "start": "1567300",
    "end": "1574650"
  },
  {
    "text": "knows ok so between this whole pool I have X amount of CPU and X amount of memory available you can modify that so",
    "start": "1574650",
    "end": "1584310"
  },
  {
    "text": "you can hide things from the ECS agent so if you have resources that you want to use for like a daemon process or",
    "start": "1584310",
    "end": "1590280"
  },
  {
    "text": "something that's not managed by ECS you can hide resources from ECS so it won't try to schedule on top of that it works",
    "start": "1590280",
    "end": "1596580"
  },
  {
    "text": "for memory ports so you can pass that flag to the agent so that you know it",
    "start": "1596580",
    "end": "1601800"
  },
  {
    "text": "doesn't try to steal your resources and how to accepting resource requests work so for a task it starts with a request",
    "start": "1601800",
    "end": "1610410"
  },
  {
    "text": "so the task says are there available resources in this pool for me to run and",
    "start": "1610410",
    "end": "1615900"
  },
  {
    "text": "if there are it looks for a container for container resources and for the task",
    "start": "1615900",
    "end": "1622470"
  },
  {
    "text": "resource so we've talked about those before but you have the hard ones and the soft ones so what is my whole task need and then what is this individual",
    "start": "1622470",
    "end": "1628890"
  },
  {
    "text": "container named your regional cluster management engine you send it that request and either approves it or",
    "start": "1628890",
    "end": "1635670"
  },
  {
    "text": "rejects it and then the cluster management engine does it within availability zone so if you lose",
    "start": "1635670",
    "end": "1641580"
  },
  {
    "text": "availability zone you can continue to schedule to your cluster but to a different availability zone once the",
    "start": "1641580",
    "end": "1648870"
  },
  {
    "text": "regional engine has approved the request it sends it down to agent communication that you need to change the state of a",
    "start": "1648870",
    "end": "1655560"
  },
  {
    "text": "node so you go from your regional cluster management engine to the agent communication engine agent communication",
    "start": "1655560",
    "end": "1662340"
  },
  {
    "text": "service pushes that down to the individual ECS agents so from regional management to agent communication to the",
    "start": "1662340",
    "end": "1669660"
  },
  {
    "text": "EECS agent which belongs in all those nodes so if you've ever looked in the EECS console before every instance has",
    "start": "1669660",
    "end": "1676680"
  },
  {
    "text": "its own copy of the ECS agent that the EECS optimized ami starts for you",
    "start": "1676680",
    "end": "1682050"
  },
  {
    "text": "and then if that ECS agent is no longer registered you can't scheduled to that instance anymore once",
    "start": "1682050",
    "end": "1689730"
  },
  {
    "text": "that request has been sent our service will either it will acknowledge that it either performed or did not perform the",
    "start": "1689730",
    "end": "1696299"
  },
  {
    "text": "update so stopping a task starting a task restarting it the task is now",
    "start": "1696299",
    "end": "1702240"
  },
  {
    "text": "running so how do we keep in sync that's by the by your regional engine so",
    "start": "1702240",
    "end": "1707520"
  },
  {
    "text": "request so we talked about having however a cluster manager keeps track of the state changes bless you and resource",
    "start": "1707520",
    "end": "1714150"
  },
  {
    "text": "requests so you keep track of all of that and that's responsible for taking the inbound requests from different services so it knows how many resources",
    "start": "1714150",
    "end": "1720929"
  },
  {
    "text": "are left in the pool and that's how you'd maybe know at some point I need to scale my cluster up so how could you do",
    "start": "1720929",
    "end": "1728190"
  },
  {
    "text": "this either from the CLI or write something yourself so you can use something called cluster query language",
    "start": "1728190",
    "end": "1734179"
  },
  {
    "text": "which will return things that match your filter so in this example it's an ACS",
    "start": "1734179",
    "end": "1740370"
  },
  {
    "text": "demo and it will return everything that matches the instance type to to but anything in the t2 family",
    "start": "1740370",
    "end": "1747059"
  },
  {
    "text": "you can also match just a specific type Camacho multiple attributes so I can",
    "start": "1747059",
    "end": "1754320"
  },
  {
    "text": "basically use this query language to do anything that I want to do against my pool of resources so see how many are",
    "start": "1754320",
    "end": "1759809"
  },
  {
    "text": "available this is under the hood how you would end up writing a task placement policy yourself is what do I have left",
    "start": "1759809",
    "end": "1766590"
  },
  {
    "text": "that matches this expression is it enough to run my is it enough to run my task I can chain those obviously couple",
    "start": "1766590",
    "end": "1774870"
  },
  {
    "text": "examples so on this one I can go via availability zone that's not us East 1d",
    "start": "1774870",
    "end": "1782250"
  },
  {
    "text": "oops I tried to make purple dots that",
    "start": "1782250",
    "end": "1788250"
  },
  {
    "text": "flew everywhere and I don't think I've really quite figured out the whole PowerPoint animation situation because",
    "start": "1788250",
    "end": "1793500"
  },
  {
    "text": "they kind of just moved in a big chunk but this is example of bin packing and then with an availability zone so I'm",
    "start": "1793500",
    "end": "1799650"
  },
  {
    "text": "trying to I'm spreading them between availability zones but then I've been packing within that so using their",
    "start": "1799650",
    "end": "1805919"
  },
  {
    "text": "resources as effectively as possible but still spreading it across availability zone and then also on memory and can do",
    "start": "1805919",
    "end": "1814169"
  },
  {
    "text": "distinct instance one caveat is that distinct instance",
    "start": "1814169",
    "end": "1819410"
  },
  {
    "text": "does not quite work the same way that that one per host the traditional I",
    "start": "1819410",
    "end": "1828200"
  },
  {
    "text": "think idea of one per host is that I run one copy of this task and only one on",
    "start": "1828200",
    "end": "1833420"
  },
  {
    "text": "every single instance that's a part of this that's a part of this group and I",
    "start": "1833420",
    "end": "1838520"
  },
  {
    "text": "think it does kind of work like that it's not oh does not always work as intended I think for auto-scaling so it",
    "start": "1838520",
    "end": "1845240"
  },
  {
    "text": "works when you're actually deploying the task but I do not believe it will currently account for auto-scaling so when a new instance comes up it does",
    "start": "1845240",
    "end": "1851480"
  },
  {
    "text": "not automatically start a new copy of that task you can do this in the console",
    "start": "1851480",
    "end": "1856640"
  },
  {
    "text": "so if you want to get started with placement the default is this AZ balance",
    "start": "1856640",
    "end": "1862190"
  },
  {
    "text": "spread so spread as evenly as possible between all the availability zones and my clusters running in you can choose",
    "start": "1862190",
    "end": "1869360"
  },
  {
    "text": "not to change anything it will just default to this it will work quite well like this there's a couple templates to",
    "start": "1869360",
    "end": "1876110"
  },
  {
    "text": "help you get started or you could write a custom one you don't have to write cluster query expressions to do this you",
    "start": "1876110",
    "end": "1882950"
  },
  {
    "text": "can do the dropdowns so this one would spread on availability zone then my instance ID and then my memory",
    "start": "1882950",
    "end": "1890830"
  },
  {
    "text": "scheduling for containers just like your calendar I don't dry clean or anything though so this is always seen on my",
    "start": "1890830",
    "end": "1898460"
  },
  {
    "text": "calendar two different ways I think that are important for looking at the looking",
    "start": "1898460",
    "end": "1904370"
  },
  {
    "text": "at the task scheduling so the freshman is load so what does that mean in in real life that's how would I run a task",
    "start": "1904370",
    "end": "1909830"
  },
  {
    "text": "in response to something like the cloud watch alarm so my CPU exceeds the threshold I know",
    "start": "1909830",
    "end": "1914960"
  },
  {
    "text": "that there's a typo in here before anyone points it out if it exceeds a",
    "start": "1914960",
    "end": "1920000"
  },
  {
    "text": "certain CPU threshold or if it exceeds the number of memory or the number of",
    "start": "1920000",
    "end": "1925730"
  },
  {
    "text": "messages in a queue scale up either by a couple instances or just by one and then",
    "start": "1925730",
    "end": "1931580"
  },
  {
    "text": "obviously you have to have the opposite two so don't just keep scaling up until infinity have the inverse of that rule",
    "start": "1931580",
    "end": "1937010"
  },
  {
    "text": "too so that if your your if your CPU goes down below a certain threshold remove instances and so this is the cron",
    "start": "1937010",
    "end": "1944929"
  },
  {
    "text": "kind of ones that I was talking about earlier I can run tasks in response to other",
    "start": "1944929",
    "end": "1950220"
  },
  {
    "text": "things like a specific time or a cron expression so although you do not typically run actual cron inside the the",
    "start": "1950220",
    "end": "1958200"
  },
  {
    "text": "containers you can do it like this so run the action in the container based on either the evaluation of the cron",
    "start": "1958200",
    "end": "1964500"
  },
  {
    "text": "expression or a specific time so I will take questions now if anyone has them",
    "start": "1964500",
    "end": "1971310"
  },
  {
    "text": "maybe Paul wall will play with us but maybe not yeah sorry what is",
    "start": "1971310",
    "end": "1981870"
  },
  {
    "text": "resource-constrained you gave the example of CPU is it possible if there's enough overall in the pool to shuffle",
    "start": "1981870",
    "end": "1988500"
  },
  {
    "text": "instances off onto another host and drop the container on they're not current so",
    "start": "1988500",
    "end": "1994560"
  },
  {
    "text": "the question was because I think we started with the microphone halfway through is is it possible to rebalance tasks so if your pool has a sufficient",
    "start": "1994560",
    "end": "2001490"
  },
  {
    "text": "CPU but the only instance matching the constraints does not have enough CPU",
    "start": "2001490",
    "end": "2006950"
  },
  {
    "text": "will ECS rebalance currently no I believe that there are enough events",
    "start": "2006950",
    "end": "2013490"
  },
  {
    "text": "exposed oh yeah you get a beer if you ask a question so start thinking of exciting yeah now everyone's excited",
    "start": "2013490",
    "end": "2021880"
  },
  {
    "text": "this is the fun track not right now ECS",
    "start": "2021880",
    "end": "2028130"
  },
  {
    "text": "exposes a ton of message of the ton of events so I feel like it must be you",
    "start": "2028130",
    "end": "2033290"
  },
  {
    "text": "could kind of do it yourself but you'd have to pull for a lot of information and it's not native right now but I",
    "start": "2033290",
    "end": "2039470"
  },
  {
    "text": "would love to see it I've seen a few open source projects in fact I think there's one on Nathan pecks",
    "start": "2039470",
    "end": "2044990"
  },
  {
    "text": "repo that does a lambda event based reshuffling rebalancing on the cluster",
    "start": "2044990",
    "end": "2052030"
  },
  {
    "text": "it's not built in at the moment by the way and I said this to people in between",
    "start": "2052030",
    "end": "2058040"
  },
  {
    "text": "but if you have specific ideas or requests like that definitely email or",
    "start": "2058040",
    "end": "2064280"
  },
  {
    "text": "or tweet or DM or use the AWS wish list hashtag because we do actually look at",
    "start": "2064280",
    "end": "2070639"
  },
  {
    "text": "them and actually every time someone sends me a feature request in my email or on Twitter I copy and paste it and I",
    "start": "2070640",
    "end": "2076850"
  },
  {
    "text": "send it to the person and charter to that team which I'm sure he'll enjoy but me I had to stand next to you when",
    "start": "2076850",
    "end": "2084830"
  },
  {
    "text": "we were in questions sorry cameraman answer twitch people I'm not drinking a beer while working these are for",
    "start": "2084830",
    "end": "2090950"
  },
  {
    "text": "customers honest grey shirt yeah now",
    "start": "2090950",
    "end": "2097160"
  },
  {
    "text": "it's balls for delegation so the",
    "start": "2097160",
    "end": "2109670"
  },
  {
    "text": "question is how the balancing would work in relation with bridge or host",
    "start": "2109670",
    "end": "2117740"
  },
  {
    "text": "networking I'm not sure I follow the question so the the networking type",
    "start": "2117740",
    "end": "2124790"
  },
  {
    "text": "shouldn't have a huge impact on how you're allocating resources the networker type to me determines how",
    "start": "2124790",
    "end": "2131690"
  },
  {
    "text": "those containers communicate with each other okay let me rephrase the question so let's say I have a container which is",
    "start": "2131690",
    "end": "2138170"
  },
  {
    "text": "container type a and let's say I would like to run two tasks and in my auto",
    "start": "2138170",
    "end": "2146570"
  },
  {
    "text": "scaling group instance sorry ec2 auto scaling group I have say let's say one",
    "start": "2146570",
    "end": "2154100"
  },
  {
    "text": "instance although I would like to get more so that means that two container of",
    "start": "2154100",
    "end": "2161960"
  },
  {
    "text": "similar type needs to run on the same host so if I use host mode network or",
    "start": "2161960",
    "end": "2168260"
  },
  {
    "text": "BRE bridge Network then how the balancing is going to work if I do not define dynamic port yes so if the",
    "start": "2168260",
    "end": "2180109"
  },
  {
    "text": "scheduler within ACS needs to schedule two containers onto a host and you are using a mode whereby you can only have",
    "start": "2180109",
    "end": "2187670"
  },
  {
    "text": "one on that host so for example with host based networking if they're using the same port like port 80",
    "start": "2187670",
    "end": "2193609"
  },
  {
    "text": "obviously there's only one port 80 on the box you're only gonna get one running on the box ECS will fail to shed your the other one",
    "start": "2193609",
    "end": "2200150"
  },
  {
    "text": "so that would be a good case for when you have auto scaling to scale up your requirements ideally though you want to",
    "start": "2200150",
    "end": "2206030"
  },
  {
    "text": "get away from the idea of locking into individual ports so user network mode like a WSB PC whereby actually the host",
    "start": "2206030",
    "end": "2213530"
  },
  {
    "text": "port does matter anymore because each task gets its only and I and the port is considered a resource constraint so if",
    "start": "2213530",
    "end": "2219630"
  },
  {
    "text": "you're running something where it's bound to a specific port the scheduler respects that because it's technically a",
    "start": "2219630",
    "end": "2226260"
  },
  {
    "text": "constraint so basically we have a use case where we need to do the SSL",
    "start": "2226260",
    "end": "2235050"
  },
  {
    "text": "termination even on the container level so that means we are running dotnet and",
    "start": "2235050",
    "end": "2241050"
  },
  {
    "text": "dotnet core application under the crystal but on top of that we use ng",
    "start": "2241050",
    "end": "2247710"
  },
  {
    "text": "annex to do the SSL termination but if you do so then that means you need to",
    "start": "2247710",
    "end": "2253200"
  },
  {
    "text": "occupy 4 port 443 and which is the host",
    "start": "2253200",
    "end": "2258900"
  },
  {
    "text": "port then actually the balancing of the tasks doesn't work at that point if you",
    "start": "2258900",
    "end": "2269340"
  },
  {
    "text": "have hard requirements or if something must run on 4 for 3 for example and then",
    "start": "2269340",
    "end": "2274580"
  },
  {
    "text": "use the AWS PPC networking mode or something whereby each task will get its",
    "start": "2274580",
    "end": "2280290"
  },
  {
    "text": "own 443 basically even on the same host because there's multiple ionized",
    "start": "2280290",
    "end": "2285960"
  },
  {
    "text": "multiply 2 addresses that each one of them will have their own 4 for 3 if you don't use that networking mode yeah with",
    "start": "2285960",
    "end": "2295410"
  },
  {
    "text": "SSL termination not on the balancer within the container I think that's gonna be your main option or use a",
    "start": "2295410",
    "end": "2300930"
  },
  {
    "text": "dynamic randomly allocated port and use something like DNS based service discovery for that like console or route",
    "start": "2300930",
    "end": "2307980"
  },
  {
    "text": "53 or something so common pattern is to use the lambda function that listens to the event stream on ACS when a container",
    "start": "2307980",
    "end": "2314100"
  },
  {
    "text": "comes up it registers it in D and a similar route 53 but that is going to be integrated into e CS natively very soon",
    "start": "2314100",
    "end": "2322520"
  },
  {
    "text": "yes sir did you want to be so one of the issues we had when we",
    "start": "2322520",
    "end": "2330590"
  },
  {
    "text": "started wanted to auto scaling using ECS was that we couldn't figure out a way of configuring auto scaling for ec2 and ECS",
    "start": "2330590",
    "end": "2337850"
  },
  {
    "text": "at the same time so we ended up having loads of we just hold UCS to run 20 applications but it kept failing because",
    "start": "2337850",
    "end": "2343820"
  },
  {
    "text": "it couldn't settle them and then we just let easy to scale up and down is there an easy way to kind of apply the same",
    "start": "2343820",
    "end": "2349870"
  },
  {
    "text": "scaling rules for ECS and ec2 or do we have to just do it properly and",
    "start": "2349870",
    "end": "2356170"
  },
  {
    "text": "configure them the same things in two different places so I always kept it as two separate sets of rules because I",
    "start": "2356170",
    "end": "2362600"
  },
  {
    "text": "wanted to do this specific service level auto scaling because I wanted different requirements for each service and then I",
    "start": "2362600",
    "end": "2370040"
  },
  {
    "text": "have my cluster level auto scaling rules that took care of the cluster itself I",
    "start": "2370040",
    "end": "2377770"
  },
  {
    "text": "do not know of a more elegant way to take results of service scaling measures",
    "start": "2377770",
    "end": "2384560"
  },
  {
    "text": "as inputs other than doing a custom cloud watch metric like watch for the",
    "start": "2384560",
    "end": "2390260"
  },
  {
    "text": "event to say if you've tried to schedule this task and you can't then scale the",
    "start": "2390260",
    "end": "2395540"
  },
  {
    "text": "class drop but I've always scaled my cluster separately because they didn't feel like they went together for me but",
    "start": "2395540",
    "end": "2401630"
  },
  {
    "text": "I think generally most people scale those separately hence they're separated",
    "start": "2401630",
    "end": "2406790"
  },
  {
    "text": "out in the other problem if if you do have the recurrent scale both at the same time then yeah I think ultimately",
    "start": "2406790",
    "end": "2411980"
  },
  {
    "text": "that's something that will be need to be configured on both sides the application or two scaling and the instance auto scaling yeah there's there's another",
    "start": "2411980",
    "end": "2418520"
  },
  {
    "text": "repo linked from Nathan packs like awesome ECS that has an more advanced",
    "start": "2418520",
    "end": "2424340"
  },
  {
    "text": "auto scaling like scaling scale out based on lambda there it's quite cool it's quite flexible worth checking out",
    "start": "2424340",
    "end": "2430550"
  },
  {
    "text": "it's it's worth saying that before ECS did native auto scaling so Nathan and I",
    "start": "2430550",
    "end": "2436100"
  },
  {
    "text": "worked together before this also and we wrote something in in lambda and route",
    "start": "2436100",
    "end": "2443390"
  },
  {
    "text": "53 to handle that and then we also had something to handle like the socket connections where we needed SSL",
    "start": "2443390",
    "end": "2448850"
  },
  {
    "text": "determinate through nginx on the actual container but that's probably not Amazon",
    "start": "2448850",
    "end": "2454640"
  },
  {
    "text": "information security approved so we're possibly not gonna say that one on",
    "start": "2454640",
    "end": "2459859"
  },
  {
    "text": "Twitch but there's these are all possible I actually think Lind is that if you're not happy with the built in one writing",
    "start": "2459859",
    "end": "2467030"
  },
  {
    "text": "something custom and lambdas totally want to go um does EC s support non",
    "start": "2467030",
    "end": "2473809"
  },
  {
    "text": "ephemeral storage nine ephemeral storage meaning like a volume mounted or EFS EFS",
    "start": "2473809",
    "end": "2484750"
  },
  {
    "text": "so EFS is normally the go-to option I mean ultimately the architectural advice there should normally be try and get the",
    "start": "2486280",
    "end": "2493339"
  },
  {
    "text": "state outside of your containers anyway right think about things like s3 you think about dynamo RDS etc to move that",
    "start": "2493339",
    "end": "2499700"
  },
  {
    "text": "state out ultimately it's going to give you a better architecture more scalable scale pull independently etc that said",
    "start": "2499700",
    "end": "2506450"
  },
  {
    "text": "there are always some light occasions where you're running some third-party app or something where you need persistent storage in a file system form",
    "start": "2506450",
    "end": "2513400"
  },
  {
    "text": "EFS is the best option for that today and if you're looking for some people talking about doing that on Amazon",
    "start": "2513400",
    "end": "2520250"
  },
  {
    "text": "Arun Gupta who's on the open source team has written a couple of blog posts about doing persistent storage so I've never",
    "start": "2520250",
    "end": "2527809"
  },
  {
    "text": "wanted to keep my storage that way but there are definitely people out there doing it and EFS is probably the best",
    "start": "2527809",
    "end": "2534230"
  },
  {
    "text": "answer any other questions okay",
    "start": "2534230",
    "end": "2540579"
  },
  {
    "text": "you",
    "start": "2542670",
    "end": "2544730"
  }
]