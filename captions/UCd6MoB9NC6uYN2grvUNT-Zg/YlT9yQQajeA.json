[
  {
    "text": "In this video, you’ll see how to build generative\nAI applications using Amazon OpenSearch Service",
    "start": "896",
    "end": "6750"
  },
  {
    "text": "Machine Learning (ML) connectors.",
    "start": "6750",
    "end": "8475"
  },
  {
    "text": "With ML connectors, you can integrate with\nthird-party services, leverage existing open-source",
    "start": "8942",
    "end": "14469"
  },
  {
    "text": "ML algorithms to develop new ML features, and \nbuild semantic search to power your applications.",
    "start": "14469",
    "end": "20584"
  },
  {
    "text": "OpenSearch is a flexible, scalable, open-source\nengine built on Apache Lucene that can be",
    "start": "22136",
    "end": "27210"
  },
  {
    "text": "used as a vector database.",
    "start": "27211",
    "end": "28784"
  },
  {
    "text": "A typical ingestion pattern will look like this, \nwhere your enterprises’ raw data is converted",
    "start": "30000",
    "end": "34841"
  },
  {
    "text": "to vector embeddings using ML models.",
    "start": "34841",
    "end": "37326"
  },
  {
    "text": "Vector embeddings are numerical representations\nof text, image, audio, and video data.",
    "start": "37906",
    "end": "43410"
  },
  {
    "text": "The metadata, along with its vectors, is \nthen ingested and stored in OpenSearch.",
    "start": "44681",
    "end": "48852"
  },
  {
    "text": "Key challenges today include generating and\nhydrating vectors, keeping vectors up to date",
    "start": "50142",
    "end": "54960"
  },
  {
    "text": "with data changes, and building and maintaining\nmiddleware applications to integrate external",
    "start": "54960",
    "end": "60090"
  },
  {
    "text": "text-embedding models into search and \ningestion pipelines.",
    "start": "60090",
    "end": "64116"
  },
  {
    "text": "With the latest release of OpenSearch, this\nundifferentiated heavy lifting has been removed",
    "start": "65277",
    "end": "70180"
  },
  {
    "text": "and replaced with a more seamless vector \nhydration process.",
    "start": "70180",
    "end": "73511"
  },
  {
    "text": "Starting with version 2.9, managed OpenSearch\nService introduced an ML framework that includes",
    "start": "74838",
    "end": "80850"
  },
  {
    "text": "an ML connector, making it simpler to integrate\nwith external ML models, specifically those",
    "start": "80850",
    "end": "87008"
  },
  {
    "text": "hosted on Amazon SageMaker, Amazon Bedrock,\nCohere, and OpenAI.",
    "start": "87009",
    "end": "92640"
  },
  {
    "text": "The framework also includes an ML Commons\nplugin.",
    "start": "93855",
    "end": "96696"
  },
  {
    "text": "ML Commons for OpenSearch simplifies the \ndevelopment of machine learning features by ",
    "start": "97276",
    "end": "101606"
  },
  {
    "text": "providing a set of common ML algorithms \nthrough transport and REST API calls.",
    "start": "101606",
    "end": "106579"
  },
  {
    "text": "These integrations are part of the model-serving\nframework that aims to make it easier to operationalize",
    "start": "108075",
    "end": "113159"
  },
  {
    "text": "OpenSearch integrations with external models.",
    "start": "113159",
    "end": "115629"
  },
  {
    "text": "The framework includes ML extensibility through\nthe ML connector and its blueprint.",
    "start": "117143",
    "end": "121639"
  },
  {
    "text": "Amazon ML access control leverages the OpenSearch\nSecurity plugin and ML Commons to manage access",
    "start": "123154",
    "end": "129660"
  },
  {
    "text": "to models that are deployed externally.",
    "start": "129660",
    "end": "131655"
  },
  {
    "text": "In this demo, we’ll leverage two endpoints that \nhave been deployed on different external services:",
    "start": "133188",
    "end": "138242"
  },
  {
    "text": "SageMaker will host a text embedding model, \nand OpenAI will host a large language model.",
    "start": "138653",
    "end": "144294"
  },
  {
    "text": "First, we’ll create a connector.",
    "start": "145772",
    "end": "147136"
  },
  {
    "text": "Then, we’ll leverage the model access to\nsecurely access the external endpoints.",
    "start": "148558",
    "end": "152640"
  },
  {
    "text": "We’ll register the connector on Amazon OpenSearch\nService, which will return a model ID.",
    "start": "154079",
    "end": "158984"
  },
  {
    "text": "Next, we’ll use the model ID to deploy the model.",
    "start": "160424",
    "end": "163004"
  },
  {
    "text": "We’ll use the model to create a neural pipeline,\nand then we’ll create a vector index.",
    "start": "164557",
    "end": "169247"
  },
  {
    "text": "Next, we’ll ingest documents into OpenSearch\nService, and the service will leverage the",
    "start": "170612",
    "end": "175250"
  },
  {
    "text": "ML connector through the index pipeline to\ngenerate the required vectors for specific fields.",
    "start": "175250",
    "end": "180972"
  },
  {
    "text": "Finally, we’ll look at how to perform a\nvector search, and how the vector embedding",
    "start": "182356",
    "end": "185560"
  },
  {
    "text": "for the required query term is generated.",
    "start": "185560",
    "end": "187800"
  },
  {
    "text": "Now let’s get started.",
    "start": "188529",
    "end": "189559"
  },
  {
    "text": "This is the IAM console, where two roles have\nbeen created for the purposes of this demo:",
    "start": "192182",
    "end": "197750"
  },
  {
    "text": "the openai-role and the sagemaker-role.",
    "start": "197750",
    "end": "201150"
  },
  {
    "text": "To get started, let’s look at the SageMaker role.",
    "start": "202309",
    "end": "205000"
  },
  {
    "text": "This role has full access to the SageMaker\nservice.",
    "start": "206589",
    "end": "209346"
  },
  {
    "text": "Alternatively, we could have provided invoke\naccess to a specific model or a model group.",
    "start": "209776",
    "end": "214734"
  },
  {
    "text": "This trust policy ensures the tool has access\nto create the connector in OpenSearch Service.",
    "start": "216024",
    "end": "221184"
  },
  {
    "text": "Now let’s see how we’ve mapped the role\nin OpenSearch Dashboards.",
    "start": "222643",
    "end": "225700"
  },
  {
    "text": "We’ll navigate to the Security plugin and\nlook at the ml_full_access role.",
    "start": "226878",
    "end": "230378"
  },
  {
    "text": "Here, we can see that the ml full_access_role\nhas been mapped to the sagemaker-role",
    "start": "236380",
    "end": "240526"
  },
  {
    "text": "that we just saw.",
    "start": "240526",
    "end": "241628"
  },
  {
    "text": "Now, let’s switch to the Postman API client,\nwhere we’ve created an application to",
    "start": "242432",
    "end": "246419"
  },
  {
    "text": "deploy a connector.",
    "start": "246419",
    "end": "247590"
  },
  {
    "text": "To create a connector, send a POST request\nto the OpenSearch Service domain endpoint.",
    "start": "250264",
    "end": "254589"
  },
  {
    "text": "You can use curl, python, or the tool Postman.",
    "start": "255094",
    "end": "258078"
  },
  {
    "text": "Let’s review the application’s configuration\nto create the ML connector.",
    "start": "258863",
    "end": "262672"
  },
  {
    "text": "On the Authorization tab, we’ve set the\nauthorization type to AWS Signature.",
    "start": "263364",
    "end": "268805"
  },
  {
    "text": "We’ve provided the IAM access key ID and\nIAM secret access key to create the ML Connector.",
    "start": "270413",
    "end": "276404"
  },
  {
    "text": "Here, we can see the advanced configuration\nsettings.",
    "start": "280000",
    "end": "282870"
  },
  {
    "text": "We’ve provided the Region where the \nOpenSearch Service domain is deployed",
    "start": "283169",
    "end": "286515"
  },
  {
    "text": "and a valid Session token.",
    "start": "286515",
    "end": "287989"
  },
  {
    "text": "The service name for managed OpenSearch \nService is es.",
    "start": "288476",
    "end": "291468"
  },
  {
    "text": "The Body tab contains a JSON blueprint.",
    "start": "292926",
    "end": "295173"
  },
  {
    "text": "This blueprint contains several important\nkey-value pairs, including the service name",
    "start": "296707",
    "end": "301500"
  },
  {
    "text": "and region where the remote model exists.",
    "start": "301500",
    "end": "304041"
  },
  {
    "text": "This is the ARN for the SageMaker role \nthat has either full or invoke access to",
    "start": "305406",
    "end": "310000"
  },
  {
    "text": "the SageMaker service.",
    "start": "310000",
    "end": "311382"
  },
  {
    "text": "This is the URL to the external endpoint for\nour model deployed on SageMaker.",
    "start": "315290",
    "end": "319788"
  },
  {
    "text": "The blueprint also includes the request body,\nwhich will be passed to our model as an input.",
    "start": "321340",
    "end": "325845"
  },
  {
    "text": "You can write your own pre- and post-process\nfunctions to parse the neural search request",
    "start": "328464",
    "end": "333190"
  },
  {
    "text": "and response for your model format.",
    "start": "333190",
    "end": "335160"
  },
  {
    "text": "Let’s send a request to our ML plugin to\ncreate a connector.",
    "start": "336656",
    "end": "339641"
  },
  {
    "text": "We’ll copy the connector ID to use in a moment.",
    "start": "341156",
    "end": "343576"
  },
  {
    "text": "Now let's switch to the Dev Tools console in \nOpenSearch Dashboards to register the connector.",
    "start": "345109",
    "end": "349638"
  },
  {
    "text": "We’ll paste the connector ID that we just copied.",
    "start": "351190",
    "end": "353506"
  },
  {
    "text": "Next, let’s check for existing model groups.",
    "start": "354871",
    "end": "357067"
  },
  {
    "text": "We’ve already saved the ID for one of the\nexisting model groups, which we’ll use in",
    "start": "361761",
    "end": "365438"
  },
  {
    "text": "the next step.",
    "start": "365438",
    "end": "366219"
  },
  {
    "text": "Next, we’ll register the connector within\nthe model group.",
    "start": "367808",
    "end": "370752"
  },
  {
    "text": "We’ve already specified the model group\nID, so we just need to update the description",
    "start": "371874",
    "end": "375949"
  },
  {
    "text": "and connector ID.",
    "start": "375949",
    "end": "377211"
  },
  {
    "text": "Note the SageMaker embedding model we’re using.",
    "start": "378613",
    "end": "380716"
  },
  {
    "text": "A task ID has been created.",
    "start": "385428",
    "end": "387208"
  },
  {
    "text": "We’ll copy the task ID and use it to check\nthe registration status.",
    "start": "387732",
    "end": "391304"
  },
  {
    "text": "The registration has been completed.",
    "start": "397123",
    "end": "398499"
  },
  {
    "text": "Next, we’ll copy the model ID and use it\nto deploy our model.",
    "start": "400000",
    "end": "403389"
  },
  {
    "text": "Another task ID has been generated.",
    "start": "409242",
    "end": "411276"
  },
  {
    "text": "Let's use it to check the deployment status.",
    "start": "411519",
    "end": "413447"
  },
  {
    "text": "The model deployment has been completed, \nand we can now test the deployed model using",
    "start": "416102",
    "end": "420000"
  },
  {
    "text": "the predict API.",
    "start": "420000",
    "end": "421538"
  },
  {
    "text": "Text embeddings were generated by the model\nID, indicating the model is working as expected.",
    "start": "427765",
    "end": "433037"
  },
  {
    "text": "Next, let’s quickly retrieve information\nabout the model.",
    "start": "434103",
    "end": "436936"
  },
  {
    "text": "Let’s also review the connector information.",
    "start": "439610",
    "end": "441518"
  },
  {
    "text": "Here, we can see the endpoint the connector\npoints to, and the input the connector takes",
    "start": "453467",
    "end": "458229"
  },
  {
    "text": "for generating the embedding.",
    "start": "458229",
    "end": "459700"
  },
  {
    "text": "Next, we’ll convert an existing lexical\nindex called demostore into a semantic search",
    "start": "461720",
    "end": "466220"
  },
  {
    "text": "index that contains vectors.",
    "start": "466220",
    "end": "468030"
  },
  {
    "text": "Let's search the lexical index.",
    "start": "469395",
    "end": "471039"
  },
  {
    "text": "The results include information about products\nfrom a sample retail data set, which is available",
    "start": "472480",
    "end": "477289"
  },
  {
    "text": "on GitHub.",
    "start": "477289",
    "end": "478113"
  },
  {
    "text": "We’ll convert the product name and description\nfields to vectors.",
    "start": "479684",
    "end": "482690"
  },
  {
    "text": "To do this, first let’s create a neural pipeline \nusing the model ID we created earlier.",
    "start": "484092",
    "end": "489174"
  },
  {
    "text": "Here, we’ve specified the fields coming\nfrom our data source, and the names of",
    "start": "493757",
    "end": "497406"
  },
  {
    "text": "the corresponding vector fields.",
    "start": "497407",
    "end": "499458"
  },
  {
    "text": "The pipeline has been created.",
    "start": "502479",
    "end": "503825"
  },
  {
    "text": "Next, we’ll create the semantic search index.",
    "start": "504293",
    "end": "506654"
  },
  {
    "text": "Notice that index.knn is set to true.",
    "start": "507607",
    "end": "510302"
  },
  {
    "text": "This setting tells the plugin to create native\nlibrary indices for the vector index.",
    "start": "510845",
    "end": "515425"
  },
  {
    "text": "Additionally, the description_v and name_v\nfields that we saw in the pipeline have been",
    "start": "516827",
    "end": "521495"
  },
  {
    "text": "defined here.",
    "start": "521496",
    "end": "522615"
  },
  {
    "text": "Notice the dimension for these fields is 384.",
    "start": "523457",
    "end": "526548"
  },
  {
    "text": "This is the dimension that our SageMaker model\nsupports.",
    "start": "526979",
    "end": "530209"
  },
  {
    "text": "The semantic search index has been created.",
    "start": "532715",
    "end": "534794"
  },
  {
    "text": "Finally, we’ll reindex the data from the\nlexical index to the newly created semantic",
    "start": "536308",
    "end": "540589"
  },
  {
    "text": "search index.",
    "start": "540589",
    "end": "541640"
  },
  {
    "text": "The reindexing is complete and 2,465 documents\nare now available.",
    "start": "543772",
    "end": "548842"
  },
  {
    "text": "The desc_v and the name_v fields now contain\nthe vectors generated through the neural pipeline.",
    "start": "556883",
    "end": "562320"
  },
  {
    "text": "We can also see the other metadata that was\nadded to the index.",
    "start": "563822",
    "end": "566762"
  },
  {
    "text": "Now, let's perform a quick search using a\nsample query.",
    "start": "568258",
    "end": "570712"
  },
  {
    "text": "Because we are using the neural plugin, \nwe don't have to convert our input into",
    "start": "572226",
    "end": "576058"
  },
  {
    "text": "vectors again.",
    "start": "576058",
    "end": "577133"
  },
  {
    "text": "Rather, we can just call a neural query and\npass the model ID as a parameter to convert",
    "start": "577600",
    "end": "582821"
  },
  {
    "text": "the input into vectors.",
    "start": "582822",
    "end": "584261"
  },
  {
    "text": "Let's also add a price field.",
    "start": "585626",
    "end": "586847"
  },
  {
    "text": "Now we’ll run the search.",
    "start": "589761",
    "end": "590782"
  },
  {
    "text": "As expected, the price, name, and description\nare returned for each product within the scope",
    "start": "593120",
    "end": "597730"
  },
  {
    "text": "of our query.",
    "start": "597730",
    "end": "598626"
  },
  {
    "text": "The most relevant results are shown first.",
    "start": "600159",
    "end": "602240"
  },
  {
    "text": "We can also do a hybrid search.",
    "start": "603325",
    "end": "604681"
  },
  {
    "text": "Hybrid search enables us to fine tune our\nquery results by combining vector search and",
    "start": "606794",
    "end": "612150"
  },
  {
    "text": "full-text search in the same query.",
    "start": "612150",
    "end": "614540"
  },
  {
    "text": "This is the same query we ran before, but\nthis time it will only return products with",
    "start": "615737",
    "end": "620460"
  },
  {
    "text": "prices above $200.",
    "start": "620460",
    "end": "622527"
  },
  {
    "text": "Now let’s see how we can use the same flow\nto integrate OpenSearch with a large language",
    "start": "623998",
    "end": "628016"
  },
  {
    "text": "model hosted on OpenAI.",
    "start": "628017",
    "end": "629673"
  },
  {
    "text": "Let’s quickly review the application configuration\nin the Postman API client.",
    "start": "631188",
    "end": "635326"
  },
  {
    "text": "Note the OpenAI model we’re using.",
    "start": "636094",
    "end": "638069"
  },
  {
    "text": "We’ve configured access to the specified\nendpoint using an API key that has been",
    "start": "639621",
    "end": "644029"
  },
  {
    "text": "configured in AWS Secrets Manager.",
    "start": "644029",
    "end": "646531"
  },
  {
    "text": "We’ve also provided the credentials for\nthe openai-role we saw earlier.",
    "start": "648046",
    "end": "651568"
  },
  {
    "text": "Here, we’ve specified the public endpoint\nfor our model.",
    "start": "655345",
    "end": "658308"
  },
  {
    "text": "The request body includes the parameters required\nto make predictions.",
    "start": "659841",
    "end": "663157"
  },
  {
    "text": "Let’s execute this template.",
    "start": "665093",
    "end": "666320"
  },
  {
    "text": "A connecter ID has been created.",
    "start": "668658",
    "end": "670388"
  },
  {
    "text": "Let’s copy it and return to the Dev Tools console.",
    "start": "670706",
    "end": "673160"
  },
  {
    "text": "First, we’ll paste the connector ID.",
    "start": "675853",
    "end": "677761"
  },
  {
    "text": "Next, we’ll register the connector with\nthe same model group we used earlier.",
    "start": "678976",
    "end": "682301"
  },
  {
    "text": "A task ID is returned.",
    "start": "683826",
    "end": "685579"
  },
  {
    "text": "Let’s use it to check the registration status.",
    "start": "685691",
    "end": "687790"
  },
  {
    "text": "The registration has been completed.",
    "start": "691151",
    "end": "692700"
  },
  {
    "text": "Next, we’ll copy the ID for our OpenAI large\nlanguage model and use it to deploy the model.",
    "start": "693242",
    "end": "698478"
  },
  {
    "text": "Next, we’ll use the task ID to check if\nthe deployment is complete.",
    "start": "705078",
    "end": "708591"
  },
  {
    "text": "The model deployment has been completed.",
    "start": "712462",
    "end": "714186"
  },
  {
    "text": "Next, we’ll test the LLM model using the\npredict API and pass the prompts as messages.",
    "start": "715719",
    "end": "721428"
  },
  {
    "text": "Let’s review the results.",
    "start": "727038",
    "end": "728223"
  },
  {
    "text": "The LLM model quickly assumed the persona\nof an assistant and answered the question,",
    "start": "732637",
    "end": "737839"
  },
  {
    "text": "“What’s AWS?”",
    "start": "737839",
    "end": "739733"
  },
  {
    "text": "With the model working as expected, we could\nnow use a Retrieval Augmented Generation",
    "start": "740593",
    "end": "745473"
  },
  {
    "text": "framework to integrate our model with an \ninformation retrieval system.",
    "start": "745473",
    "end": "749228"
  },
  {
    "text": "In summary, OpenSearch ML connectors \ncan help you:",
    "start": "752070",
    "end": "755599"
  },
  {
    "text": "Create and personalize semantic search applications,",
    "start": "756216",
    "end": "759169"
  },
  {
    "text": "Build native integrations with active embedding\nservices,",
    "start": "759562",
    "end": "762523"
  },
  {
    "text": "Implement external ML connectors to power\nyour applications,",
    "start": "763140",
    "end": "766687"
  },
  {
    "text": "Secure and manage access to ML models, and",
    "start": "767117",
    "end": "770266"
  },
  {
    "text": "Leverage the distributed design of OpenSearch\nto build stable, scalable vector databases.",
    "start": "770753",
    "end": "775779"
  },
  {
    "text": "With the latest version, you can now create\nML connectors to specific model hosts through",
    "start": "777145",
    "end": "781970"
  },
  {
    "text": "the Amazon OpenSearch Console user interface\non the Integrations page.",
    "start": "781970",
    "end": "786702"
  },
  {
    "text": "You’ve just seen how to build generative\nAI applications using Amazon OpenSearch",
    "start": "788143",
    "end": "792084"
  },
  {
    "text": "Service ML connectors.",
    "start": "792084",
    "end": "793543"
  },
  {
    "text": "You can learn more about this topic in the\ndescription and links for this video.",
    "start": "795076",
    "end": "798649"
  },
  {
    "text": "Thanks for watching.",
    "start": "799566",
    "end": "800457"
  },
  {
    "text": "Now it’s your turn to try.",
    "start": "800943",
    "end": "802119"
  }
]