[
  {
    "start": "0",
    "end": "198000"
  },
  {
    "text": "good afternoon everyone my name is lei talsad hoon I'm a senior Solutions Architect at",
    "start": "1159",
    "end": "6810"
  },
  {
    "text": "Amazon Web Services and we're so excited to have you here for reinvent 2018 and",
    "start": "6810",
    "end": "12440"
  },
  {
    "text": "delighted to have you in our data Lakes session today it is my great pleasure to",
    "start": "12440",
    "end": "19230"
  },
  {
    "text": "be on stage with Greg and Varun from Cisco today who are going to share their journey and their technical architecture",
    "start": "19230",
    "end": "25859"
  },
  {
    "text": "for building data lakes on AWS just so",
    "start": "25859",
    "end": "32219"
  },
  {
    "text": "quickly recap cisco presented yesterday on their legacy application migrations",
    "start": "32219",
    "end": "37230"
  },
  {
    "text": "into AWS and you can find that session later uploaded on youtube so please",
    "start": "37230",
    "end": "42510"
  },
  {
    "text": "check that out and tomorrow we'll be holding a chalk talk for customizing",
    "start": "42510",
    "end": "48000"
  },
  {
    "text": "data lakes for your enterprise where we'll dive even deeper in the technology",
    "start": "48000",
    "end": "53989"
  },
  {
    "text": "so first I'm going to let Cisco share their story and their journey for building data lakes on AWS and really",
    "start": "54199",
    "end": "62609"
  },
  {
    "text": "get to D dive deep in their technical architecture I'm very excited and you know thrilled that they can share that",
    "start": "62609",
    "end": "68640"
  },
  {
    "text": "with you and for the second half of the session I'm going to dive deeper into some of the technical best practices and",
    "start": "68640",
    "end": "75020"
  },
  {
    "text": "architectural patterns that you can use to build your own data Lake on AWS and",
    "start": "75020",
    "end": "80240"
  },
  {
    "text": "with that I'm delighted to introduce Greg Nelson thank you all right",
    "start": "80240",
    "end": "86930"
  },
  {
    "text": "so thank you lathe and thank you AWS for allowing Cisco to share our story into",
    "start": "87270",
    "end": "94649"
  },
  {
    "text": "the cloud and the building of our data Lake I'm Greg Nelson I'm responsible for reporting and analytics at Cisco and",
    "start": "94649",
    "end": "101670"
  },
  {
    "text": "before we get into the discussion I want to tell you a little bit about the company so we are the global leader and",
    "start": "101670",
    "end": "107280"
  },
  {
    "text": "selling marketing and distributing food to restaurants health care and",
    "start": "107280",
    "end": "112409"
  },
  {
    "text": "educational facilities lodging establishments and really any customer that prepares meals away from home in",
    "start": "112409",
    "end": "118740"
  },
  {
    "text": "fact while you're here at the conference there's a very good chance that you'll eat or consume food that's been",
    "start": "118740",
    "end": "124670"
  },
  {
    "text": "hand-delivered by one of our trucks so know you'll notice on the slide our four",
    "start": "124670",
    "end": "129720"
  },
  {
    "text": "strategic priorities one of the first things you should see is customer and that's actually our customers",
    "start": "129720",
    "end": "135120"
  },
  {
    "text": "sorry that's actually our our vision statement is to be our customers most valued and trusted business partner one",
    "start": "135120",
    "end": "141540"
  },
  {
    "text": "of the other things you'll see is operational excellence so at the heart of a lot of what we do at Cisco we're",
    "start": "141540",
    "end": "147090"
  },
  {
    "text": "logistics company we have over 100 physical warehouse locations where we",
    "start": "147090",
    "end": "152700"
  },
  {
    "text": "ship and deliver food to our customers as well as the third largest lea privately owned fleet of delivery",
    "start": "152700",
    "end": "158790"
  },
  {
    "text": "vehicles in the continent to give you a little bit of a better feel of the size",
    "start": "158790",
    "end": "165090"
  },
  {
    "text": "of Cisco we do about sixty billion in revenue annually we're number 54 on the Fortune 100 and",
    "start": "165090",
    "end": "172530"
  },
  {
    "text": "we've been doing this for almost 50 years so we started in 1969 and we're a company that has grown mostly through",
    "start": "172530",
    "end": "179760"
  },
  {
    "text": "mergers and acquisitions and if you take a look this is where we're doing business today places like the US Canada",
    "start": "179760",
    "end": "186480"
  },
  {
    "text": "Mexico parts of South America and over in Europe as well overseas so now you",
    "start": "186480",
    "end": "192989"
  },
  {
    "text": "know a little bit about Cisco why the data leak at Cisco so at Cisco we had an",
    "start": "192989",
    "end": "200459"
  },
  {
    "start": "198000",
    "end": "198000"
  },
  {
    "text": "ADW we had a central place to store our data we could do reporting and analytics why did we need a data Lake and the",
    "start": "200459",
    "end": "208319"
  },
  {
    "text": "problem came into play where we started running out of capacity and having performance constraints and",
    "start": "208319",
    "end": "214609"
  },
  {
    "text": "while we had those issues new business cases that we wanted to make available so you'll see things like",
    "start": "214609",
    "end": "221120"
  },
  {
    "text": "clickstream machine learning data science social media data as well as",
    "start": "221120",
    "end": "228990"
  },
  {
    "text": "just transactional erp data from companies we were acquiring so we were",
    "start": "228990",
    "end": "235530"
  },
  {
    "text": "kind of in this paradigm where we had to do this cost-benefit analysis of whenever we wanted to move this data",
    "start": "235530",
    "end": "241980"
  },
  {
    "text": "into the enterprise data warehouse we're kind of looking at the ROI is you know can we do this and if we can do it is",
    "start": "241980",
    "end": "249660"
  },
  {
    "text": "that the right data set if we're running out of space do we need to buy more infrastructure not usually meant",
    "start": "249660",
    "end": "255769"
  },
  {
    "text": "spending millions of dollars of upskilling and buying new appliances and that's what I call the wall of business",
    "start": "255769",
    "end": "264720"
  },
  {
    "text": "constraint so you see it there it's a nice pink wall should be on the slide and I know what you're thinking thinking",
    "start": "264720",
    "end": "272820"
  },
  {
    "text": "Greg hey it's a short wall it's pink can you get over this thing what's going on",
    "start": "272820",
    "end": "278130"
  },
  {
    "text": "and what I found out was the pink is actually ectoplasm and for you guys that",
    "start": "278130",
    "end": "284100"
  },
  {
    "text": "are not Ghostbuster fans that's pink slime it's not easy to get over I've seen people that are in very good shape",
    "start": "284100",
    "end": "291210"
  },
  {
    "text": "you know they had the 26.2 sticker on the back of their vehicle run up slip",
    "start": "291210",
    "end": "296310"
  },
  {
    "text": "off this thing and fall on their face so I'm making light of a situation but the",
    "start": "296310",
    "end": "301320"
  },
  {
    "text": "reality was we really kind of came to this point where we had to make a decision where we going to invest",
    "start": "301320",
    "end": "307890"
  },
  {
    "text": "additional capital to advance in some of these use cases or we were not going to",
    "start": "307890",
    "end": "314190"
  },
  {
    "text": "advance our data strategy at Cisco at all so what were we going to do that's",
    "start": "314190",
    "end": "321360"
  },
  {
    "start": "321000",
    "end": "321000"
  },
  {
    "text": "where I introduced you seed and that's our branded data Lake stands for Cisco's",
    "start": "321360",
    "end": "326820"
  },
  {
    "text": "ecosystem for enterprise data and that gave us that central place that central",
    "start": "326820",
    "end": "331890"
  },
  {
    "text": "repository to store data but so much more so now we have actually our BI tools in the infrastructure we have",
    "start": "331890",
    "end": "339720"
  },
  {
    "text": "analytic engines we have MDM data new types of data we couldn't work with",
    "start": "339720",
    "end": "345210"
  },
  {
    "text": "before like images and text but the main thing that that really",
    "start": "345210",
    "end": "350950"
  },
  {
    "text": "moving into this building the data like changed for us was this paradigm which by leveraging the cost-effectiveness of",
    "start": "350950",
    "end": "359290"
  },
  {
    "text": "the cloud we could now not have to do an ROI analysis every time we wanted to put",
    "start": "359290",
    "end": "364780"
  },
  {
    "text": "more data in the data lake so for example we could stand up an s3 instance",
    "start": "364780",
    "end": "370140"
  },
  {
    "text": "put third-party data in there expose it to a data scientist or a data analyst",
    "start": "370140",
    "end": "377110"
  },
  {
    "text": "they could prove out their hypotheses if it had value we could scale it and if it didn't we would deprecated that asset",
    "start": "377110",
    "end": "383710"
  },
  {
    "text": "and so that's a whole new way a whole new paradigm of how we look at Cisco and that's one of the biggest value props",
    "start": "383710",
    "end": "390430"
  },
  {
    "text": "that I see by moving into the lake with AWS there's some other business outcomes",
    "start": "390430",
    "end": "396850"
  },
  {
    "text": "that I'll get to in a minute before we do that I want to tell you a little bit about the journey so in the",
    "start": "396850",
    "end": "404560"
  },
  {
    "text": "journey of moving to the cloud we definitely had some failures and it wasn't easy but there was kind of three",
    "start": "404560",
    "end": "411430"
  },
  {
    "text": "guiding principles that we found and we learned so one was start small we found",
    "start": "411430",
    "end": "417220"
  },
  {
    "text": "a use case that we could start small on and goes back to we didn't have to make this big capital investment in a new",
    "start": "417220",
    "end": "424780"
  },
  {
    "text": "appliance or new infrastructure secondly we learned we could pay for what we use and I'll talk about that in a minute",
    "start": "424780",
    "end": "430870"
  },
  {
    "text": "and lastly fail fast and pivot so the flexibility of the AWS marketplace I'll",
    "start": "430870",
    "end": "436960"
  },
  {
    "text": "give you an example of that in a second and the really the way we attacked this was by data subject area so for for us",
    "start": "436960",
    "end": "444310"
  },
  {
    "text": "that met things like merchandising sales supply chain operations and we started",
    "start": "444310",
    "end": "449860"
  },
  {
    "text": "with a use case around merchandising and basically we took that legacy data put",
    "start": "449860",
    "end": "455590"
  },
  {
    "text": "it up in the cloud and exposed it to our merchandisers and they did business",
    "start": "455590",
    "end": "460990"
  },
  {
    "text": "acceptance testing we found out hey this thing this thing works so let's move on to the next subject area we think we can",
    "start": "460990",
    "end": "467620"
  },
  {
    "text": "really scale this out and we moved on to sales and sales we actually do about 80%",
    "start": "467620",
    "end": "475150"
  },
  {
    "text": "of our analytics around sales so sales was tough and that's where we started I",
    "start": "475150",
    "end": "480180"
  },
  {
    "text": "guess in facing some of our challenges and we had some failures there so some of the",
    "start": "480180",
    "end": "485840"
  },
  {
    "text": "ETL patterns they were using for a traditional relational database they",
    "start": "485840",
    "end": "491090"
  },
  {
    "text": "didn't work in the cloud to be honest and at least they weren't performant so we were able to fail fast and basically",
    "start": "491090",
    "end": "500210"
  },
  {
    "text": "reached out to our AWS partners and spin up things like EMR clusters and use PI",
    "start": "500210",
    "end": "505550"
  },
  {
    "text": "spark patterns to do up certs instead of updates and that's because we use",
    "start": "505550",
    "end": "510680"
  },
  {
    "text": "redshift as our column their datastore kind of our central database application",
    "start": "510680",
    "end": "516198"
  },
  {
    "text": "for atw so we learned from that kind of an pivoted quickly and we did not lose",
    "start": "516199",
    "end": "523760"
  },
  {
    "text": "our budget or it wasn't at the risk of our timeline or a project timeline so",
    "start": "523760",
    "end": "528829"
  },
  {
    "text": "that was great moving kind of fruit through the different subject areas you kind of",
    "start": "528829",
    "end": "534050"
  },
  {
    "text": "seemed to move to supply chain and then all data sources and then finally we're in the cloud so our data ecosystem at",
    "start": "534050",
    "end": "540500"
  },
  {
    "text": "Cisco is fully in the cloud as a little over a year now so we're very proud of that and that's something we want to",
    "start": "540500",
    "end": "547760"
  },
  {
    "text": "continue to leverage two things before I move on from this slide we talked about",
    "start": "547760",
    "end": "553010"
  },
  {
    "text": "pay for what you use so as we were deploying these different subject areas we found out that we needed to do",
    "start": "553010",
    "end": "560300"
  },
  {
    "text": "end-to-end integration testing on a production like environment and that's what AWS gave us the flexibility to do",
    "start": "560300",
    "end": "567709"
  },
  {
    "text": "so we could actually spin up a production like mirror and do our testing it meant we were gonna pay a",
    "start": "567709",
    "end": "574220"
  },
  {
    "text": "little bit more maybe for a couple months but then once we deployed that we could spin that infrastructure down",
    "start": "574220",
    "end": "580250"
  },
  {
    "text": "that's flexibility we didn't have before secondly I'll call out when we were",
    "start": "580250",
    "end": "585529"
  },
  {
    "text": "about to go fully GA on seed so we're about to send out the comms to marketing",
    "start": "585529",
    "end": "590750"
  },
  {
    "text": "and throw the big party we had a bad day to load it was actually at the s3",
    "start": "590750",
    "end": "596839"
  },
  {
    "text": "staging layer so the great thing about s3 is it comes with data redundancy out",
    "start": "596839",
    "end": "603740"
  },
  {
    "text": "of the box it makes three copies of your data per availability zone and you can",
    "start": "603740",
    "end": "609589"
  },
  {
    "text": "set that up to scale across the globe so typically what that meant was go back to our data source systems find the deltas",
    "start": "609589",
    "end": "617720"
  },
  {
    "text": "figure out what reports we missed and that was usually a week-long exercise instead we were able to go to that s3",
    "start": "617720",
    "end": "625700"
  },
  {
    "text": "structure and point it to the backup and by the end of the day we had our",
    "start": "625700",
    "end": "630710"
  },
  {
    "text": "warehouse loaded in our reports delivered and so that was a big win for us as we were going live with seed so",
    "start": "630710",
    "end": "638210"
  },
  {
    "text": "lastly I'll talk a little bit about outcomes and then we'll hand it over to Varun to talk about some of the best",
    "start": "638210",
    "end": "645380"
  },
  {
    "text": "practices so this right here what we're showing you is some of the reporting we",
    "start": "645380",
    "end": "651110"
  },
  {
    "text": "do in Cisco and so we talked about how long we've been in business and some of these reports they've been in existence",
    "start": "651110",
    "end": "658190"
  },
  {
    "text": "for 20-plus years so that's why you look at the bottom left corner they're",
    "start": "658190",
    "end": "664850"
  },
  {
    "text": "formatted report and you can kind of think of that as York and reporting kind",
    "start": "664850",
    "end": "669860"
  },
  {
    "text": "of columnar in nature and we have a lot of reports I can tell you horror stories",
    "start": "669860",
    "end": "675380"
  },
  {
    "text": "of us bursting out over two hundred thousand reports a day but we knew we",
    "start": "675380",
    "end": "680750"
  },
  {
    "text": "wanted to do more things on the right so some of the exploratory and higher-value",
    "start": "680750",
    "end": "686330"
  },
  {
    "text": "analytics so things like visual analytics data science predictive and",
    "start": "686330",
    "end": "691370"
  },
  {
    "text": "prescriptive analytics and what AWS we allowed us to do was to do both and",
    "start": "691370",
    "end": "696710"
  },
  {
    "text": "that's hard to do so we were able to modernize our infrastructure so by",
    "start": "696710",
    "end": "702140"
  },
  {
    "text": "moving into AWS we actually have seen a 50% increase in our data load times and",
    "start": "702140",
    "end": "708770"
  },
  {
    "text": "I that also means data or report delivery so we're actually delivering faster we're modernizing our",
    "start": "708770",
    "end": "715070"
  },
  {
    "text": "infrastructure and there's more room to grow there so we can make things even faster while we're doing that we had",
    "start": "715070",
    "end": "722089"
  },
  {
    "text": "some use cases that we weren't able to enable around machine learning and data",
    "start": "722089",
    "end": "727190"
  },
  {
    "text": "science so things like personal recommendation engines that we really couldn't do its scale before so think",
    "start": "727190",
    "end": "733910"
  },
  {
    "text": "about search and icing and customers who bought this also about this we could expose that to our e-commerce experience",
    "start": "733910",
    "end": "741589"
  },
  {
    "text": "as well as our sales staff and make recommendations to them of as far as what kind of products maybe they should",
    "start": "741589",
    "end": "748460"
  },
  {
    "text": "be suggesting to custom another was you can see their churn model and also a use case around being",
    "start": "748460",
    "end": "757139"
  },
  {
    "text": "able to search for for items and what we did for that was around leveraging",
    "start": "757139",
    "end": "763260"
  },
  {
    "text": "dynamodb and elastic search we actually made a capability and put a UI on top of",
    "start": "763260",
    "end": "769050"
  },
  {
    "text": "being able to look for items substitutes so now with attribute data I can do",
    "start": "769050",
    "end": "774060"
  },
  {
    "text": "things like type in chicken breast frozen six outs and actually get a list",
    "start": "774060",
    "end": "780750"
  },
  {
    "text": "of results of comparable products that I can use as a substitute and I can even",
    "start": "780750",
    "end": "785940"
  },
  {
    "text": "give SKUs and using unstructured data we can show pictures of that now that was all done in the data team that wasn't an",
    "start": "785940",
    "end": "792779"
  },
  {
    "text": "active effort so these are things these are powers that we are we've kind of unlocked within this infrastructure and",
    "start": "792779",
    "end": "801269"
  },
  {
    "text": "I'll leave you with this before I turn it over one of the things I guess is an",
    "start": "801269",
    "end": "807440"
  },
  {
    "text": "unintended outcome and that is before we moved into the cloud of AWS I would call",
    "start": "807440",
    "end": "814410"
  },
  {
    "text": "our team a team of data developers so you think about DBAs data architects",
    "start": "814410",
    "end": "820589"
  },
  {
    "text": "data modelers ETL developers sequel developers and what AWS has allowed us",
    "start": "820589",
    "end": "825779"
  },
  {
    "text": "to do is get closer to the open source cloud so we're using tools now like EMR",
    "start": "825779",
    "end": "832019"
  },
  {
    "text": "PI spark hive Pig we're building our own custom data applications and really",
    "start": "832019",
    "end": "839699"
  },
  {
    "text": "that's raise the bar in the skill level that we have at Cisco and so now we've",
    "start": "839699",
    "end": "844889"
  },
  {
    "text": "moved from what I call a team of data developers to a team of data technologists so with that thank you for",
    "start": "844889",
    "end": "852449"
  },
  {
    "text": "your time I want to hand it over to Varun he's going to talk a little bit about some best practices we found while",
    "start": "852449",
    "end": "857490"
  },
  {
    "text": "moving in building our data like [Applause]",
    "start": "857490",
    "end": "867390"
  },
  {
    "text": "thank you Greg so as we get started you heard Greg talk about the why I'm here to talk about the",
    "start": "867390",
    "end": "874750"
  },
  {
    "text": "what and the how and before we get started I wanted to take a little bit time to specifically introduce to you",
    "start": "874750",
    "end": "881710"
  },
  {
    "text": "seed data repository data repository is the endpoint that we have that enables",
    "start": "881710",
    "end": "888070"
  },
  {
    "text": "our data like what is in that seed data repository so we have all our data we",
    "start": "888070",
    "end": "894040"
  },
  {
    "text": "have the transactions the observations the interactions from third party from all the different other sources and we",
    "start": "894040",
    "end": "900850"
  },
  {
    "text": "have our models that we use on our data set from personalization price migration assortment or customer eggs whisk and",
    "start": "900850",
    "end": "907750"
  },
  {
    "text": "other models that that that that I in that our data scientists and other individuals really need now as we move",
    "start": "907750",
    "end": "915940"
  },
  {
    "text": "towards and start talking about the how I want to take you through the architecture talk about some of the",
    "start": "915940",
    "end": "922660"
  },
  {
    "text": "customizations that we have done on the data like reference architecture from Amazon talk some of about talk a little",
    "start": "922660",
    "end": "930220"
  },
  {
    "text": "bit about the some of the lessons that we have learned and then finally finish off with where we are on the roadmap and",
    "start": "930220",
    "end": "935680"
  },
  {
    "text": "where we are trying to go so with that said let's quickly go talk about the",
    "start": "935680",
    "end": "940930"
  },
  {
    "text": "architecture as you see from a seed architecture perspective we use the",
    "start": "940930",
    "end": "946960"
  },
  {
    "start": "942000",
    "end": "942000"
  },
  {
    "text": "reference architecture that Amazon has for a data Lake implementation that reference architecture is an API",
    "start": "946960",
    "end": "952750"
  },
  {
    "text": "endpoint that uses API gateway to login to the micro services for data Lake now",
    "start": "952750",
    "end": "958480"
  },
  {
    "text": "those micro services are lambda enabled and those micro services have",
    "start": "958480",
    "end": "964260"
  },
  {
    "text": "opportunities to or solutions to create packages configure packages load data",
    "start": "964260",
    "end": "970350"
  },
  {
    "text": "update configure security and so on and so forth that is the place that we started I want to take your attention to",
    "start": "970350",
    "end": "976540"
  },
  {
    "text": "the items in the pink the basic services on the reference architecture is really there but from a data lag implementation",
    "start": "976540",
    "end": "984220"
  },
  {
    "text": "the first thing that you really want to sort out even before you do security from our perspective was to figure out",
    "start": "984220",
    "end": "989620"
  },
  {
    "text": "how we are going to ingest data how we are going to ingest lots of big data how are we going to ingest",
    "start": "989620",
    "end": "995089"
  },
  {
    "text": "lots of small data how are we going to ingest data on a periodic basis so we wrote our own custom module we like to",
    "start": "995089",
    "end": "1000819"
  },
  {
    "text": "call it gravity that allows us to ingest data on a regular basis to our data link",
    "start": "1000819",
    "end": "1007029"
  },
  {
    "text": "the source can be a red ship environment it can be an s3 bucket it can be any other external data source that are",
    "start": "1007029",
    "end": "1012759"
  },
  {
    "text": "sitting out there and we can ingest that data set within our environment the other thing that we added post that was",
    "start": "1012759",
    "end": "1019869"
  },
  {
    "text": "our integration to some of the outbound services that AWS provides so here you",
    "start": "1019869",
    "end": "1024938"
  },
  {
    "text": "see we have an integration to DynamoDB and integration to elasticsearch now why",
    "start": "1024939",
    "end": "1030220"
  },
  {
    "text": "is this important so from a reference architecture perspective this is where you separate compute and storage all our",
    "start": "1030220",
    "end": "1036459"
  },
  {
    "text": "data is sitting in s3 and through dynamodb and elasticsearch we have an opportunity to enable endpoints that",
    "start": "1036459",
    "end": "1042970"
  },
  {
    "text": "give our customers API base access give our customers a search based access to",
    "start": "1042970",
    "end": "1048759"
  },
  {
    "text": "our data at the end of the day that frees up your data to be more of a services oriented work versus trying to",
    "start": "1048759",
    "end": "1056379"
  },
  {
    "text": "just go by doing all kinds of reporting and sequel on the data set the third integration that we added was of",
    "start": "1056379",
    "end": "1062830"
  },
  {
    "text": "security because now we were ready to deploy the solution out to the workforce and the final fourth integration that we",
    "start": "1062830",
    "end": "1069490"
  },
  {
    "text": "added to this data structure was for making sure our data was available to be",
    "start": "1069490",
    "end": "1075249"
  },
  {
    "text": "used in the at scale infrastructure what does that mean the data that is sitting in s3 I can spin up my own redshift",
    "start": "1075249",
    "end": "1082269"
  },
  {
    "text": "cluster and work with it depending on my needs I can query the data in Amazon Athena I can even take the data to build",
    "start": "1082269",
    "end": "1089169"
  },
  {
    "text": "models in EMR as well as stage maker as the case may be so given this architecture these are the",
    "start": "1089169",
    "end": "1095499"
  },
  {
    "text": "four customizations that we've been able to do on the reference architecture to meet our need I want to pull two of",
    "start": "1095499",
    "end": "1101889"
  },
  {
    "text": "these customizations out and talk about it and a little more detail the first customization that I want to pull out",
    "start": "1101889",
    "end": "1107769"
  },
  {
    "text": "was it's specific to the dynamo DB and elastic search so let's pull that out",
    "start": "1107769",
    "end": "1113649"
  },
  {
    "text": "and talk about it now the data is sitting in seed already every day we have new services coming from Amazon",
    "start": "1113649",
    "end": "1119860"
  },
  {
    "text": "there are use cases that are coming around time series databases that we saw this morning there are use cases that are coming",
    "start": "1119860",
    "end": "1126070"
  },
  {
    "text": "around on the graph based databases that we see in the more we want to enable our developers to have",
    "start": "1126070",
    "end": "1131710"
  },
  {
    "text": "the right tool for the right jobs and that's why data in our data Lake can be consumed through any such service that",
    "start": "1131710",
    "end": "1138340"
  },
  {
    "text": "amazon has and enables us to be successful and that's how just separating compute from storage gives us",
    "start": "1138340",
    "end": "1144910"
  },
  {
    "text": "the use cases that we need for for Island consumers the second use case that I want to talk about is specific to",
    "start": "1144910",
    "end": "1152820"
  },
  {
    "text": "the at scale prototyping or the at scale work that we have in our Dalek so we",
    "start": "1152820",
    "end": "1158710"
  },
  {
    "text": "give our data scientist endpoint to build their own EMR clusters or an ec2",
    "start": "1158710",
    "end": "1165790"
  },
  {
    "text": "instance with Python libraries or in this case our libraries pre-loaded and",
    "start": "1165790",
    "end": "1171070"
  },
  {
    "text": "available to them to do their analysis now what has what that has done is that",
    "start": "1171070",
    "end": "1176620"
  },
  {
    "text": "has really accelerated the innovation that our data scientists are bringing to the table we have built solutions that",
    "start": "1176620",
    "end": "1183010"
  },
  {
    "text": "we couldn't build before we have a live driver compliance dashboard that's",
    "start": "1183010",
    "end": "1189010"
  },
  {
    "text": "enabled via the IOT devices that we have on our trucks so now in real time we can",
    "start": "1189010",
    "end": "1194020"
  },
  {
    "text": "give feedback to our drivers on what they should be doing to meet the delivery window and their performance",
    "start": "1194020",
    "end": "1199690"
  },
  {
    "text": "expectations similarly we've been able to integrate and build a custom recommendation engine that looks at our",
    "start": "1199690",
    "end": "1205840"
  },
  {
    "text": "plic streams data that looks at our transactions and is able to recommend",
    "start": "1205840",
    "end": "1210880"
  },
  {
    "text": "the right products for our sales force for our customers and even on the e-commerce Channel and then finally",
    "start": "1210880",
    "end": "1216970"
  },
  {
    "text": "we've had instances where cisco carries about 500,000 different items there is",
    "start": "1216970",
    "end": "1223480"
  },
  {
    "text": "an opportunity for us to take what is sold in one region available to other the other region and through the",
    "start": "1223480",
    "end": "1230470"
  },
  {
    "text": "investments that we've been or our data scientists have been doing on the platform we can do some of those use cases because there's a lot of search",
    "start": "1230470",
    "end": "1237520"
  },
  {
    "text": "and matching that is available on those environments moving on not all of this",
    "start": "1237520",
    "end": "1244630"
  },
  {
    "start": "1243000",
    "end": "1243000"
  },
  {
    "text": "has been without a lot of learning so I want to consolidate learning into four",
    "start": "1244630",
    "end": "1250300"
  },
  {
    "text": "things that that I want to leave you with what are those four things the first is a catalog centralized",
    "start": "1250300",
    "end": "1257320"
  },
  {
    "text": "repository the data in the data lake is really not",
    "start": "1257320",
    "end": "1262790"
  },
  {
    "text": "very useful if you don't have good metadata to go with it and this is where",
    "start": "1262790",
    "end": "1267830"
  },
  {
    "text": "a foundational data structure needs to be able to integrate solidly in",
    "start": "1267830",
    "end": "1273200"
  },
  {
    "text": "everything that are or that your end-users are doing so there is this opportunity for you to make sure that",
    "start": "1273200",
    "end": "1279350"
  },
  {
    "text": "everything that you bring to the lake is catalog curated and it's available for people to consume that's was one thing",
    "start": "1279350",
    "end": "1286070"
  },
  {
    "text": "that we ran into fairly quickly the other thing that I wanted to talk about is data as a service you heard Greg",
    "start": "1286070",
    "end": "1292580"
  },
  {
    "text": "mentioned that we had thousands of reports so two hundred thousand to be exact one of the things that a data Lake",
    "start": "1292580",
    "end": "1298790"
  },
  {
    "text": "gives you is this a publish/subscribe architecture so end-users can publish",
    "start": "1298790",
    "end": "1304910"
  },
  {
    "text": "content there are consumers that can subscribe to content and there is no need for us to burst a lot of reports",
    "start": "1304910",
    "end": "1310640"
  },
  {
    "text": "out there that has worked out to be a more of a change management issue then",
    "start": "1310640",
    "end": "1316280"
  },
  {
    "text": "more of a technology issue so that that's something that you'll get into when you talk about the migration third",
    "start": "1316280",
    "end": "1323420"
  },
  {
    "text": "thing that I want to talk about is a rapid experimentation platform if you're building a data Lake and if you're not",
    "start": "1323420",
    "end": "1328850"
  },
  {
    "text": "allowing your end-users to take advantage of the data Lake by giving the builders the tools that they need it's",
    "start": "1328850",
    "end": "1334550"
  },
  {
    "text": "all of the data is sitting here without really being used so there is this opportunity for you to really unlock and",
    "start": "1334550",
    "end": "1340250"
  },
  {
    "text": "enable all the individuals that can do something with it to really use the platform and this this specific case",
    "start": "1340250",
    "end": "1346910"
  },
  {
    "text": "what one thing that that came to for was we have all heard of shadow analytics",
    "start": "1346910",
    "end": "1353360"
  },
  {
    "text": "done in the business teams people have their own data bases they they configure the database update it work on it and",
    "start": "1353360",
    "end": "1360710"
  },
  {
    "text": "they do the enough analysis in a Scylla the opportunity there is is that you need to talk to those individuals and",
    "start": "1360710",
    "end": "1367220"
  },
  {
    "text": "bring them to the fore the only reason they are not on the platform is because you cannot in create iterate as quickly",
    "start": "1367220",
    "end": "1372470"
  },
  {
    "text": "as they want to and this is an opportunity for you to kind of bring those individual back into the fold to",
    "start": "1372470",
    "end": "1377540"
  },
  {
    "text": "work with you as you build that platform and then finally data Lake is an",
    "start": "1377540",
    "end": "1383990"
  },
  {
    "text": "investment you are putting a lot of data in there if it's manual if you're updating data if you have manual",
    "start": "1383990",
    "end": "1391070"
  },
  {
    "text": "Corrections and if you're doing a lot of reprocessing it's really not going to scale and work for you invest in automation every",
    "start": "1391070",
    "end": "1398419"
  },
  {
    "text": "repeatable tasks that you have in your environment that needs to be automated and be available for your end users to",
    "start": "1398419",
    "end": "1404570"
  },
  {
    "text": "take advantage of so with that lessons learned I want to talk about what are our priorities as we",
    "start": "1404570",
    "end": "1413480"
  },
  {
    "start": "1409000",
    "end": "1409000"
  },
  {
    "text": "move forward last year we stood before you and spoke as a CT seed was the starting point of",
    "start": "1413480",
    "end": "1420559"
  },
  {
    "text": "everything we had a basic data ecosystem on cloud and we had rudimentary tools to",
    "start": "1420559",
    "end": "1426860"
  },
  {
    "text": "work with the data set we had predominantly batch based scenarios since then we've made the journey we're",
    "start": "1426860",
    "end": "1433309"
  },
  {
    "text": "moving towards seedling to sapling to finally to a tree so what do these different things mean for us from a",
    "start": "1433309",
    "end": "1440360"
  },
  {
    "text": "seedling perspective we want to build a centralized repository that is curated",
    "start": "1440360",
    "end": "1445460"
  },
  {
    "text": "that is catalogued and that is available for our end users with good kind of data",
    "start": "1445460",
    "end": "1452419"
  },
  {
    "text": "provenance as well as the opportunity for automation around how I update the",
    "start": "1452419",
    "end": "1457820"
  },
  {
    "text": "data in as we mature we want to make",
    "start": "1457820",
    "end": "1462830"
  },
  {
    "text": "sure that our data scientist and all other individuals that are on our platform are able to build and scale our",
    "start": "1462830",
    "end": "1470720"
  },
  {
    "text": "machine learning models and I like to call that enabling data as a service so",
    "start": "1470720",
    "end": "1476720"
  },
  {
    "text": "it could be these individuals are coming to you with a notebook endpoint they are",
    "start": "1476720",
    "end": "1482840"
  },
  {
    "text": "data developers that are trying to integrate your data into their products so they need an API endpoint they are",
    "start": "1482840",
    "end": "1489110"
  },
  {
    "text": "coming to you with with a sequel query whatever have you there is an opportunity for you to enable data as a",
    "start": "1489110",
    "end": "1495049"
  },
  {
    "text": "service and that's where we we think a sapling is kind of a place and then finally a tree tree for us means really",
    "start": "1495049",
    "end": "1502970"
  },
  {
    "text": "accelerating the analytics Vala velocity of the organization we need to iterate quickly and rapidly on the analysis and",
    "start": "1502970",
    "end": "1509240"
  },
  {
    "text": "make sure we have a good understanding of what is production and what is exploration and that's how I see as",
    "start": "1509240",
    "end": "1515780"
  },
  {
    "text": "maturing as we move forward with that I'll forward it too late to talk more from a best practices perspective for",
    "start": "1515780",
    "end": "1522200"
  },
  {
    "text": "add italic",
    "start": "1522200",
    "end": "1524649"
  },
  {
    "text": "thank you very much Varun and thank you Greg again also for sharing your story",
    "start": "1529110",
    "end": "1534720"
  },
  {
    "text": "really quite inspiring how many folks here are currently starting to look at our building data like just a quick show",
    "start": "1534720",
    "end": "1540820"
  },
  {
    "text": "of hands Wow really great numbers thank you so much for sharing that so I'm",
    "start": "1540820",
    "end": "1546490"
  },
  {
    "text": "gonna dive into some best practices and patterns that we see across customers I have the great fortune to work with a",
    "start": "1546490",
    "end": "1553690"
  },
  {
    "text": "small group of our millions of active customers every month in the Houston Texas area and again I'm very glad to to",
    "start": "1553690",
    "end": "1560919"
  },
  {
    "text": "be with you here today but first just quick background on where our customers are coming from for a long time",
    "start": "1560919",
    "end": "1567750"
  },
  {
    "text": "analytics and data architectures have existed in these data silos where these",
    "start": "1567750",
    "end": "1573460"
  },
  {
    "text": "engines and platforms are dissimilar they are heterogeneous and that presents",
    "start": "1573460",
    "end": "1579790"
  },
  {
    "text": "some challenges right there are both licenses and licensing implications infrastructure scaling implications and",
    "start": "1579790",
    "end": "1586870"
  },
  {
    "text": "I want to talk about four key functional challenges that we have seen with",
    "start": "1586870",
    "end": "1593260"
  },
  {
    "start": "1589000",
    "end": "1589000"
  },
  {
    "text": "customers the first is that in these dissimilar data engines and data",
    "start": "1593260",
    "end": "1598390"
  },
  {
    "text": "platforms it becomes very difficult to perform queries and analysis over all of",
    "start": "1598390",
    "end": "1604000"
  },
  {
    "text": "your data and you have to introduce complex sometimes fragile extract transform and load or ETL logic into",
    "start": "1604000",
    "end": "1610929"
  },
  {
    "text": "that and that produces data copies I know customers who have dozens of",
    "start": "1610929",
    "end": "1616510"
  },
  {
    "text": "copies of their data or more and it produces a complex scheduling and so",
    "start": "1616510",
    "end": "1622900"
  },
  {
    "text": "forth in those in those systems the second is the ability to support data",
    "start": "1622900",
    "end": "1628929"
  },
  {
    "text": "types and formats with a for instance a traditional relational database architecture you're stuck with schemas",
    "start": "1628929",
    "end": "1636040"
  },
  {
    "text": "that's enforced when you write the data and it becomes hard to adapt to new data types right data types being inch or",
    "start": "1636040",
    "end": "1643390"
  },
  {
    "text": "string or something else or data formats whether that's something unstructured and this also is attached to the next",
    "start": "1643390",
    "end": "1649299"
  },
  {
    "text": "topic the next challenge which is streamed real-time streaming data the traditional data architectures we",
    "start": "1649299",
    "end": "1655300"
  },
  {
    "text": "customers have found it hard to scale traditional architectures for or the throughput of real-time",
    "start": "1655300",
    "end": "1660520"
  },
  {
    "text": "processing as well as the dynamic nature of real-time processing right that dynamic processing or dynamic data types",
    "start": "1660520",
    "end": "1668230"
  },
  {
    "text": "of streaming data that often comes in right oftentimes you'll add a new field into your streaming data or for instance",
    "start": "1668230",
    "end": "1675490"
  },
  {
    "text": "in your sensor data from IOT you're gonna add new telemetry a new field to that stream and it becomes hard to adapt",
    "start": "1675490",
    "end": "1681429"
  },
  {
    "text": "that to you know your traditional data like a traditional data architecture and",
    "start": "1681429",
    "end": "1686789"
  },
  {
    "text": "lastly because these systems are dissimilar whether it's in a relational database or a Hadoop cluster or an e DW",
    "start": "1686789",
    "end": "1694929"
  },
  {
    "text": "on premise they have different security controls different api's different",
    "start": "1694929",
    "end": "1700510"
  },
  {
    "text": "control planes and different scaling mechanisms that it becomes very hard or if not impossible to apply consistent",
    "start": "1700510",
    "end": "1708210"
  },
  {
    "text": "security and government policy governance policies over those data sources and so translating that to why",
    "start": "1708210",
    "end": "1716530"
  },
  {
    "start": "1714000",
    "end": "1714000"
  },
  {
    "text": "customers build data lakes first of all data Lake is a single repositories to collect all of your data",
    "start": "1716530",
    "end": "1723250"
  },
  {
    "text": "whether that's business data from ERP transactions streaming data from",
    "start": "1723250",
    "end": "1728470"
  },
  {
    "text": "clickstream or sensors as well as even unstructured data like imagery we have customers in oil and gas collecting",
    "start": "1728470",
    "end": "1734590"
  },
  {
    "text": "seismic data a video clip data and more all on s3 which we'll talk about and",
    "start": "1734590",
    "end": "1741059"
  },
  {
    "text": "that's really the important aspect that you need a storage platform that can support that rapid growth of data and",
    "start": "1741059",
    "end": "1748559"
  },
  {
    "text": "virtually unlimited types of data the second reason is that we see data lakes",
    "start": "1748559",
    "end": "1756940"
  },
  {
    "text": "being used to support a diverse set of customers like Varun and Greg mentioned",
    "start": "1756940",
    "end": "1762100"
  },
  {
    "text": "you have not only traditional business analysts but now you have data scientists and even do your developers",
    "start": "1762100",
    "end": "1768190"
  },
  {
    "text": "who need access to data to make more intelligent applications and so as you're building a data like these should",
    "start": "1768190",
    "end": "1773230"
  },
  {
    "text": "be some of your tenants they should be able to consider a wide range of our support support Voort virtually all of",
    "start": "1773230",
    "end": "1780370"
  },
  {
    "text": "your data consumers in your business and external customers and with that comes",
    "start": "1780370",
    "end": "1785950"
  },
  {
    "text": "access mechanisms right developers need access to api's your analysts still want",
    "start": "1785950",
    "end": "1792010"
  },
  {
    "text": "to do business intelligence using sequel queries and machine learning experts and",
    "start": "1792010",
    "end": "1797440"
  },
  {
    "text": "your data scientists need access to store and collect training data and use",
    "start": "1797440",
    "end": "1803980"
  },
  {
    "text": "things like notebooks on Jupiter or Zeppelin and so forth to train their machine learning algorithms so just to",
    "start": "1803980",
    "end": "1812440"
  },
  {
    "start": "1811000",
    "end": "1811000"
  },
  {
    "text": "quickly recap collecting all of your data is a key part of one place is a key port part of your data Lake strategy you",
    "start": "1812440",
    "end": "1820269"
  },
  {
    "text": "should be able to dive in meaning you should be able to both find an individual record in your data using",
    "start": "1820269",
    "end": "1826419"
  },
  {
    "text": "something like sequel or a REST API and you should also be able to perform a large aggregation right if you need to",
    "start": "1826419",
    "end": "1832720"
  },
  {
    "text": "run a query over just the last day of data or the last ten years of data that",
    "start": "1832720",
    "end": "1838149"
  },
  {
    "text": "you have you should be able to do that with ease and support you know wide variety of tools like BI tools on the",
    "start": "1838149",
    "end": "1845350"
  },
  {
    "text": "market place Jupiter notebooks for machine learning and it should be capable for you know the very fast",
    "start": "1845350",
    "end": "1852129"
  },
  {
    "text": "changing landscape for big data and machine learning right seems like every day there's a new machine learning",
    "start": "1852129",
    "end": "1858639"
  },
  {
    "text": "framework there's a new big data tool from Hadoop right and you need to be able to support that through using open",
    "start": "1858639",
    "end": "1864580"
  },
  {
    "text": "file open file formats like CSV Park AOR C and so forth especially for your for",
    "start": "1864580",
    "end": "1870820"
  },
  {
    "text": "your business related data the last tenant I want to mention about data Lake and then we'll dive into what",
    "start": "1870820",
    "end": "1876759"
  },
  {
    "text": "are the key AWS components and how you can do this Varun Varun mentioned this but you really need to consider",
    "start": "1876759",
    "end": "1883929"
  },
  {
    "text": "separating your data from your storage from your separating your storage from compute excuse me when you have a",
    "start": "1883929",
    "end": "1890529"
  },
  {
    "text": "traditional architecture you oftentimes have this vertically scale or even horizontally scale a data warehouse or",
    "start": "1890529",
    "end": "1896859"
  },
  {
    "text": "Hadoop cluster simply just to add more storage or vice versa right for instance Hadoop HDFS has a default replication of",
    "start": "1896859",
    "end": "1904539"
  },
  {
    "text": "3x and you have to kind of scale out data nodes where you may not need compute so by separating your storage",
    "start": "1904539",
    "end": "1912039"
  },
  {
    "text": "from compute that gives you a lot of advantages right you have the ability to use the right tools for the job so you",
    "start": "1912039",
    "end": "1919359"
  },
  {
    "text": "can bring say spark to your data Lake or you can bring flink you can bring ML",
    "start": "1919359",
    "end": "1925599"
  },
  {
    "text": "frameworks like sir pho or MX net to the data lake so you can use exactly the right tool for the job you're allowed you're now have",
    "start": "1925599",
    "end": "1932889"
  },
  {
    "text": "the flexibility to scale these independently right as your data grows it may grow at a much higher velocity at",
    "start": "1932889",
    "end": "1938919"
  },
  {
    "text": "a much higher rate than just the compute itself third you have the ability to use",
    "start": "1938919",
    "end": "1945279"
  },
  {
    "text": "temporary compute resources on your data meaning oftentimes you may produce a pre",
    "start": "1945279",
    "end": "1950350"
  },
  {
    "text": "computed view or turn out a report from the data lake but you don't need a cluster or ec2 instances running 24/7",
    "start": "1950350",
    "end": "1957850"
  },
  {
    "text": "you can simply run that and orchestrate your orchestrate your compute just like you would orchestrate the job itself and",
    "start": "1957850",
    "end": "1963879"
  },
  {
    "text": "we'll dive into that a little bit later so s3 by far is the the most popular",
    "start": "1963879",
    "end": "1973649"
  },
  {
    "start": "1968000",
    "end": "1968000"
  },
  {
    "text": "object storage in the cloud and for many reasons forms the best place to build",
    "start": "1973649",
    "end": "1979990"
  },
  {
    "text": "your data lake on top of that s3 has designed to provide eleven nines of",
    "start": "1979990",
    "end": "1985179"
  },
  {
    "text": "durability by storing multiple copies of your data in multiple facilities and we handle the replication the error",
    "start": "1985179",
    "end": "1991149"
  },
  {
    "text": "checking and so forth and it's very economical right and we have the different storage tiers so we see that",
    "start": "1991149",
    "end": "1997000"
  },
  {
    "text": "s3 is the center of the data Lake for collecting and storing your transactions data imagery data audio from call center",
    "start": "1997000",
    "end": "2005789"
  },
  {
    "text": "and the use cases are almost endless now let's talk about some of the key",
    "start": "2005789",
    "end": "2011159"
  },
  {
    "text": "components of the data Lake the first I want to mention is data ingestion and while I would love to go in depth and",
    "start": "2011159",
    "end": "2016590"
  },
  {
    "text": "each one of these icons here I'm going to kind of glance over them but just know that we're talking about high-level",
    "start": "2016590",
    "end": "2023159"
  },
  {
    "text": "AWS services right we're not talking about building compute infrastructure every time you want to build the data",
    "start": "2023159",
    "end": "2028620"
  },
  {
    "text": "Lake you can use our high level services so the first is ingestion all right and you need a scalable pipeline scalable",
    "start": "2028620",
    "end": "2035490"
  },
  {
    "text": "network connection most likely back from your data center for instance to collect things from your transactional databases",
    "start": "2035490",
    "end": "2042090"
  },
  {
    "text": "from api's maybe that you're still running on premise things like the req connect can help you do that in Storage",
    "start": "2042090",
    "end": "2048000"
  },
  {
    "text": "Gateway when you're considering your",
    "start": "2048000",
    "end": "2053190"
  },
  {
    "text": "data Lake building on top of s3 you do need the ability to quickly search and find your objects in the data Lake and",
    "start": "2053190",
    "end": "2060270"
  },
  {
    "text": "apply perhaps a secondary index over your data like by using dynamodb or",
    "start": "2060270",
    "end": "2065520"
  },
  {
    "text": "elasticsearch and when you're using Hadoop frameworks you need a hive meta store to run sequel queries over that",
    "start": "2065520",
    "end": "2072740"
  },
  {
    "text": "over the data Lake to use s3 as the external file system for those frameworks so included here is also the",
    "start": "2072740",
    "end": "2079679"
  },
  {
    "text": "AWS Glu data catalog which allows you to run crawlers and infer the external",
    "start": "2079679",
    "end": "2085408"
  },
  {
    "text": "table definitions for the data in your data Lakes so you can simply collect data and s3 in a CSV format o RC park'",
    "start": "2085409",
    "end": "2093300"
  },
  {
    "text": "and run a crawler and it will produce your table data definition language for",
    "start": "2093300",
    "end": "2098850"
  },
  {
    "text": "that and allow you to query right away security is always job 0 and if you've",
    "start": "2098850",
    "end": "2104580"
  },
  {
    "text": "built on AWS so far this tools and services are consistent right you have AWS Identity and Access Management",
    "start": "2104580",
    "end": "2111140"
  },
  {
    "text": "policies for your users and groups and roles that you can use to control access",
    "start": "2111140",
    "end": "2116160"
  },
  {
    "text": "to s3 buckets control access to the glue date table and databases and you can use",
    "start": "2116160",
    "end": "2123480"
  },
  {
    "text": "cloud trail for instance to audit and log the things that are happening the changes that are occurring in your data lake with for instance with the data",
    "start": "2123480",
    "end": "2132420"
  },
  {
    "text": "reference data like reference architecture you may want to apply additional api's and UI's where you can",
    "start": "2132420",
    "end": "2137970"
  },
  {
    "text": "use api gateway Cognito for authentication and federated Identity Management from say Active Directory or",
    "start": "2137970",
    "end": "2145530"
  },
  {
    "text": "even Facebook and Google using Cognito allowing you to present an authentication layer or service to your",
    "start": "2145530",
    "end": "2152310"
  },
  {
    "text": "API in your front ends and last but definitely not least we have to talk about the analytics into serving so",
    "start": "2152310",
    "end": "2160890"
  },
  {
    "text": "there is a breadth and depth of services that you can use to provide analytics to",
    "start": "2160890",
    "end": "2167190"
  },
  {
    "text": "your end users for example Amazon EMR provides Hadoop frameworks like spark",
    "start": "2167190",
    "end": "2173790"
  },
  {
    "text": "flink hive presto that allow you to run a cluster on demand in just minutes and",
    "start": "2173790",
    "end": "2178890"
  },
  {
    "text": "run those queries by using s3 as the external file system just like if you're",
    "start": "2178890",
    "end": "2184230"
  },
  {
    "text": "if you have any experience with hadoop you know you have to use for instance HDFS s3 allows you to use that almost as",
    "start": "2184230",
    "end": "2190590"
  },
  {
    "text": "if it were HDFS so you can spin up a cluster just when you need point it at the s3 location for your",
    "start": "2190590",
    "end": "2196020"
  },
  {
    "text": "table definition and run your query right away and finally you can terminate that cluster when you're done and you",
    "start": "2196020",
    "end": "2203160"
  },
  {
    "text": "can use spot instances so we'll talk about some of these other ones but there there are these higher-level services you can use as well Athena Amazon Athena",
    "start": "2203160",
    "end": "2210090"
  },
  {
    "text": "for instance where you can run a sequel query directly on top of your data and s3 without provisioning a cluster and",
    "start": "2210090",
    "end": "2216510"
  },
  {
    "text": "paid just for the query allowing ad hoc analytics and BI use cases directly on",
    "start": "2216510",
    "end": "2222270"
  },
  {
    "text": "top of your data Lake so there there's a wealth of information here but what I want you to take away is that again",
    "start": "2222270",
    "end": "2228060"
  },
  {
    "text": "there's just a breadth of portfolio here that you can use that you can use the best tool for the job right there's not",
    "start": "2228060",
    "end": "2233820"
  },
  {
    "text": "one perfect thing and there are certain things that are work better for the use case right so redshift is awesome for",
    "start": "2233820",
    "end": "2240450"
  },
  {
    "text": "your high throughput bi use cases where where you need scheduled reports and you",
    "start": "2240450",
    "end": "2246030"
  },
  {
    "text": "have lots of concurrent concurrent reporting queries but your data Lake is great to dive in on all of your data and",
    "start": "2246030",
    "end": "2253110"
  },
  {
    "text": "run more ad hoc queries and data exploration for instance so s3 to recap",
    "start": "2253110",
    "end": "2262460"
  },
  {
    "text": "highly durable right very cost effective giving you around 2.3 cents per gig per",
    "start": "2262460",
    "end": "2268320"
  },
  {
    "text": "month and we announce things like intelligent tearing to move your data between our storage classes which offer",
    "start": "2268320",
    "end": "2274020"
  },
  {
    "text": "lower cost points for your data in s3 and it's very high performance so when",
    "start": "2274020",
    "end": "2280770"
  },
  {
    "text": "you look at for instance presto spark and you access your data over s3 because s3 is presented as a REST API it can",
    "start": "2280770",
    "end": "2289710"
  },
  {
    "text": "make multiple read and write requests over the data so you still have very high performance for big data queries and analysis it's as I mentioned very",
    "start": "2289710",
    "end": "2297360"
  },
  {
    "text": "highly integrated as you saw previously with a gesture our security and monitoring analytic services and like",
    "start": "2297360",
    "end": "2304350"
  },
  {
    "text": "you heard in Keeton in the keynote you have higher level services where you can train now recommendation personalization",
    "start": "2304350",
    "end": "2310470"
  },
  {
    "text": "services over the data and s3 so when I",
    "start": "2310470",
    "end": "2318300"
  },
  {
    "start": "2316000",
    "end": "2316000"
  },
  {
    "text": "practice this with some folks they told me Lafe you can you find like a better picture for the security like that",
    "start": "2318300",
    "end": "2324630"
  },
  {
    "text": "doesn't look secure so my point is don't don't let you don't do your data like like that but",
    "start": "2324630",
    "end": "2330089"
  },
  {
    "text": "the key thing is consider your security from default from the default encryption",
    "start": "2330089",
    "end": "2335430"
  },
  {
    "text": "should be by default right we have the services and capabilities virtually a checkbox to upload your objects using a",
    "start": "2335430",
    "end": "2342119"
  },
  {
    "text": "Amazon key management service with encryption so you should take advantage of that take advantage of the you know",
    "start": "2342119",
    "end": "2348779"
  },
  {
    "text": "the Triple A features authenticate authorize and audit so iam policies you",
    "start": "2348779",
    "end": "2354720"
  },
  {
    "text": "can use to control access to your objects down to the object level on the iam users and roles as well as the",
    "start": "2354720",
    "end": "2362490"
  },
  {
    "text": "bucket policies themselves where you can control access you can now you can now also use the glue data catalog resource",
    "start": "2362490",
    "end": "2368970"
  },
  {
    "text": "based policies so you can control access at the meta store level of who can use those table definitions and databases to",
    "start": "2368970",
    "end": "2375779"
  },
  {
    "text": "clear the data and s3 so you have fine grain control over that you can use s3 V",
    "start": "2375779",
    "end": "2380940"
  },
  {
    "text": "PC endpoints to keep the traffic within your virtual private cloud as well four additional layers of security and audit",
    "start": "2380940",
    "end": "2388680"
  },
  {
    "text": "and compliance is important for you know our most stringent customers including",
    "start": "2388680",
    "end": "2393809"
  },
  {
    "text": "finra including cisco where they need the ability to see exactly what has occurred in the database and their data",
    "start": "2393809",
    "end": "2398910"
  },
  {
    "text": "lake at any time so clout trail now is enabled by default in AWS accounts and",
    "start": "2398910",
    "end": "2404849"
  },
  {
    "text": "you should you should check that regularly audit that make sure things and we have things like guard duty that",
    "start": "2404849",
    "end": "2410490"
  },
  {
    "text": "detect anomalies there take advantage of the lifecycle management policies so you",
    "start": "2410490",
    "end": "2415950"
  },
  {
    "text": "may decide we want to keep the data for seven years but after that we actually",
    "start": "2415950",
    "end": "2421470"
  },
  {
    "text": "don't want to keep it because it's you know we're not obliged to and we don't want to be responsible for it so you can",
    "start": "2421470",
    "end": "2426809"
  },
  {
    "text": "even use life cycle policies to expire the data or move it to glacier just to keep it long-term right a glacier our",
    "start": "2426809",
    "end": "2433680"
  },
  {
    "text": "archive storage and last but not least of course the compliance we have the",
    "start": "2433680",
    "end": "2439769"
  },
  {
    "text": "most security and compliance certifications where you can inherit some of ours at the in that shared",
    "start": "2439769",
    "end": "2445410"
  },
  {
    "text": "responsibility model and you can build secure data lakes on top of AWS now",
    "start": "2445410",
    "end": "2451920"
  },
  {
    "start": "2451000",
    "end": "2451000"
  },
  {
    "text": "again i hope you guys get it see now there have a data lake and people are like growing on the data lake so they're they're using it now the point is you",
    "start": "2451920",
    "end": "2459240"
  },
  {
    "text": "built your data lake now and you've collected your data and you've identified some use cases work",
    "start": "2459240",
    "end": "2464310"
  },
  {
    "text": "backwards from that the few things you want to keep in mind is you should as much as possible query that data in",
    "start": "2464310",
    "end": "2470310"
  },
  {
    "text": "place use services like Hadoop on EMR and spark and flink that allow you to",
    "start": "2470310",
    "end": "2475650"
  },
  {
    "text": "run high spark jobs and sparks equal and hive directly on that data that will reduce the amount of data moved from",
    "start": "2475650",
    "end": "2483660"
  },
  {
    "text": "your s3 buckets and that sort of thing that will help you scale achieve a higher scale use things like sage maker",
    "start": "2483660",
    "end": "2490050"
  },
  {
    "text": "or the deep learning AMI when you're running training jobs that can read and write the data from s3 store your model",
    "start": "2490050",
    "end": "2496170"
  },
  {
    "text": "artifact in s3 that will also produce some additional efficiencies now this is",
    "start": "2496170",
    "end": "2502620"
  },
  {
    "text": "kind of on the flip side right when you have your data stored in s3 you may need",
    "start": "2502620",
    "end": "2509580"
  },
  {
    "text": "to provide additional access mechanisms like like I Cisco they needed to provide API access to you know key value stores",
    "start": "2509580",
    "end": "2517310"
  },
  {
    "text": "they needed search api's and so from s3 you can easily load your data from s3 to",
    "start": "2517310",
    "end": "2525150"
  },
  {
    "text": "a dynamodb table using say for instance hive which we have a dynamo DB connector",
    "start": "2525150",
    "end": "2530790"
  },
  {
    "text": "for or move and moving it to elastic search for search api is full text search so it's it's really good idea to",
    "start": "2530790",
    "end": "2538560"
  },
  {
    "text": "get comfortable with the the idea that you know it's it's like pickup trucks or cars vehicles right you have pickup",
    "start": "2538560",
    "end": "2544680"
  },
  {
    "text": "trucks you have 18 wheelers you have Corvettes and you know sports supercars",
    "start": "2544680",
    "end": "2549900"
  },
  {
    "text": "not all of them are going to be used to tow you know all the food that Cisco brings to reinvent or the restaurants",
    "start": "2549900",
    "end": "2555450"
  },
  {
    "text": "near your area right so there are purpose-built engines and frameworks for the job and it's really good to get",
    "start": "2555450",
    "end": "2561570"
  },
  {
    "text": "comfortable with kind of manipulating your data and loading it into a purpose-built engine for your queries",
    "start": "2561570",
    "end": "2566850"
  },
  {
    "text": "and your applications now ml has been a big focus and",
    "start": "2566850",
    "end": "2572100"
  },
  {
    "text": "hopefully you guys you enjoyed the keynote and some of the announcements we made there but the MLS is a key",
    "start": "2572100",
    "end": "2577560"
  },
  {
    "text": "component and you know I personally believe when we look at this and you know 50 years in the history books this",
    "start": "2577560",
    "end": "2583590"
  },
  {
    "text": "right now will be kind of a renaissance for machine learning and I'll get off my soapbox here but the data like makes a",
    "start": "2583590",
    "end": "2590100"
  },
  {
    "text": "great place to store and collect your training data right and we we talk about supervised machine learning an unsupervised machine learning you",
    "start": "2590100",
    "end": "2596520"
  },
  {
    "text": "need high quality data that's labeled and you need to collect a lot of it to train an accurate model all right so the",
    "start": "2596520",
    "end": "2603150"
  },
  {
    "text": "data Lake makes a great place to store that data so you can do more advanced analytics on top of it and progress to",
    "start": "2603150",
    "end": "2608760"
  },
  {
    "text": "the right like you saw would cisco progressed to a machine learning prediction predictive analytics and",
    "start": "2608760",
    "end": "2615180"
  },
  {
    "text": "statistics on your on your data lake now",
    "start": "2615180",
    "end": "2620370"
  },
  {
    "text": "I'll leave you with some best practices both on storage and compute for the data Lake and we'll talk about those",
    "start": "2620370",
    "end": "2626580"
  },
  {
    "text": "separately all right so when you're talking about",
    "start": "2626580",
    "end": "2634830"
  },
  {
    "start": "2628000",
    "end": "2628000"
  },
  {
    "text": "querying your data an s3 a key component of that is producing efficiency by reducing the amount of data that's read",
    "start": "2634830",
    "end": "2642510"
  },
  {
    "text": "and read from s3 and so we can do that in three ways the concept of a",
    "start": "2642510",
    "end": "2649230"
  },
  {
    "text": "partitioned column is essentially that it adds a column from your data and it projects that into the object name of",
    "start": "2649230",
    "end": "2657090"
  },
  {
    "text": "your of your file so s3 is a key value object store right and the key includes",
    "start": "2657090",
    "end": "2662270"
  },
  {
    "text": "the column and its value what that does is when you run a query and you produce",
    "start": "2662270",
    "end": "2668040"
  },
  {
    "text": "a you know you add a where clause or predicate to to filter that partition",
    "start": "2668040",
    "end": "2674640"
  },
  {
    "text": "column will reduce the amount of data that's read from s3 so that will reduce the amount of data scanned and make your",
    "start": "2674640",
    "end": "2680490"
  },
  {
    "text": "queries faster and indirectly it'll make it more more cost-effective because you",
    "start": "2680490",
    "end": "2686460"
  },
  {
    "text": "can run your clusters faster the athina queries which chart which are cost by",
    "start": "2686460",
    "end": "2691680"
  },
  {
    "text": "per terabyte scanned will become cheaper so this is a key best practice that you should apply right away compression is",
    "start": "2691680",
    "end": "2699390"
  },
  {
    "text": "another one where again the Presto frameworks and SPARC support compress data gzip snappy B zip lzo that you can",
    "start": "2699390",
    "end": "2708000"
  },
  {
    "text": "use to compress the data that's stored in s3 in that native format so you can take advantage of that that will again",
    "start": "2708000",
    "end": "2713820"
  },
  {
    "text": "reduce the data scanned and it will also reduce the storage consumption on s3",
    "start": "2713820",
    "end": "2719580"
  },
  {
    "text": "right if you get a five to one compression ratio you're using 5x less storage on s3 and you're saving money",
    "start": "2719580",
    "end": "2725790"
  },
  {
    "text": "that way so that's that's a shoo-in and the last thing is using a columnar file formats",
    "start": "2725790",
    "end": "2732720"
  },
  {
    "text": "so columnar file formats essentially what it is is that when you have a data",
    "start": "2732720",
    "end": "2739569"
  },
  {
    "text": "link typically you move to a more denormalized table structure a physical data model that's more denormalized I",
    "start": "2739569",
    "end": "2745029"
  },
  {
    "text": "mean you're meaning you're gonna do more of your joins in advance before you store that in s3 to simplify your",
    "start": "2745029",
    "end": "2750609"
  },
  {
    "text": "queries right and that eventually produces a very wide table with maybe",
    "start": "2750609",
    "end": "2757150"
  },
  {
    "text": "hundreds of columns you don't want to retrieve all hunt you know 200 columns",
    "start": "2757150",
    "end": "2762760"
  },
  {
    "text": "that you have in there when you're only querying just for two columns right so the columnar file format is stored in",
    "start": "2762760",
    "end": "2768400"
  },
  {
    "text": "columns instead of rows and when you again reduce the columns that you're querying and say a sequel query or PI",
    "start": "2768400",
    "end": "2774369"
  },
  {
    "text": "spark job it reduces the data scanned again because it's just retrieving those columns not every single not every",
    "start": "2774369",
    "end": "2781690"
  },
  {
    "text": "single column row by row so again another efficiency so when you can combine all three of these in this",
    "start": "2781690",
    "end": "2787720"
  },
  {
    "text": "example here where we have some of our product reviews from amazon.com where we",
    "start": "2787720",
    "end": "2792819"
  },
  {
    "text": "store we're storing it with a product core product category partitioned column we're using park' which is a patchy",
    "start": "2792819",
    "end": "2799930"
  },
  {
    "text": "park' which is a columnar file format that you can store on s3 and we're using snappy compression for that optimization",
    "start": "2799930",
    "end": "2807220"
  },
  {
    "text": "there two other storage best practices you can use the lifecycle rules to",
    "start": "2807220",
    "end": "2812410"
  },
  {
    "text": "automatically tear the data especially down to the infrequently access storage",
    "start": "2812410",
    "end": "2817480"
  },
  {
    "text": "class of s3 in most cases the data leak is not you're not going to be retrieving all of that data all the time right so",
    "start": "2817480",
    "end": "2823990"
  },
  {
    "text": "the infrequently accessed Sorge class can present you some optimization in",
    "start": "2823990",
    "end": "2829299"
  },
  {
    "text": "both your in your cost and s3 and you can use the s3 storage class analysis feature to recommend candidates of data",
    "start": "2829299",
    "end": "2838119"
  },
  {
    "text": "that can be moved to infrequently accessed and we now of course have the intelligent tearing in s3 as well that",
    "start": "2838119",
    "end": "2846460"
  },
  {
    "text": "we know that we announced s3 select and glacier select I get this question a lot you know basically what is it they they",
    "start": "2846460",
    "end": "2853660"
  },
  {
    "text": "allow you to retrieve subsets of your data from an individual s3 or glacier object you",
    "start": "2853660",
    "end": "2859770"
  },
  {
    "text": "a sequel syntax what that means in the context of a data like those that you you actually want to push down as three",
    "start": "2859770",
    "end": "2866550"
  },
  {
    "text": "select down to push down your predicates down to the s3 select layer and we",
    "start": "2866550",
    "end": "2871890"
  },
  {
    "text": "recently added this feature into spark and hive and press though on EMR that",
    "start": "2871890",
    "end": "2877410"
  },
  {
    "text": "it'll actually use s3 select to push down some of your predicates down to s3 select to improve the performance again",
    "start": "2877410",
    "end": "2884820"
  },
  {
    "text": "and reduce the data scan so take advantage of these and you could continue to see us expand the",
    "start": "2884820",
    "end": "2890340"
  },
  {
    "text": "capabilities that we that we'll add for us three select and glacier select",
    "start": "2890340",
    "end": "2895760"
  },
  {
    "start": "2895000",
    "end": "2895000"
  },
  {
    "text": "lastly just want to recap on compute best practices because you've separated",
    "start": "2897290",
    "end": "2903420"
  },
  {
    "text": "to storage from compute you can use temporary compute resources the data is",
    "start": "2903420",
    "end": "2909030"
  },
  {
    "text": "persisted on s3 so you're not worried about that so you can build up a cluster and teardown when you're done if you're",
    "start": "2909030",
    "end": "2914970"
  },
  {
    "text": "doing an ad hoc query or if you're doing a pie spark job to produce some kind of output or running ml training jobs and",
    "start": "2914970",
    "end": "2922230"
  },
  {
    "text": "you want to spin up a cluster train the model teardown when you're done so the ability to use temporary compute",
    "start": "2922230",
    "end": "2928350"
  },
  {
    "text": "resources gives you that true elasticity of AWS that we provide and it also",
    "start": "2928350",
    "end": "2934230"
  },
  {
    "text": "presents the opportunity to use Amazon ec2 spot where you can use our unused",
    "start": "2934230",
    "end": "2939540"
  },
  {
    "text": "capacity up to 80 to 90% cost savings versus the on-demand ec2 rate so you can",
    "start": "2939540",
    "end": "2947400"
  },
  {
    "text": "use ec2 spot with these ephemeral temporary clusters to save a lot of money quite frankly and we and you know",
    "start": "2947400",
    "end": "2955200"
  },
  {
    "text": "we have a lot of ec2 spot capacity let me just say that so you can you can take advantage of that you can mix-and-match",
    "start": "2955200",
    "end": "2960230"
  },
  {
    "text": "ec2 instance types and ec2 families to get the best ec2 spot rate possible and",
    "start": "2960230",
    "end": "2969120"
  },
  {
    "text": "with that you orchestrate the compute just like your job right you you don't use say like a cron job you actually",
    "start": "2969120",
    "end": "2974820"
  },
  {
    "text": "want to use the scheduler data a workflow engine like data pipelines step functions some folks use open source",
    "start": "2974820",
    "end": "2981300"
  },
  {
    "text": "like air flow Apache airflow from air B&B or azkaban to orchestrate the job",
    "start": "2981300",
    "end": "2987330"
  },
  {
    "text": "right based on a schedule based on some kind of threshold or based on some kind of other predetermined",
    "start": "2987330",
    "end": "2993920"
  },
  {
    "text": "requirements to trigger a job right you want that to be automated and you orchestrate the cluster itself with the",
    "start": "2993920",
    "end": "3001250"
  },
  {
    "text": "job right so you're going to use the AWS SDKs to provision an EMR customer or to",
    "start": "3001250",
    "end": "3007400"
  },
  {
    "text": "run an Athena query or stage maker training job for example so you orchestrate the compute with the job",
    "start": "3007400",
    "end": "3012890"
  },
  {
    "text": "itself and Ike I mentioned when you use your data like think about how you can really provide access to the data for",
    "start": "3012890",
    "end": "3020960"
  },
  {
    "text": "all your customers right maybe Athena or EMRs and the the perfect fit for the type of queries they want to",
    "start": "3020960",
    "end": "3026300"
  },
  {
    "text": "run so you can load some of that data that they need into dynamodb elasticsearch service Neptune for graphs",
    "start": "3026300",
    "end": "3032860"
  },
  {
    "text": "or even the time series database that we recently announced so getting comfortable with that capability of you",
    "start": "3032860",
    "end": "3039980"
  },
  {
    "text": "know using the just the right engine you know you have you have access to database engines that you probably wouldn't have had access to before and",
    "start": "3039980",
    "end": "3046430"
  },
  {
    "text": "so it opens up a lot of opportunities to really experiment start small and fail",
    "start": "3046430",
    "end": "3051920"
  },
  {
    "text": "fast this morning we announced the lake",
    "start": "3051920",
    "end": "3057290"
  },
  {
    "text": "data lake formation in preview and we've had the data lake on AWS solutions on",
    "start": "3057290",
    "end": "3064100"
  },
  {
    "text": "our on our page for a while now and we've added things like Active Directory features so you could take a look at",
    "start": "3064100",
    "end": "3069650"
  },
  {
    "text": "this you can take a look at our new data Lake formation as well and this is one",
    "start": "3069650",
    "end": "3075140"
  },
  {
    "text": "will continue to expand so this is the same one that Cisco started with and extended all right so you can you can",
    "start": "3075140",
    "end": "3080930"
  },
  {
    "text": "start with this and expand it to your business needs and this is up by the way",
    "start": "3080930",
    "end": "3086960"
  },
  {
    "text": "this is one-click confirmation deployed the whole architecture at the api's lambda functions s3 buckets etc so want",
    "start": "3086960",
    "end": "3098540"
  },
  {
    "text": "to just recap and thank you all for your time think about again the decoupling of storage compute when you're when you're",
    "start": "3098540",
    "end": "3103910"
  },
  {
    "text": "building de Lake if you're not doing that you're really missing out on one of the key components think about how it",
    "start": "3103910",
    "end": "3110540"
  },
  {
    "text": "can grow your analytics capabilities right you can move away just from scheduled formatted reports allow people",
    "start": "3110540",
    "end": "3118100"
  },
  {
    "text": "to run ad hoc queries to find exactly the data you need and you can expand your analytics to more users than ever",
    "start": "3118100",
    "end": "3124940"
  },
  {
    "text": "before right previously you might have been constrained on your resources who you allow to query",
    "start": "3124940",
    "end": "3131090"
  },
  {
    "text": "your data now you can expand that virtually infinitely centralized and secure using the data Lakes so you can",
    "start": "3131090",
    "end": "3136970"
  },
  {
    "text": "you can use s3 as that main storage and allows you to use consistent security bucket policies over your data use the",
    "start": "3136970",
    "end": "3143690"
  },
  {
    "text": "cost optimization features like spot and temporary clusters use the open data",
    "start": "3143690",
    "end": "3149630"
  },
  {
    "text": "formats and the best practices that we talked about like parquet and partition",
    "start": "3149630",
    "end": "3155180"
  },
  {
    "text": "columns and compression and give yourself flexibility that's really important to give yourself that agility",
    "start": "3155180",
    "end": "3161360"
  },
  {
    "text": "that nimbleness is that's a word where you can use multiple tools and experiment very quickly so you can try",
    "start": "3161360",
    "end": "3167840"
  },
  {
    "text": "any things and find out what's going to work best for you and so that thank you very much",
    "start": "3167840",
    "end": "3173960"
  },
  {
    "text": "we really hope to get your feedback on the session so please fill out the feedback in this mobile app and we'll be",
    "start": "3173960",
    "end": "3180710"
  },
  {
    "text": "able to take some questions thank you [Applause]",
    "start": "3180710",
    "end": "3191729"
  },
  {
    "text": "so the question was what is the best practice for collecting and presenting the latest and greatest data with the",
    "start": "3219150",
    "end": "3226510"
  },
  {
    "text": "date as the partition column is that right yep",
    "start": "3226510",
    "end": "3232710"
  },
  {
    "text": "you want me to take it yeah all right so so so we need traditionally look at it right something like an order is just a",
    "start": "3260480",
    "end": "3267360"
  },
  {
    "text": "transaction right so when we think about data leaks we think about going at two speeds so we have our data like form or",
    "start": "3267360",
    "end": "3274830"
  },
  {
    "text": "more of a loosely coupled analysis transactions that you're talking about we have a whole warehouse for it so we",
    "start": "3274830",
    "end": "3280500"
  },
  {
    "text": "put it in the staging area clamp it up all the way to the warehouse to the data Mart area and then use that to feed back",
    "start": "3280500",
    "end": "3287820"
  },
  {
    "text": "into the lake so we are always pulling the latest and greatest from a transaction perspective in this case the",
    "start": "3287820",
    "end": "3293190"
  },
  {
    "text": "value of the data lake for us is taking the transaction combining with the observations and interactions which are",
    "start": "3293190",
    "end": "3299130"
  },
  {
    "text": "not as structured and not part of our redshift infrastructure to give us the value that we need so you're not trying",
    "start": "3299130",
    "end": "3304440"
  },
  {
    "text": "to solve all design patterns in the lake itself it's the opportunity where we're trying to use the right tools for the",
    "start": "3304440",
    "end": "3309750"
  },
  {
    "text": "right job yes yeah yes and that that",
    "start": "3309750",
    "end": "3321090"
  },
  {
    "text": "could be your design pattern and off side we can discuss more in detail we're talking about one implementation where",
    "start": "3321090",
    "end": "3326430"
  },
  {
    "text": "transaction is one way and observation and interactions are other all right",
    "start": "3326430",
    "end": "3332750"
  },
  {
    "text": "yes yes",
    "start": "3344720",
    "end": "3348230"
  },
  {
    "text": "yeah you want me today yeah I'll take all right yeah you wanna take it good no",
    "start": "3357400",
    "end": "3363170"
  },
  {
    "text": "I can talk about not implementation and late can't talk about the best practice which one would be like you want to talk",
    "start": "3363170",
    "end": "3368180"
  },
  {
    "text": "to the best practice and yeah I can mention best practices so glue as you said provides a managed ETL environment",
    "start": "3368180",
    "end": "3375050"
  },
  {
    "text": "with spark both PI spark and Scala now and so the question for everyone was you",
    "start": "3375050",
    "end": "3380060"
  },
  {
    "text": "know when do you kind of decide between using EMR as a self kind of self-managed spark environment versus glue and I",
    "start": "3380060",
    "end": "3387260"
  },
  {
    "text": "would say it's really not kind of mutually exclusive I think you may find that some jobs kind of lend themselves",
    "start": "3387260",
    "end": "3393110"
  },
  {
    "text": "to glue very easily where you may have more complex jobs that are longer running right things that take a lot",
    "start": "3393110",
    "end": "3398540"
  },
  {
    "text": "longer and more complex more data and give you more control over spark execution memory etc where you use it on",
    "start": "3398540",
    "end": "3405350"
  },
  {
    "text": "the EMR it's not really there's not a hard and fast rule but I would say it doesn't have to be all or nothing either",
    "start": "3405350",
    "end": "3410420"
  },
  {
    "text": "right and just to build on that right so when we went live we didn't have glue",
    "start": "3410420",
    "end": "3415580"
  },
  {
    "text": "glue was not GA at that time right so what we did was we did a classic 8020 Pareto we said 80% of our ETL loads are",
    "start": "3415580",
    "end": "3423530"
  },
  {
    "text": "okay the way they are 20% of them are we are having issues meeting the SFA's we",
    "start": "3423530",
    "end": "3429530"
  },
  {
    "text": "are not able to load them on time so we really rewrote them on the EMR stack that makes it distributed and run faster",
    "start": "3429530",
    "end": "3436370"
  },
  {
    "text": "that's how we solve it and right now we're actively looking at glue so we use glue for cataloging a lot but we don't",
    "start": "3436370",
    "end": "3442910"
  },
  {
    "text": "necessarily use blue for our ETL jobs",
    "start": "3442910",
    "end": "3447579"
  },
  {
    "text": "yeah sure so the question was we had a",
    "start": "3452820",
    "end": "3458200"
  },
  {
    "text": "custom integration tool called gravity and and and we wanted to give you more",
    "start": "3458200",
    "end": "3464170"
  },
  {
    "text": "details on that right so when you really look at the data like reference architecture it does few things very",
    "start": "3464170",
    "end": "3469930"
  },
  {
    "text": "well right it gives you an endpoint it hooks up into AWS services where you can create packages and do a whole lot of",
    "start": "3469930",
    "end": "3476080"
  },
  {
    "text": "other things but from an ingestion perspective ingestion is very specific to how you implement write ingestion is",
    "start": "3476080",
    "end": "3483070"
  },
  {
    "text": "very specific to how you partition the data how you make it compressed what are the D normally",
    "start": "3483070",
    "end": "3488140"
  },
  {
    "text": "to build that will give you a perspective on your transactions so all the best practices around compression",
    "start": "3488140",
    "end": "3494830"
  },
  {
    "text": "around making sure that the the partition is selected the right way the",
    "start": "3494830",
    "end": "3502090"
  },
  {
    "text": "right joint conditions are updated is happening in that custom module and we have a demo tomorrow where we will be",
    "start": "3502090",
    "end": "3508180"
  },
  {
    "text": "opening it up and in showing you how gravity works indicate",
    "start": "3508180",
    "end": "3513420"
  },
  {
    "text": "got it",
    "start": "3521780",
    "end": "3524440"
  },
  {
    "text": "so so the question is is that you really maybe when we pull the data out of a",
    "start": "3527420",
    "end": "3532770"
  },
  {
    "text": "relational tool we don't want to use the tax the relation system did we use CDC the answer is no so we did not use a CDC",
    "start": "3532770",
    "end": "3539250"
  },
  {
    "text": "tool the input on our lake from a relational perspective is back feed from our egg-shaped environment and we load",
    "start": "3539250",
    "end": "3545970"
  },
  {
    "text": "that in batch",
    "start": "3545970",
    "end": "3548599"
  },
  {
    "text": "yep so the question is how do what is the best practice our recommendation for partitioning your different customers",
    "start": "3565400",
    "end": "3572040"
  },
  {
    "text": "data I think it will depend you know based on their bucket policies right and",
    "start": "3572040",
    "end": "3577440"
  },
  {
    "text": "and what are the querying patterns you may go as high as putting that data in different s3 buckets or you may include",
    "start": "3577440",
    "end": "3583619"
  },
  {
    "text": "it in the s3 partition partition column the thing to remember though if you put it in a partition column is the key",
    "start": "3583619",
    "end": "3589770"
  },
  {
    "text": "value of an object is not encrypted right so if you're putting any sensitive information there it should be some kind",
    "start": "3589770",
    "end": "3596460"
  },
  {
    "text": "of hash or something like that where you then query based on that but if you if",
    "start": "3596460",
    "end": "3601980"
  },
  {
    "text": "you want to do that you can use the partition column in your in the s3 key and you can control you know a bucket",
    "start": "3601980",
    "end": "3607859"
  },
  {
    "text": "policy through the prefix right of the of the key so you can say you know this customer has access to s3 bucket slash",
    "start": "3607859",
    "end": "3614850"
  },
  {
    "text": "customer equals etc and they'll have access to that prefect through through the bucket policy or I am policy yes yep",
    "start": "3614850",
    "end": "3631170"
  },
  {
    "text": "so the question was should you do you sometimes have to partition your data multiple times or keep some copies of the data in some some instances yes",
    "start": "3631170",
    "end": "3638550"
  },
  {
    "text": "depending on how diverse those query requirements are and I'll add one thing to it right space is cheap confusion is not cheap",
    "start": "3638550",
    "end": "3647660"
  },
  {
    "text": "especially when you have big query different query patterns right that's like that's the key thing before we even",
    "start": "3648800",
    "end": "3654150"
  },
  {
    "text": "go into partition columns you may use something like a date time or something to partition the data initially but once",
    "start": "3654150",
    "end": "3659880"
  },
  {
    "text": "you start to understand and look at the query history what is everyone filtering on and where are the joins where the where Clause is",
    "start": "3659880",
    "end": "3666000"
  },
  {
    "text": "on what column then you'll start to get a better intuition okay maybe we should partition the data on this other column",
    "start": "3666000",
    "end": "3672030"
  },
  {
    "text": "or do some nested you can nest partitioned columns as well so you may end up doing that as well and I think",
    "start": "3672030",
    "end": "3678869"
  },
  {
    "text": "we're out of time I think we're gonna have to come downstage if you guys want to stick around we'll come down here thank you very much good evening thank",
    "start": "3678869",
    "end": "3683880"
  },
  {
    "text": "you thank you",
    "start": "3683880",
    "end": "3686660"
  }
]