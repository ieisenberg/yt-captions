[
  {
    "text": "so thank you again my name is that I'm in Nepal seeing I lead microbiome their division at ego genomics and my",
    "start": "30",
    "end": "7020"
  },
  {
    "text": "colleague Nick James is joining me here today and he's going to tell you all the exciting stuff and I'm gonna tell you more the intro so bear with me for a few",
    "start": "7020",
    "end": "13799"
  },
  {
    "text": "minutes while I give you some introduction set the scene the sort challenges we're working on how working",
    "start": "13799",
    "end": "20430"
  },
  {
    "text": "with Unilever et cetera and there Nick will get into the details and there's a lot of detail he'll cover so bear with",
    "start": "20430",
    "end": "26400"
  },
  {
    "text": "us as we get them thank you so as I said",
    "start": "26400",
    "end": "31769"
  },
  {
    "text": "we're gonna cover the intro there's always the case of who we are being around for nine ten years city Hingston",
    "start": "31769",
    "end": "37829"
  },
  {
    "text": "UK that's the welcome genome campus the big campus in the UK for genomics etc be",
    "start": "37829",
    "end": "44640"
  },
  {
    "text": "from day one we've been an AWS partner we build on AWS etc and we've really",
    "start": "44640",
    "end": "50180"
  },
  {
    "text": "optimized around AWS and other clouds other cloud type providers and as you",
    "start": "50180",
    "end": "57960"
  },
  {
    "text": "expand past human genomics and you get into things like microbiome II semesters you know it makes you get to life scientists lot going on there and in",
    "start": "57960",
    "end": "63870"
  },
  {
    "text": "fact today were a lots of what we're gonna talk about in the workflow and the workflow manager are very applicable in",
    "start": "63870",
    "end": "69330"
  },
  {
    "text": "those messaging omics microbiome X words there was a challenge the challenge is",
    "start": "69330",
    "end": "75890"
  },
  {
    "text": "is a very real challenge scientists have access to a lot of data and this data is",
    "start": "75890",
    "end": "81500"
  },
  {
    "text": "it's just so much of it prevalent internal external cloud were terrific",
    "start": "81500",
    "end": "87600"
  },
  {
    "text": "such a reuse you can't name it it's just too much data available problem is scientists can't use the data and",
    "start": "87600",
    "end": "93350"
  },
  {
    "text": "there's a need to be able to bring their data into an ingestible form understand the value of data and all that good",
    "start": "93350",
    "end": "99390"
  },
  {
    "text": "stuff and curate the data research and that's that these are real upstream challenges before you even get to things like analysis and big you know AI and",
    "start": "99390",
    "end": "106920"
  },
  {
    "text": "things like that to do that you've got a lot of variability and all these different datasets it's it's very complicated",
    "start": "106920",
    "end": "112500"
  },
  {
    "text": "what's going on and it leads to some modeling challenges and uncertainty and risks in your models when you do your",
    "start": "112500",
    "end": "118110"
  },
  {
    "text": "analysis these problems are needs to be resolved scientist needs to be our self service to this you know these upstream",
    "start": "118110",
    "end": "125579"
  },
  {
    "text": "problems without the needs for massive amounts of bio fermentations etc in the system there are certain characteristics",
    "start": "125579",
    "end": "132300"
  },
  {
    "text": "of attributes at the bottom list here you need a trustworthy and validated evidence questions you need to",
    "start": "132300",
    "end": "140670"
  },
  {
    "text": "understand the questions being asked we contextualized the questions being asked for I'm doing a study on topic a it's very different to doing a study on topic",
    "start": "140670",
    "end": "147990"
  },
  {
    "text": "B and they may involve similar types of datasets for example an emu oncology to looking at certain types of skin",
    "start": "147990",
    "end": "154800"
  },
  {
    "text": "microbiome they may involve reuse of certain types of data but the nature of",
    "start": "154800",
    "end": "160140"
  },
  {
    "text": "that used to get context that used is very different so we need to work that and then data becomes an asset and for",
    "start": "160140",
    "end": "165360"
  },
  {
    "text": "all those folks out there who worry about buying dates or generating data that licensing data etc there's a loss",
    "start": "165360",
    "end": "170460"
  },
  {
    "text": "of objects being rolled out there and you had to need to manage that and then the last point here is is that these are complex questions being asked in studies",
    "start": "170460",
    "end": "177480"
  },
  {
    "text": "so scientists are faced with having to look at combination of datasets not just single data sets so roll this up into a",
    "start": "177480",
    "end": "184740"
  },
  {
    "text": "world of what this means in terms of trends we've all seen this we all know this we've gone from seeing our desktop",
    "start": "184740",
    "end": "190260"
  },
  {
    "text": "you know in the sequencing world with a handful of sequences medical secrecy 100k all the way to billions the",
    "start": "190260",
    "end": "196830"
  },
  {
    "text": "sequences and there are billions of sequences being rolled out especially when again if you're the microbiome x-metal genomics it's far more complex",
    "start": "196830",
    "end": "203160"
  },
  {
    "text": "than human genomics clinical genomics it's very hard to industrialize and standards that standardize that world",
    "start": "203160",
    "end": "208470"
  },
  {
    "text": "and there was just so much Pat horsepower you need new to be I do at speed and you new thinking you need things like AWS and you need things like",
    "start": "208470",
    "end": "215820"
  },
  {
    "text": "on-demand and so you know eagle has has been working on this for ten years ago and so you'll see here these terms here",
    "start": "215820",
    "end": "222000"
  },
  {
    "text": "eke a slogging any hive you know there's some technologies and Nick will talk about that in a minute so split this",
    "start": "222000",
    "end": "227190"
  },
  {
    "text": "into two sides of the coin you've got the side which says how do I make the seamless how to make the",
    "start": "227190",
    "end": "232920"
  },
  {
    "text": "scalable how to put it on cloud and all that good stuff that's the left hand side you're seeing here that's where we",
    "start": "232920",
    "end": "238710"
  },
  {
    "text": "should be absolutely need to be and if we're not there already if it's not industrialized",
    "start": "238710",
    "end": "243870"
  },
  {
    "text": "for the scientists to directly use point-and-click then we're probably behind the time so the thing is is that",
    "start": "243870",
    "end": "249960"
  },
  {
    "text": "with the advent of some of these AI techniques and with the statistical",
    "start": "249960",
    "end": "255810"
  },
  {
    "text": "power we have we can now move to a world where it's questions based where it's actually far more evolved where",
    "start": "255810",
    "end": "261600"
  },
  {
    "text": "scientists can actually pose present the questions they're asking the data can be self-service to them and",
    "start": "261600",
    "end": "268500"
  },
  {
    "text": "this this changes paradigm change in how data is access used and then analyzed",
    "start": "268500",
    "end": "274199"
  },
  {
    "text": "etc is a inflection point for the way the R&D is being done in industry today",
    "start": "274199",
    "end": "279930"
  },
  {
    "text": "so you watch that space please so what we're going to do today is we're going to talk about technology which is embedded a workflow managers technology",
    "start": "279930",
    "end": "286500"
  },
  {
    "text": "which is embed in both these left and right sides and the importance and how",
    "start": "286500",
    "end": "291720"
  },
  {
    "text": "we've got to developing that technology super then another context if we're as",
    "start": "291720",
    "end": "297330"
  },
  {
    "text": "scientists today we'll sit down and go through some analysis it'll be a it'll be a fairly kludgy manual process",
    "start": "297330",
    "end": "304110"
  },
  {
    "text": "they'll meet constraints of time and cost but innovation will be suppressed because they won't be using the datasets",
    "start": "304110",
    "end": "309300"
  },
  {
    "text": "available to them we understand that that's a problem you can you can offer terabytes of data to a scientist they'll",
    "start": "309300",
    "end": "314970"
  },
  {
    "text": "only use the data they understand at hand in typically their data so if you",
    "start": "314970",
    "end": "320219"
  },
  {
    "text": "want to unkind this you have to provide a self-service approach and you know hence the right-hand side of the clock",
    "start": "320219",
    "end": "325560"
  },
  {
    "text": "this slide I mentioned earlier and you have to allow the system to use appropriate deep learning techniques use",
    "start": "325560",
    "end": "331860"
  },
  {
    "text": "appropriate con sport cloud analyses so you've got the objects working your the",
    "start": "331860",
    "end": "337169"
  },
  {
    "text": "AI working and you're able to provide in this case the biologist data at hand",
    "start": "337169",
    "end": "342810"
  },
  {
    "text": "which they need in a very seamless iterative fashion without armies of by informaticians needed in support you",
    "start": "342810",
    "end": "349590"
  },
  {
    "text": "build this all into an architecture now we're going to get to you know how Nix Nix deep dives going to you know find",
    "start": "349590",
    "end": "357630"
  },
  {
    "text": "its way in the slot into this you've got whole series of activities needed you need to think about catalogs you need",
    "start": "357630",
    "end": "362880"
  },
  {
    "text": "about curation you need to think about valuation discovery engines you need to think about data access layers and a",
    "start": "362880",
    "end": "369120"
  },
  {
    "text": "faster non-functional requirements pieces etc such a high level functional",
    "start": "369120",
    "end": "374250"
  },
  {
    "text": "architecture for you and you can see where the red box is painted this is where the the workflow manager kicks in",
    "start": "374250",
    "end": "380279"
  },
  {
    "text": "and actually we're using start and deploy the workflow manager across this flow so the importance of speed cost are",
    "start": "380279",
    "end": "387870"
  },
  {
    "text": "you know on demand accessory become extremely relative to to relevant across",
    "start": "387870",
    "end": "393690"
  },
  {
    "text": "you know a functional architecture not just in certain parts of it a cool touch upon that just mentioning",
    "start": "393690",
    "end": "399930"
  },
  {
    "text": "Unilever's the context of the case study so as you can imagine these last CBC PG",
    "start": "399930",
    "end": "407100"
  },
  {
    "text": "or FMCG companies have R&D they're doing a lot of microbiology a lot of",
    "start": "407100",
    "end": "412290"
  },
  {
    "text": "microbiome lots of you know where the 16s to metagenomics etc and as they go",
    "start": "412290",
    "end": "417510"
  },
  {
    "text": "through this they're doing a lot of study work at scale studies and they're trying to process this data with optics",
    "start": "417510",
    "end": "423750"
  },
  {
    "text": "in mind with scaled use in mind for scientists so here what you have here is a very simple view whereby",
    "start": "423750",
    "end": "430230"
  },
  {
    "text": "informaticians in the middle we need to get the by for musicians we need to industrialize that portion of the flow so that scientists can work with you",
    "start": "430230",
    "end": "437160"
  },
  {
    "text": "know the lead by four musicians that's necessary but really it's the scientists running the workflows and so to put this",
    "start": "437160",
    "end": "442710"
  },
  {
    "text": "in series of requirements we have the background the requirements so Unilever have certain needs from their",
    "start": "442710",
    "end": "447950"
  },
  {
    "text": "engagements that's a that's an industrial level and as you can see here",
    "start": "447950",
    "end": "453030"
  },
  {
    "text": "this requires new bio from biology to be applied new informatics programs etc and",
    "start": "453030",
    "end": "459810"
  },
  {
    "text": "access to public data you need this underlying architecture must be evolvable I mean these are things we",
    "start": "459810",
    "end": "466350"
  },
  {
    "text": "understand AWS you know developers understand these things but you're seeing the whole CPG FMCG biopharma",
    "start": "466350",
    "end": "472230"
  },
  {
    "text": "market's waking up to these requirements and then of course you need to be able to use the robust solutions that AWS",
    "start": "472230",
    "end": "478620"
  },
  {
    "text": "offer here in this case and you can see the outcome on the right hand side there's a speed in there's a scale at",
    "start": "478620",
    "end": "483900"
  },
  {
    "text": "what which scientists can now use the data use the workflows and this is what you want you want many scientists are",
    "start": "483900",
    "end": "490110"
  },
  {
    "text": "actually tapping directly running you want things running much faster you want 20 times speed you know a minimum and",
    "start": "490110",
    "end": "495750"
  },
  {
    "text": "you want op x2 accordingly come down and Nick will talk about some of that and then of course you want to know that the",
    "start": "495750",
    "end": "501240"
  },
  {
    "text": "public cloud works in these environments yes confidence confidential data and these very confidential workflows and my",
    "start": "501240",
    "end": "508410"
  },
  {
    "text": "last slide is just to reflect that this does lead to really credible powerful",
    "start": "508410",
    "end": "513780"
  },
  {
    "text": "outcomes this is a nature scientific reports paper from this year and it shows why first-of-a-kind",
    "start": "513780",
    "end": "520229"
  },
  {
    "text": "all microbiome the product claim and these are very hard product claims to get very thorough",
    "start": "520229",
    "end": "525910"
  },
  {
    "text": "studies you know please do let us know we'll send you the paper it's not a problem you know contact software's and again you know P Kela here just",
    "start": "525910",
    "end": "531430"
  },
  {
    "text": "commenting on what it's done for the team there so now when I hand over to Nick and Nick will give you more details",
    "start": "531430",
    "end": "537910"
  },
  {
    "text": "about that workflow manager thanks Nick",
    "start": "537910",
    "end": "541800"
  },
  {
    "text": "so as remedies have been introducing I get to talk about how we run workflow so",
    "start": "544080",
    "end": "550540"
  },
  {
    "text": "it's equal genomics and how we've used the Amazon clouds to do that so before I",
    "start": "550540",
    "end": "557410"
  },
  {
    "text": "go into that technical side I just wanted to take a quick step back and think about the process that I remained",
    "start": "557410",
    "end": "564190"
  },
  {
    "text": "just been describing them for more so he mentioned as display informatician so there's a there's an energies of",
    "start": "564190",
    "end": "570130"
  },
  {
    "text": "scientists he's producing this this raw data and the handless to the mathematician so what's happening at",
    "start": "570130",
    "end": "576850"
  },
  {
    "text": "this point so they're basically running a series of steps",
    "start": "576850",
    "end": "583210"
  },
  {
    "text": "these are bioinformatics tools that each variety of automatics tools so basically",
    "start": "583210",
    "end": "588840"
  },
  {
    "text": "complex algorithms and a very wide range",
    "start": "588840",
    "end": "594310"
  },
  {
    "text": "of things that they're doing here and there each one requires a high level expertise we need to understand the",
    "start": "594310",
    "end": "600910"
  },
  {
    "text": "function for this tool is doing and and therefore we need to understand the",
    "start": "600910",
    "end": "606250"
  },
  {
    "text": "parameters that are going to get the best result we're using this tool and each one can be complicated to install",
    "start": "606250",
    "end": "612660"
  },
  {
    "text": "they can be produced by by a variety of",
    "start": "612660",
    "end": "617740"
  },
  {
    "text": "people who answer them may be varying levels of documentation and things like this so then that result here is that",
    "start": "617740",
    "end": "623950"
  },
  {
    "text": "this process is time consuming and challenging for for aspiring musician",
    "start": "623950",
    "end": "630120"
  },
  {
    "text": "and so we sort of painted just picture of a fairly ad hoc process using a lot",
    "start": "630120",
    "end": "635980"
  },
  {
    "text": "of manual expertise and time but actually if we break this process down",
    "start": "635980",
    "end": "642490"
  },
  {
    "text": "what's really happening is we are going from step one and step one flying to step two step two use for Lang to step",
    "start": "642490",
    "end": "649420"
  },
  {
    "text": "three and step four so we can define these steps and we can document and and really understand explicitly",
    "start": "649420",
    "end": "657740"
  },
  {
    "text": "what's happening and once we've done that then we can take another step when",
    "start": "657740",
    "end": "662750"
  },
  {
    "text": "we can represent this programmatically so we have a workflow configuration file of some sort of course when we've got",
    "start": "662750",
    "end": "669920"
  },
  {
    "text": "got this far then we've taken a very big step in towards enabling reproducibility",
    "start": "669920",
    "end": "675140"
  },
  {
    "text": "so we're not relying on this manual sequence of steps we can we can run this program at ik steps again and again so",
    "start": "675140",
    "end": "684740"
  },
  {
    "text": "how do we run these steps well we use a workflow engine and there's a very wide",
    "start": "684740",
    "end": "691040"
  },
  {
    "text": "variety of workflow engines out there and they'll have different advantages and disadvantages or strengths and",
    "start": "691040",
    "end": "696110"
  },
  {
    "text": "weaknesses so for us in this scenario then we have some a number of very key",
    "start": "696110",
    "end": "702310"
  },
  {
    "text": "requirements which are essential for us and the first of these is that the workflow engine is very scalable we're",
    "start": "702310",
    "end": "709760"
  },
  {
    "text": "working with very large datasets and we need to be able to scale up to hundreds of machines and cope with large data",
    "start": "709760",
    "end": "715970"
  },
  {
    "text": "files and we need something which is reproducible this is essential in scientific environment if we take the",
    "start": "715970",
    "end": "722000"
  },
  {
    "text": "same workflow the same data we end up with the same results we need something which is traceable we need to be able to",
    "start": "722000",
    "end": "728000"
  },
  {
    "text": "monitor the the workflow run and we need to be able to see what each job is doing",
    "start": "728000",
    "end": "733760"
  },
  {
    "text": "and this is useful for development as well of course and we use something which is robust because sometimes jobs",
    "start": "733760",
    "end": "739430"
  },
  {
    "text": "are going to fail they might just need rerunning we need to track of this process and not just fall over at the",
    "start": "739430",
    "end": "745550"
  },
  {
    "text": "first hurdle and we need something which is capable of handling complicated",
    "start": "745550",
    "end": "751970"
  },
  {
    "text": "workflow so for example if we have a sequence file and we measured a quality of that in one step then we might want",
    "start": "751970",
    "end": "759020"
  },
  {
    "text": "to fly to a different branch if that quality is not very high rather than flowing down the main branch and waste",
    "start": "759020",
    "end": "764990"
  },
  {
    "text": "compute resources so we need to handle conditional data flow and this sort of thing so that at Eagle we infer based on",
    "start": "764990",
    "end": "773000"
  },
  {
    "text": "our workflow approach on an open source software called a hive Andy hi first been developed",
    "start": "773000",
    "end": "778610"
  },
  {
    "text": "since 2004 at the European bioinformatics Institute or EBI EBI is a",
    "start": "778610",
    "end": "784730"
  },
  {
    "text": "world leading research in producing and hosting a very wide",
    "start": "784730",
    "end": "790600"
  },
  {
    "text": "variety of biology related databases which are illustrated in the diagram",
    "start": "790600",
    "end": "796750"
  },
  {
    "text": "here any hive is involved in in many of these processes in processing this data",
    "start": "796750",
    "end": "804450"
  },
  {
    "text": "so basically he hi was a very tried and tested workflow engine in agile I mix",
    "start": "804450",
    "end": "810910"
  },
  {
    "text": "large-scale environment so before I talk",
    "start": "810910",
    "end": "816310"
  },
  {
    "text": "about how we utilize the hive to to run our workflows in a production environment in the in the cloud then I",
    "start": "816310",
    "end": "822700"
  },
  {
    "text": "just need to explain some key concepts so that we can understand the process but we are needed to take so firstly",
    "start": "822700",
    "end": "831400"
  },
  {
    "text": "hive is a blackboard system and what this means is that when we run a",
    "start": "831400",
    "end": "837340"
  },
  {
    "text": "workflow this workflow templates is loaded into this blackboard database and this database contains now the structure",
    "start": "837340",
    "end": "845650"
  },
  {
    "text": "and workflow it understands D the flow from step to step and parameters that are needed and and secondly it has a",
    "start": "845650",
    "end": "853510"
  },
  {
    "text": "record of all the jobs the different steps so how does he have actually run",
    "start": "853510",
    "end": "859210"
  },
  {
    "text": "jobs if it's not the central blackboard process through which is which is",
    "start": "859210",
    "end": "864370"
  },
  {
    "text": "running them well they're processed by autonomous agents which we call workers and in this diagram here on the right",
    "start": "864370",
    "end": "872530"
  },
  {
    "text": "then we have a worker for analysis a this is a round worker which is matching the the diagram under blackboard on on",
    "start": "872530",
    "end": "880570"
  },
  {
    "text": "the left and we can see there's a one job ready for that analysis we also have an analysis B which is a a",
    "start": "880570",
    "end": "886470"
  },
  {
    "text": "diamond-shaped worker and there's no jobs for that analysis and what happens",
    "start": "886470",
    "end": "892420"
  },
  {
    "text": "is that the worker queries the blackboard to see if there's any jobs that match its analysis type and in this",
    "start": "892420",
    "end": "899170"
  },
  {
    "text": "case we have a job which allows us a worker can process so it claims that job",
    "start": "899170",
    "end": "905710"
  },
  {
    "text": "and starts running it but it also updates the status of of the blackboard and then as its processed that job and",
    "start": "905710",
    "end": "914470"
  },
  {
    "text": "it completes it updates the status to done and we create five more jobs in this example so we can",
    "start": "914470",
    "end": "921470"
  },
  {
    "text": "see job that job creation is a dynamic process any-any - so hopefully it's this clear",
    "start": "921470",
    "end": "929750"
  },
  {
    "text": "that this this is approach we've we don't have a load on the central system it's the workers which are claiming jobs",
    "start": "929750",
    "end": "935810"
  },
  {
    "text": "and creating new jobs so it's a very scalable design sort of inherent to the design of life so I mentioned a lot",
    "start": "935810",
    "end": "942290"
  },
  {
    "text": "about this dissy hive worker so what what is an e hive worker so firstly it",
    "start": "942290",
    "end": "948290"
  },
  {
    "text": "contains the bio formatic stalk this is obviously - but all we're interested in doing the interesting algorithm then",
    "start": "948290",
    "end": "955970"
  },
  {
    "text": "secondly we have something that we call a runnable and basically this is a fairly simple wrapper around the",
    "start": "955970",
    "end": "962810"
  },
  {
    "text": "bioinformatics tool which handles some very useful functions and firstly it",
    "start": "962810",
    "end": "967880"
  },
  {
    "text": "will handle the input parameters and so for example does the parameter have a",
    "start": "967880",
    "end": "973460"
  },
  {
    "text": "value when it should should when it's an essential parameter and it's not value in the correct format but we're",
    "start": "973460",
    "end": "978710"
  },
  {
    "text": "expecting and then it will handle the input fetching of the input data and get",
    "start": "978710",
    "end": "985220"
  },
  {
    "text": "that make that available for the bioinformatics tool to run from whatever location that it's been in and then of",
    "start": "985220",
    "end": "990320"
  },
  {
    "text": "course it will run the bath mat extraordin again this is can be a fairly complicated process and it's good C",
    "start": "990320",
    "end": "997040"
  },
  {
    "text": "managed up within a runnable so for example if if a file is of a different",
    "start": "997040",
    "end": "1002140"
  },
  {
    "text": "type then we might want to use a different set of parameters and we can make that decision as for the runnable is running the barf",
    "start": "1002140",
    "end": "1009340"
  },
  {
    "text": "Attucks tool and then we once the tool is completed its created our app that fast we write those two our output",
    "start": "1009340",
    "end": "1015160"
  },
  {
    "text": "location and within all of these processes we've described a knee high visits logging information so if",
    "start": "1015160",
    "end": "1021580"
  },
  {
    "text": "something goes wrong in the input fetching stage the file doesn't exist for something like this it's logged and",
    "start": "1021580",
    "end": "1026740"
  },
  {
    "text": "we can see what's happening so we've introduced that you have worker so how",
    "start": "1026740",
    "end": "1032949"
  },
  {
    "text": "does he have actually run workers so it uses a batch serjilla to do this so he",
    "start": "1032950",
    "end": "1040000"
  },
  {
    "text": "had monitors deed to blackboard and sees whether their jobs for to ready to be done in this case this five ready here",
    "start": "1040000",
    "end": "1045490"
  },
  {
    "text": "and then it checks the two batch edge of our sees the first work is already in the queue or whether those workers",
    "start": "1045490",
    "end": "1051400"
  },
  {
    "text": "running and it makes a decision but in this case we need to start more workers so it plays workers in the queue so",
    "start": "1051400",
    "end": "1058660"
  },
  {
    "text": "we're on to announce this B here which is a diamond shape one so we've we've requested some diamond shape workers in",
    "start": "1058660",
    "end": "1064510"
  },
  {
    "text": "the queue so it's a scheduler which finds the available resources it's what it's designed to do and it when when it",
    "start": "1064510",
    "end": "1072220"
  },
  {
    "text": "finds so I resource then it watches those workers so no but II hive is not",
    "start": "1072220",
    "end": "1078910"
  },
  {
    "text": "running each job one by one it doesn't it's not managing millions of jobs it's what we're doing is with requesting",
    "start": "1078910",
    "end": "1085570"
  },
  {
    "text": "workers to start and they will handle the fetching the job so it's quite a scalable design so when a worker starts",
    "start": "1085570",
    "end": "1093160"
  },
  {
    "text": "then it creates a black boards and they get the job so we've got three jobs running and C jobs ready now and then as",
    "start": "1093160",
    "end": "1101170"
  },
  {
    "text": "those jobs completed they're going to create more jobs so you're on to analysis to see now and what we find is",
    "start": "1101170",
    "end": "1107290"
  },
  {
    "text": "that the diamond shaped work is we had three of them so there's only two jobs left so two two workers have claimed two",
    "start": "1107290",
    "end": "1113590"
  },
  {
    "text": "more jobs and we've got one worker with no jobs less so that worker just exits and he - monitoring blackboard and C",
    "start": "1113590",
    "end": "1122560"
  },
  {
    "text": "says now 24 jobs ready in the third analysis analysis C and this is our rectangular-shaped analysis and so it",
    "start": "1122560",
    "end": "1130000"
  },
  {
    "text": "submits these requests into the queue and to the scheduler and and about",
    "start": "1130000",
    "end": "1135130"
  },
  {
    "text": "scheduler will then find resources available and run those workers and what",
    "start": "1135130",
    "end": "1140170"
  },
  {
    "text": "we can see here is those workers will claim jobs and we actually have jobs from multiple analysis running at the",
    "start": "1140170",
    "end": "1146650"
  },
  {
    "text": "same time so he has quite dynamic and perfectly happy to run different steps of the workflow all at the same time and",
    "start": "1146650",
    "end": "1154200"
  },
  {
    "text": "there's still lots of jobs there ready to be run so he hive will put more",
    "start": "1154200",
    "end": "1160180"
  },
  {
    "text": "workers into the key and as a veil of the resources available in a scheduler",
    "start": "1160180",
    "end": "1165940"
  },
  {
    "text": "then it will run the jobs so that was a very quick introduction to some key",
    "start": "1165940",
    "end": "1171130"
  },
  {
    "text": "concepts about he - so now I'd like to talk about how we've adapted this and",
    "start": "1171130",
    "end": "1177070"
  },
  {
    "text": "we've managed to run our pipelines in a diverse cloud and in fact how we brought",
    "start": "1177070",
    "end": "1182530"
  },
  {
    "text": "it back again so we started with this open-source software he hybrids developed at EBI and",
    "start": "1182530",
    "end": "1190450"
  },
  {
    "text": "in fact as it was developed in the EBI they had cluster management system",
    "start": "1190450",
    "end": "1197530"
  },
  {
    "text": "called LSF in mind which is what they have on site there so it's primarily designed to run on OSF and we want to",
    "start": "1197530",
    "end": "1204250"
  },
  {
    "text": "make this a part of our platform and to do this we need to be able to run in a",
    "start": "1204250",
    "end": "1209500"
  },
  {
    "text": "cloud environment this is this is our approach what we wanted to take so",
    "start": "1209500",
    "end": "1215770"
  },
  {
    "text": "secondly the original a hive its requires a lot of technical knowledge to run it's primarily a command-line tool",
    "start": "1215770",
    "end": "1224610"
  },
  {
    "text": "and we want to enable the the scientist he's not not a software programmer to be",
    "start": "1224610",
    "end": "1230620"
  },
  {
    "text": "able to run the pipeline without any knowledge of he - and lastly we want to",
    "start": "1230620",
    "end": "1235870"
  },
  {
    "text": "then integrate it with other components of the legal dynamics platform so going",
    "start": "1235870",
    "end": "1243190"
  },
  {
    "text": "back to 2008 Sai which is a very long time ago in in in the area of this world",
    "start": "1243190",
    "end": "1248950"
  },
  {
    "text": "then we adopted as approach of representing any hard worker as an ad",
    "start": "1248950",
    "end": "1254770"
  },
  {
    "text": "obvious instance so it's a fairly straightforward we have an ami which contains all the installation and we",
    "start": "1254770",
    "end": "1262570"
  },
  {
    "text": "have an ec2 instances which is running with the blackboard on and as the processing jobs",
    "start": "1262570",
    "end": "1267580"
  },
  {
    "text": "it starts the appropriate am i an instance based on the appropriate ami and that it's like now our worker and it",
    "start": "1267580",
    "end": "1274960"
  },
  {
    "text": "grabs the jobs and processes and as I've described and we fetched and right inputs from the Amazon s3 so I should",
    "start": "1274960",
    "end": "1283750"
  },
  {
    "text": "say that this was this very early days of Amazon Cloud and were sort of quite",
    "start": "1283750",
    "end": "1292960"
  },
  {
    "text": "early in right in running workflows in production in a cloud so is it was a",
    "start": "1292960",
    "end": "1298210"
  },
  {
    "text": "fact of approach but there were issues with the resource usage so we're having",
    "start": "1298210",
    "end": "1305260"
  },
  {
    "text": "to specify each each workflow step needs different resources and they're very very quite widely and so we have a bit",
    "start": "1305260",
    "end": "1314440"
  },
  {
    "text": "of a bottleneck where we we've been using one instance and then we need to switch to another and we need to wait for the instance to",
    "start": "1314440",
    "end": "1321120"
  },
  {
    "text": "terminate and start again and so we can we can improve that slightly by using",
    "start": "1321120",
    "end": "1326130"
  },
  {
    "text": "sort of an average instance type but again that's not ideal and also we have",
    "start": "1326130",
    "end": "1332820"
  },
  {
    "text": "a tendency to and write the whole workflow onto a single ami and this has several disadvantages for it's slow so",
    "start": "1332820",
    "end": "1339780"
  },
  {
    "text": "development time it's different - it's difficult to test an individual analysis",
    "start": "1339780",
    "end": "1344790"
  },
  {
    "text": "so it becomes merged into a workflow as a whole and that means what we're not getting nice reusable analysis steps",
    "start": "1344790",
    "end": "1351390"
  },
  {
    "text": "which is what we really want but one thing that we learnt during this process where we could be installing 30 40",
    "start": "1351390",
    "end": "1357330"
  },
  {
    "text": "different biome Attucks tools in one workflow is we started automating",
    "start": "1357330",
    "end": "1363390"
  },
  {
    "text": "installation of the biometrics tools rather than doing this by hand we use the configuration management tool called",
    "start": "1363390",
    "end": "1369510"
  },
  {
    "text": "chef to do this and this meant that we could have a cookbook which be a github repository repository and it contained a",
    "start": "1369510",
    "end": "1377130"
  },
  {
    "text": "version version of each cookbook and we could also test that installation so we",
    "start": "1377130",
    "end": "1382800"
  },
  {
    "text": "we we improved the the whole installation process automated it which was a big help",
    "start": "1382800",
    "end": "1389420"
  },
  {
    "text": "so again of course along came docker and we started to think well can we",
    "start": "1389420",
    "end": "1395010"
  },
  {
    "text": "represent our hard worker as a docker container and there are two main challenges here can we make any hard",
    "start": "1395010",
    "end": "1402480"
  },
  {
    "text": "worker into a docker image firstly and then once we've done this how can we manage scheduling this container across",
    "start": "1402480",
    "end": "1410250"
  },
  {
    "text": "the cluster and are there any tools that will help us with this this is back in 2015 so I'd like to talk now about that",
    "start": "1410250",
    "end": "1418050"
  },
  {
    "text": "first challenge making that docker image so we start with a docker container here",
    "start": "1418050",
    "end": "1424590"
  },
  {
    "text": "and as I mentioned we already had a good library of bioinformatics tool cookbooks",
    "start": "1424590",
    "end": "1430890"
  },
  {
    "text": "and so it was surprisingly easy to point chef at building a docker container so",
    "start": "1430890",
    "end": "1438780"
  },
  {
    "text": "we've already got our installation and we can just run that on the container so so we've got a mathematics tool",
    "start": "1438780",
    "end": "1444240"
  },
  {
    "text": "installed and we also got a versioned installation for that and using surface",
    "start": "1444240",
    "end": "1450780"
  },
  {
    "text": "right to incorporate our Honorable code which is now a separate github repository and",
    "start": "1450780",
    "end": "1456200"
  },
  {
    "text": "again this is a version software and we can integrate all these things to get around the other dependencies which are",
    "start": "1456200",
    "end": "1462769"
  },
  {
    "text": "needed for the runnable to to to work and now we can run each test that each",
    "start": "1462769",
    "end": "1470809"
  },
  {
    "text": "runnable has such as running the bioinformatics tool so so we now have a",
    "start": "1470809",
    "end": "1476359"
  },
  {
    "text": "tool install you can actually run the tests in a meaningful way with some test data and we can also test various",
    "start": "1476359",
    "end": "1484399"
  },
  {
    "text": "aspects of the runnable such as validating input parameters and whether it's creating expected number of jobs",
    "start": "1484399",
    "end": "1489559"
  },
  {
    "text": "given the test data that we've given it and once we built this container we can",
    "start": "1489559",
    "end": "1494809"
  },
  {
    "text": "then mount the runnable code into the",
    "start": "1494809",
    "end": "1500179"
  },
  {
    "text": "container so we can develop that code on our laptop who our idea of choice and we can actually run it run the tests in",
    "start": "1500179",
    "end": "1506719"
  },
  {
    "text": "effectively a production environment and I'm having got this far then it's it's a",
    "start": "1506719",
    "end": "1513979"
  },
  {
    "text": "fairly logical step just to have a nice continuous integration environment so",
    "start": "1513979",
    "end": "1519619"
  },
  {
    "text": "when when we make a commit to our runnable repository or in fact on a",
    "start": "1519619",
    "end": "1525289"
  },
  {
    "text": "weekly basis as well then Travis will trigger a builder and Jeff will build",
    "start": "1525289",
    "end": "1530509"
  },
  {
    "text": "this container with runnable and both Mattox tool and then it will run the test that we've been looking at and if",
    "start": "1530509",
    "end": "1538489"
  },
  {
    "text": "those tests pass then it will push that image to the repository for analysis",
    "start": "1538489",
    "end": "1544070"
  },
  {
    "text": "step in docker hub so just to just pause",
    "start": "1544070",
    "end": "1549729"
  },
  {
    "text": "slightly and consider some questions that I found very relevant about how we",
    "start": "1549729",
    "end": "1556849"
  },
  {
    "text": "building docker images so firstly do we know how the image that we were using",
    "start": "1556849",
    "end": "1562039"
  },
  {
    "text": "was built so in other words if we don't should we be trusting that that image",
    "start": "1562039",
    "end": "1568609"
  },
  {
    "text": "and really we should have some kind of documentation about this and so secondly",
    "start": "1568609",
    "end": "1574940"
  },
  {
    "text": "does that build still work now so if the last time it was built was a year ago and can we rely on that actually working",
    "start": "1574940",
    "end": "1580729"
  },
  {
    "text": "now I mean very often but dependencies and things are going to change and the bill just won't work anymore",
    "start": "1580729",
    "end": "1587220"
  },
  {
    "text": "so again we need a process of continuous integration do to ensure that we know",
    "start": "1587220",
    "end": "1593340"
  },
  {
    "text": "that it's still working and just in fact when when the build works that's no guarantee that the Dharana move",
    "start": "1593340",
    "end": "1600020"
  },
  {
    "text": "automatic stool is actually working so we should be running tests on the on the",
    "start": "1600020",
    "end": "1605610"
  },
  {
    "text": "software having built it as well and again continuous integration is a good solution to that and lastly do we have",
    "start": "1605610",
    "end": "1614300"
  },
  {
    "text": "any need to run any other container types of of and docker so for example if",
    "start": "1614300",
    "end": "1620340"
  },
  {
    "text": "we're running in HPC environment and Dockers might we might not be allowed to",
    "start": "1620340",
    "end": "1626520"
  },
  {
    "text": "run docker basically so you do we need to bear in mind as we're choosing how to",
    "start": "1626520",
    "end": "1633840"
  },
  {
    "text": "build our images so now we got to the point that we have a workflow",
    "start": "1633840",
    "end": "1639480"
  },
  {
    "text": "configuration file its own github repository and it's a version so we've",
    "start": "1639480",
    "end": "1644820"
  },
  {
    "text": "got version 1 in this example and then each analysis step within the workflow",
    "start": "1644820",
    "end": "1649890"
  },
  {
    "text": "refers to a specific docker image so in this example analysis a is pointing",
    "start": "1649890",
    "end": "1656850"
  },
  {
    "text": "towards the D analysis a Dokic repository and it's in fact it's using",
    "start": "1656850",
    "end": "1662400"
  },
  {
    "text": "the version 1 tag so we can have multiple tags of a C for each docker",
    "start": "1662400",
    "end": "1667460"
  },
  {
    "text": "repository and then allows us B is pointing to the Russian to tag in the",
    "start": "1667460",
    "end": "1672530"
  },
  {
    "text": "docker image B and an RCC is pointing to version 1 tag of the repository for the",
    "start": "1672530",
    "end": "1680250"
  },
  {
    "text": "analysis C image and what that means is",
    "start": "1680250",
    "end": "1685320"
  },
  {
    "text": "that once we deployed our version 1 of the workflow we can very happily go on",
    "start": "1685320",
    "end": "1691230"
  },
  {
    "text": "to version 2 we can say we can deploy version 2 at for another customer and",
    "start": "1691230",
    "end": "1697950"
  },
  {
    "text": "not worry about breaking version 1 or we can have multiple versions whatever so in this case they're version",
    "start": "1697950",
    "end": "1704760"
  },
  {
    "text": "2 and analysis a is using version 3 now it's using a different Targ analysis B is using version T so we can define all",
    "start": "1704760",
    "end": "1712200"
  },
  {
    "text": "these things and that's version you know in our workflow configuration file talked a lot about versioning and why is",
    "start": "1712200",
    "end": "1719360"
  },
  {
    "text": "that well it's essential in scientific workflows and reproducibility is key and",
    "start": "1719360",
    "end": "1726290"
  },
  {
    "text": "in fact it's quite a significant problem now that there's sort of a number of her",
    "start": "1726290",
    "end": "1732770"
  },
  {
    "text": "publications where other other groups are not able to replicate the work in that publication and that's that's a",
    "start": "1732770",
    "end": "1739340"
  },
  {
    "text": "major issue and reasons for this are the steps are not well enough to find so we",
    "start": "1739340",
    "end": "1745280"
  },
  {
    "text": "need to be very explicit about exactly what each workflow staff is doing and we need a very clear instructions about",
    "start": "1745280",
    "end": "1752240"
  },
  {
    "text": "that and the versioning is obviously a key part of that and this is also",
    "start": "1752240",
    "end": "1757480"
  },
  {
    "text": "important in quality control and compliance and having processes and when",
    "start": "1757480",
    "end": "1764110"
  },
  {
    "text": "Angie's a scientist runs workflows and we generate a workflow run report and",
    "start": "1764110",
    "end": "1770300"
  },
  {
    "text": "that contains a description of a workflow vector and they can understand so it's not the fragran attic",
    "start": "1770300",
    "end": "1776120"
  },
  {
    "text": "configuration for is a description diagram etc and it also shows each tool",
    "start": "1776120",
    "end": "1782990"
  },
  {
    "text": "and version of that was using that workflow and this is really important because it might be in three months time",
    "start": "1782990",
    "end": "1788780"
  },
  {
    "text": "but they want to run the same workflow again with the same data and they would expect the same results so it's important to know what which tools",
    "start": "1788780",
    "end": "1795410"
  },
  {
    "text": "versions and they really do want to know that so another aspect that we've added",
    "start": "1795410",
    "end": "1801950"
  },
  {
    "text": "to the hub worker is the logging and",
    "start": "1801950",
    "end": "1807460"
  },
  {
    "text": "having a hundreds of workers or running jobs we need to keep track of that so we",
    "start": "1807460",
    "end": "1812930"
  },
  {
    "text": "added the ability to using fluency and elastic search to log these jobs to this",
    "start": "1812930",
    "end": "1819500"
  },
  {
    "text": "central logging system and once we've got data and elastic search then",
    "start": "1819500",
    "end": "1825710"
  },
  {
    "text": "obviously we can use Cabana so you visualize that it's very dynamic way to create nice interfaces to elastic",
    "start": "1825710",
    "end": "1832790"
  },
  {
    "text": "search so in this example then we've got some in the top left we've got the the job count over time so it's a good way",
    "start": "1832790",
    "end": "1839180"
  },
  {
    "text": "of just monitoring what's going on and then errors warnings and a job even job",
    "start": "1839180",
    "end": "1845450"
  },
  {
    "text": "analysis / / / analysis so now I'd like to talk about",
    "start": "1845450",
    "end": "1851960"
  },
  {
    "text": "the second problem that we had which is how do we deal with scheduling these containers so just to briefly sort of",
    "start": "1851960",
    "end": "1860120"
  },
  {
    "text": "recap then a beehive was originally designed to schedule jobs on the Alice F",
    "start": "1860120",
    "end": "1868850"
  },
  {
    "text": "cluster and to do this it uses an alice F interface and in fact there's it also",
    "start": "1868850",
    "end": "1876379"
  },
  {
    "text": "has another interface it can schedule jobs on an SGD cluster so the obvious",
    "start": "1876379",
    "end": "1881950"
  },
  {
    "text": "thing for us is that we'd make another interface Ataka interface and at which",
    "start": "1881950",
    "end": "1887450"
  },
  {
    "text": "edge of jobs to some kind of docker scheduler as well so we could write this",
    "start": "1887450",
    "end": "1893240"
  },
  {
    "text": "interface quite easily but what about the cluster what tools are ever is going",
    "start": "1893240",
    "end": "1899330"
  },
  {
    "text": "to are going to be suitable for beehive so this goes back to 2015 and obviously",
    "start": "1899330",
    "end": "1904690"
  },
  {
    "text": "this is quite fast-moving fields and things are different now but at the time",
    "start": "1904690",
    "end": "1910639"
  },
  {
    "text": "we were looking for some key criteria and it has to be a good Chad scheduler",
    "start": "1910639",
    "end": "1918110"
  },
  {
    "text": "obviously it needs to be able to schedule containers thousands of containers and yes there were various",
    "start": "1918110",
    "end": "1926710"
  },
  {
    "text": "tools that could do that and we need something with a queryable interface so we need Evod needs to be saying and it",
    "start": "1926710",
    "end": "1933049"
  },
  {
    "text": "needs to find out how many workers are running again that's possible and we",
    "start": "1933049",
    "end": "1938629"
  },
  {
    "text": "need to be able to allocate memory resources and CPU usage before the beehive before the worker starts and",
    "start": "1938629",
    "end": "1947120"
  },
  {
    "text": "this is really important with biometrics tools because they can quite happily carry on some cases for an hour RT we're",
    "start": "1947120",
    "end": "1952909"
  },
  {
    "text": "using very little memory for instance and then that memory can really expand and use say 50 gigabytes so we don't",
    "start": "1952909",
    "end": "1960830"
  },
  {
    "text": "want our decision-making to cram lots of containers all under one machine just",
    "start": "1960830",
    "end": "1967879"
  },
  {
    "text": "because they don't happen to be using much resource at a time and then for all of them to start needing lots of memory",
    "start": "1967879",
    "end": "1973419"
  },
  {
    "text": "so we know this happens in advance so we want to be able to allocate it and then we need to be able to auto we need this",
    "start": "1973419",
    "end": "1980480"
  },
  {
    "text": "to be able to auto scale so this is really important as I've been describing II - create these jobs that",
    "start": "1980480",
    "end": "1985910"
  },
  {
    "text": "dynamically and that start of the workflow we we typically just have one job so we don't want a hundred instances",
    "start": "1985910",
    "end": "1991820"
  },
  {
    "text": "running to process our one job and we need to scale down again and there weren't really any solutions at the time",
    "start": "1991820",
    "end": "1998900"
  },
  {
    "text": "that would do the memory allocation and or tesco we also need a queue so",
    "start": "1998900",
    "end": "2006520"
  },
  {
    "text": "obviously if we we don't have enough instances running we don't want to lose that request for a container so we know",
    "start": "2006520",
    "end": "2013270"
  },
  {
    "text": "what to put it in a key and again there wasn't really anything at the time and lastly we needed something which is not",
    "start": "2013270",
    "end": "2020530"
  },
  {
    "text": "tied to a particular cloud platform so we have customers which however have",
    "start": "2020530",
    "end": "2027430"
  },
  {
    "text": "around their preference of provider so we need we need to be fairly cross-platform in our choice and there",
    "start": "2027430",
    "end": "2034060"
  },
  {
    "text": "were a number of tools here so as you can see there wasn't anything at the",
    "start": "2034060",
    "end": "2040000"
  },
  {
    "text": "time that exactly met our requirement so we decided to put an installation",
    "start": "2040000",
    "end": "2046630"
  },
  {
    "text": "together and we could this a docker swarm Orchestrator and we tried to choose the the biggest sort of starting",
    "start": "2046630",
    "end": "2053610"
  },
  {
    "text": "components that we can rather than writing lots of stuff that's already there so we looked at container",
    "start": "2053610",
    "end": "2060669"
  },
  {
    "text": "scheduler and we chose the docker swarm for this this is a legacy version of Dokka swarm and then to enable an auto",
    "start": "2060670",
    "end": "2067690"
  },
  {
    "text": "scaling feature we chose the docker machine api so this this doctor machine",
    "start": "2067690",
    "end": "2073659"
  },
  {
    "text": "handles starting instances and adding them into a strong and and terminating",
    "start": "2073660",
    "end": "2078669"
  },
  {
    "text": "instances and removing them obviously and it's also very cross cross-platform so all that cross-platform work is",
    "start": "2078670",
    "end": "2084460"
  },
  {
    "text": "handled for us in dhaka machine and we needed to write the rest interfacing and",
    "start": "2084460",
    "end": "2090129"
  },
  {
    "text": "queue and basically we just did that from from scratch really we used",
    "start": "2090130",
    "end": "2096129"
  },
  {
    "text": "RabbitMQ for the key and so these components add up and that's basically",
    "start": "2096130",
    "end": "2102490"
  },
  {
    "text": "what makes the doctor swim Orchestrator so just to show you how this works in",
    "start": "2102490",
    "end": "2107800"
  },
  {
    "text": "practice then we make a container post request this goes to the rest interface",
    "start": "2107800",
    "end": "2113710"
  },
  {
    "text": "of the ESA and then when we make the request",
    "start": "2113710",
    "end": "2119030"
  },
  {
    "text": "the DSO puts that in the queue so now that we have a container in the queue we",
    "start": "2119030",
    "end": "2125570"
  },
  {
    "text": "don't have any instances running so we create a new swarm so docker swarm uses",
    "start": "2125570",
    "end": "2130640"
  },
  {
    "text": "a discovery service so we use console here and we register this nice warm and",
    "start": "2130640",
    "end": "2136550"
  },
  {
    "text": "then we used a docker machine API to add instances to this swarm when we have",
    "start": "2136550",
    "end": "2142010"
  },
  {
    "text": "things in the queue and then that instance is registered in dhaka swarm",
    "start": "2142010",
    "end": "2148130"
  },
  {
    "text": "and dhaka swarm pulls that image from docker hub and runs the container and",
    "start": "2148130",
    "end": "2155330"
  },
  {
    "text": "does the scheduling bit with the resources available so we can paste any",
    "start": "2155330",
    "end": "2160760"
  },
  {
    "text": "number of jobs into into this key so here we've got some large containers for higher memory requirements and some",
    "start": "2160760",
    "end": "2166310"
  },
  {
    "text": "small containers these go into the key and the DSO starts the containers using",
    "start": "2166310",
    "end": "2173690"
  },
  {
    "text": "resources as available so we have more jobs so it started more instances so we",
    "start": "2173690",
    "end": "2179930"
  },
  {
    "text": "can see we can have for small containers on one instance here and just one large",
    "start": "2179930",
    "end": "2186550"
  },
  {
    "text": "container fits on and it takes up a whole instance so so obviously the the",
    "start": "2186550",
    "end": "2191660"
  },
  {
    "text": "resource usage is a lot more efficient already and we still got two large",
    "start": "2191660",
    "end": "2197990"
  },
  {
    "text": "containers as well in the key so at this point if we queried the container end point then we can see that there are",
    "start": "2197990",
    "end": "2204650"
  },
  {
    "text": "two key jobs and five running we obviously get a lot more information than that versus an example and as we've",
    "start": "2204650",
    "end": "2210890"
  },
  {
    "text": "got those two containers in the key store then the DSO starts up some more instances so you can see how it's",
    "start": "2210890",
    "end": "2216700"
  },
  {
    "text": "scaling out based on the on the demands here and there's potato start running",
    "start": "2216700",
    "end": "2223160"
  },
  {
    "text": "and now in our endpoint we have seven running containers so now I'd just like",
    "start": "2223160",
    "end": "2230390"
  },
  {
    "text": "to put these two stages together and how how this will works in in practice so we",
    "start": "2230390",
    "end": "2238820"
  },
  {
    "text": "have a workflow with different steps and each step needs different resources so in our society via writes analysis B is",
    "start": "2238820",
    "end": "2245900"
  },
  {
    "text": "seven half gigabytes now Sears 100 gigabytes so now he hive look",
    "start": "2245900",
    "end": "2255020"
  },
  {
    "text": "sadly the blackboard and sees those jobs to be run in this is for analysis Isis to give rights so it makes a request",
    "start": "2255020",
    "end": "2261170"
  },
  {
    "text": "sort of DSO and this to you bucket a net goes into the queue and as we describe",
    "start": "2261170",
    "end": "2266210"
  },
  {
    "text": "the DSA creates the swarm starts an instance and then Dhaka swarm pulls this",
    "start": "2266210",
    "end": "2271460"
  },
  {
    "text": "image from Dhaka hub and there's our ticking right container running so the instance fit its port here is the",
    "start": "2271460",
    "end": "2278410"
  },
  {
    "text": "diverse and instance for image for",
    "start": "2278410",
    "end": "2283480"
  },
  {
    "text": "analysis a so it's specific forest analysis now mrs.annie hard worker and",
    "start": "2283480",
    "end": "2289220"
  },
  {
    "text": "that worker queries the blackboard and grabs that job so we now one job running on that",
    "start": "2289220",
    "end": "2294830"
  },
  {
    "text": "instance and that's that job runs it fetches some data from the catalog which",
    "start": "2294830",
    "end": "2299840"
  },
  {
    "text": "is stored in s3 and as its processing then it'll to write the output back back",
    "start": "2299840",
    "end": "2306470"
  },
  {
    "text": "to the catalog and then we created five more jobs and these will go into D key",
    "start": "2306470",
    "end": "2314990"
  },
  {
    "text": "and get processed in the same way so these are analysis speed there are seven",
    "start": "2314990",
    "end": "2321920"
  },
  {
    "text": "and a half gigabyte memory container so",
    "start": "2321920",
    "end": "2327320"
  },
  {
    "text": "we need to start up some more I can instances to as we have more containers and then these and he start",
    "start": "2327320",
    "end": "2336620"
  },
  {
    "text": "up and it grabs these jobs and and processes so so that's how we've put",
    "start": "2336620",
    "end": "2344510"
  },
  {
    "text": "these four components together so just a quick note on spot instances we yeah I",
    "start": "2344510",
    "end": "2350870"
  },
  {
    "text": "mean a great idea we get 80% reduction on the under one price so it's just a",
    "start": "2350870",
    "end": "2358720"
  },
  {
    "text": "huge cost saving and just a couple of nights that the incident cyclic and",
    "start": "2358720",
    "end": "2365680"
  },
  {
    "text": "reduction can vary widely so it's not necessarily what you'd expect some more",
    "start": "2365680",
    "end": "2371500"
  },
  {
    "text": "expensive on-demand price can be cheaper has a spot instance and also the",
    "start": "2371500",
    "end": "2377180"
  },
  {
    "text": "availability zone is it can vary a lot so some evaluate availability zones such as in in red",
    "start": "2377180",
    "end": "2383570"
  },
  {
    "text": "here obviously have some very high peaks whereas others are very stable so just",
    "start": "2383570",
    "end": "2388940"
  },
  {
    "text": "need needs to bear this in mind so exposes are great we got a big",
    "start": "2388940",
    "end": "2394160"
  },
  {
    "text": "reduction but obviously they're coming to caveat that the essence can be terminated at any time so how do we cope",
    "start": "2394160",
    "end": "2400310"
  },
  {
    "text": "with this in SLA sort of production environment well so with the system are",
    "start": "2400310",
    "end": "2406460"
  },
  {
    "text": "described then we have multi-layered mitigation for this so he hive is",
    "start": "2406460",
    "end": "2412840"
  },
  {
    "text": "monitoring each of these jobs and if a job fails the aware he knows how to handle this and in fact it's also",
    "start": "2412840",
    "end": "2419870"
  },
  {
    "text": "monitoring the worker so if a worker disappears it's also tracking this and it can request more workers if they",
    "start": "2419870",
    "end": "2425840"
  },
  {
    "text": "disappear so we got the the DSO also has its own queue and these jobs are not",
    "start": "2425840",
    "end": "2431240"
  },
  {
    "text": "going to disappear if we lose a lot synthesis instances and and finally yeah",
    "start": "2431240",
    "end": "2437630"
  },
  {
    "text": "doc Amish and docker swarm works by having docker tour manager components",
    "start": "2437630",
    "end": "2447170"
  },
  {
    "text": "and if if we lose those then we could in theory lobby LTS our swarm so again the",
    "start": "2447170",
    "end": "2454220"
  },
  {
    "text": "DSA can have the Vista can just start nice or managers and we can carry on so",
    "start": "2454220",
    "end": "2461720"
  },
  {
    "text": "so that's kind of a overview of the system but we implemented so now I just like to talk about how we put this into",
    "start": "2461720",
    "end": "2468290"
  },
  {
    "text": "practice with some real data so here we",
    "start": "2468290",
    "end": "2475700"
  },
  {
    "text": "have a tool that we were looking at called need data and what we noticed as we're developing our muscle dynamics",
    "start": "2475700",
    "end": "2481970"
  },
  {
    "text": "workflows but list always taking a lot longer than we expected it wasn't the sort of the key part of the workflow",
    "start": "2481970",
    "end": "2487220"
  },
  {
    "text": "it's it's kind of a preliminary step to to to increase the quality of the data",
    "start": "2487220",
    "end": "2494030"
  },
  {
    "text": "first and looking at these these these charts what we tried is that we noticed",
    "start": "2494030",
    "end": "2501440"
  },
  {
    "text": "with seven and half gigabytes of over two was working it was using awesome memory and taking taking much longer so",
    "start": "2501440",
    "end": "2507440"
  },
  {
    "text": "we tried some different parameters basically we added which we gave it more memory we tried some different",
    "start": "2507440",
    "end": "2512930"
  },
  {
    "text": "parameters at all itself and what what he does is it it records",
    "start": "2512930",
    "end": "2521450"
  },
  {
    "text": "the metrics of each job so we can easily see the average time per job and look",
    "start": "2521450",
    "end": "2528020"
  },
  {
    "text": "looking for this table them by by changing his parameters we've gone from nearly seven hours to just under one",
    "start": "2528020",
    "end": "2533450"
  },
  {
    "text": "hour which is a big time-saving and that's sort of sort of interesting but",
    "start": "2533450",
    "end": "2539750"
  },
  {
    "text": "how does that translate to cost so that's about two and a half dollars for this step for one sample to a lesser",
    "start": "2539750",
    "end": "2546590"
  },
  {
    "text": "than a third of a dollar so that doesn't sound very much but typical metagenomics",
    "start": "2546590",
    "end": "2552110"
  },
  {
    "text": "run could be say a thousand samples so just for this single step in a pipeline and the cost could be to a half thousand",
    "start": "2552110",
    "end": "2560060"
  },
  {
    "text": "down to three hundred dollars and as I mentioned we're using spot instances so",
    "start": "2560060",
    "end": "2566570"
  },
  {
    "text": "actually the resulting reduction comes down to forty dollars",
    "start": "2566570",
    "end": "2572230"
  },
  {
    "text": "so I mentioned we have written this message Amex workflow and so the a modest workflow is basically two things",
    "start": "2573590",
    "end": "2580340"
  },
  {
    "text": "it's to to characterize the gene function of every sequence and it's also",
    "start": "2580340",
    "end": "2586610"
  },
  {
    "text": "secondly to identify the species the taxonomy of all the bacteria based on",
    "start": "2586610",
    "end": "2593330"
  },
  {
    "text": "the sequence that we have in the sample so we're not we're not sequencing a single species here was we're sequencing",
    "start": "2593330",
    "end": "2600080"
  },
  {
    "text": "all the DNA in in a sample containing many different bacteria and the inputs",
    "start": "2600080",
    "end": "2607190"
  },
  {
    "text": "to this pipeline was about in this example was about 2 billion read pairs and it took about 30 hours across 50",
    "start": "2607190",
    "end": "2614600"
  },
  {
    "text": "instances and that's about 200 CPUs and total cost for this pipeline run was",
    "start": "2614600",
    "end": "2620780"
  },
  {
    "text": "less than $50 so just some conclusions",
    "start": "2620780",
    "end": "2628580"
  },
  {
    "text": "and perspective so what we've enabled here is a scientist who's not a computer",
    "start": "2628580",
    "end": "2639140"
  },
  {
    "text": "programmer to upload their data and curate it in the in the catalog and in",
    "start": "2639140",
    "end": "2647840"
  },
  {
    "text": "fact they share that data with other users and collaborate but very all the other users",
    "start": "2647840",
    "end": "2653380"
  },
  {
    "text": "can also run that pipeline from the catalog interface without knowing",
    "start": "2653380",
    "end": "2658390"
  },
  {
    "text": "anything about ie hide and in fact they can monitor the pipeline and I could see all the pipeline steps and how many jobs",
    "start": "2658390",
    "end": "2664030"
  },
  {
    "text": "are running and how many jobs are done as the pipeline is is running and then",
    "start": "2664030",
    "end": "2670720"
  },
  {
    "text": "actually the people uploading a data can be in one country and the people processing the data can be in another",
    "start": "2670720",
    "end": "2675910"
  },
  {
    "text": "country so all these has been has been a great",
    "start": "2675910",
    "end": "2683460"
  },
  {
    "text": "improvement in the use of the technology but it's been very successfully in the",
    "start": "2683460",
    "end": "2691570"
  },
  {
    "text": "Amazon Cloud but can we run this now in an HPC environment for example unless GE",
    "start": "2691570",
    "end": "2697900"
  },
  {
    "text": "cluster so the first thing to note is the SV cluster already has its own",
    "start": "2697900",
    "end": "2702940"
  },
  {
    "text": "scheduler it's very happy with that and probably having doctor swarm as well it",
    "start": "2702940",
    "end": "2707950"
  },
  {
    "text": "would be a very bad idea as you don't really want to schedulers so could we",
    "start": "2707950",
    "end": "2713800"
  },
  {
    "text": "use an rst interface to share our document owners across across",
    "start": "2713800",
    "end": "2719490"
  },
  {
    "text": "environment well many HPC admins are not",
    "start": "2719490",
    "end": "2724990"
  },
  {
    "text": "very happy to run docker in this environment for a variety of reasons but",
    "start": "2724990",
    "end": "2730950"
  },
  {
    "text": "we can use another container type called singularity which is very well fitted to",
    "start": "2730950",
    "end": "2736420"
  },
  {
    "text": "two HPC in fact it's become very very popular over the last year or so in this",
    "start": "2736420",
    "end": "2742210"
  },
  {
    "text": "environment so what we have is a an SG singularity interface for a hive and",
    "start": "2742210",
    "end": "2747820"
  },
  {
    "text": "that can actually use our docket images and translate them into a singularity",
    "start": "2747820",
    "end": "2752980"
  },
  {
    "text": "and run across the SG cluster so basically we can have we have not just one workflow template the same file and",
    "start": "2752980",
    "end": "2760900"
  },
  {
    "text": "we can run that on the Amazon Cloud or on an in internal SGI cluster so we have",
    "start": "2760900",
    "end": "2770140"
  },
  {
    "text": "our catalog which can have the data in different places for different studies and we have e hive which is able to run",
    "start": "2770140",
    "end": "2778119"
  },
  {
    "text": "the same analysis in different so what this means is we can move our",
    "start": "2778119",
    "end": "2784150"
  },
  {
    "text": "analytic sort of data and lock the data to the lm6 and this is a concept we can",
    "start": "2784150",
    "end": "2790620"
  },
  {
    "text": "be known as federated Alex and results integration and in fact we can actually",
    "start": "2790620",
    "end": "2796470"
  },
  {
    "text": "go a step further here and we can run say the first step of our pipeline on",
    "start": "2796470",
    "end": "2801970"
  },
  {
    "text": "our internal compute and then a third third step on AWS cloud so I mentioned",
    "start": "2801970",
    "end": "2810850"
  },
  {
    "text": "that we've enabled our end user scientists to run a pipeline without",
    "start": "2810850",
    "end": "2817060"
  },
  {
    "text": "having to use ie hive and a hands-on way we've got this web interface so how did",
    "start": "2817060",
    "end": "2822760"
  },
  {
    "text": "we do that because II heart was a fairly command line driven tool so we we we we",
    "start": "2822760",
    "end": "2828760"
  },
  {
    "text": "wrote a rest interface for it and with a pipeline a catalog to keep track of all",
    "start": "2828760",
    "end": "2833860"
  },
  {
    "text": "the runs and this rest interface will allow the catalog to cuidar unstated and",
    "start": "2833860",
    "end": "2839020"
  },
  {
    "text": "things like this so this is an example of how we've integrated two different",
    "start": "2839020",
    "end": "2844890"
  },
  {
    "text": "components of the Eagle platform and as the Remender introduced so this is our",
    "start": "2844890",
    "end": "2851100"
  },
  {
    "text": "functional architecture diagram so what I just showed is represented here in the",
    "start": "2851100",
    "end": "2856450"
  },
  {
    "text": "workflow manager SEC diagram and now we're looking at how we can integrate",
    "start": "2856450",
    "end": "2863070"
  },
  {
    "text": "say a hive with he discover and what this would mean is that we run a heavy",
    "start": "2863070",
    "end": "2870340"
  },
  {
    "text": "duty work flow analysis and then when we get the results from that we've got new",
    "start": "2870340",
    "end": "2875430"
  },
  {
    "text": "outputs and data and we can integrate that and link it to the sort of",
    "start": "2875430",
    "end": "2882700"
  },
  {
    "text": "knowledge graph in he discover and sort of we've always dated together so we can",
    "start": "2882700",
    "end": "2889150"
  },
  {
    "text": "do these sort of complex queries so that's kind of where we are at the moment now so finally you know we've",
    "start": "2889150",
    "end": "2900250"
  },
  {
    "text": "been using this platform running these workflows curating them properly in the first place in the catalog and running",
    "start": "2900250",
    "end": "2907780"
  },
  {
    "text": "them with this this interface and and the time-saving and",
    "start": "2907780",
    "end": "2914980"
  },
  {
    "text": "discover that some neighborhoods has increased by 20 times for for them so",
    "start": "2914980",
    "end": "2924910"
  },
  {
    "text": "thank you very much and please if you have any questions",
    "start": "2924910",
    "end": "2931740"
  }
]