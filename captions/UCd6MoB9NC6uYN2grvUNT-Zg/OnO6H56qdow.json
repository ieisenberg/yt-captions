[
  {
    "text": "good afternoon good afternoon my name is Brian Hammond I'm here to introduce our speakers and our topic this afternoon",
    "start": "410",
    "end": "7580"
  },
  {
    "text": "this is my third AWS reinvent and one thing I think is really cool as we see the announcements that reinvent and then",
    "start": "7580",
    "end": "14009"
  },
  {
    "text": "as early as one year later we see them already deployed in the enterprise so the pace of innovation is very fast but",
    "start": "14009",
    "end": "20100"
  },
  {
    "text": "the pace of adoption is also very fast and large enterprises that's a good example what we're gonna see today the",
    "start": "20100",
    "end": "25560"
  },
  {
    "text": "solution we're going to cover involves processing and handling massive amounts of genomics and DNA data is build 100%",
    "start": "25560",
    "end": "32668"
  },
  {
    "text": "on native AWS cloud technology and it is almost entirely serverless so a very innovative solution the company has",
    "start": "32669",
    "end": "39210"
  },
  {
    "text": "built this is called core tempo we have Ryan Smith from court ewa here today is one of our speakers ryan is a software",
    "start": "39210",
    "end": "45270"
  },
  {
    "text": "development engineer in their bioinformatics group and Cordova is a the Agri spin-off of Dow DuPont I wish",
    "start": "45270",
    "end": "53850"
  },
  {
    "text": "were probably all familiar with core Java partnered with Sogeti to build the solution so jetty is the technology and",
    "start": "53850",
    "end": "59820"
  },
  {
    "text": "engineering services division of the Capgemini group and we have scott warren one of our national solution architects",
    "start": "59820",
    "end": "65070"
  },
  {
    "text": "on AWS as our other speaker here today to cover the solution so with that guys are you ready yeah thanks well thank you",
    "start": "65070",
    "end": "75540"
  },
  {
    "text": "I'm Ryan Smith I'm a software development leader at court Eva I actually started at DuPont pioneer in",
    "start": "75540",
    "end": "82680"
  },
  {
    "text": "2009 and I've spent my career creating",
    "start": "82680",
    "end": "87710"
  },
  {
    "text": "software used for biological research to",
    "start": "87710",
    "end": "93229"
  },
  {
    "text": "move the world forward in in terms of science space at court have I've had the",
    "start": "93229",
    "end": "101100"
  },
  {
    "text": "opportunity to work on projects including imaging geographic information systems crop modeling and most recently",
    "start": "101100",
    "end": "108270"
  },
  {
    "text": "and bioinformatics I'll start off with a",
    "start": "108270",
    "end": "113549"
  },
  {
    "text": "geography lesson does any Manoa anyone know what this is North America yes good",
    "start": "113549",
    "end": "120180"
  },
  {
    "text": "okay so let me let me zoom in here a little bit so we'll zoom into Midwest",
    "start": "120180",
    "end": "127680"
  },
  {
    "text": "and anyone know what this is this this",
    "start": "127680",
    "end": "133590"
  },
  {
    "text": "is this is where court ewa is based out of and if your note you'll notice that",
    "start": "133590",
    "end": "139269"
  },
  {
    "text": "there is a lot of green in this image and given the land usage in iowa and",
    "start": "139269",
    "end": "144609"
  },
  {
    "text": "given the the market position that we have about twenty five percent of all",
    "start": "144609",
    "end": "149650"
  },
  {
    "text": "the green stuff that you see in this image comes from seed produced by court",
    "start": "149650",
    "end": "155319"
  },
  {
    "text": "ewa we have a great responsibility to our customers to be able to produce that",
    "start": "155319",
    "end": "161439"
  },
  {
    "text": "seed and produce products that they that will grow well in these conditions and i",
    "start": "161439",
    "end": "168459"
  },
  {
    "text": "think you'll learn a lot about scale this week at reinvent but this is this",
    "start": "168459",
    "end": "175510"
  },
  {
    "text": "is scale here something that you can actually see from space and this is what we're working on in the bioinformatics",
    "start": "175510",
    "end": "183790"
  },
  {
    "text": "space we mainly work on DNA data and that comes from sequencing technology",
    "start": "183790",
    "end": "191159"
  },
  {
    "text": "based on Illumina sequencing machines is mainly what we're using at court Ava we",
    "start": "191159",
    "end": "198220"
  },
  {
    "text": "have entire rows of aluminum machines that are producing this data for us if",
    "start": "198220",
    "end": "204459"
  },
  {
    "text": "you're not familiar with how Illumina sequencing works what happens is the DNA",
    "start": "204459",
    "end": "210159"
  },
  {
    "text": "is randomly fragmented into these small fragments and those fragments are fixed",
    "start": "210159",
    "end": "216819"
  },
  {
    "text": "to a basically a glass plate and the",
    "start": "216819",
    "end": "222099"
  },
  {
    "text": "chemistry is such that they they stand up on on end and then the chemistry can",
    "start": "222099",
    "end": "227349"
  },
  {
    "text": "be changed so that they bend over and create this bridge and in that bridge",
    "start": "227349",
    "end": "232590"
  },
  {
    "text": "state a double-stranded synthesis takes place and two strands from that one",
    "start": "232590",
    "end": "239319"
  },
  {
    "text": "strand can be produced once that strand second strand is produced the chemistry",
    "start": "239319",
    "end": "244900"
  },
  {
    "text": "has changed again so that they those two strands then are go straight up or",
    "start": "244900",
    "end": "251859"
  },
  {
    "text": "straight up this happens a number of times we repeat this process so one",
    "start": "251859",
    "end": "257380"
  },
  {
    "text": "becomes 2 2 becomes 4 4 becomes 8 and the signal of each DNA strand is",
    "start": "257380",
    "end": "265200"
  },
  {
    "text": "significantly amplified and you end up with a number of these",
    "start": "265200",
    "end": "270720"
  },
  {
    "text": "dense clusters of repeated DNA fragments that are the same and we use that",
    "start": "270720",
    "end": "277360"
  },
  {
    "text": "amplified signal to be able to image those DNA fragments and come up with",
    "start": "277360",
    "end": "284650"
  },
  {
    "text": "what the sequence is now the reason I'm talking about this is because the length",
    "start": "284650",
    "end": "290830"
  },
  {
    "text": "of that fragment is very important as you can imagine if the fragment length is too small it won't be long enough to",
    "start": "290830",
    "end": "298180"
  },
  {
    "text": "bend over and create that amplification bridge if it is too long it won't be",
    "start": "298180",
    "end": "303729"
  },
  {
    "text": "able to stand up straight and it'll just always be bowed over so there's a narrow range of lengths about 50 to 150 base",
    "start": "303729",
    "end": "311949"
  },
  {
    "text": "pairs that it needs to be in order to make this work and that impacts the data",
    "start": "311949",
    "end": "318099"
  },
  {
    "text": "that comes out of this process so this is what we get out of the process or",
    "start": "318099",
    "end": "325090"
  },
  {
    "text": "about 150 base pair read lengths and we need to align those reads to a reference",
    "start": "325090",
    "end": "331780"
  },
  {
    "text": "assembly the reference assembly can be billions of base pairs long so imagine trying to align a hundred and fifty",
    "start": "331780",
    "end": "338169"
  },
  {
    "text": "character string to a string that is a couple of billion characters long a",
    "start": "338169",
    "end": "344139"
  },
  {
    "text": "difficult problem but one that has been solved and there are efficient open-source solutions for doing this and",
    "start": "344139",
    "end": "351159"
  },
  {
    "text": "we employ those solutions to work but of course you have to provide your own iron",
    "start": "351159",
    "end": "357039"
  },
  {
    "text": "to be able to do the processing that happens millions of times and do the storage of the results of that file so",
    "start": "357039",
    "end": "367710"
  },
  {
    "text": "we have such scale that at court Ava every six hours we produce as much",
    "start": "367949",
    "end": "374590"
  },
  {
    "text": "genetic data as existed in the entire public sphere in 2008 so an incredible",
    "start": "374590",
    "end": "381130"
  },
  {
    "text": "amount of data that we're producing and our on-prem system for handling that",
    "start": "381130",
    "end": "386889"
  },
  {
    "text": "data was not going to keep up we had a 35 node Hadoop cluster with two",
    "start": "386889",
    "end": "393580"
  },
  {
    "text": "petabytes of storage and those two petabytes were filled and we had to come",
    "start": "393580",
    "end": "399339"
  },
  {
    "text": "up with had he's hard conversations with our research partners where we said no you can't do this experiment right now we",
    "start": "399339",
    "end": "406990"
  },
  {
    "text": "don't have the storage to store the data that's coming out of it and that wasn't",
    "start": "406990",
    "end": "412390"
  },
  {
    "text": "a great situation to be in and take on that and include the fact that DNA",
    "start": "412390",
    "end": "418630"
  },
  {
    "text": "sequencing technology is going is going to continue to decrease in price the",
    "start": "418630",
    "end": "424360"
  },
  {
    "text": "demand in the future was also going to increase and we knew we just didn't want",
    "start": "424360",
    "end": "429940"
  },
  {
    "text": "to continue to buy more and more nodes to add to that cluster just to be able to store the data we knew that we needed",
    "start": "429940",
    "end": "436270"
  },
  {
    "text": "to change directions so we looked at different cloud solutions and decided on",
    "start": "436270",
    "end": "444400"
  },
  {
    "text": "using AWS because they really understood our research needs we could talk to them",
    "start": "444400",
    "end": "451270"
  },
  {
    "text": "about the problems we were having and they had dealt with signal similar issues and they were able to point us to",
    "start": "451270",
    "end": "457690"
  },
  {
    "text": "the right services for those things they weren't shoehorning us into a particular solution just because that's what they",
    "start": "457690",
    "end": "463630"
  },
  {
    "text": "had we also knew that Amazon Elastic MapReduce was probably going to be",
    "start": "463630",
    "end": "469810"
  },
  {
    "text": "needed given that our on-prem system was Hadoop was a Hadoop system so that was",
    "start": "469810",
    "end": "476740"
  },
  {
    "text": "really a key selling point for us was being able to use EMR to process this",
    "start": "476740",
    "end": "483010"
  },
  {
    "text": "data the other part of that is s3 being",
    "start": "483010",
    "end": "488110"
  },
  {
    "text": "able to store the data decoupled from the compute that goes with it and being",
    "start": "488110",
    "end": "493390"
  },
  {
    "text": "able to decouple those two things the compute from the storage gave us some cost efficiencies that allowed us to",
    "start": "493390",
    "end": "502170"
  },
  {
    "text": "order up the right amount of compute for what the data we needed to do and also",
    "start": "502170",
    "end": "507600"
  },
  {
    "text": "store it in a you know in a way that would work for us so the way we that we",
    "start": "507600",
    "end": "517000"
  },
  {
    "text": "use the DNA data that we're producing there are a number of ways that we're",
    "start": "517000",
    "end": "522610"
  },
  {
    "text": "using it for so the first one in genome wide variation screening I don't know if",
    "start": "522610",
    "end": "528070"
  },
  {
    "text": "you guys are familiar with familiar with 23andme but that's basically what we're",
    "start": "528070",
    "end": "533590"
  },
  {
    "text": "doing here where you send off your sample to 23andme and it comes up with some",
    "start": "533590",
    "end": "538930"
  },
  {
    "text": "variations between your genetics that are in you versus the rest of the human",
    "start": "538930",
    "end": "544000"
  },
  {
    "text": "population and those variations might be things like you're susceptible to these diseases or you have these color of eyes",
    "start": "544000",
    "end": "551530"
  },
  {
    "text": "you have these certain traits about you we're doing the same sorts of things for cereal crops so is this particular corn",
    "start": "551530",
    "end": "560140"
  },
  {
    "text": "seed going to be drought resistant is this going to be disease resistant",
    "start": "560140",
    "end": "565240"
  },
  {
    "text": "what's the flowering time what are these different aspects of this seed that we're looking at that's the sort of",
    "start": "565240",
    "end": "570310"
  },
  {
    "text": "analysis that we're looking at for these for this data we're also doing transformational assays so we're",
    "start": "570310",
    "end": "577690"
  },
  {
    "text": "checking to see if the transformation event that we produced is what we expected did it mess up any of the other",
    "start": "577690",
    "end": "584170"
  },
  {
    "text": "genetics did it land in the right position was the entire cassette added properly make sure that that's working",
    "start": "584170",
    "end": "590470"
  },
  {
    "text": "for quality control our lab is synthesizing DNA fragments so we want to",
    "start": "590470",
    "end": "596410"
  },
  {
    "text": "make sure that that synthesis process happened properly and finally a whole genome assembly there's actually a wide",
    "start": "596410",
    "end": "605650"
  },
  {
    "text": "range of variation between corn seeds can actually be as wide of difference between a human and a chimpanzee and",
    "start": "605650",
    "end": "612780"
  },
  {
    "text": "just the normal variation screening isn't enough to understand those differences you have to actually",
    "start": "612780",
    "end": "618460"
  },
  {
    "text": "assemble the entire genome to be able to understand that full difference but the",
    "start": "618460",
    "end": "625540"
  },
  {
    "text": "remainder of the talk will talk about just genome wide variation screening and quality control and the applications",
    "start": "625540",
    "end": "631930"
  },
  {
    "text": "that we built around those two problems so the applications that we had already",
    "start": "631930",
    "end": "640270"
  },
  {
    "text": "built were snip finder that was for the whole genome alignment of short reads",
    "start": "640270",
    "end": "645700"
  },
  {
    "text": "that's for the whole genome variation screening and what we're looking for our",
    "start": "645700",
    "end": "651250"
  },
  {
    "text": "single nucleotide polymorphisms single changes in the DNA 4e as compared to a",
    "start": "651250",
    "end": "658570"
  },
  {
    "text": "reference sequence the input data size is quite large for each one of these",
    "start": "658570",
    "end": "663700"
  },
  {
    "text": "samples between 50 and 500 gigabytes and when you are comparing a number of",
    "start": "663700",
    "end": "669320"
  },
  {
    "text": "different samples that upper-end can almost be unbounded amount of data so",
    "start": "669320",
    "end": "674660"
  },
  {
    "text": "the processing needs are quite are quite",
    "start": "674660",
    "end": "679850"
  },
  {
    "text": "strong they're on the vector quality control side this is where we're",
    "start": "679850",
    "end": "685250"
  },
  {
    "text": "synthesizing DNA fragment for using for use in a transgenic event and we need to",
    "start": "685250",
    "end": "691370"
  },
  {
    "text": "make sure that that's a synthesis event happened properly and make sure that the",
    "start": "691370",
    "end": "697010"
  },
  {
    "text": "DNA sequence that we designed is what we got out that is important so that when",
    "start": "697010",
    "end": "703610"
  },
  {
    "text": "we do the transformation event that it's going to work as we expected and it's also important for regulatory purposes",
    "start": "703610",
    "end": "709780"
  },
  {
    "text": "for government agencies that we can prove that that that synthesis was",
    "start": "709780",
    "end": "715190"
  },
  {
    "text": "correct the input data size is actually quite a bit smaller here less than 10 megabytes for each sample but there are",
    "start": "715190",
    "end": "722900"
  },
  {
    "text": "tens of thousands of samples that are run each year and we need to be able to",
    "start": "722900",
    "end": "728200"
  },
  {
    "text": "meet that demand so when we started off on this project we had to come up with a",
    "start": "728200",
    "end": "735590"
  },
  {
    "text": "name and the name that we came up with was Theseus and I had the same reaction",
    "start": "735590",
    "end": "742970"
  },
  {
    "text": "as maybe you guys are what's this what's that mean well it's actually the name of",
    "start": "742970",
    "end": "748100"
  },
  {
    "text": "a ancient thought experiment called the Ship of Theseus where if you have a ship",
    "start": "748100",
    "end": "754220"
  },
  {
    "text": "that's made out of wood planks and over time those wood planks rot out and need",
    "start": "754220",
    "end": "759620"
  },
  {
    "text": "to be replaced you replace those boards and over time you might replace every",
    "start": "759620",
    "end": "765440"
  },
  {
    "text": "single board on the ship at the end is it the same ship that you started with or is it a different ship given that",
    "start": "765440",
    "end": "773090"
  },
  {
    "text": "it's completely different board and we had that kind of ask our solve these same questions with the applications",
    "start": "773090",
    "end": "780650"
  },
  {
    "text": "that were writing where every single line of code is going to be replaced with something else we we thought",
    "start": "780650",
    "end": "786110"
  },
  {
    "text": "Theseus was a good name for this project so if you guys don't learn anything else from this then you have a good name for",
    "start": "786110",
    "end": "791990"
  },
  {
    "text": "your next project and you can send your royalties to me or Scott",
    "start": "791990",
    "end": "797920"
  },
  {
    "text": "given that we already had these applications in place we had a pretty good idea of what the user interaction",
    "start": "798650",
    "end": "805380"
  },
  {
    "text": "was going to look like for each application for snip finder there was a",
    "start": "805380",
    "end": "810470"
  },
  {
    "text": "pipeline that transforms the data into a query Abul state and then users come in",
    "start": "810470",
    "end": "816420"
  },
  {
    "text": "and create ad hoc queries to understand that data and make sense of it on the",
    "start": "816420",
    "end": "823590"
  },
  {
    "text": "vqc side the user interaction is a little bit different where all of the data is processed upfront before",
    "start": "823590",
    "end": "831210"
  },
  {
    "text": "entering the data and the user interaction is basically to make sure that the data was processed properly and",
    "start": "831210",
    "end": "838950"
  },
  {
    "text": "that the conclusions that the system made about the data are correct and",
    "start": "838950",
    "end": "844440"
  },
  {
    "text": "finally okay we've made that conclusion what do we do now with this with this",
    "start": "844440",
    "end": "849870"
  },
  {
    "text": "vector do we throw it out or do we continue on with it given those user",
    "start": "849870",
    "end": "858420"
  },
  {
    "text": "interactions early on in the project we came up with some guiding principles of",
    "start": "858420",
    "end": "864420"
  },
  {
    "text": "what we wanted the project to look like we knew that we had time-sensitive workloads we needed to be able to fill",
    "start": "864420",
    "end": "871440"
  },
  {
    "text": "these demands for turnaround time and had very tight user expectations on how",
    "start": "871440",
    "end": "879810"
  },
  {
    "text": "quickly the data needed to be processed but we also know we had a relatively small user base maybe five users and",
    "start": "879810",
    "end": "886820"
  },
  {
    "text": "each of those users is a power user and they might be executing something very",
    "start": "886820",
    "end": "894600"
  },
  {
    "text": "complex and we need to be able to fill that need for that very complex query",
    "start": "894600",
    "end": "901110"
  },
  {
    "text": "that they're running so being able to understand that you know most of the",
    "start": "901110",
    "end": "907560"
  },
  {
    "text": "time these applications aren't going to be running because there's a small user base but when they are running they need",
    "start": "907560",
    "end": "912930"
  },
  {
    "text": "to have great performance we thought the serverless architecture really fit well",
    "start": "912930",
    "end": "918960"
  },
  {
    "text": "with that user pattern so the basic idea that we had was that if there's no if",
    "start": "918960",
    "end": "925620"
  },
  {
    "text": "there's no one using the system there should be no servers running that's really the guiding principle",
    "start": "925620",
    "end": "931170"
  },
  {
    "text": "we had other guiding principles were immutable infrastructure so we didn't",
    "start": "931170",
    "end": "936959"
  },
  {
    "text": "want to have to treat our servers like pets we wanted them to be livestock we",
    "start": "936959",
    "end": "942240"
  },
  {
    "text": "just wanted to spin them up and do the processing that they're required for not",
    "start": "942240",
    "end": "947279"
  },
  {
    "text": "have to worry about patching or while it's running or making other things we just wanted to spin them up do the",
    "start": "947279",
    "end": "953160"
  },
  {
    "text": "processing and then turn them back off and finally we wanted to automate everything there were a number of",
    "start": "953160",
    "end": "959089"
  },
  {
    "text": "one-off manual processes in our legacy system and we wanted to really avoid",
    "start": "959089",
    "end": "964709"
  },
  {
    "text": "that because we didn't want to have to worry about that long-term maintenance that we had we just wanted to make sure",
    "start": "964709",
    "end": "970529"
  },
  {
    "text": "we could automate everything and AWS gave us the tools to be able to do that",
    "start": "970529",
    "end": "975930"
  },
  {
    "text": "automation given these guiding",
    "start": "975930",
    "end": "982260"
  },
  {
    "text": "principles and where everything said we came up with the the fact that there's",
    "start": "982260",
    "end": "987720"
  },
  {
    "text": "really a difference here between these two applications and there should be a difference in design of how we architect",
    "start": "987720",
    "end": "993779"
  },
  {
    "text": "these applications we don't want to have everything use the same service just",
    "start": "993779",
    "end": "999930"
  },
  {
    "text": "because we're doing it at the same time and maybe we understand that service really well we should really tune the",
    "start": "999930",
    "end": "1005660"
  },
  {
    "text": "architecture of each application for the specific needs of that application so at",
    "start": "1005660",
    "end": "1011570"
  },
  {
    "text": "that point we turn to Sogeti because this was the one of the first projects",
    "start": "1011570",
    "end": "1018290"
  },
  {
    "text": "we've done in AWS and we really didn't have the expertise in-house to be able to build production applications so",
    "start": "1018290",
    "end": "1025579"
  },
  {
    "text": "that's when we turned to see jetty and and with Scotts help we're able to create them",
    "start": "1025579",
    "end": "1032860"
  },
  {
    "text": "all right Thank You Ryan so Ryan did a good job of kind of laying out what we",
    "start": "1035260",
    "end": "1040910"
  },
  {
    "text": "had on premise what our data was what problem we had to solve so now I'm gonna take the time to walk you through how we",
    "start": "1040910",
    "end": "1046819"
  },
  {
    "text": "solve that in AWS which services we use how we plug them together things like that so the first application that we",
    "start": "1046819",
    "end": "1054050"
  },
  {
    "text": "have is snip finder and so this is the one that has the large amounts of data that runs less frequently and so the big",
    "start": "1054050",
    "end": "1061610"
  },
  {
    "text": "thing here is the short reads that Ryan talked about we need to align those decide if what we found is actually a",
    "start": "1061610",
    "end": "1067820"
  },
  {
    "text": "snip or if it's a sequencing error an error on the genetic sequencer and finally we take that data and transform",
    "start": "1067820",
    "end": "1074840"
  },
  {
    "text": "it into a queryable format which in our case is a park' file",
    "start": "1074840",
    "end": "1080240"
  },
  {
    "text": "so walking through the architecture on how we completed this so the first thing the user does is they need to know what",
    "start": "1080240",
    "end": "1088190"
  },
  {
    "text": "data is out there and they need to submit a job to the system so that is done through an API call so the the user",
    "start": "1088190",
    "end": "1094460"
  },
  {
    "text": "forms that API call and that's got things like where's my data what data do I need to work with how big a cluster an",
    "start": "1094460",
    "end": "1101510"
  },
  {
    "text": "EMR cluster do I need to spin up for this what's the criticality of this different things like that so the user submits the",
    "start": "1101510",
    "end": "1108170"
  },
  {
    "text": "API call that triggers a lambda function and we use the desire for to and from",
    "start": "1108170",
    "end": "1114080"
  },
  {
    "text": "there the the lambda function writes all the details about that job into an RDS instance this was one of our first kind",
    "start": "1114080",
    "end": "1121160"
  },
  {
    "text": "of design considerations we ran into where we looked at dynamodb pretty heavily instead of RDS here and after",
    "start": "1121160",
    "end": "1128030"
  },
  {
    "text": "kind of a lot of tests and in understanding what our data was and how it worked we were ended up doing a lot of full",
    "start": "1128030",
    "end": "1134360"
  },
  {
    "text": "table scans and long table reads and everything so DynamoDB was going to be a very expensive option and not really",
    "start": "1134360",
    "end": "1140480"
  },
  {
    "text": "gain any performance or anything like that so we decided on RDS at this point so all that job information is written into",
    "start": "1140480",
    "end": "1147230"
  },
  {
    "text": "our database and at the same time that job ID is written to a queue so we then",
    "start": "1147230",
    "end": "1154850"
  },
  {
    "text": "had a cloud watch event that runs every couple minutes they triggers the same lambda function to to check that queue",
    "start": "1154850",
    "end": "1160670"
  },
  {
    "text": "so is there a new job if no the lambda function ends and nothing happens if yes then that triggers the spin up of an EMR",
    "start": "1160670",
    "end": "1169640"
  },
  {
    "text": "and this cluster is actually defined by the user so the user knows the criticality of that workload it knows",
    "start": "1169640",
    "end": "1176510"
  },
  {
    "text": "the rough idea and what the cost of this EMR cluster is going to be so they're going to tell us what type of instances they need how many of those instances to",
    "start": "1176510",
    "end": "1184340"
  },
  {
    "text": "process their data and in the critical time for them so as that data's",
    "start": "1184340",
    "end": "1189380"
  },
  {
    "text": "processing and being transformed into that park' queryable format for us there's lots of intermediate files that",
    "start": "1189380",
    "end": "1196040"
  },
  {
    "text": "need to be written and some intermediate files that needs to be referenced as that data's processing it's our first",
    "start": "1196040",
    "end": "1202040"
  },
  {
    "text": "stab at this we wrote all those intermediate files to EFS and stored all of the the intermediate files that",
    "start": "1202040",
    "end": "1208400"
  },
  {
    "text": "needed to be referenced during the data pipeline on EFS as well and so we quickly found out that we hit some EFS",
    "start": "1208400",
    "end": "1216049"
  },
  {
    "text": "burst credit limitations and our performance dropped pretty drastically after we kind of hit that credit limit",
    "start": "1216049",
    "end": "1222380"
  },
  {
    "text": "so we revamp the process halfway through and started writing a lot of those intermediate files to an intermediate s3",
    "start": "1222380",
    "end": "1228860"
  },
  {
    "text": "bucket as we were going through so then we ran into the issue of hot spotting on this 3 and again kind of degradation a",
    "start": "1228860",
    "end": "1236809"
  },
  {
    "text": "solution where as we write those intermediate files to s3 we stick a unique hash in front of the pilot file",
    "start": "1236809",
    "end": "1242780"
  },
  {
    "text": "name that distributes it evenly across the s3 nodes and avoids hot spotting for",
    "start": "1242780",
    "end": "1248090"
  },
  {
    "text": "us so our final architecture and solution has all of the intermediate files that are being written sent to s3",
    "start": "1248090",
    "end": "1253840"
  },
  {
    "text": "and anything that needs to be read comes out of EFS still so that was kind of the sweet spot for us as far as file",
    "start": "1253840",
    "end": "1260059"
  },
  {
    "text": "movement during the processing job and you can see on the right hand side there that the data is pulled from an s3",
    "start": "1260059",
    "end": "1267169"
  },
  {
    "text": "bucket where the DNA sequencer drops the genetic data as it comes off the sequencer so once that processing is",
    "start": "1267169",
    "end": "1274940"
  },
  {
    "text": "complete the final park' files there the result data is written to a separate s3",
    "start": "1274940",
    "end": "1280400"
  },
  {
    "text": "bucket where it's kind of the permanent home for that data there's also an AWS glue process that we introduced later in",
    "start": "1280400",
    "end": "1287090"
  },
  {
    "text": "our architecture to do some file cleanup and different things like that and that's a good example of when we started",
    "start": "1287090",
    "end": "1293330"
  },
  {
    "text": "this project glue wasn't really ready for primetime but as we kind of developed and build things it came into",
    "start": "1293330",
    "end": "1298970"
  },
  {
    "text": "its own and we were able to use it so Ryan will talk a little more about later how we use AWS glue for some file cleanup so when",
    "start": "1298970",
    "end": "1305700"
  },
  {
    "text": "that processing is done an SNS notification is sent out to notify the user that the the job they submitted has",
    "start": "1305700",
    "end": "1310920"
  },
  {
    "text": "been processed successfully and the result data is there for them to use so",
    "start": "1310920",
    "end": "1316910"
  },
  {
    "text": "the second portion of snip finder we've got that data off the sequencer process stored in our s3 bucket ready to query",
    "start": "1316910",
    "end": "1323840"
  },
  {
    "text": "we actually have to query it now and give the user the ability to do that so",
    "start": "1323840",
    "end": "1328890"
  },
  {
    "text": "there a couple ways we can query this data the first two are position or purity and coverage those are a little",
    "start": "1328890",
    "end": "1335760"
  },
  {
    "text": "more basic kind of sequel type queries that we we can access the files directly and do that but more complicated queries",
    "start": "1335760",
    "end": "1342450"
  },
  {
    "text": "or what we call neighborhood there if you can see in the genetic sequence we've got that red tea that's our snip",
    "start": "1342450",
    "end": "1348960"
  },
  {
    "text": "that we found so we're very interested in finding what types of things are around that snip so things like other",
    "start": "1348960",
    "end": "1356190"
  },
  {
    "text": "snips percentage of GS and C's any repetitive genetic sequences that are",
    "start": "1356190",
    "end": "1361290"
  },
  {
    "text": "around their annotations things like that and that's where we get into the really the big data type problem here",
    "start": "1361290",
    "end": "1367559"
  },
  {
    "text": "we're being able to query everything that's around that without one snip that we found and why we needed an EMR to",
    "start": "1367559",
    "end": "1374309"
  },
  {
    "text": "kind of query this data as well so if we look at how the queries happen",
    "start": "1374309",
    "end": "1379620"
  },
  {
    "text": "we built a UI for the users it's built in angularjs hosted natively in s3 so",
    "start": "1379620",
    "end": "1386460"
  },
  {
    "text": "they go in and select the data they want to query how they want to filter it what outputs they want out of that data",
    "start": "1386460",
    "end": "1391970"
  },
  {
    "text": "the UI then makes an API call back to API gateway from there that triggers a",
    "start": "1391970",
    "end": "1398010"
  },
  {
    "text": "lambda function it works very similar to the way the data pipeline worked as well so a job ID is written to RDS same kind",
    "start": "1398010",
    "end": "1405540"
  },
  {
    "text": "of job parameters also dropped on a queue and then lambda watches that queue",
    "start": "1405540",
    "end": "1410670"
  },
  {
    "text": "the same way I did for the the data ingestion pipeline there's one big difference here is we didn't want users",
    "start": "1410670",
    "end": "1417030"
  },
  {
    "text": "to submit submit too many queries or and kind of bump up our AWS bill to an area",
    "start": "1417030",
    "end": "1423450"
  },
  {
    "text": "we didn't really want to deal with so we wanted to limit the number of queries that could be running at one time and",
    "start": "1423450",
    "end": "1429179"
  },
  {
    "text": "the way we found to do that is to actually limit the number of EMR nodes that our auto scaling cluster would allow so by saying",
    "start": "1429179",
    "end": "1435870"
  },
  {
    "text": "you can only scale up to X many nodes on your EMR cluster some queries won't run",
    "start": "1435870",
    "end": "1440880"
  },
  {
    "text": "right away they're going to need to sit in the queue and wait until the number of nodes in our cluster comes back down",
    "start": "1440880",
    "end": "1446990"
  },
  {
    "text": "and so that was the way we were able to kind of regulate how many query jobs are running in a single time so lambda sees",
    "start": "1446990",
    "end": "1455160"
  },
  {
    "text": "it's a new job out there tells the EMR cluster to scale up from there the EMR cluster goes out and pulls the",
    "start": "1455160",
    "end": "1461670"
  },
  {
    "text": "data from s3 processes it in a similar way to query it where it's referencing",
    "start": "1461670",
    "end": "1467280"
  },
  {
    "text": "files on EFS and then writing the results out to our s3 bucket and then",
    "start": "1467280",
    "end": "1473309"
  },
  {
    "text": "also some metadata about those results is written 2 RDS as well and so from there",
    "start": "1473309",
    "end": "1478830"
  },
  {
    "text": "what's the query is complete the lambda function returns it back to the user and then it's available to download for the",
    "start": "1478830",
    "end": "1485130"
  },
  {
    "text": "user or view and the UI for the user so we take a look at the user interface first nit finder here this is the query",
    "start": "1485130",
    "end": "1493080"
  },
  {
    "text": "page you can see on the Left those are all our different samples so all the data that's been ingested do that ingestion data pipeline that I went",
    "start": "1493080",
    "end": "1499679"
  },
  {
    "text": "through kind of in the middle and top right are all the different filters you can apply to the data those are those",
    "start": "1499679",
    "end": "1506130"
  },
  {
    "text": "neighborhood filters that I was talking about and then on the bottom right it's all the options the user has for",
    "start": "1506130",
    "end": "1512370"
  },
  {
    "text": "exporting that data what format do they want it in what file type what what data",
    "start": "1512370",
    "end": "1517470"
  },
  {
    "text": "they want in that output different things like that so that snip finder",
    "start": "1517470",
    "end": "1523320"
  },
  {
    "text": "that's the the big data that comes in infrequently and it's kind of ad hoc as when it needs to be queried and when",
    "start": "1523320",
    "end": "1528750"
  },
  {
    "text": "somebody needs to look at it now I'm gonna dig into the vqc architecture and that's the one where like Ryan said much",
    "start": "1528750",
    "end": "1535650"
  },
  {
    "text": "smaller data normally less than 10 megabytes but we have tens of thousands of these a year that need to happen and",
    "start": "1535650",
    "end": "1540990"
  },
  {
    "text": "so it's built quite a bit differently in AWS even though we've got the exact same source data still coming off the DNA",
    "start": "1540990",
    "end": "1547230"
  },
  {
    "text": "sequencer we had to solve this problem in a little bit different way so the first big difference in the vqc",
    "start": "1547230",
    "end": "1553920"
  },
  {
    "text": "architecture is it's not a user generated query every bit of data that",
    "start": "1553920",
    "end": "1559470"
  },
  {
    "text": "comes off the sequencer needs to go through this process so the data ingestion is triggered by a Mar on",
    "start": "1559470",
    "end": "1565920"
  },
  {
    "text": "premise publisher subscriber system so we've got a in connection to AWS and each time new",
    "start": "1565920",
    "end": "1571919"
  },
  {
    "text": "data comes off that sequencer the pub/sub system sends a message that triggers a lambda function for us and",
    "start": "1571919",
    "end": "1577919"
  },
  {
    "text": "that message says this data came off the sequencer it needs to be processed and put into vqc so from there that lambda",
    "start": "1577919",
    "end": "1584520"
  },
  {
    "text": "function spins up a batch instance so we built this in batch instead of EMR so",
    "start": "1584520",
    "end": "1590279"
  },
  {
    "text": "we're working with a much smaller data we didn't want to wait for the cluster spin up and scale time and things like",
    "start": "1590279",
    "end": "1596010"
  },
  {
    "text": "that so we built this containerized and docker and runs on batch so the batch",
    "start": "1596010",
    "end": "1602309"
  },
  {
    "text": "container spins up with that reference data pulls it from the same s3 bucket the office sequencers and processes the",
    "start": "1602309",
    "end": "1610320"
  },
  {
    "text": "data for the vector quality control after that processing is complete it",
    "start": "1610320",
    "end": "1616649"
  },
  {
    "text": "writes the results out to a different as three bucket the same as in the snip finder there's some parsing some",
    "start": "1616649",
    "end": "1622260"
  },
  {
    "text": "post-processing parsing that happens in a lambda function in this case and then an SES notification is triggered that",
    "start": "1622260",
    "end": "1628230"
  },
  {
    "text": "goes out to an email goes out to a distribution list saying this data a sequence it's been run through the vqc",
    "start": "1628230",
    "end": "1633659"
  },
  {
    "text": "system and it's now available for use so now a user needs to access that data",
    "start": "1633659",
    "end": "1639779"
  },
  {
    "text": "it's in the system they've received the email how do they get to it I'm so very similar to how we did snip finder here",
    "start": "1639779",
    "end": "1644970"
  },
  {
    "text": "where we've got a UI built out for this application angularjs hosted natively an s3 the user",
    "start": "1644970",
    "end": "1651090"
  },
  {
    "text": "goes in and selects what data they want to look at and remember they're not doing any processing this time it's already all done they just need to view",
    "start": "1651090",
    "end": "1657090"
  },
  {
    "text": "the results so they select the data they want to look at an internal API call is made to",
    "start": "1657090",
    "end": "1662580"
  },
  {
    "text": "API gateway and blam to function then pulls the proper data out of s3 and RDS",
    "start": "1662580",
    "end": "1669840"
  },
  {
    "text": "for that metadata and displays it for the user to see so this is what the UI",
    "start": "1669840",
    "end": "1675600"
  },
  {
    "text": "for vqc looks like where you can see up top that is the reference the data they",
    "start": "1675600",
    "end": "1682049"
  },
  {
    "text": "selected they want to look at and some different filters and ways they can they can view that data and on the bottom is",
    "start": "1682049",
    "end": "1687840"
  },
  {
    "text": "the result of the processing that happened in AWS batch as we brought that data in so that visualization is",
    "start": "1687840",
    "end": "1694200"
  },
  {
    "text": "actually using an open source tool called j browse which is a project that's built and supported to visualize",
    "start": "1694200",
    "end": "1700770"
  },
  {
    "text": "this type of data so it worked really well posting that natively and angular in an s3",
    "start": "1700770",
    "end": "1706220"
  },
  {
    "text": "bucket so at a high level that's kind of how we solved the two problems so the",
    "start": "1706220",
    "end": "1711620"
  },
  {
    "text": "same data but how do we work with them separately in different ways in AWS we're on premise we were constrained",
    "start": "1711620",
    "end": "1717290"
  },
  {
    "text": "where this has to go into this Hadoop system so you can see two different use cases and how we solved it differently",
    "start": "1717290",
    "end": "1723080"
  },
  {
    "text": "one using EMR the other using batch so I'm going to turn it back over to Ryan now to kind of go over a few of the",
    "start": "1723080",
    "end": "1730850"
  },
  {
    "text": "conclusions we we came to as we worked through this project and some of the benefits we've seen of having the system",
    "start": "1730850",
    "end": "1736970"
  },
  {
    "text": "in AWS as opposed to on-premise thanks",
    "start": "1736970",
    "end": "1744740"
  },
  {
    "text": "God so Scott went through a number of architectural diagrams and they looked",
    "start": "1744740",
    "end": "1752000"
  },
  {
    "text": "very different from each other but really at the core the concept is very similar where you basically have a user",
    "start": "1752000",
    "end": "1759470"
  },
  {
    "text": "interacting with a website the website is hosted in s3 through static bucket",
    "start": "1759470",
    "end": "1764570"
  },
  {
    "text": "hosting and that angular webs it makes API calls over the rest interface to an",
    "start": "1764570",
    "end": "1770270"
  },
  {
    "text": "API gateway that forwards them on to a lambda the lambda then processes the",
    "start": "1770270",
    "end": "1776690"
  },
  {
    "text": "business logic and gets its data from an RDS instance then the big difference between a vqc and step finder where",
    "start": "1776690",
    "end": "1783530"
  },
  {
    "text": "we're using batch and EMR but in both cases were processing data in s3 that is",
    "start": "1783530",
    "end": "1790670"
  },
  {
    "text": "put there by a DNA sequencing machine as a result of some lab process that we",
    "start": "1790670",
    "end": "1796370"
  },
  {
    "text": "had it was really key to be able to make this difference between the two applications for us we're in VGC we had",
    "start": "1796370",
    "end": "1803990"
  },
  {
    "text": "many small jobs and batch really made sense from an orchestration perspective from a scheduling perspective and EMR",
    "start": "1803990",
    "end": "1811790"
  },
  {
    "text": "made a lot more sense for snip finder where we have a few big jobs and we really need the MapReduce system to be",
    "start": "1811790",
    "end": "1817730"
  },
  {
    "text": "able to efficiently process that data making this this architectural difference between the two applications",
    "start": "1817730",
    "end": "1824650"
  },
  {
    "text": "was a really big big win for us Scot talked a little bit about glue and",
    "start": "1824650",
    "end": "1832429"
  },
  {
    "text": "I'll just say again it was introduced later on in the project we we found out",
    "start": "1832429",
    "end": "1840860"
  },
  {
    "text": "about it and looked at it for maybe using it as the usual pipeline process",
    "start": "1840860",
    "end": "1845980"
  },
  {
    "text": "but we needed some additional things that EMR offered like being able to",
    "start": "1845980",
    "end": "1851299"
  },
  {
    "text": "mount an EMR mount or in EFS mount but we still really liked the idea of having",
    "start": "1851299",
    "end": "1857750"
  },
  {
    "text": "a server less EMR cluster so we thought maybe we could use this for data cleanup",
    "start": "1857750",
    "end": "1863090"
  },
  {
    "text": "after a post processing after the processing is done maybe the metadata changes or a sample",
    "start": "1863090",
    "end": "1871160"
  },
  {
    "text": "needs to be moved around from one folder to another or two fields need to be merged together or something like that",
    "start": "1871160",
    "end": "1877010"
  },
  {
    "text": "in the post-processing step and that was what was where AWS glue was really useful and it gave us the ability to do",
    "start": "1877010",
    "end": "1885380"
  },
  {
    "text": "these things without having to spin up an entire cluster this is just a",
    "start": "1885380",
    "end": "1891710"
  },
  {
    "text": "screenshot of a zeppelin notepad we used for developing the PI spark code that",
    "start": "1891710",
    "end": "1897590"
  },
  {
    "text": "would run on AWS glue being able to run those queries interactively and come up",
    "start": "1897590",
    "end": "1904190"
  },
  {
    "text": "with what the pattern needed to be and what the software needed to look like was was very helpful for us as well",
    "start": "1904190",
    "end": "1911120"
  },
  {
    "text": "there are other technologies that have come up from AWS and the two years that",
    "start": "1911120",
    "end": "1917360"
  },
  {
    "text": "we've been working on this that we would like to be able to use as well not just glue but also service Aurora is",
    "start": "1917360",
    "end": "1926510"
  },
  {
    "text": "something high on our list to be able to move over to that so we don't have a long-running RDS instance that's really",
    "start": "1926510",
    "end": "1933110"
  },
  {
    "text": "high on our list but there's other technologies that get introduced all the time that we're looking forward to using",
    "start": "1933110",
    "end": "1940419"
  },
  {
    "text": "so I remember a couple months ago logging into the AWS console and seeing",
    "start": "1941380",
    "end": "1948559"
  },
  {
    "text": "this picture of all these ec2 instances running and I thought oh boy the bill on",
    "start": "1948559",
    "end": "1954530"
  },
  {
    "text": "this is going to be bad if these don't automatically turn off once this is done I better check in tomorrow morning to",
    "start": "1954530",
    "end": "1961190"
  },
  {
    "text": "make sure that these all go go way as we expect them to and they did in",
    "start": "1961190",
    "end": "1966740"
  },
  {
    "text": "fact the next morning I came in and I looked at this and they had all spun down as we had expected them to we don't",
    "start": "1966740",
    "end": "1974059"
  },
  {
    "text": "have to pay for that compute time when we're not using and we're only using the resources that we're that we need as we",
    "start": "1974059",
    "end": "1980780"
  },
  {
    "text": "need it one other thing is that you know",
    "start": "1980780",
    "end": "1989570"
  },
  {
    "text": "we mentioned that court ewa is the agricultural division of the merger",
    "start": "1989570",
    "end": "1994580"
  },
  {
    "text": "between Dow and DuPont and as a heritage DuPont employee we kind of built our",
    "start": "1994580",
    "end": "2000910"
  },
  {
    "text": "system around the DuPont data systems that we had and when we went through",
    "start": "2000910",
    "end": "2006070"
  },
  {
    "text": "this merger we noticed that Waldow has a lot of snipped data as well that needs to be incorporated into the",
    "start": "2006070",
    "end": "2012610"
  },
  {
    "text": "these systems and it was expected that we would be able to query this through the same application so normally that",
    "start": "2012610",
    "end": "2020620"
  },
  {
    "text": "might have been a very painful process but you know being able to move this",
    "start": "2020620",
    "end": "2025929"
  },
  {
    "text": "data around halfway across the country maybe poking holes and firewalls",
    "start": "2025929",
    "end": "2033480"
  },
  {
    "text": "negotiating server time to be able to do the compute for this it was actually really easy to be able to service this",
    "start": "2033480",
    "end": "2040480"
  },
  {
    "text": "need where we just ordered up a snowball had it shipped to dowse datacenter they",
    "start": "2040480",
    "end": "2045910"
  },
  {
    "text": "loaded up the data and then shipped it back to AWS it was imported into our s3",
    "start": "2045910",
    "end": "2052179"
  },
  {
    "text": "bucket we created an ad hoc EMR cluster to do the transformation on that data",
    "start": "2052179",
    "end": "2057310"
  },
  {
    "text": "and put it into our format add the right metadata and we had it in our application pretty pretty easily so it",
    "start": "2057310",
    "end": "2065740"
  },
  {
    "text": "was really a really good win for us to be able to do that and we didn't have to",
    "start": "2065740",
    "end": "2071080"
  },
  {
    "text": "deal with these resource contentions where you know you don't merge with a multi-billion dollar company every day",
    "start": "2071080",
    "end": "2077260"
  },
  {
    "text": "you don't really plan your data center around being able to do this processing so being able to spin that up on demand",
    "start": "2077260",
    "end": "2084580"
  },
  {
    "text": "was was really helpful finally the",
    "start": "2084580",
    "end": "2090429"
  },
  {
    "text": "business impacts have been grade we don't have the resource contention that",
    "start": "2090429",
    "end": "2096310"
  },
  {
    "text": "used to have with our arm front system there might be situations where different user groups are using the same",
    "start": "2096310",
    "end": "2103770"
  },
  {
    "text": "the same resource at the same time and it might take an additional amount of",
    "start": "2103770",
    "end": "2108850"
  },
  {
    "text": "time to do that processing based on what other jobs are going on in AWS we don't have that because there those resources",
    "start": "2108850",
    "end": "2115930"
  },
  {
    "text": "are separated we don't have to deal with that contention I also sleep better at night because I don't have to worry",
    "start": "2115930",
    "end": "2122230"
  },
  {
    "text": "about our data being lost in a catastrophic event because all of it is",
    "start": "2122230",
    "end": "2128830"
  },
  {
    "text": "stored in one physical location with Amazon it's stored in multiple locations and we don't have to worry about nearly",
    "start": "2128830",
    "end": "2137350"
  },
  {
    "text": "as much having a major catastrophe like that and finally being able to enable",
    "start": "2137350",
    "end": "2143950"
  },
  {
    "text": "growth for our laboratory partners being able to say you know you can do this",
    "start": "2143950",
    "end": "2149140"
  },
  {
    "text": "experiment you can run these samples you can you can do these things and the data processing isn't going to be a factor",
    "start": "2149140",
    "end": "2154660"
  },
  {
    "text": "we'll be able to handle it we'll be able to store that data once it's done is a really good feeling as well so I",
    "start": "2154660",
    "end": "2162730"
  },
  {
    "text": "appreciate you all taking the time to listen to us and thank you for coming and we'll have any questions that you",
    "start": "2162730",
    "end": "2169300"
  },
  {
    "text": "have now [Applause]",
    "start": "2169300",
    "end": "2176530"
  },
  {
    "text": "any questions",
    "start": "2176530",
    "end": "2179920"
  },
  {
    "text": "yeah so the bioinformatics tools the",
    "start": "2188410",
    "end": "2193960"
  },
  {
    "text": "open-source tools are assumed a file system and need to interact with things on a regular file system so we needed to",
    "start": "2193960",
    "end": "2201700"
  },
  {
    "text": "be able to mount that file system across the cluster for each one of the each one",
    "start": "2201700",
    "end": "2207490"
  },
  {
    "text": "of the cluster nodes that it had that was really the driving part part of that was the off-the-shelf bioinformatic tool",
    "start": "2207490",
    "end": "2216119"
  },
  {
    "text": "yes Becky no we have not had to work",
    "start": "2216450",
    "end": "2229299"
  },
  {
    "text": "with other sequencing data we've just strictly start with Illumina yeah we had",
    "start": "2229299",
    "end": "2243130"
  },
  {
    "text": "a pretty good understanding that bowtie was the right aligner for us given the legacy system that we had so we just",
    "start": "2243130",
    "end": "2250329"
  },
  {
    "text": "went with that",
    "start": "2250329",
    "end": "2252809"
  },
  {
    "text": "all right other questions yes",
    "start": "2257300",
    "end": "2262150"
  },
  {
    "text": "asking about glue compacting files dealing with small files yes I we did",
    "start": "2273570",
    "end": "2279550"
  },
  {
    "text": "run into that where there was issues and we needed to find the right balance between small and large size sorry I",
    "start": "2279550",
    "end": "2287590"
  },
  {
    "text": "can't give you any better details than that yeah",
    "start": "2287590",
    "end": "2295890"
  },
  {
    "text": "yes so for the sequencing data we do keep it hot in s3 we don't do any",
    "start": "2311599",
    "end": "2319440"
  },
  {
    "text": "glacier storage for that at this time I think that was kind of out the storage",
    "start": "2319440",
    "end": "2325650"
  },
  {
    "text": "of data off sequencing machines was outside of this project was in a different projects actually but yeah the",
    "start": "2325650",
    "end": "2332970"
  },
  {
    "text": "decision was made in that project to keep it hot in s3 for the duration I",
    "start": "2332970",
    "end": "2338510"
  },
  {
    "text": "think there's some regulatory compliance to on how long we have to keep that raw data around and we keep xx accessible",
    "start": "2338510",
    "end": "2345779"
  },
  {
    "text": "for for compliance issues yes",
    "start": "2345779",
    "end": "2352279"
  },
  {
    "text": "is there any particular reason why you want to read from a virus instead of put",
    "start": "2363760",
    "end": "2370100"
  },
  {
    "text": "erasing and array in the media file into an s3 bucket mainly we were using EFS",
    "start": "2370100",
    "end": "2379850"
  },
  {
    "text": "because the like the bowtie liner was requiring that it worked with a file",
    "start": "2379850",
    "end": "2385370"
  },
  {
    "text": "file system object so that's that was the main driver for that yeah an open",
    "start": "2385370",
    "end": "2390620"
  },
  {
    "text": "source tool were using to process the data requires it to live on a file system it wasn't s3 compatible okay what's the",
    "start": "2390620",
    "end": "2397520"
  },
  {
    "text": "main issue yeah",
    "start": "2397520",
    "end": "2406180"
  },
  {
    "text": "right I think on this one we actually deleted after the processing is complete right yeah the internment files that",
    "start": "2411619",
    "end": "2417150"
  },
  {
    "text": "we're writing and deleted right after right afterwards I think there's like a cleanup process that runs like if if the",
    "start": "2417150",
    "end": "2423240"
  },
  {
    "text": "cluster dies for some reason and that cleanup process doesn't run we have some some triggers that go in and and clean",
    "start": "2423240",
    "end": "2429630"
  },
  {
    "text": "up that file so on the EFS thing where you guys actually mounting the EFS",
    "start": "2429630",
    "end": "2435420"
  },
  {
    "text": "volumes within the batch containers and then consuming the data in there we",
    "start": "2435420",
    "end": "2440760"
  },
  {
    "text": "mount it from the like we create a custom ami for the outer for the outer",
    "start": "2440760",
    "end": "2447210"
  },
  {
    "text": "EC to mount it there and then pass it down I know we did look at trying to",
    "start": "2447210",
    "end": "2453960"
  },
  {
    "text": "mount it within our docker containers and I ran into some roadblocks there yeah yeah",
    "start": "2453960",
    "end": "2462650"
  },
  {
    "text": "yep we store the metadata in RDS database to be able to store it to",
    "start": "2469120",
    "end": "2475150"
  },
  {
    "text": "access it sure you want to talk about",
    "start": "2475150",
    "end": "2482830"
  },
  {
    "text": "that yeah yes so as the data processes using these open source kind of specific",
    "start": "2482830",
    "end": "2488470"
  },
  {
    "text": "DNA tools that we're talking about um it writes lots and lots of intermediate files to s3 in this case as it's doing",
    "start": "2488470",
    "end": "2494110"
  },
  {
    "text": "the processing and so just using kind of the native file names or file naming",
    "start": "2494110",
    "end": "2499690"
  },
  {
    "text": "convention even as we were going through it we noticed that it would start off performant and then very quickly",
    "start": "2499690",
    "end": "2506080"
  },
  {
    "text": "throughout the process decrease so after some investigation we did figure out",
    "start": "2506080",
    "end": "2511510"
  },
  {
    "text": "that that was the s3 hot spotting so as we write the files to s3 now I think we",
    "start": "2511510",
    "end": "2517120"
  },
  {
    "text": "put a 16 digit random hash out in front of each file name and that helps to",
    "start": "2517120",
    "end": "2523630"
  },
  {
    "text": "distribute it randomly across all the s3 nodes as we're writing those files and it avoids the hot spotting issues",
    "start": "2523630",
    "end": "2529140"
  },
  {
    "text": "basically the problem is the the key on the object in s3 that determines what",
    "start": "2529140",
    "end": "2536770"
  },
  {
    "text": "nodes in AWS that object lives on and if you're writing a number of files with",
    "start": "2536770",
    "end": "2542620"
  },
  {
    "text": "very similar prefixes they all land on the same node so when you try to access",
    "start": "2542620",
    "end": "2547630"
  },
  {
    "text": "all of those it hotspots that one node so you prefix the key with something",
    "start": "2547630",
    "end": "2553210"
  },
  {
    "text": "random and deterministic like a hash and that distributes it across the many nodes in s3 so that you don't hit that",
    "start": "2553210",
    "end": "2559780"
  },
  {
    "text": "issue no not in our case it wasn't yeah",
    "start": "2559780",
    "end": "2568900"
  },
  {
    "text": "we weren't able to do that yeah",
    "start": "2568900",
    "end": "2572910"
  },
  {
    "text": "not that we're aware of yes sir",
    "start": "2575930",
    "end": "2585130"
  },
  {
    "text": "yes that's right yep",
    "start": "2595729",
    "end": "2601640"
  },
  {
    "text": "that is that other project that I mentioned yeah outside of the scope of this project so there's another project",
    "start": "2614930",
    "end": "2629850"
  },
  {
    "text": "that it takes that raw sequence data and converts it to the fast queue format so",
    "start": "2629850",
    "end": "2635250"
  },
  {
    "text": "we received the data in our pipelines in the s3 bucket in that format ready to process because there's other genomic",
    "start": "2635250",
    "end": "2641520"
  },
  {
    "text": "work that happens in in court Ava so we're not the only consumers of that data so there's kind of a standard",
    "start": "2641520",
    "end": "2647370"
  },
  {
    "text": "pipeline to convert the raw data the last I heard from that project was that they were not streaming it directly from",
    "start": "2647370",
    "end": "2653490"
  },
  {
    "text": "the sequencing machines they were doing some intermediate work rather than",
    "start": "2653490",
    "end": "2658800"
  },
  {
    "text": "hooking the the sequencer directly up to a3 yep yeah we ran into the same issue",
    "start": "2658800",
    "end": "2666930"
  },
  {
    "text": "in that project and and yeah there's I know that Luminas looking at some native s3 tools to stream it out but those",
    "start": "2666930",
    "end": "2673620"
  },
  {
    "text": "aren't ready yet yeah",
    "start": "2673620",
    "end": "2677150"
  },
  {
    "text": "I'm sorry I didn't hear that yeah yeah",
    "start": "2682730",
    "end": "2695820"
  },
  {
    "text": "so the question was how do we balance cost versus kind of user demand in SL A's so that's kind of something we try",
    "start": "2695820",
    "end": "2702540"
  },
  {
    "text": "to put back on the the business to make that decision so we run enough of these and we've got a good idea of how many",
    "start": "2702540",
    "end": "2707910"
  },
  {
    "text": "nodes and what they cost and then the time it'll take to run these things they if somebody wants to submit a big job or",
    "start": "2707910",
    "end": "2714780"
  },
  {
    "text": "big series of jobs we can give them an estimate on what that's going to cost and they need to go back to their boss or their their sponsor and justify that",
    "start": "2714780",
    "end": "2722790"
  },
  {
    "text": "with them so we're as the IT department we make this all available to them justifying the cost and how much they",
    "start": "2722790",
    "end": "2728760"
  },
  {
    "text": "want to spend on on certain projects or certain data is is on the business side to figure out",
    "start": "2728760",
    "end": "2735950"
  },
  {
    "text": "I yeah have we had any problems with s3 metadata and as I said before not yet",
    "start": "2753690",
    "end": "2760050"
  },
  {
    "text": "not that we're aware of any other",
    "start": "2760050",
    "end": "2766770"
  },
  {
    "text": "questions all right well thanks for everyone's time today and we'll hang out",
    "start": "2766770",
    "end": "2773070"
  },
  {
    "text": "if there's any any other questions after that yeah thanks [Applause]",
    "start": "2773070",
    "end": "2781019"
  }
]