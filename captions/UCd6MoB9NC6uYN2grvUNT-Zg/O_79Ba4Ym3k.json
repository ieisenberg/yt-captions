[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": "um so happy to start anywhere who wants to give us the first question okay there",
    "start": "0",
    "end": "5640"
  },
  {
    "text": "you go so what's the equivalent of DPMS job for",
    "start": "5640",
    "end": "13920"
  },
  {
    "text": "Postgres so I think all that Dennis Lussier to help answer pretty good so",
    "start": "13920",
    "end": "32250"
  },
  {
    "text": "this peak this PG agent and version four of that is actually pretty good in modern we are working my own personal",
    "start": "32250",
    "end": "39149"
  },
  {
    "text": "favorite is PG cron so PG underbar cron we're actually in our roadmap so for",
    "start": "39149",
    "end": "46020"
  },
  {
    "text": "later this year we're gonna try to incorporate a derivative that into Aurora Postgres that fixes some security",
    "start": "46020",
    "end": "52050"
  },
  {
    "text": "stuff and some so there's a little bit of engineering that we need to do without using that directly but you can use PG agent PG cron looks separately if",
    "start": "52050",
    "end": "60930"
  },
  {
    "text": "you are running Postgres by itself and we're interested in that as well separately where we're working on",
    "start": "60930",
    "end": "67560"
  },
  {
    "text": "another feature which is integration with what do you call them the",
    "start": "67560",
    "end": "73400"
  },
  {
    "text": "serverless stuff with lambda functions right so that from PL sequel you can",
    "start": "73400",
    "end": "78659"
  },
  {
    "text": "call lambda functions so a lot of the kind of things that you're doing with these scheduled jobs and stuff like that",
    "start": "78659",
    "end": "84450"
  },
  {
    "text": "if you had a trigger that you could kick off something in lambda you wouldn't need to do that as much and other people",
    "start": "84450",
    "end": "90150"
  },
  {
    "text": "who are using you know application server schedule or stuff like that so to kind of do that out of the tier so",
    "start": "90150",
    "end": "96079"
  },
  {
    "text": "scheduling in the long run I don't really like doing in the database but we are adding PG cron and there those are",
    "start": "96079",
    "end": "104729"
  },
  {
    "text": "the other options as far as I know agent is not on AWS now so those are the two",
    "start": "104729",
    "end": "111570"
  },
  {
    "text": "community options for doing that BG agent and PG cron they're probably a couple more but those are the two that",
    "start": "111570",
    "end": "116700"
  },
  {
    "text": "that have my attention that's so to be clear you could use PG a",
    "start": "116700",
    "end": "121890"
  },
  {
    "start": "120000",
    "end": "260000"
  },
  {
    "text": "mpg cron if you're running self-managed Postgres on ec2 that we haven't incorporated equivalence into RDS for",
    "start": "121890",
    "end": "129060"
  },
  {
    "text": "Rory yet yes but we will soon how do you handle",
    "start": "129060",
    "end": "135920"
  },
  {
    "text": "connections greater than 200 meaning number of connections greater than 200 so you can throw hardware at it and just",
    "start": "135920",
    "end": "141920"
  },
  {
    "text": "scale up to a bigger instance which is kind of the flip answer so for those of",
    "start": "141920",
    "end": "148070"
  },
  {
    "text": "you don't know Postgres has a process per connection model all right so every connection to a post for instance whether it's self-managed",
    "start": "148070",
    "end": "155060"
  },
  {
    "text": "RDS Aurora or on-premise every connection causes the post res instance",
    "start": "155060",
    "end": "160070"
  },
  {
    "text": "to spawn a separate OS process on the the database host which takes up CPU and",
    "start": "160070",
    "end": "165470"
  },
  {
    "text": "memory and other resources you can control how much memory each connection takes with various parameters like work",
    "start": "165470",
    "end": "171290"
  },
  {
    "text": "them for example and in some cases you won't work them to be bigger because you might be doing big sorts you might have",
    "start": "171290",
    "end": "177590"
  },
  {
    "text": "quarries that or building indexes or do it you know I'm doing queries on large tables with group eyes etc but if you",
    "start": "177590",
    "end": "184580"
  },
  {
    "text": "set the memory parameters big then that means that you're gonna run out of memory when you have a larger number of connections you can set the memory",
    "start": "184580",
    "end": "191270"
  },
  {
    "text": "parameters smaller and you'll fit more connections at the cost of whether you know it might be cost a performance",
    "start": "191270",
    "end": "197060"
  },
  {
    "text": "based upon what that memory might be used for a lot of customers when they want to support thousands of connections",
    "start": "197060",
    "end": "202519"
  },
  {
    "text": "to a postgrads instance will usually put a Pooler in front of of either PG pool or PG bouncer or the ones we typically",
    "start": "202519",
    "end": "208280"
  },
  {
    "text": "see today and RDS and Aurora you need to setup your own connection pooling",
    "start": "208280",
    "end": "213700"
  },
  {
    "text": "meaning a lot of customers they'll set up a separate ec2 instance with PG pool on it point their apps at PG pool and",
    "start": "213700",
    "end": "220340"
  },
  {
    "text": "then point that configuration at the actual running instance that means you",
    "start": "220340",
    "end": "225500"
  },
  {
    "text": "have to run your own puller and so we've gotten lots of customer requests to build some sort of managed connection",
    "start": "225500",
    "end": "231980"
  },
  {
    "text": "pooling into our managed Postgres instances and we are looking at that we have some ideas on how we can solve that",
    "start": "231980",
    "end": "238670"
  },
  {
    "text": "problem was not solved today having said all that we do have customers or",
    "start": "238670",
    "end": "243950"
  },
  {
    "text": "hardware at it the biggest instance we support in the r4 class has almost half",
    "start": "243950",
    "end": "249620"
  },
  {
    "text": "a terabyte of main memory we're adding support now for the r5 that's already available in some regions the r5 24",
    "start": "249620",
    "end": "255590"
  },
  {
    "text": "Excel has three-quarters of terabytes of main memory so",
    "start": "255590",
    "end": "260530"
  },
  {
    "start": "260000",
    "end": "380000"
  },
  {
    "text": "the question is that with Aurora already yes both well RTS runs everywhere on our 5s already our 5s are being rolled out",
    "start": "261530",
    "end": "268220"
  },
  {
    "text": "through the Aurora post gracefully now you'll see it in some regions already but like I said you can throw hardware",
    "start": "268220",
    "end": "273830"
  },
  {
    "text": "at it if you really need the memory and the CPU to support you know hundreds or thousands of connections you can",
    "start": "273830",
    "end": "279110"
  },
  {
    "text": "certainly do it works a little better in Aurora just because we've done some tweaks to make it scale better yeah hi",
    "start": "279110",
    "end": "296979"
  },
  {
    "text": "I'll say that again I'm David wine I work with Kevin I'm a principal engineer",
    "start": "296979",
    "end": "302509"
  },
  {
    "text": "and director of engine development for or Postgres what Kevin said is",
    "start": "302509",
    "end": "307759"
  },
  {
    "text": "absolutely correct connections in Postgres use a lot of memory I will",
    "start": "307759",
    "end": "313220"
  },
  {
    "text": "throw a plugin for Aurora we did some work inside the Postgres engine around",
    "start": "313220",
    "end": "319789"
  },
  {
    "text": "areas where Postgres bottlenecks with large number of connections and even if",
    "start": "319789",
    "end": "325340"
  },
  {
    "text": "you run benchmarks with Postgres you'll see as you keep adding more concurrent workers you actually start getting less",
    "start": "325340",
    "end": "331820"
  },
  {
    "text": "throughput so it doesn't even Plateau it really hits a knee and drops off because the contention points becomes so severe",
    "start": "331820",
    "end": "339199"
  },
  {
    "text": "that as you at work it just makes everything slower we don't have that with Aurora you we in our benchmarks can",
    "start": "339199",
    "end": "345800"
  },
  {
    "text": "run up to 4,000 connections that's not actually a limit that's just how high",
    "start": "345800",
    "end": "351020"
  },
  {
    "text": "I've run them in the benchmark and we don't hit that knee you know you'll plateau it throughput at",
    "start": "351020",
    "end": "356840"
  },
  {
    "text": "some point but in summary we've done a lot of things inside the engine to",
    "start": "356840",
    "end": "362000"
  },
  {
    "text": "remove the contention points with a high number of concurrent users so Aurora",
    "start": "362000",
    "end": "368590"
  },
  {
    "text": "does better with large number of connections than RDS Postgres",
    "start": "368590",
    "end": "374860"
  },
  {
    "text": "all right who's next great so the",
    "start": "377639",
    "end": "389800"
  },
  {
    "start": "380000",
    "end": "470000"
  },
  {
    "text": "question is does performance insights come with a starter template or best practices on how to tweak or tune",
    "start": "389800",
    "end": "396490"
  },
  {
    "text": "Postgres so today performance insights you turn it on for your instance and you start",
    "start": "396490",
    "end": "402130"
  },
  {
    "text": "looking at the display and it'll show you just the top sequel I think by default it sorts by sequel statement by",
    "start": "402130",
    "end": "407650"
  },
  {
    "text": "load and there's different ways you can resort and redisplay the data today it",
    "start": "407650",
    "end": "414760"
  },
  {
    "text": "doesn't give any recommendations it doesn't tell you gee it looks like this query is kind of going sideways you",
    "start": "414760",
    "end": "419949"
  },
  {
    "text": "should add an index or you know it doesn't tell you anything on the next steps so out of the box it will just",
    "start": "419949",
    "end": "427180"
  },
  {
    "text": "show you the things that are consuming the most resources the team that built",
    "start": "427180",
    "end": "432220"
  },
  {
    "text": "performance insights has all kinds of interesting plans for ways that we might be able to make recommendations and give",
    "start": "432220",
    "end": "438669"
  },
  {
    "text": "advice on what performance insights is seeing but you know today that's that's",
    "start": "438669",
    "end": "444280"
  },
  {
    "text": "the the baseline now you know there's lots of blog posts that we put out on things related to say Oracle to Postgres",
    "start": "444280",
    "end": "452110"
  },
  {
    "text": "or Oracle to R or post res migrations and I think there are some on performance tuning in general but",
    "start": "452110",
    "end": "458919"
  },
  {
    "text": "there's nothing built into the performance insights tool is that what you're asking is there something built into the tool itself right so the",
    "start": "458919",
    "end": "470710"
  },
  {
    "start": "470000",
    "end": "545000"
  },
  {
    "text": "metrics are still in cloud watch which metrics are you thinking of transaction",
    "start": "470710",
    "end": "478150"
  },
  {
    "text": "repre yeah those metrics are more instance level because P I really looks",
    "start": "478150",
    "end": "484419"
  },
  {
    "text": "at it the way P works as it samples your instance and looks at what's running every second and so it just sees what",
    "start": "484419",
    "end": "490750"
  },
  {
    "text": "queries are running what they're waiting on and then it builds up this time model view of your instance over time and it's",
    "start": "490750",
    "end": "497740"
  },
  {
    "text": "only recently that recently that they added counter based metric so it actually looks at how many times something has happened like commits for",
    "start": "497740",
    "end": "503860"
  },
  {
    "text": "example so P I now start is starting to show counter based stuff as well but the",
    "start": "503860",
    "end": "509650"
  },
  {
    "text": "original implementation was all about just sample the instance just see what's look what's happening and then build up this",
    "start": "509650",
    "end": "515919"
  },
  {
    "text": "graphical view of what's happening in the instance over time you can also see",
    "start": "515919",
    "end": "521020"
  },
  {
    "text": "why it's easy for piata store that data just as these one-second samples just stores it for seven day seven day",
    "start": "521020",
    "end": "526420"
  },
  {
    "text": "rolling window but the counter based metrics is probably going to give more of what you're looking for so you should",
    "start": "526420",
    "end": "532810"
  },
  {
    "text": "take a look because I think we only launched that the kind of a stuff like last week so it's all brand-new stuff in",
    "start": "532810",
    "end": "539050"
  },
  {
    "text": "performance insights for counter based metrics it does everything for",
    "start": "539050",
    "end": "546700"
  },
  {
    "start": "545000",
    "end": "670000"
  },
  {
    "text": "performance insights go in the cloud watch I don't think everything from P I goes into cloud watch enhance monitoring",
    "start": "546700",
    "end": "551800"
  },
  {
    "text": "the OS stats does P I does not VI does have an API you can pull it out yourself",
    "start": "551800",
    "end": "557650"
  },
  {
    "text": "and do whatever you want with it but it doesn't automatically flow in the cloud watch so some of some of the metrics but",
    "start": "557650",
    "end": "573940"
  },
  {
    "text": "not the individual sequel statement data right the brand new counter base metrics",
    "start": "573940",
    "end": "584980"
  },
  {
    "text": "may they flow okay thanks",
    "start": "584980",
    "end": "590910"
  },
  {
    "text": "so the question is about Active Directory Integration which exists now in redshift why doesn't",
    "start": "611810",
    "end": "619020"
  },
  {
    "text": "exist in RDS post Crescent or Postgres just because the calendar hasn't moved",
    "start": "619020",
    "end": "624240"
  },
  {
    "text": "forward far enough yet it is it is it is in our in our roadmap for later this",
    "start": "624240",
    "end": "630480"
  },
  {
    "text": "year you saw if you saw me this morning you know I talked a little bit about the integration we did with I am auth",
    "start": "630480",
    "end": "636470"
  },
  {
    "text": "identity access management authentication which we launched for both the word over and RDS earlier this",
    "start": "636470",
    "end": "642390"
  },
  {
    "text": "year that was a step along the path of making it easy for customers to integrate with their existing on",
    "start": "642390",
    "end": "648990"
  },
  {
    "text": "typically on-premises Active Directory domains and so we are adding we are",
    "start": "648990",
    "end": "655890"
  },
  {
    "text": "planning to add that capability later this year for both our DSN aurora and if you look more broadly at the other RDS",
    "start": "655890",
    "end": "661920"
  },
  {
    "text": "engines as well depending on the engine",
    "start": "661920",
    "end": "665510"
  },
  {
    "text": "the status of multi master we could spend the next 45 hours talking about that huh",
    "start": "668899",
    "end": "674510"
  },
  {
    "start": "670000",
    "end": "975000"
  },
  {
    "text": "so I'll ask a question back first what do you mean by multi master so",
    "start": "674510",
    "end": "681420"
  },
  {
    "text": "bi-directional replication that's one way to implement multi master what what problem are you trying to solve with",
    "start": "681420",
    "end": "687120"
  },
  {
    "text": "multi master",
    "start": "687120",
    "end": "689750"
  },
  {
    "text": "okay well I'll tell you what kinds of problems customers other customers have told us they're trying to solve and",
    "start": "693040",
    "end": "699870"
  },
  {
    "text": "really depends there's kind of two types of multi-master or two phases of multi-master we're working on so one is",
    "start": "699870",
    "end": "707560"
  },
  {
    "text": "what we call multi master in region where you you know think of the aurora architecture you have the shared storage",
    "start": "707560",
    "end": "713590"
  },
  {
    "text": "volume across multiple AZ's and you have a read/write instance and a bunch of read-only instances all sharing full",
    "start": "713590",
    "end": "719770"
  },
  {
    "text": "access to the same database on storage well what if you could make those instances all of them readwrite and so",
    "start": "719770",
    "end": "725710"
  },
  {
    "text": "just one of them so that's multi master in region multiple masters one database how do you",
    "start": "725710",
    "end": "731500"
  },
  {
    "text": "make them play nice so that you can do full-on OLTP transactions to the whole database from any master and not corrupt",
    "start": "731500",
    "end": "738640"
  },
  {
    "text": "things and not make you know confuse yourself and your users in your code so",
    "start": "738640",
    "end": "743670"
  },
  {
    "text": "that's the multi master in region project now that started first with the roar of my sequel something we announced",
    "start": "743670",
    "end": "750640"
  },
  {
    "text": "but more than a year ago and so where my sequel has been running a preview they",
    "start": "750640",
    "end": "756220"
  },
  {
    "text": "have plans to go GA sometime this year with multi master in region or postgrads",
    "start": "756220",
    "end": "761620"
  },
  {
    "text": "we started the whole project more than two years after Aurora my sequel so we're behind them in terms of timeline",
    "start": "761620",
    "end": "767830"
  },
  {
    "text": "for delivering big features like multi master not an easy thing to implement so",
    "start": "767830",
    "end": "773200"
  },
  {
    "text": "our target right now is sometime next year for multi master in region the",
    "start": "773200",
    "end": "778420"
  },
  {
    "text": "other multi master is multi master multi region which by definition is not one one copy of the database it's copy in",
    "start": "778420",
    "end": "785770"
  },
  {
    "text": "this region and a copy in another region and now I've got reads and writes going on in both places how do you make sure",
    "start": "785770",
    "end": "791440"
  },
  {
    "text": "that they don't step on each other when you have much higher latency all right deletes it between regions and so really",
    "start": "791440",
    "end": "799810"
  },
  {
    "text": "the first phase of that project is what we call global database you can see that already there for R or my sequel and",
    "start": "799810",
    "end": "805930"
  },
  {
    "text": "global database this first phase is simply cross region replication where the Aurora storage nodes will replicate",
    "start": "805930",
    "end": "812800"
  },
  {
    "text": "or copy their their log changes from the source regions to the target region and",
    "start": "812800",
    "end": "818590"
  },
  {
    "text": "there was an Aurora storage cluster in the target that receives the log changes and applies them to database pages and keeps a copy hydrated for read traffic",
    "start": "818590",
    "end": "826480"
  },
  {
    "text": "initially it'll be cross region read replicas that's project that's coming sometime this year for Postgres as well",
    "start": "826480",
    "end": "832980"
  },
  {
    "text": "but you know the the next phase where it's full-on read/write access from both sides is something that's you know",
    "start": "832980",
    "end": "839339"
  },
  {
    "text": "generally on the plans for next year for both overall my sequin or Postgres a lot",
    "start": "839339",
    "end": "844510"
  },
  {
    "text": "of details to work out on how you manage conflict resolution in it in that kind of multi master multi-region environment",
    "start": "844510",
    "end": "853180"
  },
  {
    "text": "now the use cases that we see for multi master in region are typically high",
    "start": "853180",
    "end": "858220"
  },
  {
    "text": "availability I want to just keep writing to my database if a node fails and you",
    "start": "858220",
    "end": "863709"
  },
  {
    "text": "know high availability or even fast failover isn't fast enough I just want to be connected to and write to one and",
    "start": "863709",
    "end": "870070"
  },
  {
    "text": "if it fails want to keep writing to the other and if it's truly it's segmented like that we're one of them isn't really being used and there's not a lot of",
    "start": "870070",
    "end": "876070"
  },
  {
    "text": "conflicts to worry about the other is where our customers want to scale writes beyond the limits of even the biggest",
    "start": "876070",
    "end": "882399"
  },
  {
    "text": "head node we can give them beyond an r5 24 XL with 96 V CPUs that said or they that we have a workload that just has to",
    "start": "882399",
    "end": "888730"
  },
  {
    "text": "scale bigger than that then they need multiple head nodes multiple big head notes banging on the same database and",
    "start": "888730",
    "end": "894069"
  },
  {
    "text": "then they need to worry about segmenting their workloads so it doesn't attack the same data at the same time because if",
    "start": "894069",
    "end": "901300"
  },
  {
    "text": "you do that if you have too much contention between knows it will not scale because you're just gonna thrash",
    "start": "901300",
    "end": "906699"
  },
  {
    "text": "sending the data back and forth there's no de updates the data isn't that one than this one in that one so you always",
    "start": "906699",
    "end": "912220"
  },
  {
    "text": "need to partition your access in these multi master environments so that's kind",
    "start": "912220",
    "end": "918670"
  },
  {
    "text": "of the workloads we see for multi master in region multi master multi-region you know it's it's usually about latency low",
    "start": "918670",
    "end": "925000"
  },
  {
    "text": "latency for customer access so those workloads really need to be segmented as well but it might be wary of customers",
    "start": "925000",
    "end": "931329"
  },
  {
    "text": "who you know want to access their data in Europe and I want to have one big database and customers in the u.s. want",
    "start": "931329",
    "end": "937480"
  },
  {
    "text": "to access their data as well it can usually be segmented well so they're not going to step on each other if I go",
    "start": "937480",
    "end": "942670"
  },
  {
    "text": "travel in Europe and I log on to a system there maybe my data will do them you know it'll be replicated over so it'll be available in the European copy",
    "start": "942670",
    "end": "949000"
  },
  {
    "text": "as well but most of the time there won't be conflict so multi-master multi-region",
    "start": "949000",
    "end": "954310"
  },
  {
    "text": "makes sense for use cases like that or also they happen to be a dr too after recovery fail over target it's",
    "start": "954310",
    "end": "961200"
  },
  {
    "text": "like I said we could talk for a lot longer about multi-master but that's just kind of a high level state of the",
    "start": "961200",
    "end": "966600"
  },
  {
    "text": "world for multi-master you know or other questions yeah so a",
    "start": "966600",
    "end": "979020"
  },
  {
    "start": "975000",
    "end": "1030000"
  },
  {
    "text": "very specific question is there a date for performance insights and gov cloud for Aurora Postgres there probably is I",
    "start": "979020",
    "end": "986760"
  },
  {
    "text": "don't know what the date is guess it's sometime this year but I only",
    "start": "986760",
    "end": "992400"
  },
  {
    "text": "want to promise that I'd have to check with the team more generally we are rolling out performance insights beyond",
    "start": "992400",
    "end": "999060"
  },
  {
    "text": "or or Postgres where it started to the other RDS and Aurora engines and so it's",
    "start": "999060",
    "end": "1004640"
  },
  {
    "text": "available in RDS for post-grad starting with post res 10 and now post goes 11 I",
    "start": "1004640",
    "end": "1009830"
  },
  {
    "text": "don't think we've launched it for or my sequel RDS my sequel etc so we're rolling out across the engines and",
    "start": "1009830",
    "end": "1015890"
  },
  {
    "text": "across the regions including gov cloud eventually but I don't know specific",
    "start": "1015890",
    "end": "1021530"
  },
  {
    "text": "dates for for what when it will become available in gov cloud yep",
    "start": "1021530",
    "end": "1029230"
  },
  {
    "text": "question is about all the RDS engines there's technically seven of them which",
    "start": "1042179",
    "end": "1048610"
  },
  {
    "text": "are the top ones what's the adoption rates etc so you know we don't disclose specific numbers about those I can't say",
    "start": "1048610",
    "end": "1055840"
  },
  {
    "text": "that they're all growing healthy businesses some people think that I got this question earlier today so how do",
    "start": "1055840",
    "end": "1062409"
  },
  {
    "text": "you convince people to move off of Oracle to postgrads and I said we don't if you want to run Oracle in AWS we'd",
    "start": "1062409",
    "end": "1068620"
  },
  {
    "text": "love to have you we have a very healthy RDS Oracle business very healthy RDS sequel server business it's yes when",
    "start": "1068620",
    "end": "1074980"
  },
  {
    "text": "customers come to us and say I want to get off of Oracle then we can help them but we don't go out there campaigning to",
    "start": "1074980",
    "end": "1080350"
  },
  {
    "text": "convince them they should move off of Oracle so they're all healthy growing businesses what I can say is that Aurora",
    "start": "1080350",
    "end": "1086950"
  },
  {
    "text": "is the fastest growing service in the history of Amazon or a history of AWS and that was before we launched a world",
    "start": "1086950",
    "end": "1094630"
  },
  {
    "text": "Postgres aurora my sequel by itself launched in July of 2015 and over the",
    "start": "1094630",
    "end": "1100240"
  },
  {
    "text": "next couple years became the fastest growing thing we ever launched now it's not fair to measure in the first month of growth of course after you know gets",
    "start": "1100240",
    "end": "1107470"
  },
  {
    "text": "up to his you know a sizable level it's been the fastest growing or where postgrads has accelerated to growth",
    "start": "1107470",
    "end": "1114429"
  },
  {
    "text": "let's put it that way but they're all growing businesses and like I said growing in healthy",
    "start": "1114429",
    "end": "1120639"
  },
  {
    "text": "businesses where if you just look at what we launched for each of the those",
    "start": "1120639",
    "end": "1125649"
  },
  {
    "text": "engines we you know we continue to accelerate the number of services we launch for each engine that's because we",
    "start": "1125649",
    "end": "1131590"
  },
  {
    "text": "keep on getting more customer demand for new features in each of the seven engines so we're not slowing down we're",
    "start": "1131590",
    "end": "1138220"
  },
  {
    "text": "speeding up pretty much for all seven engines more questions",
    "start": "1138220",
    "end": "1146970"
  },
  {
    "text": "trying to think what questions you didn't ask that you should be asking me you should be asking me about outbound",
    "start": "1147750",
    "end": "1154840"
  },
  {
    "start": "1150000",
    "end": "1270000"
  },
  {
    "text": "replication for Aurora Postgres hi hi Dave well let me tell you Dave let me",
    "start": "1154840",
    "end": "1166420"
  },
  {
    "text": "tell you about a Ponting replication first we've talked about it multiple times today you know standard Postgres",
    "start": "1166420",
    "end": "1171910"
  },
  {
    "text": "supports lots of ways to replicate how to the postcards instance RDS postcards we added support for replication slots a",
    "start": "1171910",
    "end": "1178090"
  },
  {
    "text": "standard postcards feature several years ago and database migration service sits on top of that feature when we launched",
    "start": "1178090",
    "end": "1185110"
  },
  {
    "text": "or postcards we launched it without any support group on replication primarily because we rewrote the logging and",
    "start": "1185110",
    "end": "1191560"
  },
  {
    "text": "storage layers and that meant that we took away replication slots and we had to reimplemented on't want to wait to",
    "start": "1191560",
    "end": "1198550"
  },
  {
    "text": "get that done to launch it or Postgres so we launched without it and we've been working to catch up and add support for",
    "start": "1198550",
    "end": "1204820"
  },
  {
    "text": "replication slots to enable app on replication ever since and you know our",
    "start": "1204820",
    "end": "1211480"
  },
  {
    "text": "target is to launch that this year I can say it's coming pretty soon now but I can't give you a specific date but it's",
    "start": "1211480",
    "end": "1218950"
  },
  {
    "text": "coming soon and it will behave just like it does an RDS for Postgres support for replication slots with the same logical",
    "start": "1218950",
    "end": "1225550"
  },
  {
    "text": "decoders that you see in RDS Postgres and of course support for DNS database migration service so DMS will be able to",
    "start": "1225550",
    "end": "1233140"
  },
  {
    "text": "use Aurora Postgres as a source to replicate from Aurora Postgres to wherever DMS met can replicate to which",
    "start": "1233140",
    "end": "1240160"
  },
  {
    "text": "is a whole bunch of different kinds of targets including things like redshift or another post quiz instance or Oracle",
    "start": "1240160",
    "end": "1246280"
  },
  {
    "text": "on-premises which a lot of customers want to do as a fallback when they migrate from Oracle 2 or postcards they",
    "start": "1246280",
    "end": "1251530"
  },
  {
    "text": "want to replicate back just in case they think they have they find a need to fall back and and you know cancel their",
    "start": "1251530",
    "end": "1256900"
  },
  {
    "text": "migration so you know the launch of outbound replication will enable all of",
    "start": "1256900",
    "end": "1262420"
  },
  {
    "text": "those scenarios and others that our customers will think of that we haven't thought of yet so that's coming soon the",
    "start": "1262420",
    "end": "1271840"
  },
  {
    "start": "1270000",
    "end": "1400000"
  },
  {
    "text": "other feature well it's a 4pg logical not in the first",
    "start": "1271840",
    "end": "1278340"
  },
  {
    "text": "release but in a subsequent release it will support PG logical so the initial",
    "start": "1278340",
    "end": "1284070"
  },
  {
    "text": "goal is to support replication slots in round one and then add PG logical in",
    "start": "1284070",
    "end": "1289590"
  },
  {
    "text": "round two the other feature lots of customers ask about is feature I've",
    "start": "1289590",
    "end": "1295440"
  },
  {
    "text": "already mentioned a little bit global what we call global database but the way customers asked for it typically is I want to implement cross region",
    "start": "1295440",
    "end": "1302550"
  },
  {
    "text": "replication so I can have an instance in a different region to protect myself against disaster recovery or to provide",
    "start": "1302550",
    "end": "1309870"
  },
  {
    "text": "disaster recovery and you know again that feature is is coming it's what we",
    "start": "1309870",
    "end": "1317460"
  },
  {
    "text": "call global database it's based upon Aurora storage based replication now you might say wait a minute couldn't I use",
    "start": "1317460",
    "end": "1323280"
  },
  {
    "text": "that outbound replication thing you just told me about to keep an instance in another region hydrated yeah we're not",
    "start": "1323280",
    "end": "1329340"
  },
  {
    "text": "going to block you from doing that we just pretty sure that the performance won't be as good as the storage based",
    "start": "1329340",
    "end": "1336330"
  },
  {
    "text": "replication that we're working on and you might find that the lag in your your",
    "start": "1336330",
    "end": "1343550"
  },
  {
    "text": "instance in another region the lag for that instance when being hydrated by the",
    "start": "1343550",
    "end": "1349440"
  },
  {
    "text": "alpine replication that we're going to launch soon the lag might not be acceptable for what you need at least",
    "start": "1349440",
    "end": "1355020"
  },
  {
    "text": "for our po4 recovery point objective for that disaster recovery copy now it might",
    "start": "1355020",
    "end": "1360660"
  },
  {
    "text": "be fine maybe your RPO is not really stringent in the case of Aurora storage",
    "start": "1360660",
    "end": "1365880"
  },
  {
    "text": "based replication we expect the the lag to be very very small what we're seeing with the same type of replication for",
    "start": "1365880",
    "end": "1372900"
  },
  {
    "text": "Aurora my sequel is lags typically of a second or less for regions that aren't",
    "start": "1372900",
    "end": "1378660"
  },
  {
    "text": "too far apart like say Northern Virginia to Oregon which is still 3,000 miles but",
    "start": "1378660",
    "end": "1383970"
  },
  {
    "text": "the lag is pretty short so you know if you have RPO of seconds or minutes over",
    "start": "1383970",
    "end": "1389580"
  },
  {
    "text": "a storage based replication will easily fit within within your rbo requirements",
    "start": "1389580",
    "end": "1395660"
  },
  {
    "text": "yes",
    "start": "1395660",
    "end": "1398660"
  },
  {
    "start": "1400000",
    "end": "1485000"
  },
  {
    "text": "right so the question is the document documentation seems to imply that PG",
    "start": "1415840",
    "end": "1421549"
  },
  {
    "text": "logical is supported now that's true for RDS Postgres if it implies that",
    "start": "1421549",
    "end": "1426679"
  },
  {
    "text": "supportive 4r or Postgres and that's a mistake in the documentation since we know logical replication isn't supported",
    "start": "1426679",
    "end": "1432080"
  },
  {
    "text": "at all yet when it is it will be initially replication slots not the new",
    "start": "1432080",
    "end": "1438370"
  },
  {
    "text": "pub/sub capability that you might be referring to publication SUBSCRIBE",
    "start": "1438370",
    "end": "1443950"
  },
  {
    "text": "capability the there is kind of one exception so if you're using the new",
    "start": "1443950",
    "end": "1450080"
  },
  {
    "text": "pops up style logical replication you can't use that to replicate into or or",
    "start": "1450080",
    "end": "1456019"
  },
  {
    "text": "Postgres yet because you actually have to support have support for that new capability in the target instance not",
    "start": "1456019",
    "end": "1462559"
  },
  {
    "text": "just in the source the way it works in Postgres and we do plan to add enough",
    "start": "1462559",
    "end": "1468470"
  },
  {
    "text": "support so that you'll be able to use that replication to replicate into a riposte grows when we do this initial",
    "start": "1468470",
    "end": "1473750"
  },
  {
    "text": "launch so that's a little nuance that that it's hard to get across you're clearly in documentation especially",
    "start": "1473750",
    "end": "1480289"
  },
  {
    "text": "since we haven't launched it at all yet if you follow",
    "start": "1480289",
    "end": "1484778"
  },
  {
    "start": "1485000",
    "end": "1520000"
  },
  {
    "text": "with PG logical I think you can replicate into RDS PO I know you can replicate an RDS Postgres I don't think",
    "start": "1491330",
    "end": "1498470"
  },
  {
    "text": "you can replicate into Aurora Postgres now right see Richards gonna correct me",
    "start": "1498470",
    "end": "1506590"
  },
  {
    "text": "okay so engineers have tested and confirmed it works so I just need to make sure the documentation clarifies",
    "start": "1508360",
    "end": "1515029"
  },
  {
    "text": "that then yeah",
    "start": "1515029",
    "end": "1519700"
  },
  {
    "start": "1520000",
    "end": "1585000"
  },
  {
    "text": "yeah some of you pointed out rightly so that we have a lot of documentation on how to migrate from Oracle on premises",
    "start": "1549919",
    "end": "1556679"
  },
  {
    "text": "to RDS in a row of Postgres we have a 350 plus page manual on how to do that",
    "start": "1556679",
    "end": "1562710"
  },
  {
    "text": "and other blog posts etc we don't have the equivalent for how to get from postcards on-premises into a were",
    "start": "1562710",
    "end": "1568950"
  },
  {
    "text": "already s we have smaller pieces but it's not nearly as well organized with coherent and that's what Dennis is",
    "start": "1568950",
    "end": "1574259"
  },
  {
    "text": "referring to you know based upon that feedback today we're organizing some people to work on that and and make that",
    "start": "1574259",
    "end": "1581970"
  },
  {
    "text": "much easier any more questions yes you",
    "start": "1581970",
    "end": "1590399"
  },
  {
    "start": "1585000",
    "end": "1710000"
  },
  {
    "text": "first when do we expect compatibility",
    "start": "1590399",
    "end": "1596669"
  },
  {
    "text": "post goes 11 well we already did that for RDS postcards last week we just launched postcodes 11.1 and it's coming",
    "start": "1596669",
    "end": "1606659"
  },
  {
    "text": "soon for aurora Postgres slightly more generally our goal is to catch up to community releases a lot quicker than we",
    "start": "1606659",
    "end": "1613559"
  },
  {
    "text": "have been both for RDS posters in Aurora Postgres you know we are working on",
    "start": "1613559",
    "end": "1619859"
  },
  {
    "text": "getting I think I mentioned this this morning getting post grows 12 into the RDS preview environment when it makes",
    "start": "1619859",
    "end": "1626580"
  },
  {
    "text": "sense when there's you know beta drop that we can work with so you know that's",
    "start": "1626580",
    "end": "1632220"
  },
  {
    "text": "typically in the spring from the community Spring is three days away so",
    "start": "1632220",
    "end": "1638480"
  },
  {
    "text": "it's coming soon you know so we do have ideas on how we can do that for Aurora",
    "start": "1638480",
    "end": "1644249"
  },
  {
    "text": "Postgres in the preview environment as well this will let you start testing post gras' twelve features for your",
    "start": "1644249",
    "end": "1651299"
  },
  {
    "text": "applications for your environments before its production from the community you won't have to spin up your own",
    "start": "1651299",
    "end": "1657570"
  },
  {
    "text": "manually managed beta instance in ec2 or on-premises you'll be able to go to the",
    "start": "1657570",
    "end": "1663749"
  },
  {
    "text": "RDS preview environment just push a couple buttons spin up an instance and start testing we did that with postcodes",
    "start": "1663749",
    "end": "1669179"
  },
  {
    "text": "11 in the preview environment for hard es Postgres we planning to do it for both RDS and Aurora in the preview",
    "start": "1669179",
    "end": "1675539"
  },
  {
    "text": "environment for Postgres 12 and then when PG 12 actually goes yeah but from the community this fall",
    "start": "1675539",
    "end": "1681110"
  },
  {
    "text": "our goal is to catch up as quickly as we can it's easier in RDS Postgres we don't",
    "start": "1681110",
    "end": "1686990"
  },
  {
    "text": "have so much of a merge burden or just tooling and upgrade work but so our goal",
    "start": "1686990",
    "end": "1694400"
  },
  {
    "text": "is really to be within 30 to 60 days of the community so sometime late this year",
    "start": "1694400",
    "end": "1699920"
  },
  {
    "text": "now we don't always meet those goals sometimes things slip but that's that's what we're targeting for major version",
    "start": "1699920",
    "end": "1707120"
  },
  {
    "text": "support Welty Dave so Dave's question is",
    "start": "1707120",
    "end": "1718220"
  },
  {
    "start": "1710000",
    "end": "1770000"
  },
  {
    "text": "when we launch Aurora Postgres 11 will we support major version upgrade from",
    "start": "1718220",
    "end": "1723590"
  },
  {
    "text": "previous versions of or Postgres because those of you who are active users know",
    "start": "1723590",
    "end": "1728780"
  },
  {
    "text": "that if you have an Aurora Postgres 9.6 instance today you can't do a major version upgrade or Postgres 10 which is",
    "start": "1728780",
    "end": "1735650"
  },
  {
    "text": "kind of not not a great experience you have post res 10 waiting for you there but to get there you have to do a dump",
    "start": "1735650",
    "end": "1740750"
  },
  {
    "text": "and load and take downtime and it's painful and you can't even do outbound replication yet to shorten that and so",
    "start": "1740750",
    "end": "1747020"
  },
  {
    "text": "we are working on adding support for me mvu as we call it major version upgrade I believe the timing right now is we'll",
    "start": "1747020",
    "end": "1754100"
  },
  {
    "text": "have mvu before we have or Postgres 11 if not they'll be about in sync so yeah",
    "start": "1754100",
    "end": "1762620"
  },
  {
    "text": "you know these are somewhat moving targets but I think they'll happen at the same time",
    "start": "1762620",
    "end": "1768610"
  },
  {
    "text": "but we're not gonna hold one for the other as Denis points out because customers will independently benefit",
    "start": "1797360",
    "end": "1803100"
  },
  {
    "text": "from each so when each is ready we're gonna launch each one right the man at",
    "start": "1803100",
    "end": "1814590"
  },
  {
    "text": "the red shirt Aurora post-grad service",
    "start": "1814590",
    "end": "1822090"
  },
  {
    "text": "so what is that project well it's just it's functionally the same as a row of my sequel server this which is GA and",
    "start": "1822090",
    "end": "1828570"
  },
  {
    "text": "there's a whole bunch of dock and you know blogs etc whatever you read about or or my sequel service is roughly",
    "start": "1828570",
    "end": "1834389"
  },
  {
    "text": "equivalent to what you'll see with or a post gross surplus or a postcode serverless is in public preview you can",
    "start": "1834389",
    "end": "1839610"
  },
  {
    "text": "actually sign up if you literally just google for or Postgres service preview you'll find the signup page you know and",
    "start": "1839610",
    "end": "1846389"
  },
  {
    "text": "we're busy white listing more customers into that preview but what is service what is there or Postgres herbalist",
    "start": "1846389",
    "end": "1851610"
  },
  {
    "text": "basically gives you an order posters end point where you don't provision a server you don't say hey I won an r4 dot for",
    "start": "1851610",
    "end": "1858210"
  },
  {
    "text": "Excel and you know and provision all the details of that of that instance you basically just get an endpoint that you",
    "start": "1858210",
    "end": "1866370"
  },
  {
    "text": "start you know creating tables inserting rows doing transactions on and you",
    "start": "1866370",
    "end": "1871380"
  },
  {
    "text": "specify a lower and an upper bound on aurora compute units which is really a combination of CPUs and and memory and",
    "start": "1871380",
    "end": "1879809"
  },
  {
    "text": "we will scale up and down between your upper and lower bounds based upon how busy you make your endpoint and so you",
    "start": "1879809",
    "end": "1887700"
  },
  {
    "text": "don't have to think about you know like I said provisioning and then oh wait my workloads gonna scale up spike you know",
    "start": "1887700",
    "end": "1893549"
  },
  {
    "text": "tomorrow so I need to manually scale up we also maintain a a proxy in front of",
    "start": "1893549",
    "end": "1899130"
  },
  {
    "text": "your instances so that that proxy holds your connection state while we actually scale underneath your",
    "start": "1899130",
    "end": "1905460"
  },
  {
    "text": "those connections so when those scaling up and down happens you don't lose your",
    "start": "1905460",
    "end": "1910620"
  },
  {
    "text": "connections to the instance so the idea with serverless is it's ideal for",
    "start": "1910620",
    "end": "1916680"
  },
  {
    "text": "workloads that have lots of variability whether it's predictable variability like you know you know that people are",
    "start": "1916680",
    "end": "1923220"
  },
  {
    "text": "going to be busy on your systems during the day and but you want them to kind of when when you developer stop working at",
    "start": "1923220",
    "end": "1928980"
  },
  {
    "text": "8:00 or 9:00 or 10:00 at night you want it to scale down to a lower level that costs you less and then scale back up",
    "start": "1928980",
    "end": "1934140"
  },
  {
    "text": "when people get busy the next morning or you might have a you know production system that has unpredictable",
    "start": "1934140",
    "end": "1939660"
  },
  {
    "text": "variability it might be based upon news cycles and it's an ad server for media and so you don't know when there's gonna",
    "start": "1939660",
    "end": "1945450"
  },
  {
    "text": "be spikes in both cases server lists will provide a more cost-effective",
    "start": "1945450",
    "end": "1950880"
  },
  {
    "text": "experience than provisioning for peak in the traditional what we call provisioned model because in the provision world",
    "start": "1950880",
    "end": "1957420"
  },
  {
    "text": "either have to provision for peak or you have to plan scaling operations which cause disruption to your application",
    "start": "1957420",
    "end": "1963750"
  },
  {
    "text": "environments to users who are connected and so server list is designed for those unpredictable workloads and so it's a",
    "start": "1963750",
    "end": "1971610"
  },
  {
    "text": "really cool technology to play around with in the preview again you can see",
    "start": "1971610",
    "end": "1976770"
  },
  {
    "text": "some of the more developed documentation if you look at they were my sequel server lists documentation and and",
    "start": "1976770",
    "end": "1983580"
  },
  {
    "text": "blocks any follow-on questions if we",
    "start": "1983580",
    "end": "1993450"
  },
  {
    "start": "1990000",
    "end": "2055000"
  },
  {
    "text": "ready when we go GA which is sometime later this year yes",
    "start": "1993450",
    "end": "2000940"
  },
  {
    "text": "so you're asking if applications need to be modified for service no they don't",
    "start": "2010760",
    "end": "2017010"
  },
  {
    "text": "even know it's a service instance it's still Postgres or my sequel anymore my sequel case it still looks just like",
    "start": "2017010",
    "end": "2024350"
  },
  {
    "text": "your normal database end point it just so happens that under the covers we're automatically scaling up and down the",
    "start": "2024350",
    "end": "2031730"
  },
  {
    "text": "compute resources computer memory resources behind that end point but you",
    "start": "2031730",
    "end": "2037020"
  },
  {
    "text": "don't need to change anything it's not like it's a limited version of postcards that doesn't support all the sequel dialect it's just a row of postcards",
    "start": "2037020",
    "end": "2043980"
  },
  {
    "text": "where we're doing some you know control plane stuff to scale it up and down and",
    "start": "2043980",
    "end": "2049679"
  },
  {
    "text": "the proxy of course to keep your connection state but know there's nothing you need to change yeah yeah",
    "start": "2049679",
    "end": "2058800"
  },
  {
    "start": "2055000",
    "end": "2145000"
  },
  {
    "text": "good point today or where our service is just one instance it's not it does not have support for read replicas in the",
    "start": "2058800",
    "end": "2066240"
  },
  {
    "text": "the provisioned Aurora model thanks for the reminder Richard doesn't change how your application works functionally but",
    "start": "2066240",
    "end": "2073740"
  },
  {
    "text": "it does change how you think about scaling because it currently doesn't have a scale out to model it only has a",
    "start": "2073740",
    "end": "2080429"
  },
  {
    "text": "scale up and down model now one other little quirk and servlets you can set your lower limit to zero which means",
    "start": "2080429",
    "end": "2087330"
  },
  {
    "text": "when your instance goes idle under the covers we're going to scale down to zero we'll still hold on to your connections and of course your storage is there but",
    "start": "2087330",
    "end": "2094200"
  },
  {
    "text": "what happens is the next time you you touch the instance we have to go provision we have to we have to grab an",
    "start": "2094200",
    "end": "2101430"
  },
  {
    "text": "instance out of the warm pool and connect it in there's a little bit of extra latency for that first touch you",
    "start": "2101430",
    "end": "2107430"
  },
  {
    "text": "might not want to have that wait and see so you might set your lowest level to the to the to the lowest nonzero level",
    "start": "2107430",
    "end": "2112980"
  },
  {
    "text": "and so the difference in cost there all right but depends on your workload",
    "start": "2112980",
    "end": "2118320"
  },
  {
    "text": "requirements do you care about that first touch slowness or not if you don't care then you just let it scale down to zero if you do carry you scale down to",
    "start": "2118320",
    "end": "2125670"
  },
  {
    "text": "the lowest level and on the upper limit of course you don't have said to the max because you may not want to spend the amount of money it cost to run at the",
    "start": "2125670",
    "end": "2131849"
  },
  {
    "text": "Mac might know that you have spikes where it's okay for the spikes to get a little bit higher latency because you're not",
    "start": "2131849",
    "end": "2136990"
  },
  {
    "text": "going to scale up to to the largest size",
    "start": "2136990",
    "end": "2141300"
  },
  {
    "start": "2145000",
    "end": "2170000"
  },
  {
    "text": "right well we did launch per second billing for all of our DS recently so",
    "start": "2152970",
    "end": "2158730"
  },
  {
    "text": "but yeah it's billed by second but it's also not billed in the same way for",
    "start": "2158730",
    "end": "2164829"
  },
  {
    "text": "instance and the wide-eyed idealistic",
    "start": "2164829",
    "end": "2242609"
  },
  {
    "text": "the wide idealistic crazy people say that service is the only way people will use databases in a few years and the",
    "start": "2242609",
    "end": "2249160"
  },
  {
    "text": "whole provision model is going to go away so",
    "start": "2249160",
    "end": "2252869"
  },
  {
    "text": "well for those of you don't know you",
    "start": "2260470",
    "end": "2285680"
  },
  {
    "start": "2265000",
    "end": "2370000"
  },
  {
    "text": "know in standard provisioned RDS and Aurora you pay for instance hours or instance seconds now but you know you",
    "start": "2285680",
    "end": "2293690"
  },
  {
    "text": "pay per for how much you use but then you turn your instance off and you stop paying but most customers don't turn the",
    "start": "2293690",
    "end": "2300590"
  },
  {
    "text": "databases off you leave your database up and running generally 24/7 365 and so if you know",
    "start": "2300590",
    "end": "2305869"
  },
  {
    "text": "you're gonna do that you can buy a reserved instance which is a billing mechanism that will give you a discount",
    "start": "2305869",
    "end": "2311300"
  },
  {
    "text": "of up to roughly two-thirds off if you buy a three-year upfront reserved instance which means you pay us for",
    "start": "2311300",
    "end": "2317510"
  },
  {
    "text": "three years upfront right so pay it all upfront will give you two-thirds off roughly if you look at the pricing on",
    "start": "2317510",
    "end": "2323660"
  },
  {
    "text": "our webpage and so that's what rich is referring to the cost of running provisioned with a three-year upfront",
    "start": "2323660",
    "end": "2330950"
  },
  {
    "text": "reserved instances pretty low the pricing for service is of course designed to be very different and if you",
    "start": "2330950",
    "end": "2337490"
  },
  {
    "text": "look at it if you have a steady-state workload and you run it on server list it's gonna be a lot more expensive than running it on on provision with our eyes",
    "start": "2337490",
    "end": "2344000"
  },
  {
    "text": "it's only when you have this really variable workload we run provision you have to provision for the peak or else",
    "start": "2344000",
    "end": "2349460"
  },
  {
    "text": "you not servicing your customers well for all those peak times and if your provision for peak and by our eyes it's",
    "start": "2349460",
    "end": "2355220"
  },
  {
    "text": "it's still going to be more expensive than service so that the pricing of service is designed to the",
    "start": "2355220",
    "end": "2361060"
  },
  {
    "text": "cost-effective for those really variable workloads even in the face of the maxed",
    "start": "2361060",
    "end": "2366650"
  },
  {
    "text": "out our eyes",
    "start": "2366650",
    "end": "2369099"
  },
  {
    "start": "2370000",
    "end": "2450000"
  },
  {
    "text": "yeah so a good question we do have a feature called start stop start for RDS",
    "start": "2375910",
    "end": "2381680"
  },
  {
    "text": "instances where you can stop the instance and we want to leave the storage because it used to be if you stopped your instance we automatically",
    "start": "2381680",
    "end": "2387440"
  },
  {
    "text": "deleted your database part of the reason that we didn't just on make it easy for",
    "start": "2387440",
    "end": "2392900"
  },
  {
    "text": "you to leave the storage around is because we're always you know worried about upgrades and patches and things like that that's why we don't let you do",
    "start": "2392900",
    "end": "2399200"
  },
  {
    "text": "a stock stop of your newest instance for more than seven days because we want you to start up the instance so that we can",
    "start": "2399200",
    "end": "2405440"
  },
  {
    "text": "do whatever maintenance we might need to do under the covers right so that's why the limit on stopping your ideas",
    "start": "2405440",
    "end": "2411440"
  },
  {
    "text": "instance of seven days now you could stop your instance and then start it up for an hour and then shut it down again",
    "start": "2411440",
    "end": "2417499"
  },
  {
    "text": "but if you can do that long term out question while you're doing that at all that's that's where you take a snapshot",
    "start": "2417499",
    "end": "2423109"
  },
  {
    "text": "and just keep the snapshot lying around but stop start is is a different way to do it's kind of like poor man's service",
    "start": "2423109",
    "end": "2429380"
  },
  {
    "text": "we have to do it manually your connection state is not maintained across the stop but you might just stop",
    "start": "2429380",
    "end": "2435019"
  },
  {
    "text": "your instances on Friday night and start about Monday morning so you're not paying for them over the weekend that's the typical use case for our data stop",
    "start": "2435019",
    "end": "2441529"
  },
  {
    "text": "start but it is kind of like a stepping stone to service when you think it",
    "start": "2441529",
    "end": "2446989"
  },
  {
    "text": "through it's it's kind of designed for those those the similar scenarios other",
    "start": "2446989",
    "end": "2454579"
  },
  {
    "start": "2450000",
    "end": "2480000"
  },
  {
    "text": "questions well it is after 5:00 on the Monday we've had a long day of lots of",
    "start": "2454579",
    "end": "2460999"
  },
  {
    "text": "content not just in this room I know in other rooms I know we've all been bouncing back and forth you know we're",
    "start": "2460999",
    "end": "2468109"
  },
  {
    "text": "we're here all week and got a bunch of sessions from a Tobias people the rest of the week scattered through the",
    "start": "2468109",
    "end": "2474109"
  },
  {
    "text": "through the regular conference agenda as well yes there's a question oh they're",
    "start": "2474109",
    "end": "2480440"
  },
  {
    "start": "2480000",
    "end": "2530000"
  },
  {
    "text": "sorry",
    "start": "2480440",
    "end": "2482798"
  },
  {
    "text": "so question is which ec2 instance types we run so in RDS for Postgres we support",
    "start": "2488080",
    "end": "2494390"
  },
  {
    "text": "TSM's and ours teaser the smaller or less capable the more cost-effective and",
    "start": "2494390",
    "end": "2501290"
  },
  {
    "text": "last customers will run their tests of QA stuff on T's M's have more CPU less",
    "start": "2501290",
    "end": "2508640"
  },
  {
    "text": "memory per CPU so they're good for CPU intensive workloads and then the ours are where we typically see customers run",
    "start": "2508640",
    "end": "2514250"
  },
  {
    "text": "the biggest database workloads because they've more memory and databases like memory that's for RDS Postgres and",
    "start": "2514250",
    "end": "2520820"
  },
  {
    "text": "Aurora today we run on the are fours and our 5s our 5s in some regions it's",
    "start": "2520820",
    "end": "2526310"
  },
  {
    "text": "rolling out to the other regions it you know on an ongoing basis sorry our 5 is",
    "start": "2526310",
    "end": "2536600"
  },
  {
    "start": "2530000",
    "end": "2653000"
  },
  {
    "text": "available for world plus squares in some regions not in the ones you care about I know that yeah yeah I know it's coming",
    "start": "2536600",
    "end": "2546970"
  },
  {
    "text": "and you know so because Aurora itself has some extra components on the",
    "start": "2546970",
    "end": "2553280"
  },
  {
    "text": "database host that use memory it's more work for us to make overall fit on instances with less memory we are",
    "start": "2553280",
    "end": "2560300"
  },
  {
    "text": "working on adding support for Aurora 2 down in the T class instances because",
    "start": "2560300",
    "end": "2565970"
  },
  {
    "text": "customers still want to run test fqa on the smallest instance they can find for Aurora as well and so we know that you",
    "start": "2565970",
    "end": "2573290"
  },
  {
    "text": "know that's a big ask from customers and so it's coming it's just it's a lot of work to put or a Postgres on a memory",
    "start": "2573290",
    "end": "2578720"
  },
  {
    "text": "diet to make it fit in and behave well so you can still get you know your workloads to work on the smaller T T",
    "start": "2578720",
    "end": "2585620"
  },
  {
    "text": "class instances the other more exotic ones you know you know the extremely",
    "start": "2585620",
    "end": "2591260"
  },
  {
    "text": "high memory instances are challenges for most databases because they're typically for socket Numa machines that means",
    "start": "2591260",
    "end": "2597170"
  },
  {
    "text": "non-uniform memory access which means that the latency to access memory that's owned by another socket in that four",
    "start": "2597170",
    "end": "2603170"
  },
  {
    "text": "socket machine is much higher than my local memory and it gets complicated to make high contention workloads like",
    "start": "2603170",
    "end": "2609440"
  },
  {
    "text": "database workloads work well in those big memory for Saka machines so we haven't seen a need to go that big",
    "start": "2609440",
    "end": "2616850"
  },
  {
    "text": "because the are 524 XL is a to socket machine that scales pretty well so you",
    "start": "2616850",
    "end": "2622190"
  },
  {
    "text": "know we're kind of waiting and seeing whether there's real customer demand for the really really big memory instances",
    "start": "2622190",
    "end": "2629350"
  },
  {
    "text": "okay well maybe tapped out well thanks",
    "start": "2631060",
    "end": "2639050"
  },
  {
    "text": "thanks for coming to the last session this has been great set of questions and",
    "start": "2639050",
    "end": "2644800"
  },
  {
    "text": "we look forward to seeing you all week at least some of you all week at the rest of our sessions",
    "start": "2644800",
    "end": "2652119"
  }
]