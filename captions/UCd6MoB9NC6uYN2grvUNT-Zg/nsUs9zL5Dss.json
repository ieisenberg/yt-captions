[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "so welcome everyone to the last session of the way I hope you're not too tired",
    "start": "709",
    "end": "7950"
  },
  {
    "text": "my name is sis evaluator guy huh I am a Technical Account Manager at AWS for",
    "start": "7950",
    "end": "13040"
  },
  {
    "text": "those of you who are not familiar with enterprise support or times at AWS we",
    "start": "13040",
    "end": "19560"
  },
  {
    "text": "often get engaged when customers have already billed our environments so we",
    "start": "19560",
    "end": "24869"
  },
  {
    "text": "either help them with their struggles or we work together with them to continuously improve and expand those",
    "start": "24869",
    "end": "31650"
  },
  {
    "text": "environments so I hope I can give you some advantage here today and if you're",
    "start": "31650",
    "end": "37829"
  },
  {
    "text": "in this session you probably want to learn more about building a modern data platform in AWS if not you might want to",
    "start": "37829",
    "end": "44520"
  },
  {
    "text": "check if you're in the right theater but we're going to discuss some of the core",
    "start": "44520",
    "end": "49590"
  },
  {
    "text": "components to build modern data platform and to perform some of the most common",
    "start": "49590",
    "end": "55800"
  },
  {
    "text": "types of analytics so we hear from companies all the time that they want to",
    "start": "55800",
    "end": "62789"
  },
  {
    "text": "extract more value from their data but they struggle to capture or store and analyze all the dates are they generate",
    "start": "62789",
    "end": "70010"
  },
  {
    "text": "traditional silos weren't built to work well together and make it difficult to",
    "start": "70010",
    "end": "75119"
  },
  {
    "text": "consolidate data and to perform comprehensive and efficient analysis",
    "start": "75119",
    "end": "80479"
  },
  {
    "text": "today's business applications and operational processes need to access data in real time and we have different",
    "start": "80479",
    "end": "88170"
  },
  {
    "text": "users with very different requirements do you have any data developers in the",
    "start": "88170",
    "end": "93509"
  },
  {
    "text": "room some data scientists some business",
    "start": "93509",
    "end": "99780"
  },
  {
    "text": "analysts builders cloud unicorns Oh a",
    "start": "99780",
    "end": "107460"
  },
  {
    "text": "lot of those ok that was unexpected ok well welcome everyone so I'm of course",
    "start": "107460",
    "end": "114270"
  },
  {
    "text": "all of you want to use your own choice of tools and frameworks right so how do",
    "start": "114270",
    "end": "120329"
  },
  {
    "text": "we build a data platform that can erase all these requirements a little platform that can go beyond insights from",
    "start": "120329",
    "end": "126360"
  },
  {
    "text": "operational reporting on historical data to being able to perform a healer and real-time analytics to accurately",
    "start": "126360",
    "end": "133470"
  },
  {
    "text": "predict future outcomes a high-level these are the main components so let's",
    "start": "133470",
    "end": "140670"
  },
  {
    "start": "135000",
    "end": "223000"
  },
  {
    "text": "start with the data warehouse a data warehouse is a database and it's optimized to analyze relational data",
    "start": "140670",
    "end": "147290"
  },
  {
    "text": "data structure and schema have been defined in advance to optimized for fast complex sequel queries data has been",
    "start": "147290",
    "end": "155130"
  },
  {
    "text": "clean and rich transformed so it can act as that single source of truth for data",
    "start": "155130",
    "end": "160350"
  },
  {
    "text": "that your users can trust now the other component is the bits like an italic is",
    "start": "160350",
    "end": "166380"
  },
  {
    "text": "a single centralized repository where you can store structured semi-structured",
    "start": "166380",
    "end": "171720"
  },
  {
    "text": "and unstructured data at any scale and you can show your they tell without any preliminary processing or transformation",
    "start": "171720",
    "end": "178980"
  },
  {
    "text": "you can then process it in place and perform different types of analytics on top Business Intelligence real-time",
    "start": "178980",
    "end": "186120"
  },
  {
    "text": "analytics machine learning big data processing you name it now the main challenge with a data Lake",
    "start": "186120",
    "end": "193709"
  },
  {
    "text": "is that raw data is a soul with no oversight of his content for an italic to make data usable it needs the fine",
    "start": "193709",
    "end": "200850"
  },
  {
    "text": "mechanisms to catalog and secure the data without this elements data cannot",
    "start": "200850",
    "end": "207120"
  },
  {
    "text": "be found or trusted and that's how a vitalik and SAP becoming elitist firm so",
    "start": "207120",
    "end": "214560"
  },
  {
    "text": "in order to be able to respond to all your different user requirements your data leak needs to have governance",
    "start": "214560",
    "end": "219930"
  },
  {
    "text": "semantic consistency and access controls AWS provides a variety of building",
    "start": "219930",
    "end": "227010"
  },
  {
    "text": "blocks that you can use a secure inflexible data platform there are managed services that you can use to",
    "start": "227010",
    "end": "232880"
  },
  {
    "text": "collect store process find and analyze the structure and unstructured data in",
    "start": "232880",
    "end": "239220"
  },
  {
    "text": "many different ways so these are the services right red shift blue Athena",
    "start": "239220",
    "end": "246390"
  },
  {
    "start": "241000",
    "end": "263000"
  },
  {
    "text": "these are just some of the services there are many many more so to see them",
    "start": "246390",
    "end": "252299"
  },
  {
    "text": "all it's gonna take about four hours I hope you're comfortable now we're going to see some of the main components to",
    "start": "252299",
    "end": "258780"
  },
  {
    "text": "build the data like infrastructure and to perform different types of analytics on tops",
    "start": "258780",
    "end": "264020"
  },
  {
    "start": "263000",
    "end": "273000"
  },
  {
    "text": "right so let's start with where data is a store and how can you organize manage",
    "start": "264020",
    "end": "270840"
  },
  {
    "text": "and prepare it for analytics s3 of",
    "start": "270840",
    "end": "276120"
  },
  {
    "start": "273000",
    "end": "381000"
  },
  {
    "text": "course is the optimal foundation for the beta Lake because of its virtually",
    "start": "276120",
    "end": "281430"
  },
  {
    "text": "unlimited scalability and performance it allows you to decouple or to provision",
    "start": "281430",
    "end": "288289"
  },
  {
    "text": "storage and compute independently so you can scale as you need and in a",
    "start": "288289",
    "end": "293669"
  },
  {
    "text": "cost-efficient manner s3 is that single centralized repository where you can",
    "start": "293669",
    "end": "299039"
  },
  {
    "text": "store data in its native format without any transformation and schema definitions now for security it provides",
    "start": "299039",
    "end": "307500"
  },
  {
    "text": "different options for encryption both at rest and in transit including across regions you can configure and enforce",
    "start": "307500",
    "end": "315539"
  },
  {
    "text": "access controls and security policies not only at the packet level but also at the object level with cloud trail dates",
    "start": "315539",
    "end": "323009"
  },
  {
    "text": "events you can now delete how when and who is accessing individual objects in",
    "start": "323009",
    "end": "328199"
  },
  {
    "text": "s3 it also provision it also provides a daily inventory report where you can see",
    "start": "328199",
    "end": "335400"
  },
  {
    "text": "a list with all of the odds all of the objects in your backyard with important details like its encryption status",
    "start": "335400",
    "end": "343520"
  },
  {
    "text": "another tool that you can use is s3 storage class analysis it allows you to",
    "start": "343520",
    "end": "351780"
  },
  {
    "text": "monitor the access patterns across all the objects in the data lake and to identify opportunities to move data to",
    "start": "351780",
    "end": "359039"
  },
  {
    "text": "lower cost storage tiers for example infrequent access or even glacier and you can use this information to create",
    "start": "359039",
    "end": "365820"
  },
  {
    "text": "an s3 lifecycle policy to make that data transfer across tiers automatically and",
    "start": "365820",
    "end": "371360"
  },
  {
    "text": "for those access patterns unknown or access patterns that change you can also",
    "start": "371360",
    "end": "376949"
  },
  {
    "text": "use the s3 intelligent hearing storage class",
    "start": "376949",
    "end": "381800"
  },
  {
    "start": "381000",
    "end": "488000"
  },
  {
    "text": "so I WS look either one us glue is a",
    "start": "383690",
    "end": "390960"
  },
  {
    "text": "fully managed extract transform and load service and it allows you to catalog",
    "start": "390960",
    "end": "396510"
  },
  {
    "text": "your data and clean it and reach it and move it reliably across data stores so",
    "start": "396510",
    "end": "403820"
  },
  {
    "text": "the glue data catalog will store metadata about your data sources your transformations and your targets",
    "start": "403820",
    "end": "409890"
  },
  {
    "text": "it's a drop-in replacement for a hive meta store and it also provides the job system so you can define a schedule and",
    "start": "409890",
    "end": "416850"
  },
  {
    "text": "run ETL operations so the glue colors will scan the data infer the schema and",
    "start": "416850",
    "end": "424560"
  },
  {
    "text": "construct the data catalog these crawlers will use built-in classifiers for known formats like CSV JSON and",
    "start": "424560",
    "end": "432420"
  },
  {
    "text": "columnar formats like Parker but you can also use your own classifiers or take",
    "start": "432420",
    "end": "438030"
  },
  {
    "text": "them from the AWS glue community for other data formats or if you want to customize the tables um",
    "start": "438030",
    "end": "445100"
  },
  {
    "text": "Blu will use the metadata in the data catalog to automatically generate the script in scale and price park that you",
    "start": "445100",
    "end": "452970"
  },
  {
    "text": "can use to modify to four different material operations so for example you",
    "start": "452970",
    "end": "459360"
  },
  {
    "text": "can extract clean and transform raw data and store the results in a different repository where you can query and",
    "start": "459360",
    "end": "465450"
  },
  {
    "text": "analyze it so let's say you were landing CSV files you can transform them into a",
    "start": "465450",
    "end": "472080"
  },
  {
    "text": "relational form and save them save that a tie in redshift and you can automate",
    "start": "472080",
    "end": "478290"
  },
  {
    "text": "those scripts with jobs based on a schedule chaining them or even trigger",
    "start": "478290",
    "end": "483450"
  },
  {
    "text": "by events for example every time new data arrives in the data like so there",
    "start": "483450",
    "end": "489990"
  },
  {
    "start": "488000",
    "end": "554000"
  },
  {
    "text": "are a lot of steps involved in building a data like let's start so we have to",
    "start": "489990",
    "end": "496500"
  },
  {
    "text": "set up the storage we talked about this tree so you have to set up the buckets and partitions you have to then move the",
    "start": "496500",
    "end": "503460"
  },
  {
    "text": "data so from all your different sources you don't have to clean and prepare the",
    "start": "503460",
    "end": "508650"
  },
  {
    "text": "data so for example removing duplicates or again transforming to a columnar",
    "start": "508650",
    "end": "514169"
  },
  {
    "text": "format for example Park here that will optimize performance and of",
    "start": "514169",
    "end": "519279"
  },
  {
    "text": "course you have to catalogue the data now they tie needs to be secured based on your different compliance",
    "start": "519279",
    "end": "525069"
  },
  {
    "text": "requirements and access restricted based on those requirements and finally you",
    "start": "525069",
    "end": "530379"
  },
  {
    "text": "have to make data available to the different people in your organization so",
    "start": "530379",
    "end": "535480"
  },
  {
    "text": "you have to label the data in the catalog and make it accessible to users okay so as you can see all this takes a",
    "start": "535480",
    "end": "544240"
  },
  {
    "text": "lot of time because a lot of these is don't manually a fully productive data Lake can take months to implement and",
    "start": "544240",
    "end": "550660"
  },
  {
    "text": "that's why we announce AWS lake formation in the last rement so something just said during the",
    "start": "550660",
    "end": "557439"
  },
  {
    "start": "554000",
    "end": "619000"
  },
  {
    "text": "keynote well you can set up your data leak manually the work it takes to set",
    "start": "557439",
    "end": "563079"
  },
  {
    "text": "it up and load the data set up security can take a lot of time so let formation",
    "start": "563079",
    "end": "569800"
  },
  {
    "text": "will simplify this manual process by automating many of these steps so you",
    "start": "569800",
    "end": "575139"
  },
  {
    "text": "can build your date talak with just a few clicks from a dashboard you have to specify where your data resides and what",
    "start": "575139",
    "end": "583930"
  },
  {
    "text": "access and security policies you want to apply like formation will collect the data from your database and your updates",
    "start": "583930",
    "end": "590740"
  },
  {
    "text": "databases and your object storage it will load it into your new s3 data Lake it will clean and classify dates are",
    "start": "590740",
    "end": "599230"
  },
  {
    "text": "using machine learning algorithms it will then secure access to sensitive data and finally your users can access a",
    "start": "599230",
    "end": "606579"
  },
  {
    "text": "centralized data catalog with all that will describe all the available data sets and it's relevant usage easy right",
    "start": "606579",
    "end": "614399"
  },
  {
    "text": "so if you wanna check the preview just go to that URL moving on",
    "start": "614399",
    "end": "621129"
  },
  {
    "start": "619000",
    "end": "630000"
  },
  {
    "text": "so we've we have the data stored it's secured its catalog so it's ready for",
    "start": "621129",
    "end": "628059"
  },
  {
    "text": "analysis and here's one of the farm begins let's start for operational",
    "start": "628059",
    "end": "633519"
  },
  {
    "start": "630000",
    "end": "678000"
  },
  {
    "text": "reporting for operational analytics like for example application monitoring or",
    "start": "633519",
    "end": "638610"
  },
  {
    "text": "fixed imminent extreme analysis or log analysis Amazon Elastic service allows",
    "start": "638610",
    "end": "645429"
  },
  {
    "text": "you to search and filter an aggregate and visualize data in near real-time",
    "start": "645429",
    "end": "650569"
  },
  {
    "text": "it's a fully managed service so it will provision all the resources for your elasticsearch domain and it will also",
    "start": "650569",
    "end": "657299"
  },
  {
    "text": "automatically detect and replace any failing notes it provides built in the",
    "start": "657299",
    "end": "662909"
  },
  {
    "text": "integration with ingestion tools like love stash and visualization tools like Havana and it also supports the",
    "start": "662909",
    "end": "669059"
  },
  {
    "text": "elasticsearch open source API so if you're already using elastic search in your environments you can continue to",
    "start": "669059",
    "end": "674669"
  },
  {
    "text": "use your own code and applications so",
    "start": "674669",
    "end": "680509"
  },
  {
    "start": "678000",
    "end": "758000"
  },
  {
    "text": "real-time analytics kenosis allows you",
    "start": "680509",
    "end": "688289"
  },
  {
    "text": "to collect process and analyze streams of data so we're talking about telemetry",
    "start": "688289",
    "end": "695189"
  },
  {
    "text": "data from IOT devices or website click streams or application and",
    "start": "695189",
    "end": "700589"
  },
  {
    "text": "infrastructure locks it analyzes data as it arrives in the data like I need allows you to respond in real-time it",
    "start": "700589",
    "end": "709049"
  },
  {
    "text": "provides many advantages right we've talked about it accelerates intake because you don't have to batch data before you submit but also if you will",
    "start": "709049",
    "end": "717720"
  },
  {
    "text": "if you using it for application logs it prevents data log see for example one of the application servers fails and you'll",
    "start": "717720",
    "end": "724229"
  },
  {
    "text": "also help you reduce that local storage in those producers Genesis is server",
    "start": "724229",
    "end": "730799"
  },
  {
    "text": "less so you just have to point at an incoming data stream and specify where",
    "start": "730799",
    "end": "736679"
  },
  {
    "text": "you want to start the results Genesis and obviously you have to also write the",
    "start": "736679",
    "end": "742379"
  },
  {
    "text": "query so you can use the available templates Kinesis will take care of continuously running those queries while",
    "start": "742379",
    "end": "749069"
  },
  {
    "text": "they ty is in transit and sending the resource to the destinations so for",
    "start": "749069",
    "end": "759749"
  },
  {
    "start": "758000",
    "end": "816000"
  },
  {
    "text": "interactive analysis athena allows you to query data directly from the s3 data",
    "start": "759749",
    "end": "766799"
  },
  {
    "text": "like juiciest and our sequel queries athena is cerberus so again no infrastructure to set up or monitor I",
    "start": "766799",
    "end": "773659"
  },
  {
    "text": "think allows you to run other queries against the data set seen in s",
    "start": "773659",
    "end": "779440"
  },
  {
    "text": "three without having to move data or load it into a different analytics platform it can process structure and an",
    "start": "779440",
    "end": "787780"
  },
  {
    "text": "instructor data because it supports formats like CSV and JSON and also columnar formats like perky and Orsi",
    "start": "787780",
    "end": "796590"
  },
  {
    "text": "Athena also provides integration with tools like Amazon quick cite for those of you",
    "start": "796590",
    "end": "802840"
  },
  {
    "text": "who are not familiar with quick side is the AWS tool for business intelligence",
    "start": "802840",
    "end": "808420"
  },
  {
    "text": "so to create visualizations and dashboards but you can also use your own BI tools if you prefer using the JDBC",
    "start": "808420",
    "end": "814900"
  },
  {
    "text": "driver so we need a processing with the",
    "start": "814900",
    "end": "820590"
  },
  {
    "text": "Apache spark and Hadoop ecosystem so any more uses Hadoop to send a to distribute",
    "start": "820590",
    "end": "829090"
  },
  {
    "text": "the data across an elastic cluster of ec2 instances it's a fully managed",
    "start": "829090",
    "end": "834100"
  },
  {
    "text": "service so it will provision and maintain the infrastructure and the software of the Hadoop cluster for you",
    "start": "834100",
    "end": "841350"
  },
  {
    "text": "with EMR when you set up your email cluster or your job flow you can specify",
    "start": "841350",
    "end": "848970"
  },
  {
    "text": "how many on what type of 52 instances you want to provision and that includes reserved instances and spot instances",
    "start": "848970",
    "end": "855910"
  },
  {
    "text": "for a lower cost you can also set up a permanent cluster that stays up all the",
    "start": "855910",
    "end": "861010"
  },
  {
    "text": "time or a temporary cluster that will terminate after the job is complete",
    "start": "861010",
    "end": "866100"
  },
  {
    "text": "Newmar provides a lot of flexibility it supports many many frameworks like a",
    "start": "866100",
    "end": "871120"
  },
  {
    "text": "spark HBase press stuff link and it also connects as data from other data stores",
    "start": "871120",
    "end": "877780"
  },
  {
    "text": "like Amazon s3 and Amazon DynamoDB if you're using notebooks EMR allows you to",
    "start": "877780",
    "end": "887140"
  },
  {
    "text": "create applications and collaborates in a managed environment based on the Jupiter notebooks so let's move on and",
    "start": "887140",
    "end": "894550"
  },
  {
    "text": "we're back to the data warehouse so for data warehousing redshift allows you to",
    "start": "894550",
    "end": "901120"
  },
  {
    "text": "execute complex analytic queries against petabytes of structured data",
    "start": "901120",
    "end": "906820"
  },
  {
    "text": "Rajeev uses machine learning a massive parallel architecture",
    "start": "906820",
    "end": "911860"
  },
  {
    "text": "computer optimized the store hardware and subset result caching to provide",
    "start": "911860",
    "end": "917140"
  },
  {
    "text": "high throughput and sub-second response times even when you have thousands of concurrent users and queries with Rett",
    "start": "917140",
    "end": "924880"
  },
  {
    "text": "if you have full control of your queries and your queues you can use wlm workload",
    "start": "924880",
    "end": "930220"
  },
  {
    "text": "management to add more queues and specify how the queries are routed to disk use and you can use qmr put in",
    "start": "930220",
    "end": "936880"
  },
  {
    "text": "management rules to create rules with the specific conditions and actions for",
    "start": "936880",
    "end": "942640"
  },
  {
    "text": "each one of those queues so for example you might want to block all the queries",
    "start": "942640",
    "end": "948040"
  },
  {
    "text": "that are using nested loops to identify opportunities to improve your user syntax or you might want to consider",
    "start": "948040",
    "end": "955480"
  },
  {
    "text": "hoping or even aborting queries that run for more than a specified time in a specific you to keep that tube going and",
    "start": "955480",
    "end": "963330"
  },
  {
    "text": "full burst of demand for those peak times when the number of concurrent where is increases redshift now provides",
    "start": "963330",
    "end": "970900"
  },
  {
    "text": "concurrency scaling so it allows you to deal with this pick of the month without having to continuously predict capacity",
    "start": "970900",
    "end": "978730"
  },
  {
    "text": "or without having to over provision that that data warehouse class right it will",
    "start": "978730",
    "end": "986380"
  },
  {
    "text": "add more processing power just one you need it in seconds and in a transparent manner now ratsy F is complemented with",
    "start": "986380",
    "end": "993940"
  },
  {
    "text": "threat to the spectrum receiver spectrum allows you to query data directly from",
    "start": "993940",
    "end": "998950"
  },
  {
    "text": "the s3 data like but you saw that with a pi√±a hey thank you you've been paying",
    "start": "998950",
    "end": "1003960"
  },
  {
    "text": "attention so yes but with a spectrum you can have the hot data in the redshift",
    "start": "1003960",
    "end": "1010920"
  },
  {
    "text": "placer where it can benefit from the performance of the local drives and use a spectrum to query the cold daytime the",
    "start": "1010920",
    "end": "1017340"
  },
  {
    "text": "s3 data Lake and you can have queries that expand both the frequently accessed data in redshift and the full available",
    "start": "1017340",
    "end": "1023940"
  },
  {
    "text": "datasets in the s3 data link so it provides virtually unlimited scalability and without having to move data to to",
    "start": "1023940",
    "end": "1032910"
  },
  {
    "text": "the data warehouse or without having to scale up the data warehouse just to have more storage",
    "start": "1032910",
    "end": "1039329"
  },
  {
    "text": "um the rumor that with spectrum you pay",
    "start": "1039329",
    "end": "1045089"
  },
  {
    "text": "for the data process to execute your queries so you might want to consider",
    "start": "1045089",
    "end": "1050610"
  },
  {
    "text": "using columnar formats like parquet with not only will optimize performance but",
    "start": "1050610",
    "end": "1055950"
  },
  {
    "text": "also reduce cost so we've seen a lot of building blocks that you can use to",
    "start": "1055950",
    "end": "1062669"
  },
  {
    "start": "1057000",
    "end": "1158000"
  },
  {
    "text": "create that data platform and that you can use to respond to very different user requirements and including",
    "start": "1062669",
    "end": "1068610"
  },
  {
    "text": "requirements that keep appearing right with different with new user needs it's",
    "start": "1068610",
    "end": "1075779"
  },
  {
    "text": "important that also you leverage the flexibility and automation we continuously improve and provide new",
    "start": "1075779",
    "end": "1082889"
  },
  {
    "text": "features for example ready Fred recently announced automatic wlm so make sure you",
    "start": "1082889",
    "end": "1088769"
  },
  {
    "text": "keep including and evolving those data platforms governance as we said we're",
    "start": "1088769",
    "end": "1094799"
  },
  {
    "text": "here to build atuh data legs no data swamps so make sure you know what they",
    "start": "1094799",
    "end": "1099840"
  },
  {
    "text": "ties in the data leak and who can access the data and finally I don't know if",
    "start": "1099840",
    "end": "1105269"
  },
  {
    "text": "I've mentioned this today but you might want to consider using columnar formats like parkette that will optimize",
    "start": "1105269",
    "end": "1111269"
  },
  {
    "text": "performance so we've seen all these building blocks now let's hear how paddy",
    "start": "1111269",
    "end": "1117120"
  },
  {
    "text": "power vesper has been using on combining all these building blocks to build a data platform that has continuously",
    "start": "1117120",
    "end": "1123240"
  },
  {
    "text": "grown and evolved with their business to tell us more we have Stephen Coltrane",
    "start": "1123240",
    "end": "1128250"
  },
  {
    "text": "director of data and Rinella provides ahead of data delivery from Paddy Power Vetter",
    "start": "1128250",
    "end": "1134809"
  },
  {
    "text": "[Applause] good afternoon so I'd like to introduce",
    "start": "1135310",
    "end": "1142730"
  },
  {
    "text": "her organization and provide a bit of context for the journey we've been on for the last two three years to build",
    "start": "1142730",
    "end": "1149269"
  },
  {
    "text": "our modern data platform and my colleague Juana will talk through that journey and share some lessons we've",
    "start": "1149269",
    "end": "1155149"
  },
  {
    "text": "learned from preventing these various building blocks so patty rabbit fair was",
    "start": "1155149",
    "end": "1161509"
  },
  {
    "start": "1158000",
    "end": "1223000"
  },
  {
    "text": "joy was formed in 2016 when the two organizations merged and that created",
    "start": "1161509",
    "end": "1167210"
  },
  {
    "text": "when the largest sports betting and gaming operators in the globe we've 8,000 employees located in offices",
    "start": "1167210",
    "end": "1174919"
  },
  {
    "text": "across Europe the United States and Australia so we operate six brands",
    "start": "1174919",
    "end": "1180019"
  },
  {
    "text": "globally and each of those brands is focused on a particular market or where",
    "start": "1180019",
    "end": "1185029"
  },
  {
    "text": "there's overlap those brands are focused on particular customer segments within those markets and we offer a mix of",
    "start": "1185029",
    "end": "1191899"
  },
  {
    "text": "sports book gaming exchange and daily fantasy sports products across those",
    "start": "1191899",
    "end": "1197120"
  },
  {
    "text": "markets and we deliver those through a range of online and retail channels so",
    "start": "1197120",
    "end": "1202399"
  },
  {
    "text": "we're a daily ritual organization and this creates serious demand for our data services so in 2018 we added over 6",
    "start": "1202399",
    "end": "1210470"
  },
  {
    "text": "million active customers worldwide and those customers generated over 80 billion in traded transactions across",
    "start": "1210470",
    "end": "1217580"
  },
  {
    "text": "all those platforms with our biggest payout being 4.5 million on the World Cup final last year couple our numbers",
    "start": "1217580",
    "end": "1225440"
  },
  {
    "text": "and in online we did over 3 billion transactions and that's quite spiky it's",
    "start": "1225440",
    "end": "1230779"
  },
  {
    "text": "not it's not consistent so we've peaks and troughs running on sporting events and one of our highest-rated events was",
    "start": "1230779",
    "end": "1237230"
  },
  {
    "text": "the Melbourne Cup with 35 million pounds so when we merged we had we needed to",
    "start": "1237230",
    "end": "1244009"
  },
  {
    "start": "1241000",
    "end": "1309000"
  },
  {
    "text": "transform a data capability and but Paddy Power and Betfair individually are quite mature on-premise data platforms",
    "start": "1244009",
    "end": "1251330"
  },
  {
    "text": "but following the merger that created a challenge that we now has essentially two silos where we had two sets of data",
    "start": "1251330",
    "end": "1257690"
  },
  {
    "text": "models two sets of subtly different KPIs and metrics and I meant that our business really very difficult to",
    "start": "1257690",
    "end": "1264470"
  },
  {
    "text": "measure performance so we we then had a challenge to really rapidly build add a",
    "start": "1264470",
    "end": "1270500"
  },
  {
    "text": "new platform that could serve us both back runs consistently on a single platform so both organizations pre-merger were",
    "start": "1270500",
    "end": "1278220"
  },
  {
    "text": "working with cloud providers while being predominantly still on-premise and what our strategy was were image to move to a",
    "start": "1278220",
    "end": "1284520"
  },
  {
    "text": "fully public cloud hosted solution for data and analytics so while the merger created a great challenge for us it was",
    "start": "1284520",
    "end": "1291030"
  },
  {
    "text": "also a great opportunity to rapidly accelerate that migration from on-premise organization to a fully",
    "start": "1291030",
    "end": "1297330"
  },
  {
    "text": "public high public cloud hosted platform so with that with that context I'm gonna",
    "start": "1297330",
    "end": "1303570"
  },
  {
    "text": "hand over to one and able to talk to the actual Iranian like Steven mentioned",
    "start": "1303570",
    "end": "1313590"
  },
  {
    "start": "1309000",
    "end": "2035000"
  },
  {
    "text": "2016 was a great opportunity for us with the merger of paddy power and better and from then on we started our journey to",
    "start": "1313590",
    "end": "1322410"
  },
  {
    "text": "bring the two data on premise data sets together into modern data platform in",
    "start": "1322410",
    "end": "1327990"
  },
  {
    "text": "AWS so I'll walk you through our steps and we'll start with the middle of 2016",
    "start": "1327990",
    "end": "1335299"
  },
  {
    "text": "so AWS has been around in one of the heritage organization and we have had",
    "start": "1335299",
    "end": "1341669"
  },
  {
    "text": "some early on proven business value with a first implementation in s3 redshift",
    "start": "1341669",
    "end": "1348809"
  },
  {
    "text": "and in AWS EMR and the proven business benefits were that lock processing that",
    "start": "1348809",
    "end": "1355169"
  },
  {
    "text": "took a lot of time to happen so it took quite one and a half days it went down",
    "start": "1355169",
    "end": "1362309"
  },
  {
    "text": "to about 20 minutes and queries over these data sets went down with the help of EMR from hours to sub minutes so that",
    "start": "1362309",
    "end": "1370260"
  },
  {
    "text": "was one of the very first use cases that helped build the trust and confidence in",
    "start": "1370260",
    "end": "1375860"
  },
  {
    "text": "AWS as a whole in in 2016 we were already as mentioned using s3 and",
    "start": "1375860",
    "end": "1383070"
  },
  {
    "text": "redshift and as we moved on onboarding more and more data sets into this unified data set we had to upgrade or up",
    "start": "1383070",
    "end": "1391320"
  },
  {
    "text": "size redshift to now 14 dc1 nodes and in",
    "start": "1391320",
    "end": "1398040"
  },
  {
    "text": "the strategic implementation we weren't just moving into the AWS cloud we're also onboarding",
    "start": "1398040",
    "end": "1403430"
  },
  {
    "text": "other strategic technologies so we started leveraging AWS kms service for",
    "start": "1403430",
    "end": "1408920"
  },
  {
    "text": "encryption purposes for our ETL tool for our data integration tool which is the",
    "start": "1408920",
    "end": "1414080"
  },
  {
    "text": "third-party tool deployed to the cloud as well leveraging Lisa two instances we",
    "start": "1414080",
    "end": "1419480"
  },
  {
    "text": "knew from the very beginning we're going to be a large-scale operation in in cloud so we invested early on in full",
    "start": "1419480",
    "end": "1426860"
  },
  {
    "text": "automation of our AWS account management or AWS user management and security that",
    "start": "1426860",
    "end": "1434630"
  },
  {
    "text": "was very very important thing and for that we've used a number of AWS services here again by the beginning of 2017 we",
    "start": "1434630",
    "end": "1444110"
  },
  {
    "text": "were now at 30 terabytes of data and redshift about 19,000 daily queries and",
    "start": "1444110",
    "end": "1451070"
  },
  {
    "text": "about 290 give-and-take active users with this came another up size 220 dc1",
    "start": "1451070",
    "end": "1459680"
  },
  {
    "text": "notes so you can say from 2015 actually when we had four nodes in Ranchi we were",
    "start": "1459680",
    "end": "1465200"
  },
  {
    "text": "now at 20 nodes so it's a pretty rapid increase there at this point we realized",
    "start": "1465200",
    "end": "1472190"
  },
  {
    "text": "that obviously onboarding datasets you can just on more data settings just",
    "start": "1472190",
    "end": "1477560"
  },
  {
    "text": "leave them there hope for everything to go well we were we are a 24/7 operation",
    "start": "1477560",
    "end": "1483050"
  },
  {
    "text": "we need to not only make the data available but we need our platforms to be extremely stable and we need them to",
    "start": "1483050",
    "end": "1489560"
  },
  {
    "text": "be performant so it's absolutely necessary for us to get ready and have a collaborative engagement with AWS",
    "start": "1489560",
    "end": "1496070"
  },
  {
    "text": "support and we've done a lot work with it with Isabel here to make sure that our processes as far as issue resolution",
    "start": "1496070",
    "end": "1503260"
  },
  {
    "text": "escalations is as fast as possible and as efficiently as possible so we've done",
    "start": "1503260",
    "end": "1508850"
  },
  {
    "text": "that through governance of large scale operation at the beginning of 2017 and we have ripped the benefits of that to",
    "start": "1508850",
    "end": "1515030"
  },
  {
    "text": "date so that's an advice that I can give you make sure you think about your operations early on your in your process",
    "start": "1515030",
    "end": "1521600"
  },
  {
    "text": "if you are moving to the cloud as we as we moved on we continued leveraging ec2",
    "start": "1521600",
    "end": "1527300"
  },
  {
    "text": "instances for further deployment of strategic tooling in our data architecture and",
    "start": "1527300",
    "end": "1533010"
  },
  {
    "text": "in 2017 also we realized that redshift is not the answer to all the questions we had and all the use cases so we took",
    "start": "1533010",
    "end": "1541200"
  },
  {
    "text": "on the AWS RDS post-grad for our low",
    "start": "1541200",
    "end": "1546630"
  },
  {
    "text": "latency requirements this was one of the very first times that we were doing some",
    "start": "1546630",
    "end": "1552960"
  },
  {
    "text": "adding a new service and making sure that we are using the appropriate service for our use cases so moving on",
    "start": "1552960",
    "end": "1560640"
  },
  {
    "text": "to 2017 now we were almost there we thought combined datasets this was",
    "start": "1560640",
    "end": "1566850"
  },
  {
    "text": "extremely important for us because you know reporting of two separate data sets of two separate data models of two",
    "start": "1566850",
    "end": "1572850"
  },
  {
    "text": "separate on-prem data warehouses does not really work in the real world right you need one data set",
    "start": "1572850",
    "end": "1578970"
  },
  {
    "text": "that is able to be queried and analyzed and business decisions to be made up so by this point we were already having the",
    "start": "1578970",
    "end": "1586350"
  },
  {
    "text": "majority data and we've done probably our latest or last biggest upsides of",
    "start": "1586350",
    "end": "1592350"
  },
  {
    "text": "redshift to 20:28 nodes and we continue to leverage it AWS ec2 instances for",
    "start": "1592350",
    "end": "1598530"
  },
  {
    "text": "additional strategic technologies that we've deployed to the cloud in 2017",
    "start": "1598530",
    "end": "1605309"
  },
  {
    "text": "there was a game-changer for us so if the keynote speaker mentioned this earlier today that redshift has improved",
    "start": "1605309",
    "end": "1612270"
  },
  {
    "text": "in the past few years ten times and the first who can confirm that I've been",
    "start": "1612270",
    "end": "1617760"
  },
  {
    "text": "there on the journey and I can I have seen that improvements surface and one",
    "start": "1617760",
    "end": "1623520"
  },
  {
    "text": "of the the reason I mentioned this now is because the node type of dc2 launch",
    "start": "1623520",
    "end": "1630330"
  },
  {
    "text": "was the breaking point for us at this point with the no tape challenge with the node type change we have seen an",
    "start": "1630330",
    "end": "1636809"
  },
  {
    "text": "uptake a significant increase of the usage of our platform we were now at 62",
    "start": "1636809",
    "end": "1643049"
  },
  {
    "text": "terabytes of data we had about 439 active users and with at about 30,000",
    "start": "1643049",
    "end": "1648720"
  },
  {
    "text": "daily queries versus the 19,000 we were seeing earlier with obviously with",
    "start": "1648720",
    "end": "1656160"
  },
  {
    "text": "onboarding more data we are now moving to new capabilities so we have built our in-house streaming data they are",
    "start": "1656160",
    "end": "1663000"
  },
  {
    "text": "streaming processing framework with the help of AWS services namely the AWS SNS and SQS",
    "start": "1663000",
    "end": "1669360"
  },
  {
    "text": "services so we were seeing more and more s3 stream data out there with 200",
    "start": "1669360",
    "end": "1678539"
  },
  {
    "text": "terabytes of data in an s3 we it was now time to make sure that our s3 data Lake",
    "start": "1678539",
    "end": "1685320"
  },
  {
    "text": "data is being used and surfaced so we are seeing the first implementation of AWS spectrum and combination with the",
    "start": "1685320",
    "end": "1692789"
  },
  {
    "text": "ratio were already using for some productionize workloads and use cases",
    "start": "1692789",
    "end": "1699919"
  },
  {
    "text": "Aurora came out in 2018 and we jumped right in we evaluated the bells and whistles that",
    "start": "1700370",
    "end": "1707970"
  },
  {
    "text": "AWS Aurora came out with I realized that's something we want to adopt and replace the early on AWS post gray RDS",
    "start": "1707970",
    "end": "1716490"
  },
  {
    "text": "database that we had in place so at this point it was another important moment",
    "start": "1716490",
    "end": "1721649"
  },
  {
    "text": "for us where we've upgraded basically from the older version to a table Sora",
    "start": "1721649",
    "end": "1730220"
  },
  {
    "text": "automation has been a very important trend for us throughout our journey and at this point we started investing more",
    "start": "1730220",
    "end": "1737730"
  },
  {
    "text": "and more into making sure that we use all sorts of AWS services for automation",
    "start": "1737730",
    "end": "1742740"
  },
  {
    "text": "and operations to automate as much as possible so I know aw lambda X for example has been a topic",
    "start": "1742740",
    "end": "1749610"
  },
  {
    "text": "everybody's talking about it we've been using it for quite a while and our data operations they not DevOps",
    "start": "1749610",
    "end": "1754950"
  },
  {
    "text": "what we call them are very very much familiar with this with this technology",
    "start": "1754950",
    "end": "1760139"
  },
  {
    "text": "and have made very many use cases off of it if a year before we focused a lot on",
    "start": "1760139",
    "end": "1769889"
  },
  {
    "text": "making sure our operations are up to speed and they're as good as great as",
    "start": "1769889",
    "end": "1776100"
  },
  {
    "text": "they can be it was now time to establish some ways of working with the product",
    "start": "1776100",
    "end": "1782039"
  },
  {
    "text": "teams and this is because we are pushing AWS a lot we have very complex use cases",
    "start": "1782039",
    "end": "1789269"
  },
  {
    "text": "and our use case is a lot of the times translate into feature requests that we",
    "start": "1789269",
    "end": "1795029"
  },
  {
    "text": "are happily to collaborate with AWS to build and specific in AWS redshift and members on spectrum",
    "start": "1795029",
    "end": "1803010"
  },
  {
    "text": "so we have established some very good governance and ways of working with the product teams to make sure that our",
    "start": "1803010",
    "end": "1809429"
  },
  {
    "text": "requirements and our unique requirements translate into features on their product roadmap so moving into 2018 we were now",
    "start": "1809429",
    "end": "1819630"
  },
  {
    "text": "with our combined data set there we started having streams data a lot of automation took place it was time for us",
    "start": "1819630",
    "end": "1826380"
  },
  {
    "text": "to make use of our data or combine data sets so in mid 2018 we're seeing first",
    "start": "1826380",
    "end": "1833039"
  },
  {
    "text": "implementations of AWS stage maker for advanced analytics capabilities and ml models and as mentioned of automation",
    "start": "1833039",
    "end": "1843600"
  },
  {
    "text": "remains at the forefront of this spinning up and down instance is extremely important for us and not only",
    "start": "1843600",
    "end": "1848880"
  },
  {
    "text": "spinning up and down instances for cross management but also for the peak periods that Stephen we're talking about",
    "start": "1848880",
    "end": "1855049"
  },
  {
    "text": "additionally it is extremely important as you know to be able to deploy configurations and spin up ec2 instances",
    "start": "1855049",
    "end": "1862529"
  },
  {
    "text": "but only spin them up also install what you need to install on there in a quick",
    "start": "1862529",
    "end": "1867809"
  },
  {
    "text": "fashion so we are we can say that we've messed a lot in infrastructure as code",
    "start": "1867809",
    "end": "1872870"
  },
  {
    "text": "cloud formation is very important to that for us and we are able to do that today",
    "start": "1872870",
    "end": "1880010"
  },
  {
    "text": "using the in-house builds during processing framework we made use of AWS",
    "start": "1880370",
    "end": "1887250"
  },
  {
    "text": "kinases and have implemented that for very very extremely low latency",
    "start": "1887250",
    "end": "1894149"
  },
  {
    "text": "requirements so we are our AWS Canisius implementation goes from transaction so",
    "start": "1894149",
    "end": "1901529"
  },
  {
    "text": "from something happening to an operation alert being received in a sub minute fashion because of this in flight",
    "start": "1901529",
    "end": "1908429"
  },
  {
    "text": "streams query possibility",
    "start": "1908429",
    "end": "1912830"
  },
  {
    "text": "I was telling of you guys about advanced analytics news cases this is very",
    "start": "1914290",
    "end": "1919780"
  },
  {
    "text": "important to us and will continue to be important for us like a lot of people in this room I assume we are making use of",
    "start": "1919780",
    "end": "1927070"
  },
  {
    "text": "AWS dynamodb right now and Dax four ml pipelines alongside additional ec2",
    "start": "1927070",
    "end": "1934690"
  },
  {
    "text": "instance is being spun up for additional strategic and we are looking and are",
    "start": "1934690",
    "end": "1939970"
  },
  {
    "text": "implementing already automated deployments of applications finally",
    "start": "1939970",
    "end": "1947590"
  },
  {
    "text": "where we are today we are continuing to make sure that our AWS account management and account governance is up",
    "start": "1947590",
    "end": "1953590"
  },
  {
    "text": "to par and this is goes from security management to making sure that we",
    "start": "1953590",
    "end": "1959919"
  },
  {
    "text": "operate lean making sure that our cost is optimized for different services we use and basically making sure that our",
    "start": "1959919",
    "end": "1966790"
  },
  {
    "text": "workloads are matched to the appropriate AWS service we constantly analyze that to make sure that red shift the use",
    "start": "1966790",
    "end": "1974380"
  },
  {
    "text": "cases we have a red shift are appropriate for ratchet the use cases we have on Aurora are appropriate Fedora and so on with all the other we all the",
    "start": "1974380",
    "end": "1981250"
  },
  {
    "text": "other services at this point we are continuing to evaluate new technologies",
    "start": "1981250",
    "end": "1987330"
  },
  {
    "text": "AWS are releasing are continuing to release new technologies and we want to make sure that we are staying from the",
    "start": "1987330",
    "end": "1993490"
  },
  {
    "text": "central with that to not miss the opportunity so right now we are onboarding glue capabilities as a",
    "start": "1993490",
    "end": "1999160"
  },
  {
    "text": "substitute to the early on EMR which is still present and functioning and we are",
    "start": "1999160",
    "end": "2005250"
  },
  {
    "text": "exploring escena capabilities for our potential use cases lastly but not least",
    "start": "2005250",
    "end": "2011480"
  },
  {
    "text": "we are while we continue to onboard new technologies we want to do that building",
    "start": "2011480",
    "end": "2018570"
  },
  {
    "text": "on the lessons we've learned and making sure that we don't forget about all the other technologies that are now mature",
    "start": "2018570",
    "end": "2024870"
  },
  {
    "text": "so we are continuing to focus on operational excellence and in continuously improving the AWS services",
    "start": "2024870",
    "end": "2032610"
  },
  {
    "text": "with initially onboarding so what are the eight of these benefits that we've",
    "start": "2032610",
    "end": "2038730"
  },
  {
    "start": "2035000",
    "end": "2360000"
  },
  {
    "text": "seen in our journey and this has been a two and a half year journey and there's been a lot of day ones in that let me",
    "start": "2038730",
    "end": "2045900"
  },
  {
    "text": "tell that the keynote mentioned they want we had a lot of day ones and I think you just",
    "start": "2045900",
    "end": "2052290"
  },
  {
    "text": "need to expect that that's going to be the case for your for your journey as well so I'm probably you can probably",
    "start": "2052290",
    "end": "2059460"
  },
  {
    "text": "relate to some of these I'll tell you the Patty power better version of it aw has been scalable and this is",
    "start": "2059460",
    "end": "2066300"
  },
  {
    "text": "extremely important I already mentioned this we started a 5 node cluster we are now at 30 node clusters in redshift",
    "start": "2066300",
    "end": "2073398"
  },
  {
    "text": "Athena Aurora and all the other services are scalable it extremely important because",
    "start": "2073399",
    "end": "2080550"
  },
  {
    "text": "AWS helped us and walked the journey with us so rather than us trying to predict",
    "start": "2080550",
    "end": "2086368"
  },
  {
    "text": "where we should be in two years time and how much capacity we should have for an on-prem data store we were able to grow",
    "start": "2086369",
    "end": "2094980"
  },
  {
    "text": "as Arnie's guru AWS proved to be very",
    "start": "2094980",
    "end": "2101550"
  },
  {
    "text": "varied and very flexible and this is extremely important for a mixed workload if you're if we could not fit the",
    "start": "2101550",
    "end": "2109830"
  },
  {
    "text": "requirements that we had for a SS Kinesis for example on something like it obvious wretched there are completely",
    "start": "2109830",
    "end": "2115050"
  },
  {
    "text": "separate technologies meant for completely different purposes so the fact that we had options it was",
    "start": "2115050",
    "end": "2121170"
  },
  {
    "text": "extremely extremely important for us 24/7 operation we cannot afford to lose",
    "start": "2121170",
    "end": "2127830"
  },
  {
    "text": "or be down for minutes not even minutes it's not acceptable in our in our world",
    "start": "2127830",
    "end": "2133260"
  },
  {
    "text": "so having a provider that has operational support and focus on",
    "start": "2133260",
    "end": "2139650"
  },
  {
    "text": "continuous improvement was extremely important and this is where the work with AWS support including Isabel kami",
    "start": "2139650",
    "end": "2144990"
  },
  {
    "text": "came into play and another another item very important to us we have a large data engineering and data automation",
    "start": "2144990",
    "end": "2153080"
  },
  {
    "text": "developer base and for the complex work play work work items we have they need",
    "start": "2153080",
    "end": "2161580"
  },
  {
    "text": "to be very very proficient so we need readily available information and readily available",
    "start": "2161580",
    "end": "2168170"
  },
  {
    "text": "knowledge for them to get up to speed and then become the experts in the room and that has been the case with AWS and",
    "start": "2168170",
    "end": "2175530"
  },
  {
    "text": "just just to give you guys some insight we've been on calls with AWS support and SS",
    "start": "2175530",
    "end": "2181019"
  },
  {
    "text": "engineers have been extremely supportive with making sure that they share the knowledge with our developers and our",
    "start": "2181019",
    "end": "2186180"
  },
  {
    "text": "engineers and we become self-sufficient which is the best thing you can get lastly but not least Coletti AWS has",
    "start": "2186180",
    "end": "2195299"
  },
  {
    "text": "been is continuing to be very collaborative as far as extending the product for our unique requirements the",
    "start": "2195299",
    "end": "2203329"
  },
  {
    "text": "the VL MQM are rules that AWS has released and isabel was mentioning is",
    "start": "2203329",
    "end": "2208890"
  },
  {
    "text": "something that we needed extremely bad and if something the collaborative will worked on to continue to improve and",
    "start": "2208890",
    "end": "2216239"
  },
  {
    "text": "that's just an example so what have we learned where we live we've learned a lot of things but just to give you guys",
    "start": "2216239",
    "end": "2223099"
  },
  {
    "text": "an idea of what we've seen throughout this journey so as you've seen there's a",
    "start": "2223099",
    "end": "2229920"
  },
  {
    "text": "lot of benefits so using AWS but obviously AWS services are cut edge",
    "start": "2229920",
    "end": "2235880"
  },
  {
    "text": "technology their newly released platforms and with that comes some risk",
    "start": "2235880",
    "end": "2241469"
  },
  {
    "text": "you need to be aware that there will be risks that need to be mitigated together",
    "start": "2241469",
    "end": "2247019"
  },
  {
    "text": "with AWS and that has worked quite well for us another item is extremely",
    "start": "2247019",
    "end": "2252210"
  },
  {
    "text": "important to manage your workload to the appropriate database service we've learned that the hard way using redshift",
    "start": "2252210",
    "end": "2258180"
  },
  {
    "text": "for everything I realizing that's not appropriate and that's the moment when you when we stepped up and looked at",
    "start": "2258180",
    "end": "2263579"
  },
  {
    "text": "what is the use case and what is the aw service during to the different best",
    "start": "2263579",
    "end": "2268650"
  },
  {
    "text": "practices and understanding the different best practices for each of the SS service is key and more importantly",
    "start": "2268650",
    "end": "2274829"
  },
  {
    "text": "working with your stakeholders to make sure they understand it is even more important if we had to do this all over",
    "start": "2274829",
    "end": "2283140"
  },
  {
    "text": "again we probably would have inferred invested a little more early on on the archiving and automation of the data",
    "start": "2283140",
    "end": "2290160"
  },
  {
    "text": "itself because we saw ourselves needing to do that quite late in the game just",
    "start": "2290160",
    "end": "2296729"
  },
  {
    "text": "to make sure that we are using redshift capacity or any other aw service",
    "start": "2296729",
    "end": "2302519"
  },
  {
    "text": "capacity appropriately another item is value of the new service",
    "start": "2302519",
    "end": "2309519"
  },
  {
    "text": "is moving from two large on-prem datasets on platforms that worked and",
    "start": "2309519",
    "end": "2315849"
  },
  {
    "text": "did the job to brand newest technology in the cloud is not easy to swallow by",
    "start": "2315849",
    "end": "2322839"
  },
  {
    "text": "your users and we have hundreds of analysts and there are scientists in our organization that need to move their",
    "start": "2322839",
    "end": "2328839"
  },
  {
    "text": "reports their dashboards there were close to the new platform so with this",
    "start": "2328839",
    "end": "2334589"
  },
  {
    "text": "proving the value of the new services early on is going to help a lot with the adoption of the new platforms and last",
    "start": "2334589",
    "end": "2342599"
  },
  {
    "text": "understanding of product roadmap is very very important it's very important to",
    "start": "2342599",
    "end": "2347859"
  },
  {
    "text": "understand what's coming from a SS and what will be released what's the timeframe and why you should invest internally in making it work by yourself",
    "start": "2347859",
    "end": "2356609"
  },
  {
    "text": "and this has been very important to us so where we are and where are we going",
    "start": "2356609",
    "end": "2362979"
  },
  {
    "start": "2360000",
    "end": "2466000"
  },
  {
    "text": "to do moving forward we are mature we have a mature set of capabilities on our",
    "start": "2362979",
    "end": "2369939"
  },
  {
    "text": "integrated data platform we need to mature and fine-tune those and we will",
    "start": "2369939",
    "end": "2374949"
  },
  {
    "text": "continue to make sure that we use the appropriate database services for our loads we will continue to assess new",
    "start": "2374949",
    "end": "2380380"
  },
  {
    "text": "technologies and make sure that the new technologies are scrutinized and they",
    "start": "2380380",
    "end": "2385839"
  },
  {
    "text": "are ready to be used for our use cases we'll continue focusing focus on the",
    "start": "2385839",
    "end": "2392799"
  },
  {
    "text": "combine making use of the combined datasets for advanced analytics abuse cases and the automation operational",
    "start": "2392799",
    "end": "2400689"
  },
  {
    "text": "excellence and collaborate governance with our stakeholders is going something that we're going to continuously do so",
    "start": "2400689",
    "end": "2407259"
  },
  {
    "text": "this is something that we will continue to do thank you so much for listening",
    "start": "2407259",
    "end": "2413679"
  },
  {
    "text": "and I'll pass it on to Isabelle thank you [Applause]",
    "start": "2413679",
    "end": "2421880"
  },
  {
    "text": "awesome so I hope you can't take too hard this advice all these tips and",
    "start": "2421880",
    "end": "2429500"
  },
  {
    "text": "recommendation that we have provided today use the building blocks combine",
    "start": "2429500",
    "end": "2434700"
  },
  {
    "text": "them try them find the ones that fit each the right use case I cannot stress",
    "start": "2434700",
    "end": "2442620"
  },
  {
    "text": "how important and how much these recommendation are gonna help you so if you didn't take a picture you'll be able",
    "start": "2442620",
    "end": "2448800"
  },
  {
    "text": "to download these lights soon so thank you so much for joining us I hope you've",
    "start": "2448800",
    "end": "2454710"
  },
  {
    "text": "enjoyed please remember to complete the",
    "start": "2454710",
    "end": "2460080"
  },
  {
    "text": "survey give us your feedback we really appreciate it thank you so much and enjoy the rest of the evening",
    "start": "2460080",
    "end": "2465940"
  },
  {
    "text": "[Applause]",
    "start": "2465940",
    "end": "2468308"
  }
]