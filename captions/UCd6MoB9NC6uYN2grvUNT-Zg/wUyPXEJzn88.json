[
  {
    "text": "you know hear me all right hello everybody thank you so much for coming this morning to you to our session about",
    "start": "290",
    "end": "6870"
  },
  {
    "text": "C I asked the Center for Internet Security and how they analyzed billions of NetFlow network traffic records every",
    "start": "6870",
    "end": "14400"
  },
  {
    "text": "day on AWS I'm Bob Stratton I'm a consultant at the public sector",
    "start": "14400",
    "end": "20250"
  },
  {
    "text": "professional services group here at AWS this is my good friend and colleague Oliver Ottawa Oliver and I were partners",
    "start": "20250",
    "end": "27599"
  },
  {
    "text": "in crime for this project and we're delighted to have Brian Copeland with us today brian is c CQ @ CI s so Brian's",
    "start": "27599",
    "end": "36510"
  },
  {
    "text": "going to kick off our session today by telling us a little bit about CAS who they are what they do some of the",
    "start": "36510",
    "end": "41520"
  },
  {
    "text": "services that they offer and how it is that they come to care about NetFlow and",
    "start": "41520",
    "end": "47520"
  },
  {
    "text": "the challenge basically that they had around managing the huge volumes of mail flow data and then all of our myself",
    "start": "47520",
    "end": "53370"
  },
  {
    "text": "we're going to talk a little bit about how we tried to address that challenge using AWS services to build the derelict",
    "start": "53370",
    "end": "59180"
  },
  {
    "text": "platform to allow their analyst to make sense of all of this net flow data and then Brian's going to come back on stage",
    "start": "59180",
    "end": "65369"
  },
  {
    "text": "again and tell us whether the solution Oliver and I came up with actually worked or not whether it's any good so he shares some of the results of that",
    "start": "65369",
    "end": "71610"
  },
  {
    "text": "and then hopefully at the end I'll have some time for questions and answers so",
    "start": "71610",
    "end": "77009"
  },
  {
    "text": "we're hoping to educate you today with some useful architectural concepts we'll explain some of the rationale for why we",
    "start": "77009",
    "end": "82439"
  },
  {
    "text": "adopted the approaches that we did adopt and hopefully that will empower you to reuse some of those concepts maybe and",
    "start": "82439",
    "end": "87990"
  },
  {
    "text": "our attackers see half of you have similar least kisses we are going to",
    "start": "87990",
    "end": "93090"
  },
  {
    "text": "talk about data leaks today because the architecture that we implemented at CAS is fundamentally a dead leg architecture",
    "start": "93090",
    "end": "98729"
  },
  {
    "text": "but this is not a dead Lake pitch we're not going to talk a lot about and all the rationale for why I did Alex on AWS",
    "start": "98729",
    "end": "104520"
  },
  {
    "text": "oh good we're going to assume that you guys already know that little X on AWS are good and that s3 makes a great",
    "start": "104520",
    "end": "109920"
  },
  {
    "text": "storage therefore forget Alex so we're not going to talk a lot about a bit quite a little X we're just going to",
    "start": "109920",
    "end": "115680"
  },
  {
    "text": "talk about the highway we create at this particular one okay so I'm going to hand over to Brian Knight to tell us a bit so",
    "start": "115680",
    "end": "121290"
  },
  {
    "text": "yes thank you Ron okay thanks Bob everybody good morning I get my name is Brian Klock and I'm the",
    "start": "121290",
    "end": "127020"
  },
  {
    "text": "CTO for CIS so I want to talk to you for a minute about CIS in general before I",
    "start": "127020",
    "end": "132180"
  },
  {
    "text": "talk about this Pacific challenge that we that we just ultimately solved with AWS and stand in front of Bob here so step aside so see I",
    "start": "132180",
    "end": "140520"
  },
  {
    "text": "asked basically we're broken up into into two halves if you will so if the first half of the company is focused on",
    "start": "140520",
    "end": "146010"
  },
  {
    "text": "security best practices and that's specifically around our our benchmarks and our our controls products the",
    "start": "146010",
    "end": "153840"
  },
  {
    "text": "benchmarks controls so the benchmarks are our best practice hardening guys any of you can go to our website and",
    "start": "153840",
    "end": "159180"
  },
  {
    "text": "download today and they are our PDF documents that'll tell you for instance how to secure a Windows Server and so to",
    "start": "159180",
    "end": "165300"
  },
  {
    "text": "literally list off the things that you ought to do to make a Windows machine more secure and there's a couple of",
    "start": "165300",
    "end": "171450"
  },
  {
    "text": "different levels there's a there's a level one there's a level two level one being or your baseline level of security that you ought to have when configuring",
    "start": "171450",
    "end": "178560"
  },
  {
    "text": "server securely level two being the more extreme level where if you really want",
    "start": "178560",
    "end": "183570"
  },
  {
    "text": "to go all out and really hard in the system that you can go to that extent the other thing that we do - were also",
    "start": "183570",
    "end": "190290"
  },
  {
    "text": "the home of the sea-ice controls some of you may have may have been familiar with the controls back when they're under stands they used to be the SANS top 20",
    "start": "190290",
    "end": "197550"
  },
  {
    "text": "today they are the CIS controls and these are the top 20 things your organization can do to make yourself",
    "start": "197550",
    "end": "202770"
  },
  {
    "text": "more secure overall in general so it's things like you know having a good asset",
    "start": "202770",
    "end": "207959"
  },
  {
    "text": "inventory software inventory doing vulnerability assessments having this response plan that type of stuff all on",
    "start": "207959",
    "end": "213900"
  },
  {
    "text": "priority order again freely downloadable I encourage you all to check them out they are the top they are the 20 things",
    "start": "213900",
    "end": "220200"
  },
  {
    "text": "you can do but then there's a hundred and seventy or so sub controls under those so it gets to be pretty detailed",
    "start": "220200",
    "end": "225540"
  },
  {
    "text": "and and pretty informative the other half of the organization within CIS is",
    "start": "225540",
    "end": "231900"
  },
  {
    "text": "more operational so we have half which is there are our best practice side of the house then we have the other half which is much more operationally focused",
    "start": "231900",
    "end": "237690"
  },
  {
    "text": "and that's it comes in the form of the multi-state and elections infrastructure information analysis centers so the MS",
    "start": "237690",
    "end": "244530"
  },
  {
    "text": "and III sects so MSN the I I sack are both federally funded by Homeland Security to provide a variety of",
    "start": "244530",
    "end": "251010"
  },
  {
    "text": "cybersecurity services to all state local territorial tribal government entities as well as elections officials",
    "start": "251010",
    "end": "257459"
  },
  {
    "text": "as of last year and so we do that a number of different ways membership within both these ice acts is entirely",
    "start": "257459",
    "end": "263729"
  },
  {
    "text": "free and if any of you are happened a state or local government entity and are not a member of the MSS I encourage",
    "start": "263729",
    "end": "269800"
  },
  {
    "text": "you to join and it's an entirely free service available to you we do things there like we run a 24/7 security",
    "start": "269800",
    "end": "275380"
  },
  {
    "text": "operation center that's this analyzing and providing managed security services analyzing security events we have a",
    "start": "275380",
    "end": "282520"
  },
  {
    "text": "computer emergency response team but those things like Incident Response malware analysis forensics we can send a",
    "start": "282520",
    "end": "288310"
  },
  {
    "text": "team people on site to manage and mitigate the incident for you we have an intelligence team that",
    "start": "288310",
    "end": "293919"
  },
  {
    "text": "basically is put together to to make sense of all the data that we have coming in and issue out cybersecurity",
    "start": "293919",
    "end": "299979"
  },
  {
    "text": "products for folks for situational awareness and that sort of thing so just a wide variety of services but for today",
    "start": "299979",
    "end": "307389"
  },
  {
    "text": "I'm gonna focus primarily on our network monitoring service because that's really where we we found ourselves with it was",
    "start": "307389",
    "end": "314830"
  },
  {
    "text": "sort of a problem as we as we've grown over the years so specifically we have this program called Albert Albert is a",
    "start": "314830",
    "end": "322150"
  },
  {
    "text": "network intrusion detection system that we have deployed throughout the country so it's been deployed in all 50 states",
    "start": "322150",
    "end": "328150"
  },
  {
    "text": "in most cases at a state government level in many cases look government levels as well as some election",
    "start": "328150",
    "end": "333520"
  },
  {
    "text": "organizations we have Albert sensors but you can think of an Albert sensor is it's it's a it's a Dell 1u server it's a",
    "start": "333520",
    "end": "339340"
  },
  {
    "text": "piece of commodity hardware running oracle unbreakable linux on the on the operating system level so open source",
    "start": "339340",
    "end": "345310"
  },
  {
    "text": "there as well as a couple different open source piece of software which i'll talk about in just a minute but these devices",
    "start": "345310",
    "end": "351250"
  },
  {
    "text": "are fully managed as well as monitored by us within within CIS and so we deploy",
    "start": "351250",
    "end": "357099"
  },
  {
    "text": "them and we're getting them out there we're also maintaining them issuing updates to all the signatures on them",
    "start": "357099",
    "end": "362310"
  },
  {
    "text": "and we have around just under 400 of these sensors i think this number is a little out of date now at this point in",
    "start": "362310",
    "end": "367419"
  },
  {
    "text": "time also talked about albert so that running the two main services so the one",
    "start": "367419",
    "end": "372699"
  },
  {
    "text": "is these albert sensors are are deployed as I mentioned at the network edge but",
    "start": "372699",
    "end": "377860"
  },
  {
    "text": "just behind whatever handles the network address translation for the organization's so typically behind a firewall or like a core router or",
    "start": "377860",
    "end": "384280"
  },
  {
    "text": "something like that so they're literally monitoring all incoming and outgoing traffic out of it into an out of an",
    "start": "384280",
    "end": "389320"
  },
  {
    "text": "organization at a network layer and so one of the things that the devices do is they generate net flow data which",
    "start": "389320",
    "end": "395470"
  },
  {
    "text": "you may be familiar with but for those who are not you can think of NetFlow as just a record of all communication",
    "start": "395470",
    "end": "401860"
  },
  {
    "text": "that's that's happened across your network I like to explain it's people that don't have any idea what it is as a looking at your phone bill so if I were",
    "start": "401860",
    "end": "408010"
  },
  {
    "text": "to call Bob for instance and I I talked to him for half an hour you'd see on our phone bill that you see",
    "start": "408010",
    "end": "414070"
  },
  {
    "text": "my phone number reaching out to Bob the time was was thirty minutes you don't know what was",
    "start": "414070",
    "end": "419170"
  },
  {
    "text": "said though during the actual conversation NetFlow is very much that way where we know that source IP reached",
    "start": "419170",
    "end": "424750"
  },
  {
    "text": "out to destination IP we have specific port information there we know that maybe it occurred over a 15-minute time",
    "start": "424750",
    "end": "430690"
  },
  {
    "text": "period we know the amount of traffic that was sent but it doesn't get granular enough to say you know what",
    "start": "430690",
    "end": "435790"
  },
  {
    "text": "specifically was going on in that communication so it's useful to us though we like to run ad-hoc queries",
    "start": "435790",
    "end": "442360"
  },
  {
    "text": "against the data to go back and look retroactively we looked for malicious activity and so that's where we focus",
    "start": "442360",
    "end": "448480"
  },
  {
    "text": "much of this talk today will be focused around the net flow piece the other piece of Albert is focused on network",
    "start": "448480",
    "end": "455800"
  },
  {
    "text": "intrusion detection and we do that with open source software called Cercado and sericata is a high-performance IDs",
    "start": "455800",
    "end": "463930"
  },
  {
    "text": "engine that's running on these boxes and it's all signature based and so we run around 27,000 signatures on each one of",
    "start": "463930",
    "end": "470500"
  },
  {
    "text": "the Albert's sensors we have deployed it varies on a per sensor for sensor basis some are more some are less but on",
    "start": "470500",
    "end": "476560"
  },
  {
    "text": "average it's around 27,000 and the sensors are all generating security",
    "start": "476560",
    "end": "481870"
  },
  {
    "text": "events all the events are then shipped off to our 24/7 sock and analyzed by sake analysts so on average we're",
    "start": "481870",
    "end": "488380"
  },
  {
    "text": "looking at around 10,000 events analyzed a month by our sake analysts and of those they escalate out about 5,000 of",
    "start": "488380",
    "end": "495160"
  },
  {
    "text": "those on average out to state and local governments around the country saying you know here's the sensor saw here's",
    "start": "495160",
    "end": "500919"
  },
  {
    "text": "why we think it's malicious and here's what you ought to go do about it and then it's on them to go and take some sort of action to mediate the host to",
    "start": "500919",
    "end": "507370"
  },
  {
    "text": "reimage the system to what you know whatever the case may be so our",
    "start": "507370",
    "end": "513190"
  },
  {
    "text": "challenge in all this so as we grew the sensor network over the years you know we started off with Albert back in",
    "start": "513190",
    "end": "519479"
  },
  {
    "text": "2010/2011 timeframe we were dealing with a small number of sensors and so a small amount of data",
    "start": "519479",
    "end": "524560"
  },
  {
    "text": "now as I mentioned we're nearly 400 sensors and we're collecting around 48",
    "start": "524560",
    "end": "530350"
  },
  {
    "text": "million records per minute every minute that's an average and so obviously during the day there's",
    "start": "530350",
    "end": "535930"
  },
  {
    "text": "more there's more information coming in in the evenings and the weekends holidays there's less but on average it's around 48 million records per",
    "start": "535930",
    "end": "542740"
  },
  {
    "text": "minute and so I mentioned there in the second bullet it's inconsistent so it's we have these EDS and flows and so you",
    "start": "542740",
    "end": "548950"
  },
  {
    "text": "know the infrastructure would be under a much higher load during the work day but then you know it obviously gets much",
    "start": "548950",
    "end": "554110"
  },
  {
    "text": "quieter in the evening in the weekend so it's it's not consistent data and not consistent flow several petabytes of",
    "start": "554110",
    "end": "561519"
  },
  {
    "text": "data is what we're talking about and so we just had this massive amount of data and there's really no end in sight it's just gonna continue to grow both as we",
    "start": "561519",
    "end": "569010"
  },
  {
    "text": "issue new sensors out there and in and amongst the state and local government space as well as as those specific",
    "start": "569010",
    "end": "576130"
  },
  {
    "text": "networks that we've been monitoring for a period of time continue to grow in size themselves and so like for instance",
    "start": "576130",
    "end": "582040"
  },
  {
    "text": "when we first started Albert we were commonly issuing one gauge sensors to monitor one gig networks now we probably",
    "start": "582040",
    "end": "587980"
  },
  {
    "text": "issue probably more 10 gig sensors we even have some 40 gig and hundred gig centers that were that we have deployed",
    "start": "587980",
    "end": "593320"
  },
  {
    "text": "and so all these sensors obviously are monitoring all this traffic and then then sending massive amounts of log flow",
    "start": "593320",
    "end": "599290"
  },
  {
    "text": "and log data back to us prior to move into the cloud we had a local sandor",
    "start": "599290",
    "end": "604870"
  },
  {
    "text": "restored everything we still have it today we're just about fully off of that but the sand you know it it held",
    "start": "604870",
    "end": "612790"
  },
  {
    "text": "terabytes worth of data when we're working now in the orders of 92 to petabytes and so prior to to solving",
    "start": "612790",
    "end": "619480"
  },
  {
    "text": "this challenge we knew that our sand was you know under resource in terms of capability and speed and performance as",
    "start": "619480",
    "end": "626440"
  },
  {
    "text": "well as just we just didn't have the actual capacity in disk space on the sand so we had to do something so we're",
    "start": "626440",
    "end": "632320"
  },
  {
    "text": "at a point in time where we either we're going to purchase another sand on-prem build out a different on-premise",
    "start": "632320",
    "end": "639910"
  },
  {
    "text": "solution or go with the cloud the other main driver for us as far as challenges is last bullet here and it's really one",
    "start": "639910",
    "end": "646600"
  },
  {
    "text": "of the main things that was frustrating for us in terms of security event analysis was that the queries that we",
    "start": "646600",
    "end": "652540"
  },
  {
    "text": "would run depending on length I'm in the scope of the query could take anywhere from hours to days to even",
    "start": "652540",
    "end": "658000"
  },
  {
    "text": "weeks to complete and so you could kick off a query on a Monday and you wouldn't have your results potentially back until the",
    "start": "658000",
    "end": "663519"
  },
  {
    "text": "following Monday or Tuesday the next week it's totally unacceptable you know so our our analysts would be looking at",
    "start": "663519",
    "end": "668740"
  },
  {
    "text": "something on a Monday and totally have forgotten what they were looking for by the time their the results came back so",
    "start": "668740",
    "end": "673750"
  },
  {
    "text": "we had to had to fix it so here's our desired solution that we knew we wanted",
    "start": "673750",
    "end": "679120"
  },
  {
    "text": "to build so we we have some requirements in place with the federal government as well as some of our constituents we keep",
    "start": "679120",
    "end": "685360"
  },
  {
    "text": "six months worth of the NetFlow data on sort of a sliding window so the older data rolls off and we just keep a",
    "start": "685360",
    "end": "690990"
  },
  {
    "text": "continuous six months six months worth of data obviously needed to be very high-performance we wanted to move from",
    "start": "690990",
    "end": "697420"
  },
  {
    "text": "something it took days and weeks down to minutes and seconds for for speed and and free event analysis it had to be",
    "start": "697420",
    "end": "704139"
  },
  {
    "text": "obviously cost effective because cost is always a factor and because we are the",
    "start": "704139",
    "end": "709329"
  },
  {
    "text": "Center for Internet Security it has to be very secure obviously it's sort of inherent in our name and we are are",
    "start": "709329",
    "end": "716500"
  },
  {
    "text": "trusted with state and local government data and we obviously wanted to take that very seriously we want to make sure",
    "start": "716500",
    "end": "721899"
  },
  {
    "text": "that we're securing it properly the other thing it's very interesting to us too is is you know building a solutions",
    "start": "721899",
    "end": "729040"
  },
  {
    "text": "today that also is able to grow and and have additional capability and features tomorrow and so we're continuing to",
    "start": "729040",
    "end": "734889"
  },
  {
    "text": "looking at what we want to do next with the data and so we want to make sure that whatever platform that we stored the data in now what's going to be able",
    "start": "734889",
    "end": "741639"
  },
  {
    "text": "to grow with us as we grew both in scale as well as in capability and so before I",
    "start": "741639",
    "end": "748209"
  },
  {
    "text": "turn it back over to Bob it's when I mentioned one thing we sigh I mentioned briefly we looked at on-prem and cloud solutions I just wanted to share just a",
    "start": "748209",
    "end": "753819"
  },
  {
    "text": "story about that quickly and so prior to moving to the cloud we we were an on-prem only shop and so we built",
    "start": "753819",
    "end": "760829"
  },
  {
    "text": "infrastructure on Prem we had our data center we had our sand we're storing all of our data and so when we look to a new",
    "start": "760829",
    "end": "766120"
  },
  {
    "text": "solution and we said well clearly we want to build something on Prem right because that's what we're familiar with that's what we're comfortable with cloud",
    "start": "766120",
    "end": "772870"
  },
  {
    "text": "the time was a fairly dirty word within our organization you know there wasn't a lot of comfort I guess you could say",
    "start": "772870",
    "end": "778420"
  },
  {
    "text": "with with folks there as far as storing data in the cloud and making sure that it was properly secured so we defaulted",
    "start": "778420",
    "end": "786069"
  },
  {
    "text": "to on Prem and going down that road so along those lines we were going to build out a Hadoop cluster and so we had worked at",
    "start": "786069",
    "end": "793070"
  },
  {
    "text": "the vendor that loaned us about well an entire rack of servers essentially and they shipped us the servers and we get",
    "start": "793070",
    "end": "799430"
  },
  {
    "text": "all the servers in and we get them start unboxing and we open these things up and we're like okay great we're gonna plug",
    "start": "799430",
    "end": "804470"
  },
  {
    "text": "them in it's gonna work out fine and they have the wrong power supplies and then it's like man this is why we don't",
    "start": "804470",
    "end": "810350"
  },
  {
    "text": "want to do this this is why we this is just like first of a series of problems that just led us down the road of we",
    "start": "810350",
    "end": "816290"
  },
  {
    "text": "don't want to build on Prem anymore we wanted to move into the cloud and so we we decided to go that route so with that",
    "start": "816290",
    "end": "822080"
  },
  {
    "text": "I am going to hand it off to all right",
    "start": "822080",
    "end": "831980"
  },
  {
    "text": "so now you have a sense for the kind of the magnitude of the challenge let's talk a little bit about how we approached coming up with a solution for",
    "start": "831980",
    "end": "838670"
  },
  {
    "text": "for this so before we enter architectures let's just kind of restate",
    "start": "838670",
    "end": "843920"
  },
  {
    "text": "the requirements in the form of kind of mission statements that will map to components of the architecture so starting with ingestion Brian's just",
    "start": "843920",
    "end": "850400"
  },
  {
    "text": "told you about how they've got these sensors deployed that are generating massive volumes of net flow and deep",
    "start": "850400",
    "end": "855440"
  },
  {
    "text": "packet inspection records every couple of minutes these sensors will cut a new file and there's hundreds of the sensors",
    "start": "855440",
    "end": "862040"
  },
  {
    "text": "so we need to ingest that data and then get it in to AWS and then transform it into formats that are useful for",
    "start": "862040",
    "end": "868550"
  },
  {
    "text": "querying and store them and cost efficient and secure storage we need to",
    "start": "868550",
    "end": "874670"
  },
  {
    "text": "provide the analyst access to your points during if the animals can't actually run the queries so we need to provide them with access to fast and",
    "start": "874670",
    "end": "881360"
  },
  {
    "text": "seamless sequel query capabilities so they can query all of the records including records are just fresh off the",
    "start": "881360",
    "end": "887690"
  },
  {
    "text": "sensors with minimal latency we want to try and get the latency down to just a few minutes and but also access to the",
    "start": "887690",
    "end": "893300"
  },
  {
    "text": "oldest records as well Brian mentioned six months retention was the goal for now and we want to do all of this on an",
    "start": "893300",
    "end": "899480"
  },
  {
    "text": "architecture that's secure cost-effective well instrumented reliable and scalable and also fit your",
    "start": "899480",
    "end": "904880"
  },
  {
    "text": "proof fit your proof not only to handle the increasing growth of the service the increasing volumes that were",
    "start": "904880",
    "end": "911000"
  },
  {
    "text": "experiencing but also get your proof from an architectural and figure perspective we wanted to put in place a foundation that would allow a new",
    "start": "911000",
    "end": "917390"
  },
  {
    "text": "features to be added leaders such as automated analyst machine learning and other cool figures",
    "start": "917390",
    "end": "922769"
  },
  {
    "text": "so I said I wasn't going to do the d-league pitchin and neither I am but I wanted to show our standard it licks",
    "start": "922769",
    "end": "928120"
  },
  {
    "text": "slide first of all and you'll see the means football point of the slide is s3 at the center and you'll notice there",
    "start": "928120",
    "end": "933790"
  },
  {
    "text": "are many options for bringing geta into the s3 bsdl Lake and there are many",
    "start": "933790",
    "end": "938860"
  },
  {
    "text": "options for analyzing the data once it's in the data lake but s3 is a center that's a secure reliable durable and",
    "start": "938860",
    "end": "944740"
  },
  {
    "text": "cost a fact of storage layer and the architecture we come up with is no different that centers on s3 as they",
    "start": "944740",
    "end": "949870"
  },
  {
    "text": "it's a central storage for Italy so moving on to the ingestion so back to",
    "start": "949870",
    "end": "956050"
  },
  {
    "text": "our mission statement we need to ingest huge volumes of these files containing",
    "start": "956050",
    "end": "961149"
  },
  {
    "text": "natural and deep packet inspection from the sensors every couple of minutes Brian mentioned the volume is variable",
    "start": "961149",
    "end": "967059"
  },
  {
    "text": "so it's not the same the weekends as during the week and different times of the day has larger larger volumes of",
    "start": "967059",
    "end": "972850"
  },
  {
    "text": "files so the volumes go up and down a lot and over time the average volume is increasing as they might have traffic on",
    "start": "972850",
    "end": "978220"
  },
  {
    "text": "the network increases so we need to provide a mechanism that little latest answers to upload the data ts3 basically",
    "start": "978220",
    "end": "984490"
  },
  {
    "text": "built a big virtual server in AWS and the server it's based in the ec2",
    "start": "984490",
    "end": "990430"
  },
  {
    "text": "instances running in an auto scaling group but not allows us to expand and contract the amount of computing",
    "start": "990430",
    "end": "996309"
  },
  {
    "text": "resource as the demands increase in decrease during the course of the day during the course of the weekend over",
    "start": "996309",
    "end": "1001620"
  },
  {
    "text": "time the average size of this cluster of ec2 servers will increases the average volumes continue to grow up can you do",
    "start": "1001620",
    "end": "1009089"
  },
  {
    "text": "increase so in front of this auto scaling group of ec2 instances we put a network load balancer with an elastic IP",
    "start": "1009089",
    "end": "1015809"
  },
  {
    "text": "address so that the sensors know where to send send their files use the SCP protocol to help load the files to these",
    "start": "1015809",
    "end": "1023000"
  },
  {
    "text": "ec2 servers the load balancer takes care of spreading the load across the different servers and we secure it using",
    "start": "1023000",
    "end": "1029880"
  },
  {
    "text": "techniques like IP whitelisting and with SCP we've got SSH key so we're always certain that we're getting data only",
    "start": "1029880",
    "end": "1036329"
  },
  {
    "text": "from authorized sensors and as each foul gets uploaded from the sensor into one",
    "start": "1036329",
    "end": "1041399"
  },
  {
    "text": "of these ec2 instances we immediately run some software against that file to convert it from a proprietary binary",
    "start": "1041399",
    "end": "1047490"
  },
  {
    "text": "format and to CSV formats that we can then upload to s3",
    "start": "1047490",
    "end": "1053570"
  },
  {
    "text": "so as each file that's uploaded to s3 it's not quite ready for querying yet because we need to do some additional",
    "start": "1053690",
    "end": "1059220"
  },
  {
    "text": "enrichment and structuring of the data so as each file lands in the s3 bucket we immediately trigger a lambda function",
    "start": "1059220",
    "end": "1065070"
  },
  {
    "text": "to run against that file and of course there are like many many many files uploading all simultaneously from this",
    "start": "1065070",
    "end": "1071160"
  },
  {
    "text": "cluster and the lambdas will be firing concurrently and the lambda function we call it the enriched lambda function it's going to go through each vowel and",
    "start": "1071160",
    "end": "1077910"
  },
  {
    "text": "look at each record within the file and enrich it by adding additional phases that the analyst told us that they",
    "start": "1077910",
    "end": "1083730"
  },
  {
    "text": "needed to make the records more useful things like Li ap autonomous system number directionality indicators et",
    "start": "1083730",
    "end": "1089789"
  },
  {
    "text": "cetera and then around the function is going to write the data back into s3 into a new set of folders in s3 and the",
    "start": "1089789",
    "end": "1097169"
  },
  {
    "text": "folder structure is important you'll see on the bottom left there that we have a like a prefix structure or folders for",
    "start": "1097169",
    "end": "1102840"
  },
  {
    "text": "each sensor so each sensor gets its own set of top level folders and then we also have folders for the time ranges",
    "start": "1102840",
    "end": "1108870"
  },
  {
    "text": "within the data so we take the time stamp of the records in the file and break it into year month our day an hour",
    "start": "1108870",
    "end": "1116280"
  },
  {
    "text": "so that way basically each folder contains the data for a particular sensor for a particular R and that makes it very quick and easy for us to find",
    "start": "1116280",
    "end": "1122640"
  },
  {
    "text": "the data for any given sensor for any given time range and this is very important as we'll see in a minute for whenever we go to query the data we need",
    "start": "1122640",
    "end": "1129000"
  },
  {
    "text": "to know exactly where that data lives in order to quickly get to the data that we care about so nearly all times equal",
    "start": "1129000",
    "end": "1135330"
  },
  {
    "text": "access this is for the analyst need access to get it with very little yet and so so we've written these files so",
    "start": "1135330",
    "end": "1141419"
  },
  {
    "text": "far event or just hardly any latency because as each file comes in from the sensor we process it through the receiver instances and then through the",
    "start": "1141419",
    "end": "1147690"
  },
  {
    "text": "enriched lambda we store it and to make it available to the analysts through",
    "start": "1147690",
    "end": "1152730"
  },
  {
    "text": "sequel queries we need to update something called the glue catalog now it's our service where we store metadata",
    "start": "1152730",
    "end": "1157890"
  },
  {
    "text": "that allows the fact of layers to define tables that can be queried using standard sequel and those tables map to",
    "start": "1157890",
    "end": "1165870"
  },
  {
    "text": "data stored in s3 and the table server properly called partitions you can have many partitions in a table and each",
    "start": "1165870",
    "end": "1170970"
  },
  {
    "text": "partition maps to exactly one of those folders that I talked about before and that's how we get really efficient query",
    "start": "1170970",
    "end": "1177030"
  },
  {
    "text": "processing if you're interested in data for just one sensor say for one day we can get directly to the set of folders",
    "start": "1177030",
    "end": "1182760"
  },
  {
    "text": "that store the data for that sensor and that day a technique called partition pruning this is really important to map the",
    "start": "1182760",
    "end": "1188559"
  },
  {
    "text": "partitions in the table to the folder structure that we've used in s3 and the lambda function as it creates a new",
    "start": "1188559",
    "end": "1194740"
  },
  {
    "text": "folder in s3 will update the partition of that table in the glue catalog so as",
    "start": "1194740",
    "end": "1200470"
  },
  {
    "text": "soon as that theater arrives in s3 the analyst can immediately start running queries so we've not introduced any",
    "start": "1200470",
    "end": "1205539"
  },
  {
    "text": "additional latency there at all that's how we get the new real-time access and we managed to keep a lien see down to",
    "start": "1205539",
    "end": "1210759"
  },
  {
    "text": "about under five minutes between the sensor seeing the data and they they all know it's been able to query it and one",
    "start": "1210759",
    "end": "1218500"
  },
  {
    "text": "point I should have mentioned before is one of the fundamental tenants of coming up with an architecture that's scalable",
    "start": "1218500",
    "end": "1224039"
  },
  {
    "text": "across these huge volumes of data and also going to be scalable into the future is to try and have everything as",
    "start": "1224039",
    "end": "1229419"
  },
  {
    "text": "parallelizable that's a real word parallelizable so busily not have any bottlenecks at all in the architecture",
    "start": "1229419",
    "end": "1235149"
  },
  {
    "text": "that's what we want to Sonoma or how many sensors we have and how many fouls they send we want to be able to process",
    "start": "1235149",
    "end": "1240700"
  },
  {
    "text": "it with wrote adding latency just by adding additional parallel processing capacity so you've seen how we did that",
    "start": "1240700",
    "end": "1246519"
  },
  {
    "text": "with the ec2 instance is not a scaling the lambda functions do the same thing processing each vowel concurrently the",
    "start": "1246519",
    "end": "1255299"
  },
  {
    "text": "final thing I wanted to mention here is the lambda function also updates and SQS queue that will trigger an ETL process",
    "start": "1255299",
    "end": "1262690"
  },
  {
    "text": "we're going to talk a little bit more about that in just a bit but the files these near-real-time files that we've landed we call these stage 0 these are great",
    "start": "1262690",
    "end": "1269409"
  },
  {
    "text": "for low latency queries but as you can imagine we end up with a lot of relatively small files accumulating over",
    "start": "1269409",
    "end": "1274509"
  },
  {
    "text": "time while they're great for near real-time access they're not so efficient for long term queries so we",
    "start": "1274509",
    "end": "1279580"
  },
  {
    "text": "need a process to visit it condense to compact those files into smaller numbers of bigger files",
    "start": "1279580",
    "end": "1285490"
  },
  {
    "text": "so that we could store them more cost-effectively and process the queries more cost-effectively and faster over",
    "start": "1285490",
    "end": "1291429"
  },
  {
    "text": "larger periods of time these are the queries that run across new days and months of data this is ETL processor i'm",
    "start": "1291429",
    "end": "1297340"
  },
  {
    "text": "not going to talk much about now because Oliver's going to take us through a deep dive of that in just a little bit the",
    "start": "1297340",
    "end": "1303039"
  },
  {
    "text": "ETL process results any second set of files we call that stage one and a second set of tables that map to those",
    "start": "1303039",
    "end": "1309789"
  },
  {
    "text": "files because those two stage 1 tables so now we've got two sets of of table stage 0 and stage 1 stage 0 is optimized",
    "start": "1309789",
    "end": "1316389"
  },
  {
    "text": "for the near real-time stage wellness for they the longer-term analysis in the last",
    "start": "1316389",
    "end": "1323280"
  },
  {
    "text": "part of the equation is we wanted to hide that implementation detail of at stage zero in stage one from the analyst",
    "start": "1323280",
    "end": "1328950"
  },
  {
    "text": "Donna's just want to run a query with it caring about which table to query so we",
    "start": "1328950",
    "end": "1333960"
  },
  {
    "text": "did that absolutely just a simple view that effectively just does a union between two queries the view queries",
    "start": "1333960",
    "end": "1339840"
  },
  {
    "text": "both stage zero and stage one tables and does the Union of the record sets it's",
    "start": "1339840",
    "end": "1344970"
  },
  {
    "text": "coming back and if you look at the sequel here you can see that each of those sub queries has a where clause that constrains the query to a",
    "start": "1344970",
    "end": "1352200"
  },
  {
    "text": "particular time range so here you can see that we're looking at stage zero for any R any time period that's less than",
    "start": "1352200",
    "end": "1358650"
  },
  {
    "text": "that timestamp and the timestamp represents a from current time so it's like six hours back in time so what this",
    "start": "1358650",
    "end": "1365640"
  },
  {
    "text": "means is that you know six hours from now we're going to be pulling the data from from stage zero tables anything",
    "start": "1365640",
    "end": "1371160"
  },
  {
    "text": "more than six hours old we're going to pull it from stage one tables and the analysts don't need to know what or care a bit about that detail behind the",
    "start": "1371160",
    "end": "1377400"
  },
  {
    "text": "scenes and so those timestamps are hard-coded in the view you actually have lambda function that runs every our",
    "start": "1377400",
    "end": "1383280"
  },
  {
    "text": "behind the scenes it just updates the definition of the view so that timestamp has always changed to reflect like six",
    "start": "1383280",
    "end": "1388710"
  },
  {
    "text": "hours lag from the current time and finally the architecture all needed to",
    "start": "1388710",
    "end": "1394860"
  },
  {
    "text": "be secured cost-effective well instrumented reliable and scalable and future proof and to make sure we",
    "start": "1394860",
    "end": "1401370"
  },
  {
    "text": "addressed all of those factors we drew on the AWS well architected framework I'm sure most of you are probably",
    "start": "1401370",
    "end": "1406890"
  },
  {
    "text": "familiar with that you're not you should definitely google it there's a very good white paper that talks about the well",
    "start": "1406890",
    "end": "1412590"
  },
  {
    "text": "architected framework and provides five pillars bit dimensions that you should think about for any application",
    "start": "1412590",
    "end": "1418680"
  },
  {
    "text": "architecture that you're deploying AWS in fact there's actually a tool known AWS console the well architected tool",
    "start": "1418680",
    "end": "1424620"
  },
  {
    "text": "that will allow you to do an audit of your processes and your architecture to help you gauge how compliant you are",
    "start": "1424620",
    "end": "1430560"
  },
  {
    "text": "with these best practices so we drew heavily upon the well architected framework as we architected this solution and we run through the tool at",
    "start": "1430560",
    "end": "1437490"
  },
  {
    "text": "the end to make sure that we and we're compliant with all the best practices so this point when I hand over the Oliver",
    "start": "1437490",
    "end": "1443610"
  },
  {
    "text": "he's going to talk us through the ETL process thank you Bob hello everybody",
    "start": "1443610",
    "end": "1450760"
  },
  {
    "text": "so talking about the well architected framework the goal of the ETL is to",
    "start": "1450760",
    "end": "1456880"
  },
  {
    "text": "optimize around two of the pillars performance and cost in this case that",
    "start": "1456880",
    "end": "1463210"
  },
  {
    "text": "means that we want to make queries from faster and cheaper in the next few slides we're gonna go over the",
    "start": "1463210",
    "end": "1470320"
  },
  {
    "text": "architectural tools and techniques that we use to achieve this we're also going to provide a little bit more context",
    "start": "1470320",
    "end": "1476679"
  },
  {
    "text": "about the engineer decisions that we take along the way to create this solution now reviewing what Bob",
    "start": "1476679",
    "end": "1484600"
  },
  {
    "text": "mentioned we have two stages these are two separate tables state 0 stage 1",
    "start": "1484600",
    "end": "1490590"
  },
  {
    "text": "stage 0 is optimized for real near real-time queries put files as quickly",
    "start": "1490590",
    "end": "1497590"
  },
  {
    "text": "as possible these are CSV files then we end up with a viable number of files stage 1 is",
    "start": "1497590",
    "end": "1505630"
  },
  {
    "text": "optimized for longer-term queries we take stage 0 files there's an ETL",
    "start": "1505630",
    "end": "1511900"
  },
  {
    "text": "process that consolidates and converts those files into optimized 4k files this",
    "start": "1511900",
    "end": "1518830"
  },
  {
    "text": "is a feat number of larger files now why do we do this when an analyst at the CIA",
    "start": "1518830",
    "end": "1526510"
  },
  {
    "text": "security operation center they need to retro actively investigate an event they",
    "start": "1526510",
    "end": "1532780"
  },
  {
    "text": "they may run queries that could span weeks or more months worth of data you",
    "start": "1532780",
    "end": "1538150"
  },
  {
    "text": "can imagine at this volume this is massive amounts of data having this",
    "start": "1538150",
    "end": "1543700"
  },
  {
    "text": "optimized files means that when you query you can target specific files or",
    "start": "1543700",
    "end": "1550419"
  },
  {
    "text": "even parts of the files based on the query based on the fields of the query",
    "start": "1550419",
    "end": "1555910"
  },
  {
    "text": "or the filters use in the query if you use an a tool as Athena for example",
    "start": "1555910",
    "end": "1562799"
  },
  {
    "text": "being able to pull less data and process less data means that the query will run",
    "start": "1562799",
    "end": "1569290"
  },
  {
    "text": "faster with the fina or redshift the pricing model is that you pay as you go",
    "start": "1569290",
    "end": "1575380"
  },
  {
    "text": "so it not only goes faster it's also cheaper faster and cheaper",
    "start": "1575380",
    "end": "1582000"
  },
  {
    "text": "let's review the architecture of the ETL process at a high level it's an auto",
    "start": "1582860",
    "end": "1590220"
  },
  {
    "text": "scaling group of ec2 instances the entry point into the ETL processes and SQS",
    "start": "1590220",
    "end": "1596100"
  },
  {
    "text": "queue the the auto scaling group scales in and out we can scale out as much as",
    "start": "1596100",
    "end": "1603090"
  },
  {
    "text": "needed and then once our partitions our processes can be scaling this is all",
    "start": "1603090",
    "end": "1608520"
  },
  {
    "text": "driven by the depth of the queue and then each ec2 instance in the auto",
    "start": "1608520",
    "end": "1615030"
  },
  {
    "text": "scaling group is processing one partition at a time that means as what",
    "start": "1615030",
    "end": "1621060"
  },
  {
    "text": "was mentioning we can create parallel paralyze the ETL process as much as",
    "start": "1621060",
    "end": "1626520"
  },
  {
    "text": "needed the the sqs queue contains messages for",
    "start": "1626520",
    "end": "1632010"
  },
  {
    "text": "the specific partitions so the state 0 that was mentioning the lambda functions",
    "start": "1632010",
    "end": "1638580"
  },
  {
    "text": "once the partition is ready to be ETA we'll put a message in the queue the",
    "start": "1638580",
    "end": "1643770"
  },
  {
    "text": "queue contains the partition values what is the censor time an hour that needs to",
    "start": "1643770",
    "end": "1650310"
  },
  {
    "text": "be ETL an ec2 instance will pull one of those messages it processes one",
    "start": "1650310",
    "end": "1656730"
  },
  {
    "text": "partition at a time and then there's a script that reads the CSV file from that",
    "start": "1656730",
    "end": "1662190"
  },
  {
    "text": "partition in state zero creates a data frame and then converts that file into a",
    "start": "1662190",
    "end": "1669450"
  },
  {
    "text": "pocketed data frame and put and then writes the files as s perking files into",
    "start": "1669450",
    "end": "1675930"
  },
  {
    "text": "the stage 1 st path once it's finished",
    "start": "1675930",
    "end": "1681120"
  },
  {
    "text": "creating the park' files it will add the partition to the glue catalog the stage",
    "start": "1681120",
    "end": "1687990"
  },
  {
    "text": "1 table and at that point that data becomes available to be queried now some",
    "start": "1687990",
    "end": "1698160"
  },
  {
    "text": "of you that might be familiar with data like architectures enable us might be",
    "start": "1698160",
    "end": "1703770"
  },
  {
    "text": "asking you know why not use something like fire hose to solve this the answer",
    "start": "1703770",
    "end": "1709800"
  },
  {
    "text": "is that initially we did fire hose as this really cool feature where he can",
    "start": "1709800",
    "end": "1715460"
  },
  {
    "text": "stream data into a firehose and then it will convert that into part K for you as",
    "start": "1715460",
    "end": "1721370"
  },
  {
    "text": "we were working with CIS within a couple of screens were able to get an",
    "start": "1721370",
    "end": "1726440"
  },
  {
    "text": "end-to-end solution with this approach now as we started observing what kind of",
    "start": "1726440",
    "end": "1733250"
  },
  {
    "text": "queries analysts were running it became very obvious that they would target specific sensors and specific times so",
    "start": "1733250",
    "end": "1740539"
  },
  {
    "text": "partition by sensor in time was a important thing for us to to get performance benefits in this platform",
    "start": "1740539",
    "end": "1749110"
  },
  {
    "text": "today firehose only allows the partition base on ingestion time and then to",
    "start": "1749110",
    "end": "1755960"
  },
  {
    "text": "partition based on sensor in in capture time required going into the payload of",
    "start": "1755960",
    "end": "1761659"
  },
  {
    "text": "the data to be able to create the right partitions we can see they're put in",
    "start": "1761659",
    "end": "1769070"
  },
  {
    "text": "taking those part K files from firehose and then trying to repartition in that",
    "start": "1769070",
    "end": "1774309"
  },
  {
    "text": "that data again but then we realize that this data is coming in micro batches we",
    "start": "1774309",
    "end": "1781490"
  },
  {
    "text": "ingest the data as files they come from a sensor so we know what is the sensor",
    "start": "1781490",
    "end": "1787159"
  },
  {
    "text": "and the date the time that it was captured so we parted streaming that",
    "start": "1787159",
    "end": "1793039"
  },
  {
    "text": "theory partitioning shuffling it again turned out to be you know busy work and also not cost-effective",
    "start": "1793039",
    "end": "1799580"
  },
  {
    "text": "so that that influenced us to take an approach or with we would ETL one",
    "start": "1799580",
    "end": "1806899"
  },
  {
    "text": "partition at a time additionally we realize that we wanted",
    "start": "1806899",
    "end": "1813260"
  },
  {
    "text": "to have better control in terms of the per K files being produced by the ETL process you know we wanted to apply",
    "start": "1813260",
    "end": "1819409"
  },
  {
    "text": "techniques as Spock 18 and also have better control of row groups inside that",
    "start": "1819409",
    "end": "1824510"
  },
  {
    "text": "park' and then those became factors in why we wrote a script to sort of like",
    "start": "1824510",
    "end": "1832669"
  },
  {
    "text": "take control of those fields at the level in the same way maybe some of you",
    "start": "1832669",
    "end": "1841990"
  },
  {
    "text": "familiar with data like solutions might be asking why not EMR or new job or spark I came the",
    "start": "1841990",
    "end": "1853030"
  },
  {
    "text": "answer is that initially we did now as we started cranking up the volume and",
    "start": "1853030",
    "end": "1858610"
  },
  {
    "text": "iterating we we realized that we needed to address certain factors and",
    "start": "1858610",
    "end": "1863980"
  },
  {
    "text": "challenges one of them is that when you have a cluster you read data from a",
    "start": "1863980",
    "end": "1871120"
  },
  {
    "text": "number of sensors and you want to ETL those sensors that the data is distributed across over the cluster if",
    "start": "1871120",
    "end": "1879190"
  },
  {
    "text": "you need to repartition the data that creates a lot of network shuffling from a computational and network stand",
    "start": "1879190",
    "end": "1886050"
  },
  {
    "text": "standpoint that's expensive but it also could potentially become a bottleneck at",
    "start": "1886050",
    "end": "1891490"
  },
  {
    "text": "the volume that we're talking about here there's another characteristic about the",
    "start": "1891490",
    "end": "1898120"
  },
  {
    "text": "data here which is the data is queue by sensors you have very large sensors like",
    "start": "1898120",
    "end": "1904270"
  },
  {
    "text": "you know sensor in taxes and then smaller stands were liking in a small locality so what that does is that you",
    "start": "1904270",
    "end": "1912580"
  },
  {
    "text": "know some sensors may capture a lot of data but then the vast majority of the sensors at the same amount of data when",
    "start": "1912580",
    "end": "1921220"
  },
  {
    "text": "the data is distributed across in a cluster and you're trying to ETL you may finish most of the sensors very quickly",
    "start": "1921220",
    "end": "1927940"
  },
  {
    "text": "and then end up with a few stragglers then those stragglers because the data",
    "start": "1927940",
    "end": "1933250"
  },
  {
    "text": "is distributed across the cluster notes and closer makes it harder to scale down",
    "start": "1933250",
    "end": "1940380"
  },
  {
    "text": "based on based on those larger sensors also having like a big job creates",
    "start": "1940380",
    "end": "1948910"
  },
  {
    "text": "challenges if something goes wrong like if you something goes wrong you have to either redo the whole job or built logic",
    "start": "1948910",
    "end": "1955630"
  },
  {
    "text": "to figure out what went wrong to just redo that part there were some other factors like when we're using spark",
    "start": "1955630",
    "end": "1963750"
  },
  {
    "text": "things like having multi-part uploads in s3 dynamic partitioning and using pocketing",
    "start": "1963750",
    "end": "1971590"
  },
  {
    "text": "that were challenged challenging using s3 using spark but now one point that I",
    "start": "1971590",
    "end": "1978550"
  },
  {
    "text": "want to make is that a lot of these factors one way or the other we experimented with various solutions as",
    "start": "1978550",
    "end": "1984490"
  },
  {
    "text": "we iterated and the worst solutions to some of these challenges but now the combination of all these factors then",
    "start": "1984490",
    "end": "1992710"
  },
  {
    "text": "they need to have better control of all these factors is what drove us and",
    "start": "1992710",
    "end": "1997840"
  },
  {
    "text": "guided us to to the solution of having single ec2 instances managing one",
    "start": "1997840",
    "end": "2003450"
  },
  {
    "text": "partition at a time to better scale horizontally this is not an arc on one",
    "start": "2003450",
    "end": "2009210"
  },
  {
    "text": "tool versus the other it's more of a testament of iterating grabbing feedback",
    "start": "2009210",
    "end": "2015510"
  },
  {
    "text": "and then you know based on that feedback optimising based on the specifics of the",
    "start": "2015510",
    "end": "2021630"
  },
  {
    "text": "use case and he's in the the right tool for the job I've mentioned bucketing a",
    "start": "2021630",
    "end": "2028980"
  },
  {
    "text": "number of times in anon I wanted to expand on pocketing because this was a technique that significantly improved",
    "start": "2028980",
    "end": "2036240"
  },
  {
    "text": "performance for this use case before I talk about pocketing let's review",
    "start": "2036240",
    "end": "2041970"
  },
  {
    "text": "partitioning so as Bob mentioned partitioning works by if you see the",
    "start": "2041970",
    "end": "2047670"
  },
  {
    "text": "sequel statement here partition by you have P sensor year date our those are the partition fields and they will map",
    "start": "2047670",
    "end": "2054330"
  },
  {
    "text": "to a folder structure and s3 and this works really well for for",
    "start": "2054330",
    "end": "2061050"
  },
  {
    "text": "values as sensor you have you know few hundred sensors so you have at the top level a few hundred folders and even as",
    "start": "2061050",
    "end": "2067860"
  },
  {
    "text": "you go at the P our level where you have you know thousands of combinations it",
    "start": "2067860",
    "end": "2073980"
  },
  {
    "text": "works pretty well now for a field as IP addresses IP addresses you have billions",
    "start": "2073980",
    "end": "2081090"
  },
  {
    "text": "of IP addresses over four billion for ipv4 it wouldn't make sense to have",
    "start": "2081090",
    "end": "2086730"
  },
  {
    "text": "billions of folders IP addresses it's what we call a a field that has high",
    "start": "2086730",
    "end": "2094230"
  },
  {
    "text": "cardinality that means that you have a very large space of unique values",
    "start": "2094230",
    "end": "2100100"
  },
  {
    "text": "for high cardinality type of field we use bucketing so you see when we define",
    "start": "2100100",
    "end": "2106700"
  },
  {
    "text": "the table we'll see in the sequel statement defined the table says cluster by sip that's short for source IP into",
    "start": "2106700",
    "end": "2115400"
  },
  {
    "text": "seven buckets what that is doing is that based on the value of of the source IP",
    "start": "2115400",
    "end": "2122330"
  },
  {
    "text": "it will distribute the the records associated with that value into seven",
    "start": "2122330",
    "end": "2129290"
  },
  {
    "text": "files and predictable using its predictable using a hash function so it",
    "start": "2129290",
    "end": "2135290"
  },
  {
    "text": "hashes and then uses the mod function which is just dividing and based on the reminder you get a determined number of",
    "start": "2135290",
    "end": "2143720"
  },
  {
    "text": "files so using this simple algorithm when we write the bucket that files we can",
    "start": "2143720",
    "end": "2150290"
  },
  {
    "text": "reliably predict where the data is going to be for a specific value and then when",
    "start": "2150290",
    "end": "2156470"
  },
  {
    "text": "you query you apply the same the same algorithm and you know - like Athena",
    "start": "2156470",
    "end": "2161900"
  },
  {
    "text": "knows which file so you end up with seven files but based on that value you",
    "start": "2161900",
    "end": "2167330"
  },
  {
    "text": "can actually know which of those seven files may contain the value that you're looking for so putting it all together",
    "start": "2167330",
    "end": "2177460"
  },
  {
    "text": "here's an example of a hypothetical query in this case we have acquired that",
    "start": "2177460",
    "end": "2184790"
  },
  {
    "text": "is selecting destination IP and then it's trying to target a specific sense or a specific our specific destination",
    "start": "2184790",
    "end": "2192590"
  },
  {
    "text": "port the specific source IP because the queries using as you note in the word",
    "start": "2192590",
    "end": "2198560"
  },
  {
    "text": "close the underscore sensor the underscore our those are the partition fields so it can quickly figure out what",
    "start": "2198560",
    "end": "2205940"
  },
  {
    "text": "is the folder that may contain the data it's also using if you see the last part",
    "start": "2205940",
    "end": "2213290"
  },
  {
    "text": "of the query it's using add source I'd be looking for a specific IP the zip I",
    "start": "2213290",
    "end": "2219770"
  },
  {
    "text": "see IP is the source IP and this is the bucketing field so because it's using",
    "start": "2219770",
    "end": "2225320"
  },
  {
    "text": "the bucketing field under that folder there's seven files but it can predict",
    "start": "2225320",
    "end": "2230570"
  },
  {
    "text": "which file may contain that now it knows one of those seven files",
    "start": "2230570",
    "end": "2236410"
  },
  {
    "text": "then because it's also using this the nation IP this is a columnar format so",
    "start": "2236410",
    "end": "2241910"
  },
  {
    "text": "this is we're using that column as our Select field it's able to go into the parquet file and it's keyed larger parts",
    "start": "2241910",
    "end": "2249740"
  },
  {
    "text": "of the that single file and it's also using D ports so the park' format has an",
    "start": "2249740",
    "end": "2256040"
  },
  {
    "text": "ended so it knows also which portions of the file may contain this specific port",
    "start": "2256040",
    "end": "2261970"
  },
  {
    "text": "again we'd reduce the amount of data being transferred and processed by the",
    "start": "2261970",
    "end": "2267650"
  },
  {
    "text": "query tool so by virtue of of processing and transferring less data it makes it",
    "start": "2267650",
    "end": "2272750"
  },
  {
    "text": "faster and cheaper I'm going to turn it over to Brian and he's gonna show you",
    "start": "2272750",
    "end": "2277850"
  },
  {
    "text": "some of the results okay thanks Oliver so up into this point I",
    "start": "2277850",
    "end": "2285260"
  },
  {
    "text": "talked about the challenges that we had with the data and things we wanted to do with it Bob and Oliver talked about the",
    "start": "2285260",
    "end": "2290390"
  },
  {
    "text": "different iterations of ways that they tried we all tried to solve the problem that's all well and good but at the end",
    "start": "2290390",
    "end": "2296540"
  },
  {
    "text": "of day did it work like did we make any improvements today what's what's what's",
    "start": "2296540",
    "end": "2301550"
  },
  {
    "text": "the what's the punchline and so that's we're gonna talk about right now so results are in and did it all work out",
    "start": "2301550",
    "end": "2310240"
  },
  {
    "text": "yes it did of course otherwise I wouldn't be here probably talking you off I didn't",
    "start": "2310240",
    "end": "2315510"
  },
  {
    "text": "so let me run through a couple of examples these are these are some common queries that our analysts run on a",
    "start": "2315510",
    "end": "2320940"
  },
  {
    "text": "regular basis to kind of give you a sense for what it looked like free cloud and then and then post cloud so the",
    "start": "2320940",
    "end": "2328500"
  },
  {
    "text": "first one I'm going to talk about it's just a simple query that we run where we're counting all the records generated by a single sensor right and so the old",
    "start": "2328500",
    "end": "2336119"
  },
  {
    "text": "way of running it across our sand local within our data center would take about 15 minutes to run which is not not",
    "start": "2336119",
    "end": "2341490"
  },
  {
    "text": "terrible it's it's pretty respectable AWS two minutes all right so that's that's good that's that's seven and a",
    "start": "2341490",
    "end": "2348359"
  },
  {
    "text": "half times faster overall which is which is which is fine however when you wanted",
    "start": "2348359",
    "end": "2354240"
  },
  {
    "text": "to how all records generated by all sensors this is a much broader query obviously and it's much larger in scope",
    "start": "2354240",
    "end": "2360030"
  },
  {
    "text": "and so it would take us around 36 hours to run this query to get the results back when we started it to when we",
    "start": "2360030",
    "end": "2366330"
  },
  {
    "text": "actually got the results turned around back to us so this is obviously one of our longer queries to run so 36 hours",
    "start": "2366330",
    "end": "2372600"
  },
  {
    "text": "down to 3 minutes this is huge 720 times faster so this is this is the",
    "start": "2372600",
    "end": "2379440"
  },
  {
    "text": "sort of performance we were hoping to get out of this and and we've actually able to realize so I'll just I'll run",
    "start": "2379440",
    "end": "2385320"
  },
  {
    "text": "through just a couple more actually this is this next one here is a screen shot the kind of guys that give you a sense of how the analysts interacted the data",
    "start": "2385320",
    "end": "2391740"
  },
  {
    "text": "so this is where they write their sequel queries this particular query as essentially selecting the sum of all the",
    "start": "2391740",
    "end": "2397380"
  },
  {
    "text": "bytes generated across the across the sensors for for a given hour and you can",
    "start": "2397380",
    "end": "2402630"
  },
  {
    "text": "see there and that in that red box it took three minutes to run and this is a query again it would take several hours",
    "start": "2402630",
    "end": "2408210"
  },
  {
    "text": "to run prior but as Oliver talked about the the advent for us anyway with",
    "start": "2408210",
    "end": "2413310"
  },
  {
    "text": "partitioning a bucket in the data allowed us to just scan the data we were interested in so if you write a very",
    "start": "2413310",
    "end": "2418490"
  },
  {
    "text": "sort of concise query it's going to run much much faster because it's only looking at just a slice of all the data",
    "start": "2418490",
    "end": "2424440"
  },
  {
    "text": "within the within s3 that's so that's that's that's the point there but just",
    "start": "2424440",
    "end": "2429930"
  },
  {
    "text": "says it scanned 12 just over 12 gigabytes worth of data out of you know there's several petabytes of potential",
    "start": "2429930",
    "end": "2435869"
  },
  {
    "text": "data it could have scanned just a couple more and so these these are really interesting because these are the",
    "start": "2435869",
    "end": "2441990"
  },
  {
    "text": "probably the most frequently run queries by our analysts so we're typically looking for things like you know we'll",
    "start": "2441990",
    "end": "2447690"
  },
  {
    "text": "get a request from someone within one of the states say you know tell me all the times you guys see traffic destined to a specific you know",
    "start": "2447690",
    "end": "2454830"
  },
  {
    "text": "malicious IP address over a specific port over over the past week you know a very common query we get all the time",
    "start": "2454830",
    "end": "2461360"
  },
  {
    "text": "old way to take two days to get the results back on this so you know somebody call us Monday morning and we'd",
    "start": "2461360",
    "end": "2466620"
  },
  {
    "text": "be able to get back to them on a Wednesday morning so great that's wonderful here's your answer two days later down to 19 minutes",
    "start": "2466620",
    "end": "2474060"
  },
  {
    "text": "within with any W AWS so I mean much quicker turnaround here is you know again exactly what we're looking for 150",
    "start": "2474060",
    "end": "2480300"
  },
  {
    "text": "times faster that then previously I do just one more of these again all traffic destined to a set of",
    "start": "2480300",
    "end": "2488280"
  },
  {
    "text": "IP address it's not just a single IP is as the last crew was but a set of IPs over poor 80s so any web traffic over a",
    "start": "2488280",
    "end": "2495300"
  },
  {
    "text": "one week time period again old way three days to run this query and in the cloud",
    "start": "2495300",
    "end": "2501090"
  },
  {
    "text": "12 minutes so it's worked we you know we were successful in being able to store",
    "start": "2501090",
    "end": "2506130"
  },
  {
    "text": "the data and query the data in a much more rapid way this particular one 360",
    "start": "2506130",
    "end": "2511440"
  },
  {
    "text": "times faster than than the old way of doing things so this is this is huge for us again this just a representation of",
    "start": "2511440",
    "end": "2517950"
  },
  {
    "text": "that same query it shows two point two terabytes worth of data scan in this particular query and to turn around time",
    "start": "2517950",
    "end": "2523560"
  },
  {
    "text": "of 19 minutes which is which is pretty fantastic so obviously this is just the",
    "start": "2523560",
    "end": "2529110"
  },
  {
    "text": "beginning for us we had some very immediate needs because we were rapidly running out of space and the ability to query the data we had to find a place to",
    "start": "2529110",
    "end": "2534900"
  },
  {
    "text": "store it and we had to find a way to query it we've you know we've solved for that we're comfortable with the solution today but now that you know because the",
    "start": "2534900",
    "end": "2542310"
  },
  {
    "text": "data is within AWS we're able to then leverage additional tools so we're looking at things like redshift spectrum",
    "start": "2542310",
    "end": "2548030"
  },
  {
    "text": "one of the things we're building in house is we're building a different user interface for the analysts that are maybe not so sequel savvy and so we're",
    "start": "2548030",
    "end": "2556230"
  },
  {
    "text": "building interface where it's literally point-and-click where they can put an IP a source IP destination IP select the",
    "start": "2556230",
    "end": "2561690"
  },
  {
    "text": "drop-down of different ports and maybe well-known protocols and make it really user friendly building a nice user",
    "start": "2561690",
    "end": "2567930"
  },
  {
    "text": "interface because eventually we'd like to turn this functionality over to our",
    "start": "2567930",
    "end": "2573030"
  },
  {
    "text": "constituents to allow them to query their own data it's going to give them access to their own data let them run",
    "start": "2573030",
    "end": "2578130"
  },
  {
    "text": "their own queries then we we were never comfortable doing that having them run their own queries and then having them sit and wait for a",
    "start": "2578130",
    "end": "2583619"
  },
  {
    "text": "couple days to get the results back now that we have this this really high-performance infrastructure set up where we're in a",
    "start": "2583619",
    "end": "2590430"
  },
  {
    "text": "much better place to do that we want to collect more data so we obviously collect a massive mountain date today",
    "start": "2590430",
    "end": "2596250"
  },
  {
    "text": "but there's even more that we could do potentially in the future with additional data types that the sensors themselves have the ability to to",
    "start": "2596250",
    "end": "2602250"
  },
  {
    "text": "collect and so we want to collect just more data do some additional alerting and correlation on that data so this is",
    "start": "2602250",
    "end": "2607980"
  },
  {
    "text": "again just the beginning for us and now that we have the infrastructure built out we're in a really good spot to just expand it even further I actually gave a",
    "start": "2607980",
    "end": "2614940"
  },
  {
    "text": "talk yesterday I'll know if any of you were in it it was nine o'clock yesterday morning on where we're headed now with with machine learning so again the the",
    "start": "2614940",
    "end": "2622829"
  },
  {
    "text": "immediate need was to be able to query NetFlow data in an ad hoc way and some of the examples I showed earlier but",
    "start": "2622829",
    "end": "2629339"
  },
  {
    "text": "where we'd like to go Swede liked to use things like machine learning for instance to be able to find the anomalies than the data in a much more",
    "start": "2629339",
    "end": "2635609"
  },
  {
    "text": "automated fashion because as we have more data and we have more sensors we're not going to be able to grow our group",
    "start": "2635609",
    "end": "2640859"
  },
  {
    "text": "of analysts and team analyst commensurate to that we could have a couple hundred analysts and we that's just it's just not feasible and so we",
    "start": "2640859",
    "end": "2647130"
  },
  {
    "text": "want to leverage things like machine learning to be able to help find those anomalies in the data and and present",
    "start": "2647130",
    "end": "2653339"
  },
  {
    "text": "end up to the analyst for for additional analysis that's our presentation that's",
    "start": "2653339",
    "end": "2659369"
  },
  {
    "text": "the story hopefully it was interesting to you all and hopefully we could take something back I think we've got about",
    "start": "2659369",
    "end": "2665430"
  },
  {
    "text": "five minutes left for any questions anybody might have and otherwise we'll be around after the talk to answer questions too so thanks I think they're",
    "start": "2665430",
    "end": "2683819"
  },
  {
    "text": "going to take questions if anyone has one oh I was asking for like desk",
    "start": "2683819",
    "end": "2693309"
  },
  {
    "text": "you know Runyon the ETL servers you know in desk how do you find like management of that are you Runyan kubernetes like",
    "start": "2693309",
    "end": "2703329"
  },
  {
    "text": "the desk service essentially containers in villages group its they're running",
    "start": "2703329",
    "end": "2714969"
  },
  {
    "text": "this discreet so it's one Easter's pulling a message from ask us Q and then",
    "start": "2714969",
    "end": "2721719"
  },
  {
    "text": "and running running a discrete and doing all the work so the final injection you",
    "start": "2721719",
    "end": "2734559"
  },
  {
    "text": "are using the SCP instead of that why not directly right into the s3 bucket and you use the lambda to do the",
    "start": "2734559",
    "end": "2741549"
  },
  {
    "text": "enrichment part of it so why did we use SCP receivers rather than going directly to the edge of it yeah so we we needed",
    "start": "2741549",
    "end": "2749979"
  },
  {
    "text": "so I guess it's an architecture that may evolve over time at some point know of the SFTP gateway service for s3 or give",
    "start": "2749979",
    "end": "2757029"
  },
  {
    "text": "us a bell a there are a few restrictions these sensors needed to be able to access a specific IP address through",
    "start": "2757029",
    "end": "2764170"
  },
  {
    "text": "firewalls they are very restrict restrictive own firewall rules so we needed to be able to go to a predictable",
    "start": "2764170",
    "end": "2770049"
  },
  {
    "text": "elastic IP address which going directly to f3 would not have given us that's why we needed the network load balancer with",
    "start": "2770049",
    "end": "2775509"
  },
  {
    "text": "the e IP and we also needed to be able to run some proprietary software that",
    "start": "2775509",
    "end": "2781089"
  },
  {
    "text": "would convert the binary file formats into CSV formats we need to compute engine to do that we need to DC two",
    "start": "2781089",
    "end": "2786729"
  },
  {
    "text": "instances to do that as well but we're done using the lambda say again that",
    "start": "2786729",
    "end": "2792759"
  },
  {
    "text": "could be done using the lambda V conversion and all yeah actually we we had limitations with the amount of disk",
    "start": "2792759",
    "end": "2798999"
  },
  {
    "text": "space unlimited memory on the lambda function to run this particular software suite so we need a DCT platform to do",
    "start": "2798999",
    "end": "2805569"
  },
  {
    "text": "that the question is on the future side of it like right now using a Tina for",
    "start": "2805569",
    "end": "2811599"
  },
  {
    "text": "the query mhm what are the use cases we are your thing to use the red chip spectrum and that cannot be done using Athena right",
    "start": "2811599",
    "end": "2818420"
  },
  {
    "text": "because we are in a similar stages of life anyways yeah that's a really good",
    "start": "2818420",
    "end": "2823820"
  },
  {
    "text": "question so in theory they're both doing the same thing because it is an s3 and spectrum can access the data from",
    "start": "2823820",
    "end": "2829430"
  },
  {
    "text": "yesterday spectrum gives you more concurrent series the ability to have more concurrency because you can size your redshift cluster and the amount of",
    "start": "2829430",
    "end": "2837310"
  },
  {
    "text": "parallelization trouble saying that word every time but you get a redshift",
    "start": "2837310",
    "end": "2842510"
  },
  {
    "text": "spectrum is dependent on the number of slices in your edge of cluster so you can actually get a lot more concurrency",
    "start": "2842510",
    "end": "2847550"
  },
  {
    "text": "right a redshift spectrum then you come from the Athena service you pay more for it but you can get more concurrency",
    "start": "2847550",
    "end": "2852920"
  },
  {
    "text": "which makes it faster for queries involves scanning huge amounts of get a nursery so if you get large scan queries",
    "start": "2852920",
    "end": "2859340"
  },
  {
    "text": "then spectrum can get you better performance that's kind of the primary primaries gets response thank you any",
    "start": "2859340",
    "end": "2866240"
  },
  {
    "text": "other questions yes can you elaborate on why not using",
    "start": "2866240",
    "end": "2875180"
  },
  {
    "text": "EMR white going head with your own ec2 instances and manage because EMR has",
    "start": "2875180",
    "end": "2882800"
  },
  {
    "text": "it's a platform service right so can you elaborate on specific",
    "start": "2882800",
    "end": "2888290"
  },
  {
    "text": "challenges so so the main thing of it",
    "start": "2888290",
    "end": "2896780"
  },
  {
    "text": "MRR is when we talked earlier but one of the tenants of the architecture is we want that massive concurrency without",
    "start": "2896780",
    "end": "2902540"
  },
  {
    "text": "any bottlenecks so one of the ways to do this is since the theater was coming in in these micro batches that were coming",
    "start": "2902540",
    "end": "2908780"
  },
  {
    "text": "from specific sensors with data for specific time intervals we were able to avoid doing expensive computation to",
    "start": "2908780",
    "end": "2916250"
  },
  {
    "text": "create the partitions the data actually coming in fit quite nicely into the existing partitions by sensor and by",
    "start": "2916250",
    "end": "2921770"
  },
  {
    "text": "time we didn't want to do anything to kind of break that natural partitioning of the data because that allowed us to",
    "start": "2921770",
    "end": "2927560"
  },
  {
    "text": "have very very concurrent architecture obviously the more sensors we have we could just run more concurrent processes",
    "start": "2927560",
    "end": "2934250"
  },
  {
    "text": "processing the data from each sensor or each time window for each sensor each partition effectively so it did not want",
    "start": "2934250",
    "end": "2940430"
  },
  {
    "text": "to do is you run a spark job for example that would be reading yet across many sensors all at once only to have to",
    "start": "2940430",
    "end": "2946730"
  },
  {
    "text": "split that yet for him back right again to partition it by sensor so that's why we come up with the idea of trying to process the data",
    "start": "2946730",
    "end": "2953230"
  },
  {
    "text": "partitioned by partition but on a massively current concurrent way using an ec2 instance for each partition and",
    "start": "2953230",
    "end": "2960230"
  },
  {
    "text": "it also handles the problem that Oliver talked about where some sensors generate",
    "start": "2960230",
    "end": "2965660"
  },
  {
    "text": "disproportionately huge volumes of data compared to other sensors so with a single EMR cluster the whole cluster",
    "start": "2965660",
    "end": "2971420"
  },
  {
    "text": "sets around while you're processing a small number of the huge partitions whereas the waivered unit where we can",
    "start": "2971420",
    "end": "2978470"
  },
  {
    "text": "do a petition by partition as we can actually scale that whole cluster up and down based on the number of partitions",
    "start": "2978470",
    "end": "2983570"
  },
  {
    "text": "that were processing concurrently so there's more cost to fight of architecture and again that's for this",
    "start": "2983570",
    "end": "2988610"
  },
  {
    "text": "particular use case every use case can be different but in this particular scenario would we really come up with a more cost-effective and more scalable",
    "start": "2988610",
    "end": "2995090"
  },
  {
    "text": "architecture by doing it this way something and actually if we also mean",
    "start": "2995090",
    "end": "3010450"
  },
  {
    "text": "Oliver mentioned that was kind of our first approach with G's fire hose and we're actually seen glue jobs to do that",
    "start": "3010450",
    "end": "3017440"
  },
  {
    "text": "and it worked great in terms of time to value like we got the whole thing up and running really quickly but we were",
    "start": "3017440",
    "end": "3022900"
  },
  {
    "text": "testing Leonce relatively small volumes and once we started to crank up to more realistic volumes we realized that both",
    "start": "3022900",
    "end": "3028060"
  },
  {
    "text": "the cost was becoming prohibitive and we were getting performance bottlenecks in a system so it just wasn't scaling the",
    "start": "3028060",
    "end": "3033220"
  },
  {
    "text": "way we wanted to wishes was just approached any other questions were you",
    "start": "3033220",
    "end": "3044500"
  },
  {
    "text": "guys dealing with disparate different types of tools and if so how did you do the normalization of the fields to get",
    "start": "3044500",
    "end": "3052270"
  },
  {
    "text": "it ready for retail I kind of missed the first part of that question were you guys dealing with different disparate",
    "start": "3052270",
    "end": "3059020"
  },
  {
    "text": "tool or sensor tools and if so how do you guys do the normalization project",
    "start": "3059020",
    "end": "3064300"
  },
  {
    "text": "idea so the first part is actually we burnt the sensor data that was coming in very consistently CAS have deployed as a",
    "start": "3064300",
    "end": "3070750"
  },
  {
    "text": "consistent software suite across all of the sensors so we really did not have a normalization challenge in this case",
    "start": "3070750",
    "end": "3076080"
  },
  {
    "text": "yeah so thank you everyone for coming please",
    "start": "3076080",
    "end": "3083210"
  },
  {
    "text": "complete the survey that is in the app and the next session is at 11:15",
    "start": "3083210",
    "end": "3088330"
  },
  {
    "text": "fireside chat with Indy Jesse",
    "start": "3088330",
    "end": "3092140"
  },
  {
    "text": "[Applause]",
    "start": "3094610",
    "end": "3097780"
  }
]