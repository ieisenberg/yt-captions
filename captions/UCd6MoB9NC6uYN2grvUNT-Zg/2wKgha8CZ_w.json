[
  {
    "start": "0",
    "end": "96000"
  },
  {
    "text": "so a few weeks back a colleague of mine on the EBS team told me a story about when he first moved to",
    "start": "160",
    "end": "6040"
  },
  {
    "text": "Seattle now he decided that he wanted to get a bike to commute to and from work and for those of you that have never",
    "start": "6040",
    "end": "12080"
  },
  {
    "text": "visited Seattle Seattle has some hills and those Hills make San Francisco streets look",
    "start": "12080",
    "end": "17640"
  },
  {
    "text": "tame so anyway he buys his bike and he tries to commute home unfortunately he",
    "start": "17640",
    "end": "23080"
  },
  {
    "text": "doesn't even make it all the way home he has to get off his bike and walk come to find out he he's only ever",
    "start": "23080",
    "end": "30400"
  },
  {
    "text": "ridden a single speed bike and he didn't realize how to use the gears on his new bike once he figured out the",
    "start": "30400",
    "end": "37320"
  },
  {
    "text": "gears he was able to climb Hills I'm Mark and I'm a software",
    "start": "37320",
    "end": "42719"
  },
  {
    "text": "engineer on the EBS team and I've been focusing on the core IO path with EBS for the last four",
    "start": "42719",
    "end": "48680"
  },
  {
    "text": "years now my goal today is that by the end of the talk you'll know a few more tips and",
    "start": "48680",
    "end": "55120"
  },
  {
    "text": "tricks that will help you improve the performance of your perform of your application much like my colleague",
    "start": "55120",
    "end": "61199"
  },
  {
    "text": "learned how to use his bike so how many people are returning",
    "start": "61199",
    "end": "67360"
  },
  {
    "text": "after D's talk a good number of people and some new faces that's great um so for those",
    "start": "67360",
    "end": "74119"
  },
  {
    "text": "of you that are just joining I'm going to spend a few minutes to recap what DG just talked about and go over an",
    "start": "74119",
    "end": "79720"
  },
  {
    "text": "overview of what EBS is so Amazon EBS is",
    "start": "79720",
    "end": "84960"
  },
  {
    "text": "block storage for ec2 now we look like a block device to your instance",
    "start": "84960",
    "end": "90680"
  },
  {
    "text": "but it's important to remember that EBS is not just a bunch of discs so what is EBS EBS is persistent",
    "start": "90680",
    "end": "99320"
  },
  {
    "start": "96000",
    "end": "96000"
  },
  {
    "text": "Network block storage as a service now the service part of that is really cool",
    "start": "99320",
    "end": "104680"
  },
  {
    "text": "it means that we can iterate and improve the performance and improve the feature set on EBS without you having to do",
    "start": "104680",
    "end": "113079"
  },
  {
    "text": "anything now EBS is also a zonal service and that means that once your volume is",
    "start": "113079",
    "end": "118399"
  },
  {
    "text": "created all of the resources for your volume live within a single availability Zone and that volume can be attached to",
    "start": "118399",
    "end": "125680"
  },
  {
    "text": "any instance in that availability zone so EBS has other features such as",
    "start": "125680",
    "end": "130759"
  },
  {
    "text": "snapshots and encryption um designed for five NES of availability and we create over two",
    "start": "130759",
    "end": "137360"
  },
  {
    "text": "million volumes every",
    "start": "137360",
    "end": "140480"
  },
  {
    "start": "144000",
    "end": "144000"
  },
  {
    "text": "day EBS has a few different volume types uh EBS magnetic which is the original",
    "start": "144280",
    "end": "149959"
  },
  {
    "text": "volume type and two SSD backed volume types general purpose volumes or gp2 and",
    "start": "149959",
    "end": "155400"
  },
  {
    "text": "provision iops volumes which we refer to as P",
    "start": "155400",
    "end": "159680"
  },
  {
    "text": "Ops so EBS magnetic volumes these are best for infrequently",
    "start": "162560",
    "end": "168519"
  },
  {
    "text": "accessed data so say you take a snapshot of your database you can put you can stuff the that snapshot onto the",
    "start": "168519",
    "end": "174599"
  },
  {
    "text": "magnetic drive so that it's next to your instance in case you need it quickly it's also good for log storage",
    "start": "174599",
    "end": "181280"
  },
  {
    "text": "so application logs and and system logs and other things like that and if you have cost sensitive workloads magnetic",
    "start": "181280",
    "end": "187400"
  },
  {
    "text": "still a good choice for you now gp2 volumes I'm not going to go",
    "start": "187400",
    "end": "193040"
  },
  {
    "text": "through all of the slides that dugle did to explain gp2 volumes but the bottom line with gp2 volumes is that they just",
    "start": "193040",
    "end": "200599"
  },
  {
    "text": "work we've designed these to be applicable to most workloads and they",
    "start": "200599",
    "end": "205840"
  },
  {
    "text": "have an iops Baseline uh that's up to 10,000 gab and that is allocated uh",
    "start": "205840",
    "end": "212959"
  },
  {
    "text": "based on the size of your volume gp2 volumes also for smaller",
    "start": "212959",
    "end": "218560"
  },
  {
    "text": "volumes have a burst bucket and this burst bucket allows you to to drive",
    "start": "218560",
    "end": "223799"
  },
  {
    "text": "3,000 iops for up to 30 minutes and now when you're not driving that 3,000 iops",
    "start": "223799",
    "end": "228840"
  },
  {
    "text": "that bucket continuously refills at your Baseline rate so it's a little confusing",
    "start": "228840",
    "end": "233879"
  },
  {
    "text": "with the with the burst bucket but really I don't want you to have to think about that these these just work and in",
    "start": "233879",
    "end": "240959"
  },
  {
    "text": "fact they work so well that most of our gp2 volumes don't even come close to exhausting their burst",
    "start": "240959",
    "end": "248000"
  },
  {
    "text": "bucket now for your mission critical workloads those are your critical",
    "start": "250079",
    "end": "255400"
  },
  {
    "text": "production database that can't even tolerate a stall on a on a select um",
    "start": "255400",
    "end": "261320"
  },
  {
    "text": "things like that we recommend provision iops volumes now these can be provisioned up",
    "start": "261320",
    "end": "266880"
  },
  {
    "text": "to 20,000 iops and that those iops are independent of the size of the",
    "start": "266880",
    "end": "272120"
  },
  {
    "text": "volume these also have a higher throughput and a higher performance",
    "start": "272120",
    "end": "277680"
  },
  {
    "text": "consistency so now let's jump into why you're all here but before we get started let's",
    "start": "280479",
    "end": "287560"
  },
  {
    "start": "286000",
    "end": "286000"
  },
  {
    "text": "talk about a little Theory so John little is a professor and he spent most",
    "start": "287560",
    "end": "293240"
  },
  {
    "text": "of of his research uh focusing on operations and in the 50s and 60s he was",
    "start": "293240",
    "end": "298600"
  },
  {
    "text": "thinking about EBS wait that's a lie he was thinking about a",
    "start": "298600",
    "end": "304000"
  },
  {
    "text": "bank it turns out that his research then was actually still applicable to",
    "start": "304000",
    "end": "310800"
  },
  {
    "text": "EBS and he proposed a theorem and that theorem seems obvious now that it's on",
    "start": "310800",
    "end": "317400"
  },
  {
    "text": "paper but nobody had really thought about it and it's that the wait time of a request in his case the patron waiting",
    "start": "317400",
    "end": "324759"
  },
  {
    "text": "to do transactions is equal to the Q length divided by the arrival rate",
    "start": "324759",
    "end": "331280"
  },
  {
    "text": "and so this is exactly how EBS works and this is exactly how most systems work now it's also important to note",
    "start": "331280",
    "end": "338400"
  },
  {
    "text": "that these three values as they pertain to EBS are available as cloudwatch metrics and we'll talk a little bit more",
    "start": "338400",
    "end": "344360"
  },
  {
    "text": "about that",
    "start": "344360",
    "end": "346800"
  },
  {
    "text": "later so when we talk about performance optimization we look at these three",
    "start": "351440",
    "end": "357080"
  },
  {
    "text": "things there are a few other things but these are the core things that we think think about the iops are the rate of IO",
    "start": "357080",
    "end": "363120"
  },
  {
    "text": "that is going to your volume the latency which is the time that those iops take to complete and the throughput now your",
    "start": "363120",
    "end": "370639"
  },
  {
    "text": "database workloads are going to be focused mainly on iops and latency uh your streaming video streaming uh log",
    "start": "370639",
    "end": "377319"
  },
  {
    "text": "processing Splunk Kafka things like that are more concerned about throughput",
    "start": "377319",
    "end": "383479"
  },
  {
    "text": "workloads so to achieve end to end performance optimization you need to look at these",
    "start": "383880",
    "end": "389120"
  },
  {
    "start": "384000",
    "end": "384000"
  },
  {
    "text": "four components for many of you this is going to be reviewed but let's make sure everybody's on the same page you have",
    "start": "389120",
    "end": "396199"
  },
  {
    "text": "your ec2 instance that's where your application is running your application is",
    "start": "396199",
    "end": "401360"
  },
  {
    "text": "submitting IO and that IO is targeted at an EBS",
    "start": "401360",
    "end": "407080"
  },
  {
    "text": "volume to get to that EBS volume there needs to be a network Link in play and",
    "start": "407080",
    "end": "412360"
  },
  {
    "text": "these four key components work together so to maximize performance it's important to arrange them into a",
    "start": "412360",
    "end": "418240"
  },
  {
    "text": "balanced system",
    "start": "418240",
    "end": "421400"
  },
  {
    "text": "let's start by looking at the io component all right I'll admit I'm a",
    "start": "424720",
    "end": "430919"
  },
  {
    "text": "little bit of a scuba diver so I used a dive slide because we're going to go on a deep dive into the kernel now a lot of",
    "start": "430919",
    "end": "436800"
  },
  {
    "text": "you have focused on storage optimization but your your focus has been on on",
    "start": "436800",
    "end": "442800"
  },
  {
    "text": "premise systems getting an IO to the storage layer in a virtualized system is a",
    "start": "442800",
    "end": "448879"
  },
  {
    "text": "little bit different and so I thought it would be important to take some time to explain how that",
    "start": "448879",
    "end": "455479"
  },
  {
    "start": "456000",
    "end": "456000"
  },
  {
    "text": "happens so you have your instance your instance is running an application your application wants to do",
    "start": "456879",
    "end": "463039"
  },
  {
    "text": "an IO that could be a read or a write could be written in Java or assembly doesn't matter eventually that read gets",
    "start": "463039",
    "end": "470879"
  },
  {
    "text": "converted into a system call into the kernel within your instance along with that system call is",
    "start": "470879",
    "end": "476879"
  },
  {
    "text": "a buffer and that buffer is where the data goes goes now if it's a read the EBS volume will put the data there if",
    "start": "476879",
    "end": "483360"
  },
  {
    "text": "it's a write that's the data that's going to get recorded on your EBS volume and so the colonel knows that",
    "start": "483360",
    "end": "489919"
  },
  {
    "text": "it's not talking to a real block device underneath it's talking to virtualized",
    "start": "489919",
    "end": "495800"
  },
  {
    "text": "storage and so it contacts an IO domain now for every ec2 instance it has",
    "start": "495800",
    "end": "503000"
  },
  {
    "text": "a communication path with an IO domain and that IO domain has all the software that directly accesses the EBS volume",
    "start": "503000",
    "end": "510159"
  },
  {
    "text": "the problem is that IO domain doesn't have access to your instance's",
    "start": "510159",
    "end": "515279"
  },
  {
    "text": "memory and so the instance has to alert the io domain and say I'm going to Grant",
    "start": "515279",
    "end": "521360"
  },
  {
    "text": "you permission to this piece of memory and that process is called a grant",
    "start": "521360",
    "end": "527680"
  },
  {
    "text": "mapping and now this process is actually really fast and and pretty neat um it's",
    "start": "528160",
    "end": "533880"
  },
  {
    "text": "fast for bulk data transfers but it can be a little bit costant costly from a",
    "start": "533880",
    "end": "539560"
  },
  {
    "text": "CPU perspective because every time you do one of these it requires at least one tlb flush as we switch back and forth uh",
    "start": "539560",
    "end": "547519"
  },
  {
    "text": "between the guest instance your your ec2 instance and the io",
    "start": "547519",
    "end": "554079"
  },
  {
    "text": "domain so once we've got that IO",
    "start": "555120",
    "end": "559480"
  },
  {
    "text": "mapped now your instance is able to create a request and that request looks a little bit differently a little bit",
    "start": "560200",
    "end": "566880"
  },
  {
    "text": "different from a normal block.io request but really it's about the same it has",
    "start": "566880",
    "end": "574800"
  },
  {
    "text": "the location that you want to read from or write from the amount of data that you want to read and a bunch uh 11 to be",
    "start": "574800",
    "end": "581839"
  },
  {
    "text": "more specific uh slots for these Grant mappings now that instance communicates",
    "start": "581839",
    "end": "588519"
  },
  {
    "text": "with the io domain for these requests through a single memory page that is mapped and that your instance and IO",
    "start": "588519",
    "end": "596440"
  },
  {
    "text": "domain have access to and it's treated as a circular que or ring buffer um and",
    "start": "596440",
    "end": "603040"
  },
  {
    "text": "ring buffers as some of you probably are familiar with are most efficient whenever request is the same size and so",
    "start": "603040",
    "end": "609519"
  },
  {
    "text": "you're probably thinking why only 11 Grant references well it turns out that if we",
    "start": "609519",
    "end": "615440"
  },
  {
    "text": "add a 12th Grant reference in there we can't fit 32 ios's into the que and we want that que to be base two and we want",
    "start": "615440",
    "end": "622480"
  },
  {
    "text": "to have enough a large enough queue that you can submit enough IO the problem is",
    "start": "622480",
    "end": "630480"
  },
  {
    "text": "is this only is just under a me and a half of data um so it's not a whole lot",
    "start": "630480",
    "end": "635800"
  },
  {
    "text": "of data that can be outstanding and if you're submitting larger ios's they'll get split down into",
    "start": "635800",
    "end": "643160"
  },
  {
    "text": "44k so the 3.8 kernel in Linux included a number of",
    "start": "643880",
    "end": "651600"
  },
  {
    "text": "features and so if you remember I was talking about the tlb flushes before if you're trying to do a lot of of IO your",
    "start": "654240",
    "end": "660800"
  },
  {
    "text": "instance is going to try and submit an IO we have to do a tlb flush IO domain has to do that Grant mapping may do a",
    "start": "660800",
    "end": "666639"
  },
  {
    "text": "tlb flush on the way out and then when the iio is done we'll have to repeat the process again and so the CPU is doing a",
    "start": "666639",
    "end": "671839"
  },
  {
    "text": "lot of extra work it turns out in computer science especially that when you question your",
    "start": "671839",
    "end": "677760"
  },
  {
    "text": "assumptions often times you end up with something even better and what the Linux kernel",
    "start": "677760",
    "end": "684240"
  },
  {
    "text": "Engineers realized is that modern CPUs are really good at copying memory",
    "start": "684240",
    "end": "689800"
  },
  {
    "text": "particularly memory that's been recently accessed and so what they did is wrote",
    "start": "689800",
    "end": "696720"
  },
  {
    "text": "in some capabilities that allow your instance and the io domain to pre-establish a pool of memory that they",
    "start": "696720",
    "end": "703560"
  },
  {
    "text": "both have access to all the time and that's established when the volume is attached to your instance when your instance wants to do",
    "start": "703560",
    "end": "710399"
  },
  {
    "text": "an IO now all it does is treats that pool of memory as a bounce buffer and actually copies the memory back and",
    "start": "710399",
    "end": "717399"
  },
  {
    "text": "forth and the amazing thing about this is we've seen significant both iops and",
    "start": "717399",
    "end": "722519"
  },
  {
    "text": "latency improvements uh depending on the instance type and the the volumes that are going underneath that uh almost",
    "start": "722519",
    "end": "728800"
  },
  {
    "text": "double when using this method as opposed to doing tlb flushes for every other or for every single",
    "start": "728800",
    "end": "736160"
  },
  {
    "text": "IO another feature that's in the 3.8 kernel is the the concept of indirect",
    "start": "736880",
    "end": "742920"
  },
  {
    "text": "grants so instead of putting each grant that makes up that IO into the read request",
    "start": "742920",
    "end": "749959"
  },
  {
    "text": "what happens now is you can put those grants into one of those pages and then just include a reference in that request",
    "start": "749959",
    "end": "756160"
  },
  {
    "text": "to that page so your IO request can be significantly larger Now by default we",
    "start": "756160",
    "end": "763000"
  },
  {
    "text": "still allow 32 requests and those requests are 128k maximum and and what",
    "start": "763000",
    "end": "768320"
  },
  {
    "text": "we found is that that's a good balance between the amount of memory required",
    "start": "768320",
    "end": "773639"
  },
  {
    "text": "and the actual performance benefits",
    "start": "773639",
    "end": "780839"
  },
  {
    "text": "so let's come back up a little bit stop for a moment do a safety stop make sure we've shucking out the cobwebs for those",
    "start": "780880",
    "end": "787079"
  },
  {
    "text": "of you that got a little confused I'm going to show you a smaller example of indirect references so this is before",
    "start": "787079",
    "end": "794000"
  },
  {
    "text": "the 38 konel if you want to submit 128 K that gets broken up into three",
    "start": "794000",
    "end": "800120"
  },
  {
    "text": "requests a 44k 2 44k of 40K now if you notice that last request",
    "start": "800120",
    "end": "807040"
  },
  {
    "text": "is not a full request so we're actually able to put even less data into the queue and those three requests each take",
    "start": "807040",
    "end": "814920"
  },
  {
    "text": "up a slot in the queue now I'm a computer scientist so I start counting at zero um but there's 32 slots here I",
    "start": "814920",
    "end": "821560"
  },
  {
    "text": "promise we only have the ability to put nine of these 128k requests into the",
    "start": "821560",
    "end": "827680"
  },
  {
    "text": "que with indirect references same 128 K",
    "start": "827680",
    "end": "832800"
  },
  {
    "text": "IO just takes up a single slot so a tip ensure you're using a",
    "start": "832800",
    "end": "840839"
  },
  {
    "text": "kernel version that's greater than 3.8 now fortunately every version of Amazon Linux uh since 2003 or",
    "start": "840839",
    "end": "849519"
  },
  {
    "text": "201309 has these features enabled Aon 2404 a r 7 and that also carries over to",
    "start": "849519",
    "end": "856160"
  },
  {
    "text": "Centos 7 uh also have these features enabled now you don't have to be using one of these amies one of these",
    "start": "856160",
    "end": "862920"
  },
  {
    "text": "distributions as long as you've got the 3.8 kernel or better running in your instance you'll get the benefits of this",
    "start": "862920",
    "end": "870680"
  },
  {
    "text": "so as it relates to EBS QEP is the pending IO and",
    "start": "873839",
    "end": "879279"
  },
  {
    "start": "877000",
    "end": "877000"
  },
  {
    "text": "flight you need to have IO and Q in order for it to be processed by EBS so you need to keep the ABS volume",
    "start": "879279",
    "end": "886440"
  },
  {
    "text": "busy at all times now there's no perfect qpth sometimes it'll be dependent upon",
    "start": "886440",
    "end": "892079"
  },
  {
    "text": "your application uh but if you're getting the latency that you're expecting you're probably driving enough",
    "start": "892079",
    "end": "899639"
  },
  {
    "text": "Q depth and what you need to understand is that once you have what you expect you",
    "start": "899639",
    "end": "905480"
  },
  {
    "text": "don't need to worry about this and for some workloads it's a tunable and I'll demonstrate that a little bit",
    "start": "905480",
    "end": "910839"
  },
  {
    "text": "later but the important takeaway here is that we have to have something in the queue for EBS to process",
    "start": "910839",
    "end": "918040"
  },
  {
    "text": "it now types of iOS that we have this is a big long chart I don't expect you to",
    "start": "918040",
    "end": "923199"
  },
  {
    "text": "memorize it um but it's just kind of a sample of the different types of workloads that run on AWS",
    "start": "923199",
    "end": "929360"
  },
  {
    "text": "most of your database workloads are listed at the top and they do smaller random iOS so they're more concerned",
    "start": "929360",
    "end": "934880"
  },
  {
    "text": "about iops I've thrown gluster on the bottom I ran out of room uh but Kafka",
    "start": "934880",
    "end": "940440"
  },
  {
    "text": "and other things like that are larger sequential workloads uh they're looking more at",
    "start": "940440",
    "end": "947240"
  },
  {
    "start": "949000",
    "end": "949000"
  },
  {
    "text": "throughput so our customers run a very diverse set of workloads from e-commerce websites",
    "start": "949759",
    "end": "956120"
  },
  {
    "text": "storing customer metadata perhaps for your video game or whatever your e-commerce site um some",
    "start": "956120",
    "end": "963040"
  },
  {
    "text": "big data log analysis things like that to simplify the rest of this talk I'm",
    "start": "963040",
    "end": "968240"
  },
  {
    "text": "going to use benchmarks uh and I'm specifically choosing one that represents transactional",
    "start": "968240",
    "end": "974199"
  },
  {
    "text": "workloads I'll be using a tool called CIS bench some of you are probably familiar with that uh with my sqls the",
    "start": "974199",
    "end": "979639"
  },
  {
    "text": "underlying database now the choices are just random I picked ones that I was",
    "start": "979639",
    "end": "984880"
  },
  {
    "text": "comfortable with but these tips that I'm about to share will apply to any of",
    "start": "984880",
    "end": "990800"
  },
  {
    "text": "them now benchmarks are great for course tuning they're great for showing differences between different volume",
    "start": "992519",
    "end": "998279"
  },
  {
    "text": "types and things like that um but really to get the best feel for how your workload is going to perform on an EBS",
    "start": "998279",
    "end": "1004920"
  },
  {
    "text": "volume you need to use your real world production",
    "start": "1004920",
    "end": "1009360"
  },
  {
    "start": "1011000",
    "end": "1011000"
  },
  {
    "text": "workload so I picked a surprisingly common instance type and configuration",
    "start": "1012120",
    "end": "1017680"
  },
  {
    "text": "uh to to to start our journey today and I picked an M2 for extra large instance I attached a 500 gig EES magnetic volume",
    "start": "1017680",
    "end": "1025798"
  },
  {
    "text": "um and then I've run the the CIS bench workload a few different times and so to show how increasing QEP",
    "start": "1025799",
    "end": "1033319"
  },
  {
    "start": "1031000",
    "end": "1031000"
  },
  {
    "text": "can change the number of transactions your database can do I ran that CIS bench workload a few",
    "start": "1033319",
    "end": "1038480"
  },
  {
    "text": "times increasing the number of my SQL threads which is really increasing the Q depth to the database now I don't really",
    "start": "1038480",
    "end": "1045959"
  },
  {
    "text": "want to focus on specifics that may not be relevant to your partic particular application but what really matters is",
    "start": "1045959",
    "end": "1052720"
  },
  {
    "text": "the relative performance here and so I've removed the values on the X and y- axis um but I do want to call out a few",
    "start": "1052720",
    "end": "1060320"
  },
  {
    "text": "things this graph is linear on the y- axis um starting at zero so it's a",
    "start": "1060320",
    "end": "1065679"
  },
  {
    "text": "normal graph I didn't play any tricks with log scale or skipping lines or anything like that uh and then the x-",
    "start": "1065679",
    "end": "1073200"
  },
  {
    "text": "axis I started with two threads and then I increased it to some number um and so",
    "start": "1073200",
    "end": "1078960"
  },
  {
    "text": "what you'll notice is that as you're going through this that as you increase the number of threads that are writing",
    "start": "1078960",
    "end": "1084960"
  },
  {
    "text": "to your volume the transaction rate increases now there's a point where that",
    "start": "1084960",
    "end": "1090559"
  },
  {
    "text": "transaction rate tails off and you're actually submitting data to the EBS volume faster than the EBS",
    "start": "1090559",
    "end": "1097720"
  },
  {
    "text": "volume can process",
    "start": "1097720",
    "end": "1100559"
  },
  {
    "text": "it now most applications allow control over the number of concurrent operations you should Benchmark with",
    "start": "1102760",
    "end": "1109520"
  },
  {
    "start": "1103000",
    "end": "1103000"
  },
  {
    "text": "different settings and if you're writing your own application ensure that you're able to handle concurrent requests to",
    "start": "1109520",
    "end": "1115880"
  },
  {
    "text": "EBS now you can think of this parallelism and increasing QEP and increasing the number of threads that",
    "start": "1115880",
    "end": "1121320"
  },
  {
    "text": "are writing to the my or to your underlying storage similar to the gears on my colleague's",
    "start": "1121320",
    "end": "1128640"
  },
  {
    "text": "bike all right so now that we've taken a look at let's look at the instance in the network",
    "start": "1136039",
    "end": "1142840"
  },
  {
    "text": "link now I mentioned before that that was a surprisingly common choice and the M24 extra large was released in 2009",
    "start": "1142880",
    "end": "1150240"
  },
  {
    "text": "with an Intel xon CPU early last uh early last year we",
    "start": "1150240",
    "end": "1157480"
  },
  {
    "text": "released the R3 family to replace as the Next Generation in our high memory instance families and so the R32 extra",
    "start": "1157480",
    "end": "1164720"
  },
  {
    "text": "large has very similar specs to the M24 extra large now don't get confused by the 2x large 4X large differentiation",
    "start": "1164720",
    "end": "1171080"
  },
  {
    "text": "these really are equivalent to each other the important thing to notice here",
    "start": "1171080",
    "end": "1177320"
  },
  {
    "text": "and one thing that I think is really cool is you get better performance at 28 cents an hour less and so that's if",
    "start": "1177320",
    "end": "1184120"
  },
  {
    "text": "you're running that instance 247 for an entire month that comes out to about $200 a",
    "start": "1184120",
    "end": "1191720"
  },
  {
    "start": "1192000",
    "end": "1192000"
  },
  {
    "text": "month so alongside of choosing your instance you need to choose the attributes that that are important for",
    "start": "1193320",
    "end": "1198919"
  },
  {
    "text": "the performance of that instance and as it pertains to EBS that attribute is EBS",
    "start": "1198919",
    "end": "1205280"
  },
  {
    "text": "optimized with EBS optimized instances we allocate a separate amount of bandwidth",
    "start": "1205280",
    "end": "1211559"
  },
  {
    "text": "just for your EBS traffic and so if you're not using EBS",
    "start": "1211559",
    "end": "1218080"
  },
  {
    "text": "optimized what happens is the application the internet the network traffic going to your application",
    "start": "1218080",
    "end": "1223280"
  },
  {
    "text": "actually shares that network with the EBS volume and so some days you may have great performance on your s volume some",
    "start": "1223280",
    "end": "1229320"
  },
  {
    "text": "days you may not and you really don't know is it somebody driving a whole lot of load to your application or is it",
    "start": "1229320",
    "end": "1234679"
  },
  {
    "text": "your application driving load to the EBS volume or is the EBS volume not performing well and not performing to",
    "start": "1234679",
    "end": "1240280"
  },
  {
    "text": "what you need EBS optimized instances remove some",
    "start": "1240280",
    "end": "1245360"
  },
  {
    "text": "of that confusion and now these are EBS",
    "start": "1245360",
    "end": "1250440"
  },
  {
    "text": "optimized or newer generation instances are actually EBS optimized by default and this includes our M4 C4 and D2",
    "start": "1250440",
    "end": "1256400"
  },
  {
    "text": "instance families um um and the 8 exra large instances actually have a 10 GB",
    "start": "1256400",
    "end": "1262960"
  },
  {
    "text": "Network the the EBS optimized by default larger instances",
    "start": "1262960",
    "end": "1269280"
  },
  {
    "text": "also have an additional abbs bandwidth um for the ones that that have just the shared 10 gigb",
    "start": "1269280",
    "end": "1275600"
  },
  {
    "text": "Network it's a lot of traffic um and and a lot of room and so it has the same",
    "start": "1275600",
    "end": "1280760"
  },
  {
    "text": "caveat of a non EBS optimized instance um but there's a lot of buffer there and",
    "start": "1280760",
    "end": "1286120"
  },
  {
    "text": "this is where you're going to get the maximum IO and maximum iops from your instance uh each eight extra large",
    "start": "1286120",
    "end": "1293279"
  },
  {
    "text": "instance can support approximately 48,000 iops and that's if you're doing 16",
    "start": "1293279",
    "end": "1299480"
  },
  {
    "text": "Kio so if you're not using EBS optimized today it's really easy to change all you",
    "start": "1301320",
    "end": "1307000"
  },
  {
    "text": "have to do is stop your instance and that can be done either in the CLI or the console modify the instance",
    "start": "1307000",
    "end": "1313880"
  },
  {
    "text": "attribute and restart the instance it's pretty painless just a",
    "start": "1313880",
    "end": "1320240"
  },
  {
    "text": "little bit of downtime just like a reboot and it doesn't take a significant amount of time so let's update our Benchmark",
    "start": "1320240",
    "end": "1326919"
  },
  {
    "text": "configuration uh the only thing I've changed here is that I'm using an R32 extra large",
    "start": "1326919",
    "end": "1333919"
  },
  {
    "text": "now and just with that one simple change I've gotten a 25% increase in",
    "start": "1334600",
    "end": "1341880"
  },
  {
    "text": "transactions so another example of this and something that one of our customers Pearson has allowed us to do or this has",
    "start": "1342880",
    "end": "1349159"
  },
  {
    "text": "allowed them to do uh recently we worked with them to switch their M1 Fleet to",
    "start": "1349159",
    "end": "1354559"
  },
  {
    "text": "M3s uh and t2s and they saved more than 10% on their bill because of the extra",
    "start": "1354559",
    "end": "1360240"
  },
  {
    "text": "performance that they could get from the newer instance types I think that's super",
    "start": "1360240",
    "end": "1367120"
  },
  {
    "text": "cool so AWS has a few different instance families and they have a different balance of compute memory uh some of",
    "start": "1367520",
    "end": "1375640"
  },
  {
    "start": "1368000",
    "end": "1368000"
  },
  {
    "text": "them are mixed but what you probably want to do is make sure that you're choosing the",
    "start": "1375640",
    "end": "1380960"
  },
  {
    "text": "right instance and instance family uh for your workload you're not going to want to choose a 20,000 iops volume and",
    "start": "1380960",
    "end": "1388120"
  },
  {
    "text": "try and attach it to a T2 micro instance probably not going to get the performance you're",
    "start": "1388120",
    "end": "1393320"
  },
  {
    "text": "expecting once you've chosen that family and that that right mixture of CPU and",
    "start": "1393320",
    "end": "1399960"
  },
  {
    "text": "memory it to ensure the best performance you should select the current generation of that instance",
    "start": "1399960",
    "end": "1406880"
  },
  {
    "text": "family",
    "start": "1406880",
    "end": "1409880"
  },
  {
    "text": "all right so we've talked about iio the instance Network link let's focus on the EBS volume a little bit so I mentioned",
    "start": "1414320",
    "end": "1421200"
  },
  {
    "text": "before that EBS has a couple different volume types and they really boil down to Magnetic backed and SSD backed",
    "start": "1421200",
    "end": "1427720"
  },
  {
    "text": "volumes now when you're looking at transactional workloads uh you're thinking about iops and latency and so",
    "start": "1427720",
    "end": "1434720"
  },
  {
    "text": "for those types of workloads you're going to want to focus on our ssdb and now I lumped gp2 and Pi Ops together",
    "start": "1434720",
    "end": "1442440"
  },
  {
    "text": "here mainly because the performance is very similar it's the performance consistency that's different between the",
    "start": "1442440",
    "end": "1448159"
  },
  {
    "text": "two and so in a benchmark you're probably not going to notice the difference between gp2 and Pi Ops but",
    "start": "1448159",
    "end": "1453720"
  },
  {
    "text": "you'll notice that over time also I recommend that you use a",
    "start": "1453720",
    "end": "1460440"
  },
  {
    "start": "1457000",
    "end": "1457000"
  },
  {
    "text": "modern file system um modern file systems tend to be journaled they tend to handle larger volumes better and more",
    "start": "1460440",
    "end": "1468640"
  },
  {
    "text": "efficiently for example ext4 and xfs uh don't actually allocate the blocks when",
    "start": "1468640",
    "end": "1474159"
  },
  {
    "text": "you do the initial system call they wait until the data is all present and then they allocate the blocks so that your",
    "start": "1474159",
    "end": "1479880"
  },
  {
    "text": "data is contained together other file systems uh older",
    "start": "1479880",
    "end": "1485159"
  },
  {
    "text": "file systems rather do that as part of the system call so every read request or write request that those blocks are",
    "start": "1485159",
    "end": "1491159"
  },
  {
    "text": "allocated and if you have a lot of data that's going on they may be in different parts of your disk now while there's no need to",
    "start": "1491159",
    "end": "1498120"
  },
  {
    "text": "partition your volumes just create them and attach them if your application",
    "start": "1498120",
    "end": "1504679"
  },
  {
    "text": "requires it or your operating system requires it ensure that they're aligned on 4K",
    "start": "1504679",
    "end": "1510520"
  },
  {
    "text": "boundaries and when your partitions are not aligned what happens is uh sorry",
    "start": "1510520",
    "end": "1515720"
  },
  {
    "text": "ssds are 4K page line and when your partitions are not aligned EBS has to read at least one more page to service",
    "start": "1515720",
    "end": "1522840"
  },
  {
    "text": "your IO request and now we'll still probably deliver the iops that you expect the latencies on those",
    "start": "1522840",
    "end": "1528600"
  },
  {
    "text": "transactions are going to be higher and so what does that look like there's Tools in both windows and Linux",
    "start": "1528600",
    "end": "1535960"
  },
  {
    "text": "uh dis part is the tool in Windows that's a command line tool to manage your partitions uh f disk in in Linux um",
    "start": "1535960",
    "end": "1544360"
  },
  {
    "text": "so I took a an EBS volume and I put two partitions on it and so if you notice",
    "start": "1544360",
    "end": "1550440"
  },
  {
    "text": "the first one starts at sector 2048 it's divisible by 8 which means that it's 4K",
    "start": "1550440",
    "end": "1556159"
  },
  {
    "text": "aligned now 8 that's a weird number Linux shows or displays this in",
    "start": "1556159",
    "end": "1564679"
  },
  {
    "text": "in 512 by sectors because that's what we advertise is the native for EBS volumes",
    "start": "1564679",
    "end": "1570760"
  },
  {
    "text": "and 8 * 512 is 4K if you notice the second one though",
    "start": "1570760",
    "end": "1576000"
  },
  {
    "text": "the start of that partition is offset by two sectors or 1K and so anytime I do an",
    "start": "1576000",
    "end": "1581120"
  },
  {
    "text": "IO on that partition I'm going to have to do more",
    "start": "1581120",
    "end": "1586158"
  },
  {
    "text": "work let's talk a little bit about pre-warming now we've always discussed",
    "start": "1586600",
    "end": "1593440"
  },
  {
    "text": "the need to pre-warm your volumes before you Benchmark but in most cases it's really not necessary how many people",
    "start": "1593440",
    "end": "1599880"
  },
  {
    "text": "have tried to pre-warm a really large volume a lot of you you're probably in a",
    "start": "1599880",
    "end": "1605320"
  },
  {
    "text": "lot of pain weren't you yeah we noticed that pre-warming large volumes took a really long time so",
    "start": "1605320",
    "end": "1612080"
  },
  {
    "text": "we've been working and right now I'm happy to tell you that it's no longer necess necessary",
    "start": "1612080",
    "end": "1618000"
  },
  {
    "text": "to pre to pre-warm a newly created volume all you have to do is create it",
    "start": "1618000",
    "end": "1623080"
  },
  {
    "start": "1622000",
    "end": "1622000"
  },
  {
    "text": "attach it mount it and go your volume is going to achieve its expected",
    "start": "1623080",
    "end": "1630360"
  },
  {
    "text": "performance now what you might want to do uh volumes restored from snapshots are a little different um we still need",
    "start": "1630360",
    "end": "1635760"
  },
  {
    "text": "to get that data from S3 EBS gets that data in the background on demand um so",
    "start": "1635760",
    "end": "1642200"
  },
  {
    "text": "we'll load from S3 we'll pull that data down from S3 uh if your IO patterns",
    "start": "1642200",
    "end": "1647679"
  },
  {
    "text": "require us to move one of those segments earlier we'll do that um and so we've",
    "start": "1647679",
    "end": "1653640"
  },
  {
    "text": "done a little bit of work to improve the performance of getting data down from S3 but if you're noticing some",
    "start": "1653640",
    "end": "1659320"
  },
  {
    "text": "performance problems here and some performance issues one of the things that you can do is read the areas of your disk um to force us to load those",
    "start": "1659320",
    "end": "1668279"
  },
  {
    "text": "Those portions and that",
    "start": "1668279",
    "end": "1671440"
  },
  {
    "start": "1673000",
    "end": "1673000"
  },
  {
    "text": "data right so one of the cool things about EBS is that we do offer a different number of volume types and you can mix and match those volumes on your",
    "start": "1674519",
    "end": "1681600"
  },
  {
    "text": "instance and so what I've done now to our configuration I've built upon the R32 extra",
    "start": "1681600",
    "end": "1687120"
  },
  {
    "text": "large and I've split the boot volume and the data volume and so I've created an 8 GB gp2 volume and a 500 gig gp2 volume",
    "start": "1687120",
    "end": "1695480"
  },
  {
    "text": "for the data and so just by switching from",
    "start": "1695480",
    "end": "1700559"
  },
  {
    "start": "1698000",
    "end": "1698000"
  },
  {
    "text": "magnetic to gp2 I've increased performance 19% or more than 50% over",
    "start": "1700559",
    "end": "1706480"
  },
  {
    "text": "our original Baseline",
    "start": "1706480",
    "end": "1709440"
  },
  {
    "start": "1713000",
    "end": "1713000"
  },
  {
    "text": "so both gp2 and Pi Ops offer better performance over EBS magnetic volumes so",
    "start": "1713840",
    "end": "1719000"
  },
  {
    "text": "you should use our ssdb volumes when performance",
    "start": "1719000",
    "end": "1723240"
  },
  {
    "start": "1726000",
    "end": "1726000"
  },
  {
    "text": "matters so last year EBS changed what we count as an IO we changed to 256 K size",
    "start": "1726480",
    "end": "1733519"
  },
  {
    "text": "and so now it's possible to be both iops bound and throughput bound",
    "start": "1733519",
    "end": "1738600"
  },
  {
    "text": "you can send 20,000 e iops to an EBS volume that's provisioned with 20,000 iops if you're sending smaller iOS and",
    "start": "1738600",
    "end": "1746880"
  },
  {
    "text": "if you do the math that works out to 16 Kos you can also send 320 megabytes per",
    "start": "1746880",
    "end": "1753760"
  },
  {
    "text": "second if you're doing larger iOS and so 16k is our crossover point but if you",
    "start": "1753760",
    "end": "1759279"
  },
  {
    "text": "submit 256k you'll get 320 megabytes per second and if you submit 4K you'll get",
    "start": "1759279",
    "end": "1764679"
  },
  {
    "text": "20,000 iops now if you're scratching your had about what I said before that we've optimized in your instance and for",
    "start": "1764679",
    "end": "1771559"
  },
  {
    "text": "128 Kos let me explain that a little bit in the background if you're submitting",
    "start": "1771559",
    "end": "1777760"
  },
  {
    "text": "sequential IO EBS will merge that IO for you so even if you're doing small",
    "start": "1777760",
    "end": "1782840"
  },
  {
    "text": "sequential iOS we'll try to gather up as much of that as we can to make a 256 K",
    "start": "1782840",
    "end": "1788640"
  },
  {
    "text": "to give you the throughput that you're",
    "start": "1788640",
    "end": "1792039"
  },
  {
    "text": "expecting so what does this mean if you've got a smaller provision iabs volume one that may not have our full",
    "start": "1793960",
    "end": "1799640"
  },
  {
    "text": "limits of 8 thou or 20,000 iops so in this example I'm using a volume provision with 8,000 iops and I'm able",
    "start": "1799640",
    "end": "1806760"
  },
  {
    "text": "to drive 8,000 iops whether I'm doing 8K 16k or 32k because those are all underneath our",
    "start": "1806760",
    "end": "1813120"
  },
  {
    "text": "throughput limits but what I'm not able to do is submit 16,000 iops because",
    "start": "1813120",
    "end": "1821399"
  },
  {
    "text": "especially if they're random we're not going to be able to merge those and that's beyond the capabilities of your volume",
    "start": "1821399",
    "end": "1828399"
  },
  {
    "text": "similarly for a throughput workload if your IO size is larger than we can you",
    "start": "1828399",
    "end": "1833559"
  },
  {
    "text": "can't submit 864 Kos 512 megabytes per second volume is",
    "start": "1833559",
    "end": "1840720"
  },
  {
    "text": "only capable of 320 megabytes per second and so in this case what your application would see is 5,064",
    "start": "1840720",
    "end": "1848640"
  },
  {
    "text": "kios but what if that's not enough what if you need more than 16 terab of storage what if you need more than",
    "start": "1850559",
    "end": "1856600"
  },
  {
    "text": "20,000 iops what if you need more than 320 megabytes per second well one of the things that you",
    "start": "1856600",
    "end": "1863080"
  },
  {
    "text": "can do within your instance is you can raid and when we talk about raid we're usually talking about raid zero increas",
    "start": "1863080",
    "end": "1869799"
  },
  {
    "text": "in capacity or performance rarely we want to talk about redundancy let's take a moment to dive",
    "start": "1869799",
    "end": "1875799"
  },
  {
    "text": "into that when you do a raid one or a mirror within your instance every write that",
    "start": "1875799",
    "end": "1882200"
  },
  {
    "text": "you write to your EBS volume has to be written twice and so you're going to effectively have the bandwidth of your",
    "start": "1882200",
    "end": "1888559"
  },
  {
    "text": "EBS optimized instance now EBS already has a durability that's greater than a hard",
    "start": "1888559",
    "end": "1895360"
  },
  {
    "text": "drive if that's not good enough for you what we recommend is that you do",
    "start": "1895360",
    "end": "1900799"
  },
  {
    "text": "application Level redundancy and even better if you're doing that to a different availability Zone it'll give you a more available",
    "start": "1900799",
    "end": "1909600"
  },
  {
    "text": "service the other important thing to remember is that when you're striping volumes together you don't want to mix",
    "start": "1909600",
    "end": "1914720"
  },
  {
    "text": "volume types you don't want to create a raid set set with six magnetic volumes and three gp2 volumes you're probably",
    "start": "1914720",
    "end": "1921679"
  },
  {
    "text": "not going to get the performance You're Expecting out of that your performance is going to look more like the magnetic volume than it does the gp2",
    "start": "1921679",
    "end": "1928320"
  },
  {
    "text": "volume when you do stripe a volume you need to take some special attention when",
    "start": "1928320",
    "end": "1933399"
  },
  {
    "start": "1930000",
    "end": "1930000"
  },
  {
    "text": "you're taking snapshots and so for a database you're going to want to flush and lock your",
    "start": "1933399",
    "end": "1939279"
  },
  {
    "text": "tables uh the file system you might be able to sync or freeze um and then you",
    "start": "1939279",
    "end": "1944480"
  },
  {
    "text": "can do the snapshot of all the volumes together and this will give you a point in time",
    "start": "1944480",
    "end": "1950120"
  },
  {
    "text": "snapshot now when that snapshot API call Returns the create snapshot API returns",
    "start": "1950120",
    "end": "1955760"
  },
  {
    "text": "that means that it's safe to resume IO we've recorded all of the data that we need to record to complete those",
    "start": "1955760",
    "end": "1964159"
  },
  {
    "text": "snapshots now many Journal file systems and databases will work without this but",
    "start": "1966440",
    "end": "1972080"
  },
  {
    "text": "what I really want you to do if you're going to rely on that I want you to test it before you rely on it in a failure",
    "start": "1972080",
    "end": "1977279"
  },
  {
    "text": "scenario I don't want you to be",
    "start": "1977279",
    "end": "1980679"
  },
  {
    "text": "sad so let's revisit our system we've optimized everything we've got the right",
    "start": "1982480",
    "end": "1988519"
  },
  {
    "start": "1983000",
    "end": "1983000"
  },
  {
    "text": "ec2 instance with enough EBS optimized bandwidth um the right EBS volumes and our application is tuned to drive a",
    "start": "1988519",
    "end": "1995039"
  },
  {
    "text": "bunch of IO and so we've got a nice balanced system we've maximized ec2 and EBS performance how do we ensure that we",
    "start": "1995039",
    "end": "2002480"
  },
  {
    "text": "maintain this performance and so one of the things that we've got Within Amazon is our cloudwatch service now if you're",
    "start": "2002480",
    "end": "2009440"
  },
  {
    "start": "2008000",
    "end": "2008000"
  },
  {
    "text": "not using cloudwatch today I highly recommend it cloudwatch provides metrics for all",
    "start": "2009440",
    "end": "2014840"
  },
  {
    "text": "AWS services and EBS is no different you can take a look at the",
    "start": "2014840",
    "end": "2020480"
  },
  {
    "text": "metrics for your volume determine if your volume or volumes are performing as",
    "start": "2020480",
    "end": "2025519"
  },
  {
    "text": "expected a tip here all the EBS metrics are pre or prefixed with the word volume",
    "start": "2025519",
    "end": "2032279"
  },
  {
    "text": "so there's some metrics in there that are related to your instance that are that say dis iio and dis read right",
    "start": "2032279",
    "end": "2037880"
  },
  {
    "text": "those are actually your local instance storage and not your EBS",
    "start": "2037880",
    "end": "2043320"
  },
  {
    "start": "2045000",
    "end": "2045000"
  },
  {
    "text": "volumes so to show you what it looks like when you're exceeding the EBS optimized limits of your instance I've",
    "start": "2045240",
    "end": "2052919"
  },
  {
    "text": "taken the same 2000p Ops volume and attach it to three different instances the first line you'll notice",
    "start": "2052919",
    "end": "2060200"
  },
  {
    "text": "that I'm only getting 128 megabytes per second that's because I attached it to an m42 extra large and that's all the",
    "start": "2060200",
    "end": "2067079"
  },
  {
    "text": "EBS optimized bandwidth that that instance type supports now I bumped up the size to an",
    "start": "2067079",
    "end": "2073000"
  },
  {
    "text": "m44 exra large and I'm still getting great performance I'm just still limited by that instance 256 megabytes per",
    "start": "2073000",
    "end": "2079560"
  },
  {
    "text": "second if I go all the way up to an m410 x large I'm getting the capabilities of",
    "start": "2079560",
    "end": "2086480"
  },
  {
    "text": "the volume and the m410 x large actually has even more capability if you wanted to attach more volumes to",
    "start": "2086480",
    "end": "2093158"
  },
  {
    "text": "it now cloudwatch metrics are provided on all EBS volumes for",
    "start": "2094440",
    "end": "2100320"
  },
  {
    "text": "provision iops volumes they're provided at a one minute granularity and for everything else they're a 5minute granularity and those are free within",
    "start": "2100320",
    "end": "2106280"
  },
  {
    "text": "your Cloud watch console but if Cloud watch isn't enough if you need something more frequently",
    "start": "2106280",
    "end": "2112839"
  },
  {
    "text": "there's a number of tools that you can use to even monitor the performance a little bit more granular iostat is one",
    "start": "2112839",
    "end": "2121079"
  },
  {
    "text": "of them in Linux and here I've shown an example on 1 second boundaries and it's",
    "start": "2121079",
    "end": "2127240"
  },
  {
    "text": "important to remember with iostat that very first line that it shows you is going to be an average of that volume's",
    "start": "2127240",
    "end": "2133800"
  },
  {
    "text": "performance since the instance booted so it's not a snapshot in time iostat provides a lot of",
    "start": "2133800",
    "end": "2140440"
  },
  {
    "text": "information about how many requests are merged how many requests your application's submitting the Q depth the",
    "start": "2140440",
    "end": "2146599"
  },
  {
    "text": "average AE is is the latency um and then the average request size now the",
    "start": "2146599",
    "end": "2152319"
  },
  {
    "text": "important thing to remember with parallel storage systems is that utilization and service",
    "start": "2152319",
    "end": "2158359"
  },
  {
    "text": "time don't mean what you think they mean utilization is actually the amount",
    "start": "2158359",
    "end": "2164160"
  },
  {
    "text": "of time the device spent servicing at least one IO there's no bonus points in",
    "start": "2164160",
    "end": "2170280"
  },
  {
    "text": "utilization for submitting two iOS or three ios's so it doesn't really tell you about the capabilities of your",
    "start": "2170280",
    "end": "2175520"
  },
  {
    "text": "volume it just tells you how active your volume is and the other thing with service time is it really doesn't do a",
    "start": "2175520",
    "end": "2181960"
  },
  {
    "text": "good job of taking parallelism into account if you're submitting 10o",
    "start": "2181960",
    "end": "2187480"
  },
  {
    "text": "all at the same time and they complete 1 millisecond later service time is going to report that they each took 100 micros",
    "start": "2187480",
    "end": "2195119"
  },
  {
    "text": "not really reality So within your windows instance",
    "start": "2195119",
    "end": "2200800"
  },
  {
    "text": "for anybody that's using Windows um there's a tool called perfmon and",
    "start": "2200800",
    "end": "2205880"
  },
  {
    "text": "backing up perfmon it provides about the same amount of data as iostat uh the the",
    "start": "2205880",
    "end": "2211880"
  },
  {
    "text": "windows uh performance Gathering metric system uh feeds data into here and you're actually able to extract the raw",
    "start": "2211880",
    "end": "2218359"
  },
  {
    "text": "data from perfmon as well so if you want to do different slicing and dicing on it",
    "start": "2218359",
    "end": "2223720"
  },
  {
    "text": "more than perfmon can do that's available to you as",
    "start": "2223720",
    "end": "2227838"
  },
  {
    "text": "well so really the important things here are to select the right instance for your workload don't take that T2 micro",
    "start": "2230359",
    "end": "2236599"
  },
  {
    "text": "and attach a 20,000 PS volume to it select the right volume for your workload if you're concerned about",
    "start": "2236599",
    "end": "2242720"
  },
  {
    "text": "latency in iops you might want to consider an dbed volume increase the parallelism of IO",
    "start": "2242720",
    "end": "2251040"
  },
  {
    "text": "and make sure that your your volume and your whole system is performing as you expected by monitoring with",
    "start": "2251040",
    "end": "2258720"
  },
  {
    "text": "cloudwatch now for another Deep dive uh Tomorrow there's a talk about EBS Amazon",
    "start": "2258720",
    "end": "2264920"
  },
  {
    "text": "EBS and Cassandra this is specific to Cassandra and shows how crowd strike was able to get 1 million rights per second",
    "start": "2264920",
    "end": "2270480"
  },
  {
    "text": "using EBS volumes on 60 nodes so I've got some friends with from",
    "start": "2270480",
    "end": "2275880"
  },
  {
    "text": "EBS and we're happy to take some questions one-onone after this session but thank you all for for attending",
    "start": "2275880",
    "end": "2281720"
  },
  {
    "text": "today and hopefully this was helpful",
    "start": "2281720",
    "end": "2288359"
  }
]