[
  {
    "start": "0",
    "end": "78000"
  },
  {
    "text": "Oh",
    "start": "0",
    "end": "2030"
  },
  {
    "text": "good morning everyone Thanks my name is Robert Aumann balachander I",
    "start": "10380",
    "end": "16290"
  },
  {
    "text": "work as a Solutions Architect with canvas first of all thanks for joining",
    "start": "16290",
    "end": "22680"
  },
  {
    "text": "the Dublin are busy working day you make sure that the next 90 minutes is",
    "start": "22680",
    "end": "28560"
  },
  {
    "text": "productive and hope you take some insights into how the customers are",
    "start": "28560",
    "end": "36739"
  },
  {
    "text": "building get Alex on AWS before I get",
    "start": "36739",
    "end": "42600"
  },
  {
    "text": "started a couple of coats we will have separate Q&A sessions towards the end",
    "start": "42600",
    "end": "49470"
  },
  {
    "text": "but over at any point in time if you have any questions please feel free to",
    "start": "49470",
    "end": "55740"
  },
  {
    "text": "oppose those questions through the questions panel you see in the webinar window we will take up questions towards",
    "start": "55740",
    "end": "64049"
  },
  {
    "text": "the end right you will also have a short survey towards the end and we are trying",
    "start": "64049",
    "end": "69810"
  },
  {
    "text": "to get some feedback from you in terms of how this was useful for you and more importantly how we can improve the",
    "start": "69810",
    "end": "77369"
  },
  {
    "text": "experience in subsequent webinars right so with that as the context let's get",
    "start": "77369",
    "end": "84390"
  },
  {
    "start": "78000",
    "end": "78000"
  },
  {
    "text": "started so what I plan to cover in the next 90 minutes is broad three topics",
    "start": "84390",
    "end": "91250"
  },
  {
    "text": "first we will start off with what are the key business flavors for building",
    "start": "91250",
    "end": "96570"
  },
  {
    "text": "data like what we see customers using business drivers to build and it'll make",
    "start": "96570",
    "end": "103770"
  },
  {
    "text": "what are the use cases that are typically customers derivative near a lake and then we'll move on to how you",
    "start": "103770",
    "end": "111210"
  },
  {
    "text": "can build a theory like how you can design and build of AWS and finally we",
    "start": "111210",
    "end": "118530"
  },
  {
    "text": "will look at some of the production use cases what are the customers are doing in terms of building literally AWS see a",
    "start": "118530",
    "end": "126210"
  },
  {
    "start": "122000",
    "end": "122000"
  },
  {
    "text": "folder for the next 90 minutes all right so let's get started",
    "start": "126210",
    "end": "131390"
  },
  {
    "text": "so the first topic is benefits of building our data leak right we see",
    "start": "131390",
    "end": "138379"
  },
  {
    "text": "three or four main benefits why customers are building",
    "start": "138379",
    "end": "144140"
  },
  {
    "text": "the first thing is obviously in many organizations data is distributed across",
    "start": "144310",
    "end": "149610"
  },
  {
    "text": "different locations this could be different departments different systems",
    "start": "149610",
    "end": "155220"
  },
  {
    "text": "data typically moves in silos and as an organization you don't have a single",
    "start": "155220",
    "end": "162670"
  },
  {
    "text": "source of truth but you can ask your business questions and get reliable answers from the single source of truth",
    "start": "162670",
    "end": "170460"
  },
  {
    "text": "Aramaic allows you to store all such data into a common data store and",
    "start": "170460",
    "end": "176890"
  },
  {
    "text": "analyze all of the data from all those different sources in a centralized location so all the various teams that",
    "start": "176890",
    "end": "185440"
  },
  {
    "text": "are trying to get some insights out of your data can go to this single centralized location where they can",
    "start": "185440",
    "end": "191830"
  },
  {
    "text": "access the data and then start answering some of those business questions from",
    "start": "191830",
    "end": "197530"
  },
  {
    "start": "195000",
    "end": "195000"
  },
  {
    "text": "the data leak so that's number one benefit of I didn't leak the second",
    "start": "197530",
    "end": "203800"
  },
  {
    "text": "question that most organizations are post of me is how can I collect data from different sources right this could",
    "start": "203800",
    "end": "211269"
  },
  {
    "text": "be your servers your different systems like ERP is yes here as your basis may",
    "start": "211269",
    "end": "218050"
  },
  {
    "text": "be real-time systems mysociety devices didn't lose and different sources how",
    "start": "218050",
    "end": "224860"
  },
  {
    "text": "can I collect them quickly and store them efficiently to make them available for processing right I didn't like",
    "start": "224860",
    "end": "233470"
  },
  {
    "text": "allows you to quickly ingest all such data from all those different various sources without fitting in fitting into",
    "start": "233470",
    "end": "241750"
  },
  {
    "text": "a predefined schema so what we mean by a predefined schema here is that data leg",
    "start": "241750",
    "end": "246910"
  },
  {
    "text": "does not restrict your data source to ingest data into a specific schema right",
    "start": "246910",
    "end": "253030"
  },
  {
    "text": "but rather when you are trying to infer from your data leg when you're trying to process the data that's when you infer",
    "start": "253030",
    "end": "260709"
  },
  {
    "text": "the scheme right and if economy again key we will see as part of this lemon",
    "start": "260709",
    "end": "266560"
  },
  {
    "text": "are how you can will such system for your processing systems can basically in",
    "start": "266560",
    "end": "272110"
  },
  {
    "start": "269000",
    "end": "269000"
  },
  {
    "text": "first you know when we are processing the data and the next benefit is obviously around",
    "start": "272110",
    "end": "279960"
  },
  {
    "text": "data is going to continue to grow you're going to continue to collect more and more data how can you scale up my",
    "start": "279960",
    "end": "287229"
  },
  {
    "text": "infrastructure with the volume of data being generated it especially the devices that are now waking up and",
    "start": "287229",
    "end": "294069"
  },
  {
    "text": "connect with me to the Internet to the cloud where they come with potentially a",
    "start": "294069",
    "end": "299530"
  },
  {
    "text": "lot of computer these are going to IOT devices which have a lot of computing",
    "start": "299530",
    "end": "304720"
  },
  {
    "text": "capabilities on them and they can ingest data at a pretty rapid clip so how would",
    "start": "304720",
    "end": "310690"
  },
  {
    "text": "you scale up your infrastructure for collecting all of that data right you",
    "start": "310690",
    "end": "317860"
  },
  {
    "text": "can see in the webinar where if you're able to split your escalation computer you can independently scale them your",
    "start": "317860",
    "end": "324970"
  },
  {
    "text": "story it's inert and independently scale for your computer layer we're trying to",
    "start": "324970",
    "end": "331870"
  },
  {
    "text": "process the data can again in apparently scale if you process changed right",
    "start": "331870",
    "end": "337979"
  },
  {
    "start": "334000",
    "end": "334000"
  },
  {
    "text": "benefit of it and finally how can I",
    "start": "337979",
    "end": "343240"
  },
  {
    "text": "apply different processing engines different processing frameworks to",
    "start": "343240",
    "end": "348810"
  },
  {
    "text": "process the same data again this is more from you know have a different person",
    "start": "348810",
    "end": "355630"
  },
  {
    "text": "has how different what they want to see in terms of the output how can you use",
    "start": "355630",
    "end": "362889"
  },
  {
    "text": "different processing engines to process the same data through gerenuk allows you",
    "start": "362889",
    "end": "369400"
  },
  {
    "text": "to use different such processing engines so these can be for example example",
    "start": "369400",
    "end": "375039"
  },
  {
    "text": "where those are what can be an ad hoc query engine body risky is hard allows",
    "start": "375039",
    "end": "382840"
  },
  {
    "text": "you to use different crossing agents which can infer schema on read and then",
    "start": "382840",
    "end": "388389"
  },
  {
    "text": "you know allow you to process the data on your use cases right again we will",
    "start": "388389",
    "end": "393759"
  },
  {
    "text": "see how it goes offers three options few choices for you to do these kind of use",
    "start": "393759",
    "end": "401590"
  },
  {
    "start": "398000",
    "end": "398000"
  },
  {
    "text": "cases right the other aspect",
    "start": "401590",
    "end": "408540"
  },
  {
    "text": "of greater lake is the expanding access requirements today we've got lots of",
    "start": "408540",
    "end": "416430"
  },
  {
    "text": "different folks coming into your organizations and they have a need to access your data right there are folks",
    "start": "416430",
    "end": "423870"
  },
  {
    "text": "like business users who typically care about aggregated data and they probably want to use some kind of a dashboard to",
    "start": "423870",
    "end": "431150"
  },
  {
    "text": "look at your data that could be data analyst and data scientist who again",
    "start": "431150",
    "end": "437310"
  },
  {
    "text": "wants to access your data right and that can be some engagement platforms or",
    "start": "437310",
    "end": "443130"
  },
  {
    "text": "automation right that you want to click off based on your data right so these",
    "start": "443130",
    "end": "448500"
  },
  {
    "text": "are different personas who have different needs different requirements and they would want to use different",
    "start": "448500",
    "end": "455940"
  },
  {
    "text": "tools to process your data and start making some sense out of your data right",
    "start": "455940",
    "end": "462090"
  },
  {
    "text": "and when they want to do that they should be able to find what information",
    "start": "462090",
    "end": "468390"
  },
  {
    "text": "is there in my data Lake and access it in a secure fashion as well right so",
    "start": "468390",
    "end": "473730"
  },
  {
    "text": "that's going to be another important requirement of data Lake and again as we go through this session we will see how",
    "start": "473730",
    "end": "479640"
  },
  {
    "text": "different person oz can use different tools to process your data and make some",
    "start": "479640",
    "end": "485910"
  },
  {
    "start": "482000",
    "end": "482000"
  },
  {
    "text": "sensor of your data so with that as a",
    "start": "485910",
    "end": "492300"
  },
  {
    "text": "context when you move forward so you have these different systems which are",
    "start": "492300",
    "end": "497340"
  },
  {
    "text": "now trying to push data so you have an exponential growth in terms of your data so this could be your transactions which",
    "start": "497340",
    "end": "504690"
  },
  {
    "text": "are happening in your OLTP systems your databases you want to capture data from there you probably want to capture data",
    "start": "504690",
    "end": "512880"
  },
  {
    "text": "from your ERP is your web web logs maybe from servers where they're generating",
    "start": "512880",
    "end": "518880"
  },
  {
    "text": "lots of logs maybe directly from mobile devices in terms of what our users are doing in",
    "start": "518880",
    "end": "524130"
  },
  {
    "text": "your app and of course from social media where abusers are again generating data",
    "start": "524130",
    "end": "529470"
  },
  {
    "text": "base on your brand right so you need to be able to capture data from all these",
    "start": "529470",
    "end": "534570"
  },
  {
    "text": "different sources and one of the key aspects is these different sources",
    "start": "534570",
    "end": "540120"
  },
  {
    "text": "generate data at French speed and different scale right some of them can generate data at a very",
    "start": "540120",
    "end": "547470"
  },
  {
    "text": "near real-time at pretty pretty rapid clip and you should be able to ingest",
    "start": "547470",
    "end": "552890"
  },
  {
    "text": "real-time data into your data Lake and maybe some of your systems like your ERP",
    "start": "552890",
    "end": "563190"
  },
  {
    "text": "or transactions you may want to extract data on a fish your data like right so you would want it you won't won't be",
    "start": "563190",
    "end": "569280"
  },
  {
    "start": "569000",
    "end": "569000"
  },
  {
    "text": "able to pull data from all these different data sources and push into your data Lake and start making some",
    "start": "569280",
    "end": "575130"
  },
  {
    "text": "sense or of your data right so so that's going to be another aspect of your data",
    "start": "575130",
    "end": "580470"
  },
  {
    "text": "Lake right right so when you're building out your data like what are the typical",
    "start": "580470",
    "end": "585510"
  },
  {
    "text": "characteristics that you think of your data like right now obviously what is evident in the last ten minutes is that",
    "start": "585510",
    "end": "592560"
  },
  {
    "text": "you should be able to collect a lot of data collectively anything right shoes",
    "start": "592560",
    "end": "597840"
  },
  {
    "text": "you be able to collect data about anything from all your sources and then bring that into your data Lake and",
    "start": "597840",
    "end": "605720"
  },
  {
    "text": "obviously of those different personas when they want to access your data they",
    "start": "605720",
    "end": "611310"
  },
  {
    "text": "probably want to dive in into your data Lake at different granularity right",
    "start": "611310",
    "end": "616800"
  },
  {
    "text": "maybe a business user has requirements to just look at aggregated data and make",
    "start": "616800",
    "end": "623280"
  },
  {
    "text": "some sensor of that data and maybe three months down the line you assemble a team",
    "start": "623280",
    "end": "628620"
  },
  {
    "text": "to build some machine learning use cases and you have a bunch of developers who",
    "start": "628620",
    "end": "634050"
  },
  {
    "text": "are trying to build machine learning models and they probably have to dive very deep and acts as a lot of data to",
    "start": "634050",
    "end": "641130"
  },
  {
    "text": "train their models right so these different personas would want to dive into your data Lake at different",
    "start": "641130",
    "end": "648150"
  },
  {
    "text": "granularity and make some decisions make some sense out of the data so a derelict",
    "start": "648150",
    "end": "654300"
  },
  {
    "text": "should allow these different person has to dive in at any granularity and again",
    "start": "654300",
    "end": "659580"
  },
  {
    "text": "we will see that as part of today's that discussion and third aspect is flexible",
    "start": "659580",
    "end": "666030"
  },
  {
    "text": "access so these different teams should be able to use the tool of their choice",
    "start": "666030",
    "end": "671280"
  },
  {
    "text": "whatever appeals to them to process data order of the day telling a business user maybe would want to use",
    "start": "671280",
    "end": "678650"
  },
  {
    "text": "dashboards to look at data and maybe a developer would like to spin up a cluster and run some Python code to",
    "start": "678650",
    "end": "686520"
  },
  {
    "text": "process the data and maybe there is a data analyst or a data scientist who",
    "start": "686520",
    "end": "691730"
  },
  {
    "text": "works with sequel a lot and they would just want to use a sequel language to",
    "start": "691730",
    "end": "697310"
  },
  {
    "text": "access your data right so the true data like should I love you to access that",
    "start": "697310",
    "end": "703860"
  },
  {
    "text": "data through different tools which makes more sense for what type of person you",
    "start": "703860",
    "end": "709950"
  },
  {
    "text": "are who are trying to access the data so flexible access is going to be another key aspect of your data like and finally",
    "start": "709950",
    "end": "717690"
  },
  {
    "text": "future proofing which means that your business use cases evolve continuously",
    "start": "717690",
    "end": "723300"
  },
  {
    "text": "so you would probably build off your data Lake based on few use cases today and as those use cases evolve in future",
    "start": "723300",
    "end": "731190"
  },
  {
    "text": "your data layer should be able to support all of those use cases in future right so you need to be able to",
    "start": "731190",
    "end": "737520"
  },
  {
    "start": "735000",
    "end": "735000"
  },
  {
    "text": "future-proof your data like as well so",
    "start": "737520",
    "end": "743670"
  },
  {
    "text": "with that has the characteristics of a rainy lake so let's start diving little deep into a derelict and try to",
    "start": "743670",
    "end": "749850"
  },
  {
    "text": "understand what are the different components of a data Lake the first",
    "start": "749850",
    "end": "754890"
  },
  {
    "text": "thing is ingestion and storage so how do you ingest data into your derelict and",
    "start": "754890",
    "end": "759990"
  },
  {
    "text": "how do you durably store it and make it available for your processing systems right the second aspect is before people",
    "start": "759990",
    "end": "768120"
  },
  {
    "text": "process the data they should be able to find out what data lives in the data like how do people know what what data",
    "start": "768120",
    "end": "775380"
  },
  {
    "text": "lives in my data Lake right so you need a robust catalog which tells people what",
    "start": "775380",
    "end": "782370"
  },
  {
    "text": "kind of data is living in my data Lake right this is data about data this is metadata so you need a robust",
    "start": "782370",
    "end": "788700"
  },
  {
    "text": "cataloguing system and maybe you need a good search mechanisms for your catalog",
    "start": "788700",
    "end": "796050"
  },
  {
    "text": "right so people should be able to search what lives in my data Lake and then once",
    "start": "796050",
    "end": "802770"
  },
  {
    "text": "they're able to discover through the catalog on the search mechanism they can go and process the data",
    "start": "802770",
    "end": "809760"
  },
  {
    "text": "using again the tool of choice depending upon what type of person I am and then",
    "start": "809760",
    "end": "815250"
  },
  {
    "text": "serve answers right serve answers that my business is looking for and finally",
    "start": "815250",
    "end": "821699"
  },
  {
    "text": "while we do all of that you want to be making sure that your data Lake is",
    "start": "821699",
    "end": "827100"
  },
  {
    "text": "secured its protected so you need to think about your access control your",
    "start": "827100",
    "end": "832649"
  },
  {
    "text": "data encryption requirements your data classification requirements all those aspects right so I think broadly these",
    "start": "832649",
    "end": "839310"
  },
  {
    "text": "are the four different high-level components of a data Lake and what we",
    "start": "839310",
    "end": "844589"
  },
  {
    "text": "will do is in the next one or we will look at each of those components and understand what are the characteristics",
    "start": "844589",
    "end": "850800"
  },
  {
    "text": "of each component and what arable services and what tools can you actually use to build each one of those",
    "start": "850800",
    "end": "857639"
  },
  {
    "start": "853000",
    "end": "853000"
  },
  {
    "text": "components all right so so we discussed",
    "start": "857639",
    "end": "864870"
  },
  {
    "text": "about what are the characteristics of a radar Lake we discussed about the requirements for the data like to be",
    "start": "864870",
    "end": "871070"
  },
  {
    "text": "scalable to be flexible and allow you to use a different tool of choice to",
    "start": "871070",
    "end": "876240"
  },
  {
    "text": "process your data so those are the typical characteristics of derenick most of you might have dealt with Hadoop",
    "start": "876240",
    "end": "884339"
  },
  {
    "text": "as a technology in your past and you might be wondering why not use Hadoop as",
    "start": "884339",
    "end": "890250"
  },
  {
    "text": "a data Lake technology it's scalable it can scale on demand it's extensible it's",
    "start": "890250",
    "end": "896670"
  },
  {
    "text": "flexible there are plethora of tools available in the Hadoop ecosystem so you can use all of that to process your data",
    "start": "896670",
    "end": "903839"
  },
  {
    "text": "so Hadoop seems to be a perfect choice for building your data Lake so why not",
    "start": "903839",
    "end": "909300"
  },
  {
    "text": "use for loop as a data Lake technology I think one of the challenges today and I",
    "start": "909300",
    "end": "917160"
  },
  {
    "text": "typically call this out as a post cloud era cloud era a challenge is that the",
    "start": "917160",
    "end": "925380"
  },
  {
    "text": "sheer nature of computer and storage being coupled in Hadoop as one of the",
    "start": "925380",
    "end": "932100"
  },
  {
    "text": "important challenges for building a reader like right so when you are spinning up a Hadoop cluster you are",
    "start": "932100",
    "end": "939149"
  },
  {
    "text": "spinning up both the computer and storage and they are tight coupled and data lives within that",
    "start": "939149",
    "end": "945930"
  },
  {
    "text": "Hadoop cluster right now what if what is a challenge with this kind of setup the",
    "start": "945930",
    "end": "952050"
  },
  {
    "text": "number one so let me give you a perspective so let's say you have spun up a Hadoop cluster maybe you have spun",
    "start": "952050",
    "end": "958079"
  },
  {
    "text": "up a 10 note out of cluster or 100 node cluster depending upon your requirements",
    "start": "958079",
    "end": "963269"
  },
  {
    "text": "and most of the time you size your cluster based on the storage requirements you anticipate how much",
    "start": "963269",
    "end": "969870"
  },
  {
    "text": "amount of data you are going to collect and then you size your cluster now let's",
    "start": "969870",
    "end": "975959"
  },
  {
    "text": "say you have a 10 load cluster and it has got about a terabyte of storage on each of the nodes you have over 10",
    "start": "975959",
    "end": "981329"
  },
  {
    "text": "terabytes of storage and you are collecting data right and as you are",
    "start": "981329",
    "end": "987899"
  },
  {
    "text": "collecting more and more data you realize let's say six months down the line your storage requirements starts",
    "start": "987899",
    "end": "994110"
  },
  {
    "text": "growing and you are hitting that 10 terabyte mark pretty quickly right so",
    "start": "994110",
    "end": "999660"
  },
  {
    "text": "you gotta go and scale your storage right so when you're gonna go and scale your storage your storage cannot scale",
    "start": "999660",
    "end": "1007070"
  },
  {
    "text": "independently right you're gonna go and scale your compute as well maybe your computer requirements are good enough",
    "start": "1007070",
    "end": "1014240"
  },
  {
    "text": "for your requirements right your compute is perfectly fine it's able to run all your jobs at the kind of essays that you",
    "start": "1014240",
    "end": "1022910"
  },
  {
    "text": "expect but it stories requirements of increased right and you want to scale your storage you also have to go and",
    "start": "1022910",
    "end": "1029270"
  },
  {
    "text": "scale your computer right which means that you're gonna end up having some idle resources in your computer layer",
    "start": "1029270",
    "end": "1035418"
  },
  {
    "text": "but you can't avoid that because that has to scale because the stories just escape so that's one challenge if you",
    "start": "1035419",
    "end": "1042798"
  },
  {
    "text": "flip that around so let's say I am doing perfectly fine with storage and I'm not",
    "start": "1042799",
    "end": "1048079"
  },
  {
    "text": "growing as quickly I'm actually good there but on my computer side what's",
    "start": "1048079",
    "end": "1055280"
  },
  {
    "text": "happening now is there are different teams which are coming up and asking for different resources right you",
    "start": "1055280",
    "end": "1062030"
  },
  {
    "text": "anticipated few use cases and your sizes cluster and maybe a new team has got found which is trying to do some new use",
    "start": "1062030",
    "end": "1069200"
  },
  {
    "text": "cases maybe somebody is trying to build machine learning model and they come back to you and say I want to run this",
    "start": "1069200",
    "end": "1075740"
  },
  {
    "text": "training X on the cluster and I need to look at the entire year's worth of data so then",
    "start": "1075740",
    "end": "1082520"
  },
  {
    "text": "you're gonna go back to them and say that you know no hang on I don't have enough compute capabilities because my",
    "start": "1082520",
    "end": "1088490"
  },
  {
    "text": "jobs are already utilizing all of that I'm already peeking at my compute so you",
    "start": "1088490",
    "end": "1093530"
  },
  {
    "text": "can either come back Saturday morning and you have a window of six hours in",
    "start": "1093530",
    "end": "1098540"
  },
  {
    "text": "Saturday and you can run your job right if that's not good enough for you I'll have to go and add more compute that's",
    "start": "1098540",
    "end": "1104780"
  },
  {
    "text": "gonna take me a few more weeks you can come back to me after a few more weeks right so even if your storage",
    "start": "1104780",
    "end": "1110390"
  },
  {
    "text": "requirements are going to be okay if you have got additional compute requirements",
    "start": "1110390",
    "end": "1116330"
  },
  {
    "text": "coming in right again you got a scale both your computing storage storage together if you're able to decouple",
    "start": "1116330",
    "end": "1123730"
  },
  {
    "text": "compute and storage right where I'm able to have a different storage layer and a",
    "start": "1123730",
    "end": "1130160"
  },
  {
    "text": "different computer layer and they're independent of each other all these challenges can go of it right your",
    "start": "1130160",
    "end": "1136190"
  },
  {
    "text": "storage can independently scale your compute can independently scale and potentially you can even have different",
    "start": "1136190",
    "end": "1143870"
  },
  {
    "text": "compute layers coming in right and accessing the same storage layer right",
    "start": "1143870",
    "end": "1150320"
  },
  {
    "text": "so these compute layers can be different processing engines depending upon what type of person you are and this can be",
    "start": "1150320",
    "end": "1157040"
  },
  {
    "text": "even transient in nature you can have one compute engine coming up let's say once in a day processing some data for",
    "start": "1157040",
    "end": "1164210"
  },
  {
    "text": "every R and then dying after that right you could probably spin up a computer layer just for the machine learning use",
    "start": "1164210",
    "end": "1170720"
  },
  {
    "text": "training use case that runs for maybe some 15 20 hours and then once it is done I can again get terminated all",
    "start": "1170720",
    "end": "1178760"
  },
  {
    "text": "right so all those benefits will come when you are able to decouple your computer and storage right so the next",
    "start": "1178760",
    "end": "1186310"
  },
  {
    "text": "one R or so we will see how you are able to decouple compute and storage in AWS",
    "start": "1186310",
    "end": "1192170"
  },
  {
    "text": "and use different compute options to build a true data link the other a spur",
    "start": "1192170",
    "end": "1201910"
  },
  {
    "start": "1194000",
    "end": "1194000"
  },
  {
    "text": "is most of you again would have dealt with some kind of a data warehouse in",
    "start": "1201910",
    "end": "1206920"
  },
  {
    "text": "your past so when people think about data leak there is also this",
    "start": "1206920",
    "end": "1213910"
  },
  {
    "text": "misconception that a data leak is probably a replacement for your data",
    "start": "1213910",
    "end": "1219160"
  },
  {
    "text": "warehouse right so you forgot already read about hosts sitting in my on-premises when I let's say trying to",
    "start": "1219160",
    "end": "1225790"
  },
  {
    "text": "build data leak it's probably going to replace my data warehouse right our view",
    "start": "1225790",
    "end": "1231550"
  },
  {
    "text": "here is that your data warehouse is going to be a complementary to your data",
    "start": "1231550",
    "end": "1238030"
  },
  {
    "text": "Lake it's not going to be a replacement for your data warehouse right a daedalic",
    "start": "1238030",
    "end": "1243070"
  },
  {
    "text": "is going to complement your data whereas use cases however a data Lake can be a",
    "start": "1243070",
    "end": "1248500"
  },
  {
    "text": "source for your data warehouse so data warehouse could be used for a specific",
    "start": "1248500",
    "end": "1253810"
  },
  {
    "text": "use case so maybe there is a business user who is trying to make some decisions through a dashboard and that",
    "start": "1253810",
    "end": "1260110"
  },
  {
    "text": "kind of use cases can be powered through a data warehouse so ideally it can actually be a source for your data",
    "start": "1260110",
    "end": "1267070"
  },
  {
    "text": "warehouse right now there is also going to be certain amount of differences between a data Lake and a data warehouse",
    "start": "1267070",
    "end": "1274150"
  },
  {
    "text": "right I have data warehouse on the right-hand side allows you to do what we",
    "start": "1274150",
    "end": "1279160"
  },
  {
    "text": "call as schema on right so you're gonna predefined schemas in your data warehouse and when you're collecting",
    "start": "1279160",
    "end": "1285670"
  },
  {
    "text": "data you're going to collect the data and process the data or transform it and then write it into that particular",
    "start": "1285670",
    "end": "1292060"
  },
  {
    "text": "schema so schema is fixed when you write data into your data warehouse versus",
    "start": "1292060",
    "end": "1298330"
  },
  {
    "text": "data Lake allows you to do what we call a schema and read where depending upon",
    "start": "1298330",
    "end": "1304180"
  },
  {
    "text": "what is the processing engine it can define on what scheme oyd it wants to",
    "start": "1304180",
    "end": "1309250"
  },
  {
    "text": "infer the data and add a delay can allow you to do that so that's going to be one key difference right and obviously an",
    "start": "1309250",
    "end": "1316630"
  },
  {
    "text": "era where ho's allows you to process only structured data Avira Lake allows you to keep data in",
    "start": "1316630",
    "end": "1323650"
  },
  {
    "text": "whatever formats that you want to keep and process data in that format it can",
    "start": "1323650",
    "end": "1328780"
  },
  {
    "text": "be structured unstructured semi-structured data Lake does not really restrict you in terms of what",
    "start": "1328780",
    "end": "1335230"
  },
  {
    "text": "kind data you can store in editor data like I did a warehouse if you want to introduce",
    "start": "1335230",
    "end": "1341140"
  },
  {
    "text": "new content let's say you want to introduce a new dashboard or a new",
    "start": "1341140",
    "end": "1346240"
  },
  {
    "text": "schema it's gonna take some time to introduce new data because you have to again build out those new schema build",
    "start": "1346240",
    "end": "1352570"
  },
  {
    "text": "out your ETL pipelines to transform data it's going to be time consuming to introduce new data but a derelict allows",
    "start": "1352570",
    "end": "1358600"
  },
  {
    "text": "you to ingest data at a rapid clip you can ingest data from new data sources because the schema inference is going to",
    "start": "1358600",
    "end": "1365740"
  },
  {
    "text": "be done when you're processing the data right in terms of use cases data",
    "start": "1365740",
    "end": "1371860"
  },
  {
    "text": "warehouse typically allows you to do bi type of use cases with probably very",
    "start": "1371860",
    "end": "1377380"
  },
  {
    "text": "limited predictive use cases a data Lake on the other hand allows you to do a",
    "start": "1377380",
    "end": "1382750"
  },
  {
    "text": "wide variety of use cases it can do your bi use cases it can do data science it",
    "start": "1382750",
    "end": "1387760"
  },
  {
    "text": "can do advanced analytics maybe predictive analytics machine learning",
    "start": "1387760",
    "end": "1393700"
  },
  {
    "text": "all of these use cases can be supported by a data Lake in terms of granularity",
    "start": "1393700",
    "end": "1401440"
  },
  {
    "text": "your data warehouse allows you to keep data probably at the summary level",
    "start": "1401440",
    "end": "1407429"
  },
  {
    "text": "aggregated level and you can start making some sense or of that data when you really want to go deep when you",
    "start": "1407429",
    "end": "1413799"
  },
  {
    "text": "really want to drill down and go pretty deep into your data you might not keep",
    "start": "1413799",
    "end": "1418929"
  },
  {
    "text": "it in a data warehouse because it then becomes an expensive proposition to keep all of your raw data into your data",
    "start": "1418929",
    "end": "1425440"
  },
  {
    "text": "warehouse right versus in a data Lake you can keep your data at the lowest",
    "start": "1425440",
    "end": "1431080"
  },
  {
    "text": "level of granularity and different people can dive in at whatever granularity that they would like to dive",
    "start": "1431080",
    "end": "1438610"
  },
  {
    "text": "in right and at the last you do get a lot of flexibility in terms of what",
    "start": "1438610",
    "end": "1444100"
  },
  {
    "text": "tools you can use in your data Lake and in a data warehouse you are typically limited by a sequel based tool that's",
    "start": "1444100",
    "end": "1452470"
  },
  {
    "text": "what it's mostly powerful for right so those are the kind of differences so I want to I want to kind of get this",
    "start": "1452470",
    "end": "1459100"
  },
  {
    "text": "context set up front because when we progress we will definitely talk about",
    "start": "1459100",
    "end": "1464289"
  },
  {
    "text": "data warehousing use cases right so that our us is still part of the oral story but it's it's basically a part of",
    "start": "1464289",
    "end": "1471910"
  },
  {
    "text": "the derelict and you riddle it can be a source of your data warehouse right so if you can keep this differences as some",
    "start": "1471910",
    "end": "1480490"
  },
  {
    "text": "bit of context you'll be able to appreciate it when we discuss about the data warehousing use cases all right so",
    "start": "1480490",
    "end": "1490980"
  },
  {
    "text": "we saw the business drivers in terms of why organizations are building a derelict so let's move on to the next",
    "start": "1490980",
    "end": "1497650"
  },
  {
    "text": "topic how you as a customer can design and build a data Lake right so that's",
    "start": "1497650",
    "end": "1503860"
  },
  {
    "text": "going to be the next topic we're going to recap we have the four important",
    "start": "1503860",
    "end": "1509380"
  },
  {
    "text": "components of a data link the ingestion and storage and then the catalog and search the process and serve layer and",
    "start": "1509380",
    "end": "1517510"
  },
  {
    "text": "finally the security layer right so in terms of designing and building your data Lake we will now dive deep into",
    "start": "1517510",
    "end": "1524080"
  },
  {
    "text": "each of this component starting with ingestion and storage so",
    "start": "1524080",
    "end": "1529510"
  },
  {
    "text": "in this topic we will see what are the different ways you can ingest data into your data Lake and how you can durably",
    "start": "1529510",
    "end": "1536740"
  },
  {
    "text": "store deter into your data Lake so when",
    "start": "1536740",
    "end": "1542350"
  },
  {
    "start": "1542000",
    "end": "1542000"
  },
  {
    "text": "it comes to building a derelict on AWS you get a lot of data ingestion options",
    "start": "1542350",
    "end": "1548910"
  },
  {
    "text": "we will speak about some of them and we will dive deep into one specific service",
    "start": "1548910",
    "end": "1554050"
  },
  {
    "text": "so if you are already having infrastructure on premises you have multiple options in terms of",
    "start": "1554050",
    "end": "1560980"
  },
  {
    "text": "connecting to AWS and ingesting data into AWS starting with AWS Direct",
    "start": "1560980",
    "end": "1567070"
  },
  {
    "text": "Connect which allows you to establish a dedicated network which is private to",
    "start": "1567070",
    "end": "1573070"
  },
  {
    "text": "your organization and it can connect to your data center to AWS and you can use",
    "start": "1573070",
    "end": "1578860"
  },
  {
    "text": "that private network to ingest data into AWS so this lets you operate in a hybrid",
    "start": "1578860",
    "end": "1585220"
  },
  {
    "text": "fashion where some of your on-premise systems can remain on promise and they",
    "start": "1585220",
    "end": "1590530"
  },
  {
    "text": "can ingest data into a table base and you can start building your data leak in AWS you also get a number of ISP",
    "start": "1590530",
    "end": "1599500"
  },
  {
    "text": "connectors which can again extract data from your data sources and ingest into AWS right",
    "start": "1599500",
    "end": "1606870"
  },
  {
    "text": "we have a broad set of ISP connectors so these can be your data integration",
    "start": "1606870",
    "end": "1611950"
  },
  {
    "text": "partners your ETL tools that you are already using all of them half-god",
    "start": "1611950",
    "end": "1617110"
  },
  {
    "text": "connectivity - aw services to ingest data into you did a lake in AWS right so",
    "start": "1617110",
    "end": "1623860"
  },
  {
    "text": "think of these tools are Slayer like your infirmity girls of the world your your methylenes of the world so all of",
    "start": "1623860",
    "end": "1630010"
  },
  {
    "text": "them have got connectors to our AWS services right the next one is kinases",
    "start": "1630010",
    "end": "1636370"
  },
  {
    "text": "again we will dive deep into what kinases so I'll skip that for now and then you got something called as storage",
    "start": "1636370",
    "end": "1643330"
  },
  {
    "text": "gateway which is an appliance which is a software appliance that you can deploy",
    "start": "1643330",
    "end": "1649210"
  },
  {
    "text": "in your unpromising environment and you can use that to again ingest data into",
    "start": "1649210",
    "end": "1654610"
  },
  {
    "text": "AWS right so we provide a virtual machine image a VM image that you can",
    "start": "1654610",
    "end": "1661350"
  },
  {
    "text": "deploy in your on-premise environment as a virtual machine it can connect to your",
    "start": "1661350",
    "end": "1666610"
  },
  {
    "text": "existing infrastructure and it can start ingesting data into AWS and then there",
    "start": "1666610",
    "end": "1673330"
  },
  {
    "text": "is s3 transfer acceleration we will talk about s3 in detail but what s3 transfer",
    "start": "1673330",
    "end": "1678550"
  },
  {
    "text": "acceleration allows you to do is use our edge network where you got lots of edge",
    "start": "1678550",
    "end": "1684940"
  },
  {
    "text": "locations across the globe use that to rapidly ingest data into s3 right",
    "start": "1684940",
    "end": "1691560"
  },
  {
    "text": "so that again speeds up your ingestion speeds into the s3 service with respect",
    "start": "1691560",
    "end": "1698290"
  },
  {
    "text": "to s3 service will again talk about it slightly ahead in terms of what capabilities history offers and why it",
    "start": "1698290",
    "end": "1704830"
  },
  {
    "text": "makes sense to build your data like on top of AWS and above history and finally",
    "start": "1704830",
    "end": "1710080"
  },
  {
    "text": "you got a table a snowball which is physical appliance that we can ship to",
    "start": "1710080",
    "end": "1715570"
  },
  {
    "text": "your promises so it has got about 100 terabytes of storage it gets shipped",
    "start": "1715570",
    "end": "1720610"
  },
  {
    "text": "your promises you can hook up that to your network start transferring data into the appliance and once you are done",
    "start": "1720610",
    "end": "1727510"
  },
  {
    "text": "you can ship it back to a datacenter and we will copy that data into your s3",
    "start": "1727510",
    "end": "1733510"
  },
  {
    "text": "buckets and you can start processing your data right I'm again on top of all of these options",
    "start": "1733510",
    "end": "1738969"
  },
  {
    "text": "now we provide robust api's right so we provide a CLI we provide the SDKs for",
    "start": "1738969",
    "end": "1746139"
  },
  {
    "text": "all popular languages be it Java Python dotnet Ruby all of these popular",
    "start": "1746139",
    "end": "1752729"
  },
  {
    "text": "languages we provide SDKs so you can use any of these SDKs or see allies and",
    "start": "1752729",
    "end": "1757949"
  },
  {
    "text": "never underestimate the power of CLI NSTIC here it I'm up I'm a fan of CLI",
    "start": "1757949",
    "end": "1763299"
  },
  {
    "text": "you can use this for example use the s3 CLI to simply start pushing data into s3",
    "start": "1763299",
    "end": "1769569"
  },
  {
    "text": "right there definitely very very powerful so that's another choice if none of those above charges work for you",
    "start": "1769569",
    "end": "1776699"
  },
  {
    "text": "so with that as the context from a data ingestion perspective let's now dive",
    "start": "1776699",
    "end": "1782499"
  },
  {
    "text": "little deep into one specific service one specific platform called as amazon",
    "start": "1782499",
    "end": "1787749"
  },
  {
    "text": "kinases right and we'll talk about how you can use kinases to ingest data into",
    "start": "1787749",
    "end": "1792909"
  },
  {
    "start": "1789000",
    "end": "1789000"
  },
  {
    "text": "AWS so the amazon kinases has become a",
    "start": "1792909",
    "end": "1798999"
  },
  {
    "text": "platform by itself so it now offers three different services right you use",
    "start": "1798999",
    "end": "1804819"
  },
  {
    "text": "these three services to ingest streaming data into AWS and again start processing",
    "start": "1804819",
    "end": "1811509"
  },
  {
    "text": "streaming data in your real time right there are three different services the",
    "start": "1811509",
    "end": "1817539"
  },
  {
    "text": "first one is a kinases streams second one is kinases firehose and third one is",
    "start": "1817539",
    "end": "1823419"
  },
  {
    "text": "kinases analytics right and we will again start looking at each of those services in little but little bit more",
    "start": "1823419",
    "end": "1831069"
  },
  {
    "start": "1828000",
    "end": "1828000"
  },
  {
    "text": "detail and understand what it can offer in terms of capability is starting first",
    "start": "1831069",
    "end": "1836259"
  },
  {
    "text": "with kinases streams so what kinases streams allows you to do is to create an",
    "start": "1836259",
    "end": "1842289"
  },
  {
    "text": "arbitrary stream so for example you have got data coming from various data",
    "start": "1842289",
    "end": "1847899"
  },
  {
    "text": "sources so this can be your your websites your servers your mobile apps",
    "start": "1847899",
    "end": "1853479"
  },
  {
    "text": "your on to my systems there can be coming from different data sources you",
    "start": "1853479",
    "end": "1860289"
  },
  {
    "text": "can go to kinases streams and create a arbitrary binary stream which is multi",
    "start": "1860289",
    "end": "1867190"
  },
  {
    "text": "availability zone by default for most of you who might not know what an availability zone is an availability",
    "start": "1867190",
    "end": "1873309"
  },
  {
    "text": "zone is one or more data centers that operate in a region and in each region",
    "start": "1873309",
    "end": "1881049"
  },
  {
    "text": "we've got multiple availability zones and each availability zone has got its",
    "start": "1881049",
    "end": "1886090"
  },
  {
    "text": "own independent infrastructure in terms of power cooling internet backups and",
    "start": "1886090",
    "end": "1892480"
  },
  {
    "text": "all of that and each availability zone has got independent all of the",
    "start": "1892480",
    "end": "1897519"
  },
  {
    "text": "infrastructure they are isolated from each other and you can operate in a highly available fashion right so when",
    "start": "1897519",
    "end": "1903549"
  },
  {
    "text": "you deploy kinases stream it is deployed across multiple availability zone by",
    "start": "1903549",
    "end": "1909039"
  },
  {
    "text": "default and from all those different data sources you can ingest data in",
    "start": "1909039",
    "end": "1914909"
  },
  {
    "text": "near-real-time into that infrastructure and that infrastructure is fully managed",
    "start": "1914909",
    "end": "1920110"
  },
  {
    "text": "by AWS you don't have to manage any infrastructure on your behalf AWS will do it for you and as those data are",
    "start": "1920110",
    "end": "1927429"
  },
  {
    "text": "getting ingested into that infrastructure on the other side you can have various applications consuming the",
    "start": "1927429",
    "end": "1935440"
  },
  {
    "text": "data again in near real-time and doing various use cases right maybe you can",
    "start": "1935440",
    "end": "1941649"
  },
  {
    "text": "have one application which is simply collecting the data and pushing the data into s3 which is our storage service and",
    "start": "1941649",
    "end": "1948909"
  },
  {
    "text": "maybe you have another application which is simultaneously processing the same",
    "start": "1948909",
    "end": "1954340"
  },
  {
    "text": "data and doing a sliding window analysis it's trying to probably detect some",
    "start": "1954340",
    "end": "1959919"
  },
  {
    "text": "anomalies and alert your systems when some anomaly is being detected and maybe",
    "start": "1959919",
    "end": "1965379"
  },
  {
    "text": "you have a third application which is simply loading the data into your redshift which is a data warehouse I'll",
    "start": "1965379",
    "end": "1970419"
  },
  {
    "text": "talk about it in detail and you can use that to serve your business use cases right so either you can have different",
    "start": "1970419",
    "end": "1977500"
  },
  {
    "text": "applications that you deploy all of them can connect to that stream concurrently",
    "start": "1977500",
    "end": "1983889"
  },
  {
    "text": "and process that real-time data are concurrently and start answering your questions right that's what Kinesis",
    "start": "1983889",
    "end": "1990610"
  },
  {
    "text": "streams can offer you so by default kanessa streams allows you to retain",
    "start": "1990610",
    "end": "1997960"
  },
  {
    "text": "data for 24 hours so whatever data that you ingest into Kinesis streams",
    "start": "1997960",
    "end": "2003149"
  },
  {
    "text": "it is retained by default for 24 hours and after that we will automatically",
    "start": "2003149",
    "end": "2008789"
  },
  {
    "text": "expire the data from the stream of course if you want to read in it for slightly longer period you can extend it",
    "start": "2008789",
    "end": "2016200"
  },
  {
    "text": "up to 7 days right that's with respect to the data retention and you can replay",
    "start": "2016200",
    "end": "2023669"
  },
  {
    "text": "the data within any 24 hour window in that stream so let's say you are",
    "start": "2023669",
    "end": "2028950"
  },
  {
    "text": "ingesting continuously into the stream and one of the applications it it",
    "start": "2028950",
    "end": "2035219"
  },
  {
    "text": "crashed for some reason maybe you had an infrastructure challenge or the application just crashed for whatever",
    "start": "2035219",
    "end": "2041339"
  },
  {
    "text": "reason and you now bring up the application let's say couple of hours later now the application has to process",
    "start": "2041339",
    "end": "2048628"
  },
  {
    "text": "from where it left right so the application has to go back in time and process from that particular point in",
    "start": "2048629",
    "end": "2055559"
  },
  {
    "text": "time right if it crash for example at 10 o'clock in the morning and you retire the application at 2 o'clock in the noon",
    "start": "2055559",
    "end": "2062520"
  },
  {
    "text": "the application has to go back in time and process all the data from 10 o'clock",
    "start": "2062520",
    "end": "2067529"
  },
  {
    "text": "right so Kinesis streams allows you to replay within the 24 hour window if you",
    "start": "2067529",
    "end": "2073349"
  },
  {
    "text": "want to go back in time and then process the data and then on the injection site",
    "start": "2073349",
    "end": "2079770"
  },
  {
    "text": "if let's say your throughput starts increasing let's say all those devices are ingesting more and more data what",
    "start": "2079770",
    "end": "2087299"
  },
  {
    "text": "you can do for your kinases stream is you can dynamically scale out and scale",
    "start": "2087299",
    "end": "2093179"
  },
  {
    "text": "in the number of shards behind your stream right so when you create a stream you deploy something called as the",
    "start": "2093179",
    "end": "2100260"
  },
  {
    "text": "number of sharks behind a stream and as your throughput requirements increases you can scale out and scale in the",
    "start": "2100260",
    "end": "2107160"
  },
  {
    "text": "number of shards behind your stream the service will automatically take care of splitting the number of shards or",
    "start": "2107160",
    "end": "2113819"
  },
  {
    "text": "emerging the number of shards depending upon what you're doing on on the stream and finally in terms of stream",
    "start": "2113819",
    "end": "2120960"
  },
  {
    "text": "processing you have lots of choices you can run your own applications on ec2 or",
    "start": "2120960",
    "end": "2126720"
  },
  {
    "text": "you can use lambda which is basically a service that we provide but you can",
    "start": "2126720",
    "end": "2132150"
  },
  {
    "text": "upload a piece of code and that piece of code gets invoked whenever there is any coming into your stream or you can use",
    "start": "2132150",
    "end": "2139500"
  },
  {
    "text": "any of the popular stream processing engines be it let's say Apache spark or",
    "start": "2139500",
    "end": "2145110"
  },
  {
    "text": "Apache splink all of them have got connectors to kinases and you can deploy a cluster and start processing data from",
    "start": "2145110",
    "end": "2152640"
  },
  {
    "start": "2149000",
    "end": "2149000"
  },
  {
    "text": "your kinases trip so that's kinases stream in a nutshell",
    "start": "2152640",
    "end": "2158850"
  },
  {
    "text": "which allows you to ingest data in real time and process the data in near real",
    "start": "2158850",
    "end": "2165120"
  },
  {
    "text": "time and you're got lots of choices in terms of how you can process the data and start making some sense out of the",
    "start": "2165120",
    "end": "2171930"
  },
  {
    "text": "data right so that's number one service the second service within kinases family",
    "start": "2171930",
    "end": "2177150"
  },
  {
    "text": "is kinases firehorse where again it allows you to create something called as a string a delivery",
    "start": "2177150",
    "end": "2185280"
  },
  {
    "text": "stream and when you create that stream you can again ingest it off from all",
    "start": "2185280",
    "end": "2191790"
  },
  {
    "text": "your data sources again this can be your mobile device as your servers or your iot devices all of them can ingest data",
    "start": "2191790",
    "end": "2197940"
  },
  {
    "text": "into that particular string we would automatically provision all the",
    "start": "2197940",
    "end": "2204390"
  },
  {
    "text": "infrastructure behind the string we would take care of automatically",
    "start": "2204390",
    "end": "2209670"
  },
  {
    "text": "partitioning your key space to improve throughput whenever there is an increase in your throughput and we would also",
    "start": "2209670",
    "end": "2216150"
  },
  {
    "text": "take care of the scaling aspect whenever let's say your ingestion side increases",
    "start": "2216150",
    "end": "2222210"
  },
  {
    "text": "the throughput we can automatically again increase the infrastructure behind firehose while we do all of that",
    "start": "2222210",
    "end": "2230400"
  },
  {
    "text": "on the other side we can automatically batch we can compress the data and",
    "start": "2230400",
    "end": "2236960"
  },
  {
    "text": "encrypt the data and land the data into",
    "start": "2236960",
    "end": "2242310"
  },
  {
    "text": "any of those three destinations so kinases firehose can automatically batch",
    "start": "2242310",
    "end": "2249690"
  },
  {
    "text": "compress and encrypt streaming data and land the data into either s3 redshift or",
    "start": "2249690",
    "end": "2257610"
  },
  {
    "text": "Una's an elastic elastic search service right so if you want data to be",
    "start": "2257610",
    "end": "2264090"
  },
  {
    "text": "available to any of those three destinations kinases fire rows can automatically interested I into",
    "start": "2264090",
    "end": "2271000"
  },
  {
    "text": "those three destinations right so now that fireworks can do all of that it",
    "start": "2271000",
    "end": "2277210"
  },
  {
    "start": "2274000",
    "end": "2274000"
  },
  {
    "text": "sounds very very similar to string so why would we build two different services to do similar things right but",
    "start": "2277210",
    "end": "2285700"
  },
  {
    "text": "there are some key differences between what streams can do and what fire rows can do right so streams allows you to",
    "start": "2285700",
    "end": "2292780"
  },
  {
    "text": "build an arbitrary stream that you can use to ingest near real-time data and",
    "start": "2292780",
    "end": "2298440"
  },
  {
    "text": "process data on the other side right fire hose is fully managed but you don't",
    "start": "2298440",
    "end": "2305530"
  },
  {
    "text": "have to provision any underlying resources at all and it can automatically load data into those",
    "start": "2305530",
    "end": "2311440"
  },
  {
    "text": "destinations right in terms of provisioning and resource management",
    "start": "2311440",
    "end": "2317580"
  },
  {
    "text": "though Kinesis streams manages infrastructure on your behalf you still",
    "start": "2317580",
    "end": "2323230"
  },
  {
    "text": "provision something called as a shard there is a notion of shard that is still provision each shard can allow you to",
    "start": "2323230",
    "end": "2331300"
  },
  {
    "text": "ingest up to one MB per second and egress at two MB per second if you need",
    "start": "2331300",
    "end": "2337600"
  },
  {
    "text": "certain throughput you provisions or the number of shards if you need let's say 100 MB per second of throughput you",
    "start": "2337600",
    "end": "2345460"
  },
  {
    "text": "probably would deploy hundred shards behind your stream right versus firehose",
    "start": "2345460",
    "end": "2352420"
  },
  {
    "text": "where you don't have to deploy any shards we would again automatically",
    "start": "2352420",
    "end": "2358470"
  },
  {
    "text": "deploy infrastructure for you right so you don't have to provision any",
    "start": "2358470",
    "end": "2363700"
  },
  {
    "text": "underlying resources at all AWS will take care of automatically doing that for you on the streams whenever you want",
    "start": "2363700",
    "end": "2371080"
  },
  {
    "text": "more throughput you are going to explicitly scale right so as a developer you are going to control how much",
    "start": "2371080",
    "end": "2377710"
  },
  {
    "text": "throughput I need from my stream so you would scale the number of shards based",
    "start": "2377710",
    "end": "2382900"
  },
  {
    "text": "on again metrics we provide metrics or a shard level you can go and use those metrics to scale here on the firo site",
    "start": "2382900",
    "end": "2390760"
  },
  {
    "text": "you don't have to explicitly do any scaling action AWS will automatically do",
    "start": "2390760",
    "end": "2396910"
  },
  {
    "text": "it for you the fire service automatically looks at what is your ingestion throughput and it",
    "start": "2396910",
    "end": "2402160"
  },
  {
    "text": "will automatically scale the infrastructure right this next aspect is data",
    "start": "2402160",
    "end": "2409060"
  },
  {
    "text": "partitioning because you are deploying a sharded infrastructure in Kinesis streams you control how delegates",
    "start": "2409060",
    "end": "2415450"
  },
  {
    "text": "partitioned in your payload you nominate a particular key as your partition key",
    "start": "2415450",
    "end": "2421870"
  },
  {
    "text": "and Kinesis streams will use that partition key to send the data to the right sharks behind the infrastructure",
    "start": "2421870",
    "end": "2428650"
  },
  {
    "text": "right in FiOS you don't have to do any kind of partitioning fire all automatic",
    "start": "2428650",
    "end": "2434530"
  },
  {
    "text": "do partitioning for you right from an application development perspective in",
    "start": "2434530",
    "end": "2440590"
  },
  {
    "text": "streams you get access to the low-level gate api's and put ap ice so you can use those gate and put API is to ingest",
    "start": "2440590",
    "end": "2447700"
  },
  {
    "text": "directly into the stream we also provide higher-level libraries in SDKs to do that and you can",
    "start": "2447700",
    "end": "2455170"
  },
  {
    "text": "use that to write your own consumers to consume data on the firo site we provide",
    "start": "2455170",
    "end": "2461830"
  },
  {
    "text": "agents that you can deploy on your servers to ingest data into firehose and on the other side very trying to consume",
    "start": "2461830",
    "end": "2468580"
  },
  {
    "text": "data the destinations are fixed so destinations are only s3 redshift and",
    "start": "2468580",
    "end": "2475510"
  },
  {
    "text": "Amazon Elastic search service so firewalls can only land data into one of those three destinations however if you",
    "start": "2475510",
    "end": "2482140"
  },
  {
    "text": "want to deploy your own application and if you want to push to let's say some other dashboard you can't do that with",
    "start": "2482140",
    "end": "2487330"
  },
  {
    "text": "FiOS you'll have to go to Kinesis streams for that and finally in terms of latency this is data latency Kinesis",
    "start": "2487330",
    "end": "2495670"
  },
  {
    "text": "streams allows you to do near real-time use cases but it can deliver data to",
    "start": "2495670",
    "end": "2502270"
  },
  {
    "text": "your consumer applications in near real-time this can be in the order of couple of seconds and firehose has a",
    "start": "2502270",
    "end": "2512220"
  },
  {
    "text": "notion of buffering built in so before it lands data into those three",
    "start": "2512220",
    "end": "2517390"
  },
  {
    "text": "destinations s3 redshift or elasticsearch it's going to automatically buffer data the minimum",
    "start": "2517390",
    "end": "2523690"
  },
  {
    "text": "buffer is either 60 seconds or 1 MB of data so data comes into those three",
    "start": "2523690",
    "end": "2529990"
  },
  {
    "text": "destinations only if either of that happens either 60 seconds or 1 MB of data right",
    "start": "2529990",
    "end": "2536920"
  },
  {
    "text": "so hopefully this difference is clear in terms of what streams can do and what firehose can do again depending upon",
    "start": "2536920",
    "end": "2542680"
  },
  {
    "text": "your use cases you can choose either of those choices the third service that is",
    "start": "2542680",
    "end": "2553240"
  },
  {
    "start": "2543000",
    "end": "2543000"
  },
  {
    "text": "offered under cleanest platform is kinases analytics so let's say you are",
    "start": "2553240",
    "end": "2558850"
  },
  {
    "text": "ingesting data into either stream or fire rose from all those different data",
    "start": "2558850",
    "end": "2564010"
  },
  {
    "text": "sources and you are again having multiple consumer applications processing data out of your stream what",
    "start": "2564010",
    "end": "2571540"
  },
  {
    "text": "you can do with mrs. analytics is go to kinases analytics and say here is my",
    "start": "2571540",
    "end": "2577240"
  },
  {
    "text": "streaming source my streaming source is either Kinesis stream or kinases firehose that's where it is coming in",
    "start": "2577240",
    "end": "2583810"
  },
  {
    "text": "and run this piece of sequel against",
    "start": "2583810",
    "end": "2588820"
  },
  {
    "text": "that streaming source so kinases analytics takes you takes a sequel input",
    "start": "2588820",
    "end": "2594070"
  },
  {
    "text": "from you and it can hook up to any of those two sources and run the sequel",
    "start": "2594070",
    "end": "2599410"
  },
  {
    "text": "against that streaming source and the output of that particular sequel that",
    "start": "2599410",
    "end": "2604600"
  },
  {
    "text": "you give can be pushed again into a stream or FiOS right so let's say you",
    "start": "2604600",
    "end": "2610750"
  },
  {
    "text": "got data coming into your stream or FiOS and you can't write your own",
    "start": "2610750",
    "end": "2616720"
  },
  {
    "text": "applications it takes a lot of time for me to write my own application but I can write sequel pretty quickly right so I",
    "start": "2616720",
    "end": "2623350"
  },
  {
    "text": "can go to give mrs. analytics and give my sequel to Kinesis analytics and that",
    "start": "2623350",
    "end": "2629260"
  },
  {
    "text": "sequel is run against the stream and the output of that sequel can be again",
    "start": "2629260",
    "end": "2635920"
  },
  {
    "start": "2633000",
    "end": "2633000"
  },
  {
    "text": "further pushed into another stream or a firehose here is a simple example so",
    "start": "2635920",
    "end": "2641830"
  },
  {
    "text": "reinvent is actually live now it's happening this entire week in in Vegas and lots of people are tweeting about",
    "start": "2641830",
    "end": "2648700"
  },
  {
    "text": "Tremont so let's say you are in you are interested in finding about Greenland as",
    "start": "2648700",
    "end": "2654280"
  },
  {
    "text": "a hashtag right so what you can do is let's say you are ingesting all of the",
    "start": "2654280",
    "end": "2659650"
  },
  {
    "text": "tooter firehose data into your kinases stream or kinases firehose you can go to",
    "start": "2659650",
    "end": "2665160"
  },
  {
    "text": "kinases analytics and say I'm interested only on reading on hashtag",
    "start": "2665160",
    "end": "2670550"
  },
  {
    "text": "give that particular query to Kinesis analytics and that query gets run",
    "start": "2670550",
    "end": "2675740"
  },
  {
    "text": "against the streaming beta and the output of the query can be again sent into your another stream or another",
    "start": "2675740",
    "end": "2682550"
  },
  {
    "text": "firehose the key thing to remember here is that that sequel is being run on",
    "start": "2682550",
    "end": "2688660"
  },
  {
    "text": "streaming data right so as data arrives into your streaming source that sequel",
    "start": "2688660",
    "end": "2694880"
  },
  {
    "text": "is run against that which means that you can do things like sliding-window",
    "start": "2694880",
    "end": "2699980"
  },
  {
    "text": "analysis or tumbling window analysis those kind of analysis against streaming",
    "start": "2699980",
    "end": "2705620"
  },
  {
    "text": "data right so hopefully that gave you",
    "start": "2705620",
    "end": "2711680"
  },
  {
    "text": "some kind of a context in terms of what you can use to build your streaming use",
    "start": "2711680",
    "end": "2717410"
  },
  {
    "text": "cases you can use either kinases streams which can allow you to ingest in near-real-time",
    "start": "2717410",
    "end": "2722930"
  },
  {
    "text": "and process data in near real-time or you can use firehose if you want to let's say Lant data in period of three",
    "start": "2722930",
    "end": "2729950"
  },
  {
    "text": "destinations s3 redshifted elasticsearch service and finally if you want to run a",
    "start": "2729950",
    "end": "2735140"
  },
  {
    "text": "sequel on streaming data kinases analytics can do all of that right so",
    "start": "2735140",
    "end": "2740270"
  },
  {
    "text": "these three services helps you ingest data integrator Lake and on top of that",
    "start": "2740270",
    "end": "2745580"
  },
  {
    "text": "you can use any of the other partner tools or a table services changes from your OLTP systems or from your other",
    "start": "2745580",
    "end": "2752180"
  },
  {
    "text": "systems on premise so the next aspect is ok we are able to ingest data into the",
    "start": "2752180",
    "end": "2758870"
  },
  {
    "text": "data Lake how do I know store the data right because streams is not going to give you a persistent storage it's going",
    "start": "2758870",
    "end": "2765770"
  },
  {
    "text": "to expire after seven days anyway right so how do i durably store data so Amazon",
    "start": "2765770",
    "end": "2771890"
  },
  {
    "text": "s3 is a service which allows you to durably store data it's a service that",
    "start": "2771890",
    "end": "2777260"
  },
  {
    "text": "allows you to store anything it can store any binary object we call that as an object storage it's extremely",
    "start": "2777260",
    "end": "2784880"
  },
  {
    "text": "scalable it can scale to any number of objects that you can store within the",
    "start": "2784880",
    "end": "2790070"
  },
  {
    "text": "service it has virtually unlimited capacity and it offers eleven lines of",
    "start": "2790070",
    "end": "2796940"
  },
  {
    "text": "durability for your data so Amazon s3 is a region level service so it is multi",
    "start": "2796940",
    "end": "2803720"
  },
  {
    "text": "available zone by default and whatever data that you store in Amazon s3 whether it's a a",
    "start": "2803720",
    "end": "2810290"
  },
  {
    "text": "single byte or 1gb or terabytes of data all of them get eleven nines of",
    "start": "2810290",
    "end": "2817420"
  },
  {
    "text": "durability right so ninety-nine point nine nine after that extremely extremely",
    "start": "2817420",
    "end": "2824050"
  },
  {
    "text": "durable you can be rest assured that your data is durable when you store it",
    "start": "2824050",
    "end": "2829790"
  },
  {
    "start": "2828000",
    "end": "2828000"
  },
  {
    "text": "in s3 right and that's an SLA that the service offers within s3 you get",
    "start": "2829790",
    "end": "2836170"
  },
  {
    "text": "multiple storage options standard is a default you also get standard infrequent",
    "start": "2836170",
    "end": "2842840"
  },
  {
    "text": "access and also glazier infrequent access is great for data that you",
    "start": "2842840",
    "end": "2848150"
  },
  {
    "text": "typically not access frequently but maybe once in a while so think of these as data like your log files maybe you're",
    "start": "2848150",
    "end": "2855740"
  },
  {
    "text": "collecting log files and you would use log files maybe once in a week or once",
    "start": "2855740",
    "end": "2861740"
  },
  {
    "text": "in a month when somebody wants to look at the log file for some troubleshooting use cases right glazier is really cold",
    "start": "2861740",
    "end": "2868460"
  },
  {
    "text": "storage where you want to keep data for your archival purposes or for compliance",
    "start": "2868460",
    "end": "2874370"
  },
  {
    "text": "purposes maybe a couple of years on the line if there is a compliance query or some query from some team you won't dig",
    "start": "2874370",
    "end": "2881630"
  },
  {
    "text": "up your archive and you know give some data right that's glacier now this suits",
    "start": "2881630",
    "end": "2886730"
  },
  {
    "text": "very well for your data Lake use cases right because data typically arrives hot",
    "start": "2886730",
    "end": "2891920"
  },
  {
    "text": "and then it starts becoming little warm and eventually it becomes cold right so",
    "start": "2891920",
    "end": "2897470"
  },
  {
    "text": "you're collecting data and immediately as you collect data you start using that to answer some questions and maybe it",
    "start": "2897470",
    "end": "2904820"
  },
  {
    "text": "starts fearing and starts becoming slightly warm maybe after a week or after a month nobody is looking at that",
    "start": "2904820",
    "end": "2910700"
  },
  {
    "text": "particular data right and then really after 90 days or six months that data is",
    "start": "2910700",
    "end": "2916730"
  },
  {
    "text": "not even used right this happens in most of our data like use cases and it makes",
    "start": "2916730",
    "end": "2922250"
  },
  {
    "text": "perfectly sense to use these three different storage classes to to leverage",
    "start": "2922250",
    "end": "2928970"
  },
  {
    "start": "2923000",
    "end": "2923000"
  },
  {
    "text": "those use cases right so what can you do with this three different storage classes so you can keep your hot data in",
    "start": "2928970",
    "end": "2936710"
  },
  {
    "text": "your s3 standard and start tearing that to a3i F for your warm data and use glacier for your",
    "start": "2936710",
    "end": "2944570"
  },
  {
    "text": "really cold archival and compliance requirements both s3 and s3 ie they",
    "start": "2944570",
    "end": "2951800"
  },
  {
    "text": "operate in a common namespace so when you go to s3 and create a bucket you get a unique dns endpoint that you can use",
    "start": "2951800",
    "end": "2958910"
  },
  {
    "text": "to a query install data into that bucket and then you get a number of options",
    "start": "2958910",
    "end": "2966790"
  },
  {
    "text": "whether in terms of security or even notifications versioning where you can",
    "start": "2966790",
    "end": "2973070"
  },
  {
    "text": "keep multiple copies or even cross region replication you can do all of that across both of those storage",
    "start": "2973070",
    "end": "2980390"
  },
  {
    "text": "classes and then you can use lifecycle policies to automatically clear data",
    "start": "2980390",
    "end": "2986450"
  },
  {
    "text": "across these different storage classes so you get your data can arrive in standard and you can set a policy at the",
    "start": "2986450",
    "end": "2992510"
  },
  {
    "text": "bucket level saying that after 30 days movie to infrequent access now for 90",
    "start": "2992510",
    "end": "2997700"
  },
  {
    "text": "days movie to Glacier that's a one-time configuration that you do and AWS takes",
    "start": "2997700",
    "end": "3003040"
  },
  {
    "text": "care of automatic transitioning across all those three difference for its classes you get the same level of",
    "start": "3003040",
    "end": "3010030"
  },
  {
    "text": "durability for both s3 and s3 ia s3 offers you four nines of availability as",
    "start": "3010030",
    "end": "3016720"
  },
  {
    "text": "an SLA three nines for ia and extremely scalable and virtually you get access to",
    "start": "3016720",
    "end": "3023380"
  },
  {
    "text": "unlimited capacity right so s3 becomes a",
    "start": "3023380",
    "end": "3028810"
  },
  {
    "text": "great choice for your storage requirements of your data Lake and lots and lots of customers are building data",
    "start": "3028810",
    "end": "3035980"
  },
  {
    "text": "Lake on top of s3 right and we will see when we get to the processing side but you can use different processing engines",
    "start": "3035980",
    "end": "3041980"
  },
  {
    "text": "that can directly query data that is sitting out of history all right so we",
    "start": "3041980",
    "end": "3049510"
  },
  {
    "text": "looked at the ingestion and storage bit the next component is the catalog and search so let's say you are ingesting",
    "start": "3049510",
    "end": "3054820"
  },
  {
    "text": "lots of data into into s3 you are collecting all the data it's making it's getting so durably in s3 how do people",
    "start": "3054820",
    "end": "3062680"
  },
  {
    "text": "in your organization discover what is there in your data Lake that's what we're going to discuss next",
    "start": "3062680",
    "end": "3068859"
  },
  {
    "text": "right so why do what do you have to build a data catalog right so you're collecting",
    "start": "3068859",
    "end": "3076320"
  },
  {
    "start": "3072000",
    "end": "3072000"
  },
  {
    "text": "lots and lots of data how do people know what is there in my data Lake right a",
    "start": "3076320",
    "end": "3081870"
  },
  {
    "text": "data catalog allows you to extract metadata about your data and expose that",
    "start": "3081870",
    "end": "3088200"
  },
  {
    "text": "to users so that they can discover about what lives in my data lake all right so",
    "start": "3088200",
    "end": "3095040"
  },
  {
    "text": "probably you would want to expose things like in what they of formats am i keeping data is it JSON is it CSV is",
    "start": "3095040",
    "end": "3102630"
  },
  {
    "text": "that park' is it or C is it compressed right so all this information about your",
    "start": "3102630",
    "end": "3108390"
  },
  {
    "text": "data needs to be exposed as metadata for your users right you also have to think",
    "start": "3108390",
    "end": "3116040"
  },
  {
    "text": "about there our classification requirements is it sensitive data is a classified data so should i expose that",
    "start": "3116040",
    "end": "3122130"
  },
  {
    "text": "as a metadata right and maybe you want to add additional tags maybe people have",
    "start": "3122130",
    "end": "3130500"
  },
  {
    "text": "to search based on a tag right maybe they want to search based on some tweets",
    "start": "3130500",
    "end": "3135900"
  },
  {
    "text": "are based on some specific departments in your organization so you have to enable data discovery through some",
    "start": "3135900",
    "end": "3142980"
  },
  {
    "text": "search mechanisms so the people can discover what is there in a lake and then go and extract the data and process",
    "start": "3142980",
    "end": "3149880"
  },
  {
    "text": "the data right so let's say this machine learning team is now set up and they want to look at all the data that",
    "start": "3149880",
    "end": "3156840"
  },
  {
    "text": "belongs to a particular department in your organization and build a model how",
    "start": "3156840",
    "end": "3162750"
  },
  {
    "text": "do they know in which bucket or which folder in may s3 that this data lives so",
    "start": "3162750",
    "end": "3168030"
  },
  {
    "text": "that's the metadata that you want to expose right and maybe you want to build",
    "start": "3168030",
    "end": "3173430"
  },
  {
    "text": "an API layer on top of it right because this probably is going to get exposed to different teams in your organization",
    "start": "3173430",
    "end": "3179790"
  },
  {
    "text": "so you probably have to build different API a pas there are those people to query the API and query your metal",
    "start": "3179790",
    "end": "3187170"
  },
  {
    "text": "metadata right in certain use cases we have seen customers even want to expose their API to external parties where they",
    "start": "3187170",
    "end": "3193860"
  },
  {
    "text": "build that as a service and they have a business model that is exposed to even",
    "start": "3193860",
    "end": "3201240"
  },
  {
    "start": "3197000",
    "end": "3197000"
  },
  {
    "text": "third party developers right so that's the requirement for building a",
    "start": "3201240",
    "end": "3207299"
  },
  {
    "text": "data catalog so now how do how do you go and build a data catalog so let's say",
    "start": "3207299",
    "end": "3212579"
  },
  {
    "text": "you are collecting data into s3 all of that is coming to an s3 bucket s3 like",
    "start": "3212579",
    "end": "3218099"
  },
  {
    "text": "we discussed allows you to do what we call as even notification so whenever you push a new data into an s3 bucket a",
    "start": "3218099",
    "end": "3225540"
  },
  {
    "text": "new file a new object and even can be triggered and that event can trigger",
    "start": "3225540",
    "end": "3231510"
  },
  {
    "text": "what we call as a lambda function so a WS lambda is a service that allows you to give us a piece of code and we would",
    "start": "3231510",
    "end": "3238980"
  },
  {
    "text": "run the piece of code in some infrastructure that we manage and that piece of code gets executed in response",
    "start": "3238980",
    "end": "3245609"
  },
  {
    "text": "to events in your pipeline so if s 3 generates an event that piece of code",
    "start": "3245609",
    "end": "3251010"
  },
  {
    "text": "can be automatically invoked right in that piece of code you can have your own logic to extract metadata right so now",
    "start": "3251010",
    "end": "3259109"
  },
  {
    "text": "you can look at your object and say this the object is stored in JSON it's compressed it's probably having this",
    "start": "3259109",
    "end": "3266400"
  },
  {
    "text": "information on all that metadata can be extracted out of the object and once you",
    "start": "3266400",
    "end": "3271770"
  },
  {
    "text": "extract it you can probably store it in a no sequel database so Amazon DynamoDB is a managed",
    "start": "3271770",
    "end": "3277440"
  },
  {
    "text": "no sequel database that we offer and that can be used to act as your",
    "start": "3277440",
    "end": "3282859"
  },
  {
    "text": "persistent storage for your metadata and once data comes in there again another",
    "start": "3282859",
    "end": "3288030"
  },
  {
    "text": "lambda function can pick up that data and push into an elastic search service",
    "start": "3288030",
    "end": "3293130"
  },
  {
    "text": "which can be used by your different teams to query data and search what is",
    "start": "3293130",
    "end": "3298470"
  },
  {
    "text": "there in my catalog right so you can build a pipeline like this pretty quickly and this is basically a do to",
    "start": "3298470",
    "end": "3305490"
  },
  {
    "text": "yourself pipeline if you want use these different combinations to build your own data catalog but what we saw was lot of",
    "start": "3305490",
    "end": "3314130"
  },
  {
    "text": "customers were building this kind of or do it yourself own catalog by tying up",
    "start": "3314130",
    "end": "3319980"
  },
  {
    "text": "all the different services and they came back to us and said can you build can",
    "start": "3319980",
    "end": "3326160"
  },
  {
    "text": "you manage this on B of us right now why can't AWS offer a service to do this on",
    "start": "3326160",
    "end": "3331740"
  },
  {
    "text": "customers behalf that's where AWS glue offers a data catalog AWS clue is a",
    "start": "3331740",
    "end": "3341630"
  },
  {
    "text": "managed transform engine you can use this to use this service to basically run your ETL pipelines right but one of",
    "start": "3341630",
    "end": "3349349"
  },
  {
    "text": "the key aspects of AWS glue is the data catalog aspect right and we'll talk",
    "start": "3349349",
    "end": "3354960"
  },
  {
    "text": "about how you can use the glue catalog to enable your metadata discovery and",
    "start": "3354960",
    "end": "3361140"
  },
  {
    "text": "search right the glue service is built on Apache spark so whenever you define your ETL jobs within glue it runs on the",
    "start": "3361140",
    "end": "3368910"
  },
  {
    "text": "apache spark infrastructure that is completely managed by AWS on your behalf",
    "start": "3368910",
    "end": "3374910"
  },
  {
    "start": "3370000",
    "end": "3370000"
  },
  {
    "text": "right so at a high level what arab Lewis glue offers is three main components one",
    "start": "3374910",
    "end": "3381210"
  },
  {
    "text": "is the data catalog where glue maintains a hive compatible metadata store that",
    "start": "3381210",
    "end": "3390540"
  },
  {
    "text": "can be used as a central catalog repository right it maintains a hive",
    "start": "3390540",
    "end": "3396900"
  },
  {
    "text": "compatible meta store that automatically gets populated and I'll talk about how the catalog gets populated but the key",
    "start": "3396900",
    "end": "3404010"
  },
  {
    "text": "thing is other processing engines whether if let's say your own spark cluster or your own presto cluster all",
    "start": "3404010",
    "end": "3410400"
  },
  {
    "text": "of them can easily connect to this catalog and start running queries",
    "start": "3410400",
    "end": "3415920"
  },
  {
    "text": "against this catalog right the catalog component has built-in crawlers that you",
    "start": "3415920",
    "end": "3422730"
  },
  {
    "text": "can define which can automatically crawl various data sources and extract",
    "start": "3422730",
    "end": "3428250"
  },
  {
    "text": "metadata from those data sources and populate your catalog for example there",
    "start": "3428250",
    "end": "3434190"
  },
  {
    "text": "are built-in catalogs for s3 where it can automatically crawl your s3 buckets and whenever there are new",
    "start": "3434190",
    "end": "3441089"
  },
  {
    "text": "objects available in the rs3 bucket it can extract metadata about those new objects and populate your catalog right",
    "start": "3441089",
    "end": "3450680"
  },
  {
    "text": "again similarly it can crawl your databases you can crawl your data",
    "start": "3450680",
    "end": "3455760"
  },
  {
    "text": "warehouses and we'll keep adding more and more crawlers to this you can schedule the crawler to run on a",
    "start": "3455760",
    "end": "3462599"
  },
  {
    "text": "specific frequency or based on a cron expression or based on an event right so",
    "start": "3462599",
    "end": "3468420"
  },
  {
    "text": "all the flexibility you get it and we would automatically crawl your data sources and extract metadata",
    "start": "3468420",
    "end": "3474380"
  },
  {
    "text": "and then if you want to transform the",
    "start": "3474380",
    "end": "3479520"
  },
  {
    "text": "data there is a job authoring piece where you can go to the glue service and",
    "start": "3479520",
    "end": "3485660"
  },
  {
    "text": "you can define what is your ETL pipeline visually you can say here is my source",
    "start": "3485660",
    "end": "3492000"
  },
  {
    "text": "this is where my data loose and I want to extract from this particular source",
    "start": "3492000",
    "end": "3497190"
  },
  {
    "text": "and do a bit of transformation and push that to my destination you can define that and glue can automatically generate",
    "start": "3497190",
    "end": "3505490"
  },
  {
    "text": "code the code that it generates is a pythons code and you can further",
    "start": "3505490",
    "end": "3511650"
  },
  {
    "text": "customize that Python code if you would want to you can bring additional Python",
    "start": "3511650",
    "end": "3516870"
  },
  {
    "text": "libraries if you'd want to and then it packages all of that and once you are",
    "start": "3516870",
    "end": "3521880"
  },
  {
    "text": "ready glucan again executes that job on",
    "start": "3521880",
    "end": "3527010"
  },
  {
    "text": "an infrastructure that manages for you so you can say that okay this is my ETL",
    "start": "3527010",
    "end": "3533490"
  },
  {
    "text": "job and this is my source this is my destination this is my piece of code and I want this little job to run on a",
    "start": "3533490",
    "end": "3541170"
  },
  {
    "text": "specified interval or of specified frequency or based on a cron expression you can do that and glue will run that",
    "start": "3541170",
    "end": "3549210"
  },
  {
    "text": "job on an infrastructure that glue managers that is basically a spark",
    "start": "3549210",
    "end": "3554220"
  },
  {
    "text": "cluster that glue manages on your behalf and your job gets run into the",
    "start": "3554220",
    "end": "3559800"
  },
  {
    "text": "infrastructure you are only charged whenever your job runs if your job does",
    "start": "3559800",
    "end": "3565050"
  },
  {
    "text": "not run your job you are not being charged it's completely serverless from your perspective all the infrastructure",
    "start": "3565050",
    "end": "3570750"
  },
  {
    "text": "management is done by the glue service for you right so these are the three",
    "start": "3570750",
    "end": "3576300"
  },
  {
    "text": "components at glue offers but from a cataloging perspective glue now exposes",
    "start": "3576300",
    "end": "3581820"
  },
  {
    "text": "a centralized catalog that lose outside of your storage and compute it's",
    "start": "3581820",
    "end": "3587430"
  },
  {
    "text": "separate it lives outside so that any processing engine can hook up to this",
    "start": "3587430",
    "end": "3593160"
  },
  {
    "text": "catalog and start processing your data right so this is a third perspective you",
    "start": "3593160",
    "end": "3598860"
  },
  {
    "text": "spoke about the storage and compute being decoupled this is a third perspective",
    "start": "3598860",
    "end": "3603920"
  },
  {
    "text": "your metadata is also decoupled and your different processing engines can hook up",
    "start": "3603920",
    "end": "3609920"
  },
  {
    "text": "to this metadata and find what is there in a catalog and use that to process my",
    "start": "3609920",
    "end": "3614960"
  },
  {
    "text": "data all right so now on top of the",
    "start": "3614960",
    "end": "3621260"
  },
  {
    "text": "metadata if you want to let's say expose an API write that your internal teams",
    "start": "3621260",
    "end": "3626690"
  },
  {
    "text": "and external teams can use to search an extract information about your metadata",
    "start": "3626690",
    "end": "3632720"
  },
  {
    "text": "you can probably build an API on top of it again AWS provides a service called",
    "start": "3632720",
    "end": "3638840"
  },
  {
    "text": "as a PA gateway which allows you to quickly define API s-- so you can go to",
    "start": "3638840",
    "end": "3643880"
  },
  {
    "text": "the service and start clearing api is for your metadata and catalog and use",
    "start": "3643880",
    "end": "3650150"
  },
  {
    "text": "that API to build a website which your users can go to to discover what is",
    "start": "3650150",
    "end": "3656240"
  },
  {
    "text": "there in my data link and then start extracting data or to process the data right so what API gateway allows you to",
    "start": "3656240",
    "end": "3662900"
  },
  {
    "text": "do is hook up different backends so your backends can be a lambda function or",
    "start": "3662900",
    "end": "3668450"
  },
  {
    "text": "your own ec2 value got an app deployed or any public endpoint alright so these",
    "start": "3668450",
    "end": "3674900"
  },
  {
    "text": "functions can in turn hook up to your elasticsearch or to glue which is where",
    "start": "3674900",
    "end": "3681290"
  },
  {
    "text": "your actual catalog lives and whenever user submits a query the API gateway can",
    "start": "3681290",
    "end": "3687800"
  },
  {
    "text": "call you a lambda function or your own easy to endpoint which further submits the query to your own elasticsearch",
    "start": "3687800",
    "end": "3694670"
  },
  {
    "text": "cluster or to glue to basically query your catalog right and then your users",
    "start": "3694670",
    "end": "3702170"
  },
  {
    "text": "they can use an endpoint which is behind a CDN so CloudFront is our CDN service",
    "start": "3702170",
    "end": "3709400"
  },
  {
    "text": "so your users can connect to that end point and then query your api gateway",
    "start": "3709400",
    "end": "3715750"
  },
  {
    "text": "api gateway also has a cache built in so any of the frequently used responses can",
    "start": "3715750",
    "end": "3723410"
  },
  {
    "text": "be automatically cached within the API gateway itself and then you also get monitoring through the cloud watch",
    "start": "3723410",
    "end": "3730370"
  },
  {
    "text": "service right so if you are interested in putting on API on top of your catalog you can definitely look at API gateway",
    "start": "3730370",
    "end": "3737420"
  },
  {
    "text": "as a choice there so that kind of sums up the catalog on the search component",
    "start": "3737420",
    "end": "3744200"
  },
  {
    "text": "of your data Lake right so now we got data coming in through kinases and other",
    "start": "3744200",
    "end": "3749809"
  },
  {
    "text": "various tools it gets ingested into your data Lake and then you use s3 to durably",
    "start": "3749809",
    "end": "3756589"
  },
  {
    "text": "store your data and then you can use your own combination of you know lambda",
    "start": "3756589",
    "end": "3763339"
  },
  {
    "text": "DynamoDB elasticsearch or glue to basically extract metadata about your",
    "start": "3763339",
    "end": "3769400"
  },
  {
    "text": "data and then populate a catalog that lives outside of your processing engines",
    "start": "3769400",
    "end": "3774619"
  },
  {
    "text": "so that anybody any team can now come and discover data and start processing",
    "start": "3774619",
    "end": "3781849"
  },
  {
    "text": "the data to answer questions right so let's move on to the next component",
    "start": "3781849",
    "end": "3787549"
  },
  {
    "text": "which is the actual processing and serving layer right how you can use",
    "start": "3787549",
    "end": "3793490"
  },
  {
    "text": "different processing engines to process your data so before I jump on to that",
    "start": "3793490",
    "end": "3798770"
  },
  {
    "text": "again quickly wanted to remind that when it comes to processing there are going to be these different personas who come",
    "start": "3798770",
    "end": "3807290"
  },
  {
    "text": "from different backgrounds and different tools excite them right if it's a",
    "start": "3807290",
    "end": "3813589"
  },
  {
    "text": "developer maybe they want to write Python code or a Scala code or a Java program if it's a business user they",
    "start": "3813589",
    "end": "3819920"
  },
  {
    "text": "want to go to a dashboard if it's a data scientist the data analyst they want to write a sequel right so these different",
    "start": "3819920",
    "end": "3826510"
  },
  {
    "text": "personas they would want to use different tools to process your data and that's going to be a key characteristics",
    "start": "3826510",
    "end": "3834260"
  },
  {
    "text": "of your data Lake right so giving flexibility for different teams to use different tools so from that perspective",
    "start": "3834260",
    "end": "3840910"
  },
  {
    "text": "again AWS offers different choices depending upon who you are right and",
    "start": "3840910",
    "end": "3847819"
  },
  {
    "text": "we'll talk about these five different components in terms of the processing and serving options starting out with",
    "start": "3847819",
    "end": "3854540"
  },
  {
    "text": "Amazon EMR so Amazon EMR is short for elastic MapReduce it's a managed Hadoop",
    "start": "3854540",
    "end": "3861559"
  },
  {
    "text": "as a service where you can go to the service and quickly spin up a Hadoop cluster in a matter of minutes and use",
    "start": "3861559",
    "end": "3868609"
  },
  {
    "text": "that to process vast amount of right it allows you to run at run your",
    "start": "3868609",
    "end": "3876200"
  },
  {
    "text": "Hadoop clusters pretty quickly at the lowest cost possible we'll talk about that and it can directly connect to your",
    "start": "3876200",
    "end": "3885860"
  },
  {
    "text": "s3 bucket and start processing data that is living in your s3 bucket and we again",
    "start": "3885860",
    "end": "3891290"
  },
  {
    "text": "dive deep and understand how exactly you can do that so if you are up if you are",
    "start": "3891290",
    "end": "3896300"
  },
  {
    "text": "a person who have deployed a Hadoop infrastructure in the past you may be able to appreciate this let's say I want to bring up how to cluster so what I got",
    "start": "3896300",
    "end": "3903020"
  },
  {
    "text": "to do is I first go and spin up infrastructure bring up let's say a few word machines and then get my HDFS up",
    "start": "3903020",
    "end": "3911600"
  },
  {
    "text": "and running and then get my MapReduce layer configured get my entire Hadoop",
    "start": "3911600",
    "end": "3916970"
  },
  {
    "text": "basically configured and then once my Hadoop is up and running I then go and",
    "start": "3916970",
    "end": "3922340"
  },
  {
    "text": "install all my applications it can be my hive it can be my spark it can be my",
    "start": "3922340",
    "end": "3928850"
  },
  {
    "text": "presto all the applications that you want to install you get all that installed in that infrastructure and",
    "start": "3928850",
    "end": "3934780"
  },
  {
    "text": "once all of that is done you now start running your first job right now that's",
    "start": "3934780",
    "end": "3942230"
  },
  {
    "text": "that's what it takes for any hadoop infrastructure to come up right now",
    "start": "3942230",
    "end": "3947450"
  },
  {
    "text": "amazon EMR can take care of all of that aspect where you can go to the service and say I want a 100 node cluster of",
    "start": "3947450",
    "end": "3955610"
  },
  {
    "text": "this particular configuration I want these applications installed on it these are the versions of application",
    "start": "3955610",
    "end": "3961670"
  },
  {
    "text": "that I want to be installed it can do all of the provisioning aspect get your Hadoop install get all the other",
    "start": "3961670",
    "end": "3967370"
  },
  {
    "text": "publications installed on it and you can quickly go and run your jobs on it and all of that can happen in a matter of",
    "start": "3967370",
    "end": "3974090"
  },
  {
    "text": "minutes within about 10 minutes you can have the entire cluster up and running and you can start running your jobs on",
    "start": "3974090",
    "end": "3980150"
  },
  {
    "text": "it on top of that it also exposes something called as MRFs which is EMRs",
    "start": "3980150",
    "end": "3987290"
  },
  {
    "text": "own implementation of HDFS that exposes s3 as an HDFS file system for your",
    "start": "3987290",
    "end": "3994430"
  },
  {
    "text": "applications right so you don't have to make any changes in your applications you can run the same spark or presto",
    "start": "3994430",
    "end": "4001660"
  },
  {
    "text": "query you can simply point to to an s3 it and your application still thinks",
    "start": "4001660",
    "end": "4007009"
  },
  {
    "text": "that they are talking to s3 but iam orifice exposes that as a hdfs right so",
    "start": "4007009",
    "end": "4014089"
  },
  {
    "text": "with this EMR decouples your compute lair and storage layer where your entire",
    "start": "4014089",
    "end": "4019940"
  },
  {
    "text": "computer layer can come on demand run a job against your s3 and then when a job",
    "start": "4019940",
    "end": "4025549"
  },
  {
    "text": "is complete the entire computer layer can get terminated right this is a very very",
    "start": "4025549",
    "end": "4031190"
  },
  {
    "text": "powerful way of looking at compute and storage decoupled where you get lots of flexibility in terms of independently",
    "start": "4031190",
    "end": "4038839"
  },
  {
    "text": "scaling your compute on storage and potentially having even multiple compute",
    "start": "4038839",
    "end": "4044000"
  },
  {
    "text": "Liars directly querying your storage layer and then EMR gives you access to",
    "start": "4044000",
    "end": "4052250"
  },
  {
    "text": "all the popular applications that you have been using so EMR does not let me",
    "start": "4052250",
    "end": "4059329"
  },
  {
    "text": "take a step back yema works with the apache distribution so you get all the",
    "start": "4059329",
    "end": "4064579"
  },
  {
    "text": "popular applications that is available in the Apache ecosystem right whether it",
    "start": "4064579",
    "end": "4069680"
  },
  {
    "text": "is spark or high or presto or Zeppelin or flink or Phoenix or any popular",
    "start": "4069680",
    "end": "4076970"
  },
  {
    "text": "application that you see in the Hadoop ecosystem EMR can support all of that",
    "start": "4076970",
    "end": "4082750"
  },
  {
    "text": "and more importantly you can have multiple clusters",
    "start": "4082750",
    "end": "4088130"
  },
  {
    "text": "spinning up through EMR querying data that is sitting out of history you can have long-running cluster where",
    "start": "4088130",
    "end": "4094549"
  },
  {
    "text": "interactive queries are being submitted through maybe presto or Spock sequel or you can have a transient cluster which",
    "start": "4094549",
    "end": "4100790"
  },
  {
    "text": "comes up once in a day or once in few hours does some batch jobs and then once a job is complete it gets terminated or",
    "start": "4100790",
    "end": "4107960"
  },
  {
    "text": "maybe you can have different workload specific clusters if let's say that machine learning team comes to you and",
    "start": "4107960",
    "end": "4113389"
  },
  {
    "text": "say I want to crunch my entire years worth of data don't touch your operational pipeline your operational",
    "start": "4113389",
    "end": "4119719"
  },
  {
    "text": "cluster spin up a new cluster and that team can use that to crunch all the data",
    "start": "4119719",
    "end": "4124940"
  },
  {
    "text": "right so that's kind of flexibility you get it and if your metadata is external",
    "start": "4124940",
    "end": "4130790"
  },
  {
    "text": "if you're using glue catalog or even if you are using hive can store the metadata in an external database right",
    "start": "4130790",
    "end": "4136488"
  },
  {
    "text": "so you meditate also loose of a cluster giving you the flexibility so here is a quick comparison on running",
    "start": "4136489",
    "end": "4144109"
  },
  {
    "text": "Hadoop on ec2 versus EMR but with Hadoop on ec2 you are going to spin up the",
    "start": "4144109",
    "end": "4149750"
  },
  {
    "text": "infrastructure upfront and set up for three times the replication for your",
    "start": "4149750",
    "end": "4155210"
  },
  {
    "text": "HDFS right and it's going to be a single availability zone set up and you will",
    "start": "4155210",
    "end": "4161359"
  },
  {
    "text": "pay for storage units upfront and your cluster has to be 24 bar 7 operational",
    "start": "4161359",
    "end": "4168650"
  },
  {
    "text": "right you can't turn off the cluster because data lives within the cluster now versus EMR you are keeping data in",
    "start": "4168650",
    "end": "4175548"
  },
  {
    "text": "s3 and you are not provisioning three times the three times capacity for a",
    "start": "4175549",
    "end": "4182540"
  },
  {
    "text": "HDFS layer because you are now going to leverage s3 zone multi easy love earnings or durability and your computer",
    "start": "4182540",
    "end": "4189798"
  },
  {
    "text": "can directly access s3 right you are you can only pay for however long your",
    "start": "4189799",
    "end": "4195949"
  },
  {
    "text": "cluster runs if your cluster runs for one hour you only pay for one hour right and remember ec2 has gone to per second",
    "start": "4195949",
    "end": "4203390"
  },
  {
    "text": "billing which means that anytime that you stop the cluster your metering",
    "start": "4203390",
    "end": "4209480"
  },
  {
    "text": "immediately stops to that particular second right so that's going to let you save a lot of money in terms of running",
    "start": "4209480",
    "end": "4216230"
  },
  {
    "text": "your cluster on EMR and on top of that you can use EMR to elastically scale up",
    "start": "4216230",
    "end": "4222530"
  },
  {
    "text": "and scale down your cluster and even Auto scale flustered right your nodes in",
    "start": "4222530",
    "end": "4228080"
  },
  {
    "text": "the cluster the slave nodes in the cluster can be automatically scaled based on auto scaling rules and",
    "start": "4228080",
    "end": "4234110"
  },
  {
    "text": "dynamically expand your computing requirements on the cluster",
    "start": "4234110",
    "end": "4239800"
  },
  {
    "text": "in addition EMR also integrates very well with spot market so spot market is",
    "start": "4239980",
    "end": "4246110"
  },
  {
    "text": "a different market in ec2 where any excess capacity that we have in our",
    "start": "4246110",
    "end": "4252340"
  },
  {
    "text": "availability zones we make it available as part of the spot market and anybody",
    "start": "4252340",
    "end": "4257420"
  },
  {
    "text": "any customer can go and bid for capacity and if you if your bid wins you get",
    "start": "4257420",
    "end": "4264409"
  },
  {
    "text": "access to that particular ec2 instance right the main advantage of operating in",
    "start": "4264409",
    "end": "4270139"
  },
  {
    "text": "spot market is that the instances available in this part market are available at significantly",
    "start": "4270139",
    "end": "4276830"
  },
  {
    "text": "reduced pricing when compared to the on-demand hourly pricing that you get on",
    "start": "4276830",
    "end": "4282409"
  },
  {
    "text": "easy to write each instance family each instance type in a particular",
    "start": "4282409",
    "end": "4288620"
  },
  {
    "text": "availability zone in a particular region is a spot market and EMR makes it easier",
    "start": "4288620",
    "end": "4294500"
  },
  {
    "text": "to integrate spot instances in your cluster right you can go to M R and say I want these ten instances to be off",
    "start": "4294500",
    "end": "4302179"
  },
  {
    "text": "spot instances and this is the maximum price that I'm willing to pay and II more will take care of doing that",
    "start": "4302179",
    "end": "4308239"
  },
  {
    "text": "bidding for you doing the rebuilding for you all of that is automatically done by EMR on your behalf so it spot market you",
    "start": "4308239",
    "end": "4315830"
  },
  {
    "text": "are looking at lowering lot of your cost in terms of running your EMR and lets",
    "start": "4315830",
    "end": "4321409"
  },
  {
    "text": "you run EMR at the lowest cost possible on AWS in fact many of our customers I",
    "start": "4321409",
    "end": "4327560"
  },
  {
    "text": "mean as large customers use this extensively be it Netflix or FINRA they",
    "start": "4327560",
    "end": "4334070"
  },
  {
    "text": "use extensively spot instances to lower their cost of running EMR lots and lots",
    "start": "4334070",
    "end": "4343159"
  },
  {
    "text": "of customers use EMR this small snapshot of all the customers that are using EMR",
    "start": "4343159",
    "end": "4348219"
  },
  {
    "text": "thousands of thousands of customers are using EMR it's a pretty large service by",
    "start": "4348219",
    "end": "4353570"
  },
  {
    "text": "itself since launch we have launched more than 15 million clusters on the",
    "start": "4353570",
    "end": "4359239"
  },
  {
    "text": "service so pretty pretty big service that the team operates right so that's one of the choices in terms of using EMR",
    "start": "4359239",
    "end": "4367280"
  },
  {
    "text": "for all your Hadoop workloads and the key takeaway is that it allows you to",
    "start": "4367280",
    "end": "4373370"
  },
  {
    "text": "quickly spin up a cluster with all the Hadoop applications that you would want to install on them and more importantly",
    "start": "4373370",
    "end": "4380989"
  },
  {
    "text": "it allows you to decouple compute and storage but you can keep your data in s3 and decouple the compute layer from the",
    "start": "4380989",
    "end": "4389330"
  },
  {
    "text": "storage right I think about transient clusters multiple clusters and workloads specific clusters processing the same",
    "start": "4389330",
    "end": "4396230"
  },
  {
    "text": "data alright so that's the first processing engine the next one is",
    "start": "4396230",
    "end": "4402130"
  },
  {
    "text": "redshift which is basically a relational data warehouse that can be spun up for all your data",
    "start": "4402130",
    "end": "4409610"
  },
  {
    "text": "whereas in requirements right so this fits into those data whereas in use cases where potentially you have lots of",
    "start": "4409610",
    "end": "4416900"
  },
  {
    "text": "dashboards and bi use cases that requires a data warehouse where you are",
    "start": "4416900",
    "end": "4421940"
  },
  {
    "text": "building up schemas which are going to support those complex joins the drill",
    "start": "4421940",
    "end": "4427220"
  },
  {
    "text": "downs that you typically do on your typical bi reporting use cases write",
    "start": "4427220",
    "end": "4432460"
  },
  {
    "text": "redshift fits into that space it gives you a managed fully managed relational data warehouse and follows an MPP",
    "start": "4432460",
    "end": "4440870"
  },
  {
    "text": "architecture and we'll talk about the architecture and you can start at 25",
    "start": "4440870",
    "end": "4447500"
  },
  {
    "text": "cents an hour and your effective per terabyte per year cost can be as low as",
    "start": "4447500",
    "end": "4453280"
  },
  {
    "text": "thousand dollars all right for those of you have managed data warehouses in the on premised world this can be very very",
    "start": "4453280",
    "end": "4460130"
  },
  {
    "text": "compelling from a cost perspective so how does the lecture for architecture looks like so when you spin up when you",
    "start": "4460130",
    "end": "4466550"
  },
  {
    "text": "go to redshift you're going to spin up a cluster the cluster has a leader node which exposes JDBC ODBC endpoint so any",
    "start": "4466550",
    "end": "4474230"
  },
  {
    "text": "of your sequel clients or bi tools can readily connect to the leader node using the JDBC ODBC endpoint and behind the",
    "start": "4474230",
    "end": "4481430"
  },
  {
    "text": "middle node there are these compute nodes and you will choose when you spend up the cluster how many compute nodes",
    "start": "4481430",
    "end": "4486860"
  },
  {
    "text": "you want and those compute nodes are the ones which store data on them and whenever",
    "start": "4486860",
    "end": "4493400"
  },
  {
    "text": "you submit a query the query goes to the leader node the leader node computes the query execution plan generates a C++",
    "start": "4493400",
    "end": "4501320"
  },
  {
    "text": "code that gets sent to all the compute nodes and all the compute nodes work in",
    "start": "4501320",
    "end": "4506630"
  },
  {
    "text": "parallel and execute the query and send back the results to the leader node which does the final aggregation and",
    "start": "4506630",
    "end": "4512600"
  },
  {
    "text": "sends it back to the sequel client right the compute nodes are offered on two platforms SSD platforms called as DC 1",
    "start": "4512600",
    "end": "4520370"
  },
  {
    "text": "DC 2 and magnetic disk based platforms which is DC ds1 and BS 2 right and you",
    "start": "4520370",
    "end": "4527900"
  },
  {
    "text": "can start with a single compute node and go all the way to a hundred compute node and going all the way up to 2 petabytes",
    "start": "4527900",
    "end": "4534980"
  },
  {
    "text": "of storage and 2 petabytes of compressed storage on the cluster so Rick Schiff does a bunch of things so",
    "start": "4534980",
    "end": "4543060"
  },
  {
    "text": "from a ru perspective this is going to be the key aspect when comeback when you talk about very performance right",
    "start": "4543060",
    "end": "4549510"
  },
  {
    "text": "because your query directly translates to ru operations on the cluster so redshift does bunch of things from",
    "start": "4549510",
    "end": "4555870"
  },
  {
    "text": "reducing the i/o first thing is it stores data in a columnar format so most of your data whereas in queries they're",
    "start": "4555870",
    "end": "4562740"
  },
  {
    "text": "not going to scan all the columns in your table they're going to pick only couple of columns and you do lot of",
    "start": "4562740",
    "end": "4569580"
  },
  {
    "text": "group bys and order base on those columns so redshift stores in a columnar format allows you to extract only those",
    "start": "4569580",
    "end": "4576750"
  },
  {
    "text": "specific columns data is stored in compressed format so the two petabyte",
    "start": "4576750",
    "end": "4582030"
  },
  {
    "text": "that you get on the largest cluster is for two para bytes of compressed storage",
    "start": "4582030",
    "end": "4588510"
  },
  {
    "text": "right and then the third thing is it maintains something called as zone maps where if you sort your table redshift in",
    "start": "4588510",
    "end": "4597960"
  },
  {
    "text": "the memory maintains the minimum and maximum value for all the blocks in the",
    "start": "4597960",
    "end": "4603510"
  },
  {
    "text": "underlying storage so that when you fire a query and if the query has a filter",
    "start": "4603510",
    "end": "4609030"
  },
  {
    "text": "Clause it can go to the zone map and infer which blocks to be read and which",
    "start": "4609030",
    "end": "4614160"
  },
  {
    "text": "blocks will be skipped based on their information right so can effectively skip a lot of blocks based on this zone",
    "start": "4614160",
    "end": "4620490"
  },
  {
    "text": "map and the fourth aspect is redshift has direct attached storage which means",
    "start": "4620490",
    "end": "4627030"
  },
  {
    "text": "there is no network in between the computer and storage and it can speed up all your queries and also it has large",
    "start": "4627030",
    "end": "4633840"
  },
  {
    "text": "data block sizes which means that it can do a lot of throughput on a single i/o operation so now the things being done",
    "start": "4633840",
    "end": "4640020"
  },
  {
    "text": "from your perspective to reduce the amount of i/o on the cluster and like I",
    "start": "4640020",
    "end": "4646470"
  },
  {
    "text": "said earlier it follows an MPP architecture which means that every operation in the cluster whether it's",
    "start": "4646470",
    "end": "4651930"
  },
  {
    "text": "your query or data loading or backups or data export every operation in the",
    "start": "4651930",
    "end": "4657870"
  },
  {
    "text": "cluster that you do is distributed to all the nodes in the cluster right you",
    "start": "4657870",
    "end": "4664530"
  },
  {
    "text": "can start simple you can start very very small with a single node in the cluster and as your needs increase you can start",
    "start": "4664530",
    "end": "4672660"
  },
  {
    "text": "from one no to multiple nodes and go all the way up to hundreds of nodes in the cluster right so for example on the right hand",
    "start": "4672660",
    "end": "4679800"
  },
  {
    "text": "side on a single ds2 8xl node you get 16",
    "start": "4679800",
    "end": "4684900"
  },
  {
    "text": "TB of storage in a single node and if you start adding more and more nodes into the cluster you can go all the way",
    "start": "4684900",
    "end": "4690240"
  },
  {
    "text": "up to 128 nodes giving you about 2 petabytes of compressed arrays on the",
    "start": "4690240",
    "end": "4695910"
  },
  {
    "text": "cluster right so you don't have to upfront provision for your growth requirements you can start small and as",
    "start": "4695910",
    "end": "4702690"
  },
  {
    "text": "you grow you can keep adding more nodes into the cluster the service is fully",
    "start": "4702690",
    "end": "4707940"
  },
  {
    "text": "managed which means that backups are fully managed you don't have to manage",
    "start": "4707940",
    "end": "4713430"
  },
  {
    "text": "backups so we keep multiple copies of data within the cluster we also maintain",
    "start": "4713430",
    "end": "4718910"
  },
  {
    "text": "incremental backups to s3 so which means it whenever you load data into the",
    "start": "4718910",
    "end": "4724110"
  },
  {
    "text": "cluster we automatically take backups to s3 so that if you want to let's say",
    "start": "4724110",
    "end": "4729150"
  },
  {
    "text": "restore to the back cluster to a point in time you can go back and restore the cluster it's also fully managed from a",
    "start": "4729150",
    "end": "4735870"
  },
  {
    "text": "failure perspective if you have rigid disk failures or node failures or",
    "start": "4735870",
    "end": "4741150"
  },
  {
    "text": "network failures or entire availability zone failures a red shift will take care of all of that and automatically replace",
    "start": "4741150",
    "end": "4747560"
  },
  {
    "text": "the underlying nodes and disks on your behalf so you don't have to do any of those from your perspective security is",
    "start": "4747560",
    "end": "4756750"
  },
  {
    "text": "built in so red shift we can be you can deploy that in your own V PC in your own",
    "start": "4756750",
    "end": "4764040"
  },
  {
    "text": "private sub networks within the V PC you get access to data address encryption",
    "start": "4764040",
    "end": "4770430"
  },
  {
    "text": "you can encrypt data address you using your own encryption keys and it's also certified for lots of compliance",
    "start": "4770430",
    "end": "4777570"
  },
  {
    "text": "requirements like your sock 1 2 3 or FedRAMP or PCI DSS all the compliance",
    "start": "4777570",
    "end": "4783180"
  },
  {
    "text": "requirements are available out of the box the surface right so you can be rest assured that security is covered from",
    "start": "4783180",
    "end": "4789180"
  },
  {
    "text": "the service perspective it's also a continuously evolving platform so we",
    "start": "4789180",
    "end": "4798300"
  },
  {
    "text": "keep adding features to the service and you get access to those features in your",
    "start": "4798300",
    "end": "4804120"
  },
  {
    "text": "service so when we launched a new feature it gets automatically passed in",
    "start": "4804120",
    "end": "4809220"
  },
  {
    "text": "the cluster you don't have to do anything you wake up the next morning and you have the feature available for",
    "start": "4809220",
    "end": "4814590"
  },
  {
    "text": "you to use right and lastly I does a large ecosystem but any of your existing",
    "start": "4814590",
    "end": "4821760"
  },
  {
    "text": "data integration tools or ba tools can work with redshift for example if you're using tableau or informatica or click or",
    "start": "4821760",
    "end": "4829490"
  },
  {
    "text": "MicroStrategy or new tools like looker and Mattingly and all of them can work",
    "start": "4829490",
    "end": "4835440"
  },
  {
    "text": "with rich shift as well right so that's the second processing engine and the",
    "start": "4835440",
    "end": "4843300"
  },
  {
    "text": "third one is elasticsearch service I'm I'm fairly confident that most of",
    "start": "4843300",
    "end": "4848490"
  },
  {
    "text": "you are aware or aware of what elasticsearch is it's an open source search engine that is popular for log",
    "start": "4848490",
    "end": "4855840"
  },
  {
    "text": "analytics and full-text search so Amazon Elastic search service allows you to quickly spin up a cluster of",
    "start": "4855840",
    "end": "4862280"
  },
  {
    "text": "elasticsearch nodes fully managed by the service so it can automatically so you",
    "start": "4862280",
    "end": "4868620"
  },
  {
    "text": "go to service and say you need how many number of nodes in the cluster you can automatically provision that for you you",
    "start": "4868620",
    "end": "4874170"
  },
  {
    "text": "can scale the cluster as your as your need increases it gives you a highly",
    "start": "4874170",
    "end": "4879270"
  },
  {
    "text": "available endpoint that your applications can connect to and it's also integrated with other AWS services",
    "start": "4879270",
    "end": "4886080"
  },
  {
    "text": "right so this is how the underlying architecture looks like so when you go to the service and spin up the cluster",
    "start": "4886080",
    "end": "4893550"
  },
  {
    "text": "you can spin up one or more nodes in the cluster we put up an elastic load balancing in front of your nodes and",
    "start": "4893550",
    "end": "4899250"
  },
  {
    "text": "expose a DNS endpoint and you can use that to send all your elastic search api",
    "start": "4899250",
    "end": "4904470"
  },
  {
    "text": "queries right and you get both audit trail and monitoring out of the box it's",
    "start": "4904470",
    "end": "4912060"
  },
  {
    "text": "it's this is fairly evident self-explanatory so just like any other",
    "start": "4912060",
    "end": "4917280"
  },
  {
    "text": "elastic search cluster you can deploy multiple nodes in the cluster and you",
    "start": "4917280",
    "end": "4922890"
  },
  {
    "text": "have you have both shards and replicas this is nothing new this is the same as you create an elastic search running",
    "start": "4922890",
    "end": "4929910"
  },
  {
    "text": "elsewhere but the service basically manage all of that for you in addition the service can manage backups for you",
    "start": "4929910",
    "end": "4938370"
  },
  {
    "text": "you get automated snapshots you get Headmaster's if you want to enable that you also get availability zone awareness",
    "start": "4938370",
    "end": "4945780"
  },
  {
    "text": "you can deploy the service across multiple availability zone and you get keep on out of the box and you can",
    "start": "4945780",
    "end": "4952140"
  },
  {
    "text": "control security through V PC and access policies right so that's another choice from a processing perspective or if you",
    "start": "4952140",
    "end": "4959400"
  },
  {
    "text": "have log Analytics use cases key burner dashboarding use cases or full-text",
    "start": "4959400",
    "end": "4965610"
  },
  {
    "text": "search use cases elastic search service can help you that and then you may have",
    "start": "4965610",
    "end": "4973470"
  },
  {
    "text": "this different persona where they just want to run sequel query and you got data sitting in s3 and you want to run",
    "start": "4973470",
    "end": "4980700"
  },
  {
    "text": "sequel query and spinning up a EMR cluster or a redshift cluster might be challenging for you so Amazon Athena can",
    "start": "4980700",
    "end": "4989490"
  },
  {
    "text": "allow you to run interactive sequel queries against your s3 data right the",
    "start": "4989490",
    "end": "4995880"
  },
  {
    "text": "service allows you to submit a sequel query to it and it runs that sequel",
    "start": "4995880",
    "end": "5002450"
  },
  {
    "text": "query against your s3 data the entire infrastructure is managed by the service",
    "start": "5002450",
    "end": "5007580"
  },
  {
    "text": "and you only pay per query if you fire a query if the query runs for certain",
    "start": "5007580",
    "end": "5013310"
  },
  {
    "text": "duration and scans X amounts of data you only pay during that time if you don't",
    "start": "5013310",
    "end": "5018380"
  },
  {
    "text": "turn any query you don't pay anything to this service right so you can run a query nothing.now in three simple steps",
    "start": "5018380",
    "end": "5024980"
  },
  {
    "text": "simply log into the console create a table just like how you would create a table in our using a hive DDL statement",
    "start": "5024980",
    "end": "5031130"
  },
  {
    "text": "or use the glue cat data catalog and then start firing a query or in the console it can be pretty pretty I quit",
    "start": "5031130",
    "end": "5038900"
  },
  {
    "text": "to start running queries against Athena right you don't know data any into the service",
    "start": "5038900",
    "end": "5044900"
  },
  {
    "text": "it can fire the fire that query against data that is sitting in s3 directly",
    "start": "5044900",
    "end": "5049910"
  },
  {
    "text": "it supports wide variety of formats text CSV JSON web logs",
    "start": "5049910",
    "end": "5055810"
  },
  {
    "text": "Calma formats like RC Parque Avro a lot of popular open formats are supported it",
    "start": "5055810",
    "end": "5062750"
  },
  {
    "text": "directly streams data from s3 on your query runs against the s3 data right you",
    "start": "5062750",
    "end": "5069920"
  },
  {
    "text": "can either use the console or use the JDBC ODBC driver which means that you can connect any of",
    "start": "5069920",
    "end": "5076159"
  },
  {
    "text": "your applications to Athena as well you can use an SE sequel I don't have to",
    "start": "5076159",
    "end": "5082849"
  },
  {
    "text": "learn a new language the same sequel query that you can write against Athena as well supports complex joins and",
    "start": "5082849",
    "end": "5089150"
  },
  {
    "text": "extract queries and also window functions and also supports partitioning by any any key right you can partition",
    "start": "5089150",
    "end": "5095630"
  },
  {
    "text": "by any column in your table under the",
    "start": "5095630",
    "end": "5100760"
  },
  {
    "text": "hood we run familiar technologies we run a press the cluster which is waiting for",
    "start": "5100760",
    "end": "5107209"
  },
  {
    "text": "you we run a one pull of pressed or cluster whenever you submit a query that query goes to the Presto cluster and it",
    "start": "5107209",
    "end": "5113510"
  },
  {
    "text": "runs against your s3 bucket right and we basically use a high res the meta store",
    "start": "5113510",
    "end": "5119059"
  },
  {
    "text": "which means that it can again work with gluta at deira catalog as well so here",
    "start": "5119059",
    "end": "5125239"
  },
  {
    "text": "is a simple pipeline quickly your data could be getting ingested to s3 you could be using EMR to cleanse and",
    "start": "5125239",
    "end": "5130820"
  },
  {
    "text": "transform and again load the cleanse data into s3 and then finally you use",
    "start": "5130820",
    "end": "5136280"
  },
  {
    "text": "redshift for your - boring use cases right I think I can be used for any a",
    "start": "5136280",
    "end": "5141530"
  },
  {
    "text": "dot use cases so let's see if that it analyst the scientist comes and says I want to run this ad hoc query you don't",
    "start": "5141530",
    "end": "5147619"
  },
  {
    "text": "have to touch the operational pipeline you can go to Athena and run that query against your raw data in and out of",
    "start": "5147619",
    "end": "5154039"
  },
  {
    "text": "fashion similarly you can also use that in order to query your aggregated data in a in an ad hoc fashion right so",
    "start": "5154039",
    "end": "5160429"
  },
  {
    "text": "that's the sweet spot for Athena right it's it's pay per query model you pay",
    "start": "5160429",
    "end": "5167119"
  },
  {
    "text": "for five dollars per terabyte of data being scanned if you scan less obviously you pay less err on a pro-rata basis if",
    "start": "5167119",
    "end": "5174380"
  },
  {
    "text": "you store data in a compressed on columnar format with partitioning in place you can save a lot of money in",
    "start": "5174380",
    "end": "5182269"
  },
  {
    "text": "terms of what you paid to Athena right and finally for all those business users",
    "start": "5182269",
    "end": "5190610"
  },
  {
    "text": "where you have a dashboarding requirements Amazon quick site is a service that allows any business user to",
    "start": "5190610",
    "end": "5197989"
  },
  {
    "text": "quickly visualize data right you can go to the service and quickly start visualizing data they're sitting out of",
    "start": "5197989",
    "end": "5203959"
  },
  {
    "text": "your data sources so typically could site is meant for data consumers or business",
    "start": "5203959",
    "end": "5211490"
  },
  {
    "text": "users who would want to use some kind of UI based dashboard to look at their data",
    "start": "5211490",
    "end": "5217730"
  },
  {
    "text": "right so you can go to the service and quickly define your data sets and start building dashboards through analysis and",
    "start": "5217730",
    "end": "5224810"
  },
  {
    "text": "then you can also build the storyboards where you can combine dashboards as a",
    "start": "5224810",
    "end": "5230660"
  },
  {
    "text": "story and then start sharing that story board with various users in your organization right I'm going to quickly",
    "start": "5230660",
    "end": "5237920"
  },
  {
    "text": "go through this I think we are right on time so it has integration with various AWS",
    "start": "5237920",
    "end": "5243050"
  },
  {
    "text": "services so quick side can connect to your red shift your RDS which is your",
    "start": "5243050",
    "end": "5249670"
  },
  {
    "text": "database manage databases or athina or even directly to your s3 bucket right it",
    "start": "5249670",
    "end": "5256760"
  },
  {
    "text": "has very lots of integrations against those different data sources it has got an engine called as spice which is an",
    "start": "5256760",
    "end": "5263780"
  },
  {
    "text": "in-memory engine so you can optionally load data into this in-memory engine where it can speed up the queries for",
    "start": "5263780",
    "end": "5271250"
  },
  {
    "text": "all those dashboards so here is a sample use case but you can have dashboards",
    "start": "5271250",
    "end": "5277790"
  },
  {
    "text": "being built on quick site that can directly submit the queries to redshift",
    "start": "5277790",
    "end": "5282800"
  },
  {
    "text": "so either you use spice to run some of your a ready-made dashboards or let's",
    "start": "5282800",
    "end": "5288590"
  },
  {
    "text": "say when somebody wants to go into drilldowns that can go down to a redshift right and similarly it can",
    "start": "5288590",
    "end": "5294950"
  },
  {
    "text": "connect to any RDS databases your my sequel the sequel servers of the world",
    "start": "5294950",
    "end": "5300170"
  },
  {
    "text": "or it can even talk to Athena and you know directly query data that is sitting",
    "start": "5300170",
    "end": "5305360"
  },
  {
    "text": "out of s3 so any of those dashboard in use cases you can again think about",
    "start": "5305360",
    "end": "5310820"
  },
  {
    "text": "quick site for that right so that's the processing and the serving layer and finally on the security layer so they",
    "start": "5310820",
    "end": "5320990"
  },
  {
    "text": "operate in a model called as shared responsibility model where AWS can take care of security off the cloud where it",
    "start": "5320990",
    "end": "5328280"
  },
  {
    "text": "takes care of all the security of the underlying physical infrastructure and then you as a customer take care of the",
    "start": "5328280",
    "end": "5334730"
  },
  {
    "text": "security from your perspective in terms of whether it is your application or all the services that you deploy it's your",
    "start": "5334730",
    "end": "5341330"
  },
  {
    "text": "responsibility to me sure that you're defining the right set of security controls right so obviously",
    "start": "5341330",
    "end": "5346530"
  },
  {
    "text": "we saw in the redshift discussion you can spin up VPC which allows you to spin",
    "start": "5346530",
    "end": "5352590"
  },
  {
    "text": "up a virtual network on AWS and deploy your infrastructure within that virtual network you get network security out of",
    "start": "5352590",
    "end": "5360060"
  },
  {
    "text": "the box you can keep your entire infrastructure private if you would want to and some of our popular services like",
    "start": "5360060",
    "end": "5366420"
  },
  {
    "text": "s3 DynamoDB and kinases they also expose VPC endpoints which means that your",
    "start": "5366420",
    "end": "5372360"
  },
  {
    "text": "traffic to those services also can remain within the VPC right so here is a sample example where your EMR cluster",
    "start": "5372360",
    "end": "5378960"
  },
  {
    "text": "can run between your private subnet and access your s3 as well right you also",
    "start": "5378960",
    "end": "5385530"
  },
  {
    "text": "get I am capabilities where you can define various users and groups and",
    "start": "5385530",
    "end": "5391489"
  },
  {
    "text": "control fine-grained access policies for them who has access to what kind of services and what kind of data and if",
    "start": "5391489",
    "end": "5399270"
  },
  {
    "text": "for your encryption requirements you get kms which allows you to define your encryption keys and all our services can",
    "start": "5399270",
    "end": "5406010"
  },
  {
    "text": "use that for your addressed encryption and finally from an audit trail",
    "start": "5406010",
    "end": "5411420"
  },
  {
    "text": "perspective cloud service which again provides auditing capabilities where it",
    "start": "5411420",
    "end": "5416610"
  },
  {
    "text": "can record all the API calls made to all those different name SS services right so as a customer when you use any of",
    "start": "5416610",
    "end": "5423330"
  },
  {
    "text": "these services you use these other security services be it kms or cloud trail or VPC to make sure that you are",
    "start": "5423330",
    "end": "5431040"
  },
  {
    "text": "operating in a secured manner right so with that we come to the final section",
    "start": "5431040",
    "end": "5437910"
  },
  {
    "text": "of the of the talk which is about how you can build modern data architectures",
    "start": "5437910",
    "end": "5443190"
  },
  {
    "text": "on AWS so typically what customers are building is you have these three",
    "start": "5443190",
    "end": "5449730"
  },
  {
    "text": "different layers what we call as a lambda architecture your batch layer your real-time layer and also you're",
    "start": "5449730",
    "end": "5455130"
  },
  {
    "text": "serving layer on the right side you've got this various person ask who have different needs and they want to use",
    "start": "5455130",
    "end": "5461610"
  },
  {
    "text": "different tools to answer their questions so your data sources on the left hand side can be ingesting data",
    "start": "5461610",
    "end": "5469110"
  },
  {
    "text": "into your era Lake you can use various tools to ingest that on the serving layer we saw that we can use",
    "start": "5469110",
    "end": "5475800"
  },
  {
    "text": "different services to answer those different queries be redshift or EMR",
    "start": "5475800",
    "end": "5482130"
  },
  {
    "text": "elasticsearch or Athena the different teams can use different services to answer those questions and your data",
    "start": "5482130",
    "end": "5490380"
  },
  {
    "text": "sits in s3 so that's where all your stage to data use and that becomes your data Lake right from your ingestion",
    "start": "5490380",
    "end": "5498420"
  },
  {
    "text": "perspective you connect to those different data sources use different ingestion services be it kinases or",
    "start": "5498420",
    "end": "5504290"
  },
  {
    "text": "Direct Connect and and ingest data into",
    "start": "5504290",
    "end": "5509430"
  },
  {
    "text": "your your data Lake and use a range of services like I am and cloud trail for",
    "start": "5509430",
    "end": "5516810"
  },
  {
    "text": "securing and for your batch layer data could be coming into your s3 bucket and",
    "start": "5516810",
    "end": "5522080"
  },
  {
    "text": "you can use EMR to extract and you know do your ETL in a batch layer and push",
    "start": "5522080",
    "end": "5528150"
  },
  {
    "text": "that to your stage layer which can further go down to your redshift or elasticsearch in your speed layer you",
    "start": "5528150",
    "end": "5535050"
  },
  {
    "text": "can use kinases for your streaming requirements and you can think about using spark streaming or flink on em are",
    "start": "5535050",
    "end": "5541530"
  },
  {
    "text": "doing stream processing and again ingesting raw data into your rod in a",
    "start": "5541530",
    "end": "5547140"
  },
  {
    "text": "bucket and then if you have new use cases like your machine learning use cases again that can be done either by a",
    "start": "5547140",
    "end": "5553610"
  },
  {
    "text": "EMR and again a billiard machine learning models and serve them from your",
    "start": "5553610",
    "end": "5558870"
  },
  {
    "text": "serving layers right so this is how you typically build incrementally your data architecture segregating your batch and",
    "start": "5558870",
    "end": "5566490"
  },
  {
    "text": "streaming layers and assembling different abilities to power those use",
    "start": "5566490",
    "end": "5571740"
  },
  {
    "text": "cases right and quickly what are some of the production use cases that we see",
    "start": "5571740",
    "end": "5577710"
  },
  {
    "text": "customers doing so Netflix they have been a great customer one of our large",
    "start": "5577710",
    "end": "5582990"
  },
  {
    "text": "customers and those of you have used Netflix I'm sure you loved their",
    "start": "5582990",
    "end": "5588080"
  },
  {
    "text": "recommendation engine but based on your viewing patterns they recommend what",
    "start": "5588080",
    "end": "5593490"
  },
  {
    "text": "other shows that you're likely to watch that entire thing is basically being done to a number of EMR clusters they",
    "start": "5593490",
    "end": "5601860"
  },
  {
    "text": "have multiple EMR clusters some of them are ETL driven SSL later when production",
    "start": "5601860",
    "end": "5607170"
  },
  {
    "text": "clusters they currently two thousand plus nodes on those EMR",
    "start": "5607170",
    "end": "5613800"
  },
  {
    "text": "clusters and they also have a doc exploratory clusters which is again another 2,000 plus node cluster right",
    "start": "5613800",
    "end": "5621090"
  },
  {
    "text": "and all of that is being run out of EMR and their entire data set lives out of",
    "start": "5621090",
    "end": "5627929"
  },
  {
    "text": "s3 right so there are a large customer using the decoupled EMR no decoupled",
    "start": "5627929",
    "end": "5635369"
  },
  {
    "text": "computer in a storage model syndra is another customers of indra is a large",
    "start": "5635369",
    "end": "5641400"
  },
  {
    "text": "regulator in the u.s. think of them like our say be in india so they they kind of",
    "start": "5641400",
    "end": "5647670"
  },
  {
    "text": "regulate all the markets in the US and they have a surveillance platform which",
    "start": "5647670",
    "end": "5652920"
  },
  {
    "text": "runs on AWS primarily they have to do a lot of fraud detection and again they",
    "start": "5652920",
    "end": "5658860"
  },
  {
    "text": "use EMR extensively to do that so this is their architecture where they got",
    "start": "5658860",
    "end": "5664289"
  },
  {
    "text": "multiple EMR clusters and they also have multiple redshift clusters in their architecture again all their data lives",
    "start": "5664289",
    "end": "5672420"
  },
  {
    "text": "in s3 and s3 is being used as their data Lake right Nasdaq is another example",
    "start": "5672420",
    "end": "5678840"
  },
  {
    "text": "again in the regulated space so they are one of the largest sex exchanges and they power more than hundred market",
    "start": "5678840",
    "end": "5684869"
  },
  {
    "text": "places across 50 countries and they again implement an s3 derelict",
    "start": "5684869",
    "end": "5690539"
  },
  {
    "text": "architecture plus redshift as their warehouse so they keep two to five years",
    "start": "5690539",
    "end": "5696030"
  },
  {
    "text": "of data in s3 and the most recent which is one to two years of data in redshift",
    "start": "5696030",
    "end": "5702570"
  },
  {
    "text": "and presto on EMR is being used for their other use cases right and they",
    "start": "5702570",
    "end": "5710219"
  },
  {
    "text": "ingest over seven billion rows every day for all their analytics use cases right",
    "start": "5710219",
    "end": "5717769"
  },
  {
    "text": "lastly one more customer example a Hearst is a large publishing company",
    "start": "5717769",
    "end": "5723210"
  },
  {
    "text": "they they are they're owning some of the large magazines some of the popular",
    "start": "5723210",
    "end": "5728820"
  },
  {
    "text": "magazines like your cosmopolitan and le and all of them again they have a data",
    "start": "5728820",
    "end": "5734849"
  },
  {
    "text": "like architecture the clickstream data comes through kinases and they use spark",
    "start": "5734849",
    "end": "5740610"
  },
  {
    "text": "on anymore to process that in near-real-time and then push the same data into redshift",
    "start": "5740610",
    "end": "5745809"
  },
  {
    "text": "which is being used for their bi use cases right I know I went through these",
    "start": "5745809",
    "end": "5752020"
  },
  {
    "text": "use cases pretty quickly for the want of time but what I highly encourage is all these are public reference use cases all",
    "start": "5752020",
    "end": "5759909"
  },
  {
    "text": "of them are available on our website so if you want to learn more I highly encourage you to follow our big bear up",
    "start": "5759909",
    "end": "5766960"
  },
  {
    "text": "blog we talked about a lot of the different architectures in our Big Data blog the URL is there I highly encourage",
    "start": "5766960",
    "end": "5772360"
  },
  {
    "text": "you to follow that arraignment is happening now lots of sessions for big",
    "start": "5772360",
    "end": "5779289"
  },
  {
    "text": "data analytics in Raymond so once these sessions are over we upload them to our YouTube channel make sure that your",
    "start": "5779289",
    "end": "5787110"
  },
  {
    "text": "you're following our YouTube channel as well so that's a great place to learn and you also have a derelict solution",
    "start": "5787110",
    "end": "5793929"
  },
  {
    "text": "this is a CloudFormation template that you can quickly deploy in your own area Bueller's account and it sets ups all",
    "start": "5793929",
    "end": "5800320"
  },
  {
    "text": "the components for your data Lake and you can start ingesting data and start making some sense of it all right I",
    "start": "5800320",
    "end": "5806469"
  },
  {
    "text": "again highly encourage you to look at the data Lake solution as well from a",
    "start": "5806469",
    "end": "5811809"
  },
  {
    "text": "customer case study perspective again look for just search for case study big",
    "start": "5811809",
    "end": "5817270"
  },
  {
    "text": "data case studies you'll hit that URL again lots and lots of customer case studies out there it can learn from what",
    "start": "5817270",
    "end": "5822610"
  },
  {
    "text": "other customers are doing and finally on the market place we have a software market players where if you're looking",
    "start": "5822610",
    "end": "5828639"
  },
  {
    "text": "for products like for example tableau or their data or for example metally and",
    "start": "5828639",
    "end": "5835780"
  },
  {
    "text": "all of these products are available in one-click deployment you can go to the marketplace and deploy that in one click in your edible environment right with",
    "start": "5835780",
    "end": "5844360"
  },
  {
    "text": "that it's a wrap I hope this was useful I know that I kind of exceeded the time",
    "start": "5844360",
    "end": "5850690"
  },
  {
    "text": "slot I I can take few questions a couple",
    "start": "5850690",
    "end": "5855820"
  },
  {
    "text": "of questions maybe I'll quickly go through the questions window and I'll do",
    "start": "5855820",
    "end": "5861099"
  },
  {
    "text": "probably couple of questions before we wrap up right I'm just looking through",
    "start": "5861099",
    "end": "5870250"
  },
  {
    "text": "the questions window so what is the difference between a",
    "start": "5870250",
    "end": "5876140"
  },
  {
    "text": "derelict and a data warehouse I hope that was demystified our data warehouse is probably one of the components of",
    "start": "5876140",
    "end": "5882950"
  },
  {
    "text": "your data Lake and Darryl a can actually feed into your data warehouse did it",
    "start": "5882950",
    "end": "5893230"
  },
  {
    "text": "I'll probably take one more questions so",
    "start": "5893230",
    "end": "5904700"
  },
  {
    "text": "a major challenge is to ingest data into the data Lake other than AWS options provided is it possible to use other",
    "start": "5904700",
    "end": "5911450"
  },
  {
    "text": "non-edible options like flew for flume and scoop yes so there are lots of these",
    "start": "5911450",
    "end": "5917989"
  },
  {
    "text": "open source tools like for example flume or scoop and all of them lots of partner tools again you can use all of them to",
    "start": "5917989",
    "end": "5924860"
  },
  {
    "text": "ingest into your derelict but one key thing to remember there is when you use",
    "start": "5924860",
    "end": "5930560"
  },
  {
    "text": "any of these tools just make sure that you are bringing the data into s3 because hopefully you are able to",
    "start": "5930560",
    "end": "5936860"
  },
  {
    "text": "appreciate that if your data comes into s3 you can use various processing engines to process your data but yes the",
    "start": "5936860",
    "end": "5944060"
  },
  {
    "text": "answer is lot of this popular open source tools and partner tools allow",
    "start": "5944060",
    "end": "5949190"
  },
  {
    "text": "ingestion into the into s3 I'll probably",
    "start": "5949190",
    "end": "5957760"
  },
  {
    "text": "take one more question so where one",
    "start": "5957760",
    "end": "5967280"
  },
  {
    "text": "other question that I see is when do you use streaming versus firehose and vice",
    "start": "5967280",
    "end": "5972890"
  },
  {
    "text": "versa again I hope I hope that was clear where Kinison streams is more for near",
    "start": "5972890",
    "end": "5979790"
  },
  {
    "text": "real-time use cases and where you want to have your own applications processing data out of the stream fire rows the",
    "start": "5979790",
    "end": "5988370"
  },
  {
    "text": "minimum latency is going to be either 1 minute or 1 MB and the best nations are",
    "start": "5988370",
    "end": "5994520"
  },
  {
    "text": "either those three services right so s3 or redshift or elasticsearch so",
    "start": "5994520",
    "end": "5999860"
  },
  {
    "text": "depending upon your use cases you pick up either streams or firehose right",
    "start": "5999860",
    "end": "6006820"
  },
  {
    "text": "I think I'll wrap up with that I honestly wanted to do more Q&A I wanted",
    "start": "6006820",
    "end": "6012940"
  },
  {
    "text": "to allocate more time for Q&A I really ran out of time I'm really sorry about that but what I will do is",
    "start": "6012940",
    "end": "6019050"
  },
  {
    "text": "we have the email ids of folks who ask these questions we will individually",
    "start": "6019050",
    "end": "6024219"
  },
  {
    "text": "respond to each one of you those who have you asked questions and hopefully",
    "start": "6024219",
    "end": "6029710"
  },
  {
    "text": "we're able to answer your questions so thank you so much thanks for joining",
    "start": "6029710",
    "end": "6035050"
  },
  {
    "text": "hopefully it was productive if you have any further questions and follow-ups please do reach out to us and we are",
    "start": "6035050",
    "end": "6041560"
  },
  {
    "text": "happy to help you out right thanks a lot on you have a wonderful day ahead",
    "start": "6041560",
    "end": "6047369"
  },
  {
    "text": "you",
    "start": "6089000",
    "end": "6091060"
  }
]