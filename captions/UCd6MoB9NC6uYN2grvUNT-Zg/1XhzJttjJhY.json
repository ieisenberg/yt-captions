[
  {
    "text": "hello and welcome my name is Siva raghupati I'm a principal solutions",
    "start": "1700",
    "end": "6810"
  },
  {
    "text": "architect at amazon web services just a little bit about myself I've been at amazon for about five and a half years",
    "start": "6810",
    "end": "12570"
  },
  {
    "text": "the first two years I spent on building a couple of services amazon dynamodb and",
    "start": "12570",
    "end": "17580"
  },
  {
    "text": "Amazon RDS which is our no sequel and the sequel services I was poor member of the team last three years I've been",
    "start": "17580",
    "end": "23039"
  },
  {
    "text": "working with customers both including amazoncom on solving their big data",
    "start": "23039",
    "end": "29039"
  },
  {
    "text": "problems on AWS and delighted to be here I hope you are in a great conference let's get started so in terms of the",
    "start": "29039",
    "end": "36000"
  },
  {
    "text": "agenda we're going to I'm going to go through some of the big data challenges our customers are having and I'm going",
    "start": "36000",
    "end": "42120"
  },
  {
    "text": "to try to simplify the big data processing into multiple stages in jest or process and visualize then we'll go",
    "start": "42120",
    "end": "48210"
  },
  {
    "text": "into each stage and figure out how to pick the right technologies especially we will touch on why and how and then I",
    "start": "48210",
    "end": "54390"
  },
  {
    "text": "will come up I'll come up with a different architecture and we'll delve into a couple of design design patterns",
    "start": "54390",
    "end": "61079"
  },
  {
    "text": "and best practices you know I'm not going to be able to touch specific customer examples amazon cloud drive is",
    "start": "61079",
    "end": "68040"
  },
  {
    "text": "doing a talk at 3pm about how they used these principles within amazoncom as",
    "start": "68040",
    "end": "74100"
  },
  {
    "text": "well as something a talk with a drawl tomorrow at 9am in the attack will go through another version of how the big",
    "start": "74100",
    "end": "79619"
  },
  {
    "text": "data patterns are implemented so if you are interested in following through or watching later on those would be the two good continuations with 45 minutes I'll",
    "start": "79619",
    "end": "86250"
  },
  {
    "text": "just have time to go through the architectural patterns so what are the",
    "start": "86250",
    "end": "92850"
  },
  {
    "text": "big data challenges our customers are having so in terms of I'm sure you've seen as a chart like this in every Big",
    "start": "92850",
    "end": "97860"
  },
  {
    "text": "Data conference the volume velocity and variety of big data is ever-increasing",
    "start": "97860",
    "end": "102950"
  },
  {
    "text": "last time I looked at how much of data is there in the world apparently there's 4.4 petabyte zettabytes of data in the",
    "start": "102950",
    "end": "110009"
  },
  {
    "text": "world once it about is you know many many terabytes the way I utilize this",
    "start": "110009",
    "end": "115799"
  },
  {
    "text": "these numbers doesn't make sense after a point the way internalize this as you know every human being there's seven",
    "start": "115799",
    "end": "120810"
  },
  {
    "text": "billion people in the world we roughly have about 600 to San hundred gigabytes per human being that is the amount of",
    "start": "120810",
    "end": "127049"
  },
  {
    "text": "data that we have apparently in four of four or five years that's going to go down to about six or seven",
    "start": "127049",
    "end": "133040"
  },
  {
    "text": "tera bytes in a per person in the end in the world I think so there's a lot of data being born because there because of",
    "start": "133040",
    "end": "140520"
  },
  {
    "text": "our digital lifestyles and so practically what does it mean in terms",
    "start": "140520",
    "end": "145590"
  },
  {
    "text": "of customers using it w as you know for example the Admiral I work with a drawl",
    "start": "145590",
    "end": "150990"
  },
  {
    "text": "their ad retargeting company imagine those ads that follow through everywhere that you go on the internet once you go",
    "start": "150990",
    "end": "156600"
  },
  {
    "text": "once you go to a retailer site and by they are surfacing those ads there they're serving the bids and winning the",
    "start": "156600",
    "end": "162510"
  },
  {
    "text": "bids etc customers like that handle about 150 terabytes the data flowing in",
    "start": "162510",
    "end": "169020"
  },
  {
    "text": "every single day the request rates about 60 billion requests they serve about 60 billion requests per second that roughly",
    "start": "169020",
    "end": "175530"
  },
  {
    "text": "equates to any way of fluctuating between 500 and 7000 requests per second they need to serve this at about 3",
    "start": "175530",
    "end": "181410"
  },
  {
    "text": "millisecond latency 'he's that's the kind of Layton sees that they want the question for for us is to how do we",
    "start": "181410",
    "end": "187200"
  },
  {
    "text": "build systems that scale to that scale and then and then this data is in exponentially growing every three to",
    "start": "187200",
    "end": "193500"
  },
  {
    "text": "four months the volume and velocity seems to be doubling in terms of the variety of data instead of just in",
    "start": "193500",
    "end": "199620"
  },
  {
    "text": "addition to just handling transactional data you know we need to be handling you know video streams pictures as well as",
    "start": "199620",
    "end": "207060"
  },
  {
    "text": "audio in fact a good chunk of the data you know about Sony thirty percent of the data is taxed the rest of it is the",
    "start": "207060",
    "end": "214620"
  },
  {
    "text": "video audio and other other different types of data so in terms of the other",
    "start": "214620",
    "end": "220710"
  },
  {
    "text": "dimension for me as i speak with customers is that big data processing is moving to real-time big data processing",
    "start": "220710",
    "end": "227520"
  },
  {
    "text": "you know in other words big data is best served fresh some of the examples there are instead of running orally or weekly",
    "start": "227520",
    "end": "234120"
  },
  {
    "text": "reports people are interested in finding out what's happening right now let's say if you if you're building a system for",
    "start": "234120",
    "end": "239610"
  },
  {
    "text": "handling credit card transactions rather than running a report at the end of the day figuring out what what transact you",
    "start": "239610",
    "end": "246240"
  },
  {
    "text": "know what fraudulent transactions happen won't it be nice to be able to stop while the transaction is in flight you",
    "start": "246240",
    "end": "252300"
  },
  {
    "text": "know those are the kinds of systems people want to build for example I was working with the you know car",
    "start": "252300",
    "end": "258269"
  },
  {
    "text": "manufacturer you know they are running various campaigns at any given point in time they want to figure out what make",
    "start": "258270",
    "end": "264240"
  },
  {
    "text": "model and year is pretty popular in there site what are the people browsing these days you know people want to get these",
    "start": "264240",
    "end": "270009"
  },
  {
    "text": "kinds of instantaneous data and how do you process that in other words how do you build systems that can deal with",
    "start": "270009",
    "end": "276370"
  },
  {
    "text": "streaming data coming in and and in addition to running their batch processing jobs so luckily there is a",
    "start": "276370",
    "end": "285940"
  },
  {
    "text": "plethora of tools in the industry you know I want to call this the Apache and",
    "start": "285940",
    "end": "291400"
  },
  {
    "text": "the open-source ecosystem has a zoo of technologies in fact if you look down there's a zookeeper at the very bottom",
    "start": "291400",
    "end": "297250"
  },
  {
    "text": "there as well the guy with the little mop so you know hi Pig spark and",
    "start": "297250",
    "end": "304650"
  },
  {
    "text": "obviously the classic elephant Hadoop etc and if you look at on the right side",
    "start": "304650",
    "end": "310030"
  },
  {
    "text": "if you look at AWS we have a plethora of tools you know starting with elastic MapReduce s3 dynamodb which is your nose",
    "start": "310030",
    "end": "316270"
  },
  {
    "text": "equals so redshift etc so the one of the big challenges that customers are having",
    "start": "316270",
    "end": "321970"
  },
  {
    "text": "is what do I use what technology do I use and and why and how that was the",
    "start": "321970",
    "end": "329830"
  },
  {
    "text": "genesis of this presentation for me and then now let's let's let's try to",
    "start": "329830",
    "end": "335680"
  },
  {
    "text": "simplify this this processing right one way of thinking about big data processing is I really think of this as",
    "start": "335680",
    "end": "342610"
  },
  {
    "text": "a pipeline with data flowing in on one side and answers coming back on other side and there is a time to knowledge",
    "start": "342610",
    "end": "348039"
  },
  {
    "text": "you know in terms of and and you just going through multiple stages you know one of the stages may be in jest store",
    "start": "348039",
    "end": "354849"
  },
  {
    "text": "process and visualized the store and process not not necessarily be a single run in many cases there is a loop that",
    "start": "354849",
    "end": "361389"
  },
  {
    "text": "goes on there you you you store your process you store again you process again and then then you then their moves",
    "start": "361389",
    "end": "368889"
  },
  {
    "text": "forward so you know whatever you pick in between needs to fit in the time to",
    "start": "368889",
    "end": "374530"
  },
  {
    "text": "knowledge a time to answer in the case of real-time systems that time is in the order of milliseconds if you're talking",
    "start": "374530",
    "end": "379960"
  },
  {
    "text": "about real business real time people are happy with you know things happening in a minute or in a few seconds but so you",
    "start": "379960",
    "end": "387789"
  },
  {
    "text": "know that time varies and based on that time you try to pick different components I think that's one way of looking at it so if you actually apply",
    "start": "387789",
    "end": "394479"
  },
  {
    "text": "the paradigm and then actually you know sly you put all the tools in various categories you tend to",
    "start": "394479",
    "end": "400120"
  },
  {
    "text": "in a model 10 to emerge your app servers web servers and devices you're logging",
    "start": "400120",
    "end": "405280"
  },
  {
    "text": "frameworks such as flume log4j etc are on the left side on the injured side",
    "start": "405280",
    "end": "411220"
  },
  {
    "text": "then you're storing the data and some kind of a storage tier you know which could be as free HDFS or your no sequel",
    "start": "411220",
    "end": "417430"
  },
  {
    "text": "stores such as HBase you know Cassandra  etc and then then the data gets",
    "start": "417430",
    "end": "425229"
  },
  {
    "text": "processed by a processing tier which could be Hadoop or which beach park or you may be running stream processing",
    "start": "425229",
    "end": "432490"
  },
  {
    "text": "applications such as Tom or spark streaming and then later on there's an i",
    "start": "432490",
    "end": "437590"
  },
  {
    "text": "traded process there between store and process then it goes to visualization phase one disclaimer here I put a few",
    "start": "437590",
    "end": "444340"
  },
  {
    "text": "names of the companies there as a platform provider there's a big risk and putting those names you know that",
    "start": "444340",
    "end": "449440"
  },
  {
    "text": "doesn't mean these are the only platforms we support these are the only companies you support I pull some things in example a few things as an example so",
    "start": "449440",
    "end": "456550"
  },
  {
    "text": "if you go into the AWS marketplace we have a rich ecosystem of partners that we work with there's no big data without",
    "start": "456550",
    "end": "462100"
  },
  {
    "text": "partners I just wanted to thank them first and then you know take that into consideration as I move forward if you",
    "start": "462100",
    "end": "467979"
  },
  {
    "text": "don't see your icon that doesn't mean AWS doesn't support this etc so moving",
    "start": "467979",
    "end": "473229"
  },
  {
    "text": "forward let's deeply delve into the India's face and then in terms of the",
    "start": "473229",
    "end": "478330"
  },
  {
    "text": "types of data that we did at the india ingest face I think of that in three buckets one you're dealing with transactional data you know think",
    "start": "478330",
    "end": "484900"
  },
  {
    "text": "database is doing your classic create read update and delete operations now with big data the rate at which these",
    "start": "484900",
    "end": "491440"
  },
  {
    "text": "things need to be done is really really high like they pointed out 500 to 700 thousand requests per second this was",
    "start": "491440",
    "end": "497229"
  },
  {
    "text": "unknown even 5-10 years ago we didn't build systems that can handle now a team",
    "start": "497229",
    "end": "502750"
  },
  {
    "text": "with three engineers wants to build this in three weeks right that's the kind of request that we're getting and there are",
    "start": "502750",
    "end": "508720"
  },
  {
    "text": "technologies to be able to do that today fairly rapidly and easily and then the second piece is file data you know your",
    "start": "508720",
    "end": "515560"
  },
  {
    "text": "app servers and vectors are creating a lot of logs you know typically most you know if you look at amazon.com all that",
    "start": "515560",
    "end": "521529"
  },
  {
    "text": "you know everything that you do an arm is on site you know goes to our click stream processing the click stream processing loads all the data you know",
    "start": "521529",
    "end": "527320"
  },
  {
    "text": "and pushes it to s3 etc you know we're moving a lot of files over you know from your app servers and web",
    "start": "527320",
    "end": "532800"
  },
  {
    "text": "servers down to a place we can run click stream analytics etc and then the third piece is really the streaming data you",
    "start": "532800",
    "end": "538830"
  },
  {
    "text": "know the ideal scenario here is our the classic scenario here is the Internet of Things so I do devices you're measuring",
    "start": "538830",
    "end": "544620"
  },
  {
    "text": "devices etc measure the data depending upon how much of memory or the computing power that they have they may accumulate",
    "start": "544620",
    "end": "550829"
  },
  {
    "text": "the data are actually trickle feed that data into some kind of a store you know stream storage tier so it makes it much",
    "start": "550829",
    "end": "557579"
  },
  {
    "text": "easier for me to think about these you know the interface into three categories just to simplify them so the next",
    "start": "557579",
    "end": "563790"
  },
  {
    "text": "question is you know what kind of a storage technology are we going to use you know as I was putting through the",
    "start": "563790",
    "end": "569070"
  },
  {
    "text": "slide I mean I was realizing on the top here you know the transition from data you know this is sort of that we were",
    "start": "569070",
    "end": "575130"
  },
  {
    "text": "people when we do it just to be dealing with databases we were in the data phase then things move too big time in a big",
    "start": "575130",
    "end": "581370"
  },
  {
    "text": "data and we're doing with streaming big data in this phase right now we're going to look through I'm going to look at a couple of technologies there that are",
    "start": "581370",
    "end": "587519"
  },
  {
    "text": "very that are very prevalent Kinesis amazon kinesis and apache Kafka as part",
    "start": "587519",
    "end": "594630"
  },
  {
    "text": "of this presentation I'm not going to go into the details of each one of the technologies are more interested in painting architectural patterns using",
    "start": "594630",
    "end": "600329"
  },
  {
    "text": "that so we're running a track of deep dives 400 level talks and you know how",
    "start": "600329",
    "end": "605790"
  },
  {
    "text": "do you optimize each one of those technologies those that would be a good you know peace to complement this presentation here so let's delve into",
    "start": "605790",
    "end": "612990"
  },
  {
    "text": "stream storage why stream storage now imagine all those devices sending",
    "start": "612990",
    "end": "618690"
  },
  {
    "text": "multiple streams of random streams of data those those streams these are like we're literally hot potatoes you know",
    "start": "618690",
    "end": "624270"
  },
  {
    "text": "people walking around with hot potatoes they need to give it to someone else right you need some some steer that take",
    "start": "624270",
    "end": "629670"
  },
  {
    "text": "these requests and then these trip costs can be handed out to something like Kinesis or Kafka so literally what",
    "start": "629670",
    "end": "635970"
  },
  {
    "text": "storage stream storage does is it gives you the ability to convert random change the data into multiple you know into a",
    "start": "635970",
    "end": "642149"
  },
  {
    "text": "few initial streams of data may be there maybe a thousand or a million strains coming in in order to process this it's",
    "start": "642149",
    "end": "648180"
  },
  {
    "text": "going to be hard to process million streams at the same time so these things convert that into a few sequential streams and those the paradigms there",
    "start": "648180",
    "end": "655740"
  },
  {
    "text": "are typically that there is a shard or a partition in the case of Kafka they call",
    "start": "655740",
    "end": "660959"
  },
  {
    "text": "this partition in the case of kinases we call discharge and Kafka calls this topic we",
    "start": "660959",
    "end": "666339"
  },
  {
    "text": "call this extreme extreme comprising a multiple shots oh that's the those are the primitives there and then moving",
    "start": "666339",
    "end": "673059"
  },
  {
    "text": "along what are the other pieces what are the other facilities of stream processing the stream processing engines",
    "start": "673059",
    "end": "679660"
  },
  {
    "text": "sort of gives you the ability to decouple producers and consumers so they",
    "start": "679660",
    "end": "686019"
  },
  {
    "text": "give you a buffer in other words to write the data there the fast moving data there so these two tiers can",
    "start": "686019",
    "end": "691899"
  },
  {
    "text": "actually the pump the data a process the data their own rates depending upon their business needs so the other piece",
    "start": "691899",
    "end": "698259"
  },
  {
    "text": "that they allow you to do is to the preserve client or during which means if your producer in this case the producer",
    "start": "698259",
    "end": "704350"
  },
  {
    "text": "read is sending the data in a sequential order like 1 2 3 4 even though it's",
    "start": "704350",
    "end": "709360"
  },
  {
    "text": "going to partition one the blue and the red is going to partition one still you know the ordering is preserved that",
    "start": "709360",
    "end": "715660"
  },
  {
    "text": "allows you to do let's say if you are if you are shipping transaction logs of sorts it allows you to do processing in",
    "start": "715660",
    "end": "721240"
  },
  {
    "text": "the order stuff came and that's pretty important when you're dealing with non idempotent updates for example and and",
    "start": "721240",
    "end": "728470"
  },
  {
    "text": "then you know you can think of this as a streaming MapReduce which is a fantastic facility for example let's assume you",
    "start": "728470",
    "end": "734439"
  },
  {
    "text": "want to find out how many you know the producer one what is the what is the",
    "start": "734439",
    "end": "739720"
  },
  {
    "text": "number of red red pieces that they came in you're able to simply launch a consumer for either Kinesis at Kafka",
    "start": "739720",
    "end": "746290"
  },
  {
    "text": "these consuming framework what it does is it spawns multiple threads and ensures that a single thread is attached",
    "start": "746290",
    "end": "752589"
  },
  {
    "text": "to a single partition or a shard therefore you can do computations just give me the max off for a specific",
    "start": "752589",
    "end": "758679"
  },
  {
    "text": "device in a gimmies and then if this max triggers is you know if you let's say if the temperature needs if you have an",
    "start": "758679",
    "end": "764529"
  },
  {
    "text": "alarm beyond hundred degrees you know it keeps on processing everything and checking whether whether the trigger has reach to the trigger is reached these",
    "start": "764529",
    "end": "771220"
  },
  {
    "text": "pieces can basically triggered around downstream actions so the it is critical to pick a stream processing engine that",
    "start": "771220",
    "end": "777939"
  },
  {
    "text": "is that gives you the ability to do streaming MapReduce and and then the",
    "start": "777939",
    "end": "784209"
  },
  {
    "text": "other piece that these engines allow you to do is to be able to replay by the producer produces the data and puts it",
    "start": "784209",
    "end": "790209"
  },
  {
    "text": "into the into into this engine and the consumers can go back and forth on the stream let's say if they lost the state",
    "start": "790209",
    "end": "795819"
  },
  {
    "text": "they can go back and process from the beginning of time or end of time typically the storage is fair",
    "start": "795819",
    "end": "801090"
  },
  {
    "text": "kept there for a limited amount of time will which I'll cover that in the next slide so if you compare these two",
    "start": "801090",
    "end": "806350"
  },
  {
    "text": "technologies you know what stream storage technology should I use so both",
    "start": "806350",
    "end": "811630"
  },
  {
    "text": "canisius and Kafka has many similarities they support multiple consumers they support ordering of Records the stream",
    "start": "811630",
    "end": "818770"
  },
  {
    "text": "they allow you to do streaming map reduce the latency is low in terms of milliseconds you know tens of",
    "start": "818770",
    "end": "824380"
  },
  {
    "text": "milliseconds for both puts four puts and then maybe about 100 millisecond for gets in the case of Kinesis and then",
    "start": "824380",
    "end": "832060"
  },
  {
    "text": "both can do very highly durable storage and then you can scale them to any",
    "start": "832060",
    "end": "837340"
  },
  {
    "text": "extent you want and then you can they can also be you know set up in a way that they can run across multiple",
    "start": "837340",
    "end": "843640"
  },
  {
    "text": "availability zones so what are the differences in terms of the record life currently canisius supports 24 hours",
    "start": "843640",
    "end": "849640"
  },
  {
    "text": "this data that sits in there this hard data that sits in there sits there for 24 hours and after that the window moves",
    "start": "849640",
    "end": "855130"
  },
  {
    "text": "forward right you know whatever it happened 24 hours earlier most moves out you can't read that anymore and then it",
    "start": "855130",
    "end": "861010"
  },
  {
    "text": "goes moving forward and then now that that number is configurable in Kafka and then last but not the least many of the",
    "start": "861010",
    "end": "868300"
  },
  {
    "text": "customers even people who are very familiar with Kafka don't want to deal with you know managing a cluster",
    "start": "868300",
    "end": "873850"
  },
  {
    "text": "updating the software they're scaling it scaling down etc you know those functions come for you know come amazon",
    "start": "873850",
    "end": "883060"
  },
  {
    "text": "kinesis allows you to allows you to do that fairly very simply you don't have to deal with managing a cluster you",
    "start": "883060",
    "end": "888460"
  },
  {
    "text": "simply create a stream the stream gets created in a couple of minutes and then then you start putting records into it",
    "start": "888460",
    "end": "894760"
  },
  {
    "text": "if you if you need to nor capacity you simply call an API and say split the mice with the split the shard and then",
    "start": "894760",
    "end": "901570"
  },
  {
    "text": "you can get n number of shards you know to process that you know for example you",
    "start": "901570",
    "end": "906700"
  },
  {
    "text": "know I think I saw a good example of you know how well this can scale the Tudor",
    "start": "906700",
    "end": "911890"
  },
  {
    "text": "does roughly in the order of you know five thousand or so requests per second apparently you know this is the average",
    "start": "911890",
    "end": "917320"
  },
  {
    "text": "number of requests about 12 shots or so can handle that request and anyone can create these shots I mean I'm sure that",
    "start": "917320",
    "end": "923530"
  },
  {
    "text": "there's a lot of other amazing things is I just don't want to oversimplify their scenario I'm simply trying to make a",
    "start": "923530",
    "end": "929440"
  },
  {
    "text": "point that then you can even handle Twitter streams by creating roughly in the in the order of you know",
    "start": "929440",
    "end": "935630"
  },
  {
    "text": "tens of shots to be able to consume that data if you look at the cost of that you know it is pretty pretty inexpensive to",
    "start": "935630",
    "end": "942080"
  },
  {
    "text": "do that at scale as well so these technologies allow you to be able to even even process something like a",
    "start": "942080",
    "end": "948290"
  },
  {
    "text": "Twitter stream so moving along to the next next stage now what we've done is",
    "start": "948290",
    "end": "954650"
  },
  {
    "text": "we've a dangerous stage we looked at the data the three types of data and then they in transactional data file data and",
    "start": "954650",
    "end": "961310"
  },
  {
    "text": "streaming data now we looked at the stream storage technologies canisius and Kafka now let's go into the other stages",
    "start": "961310",
    "end": "967430"
  },
  {
    "text": "you know how do you deal with databases and storage and so now this is the way",
    "start": "967430",
    "end": "974720"
  },
  {
    "text": "not to build your database and storage",
    "start": "974720",
    "end": "979910"
  },
  {
    "text": "tier what I have here at the bottom tier is potentially a relational database management system and then I call this",
    "start": "979910",
    "end": "987020"
  },
  {
    "text": "the Swiss Army knife I like Swiss Army knives but I wanted to use that paradigm essentially why why not do this right it",
    "start": "987020",
    "end": "993200"
  },
  {
    "text": "turns out that let's let's say you're using the database you know you can use the database for many things you can do search you can do in search you can you",
    "start": "993200",
    "end": "999680"
  },
  {
    "text": "can run analytic cookies on this it's a fantastic engine what happens at scale let's assume you're you're doing the",
    "start": "999680",
    "end": "1006040"
  },
  {
    "text": "hundred thousand reads per second and then ten writes per second now in the case of dynamo I can simply ask for five",
    "start": "1006040",
    "end": "1013300"
  },
  {
    "text": "hundred thousand reads per second and ten writes per second only pay for that now it's going to be fairly hard to do in a relational database you need to be",
    "start": "1013300",
    "end": "1019330"
  },
  {
    "text": "scaling these things up for one specific dimension and then you know it's",
    "start": "1019330",
    "end": "1025810"
  },
  {
    "text": "synonymous to let's say if you want a big screwdriver you need to buy a big Swiss Army knife to be able to get that big screwdriver rather than getting just",
    "start": "1025810",
    "end": "1032140"
  },
  {
    "text": "a screwdriver for a few you know for a couple of dollars right that's the basic idea here know how else would you do",
    "start": "1032140",
    "end": "1038589"
  },
  {
    "text": "this you know I really think of the database and storage tier as a set of technologies there rather than thinking",
    "start": "1038589",
    "end": "1045520"
  },
  {
    "text": "of just a relational database you may want to think about this as a cash sequel no sequel search and then",
    "start": "1045520",
    "end": "1051490"
  },
  {
    "text": "complemented by s3 what blob storage you know are potentially Hadoop for file",
    "start": "1051490",
    "end": "1057010"
  },
  {
    "text": "storage etc and you know if you are using Amazon managed services obviously",
    "start": "1057010",
    "end": "1062710"
  },
  {
    "text": "we have Amazon Elastic cash with a cup engines there memcached and Redis and then you also have them is on DynamoDB",
    "start": "1062710",
    "end": "1069200"
  },
  {
    "text": "which is our fully scalable no sequel datastore RDS which is a relational data store and Amazon Cloud search which is",
    "start": "1069200",
    "end": "1077600"
  },
  {
    "text": "our search engine again you can replace you know substitute the names with your favorite sequel no sequel data stores",
    "start": "1077600",
    "end": "1083299"
  },
  {
    "text": "pretty much I think from an architectural perspective these things tend to do the same the question becomes you know how do you manage this in this",
    "start": "1083299",
    "end": "1089659"
  },
  {
    "text": "case managed services give you a leg up right you don't have to put engineers to do that you don't have to lock down one",
    "start": "1089659",
    "end": "1094909"
  },
  {
    "text": "or two engineers running your cluster rather than that they can focus on you know doing what what what makes sense for your business so you know in terms",
    "start": "1094909",
    "end": "1103580"
  },
  {
    "text": "of I get as an architect I think the one of the first things that get I get asked is well how do you how do you pick one",
    "start": "1103580",
    "end": "1108830"
  },
  {
    "text": "of them you know what do I pick and how do you how do you do that I think of this in in a couple of dimensions you",
    "start": "1108830",
    "end": "1114019"
  },
  {
    "text": "know data structure and query complexity and data data characteristics you know characterize this as hot warm and cold",
    "start": "1114019",
    "end": "1120500"
  },
  {
    "text": "that is the terminology that said I a walled as I was consulting or the last few years but let's see how it plays out",
    "start": "1120500",
    "end": "1126529"
  },
  {
    "text": "across our across the various services for a moment I'm going to pick all the Amazon tools there if I'm going to if I",
    "start": "1126529",
    "end": "1134330"
  },
  {
    "text": "talk about no sequel i'm going to pick dynamodb because i know that the best again you know you Cassandra experts and",
    "start": "1134330",
    "end": "1140029"
  },
  {
    "text": "HBase exports you know we'll probably rhyme with me you know in terms of utilizing that you know in addition to",
    "start": "1140029",
    "end": "1146539"
  },
  {
    "text": "dynamodb if you will a lot instead if you will so if you think of the data as structured and unstructured you know on",
    "start": "1146539",
    "end": "1153110"
  },
  {
    "text": "the top in the top top quadrant you know the two quadrants i would say i have",
    "start": "1153110",
    "end": "1159110"
  },
  {
    "text": "structured data and a simple query and the structured data on a complex query if you're doing it's structured data on",
    "start": "1159110",
    "end": "1164690"
  },
  {
    "text": "a simple query typically your no sequel engine you know plays very well there if",
    "start": "1164690",
    "end": "1169730"
  },
  {
    "text": "you're doing a structured data so the classic scenarios are give me a parent given a parent which is my directory",
    "start": "1169730",
    "end": "1175190"
  },
  {
    "text": "give me all the files in the directory right you know for example cloud drive may have a scenario where you know or if",
    "start": "1175190",
    "end": "1182870"
  },
  {
    "text": "you're if you're implementing an inbox for a given user give me all the give me all the emails that came in that thing",
    "start": "1182870",
    "end": "1188659"
  },
  {
    "text": "is fairly easily you know implemented using a no sequel store if you're cashing a result set again you can put",
    "start": "1188659",
    "end": "1194720"
  },
  {
    "text": "that in a in a cash like memcache shot red is and then again on the structured side if",
    "start": "1194720",
    "end": "1200519"
  },
  {
    "text": "you're running structured queries you can you can use a classic relational engine our search engine if you will on",
    "start": "1200519",
    "end": "1206519"
  },
  {
    "text": "the unstructured side again you have no query or a simple query get put amazon",
    "start": "1206519",
    "end": "1212759"
  },
  {
    "text": "s3 works fairly well nobody writes directly from you know line-of-business applications to ask blaze here which is",
    "start": "1212759",
    "end": "1219539"
  },
  {
    "text": "our started though I think of that as a cold storage then you can write a policy in s3 and say after six months move all",
    "start": "1219539",
    "end": "1225479"
  },
  {
    "text": "this terabytes of data automatically to a penny per gigabyte per month storage and then on the unstructured sighs let's",
    "start": "1225479",
    "end": "1232919"
  },
  {
    "text": "say if you want to run udfs on your data you know your data is very custom maybe it has a lot of binary blobs which none",
    "start": "1232919",
    "end": "1239129"
  },
  {
    "text": "of the you know the day the saturdays in the city Lizer ddc realises can deal with you can write your own udfs and",
    "start": "1239129",
    "end": "1245129"
  },
  {
    "text": "then stick that in Hadoop and process that as well so that's one way of thinking about this the other one is the",
    "start": "1245129",
    "end": "1250259"
  },
  {
    "text": "data temperature you know what is the temperature of your data I like that gauge on this is being sold at amazon.com so I pick the picture from",
    "start": "1250259",
    "end": "1256589"
  },
  {
    "text": "there so the data characteristics you know wat warm and cold when you're dealing with hard data typically you're",
    "start": "1256589",
    "end": "1262739"
  },
  {
    "text": "dealing with you know low small sizes of items in the order of you know bites the",
    "start": "1262739",
    "end": "1268200"
  },
  {
    "text": "kilobytes you know you want very low latency in a milliseconds microseconds your durability well if you typically",
    "start": "1268200",
    "end": "1275789"
  },
  {
    "text": "these are caches if you lose the cash you can always pull the data and wall up your cash from a durable data store typically the durability is low to high",
    "start": "1275789",
    "end": "1283109"
  },
  {
    "text": "again all these rules are being changed as well people keep multiple caches therefore you know you can you can run",
    "start": "1283109",
    "end": "1289139"
  },
  {
    "text": "caches pretty you know in a reliably in a durable way as well if you keep multiple copies of that in general in a",
    "start": "1289139",
    "end": "1295469"
  },
  {
    "text": "cost per gigabyte is pretty high again as you move towards the right you're dealing with bigger sizes of data you're",
    "start": "1295469",
    "end": "1301529"
  },
  {
    "text": "dealing with probably Layton sees it tastes taking longer you know milliseconds or seconds or even hours in the case of glacier if you put something",
    "start": "1301529",
    "end": "1307559"
  },
  {
    "text": "in and ask it you know get it back it's going to tell you come back in three and a half hours and I'll give you this data",
    "start": "1307559",
    "end": "1312690"
  },
  {
    "text": "back this is not much joking this is true and then again you're dueling d you",
    "start": "1312690",
    "end": "1318419"
  },
  {
    "text": "know you're going from you know tens of dollars per gigabyte on the left side two cents per gigabyte on the other side",
    "start": "1318419",
    "end": "1324450"
  },
  {
    "text": "so now in terms of a mental map you know this is by no means accurate but",
    "start": "1324450",
    "end": "1329610"
  },
  {
    "text": "is what I walk around with this is my mental map of how these the continuum all products play out on the left side",
    "start": "1329610",
    "end": "1335490"
  },
  {
    "text": "there is caches there's an intersection between the cash and no sequel you can use either either one for some of that",
    "start": "1335490",
    "end": "1340559"
  },
  {
    "text": "and again there's an intersection between you know RDS which is a relational database you know sequel and",
    "start": "1340559",
    "end": "1345809"
  },
  {
    "text": "no sequel which is our dynamodb and then again in terms of s3 there's an intersection between that and and the",
    "start": "1345809",
    "end": "1352049"
  },
  {
    "text": "HDFS and search there's an intersection between search and RDS right now if you",
    "start": "1352049",
    "end": "1357390"
  },
  {
    "text": "fall in the intersection you have a couple of options for for implementing that technology who are using our which one you use now really just to be",
    "start": "1357390",
    "end": "1365400"
  },
  {
    "text": "getting a little more concrete there what I done here is that you know taking our individual products like elastic cash DynamoDB RDS cloud search HDFS best",
    "start": "1365400",
    "end": "1374820"
  },
  {
    "text": "free and glacier and I've run these through all these you know various parameters now this was a hard slide to",
    "start": "1374820",
    "end": "1380190"
  },
  {
    "text": "make and I took me about two or three years to make them because I'm going to make someone unhappy by putting this",
    "start": "1380190",
    "end": "1385679"
  },
  {
    "text": "slide right so you're convincing my own marketing departments so I didn't put",
    "start": "1385679",
    "end": "1391140"
  },
  {
    "text": "precise numbers I put some you know in terms of milliseconds seconds to give you the spread because depending upon",
    "start": "1391140",
    "end": "1397049"
  },
  {
    "text": "the engine that you pick and depending upon how you do things it may vary but from a designers perspective this gives",
    "start": "1397049",
    "end": "1403200"
  },
  {
    "text": "me a slide where I can simply say look if I'm dealing with millisecond access times and I'm dealing with a simple key",
    "start": "1403200",
    "end": "1408330"
  },
  {
    "text": "value store maybe I can put that in a cash I can probably put that in in dynamo DB DB or I can go to a relational",
    "start": "1408330",
    "end": "1416100"
  },
  {
    "text": "store if I wanted to and then maybe I can stick this in s3 but you know what",
    "start": "1416100",
    "end": "1421650"
  },
  {
    "text": "is the difference of putting this in dynamo DB vs s3 maybe s3 will return this in about 100 milliseconds dynamo",
    "start": "1421650",
    "end": "1427140"
  },
  {
    "text": "will return this and three right maybe the cost if you compute the cost s 3 is 3 cents per gigabyte per month whereas",
    "start": "1427140",
    "end": "1434160"
  },
  {
    "text": "dynamo is around 12 cents i believe last time I computer to twelve cents per gigabyte per month so it really depends",
    "start": "1434160",
    "end": "1439650"
  },
  {
    "text": "you know what are your needs there and how do you intersect anytime I I do the computation based on these parameters",
    "start": "1439650",
    "end": "1445980"
  },
  {
    "text": "and the cost typically I do design reviews which lasts for an hour about 20 minutes into the conversation I'm going",
    "start": "1445980",
    "end": "1453150"
  },
  {
    "text": "to there's a tool called it'll be a simple calculator if you type it in your favorite search engine it'll get there if you plug it in parameters it'll give",
    "start": "1453150",
    "end": "1459210"
  },
  {
    "text": "you a dollar bill how much is your rate per month right that usually guides my designs because at that point the",
    "start": "1459210",
    "end": "1465419"
  },
  {
    "text": "customer is saying look we're good to go let's keep going or we're throwing up and saying well this will break the bank",
    "start": "1465419",
    "end": "1470700"
  },
  {
    "text": "let's not do this are we change our requirements or we go tell the upstream teams to say well that's too expensive",
    "start": "1470700",
    "end": "1476490"
  },
  {
    "text": "to do usually they are pleasantly surprised especially if you use the right tool in many cases as you run",
    "start": "1476490",
    "end": "1483450"
  },
  {
    "text": "these numbers you'll find out when we build you'll find out one tool you know that's fairly well compared to the other",
    "start": "1483450",
    "end": "1489149"
  },
  {
    "text": "when we build technologies at Amazon we try to build them we try to build them",
    "start": "1489149",
    "end": "1494399"
  },
  {
    "text": "simply you know we want to make this very simple and we want them to do a few things very very well and we price them",
    "start": "1494399",
    "end": "1500250"
  },
  {
    "text": "in a way that makes a lot of sense if customers use that the right way typically when you try to use it the",
    "start": "1500250",
    "end": "1505379"
  },
  {
    "text": "wrong way you know the cost increases tremendously so that you know has come in very handy for me to guide my designs",
    "start": "1505379",
    "end": "1512279"
  },
  {
    "text": "on AWS so well I'm I came up with a you",
    "start": "1512279",
    "end": "1518789"
  },
  {
    "text": "know sort of an example it's a video streaming app think YouTube you're building youtube or something similar to",
    "start": "1518789",
    "end": "1524159"
  },
  {
    "text": "that you know how would that look like if you build using these ideas right you have an application tier let's assume",
    "start": "1524159",
    "end": "1530429"
  },
  {
    "text": "somebody's loading video into this it's sending it to the application the application is getting the metadata out of that it's sticking the video in",
    "start": "1530429",
    "end": "1536879"
  },
  {
    "text": "Amazon s3 and then it's taking the metadata and putting that in DynamoDB and then it is also constructing a",
    "start": "1536879",
    "end": "1543509"
  },
  {
    "text": "search document you want to parley search these videos using the title and description etc and putting that in in in in cloud search and then let's say if",
    "start": "1543509",
    "end": "1552120"
  },
  {
    "text": "you have any permissions and other stuff that is you know mostly amiable for sequel type quitting you put that in in",
    "start": "1552120",
    "end": "1557220"
  },
  {
    "text": "in RDS which is out of the sequel store on the way back how does this play out let's say if somebody is looking for",
    "start": "1557220",
    "end": "1563490"
  },
  {
    "text": "that specific video you go ask to search to put to get the search document back it has the ID of the video that you want",
    "start": "1563490",
    "end": "1570629"
  },
  {
    "text": "to get you get the you know I'm looking in the cash here maybe it's a popular video it can be it is cached in this",
    "start": "1570629",
    "end": "1575970"
  },
  {
    "text": "case it turns out it's not a popular video it's a video of my kids and then then you're getting this from from",
    "start": "1575970",
    "end": "1583259"
  },
  {
    "text": "DynamoDB to the source and it basically gives you the metadata to fetch the record and this is going to the browser",
    "start": "1583259",
    "end": "1588720"
  },
  {
    "text": "and then we have a cloud from distribution that is set up on top of s3 that can actually you know stream the",
    "start": "1588720",
    "end": "1595200"
  },
  {
    "text": "data upon where I where my location is that so basically it should feel now last",
    "start": "1595200",
    "end": "1601370"
  },
  {
    "text": "yesterday I was showing it to the I worked very closely with the cloud drive team you know if you if you if you heard",
    "start": "1601370",
    "end": "1607370"
  },
  {
    "text": "of amazon prime pictures we can be allow you to if you're a prime member allow you to store as much video and pictures",
    "start": "1607370",
    "end": "1612890"
  },
  {
    "text": "as you want about yourself on amazon.com you know we built they built a system he looked at this presentation I said hey",
    "start": "1612890",
    "end": "1618950"
  },
  {
    "text": "how do you handle can you also tell your audience how do you handle you know eventual consistency here in fact the",
    "start": "1618950",
    "end": "1626060"
  },
  {
    "text": "next talk touches on that the cloud drive team touches on that piece I don't have to time to cover that if you are",
    "start": "1626060",
    "end": "1631130"
  },
  {
    "text": "interested in it come talk to me after this but you know one thing that you have to manage in this kind of scenario is you want to also have an",
    "start": "1631130",
    "end": "1637730"
  },
  {
    "text": "authoritative store the stores consistent data because other data for example when we are sending the search document that may be slightly behind",
    "start": "1637730",
    "end": "1643940"
  },
  {
    "text": "that maybe eventually consistent at that point you just want to highlight that as well now what we've done so far is we",
    "start": "1643940",
    "end": "1649280"
  },
  {
    "text": "looked at the India's face we looked at the storage space and then we've also seen what technology did you use based on you know the temperature of the data",
    "start": "1649280",
    "end": "1656000"
  },
  {
    "text": "the structure of the data etc now let's go into the processing stage right well processing is about you know think",
    "start": "1656000",
    "end": "1661400"
  },
  {
    "text": "analytics you know your batch processing or you know if you you may be running some machine learning algorithms as well",
    "start": "1661400",
    "end": "1668060"
  },
  {
    "text": "you know I'm including all this into into the processing phase then you know",
    "start": "1668060",
    "end": "1673940"
  },
  {
    "text": "in general we're generalizing this very wildly you know this may be slightly off for some of you but in general the",
    "start": "1673940",
    "end": "1679700"
  },
  {
    "text": "processing frameworks come in two categories you know batch processing frameworks and then stream processing frameworks some people are calling",
    "start": "1679700",
    "end": "1685940"
  },
  {
    "text": "stream processing micro batching as well if you're using spark streaming it simply construct a small micro batch in",
    "start": "1685940",
    "end": "1691850"
  },
  {
    "text": "terms of these streams and it processes that we also did a lab using you know how can you do micro batching with with",
    "start": "1691850",
    "end": "1699440"
  },
  {
    "text": "data going from Kinesis to amazon redshift as well so if you hear that",
    "start": "1699440",
    "end": "1705620"
  },
  {
    "text": "term it's what we really what people are really saying is it's a stream processing technology in terms of batch processing you typically take a large",
    "start": "1705620",
    "end": "1710870"
  },
  {
    "text": "amount of data and you run a Hadoop job on this or if you actually run a sequel query on this group buys an order bias",
    "start": "1710870",
    "end": "1717770"
  },
  {
    "text": "and you basically process the data it may take city anywhere in the order of you know minutes or an hour sometimes",
    "start": "1717770",
    "end": "1724340"
  },
  {
    "text": "even to process the data you know how that how would that look like let's say if you're building a recommendation",
    "start": "1724340",
    "end": "1729620"
  },
  {
    "text": "in application for the scenario that video streaming scenario we talked about your app servers are you know sending",
    "start": "1729620",
    "end": "1735050"
  },
  {
    "text": "hourly logs click stream logs to s3 and then you have this product called i'm as",
    "start": "1735050",
    "end": "1741050"
  },
  {
    "text": "AWS data pipeline i think of that as as a product that that handles ETL it wakes",
    "start": "1741050",
    "end": "1747230"
  },
  {
    "text": "up periodically never you can you can program it to basically wake up every 15 minutes and then move the data take the",
    "start": "1747230",
    "end": "1755090"
  },
  {
    "text": "data from a single storage tier process it and putting it another storage tier if you and you know in this case i'm",
    "start": "1755090",
    "end": "1761990"
  },
  {
    "text": "starting a data pipeline I've played a data pipeline template which will do this processing every 15 minutes",
    "start": "1761990",
    "end": "1767679"
  },
  {
    "text": "starting that thing see they're getting the icon rotating was something I had to learn so and then it takes the data from",
    "start": "1767679",
    "end": "1774830"
  },
  {
    "text": "s3 and then takes sends it to the DA do cluster in this case I was on EMR then",
    "start": "1774830",
    "end": "1780350"
  },
  {
    "text": "we process the data there and we take the data and put that in dynamo dB you",
    "start": "1780350",
    "end": "1785570"
  },
  {
    "text": "know this may be patterns such as you know people who watch this video watch these other videos as well all this information is in your click stream and",
    "start": "1785570",
    "end": "1791090"
  },
  {
    "text": "what this processing is doing is assembling this piece and putting that using a hash range scheme on down what",
    "start": "1791090",
    "end": "1796460"
  },
  {
    "text": "DB the hash being the first video that somebody watched and the range key being the other videos that somebody was so",
    "start": "1796460",
    "end": "1802070"
  },
  {
    "text": "you could do a simple query saying for a specific hash gay video give me all the list of the videos that people watch they can be materialized so when this",
    "start": "1802070",
    "end": "1807860"
  },
  {
    "text": "person is going to the website if you're searching for a specific video you may want to prompt them for looking at it's pretty simple to do this right now I've",
    "start": "1807860",
    "end": "1815510"
  },
  {
    "text": "given a similar example using if you're running batch analytics you know you can put the data in s3 similarly run a data",
    "start": "1815510",
    "end": "1822260"
  },
  {
    "text": "pipeline job take the data and process that on EMR and then take that and put",
    "start": "1822260",
    "end": "1827630"
  },
  {
    "text": "the data in red shift as well you know if you if you if you like sequel processing you can do the hint are",
    "start": "1827630",
    "end": "1833480"
  },
  {
    "text": "processing using Amazon direct ship directly as well so i just wanted to paint a few pictures that i'll give you a sense of how this one plays out in",
    "start": "1833480",
    "end": "1839840"
  },
  {
    "text": "real life in terms of stream processing we typically take small amounts of data and ask questions it usually takes a",
    "start": "1839840",
    "end": "1846260"
  },
  {
    "text": "short amount of time to get your answers back typically you know think of one minute metrics and the alerting scenarios that we talked about so what",
    "start": "1846260",
    "end": "1853610"
  },
  {
    "text": "processing tools should you use in that case in the case of batch processing you know typically it falls into a couple of",
    "start": "1853610",
    "end": "1859670"
  },
  {
    "text": "categories for me there is really the npp engines and the Hadoop based engines there for",
    "start": "1859670",
    "end": "1864920"
  },
  {
    "text": "example Amazon redshift is an MPP data warehouse MPP stands for massive parallel processing where you have the",
    "start": "1864920",
    "end": "1870440"
  },
  {
    "text": "notion of a master node and multiple slave nodes and your data is distributed by the by the distribution key across",
    "start": "1870440",
    "end": "1875900"
  },
  {
    "text": "all the nodes and then it can basically run your query purely across all the nodes and get the answers back now the",
    "start": "1875900",
    "end": "1881510"
  },
  {
    "text": "open source MPP engines are pressed to I pulled a couple of very popular ones press to an Impala netflix users press",
    "start": "1881510",
    "end": "1888770"
  },
  {
    "text": "to in a big way I think they're touching that in the in the in the discussion of the if you I don't know the talk is over or not but I think they're using that",
    "start": "1888770",
    "end": "1895280"
  },
  {
    "text": "Facebook uses pressed to the netflix is doing a lot of work and presto I'm sure you'll see a lot of blogs there's a",
    "start": "1895280",
    "end": "1900470"
  },
  {
    "text": "block there on how the why they use presto etc essentially boils down to they wanted something to to also process",
    "start": "1900470",
    "end": "1906950"
  },
  {
    "text": "data from s3 they want to keep s3 as the primary data store and presto works really well they're in the clutter",
    "start": "1906950",
    "end": "1912169"
  },
  {
    "text": "Impala they've done tremendous work on actually you know building a fantastic MPP engine there so it works on HDFS and",
    "start": "1912169",
    "end": "1919700"
  },
  {
    "text": "then on the right side i put Amazon Elastic MapReduce because you know you",
    "start": "1919700",
    "end": "1925220"
  },
  {
    "text": "can probably run you know cloudera stack there are you can run elastic MapReduce comes with a couple of Hadoop",
    "start": "1925220",
    "end": "1931309"
  },
  {
    "text": "distributions one is Amazon's on a dupe distribution out on distribution and then the other one is you know nap or so",
    "start": "1931309",
    "end": "1937790"
  },
  {
    "text": "you can run any Hadoop distribution of your choice and then you can bootstrap these engines on top of you can run what",
    "start": "1937790",
    "end": "1943790"
  },
  {
    "text": "is called a bootstrap action to bootstrap these Indians on top of elastic MapReduce elastic MapReduce takes PRF over if a node goes down it",
    "start": "1943790",
    "end": "1950570"
  },
  {
    "text": "automatically brings up the other node and the engines are smart enough to basically recreate the data set that they lost right and in case of stream",
    "start": "1950570",
    "end": "1957290"
  },
  {
    "text": "processing apache spark spark streaming Strom and pieces client library or some",
    "start": "1957290",
    "end": "1963380"
  },
  {
    "text": "of the technologies to use there so I just I just want to refer to this thing",
    "start": "1963380",
    "end": "1968750"
  },
  {
    "text": "there's one point I want to make out of this slide is that there's an amp lab Big Data benchmark that that has",
    "start": "1968750",
    "end": "1974840"
  },
  {
    "text": "actually tried to compare all these technologies the macro point there is high you know Tess is another think of",
    "start": "1974840",
    "end": "1982910"
  },
  {
    "text": "that as another processing engine like mapreduce you know that's a new technology that you know that that that",
    "start": "1982910",
    "end": "1988610"
  },
  {
    "text": "speeds up hive you know it runs fifty percent fat you know it could ease on if the query done in 10 minutes it runs in",
    "start": "1988610",
    "end": "1994730"
  },
  {
    "text": "5 minutes right so it comes pretty fast with instead of using high with classic MapReduce high",
    "start": "1994730",
    "end": "2000460"
  },
  {
    "text": "with tests really speeds up processing as well but if you look at the other side redshift really does fairly well",
    "start": "2000460",
    "end": "2006249"
  },
  {
    "text": "for any kind of equity whether it be a scan period aggregate query or a join query and then in the case of Impala you",
    "start": "2006249",
    "end": "2012970"
  },
  {
    "text": "know depending upon in memory if you're using in-memory processing or on disk processing the Layton sees in the case",
    "start": "2012970",
    "end": "2018190"
  },
  {
    "text": "of memory are lower and then disk is slightly higher now these numbers are already outdated those numbers will be",
    "start": "2018190",
    "end": "2024220"
  },
  {
    "text": "significantly better right now but I just wanted to make that point you know",
    "start": "2024220",
    "end": "2029529"
  },
  {
    "text": "if you're interested go to that site and check out the details there if I tabulate all of these pieces in terms of",
    "start": "2029529",
    "end": "2036210"
  },
  {
    "text": "the query latency you know it's probably one one one dimension that",
    "start": "2036210",
    "end": "2041950"
  },
  {
    "text": "differentiates them right if you for example redshift Impala and press to a",
    "start": "2041950",
    "end": "2047019"
  },
  {
    "text": "very low poly latency spark load a medium and a high of anywhere from medium to high and then in terms of",
    "start": "2047019",
    "end": "2052599"
  },
  {
    "text": "durability all of them give you fairly high durability I want to think of redshift we keep multiple copies on the",
    "start": "2052599",
    "end": "2058240"
  },
  {
    "text": "cluster as well as we push that to s3 so it is I want to think of that as durability is being super high there and",
    "start": "2058240",
    "end": "2064030"
  },
  {
    "text": "then the max 1.6 max petabytes you know I think I don't know if you notice that",
    "start": "2064030",
    "end": "2069549"
  },
  {
    "text": "in the keynote yesterday NTT DoCoMo is already using a cluster with 44 petabytes you know that was a suggested",
    "start": "2069549",
    "end": "2075669"
  },
  {
    "text": "max there so we can do much more than that way in red shift if you reach out to us so literally I think I just wanted",
    "start": "2075669",
    "end": "2083169"
  },
  {
    "text": "to the other piece to note here is the storage engines right in the case of Impala they work only on HDFS in the",
    "start": "2083169",
    "end": "2088658"
  },
  {
    "text": "case of presto works with HDFS and s3 spark works with HDFS as well as just three and the same with high so the BI",
    "start": "2088659",
    "end": "2096878"
  },
  {
    "text": "tools you know high and sequel based engines have better bi tools support",
    "start": "2096879",
    "end": "2102280"
  },
  {
    "text": "that's another dimension that you want to look at in case a stream processing again you can scale steam processing",
    "start": "2102280",
    "end": "2107589"
  },
  {
    "text": "across any number of nodes these things can handle any amount of throughput the differentiator there is really you know",
    "start": "2107589",
    "end": "2113890"
  },
  {
    "text": "how much of this is in terms of manageability in this case you know each one of the tools you know is fairly",
    "start": "2113890",
    "end": "2119799"
  },
  {
    "text": "equivalent in my mind and then you know for example you can you can bootstrap",
    "start": "2119799",
    "end": "2125160"
  },
  {
    "text": "spark streaming on top of elastic produce or in the case of strong and",
    "start": "2125160",
    "end": "2130400"
  },
  {
    "text": "trident you'll have to kind of manage your cluster yourself in the case of canisius client library you know if you",
    "start": "2130400",
    "end": "2136220"
  },
  {
    "text": "we could we give you you can use ec2 and auto scaling to be able and check pointing to be able to kind of have this",
    "start": "2136220",
    "end": "2142250"
  },
  {
    "text": "application self manage itself in terms of programming languages you know Python and Java seems to have the best support",
    "start": "2142250",
    "end": "2148730"
  },
  {
    "text": "while you know we're building support for other you know every vendor is building support for other technologies",
    "start": "2148730",
    "end": "2153830"
  },
  {
    "text": "other other programming languages as well so if you put this all together you know all of this data the visualization",
    "start": "2153830",
    "end": "2159620"
  },
  {
    "text": "tier again a word of caution here those pieces you know tableau click SAS is are",
    "start": "2159620",
    "end": "2165020"
  },
  {
    "text": "very very comprehensive platforms they do a lot more than visualization I really think of them as a platform for",
    "start": "2165020",
    "end": "2170450"
  },
  {
    "text": "bi and apps so the point I'm trying to make here is that both your data that",
    "start": "2170450",
    "end": "2175490"
  },
  {
    "text": "you're storing in the storage layer and the processing layer can be used by these layers to materialize materialize",
    "start": "2175490",
    "end": "2180920"
  },
  {
    "text": "the data set now putting it all together multistage you want to think of your pipeline as a multi stage pipeline",
    "start": "2180920",
    "end": "2186950"
  },
  {
    "text": "comprising a processed or processed or and and I really the point I'm trying to",
    "start": "2186950",
    "end": "2192320"
  },
  {
    "text": "make is a storage tier putting a storage tier in between two processing tears decouples your application and the other",
    "start": "2192320",
    "end": "2198950"
  },
  {
    "text": "piece is that you know processing application let's say if you're if you're doing stream processing you know things like canisius and Kafka clients",
    "start": "2198950",
    "end": "2205490"
  },
  {
    "text": "in the case of canisius let's say we have this canisius connectors which can take the stream coming in and then put",
    "start": "2205490",
    "end": "2211730"
  },
  {
    "text": "that in multiple data source in this case I'm painting an example where the canisius connector for dynamo DB is",
    "start": "2211730",
    "end": "2216830"
  },
  {
    "text": "putting that in dynamo DB you have another canisius client replica you know canisius connector application you know",
    "start": "2216830",
    "end": "2222590"
  },
  {
    "text": "think of this as two separate stacks running continuously moving the data to do different higher tiers and this",
    "start": "2222590",
    "end": "2227690"
  },
  {
    "text": "happens can happen in parallel and then processing frameworks that she has from",
    "start": "2227690",
    "end": "2233210"
  },
  {
    "text": "high even spark now if you if you're if the beauty of actually keeping the storage tier separately and the",
    "start": "2233210",
    "end": "2238820"
  },
  {
    "text": "processing at theater separately is in many cases let's say hive can talk to both s3 and dynamo DB and construct a",
    "start": "2238820",
    "end": "2245000"
  },
  {
    "text": "resultset that's a combination of both maybe historic data is kept in s3 maybe the hard data is kept in dynamo DB if",
    "start": "2245000",
    "end": "2251060"
  },
  {
    "text": "you want to create a view using high separating this out gives you the ability to create a view across across",
    "start": "2251060",
    "end": "2257240"
  },
  {
    "text": "these various storage tiers now our video tech told me that the blue on",
    "start": "2257240",
    "end": "2263370"
  },
  {
    "text": "black is the worst combination looks like you're able to see this so the basic idea here is that you know just to",
    "start": "2263370",
    "end": "2268680"
  },
  {
    "text": "go back in a reference architecture your apps are writing to multiple data they multiple data stores which is right",
    "start": "2268680",
    "end": "2275850"
  },
  {
    "text": "there your stream data is coming in on the left side and is getting sent to multiple data stores your processing tier is processing multiple data stores",
    "start": "2275850",
    "end": "2282480"
  },
  {
    "text": "here and then your bi and visualization layer is being fed from both the data stores as well as these processing tears",
    "start": "2282480",
    "end": "2289020"
  },
  {
    "text": "I think I'm not going to get to go into each one of these pieces I try to assemble some of those technologies that make sense in each one of the area more",
    "start": "2289020",
    "end": "2294960"
  },
  {
    "text": "importantly I want to go into some of the design patterns and then in terms of",
    "start": "2294960",
    "end": "2300330"
  },
  {
    "text": "the you know what I'm trying to take do here is to combine the notion of the data temperature and query latency so if",
    "start": "2300330",
    "end": "2306720"
  },
  {
    "text": "you if you look at the data coming in data goes into multiple tiers here you",
    "start": "2306720",
    "end": "2312180"
  },
  {
    "text": "know either going to the Kinesis our Kafka being and then connectors sending them to various tiers and then you know",
    "start": "2312180",
    "end": "2317670"
  },
  {
    "text": "in this area these are really is processing technologies right for example you can do stream processing",
    "start": "2317670",
    "end": "2323130"
  },
  {
    "text": "managing your hard data to get you know real-time insights here or you can actually take the data you can run a",
    "start": "2323130",
    "end": "2328470"
  },
  {
    "text": "native dynamo DB query to get answers in milliseconds or you can take the data to read shift and run some batch processing",
    "start": "2328470",
    "end": "2335430"
  },
  {
    "text": "on that so if you actually move forward let's say if you ought to build real time processing systems the one of the",
    "start": "2335430",
    "end": "2341490"
  },
  {
    "text": "ways of building that would be potentially using here spark streaming putting the data and canisius using",
    "start": "2341490",
    "end": "2347640"
  },
  {
    "text": "sports streaming strong or the native client to do that or the application can directly write to dynamo DB and have a",
    "start": "2347640",
    "end": "2353970"
  },
  {
    "text": "native client actually do this processing as well now if you look up",
    "start": "2353970",
    "end": "2359250"
  },
  {
    "text": "batch processing again you can move the data to s3 and then using Amazon redshift or spark or pressed or hive to",
    "start": "2359250",
    "end": "2366390"
  },
  {
    "text": "run your batch queries on top of on top of your data in s3 so in terms of",
    "start": "2366390",
    "end": "2374760"
  },
  {
    "text": "interactive analytics let's say if you have you know somebody sitting in front of a tableau and then you know slicing",
    "start": "2374760",
    "end": "2380130"
  },
  {
    "text": "and dicing the data potentially you may want to put the data and s3 copy that over to red shift or put that in spore",
    "start": "2380130",
    "end": "2386220"
  },
  {
    "text": "or presto and have interactive analytics going on there so putting this similarly know if you",
    "start": "2386220",
    "end": "2393160"
  },
  {
    "text": "want to use let's say a claudia impala you may want to take a stream and put",
    "start": "2393160",
    "end": "2398319"
  },
  {
    "text": "that in HDFS and do that now you can you know all these technologies allow you to do this in parallel so this gives you a",
    "start": "2398319",
    "end": "2403510"
  },
  {
    "text": "combination of mixing and matching various different technologies again you want to put the data in a storage tier",
    "start": "2403510",
    "end": "2409690"
  },
  {
    "text": "based on the characteristic of your data if it's a warm data or a hot data you want to email sit in caches in elastic",
    "start": "2409690",
    "end": "2415539"
  },
  {
    "text": "search for example is a great place to put some of your search kind of data in a warm in a hot store they process you",
    "start": "2415539",
    "end": "2421930"
  },
  {
    "text": "know tons of requests per second and the latent load the load latency is very low for access as well so I'm trying to",
    "start": "2421930",
    "end": "2428109"
  },
  {
    "text": "paint the difference between you know picking the right storage tier right processing tier you know based on your",
    "start": "2428109",
    "end": "2433480"
  },
  {
    "text": "needs so to summarize you know this may",
    "start": "2433480",
    "end": "2439539"
  },
  {
    "text": "be how it big at a reference architecture reference architecture is a big word I think you know I think I did",
    "start": "2439539",
    "end": "2447549"
  },
  {
    "text": "it for me by working with many customers this is how it played out fairly well for me using this design pattern in my",
    "start": "2447549",
    "end": "2453880"
  },
  {
    "text": "head and then to summarize your data that you ingest and then just here comes",
    "start": "2453880",
    "end": "2459010"
  },
  {
    "text": "in three different shapes broadly speaking transactional data file data and stream data if you're doing",
    "start": "2459010",
    "end": "2465190"
  },
  {
    "text": "transactional data think of your data store as not just a relational engine but a combination of various technologies such as sequel no sequel",
    "start": "2465190",
    "end": "2471640"
  },
  {
    "text": "search etc if you're dealing with file data coming in yes 3 by 4 is the best",
    "start": "2471640",
    "end": "2476740"
  },
  {
    "text": "tour to put in I need to find another reason why as 31 work to be able to put it elsewhere and you can think of your",
    "start": "2476740",
    "end": "2482230"
  },
  {
    "text": "storage tier as a tiered storage rather than paying three cents per gigabyte you may want to run a policy to move that to",
    "start": "2482230",
    "end": "2487480"
  },
  {
    "text": "a penny per gigabyte per month using glacier and then in the case of a stream storage something like Kinesis our Kafka",
    "start": "2487480",
    "end": "2494500"
  },
  {
    "text": "works fairly well in the case of processing tier obviously we have Amazon Elastic MapReduce running various",
    "start": "2494500",
    "end": "2500950"
  },
  {
    "text": "flavors of MPP + Hadoop engines as well as stream processing technologies such",
    "start": "2500950",
    "end": "2506619"
  },
  {
    "text": "as stream Apaches Strom and kinases the consumer client libraries so redshift",
    "start": "2506619",
    "end": "2515230"
  },
  {
    "text": "obviously is awesome at processing some of the MPP style processing and then",
    "start": "2515230",
    "end": "2520299"
  },
  {
    "text": "really want to think of the earth storage and processing tier as a nitrated phase you know there's a loop there that",
    "start": "2520299",
    "end": "2525700"
  },
  {
    "text": "happens to actually slight eunos license shape the data and the way you want that's so that you can visual the downstream visualization can happen so",
    "start": "2525700",
    "end": "2532180"
  },
  {
    "text": "in summary you know big data processing can be simplified using probably in terms of multiple stages thinking that",
    "start": "2532180",
    "end": "2538720"
  },
  {
    "text": "as an ingest or process and visualize you know use the right tool for the job in each tier you may want to think think",
    "start": "2538720",
    "end": "2544960"
  },
  {
    "text": "of you know the rights to the right store for transactional data file data and stream data and then when you think",
    "start": "2544960",
    "end": "2550750"
  },
  {
    "text": "about processing think of your curry latency as one of the lead one of the leading characteristics and then obviously we looked at the big data",
    "start": "2550750",
    "end": "2557380"
  },
  {
    "text": "reference architectures and design patterns now I think that is the end of my presentation thank you very much for",
    "start": "2557380",
    "end": "2563680"
  },
  {
    "text": "your time I'm here to take any questions either here or offline",
    "start": "2563680",
    "end": "2569099"
  }
]