[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "as customers",
    "start": "2480",
    "end": "3439"
  },
  {
    "text": "like the alexa team know firsthand as",
    "start": "3439",
    "end": "5839"
  },
  {
    "text": "machine learning models become more",
    "start": "5839",
    "end": "7359"
  },
  {
    "text": "capable",
    "start": "7359",
    "end": "8000"
  },
  {
    "text": "they increase in size and complexity",
    "start": "8000",
    "end": "10400"
  },
  {
    "text": "driving up the compute costs for",
    "start": "10400",
    "end": "11920"
  },
  {
    "text": "inference",
    "start": "11920",
    "end": "12719"
  },
  {
    "text": "we saw an opportunity to help customers",
    "start": "12719",
    "end": "14719"
  },
  {
    "text": "reduce costs by developing our own aws",
    "start": "14719",
    "end": "17600"
  },
  {
    "text": "inferential chips and ec2 info one",
    "start": "17600",
    "end": "20320"
  },
  {
    "text": "instances",
    "start": "20320",
    "end": "21279"
  },
  {
    "text": "custom built for high performance",
    "start": "21279",
    "end": "22960"
  },
  {
    "text": "machine learning inference",
    "start": "22960",
    "end": "24720"
  },
  {
    "text": "alexa what's my flash briefing today",
    "start": "24720",
    "end": "28400"
  },
  {
    "start": "28000",
    "end": "39000"
  },
  {
    "text": "the amazon alexa team migrated a",
    "start": "28400",
    "end": "30400"
  },
  {
    "text": "majority of their gpu based machine",
    "start": "30400",
    "end": "32640"
  },
  {
    "text": "learning inference workloads to amazon",
    "start": "32640",
    "end": "34640"
  },
  {
    "text": "ec2 in one instances powered by aws",
    "start": "34640",
    "end": "37760"
  },
  {
    "text": "inferentia",
    "start": "37760",
    "end": "39120"
  },
  {
    "start": "39000",
    "end": "95000"
  },
  {
    "text": "for us at alexa we do billions of",
    "start": "39120",
    "end": "41360"
  },
  {
    "text": "inferences every week",
    "start": "41360",
    "end": "42879"
  },
  {
    "text": "so we are always looking for ways to",
    "start": "42879",
    "end": "44800"
  },
  {
    "text": "improve the end user experience",
    "start": "44800",
    "end": "46879"
  },
  {
    "text": "boost performance and reduce cost we see",
    "start": "46879",
    "end": "49760"
  },
  {
    "text": "inference comprising up to 90 percent of",
    "start": "49760",
    "end": "52079"
  },
  {
    "text": "the infrastructure spend",
    "start": "52079",
    "end": "53680"
  },
  {
    "text": "for developing and running most machine",
    "start": "53680",
    "end": "55600"
  },
  {
    "text": "learning applications",
    "start": "55600",
    "end": "56960"
  },
  {
    "text": "many of our customers who run diverse",
    "start": "56960",
    "end": "59039"
  },
  {
    "text": "machine learning workloads on aws",
    "start": "59039",
    "end": "61440"
  },
  {
    "text": "inferential based info on instances such",
    "start": "61440",
    "end": "64158"
  },
  {
    "text": "as computer vision",
    "start": "64159",
    "end": "65439"
  },
  {
    "text": "natural language processing and voice",
    "start": "65439",
    "end": "67920"
  },
  {
    "text": "are saving up to 45 percent when",
    "start": "67920",
    "end": "70400"
  },
  {
    "text": "measured against gpu-based instances",
    "start": "70400",
    "end": "72720"
  },
  {
    "text": "as of today we've got 80 percent of the",
    "start": "72720",
    "end": "75360"
  },
  {
    "text": "alexa voice responses",
    "start": "75360",
    "end": "77119"
  },
  {
    "text": "synthesized using the aws inferential",
    "start": "77119",
    "end": "79759"
  },
  {
    "text": "based",
    "start": "79759",
    "end": "80240"
  },
  {
    "text": "inf1 instances we are seeing a 30",
    "start": "80240",
    "end": "83520"
  },
  {
    "text": "cost savings over gpu based instances in",
    "start": "83520",
    "end": "86080"
  },
  {
    "text": "terms of performance",
    "start": "86080",
    "end": "87439"
  },
  {
    "text": "many customers using amazon ec2 info one",
    "start": "87439",
    "end": "90320"
  },
  {
    "text": "instances",
    "start": "90320",
    "end": "91280"
  },
  {
    "text": "see significantly lower latency and a 30",
    "start": "91280",
    "end": "94400"
  },
  {
    "text": "increase in throughput on the alexa",
    "start": "94400",
    "end": "96479"
  },
  {
    "start": "95000",
    "end": "115000"
  },
  {
    "text": "text-to-speech team",
    "start": "96479",
    "end": "97920"
  },
  {
    "text": "we have been happy about this",
    "start": "97920",
    "end": "99200"
  },
  {
    "text": "performance improvement and have seen a",
    "start": "99200",
    "end": "101600"
  },
  {
    "text": "25",
    "start": "101600",
    "end": "102640"
  },
  {
    "text": "reduction in latency compared to gpu",
    "start": "102640",
    "end": "104880"
  },
  {
    "text": "based instances",
    "start": "104880",
    "end": "106000"
  },
  {
    "text": "this has been a critical improvement for",
    "start": "106000",
    "end": "107840"
  },
  {
    "text": "us as it results in faster alexa voice",
    "start": "107840",
    "end": "110640"
  },
  {
    "text": "responses",
    "start": "110640",
    "end": "111680"
  },
  {
    "text": "and enables us to innovate further on",
    "start": "111680",
    "end": "114159"
  },
  {
    "text": "alexa's voice",
    "start": "114159",
    "end": "115280"
  },
  {
    "text": "as the lowest cost machine learning",
    "start": "115280",
    "end": "117040"
  },
  {
    "text": "inference in the cloud",
    "start": "117040",
    "end": "118640"
  },
  {
    "text": "anyone with high performance machine",
    "start": "118640",
    "end": "120399"
  },
  {
    "text": "learning workloads like video analytics",
    "start": "120399",
    "end": "122799"
  },
  {
    "text": "search recommendation engines and fraud",
    "start": "122799",
    "end": "125200"
  },
  {
    "text": "detection",
    "start": "125200",
    "end": "126000"
  },
  {
    "text": "should check out these amazon ec2",
    "start": "126000",
    "end": "128080"
  },
  {
    "text": "infoint instances",
    "start": "128080",
    "end": "129520"
  },
  {
    "text": "these instances are very easy to deploy",
    "start": "129520",
    "end": "132160"
  },
  {
    "text": "and it was straightforward for us to",
    "start": "132160",
    "end": "134000"
  },
  {
    "text": "migrate our text-to-speech models",
    "start": "134000",
    "end": "136160"
  },
  {
    "text": "to run on aws inferential-based",
    "start": "136160",
    "end": "138800"
  },
  {
    "text": "informant",
    "start": "138800",
    "end": "139360"
  },
  {
    "text": "instances because the neuron software",
    "start": "139360",
    "end": "141840"
  },
  {
    "text": "development kit is integrated with",
    "start": "141840",
    "end": "143680"
  },
  {
    "text": "common machine learning frameworks",
    "start": "143680",
    "end": "145760"
  },
  {
    "text": "like tensorflow pytorch and mxnet",
    "start": "145760",
    "end": "149120"
  },
  {
    "text": "we're excited to see what customers will",
    "start": "149120",
    "end": "151040"
  },
  {
    "text": "innovate next with our aws",
    "start": "151040",
    "end": "153040"
  },
  {
    "text": "inferential based inf1 instances built",
    "start": "153040",
    "end": "156000"
  },
  {
    "text": "specifically for high performance",
    "start": "156000",
    "end": "157680"
  },
  {
    "text": "machine learning inference",
    "start": "157680",
    "end": "159040"
  },
  {
    "text": "now that we've made this big improvement",
    "start": "159040",
    "end": "160879"
  },
  {
    "text": "alexa what should i do now",
    "start": "160879",
    "end": "162879"
  },
  {
    "text": "hmm i have lots of ideas to fill your",
    "start": "162879",
    "end": "165200"
  },
  {
    "text": "day let's narrow them down",
    "start": "165200",
    "end": "171840"
  }
]