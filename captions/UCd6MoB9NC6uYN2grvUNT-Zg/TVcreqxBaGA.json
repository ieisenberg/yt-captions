[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "text": "hi my name is mohit medha",
    "start": "2320",
    "end": "4640"
  },
  {
    "text": "i'm a senior consultant with aws",
    "start": "4640",
    "end": "6319"
  },
  {
    "text": "professional services",
    "start": "6319",
    "end": "8559"
  },
  {
    "text": "today i'm going to talk about querying",
    "start": "8559",
    "end": "10559"
  },
  {
    "text": "apache hoodie data sets",
    "start": "10559",
    "end": "12000"
  },
  {
    "text": "using amazon athena",
    "start": "12000",
    "end": "17920"
  },
  {
    "text": "i'll start by explaining what apache",
    "start": "18240",
    "end": "20080"
  },
  {
    "text": "houdi does",
    "start": "20080",
    "end": "21600"
  },
  {
    "text": "and how our customers are utilizing it",
    "start": "21600",
    "end": "24960"
  },
  {
    "text": "i will also discuss support for apache",
    "start": "24960",
    "end": "27359"
  },
  {
    "text": "hoodie across the ecosystem of aws",
    "start": "27359",
    "end": "29840"
  },
  {
    "text": "services",
    "start": "29840",
    "end": "31679"
  },
  {
    "text": "finally i will review the architecture",
    "start": "31679",
    "end": "34640"
  },
  {
    "text": "for a data ingestion pipeline",
    "start": "34640",
    "end": "36399"
  },
  {
    "text": "i have set up for the purposes of this",
    "start": "36399",
    "end": "38239"
  },
  {
    "text": "demo",
    "start": "38239",
    "end": "39520"
  },
  {
    "text": "so let's get started",
    "start": "39520",
    "end": "45840"
  },
  {
    "start": "42000",
    "end": "118000"
  },
  {
    "text": "what is apache hoodie hoodie is an open",
    "start": "48559",
    "end": "51039"
  },
  {
    "text": "source data management framework",
    "start": "51039",
    "end": "53280"
  },
  {
    "text": "that is used to incrementally process",
    "start": "53280",
    "end": "55840"
  },
  {
    "text": "data",
    "start": "55840",
    "end": "56879"
  },
  {
    "text": "and for data pipeline development",
    "start": "56879",
    "end": "60320"
  },
  {
    "text": "hoodie allows you to efficiently manage",
    "start": "60320",
    "end": "62559"
  },
  {
    "text": "business requirements",
    "start": "62559",
    "end": "63840"
  },
  {
    "text": "such as data life cycle management and",
    "start": "63840",
    "end": "66560"
  },
  {
    "text": "managing data quality",
    "start": "66560",
    "end": "69280"
  },
  {
    "text": "our customers use hoodie to manage data",
    "start": "69280",
    "end": "72479"
  },
  {
    "text": "at the record level in s3",
    "start": "72479",
    "end": "74080"
  },
  {
    "text": "data lakes and it helps them simplify",
    "start": "74080",
    "end": "77280"
  },
  {
    "text": "change data capture",
    "start": "77280",
    "end": "79280"
  },
  {
    "text": "customers also use hoodie for streaming",
    "start": "79280",
    "end": "81520"
  },
  {
    "text": "data ingestion",
    "start": "81520",
    "end": "83119"
  },
  {
    "text": "and for managing data privacy use cases",
    "start": "83119",
    "end": "85840"
  },
  {
    "text": "that require record level updates and",
    "start": "85840",
    "end": "87759"
  },
  {
    "text": "deletes",
    "start": "87759",
    "end": "90240"
  },
  {
    "text": "the data sets that are managed by hoodie",
    "start": "90720",
    "end": "92560"
  },
  {
    "text": "are stored in s3 using open storage",
    "start": "92560",
    "end": "94640"
  },
  {
    "text": "formats",
    "start": "94640",
    "end": "95680"
  },
  {
    "text": "hoodie is a open source apache framework",
    "start": "95680",
    "end": "100159"
  },
  {
    "text": "while integrations are also offered with",
    "start": "100159",
    "end": "102960"
  },
  {
    "text": "presto",
    "start": "102960",
    "end": "103680"
  },
  {
    "text": "apache hive spark and the aws",
    "start": "103680",
    "end": "106880"
  },
  {
    "text": "glue catalog for near real-time access",
    "start": "106880",
    "end": "110079"
  },
  {
    "text": "to to updated data",
    "start": "110079",
    "end": "111920"
  },
  {
    "text": "in in s3",
    "start": "111920",
    "end": "115840"
  },
  {
    "start": "118000",
    "end": "332000"
  },
  {
    "text": "let's discuss some common use cases for",
    "start": "120880",
    "end": "122960"
  },
  {
    "text": "hoodie",
    "start": "122960",
    "end": "125040"
  },
  {
    "text": "a common challenge when creating data",
    "start": "125040",
    "end": "126960"
  },
  {
    "text": "pipelines is dealing with change data",
    "start": "126960",
    "end": "128879"
  },
  {
    "text": "capture",
    "start": "128879",
    "end": "130560"
  },
  {
    "text": "late arriving or incorrect data requires",
    "start": "130560",
    "end": "132720"
  },
  {
    "text": "the data to be rewritten",
    "start": "132720",
    "end": "134400"
  },
  {
    "text": "for updated records finding the right",
    "start": "134400",
    "end": "137840"
  },
  {
    "text": "files to update",
    "start": "137840",
    "end": "138959"
  },
  {
    "text": "applying the changes and then viewing",
    "start": "138959",
    "end": "140400"
  },
  {
    "text": "the data is challenging and requires",
    "start": "140400",
    "end": "142319"
  },
  {
    "text": "customers to create their own frameworks",
    "start": "142319",
    "end": "144160"
  },
  {
    "text": "or conventions",
    "start": "144160",
    "end": "146560"
  },
  {
    "text": "with hoodie late arriving data can be",
    "start": "146560",
    "end": "148560"
  },
  {
    "text": "upsetted into an existing data set",
    "start": "148560",
    "end": "151599"
  },
  {
    "text": "when changes are made hoodie will find",
    "start": "151599",
    "end": "153440"
  },
  {
    "text": "the appropriate files in s3",
    "start": "153440",
    "end": "155680"
  },
  {
    "text": "and rewrite them to incorporate any",
    "start": "155680",
    "end": "157920"
  },
  {
    "text": "inserts updates",
    "start": "157920",
    "end": "159360"
  },
  {
    "text": "or deletes that have taken place",
    "start": "159360",
    "end": "162720"
  },
  {
    "text": "hoodie also allows you to view your data",
    "start": "163440",
    "end": "166160"
  },
  {
    "text": "set at a specific point in time",
    "start": "166160",
    "end": "168080"
  },
  {
    "text": "so you can have snapshots of your data",
    "start": "168080",
    "end": "171680"
  },
  {
    "text": "each change for the data is tracked and",
    "start": "171680",
    "end": "173280"
  },
  {
    "text": "can be easily rolled back should you",
    "start": "173280",
    "end": "174720"
  },
  {
    "text": "need to undo them",
    "start": "174720",
    "end": "177120"
  },
  {
    "text": "and finally aws offers integration with",
    "start": "177120",
    "end": "179599"
  },
  {
    "text": "database migration service",
    "start": "179599",
    "end": "181599"
  },
  {
    "text": "which can simplify loading of the data",
    "start": "181599",
    "end": "185840"
  },
  {
    "text": "let's discuss data deletion use cases in",
    "start": "186800",
    "end": "189920"
  },
  {
    "text": "regards to privacy regulations",
    "start": "189920",
    "end": "192800"
  },
  {
    "text": "due to the recent privacy regulations",
    "start": "192800",
    "end": "194560"
  },
  {
    "text": "like gdpr and ccpa",
    "start": "194560",
    "end": "197680"
  },
  {
    "text": "companies across many industries need to",
    "start": "197680",
    "end": "200080"
  },
  {
    "text": "perform record level updates and",
    "start": "200080",
    "end": "201840"
  },
  {
    "text": "deletions",
    "start": "201840",
    "end": "203920"
  },
  {
    "text": "for people's right to be forgotten or",
    "start": "203920",
    "end": "205920"
  },
  {
    "text": "changes to consent",
    "start": "205920",
    "end": "207360"
  },
  {
    "text": "as to how their data can be stored",
    "start": "207360",
    "end": "210959"
  },
  {
    "text": "previously we had to create custom data",
    "start": "210959",
    "end": "213040"
  },
  {
    "text": "management and ingestion solutions to",
    "start": "213040",
    "end": "214879"
  },
  {
    "text": "track",
    "start": "214879",
    "end": "215360"
  },
  {
    "text": "individual changes and rewrite large",
    "start": "215360",
    "end": "217760"
  },
  {
    "text": "data sets for just a few changes",
    "start": "217760",
    "end": "221040"
  },
  {
    "text": "with apache hoodie we we can now use",
    "start": "221040",
    "end": "223920"
  },
  {
    "text": "familiar inserts",
    "start": "223920",
    "end": "225280"
  },
  {
    "text": "updates upset and delete operations and",
    "start": "225280",
    "end": "227920"
  },
  {
    "text": "hoodie will track transactions",
    "start": "227920",
    "end": "230159"
  },
  {
    "text": "and make calendar changes on s3",
    "start": "230159",
    "end": "233360"
  },
  {
    "text": "which simplifies our data pipelines",
    "start": "233360",
    "end": "237519"
  },
  {
    "text": "let's also talk about streaming data use",
    "start": "238640",
    "end": "240720"
  },
  {
    "text": "cases",
    "start": "240720",
    "end": "241760"
  },
  {
    "text": "now as streaming internet of things and",
    "start": "241760",
    "end": "244080"
  },
  {
    "text": "ingestion pipelines",
    "start": "244080",
    "end": "245599"
  },
  {
    "text": "we need to handle data insertion and",
    "start": "245599",
    "end": "247439"
  },
  {
    "text": "update events without creating many",
    "start": "247439",
    "end": "249360"
  },
  {
    "text": "small files",
    "start": "249360",
    "end": "250560"
  },
  {
    "text": "that can cause performance issues for",
    "start": "250560",
    "end": "252319"
  },
  {
    "text": "analytics",
    "start": "252319",
    "end": "253599"
  },
  {
    "text": "those those in the world of analytics",
    "start": "253599",
    "end": "255439"
  },
  {
    "text": "and big data are quite familiar with the",
    "start": "255439",
    "end": "257519"
  },
  {
    "text": "small file problems",
    "start": "257519",
    "end": "260079"
  },
  {
    "text": "data engineers need tools that enable",
    "start": "260079",
    "end": "262000"
  },
  {
    "text": "them to use upsearch to efficiently",
    "start": "262000",
    "end": "263680"
  },
  {
    "text": "handle streaming data ingestion",
    "start": "263680",
    "end": "265759"
  },
  {
    "text": "automate and optimize storage and enable",
    "start": "265759",
    "end": "268800"
  },
  {
    "text": "analysts to query new data immediately",
    "start": "268800",
    "end": "272879"
  },
  {
    "text": "previously we had to build custom",
    "start": "273040",
    "end": "275040"
  },
  {
    "text": "solutions at monifar",
    "start": "275040",
    "end": "276880"
  },
  {
    "text": "for querying and all of this had to be",
    "start": "276880",
    "end": "279919"
  },
  {
    "text": "managed orchestrated and monitored",
    "start": "279919",
    "end": "283120"
  },
  {
    "text": "going forward with hoodie we can",
    "start": "283120",
    "end": "284639"
  },
  {
    "text": "automatically track changes and merge",
    "start": "284639",
    "end": "286560"
  },
  {
    "text": "files",
    "start": "286560",
    "end": "287199"
  },
  {
    "text": "so that they remain optimally sized",
    "start": "287199",
    "end": "290400"
  },
  {
    "text": "now the the real value for this this",
    "start": "290400",
    "end": "293520"
  },
  {
    "text": "is in the use cases around enterprise",
    "start": "293520",
    "end": "295440"
  },
  {
    "text": "data warehouse and operational data",
    "start": "295440",
    "end": "297120"
  },
  {
    "text": "stores",
    "start": "297120",
    "end": "298560"
  },
  {
    "text": "and where data engineers and data",
    "start": "298560",
    "end": "301919"
  },
  {
    "text": "analysts",
    "start": "301919",
    "end": "302560"
  },
  {
    "text": "want to use query engine sql query",
    "start": "302560",
    "end": "305360"
  },
  {
    "text": "engines like",
    "start": "305360",
    "end": "306560"
  },
  {
    "text": "apache hive and presto for processing",
    "start": "306560",
    "end": "308800"
  },
  {
    "text": "and analytics",
    "start": "308800",
    "end": "310720"
  },
  {
    "text": "with hoodie individual changes can be",
    "start": "310720",
    "end": "312800"
  },
  {
    "text": "processed",
    "start": "312800",
    "end": "313919"
  },
  {
    "text": "much more granularly reducing the",
    "start": "313919",
    "end": "315600"
  },
  {
    "text": "overhead and we can query s3 data",
    "start": "315600",
    "end": "318560"
  },
  {
    "text": "as directly to view and provide users",
    "start": "318560",
    "end": "321199"
  },
  {
    "text": "with a near real-time",
    "start": "321199",
    "end": "322800"
  },
  {
    "text": "view over the data",
    "start": "322800",
    "end": "326240"
  },
  {
    "start": "332000",
    "end": "453000"
  },
  {
    "text": "let me explain the support that hoodie",
    "start": "332639",
    "end": "335840"
  },
  {
    "text": "has across the the spectrum of aws",
    "start": "335840",
    "end": "339039"
  },
  {
    "text": "services",
    "start": "339039",
    "end": "341520"
  },
  {
    "text": "as i mentioned hoodie provides",
    "start": "341520",
    "end": "343039"
  },
  {
    "text": "mechanisms to perform updates",
    "start": "343039",
    "end": "344800"
  },
  {
    "text": "deletes and inserts on top of s3 parquet",
    "start": "344800",
    "end": "347440"
  },
  {
    "text": "files",
    "start": "347440",
    "end": "349840"
  },
  {
    "text": "in addition to that athena amazon athena",
    "start": "350320",
    "end": "352880"
  },
  {
    "text": "supports querying the hoodie data sets",
    "start": "352880",
    "end": "354880"
  },
  {
    "text": "in in our amazon s3 back data lake",
    "start": "354880",
    "end": "357759"
  },
  {
    "text": "directly",
    "start": "357759",
    "end": "358960"
  },
  {
    "text": "we can we can create tables",
    "start": "358960",
    "end": "362000"
  },
  {
    "text": "uh we can run queries against existing",
    "start": "362000",
    "end": "364960"
  },
  {
    "text": "tables",
    "start": "364960",
    "end": "366160"
  },
  {
    "text": "or or look at queries around snapshots",
    "start": "366160",
    "end": "369840"
  },
  {
    "text": "or",
    "start": "369840",
    "end": "370080"
  },
  {
    "text": "point in time data there is no need for",
    "start": "370080",
    "end": "373199"
  },
  {
    "text": "dedicated infrastructure",
    "start": "373199",
    "end": "375120"
  },
  {
    "text": "hoodie is just a library and we can",
    "start": "375120",
    "end": "377440"
  },
  {
    "text": "include the jar files",
    "start": "377440",
    "end": "379120"
  },
  {
    "text": "as part of our data processing workflow",
    "start": "379120",
    "end": "382800"
  },
  {
    "text": "so if we're using amazon athena to to",
    "start": "382800",
    "end": "385759"
  },
  {
    "text": "query our hoodie data sets",
    "start": "385759",
    "end": "387680"
  },
  {
    "text": "uh we we have to we're basically working",
    "start": "387680",
    "end": "390080"
  },
  {
    "text": "in a serverless model and there is no",
    "start": "390080",
    "end": "392639"
  },
  {
    "text": "infrastructure to provision or manage",
    "start": "392639",
    "end": "396560"
  },
  {
    "text": "foodie is also supported on amazon emr",
    "start": "396560",
    "end": "398960"
  },
  {
    "text": "since version 3.1",
    "start": "398960",
    "end": "401039"
  },
  {
    "text": "version 5.31 which allows",
    "start": "401039",
    "end": "404880"
  },
  {
    "text": "our users to develop spark jobs on emr",
    "start": "404880",
    "end": "407440"
  },
  {
    "text": "and process",
    "start": "407440",
    "end": "408000"
  },
  {
    "text": "4d tables",
    "start": "408000",
    "end": "410720"
  },
  {
    "text": "hoodie is installed automatically on any",
    "start": "411599",
    "end": "414240"
  },
  {
    "text": "cluster that when we choose spark hive",
    "start": "414240",
    "end": "416240"
  },
  {
    "text": "wordpress 2",
    "start": "416240",
    "end": "418080"
  },
  {
    "text": "on an emr cluster so you get native",
    "start": "418080",
    "end": "420880"
  },
  {
    "text": "native integration",
    "start": "420880",
    "end": "422560"
  },
  {
    "text": "with with those services and then",
    "start": "422560",
    "end": "425919"
  },
  {
    "text": "also we have native integration with aws",
    "start": "425919",
    "end": "428400"
  },
  {
    "text": "database migration service",
    "start": "428400",
    "end": "430080"
  },
  {
    "text": "to provide another source for data as it",
    "start": "430080",
    "end": "434840"
  },
  {
    "text": "changes",
    "start": "434840",
    "end": "437360"
  },
  {
    "text": "finally hoodie automatically syncs aws",
    "start": "437360",
    "end": "439840"
  },
  {
    "text": "glued catalog",
    "start": "439840",
    "end": "440800"
  },
  {
    "text": "so the data is immediately available for",
    "start": "440800",
    "end": "442800"
  },
  {
    "text": "processing",
    "start": "442800",
    "end": "443840"
  },
  {
    "text": "by analytics engines such as aws athena",
    "start": "443840",
    "end": "448400"
  },
  {
    "text": "redshift presto spark and hive",
    "start": "448400",
    "end": "453199"
  },
  {
    "start": "453000",
    "end": "888000"
  },
  {
    "text": "in this demo i will show you how to",
    "start": "454800",
    "end": "456800"
  },
  {
    "text": "replicate data from a relational",
    "start": "456800",
    "end": "458319"
  },
  {
    "text": "database",
    "start": "458319",
    "end": "459280"
  },
  {
    "text": "in my case i've set up a amazon aurora",
    "start": "459280",
    "end": "462840"
  },
  {
    "text": "database",
    "start": "462840",
    "end": "464560"
  },
  {
    "text": "to your s3 bag data lake using",
    "start": "464560",
    "end": "467599"
  },
  {
    "text": "apache hoodie to manage inserts updates",
    "start": "467599",
    "end": "469919"
  },
  {
    "text": "and deletes",
    "start": "469919",
    "end": "471919"
  },
  {
    "text": "i will use aws glue to upset data in the",
    "start": "471919",
    "end": "474720"
  },
  {
    "text": "data lake using",
    "start": "474720",
    "end": "475840"
  },
  {
    "text": "hoodie as an embedded library",
    "start": "475840",
    "end": "479039"
  },
  {
    "text": "and finally we'll take advantage of the",
    "start": "479039",
    "end": "480720"
  },
  {
    "text": "serverless architecture of",
    "start": "480720",
    "end": "482319"
  },
  {
    "text": "amazon athena to query the hoodie data",
    "start": "482319",
    "end": "485360"
  },
  {
    "text": "set",
    "start": "485360",
    "end": "485840"
  },
  {
    "text": "in our data lake so let's get started",
    "start": "485840",
    "end": "492240"
  },
  {
    "text": "for this demo i have set up a amazon",
    "start": "492240",
    "end": "494560"
  },
  {
    "text": "aurora postgres database",
    "start": "494560",
    "end": "497360"
  },
  {
    "text": "and i've set up amazon database",
    "start": "497360",
    "end": "499840"
  },
  {
    "text": "migration service tasks",
    "start": "499840",
    "end": "501360"
  },
  {
    "text": "which are looking for change data",
    "start": "501360",
    "end": "503599"
  },
  {
    "text": "capture",
    "start": "503599",
    "end": "504800"
  },
  {
    "text": "from this from this database what i will",
    "start": "504800",
    "end": "507759"
  },
  {
    "text": "do",
    "start": "507759",
    "end": "508080"
  },
  {
    "text": "is i'll create a schema i will then",
    "start": "508080",
    "end": "510639"
  },
  {
    "text": "create a table",
    "start": "510639",
    "end": "511840"
  },
  {
    "text": "and insert a few rows into that database",
    "start": "511840",
    "end": "514640"
  },
  {
    "text": "so let's let's do that",
    "start": "514640",
    "end": "517760"
  },
  {
    "text": "once i run these queries i will",
    "start": "518159",
    "end": "522000"
  },
  {
    "text": "you can see that i have inserted seven",
    "start": "522000",
    "end": "524080"
  },
  {
    "text": "rows",
    "start": "524080",
    "end": "525120"
  },
  {
    "text": "with uh with different employee",
    "start": "525120",
    "end": "528320"
  },
  {
    "text": "details into my database and what i will",
    "start": "528320",
    "end": "531600"
  },
  {
    "text": "also do is",
    "start": "531600",
    "end": "532880"
  },
  {
    "text": "after this i will perform certain",
    "start": "532880",
    "end": "534880"
  },
  {
    "text": "updates so i will update",
    "start": "534880",
    "end": "538240"
  },
  {
    "text": "the row that corresponds to employee 2",
    "start": "538240",
    "end": "541120"
  },
  {
    "text": "and change the city",
    "start": "541120",
    "end": "542640"
  },
  {
    "text": "to new delhi i will also change the",
    "start": "542640",
    "end": "544880"
  },
  {
    "text": "salary for employee number 5",
    "start": "544880",
    "end": "547279"
  },
  {
    "text": "to 70 000 and then i will",
    "start": "547279",
    "end": "551200"
  },
  {
    "text": "i will capture certain i will insert two",
    "start": "551200",
    "end": "554240"
  },
  {
    "text": "new employee records for eight and nine",
    "start": "554240",
    "end": "557279"
  },
  {
    "text": "and delete uh record corresponding to",
    "start": "557279",
    "end": "559680"
  },
  {
    "text": "employee number three so let's do these",
    "start": "559680",
    "end": "561200"
  },
  {
    "text": "changes so before",
    "start": "561200",
    "end": "562640"
  },
  {
    "text": "i i did this this is this",
    "start": "562640",
    "end": "565839"
  },
  {
    "text": "the state of my database you can see i",
    "start": "565839",
    "end": "568080"
  },
  {
    "text": "have",
    "start": "568080",
    "end": "569040"
  },
  {
    "text": "seven records and let's uh let's track",
    "start": "569040",
    "end": "572560"
  },
  {
    "text": "one of them",
    "start": "572560",
    "end": "573600"
  },
  {
    "text": "susan is listed as",
    "start": "573600",
    "end": "577040"
  },
  {
    "text": "the city is listed as new york and",
    "start": "577040",
    "end": "578640"
  },
  {
    "text": "salary of 60 000",
    "start": "578640",
    "end": "580640"
  },
  {
    "text": "now i'm going to run my update and let's",
    "start": "580640",
    "end": "583120"
  },
  {
    "text": "let's go through with that step",
    "start": "583120",
    "end": "585920"
  },
  {
    "text": "so i've updated five rows let's uh let's",
    "start": "585920",
    "end": "589600"
  },
  {
    "text": "run the select again",
    "start": "589600",
    "end": "592480"
  },
  {
    "text": "so now i have a total of eight records",
    "start": "592480",
    "end": "595040"
  },
  {
    "text": "in my database",
    "start": "595040",
    "end": "596880"
  },
  {
    "text": "susan as you can see her city is updated",
    "start": "596880",
    "end": "599519"
  },
  {
    "text": "to new delhi",
    "start": "599519",
    "end": "600560"
  },
  {
    "text": "and the salary is 60 000. i also made",
    "start": "600560",
    "end": "603680"
  },
  {
    "text": "some",
    "start": "603680",
    "end": "604000"
  },
  {
    "text": "changes to employee number five updated",
    "start": "604000",
    "end": "607279"
  },
  {
    "text": "the salary to 70",
    "start": "607279",
    "end": "608640"
  },
  {
    "text": "000 which you can see is reflected here",
    "start": "608640",
    "end": "612560"
  },
  {
    "text": "joe is employee number five and the",
    "start": "612560",
    "end": "614160"
  },
  {
    "text": "salary is 70 000.",
    "start": "614160",
    "end": "616480"
  },
  {
    "text": "so the next step would be i i would like",
    "start": "616480",
    "end": "618800"
  },
  {
    "text": "you to",
    "start": "618800",
    "end": "620480"
  },
  {
    "text": "to see how you can query this data in",
    "start": "620480",
    "end": "622880"
  },
  {
    "text": "athena",
    "start": "622880",
    "end": "623760"
  },
  {
    "text": "so let's move over to the athena console",
    "start": "623760",
    "end": "633839"
  },
  {
    "text": "okay after my drew job runs",
    "start": "634480",
    "end": "638399"
  },
  {
    "text": "i have the changes replicated in the",
    "start": "638399",
    "end": "640160"
  },
  {
    "text": "data catalog and the hoodie data set is",
    "start": "640160",
    "end": "642320"
  },
  {
    "text": "created",
    "start": "642320",
    "end": "643839"
  },
  {
    "text": "so you can see that i have a database",
    "start": "643839",
    "end": "645680"
  },
  {
    "text": "called human resources which contains",
    "start": "645680",
    "end": "647839"
  },
  {
    "text": "the employee details table",
    "start": "647839",
    "end": "650320"
  },
  {
    "text": "let's let's look at the structure of the",
    "start": "650320",
    "end": "652079"
  },
  {
    "text": "table",
    "start": "652079",
    "end": "654000"
  },
  {
    "text": "so as a as i examine the structure",
    "start": "654000",
    "end": "657920"
  },
  {
    "text": "there are a few columns that are that",
    "start": "657920",
    "end": "660560"
  },
  {
    "text": "are used by hoodie for",
    "start": "660560",
    "end": "662000"
  },
  {
    "text": "for tracking the commit time sequence",
    "start": "662000",
    "end": "664480"
  },
  {
    "text": "number record key",
    "start": "664480",
    "end": "666240"
  },
  {
    "text": "partition path and file name where this",
    "start": "666240",
    "end": "668240"
  },
  {
    "text": "record belongs",
    "start": "668240",
    "end": "670000"
  },
  {
    "text": "and then i have my data records for",
    "start": "670000",
    "end": "672160"
  },
  {
    "text": "update times time",
    "start": "672160",
    "end": "673200"
  },
  {
    "text": "employee number name city salary schema",
    "start": "673200",
    "end": "676640"
  },
  {
    "text": "and department",
    "start": "676640",
    "end": "678320"
  },
  {
    "text": "this data is partitioned using the",
    "start": "678320",
    "end": "680720"
  },
  {
    "text": "department code",
    "start": "680720",
    "end": "681760"
  },
  {
    "text": "so i have records that belong to sales",
    "start": "681760",
    "end": "685440"
  },
  {
    "text": "department and",
    "start": "685440",
    "end": "686880"
  },
  {
    "text": "to id and purchase and and they're all",
    "start": "686880",
    "end": "689200"
  },
  {
    "text": "structured in s3 so that our athena",
    "start": "689200",
    "end": "691279"
  },
  {
    "text": "queries are optimized",
    "start": "691279",
    "end": "693360"
  },
  {
    "text": "so i'm going to go ahead and run a",
    "start": "693360",
    "end": "696160"
  },
  {
    "text": "select",
    "start": "696160",
    "end": "696720"
  },
  {
    "text": "against this data set in athena",
    "start": "696720",
    "end": "702560"
  },
  {
    "text": "you can see my query came back fairly",
    "start": "702560",
    "end": "705600"
  },
  {
    "text": "quickly",
    "start": "705600",
    "end": "707040"
  },
  {
    "text": "you can check the the the time it took",
    "start": "707040",
    "end": "709120"
  },
  {
    "text": "1.5 second to scan 2.37",
    "start": "709120",
    "end": "711920"
  },
  {
    "text": "kb of data and in my case",
    "start": "711920",
    "end": "715279"
  },
  {
    "text": "i have eight records total uh just as as",
    "start": "715279",
    "end": "718800"
  },
  {
    "text": "i was showing you in my sequel",
    "start": "718800",
    "end": "720880"
  },
  {
    "text": "editor and for for comparison's sake we",
    "start": "720880",
    "end": "724639"
  },
  {
    "text": "can see that uh we had",
    "start": "724639",
    "end": "726560"
  },
  {
    "text": "updated the record for employee susan",
    "start": "726560",
    "end": "728880"
  },
  {
    "text": "the city was set to new delhi",
    "start": "728880",
    "end": "730959"
  },
  {
    "text": "and that change is reflecting here and",
    "start": "730959",
    "end": "733839"
  },
  {
    "text": "then also we had",
    "start": "733839",
    "end": "735440"
  },
  {
    "text": "we had made some changes to the record",
    "start": "735440",
    "end": "737279"
  },
  {
    "text": "for joe we had upgraded the salary to 70",
    "start": "737279",
    "end": "739680"
  },
  {
    "text": "000.",
    "start": "739680",
    "end": "740720"
  },
  {
    "text": "so all those changes have uh have",
    "start": "740720",
    "end": "743920"
  },
  {
    "text": "migrated over",
    "start": "743920",
    "end": "744959"
  },
  {
    "text": "to our data lake and and we are able to",
    "start": "744959",
    "end": "747120"
  },
  {
    "text": "query it",
    "start": "747120",
    "end": "748160"
  },
  {
    "text": "using athena now",
    "start": "748160",
    "end": "751360"
  },
  {
    "text": "let's let's look at one other detail",
    "start": "751360",
    "end": "754560"
  },
  {
    "text": "here",
    "start": "754560",
    "end": "755200"
  },
  {
    "text": "as part of our demo i want to show you",
    "start": "755200",
    "end": "757440"
  },
  {
    "text": "if you want to recreate",
    "start": "757440",
    "end": "758959"
  },
  {
    "text": "this table you have the option of",
    "start": "758959",
    "end": "761519"
  },
  {
    "text": "generating your ddl",
    "start": "761519",
    "end": "764480"
  },
  {
    "text": "and in this case when i run the show",
    "start": "764480",
    "end": "767200"
  },
  {
    "text": "create table",
    "start": "767200",
    "end": "768320"
  },
  {
    "text": "for my employee tables it it gives me",
    "start": "768320",
    "end": "770399"
  },
  {
    "text": "the the structure",
    "start": "770399",
    "end": "772480"
  },
  {
    "text": "the data types and the the athena",
    "start": "772480",
    "end": "774480"
  },
  {
    "text": "locations so if i want to create",
    "start": "774480",
    "end": "776880"
  },
  {
    "text": "this table with let's say in a different",
    "start": "776880",
    "end": "780160"
  },
  {
    "text": "catalog",
    "start": "780160",
    "end": "780880"
  },
  {
    "text": "or in a different database i could copy",
    "start": "780880",
    "end": "782880"
  },
  {
    "text": "this ddl",
    "start": "782880",
    "end": "786560"
  },
  {
    "text": "run it and i'm this time i'm going to",
    "start": "786560",
    "end": "788560"
  },
  {
    "text": "give it a new name i'm going to give it",
    "start": "788560",
    "end": "790560"
  },
  {
    "text": "a name of employee details 2. now it",
    "start": "790560",
    "end": "793519"
  },
  {
    "text": "will",
    "start": "793519",
    "end": "794560"
  },
  {
    "text": "create a new new table in my glue data",
    "start": "794560",
    "end": "797440"
  },
  {
    "text": "catalog",
    "start": "797440",
    "end": "798560"
  },
  {
    "text": "with this name as you can see the table",
    "start": "798560",
    "end": "801040"
  },
  {
    "text": "is now created",
    "start": "801040",
    "end": "803120"
  },
  {
    "text": "and i should be able to now run select",
    "start": "803120",
    "end": "806079"
  },
  {
    "text": "statements",
    "start": "806079",
    "end": "807519"
  },
  {
    "text": "against my new table so let's hit",
    "start": "807519",
    "end": "809680"
  },
  {
    "text": "preview table",
    "start": "809680",
    "end": "811839"
  },
  {
    "text": "and athena goes out and queries my my",
    "start": "811839",
    "end": "814560"
  },
  {
    "text": "data",
    "start": "814560",
    "end": "815040"
  },
  {
    "text": "so you can see that no no data is found",
    "start": "815040",
    "end": "818480"
  },
  {
    "text": "and the reason for that is i i need to",
    "start": "818480",
    "end": "821279"
  },
  {
    "text": "add",
    "start": "821279",
    "end": "821839"
  },
  {
    "text": "partitions for for the data that that",
    "start": "821839",
    "end": "824560"
  },
  {
    "text": "i've newly added so i'm going to do that",
    "start": "824560",
    "end": "826839"
  },
  {
    "text": "here",
    "start": "826839",
    "end": "828000"
  },
  {
    "text": "so this is my employee details too i",
    "start": "828000",
    "end": "831040"
  },
  {
    "text": "will i will first",
    "start": "831040",
    "end": "834480"
  },
  {
    "text": "go ahead and add this partition and then",
    "start": "834480",
    "end": "836959"
  },
  {
    "text": "i will run the query",
    "start": "836959",
    "end": "843839"
  },
  {
    "text": "okay the query is successful so now",
    "start": "844560",
    "end": "846399"
  },
  {
    "text": "let's proceed by",
    "start": "846399",
    "end": "848720"
  },
  {
    "text": "running a select",
    "start": "848720",
    "end": "851519"
  },
  {
    "text": "okay as you can see that the partition",
    "start": "852000",
    "end": "855040"
  },
  {
    "text": "that i added i'm able to query the data",
    "start": "855040",
    "end": "857120"
  },
  {
    "text": "from it",
    "start": "857120",
    "end": "858720"
  },
  {
    "text": "and my new table is is up and running so",
    "start": "858720",
    "end": "862720"
  },
  {
    "text": "uh that's that's the the basic gist of",
    "start": "862720",
    "end": "865120"
  },
  {
    "text": "it you could",
    "start": "865120",
    "end": "866000"
  },
  {
    "text": "you could very easily capture changes",
    "start": "866000",
    "end": "868880"
  },
  {
    "text": "from from your database",
    "start": "868880",
    "end": "870240"
  },
  {
    "text": "bring them into a data lake and",
    "start": "870240",
    "end": "873440"
  },
  {
    "text": "you can you can process them in in real",
    "start": "873440",
    "end": "875920"
  },
  {
    "text": "time and be able to query the data using",
    "start": "875920",
    "end": "878160"
  },
  {
    "text": "amazon athena",
    "start": "878160",
    "end": "881040"
  },
  {
    "text": "that brings me to an end of my",
    "start": "881680",
    "end": "883120"
  },
  {
    "text": "presentation",
    "start": "883120",
    "end": "884800"
  },
  {
    "text": "i hope you enjoyed it thanks for",
    "start": "884800",
    "end": "887560"
  },
  {
    "text": "watching",
    "start": "887560",
    "end": "890560"
  }
]