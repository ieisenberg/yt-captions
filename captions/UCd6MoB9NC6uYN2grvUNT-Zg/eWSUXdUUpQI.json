[
  {
    "start": "0",
    "end": "90000"
  },
  {
    "text": "and with that i will go ahead and hand it over to shaquille to start us off thanks aurora um hello and welcome",
    "start": "480",
    "end": "7200"
  },
  {
    "text": "everyone to today's session we're going to talk about accelerating application performance using in-memory caching my name is",
    "start": "7200",
    "end": "13679"
  },
  {
    "text": "shakeel langha i'm a business development manager with amazon's relational database services",
    "start": "13679",
    "end": "18960"
  },
  {
    "text": "and uh with me speaking today will be omar zaki who is a senior product manager who focuses on elastic hash and he's",
    "start": "18960",
    "end": "25680"
  },
  {
    "text": "also part of our database services so going to our next slide here let's",
    "start": "25680",
    "end": "31039"
  },
  {
    "text": "look at what the agenda is today and essentially we'll talk about an overview of caching",
    "start": "31039",
    "end": "36640"
  },
  {
    "text": "so today's session is primarily about memcached and elastic cache we'll do",
    "start": "36640",
    "end": "43120"
  },
  {
    "text": "some introductory uh conversations around exactly what in memory caching involves on aws and",
    "start": "43120",
    "end": "50879"
  },
  {
    "text": "my apologies in advance to anyone that already knows these things we'll cover a little bit of the introductory type topics in the beginning",
    "start": "50879",
    "end": "56879"
  },
  {
    "text": "but then as we go further we'll go deeper into exactly what elastic hashes and what are the use cases and design",
    "start": "56879",
    "end": "62000"
  },
  {
    "text": "patterns uh talk about some gauchos and so on so we'll get into some advanced topics also in this session but we'll cover",
    "start": "62000",
    "end": "68080"
  },
  {
    "text": "some introductory topics for those who are new to the whole topic of caching",
    "start": "68080",
    "end": "73760"
  },
  {
    "text": "on aws so like i said an overview of memcached then we go deeper into elastic cache and then use cases in demo",
    "start": "73760",
    "end": "80960"
  },
  {
    "text": "at the end what we'll show you is actually test results uh not necessarily a live demo but hopefully everybody should benefit",
    "start": "80960",
    "end": "87759"
  },
  {
    "text": "from that so let's look at a typical web application and again i want to preface my my",
    "start": "87759",
    "end": "95119"
  },
  {
    "start": "90000",
    "end": "247000"
  },
  {
    "text": "comments here by saying that caching doesn't just apply to web applications but that's essentially what we'll talk about in",
    "start": "95119",
    "end": "101040"
  },
  {
    "text": "more detail today but in general just so you know caching applies to pretty much any kind of application that you want to run",
    "start": "101040",
    "end": "106240"
  },
  {
    "text": "where you want to speed up read-only type performance right so from the perspective of a web",
    "start": "106240",
    "end": "111439"
  },
  {
    "text": "application you've got a whole bunch of users and the users could be on handhelds they could be on browsers they could be",
    "start": "111439",
    "end": "117360"
  },
  {
    "text": "on dedicated devices these users are trying to access a set",
    "start": "117360",
    "end": "122799"
  },
  {
    "text": "of web servers or one web server which is essentially serving the web traffic",
    "start": "122799",
    "end": "128399"
  },
  {
    "text": "but the web traffic is being served based on some persistent data again the data is in this case coming",
    "start": "128399",
    "end": "134400"
  },
  {
    "text": "from a database server but doesn't have to be so it could be a file system it could be heck a mainframe it could be anything",
    "start": "134400",
    "end": "140239"
  },
  {
    "text": "where you persist your data and you can retrieve it when you need it so this is the typical setup and in",
    "start": "140239",
    "end": "146640"
  },
  {
    "text": "in this setup now as the users hit your web application uh the application is querying that",
    "start": "146640",
    "end": "152720"
  },
  {
    "text": "persistent store in this case a database server and uh you get a certain type of",
    "start": "152720",
    "end": "158319"
  },
  {
    "text": "performance you get a certain type of response rate the database result returns the results",
    "start": "158319",
    "end": "163599"
  },
  {
    "text": "um and then you serve that back to your users that's again a typical flow now if you take it a little step further",
    "start": "163599",
    "end": "170160"
  },
  {
    "text": "as you start scaling your application the number of users keeps growing you've got to add more web servers to",
    "start": "170160",
    "end": "176720"
  },
  {
    "text": "serve all these users and it's easier to scale that front end but then what happens at the back end i",
    "start": "176720",
    "end": "183120"
  },
  {
    "text": "mean if you're using database servers sometimes it might be possible for you to keep adding more database servers",
    "start": "183120",
    "end": "189680"
  },
  {
    "text": "not very easy other times you might need to shard those servers but in general that database tier the",
    "start": "189680",
    "end": "196239"
  },
  {
    "text": "persistent layer of your application is not able to scale as seamlessly as you are doing the",
    "start": "196239",
    "end": "204319"
  },
  {
    "text": "scaling of your front end and this is where the challenge comes as to how do you go about scaling",
    "start": "204319",
    "end": "209840"
  },
  {
    "text": "your application your entire architecture as your load increases as you keep adding web servers",
    "start": "209840",
    "end": "215200"
  },
  {
    "text": "you're kind of stuck at some point so what do you do next and this is where we bring in the",
    "start": "215200",
    "end": "220480"
  },
  {
    "text": "concept of a cache and what we're saying is if the type of traffic that you're",
    "start": "220480",
    "end": "226080"
  },
  {
    "text": "diverting to your persistent layer if it's a lot of read-only traffic why hit the persistent layer at all just",
    "start": "226080",
    "end": "232560"
  },
  {
    "text": "put in a caching layer in between where you store this data and it is accessible to you in a much faster way",
    "start": "232560",
    "end": "239120"
  },
  {
    "text": "with lower latency lower response times that's essentially what caching is so",
    "start": "239120",
    "end": "244799"
  },
  {
    "text": "let's go to the next slide here and let's talk about what what we just ran through right so it's the caching",
    "start": "244799",
    "end": "250480"
  },
  {
    "start": "247000",
    "end": "537000"
  },
  {
    "text": "layer is a component essentially that stores frequently access data and memory the one thing to keep in mind though is",
    "start": "250480",
    "end": "257120"
  },
  {
    "text": "it's primarily serving read traffic now there are ways of of using a cache in advanced",
    "start": "257120",
    "end": "263280"
  },
  {
    "text": "architectures there are ways of using a cache even to store persistent data right you could possibly",
    "start": "263280",
    "end": "268960"
  },
  {
    "text": "write to a cache and then from there have a lazy layer that uh picks up newly object newly modified objects in",
    "start": "268960",
    "end": "275600"
  },
  {
    "text": "the cache and then goes right through a database but you're you're risking a little bit of",
    "start": "275600",
    "end": "280639"
  },
  {
    "text": "data persistent persistence and availability in that case but in the typical case the cache is to",
    "start": "280639",
    "end": "286160"
  },
  {
    "text": "serve read-only traffic and it returns results much faster than the back-end persistent layer which",
    "start": "286160",
    "end": "291680"
  },
  {
    "text": "could be a database which is essentially why you want to use the cache and what all this allows you to do is scale your application",
    "start": "291680",
    "end": "297919"
  },
  {
    "text": "in a much easier fashion and reduce your load on your database so who uses caching or why do you use",
    "start": "297919",
    "end": "304479"
  },
  {
    "text": "caching and essentially if you look at the use cases web applications like you mentioned are primarily a huge component of caching",
    "start": "304479",
    "end": "312000"
  },
  {
    "text": "users but in general this could be anybody that needs access to data the data doesn't change as frequently as",
    "start": "312000",
    "end": "318639"
  },
  {
    "text": "you access it and you'd like to serve it out of memory rather than going to disk because",
    "start": "318639",
    "end": "323680"
  },
  {
    "text": "serving out of memory could be uh in the order of a few milliseconds whereas serving a microseconds were serving out of a",
    "start": "323680",
    "end": "330880"
  },
  {
    "text": "disk or a remote system could be a few mic milliseconds so reduced latency reduce",
    "start": "330880",
    "end": "336720"
  },
  {
    "text": "response times serve a lot of traffic and then reduce the workload on your backend system",
    "start": "336720",
    "end": "343120"
  },
  {
    "text": "that's essentially why you use caching so to take this example further",
    "start": "343120",
    "end": "348479"
  },
  {
    "text": "again in that web application scenario or any other application let's assume that you have users that you maintain profiles or in",
    "start": "348479",
    "end": "356000"
  },
  {
    "text": "this case a phone number at the first time you need access to that profile information to that phone",
    "start": "356000",
    "end": "361759"
  },
  {
    "text": "number you try and first go to your cache there's nothing in there because you didn't do anything with it so far so you",
    "start": "361759",
    "end": "366960"
  },
  {
    "text": "look in the cache it's empty that's called a cache miss then you realize you need to get that data from somewhere else be it your",
    "start": "366960",
    "end": "373120"
  },
  {
    "text": "database or your persistent store you go get the database access the phone number from there",
    "start": "373120",
    "end": "378960"
  },
  {
    "text": "and now that you have that phone number you want to populate your cache right so that it's available for you next time",
    "start": "378960",
    "end": "385520"
  },
  {
    "text": "so that's how you put the object in the cache the first time now the second time what happens is",
    "start": "385520",
    "end": "393039"
  },
  {
    "text": "you again need that phone number you again look in the cache and you found it so that's called a cache hit",
    "start": "393039",
    "end": "399039"
  },
  {
    "text": "and in this case you just return from cache you don't even have to go to the database anymore and this is where you start to reap the benefits of the cache",
    "start": "399039",
    "end": "405680"
  },
  {
    "text": "because now you save the round trip to the database and back and you serve your results out of memory",
    "start": "405680",
    "end": "412638"
  },
  {
    "text": "now the tricky case is what happens when that phone number changes so there's multiple scenarios involved",
    "start": "412800",
    "end": "419680"
  },
  {
    "text": "here but the simplest is you get an update you take that phone number you write it to your database",
    "start": "419680",
    "end": "426080"
  },
  {
    "text": "and then you at a minimum invalidate the object in your cache you mark it as obsolete",
    "start": "426080",
    "end": "431280"
  },
  {
    "text": "a good scenario would be where you actually go and update the phone number as well so that everybody gets consistent results anybody else trying",
    "start": "431280",
    "end": "437759"
  },
  {
    "text": "to access the same phone number from another node or another system would would still get the consistent",
    "start": "437759",
    "end": "443199"
  },
  {
    "text": "information but then this is not necessarily the only way to use cache there are advanced techniques where",
    "start": "443199",
    "end": "448560"
  },
  {
    "text": "you could rely on some little bit of stale data staying in the cache if that's acceptable for your use case",
    "start": "448560",
    "end": "454319"
  },
  {
    "text": "and you could rely on time to live type config parameters and then age the",
    "start": "454319",
    "end": "460400"
  },
  {
    "text": "object out of cache so there's other ways of using it but the simplest scenario is you put an object in the cache and then",
    "start": "460400",
    "end": "466960"
  },
  {
    "text": "whenever it changes you invalidate it you update it so that the next time you try to access it you have the best results",
    "start": "466960",
    "end": "473199"
  },
  {
    "text": "so you obviously by now know why you want to use caching you want to speed up your application",
    "start": "473199",
    "end": "479039"
  },
  {
    "text": "you want to reduce load on your servers you want to reduce latency you want to scale your applications much faster so",
    "start": "479039",
    "end": "484800"
  },
  {
    "text": "those are all the benefits what's the drawbacks the drawback essentially is you added another component to your architecture another",
    "start": "484800",
    "end": "491199"
  },
  {
    "text": "layer so another uh place where you need to look up data another place where you need to update data sometimes",
    "start": "491199",
    "end": "497360"
  },
  {
    "text": "and this adds a little bit of complexity to your application so you're trading availability your trading performance",
    "start": "497360",
    "end": "503680"
  },
  {
    "text": "for complexity and uh to the extent that that complexity can be managed",
    "start": "503680",
    "end": "509520"
  },
  {
    "text": "you want to use something like elasticash which is what we'll talk about in the second half of the session uh because trying to do this by yourself",
    "start": "509520",
    "end": "516159"
  },
  {
    "text": "even though it's technically possible a lot of people do it the majority of the websites out there use it um",
    "start": "516159",
    "end": "522640"
  },
  {
    "text": "we just want to make it easier we want to reduce the burden of managing the that caching layer for you reduce the",
    "start": "522640",
    "end": "527680"
  },
  {
    "text": "heavy lifting and so we have a service for this called elastic which we'll talk about in the second half of session",
    "start": "527680",
    "end": "534160"
  },
  {
    "text": "all right so going back to our agenda we've introduced what's called memcached let's talk about a little bit more about",
    "start": "534160",
    "end": "539920"
  },
  {
    "start": "537000",
    "end": "750000"
  },
  {
    "text": "what memcached actually is and um this is a free open source high performance in memory key value store it",
    "start": "539920",
    "end": "546480"
  },
  {
    "text": "was actually developed by brad fitzpatrick in 2003 so we are celebrating the 10th year anniversary of",
    "start": "546480",
    "end": "552399"
  },
  {
    "text": "memcached and um pretty much every major website out in the world",
    "start": "552399",
    "end": "558720"
  },
  {
    "text": "uses memcached in one way or another and not like i said not just",
    "start": "558720",
    "end": "563760"
  },
  {
    "text": "websites it's used by any major application that wants to do a lot of that wants to handle a lot of traffic",
    "start": "563760",
    "end": "569839"
  },
  {
    "text": "has back-end stores that do not keep up with the speed of access that the users require",
    "start": "569839",
    "end": "575279"
  },
  {
    "text": "and you want to speed up the the middle tier you want to use something like memcached and people use it a lot on ec2 people",
    "start": "575279",
    "end": "581920"
  },
  {
    "text": "use it a lot on aws and in general it's a very widely available system and like i said open",
    "start": "581920",
    "end": "588839"
  },
  {
    "text": "source so diving deeper a little bit into the architecture",
    "start": "588839",
    "end": "594640"
  },
  {
    "text": "a few things to realize there are both server-side and and client-side components to memcached there is a",
    "start": "594640",
    "end": "600240"
  },
  {
    "text": "client-side api and when you want to access the cache the api knows",
    "start": "600240",
    "end": "605920"
  },
  {
    "text": "about where to go to get the the the object that you're trying to access so you could have multiple",
    "start": "605920",
    "end": "612560"
  },
  {
    "text": "uh nodes multiple servers which are caching your your data but then the servers don't",
    "start": "612560",
    "end": "618720"
  },
  {
    "text": "talk to each other it's the client side that knows about all the servers and depending on the key",
    "start": "618720",
    "end": "624000"
  },
  {
    "text": "that you're trying to use to access an object there's multiple ways of doing this but",
    "start": "624000",
    "end": "629839"
  },
  {
    "text": "the most typical is to do consistent hashing where you take the key and figure out which node to talk to and you go access that node um to either",
    "start": "629839",
    "end": "636800"
  },
  {
    "text": "look up or to to populate data in the cache and it's it's a key value store so you",
    "start": "636800",
    "end": "642000"
  },
  {
    "text": "use a key key and then there's an object that you get back so that's the client",
    "start": "642000",
    "end": "647120"
  },
  {
    "text": "server side um split between the functionality the other things to know about here are",
    "start": "647120",
    "end": "652720"
  },
  {
    "text": "uh that aging out of aging the objects out of cache you can use things like",
    "start": "652720",
    "end": "658720"
  },
  {
    "text": "at least recently used but essentially the object in the cache is not persistent it's the application's responsibility to",
    "start": "658720",
    "end": "665600"
  },
  {
    "text": "populate the cache and make sure that whenever things go in the cache if they are new if they don't exist",
    "start": "665600",
    "end": "670800"
  },
  {
    "text": "anywhere else that they are persisted somewhere because memcached is not going to persist it for you",
    "start": "670800",
    "end": "676560"
  },
  {
    "text": "so it's typically a persistent layer used in combination with memcached as a caching layer which is serving",
    "start": "676560",
    "end": "682399"
  },
  {
    "text": "an application front end a client api which is now using a key to get access to data",
    "start": "682399",
    "end": "688720"
  },
  {
    "text": "and then gets backup value so that's the usual architecture around memcached",
    "start": "688720",
    "end": "694480"
  },
  {
    "text": "the other things to keep in mind data could be anything it could be arbitrary data it could be html snippets it could",
    "start": "694480",
    "end": "700079"
  },
  {
    "text": "be session state it could be computed results in case of like recommendation engines and so on",
    "start": "700079",
    "end": "705360"
  },
  {
    "text": "where you do a lot of complicated calculations in the back end uh you want to store intermediate",
    "start": "705360",
    "end": "710399"
  },
  {
    "text": "um calculation results in in the cache so that you don't have to go through that calculation all over again",
    "start": "710399",
    "end": "716079"
  },
  {
    "text": "so it could be final result sets it could be intermediate result sets it could be raw data whichever",
    "start": "716079",
    "end": "721600"
  },
  {
    "text": "speeds up your application it's up to you to decide what the value part of that key value pair is",
    "start": "721600",
    "end": "727040"
  },
  {
    "text": "size wise restraints are the keys can be only 250 bytes values could be a megabyte max if you",
    "start": "727040",
    "end": "733920"
  },
  {
    "text": "have needs for caching data beyond a megabyte you'd have to figure out how to split it into multiple key values and then chain",
    "start": "733920",
    "end": "740320"
  },
  {
    "text": "them together so again more work for the application it's possible people use it that way as well",
    "start": "740320",
    "end": "746320"
  },
  {
    "text": "so that kind of concludes my um introduction of memcached i'll turn it over to",
    "start": "746320",
    "end": "752880"
  },
  {
    "text": "omar now to talk about elastic cache design cases patterns and some gotchas and",
    "start": "752880",
    "end": "759040"
  },
  {
    "text": "anti-patterns and so on so go on yeah thanks shaquille so now let's uh go into what amazon",
    "start": "759040",
    "end": "765839"
  },
  {
    "text": "elastic cache is so elastic is a fully managed web service that allows you to create",
    "start": "765839",
    "end": "771760"
  },
  {
    "text": "memcached compatible clusters in the cloud like all the aws",
    "start": "771760",
    "end": "777920"
  },
  {
    "text": "services elastic cache is designed to scale as you grow and enable you to you know pay just for what you use the diagram you're",
    "start": "777920",
    "end": "785279"
  },
  {
    "text": "seeing here describes how a typical application using elastic cache is deployed so you have the you know",
    "start": "785279",
    "end": "793279"
  },
  {
    "text": "application users right on top interacting with the web servers which are running ec2 and set up inside",
    "start": "793279",
    "end": "800000"
  },
  {
    "text": "an ect ac2 a security group the application stack is where you have your memcache client",
    "start": "800000",
    "end": "806560"
  },
  {
    "text": "library and also keeps track of the memcached node configurations so this is like the",
    "start": "806560",
    "end": "812079"
  },
  {
    "text": "endpoint information you then have your cache cluster that has that has its own security group so among the benefits of",
    "start": "812079",
    "end": "819839"
  },
  {
    "text": "elastic cache over memcached is that by default it is secure and you have to explicitly allow your",
    "start": "819839",
    "end": "825360"
  },
  {
    "text": "application tier to access the caching tier so memcached you know normally runs",
    "start": "825360",
    "end": "831040"
  },
  {
    "text": "wide open so you have to deal with this security uh by yourself the other feature of elastic that",
    "start": "831040",
    "end": "837760"
  },
  {
    "text": "is that you know we give you metrics as part of cloudwatch so that you can keep tabs on your cache clusters hit rate",
    "start": "837760",
    "end": "843279"
  },
  {
    "start": "843000",
    "end": "955000"
  },
  {
    "text": "you know eviction rate which will give you a sense for you know what's going on and if some nodes are getting",
    "start": "843279",
    "end": "848639"
  },
  {
    "text": "used more than the others uh you know you can sort of know about it now moving on to the primary benefits of",
    "start": "848639",
    "end": "854160"
  },
  {
    "text": "elastic cache and these are these are related to manageability so it makes it very easy to deploy it is",
    "start": "854160",
    "end": "859360"
  },
  {
    "text": "simple to create an elastic cache cluster uh using the aws console or the sdk api",
    "start": "859360",
    "end": "864800"
  },
  {
    "text": "then the five time i'll you know go into that i'd like to show you migration is straightforward we have designed it to be completely compatible",
    "start": "864800",
    "end": "871519"
  },
  {
    "text": "uh with memcached so if you're already using memcached in your application all you need to do is to you know update",
    "start": "871519",
    "end": "877440"
  },
  {
    "text": "your configuration with the endpoints of your elastic cache nodes and your application will work as is so it's easy to administer as we",
    "start": "877440",
    "end": "885120"
  },
  {
    "text": "have observers that monitor your cache nodes and if there is ever a problem uh we will replace them for you",
    "start": "885120",
    "end": "891279"
  },
  {
    "text": "a benefit of having your caching system live in elastic cache is that all your applications and web servers uh can talk to the same",
    "start": "891279",
    "end": "898240"
  },
  {
    "text": "cache so you get the benefit of a you know distributed cache system without having to actually maintain it",
    "start": "898240",
    "end": "904480"
  },
  {
    "text": "in each of your servers elastic cache is also easy to scale you know trivial to",
    "start": "904480",
    "end": "909680"
  },
  {
    "text": "add a new cache node so one of the challenges we've had with memcache in the past is that you have to reconfigure your",
    "start": "909680",
    "end": "915839"
  },
  {
    "text": "memcached client in your application each time you add or remove a cache node from your cluster",
    "start": "915839",
    "end": "921360"
  },
  {
    "text": "so the auto discovery client that we have released make this problem go away especially for customers that",
    "start": "921360",
    "end": "927600"
  },
  {
    "text": "like to move up and down on their cluster sizes now on the security side",
    "start": "927600",
    "end": "932800"
  },
  {
    "text": "it is easy to secure and control access to your memcache layer and decide what aspects of your application",
    "start": "932800",
    "end": "939199"
  },
  {
    "text": "architecture will be able to talk to it so at node creation time you may also choose for the cluster",
    "start": "939199",
    "end": "945360"
  },
  {
    "text": "to be part of a vpc you know giving very robust security so moving on to some common you",
    "start": "945360",
    "end": "952240"
  },
  {
    "text": "know use cases and design patterns so you know as uws customers you know",
    "start": "952240",
    "end": "958079"
  },
  {
    "text": "achieve adoption of their products and services you know maintaining and improving customer experience almost always",
    "start": "958079",
    "end": "963680"
  },
  {
    "text": "requires a caching tier that can provide close to in-memory performance you know very low latency high throughput",
    "start": "963680",
    "end": "969279"
  },
  {
    "text": "and so besides improving you know the customer experience and differentiating the service caching also provides cost reduction",
    "start": "969279",
    "end": "975360"
  },
  {
    "text": "value as uh the caching tier can take away the load of read heavy workloads on the storage",
    "start": "975360",
    "end": "981120"
  },
  {
    "text": "end thereby you know mitigating the need to really scale your backend database as shakil was mentioned before",
    "start": "981120",
    "end": "986800"
  },
  {
    "text": "this week caching provides a cost effective way to get more out of your existing infrastructure so",
    "start": "986800",
    "end": "992320"
  },
  {
    "text": "even when costs you know are set aside the caching tier is able to provide lower latency and",
    "start": "992320",
    "end": "997680"
  },
  {
    "text": "almost linear horizontal scalability than backend databases which you know must deal with tasks such as transactions query",
    "start": "997680",
    "end": "1004079"
  },
  {
    "text": "processing persistence which are typically not expected from a cache",
    "start": "1004079",
    "end": "1009440"
  },
  {
    "text": "so we have elastic customers in a range of categories as you can see here you know social networks as you can",
    "start": "1009440",
    "end": "1014639"
  },
  {
    "text": "imagine have a large number of users reading and writing and interacting with content for them low latency is critical on the",
    "start": "1014639",
    "end": "1022480"
  },
  {
    "text": "gaming side we see elastic cache here being used for providing fast responses",
    "start": "1022480",
    "end": "1027600"
  },
  {
    "text": "um and for maintaining transient data such as what actions people have taken what the scores are and things like that",
    "start": "1027600",
    "end": "1034000"
  },
  {
    "text": "um media sites again these tend to be very read-centric and provide low response",
    "start": "1034000",
    "end": "1040000"
  },
  {
    "text": "times for the customers on the q a portals for instance you will see that the dominant behavior is that",
    "start": "1040000",
    "end": "1046079"
  },
  {
    "text": "you have more readers and writers so again you know caching really helps here",
    "start": "1046079",
    "end": "1051520"
  },
  {
    "text": "e-commerce use use cases similar to gaming in that a end user will interact with",
    "start": "1051520",
    "end": "1056880"
  },
  {
    "text": "the site while they do their purchase and all that can be stored in a cache you know related to this is",
    "start": "1056880",
    "end": "1062559"
  },
  {
    "text": "advertising where low latency is very important moving on to recommendation engines",
    "start": "1062559",
    "end": "1068880"
  },
  {
    "text": "again several interesting use cases here so we typically see customers storing expensive computations",
    "start": "1068880",
    "end": "1074720"
  },
  {
    "text": "and intermediate results in memory in the elastic cache and because you don't want to have to recompute these results every",
    "start": "1074720",
    "end": "1081600"
  },
  {
    "text": "time so they store this in the cache and look them up as they go again providing you know better response times",
    "start": "1081600",
    "end": "1087679"
  },
  {
    "text": "lastly another area where we have seen a lot of growth is mobile apps here you typically have a much lower",
    "start": "1087679",
    "end": "1092880"
  },
  {
    "text": "bandwidth connection so providing fast response times and low latency is essential also mobile devices rely on",
    "start": "1092880",
    "end": "1100160"
  },
  {
    "text": "the back end for a lot of computation and data which again should be rapidly accessible especially",
    "start": "1100160",
    "end": "1106640"
  },
  {
    "text": "when they are you know large number of devices accessing it um so here what you see in the screen is",
    "start": "1106640",
    "end": "1113520"
  },
  {
    "text": "a typical gaming deployment so it's also a way to see you know how to combine",
    "start": "1113520",
    "end": "1118880"
  },
  {
    "start": "1114000",
    "end": "1189000"
  },
  {
    "text": "a caching layer with a range of services we offer here at aws so walking through the",
    "start": "1118880",
    "end": "1124080"
  },
  {
    "text": "system um on the bottom left you have users coming in through a load balancer",
    "start": "1124080",
    "end": "1129600"
  },
  {
    "text": "which is then connected to an application front end uh with an auto scaling group and which can uh you know potentially",
    "start": "1129600",
    "end": "1136880"
  },
  {
    "text": "scale based on the changes in traffic so the web fear is uh then connected to an amazon elastic",
    "start": "1136880",
    "end": "1142480"
  },
  {
    "text": "stack uh to have a low latency response time to scores in this example you know what the player",
    "start": "1142480",
    "end": "1148000"
  },
  {
    "text": "is currently doing use user session state etc so this is typically used in front of amazon rds",
    "start": "1148000",
    "end": "1154400"
  },
  {
    "text": "which you can see here is set up in a multi-ac deployment where you know multi-ac is",
    "start": "1154400",
    "end": "1160799"
  },
  {
    "text": "rds feature that allows you to have a lot of have a hot standby in a second daisy in the event",
    "start": "1160799",
    "end": "1166559"
  },
  {
    "text": "of any issues so typically what you have is dynamic content with low latency and high throughput",
    "start": "1166559",
    "end": "1173440"
  },
  {
    "text": "coming out of your elastic cache but the larger objects coming out of s3 and cloudfront",
    "start": "1173440",
    "end": "1178559"
  },
  {
    "text": "and so these are sort of composed together here to provide a rich gaming experience but also",
    "start": "1178559",
    "end": "1183840"
  },
  {
    "text": "uh be able to provide you know high responsiveness",
    "start": "1183840",
    "end": "1189200"
  },
  {
    "start": "1189000",
    "end": "1323000"
  },
  {
    "text": "so some of the you know common design patterns in the use cases that we see are uh you know one very common uh use of",
    "start": "1189200",
    "end": "1196320"
  },
  {
    "text": "caching uh is database offloading so this allows your solution to be much more scalable",
    "start": "1196320",
    "end": "1202559"
  },
  {
    "text": "and responsive on the front front end by having a low latency path to the application data without having",
    "start": "1202559",
    "end": "1209440"
  },
  {
    "text": "to go to the backend database so the main benefit here is that you can independently scale your front end",
    "start": "1209440",
    "end": "1215919"
  },
  {
    "text": "to cater to a spiking load for instance without having to grow the costly backend database",
    "start": "1215919",
    "end": "1220960"
  },
  {
    "text": "and i would like to emphasize that dealing with spikey loads is not only about you know ensuring uh lower latency but",
    "start": "1220960",
    "end": "1228320"
  },
  {
    "text": "often to be able to handle a very high request rate for which a cash is perfect the other design pattern we see",
    "start": "1228320",
    "end": "1235360"
  },
  {
    "text": "is your session management this comes up often and the users are having rapid interactions with the site for example",
    "start": "1235360",
    "end": "1240480"
  },
  {
    "text": "in e-commerce or gaming use cases and rather than deal with a round-trip latency with a database every",
    "start": "1240480",
    "end": "1246320"
  },
  {
    "text": "time you know operating out of cash is uh more convenient so one thing to remember is that",
    "start": "1246320",
    "end": "1252080"
  },
  {
    "text": "typically you do want to provide persistence for this user state and want to manage that at the",
    "start": "1252080",
    "end": "1257760"
  },
  {
    "text": "application level but you know you can do that asynchronously here you use the cache to",
    "start": "1257760",
    "end": "1263360"
  },
  {
    "text": "get a response time and high request rate and then use a database for persistence",
    "start": "1263360",
    "end": "1268640"
  },
  {
    "text": "a related design pattern is to track key statistics of your entire user base",
    "start": "1268640",
    "end": "1273679"
  },
  {
    "text": "in the cache for example the example of this uh you know would be you may keep trends uh top",
    "start": "1273679",
    "end": "1279280"
  },
  {
    "text": "feeds uh leaderboards for games so in the final part um what one talk about we see a lot of",
    "start": "1279280",
    "end": "1286080"
  },
  {
    "text": "usage in maintaining things that are expensive to compute so we see this a lot in social you know",
    "start": "1286080",
    "end": "1292640"
  },
  {
    "text": "type of use cases such as lists for popular restaurants and these are expensive at scale to uh",
    "start": "1292640",
    "end": "1298640"
  },
  {
    "text": "to do so people keep these around and update them periodically and you know serve them from the cache so for instance you know",
    "start": "1298640",
    "end": "1305200"
  },
  {
    "text": "you could have a bunch of picture ids which have been tagged you want to do that tagging you know a priority based on you know",
    "start": "1305200",
    "end": "1311600"
  },
  {
    "text": "whatever computation you have and uh so an example would be you could store all those tags",
    "start": "1311600",
    "end": "1317039"
  },
  {
    "text": "associated with the pictures in your cache so moving on to some anti-patterns that",
    "start": "1317039",
    "end": "1324159"
  },
  {
    "start": "1323000",
    "end": "1534000"
  },
  {
    "text": "we have seen you know talking to a lot of our customers and helping them troubleshoot what we see fairly often is that people",
    "start": "1324159",
    "end": "1330080"
  },
  {
    "text": "will assume a certain amount of latency you know certain amount of caches and then design their applications in a way",
    "start": "1330080",
    "end": "1336640"
  },
  {
    "text": "that ends up uh you know making them somewhat brittle i would say and so what we recommend is that you",
    "start": "1336640",
    "end": "1342720"
  },
  {
    "text": "should assume uh especially in a disabled environment that you may need to deal with",
    "start": "1342720",
    "end": "1347760"
  },
  {
    "text": "unpredictability uh from time to time and build that into your application logic so that you have a you",
    "start": "1347760",
    "end": "1354799"
  },
  {
    "text": "know graceful way of dealing with any uh untoward situations that may occur",
    "start": "1354799",
    "end": "1360400"
  },
  {
    "text": "another issue that people run into is not really thinking about maintaining consistency",
    "start": "1360400",
    "end": "1366240"
  },
  {
    "text": "you know how long should an object actually live in your cache so memcache allows you to set a time to live which essentially lets the cache",
    "start": "1366240",
    "end": "1373440"
  },
  {
    "text": "know when an object has expired so this helps you maintain consistency on the",
    "start": "1373440",
    "end": "1378480"
  },
  {
    "text": "cache side because you can set a time to live that is appropriate for the type of data",
    "start": "1378480",
    "end": "1383679"
  },
  {
    "text": "you're dealing with so you don't you know serve data to your users or customers",
    "start": "1383679",
    "end": "1388720"
  },
  {
    "text": "that could be still so another area we have seen where users do something sub optimal sometimes is is",
    "start": "1388720",
    "end": "1394480"
  },
  {
    "text": "to sequentially send a single operations to the cache so what happens here is that you have the overhead of making",
    "start": "1394480",
    "end": "1401120"
  },
  {
    "text": "the request and waiting to get a response whereas memcached apis provide you the ability",
    "start": "1401120",
    "end": "1406640"
  },
  {
    "text": "to do batch operations so you only incur the connection overhead once and then get back in multiple result",
    "start": "1406640",
    "end": "1413440"
  },
  {
    "text": "sets so test upon this briefly before about this redistribution uh for when nodes go down or if they are",
    "start": "1413440",
    "end": "1420400"
  },
  {
    "text": "unreachable so generally the best practice is to set up your clients so that they wait when a node goes down",
    "start": "1420400",
    "end": "1426400"
  },
  {
    "text": "and not redistribute the key space among these surviving nodes this is because the situation may be",
    "start": "1426400",
    "end": "1432240"
  },
  {
    "text": "transient and in the case of elastic cache where we monitor nodes and if the node",
    "start": "1432240",
    "end": "1437279"
  },
  {
    "text": "fails you'll actually replace them in the unlikely event that that happens so waiting is best because only the",
    "start": "1437279",
    "end": "1443279"
  },
  {
    "text": "requests going to the failed node are impacted and the rest of the application can you know work at a",
    "start": "1443279",
    "end": "1448400"
  },
  {
    "text": "at a very high performance level the other thing sometimes people do is resolve the dns just once",
    "start": "1448400",
    "end": "1454720"
  },
  {
    "text": "when we replace the node we will map the ip name to the new ip address so however the memcached client should",
    "start": "1454720",
    "end": "1461120"
  },
  {
    "text": "resolve this from the dns but most of the memcached clients do not do that",
    "start": "1461120",
    "end": "1466880"
  },
  {
    "text": "however if you use the auto discovery client that we have released this should be taking care of you but this is something to kind of keep",
    "start": "1466880",
    "end": "1471919"
  },
  {
    "text": "in mind the the other thing is sometimes what uh customers would do",
    "start": "1471919",
    "end": "1477679"
  },
  {
    "text": "they would fill the application request on a cache miss we've seen this come up again you want to have your quote path",
    "start": "1477679",
    "end": "1483120"
  },
  {
    "text": "that will handle this and fetch it from the backend store from time to time we also see you know",
    "start": "1483120",
    "end": "1488640"
  },
  {
    "text": "memcached elastic cache being used as a durable store again this is the in memory cache so the application should be architected",
    "start": "1488640",
    "end": "1495600"
  },
  {
    "text": "to be able to lose the cache resident data for you know whatever reason so the same when you should not be using",
    "start": "1495600",
    "end": "1500720"
  },
  {
    "text": "memcached as your primary uh data store lastly i want to talk about error paths you know make sure",
    "start": "1500720",
    "end": "1507120"
  },
  {
    "text": "that the application does not fail or freeze if there's a timeout or something similar happens",
    "start": "1507120",
    "end": "1512640"
  },
  {
    "text": "really it's about you know planning for things that you may not have expected to occur",
    "start": "1512640",
    "end": "1517919"
  },
  {
    "text": "and allowing your applications to go on so that at least while some",
    "start": "1517919",
    "end": "1523120"
  },
  {
    "text": "portion of your users may have sub-optimal experience but where the vast majority would you know function just uh just fine",
    "start": "1523120",
    "end": "1531600"
  },
  {
    "text": "so talking about some best practices uh one thing is you know share your objects",
    "start": "1532000",
    "end": "1538240"
  },
  {
    "start": "1534000",
    "end": "1664000"
  },
  {
    "text": "with elastic cache or memcache you have a caching layer that sits below your web layer and all of your web servers can talk to",
    "start": "1538240",
    "end": "1544320"
  },
  {
    "text": "the same elastic cluster so by sharing this cache cluster and providing entire web tier access to it you can",
    "start": "1544320",
    "end": "1551440"
  },
  {
    "text": "actually benefit from usage patterns across all your tiers the other thing i would say use details",
    "start": "1551440",
    "end": "1556960"
  },
  {
    "text": "think about the implications of long and short details ultimately your ttl setting will",
    "start": "1556960",
    "end": "1563200"
  },
  {
    "text": "depend on your application and what level of consistency you need between your cache and the authoritative data source the other aspect that can",
    "start": "1563200",
    "end": "1569919"
  },
  {
    "text": "impact this is your logic to deal with modified data so ttls give you an easy way to deal with that uh",
    "start": "1569919",
    "end": "1575679"
  },
  {
    "text": "you know the other way would be to track when the data changes and update your cache as well that's a little complicated so most people prefer to use ttls think",
    "start": "1575679",
    "end": "1583600"
  },
  {
    "text": "about the memory you need for connections the memcacity requires memory for each connection that's the",
    "start": "1583600",
    "end": "1589360"
  },
  {
    "text": "connection overhead now this memory needs to be allocated outside of the objects in the cache and you can control this",
    "start": "1589360",
    "end": "1595520"
  },
  {
    "text": "using the statement cache the connections overhead parameter the other thing is",
    "start": "1595520",
    "end": "1601120"
  },
  {
    "text": "take advantage of the cloudwatch alarms don't be surprised when things happen we provide a lot of metrics to",
    "start": "1601120",
    "end": "1606640"
  },
  {
    "text": "look into how your cache is performing so consistent hashing we have mentioned the benefit here",
    "start": "1606640",
    "end": "1612240"
  },
  {
    "text": "is that if there is a problem with your node consistent hashing will minimize the impact on your cluster so in a you",
    "start": "1612240",
    "end": "1618799"
  },
  {
    "text": "know in a simple module hashing scheme let's say you had six nodes and then one node went down the model denominator",
    "start": "1618799",
    "end": "1625200"
  },
  {
    "text": "changes so cache will actually reshuffle all the data in the cluster and that will create a massive problem on the application side because",
    "start": "1625200",
    "end": "1631840"
  },
  {
    "text": "every single user of the application will have a negative experience uh user experience while data",
    "start": "1631840",
    "end": "1637600"
  },
  {
    "text": "is being redistributed whereas if you're using consistent hashing only the node adjacent to that will be",
    "start": "1637600",
    "end": "1642960"
  },
  {
    "text": "impacted and majority of your users will not really face any trouble the other thing we recommend is",
    "start": "1642960",
    "end": "1648559"
  },
  {
    "text": "uh to use auto discovery i kind of mentioned that before this does away with the need to restart your application client",
    "start": "1648559",
    "end": "1654960"
  },
  {
    "text": "every time you make a change to your cluster and add a node or remove a node",
    "start": "1654960",
    "end": "1662320"
  },
  {
    "text": "so you know having covered caching its benefits you know memcached and elastic so far let's walk through a",
    "start": "1662320",
    "end": "1669919"
  },
  {
    "text": "really a simple example here of uh adding a cache for a you know simple web application",
    "start": "1669919",
    "end": "1676240"
  },
  {
    "start": "1676000",
    "end": "1912000"
  },
  {
    "text": "so let's say that uh you know we want to improve the performance of an existing web app",
    "start": "1676240",
    "end": "1682640"
  },
  {
    "text": "running on ec2 so the scenario we want to think about is that you want to handle an impending",
    "start": "1682640",
    "end": "1688399"
  },
  {
    "text": "spike in traffic which would require the ability to handle a large number of requests so in our setup here",
    "start": "1688399",
    "end": "1696320"
  },
  {
    "text": "the web app stores this data in a mysql database so upon user request it queries the",
    "start": "1696320",
    "end": "1701679"
  },
  {
    "text": "database and returns the results pretty simple example i use an rds mysql here but this is applicable to just",
    "start": "1701679",
    "end": "1708000"
  },
  {
    "text": "you know about any backend data stored layer so caching can be added in uh you know two simple steps uh what you",
    "start": "1708000",
    "end": "1714720"
  },
  {
    "text": "want to do is create the caching layer first here we can uh what we show you is creating an elastic cache cluster with",
    "start": "1714720",
    "end": "1720640"
  },
  {
    "text": "two nodes and then you know you modify your existing web app to use amazon elastic",
    "start": "1720640",
    "end": "1726240"
  },
  {
    "text": "and then once you do this you will see the performance improvement uh one note here is the actual",
    "start": "1726240",
    "end": "1731440"
  },
  {
    "text": "performance improvement you will realize will really depend on a lot of factors so the most important of those we talked",
    "start": "1731440",
    "end": "1737520"
  },
  {
    "text": "about earlier which are you know read write mix the you know combined working set of applications etc",
    "start": "1737520",
    "end": "1744960"
  },
  {
    "text": "so what i'll do now is uh you know go into our aws console and just create a simple two node",
    "start": "1744960",
    "end": "1751520"
  },
  {
    "text": "elastic cache cluster i'm just gonna do the screen sharing here so just bear with me",
    "start": "1751520",
    "end": "1758799"
  },
  {
    "text": "all right",
    "start": "1760320",
    "end": "1767840"
  },
  {
    "text": "it's almost there",
    "start": "1777279",
    "end": "1781840"
  },
  {
    "text": "so can we see the screen not yet okay how about now",
    "start": "1783120",
    "end": "1791200"
  },
  {
    "text": "yep so this is our aws console so in order to launch a",
    "start": "1792840",
    "end": "1798480"
  },
  {
    "text": "uh simple as the cache cluster you would click on launch cache cluster in this case you know just call it a",
    "start": "1798480",
    "end": "1805120"
  },
  {
    "text": "demo cluster here so i'm going to create the i'm going to choose a node type of m1 small",
    "start": "1805120",
    "end": "1810880"
  },
  {
    "text": "i'm going to go ahead and put uh you know two nodes here uh leave everything else as this so we",
    "start": "1810880",
    "end": "1816559"
  },
  {
    "text": "have you know the default cache port for this example we'll do it not in the vpc",
    "start": "1816559",
    "end": "1822399"
  },
  {
    "text": "and then we'll just you know disable notifications and then continue so here what we do is uh you know use",
    "start": "1822399",
    "end": "1828880"
  },
  {
    "text": "the cache security group so this basically allows you to decide which ec2 machines get access to your cache node so we'll use the default one",
    "start": "1828880",
    "end": "1835440"
  },
  {
    "text": "here then you also specify the cache parameter group this is really a container for you know",
    "start": "1835440",
    "end": "1842000"
  },
  {
    "text": "the memcache the engine configuration values and this can be you know applied to all",
    "start": "1842000",
    "end": "1847600"
  },
  {
    "text": "all the nodes that you you create and then finally you just you know kick off the creation so what it does is",
    "start": "1847600",
    "end": "1854799"
  },
  {
    "text": "before that you just kind of summarize uh the values you have picked and then you just launch cluster and it goes ahead and takes some time",
    "start": "1854799",
    "end": "1861039"
  },
  {
    "text": "so after the the nodes are created so now you can see it's actually creating attempt this is what you will actually see uh",
    "start": "1861039",
    "end": "1867679"
  },
  {
    "text": "once the nodes are ready so this has a summary of uh you know what we did you would have uh it specifies",
    "start": "1867679",
    "end": "1873440"
  },
  {
    "text": "the class of the class uh the cache node you did and how many cache nodes were over there",
    "start": "1873440",
    "end": "1878880"
  },
  {
    "text": "you also have the configuration endpoint the single configuration endpoint which you can use with your auto discovery uh client to go into more",
    "start": "1878880",
    "end": "1887200"
  },
  {
    "text": "information on the nodes here you have the endpoint information on on both the nodes",
    "start": "1887200",
    "end": "1894480"
  },
  {
    "text": "let's go to the other part now and",
    "start": "1895360",
    "end": "1899679"
  },
  {
    "text": "go ahead and modify the web application",
    "start": "1901039",
    "end": "1906159"
  },
  {
    "text": "just want to make sure that you can see my slides great so we have created the cache",
    "start": "1906159",
    "end": "1913039"
  },
  {
    "start": "1912000",
    "end": "2033000"
  },
  {
    "text": "cluster now and now what we want to do is modify the web application so that you know we can enable caching",
    "start": "1913039",
    "end": "1918720"
  },
  {
    "text": "so to add caching first find the code that retrieves the value from a mac backend database so you need to augment that",
    "start": "1918720",
    "end": "1926559"
  },
  {
    "text": "that to add code to first look into a caching layer so the python snippet that you see here is from our web app",
    "start": "1926559",
    "end": "1932000"
  },
  {
    "text": "so what the code above does here is initially it will uh and import the memcache library it will",
    "start": "1932000",
    "end": "1938720"
  },
  {
    "text": "uh you know create connection to the memcache nodes and you specify the endpoints that i just showed you from",
    "start": "1938720",
    "end": "1945440"
  },
  {
    "text": "the console you can get that those end points using sdk as well and uh so once you do that each time you",
    "start": "1945440",
    "end": "1951519"
  },
  {
    "text": "are looking up for some data you will first check whether it is in memcache and if it's",
    "start": "1951519",
    "end": "1956960"
  },
  {
    "text": "not there then you'll go ahead and get from a backend database and then you'll set the value in uh",
    "start": "1956960",
    "end": "1962240"
  },
  {
    "text": "in memcache that the next time you need the same value you're just going to go ahead and have a cache hit",
    "start": "1962240",
    "end": "1969440"
  },
  {
    "text": "so what you see here is uh you know what a web app but look once caching is enabled",
    "start": "1971760",
    "end": "1976880"
  },
  {
    "text": "so you added two nodes for this demo setup where there were two m1 smalls",
    "start": "1976880",
    "end": "1982159"
  },
  {
    "text": "and going from left to right what you you start with the end user browser generating requests so they hit the load balancer then",
    "start": "1982159",
    "end": "1989600"
  },
  {
    "text": "distribute the load across the two web server nodes uh running in ec2 and after the web server uh",
    "start": "1989600",
    "end": "1995760"
  },
  {
    "text": "at the web server the code snippet we showed you before runs so it checks if the cache nodes have the content so these",
    "start": "1995760",
    "end": "2002000"
  },
  {
    "text": "were the two you know m1 small elastic cache nodes if there's a hit the content is returned on it and on a cache miss",
    "start": "2002000",
    "end": "2008399"
  },
  {
    "text": "you go actually to the rds mysql database so for this demo we had rds mysql 11 small",
    "start": "2008399",
    "end": "2015840"
  },
  {
    "text": "we had two cache m1 small nodes and the web servers are simulated inside",
    "start": "2015840",
    "end": "2022080"
  },
  {
    "text": "the multi-mechanized performance tool which is running on a m24 xl",
    "start": "2022080",
    "end": "2027600"
  },
  {
    "text": "ec2 instance so after all that you want to see you",
    "start": "2027600",
    "end": "2033600"
  },
  {
    "start": "2033000",
    "end": "2144000"
  },
  {
    "text": "know what performance improvement has happened so as i was mentioning we you know ran this using multi-mechanized",
    "start": "2033600",
    "end": "2038720"
  },
  {
    "text": "uh for this simple example you know it shows 600 second runtime and we collected data every five seconds",
    "start": "2038720",
    "end": "2044640"
  },
  {
    "text": "and elastic the caching layer was enabled at the 200 seconds mark so what we want to do now",
    "start": "2044640",
    "end": "2052398"
  },
  {
    "text": "is just go ahead and see the performance results so i'm again i'm going to do a screen share",
    "start": "2052399",
    "end": "2061839"
  },
  {
    "text": "not sure if you can see my screen here uh great so you know once you run uh when multi",
    "start": "2072480",
    "end": "2077679"
  },
  {
    "text": "mechanize your output will look something like this so what we have uh i'll just jump to the graph directly",
    "start": "2077679",
    "end": "2083679"
  },
  {
    "text": "so what you see is that you know you had initially response times which are averaging about you know a little",
    "start": "2083679",
    "end": "2089919"
  },
  {
    "text": "more than a second so in this example what you wanted to simulate is you know you have a back end",
    "start": "2089919",
    "end": "2095118"
  },
  {
    "text": "database and suddenly you get a spikey workload so you'll see the response times kind of go up and then but if you had caching you",
    "start": "2095119",
    "end": "2101920"
  },
  {
    "text": "start hitting in the cache and the response times actually go down so in this example it goes down to about you know point uh sorry about one",
    "start": "2101920",
    "end": "2108079"
  },
  {
    "text": "millisecond uh here now with respect to uh the transactions per second",
    "start": "2108079",
    "end": "2113520"
  },
  {
    "text": "uh initially you know once the load started hitting the back end database the spiky workload",
    "start": "2113520",
    "end": "2118640"
  },
  {
    "text": "you were doing around 80 tps and then after you enabled caching uh it went up to about roughly 1400",
    "start": "2118640",
    "end": "2126560"
  },
  {
    "text": "tps so the multi-mechanize you know just shows you the results uh over here so that concludes the",
    "start": "2126560",
    "end": "2133520"
  },
  {
    "text": "uh example and the result so i'm gonna hand it back to uh shaquille",
    "start": "2133520",
    "end": "2140960"
  },
  {
    "text": "thanks so much um so what you've seen so far is is an",
    "start": "2142400",
    "end": "2148400"
  },
  {
    "start": "2144000",
    "end": "2202000"
  },
  {
    "text": "introduction of memcache some introduction to elastic cache some advanced topics uh and and then the actual results",
    "start": "2148400",
    "end": "2154400"
  },
  {
    "text": "comparing the kind of performance you get with or without the caching layer the remaining session we can handle",
    "start": "2154400",
    "end": "2162000"
  },
  {
    "text": "questions and answers so a few clarifications up front and we will be posting the slides and the recording",
    "start": "2162000",
    "end": "2168560"
  },
  {
    "text": "so you should get a link to that after today's session so you you'll have it in your inbox um any",
    "start": "2168560",
    "end": "2175280"
  },
  {
    "text": "questions that you have please type them away into your q a box and and uh we'll take them during the session as",
    "start": "2175280",
    "end": "2181599"
  },
  {
    "text": "many as we can get through today so starting from the ones that we got",
    "start": "2181599",
    "end": "2187280"
  },
  {
    "text": "earlier uh omar the questions are is there a way to auto scale your elastic cache or in",
    "start": "2187280",
    "end": "2192560"
  },
  {
    "text": "general how do you scale your application as you're doing auto scaling on your web servers how do you go about scaling your",
    "start": "2192560",
    "end": "2199040"
  },
  {
    "text": "your caching layer as well sure um so typically what we have seen uh",
    "start": "2199040",
    "end": "2204640"
  },
  {
    "start": "2202000",
    "end": "2334000"
  },
  {
    "text": "customers do for you know for scan the caching layer they uh they",
    "start": "2204640",
    "end": "2209680"
  },
  {
    "text": "determine you know by roughly typically people use you know percentages like what their workload is",
    "start": "2209680",
    "end": "2215280"
  },
  {
    "text": "going to increase so they sort of predict what the uh workload is going to be and then they appropriately you know add a cache note",
    "start": "2215280",
    "end": "2222720"
  },
  {
    "text": "and one of the benefits of you know elastic cache mm cache in general is it's a scale-out solution so as you need that you can you know add these nodes",
    "start": "2222720",
    "end": "2229440"
  },
  {
    "text": "uh scale them out and then turn them down so for instance let's say that you're expecting a you know major event to occur",
    "start": "2229440",
    "end": "2235119"
  },
  {
    "text": "um you know uh you can estimate uh what the traffic is roughly going to be so you definitely want to have some you know comfort room",
    "start": "2235119",
    "end": "2242160"
  },
  {
    "text": "above and beyond what you can expect uh especially basically mission critical you go ahead and scale out your cash",
    "start": "2242160",
    "end": "2249359"
  },
  {
    "text": "notes by adding more and then you know once the situation has passed",
    "start": "2249359",
    "end": "2254560"
  },
  {
    "text": "you could you know bring it down so that's uh something like an elastic cache is ideal",
    "start": "2254560",
    "end": "2259920"
  },
  {
    "text": "for that all right um another question is about why do you want to use multiple nodes what",
    "start": "2259920",
    "end": "2267119"
  },
  {
    "text": "is what's the benefit is that reduced uh performance i mean improved performance is it redundancy is it",
    "start": "2267119",
    "end": "2273440"
  },
  {
    "text": "scalability what is getting multiple nodes by you sure",
    "start": "2273440",
    "end": "2279119"
  },
  {
    "text": "you know i think it's um it's a little of all of those but the primary uh benefit is",
    "start": "2279119",
    "end": "2285440"
  },
  {
    "text": "uh is for scalability and uh what are the other sort of uh things mentioned over here yeah in",
    "start": "2285440",
    "end": "2291280"
  },
  {
    "text": "redundancy primary skills and a sort of say redundancy as well because you sort of have multiple nodes",
    "start": "2291280",
    "end": "2296880"
  },
  {
    "text": "so your key space is is distributed across them so it's not",
    "start": "2296880",
    "end": "2302000"
  },
  {
    "text": "in true sense redundancy but in a sense it's increased availability because if",
    "start": "2302000",
    "end": "2307760"
  },
  {
    "text": "you know one node goes down it only impacts a small part of your key space but the primary reason to do that is is for scalability",
    "start": "2307760",
    "end": "2315119"
  },
  {
    "text": "all right so another question um do you have any advice on connection timeouts",
    "start": "2315119",
    "end": "2321440"
  },
  {
    "text": "in this case they have surges of timeouts every few days even after modifying their application",
    "start": "2321440",
    "end": "2327040"
  },
  {
    "text": "to attempt during such events so what's the best advice to handle connection timeouts",
    "start": "2327040",
    "end": "2333599"
  },
  {
    "text": "yes so connection timeout is a very important thing to deal with having worked with a lot of customers in",
    "start": "2333920",
    "end": "2341359"
  },
  {
    "start": "2334000",
    "end": "2431000"
  },
  {
    "text": "the past couple of years this is a recurring problem that we see",
    "start": "2341359",
    "end": "2346640"
  },
  {
    "text": "so uh connection panels do often happen for various reasons uh because of the you know",
    "start": "2346640",
    "end": "2353440"
  },
  {
    "text": "contention in the network or contention among other compute resources that are running",
    "start": "2353440",
    "end": "2359040"
  },
  {
    "text": "in the cloud so uh having a clear fallback mechanism",
    "start": "2359040",
    "end": "2364560"
  },
  {
    "text": "when you get time out it's very important so what are the things that can be done are number one",
    "start": "2364560",
    "end": "2371359"
  },
  {
    "text": "you can always fall back to your durable store the durable store could be your database or your nosql store so",
    "start": "2371359",
    "end": "2377520"
  },
  {
    "text": "you can fall back to your durable store to get the data so that that request is successfully completed",
    "start": "2377520",
    "end": "2383520"
  },
  {
    "text": "for your customers what happens is uh people often fail the",
    "start": "2383520",
    "end": "2388800"
  },
  {
    "text": "request hence your customer in turn get a bad experience they get a 404 or you know some server side errors",
    "start": "2388800",
    "end": "2396480"
  },
  {
    "text": "which is bad for your customers so instead if you fall back to your durable store get the",
    "start": "2396480",
    "end": "2402320"
  },
  {
    "text": "necessary data and complete the customer request successfully that would be a better experience",
    "start": "2402320",
    "end": "2407520"
  },
  {
    "text": "so having a clear fallback path is very important the second thing is uh you can also",
    "start": "2407520",
    "end": "2415520"
  },
  {
    "text": "have a retrain mechanism based on how sensitive you are to the latency",
    "start": "2415520",
    "end": "2421440"
  },
  {
    "text": "so you can retray configured appropriately it could be",
    "start": "2421440",
    "end": "2426640"
  },
  {
    "text": "two or it could be ten depending on the application all right thank you very much so uh",
    "start": "2426640",
    "end": "2433520"
  },
  {
    "start": "2431000",
    "end": "2689000"
  },
  {
    "text": "there's a few questions around pricing and billing so the first advice i'll give you on this one is uh you can check pricing very easily",
    "start": "2433520",
    "end": "2440319"
  },
  {
    "text": "just from our website uh when you go to the elastic cache page uh it describes all the the capabilities as well as all the",
    "start": "2440319",
    "end": "2447119"
  },
  {
    "text": "instance types and pricing and so on so pricing is pay as you go that's the basic most",
    "start": "2447119",
    "end": "2452640"
  },
  {
    "text": "easy way to consume elastic cash essentially you pay by the hour for every node of elastic cash that you",
    "start": "2452640",
    "end": "2458160"
  },
  {
    "text": "deploy in case you're using memcache obviously you're just running it on an ec2 node and then you're paying for",
    "start": "2458160",
    "end": "2463680"
  },
  {
    "text": "the ec2 prices but with elastic cash it's pay as you go on demand pricing based on the number of hours that you",
    "start": "2463680",
    "end": "2469280"
  },
  {
    "text": "use in elastic node so another question here is about is",
    "start": "2469280",
    "end": "2475359"
  },
  {
    "text": "there a way to check via a hash key whether a record is stored in cash which is faster than",
    "start": "2475359",
    "end": "2481280"
  },
  {
    "text": "calling get value for key the short answer is no any operation",
    "start": "2481280",
    "end": "2487359"
  },
  {
    "text": "that you do essentially has to rely on the fact that the key is either there or not there and so the speed of the operation doesn't really change",
    "start": "2487359",
    "end": "2493200"
  },
  {
    "text": "now if the question is coming from a perspective of the value being a huge value and you're incurring a lot of overhead trying to get that value back",
    "start": "2493200",
    "end": "2499839"
  },
  {
    "text": "and all you needed to check was whether it exists in cash or not you could use some some alternatives right",
    "start": "2499839",
    "end": "2505440"
  },
  {
    "text": "you could try and add the key again and if you succeed basically it didn't exist and then you'd have to call delete so if",
    "start": "2505440",
    "end": "2511520"
  },
  {
    "text": "the add and the delete is faster for you than just calling get you could do that um you could call an append with like a",
    "start": "2511520",
    "end": "2518240"
  },
  {
    "text": "very small string or a null string or something if that works for you because that again will depend on whether the key exists or",
    "start": "2518240",
    "end": "2523760"
  },
  {
    "text": "not so there aren't very easy ways of of optimizing that part but depending on your application some some",
    "start": "2523760",
    "end": "2530240"
  },
  {
    "text": "of these add delete or append type things might work all right um a question here around can elastic cache",
    "start": "2530240",
    "end": "2537119"
  },
  {
    "text": "be used uh from remotely can can it be accessed without vpc and the answer is yes it depends on how",
    "start": "2537119",
    "end": "2543280"
  },
  {
    "text": "you deploy elastic nodes you can deploy them uh with within or or outside of a vpc",
    "start": "2543280",
    "end": "2548880"
  },
  {
    "text": "and depending on how you deployed them the security policies of that zone or",
    "start": "2548880",
    "end": "2554000"
  },
  {
    "text": "region will apply so it totally depends on how exactly you're deploying them",
    "start": "2554000",
    "end": "2559839"
  },
  {
    "text": "another question what libraries are available for memcache so i'm guessing the question is around",
    "start": "2559839",
    "end": "2564880"
  },
  {
    "text": "memcache and elastic in general what what libraries what client apis are available um c c",
    "start": "2564880",
    "end": "2570800"
  },
  {
    "text": "sharp uh javascript etc yeah so so uh elastic cache is a 100 percent",
    "start": "2570800",
    "end": "2576640"
  },
  {
    "text": "memcached protocol compatible since uh you can download any memcached client",
    "start": "2576640",
    "end": "2581760"
  },
  {
    "text": "from the internet and use it so if you search for memcache decline so there are multiple sites which uh list the various",
    "start": "2581760",
    "end": "2589040"
  },
  {
    "text": "clients for each programming language so you could download any of them and start using it because of the 100",
    "start": "2589040",
    "end": "2594880"
  },
  {
    "text": "memcached protocol compatibility all right thanks so another question",
    "start": "2594880",
    "end": "2601119"
  },
  {
    "text": "here i'll give this for omar is talk a little bit about auto discovery",
    "start": "2601119",
    "end": "2606160"
  },
  {
    "text": "what is it how does it work sure uh so with auto discovery we really decided to uh you know go and address some of the",
    "start": "2606160",
    "end": "2612480"
  },
  {
    "text": "problems that customers are facing uh primarily uh having to restart their applications which uh you know would use a memcache",
    "start": "2612480",
    "end": "2619040"
  },
  {
    "text": "decline so the scenario was they would go ahead and add a node and then what would occur is that they",
    "start": "2619040",
    "end": "2625839"
  },
  {
    "text": "would have to restart the application because the part of the memcache the client code",
    "start": "2625839",
    "end": "2631040"
  },
  {
    "text": "that takes in which nodes are part of your elastic or rather you know memcached",
    "start": "2631040",
    "end": "2636160"
  },
  {
    "text": "cluster was right on top so once you started the client there was no way to really uh go ahead and you know refresh that",
    "start": "2636160",
    "end": "2642480"
  },
  {
    "text": "list so if you're using a auto discovery capable client and by the way we also kind of open",
    "start": "2642480",
    "end": "2649119"
  },
  {
    "text": "source the protocol that we use you would not have this problem so essentially what happens is each of",
    "start": "2649119",
    "end": "2655440"
  },
  {
    "text": "our memcached server nodes periodically return the list of",
    "start": "2655440",
    "end": "2662880"
  },
  {
    "text": "all the memcached the node that are part of the cluster so if the client is capable of reading that",
    "start": "2662880",
    "end": "2668800"
  },
  {
    "text": "information when it sees that change it is able to reconfigure itself and",
    "start": "2668800",
    "end": "2674480"
  },
  {
    "text": "you know we have described exactly how that happens so either you can use our uh auto discovery client for this",
    "start": "2674480",
    "end": "2680720"
  },
  {
    "text": "purpose or you can go ahead and add this protocol extension to your favorite client of choice",
    "start": "2680720",
    "end": "2688960"
  },
  {
    "text": "thanks so much so some more questions here around what's the difference between memcache",
    "start": "2688960",
    "end": "2695920"
  },
  {
    "start": "2689000",
    "end": "2783000"
  },
  {
    "text": "and elastic cache why would you use elastic cache over memcache um what's the",
    "start": "2695920",
    "end": "2701040"
  },
  {
    "text": "difference between running memcache on the same node as where your application runs versus running it in a separate cluster",
    "start": "2701040",
    "end": "2707040"
  },
  {
    "text": "somewhere so if you want to sure um the uh so i think with respect to the second part right which is",
    "start": "2707040",
    "end": "2712720"
  },
  {
    "text": "uh what is the difference between just running memcache with your own node uh versus kind of doing it in a",
    "start": "2712720",
    "end": "2718720"
  },
  {
    "text": "separate layer um that's more of an architecture question i think the the recommended practice what we would say is you would",
    "start": "2718720",
    "end": "2723920"
  },
  {
    "text": "want your caching layer to be independent of your application layer just because the uh needs to scale them",
    "start": "2723920",
    "end": "2730000"
  },
  {
    "text": "are different uh so you want to have that decoupling that just makes a robust you know architecture with respect to",
    "start": "2730000",
    "end": "2737040"
  },
  {
    "text": "what is why i would use elastic cache over memcached and just sort of managing it yourself",
    "start": "2737040",
    "end": "2742160"
  },
  {
    "text": "that really goes to the you know benefit of a managed service so the benefits that the elastic mat",
    "start": "2742160",
    "end": "2747280"
  },
  {
    "text": "service would provide is that we would monitor the nodes and there's a node failure we would deal with it and this is just one",
    "start": "2747280",
    "end": "2753760"
  },
  {
    "text": "uh one activity that is taken off the plate of our customer because you know we will handle that we will you know",
    "start": "2753760",
    "end": "2759440"
  },
  {
    "text": "upgrade the appropriate uh you know engine uh for for memcache d um also",
    "start": "2759440",
    "end": "2765920"
  },
  {
    "text": "we you know provide cloud watch alarms which are you know integrated with elastic which gives you a really good uh you",
    "start": "2765920",
    "end": "2771280"
  },
  {
    "text": "know visibility into how your cache is actually uh behaving so for those for those reasons i would use elastic cache over",
    "start": "2771280",
    "end": "2778079"
  },
  {
    "text": "memcaching all right um",
    "start": "2778079",
    "end": "2782720"
  },
  {
    "text": "another question here about when using elastic does the client need to know which key is stored on which node or is",
    "start": "2783119",
    "end": "2789119"
  },
  {
    "text": "it automatically available or resolved",
    "start": "2789119",
    "end": "2795440"
  },
  {
    "text": "so so in general essentially elastic cache and memcache are very similar in that sense because",
    "start": "2795440",
    "end": "2800839"
  },
  {
    "text": "um your hashing mechanism decides where you look to find the key value so",
    "start": "2800839",
    "end": "2808240"
  },
  {
    "text": "you do need to know exactly which which server which node that the cache",
    "start": "2808240",
    "end": "2814000"
  },
  {
    "text": "key value resides on in that sense it's not any different from memcache the difference is obviously when the",
    "start": "2814000",
    "end": "2819680"
  },
  {
    "text": "node becomes unavailable replacing it and and uh handling the auto discovery of newly",
    "start": "2819680",
    "end": "2825359"
  },
  {
    "text": "added nodes scaling that cluster up or down is much easier with elastic cache but in the sense that you need to know where to go",
    "start": "2825359",
    "end": "2831680"
  },
  {
    "text": "to get a specific key value that's the same as memcache the decision of uh how the keys are",
    "start": "2831680",
    "end": "2837760"
  },
  {
    "text": "distributed is decided on the memcached client so your client that you're using and the",
    "start": "2837760",
    "end": "2844640"
  },
  {
    "text": "hashing algorithm that you're using decides how the keys are distributed it is not on the server side all right another",
    "start": "2844640",
    "end": "2852400"
  },
  {
    "text": "question about how do you decide between large or small nodes when using",
    "start": "2852400",
    "end": "2857680"
  },
  {
    "text": "memcache or elastic hash which one do you pick when okay so there are two factors",
    "start": "2857680",
    "end": "2864640"
  },
  {
    "text": "that play into this one is how tolerant you want to be when there",
    "start": "2864640",
    "end": "2869760"
  },
  {
    "text": "is a node failure so distributing your keys among multiple nodes make you move makes you more tolerant to",
    "start": "2869760",
    "end": "2876720"
  },
  {
    "text": "failures and that's like one node fails you still have like half the keys in a different node that could take the",
    "start": "2876720",
    "end": "2882240"
  },
  {
    "text": "traffic that said uh going to bigger node size is uh big uh you get more memory and more",
    "start": "2882240",
    "end": "2889599"
  },
  {
    "text": "compute power so if you're looking to lower your latency then you might want to consider going to a bigger node",
    "start": "2889599",
    "end": "2895200"
  },
  {
    "text": "so it is a trade-off between how much performance you want to get and how much distribution you want to have to have",
    "start": "2895200",
    "end": "2902480"
  },
  {
    "text": "fault tolerance okay",
    "start": "2902480",
    "end": "2907680"
  },
  {
    "start": "2908000",
    "end": "2953000"
  },
  {
    "text": "so another question around best practices here what are the realistic alternatives to ttl for",
    "start": "2908640",
    "end": "2914960"
  },
  {
    "text": "cases when you need very accurate results and you cannot deserve backstail data basically so this",
    "start": "2914960",
    "end": "2920880"
  },
  {
    "text": "uh goes back to an example that shakil called out in the webinar during the earlier part of",
    "start": "2920880",
    "end": "2926800"
  },
  {
    "text": "where you proactively manage the values that are stored in the cache instead of relying on the ttls when you",
    "start": "2926800",
    "end": "2933839"
  },
  {
    "text": "change a value you can go ahead and change the value in the cache immediately so it's almost building a right through",
    "start": "2933839",
    "end": "2940960"
  },
  {
    "text": "a read through mechanism for your caching so that way you achieve higher consistency of the results without",
    "start": "2940960",
    "end": "2947359"
  },
  {
    "text": "securing cash all right thank you",
    "start": "2947359",
    "end": "2953039"
  },
  {
    "start": "2953000",
    "end": "3029000"
  },
  {
    "text": "so another question here about the automatic node replacement feature for uh for elastic uh can you speak a little",
    "start": "2953440",
    "end": "2960160"
  },
  {
    "text": "bit about that what is the latency and so on yeah so typically we replace the node",
    "start": "2960160",
    "end": "2965200"
  },
  {
    "text": "and a few minutes so historically based on the data it is",
    "start": "2965200",
    "end": "2970480"
  },
  {
    "text": "taking two to three minutes to replace a node so if your app was tolerant for the two",
    "start": "2970480",
    "end": "2976800"
  },
  {
    "text": "to three minute window then we would recommend not failing over the keys and uh just go to your durable store",
    "start": "2976800",
    "end": "2984480"
  },
  {
    "text": "during that period and then um once the node comes back you can start populating the keys again",
    "start": "2984480",
    "end": "2991119"
  },
  {
    "text": "all right another question here about can servers in my data center use elasticash",
    "start": "2991119",
    "end": "2997040"
  },
  {
    "text": "yeah so so right now uh you know elastic cache can only be accessed from inside aws and your your application has",
    "start": "2997040",
    "end": "3004240"
  },
  {
    "text": "to be running inside an ec2 so no you cannot we thought about this and the biggest",
    "start": "3004240",
    "end": "3010319"
  },
  {
    "text": "reason we don't allow that is because of security um we uh inside the east",
    "start": "3010319",
    "end": "3015680"
  },
  {
    "text": "inside if your application inside ec2 we have very strong controls and since security is paramount to us",
    "start": "3015680",
    "end": "3021040"
  },
  {
    "text": "we actually disable that so we cannot access any elastic cache cluster from from outside of aws",
    "start": "3021040",
    "end": "3028400"
  },
  {
    "text": "so some more questions about the amount of memory on on elastic cache nodes and",
    "start": "3028880",
    "end": "3036078"
  },
  {
    "start": "3029000",
    "end": "3127000"
  },
  {
    "text": "the availability of that memory for for use for actual caching yeah these information should be",
    "start": "3036880",
    "end": "3042000"
  },
  {
    "text": "published on the aws website if you go to the elastic node types you would see the information",
    "start": "3042000",
    "end": "3047280"
  },
  {
    "text": "about the size of the nodes detail",
    "start": "3047280",
    "end": "3054880"
  },
  {
    "text": "yeah so another question about when you talk about short ttls is it a few seconds a few minutes what does",
    "start": "3054880",
    "end": "3061920"
  },
  {
    "text": "ttl mean for for an application so that entirely relies on the application characteristics",
    "start": "3061920",
    "end": "3068319"
  },
  {
    "text": "and the data characteristics of your application so for example if your application is",
    "start": "3068319",
    "end": "3074720"
  },
  {
    "text": "tolerant to some amount of inconsistency then you could have a higher ttl or if your data doesn't change",
    "start": "3074720",
    "end": "3082160"
  },
  {
    "text": "frequently say it changes once a week then you can really have a long detail if your data changes",
    "start": "3082160",
    "end": "3087520"
  },
  {
    "text": "frequently then you can have ttl in minutes or seconds so it really depends on the application",
    "start": "3087520",
    "end": "3092880"
  },
  {
    "text": "that you're building and the type of data that you're storing in the cache",
    "start": "3092880",
    "end": "3097838"
  },
  {
    "text": "so uh there's another question here about you know is replication transfer and failover on the roadmap for zero down time so we obviously",
    "start": "3099520",
    "end": "3106559"
  },
  {
    "text": "cannot talk about a roadmap uh what our customers do today is uh they typically have another elasticash",
    "start": "3106559",
    "end": "3113680"
  },
  {
    "text": "cluster running in a different az and on the application side they are able to",
    "start": "3113680",
    "end": "3119200"
  },
  {
    "text": "go ahead and you know populate that as well so when uh failure occurs they uh manage it themselves",
    "start": "3119200",
    "end": "3125838"
  },
  {
    "text": "so i guess some more questions about crossing azs in this case",
    "start": "3127359",
    "end": "3132720"
  },
  {
    "text": "are the clust are the members in an elastic cluster are they distributed across availability zones are they",
    "start": "3132880",
    "end": "3138640"
  },
  {
    "text": "within an availability zone how fast is the replication internally do developers need to worry",
    "start": "3138640",
    "end": "3144880"
  },
  {
    "text": "about these things yeah i know definitely so one thing to keep in mind is that all the members of the elastic cache",
    "start": "3144880",
    "end": "3151359"
  },
  {
    "text": "cluster have to be inside a particular az so at this time",
    "start": "3151359",
    "end": "3156960"
  },
  {
    "text": "we don't allow the elastic cache nodes to span easies and this is primarily because we want to have uniform",
    "start": "3156960",
    "end": "3163280"
  },
  {
    "text": "response times and if you you know even though you know you're within the same region um still",
    "start": "3163280",
    "end": "3168960"
  },
  {
    "text": "there's a difference between you know going to a node in the same av and that of another az",
    "start": "3168960",
    "end": "3175440"
  },
  {
    "text": "okay so there is a question about uh on replacement in case of automatic node replacement uh",
    "start": "3176000",
    "end": "3181760"
  },
  {
    "text": "will it reuse the previous ip not necessarily the new node can come back with a different ip address",
    "start": "3181760",
    "end": "3187920"
  },
  {
    "text": "but what we make sure is we update the dns endpoint of your slice to the new ip",
    "start": "3187920",
    "end": "3194000"
  },
  {
    "text": "address so it is important that you resolve the dns to talk to the new node",
    "start": "3194000",
    "end": "3199280"
  },
  {
    "text": "but the dns endpoint itself doesn't change the ip address changes behind the scene okay another question",
    "start": "3199280",
    "end": "3207440"
  },
  {
    "text": "somewhat interesting here it says will caching help save money by minimizing server load so",
    "start": "3207440",
    "end": "3215520"
  },
  {
    "start": "3216000",
    "end": "3285000"
  },
  {
    "text": "let's start from the perspective that caching is actually being done to improve performance right so that's the",
    "start": "3216160",
    "end": "3221359"
  },
  {
    "text": "biggest benefit you're getting and what does it mean in terms of application performance to your users do",
    "start": "3221359",
    "end": "3227280"
  },
  {
    "text": "you does it give you the ability to to serve more users does that actually help you make more money because that's",
    "start": "3227280",
    "end": "3233280"
  },
  {
    "text": "essentially what caching was about right so that's that's the starting point it does reduce server load",
    "start": "3233280",
    "end": "3239200"
  },
  {
    "text": "uh but as you reduce server load you're adding more nodes in between so you're adding costs for the caching layer",
    "start": "3239200",
    "end": "3245040"
  },
  {
    "text": "so whether you save money by reducing server load because you added caching",
    "start": "3245040",
    "end": "3250319"
  },
  {
    "text": "is dependent on on the kind of cost you have for the server load and how much speed up you gain by adding",
    "start": "3250319",
    "end": "3256319"
  },
  {
    "text": "caching nodes so that's something you'd have to calculate ultimately for yourself for your architecture",
    "start": "3256319",
    "end": "3261359"
  },
  {
    "text": "and and based on the cost structure that you have but keep in mind that the way your your",
    "start": "3261359",
    "end": "3268079"
  },
  {
    "text": "scaling your application the way you're handling more load for your application with a fewer",
    "start": "3268079",
    "end": "3274079"
  },
  {
    "text": "number of nodes is essentially what will allow you to serve more load through less servers and save you money that way",
    "start": "3274079",
    "end": "3282799"
  },
  {
    "text": "okay so there is a question about auto discovery or can you link the auto discovery open source code",
    "start": "3285520",
    "end": "3290880"
  },
  {
    "text": "we have hosted the uh auto discovery code on the github so we'll be pasting the link shortly and",
    "start": "3290880",
    "end": "3298240"
  },
  {
    "text": "uh do you know if there is a ruby client that has auto discovery functionality yeah we haven't released a ruby client yet",
    "start": "3298240",
    "end": "3304640"
  },
  {
    "text": "but that is no roadmap all right so i think we're running out",
    "start": "3304640",
    "end": "3310960"
  },
  {
    "text": "of time here uh thank you very much for all the interest today we had a lot of questions there's a lot of questions that we have not been",
    "start": "3310960",
    "end": "3317359"
  },
  {
    "text": "able to come up to uh to answer um but like i said we'll be posting a recording and we'll be posting",
    "start": "3317359",
    "end": "3323520"
  },
  {
    "text": "the slides and you should get a link to that uh we also talked about the fact that we'll make a link available to",
    "start": "3323520",
    "end": "3329040"
  },
  {
    "text": "the the open source for the auto discovery and uh if you have more questions please",
    "start": "3329040",
    "end": "3334559"
  },
  {
    "text": "get in touch with either your local aws representative or shoot us an email from the aws site you",
    "start": "3334559",
    "end": "3342079"
  },
  {
    "text": "should be able to get to the contact us page and from there you can get more information or ask questions",
    "start": "3342079",
    "end": "3348160"
  },
  {
    "text": "again thank you very much for attending today and and we value your feedback please let us know how you use the service and",
    "start": "3348160",
    "end": "3354640"
  },
  {
    "text": "then what kind of results you're getting thank you very much and i'd just like to",
    "start": "3354640",
    "end": "3360559"
  },
  {
    "text": "conclude the event by saying that i did post the links to our webcast youtube channel and also to our",
    "start": "3360559",
    "end": "3366799"
  },
  {
    "text": "slideshare page there in your q a manager so you can go ahead and click directly there and you'll see the slideshows later",
    "start": "3366799",
    "end": "3373920"
  },
  {
    "text": "today and the recording tomorrow so thanks again and please don't forget to fill out that survey",
    "start": "3373920",
    "end": "3380000"
  },
  {
    "text": "and we will talk to you all soon have a great day or evening bye",
    "start": "3380000",
    "end": "3389280"
  }
]