[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "I think we're gonna go ahead and get started here as folks are still rolling in",
    "start": "0",
    "end": "5190"
  },
  {
    "text": "my name's Spence nivel I'm a s4 specialist Solutions Architect at AWS I've been at AWS for about three years",
    "start": "5190",
    "end": "11550"
  },
  {
    "text": "now and I focus on the data and analytical services so I work with customers that are adopting big data",
    "start": "11550",
    "end": "17730"
  },
  {
    "text": "applications in the cloud and help them you know build the patterns solutions in AWS in today's sessions we're going to",
    "start": "17730",
    "end": "25439"
  },
  {
    "text": "talk about service analytics and how serverless analytics could really help your big data application how to",
    "start": "25439",
    "end": "31320"
  },
  {
    "text": "streamline it how to really drive insights as quickly as possible through your analog your big data analytics the",
    "start": "31320",
    "end": "39420"
  },
  {
    "start": "38000",
    "end": "83000"
  },
  {
    "text": "format or the agenda that we're going to be going over is really what is service why serve lists why are we talking about",
    "start": "39420",
    "end": "45329"
  },
  {
    "text": "service and why do you care how does that fit into big data applications how",
    "start": "45329",
    "end": "50879"
  },
  {
    "text": "can it really help expedite and really drive those insights very very quickly",
    "start": "50879",
    "end": "56219"
  },
  {
    "text": "and going to include a set of demonstration so we're really going to show some design patterns of how to use",
    "start": "56219",
    "end": "61379"
  },
  {
    "text": "these analytical services to be able to do things like real time analytics quick",
    "start": "61379",
    "end": "66479"
  },
  {
    "text": "discovery of data analysis of data integrating things with Hadoop that sort of thing out of curiosity how many folks",
    "start": "66479",
    "end": "74670"
  },
  {
    "text": "in the room have experience with some sort of service capability lambda any of",
    "start": "74670",
    "end": "79920"
  },
  {
    "text": "those awesome so when we talk about service we're really talking about",
    "start": "79920",
    "end": "86869"
  },
  {
    "start": "83000",
    "end": "159000"
  },
  {
    "text": "architectural pattern or a an approach and if we literally look at how service",
    "start": "86869",
    "end": "92579"
  },
  {
    "text": "analytics evolved you know back in 2006 you know ec2 was launched and that",
    "start": "92579",
    "end": "97979"
  },
  {
    "text": "really launched virtualization within the cloud so you're able to spin up ec2 instances very easily and focus above",
    "start": "97979",
    "end": "105030"
  },
  {
    "text": "the OS and scale those servers very very quickly we have set of managed services",
    "start": "105030",
    "end": "110220"
  },
  {
    "text": "so if you think of things like redshift or EMR very very powerful services but they they're considered managed services",
    "start": "110220",
    "end": "116729"
  },
  {
    "text": "and the reason why that is is you as a customer are still thinking about servers even though you're not managing",
    "start": "116729",
    "end": "123180"
  },
  {
    "text": "them you're thinking about servers saying I want this big of a server I want to auto scale based on these",
    "start": "123180",
    "end": "128700"
  },
  {
    "text": "policies so you're still thinking about servers as a unit you're just not doing a lot",
    "start": "128700",
    "end": "133770"
  },
  {
    "text": "the underaged heavy lifting of installing patching monitoring those those servers that make up that cluster",
    "start": "133770",
    "end": "140450"
  },
  {
    "text": "with server lists it's really a new pattern so with server lists you don't really think about servers at all you",
    "start": "140450",
    "end": "145860"
  },
  {
    "text": "could just focus on writing your code that could be ETL code pre-processing code various capabilities like that it",
    "start": "145860",
    "end": "153840"
  },
  {
    "text": "could be a sequel based analytics ETL that sort of thing and why that matters",
    "start": "153840",
    "end": "160080"
  },
  {
    "start": "159000",
    "end": "205000"
  },
  {
    "text": "is since you don't have to worry about servers yourself you don't have to think about the scaling and the provisioning",
    "start": "160080",
    "end": "165690"
  },
  {
    "text": "of those servers the managing of those servers the services scale per per use",
    "start": "165690",
    "end": "170760"
  },
  {
    "text": "and because they're scaling per use they're also charged based on execution",
    "start": "170760",
    "end": "175770"
  },
  {
    "text": "time so if I execute some code within lambda I'm charged for every 100",
    "start": "175770",
    "end": "181020"
  },
  {
    "text": "milliseconds of that execution time if I'm actually doing ETL using glue I'm",
    "start": "181020",
    "end": "186180"
  },
  {
    "text": "charge per minute of that ETL process as it's executing so I'm never I'm never paying for idle it's always charging",
    "start": "186180",
    "end": "192450"
  },
  {
    "text": "based on the execution of me doing work which is really really powerful and you",
    "start": "192450",
    "end": "198000"
  },
  {
    "text": "don't have to worry about the availability and the thought tolerance those are all abstracted away within that managed service within the region",
    "start": "198000",
    "end": "205280"
  },
  {
    "start": "205000",
    "end": "283000"
  },
  {
    "text": "so how do these service capabilities really help augment your big data applications and we're really going to",
    "start": "205280",
    "end": "210900"
  },
  {
    "text": "go into a lot of depth here the one of the key takeaways I wanted everyone to see though is many of our customers are",
    "start": "210900",
    "end": "217620"
  },
  {
    "text": "really mixing and matching these these services so you could very easily have a have a solution a big data application",
    "start": "217620",
    "end": "224730"
  },
  {
    "text": "that pieces are running on ec2 in a virtualized environment some of it could",
    "start": "224730",
    "end": "230700"
  },
  {
    "text": "be managed services and then other pieces of that solution could be service capabilities and towards the end we're",
    "start": "230700",
    "end": "237540"
  },
  {
    "text": "going to show a demonstration that actually brings some of those together so you don't have to necessarily pick",
    "start": "237540",
    "end": "242730"
  },
  {
    "text": "one you know I only want virtualized I only want to manage I only want server lists you could have a hybrid or area a",
    "start": "242730",
    "end": "249090"
  },
  {
    "text": "mixed solution that brings in the right service for that job and we're really gonna dive into some key services and",
    "start": "249090",
    "end": "255810"
  },
  {
    "text": "some architectural approaches of using those so how can I rapidly ingest data into my application you know making",
    "start": "255810",
    "end": "262650"
  },
  {
    "text": "ingest within minutes or hours or rather than days and weeks how can I very easily for my data how can I query and analyze",
    "start": "262650",
    "end": "270150"
  },
  {
    "text": "my data so we're gonna step through some of these services it's gonna be a very very quick overview of what is lamda",
    "start": "270150",
    "end": "276630"
  },
  {
    "text": "what is quick you know glue these sorts of things but then we're really gonna dive into these patterns during this session so a lot of",
    "start": "276630",
    "end": "286050"
  },
  {
    "start": "283000",
    "end": "340000"
  },
  {
    "text": "folks in the room raised their hand about what what is lambda or service or having service capabilities or",
    "start": "286050",
    "end": "291150"
  },
  {
    "text": "experience but lambda is a service that is very very powerful what it allows you",
    "start": "291150",
    "end": "296340"
  },
  {
    "text": "to do is upload your own custom code and that code is triggered based on a set of events",
    "start": "296340",
    "end": "301650"
  },
  {
    "text": "it could be real-time data coming in through something like a Kinesis stream to be able to very quickly process those",
    "start": "301650",
    "end": "309900"
  },
  {
    "text": "streaming events or it could be more Batchelor and said you know you want to create a thumbnail based on an image",
    "start": "309900",
    "end": "315240"
  },
  {
    "text": "coming in or you want to do some processing of geospatial data so all these are event-driven",
    "start": "315240",
    "end": "320610"
  },
  {
    "text": "it could be either a synchronous or asynchronous event and you want to do some logic so when you're writing lambda",
    "start": "320610",
    "end": "325920"
  },
  {
    "text": "you write that piece of code that does that very specific logic and the ecosystem or the platform handles all",
    "start": "325920",
    "end": "332100"
  },
  {
    "text": "the rest in terms of notifying your code whenever that happens being able to you",
    "start": "332100",
    "end": "337530"
  },
  {
    "text": "know span across your AZ's that sort of thing with athena and we're gonna talk",
    "start": "337530",
    "end": "343710"
  },
  {
    "start": "340000",
    "end": "397000"
  },
  {
    "text": "about how these are put together here in a moment but with athena it you're able to do sequel based queries against your data and the reason",
    "start": "343710",
    "end": "350850"
  },
  {
    "text": "why this is important is in big data applications oftentimes folks are doing what's called schema on read what that",
    "start": "350850",
    "end": "357510"
  },
  {
    "text": "means is you're saving your data first it's it's a ELT met methodology rather",
    "start": "357510",
    "end": "363720"
  },
  {
    "text": "than ETL methodology you save your data as quickly as as possible and then what you're doing is you're querying your",
    "start": "363720",
    "end": "369660"
  },
  {
    "text": "data and you're applying the schema when you're you're executing against that query time and what Athena allows you to",
    "start": "369660",
    "end": "375510"
  },
  {
    "text": "do is very rapidly ingest your data and have that on s3 and then perform sequel",
    "start": "375510",
    "end": "380700"
  },
  {
    "text": "based queries against those data sets so in big data applications it's really great to be able to bring in new data",
    "start": "380700",
    "end": "386010"
  },
  {
    "text": "sources being able to analyze theta fuse datasets together some of which might be",
    "start": "386010",
    "end": "391140"
  },
  {
    "text": "different data sets you know one might be JSON another could be CSV and bringing those data sets together",
    "start": "391140",
    "end": "397250"
  },
  {
    "text": "another service we're going to dive into is is AWS Glu this is a service that really is made up",
    "start": "397250",
    "end": "403919"
  },
  {
    "text": "of two key parts the first is the data catalog this data catalog is a hive compliant data catalog it allows you to",
    "start": "403919",
    "end": "410729"
  },
  {
    "text": "store your metadata about your data in things like your data like on an s3 or it might be data in databases or your",
    "start": "410729",
    "end": "417630"
  },
  {
    "text": "data warehouse and this catalog is really very nicely integrated with things like Hadoop and EMR or redshift",
    "start": "417630",
    "end": "424889"
  },
  {
    "text": "spectrum to be able to bring it into your data warehouse or things like Athena which we're going to demonstrate many of those today the other piece is",
    "start": "424889",
    "end": "432750"
  },
  {
    "text": "really being able to write serverless ETL so under the covers Glu uses PI SPARC it",
    "start": "432750",
    "end": "438449"
  },
  {
    "text": "uses SPARC containers in order to execute your ETL logic but within the",
    "start": "438449",
    "end": "444000"
  },
  {
    "text": "glue service what you could do is you could build your ETL or you could have your PI SPARC code already you that",
    "start": "444000",
    "end": "451320"
  },
  {
    "text": "you've developed that you want to execute and you can execute that inside the glue environments and it'll spin up",
    "start": "451320",
    "end": "456449"
  },
  {
    "text": "the cluster it'll execute your job your shut it down and your data saved out either into your database data warehouse",
    "start": "456449",
    "end": "462810"
  },
  {
    "text": "or back to s3 Amazon Kinesis is really",
    "start": "462810",
    "end": "468660"
  },
  {
    "text": "made up of a set of services so Kinesis it includes Kinesis streams which is",
    "start": "468660",
    "end": "474389"
  },
  {
    "text": "what the first service out of the suite that we that we released which really allows you to very easily rapidly ingest",
    "start": "474389",
    "end": "481229"
  },
  {
    "text": "and scale real streaming data so it's very easy to build custom applications",
    "start": "481229",
    "end": "486510"
  },
  {
    "text": "in SPARC streaming and storm and flink or use some services that we're going to",
    "start": "486510",
    "end": "491760"
  },
  {
    "text": "demonstrate today using streams firehose allows you to really populate your data",
    "start": "491760",
    "end": "497159"
  },
  {
    "text": "like very easily based on streaming data so let's say you really want to be able to collect clickstream data or you want",
    "start": "497159",
    "end": "503940"
  },
  {
    "text": "to collect data from IOT sensors or telemetry data and store that day on s3 so you could do things like machine",
    "start": "503940",
    "end": "509580"
  },
  {
    "text": "learning deep learning or maybe you want to build some analytics and spark or or pressed or you know various languages on",
    "start": "509580",
    "end": "515580"
  },
  {
    "text": "the Hadoop ecosystem you could use firehose to very easily capture that it will store that data in s3 and then you",
    "start": "515580",
    "end": "521640"
  },
  {
    "text": "start analyzing it really really easily and we're going to demonstrate both firehose and the next service that we're",
    "start": "521640",
    "end": "527760"
  },
  {
    "text": "going to talk about is Kinesis analytics just like Athena provides sequel based access to s3 Kinesis and a little",
    "start": "527760",
    "end": "535420"
  },
  {
    "text": "provide sequel based access to streaming data so the two use cases and how they differ is they're both sequel based but",
    "start": "535420",
    "end": "542769"
  },
  {
    "text": "one is based on your data that's at rest on s3 and you want to analyze that large data sets and Kinesis analytics is",
    "start": "542769",
    "end": "549310"
  },
  {
    "text": "sequel based against your real-time data or your streaming data so if you want to do sliding windows tumblr windows",
    "start": "549310",
    "end": "555670"
  },
  {
    "text": "various filtering algorithms through sequel you can very easily do that using Kinesis analytics so I promise now we're",
    "start": "555670",
    "end": "564610"
  },
  {
    "text": "going to really dive into some of those architectural patterns I want to set the stage for some of the folks that may not have heard of some of those services",
    "start": "564610",
    "end": "570670"
  },
  {
    "text": "ahead of time so how do we apply some of these to some of the service",
    "start": "570670",
    "end": "576480"
  },
  {
    "start": "571000",
    "end": "618000"
  },
  {
    "text": "capabilities to these big data applications you know if we take a look at the different big data applications",
    "start": "576480",
    "end": "582459"
  },
  {
    "text": "we have many on AWS so you have things like data processing data warehousing reporting artificial intelligence so",
    "start": "582459",
    "end": "590920"
  },
  {
    "text": "many of these applications have different requirements so the way you do",
    "start": "590920",
    "end": "596350"
  },
  {
    "text": "real-time analytics has much different requirements than how you do a batch analytic or business reporting because",
    "start": "596350",
    "end": "604029"
  },
  {
    "text": "the way you acquire your data is different if you have real-time data coming into the constraints of that that",
    "start": "604029",
    "end": "609910"
  },
  {
    "text": "data sources is different than if you're doing batch loads from your database or data warehouse and different data sets",
    "start": "609910",
    "end": "617250"
  },
  {
    "text": "but many of them do share a lot of the same characteristics so you want to be",
    "start": "617250",
    "end": "622449"
  },
  {
    "text": "able to collect data from many many different data sources depending on what application you're writing you want to be able to dive into those data sources",
    "start": "622449",
    "end": "628839"
  },
  {
    "text": "and analyze them with a very wide number of tools you want to be able to access that data very flexibly but also very",
    "start": "628839",
    "end": "635500"
  },
  {
    "text": "securely and the other really big thing especially in the analytical space is what we call future proofing so we like",
    "start": "635500",
    "end": "641800"
  },
  {
    "text": "customers to be able to build these architectures that really lets them bring in new types of analytics very",
    "start": "641800",
    "end": "647740"
  },
  {
    "text": "very easily so you know today it might be very source of machine learning and deep learning algorithms but the type of",
    "start": "647740",
    "end": "654130"
  },
  {
    "text": "analytics and algorithms you won't you run a year from now might be very different maybe it's using the FPGAs or",
    "start": "654130",
    "end": "659139"
  },
  {
    "text": "different sources of infrastructure to be able to very quickly analyze that data so how can architectures be put",
    "start": "659139",
    "end": "664449"
  },
  {
    "text": "together that gives you that decoupling that that nice architecture to be able bringing these new analytics but also",
    "start": "664449",
    "end": "671680"
  },
  {
    "text": "very easy to iterate in and develop so if we take a look at the flow here some",
    "start": "671680",
    "end": "678970"
  },
  {
    "text": "main components of these big data applications and and we're gonna start plugging in how these service analytics or service services really fit into",
    "start": "678970",
    "end": "686470"
  },
  {
    "text": "these pipelines the first is really how do you ingest and collect the data and",
    "start": "686470",
    "end": "692370"
  },
  {
    "text": "within there we really have a couple different service category services that",
    "start": "692370",
    "end": "697509"
  },
  {
    "text": "fall into that there's really the Kinesis suite very very easy to be able to bring in new real-time data into your",
    "start": "697509",
    "end": "703689"
  },
  {
    "text": "into your data set into your your application and of course s3 hopefully everyone in the room is familiar with s3",
    "start": "703689",
    "end": "710800"
  },
  {
    "text": "to be able to store your data but once you have your data you want to be able",
    "start": "710800",
    "end": "716800"
  },
  {
    "text": "to very easily categorize and search that data so at the New York summit earlier this year glue tape went GA and",
    "start": "716800",
    "end": "724720"
  },
  {
    "text": "glue has a really great capability to be able to have a metadata catalog and we're going to demonstrate how this",
    "start": "724720",
    "end": "730509"
  },
  {
    "text": "catalog really makes it easy to be able to bring in new datasets be able to maintain datasets so that if your data",
    "start": "730509",
    "end": "736720"
  },
  {
    "text": "changes over time your catalog gets up to date and then also how that Kellog",
    "start": "736720",
    "end": "741759"
  },
  {
    "text": "could be integrated with a really a wide set of tools some service but also others that may not be of course you",
    "start": "741759",
    "end": "751180"
  },
  {
    "text": "know as soon as you bring your in your data you may want to be able to transform your data get the data into a canonical form morph the data various",
    "start": "751180",
    "end": "758350"
  },
  {
    "text": "ways a couple different patterns that folks use in the service environment to",
    "start": "758350",
    "end": "764500"
  },
  {
    "text": "do that the first is AWS lambda so that allows you to very easily write transformation",
    "start": "764500",
    "end": "770350"
  },
  {
    "text": "code to be able to transform your data most often that's done it with real time",
    "start": "770350",
    "end": "775809"
  },
  {
    "text": "data so if I have various real-time data sources coming in I want to write a transformation logic and lambda I could",
    "start": "775809",
    "end": "781959"
  },
  {
    "text": "do that very easily I write a few lines of code that transforms the data and then it will execute that as is storing",
    "start": "781959",
    "end": "787540"
  },
  {
    "text": "it out into the repository or running the analytics on that but if I have large large data sets you know maybe I",
    "start": "787540",
    "end": "794470"
  },
  {
    "text": "want to transform you know gigabytes petabytes terabytes you know very very large data sets that I want to run and",
    "start": "794470",
    "end": "801579"
  },
  {
    "text": "do transformation on AWS glue is really the right choice there because you're it's very very",
    "start": "801579",
    "end": "807549"
  },
  {
    "text": "powerful to be able to write these transformations run them in a service environment and be able to save your",
    "start": "807549",
    "end": "812949"
  },
  {
    "text": "data either back to your data like on s3 or your data warehouse or your database after you have your data transformed in",
    "start": "812949",
    "end": "820839"
  },
  {
    "text": "a particular way you know oftentimes you want to run analytics on that data so Kinesis analytics provides that sequel",
    "start": "820839",
    "end": "827470"
  },
  {
    "text": "base analytical capability on top of the streams and then Amazon Athena provides that sequel base access on top of on top",
    "start": "827470",
    "end": "835809"
  },
  {
    "text": "of your data stored on s3 what we're going to show a little bit also though",
    "start": "835809",
    "end": "840999"
  },
  {
    "text": "is you could use many of these components so you could use Kinesis firehose you could use the glue data catalog but you can very easily launch a",
    "start": "840999",
    "end": "848230"
  },
  {
    "text": "EMR cluster and and in a managed environment to run analytics and do",
    "start": "848230",
    "end": "853929"
  },
  {
    "text": "things like graph analytics or machine learning using spark or various tools out of the Hadoop ecosystem but still",
    "start": "853929",
    "end": "860470"
  },
  {
    "text": "leverage things like the glue data catalog you know ETL that sort of thing",
    "start": "860470",
    "end": "866279"
  },
  {
    "text": "and then lastly for visualization we have many many great partners in the BI space we have a really fantastic product",
    "start": "866279",
    "end": "873610"
  },
  {
    "text": "caught Amazon quick site that lets you very easily plug in these data sources and drive bi dashboards on top of that",
    "start": "873610",
    "end": "881110"
  },
  {
    "text": "data so if we take a look at you know one of these threads through and you",
    "start": "881110",
    "end": "886929"
  },
  {
    "start": "883000",
    "end": "946000"
  },
  {
    "text": "know I promise I'm gonna give a couple demonstrations here so thanks for bearing with me I our demonstrations I",
    "start": "886929",
    "end": "893439"
  },
  {
    "text": "should say but you know if we take a look at this flow here this is a very very common real-time analytical flow so",
    "start": "893439",
    "end": "900790"
  },
  {
    "text": "you know Kinesis and I'm going to show this in a minute very very easy to stand up with a few clicks of a button and",
    "start": "900790",
    "end": "906160"
  },
  {
    "text": "have have a service that will capture your data sets in a very robust way so",
    "start": "906160",
    "end": "912279"
  },
  {
    "text": "what you could do is you could set up a Kinesis firehose you could instrument your various applications so that might",
    "start": "912279",
    "end": "917769"
  },
  {
    "text": "be a web application a mobile application with some producer logic within there and then you could capture",
    "start": "917769",
    "end": "924549"
  },
  {
    "text": "all that information very very quickly using Kinesis streams and firehose lambda to be able to do transformations",
    "start": "924549",
    "end": "930789"
  },
  {
    "text": "so you know maybe different producers are producing the same data but they call fields different names and you",
    "start": "930789",
    "end": "936670"
  },
  {
    "text": "want to get the data into a canonical form too before you run your real-time analytics on it Kinesis Analects and",
    "start": "936670",
    "end": "943330"
  },
  {
    "text": "then quick sites be able to visualize so let's dive into a demonstration here",
    "start": "943330",
    "end": "948970"
  },
  {
    "start": "946000",
    "end": "1384000"
  },
  {
    "text": "real quick so what we have here is a Kinesis",
    "start": "948970",
    "end": "956770"
  },
  {
    "text": "firehose this is a Kinesis firehose",
    "start": "956770",
    "end": "961930"
  },
  {
    "text": "right here that i set up earlier this week and what you'll notice is you know",
    "start": "961930",
    "end": "967420"
  },
  {
    "text": "as soon as I can figure this firehose I configure our destination I specified s3 but you could very easily specify s3",
    "start": "967420",
    "end": "974230"
  },
  {
    "text": "Amazon Elastic search redshift multiple different destinations depending on where you want to store this real-time",
    "start": "974230",
    "end": "980050"
  },
  {
    "text": "data I specified s3 with a prefix of raw B this is my raw data coming in and what",
    "start": "980050",
    "end": "987370"
  },
  {
    "text": "what I'm demonstrating is a simulated messaging application so what we're doing is we have instrumentation",
    "start": "987370",
    "end": "993930"
  },
  {
    "text": "theoretically we actually have some agents that are publishing messages right now but you can think of it use",
    "start": "993930",
    "end": "999100"
  },
  {
    "text": "case wise many many different messaging applications around the globe that are generating chat messages and we want to",
    "start": "999100",
    "end": "1006240"
  },
  {
    "text": "capture all those messages and analyze the sediment of what people are saying on those chat applications so within",
    "start": "1006240",
    "end": "1014340"
  },
  {
    "text": "here we have this Kinesis firehose and you know this is actually I just configure this it's writing out to this",
    "start": "1014340",
    "end": "1021810"
  },
  {
    "text": "s3 location it's automatically saving the data batching it up for me so that",
    "start": "1021810",
    "end": "1028410"
  },
  {
    "text": "one s3 object contains multiple records zipping it very nice integration with",
    "start": "1028410",
    "end": "1033780"
  },
  {
    "text": "kms all that all that important aspect",
    "start": "1033780",
    "end": "1039709"
  },
  {
    "text": "so if we take a look over here at this",
    "start": "1039710",
    "end": "1045870"
  },
  {
    "text": "what this data looks like it actually the data getting written out",
    "start": "1045870",
    "end": "1052980"
  },
  {
    "text": "looks like this and I'll make this bigger for folks so this is just fake",
    "start": "1052980",
    "end": "1058020"
  },
  {
    "text": "data we have some generators that are generating fake messages with random words that sort of thing but you know",
    "start": "1058020",
    "end": "1065190"
  },
  {
    "text": "the state is actually getting pushed out to the Kinesis fire hose automatically we're having thousands and thousands of",
    "start": "1065190",
    "end": "1071370"
  },
  {
    "text": "records you know per minute it could scale much much greater than that but we are pumping data you know it's decent",
    "start": "1071370",
    "end": "1078810"
  },
  {
    "text": "data through it it's not just a couple messages here and there and what you'll notice is with Kinesis firehose it's",
    "start": "1078810",
    "end": "1084780"
  },
  {
    "text": "automatically batching this data set up for me saving it out to s3 and then if I want to run heuristic analysis on this I",
    "start": "1084780",
    "end": "1091440"
  },
  {
    "text": "could very easily query this data using Athena or EMR or various tools on top of",
    "start": "1091440",
    "end": "1097560"
  },
  {
    "text": "it next let's go ahead and you know let's say we want to analyze this data",
    "start": "1097560",
    "end": "1103650"
  },
  {
    "text": "so this data set actually has you know location information so you'll notice a",
    "start": "1103650",
    "end": "1109530"
  },
  {
    "text": "country as the nation of where the message is coming from a lat/long it has",
    "start": "1109530",
    "end": "1114630"
  },
  {
    "text": "a time stamp it also has you know some derived fields that we're doing within lamda which I'll show in a minute let's",
    "start": "1114630",
    "end": "1122760"
  },
  {
    "text": "say we wanted to analyze this data what",
    "start": "1122760",
    "end": "1128130"
  },
  {
    "text": "I'm gonna show next here is Kinesis analytics so what we stood up here is we",
    "start": "1128130",
    "end": "1134850"
  },
  {
    "text": "want to run real-time analytics of what the sentiment was both positive and negative per country of people talking",
    "start": "1134850",
    "end": "1141180"
  },
  {
    "text": "on our messaging application so I could have written this on in spark streaming your storm or you know wrote my own KCl",
    "start": "1141180",
    "end": "1147870"
  },
  {
    "text": "that read the data off or you know do very Smythe's but you know I could very easily plug in Kinesis analytics here as",
    "start": "1147870",
    "end": "1153810"
  },
  {
    "text": "well and you'll notice this sequel code is only I'll scroll down but tens of",
    "start": "1153810",
    "end": "1160320"
  },
  {
    "text": "lines of code it's not hundreds of lines of code so I could very easily write this sequel statement that does my",
    "start": "1160320",
    "end": "1165810"
  },
  {
    "text": "real-time analytics deploy it out and start getting the results out out very easily so what this looks like here is",
    "start": "1165810",
    "end": "1172460"
  },
  {
    "text": "this is actually defining the output here the output contains the country",
    "start": "1172460",
    "end": "1178350"
  },
  {
    "text": "they lapped along and then really that sediment counts so what we're doing is we're doing a sliding window what the positive and negative sediments",
    "start": "1178350",
    "end": "1185670"
  },
  {
    "text": "are per country you know since this is based on sequel here this next section",
    "start": "1185670",
    "end": "1194280"
  },
  {
    "text": "here is all I had to do the right to actually start generating that data so I'm generated into the destination",
    "start": "1194280",
    "end": "1200460"
  },
  {
    "text": "sequel stream and I could just write my you know count negative count positive",
    "start": "1200460",
    "end": "1206250"
  },
  {
    "text": "and then this is doing a a 30-second window counting the positive and",
    "start": "1206250",
    "end": "1212280"
  },
  {
    "text": "negative sediments within the within the generated data so over here we can",
    "start": "1212280",
    "end": "1221880"
  },
  {
    "text": "actually see some of the data coming in this is actually live data that's getting generated from our simulator you",
    "start": "1221880",
    "end": "1227970"
  },
  {
    "text": "know processing some of this you know we can actually take a look at some of the real-time analytics that's generating",
    "start": "1227970",
    "end": "1233760"
  },
  {
    "text": "this is completely fake data you know we're using simulator generate this but just it's real data it's real",
    "start": "1233760",
    "end": "1240300"
  },
  {
    "text": "messages going through Kinesis and being analyzed here none of this is kind of",
    "start": "1240300",
    "end": "1245310"
  },
  {
    "text": "staged in any way so but you know what's kind of interesting here is now what we",
    "start": "1245310",
    "end": "1250890"
  },
  {
    "text": "could do is we can actually set up various destinations so let's say we want to capture this and store this data",
    "start": "1250890",
    "end": "1256410"
  },
  {
    "text": "out to s3 and then be able to integrate with other types of analytics in our big data application you know what we",
    "start": "1256410",
    "end": "1263040"
  },
  {
    "text": "specified here this is the higher level definition of the streaming application",
    "start": "1263040",
    "end": "1268230"
  },
  {
    "text": "in Kinesis analytics we were just in this portion here for the sequel results we're reading off of that that messaging",
    "start": "1268230",
    "end": "1276090"
  },
  {
    "text": "stream that's generating that those fake messages and then we're writing it back out to s3 in a different location called",
    "start": "1276090",
    "end": "1282180"
  },
  {
    "text": "sediment aggregations so how how does lambda fit in here right so so far we",
    "start": "1282180",
    "end": "1288870"
  },
  {
    "text": "haven't really showed anything lambda based using AWS lambda we showed how you use Kinesis to capture data very easy to",
    "start": "1288870",
    "end": "1296010"
  },
  {
    "text": "stream the data through Kinesis analyse aggregate that data what we configured over here to do transformations though",
    "start": "1296010",
    "end": "1302370"
  },
  {
    "text": "is this record preprocessor so within lambda what you could do is before",
    "start": "1302370",
    "end": "1308430"
  },
  {
    "text": "Kinesis analytics analyzes your datasets you can actually run your own custom logic whatever logic you want through",
    "start": "1308430",
    "end": "1316260"
  },
  {
    "text": "and I happen to be doing some very rudimentary sediment analysis looking at",
    "start": "1316260",
    "end": "1322320"
  },
  {
    "text": "the texts so the actual data actually doesn't have that sediment information I'm enriching that data within the",
    "start": "1322320",
    "end": "1328560"
  },
  {
    "text": "message before it hits the analytic within this lambda function you could very easily use this to be able to",
    "start": "1328560",
    "end": "1334920"
  },
  {
    "text": "enrich it with other data sets to be able to filter your data sets transform your data sets into different you know",
    "start": "1334920",
    "end": "1342570"
  },
  {
    "text": "names if you want to get into a canonical form that sort of thing and",
    "start": "1342570",
    "end": "1348510"
  },
  {
    "text": "what the what the output ends up looking like in s3 is a report that looks like",
    "start": "1348510",
    "end": "1358710"
  },
  {
    "text": "this this could have very easily been driving a real-time dashboard or you know",
    "start": "1358710",
    "end": "1365190"
  },
  {
    "text": "various other applications but we just pumped it back to a Kinesis firehose firehose does all the heavy lifting of",
    "start": "1365190",
    "end": "1370890"
  },
  {
    "text": "storing all these files out to s3 for us and now we have all these aggregations that we could do whatever we want with",
    "start": "1370890",
    "end": "1376620"
  },
  {
    "text": "really within within our application so",
    "start": "1376620",
    "end": "1384860"
  },
  {
    "start": "1384000",
    "end": "1520000"
  },
  {
    "text": "what we just showed is a couple pieces the first was this are towards the end was actually the land of pre-processing",
    "start": "1384860",
    "end": "1391080"
  },
  {
    "text": "element so what this is doing is it's actually taking the raw events where the raw event doesn't have any other",
    "start": "1391080",
    "end": "1397080"
  },
  {
    "text": "sediment information that we're done demonstrating and the land is actually enriching it before it hits that",
    "start": "1397080",
    "end": "1403890"
  },
  {
    "text": "analytic once it hits that analytic then we start doing those aggregations that sliding-window that you saw using sequel",
    "start": "1403890",
    "end": "1411170"
  },
  {
    "text": "just through some lines that look like this to be able to very easily write that real-time analytic and save those",
    "start": "1411170",
    "end": "1417330"
  },
  {
    "text": "results out to s3 this is what the flow looks like actually this is a something",
    "start": "1417330",
    "end": "1424350"
  },
  {
    "text": "we're going to be doing in a workshop on Thursday so anyone who wants to come to the workshop we're actually using this simulator and having folks get hands-on",
    "start": "1424350",
    "end": "1431220"
  },
  {
    "text": "experience doing exactly that flow there it's the zombie data data Lake workshop that was my shameless plug but this is",
    "start": "1431220",
    "end": "1439260"
  },
  {
    "text": "really what the flow was so we had a simulator generated chats it was skin transforms the Atlanta aggregations",
    "start": "1439260",
    "end": "1445800"
  },
  {
    "text": "being done in sequel and then data again stored out the s3 and that entire flow that I just showed",
    "start": "1445800",
    "end": "1452549"
  },
  {
    "text": "you I didn't actually have to worry about servers at all I didn't have to think about how many servers do I need to handle a thousand records per second",
    "start": "1452549",
    "end": "1459599"
  },
  {
    "text": "you know two thousand records per second I'm really just writing those pieces of code you saw and the the serverless",
    "start": "1459599",
    "end": "1465809"
  },
  {
    "text": "services are really scaling based on the amount of incoming data that's happening",
    "start": "1465809",
    "end": "1471029"
  },
  {
    "text": "I didn't really have to spin up any seats you myself or anything like that how does this really fit into existing",
    "start": "1471029",
    "end": "1478469"
  },
  {
    "text": "real time applications like server list is very very powerful we have many customers that will build complete",
    "start": "1478469",
    "end": "1485249"
  },
  {
    "text": "service solutions using all the services that we just saw but you could use different pieces of these and plug in",
    "start": "1485249",
    "end": "1491549"
  },
  {
    "text": "other types of analytics and services within there so it's very easy to use things like Kinesis streams and you know",
    "start": "1491549",
    "end": "1499679"
  },
  {
    "text": "read the data off using the KCl which is the Kinesis client library you can read it off using spark streaming so you can",
    "start": "1499679",
    "end": "1506729"
  },
  {
    "text": "really mix and match a lot of these different services to be able to enrich",
    "start": "1506729",
    "end": "1511799"
  },
  {
    "text": "existing pipelines that you have but also use what makes sense for the use case in the solution that you're",
    "start": "1511799",
    "end": "1517649"
  },
  {
    "text": "building so what if we wanted to start doing some batch analytics or",
    "start": "1517649",
    "end": "1524129"
  },
  {
    "start": "1520000",
    "end": "1559000"
  },
  {
    "text": "interactive analytics I should say you know the first example that we're just looking at was actually real-time",
    "start": "1524129",
    "end": "1530159"
  },
  {
    "text": "streaming data coming in you know maybe we want to start analyzing some data that already exists and is at rest on s3",
    "start": "1530159",
    "end": "1537679"
  },
  {
    "text": "and we're going to do that next so what we're gonna do is we're gonna show how you could have large data sets on s3 how",
    "start": "1537679",
    "end": "1544229"
  },
  {
    "text": "you could very easily discover that data we're gonna do a live discovery of the data during the next demonstration we're",
    "start": "1544229",
    "end": "1551009"
  },
  {
    "text": "going to start analyzing that data to be able to understand it better and then start doing some Hadoop analytics off of",
    "start": "1551009",
    "end": "1557189"
  },
  {
    "text": "it after that so let's go ahead and switch over to that one of the things I",
    "start": "1557189",
    "end": "1564419"
  },
  {
    "start": "1559000",
    "end": "2024000"
  },
  {
    "text": "should should also show just real quick is is this here",
    "start": "1564419",
    "end": "1572250"
  },
  {
    "text": "on the real-time analytics that we're just showing just to show you guys you",
    "start": "1572250",
    "end": "1577799"
  },
  {
    "text": "know in terms of how much scale we're pushing through you know not massive massive scale we have a lot of customers",
    "start": "1577799",
    "end": "1582990"
  },
  {
    "text": "that are doing a lot more than this but you'll notice you know every minute 50 to 70,000 records that are getting",
    "start": "1582990",
    "end": "1589140"
  },
  {
    "text": "pushed through you know it could scale much greater than that but you know this is it this is a kind of actively running",
    "start": "1589140",
    "end": "1595440"
  },
  {
    "text": "as I was demoing it so let's do some some interactive analytics on some of",
    "start": "1595440",
    "end": "1602130"
  },
  {
    "text": "our data on s3 so to do that I'm",
    "start": "1602130",
    "end": "1607409"
  },
  {
    "text": "actually going to start with glue and within glue we're actually going to bring up the catalog and the reason I",
    "start": "1607409",
    "end": "1614520"
  },
  {
    "text": "like to show this is the glue Datak alak is relatively new it went GA earlier",
    "start": "1614520",
    "end": "1620190"
  },
  {
    "text": "this year but it very nicely helps you describe your data on s3 so here we",
    "start": "1620190",
    "end": "1625919"
  },
  {
    "text": "actually have a rides database RT you'll see the s3 location yeah you see the",
    "start": "1625919",
    "end": "1631049"
  },
  {
    "text": "different data types this is park' data if I remove this filter here what we can",
    "start": "1631049",
    "end": "1639539"
  },
  {
    "text": "see is a lot of different data sets so here you see redshift and Postgres and various data sets but I have it filtered",
    "start": "1639539",
    "end": "1644909"
  },
  {
    "text": "so it's only looking at a particular namespace or a database within within the catalog called the phone so let's go",
    "start": "1644909",
    "end": "1652919"
  },
  {
    "text": "ahead and crawl a new location and I'm what I'm gonna do is I'm actually slide",
    "start": "1652919",
    "end": "1658049"
  },
  {
    "text": "this select this crawler and what these crawlers do is it it runs within I am roll so you could",
    "start": "1658049",
    "end": "1664230"
  },
  {
    "text": "very easily set up least privilege for these crawlers it's all protected through I am but these crawlers are",
    "start": "1664230",
    "end": "1669750"
  },
  {
    "text": "actually crawling my data on s3 now none of this you know this is all doing it live and essentially what it's doing is",
    "start": "1669750",
    "end": "1677700"
  },
  {
    "text": "its crawling those data sources at this particular location so I told it to crawl my serverless analytics data Lake",
    "start": "1677700",
    "end": "1684780"
  },
  {
    "text": "prefix location and put all the new tables that it just discovers and",
    "start": "1684780",
    "end": "1690330"
  },
  {
    "text": "remember these aren't like real tables in a database these are files on s3 that are doing a schema on redefinition of",
    "start": "1690330",
    "end": "1696240"
  },
  {
    "text": "these tables and and it's putting it all inside default database that or a default namespace that we're just",
    "start": "1696240",
    "end": "1701850"
  },
  {
    "text": "looking at a minute ago so if we take a look over here now you know a lot of those weren't showed",
    "start": "1701850",
    "end": "1708750"
  },
  {
    "text": "you you didn't see a lot of those before a min ago right it was just that Rides database now spark' and you'll notice",
    "start": "1708750",
    "end": "1714899"
  },
  {
    "text": "you know the last updated date you know they're all right now because we just ran it but you'll notice a lot of",
    "start": "1714899",
    "end": "1720330"
  },
  {
    "text": "different things that noticed it discovered JSON data discovered CSV data so it discovers your datasets that you",
    "start": "1720330",
    "end": "1727830"
  },
  {
    "text": "then use very easily in your analytics just to show you an example you know",
    "start": "1727830",
    "end": "1732840"
  },
  {
    "text": "Twitter data really fantastic data set to analyze and query against but this",
    "start": "1732840",
    "end": "1739529"
  },
  {
    "text": "Twitter data said it discovered automatically for me and the nice thing about what it did is it discovered these",
    "start": "1739529",
    "end": "1746490"
  },
  {
    "text": "very complex JSON so this is a very nested object it contains array of",
    "start": "1746490",
    "end": "1752669"
  },
  {
    "text": "structs of array of structs of different array of structs so the crawlers automatically discovered this entire DD",
    "start": "1752669",
    "end": "1758700"
  },
  {
    "text": "off for me I didn't have to go in there and like pull out my hair trying to figure out what that crate table statement look like the crawler ran it",
    "start": "1758700",
    "end": "1765299"
  },
  {
    "text": "discovered it I could go in here and modify it and change you know different field types different names you know",
    "start": "1765299",
    "end": "1770610"
  },
  {
    "text": "maybe it discovered it as a numeric field and I want to treat it as a string you know because you know maybe it's",
    "start": "1770610",
    "end": "1776190"
  },
  {
    "text": "flight data and there's no significant number difference between flight 101 to",
    "start": "1776190",
    "end": "1781830"
  },
  {
    "text": "treat that as a string rather than the number these you know you wouldn't want to do like per carry or the average",
    "start": "1781830",
    "end": "1787440"
  },
  {
    "text": "flight number that sort of thing but it does a lot of that heavy lifting so it really lets you bring in new datasets",
    "start": "1787440",
    "end": "1793769"
  },
  {
    "text": "very very quickly within your your big data applications",
    "start": "1793769",
    "end": "1798889"
  },
  {
    "text": "so what we're going to do is we're actually going to look at a different data set here we're gonna look at this other data set that it just discovered",
    "start": "1798889",
    "end": "1805769"
  },
  {
    "text": "called taxis it's a delimited file and what we could do is now we can start looking at the data we everything that",
    "start": "1805769",
    "end": "1814169"
  },
  {
    "text": "you're seeing is done kind of actively now so we didn't have any of this data in in Glu already you know this data is",
    "start": "1814169",
    "end": "1821070"
  },
  {
    "text": "just living in s3 and now we're starting to look at the data sets here and the tool we're using now is Athena so what",
    "start": "1821070",
    "end": "1827880"
  },
  {
    "text": "we're doing is we're using the glued data catalog but you're using Athena - now query the data using sequel so even",
    "start": "1827880",
    "end": "1833789"
  },
  {
    "text": "though even though the data is CSV data we're issuing select statements on top of the datasets about the",
    "start": "1833789",
    "end": "1839790"
  },
  {
    "text": "we'll better understand that data so let's let's do a couple queries here",
    "start": "1839790",
    "end": "1845790"
  },
  {
    "text": "this first query that we're going to run is just doing a accounts min amounts a",
    "start": "1845790",
    "end": "1853340"
  },
  {
    "text": "max amount and a a keep account there so",
    "start": "1853340",
    "end": "1862530"
  },
  {
    "text": "let me switch this over here so you know",
    "start": "1862530",
    "end": "1868740"
  },
  {
    "text": "here this will take you know less than two seconds to run and we see as over ten million records so if I actually",
    "start": "1868740",
    "end": "1875190"
  },
  {
    "text": "tried to load this in Excel it wouldn't load all the records even though you know but the other thing that we start",
    "start": "1875190",
    "end": "1882720"
  },
  {
    "text": "noticing is some anomalies in the data so we did like a min total amount and Max total amounts I've never actually",
    "start": "1882720",
    "end": "1887850"
  },
  {
    "text": "been paid close to a thousand dollars to ride a taxi in New York it would be really great if I were but you start",
    "start": "1887850",
    "end": "1894330"
  },
  {
    "text": "seeing some anomalies in the data here as well but this is this is you know an",
    "start": "1894330",
    "end": "1899430"
  },
  {
    "text": "SI sequel at this point so what you know I could do is I could very easily join datasets together I can start you know",
    "start": "1899430",
    "end": "1905700"
  },
  {
    "text": "doing case statements I could do all types of different things in here in this example here we're actually doing a",
    "start": "1905700",
    "end": "1911250"
  },
  {
    "text": "case statement renaming some vendor information and you're those very very consistent performance scanning over the",
    "start": "1911250",
    "end": "1916890"
  },
  {
    "text": "data returning those records back so this is actually New York City Taxi data",
    "start": "1916890",
    "end": "1921990"
  },
  {
    "text": "this is one months of data what if we want to analyze eight years of the same",
    "start": "1921990",
    "end": "1927060"
  },
  {
    "text": "data set so one month was ten million records I'm actually going to query a different data set and do exactly that",
    "start": "1927060",
    "end": "1933260"
  },
  {
    "text": "so this is all eight years of data here",
    "start": "1933260",
    "end": "1938360"
  },
  {
    "text": "and I'm actually going to query that group by year and what I've done with",
    "start": "1938420",
    "end": "1943560"
  },
  {
    "text": "this data set is the first data set was actually CSV this other data set is actually parquet which is a column",
    "start": "1943560",
    "end": "1949290"
  },
  {
    "text": "optimized a column oriented data set for a query optimization so I actually used",
    "start": "1949290",
    "end": "1955230"
  },
  {
    "text": "glue to transform the data from CSV to parquet and as soon as I do that the responses are much much quicker both in",
    "start": "1955230",
    "end": "1962760"
  },
  {
    "text": "Athena as well as some of the other stuff that we're going to show within Hadoop using things like spark and",
    "start": "1962760",
    "end": "1968580"
  },
  {
    "text": "you'll notice you know per year about 400 million 300 million different taxi rides",
    "start": "1968580",
    "end": "1973650"
  },
  {
    "text": "but what we very easily do now is we could do a lot more complicated queries so this next query is going to be",
    "start": "1973650",
    "end": "1979800"
  },
  {
    "text": "looking at the average distance the average cost per mile the average costs",
    "start": "1979800",
    "end": "1985230"
  },
  {
    "text": "and the 99th percentile for 2016 and one things you'll notice here is within",
    "start": "1985230",
    "end": "1991530"
  },
  {
    "text": "another second or two less than four seconds here it returned back the results and the reason why it was so",
    "start": "1991530",
    "end": "1997590"
  },
  {
    "text": "fast is we have the data really very optimized for a data Lake in s3 so rather than keeping all the data in CSV",
    "start": "1997590",
    "end": "2004400"
  },
  {
    "text": "which is very non performant when you're querying the data we converted it to",
    "start": "2004400",
    "end": "2010250"
  },
  {
    "text": "this optimized format it's very very quick to be able to have our data scientists or business users interact",
    "start": "2010250",
    "end": "2015770"
  },
  {
    "text": "with our data like on s3 so so that's a",
    "start": "2015770",
    "end": "2024380"
  },
  {
    "text": "little bit about how you use the glue data Kellogg to bring in your data sets how you could discover your data how you start analyzing some of the data sets",
    "start": "2024380",
    "end": "2030920"
  },
  {
    "text": "that you want to be able to bring in and really how that fits in is is really",
    "start": "2030920",
    "end": "2038480"
  },
  {
    "text": "we're showing that part of this top flow we didn't show a quick site quite yet but we're showing how you could have your data in s3 the glue data catalog",
    "start": "2038480",
    "end": "2045140"
  },
  {
    "text": "kind of sits over all these different services integrates with the EMR which we're going to show next but then we're",
    "start": "2045140",
    "end": "2051830"
  },
  {
    "text": "just demonstrating Amazon Athena here a moment ago",
    "start": "2051830",
    "end": "2056980"
  },
  {
    "text": "but you can really put these services together so you could very easily have your real time analytics using things",
    "start": "2056980",
    "end": "2062480"
  },
  {
    "start": "2057000",
    "end": "2076000"
  },
  {
    "text": "like Kinesis Kinesis firehose you could have your batch analytics or your interactive analytics using some of",
    "start": "2062480",
    "end": "2068210"
  },
  {
    "text": "these other services and put these services together to be able to have a both real-time and batch analytics",
    "start": "2068210",
    "end": "2074360"
  },
  {
    "text": "together how out of curiosity how many folks in the room are currently working",
    "start": "2074360",
    "end": "2080360"
  },
  {
    "start": "2076000",
    "end": "2135000"
  },
  {
    "text": "on a data Lake project awesome yeah pretty good pretty good number de Lakes",
    "start": "2080360",
    "end": "2087648"
  },
  {
    "text": "very very popular being able to bring in all these both raw data as well as created data sets being able exposed",
    "start": "2087649",
    "end": "2094580"
  },
  {
    "text": "those data sets securely through different sets of users based on different skills you might have data scientists that want you know access to",
    "start": "2094580",
    "end": "2101420"
  },
  {
    "text": "the raw data you might have curated to assess for business users that sort of thing we're service fits in is we are actually",
    "start": "2101420",
    "end": "2108320"
  },
  {
    "text": "just demonstrated a lot of what is highlighted in the orange boxes there so",
    "start": "2108320",
    "end": "2113900"
  },
  {
    "text": "a very common day like architecture on AWS s3 as the central storage being able to run wide number of different",
    "start": "2113900",
    "end": "2120320"
  },
  {
    "text": "analytics on that data being able to categorize your data using services like glue but this is really where it fits in you",
    "start": "2120320",
    "end": "2127609"
  },
  {
    "text": "know those those five different services we just demonstrated very easily fits into that data Lake architecture another",
    "start": "2127609",
    "end": "2136070"
  },
  {
    "start": "2135000",
    "end": "2178000"
  },
  {
    "text": "view in terms of the data Lake is really different users might have access to different levels of data so your data",
    "start": "2136070",
    "end": "2142880"
  },
  {
    "text": "scientists and your data engineers oftentimes are using various notebooks it might be a Jupiter notebook it might",
    "start": "2142880",
    "end": "2149480"
  },
  {
    "text": "be a Zeppelin notebook it might be you know different interfaces like our studio so really different users might",
    "start": "2149480",
    "end": "2157970"
  },
  {
    "text": "be interacting with this data on s3 different ways but all of these services",
    "start": "2157970",
    "end": "2163369"
  },
  {
    "text": "that you see listed up here integrate with that s3 central storage integrates with that glue data catalog off to the",
    "start": "2163369",
    "end": "2170990"
  },
  {
    "text": "side to be able to have a central repository central source of truth of all your data so but what about your",
    "start": "2170990",
    "end": "2179420"
  },
  {
    "start": "2178000",
    "end": "2419000"
  },
  {
    "text": "existing hadoop clusters you might be running Hadoop you might be running EMR and you you know you might be asking",
    "start": "2179420",
    "end": "2185359"
  },
  {
    "text": "yourself you know I I want to leverage these things like the glue data catalog or you know Kinesis firehose but how",
    "start": "2185359",
    "end": "2191780"
  },
  {
    "text": "does it fit into my existing services I me might be running today so we're actually going to show that next here so",
    "start": "2191780",
    "end": "2202190"
  },
  {
    "text": "what I have running here is EMR cluster EMR is our elastic MapReduce and I'll go",
    "start": "2202190",
    "end": "2209210"
  },
  {
    "text": "ahead and bring it up here real quick",
    "start": "2209210",
    "end": "2212260"
  },
  {
    "text": "so we have this elastic MapReduce cluster that's running and very easy to spin up this cluster I configured it to",
    "start": "2219420",
    "end": "2227460"
  },
  {
    "text": "have a spark Zeppelin hive test press touhou' on it various applications out",
    "start": "2227460",
    "end": "2233160"
  },
  {
    "text": "of the ecosystem but what I also specified when I spun up this cluster is I want this cluster to actually use the",
    "start": "2233160",
    "end": "2240690"
  },
  {
    "text": "glue data catalog in order to be able to do things like run my queries against",
    "start": "2240690",
    "end": "2245940"
  },
  {
    "text": "this cluster or run ml jobs using spark ml that sort of thing so what we're",
    "start": "2245940",
    "end": "2251310"
  },
  {
    "text": "going to do here it next is we're going to show I could use some of those tools so I'm actually going to bring up Hugh",
    "start": "2251310",
    "end": "2258990"
  },
  {
    "text": "first so Hugh is a web interface to be",
    "start": "2258990",
    "end": "2264210"
  },
  {
    "text": "able to have folks interact with your your Hadoop cluster so let me bring that",
    "start": "2264210",
    "end": "2271410"
  },
  {
    "text": "up I'm gonna move it so you guys could",
    "start": "2271410",
    "end": "2280770"
  },
  {
    "text": "see it",
    "start": "2280770",
    "end": "2283010"
  },
  {
    "text": "and so one of the things you'll notice right over here is this is a hue in turf ace let me make it bigger for folks in",
    "start": "2293490",
    "end": "2300089"
  },
  {
    "text": "the back and very easily what I could do is I could actually you'll notice this rides database here default database",
    "start": "2300089",
    "end": "2306599"
  },
  {
    "text": "only had rights in it if I actually go back out in here and I just require you this one things you'll notice is all",
    "start": "2306599",
    "end": "2314550"
  },
  {
    "text": "these new tables just showed up automatically for me right even though I'm using Hadoop now in EMR I could be",
    "start": "2314550",
    "end": "2320099"
  },
  {
    "text": "writing all my jobs you know I could be writing hive jobs and all types of different things I'm interacting with",
    "start": "2320099",
    "end": "2326160"
  },
  {
    "text": "that glue data catalog and many of these new tables that were showing right now are the ones we actually discovered",
    "start": "2326160",
    "end": "2331800"
  },
  {
    "text": "through that crawling job we ran a moment ago so in here I could very",
    "start": "2331800",
    "end": "2337200"
  },
  {
    "text": "easily write my jobs through this web interface I could write them on the cluster themselves that sort of thing like what I could also do is I could",
    "start": "2337200",
    "end": "2344940"
  },
  {
    "text": "have my data scientists running on EMR using Zeppelin notebooks so this is a",
    "start": "2344940",
    "end": "2352650"
  },
  {
    "text": "Zeppelin notebook I have running here on top of that same cluster I just brought up and you'll notice within here I",
    "start": "2352650",
    "end": "2358619"
  },
  {
    "text": "actually said show databases these are all the different databases that are registered in glue and then down here",
    "start": "2358619",
    "end": "2365970"
  },
  {
    "text": "this is actually I ran this earlier but now if I run this again you'll notice",
    "start": "2365970",
    "end": "2371010"
  },
  {
    "text": "all those new tables show up for my data scientists that might be using Zeppelin notebooks as well so all these different",
    "start": "2371010",
    "end": "2377130"
  },
  {
    "text": "service services work really really nicely with existing services that you might be using in your big data pipeline",
    "start": "2377130",
    "end": "2385490"
  },
  {
    "text": "so essentially what we just showed there is in that data Lake you know how can we",
    "start": "2391509",
    "end": "2396980"
  },
  {
    "text": "use these other interfaces so how can we use Epling uhyou Jupiter still leverage",
    "start": "2396980",
    "end": "2402849"
  },
  {
    "text": "the capabilities within the doop cluster but integrate it with the service data",
    "start": "2402849",
    "end": "2408230"
  },
  {
    "text": "like the the glue data catalog s3 as the central storage so essentially it's that",
    "start": "2408230",
    "end": "2414049"
  },
  {
    "text": "thread through there that you that you see up on the screen so essentially what",
    "start": "2414049",
    "end": "2421670"
  },
  {
    "start": "2419000",
    "end": "2489000"
  },
  {
    "text": "we what we covered and I can stick around for some questions here is how some of these different service",
    "start": "2421670",
    "end": "2427789"
  },
  {
    "text": "capabilities fit in to your big data pipeline so things like lambda being able to do event notification our event",
    "start": "2427789",
    "end": "2435499"
  },
  {
    "text": "processing event transformation enrichments of your data how you could",
    "start": "2435499",
    "end": "2440660"
  },
  {
    "text": "very easily use things like Athena to be able analyzing querier datasets but still leverage the power of Hadoop and",
    "start": "2440660",
    "end": "2446839"
  },
  {
    "text": "and custom big data applications on top of EMR all integrated with the same glue",
    "start": "2446839",
    "end": "2452180"
  },
  {
    "text": "data catalog and s3 repository and it's really boiled down to leveraging these",
    "start": "2452180",
    "end": "2457460"
  },
  {
    "text": "surveillance capabilities so you could very very quickly and analyze your data and drive insights faster it's all about",
    "start": "2457460",
    "end": "2463730"
  },
  {
    "text": "that undef rigid heavy lifting so you could do that analytics and of course there's a really good cost savings in",
    "start": "2463730",
    "end": "2469999"
  },
  {
    "text": "the terms of the pays pay-as-you-go model so that does really the content we wanted to cover in this session I'll",
    "start": "2469999",
    "end": "2477349"
  },
  {
    "text": "stick around for some questions you know down here I wanted to thank everyone for their time and thank you",
    "start": "2477349",
    "end": "2482930"
  },
  {
    "text": "[Applause]",
    "start": "2482930",
    "end": "2491119"
  }
]