[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "hello how's everyone doing today it sounds good I'm hoped you're enjoying",
    "start": "709",
    "end": "6830"
  },
  {
    "text": "reinvent thus far it's a great Monday great weather down here in Vegas you're",
    "start": "6830",
    "end": "12090"
  },
  {
    "text": "here at MLB 401 this is 10 tips and tricks for improving your graphic you'll api with AWS app sync my name is Michael",
    "start": "12090",
    "end": "19680"
  },
  {
    "text": "Parris I'm an engineer on the app sync team and recently I've been working a lot on a new set of client frameworks",
    "start": "19680",
    "end": "24869"
  },
  {
    "text": "and tools called amplify and we're gonna talk about graph QL given that it's a",
    "start": "24869",
    "end": "30060"
  },
  {
    "text": "400 level class we are gonna get a little technical there's gonna be some code on the screen hope you enjoy that and we have a lot to cover so I'm just",
    "start": "30060",
    "end": "37890"
  },
  {
    "text": "gonna dive right in so here's our agenda I said 10 tips it might be more than 10",
    "start": "37890",
    "end": "44730"
  },
  {
    "start": "40000",
    "end": "77000"
  },
  {
    "text": "tips I'm sorry about that but we have 10 topics first we're gonna cover schema design we're gonna go over some data",
    "start": "44730",
    "end": "50760"
  },
  {
    "text": "design which more database related stuff we're gonna be talking about leveraging AWS lambda we're gonna be talking about",
    "start": "50760",
    "end": "57329"
  },
  {
    "text": "real time data some rapid development techniques offline data and use cases thereof continuous integration and",
    "start": "57329",
    "end": "64559"
  },
  {
    "text": "deployment operations and monitoring testing your api's and finally we're gonna end with schema governance which",
    "start": "64559",
    "end": "71159"
  },
  {
    "text": "is something I almost always hear from customers as they're adopting graph QL",
    "start": "71159",
    "end": "77659"
  },
  {
    "start": "77000",
    "end": "107000"
  },
  {
    "text": "so first things first we're going over schema design so every time I start",
    "start": "77659",
    "end": "84090"
  },
  {
    "text": "talking about anything graph QL I generally start with the schema because it's really the most important part of any API it impacts pretty much",
    "start": "84090",
    "end": "91500"
  },
  {
    "text": "everything else that you do from the client to the backend it's gonna impact",
    "start": "91500",
    "end": "97350"
  },
  {
    "text": "how you query your data it's gonna impact how you read data it's even gonna impact how you communicate what your API",
    "start": "97350",
    "end": "102570"
  },
  {
    "text": "is able to do to your developers and to your customers so the first thing is use",
    "start": "102570",
    "end": "109860"
  },
  {
    "start": "107000",
    "end": "189000"
  },
  {
    "text": "long descriptive names this is tip number one the reason why is that every",
    "start": "109860",
    "end": "115920"
  },
  {
    "text": "graph QL API has unique names you can't have name clashes and there's no name",
    "start": "115920",
    "end": "120930"
  },
  {
    "text": "spacing concept in graph QL so to get around this you have to have long descriptive names that are not only",
    "start": "120930",
    "end": "127530"
  },
  {
    "text": "unique but are informative to your users to tell you what data that thing actually represents",
    "start": "127530",
    "end": "132940"
  },
  {
    "text": "it's a pretty common use case to have an API that fronts multiple back-end resources maybe it's different services",
    "start": "132940",
    "end": "139570"
  },
  {
    "text": "maybe it's just different data stores for the same service and it's not abnormal for those back-end data source",
    "start": "139570",
    "end": "144790"
  },
  {
    "text": "or services to have similarly named data you could think you might have two services both of those services might",
    "start": "144790",
    "end": "151570"
  },
  {
    "text": "have a user concept and they might both named them user because when they're building them they build them in a silo",
    "start": "151570",
    "end": "156760"
  },
  {
    "text": "and then when you're exposing them you need to figure out a way to expose both of those user concepts with a unique",
    "start": "156760",
    "end": "161860"
  },
  {
    "text": "name so tip number one use long descriptive names that adhere to a pattern it's pretty common in Apps Inc",
    "start": "161860",
    "end": "169090"
  },
  {
    "text": "to see people proxying AWS api's so this is something we see often where they'll come up with a convention and you'll",
    "start": "169090",
    "end": "175930"
  },
  {
    "text": "have a service prefix and a type postfix and then if you stick by that convention and you adopt that early on you're gonna",
    "start": "175930",
    "end": "181930"
  },
  {
    "text": "have a much better time and be able to build much more much larger and more evolvable schemas as you go forward the",
    "start": "181930",
    "end": "190660"
  },
  {
    "start": "189000",
    "end": "249000"
  },
  {
    "text": "next thing we're going to talk about our input and output patterns so developing a pattern for inputs is one of the first",
    "start": "190660",
    "end": "196510"
  },
  {
    "text": "things I recommend you do in a schema you might have in this case you see a create user post mutation it has a",
    "start": "196510",
    "end": "203200"
  },
  {
    "text": "single input field named input of type create user post input there's a few",
    "start": "203200",
    "end": "208390"
  },
  {
    "text": "reasons for this the first is that it makes your API is more resilient to change you don't have to go change all of the arguments of your query mutation",
    "start": "208390",
    "end": "215410"
  },
  {
    "text": "fields which are pretty important every single time you want to add a field to a type and also from client perspectives",
    "start": "215410",
    "end": "221530"
  },
  {
    "text": "it Maps really well to how you build client applications when you're trying",
    "start": "221530",
    "end": "226600"
  },
  {
    "text": "to you know operate on an object in a front-end app you're often trying to operate on that object and if you have a",
    "start": "226600",
    "end": "232930"
  },
  {
    "text": "D structured input and you have instead of just one input type a bunch of input fields on this mutation type you're",
    "start": "232930",
    "end": "239260"
  },
  {
    "text": "gonna have two D structure that object every single time you want to operate on it so this I would say definitely I",
    "start": "239260",
    "end": "244630"
  },
  {
    "text": "definitely recommend doing off the bat now as it comes to output patterns this",
    "start": "244630",
    "end": "251920"
  },
  {
    "start": "249000",
    "end": "295000"
  },
  {
    "text": "is one I've seen go both ways so as you grow I find that this is a really useful",
    "start": "251920",
    "end": "258489"
  },
  {
    "text": "concept and it might seem unconventional but it allows you to do some pretty interesting things so in this example we",
    "start": "258489",
    "end": "265210"
  },
  {
    "text": "still have our input we have our user post mutation but instead of just returning that user post we're gonna",
    "start": "265210",
    "end": "271120"
  },
  {
    "text": "return this create user post output this pattern was first popularized by relay",
    "start": "271120",
    "end": "276760"
  },
  {
    "text": "which was a early graph QL framework and it allows you to do some neat things and in this case you can see we're actually",
    "start": "276760",
    "end": "282340"
  },
  {
    "text": "giving a reference to our root query type in the output response and what",
    "start": "282340",
    "end": "288040"
  },
  {
    "text": "that allows you to do is basically do arbitrary reads after writes which can get pretty powerful once you start using",
    "start": "288040",
    "end": "294550"
  },
  {
    "text": "them and here's an example of how you might use that so here we've got to read after write we're gonna go and go ahead",
    "start": "294550",
    "end": "301960"
  },
  {
    "text": "and call our create user post mutation field we're gonna create a post with the title graphic you let reinvent and then",
    "start": "301960",
    "end": "308740"
  },
  {
    "text": "we're gonna get back the user posts and the title which is the object we just operated on but then also we're gonna go",
    "start": "308740",
    "end": "314650"
  },
  {
    "text": "make this arbitrary query to go query or user feed so the reason you might do",
    "start": "314650",
    "end": "319660"
  },
  {
    "text": "this is that not everything is so easy as just a flat list often in the front-end you can kind of get back that",
    "start": "319660",
    "end": "325210"
  },
  {
    "text": "object and put it back in a row and be able to do that optimistic update on the client but sometimes you need the",
    "start": "325210",
    "end": "331180"
  },
  {
    "text": "backend to do a little computation before you figure out what is that thing and this allows you to do both the update and get that updated response",
    "start": "331180",
    "end": "338410"
  },
  {
    "text": "with that update taken into account in the same network quest the next pattern",
    "start": "338410",
    "end": "346630"
  },
  {
    "start": "345000",
    "end": "422000"
  },
  {
    "text": "we'll talk about are pagination patterns this is another thing that you just have to do off the bat and there's there's no",
    "start": "346630",
    "end": "352120"
  },
  {
    "text": "one-size-fits-all solution and the way I'd recommend to go about it is to really think about what are your",
    "start": "352120",
    "end": "357610"
  },
  {
    "text": "pagination requirements there's different patterns for different requirements and sometimes that one",
    "start": "357610",
    "end": "362710"
  },
  {
    "text": "pattern is just overkill for what you need to do and other times it's not so the questions you should be asking",
    "start": "362710",
    "end": "368530"
  },
  {
    "text": "yourself are things like do you need a cursor per item the inverse of this would be having a cursor per page and",
    "start": "368530",
    "end": "375160"
  },
  {
    "text": "you can think about you know I might be asking for 10 objects and then if I had a cursor per page I can get 10 objects",
    "start": "375160",
    "end": "381100"
  },
  {
    "text": "and then I can get the next 10 objects but I can't get 10 objects and then get the next 10 after the first so that's",
    "start": "381100",
    "end": "386590"
  },
  {
    "text": "kind of a difference there another thing to think about is do you need forward and backwards pagination that's another",
    "start": "386590",
    "end": "393940"
  },
  {
    "text": "topic and then the last is do the edges in your graph contain data",
    "start": "393940",
    "end": "399910"
  },
  {
    "text": "so the most common example and one you probably have heard of is if you have a social media application and you have",
    "start": "399910",
    "end": "405850"
  },
  {
    "text": "friendships in them you might have that is accepted flag on the friendship itself that friendship is really the",
    "start": "405850",
    "end": "412180"
  },
  {
    "text": "edge between two user objects and you can basically place information on that edge to build more powerful api's and",
    "start": "412180",
    "end": "418950"
  },
  {
    "text": "we'll see how that looks so here's a really simple pagination pattern it's",
    "start": "418950",
    "end": "425530"
  },
  {
    "start": "422000",
    "end": "464000"
  },
  {
    "text": "one that we use a lot with app sync it Maps really well to DynamoDB and it basically it's it's kind of a simple and",
    "start": "425530",
    "end": "432430"
  },
  {
    "text": "effective it allows you to get back a list of items and then it provides a single next token you're the TEL for",
    "start": "432430",
    "end": "438520"
  },
  {
    "text": "whether you have more objects in that paginate ad set or not as simply is that next token no if it's null you're done",
    "start": "438520",
    "end": "444610"
  },
  {
    "text": "if it's not there's more information to query and it's that easy you can see the user the users posts field doesn't",
    "start": "444610",
    "end": "453400"
  },
  {
    "text": "return a list of posts or user posts it actually returns this user post connection and then you'd have input",
    "start": "453400",
    "end": "459880"
  },
  {
    "text": "arguments on that field as well as we'll see you in a minute",
    "start": "459880",
    "end": "463830"
  },
  {
    "start": "464000",
    "end": "573000"
  },
  {
    "text": "here's how you might query that we'd have a simple paginating query it's gonna call this list objects query field",
    "start": "465060",
    "end": "472420"
  },
  {
    "text": "and it's gonna give a limit of 10 saying we want 10 objects it's also going to provide a next token which will tell us",
    "start": "472420",
    "end": "478870"
  },
  {
    "text": "where in that set we should start paginating from you'll notice that this is a cursor so the next token as you get",
    "start": "478870",
    "end": "485560"
  },
  {
    "text": "it back if it's null you're done if it's not null you've got more items to read and you can continue to paginate from",
    "start": "485560",
    "end": "491560"
  },
  {
    "text": "there here's a slightly more advanced",
    "start": "491560",
    "end": "497200"
  },
  {
    "text": "pagination pattern this is one that introduces edges so in this we have not",
    "start": "497200",
    "end": "503140"
  },
  {
    "text": "only the connection we actually have the edge as well so a connection is itself wrapper around a set of edges and this",
    "start": "503140",
    "end": "511000"
  },
  {
    "text": "is probably more traditional to what you'd see in a graph database or a traditional graph where an edge wraps",
    "start": "511000",
    "end": "517030"
  },
  {
    "text": "not only an object which we're here calling a node it also has a cursor so this is an example of where you'd have a",
    "start": "517030",
    "end": "522760"
  },
  {
    "text": "cursor per object but then you can see also here that we have that is accepted flag on the edge itself so we can put",
    "start": "522760",
    "end": "529450"
  },
  {
    "text": "data on the edge between two associated objects we're also going to introduce this page",
    "start": "529450",
    "end": "534940"
  },
  {
    "text": "info concept that allows us to give more advanced information about the pagination itself in this instance it",
    "start": "534940",
    "end": "541540"
  },
  {
    "text": "just has the hasnext page and has a previous page and the reason for that is that this is actually going to also allow us to do bi-directional pagination",
    "start": "541540",
    "end": "548770"
  },
  {
    "text": "so that we can not only go forward through a set we can move backwards through a set as well we're also going to add things like count that you can",
    "start": "548770",
    "end": "556240"
  },
  {
    "text": "use as well I've also seen people add things like aggregation queries on on things like this so you could put",
    "start": "556240",
    "end": "561610"
  },
  {
    "text": "aggregations on the connection and do if you were using elastic search for example you could do aggregations on the",
    "start": "561610",
    "end": "566980"
  },
  {
    "text": "edge associated with a user you can get the most views by user for example and",
    "start": "566980",
    "end": "573730"
  },
  {
    "start": "573000",
    "end": "614000"
  },
  {
    "text": "here's another example of how you'd use that this is going to call the list objects field again we're gonna have the",
    "start": "573730",
    "end": "579940"
  },
  {
    "text": "first 10 we're gonna call it after now instead of next token because we're doing a forward pagination you can also",
    "start": "579940",
    "end": "586089"
  },
  {
    "text": "in the inverse you could say give me the last 10 before and then that would do the reverse pagination and move",
    "start": "586089",
    "end": "591580"
  },
  {
    "text": "backwards through a set so the pros of this are that you can paginate pretty freely you have a lot of flexibility the",
    "start": "591580",
    "end": "597820"
  },
  {
    "text": "cons are that it's gonna add a lot of overhead both to your schema and also to",
    "start": "597820",
    "end": "602890"
  },
  {
    "text": "your clients because when you do things like you're gonna do things like cogeneration you're gonna see these",
    "start": "602890",
    "end": "608200"
  },
  {
    "text": "edges in this code in the code generation you're gonna have to deal with those on the client okay so next",
    "start": "608200",
    "end": "616750"
  },
  {
    "start": "614000",
    "end": "701000"
  },
  {
    "text": "thing is data design so the tip for this is to use the best data base for the job",
    "start": "616750",
    "end": "623860"
  },
  {
    "text": "the reason is now with graph QL you basically have this common interface through which you can communicate with",
    "start": "623860",
    "end": "629800"
  },
  {
    "text": "any number of back-end data sources so if you have an old legacy database that may be unfriendly to use from a client",
    "start": "629800",
    "end": "635560"
  },
  {
    "text": "application but it's really good at what it does there's not as much pressure to move away from that that database that",
    "start": "635560",
    "end": "641380"
  },
  {
    "text": "does what it does really well you can put this layer in front of it and then all of your clients can interface with graph QL instead of that native database",
    "start": "641380",
    "end": "648790"
  },
  {
    "text": "language and then basically hide and abstract away the fact that it's really complicated or hard to use API so here's",
    "start": "648790",
    "end": "655570"
  },
  {
    "text": "a few databases that we use all the time you might have heard of some of them we've got dynamo DB which is a great",
    "start": "655570",
    "end": "661630"
  },
  {
    "text": "primary datastore it scales really well it's no nonsense so you don't have to think about it you just kind of turn it on and forget about",
    "start": "661630",
    "end": "667639"
  },
  {
    "text": "it and it's gonna continue to work which is really nice we've got elasticsearch which is more",
    "start": "667639",
    "end": "673220"
  },
  {
    "text": "for ad-hoc and analytical queries it's generally considered a little dangerous to treat it as your primary data store",
    "start": "673220",
    "end": "679790"
  },
  {
    "text": "especially if data changes and you have to get into re-indexing your elastic search indexes but it's really good at",
    "start": "679790",
    "end": "685220"
  },
  {
    "text": "doing full text search and analytical queries and then you've got your relational databases which are great at",
    "start": "685220",
    "end": "690230"
  },
  {
    "text": "what they do easy to relate information you can do transactions but then you start to fight them a little bit more",
    "start": "690230",
    "end": "696980"
  },
  {
    "text": "once you hit really large-scale so one",
    "start": "696980",
    "end": "702829"
  },
  {
    "start": "701000",
    "end": "747000"
  },
  {
    "text": "of the reasons that DynamoDB works really well with graph QL is that the operations map really well to each other you can have you know here you see the",
    "start": "702829",
    "end": "709820"
  },
  {
    "text": "mutations of create X update X delete X those map really well to DynamoDB put",
    "start": "709820",
    "end": "715250"
  },
  {
    "text": "item update item and delete item you can do the same thing for batch write and batch read it's really kind of a",
    "start": "715250",
    "end": "721459"
  },
  {
    "text": "one-to-one mapping and it's really easy to use and then the same for queries you can do a get item in a scan or a query",
    "start": "721459",
    "end": "727820"
  },
  {
    "text": "to return a list and it's kind of just a really nice mapping but then one of the",
    "start": "727820",
    "end": "733760"
  },
  {
    "text": "problems you find with DynamoDB is in almost everybody I talked to that's coming from a sequel background fights",
    "start": "733760",
    "end": "739699"
  },
  {
    "text": "this when they first start using DynamoDB is that they really just miss the ability to relate information with",
    "start": "739699",
    "end": "745010"
  },
  {
    "text": "one another or relate objects and the nice part is that with graph QL you can",
    "start": "745010",
    "end": "750709"
  },
  {
    "start": "747000",
    "end": "800000"
  },
  {
    "text": "basically add some of these relational capabilities to no sequel just using the",
    "start": "750709",
    "end": "755899"
  },
  {
    "text": "same operations that you've always had access to and dynamodb but when you're querying it from a client you don't",
    "start": "755899",
    "end": "761029"
  },
  {
    "text": "necessarily have to know that it's coming from dynamodb you might think it's coming from sequel because you're",
    "start": "761029",
    "end": "766040"
  },
  {
    "text": "able to do relational things but it's really not so here's an example of how we might do this we've got a type user",
    "start": "766040",
    "end": "772970"
  },
  {
    "text": "it's got tasks a user has tasks you have a task a task has an owner or a user and",
    "start": "772970",
    "end": "778100"
  },
  {
    "text": "then all you have to do is add a resolver to that tasks field on the user object that knows how to query a GSI and",
    "start": "778100",
    "end": "784459"
  },
  {
    "text": "DynamoDB and then as you query that from the client it's going to show up as if it was all one query the same for the",
    "start": "784459",
    "end": "790190"
  },
  {
    "text": "tasks the inverse and you'll see that the user field you can do a get item against the user table to relate the",
    "start": "790190",
    "end": "796010"
  },
  {
    "text": "user with any particular object in the task table so here's an",
    "start": "796010",
    "end": "801540"
  },
  {
    "start": "800000",
    "end": "855000"
  },
  {
    "text": "example of how that might work in practice here you can see we've got our get user resolver that's gonna just do a",
    "start": "801540",
    "end": "807360"
  },
  {
    "text": "straight get item against a dynamo DB table and then we've got that tasks field that's going to run a dynamo DB",
    "start": "807360",
    "end": "813209"
  },
  {
    "text": "query operation against a GSI that we created on the tasks index that's indexed by the user ID and the thing",
    "start": "813209",
    "end": "819930"
  },
  {
    "text": "that allows us to do this is what we'd call an app sink we call it the context it's often called the context or the",
    "start": "819930",
    "end": "825149"
  },
  {
    "text": "source in an open source implementation and it allows you to do for every single object parameterize the query slightly",
    "start": "825149",
    "end": "832110"
  },
  {
    "text": "differently so that we're looking at a different subsection of the information in the task table so that we're able to do this this query so get me all the",
    "start": "832110",
    "end": "839550"
  },
  {
    "text": "tasks for some user and then everywhere you're operating on that user that query",
    "start": "839550",
    "end": "844769"
  },
  {
    "text": "is gonna work whether it's in a mutation it's an ax query anywhere nested in any other query everywhere that tasks field",
    "start": "844769",
    "end": "851160"
  },
  {
    "text": "is is included in a selection set it's gonna grab the correct information and here's the inverse of that so you can do",
    "start": "851160",
    "end": "859140"
  },
  {
    "start": "855000",
    "end": "873000"
  },
  {
    "text": "a get task it's gonna do the same get item against a dynamo DB query table and then the owner is just gonna look for",
    "start": "859140",
    "end": "864959"
  },
  {
    "text": "that same user object that has the same ID as the context sources user ID so",
    "start": "864959",
    "end": "874800"
  },
  {
    "start": "873000",
    "end": "917000"
  },
  {
    "text": "next we'll talk about how you might add search to an API so Dynamo is great at",
    "start": "874800",
    "end": "879959"
  },
  {
    "text": "being a primary data store it's great at listing things it's great at simple filter filtering but it leaves a little",
    "start": "879959",
    "end": "886410"
  },
  {
    "text": "bit to be desired when we're talking about full-text search or strong analytics so another common pattern",
    "start": "886410",
    "end": "891750"
  },
  {
    "text": "we'll see is that people will add elasticsearch as a secondary data store to dynamo DB and then that allows them",
    "start": "891750",
    "end": "898529"
  },
  {
    "text": "to get the best of both worlds where they can have that really strong foundation and then they can have the",
    "start": "898529",
    "end": "903899"
  },
  {
    "text": "rich text search and not be so afraid that their index is going to get corrupted or that their elasticsearch",
    "start": "903899",
    "end": "909240"
  },
  {
    "text": "cluster is going to go down and then you can always re-index your elasticsearch cluster from DynamoDB if the worst case",
    "start": "909240",
    "end": "914910"
  },
  {
    "text": "happens so how do you do that here's a",
    "start": "914910",
    "end": "919980"
  },
  {
    "start": "917000",
    "end": "958000"
  },
  {
    "text": "really common pattern we do it all the time it's pretty resilient if you build in exponential back-off and retries into",
    "start": "919980",
    "end": "926520"
  },
  {
    "text": "your lambda function but what you do is you add a dynamodb stream to your table you handle that DynamoDB stream",
    "start": "926520",
    "end": "933819"
  },
  {
    "text": "with an AWS lambda function and then in that lambda function you use batch elasticsearch batch index operations to",
    "start": "933819",
    "end": "940360"
  },
  {
    "text": "push data into elasticsearch this adds without much overhead and without having to worry about ETL or any other more",
    "start": "940360",
    "end": "947560"
  },
  {
    "text": "complex technologies adds a really resilient pipeline to move information from your DynamoDB tables into your",
    "start": "947560",
    "end": "953680"
  },
  {
    "text": "elastic search indexes and then once you've got the data in your elastic search indexes you can just query that",
    "start": "953680",
    "end": "959949"
  },
  {
    "start": "958000",
    "end": "990000"
  },
  {
    "text": "thing directly from your graph QL API and this is how it work an app sync but you can do the same in your own where",
    "start": "959949",
    "end": "966160"
  },
  {
    "text": "you just hit that specific indexes search endpoint and then you can pass in any elastic search DSL query to do it in",
    "start": "966160",
    "end": "973420"
  },
  {
    "text": "this case we're creating a search tasks by title field that's just going to do a elastic search match query against an",
    "start": "973420",
    "end": "980350"
  },
  {
    "text": "index that's parameterize by the argument to graph QL and here you can see that we're using that simple",
    "start": "980350",
    "end": "986170"
  },
  {
    "text": "pagination pattern as well as well another common pattern is how to store",
    "start": "986170",
    "end": "994029"
  },
  {
    "start": "990000",
    "end": "1043000"
  },
  {
    "text": "files so there's a lot of options here you can base64 encode a file and and put",
    "start": "994029",
    "end": "999730"
  },
  {
    "text": "it into dynamo DB and then read it but then you lose all the benefits of s3 that have been optimized for a long time",
    "start": "999730",
    "end": "1005870"
  },
  {
    "text": "so s3 is a low latency high availability blob storage service it has CDNs we can use cloud front to",
    "start": "1005870",
    "end": "1013350"
  },
  {
    "text": "put a content delivery network in front of those our s3 buckets to basically deliver our assets anywhere around the",
    "start": "1013350",
    "end": "1019050"
  },
  {
    "text": "world in milliseconds and then if you're gonna build a support for s3 into an",
    "start": "1019050",
    "end": "1024209"
  },
  {
    "text": "application what I'd recommend doing is only storing pointers to s3 buckets and and keys in s3 buckets in dynamodb and",
    "start": "1024209",
    "end": "1032069"
  },
  {
    "text": "then allow the client to actually pull down the file because you're gonna see a lot of optimized data flow and be able",
    "start": "1032069",
    "end": "1039449"
  },
  {
    "text": "to leverage those CD ends to the best of your ability and here's how you do it so this is",
    "start": "1039449",
    "end": "1045750"
  },
  {
    "text": "actually cooked into all of the absent clients if you just add this s3 object type to an API that has a bucket a key",
    "start": "1045750",
    "end": "1052799"
  },
  {
    "text": "in a region and you associate that s3 object type with an object in your API if you pull down that field from your",
    "start": "1052799",
    "end": "1060900"
  },
  {
    "text": "client the absent clients are going to have support to be basically find that that object in s3 and load it",
    "start": "1060900",
    "end": "1067170"
  },
  {
    "text": "through a CDN you do the same for the s3 object input to follow that input pattern in the s in the absent clients",
    "start": "1067170",
    "end": "1074340"
  },
  {
    "text": "will know how to take the file from the device store the pointer and dynamodb and then in the meantime upload that",
    "start": "1074340",
    "end": "1081060"
  },
  {
    "text": "file directly to ask three using the optimized data flows and here's how that works so you can see we have a client",
    "start": "1081060",
    "end": "1087900"
  },
  {
    "start": "1084000",
    "end": "1113000"
  },
  {
    "text": "we're calling our create user mutation we have that single input field which has a user name and in this case a",
    "start": "1087900",
    "end": "1093420"
  },
  {
    "text": "profile picture where we're giving it a bucket a key in a region what's missing here is on the client you actually pass",
    "start": "1093420",
    "end": "1098820"
  },
  {
    "text": "a local URI as well and then the client is gonna push the data through app sync which goes into dynamo DB and then",
    "start": "1098820",
    "end": "1106170"
  },
  {
    "text": "meanwhile it's gonna push the blob directly to s3 using your ion credentials here's the inverse data flow",
    "start": "1106170",
    "end": "1115020"
  },
  {
    "start": "1113000",
    "end": "1133000"
  },
  {
    "text": "where you might want to query it so you're gonna do a get item call you're gonna get a user or get user call get",
    "start": "1115020",
    "end": "1120630"
  },
  {
    "text": "the user by ID you're gonna get the location and then that's seamlessly gonna pull that blob down from s3 and",
    "start": "1120630",
    "end": "1126510"
  },
  {
    "text": "pull it into your application so that you can use it as if you didn't even notice so next we'll talk a little bit a",
    "start": "1126510",
    "end": "1135090"
  },
  {
    "start": "1133000",
    "end": "1187000"
  },
  {
    "text": "lot about lambda so a lot of our intention with app sync was to make it so you don't have to use lambda but",
    "start": "1135090",
    "end": "1141540"
  },
  {
    "text": "almost everyone ends up having to use it for some reason or another it's a really",
    "start": "1141540",
    "end": "1146970"
  },
  {
    "text": "great way of adding compute or just arbitrary compute to a graphical endpoint and it gives you scale without",
    "start": "1146970",
    "end": "1153560"
  },
  {
    "text": "barely any ops you can also use it to connect any data source so if we don't",
    "start": "1153560",
    "end": "1158910"
  },
  {
    "text": "support a data source for you you can use a lambda you can even put your lambda in the V PC to access private",
    "start": "1158910",
    "end": "1164640"
  },
  {
    "text": "data sources and then connect any other data source that's not natively supported and pull it into your API so",
    "start": "1164640",
    "end": "1173370"
  },
  {
    "text": "the tip here is to use a standard event structure so this is one that's kind of been found through practice it's just",
    "start": "1173370",
    "end": "1180360"
  },
  {
    "text": "kind of painful to have a specific set of data coming to a different lambda function every different time you call",
    "start": "1180360",
    "end": "1185520"
  },
  {
    "text": "it so the tip would be to do something like this where every lambda function",
    "start": "1185520",
    "end": "1190920"
  },
  {
    "start": "1187000",
    "end": "1230000"
  },
  {
    "text": "that you use in an API would have the same input structure and you can parameterize these in the resolvers such",
    "start": "1190920",
    "end": "1197520"
  },
  {
    "text": "that it gives your for words your arguments it forwards your type and your fieldname and then forwards the source and the",
    "start": "1197520",
    "end": "1203070"
  },
  {
    "text": "identity so that you can do any authorization checks you might need and you can associate objects with their parent so this context that source is",
    "start": "1203070",
    "end": "1209970"
  },
  {
    "text": "the same thing that we used in order to query that GSI to get the tasks for a particular user and here's doing it",
    "start": "1209970",
    "end": "1218039"
  },
  {
    "text": "again in a query it's just a different field nothing changed other than the type and field name so then a question",
    "start": "1218039",
    "end": "1225330"
  },
  {
    "text": "the next question is always do I use one lambda or two and there's no right",
    "start": "1225330",
    "end": "1230730"
  },
  {
    "start": "1230000",
    "end": "1301000"
  },
  {
    "text": "answer but here's an example of both so some reasons that you would want to use",
    "start": "1230730",
    "end": "1236850"
  },
  {
    "text": "one lambda are that you benefit from container warming especially if you're using a language of the virtual machine",
    "start": "1236850",
    "end": "1242370"
  },
  {
    "text": "like Java or C sharp having a single lambda if you have one lambda it's gonna stay warm more often you're gonna see",
    "start": "1242370",
    "end": "1248429"
  },
  {
    "text": "lower latency from that lambda and you're gonna see some performance benefits there the downside so that",
    "start": "1248429",
    "end": "1253830"
  },
  {
    "text": "you're gonna have to write a little bit of routing logic but the plus side is that it's really not that hard to do and if you're familiar with writing your own",
    "start": "1253830",
    "end": "1259860"
  },
  {
    "text": "graph QL API is using an open source implementation it looks really similar to what you might do in that case here",
    "start": "1259860",
    "end": "1265320"
  },
  {
    "text": "you can see we're building up art essentially our resolver function map where we have at the root level a type",
    "start": "1265320",
    "end": "1271950"
  },
  {
    "text": "name and then the next nested level is just the field name on that type and then every value for those keys is a",
    "start": "1271950",
    "end": "1278760"
  },
  {
    "text": "function that returns the value that should be returned by by that resolver and then you can see all you have to do",
    "start": "1278760",
    "end": "1285210"
  },
  {
    "text": "is route it you call it pass back and then it's gonna work great the cons of",
    "start": "1285210",
    "end": "1290639"
  },
  {
    "text": "this are that you're gonna have to implement that routing logic and you're only gonna have one code base so if you have multiple teams working on it you're",
    "start": "1290639",
    "end": "1297210"
  },
  {
    "text": "gonna have to deal with with figuring out how to do that an alternative is to",
    "start": "1297210",
    "end": "1303090"
  },
  {
    "start": "1301000",
    "end": "1337000"
  },
  {
    "text": "write a lambda function per resolver this is one that people also do and it",
    "start": "1303090",
    "end": "1308700"
  },
  {
    "text": "works I've seen it work effectively the pros are that you're gonna have greater separation of concerns you're also gonna",
    "start": "1308700",
    "end": "1314220"
  },
  {
    "text": "be able to deploy resolvers independently of one another which just reduces the blast radius of",
    "start": "1314220",
    "end": "1319230"
  },
  {
    "text": "what you could potentially mess up in a deployment but the cons are that if you're using a language of the virtual",
    "start": "1319230",
    "end": "1325440"
  },
  {
    "text": "machine you're gonna see more cold starts which could lead to higher latency you're also going to have to deal with n number",
    "start": "1325440",
    "end": "1331350"
  },
  {
    "text": "repose instead of just that one which is really just a personal preference so",
    "start": "1331350",
    "end": "1337039"
  },
  {
    "start": "1337000",
    "end": "1357000"
  },
  {
    "text": "then to recap should you use it it's up to you I've seen both ways work",
    "start": "1337789",
    "end": "1343169"
  },
  {
    "text": "effectively the being able to deploy functions independently is really nice but then I also understand it can be a",
    "start": "1343169",
    "end": "1349830"
  },
  {
    "text": "lot of overhead and the cold-start benefits are real and can really improve your API so the next topic we're talk",
    "start": "1349830",
    "end": "1358799"
  },
  {
    "start": "1357000",
    "end": "1397000"
  },
  {
    "text": "about is real-time data so this one's pretty cool it's interesting in that it's something that everyone likes to",
    "start": "1358799",
    "end": "1365039"
  },
  {
    "text": "talk about and there's a lot of ways to do it wrong so what are we talking about",
    "start": "1365039",
    "end": "1370169"
  },
  {
    "text": "with real-time we're talking about graphic UL subscriptions subscriptions are the third operation type in graph QL that allow you to stream information",
    "start": "1370169",
    "end": "1376740"
  },
  {
    "text": "from an API to a device as it happens and in apps Inc what it really means is",
    "start": "1376740",
    "end": "1382380"
  },
  {
    "text": "a subscription is a reaction to a mutation and it's event-driven and then",
    "start": "1382380",
    "end": "1388559"
  },
  {
    "text": "the next the the obvious question that everyone iowa's here is how do you authorize them and how do you route them",
    "start": "1388559",
    "end": "1394110"
  },
  {
    "text": "efficiently so we'll talk about that so here's a simple example we've got a",
    "start": "1394110",
    "end": "1400080"
  },
  {
    "text": "published mutation and we've got a subscribe subscription this AWS subscribed directive is built into app",
    "start": "1400080",
    "end": "1406110"
  },
  {
    "text": "sink because app sink doubles as a as a broker a real-time broker pub/sub broker and allows you to just with all only",
    "start": "1406110",
    "end": "1412500"
  },
  {
    "text": "this you'll hook up subscriptions and not have to worry about I worry about any of anything else we run it over MQTT",
    "start": "1412500",
    "end": "1420179"
  },
  {
    "text": "with WebSockets and so it's resilient and it's still easy to use so as I mentioned there's a",
    "start": "1420179",
    "end": "1427980"
  },
  {
    "text": "lot of ways to do this wrong it's not that hard to think about how you could do it wrong if you imagine that you have",
    "start": "1427980",
    "end": "1434880"
  },
  {
    "text": "a million clients and a million million messages coming through and you need to figure out for each of those messages",
    "start": "1434880",
    "end": "1440880"
  },
  {
    "text": "which of these clients do I route this to and is that user authorized for this message that would be potentially a",
    "start": "1440880",
    "end": "1447809"
  },
  {
    "text": "million checks per message which would lead you for a million messages in a million clients to a trillion operations which is not scalable and is obviously a",
    "start": "1447809",
    "end": "1455909"
  },
  {
    "text": "problem so there are solutions to this I've heard that go is a solution to this which it's not",
    "start": "1455909",
    "end": "1462860"
  },
  {
    "text": "and then but the solution that we came up with was was two parts first one is",
    "start": "1462860",
    "end": "1470210"
  },
  {
    "text": "to authorize subscriptions at connect time and the second one is to actually bake the routing logic into the name of",
    "start": "1470210",
    "end": "1477080"
  },
  {
    "text": "the topic which we'll talk about a little bit so again here's another",
    "start": "1477080",
    "end": "1482330"
  },
  {
    "text": "example of a subscription field it has a single argument in it called chatroom ID in this argument in this field is",
    "start": "1482330",
    "end": "1489380"
  },
  {
    "text": "actually incredibly important we have a limit of five arguments that you can put",
    "start": "1489380",
    "end": "1494750"
  },
  {
    "text": "on any subscription field and that's because subscriptions in apps Inc are not dependent upon the number of",
    "start": "1494750",
    "end": "1499880"
  },
  {
    "text": "connected clients but actually dependent upon the number of fields that have passed into a subscription field and",
    "start": "1499880",
    "end": "1505400"
  },
  {
    "text": "we'll talk about this so first let's think about what a subscription query",
    "start": "1505400",
    "end": "1511250"
  },
  {
    "start": "1508000",
    "end": "1593000"
  },
  {
    "text": "really is a subscription query and app saying can people do this different ways but an app sync a subscription query is",
    "start": "1511250",
    "end": "1518240"
  },
  {
    "text": "basically a request to open a topic when that subscription query is received by the server it invokes the resolver",
    "start": "1518240",
    "end": "1524780"
  },
  {
    "text": "that's attached to that subscription and the purpose of that resolver is to authorize that subscription what this",
    "start": "1524780",
    "end": "1530750"
  },
  {
    "text": "allows you to do is in this example we might have the oncreate message with a chatroom ID being passed in we also have",
    "start": "1530750",
    "end": "1537230"
  },
  {
    "text": "the context of who is the caller we know the identity at this point so what it allows us to do is go look up in",
    "start": "1537230",
    "end": "1542900"
  },
  {
    "text": "whatever back-end data store we're using is this user able to access this chatroom if he is or she is allowed to",
    "start": "1542900",
    "end": "1550310"
  },
  {
    "text": "access that chatroom then we're gonna return a pre signed MQTT topic name and",
    "start": "1550310",
    "end": "1555440"
  },
  {
    "text": "then you're gonna be able to connect immediately to that topic the trick is that that topic is actually already",
    "start": "1555440",
    "end": "1561740"
  },
  {
    "text": "parameterised to only yield messages that are related to that chatroom ID in the way that this happens is that every",
    "start": "1561740",
    "end": "1569270"
  },
  {
    "text": "value that you pass in the subscription field gets turned into essentially a bitwise operation for equality every",
    "start": "1569270",
    "end": "1576290"
  },
  {
    "text": "time a message comes in we're gonna say it does does this what is the value of",
    "start": "1576290",
    "end": "1581480"
  },
  {
    "text": "the what is the value of the attribute on that object and then we're gonna emit publishes to all of the topics that",
    "start": "1581480",
    "end": "1588080"
  },
  {
    "text": "could potentially have subscribed to this message",
    "start": "1588080",
    "end": "1592660"
  },
  {
    "start": "1593000",
    "end": "1625000"
  },
  {
    "text": "so here's kind of how it works a little deeper is if I have one subscription that's asking for the chatroom ID one",
    "start": "1593390",
    "end": "1598970"
  },
  {
    "text": "and the logged in identity is authorized to access that chatroom we're gonna return topic a topic a is mapped to the",
    "start": "1598970",
    "end": "1606590"
  },
  {
    "text": "subscription where the chatroom ID is one if you added another argument to here you'd have not just two but you'd",
    "start": "1606590",
    "end": "1613790"
  },
  {
    "text": "have chat topic a topic be possibly topic topic C and it go on and grow as the number of input arguments changes",
    "start": "1613790",
    "end": "1620240"
  },
  {
    "text": "and then you can see the second one would be yielding topic B what happens then is when the mutation happens",
    "start": "1620240",
    "end": "1628130"
  },
  {
    "start": "1625000",
    "end": "1663000"
  },
  {
    "text": "it not only mutant so we it runs the mutation and it gets back the object it then looks at all registered mutations",
    "start": "1628130",
    "end": "1634309"
  },
  {
    "text": "are all registered subscriptions and finds the 32 topics that could possibly be that be thrown based off of the the",
    "start": "1634309",
    "end": "1642650"
  },
  {
    "text": "attributes that were parameterised in the subscription field so if you remember I said there's a limit of five values that you can pass into a",
    "start": "1642650",
    "end": "1649669"
  },
  {
    "text": "subscription field the reason that it's 32 topics max is that two to the fifth is our 2 to the 5 is 32 and that's the",
    "start": "1649669",
    "end": "1656450"
  },
  {
    "text": "maximum number of topics that we might subscribe publish to for any particular mutation now this is pretty neat because",
    "start": "1656450",
    "end": "1664580"
  },
  {
    "start": "1663000",
    "end": "1684000"
  },
  {
    "text": "what it does it essentially makes it so that our subscriptions are no longer dependent upon the number of connected",
    "start": "1664580",
    "end": "1669770"
  },
  {
    "text": "clients our subscription publishes are dependent upon the number of arguments that you pass into your subscription",
    "start": "1669770",
    "end": "1676010"
  },
  {
    "text": "field if you only have one then it's only going to be potentially passing to one so next we'll talk about some rapid",
    "start": "1676010",
    "end": "1686900"
  },
  {
    "start": "1684000",
    "end": "1757000"
  },
  {
    "text": "development techniques so when I started I said we've been working on a new set of client for frameworks and tools",
    "start": "1686900",
    "end": "1692000"
  },
  {
    "text": "called AWS amplify and that in compromise our encompasses a number of",
    "start": "1692000",
    "end": "1697280"
  },
  {
    "text": "projects there is the AWS amplify CLI which is a really great way to generate",
    "start": "1697280",
    "end": "1702500"
  },
  {
    "text": "and quickly scaffold projects and deploy them to the cloud via cloud formation and then there's also at amplify",
    "start": "1702500",
    "end": "1709010"
  },
  {
    "text": "framework that makes it really easy for you to consume the applications that you develop using the amplify CLI and in",
    "start": "1709010",
    "end": "1715370"
  },
  {
    "text": "general any graph QL API we also have a new project called the graph QL transform that's an open source project",
    "start": "1715370",
    "end": "1721880"
  },
  {
    "text": "that was released with the amplify CLI that it's it's all about allowing you to",
    "start": "1721880",
    "end": "1727340"
  },
  {
    "text": "think about your application in your data in terms of the data model without necessarily thinking about the",
    "start": "1727340",
    "end": "1732679"
  },
  {
    "text": "infrastructure that's necessary to make it run and we'll see a few examples of this and Lassen includes cool tools like",
    "start": "1732679",
    "end": "1739070"
  },
  {
    "text": "code gen so if you have a absent API and you want to generate code for your iOS or Android app that's better strongly",
    "start": "1739070",
    "end": "1745309"
  },
  {
    "text": "typed and work with Swift or Java will actually go and generate those types for you and make them work really well if",
    "start": "1745309",
    "end": "1751429"
  },
  {
    "text": "amplify so that it also works with for example the s3 support that I was mentioning earlier so here's an example",
    "start": "1751429",
    "end": "1759049"
  },
  {
    "text": "of what the amplifi CLI looks like the way that it is broken out is there are a number of categories categories include",
    "start": "1759049",
    "end": "1766340"
  },
  {
    "text": "things like off storage which includes both DynamoDB storage and s3 analytics",
    "start": "1766340",
    "end": "1771499"
  },
  {
    "text": "which creates pin point endpoints API which will allow you to create not only Apps Sync endpoints but also API gateway",
    "start": "1771499",
    "end": "1777799"
  },
  {
    "text": "endpoints as well and a number of others from the CLI it's really guided",
    "start": "1777799",
    "end": "1783440"
  },
  {
    "text": "experience I'd really recommend trying it because as you type these things it's really gonna guide you through what do you need what are the smart defaults and",
    "start": "1783440",
    "end": "1789619"
  },
  {
    "text": "we've baked best practices into those and then at the end all you have to do is run amplify push that's gonna",
    "start": "1789619",
    "end": "1795529"
  },
  {
    "text": "generate a bunch of CloudFormation documents it's gonna upload it to the cloud and then you're gonna be ready to go so then amplify has as I mentioned",
    "start": "1795529",
    "end": "1805279"
  },
  {
    "start": "1802000",
    "end": "1882000"
  },
  {
    "text": "the support for app sync in the way that we support app sync and amplify is through the graphic will transform",
    "start": "1805279",
    "end": "1810740"
  },
  {
    "text": "project that I mentioned earlier what it does is it allows you to declaratively declaratively define your applications",
    "start": "1810740",
    "end": "1817070"
  },
  {
    "text": "data model using graph QL SDL and that's it you don't have to worry about defining the key scheme of your dynamodb",
    "start": "1817070",
    "end": "1823610"
  },
  {
    "text": "table you're not worrying about GSIS or LSI's you're just saying I have users users have posts and posts have comments",
    "start": "1823610",
    "end": "1829850"
  },
  {
    "text": "and then we're gonna wire that all up with best practices so that you can quickly prototype and generate those api's for your applications it comes",
    "start": "1829850",
    "end": "1837980"
  },
  {
    "text": "with a number of pre-built transformers a transformer is essentially a class",
    "start": "1837980",
    "end": "1843679"
  },
  {
    "text": "that's that defines a directive and then every time that that directive is found in an input document that class is",
    "start": "1843679",
    "end": "1850429"
  },
  {
    "text": "called and invoked in order to do some amount of manipulation on essentially your cloud formation templates a context",
    "start": "1850429",
    "end": "1857509"
  },
  {
    "text": "and then you iteratively build things from there the cool part about this is that you guys can actually write your own so we",
    "start": "1857509",
    "end": "1863140"
  },
  {
    "text": "come with it comes with a number of them we have model off connection version and searchable but you we've had customers",
    "start": "1863140",
    "end": "1870040"
  },
  {
    "text": "that have built their own to do specific lambda use cases people that have written their own resolvers to call out to specific internal REST API s and then",
    "start": "1870040",
    "end": "1877150"
  },
  {
    "text": "from your input you just have a really easy to use interface and here's kind of what it",
    "start": "1877150",
    "end": "1883930"
  },
  {
    "start": "1882000",
    "end": "1937000"
  },
  {
    "text": "looks like so if you type amplify add API it's gonna ask you do you want to build a graph QL API you're gonna say",
    "start": "1883930",
    "end": "1890200"
  },
  {
    "text": "yes and then you're gonna add it's gonna ask you do you have a schema if you have",
    "start": "1890200",
    "end": "1895510"
  },
  {
    "text": "a schema you can just copy and paste it if you don't it'll give you a few tips on how to get started and it'll actually",
    "start": "1895510",
    "end": "1900760"
  },
  {
    "text": "open up the schema graph QL file in your text editor of choice and then there you can just start writing your graph QL",
    "start": "1900760",
    "end": "1906310"
  },
  {
    "text": "schema here you can see we've got a simple post it has an app model directive on it app model what it means",
    "start": "1906310",
    "end": "1912550"
  },
  {
    "text": "is we're gonna create a DynamoDB table for you we're gonna create a datasource we're gonna create an IM role that's",
    "start": "1912550",
    "end": "1918130"
  },
  {
    "text": "scope down just to talk to that DynamoDB table we're also gonna code gen crud",
    "start": "1918130",
    "end": "1923730"
  },
  {
    "text": "resolvers for you so you're gonna get create update delete get and list and",
    "start": "1923730",
    "end": "1928780"
  },
  {
    "text": "then it's gonna be all wired up so that just with these four lines of code you've got essentially a couple hundred",
    "start": "1928780",
    "end": "1933820"
  },
  {
    "text": "lines of CloudFormation that are all ready to go and then voila this is",
    "start": "1933820",
    "end": "1939010"
  },
  {
    "text": "essentially what you get this is what your api would count would come from that simple type def definition it gets",
    "start": "1939010",
    "end": "1947110"
  },
  {
    "start": "1946000",
    "end": "2013000"
  },
  {
    "text": "a little bit more advanced than that though so here you can see an example of how would use the off directive so that",
    "start": "1947110",
    "end": "1952990"
  },
  {
    "text": "we still have type posts our type is a model a model is going to be stored in dynamodb for now we have this off",
    "start": "1952990",
    "end": "1960610"
  },
  {
    "text": "directive and what the author active does is it allows you to add authorization to the crud operations that are generated by the transform in",
    "start": "1960610",
    "end": "1967540"
  },
  {
    "text": "this instance what it does is it actually code generates authorization checks depending on how you parameterize",
    "start": "1967540",
    "end": "1974050"
  },
  {
    "text": "these rules that are put into your resolvers and then well depending on the",
    "start": "1974050",
    "end": "1979690"
  },
  {
    "text": "data store we'll hook up the authorization check for example and dynamodb if you're trying to put in",
    "start": "1979690",
    "end": "1985090"
  },
  {
    "text": "ownership off authorization rule on a update operation it's actually going to",
    "start": "1985090",
    "end": "1990700"
  },
  {
    "text": "create a dynamo DB update condition expression that is going to make sure that you cannot update that object unless you're",
    "start": "1990700",
    "end": "1996580"
  },
  {
    "text": "actually the owner of that object as designated by the logged in user credential and then that's all you have",
    "start": "1996580",
    "end": "2002850"
  },
  {
    "text": "to do you don't have to worry about writing resolvers you don't decieve lhasa t it's really easy to use you click NAMP are you type amplify push",
    "start": "2002850",
    "end": "2009480"
  },
  {
    "text": "it's up in the cloud and you can start using it here's another example so this",
    "start": "2009480",
    "end": "2015210"
  },
  {
    "start": "2013000",
    "end": "2082000"
  },
  {
    "text": "one is my favorite it's at connection this is gonna allow you to easily associate objects without worrying about",
    "start": "2015210",
    "end": "2021900"
  },
  {
    "text": "how they're associated at all so you say type posts it's a model you have type comment it's also a model you've got",
    "start": "2021900",
    "end": "2028560"
  },
  {
    "text": "then corresponding connection fields that have the same name so that name is how we join fields in two different",
    "start": "2028560",
    "end": "2034500"
  },
  {
    "text": "models so in this case you can see that the comments is a list of comment the other pro here is you don't have to",
    "start": "2034500",
    "end": "2040470"
  },
  {
    "text": "worry about the connection types you just say it's a list and we understand it's a list in that you're gonna want pagination so we're gonna add that",
    "start": "2040470",
    "end": "2045870"
  },
  {
    "text": "pagination for you after you've deployed it in this case you can see it's gonna create a one-to-many relationship",
    "start": "2045870",
    "end": "2052080"
  },
  {
    "text": "between our posts and our comments so a comment has us as a single post and our",
    "start": "2052080",
    "end": "2057690"
  },
  {
    "text": "posts have many comments so what that's gonna do is it's actually going to create a GSI on the comment table that's",
    "start": "2057690",
    "end": "2063840"
  },
  {
    "text": "on the pote where the hash key is the post ID and then it's going to implement the query field resolver on the post",
    "start": "2063840",
    "end": "2070649"
  },
  {
    "text": "comments field and then the inverse it's going to implement the get item resolver on the comment post field with this",
    "start": "2070650",
    "end": "2077580"
  },
  {
    "text": "that's all you have to do you click push and you've got it here's an example of",
    "start": "2077580",
    "end": "2083700"
  },
  {
    "start": "2082000",
    "end": "2103000"
  },
  {
    "text": "how you'd query that just to make it make it ring so here you can just do a get post ID title comments you've got",
    "start": "2083700",
    "end": "2090179"
  },
  {
    "text": "your pagination parameters in there you can get the first ten things after some cursor and you never had to write a",
    "start": "2090180",
    "end": "2096300"
  },
  {
    "text": "single line of CloudFormation you never had to open up the dynamodb portal and you never had to see velocity",
    "start": "2096300",
    "end": "2102920"
  },
  {
    "start": "2103000",
    "end": "2125000"
  },
  {
    "text": "here's one more cool tool that has been really useful as of late it's we've",
    "start": "2103220",
    "end": "2108960"
  },
  {
    "text": "added support for Android and iOS so you can create strongly typed clients as well that are generated from your API",
    "start": "2108960",
    "end": "2115160"
  },
  {
    "text": "and then we're gonna have support for iOS which is Swift Android for Java typescript flow and even JavaScript",
    "start": "2115160",
    "end": "2122610"
  },
  {
    "text": "although it's not strongly typed all right next topic we've got offline data and",
    "start": "2122610",
    "end": "2129830"
  },
  {
    "start": "2125000",
    "end": "2229000"
  },
  {
    "text": "Delta sync so we've had support for offline for a while what it is it will",
    "start": "2129830",
    "end": "2135020"
  },
  {
    "text": "build normalized caches on clients and then will persist them in various offline stores if it's an react native",
    "start": "2135020",
    "end": "2141140"
  },
  {
    "text": "we'll use async storage and sequel Lite for for Android but what we've added now",
    "start": "2141140",
    "end": "2146690"
  },
  {
    "text": "is support for what we're calling Delta sync and what this is is it allows you to only download what's changed between",
    "start": "2146690",
    "end": "2154570"
  },
  {
    "text": "going offline and coming back online so the way that works is you're gonna have",
    "start": "2154570",
    "end": "2159860"
  },
  {
    "text": "a base query your base query is gonna is gonna hit the main entity table in previous examples we are using type post",
    "start": "2159860",
    "end": "2166970"
  },
  {
    "text": "your base query is gonna list that post table you can use queries you can pull down whatever information you need then",
    "start": "2166970",
    "end": "2173090"
  },
  {
    "text": "you've got a subscription query so after you've made your base query you would open a subscription that's then I'm",
    "start": "2173090",
    "end": "2178100"
  },
  {
    "text": "gonna start streaming information and merge it into your offline cache so that while you're online you're always up to",
    "start": "2178100",
    "end": "2184160"
  },
  {
    "text": "date the subscriptions are gonna push the information down it's gonna be in your offline cache the clients gonna read from the offline cache and it's",
    "start": "2184160",
    "end": "2190190"
  },
  {
    "text": "gonna show and be responsive but then what happened before is if you went offline there was no way to easily say I",
    "start": "2190190",
    "end": "2197090"
  },
  {
    "text": "only want to get the things that changed since I last synced so where this comes in is we've added support for what we",
    "start": "2197090",
    "end": "2204050"
  },
  {
    "text": "call the Delta query the Delta query is gonna hit a different table that table",
    "start": "2204050",
    "end": "2209300"
  },
  {
    "text": "is basically going to be a transient table of temporary table that only has the information that's changed it's",
    "start": "2209300",
    "end": "2215660"
  },
  {
    "text": "going to use dynamodb time-to-live attributes in order to flush information that's that's old that we don't need",
    "start": "2215660",
    "end": "2221720"
  },
  {
    "text": "anymore and then you're gonna be able to hit that Delta query against that Delta table to only get the information that's",
    "start": "2221720",
    "end": "2227480"
  },
  {
    "text": "changed without over fetching so here's how that works so last week we also",
    "start": "2227480",
    "end": "2232880"
  },
  {
    "start": "2229000",
    "end": "2354000"
  },
  {
    "text": "released a new feature called the pipeline resolver a pipeline resolver is or previously we had resolvers resolvers",
    "start": "2232880",
    "end": "2239360"
  },
  {
    "text": "were functions that were able to go hit any one datastore pipeline resolvers",
    "start": "2239360",
    "end": "2245750"
  },
  {
    "text": "basically move it a level up and you're now able to chain multiple resolvers in unison on a single field so you can hit",
    "start": "2245750",
    "end": "2251900"
  },
  {
    "text": "multiple data sources in all in one resolution for a field this is super",
    "start": "2251900",
    "end": "2257329"
  },
  {
    "text": "useful for authorization if you have some authorization credential that lives if you want to only be able to create",
    "start": "2257329",
    "end": "2262579"
  },
  {
    "text": "messages in the chat room you're a member of for example you'd be able to first make sure that you're a member of",
    "start": "2262579",
    "end": "2267619"
  },
  {
    "text": "that chat room and then authorize to create the message we also use it for Delta sync with Delta sync we would",
    "start": "2267619",
    "end": "2274460"
  },
  {
    "text": "first create the posts in the enemy table we then also create an entry in the Delta sync table we'd set an",
    "start": "2274460",
    "end": "2280819"
  },
  {
    "text": "expiration date that is what is the longest amount of time that we want this object to be in the Delta table and then",
    "start": "2280819",
    "end": "2287930"
  },
  {
    "text": "we're gonna order that thing by timestamp and then once you're reading it this would be your base query your",
    "start": "2287930",
    "end": "2294950"
  },
  {
    "text": "base query is gonna hit the entity table it's gonna read all that information from the entity table into your offline",
    "start": "2294950",
    "end": "2300140"
  },
  {
    "text": "cache you're hopefully only gonna have to do this one time and if you don't only do it one time you're only gonna have to do it next time is when you want",
    "start": "2300140",
    "end": "2306770"
  },
  {
    "text": "to completely update your cache from there you'll then use the list deltas",
    "start": "2306770",
    "end": "2312710"
  },
  {
    "text": "query that's going to go and read against that Delta table and find all the records where the timestamp is is",
    "start": "2312710",
    "end": "2319430"
  },
  {
    "text": "greater than or equal to the last sync time so I only want the records that have been created since I last synced",
    "start": "2319430",
    "end": "2325010"
  },
  {
    "text": "from my individual client app per you know different users might have different last sync times and such that",
    "start": "2325010",
    "end": "2331520"
  },
  {
    "text": "the expiration date is greater than now so that we're not getting data that we meant to be flushed out there's actually",
    "start": "2331520",
    "end": "2337220"
  },
  {
    "text": "support for this in the absent client so you can give a base query a subscription and a delta query and all of this is",
    "start": "2337220",
    "end": "2343369"
  },
  {
    "text": "going to be happening behind the scenes for you and then the transform will eventually support kind of generating to",
    "start": "2343369",
    "end": "2349010"
  },
  {
    "text": "stuff really quickly on your behalf so",
    "start": "2349010",
    "end": "2353170"
  },
  {
    "start": "2354000",
    "end": "2569000"
  },
  {
    "text": "another thing we're going to talk about so see ICD this is a super hot topic",
    "start": "2355480",
    "end": "2361400"
  },
  {
    "text": "when something we hear a lot from customers and the good news is that yesterday we released an entirely new",
    "start": "2361400",
    "end": "2367309"
  },
  {
    "text": "service it's called the amplify console it's now a cloud service and the amplified console is a a really",
    "start": "2367309",
    "end": "2375109"
  },
  {
    "text": "no-nonsense CI CD platform as well as web hosting platform that's going to allow you to",
    "start": "2375109",
    "end": "2381589"
  },
  {
    "text": "West to host web apps with Bluegreen deployments and atomic deployments right",
    "start": "2381589",
    "end": "2387500"
  },
  {
    "text": "the bat if you're using the amplify CLI it's gonna be extremely seamless you're gonna be able to whatever you can do",
    "start": "2387500",
    "end": "2392780"
  },
  {
    "text": "with amplify CLI you're gonna be able to do with the amplify console and it's gonna be able to add CIC D really",
    "start": "2392780",
    "end": "2400070"
  },
  {
    "text": "quickly so one of the cool parts about this is the atomic deployments so atomic",
    "start": "2400070",
    "end": "2405290"
  },
  {
    "text": "deployments the problem that we're trying to solve is if you're pushing front-end code and say you're hosting",
    "start": "2405290",
    "end": "2410720"
  },
  {
    "text": "that front-end code on an s3 bucket that s3 bucket when you're pushing it it you",
    "start": "2410720",
    "end": "2416120"
  },
  {
    "text": "might not finish the deployment and if you're pushing to the same bucket you might get your application stuck in an unsafe State say you were able to push",
    "start": "2416120",
    "end": "2423200"
  },
  {
    "text": "the first half but you weren't able to push the second half then the deployment fails for some reason your applications",
    "start": "2423200",
    "end": "2429320"
  },
  {
    "text": "likely going to be broken with this what it we actually do is we use lambda at",
    "start": "2429320",
    "end": "2434750"
  },
  {
    "text": "edge in order to when you do the employment it'll push in to an entirely new bucket and then at the time that the",
    "start": "2434750",
    "end": "2441860"
  },
  {
    "text": "deployment finishes it uses lambda at edge in order to change the routing so that it just starts pointing to your new",
    "start": "2441860",
    "end": "2446930"
  },
  {
    "text": "bucket instead of the old one it also has support for github bitbucket",
    "start": "2446930",
    "end": "2452330"
  },
  {
    "text": "gitlab an AWS code commit as your push triggers so there's no more worrying",
    "start": "2452330",
    "end": "2457340"
  },
  {
    "text": "about how do I push my information to s3 you just build a you use a build spec you put it in your repository you're",
    "start": "2457340",
    "end": "2463490"
  },
  {
    "text": "able to then just use your normal developer workflow every time you push the github we get a web hook we're gonna",
    "start": "2463490",
    "end": "2469670"
  },
  {
    "text": "build that we're gonna deploy your front-end we'll deploy your back end and then we'll update the hosting using the atomic deployment another nice feature",
    "start": "2469670",
    "end": "2477950"
  },
  {
    "text": "that this comes with is the amplify CLI is launching a beta for multi environment support this has been one of",
    "start": "2477950",
    "end": "2484250"
  },
  {
    "text": "the most common tasks we've had where not only do we want multiple environments such that I can have my deb",
    "start": "2484250",
    "end": "2489770"
  },
  {
    "text": "in my prod but we have teams that are larger than one person that are trying",
    "start": "2489770",
    "end": "2495920"
  },
  {
    "text": "to share code between each other and if you have to always operate off the same cloud infrastructure you're gonna run it on and step on each other's toes so what",
    "start": "2495920",
    "end": "2503090"
  },
  {
    "text": "this does is it allows you to have essentially a branch per environment and you can even do multiple branches it's a",
    "start": "2503090",
    "end": "2510230"
  },
  {
    "text": "little bit different but you would be able to have a your master branch or",
    "start": "2510230",
    "end": "2515270"
  },
  {
    "text": "your local development feature branch you could then associate that local development feature branch with Michael",
    "start": "2515270",
    "end": "2521000"
  },
  {
    "text": "Vox when I then run amplify push it's gonna parameterize everything so that it's specific to my sandbox I can then",
    "start": "2521000",
    "end": "2528260"
  },
  {
    "text": "go to the amplified console say I'd go create I'd have a dev branch in a prod branch that are kind of our CI CD our",
    "start": "2528260",
    "end": "2534859"
  },
  {
    "text": "continuous flows when I push or merge my feature branch into that dev branch",
    "start": "2534859",
    "end": "2539960"
  },
  {
    "text": "there's gonna be another environment that's managed that's actually hosted on the amplify console and that will push",
    "start": "2539960",
    "end": "2547190"
  },
  {
    "text": "all of the changes into that branch and it'll update that stack and push it to the cloud and then as you merge that dev",
    "start": "2547190",
    "end": "2553550"
  },
  {
    "text": "branch into the prod branch it's gonna do the same thing and allow you to move changes between feature branches and",
    "start": "2553550",
    "end": "2558619"
  },
  {
    "text": "then kind of beta branches into your production branches with a lot more confidence and it allows you to share",
    "start": "2558619",
    "end": "2564050"
  },
  {
    "text": "code with your team which is really nice",
    "start": "2564050",
    "end": "2568300"
  },
  {
    "start": "2569000",
    "end": "2634000"
  },
  {
    "text": "so in terms of operations and monitoring there's a few things that I'd recommend you do the first is there's support for",
    "start": "2569980",
    "end": "2577460"
  },
  {
    "text": "cloud watching at every Amazon app sync API all you have to do is toggle a",
    "start": "2577460",
    "end": "2582890"
  },
  {
    "text": "switch and then we'll start streaming per hour basically resolver and per field logs into your cloud watch account",
    "start": "2582890",
    "end": "2588770"
  },
  {
    "text": "for you you can then go and set up your own it own alerts you can set up alerts for 4x X 5 xx latency error messages and",
    "start": "2588770",
    "end": "2596660"
  },
  {
    "text": "logs and all these nice things and it's something that you kind of just have to go to the settings page and turn it on and we find that it's kind of hidden but",
    "start": "2596660",
    "end": "2603500"
  },
  {
    "text": "I really recommend that you put it on and turn it on and then as I said you get 4 X X and 5 X X metrics as well and",
    "start": "2603500",
    "end": "2611180"
  },
  {
    "text": "then we'll see we'll have more for that in the future and then in addition to that I mentioned the analytics category",
    "start": "2611180",
    "end": "2617960"
  },
  {
    "text": "and amplify this is gonna be a pinpoint API that you can use for really powerful",
    "start": "2617960",
    "end": "2623000"
  },
  {
    "text": "client logging and analytics that's a little different than the service but it's a nice way to add analytics to your client application so you can get things",
    "start": "2623000",
    "end": "2629330"
  },
  {
    "text": "like session time number of page visits and things like that another cool thing",
    "start": "2629330",
    "end": "2636290"
  },
  {
    "text": "that you can do and this is something that you've now you're now able to do because of pipeline resolvers is you're",
    "start": "2636290",
    "end": "2643910"
  },
  {
    "text": "able to basically add resolver auditing for for particularly sensitive resolvers",
    "start": "2643910",
    "end": "2650570"
  },
  {
    "text": "in the way that you would do that is you would create an audit function so I mentioned that a pipeline resolver is",
    "start": "2650570",
    "end": "2656920"
  },
  {
    "text": "now basically a way to run in series a number of different what we would",
    "start": "2656920",
    "end": "2662620"
  },
  {
    "text": "previously have called to resolvers but now there's a new concept called appsync functions which are essentially a single",
    "start": "2662620",
    "end": "2669460"
  },
  {
    "text": "distinct operation against a data source and you can create a single audit function that would know how to push a",
    "start": "2669460",
    "end": "2675790"
  },
  {
    "text": "log into your own DynamoDB account or you could stream it into some other monitoring portal or time series",
    "start": "2675790",
    "end": "2681580"
  },
  {
    "text": "database using HTTP resolvers and with that single audit function you can then add that auto function to all of the",
    "start": "2681580",
    "end": "2688270"
  },
  {
    "text": "resolvers that you care about and every time those those functions are that resolver is invoked you're gonna get",
    "start": "2688270",
    "end": "2693820"
  },
  {
    "text": "that audit log showing up in your datastore so that you can do your own custom auditing without having to worry",
    "start": "2693820",
    "end": "2699160"
  },
  {
    "text": "about changing every single resolver and or running everything through a lambda",
    "start": "2699160",
    "end": "2704680"
  },
  {
    "text": "function next a few tips for testing",
    "start": "2704680",
    "end": "2711250"
  },
  {
    "start": "2707000",
    "end": "2822000"
  },
  {
    "text": "api's I'd break it into three categories of things that you can do one is",
    "start": "2711250",
    "end": "2717010"
  },
  {
    "text": "integration testing there's also unit testing of course but for the api's integration testing this would be",
    "start": "2717010",
    "end": "2722140"
  },
  {
    "text": "something that you would trigger upon your CI CD workflows with AWS amplify",
    "start": "2722140",
    "end": "2727420"
  },
  {
    "text": "console we actually ping out a cloud watch event that you can then attach a lambda function to so that every time an",
    "start": "2727420",
    "end": "2733870"
  },
  {
    "text": "event happens in in your CI CD flow in the console you can run your integration test and make sure that that thing",
    "start": "2733870",
    "end": "2739810"
  },
  {
    "text": "didn't break another really common workflow for this is to do basically",
    "start": "2739810",
    "end": "2745210"
  },
  {
    "text": "detect breaking changes and to prevent breaking changes and if you look at the graphical JS library there's a nice",
    "start": "2745210",
    "end": "2751990"
  },
  {
    "text": "utility that can just take any two schemas and then perform a DIF and tell you which of those changes could",
    "start": "2751990",
    "end": "2757780"
  },
  {
    "text": "potentially be B breaking and then you can add your own logic to alert on those",
    "start": "2757780",
    "end": "2763270"
  },
  {
    "text": "and and have various workflows in order to stop those from flowing into production the other is just to have",
    "start": "2763270",
    "end": "2771160"
  },
  {
    "text": "Canaries so Canaries are it's the the canary in the coal mine you need these",
    "start": "2771160",
    "end": "2776560"
  },
  {
    "text": "things are running all the time every few minutes they're just sitting there waiting for something to go wrong when",
    "start": "2776560",
    "end": "2781810"
  },
  {
    "text": "something goes wrong you alert and then you get notifications using SNS through a neat using",
    "start": "2781810",
    "end": "2787000"
  },
  {
    "text": "or any other way and then another one that's particularly interesting is",
    "start": "2787000",
    "end": "2792160"
  },
  {
    "text": "basically to just automatically create test Suites using various tools so",
    "start": "2792160",
    "end": "2797950"
  },
  {
    "text": "there's some projects on github that allow you to do this it's not that hard to do it yourself but you can basically auto generate fuzz tests against any",
    "start": "2797950",
    "end": "2804730"
  },
  {
    "text": "graph QL API just by seeing by introspecting the schema and then throwing everything that you can think",
    "start": "2804730",
    "end": "2810550"
  },
  {
    "text": "of at it and just do it randomly so this is nice for basically finding the edge cases that you wouldn't otherwise test",
    "start": "2810550",
    "end": "2816340"
  },
  {
    "text": "with integration tests and it's something we see a lot of people doing",
    "start": "2816340",
    "end": "2821069"
  },
  {
    "start": "2822000",
    "end": "2969000"
  },
  {
    "text": "so last topic is schema governance so this is one that is also a very common",
    "start": "2822780",
    "end": "2828190"
  },
  {
    "text": "question it's something that it I find happens in evolutions almost everybody",
    "start": "2828190",
    "end": "2833770"
  },
  {
    "text": "we talked to starts with a centralized schema governance a kind of plan and then they always think how do I get to",
    "start": "2833770",
    "end": "2840730"
  },
  {
    "text": "that point that we're doing to centralize schema governance and to back up when I'm talking about schema",
    "start": "2840730",
    "end": "2845830"
  },
  {
    "text": "governance I'm basically talking about who is the gatekeeper to decide what schema changes get pushed through to production and when you see a lot of",
    "start": "2845830",
    "end": "2853630"
  },
  {
    "text": "these companies they'll start where there's one team that has ownership of a repository that has that big scheme",
    "start": "2853630",
    "end": "2859390"
  },
  {
    "text": "about graph to a file or a number of schema craftwell files in it and then they are the ones that will say yes or",
    "start": "2859390",
    "end": "2865869"
  },
  {
    "text": "no to changes coming in and often that'll happen through a series of pull requests so then if you're a team that's",
    "start": "2865869",
    "end": "2872200"
  },
  {
    "text": "downstream or your client team and you need the new field you'd basically go forth that repo you'd create a change",
    "start": "2872200",
    "end": "2878350"
  },
  {
    "text": "you create a pull request and then it'd be the ownership of that team to go to side is this okay or not the alternative",
    "start": "2878350",
    "end": "2885910"
  },
  {
    "text": "to that is decentralized schema governance which takes a little bit longer to get right but it's really the",
    "start": "2885910",
    "end": "2891280"
  },
  {
    "text": "it's it's the golden ticket it allows everyone to have control of their own subgraph and there's actually features",
    "start": "2891280",
    "end": "2898480"
  },
  {
    "text": "built into the graphical language itself to allow you to do this and the main one being type extensions so it allow you to",
    "start": "2898480",
    "end": "2905770"
  },
  {
    "text": "you'd have your base API where the the root of your API defines the root of the",
    "start": "2905770",
    "end": "2910990"
  },
  {
    "text": "type query your types description your type mutation maybe any of your user types or anything else and then that'd",
    "start": "2910990",
    "end": "2916810"
  },
  {
    "text": "be the basically the route and then any other teams that need to anything would basically be able to go",
    "start": "2916810",
    "end": "2922069"
  },
  {
    "text": "add their own schema declarations that use the extension types API so that you could add fields to your query or your",
    "start": "2922069",
    "end": "2928190"
  },
  {
    "text": "mutation or your any other type in your API but the hard part of this is getting",
    "start": "2928190",
    "end": "2933200"
  },
  {
    "text": "it right generally requires a more advanced C ICD and kind of integration flow so if I'm",
    "start": "2933200",
    "end": "2938809"
  },
  {
    "text": "gonna make a change I want to be sure that it's gonna work and if I'm responsible for the service as a whole I",
    "start": "2938809",
    "end": "2944989"
  },
  {
    "text": "don't want that downstream team to be able to go break something where I don't necessarily have the way to to say no so",
    "start": "2944989",
    "end": "2952309"
  },
  {
    "text": "then a lot of these things that I mentioned before generally come first and teams work towards this because it",
    "start": "2952309",
    "end": "2957559"
  },
  {
    "text": "does become a bottleneck once you get sufficiently large that this the one team that's managing your schema can't",
    "start": "2957559",
    "end": "2963739"
  },
  {
    "text": "really keep up with the the speed of the rest of the organization so that's it",
    "start": "2963739",
    "end": "2970599"
  },
  {
    "start": "2969000",
    "end": "3389000"
  },
  {
    "text": "thank you for listening I really appreciate it it looks like we have a few minutes left so I'll open up the floor for questions",
    "start": "2970599",
    "end": "2976640"
  },
  {
    "text": "and we can do a little QA and follow me",
    "start": "2976640",
    "end": "2983588"
  },
  {
    "text": "so there's no definite timeline yet throttling and other rate-limiting is",
    "start": "2989920",
    "end": "2995750"
  },
  {
    "text": "coming in the future but I cannot unfortunately give a date necessarily right now",
    "start": "2995750",
    "end": "3002578"
  },
  {
    "text": "wait sorry one more time",
    "start": "3016200",
    "end": "3019920"
  },
  {
    "text": "to avoid hitting the file size limits or response size limits gotcha yeah so his",
    "start": "3030970",
    "end": "3039589"
  },
  {
    "text": "question was there's a there's a one megabyte limit per resolver and and techniques in order to avoid that and",
    "start": "3039589",
    "end": "3045170"
  },
  {
    "text": "get around that I mean it's a good",
    "start": "3045170",
    "end": "3051680"
  },
  {
    "text": "question I mean the the limit is a limit you could as you said kind of break down that that thing into multiple sub",
    "start": "3051680",
    "end": "3058430"
  },
  {
    "text": "resolvers but then the downside of that is we'd be running more you can also just ask AWS to increase the limit and",
    "start": "3058430",
    "end": "3065720"
  },
  {
    "text": "we can see what we can do on the clients",
    "start": "3065720",
    "end": "3075650"
  },
  {
    "text": "themselves so I'm not aware of that of the the roadmap for that but that's not",
    "start": "3075650",
    "end": "3082790"
  },
  {
    "text": "to say it's not being worked on I can definitely follow up and see but we don't have any different timelines that",
    "start": "3082790",
    "end": "3089619"
  },
  {
    "text": "yep so schema management is something that we're aware of and that it will likely see some changes in the future",
    "start": "3101420",
    "end": "3107420"
  },
  {
    "text": "basically being able to break things up into more modular pieces of not just one schema but multiple schemas and then",
    "start": "3107420",
    "end": "3113210"
  },
  {
    "text": "also versioning and modules essentially so it's where well yeah again no time",
    "start": "3113210",
    "end": "3121640"
  },
  {
    "text": "yeah yep",
    "start": "3121640",
    "end": "3133750"
  },
  {
    "text": "mm-hm",
    "start": "3152260",
    "end": "3155260"
  },
  {
    "text": "so you can definitely still continue to use dynamo for reading data it I think it depends on on what you're trying to",
    "start": "3170789",
    "end": "3178329"
  },
  {
    "text": "query by can you give an example of what your query pattern would be",
    "start": "3178329",
    "end": "3184109"
  },
  {
    "text": "so you wanna you basically want ad hoc queries on a dynamo table yeah so I mean",
    "start": "3205340",
    "end": "3212010"
  },
  {
    "text": "one of the downsides of dynamo is that when you're deploying dynamo tables you need to think about your query patterns",
    "start": "3212010",
    "end": "3217560"
  },
  {
    "text": "beforehand so if your so this may be a not an answer that you're looking for",
    "start": "3217560",
    "end": "3223020"
  },
  {
    "text": "but if you want to be able to find you know one field you want all the records where a particular field is equal to X",
    "start": "3223020",
    "end": "3228450"
  },
  {
    "text": "you need to think about that beforehand and then create that GSI or the LSI in order to be able to do that or else",
    "start": "3228450",
    "end": "3233850"
  },
  {
    "text": "you're gonna be basically querying that sparse index",
    "start": "3233850",
    "end": "3238160"
  },
  {
    "text": "so would you be able to use sequel would",
    "start": "3256200",
    "end": "3262150"
  },
  {
    "text": "you be able to use RDS relation to sequel Aurora it yeah I mean that's I",
    "start": "3262150",
    "end": "3287079"
  },
  {
    "text": "think that it is a use case driven decision like if they're if you're trying to do ad hoc queries against a dynamo table you're gonna run into some",
    "start": "3287079",
    "end": "3293530"
  },
  {
    "text": "problems at scale because I mean say you've got if you don't have a GSI on a field and you want to just find all the",
    "start": "3293530",
    "end": "3300250"
  },
  {
    "text": "records where some value is equal to n then you're gonna have to scan that entire index right right so the solution",
    "start": "3300250",
    "end": "3307930"
  },
  {
    "text": "to that is think about your query patterns beforehand and then build that GSI into the table and then you're able",
    "start": "3307930",
    "end": "3313210"
  },
  {
    "text": "to get the efficient query but if you're looking for the ad-hoc query that you're you just want to be able to on any field",
    "start": "3313210",
    "end": "3319539"
  },
  {
    "text": "do any sort of computation and figure out all the values where a is greater than B or B is greater than C then",
    "start": "3319539",
    "end": "3325450"
  },
  {
    "text": "you're really gonna be probably looking at a different database something like elastic search how many fields do you",
    "start": "3325450",
    "end": "3335589"
  },
  {
    "text": "have if you if you leave so there's a limit of five GSIS per table and if you have five fields that you know you need",
    "start": "3335589",
    "end": "3341470"
  },
  {
    "text": "this query by then you can create a GSI on that field and what that does is only",
    "start": "3341470",
    "end": "3347020"
  },
  {
    "text": "records where what basically only records where that field is filled out go into that GSI and then you're able to",
    "start": "3347020",
    "end": "3353470"
  },
  {
    "text": "query it by a value so if you want to know where the name is equal to Michael",
    "start": "3353470",
    "end": "3359109"
  },
  {
    "text": "then you'd be able to hit that GSI and find only the records where the name is equal to Michael but if you if you are",
    "start": "3359109",
    "end": "3365470"
  },
  {
    "text": "just arbitrarily saying oh today I might need the name but then tomorrow I might need some other attribute then you're",
    "start": "3365470",
    "end": "3372430"
  },
  {
    "text": "gonna have to go add another GSI to that table in order to efficiently query that index otherwise you're gonna be stuck",
    "start": "3372430",
    "end": "3378940"
  },
  {
    "text": "with a table filter and of the scan",
    "start": "3378940",
    "end": "3382890"
  },
  {
    "start": "3389000",
    "end": "3600000"
  },
  {
    "text": "so let me jump back so are we talking about the the search right all the way",
    "start": "3389540",
    "end": "3400020"
  },
  {
    "text": "at the beginning so you're asking now how do you do the streaming into",
    "start": "3400020",
    "end": "3405150"
  },
  {
    "text": "elasticsearch no sorry looks like it's stuck let's see",
    "start": "3405150",
    "end": "3418970"
  },
  {
    "text": "so with this solution you're saying this solution would not would not work yeah",
    "start": "3421670",
    "end": "3437720"
  },
  {
    "text": "yeah so this will definitely I think work for your solution is if you're gonna stream information from dynamo into elastic search elastic search has a",
    "start": "3437720",
    "end": "3443630"
  },
  {
    "text": "much richer dsl query language so it will allow you to do really complex queries over really large data sets and",
    "start": "3443630",
    "end": "3450470"
  },
  {
    "text": "also allow you to do really advanced analytical queries so that you can do aggregations and counting and stuff like",
    "start": "3450470",
    "end": "3455539"
  },
  {
    "text": "that where a dynamo won't allow you to do that kind of stuff",
    "start": "3455539",
    "end": "3460450"
  },
  {
    "text": "to move from dynamodb into elasticsearch you mean anywhere just to read it",
    "start": "3484040",
    "end": "3492950"
  },
  {
    "text": "[Music]",
    "start": "3507610",
    "end": "3510780"
  },
  {
    "text": "right so I think I think again the the answer is if you're using a scan scans",
    "start": "3518210",
    "end": "3523859"
  },
  {
    "text": "are gonna read the entire index so a scans gonna read the whole table every single time you should be adding GSIS on",
    "start": "3523859",
    "end": "3531000"
  },
  {
    "text": "the fields that you care about and then when you add that GSI you're gonna go straight to those records and it's gonna your you're no longer gonna miss records",
    "start": "3531000",
    "end": "3537690"
  },
  {
    "text": "it's gonna be able to give you exactly the data that you need right right it's",
    "start": "3537690",
    "end": "3544589"
  },
  {
    "text": "that's just a it's just a pattern of how no sequel works because dynamo is essentially a giant distributed hash",
    "start": "3544589",
    "end": "3551329"
  },
  {
    "text": "so basically when you have unstructured data that you want yeah so there's",
    "start": "3574190",
    "end": "3580410"
  },
  {
    "text": "there's I have two inches to this one is that something I've seen kind of in the graphical community in general is that",
    "start": "3580410",
    "end": "3586650"
  },
  {
    "text": "there may be a need for a map type a map type where you'd think of it almost like how you can do a map type in like",
    "start": "3586650",
    "end": "3592320"
  },
  {
    "text": "typescript where you can say the key is of type X and the value is of type Y that may be coming it's just that's",
    "start": "3592320",
    "end": "3598860"
  },
  {
    "text": "gonna take a long time because the spec is a slow-moving thing the other way I'd recommend it is if you can think about",
    "start": "3598860",
    "end": "3604590"
  },
  {
    "text": "it instead of thinking about it as a map you can think of it like an association list so instead of having just keys and",
    "start": "3604590",
    "end": "3612540"
  },
  {
    "text": "values you can basically have a key field in a value field and then you'd be able to either have that be the primary",
    "start": "3612540",
    "end": "3617760"
  },
  {
    "text": "key or have that be the in index where the hash key is gonna be like the name",
    "start": "3617760",
    "end": "3622860"
  },
  {
    "text": "of the attribute that you care about and then the value field would just be the value that's associated with it and then",
    "start": "3622860",
    "end": "3628740"
  },
  {
    "text": "it you'd basically be able to get that unstructured data in the in the form of like an association list instead of as a",
    "start": "3628740",
    "end": "3636210"
  },
  {
    "text": "map because then the map would fight your the map would fight the schema both",
    "start": "3636210",
    "end": "3641310"
  },
  {
    "text": "the Association list you can just say this is a list of type X where X is the key value so it's common task node no",
    "start": "3641310",
    "end": "3657510"
  },
  {
    "text": "deadlines but we know we also just",
    "start": "3657510",
    "end": "3663270"
  },
  {
    "text": "supported support are just really support for RDS so that's a cool one I don't know if you guys knew but there's",
    "start": "3663270",
    "end": "3668370"
  },
  {
    "text": "a new HTTP endpoint for RDS which is pretty neat so we're one of the first people to be using that so you can",
    "start": "3668370",
    "end": "3674070"
  },
  {
    "text": "actually directly run sequel queries from app sync into an RDS instance and it'll work without having to worry about",
    "start": "3674070",
    "end": "3679560"
  },
  {
    "text": "connection pooling or any of that fun stuff",
    "start": "3679560",
    "end": "3683420"
  },
  {
    "text": "it currently the bottleneck is on what's the RDS team it's an I think it I don't",
    "start": "3685100",
    "end": "3691620"
  },
  {
    "text": "know if it's technically GA yet but it does not work for sequel server I believe it's only my sequel now but I'm",
    "start": "3691620",
    "end": "3698190"
  },
  {
    "text": "sure RDS will be rolling out support for more in the future",
    "start": "3698190",
    "end": "3702440"
  },
  {
    "text": "steaming client caching or like server-side caching so client caching",
    "start": "3709060",
    "end": "3715760"
  },
  {
    "text": "works through basically the client frameworks understand the schema they",
    "start": "3715760",
    "end": "3721730"
  },
  {
    "text": "build a normalized cache based off of the queries that you're writing so then you can kind of think is like the key of",
    "start": "3721730",
    "end": "3727340"
  },
  {
    "text": "the cache is gonna be like the the value or the ast of the query that you're particularly asked for and then the",
    "start": "3727340",
    "end": "3732950"
  },
  {
    "text": "arguments of that query are gonna be taken into account and it's basically a flat map that then gets kind of stitched",
    "start": "3732950",
    "end": "3738170"
  },
  {
    "text": "together as it builds clot server-side caching there's no first-class support",
    "start": "3738170",
    "end": "3743390"
  },
  {
    "text": "for what you'd consider like a where you'd put like Redis or something in between a database and a API but there",
    "start": "3743390",
    "end": "3749270"
  },
  {
    "text": "are per request cache in the data loader pattern so that's a that's something I could talk about more if you're",
    "start": "3749270",
    "end": "3754430"
  },
  {
    "text": "interested",
    "start": "3754430",
    "end": "3756460"
  },
  {
    "text": "so it's mqtt over web sockets so MQTT is just the the implementation of the",
    "start": "3766659",
    "end": "3772789"
  },
  {
    "text": "protocol but then it's actually communicating over WebSockets yeah so",
    "start": "3772789",
    "end": "3788199"
  },
  {
    "text": "well I mean the way that yeah the way it works is the mutation would write to the Delta table and then the Delta query",
    "start": "3788199",
    "end": "3795589"
  },
  {
    "text": "would read the Delta table so I guess I kind of work together it's yeah so I",
    "start": "3795589",
    "end": "3834249"
  },
  {
    "text": "don't know if I understand question so we don't host the elasticsearch for you yeah so it's all stuff that's AB sink",
    "start": "3834249",
    "end": "3850179"
  },
  {
    "text": "[Music] right so you're right catch it ya know",
    "start": "3860430",
    "end": "3868109"
  },
  {
    "text": "so I mean I think you're talking about the limit on DynamoDB indexes where there is a limit of five and that's",
    "start": "3868109",
    "end": "3873540"
  },
  {
    "text": "something that we just can't break it's the way that it's built up so basically if you're gonna use dynamodb you're kind",
    "start": "3873540",
    "end": "3880020"
  },
  {
    "text": "of given the promise that you're gonna be able to efficiently query by five",
    "start": "3880020",
    "end": "3885030"
  },
  {
    "text": "hash keys you can configure those hash keys differently such that you can have different secondary sort keys because",
    "start": "3885030",
    "end": "3891720"
  },
  {
    "text": "you can have different LSI's as well so as long as if you share a hash key you could have more than five indexes",
    "start": "3891720",
    "end": "3896790"
  },
  {
    "text": "because you can have more LSI's but which is a local secondary index and then but you're right I mean at a",
    "start": "3896790",
    "end": "3902250"
  },
  {
    "text": "certain point the like if you need to be able to query efficiently by a hundred different fields you're probably going",
    "start": "3902250",
    "end": "3907890"
  },
  {
    "text": "to be needing to look into an elastic search or looking into a sequel in order to have our try indexes so when I say",
    "start": "3907890",
    "end": "3917369"
  },
  {
    "text": "that we automatically create them that's specifically referring to amplify right",
    "start": "3917369",
    "end": "3923579"
  },
  {
    "text": "right yeah so yeah yeah so so you're right right now when we do the",
    "start": "3923579",
    "end": "3928859"
  },
  {
    "text": "connections connections are implemented as GSIS and that would be a limit but there are actually other ways in order to create",
    "start": "3928859",
    "end": "3935930"
  },
  {
    "text": "like the concept of a connection one example would be like an associative map",
    "start": "3935930",
    "end": "3941220"
  },
  {
    "text": "so then you can actually store values in the same table with one GSI and then use",
    "start": "3941220",
    "end": "3946260"
  },
  {
    "text": "that single GSI in order to find all the associated records to do many-to-many so that you can get around it it's not",
    "start": "3946260",
    "end": "3952349"
  },
  {
    "text": "implemented yet exactly exactly so we're we're looking at it and we're thinking",
    "start": "3952349",
    "end": "3959040"
  },
  {
    "text": "the downside so that you lose some of the filtering abilities when you when you don't do it that way but exactly",
    "start": "3959040",
    "end": "3966299"
  },
  {
    "text": "it'll likely be either a new directive or a way to parameterize the connection directive to say I don't want this to be",
    "start": "3966299",
    "end": "3972240"
  },
  {
    "text": "a GSA I want this to be a map yeah and then we get around it",
    "start": "3972240",
    "end": "3978109"
  },
  {
    "text": "sure thanks everyone [Applause]",
    "start": "3978980",
    "end": "3986800"
  }
]