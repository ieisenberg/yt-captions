[
  {
    "start": "0",
    "end": "71000"
  },
  {
    "text": "hello everyone and welcome to this webinar about building a modern data architecture on AWS my name is Russell",
    "start": "30",
    "end": "7109"
  },
  {
    "text": "Nash on the Solutions Architect with AWS and I spend my time talking about all things big data and analytics so I'm",
    "start": "7109",
    "end": "13799"
  },
  {
    "text": "excited to spend the next little while with you talking about the modern data architecture now just a quick logistics",
    "start": "13799",
    "end": "19710"
  },
  {
    "text": "point if you do have any questions as we go through the webinar please use the chat window and post your question in",
    "start": "19710",
    "end": "26430"
  },
  {
    "text": "there now I won't have time to answer those questions in this hour but what",
    "start": "26430",
    "end": "31470"
  },
  {
    "text": "we'll do is we'll compile all of the questions and the answers and then provide that term that question to shake",
    "start": "31470",
    "end": "38340"
  },
  {
    "text": "back to all of you next week also with access to the recording as well so what",
    "start": "38340",
    "end": "45360"
  },
  {
    "text": "I want to do today is take you through the the modern-day data architecture as we see it and firstly talk about some of",
    "start": "45360",
    "end": "51840"
  },
  {
    "text": "the aspects that you want to see in your in your data architecture and then look",
    "start": "51840",
    "end": "57149"
  },
  {
    "text": "at four of the most common patterns and use cases that we see our customers building and talk through those in a",
    "start": "57149",
    "end": "64439"
  },
  {
    "text": "little bit of detail and kind of map them to to this architecture and you can see how it all hangs together so firstly",
    "start": "64439",
    "end": "72270"
  },
  {
    "start": "71000",
    "end": "535000"
  },
  {
    "text": "let's look at some of the facets that you want in your day or architecture so the first one is scalability so one of",
    "start": "72270",
    "end": "78780"
  },
  {
    "text": "the things that comes up a lot in my discussions with customers is that they are running out of room in their",
    "start": "78780",
    "end": "85939"
  },
  {
    "text": "database for example or they're running out of the ability to support more users or they're not getting the performance",
    "start": "85939",
    "end": "93210"
  },
  {
    "text": "out of it that they need and so they're really looking for scalability at different points in the architecture as",
    "start": "93210",
    "end": "99930"
  },
  {
    "text": "well so and that ability to be able to scale the components independently of one another is an absolutely key aspect",
    "start": "99930",
    "end": "106860"
  },
  {
    "text": "of the modern data architecture and we'll look at that as we as we go through the second is flexibility so one",
    "start": "106860",
    "end": "114600"
  },
  {
    "text": "of the the key things that's going on in the in the big data analytics space at the moment is just constant innovation",
    "start": "114600",
    "end": "120780"
  },
  {
    "text": "so there's constantly new ways of doing things being made available tweaks and",
    "start": "120780",
    "end": "126899"
  },
  {
    "text": "changes to existing open source services for example and products and so you",
    "start": "126899",
    "end": "131940"
  },
  {
    "text": "don't want to get locked into a particular way of doing things you want to be able to take advantage of these fantastic innovation",
    "start": "131940",
    "end": "140520"
  },
  {
    "text": "that's going on in the industry at the moment so that flexibility is absolutely key as well the third thing is that it",
    "start": "140520",
    "end": "148320"
  },
  {
    "text": "needs to be manageable so one of the things that we'll talk a lot about as we go through this session is how do you",
    "start": "148320",
    "end": "155640"
  },
  {
    "text": "manage a lot of these components and you'll notice that when I start talking about some of the AWS services we talked",
    "start": "155640",
    "end": "161700"
  },
  {
    "text": "a lot about undifferentiated heavy lifting and this is really work that you",
    "start": "161700",
    "end": "167220"
  },
  {
    "text": "have to do in order to to have the environment up and running but it doesn't really differentiate you in",
    "start": "167220",
    "end": "172410"
  },
  {
    "text": "whatever market you're in it doesn't really doesn't really allow you to to differentiate against your competition",
    "start": "172410",
    "end": "177750"
  },
  {
    "text": "so that's the piece that we'd like to take off your plate and move it on to",
    "start": "177750",
    "end": "182820"
  },
  {
    "text": "our so that we will do that that heavy lifting for you which allows you to focus on things that are going to have a",
    "start": "182820",
    "end": "187860"
  },
  {
    "text": "much bigger impact on your organizational in your business the",
    "start": "187860",
    "end": "193530"
  },
  {
    "text": "fourth aspect that's really important is that it needs to be cost effective so one of the things again that you'll see",
    "start": "193530",
    "end": "199890"
  },
  {
    "text": "we talk about is that you really want to make sure that the costs that you're incurring for your data architecture map",
    "start": "199890",
    "end": "206190"
  },
  {
    "text": "very closely to your usage of that architecture and so we're very much in",
    "start": "206190",
    "end": "211380"
  },
  {
    "text": "favor of a pay-as-you-go type of model where you simply pay for the storage and the compute and the services that you",
    "start": "211380",
    "end": "217440"
  },
  {
    "text": "use as opposed to having to pay upfront for something that you'll then grow into",
    "start": "217440",
    "end": "223590"
  },
  {
    "text": "that might take you two years or three years to actually fully use we think",
    "start": "223590",
    "end": "229290"
  },
  {
    "text": "that it's a much better option to be able to pay as you go and that leaves you with a much more cost-effective",
    "start": "229290",
    "end": "235130"
  },
  {
    "text": "architecture so we'll talk about a bit bit about that as we go through as well",
    "start": "235130",
    "end": "240320"
  },
  {
    "text": "so what we're going to do is we're going to look at this architecture and through a bit of a framework and that frameworks",
    "start": "240530",
    "end": "246810"
  },
  {
    "text": "going to start with the sources of data that you have and then what are the",
    "start": "246810",
    "end": "252060"
  },
  {
    "text": "options for ingesting those into your environment and then we're going to look at two separate",
    "start": "252060",
    "end": "258209"
  },
  {
    "text": "layers so the first is is called the sky layer or often called the batch layer and this is where you'll do a lot of the",
    "start": "258209",
    "end": "264840"
  },
  {
    "text": "batch process that you need to do so as files come in maybe they come in once a day or maybe",
    "start": "264840",
    "end": "270960"
  },
  {
    "text": "they come in once an hour but it's characterized by doing things in a you know kind of batch type way so I'll talk",
    "start": "270960",
    "end": "276570"
  },
  {
    "text": "through the best way to do that but then a lot of organizations are increasingly moving some of their ingestion pipeline",
    "start": "276570",
    "end": "284340"
  },
  {
    "text": "to more of a near real-time type of approach so there's data sources that",
    "start": "284340",
    "end": "289760"
  },
  {
    "text": "doesn't come at human batch it actually comes at you in in near real-time and your ability to deal with that is really",
    "start": "289760",
    "end": "296700"
  },
  {
    "text": "key because if you can if you can respond to those events in near real-time then that potentially gives",
    "start": "296700",
    "end": "302430"
  },
  {
    "text": "you an enormous advantage in the marketplace in in interacting with your",
    "start": "302430",
    "end": "307620"
  },
  {
    "text": "customers or dealing with events as they come up now then we move into the",
    "start": "307620",
    "end": "314100"
  },
  {
    "text": "serving layer so the serving layer is really the the key area where you then",
    "start": "314100",
    "end": "319320"
  },
  {
    "text": "allow your consumers your users or potentially other downstream systems to",
    "start": "319320",
    "end": "325920"
  },
  {
    "text": "then actually start to to analyze and make sense of this data and so we'll talk through and that's serving layer as",
    "start": "325920",
    "end": "332490"
  },
  {
    "text": "well and then in terms of the actual constituents now obviously there's a huge number of different different roles",
    "start": "332490",
    "end": "339150"
  },
  {
    "text": "and different people that are interested in doing analytics on this data but the key roles that we see obviously the data",
    "start": "339150",
    "end": "346770"
  },
  {
    "text": "scientists who are very interested to run all sorts of interesting algorithms across the data sets that you have you",
    "start": "346770",
    "end": "352890"
  },
  {
    "text": "then have the data analysts who are often using things like SQL and other languages to to interrogate the data",
    "start": "352890",
    "end": "359970"
  },
  {
    "text": "there's obviously business users as well who are typically using some kind of business intelligence tool to get to get",
    "start": "359970",
    "end": "367890"
  },
  {
    "text": "analysis done and then you have other systems as well so you might have an engagement platform so this really",
    "start": "367890",
    "end": "373110"
  },
  {
    "text": "relates to how do you then engage with your with your customers whether it's to make them a",
    "start": "373110",
    "end": "378390"
  },
  {
    "text": "recommendation or potentially a a next best offer etc and that all might come",
    "start": "378390",
    "end": "383850"
  },
  {
    "text": "out of the analytics that you do within your architecture and so the ability to then do that analysis and then take",
    "start": "383850",
    "end": "389280"
  },
  {
    "text": "action on that is absolutely key and then obviously in that vein you've also",
    "start": "389280",
    "end": "394770"
  },
  {
    "text": "got automation and events as well so it might be that based on something you see within the architecture some",
    "start": "394770",
    "end": "401279"
  },
  {
    "text": "analysis that you've done you then actually want to automate some kind of downstream step or take advantage of",
    "start": "401279",
    "end": "406709"
  },
  {
    "text": "some kind of event so we're gonna talk",
    "start": "406709",
    "end": "411929"
  },
  {
    "text": "as we go through this architecture now obviously I'm going to talk a lot about the the Amazon services in this space",
    "start": "411929",
    "end": "418049"
  },
  {
    "text": "but I don't want you to think that it's an all-or-nothing approach to each of these that it must be an Amazon service",
    "start": "418049",
    "end": "424349"
  },
  {
    "text": "one of the things that we see is we see a lot of environments where some of the services are Amazon many services some",
    "start": "424349",
    "end": "431939"
  },
  {
    "text": "of them are obviously a partner tools or open-source products as well so I'm",
    "start": "431939",
    "end": "439139"
  },
  {
    "text": "gonna focus on the Amazon stuff but I want you to keep in mind each of these components it's very much very much a",
    "start": "439139",
    "end": "445919"
  },
  {
    "text": "mix-and-match type of approach where if that particular service doesn't quite work for you then you can certainly spin",
    "start": "445919",
    "end": "453029"
  },
  {
    "text": "up a virtual instance and install on there whatever you like now wrapped",
    "start": "453029",
    "end": "459329"
  },
  {
    "text": "around this are some other really key elements as well so obviously security we talked about being job zero for us to",
    "start": "459329",
    "end": "467459"
  },
  {
    "text": "make sure that that's the first thing we always think about whenever we introduce a service and we encourage customers to",
    "start": "467459",
    "end": "474959"
  },
  {
    "text": "think that way as well so we have something called AWS I am which is our Identity and Access",
    "start": "474959",
    "end": "480269"
  },
  {
    "text": "Management Service so this allows you to control who can do what within the environment there's also kms a kms is",
    "start": "480269",
    "end": "488399"
  },
  {
    "text": "the key management service that allows you to manage the encryption keys for",
    "start": "488399",
    "end": "495269"
  },
  {
    "text": "the for any encryption that you do and that obviously then makes it easy to to encrypt and decrypt so that you can keep",
    "start": "495269",
    "end": "501749"
  },
  {
    "text": "the data encrypted but then those services can use them as they need to",
    "start": "501749",
    "end": "507110"
  },
  {
    "text": "AWS cloud trail can then log all the API calls to each of your services as well",
    "start": "507110",
    "end": "512879"
  },
  {
    "text": "so you can keep track of that and then cloud Watchers will then give you more of a matrix view over what's going on in",
    "start": "512879",
    "end": "519899"
  },
  {
    "text": "your environment so again just talking to the manageability aspect of the of the data architecture to have a lot of that",
    "start": "519899",
    "end": "526589"
  },
  {
    "text": "plumbing already in place for you just makes the management of the whole thing",
    "start": "526589",
    "end": "532190"
  },
  {
    "text": "much much easier so that's the framework we're gonna we're gonna talk through",
    "start": "532190",
    "end": "537530"
  },
  {
    "start": "535000",
    "end": "673000"
  },
  {
    "text": "we're gonna look at four of the main patterns that we come across when we talk to customers so the first is",
    "start": "537530",
    "end": "542710"
  },
  {
    "text": "database analytics and the use case here is typically that every organization has",
    "start": "542710",
    "end": "548360"
  },
  {
    "text": "got databases spread throughout the throughout the enterprise and the the",
    "start": "548360",
    "end": "553730"
  },
  {
    "text": "organization and the use case is that typically people are running out of wolf in that particular database they're",
    "start": "553730",
    "end": "560570"
  },
  {
    "text": "running analytics but the queries are slow or they can't get access to all the",
    "start": "560570",
    "end": "565640"
  },
  {
    "text": "history that they'd like to or the data is is is aggregated it's summarized for",
    "start": "565640",
    "end": "571550"
  },
  {
    "text": "performance reasons so they want to get down to the atomic data so we'll look at that use case so how do you how do you deal",
    "start": "571550",
    "end": "577670"
  },
  {
    "text": "with that how do you give your users access to to more data faster then we'll",
    "start": "577670",
    "end": "584270"
  },
  {
    "text": "look at flat file processing or batch processing and this is obviously a key",
    "start": "584270",
    "end": "589400"
  },
  {
    "text": "component of many architectures and this these these files might come from internal systems or often they come from",
    "start": "589400",
    "end": "594890"
  },
  {
    "text": "external feeds how do you get them in there and how do you actually then process them and then we'll move on and",
    "start": "594890",
    "end": "601940"
  },
  {
    "text": "look at the the real-time pipeline or as my colleagues like to tell me that it's it's not real-time its near real-time so",
    "start": "601940",
    "end": "609350"
  },
  {
    "text": "we'll talk about a near real-time pipeline and then we'll kind of bring it all together and have a little bit of a",
    "start": "609350",
    "end": "615050"
  },
  {
    "text": "chat towards the end around around the data like what does the data Lake look like so let's start off with database",
    "start": "615050",
    "end": "623720"
  },
  {
    "text": "analytics so this is probably one of the most common use cases or often it's the first use case from for many customers",
    "start": "623720",
    "end": "630650"
  },
  {
    "text": "and because it's the often that's the most burning issue that they have",
    "start": "630650",
    "end": "635839"
  },
  {
    "text": "because they do have a lot of databases that are often running out steam so the",
    "start": "635839",
    "end": "641810"
  },
  {
    "text": "sources you'd expect is some kind of database typically it's a traditional row based database and it might be",
    "start": "641810",
    "end": "649520"
  },
  {
    "text": "running out of steam because maybe it's wasn't really designed for analytics in the first place but it's now being asked",
    "start": "649520",
    "end": "655550"
  },
  {
    "text": "to do that or maybe it's simply that it's being overloaded with the amount of data that's being put in there and the",
    "start": "655550",
    "end": "661070"
  },
  {
    "text": "types of queries that people are asking so one of the common patterns that we",
    "start": "661070",
    "end": "667740"
  },
  {
    "text": "see is customers using a service like Amazon redshift to to deal with this issue so let's have a little bit of a",
    "start": "667740",
    "end": "674399"
  },
  {
    "start": "673000",
    "end": "934000"
  },
  {
    "text": "dive into into red ship so redshift is an MPP signal database so NP P stands",
    "start": "674399",
    "end": "679439"
  },
  {
    "text": "for massively parallel processing and it's a breed of databases that have been",
    "start": "679439",
    "end": "684449"
  },
  {
    "text": "around for a few years and essentially they take a very different approach to running queries against the data in that",
    "start": "684449",
    "end": "691170"
  },
  {
    "text": "they spread the data very widely across a large number of nodes within a cluster",
    "start": "691170",
    "end": "696269"
  },
  {
    "text": "and that means that they are absolutely optimized for analytical queries so",
    "start": "696269",
    "end": "702240"
  },
  {
    "text": "typically an analytical workload is one where you scan a lot of data and so if",
    "start": "702240",
    "end": "709139"
  },
  {
    "text": "you've got that data spread across a lot of machines and then obviously you can you can churn through it much more quickly now back to the scalability type",
    "start": "709139",
    "end": "717329"
  },
  {
    "text": "aspect you can scale this from from a couple of hundred gigabytes through to",
    "start": "717329",
    "end": "722339"
  },
  {
    "text": "the petabyte range if you need to so you know that you've got that Headroom there to grow as you need to and it is fully",
    "start": "722339",
    "end": "731100"
  },
  {
    "text": "relational so sometimes there's a little bit of confusion around some of these new breeds of databases like like MPP",
    "start": "731100",
    "end": "738660"
  },
  {
    "text": "and column databases etc that they're not fully relational MPP databases like",
    "start": "738660",
    "end": "745199"
  },
  {
    "text": "redshift are fully relational so you create tables as you would normally you can typically use the same data model",
    "start": "745199",
    "end": "751110"
  },
  {
    "text": "that you use new existing database you create the tables you create joins between tables etc so often very easy to",
    "start": "751110",
    "end": "758459"
  },
  {
    "text": "migrate because you essentially pick up the data model that you have and move it across so just looking under the hood",
    "start": "758459",
    "end": "766110"
  },
  {
    "text": "into Richard a little bit so this is the the typical architecture we have a leader node at the front and this is the",
    "start": "766110",
    "end": "773040"
  },
  {
    "text": "the node that your your tools or your sequel clients will talk to and then you've got the compute nodes where the",
    "start": "773040",
    "end": "780779"
  },
  {
    "text": "data lives and where all of the compute power is and this is the layer that compute layer that you scale out",
    "start": "780779",
    "end": "785790"
  },
  {
    "text": "horizontally as you need to so typically you have a number of tables and what you",
    "start": "785790",
    "end": "792779"
  },
  {
    "text": "do is you then when you load those tables interative they get spread all of the nodes in the cluster so each",
    "start": "792779",
    "end": "800470"
  },
  {
    "text": "of those nodes has access to its slice of that table and we call this has",
    "start": "800470",
    "end": "806889"
  },
  {
    "text": "shared nothing architecture because the nodes are extremely selfish which normally we try and discourage but in",
    "start": "806889",
    "end": "812619"
  },
  {
    "text": "this particular instance it's actually a very positive thing because then they can there's no contention issues as you",
    "start": "812619",
    "end": "819069"
  },
  {
    "text": "scale out that layer those nodes are self-contained so they've got their own CPU their own memory and their own disk",
    "start": "819069",
    "end": "826149"
  },
  {
    "text": "and they're able to just run their queries independently about the nodes then obviously occasionally there's need",
    "start": "826149",
    "end": "831549"
  },
  {
    "text": "to share data and they can obviously then do that but again we do that at a parallel level so that tongue so that",
    "start": "831549",
    "end": "838599"
  },
  {
    "text": "you can maximize your your performance so what happens is a query comes in from",
    "start": "838599",
    "end": "844269"
  },
  {
    "text": "the from the tool that SQL then gets pushed out to every single node so the",
    "start": "844269",
    "end": "849729"
  },
  {
    "text": "same query goes to every node and each of those nodes runs that query against",
    "start": "849729",
    "end": "855249"
  },
  {
    "text": "its data slice gets the intermediate result sets back and then pushes a",
    "start": "855249",
    "end": "860589"
  },
  {
    "text": "results back back back to the host there's lots of other smarts in there as well to try and speed up your queries",
    "start": "860589",
    "end": "866619"
  },
  {
    "text": "but that's one of the main reasons that databases like red shift do scale is because of that ability to spread the",
    "start": "866619",
    "end": "873069"
  },
  {
    "text": "data very widely in terms of scalability",
    "start": "873069",
    "end": "878769"
  },
  {
    "text": "as I talked about it going from from gigabytes to petabytes so you can start off with a single node if you wanted to",
    "start": "878769",
    "end": "884339"
  },
  {
    "text": "just dip your toe in the water and have a bit of a feel for how it works and so that's 160 gig of disk but then as I",
    "start": "884339",
    "end": "891069"
  },
  {
    "text": "said you can then grow that cluster up into the two petabyte range if you need",
    "start": "891069",
    "end": "896169"
  },
  {
    "text": "to so that's all good we've got we've",
    "start": "896169",
    "end": "904119"
  },
  {
    "text": "got redshift there the big question now is how do we get data from the source database into redshift so the the ETL",
    "start": "904119",
    "end": "911169"
  },
  {
    "text": "market has been around for a very long time so ETL stands for extract transform",
    "start": "911169",
    "end": "917049"
  },
  {
    "text": "and load and this is a very mature market with a lot of excellent tools out there me of which a fantastic partners",
    "start": "917049",
    "end": "923439"
  },
  {
    "text": "of ours to help you to get data out of those source systems and into redshift and basically they will interrogate that",
    "start": "923439",
    "end": "930639"
  },
  {
    "text": "so database and just push those changes straight across so just delving into",
    "start": "930639",
    "end": "935679"
  },
  {
    "start": "934000",
    "end": "1084000"
  },
  {
    "text": "that a little bit more so we've got as I said many partners in this space so if you if you actually into your favorite",
    "start": "935679",
    "end": "941889"
  },
  {
    "text": "search engine if you put in redshift data integration partners that'll lead you to the to the page on the website",
    "start": "941889",
    "end": "948939"
  },
  {
    "text": "that lists all of the all of the tools that work well with redshift we also have our own tool called the AWS",
    "start": "948939",
    "end": "954519"
  },
  {
    "text": "database migration service as well and what a lot of these tools do is to",
    "start": "954519",
    "end": "959829"
  },
  {
    "text": "reduce the burden on that source database because often that source database does still have a day job at",
    "start": "959829",
    "end": "965139"
  },
  {
    "text": "Silla is still potentially is supporting an application is that they'll actually look at the transaction log of that",
    "start": "965139",
    "end": "971859"
  },
  {
    "text": "source database and in reading the log they'll pick up all the changes that have happened to to the tables that",
    "start": "971859",
    "end": "977739"
  },
  {
    "text": "you're interested in without actually having to run a query against the database itself so designed to be quite",
    "start": "977739",
    "end": "983409"
  },
  {
    "text": "a lightweight process they pull the changes out and then they push those across to to the tiger in this case and",
    "start": "983409",
    "end": "989829"
  },
  {
    "text": "was on Richard now there's another slight variation on this which is called alt so alt is where",
    "start": "989829",
    "end": "999219"
  },
  {
    "text": "you extract from the source then you load it into your target database and then you do the transformation so that",
    "start": "999219",
    "end": "1006839"
  },
  {
    "text": "looks a bit like this so you would extract from source load it into to redshift do the transformation in",
    "start": "1006839",
    "end": "1013439"
  },
  {
    "text": "database and then move that to another table so even though I'm showing this is",
    "start": "1013439",
    "end": "1019769"
  },
  {
    "text": "two different two different clusters this typically happens within the same cluster within the same database and you're just moving data from one table",
    "start": "1019769",
    "end": "1025740"
  },
  {
    "text": "to another table but the advantage of doing the elt approach is that you're",
    "start": "1025740",
    "end": "1032100"
  },
  {
    "text": "taking advantage of the power of the rich of cluster so as I said before it's",
    "start": "1032100",
    "end": "1037230"
  },
  {
    "text": "got a lot of processing power in there and especially for customers who are bumping up against that that batch",
    "start": "1037230",
    "end": "1043619"
  },
  {
    "text": "window issue so often I talk to customers who are having issues where they're essentially running out of night",
    "start": "1043619",
    "end": "1049559"
  },
  {
    "text": "time because the amount of data that they're having to process overnight is getting bigger and bigger and they have",
    "start": "1049559",
    "end": "1055559"
  },
  {
    "text": "an SLA where they obviously they want to allow their users to start accessing the data as soon as they come in in the",
    "start": "1055559",
    "end": "1060840"
  },
  {
    "text": "morning but that batch process is taking long where longer so off we said an ELT approach can help with",
    "start": "1060840",
    "end": "1067510"
  },
  {
    "text": "that because then you're using the power of the database it's got a massive number of CPUs potentially in there to",
    "start": "1067510",
    "end": "1073510"
  },
  {
    "text": "help you to crunch through that much more quickly ELT doesn't work for every type of transformation but um but it's",
    "start": "1073510",
    "end": "1080530"
  },
  {
    "text": "definitely worth worth taking a look at so somebody who's who's done this for",
    "start": "1080530",
    "end": "1088030"
  },
  {
    "start": "1084000",
    "end": "1302000"
  },
  {
    "text": "effectively is Boingo Wireless and they obviously collect a lot of data they",
    "start": "1088030",
    "end": "1093130"
  },
  {
    "text": "moved from from a traditional row based database across to redshift and they",
    "start": "1093130",
    "end": "1099640"
  },
  {
    "text": "found that they had queries that were taking 45 minutes on their existing system that we're now running in in 20",
    "start": "1099640",
    "end": "1106150"
  },
  {
    "text": "25 seconds on redshift and they found that their loads which used to take a",
    "start": "1106150",
    "end": "1111580"
  },
  {
    "text": "couple of hours we're now taking tens of seconds so they had a 1 million row load",
    "start": "1111580",
    "end": "1117640"
  },
  {
    "text": "that took about 25 seconds to load up into redshift so um that's the type of",
    "start": "1117640",
    "end": "1122650"
  },
  {
    "text": "performance gain that you can you can see when you move to to a database that's dedicated for for that particular",
    "start": "1122650",
    "end": "1129340"
  },
  {
    "text": "process if you're interested in that use case so have a look for Boingo and",
    "start": "1129340",
    "end": "1134799"
  },
  {
    "text": "redshift and it'll take you to the case study and they also did a a presentation",
    "start": "1134799",
    "end": "1140309"
  },
  {
    "text": "on that at reinvent a couple years ago which was which is excellent too so",
    "start": "1140309",
    "end": "1147970"
  },
  {
    "text": "let's have a look at flat file processing and this is more of the more of the batch approach so let's say we",
    "start": "1147970",
    "end": "1153789"
  },
  {
    "text": "have some flat files that have come from somewhere are there internally or externally one of the fantastic places",
    "start": "1153789",
    "end": "1161289"
  },
  {
    "text": "to put that flat file is into Amazon s3 so s3 if you're not familiar is the the",
    "start": "1161289",
    "end": "1167200"
  },
  {
    "text": "simple storage service and it's an object store that is extremely low cost",
    "start": "1167200",
    "end": "1173980"
  },
  {
    "text": "so it if you look at the cost of it per gigabyte per month it's extremely low",
    "start": "1173980",
    "end": "1180030"
  },
  {
    "text": "it's very very scalable so again back to the the scalability question you don't",
    "start": "1180030",
    "end": "1185860"
  },
  {
    "text": "need to pre provision Amazon s3 you simply load data into it as you need it and it's got 11 lines of curability",
    "start": "1185860",
    "end": "1194470"
  },
  {
    "text": "which means that every object that you give to us that you want to put into s3 will",
    "start": "1194470",
    "end": "1199780"
  },
  {
    "text": "actually write that to multiple physical locations within a region so if you take",
    "start": "1199780",
    "end": "1205600"
  },
  {
    "text": "the Sydney region for example that's got multiple separate facilities within that region and will write that object to",
    "start": "1205600",
    "end": "1212260"
  },
  {
    "text": "multiple places to ensure that we can give you that that level of durability so the the great thing about that is",
    "start": "1212260",
    "end": "1219820"
  },
  {
    "text": "that then if you're using that as your basis to store data you don't need to then back it up again because basically",
    "start": "1219820",
    "end": "1225010"
  },
  {
    "text": "we've we've taken that that heavy lifting off your plate for you so how do",
    "start": "1225010",
    "end": "1233110"
  },
  {
    "text": "you get data into s3 so there's lots and lots of ways just a couple I wanted to mention so you can use open source log",
    "start": "1233110",
    "end": "1239980"
  },
  {
    "text": "aggregation or collecting type software like fluid D for example look for J",
    "start": "1239980",
    "end": "1245020"
  },
  {
    "text": "which is a logging application we've also obviously got a command-line",
    "start": "1245020",
    "end": "1251170"
  },
  {
    "text": "interface and our own SDK and there but there are many many other options as",
    "start": "1251170",
    "end": "1256540"
  },
  {
    "text": "well including if you find that you've got a lot of data that you need to move",
    "start": "1256540",
    "end": "1261700"
  },
  {
    "text": "especially historical data you can also use the AWS snowball so this is the appliance that we actually shipped to",
    "start": "1261700",
    "end": "1268630"
  },
  {
    "text": "you you load your data onto it ship it back to us and then we'll actually load that into to s3 for you so you can move",
    "start": "1268630",
    "end": "1275950"
  },
  {
    "text": "an enormous amount of data and very quickly using that kind of approach so",
    "start": "1275950",
    "end": "1281260"
  },
  {
    "text": "let's say we do that we we've got data in s3 and now we want to do some kind of transformation on it so this is where we",
    "start": "1281260",
    "end": "1288280"
  },
  {
    "text": "see technology like Hadoop really coming into its own and I thought it might be worth just giving you a quick potted",
    "start": "1288280",
    "end": "1294640"
  },
  {
    "text": "history of Hadoop because that history I think then really kind of talks to to",
    "start": "1294640",
    "end": "1300370"
  },
  {
    "text": "some of the stuff we want to want to mention later on so if you look at how we did things in in the good old days",
    "start": "1300370",
    "end": "1306330"
  },
  {
    "start": "1302000",
    "end": "1611000"
  },
  {
    "text": "before Hadoop it's almost difficult to imagine a world without Hadoop but there",
    "start": "1306330",
    "end": "1311440"
  },
  {
    "text": "was there was a time where we we didn't have the elephant in our lives and if we wanted to do processing on data we would",
    "start": "1311440",
    "end": "1317920"
  },
  {
    "text": "load it up onto a machine and then that machine would have a certain amount of compute power and then obviously we would we would churn through our churn",
    "start": "1317920",
    "end": "1324580"
  },
  {
    "text": "through our job and if we wanted to make it go faster we would then get a bigger machine with more compute power which",
    "start": "1324580",
    "end": "1330520"
  },
  {
    "text": "would allow us to order faster or load more data up but as as",
    "start": "1330520",
    "end": "1336440"
  },
  {
    "text": "Grace Hopper famously said in the pioneer days they used oxen for heavy",
    "start": "1336440",
    "end": "1341449"
  },
  {
    "text": "pulling when one ox couldn't budge a log they didn't try to grow a bigger ox now",
    "start": "1341449",
    "end": "1347239"
  },
  {
    "text": "she was ahead of her time here she was she talked about this in the 1960s but she was referring directly using",
    "start": "1347239",
    "end": "1353479"
  },
  {
    "text": "clusters of machines to distribute the load and if you're not familiar with Grace Hopper look her up online she's",
    "start": "1353479",
    "end": "1360579"
  },
  {
    "text": "really really interesting and obviously had quite a heavy influence on i.t so",
    "start": "1360579",
    "end": "1368059"
  },
  {
    "text": "what was she was suggesting and obviously what what then happened was that you could if you could distribute",
    "start": "1368059",
    "end": "1373219"
  },
  {
    "text": "your processing across a large number of machines then suddenly you you you can",
    "start": "1373219",
    "end": "1378379"
  },
  {
    "text": "go much much wider with your processing the problem though is that you need something that's going to manage that",
    "start": "1378379",
    "end": "1383449"
  },
  {
    "text": "because you need to know where bits of data are because they could potentially be on different machines you need",
    "start": "1383449",
    "end": "1389899"
  },
  {
    "text": "something to coordinate the processing between each of the machines and so this is really where where Hadoop came into",
    "start": "1389899",
    "end": "1397969"
  },
  {
    "text": "its own as a framework for managing that these kind of large distributed clusters",
    "start": "1397969",
    "end": "1404379"
  },
  {
    "text": "so if you look at the the Hadoop stack so you start off with some kind of infrastructure now if this is on premise",
    "start": "1404379",
    "end": "1411019"
  },
  {
    "text": "obviously there's going to be physical machines in the cloud there's will be virtual machines HDFS is the data layer",
    "start": "1411019",
    "end": "1416599"
  },
  {
    "text": "so this is Hadoop distributed file system so this keeps track of where all the bits of data are in the cluster map",
    "start": "1416599",
    "end": "1423649"
  },
  {
    "text": "produce was the original process layer that was designed to work across HDFS a",
    "start": "1423649",
    "end": "1428989"
  },
  {
    "text": "Duke was the framework and this kind of served us well for a small amount of",
    "start": "1428989",
    "end": "1434089"
  },
  {
    "text": "time but then IT professionals being the impatient Bunch that they are said well we can make this",
    "start": "1434089",
    "end": "1440899"
  },
  {
    "text": "better we're finding map producers a little bit clunky and hard to work with we'd prefer",
    "start": "1440899",
    "end": "1446359"
  },
  {
    "text": "a kind of a higher-level language and so applications like higher than Pig were",
    "start": "1446359",
    "end": "1451819"
  },
  {
    "text": "born so hive was one still is a sequel",
    "start": "1451819",
    "end": "1457129"
  },
  {
    "text": "like language that then then will then translate your kind of sequel commands into into MapReduce for you so you're",
    "start": "1457129",
    "end": "1463309"
  },
  {
    "text": "not having to deal down at the my produce level and similarly Peters got its own higher level language for",
    "start": "1463309",
    "end": "1469520"
  },
  {
    "text": "manipulation and that was great for a while and then somebody said look",
    "start": "1469520",
    "end": "1475610"
  },
  {
    "text": "MapReduce is getting a little bit slow and clunky we'd like to speak that up and so we saw a couple of new entrants",
    "start": "1475610",
    "end": "1482000"
  },
  {
    "text": "into the process layer namely tears and spark and these guys are really really",
    "start": "1482000",
    "end": "1487040"
  },
  {
    "text": "taken over from MapReduce now a rapid uses you're they've kind of considered to be kind of the old world if you like",
    "start": "1487040",
    "end": "1494080"
  },
  {
    "text": "and along with that then sprung up a massive number of applications as well",
    "start": "1494080",
    "end": "1499780"
  },
  {
    "text": "so we saw things like presto for example again designed to give very seek very fast signal access to data HBase for no",
    "start": "1499780",
    "end": "1507680"
  },
  {
    "text": "sequel applications and then spark itself comes with a number of different modules so there's a sequel module",
    "start": "1507680",
    "end": "1514190"
  },
  {
    "text": "there's a streaming module etc there's an our module graph database etc really",
    "start": "1514190",
    "end": "1521780"
  },
  {
    "text": "kind of filling out that that ecosystem within the Hadoop framework so that's",
    "start": "1521780",
    "end": "1527030"
  },
  {
    "text": "all well and good and so obviously you can do this with the Amazon you those machines at the bottom you can spin them",
    "start": "1527030",
    "end": "1533330"
  },
  {
    "text": "up as ec2 machine you can install your hadoop distribution of choice and then install your applications on top but we",
    "start": "1533330",
    "end": "1540410"
  },
  {
    "text": "saw a lot of customers doing this and we thought wouldn't it be nice if there was a managed service that took away some of",
    "start": "1540410",
    "end": "1546650"
  },
  {
    "text": "that some of that heavy lifting so we introduced Amazon EMR and this is a managed to dupe service so what this",
    "start": "1546650",
    "end": "1553970"
  },
  {
    "text": "does is it provisions the machines for you it will then install I do it'll make",
    "start": "1553970",
    "end": "1560210"
  },
  {
    "text": "sure the nodes are talking to each other it'll install the applications that you want so that you don't have to do that",
    "start": "1560210",
    "end": "1565640"
  },
  {
    "text": "you can basically just focus on actually using the applications and doing",
    "start": "1565640",
    "end": "1570890"
  },
  {
    "text": "something interesting with them now EMR introduces another really important element which is something called EMR FS",
    "start": "1570890",
    "end": "1577940"
  },
  {
    "text": "what this does is this allows you to use s3 as your data layer and this has a",
    "start": "1577940",
    "end": "1584630"
  },
  {
    "text": "couple of really important benefits the primary one being that you can now decouple your storage layer from your",
    "start": "1584630",
    "end": "1591620"
  },
  {
    "text": "computer with your Hadoop clusters so what that means is that you can just load that into s3 as as you",
    "start": "1591620",
    "end": "1597840"
  },
  {
    "text": "- without necessarily having to scale up your Hadoop clusters you can then bring",
    "start": "1597840",
    "end": "1605130"
  },
  {
    "text": "up the Hadoop clusters that are much more closely matched to the type of processing that you that you want to do",
    "start": "1605130",
    "end": "1611480"
  },
  {
    "start": "1611000",
    "end": "1757000"
  },
  {
    "text": "so let me give you an example of that so within the the ec2 family of virtual",
    "start": "1611480",
    "end": "1617250"
  },
  {
    "text": "machines we have a number of different families that have different characteristics depending on what you want to do with them so there's this",
    "start": "1617250",
    "end": "1623730"
  },
  {
    "text": "general purpose there's ones that are more compute heavy ones that have got more memory etc and if you look at the",
    "start": "1623730",
    "end": "1629610"
  },
  {
    "text": "types of big data analysis that people typically do they can lend themselves over a specific configuration so batch",
    "start": "1629610",
    "end": "1637020"
  },
  {
    "text": "processes obviously aren't too fussy but things like machine learning really loves CPU and so if you can have a much",
    "start": "1637020",
    "end": "1643530"
  },
  {
    "text": "more CPU intense type of instance that's can obviously assist that that that",
    "start": "1643530",
    "end": "1649470"
  },
  {
    "text": "machine learning workload similarly with a lot of the interactive analysis spark and presto etc they love memory if you",
    "start": "1649470",
    "end": "1655800"
  },
  {
    "text": "can provide a distance with more memory it's obviously going to be a lot more efficient so that ability of EMR to",
    "start": "1655800",
    "end": "1661170"
  },
  {
    "text": "allow you to then spin up clusters separate to the storage layer allows you",
    "start": "1661170",
    "end": "1666480"
  },
  {
    "text": "to bring up clusters that are much more closely matched to the type of workload",
    "start": "1666480",
    "end": "1671690"
  },
  {
    "text": "the other really interesting thing that you can now do if you see if you've",
    "start": "1671900",
    "end": "1677190"
  },
  {
    "text": "decoupled your compute from your storage is you can take advantage of this relationship between costs and time in",
    "start": "1677190",
    "end": "1683490"
  },
  {
    "text": "the cloud so the way that you pay for your compute in the cloud is essentially",
    "start": "1683490",
    "end": "1688560"
  },
  {
    "text": "the number of CPUs that you are using and the time that you're using them so let's say you have a job that you throw",
    "start": "1688560",
    "end": "1694680"
  },
  {
    "text": "one CPU at it and it takes ten hours to run you're going to pay for that one CPU",
    "start": "1694680",
    "end": "1700140"
  },
  {
    "text": "for ten hours but let's say that that particular job is one that lends itself to be distributed across a number of",
    "start": "1700140",
    "end": "1706170"
  },
  {
    "text": "CPUs if you then throw ten CPUs at it it then might take an hour but the key",
    "start": "1706170",
    "end": "1715680"
  },
  {
    "text": "thing is you're still paying for exactly the same amount because you know you're paying for ten times the number of CPUs but you're only paying for them for a",
    "start": "1715680",
    "end": "1721830"
  },
  {
    "text": "tenth of the time so the cost is exactly the same but the impact to you is that",
    "start": "1721830",
    "end": "1726900"
  },
  {
    "text": "you're now obviously finishing your job much much sooner and so this is one of the key aspects of",
    "start": "1726900",
    "end": "1732770"
  },
  {
    "text": "the modern donor architecture is the scalability and the ability to scale",
    "start": "1732770",
    "end": "1738120"
  },
  {
    "text": "very widely but for a short period of time so you go wide you do what you need",
    "start": "1738120",
    "end": "1743760"
  },
  {
    "text": "to do and then you then you either scale back in or you shut that cluster down completely which is very difficult to do",
    "start": "1743760",
    "end": "1750650"
  },
  {
    "text": "obviously if you if you have to buy those machines then obviously you're not going to scale them out as widely as you",
    "start": "1750650",
    "end": "1756240"
  },
  {
    "text": "potentially could now the other aspect of this as well around cost is that you",
    "start": "1756240",
    "end": "1763230"
  },
  {
    "start": "1757000",
    "end": "1880000"
  },
  {
    "text": "can also then take advantage of the spot market so the spot market is any unused",
    "start": "1763230",
    "end": "1770330"
  },
  {
    "text": "ec2 capacity that we have to forgive an instance type will make available to the",
    "start": "1770330",
    "end": "1775650"
  },
  {
    "text": "spot market for customers to then bid on how much they'll pay for it now the the",
    "start": "1775650",
    "end": "1781049"
  },
  {
    "text": "spot price is obviously it's a fluid kind of price because the it's a fluid",
    "start": "1781049",
    "end": "1786150"
  },
  {
    "text": "market depending on supply and demand but typically the spot price is a",
    "start": "1786150",
    "end": "1791580"
  },
  {
    "text": "fraction of the on-demand price so to give you an example of that if you look at the the m3 to extra-large which is a",
    "start": "1791580",
    "end": "1797460"
  },
  {
    "text": "fairly kind of middle-of-the-road instance type in the city region the",
    "start": "1797460",
    "end": "1804419"
  },
  {
    "text": "on-demand price for that is 75 cents so the undermanned price is where you say I'm going to use this this instance and",
    "start": "1804419",
    "end": "1811740"
  },
  {
    "text": "basically you can then use it for as long as you want and then when you finish with it you can you can stop paying for it that'll cost you 75 cents",
    "start": "1811740",
    "end": "1818610"
  },
  {
    "text": "an hour the spot price as I said it does fluctuate but it's often around the 8",
    "start": "1818610",
    "end": "1824669"
  },
  {
    "text": "cents an hour mark so you can see that's a significant difference and that then potentially allows you to spin up a very",
    "start": "1824669",
    "end": "1832140"
  },
  {
    "text": "very large cluster and get your processing done even more quickly but but obviously save a lot of money on",
    "start": "1832140",
    "end": "1838380"
  },
  {
    "text": "that now I can hear you asking what sounds fantastic what and I run everything on",
    "start": "1838380",
    "end": "1843570"
  },
  {
    "text": "spot why would I ever pay the on-demand price well the caveat here is that if",
    "start": "1843570",
    "end": "1848640"
  },
  {
    "text": "the spot price goes up and it goes above what your maximum bid was then we may",
    "start": "1848640",
    "end": "1854370"
  },
  {
    "text": "take that instance away from you so you really need to use it for workloads that can tolerate the loss of",
    "start": "1854370",
    "end": "1861720"
  },
  {
    "text": "a node and we've bait we've baked that functionality into EMR to allow you to very easily take advantage of the spot",
    "start": "1861720",
    "end": "1868230"
  },
  {
    "text": "market spin up very very big clusters but not be impacted if if one or two of",
    "start": "1868230",
    "end": "1876420"
  },
  {
    "text": "those nodes then gets taken away from you so go back to our back to our",
    "start": "1876420",
    "end": "1885360"
  },
  {
    "start": "1880000",
    "end": "2070000"
  },
  {
    "text": "framework so you know you know scale our box here we've got data laning and s3",
    "start": "1885360",
    "end": "1892070"
  },
  {
    "text": "EMR or Hadoop doing some transformation putting the data back in s3 I also just wanted to quickly give you a glimpse",
    "start": "1892070",
    "end": "1898500"
  },
  {
    "text": "into a service that we've got coming out later this year which is aw it's glue so",
    "start": "1898500",
    "end": "1904700"
  },
  {
    "text": "again and if he heard me said that so there's a couple of times that we saw customers running very very similar",
    "start": "1904700",
    "end": "1910830"
  },
  {
    "text": "types of workloads and we saw this across across customers across industries etc and that was you know",
    "start": "1910830",
    "end": "1918180"
  },
  {
    "text": "using Hadoop and EMR to do transformation so when we see that often we look at that as an opportunity to",
    "start": "1918180",
    "end": "1924600"
  },
  {
    "text": "maybe introduce a managed service to take some of that heavy lifting off you so blue is designed to to be that kind",
    "start": "1924600",
    "end": "1931980"
  },
  {
    "text": "of transformation as a service piece so it's going to be a managed transform engine it's going to have a job",
    "start": "1931980",
    "end": "1939060"
  },
  {
    "text": "scheduler built in and also a data catalog so you can see where where dad has come from and where it's where it's",
    "start": "1939060",
    "end": "1945450"
  },
  {
    "text": "going and let's built on Apache spark so",
    "start": "1945450",
    "end": "1951600"
  },
  {
    "text": "just a quick screen shot so you can see basically that you're managing your jobs there and you can see how many rows have",
    "start": "1951600",
    "end": "1958140"
  },
  {
    "text": "been read written errors duration met cetera so you can keep track of of the",
    "start": "1958140",
    "end": "1963600"
  },
  {
    "text": "the transformations that are running in that environment so that's currently in preview so if you interested in having a",
    "start": "1963600",
    "end": "1969180"
  },
  {
    "text": "bit of a look at that sign up for the preview if you just look for aw ass glue",
    "start": "1969180",
    "end": "1974640"
  },
  {
    "text": "preview in your search engine it'll take you there and that previous currently in the US but you can get you get your your",
    "start": "1974640",
    "end": "1982710"
  },
  {
    "text": "hands on that habit of a play with it so so we've done a transformation in that",
    "start": "1982710",
    "end": "1988590"
  },
  {
    "text": "layer now in the serving layer how are we going to get this data to our to our consumers so we could load the data from",
    "start": "1988590",
    "end": "1995970"
  },
  {
    "text": "s3 to interrupt and that's quite common we could also then use a different EMR",
    "start": "1995970",
    "end": "2002760"
  },
  {
    "text": "cluster or Hadoop cluster to also access s3 so if you look at back to our stack a",
    "start": "2002760",
    "end": "2008910"
  },
  {
    "text": "lot of the applications that fit into Hadoop are really designed for interrogation and analysis of data so",
    "start": "2008910",
    "end": "2017280"
  },
  {
    "text": "spark R for example allows you to run run our against that data spikes equal hive etc and if you look at that one on",
    "start": "2017280",
    "end": "2024390"
  },
  {
    "text": "the top left presto came out of Facebook and it was really designed to sidestep a",
    "start": "2024390",
    "end": "2031500"
  },
  {
    "text": "lot of the inherent inefficiencies in the in the framework to really give",
    "start": "2031500",
    "end": "2038970"
  },
  {
    "text": "users very fast sequel access to data and HT FS and also in s3 so we saw this",
    "start": "2038970",
    "end": "2046440"
  },
  {
    "text": "became a common pattern with customers that were spinning up clusters to use presto to then access their s3 data and",
    "start": "2046440",
    "end": "2053300"
  },
  {
    "text": "so again we thought well maybe that lends itself to to a managed service so so that's what we did so he introduced",
    "start": "2053300",
    "end": "2060050"
  },
  {
    "text": "Amazon Athena which is built on presto so it's presto underneath and it gives",
    "start": "2060050",
    "end": "2066240"
  },
  {
    "text": "you that sequel access to your to your s3 data now it is service so basically",
    "start": "2066240",
    "end": "2073710"
  },
  {
    "start": "2070000",
    "end": "2125000"
  },
  {
    "text": "you don't have to actually provision or spin anything up you simply just run the query and we will then provide you the",
    "start": "2073710",
    "end": "2082139"
  },
  {
    "text": "cluster that will then access the data and give you the result set back and you",
    "start": "2082140",
    "end": "2087810"
  },
  {
    "text": "then pay that / you pay per query not per an opera or anything like that so",
    "start": "2087810",
    "end": "2094070"
  },
  {
    "text": "very simply how it works is you've got data in s3 and you have users who want to run sequel queries across that data",
    "start": "2094070",
    "end": "2101000"
  },
  {
    "text": "at the time that they run the query will then move that cluster into your account",
    "start": "2101000",
    "end": "2106290"
  },
  {
    "text": "you can run that query it will then interrogate s3 bring the result set back give that back to the the users and then",
    "start": "2106290",
    "end": "2114600"
  },
  {
    "text": "that cluster will disappear so as I said it's not you don't have to worry about",
    "start": "2114600",
    "end": "2119790"
  },
  {
    "text": "any infrastructure you're basically just running queries against your s3 data",
    "start": "2119790",
    "end": "2125930"
  },
  {
    "start": "2125000",
    "end": "2348000"
  },
  {
    "text": "I've introduced a couple of different options for running sequel queries and so that the question that often comes up",
    "start": "2126800",
    "end": "2132680"
  },
  {
    "text": "is well how are these all different and when and when should I use the different types of sequel engine and so I just",
    "start": "2132680",
    "end": "2139280"
  },
  {
    "text": "thought it might be worth just giving you a quick comparison of these so if you look at hive and spark sequel on the",
    "start": "2139280",
    "end": "2145130"
  },
  {
    "text": "left there we'll compare that with presto Athena and redshift so if you look at the data structure that they",
    "start": "2145130",
    "end": "2151100"
  },
  {
    "text": "support the first three can deal with with highly structured data or semi",
    "start": "2151100",
    "end": "2158150"
  },
  {
    "text": "structured data whereas Amazon redshift is a is a database so really requires",
    "start": "2158150",
    "end": "2164480"
  },
  {
    "text": "that data to be to be fully structured the languages that supports so the the",
    "start": "2164480",
    "end": "2171230"
  },
  {
    "text": "last three presto Athena and redshift support ANSI standard sequel as the spark simpler than hive gives you that",
    "start": "2171230",
    "end": "2177530"
  },
  {
    "text": "kind of sequel like language and the data stores that they can access",
    "start": "2177530",
    "end": "2184520"
  },
  {
    "text": "so hive and Spike see who can talk to s3 HDFS actually and other data stores as",
    "start": "2184520",
    "end": "2189800"
  },
  {
    "text": "well presto similarly primarily s3 in HDFS but also has connectors for for other",
    "start": "2189800",
    "end": "2196010"
  },
  {
    "text": "databases athena is s3 only and redshift talks to its own local store so in order",
    "start": "2196010",
    "end": "2203480"
  },
  {
    "text": "for rich if to to query data you actually load it in to redshift as opposed to the other three where you're",
    "start": "2203480",
    "end": "2208820"
  },
  {
    "text": "actually just pointing the engine at data that lives that lives somewhere else and then in terms of performance",
    "start": "2208820",
    "end": "2216640"
  },
  {
    "text": "this is kind of the gradient we're talking about so ivan sparks sequel they're not the",
    "start": "2216640",
    "end": "2223400"
  },
  {
    "text": "fastest sequel engines out there because they're a bit more general-purpose this or they're sitting on a more general-purpose framework whereas presto",
    "start": "2223400",
    "end": "2231950"
  },
  {
    "text": "is designed specifically for sequel so it's a lot faster and then a theater has a very similar performance footprint to",
    "start": "2231950",
    "end": "2239000"
  },
  {
    "text": "Presto because it's essentially presto and think the covers but then you'll find redshift is going to be the fastest",
    "start": "2239000",
    "end": "2244580"
  },
  {
    "text": "mainly because that data is is local so it's local to the to the processing",
    "start": "2244580",
    "end": "2249860"
  },
  {
    "text": "nodes but also it's a dedicated database engine specifically for for farcical",
    "start": "2249860",
    "end": "2255860"
  },
  {
    "text": "queries so a use case then there's these facets lend itself to so we find that higher",
    "start": "2255860",
    "end": "2263690"
  },
  {
    "text": "than spikes equal tend to be used more for transformation type of workloads as",
    "start": "2263690",
    "end": "2269600"
  },
  {
    "text": "opposed to queries where you can't expecting the results set back it's more for transforming data from one format to",
    "start": "2269600",
    "end": "2275120"
  },
  {
    "text": "another for example or or you're actually putting some business logic into that transformation as well and then presto for sequel queries that are",
    "start": "2275120",
    "end": "2284840"
  },
  {
    "text": "gonna hit s3 or HDFS a thinner similar but but you want a more serverless",
    "start": "2284840",
    "end": "2290660"
  },
  {
    "text": "approach where you just want to run the queries without having to run a worry about a server and then redshift is much",
    "start": "2290660",
    "end": "2296120"
  },
  {
    "text": "more a fully-featured sequel database so think about more in a data warehouse or",
    "start": "2296120",
    "end": "2301280"
  },
  {
    "text": "data analytics data Mart for example where you need some of the facets that a",
    "start": "2301280",
    "end": "2307190"
  },
  {
    "text": "database will give you now it's not all or nothing as well we have a lot of customers that use a combination of",
    "start": "2307190",
    "end": "2313700"
  },
  {
    "text": "these so one quite common pattern is to use redshift for very hot data for example where you've got people that are",
    "start": "2313700",
    "end": "2319930"
  },
  {
    "text": "running queries are expecting a very quick result that might be let's say the last two months worth of data and then",
    "start": "2319930",
    "end": "2327620"
  },
  {
    "text": "the the older stuff still needs to be able to be queried but maybe not quite",
    "start": "2327620",
    "end": "2333620"
  },
  {
    "text": "as quickly so often customers will put that data into s3 and then have a presto",
    "start": "2333620",
    "end": "2339260"
  },
  {
    "text": "cluster or Athena sitting on top of that so that their users still gets equal access but don't let it quite as quickly",
    "start": "2339260",
    "end": "2348430"
  },
  {
    "start": "2348000",
    "end": "2417000"
  },
  {
    "text": "now one of the best examples I've seen of a customer who has successfully",
    "start": "2349420",
    "end": "2354620"
  },
  {
    "text": "decoupled their compute from their storage is FINRA so this is the Financial Industry Regulatory Authority",
    "start": "2354620",
    "end": "2359990"
  },
  {
    "text": "in the US now they're tasked with looking at a lot of the the trade data",
    "start": "2359990",
    "end": "2365090"
  },
  {
    "text": "that goes through the financial markets specifically looking for things like fraud etc so often they'll look at",
    "start": "2365090",
    "end": "2372190"
  },
  {
    "text": "upwards of 75 billion records a day and so for them this ability to scale up and",
    "start": "2372190",
    "end": "2380270"
  },
  {
    "text": "down is absolutely crucial because if I have a particularly big trading day then obviously they're going to have more",
    "start": "2380270",
    "end": "2385460"
  },
  {
    "text": "data that they need to crunch through and if I have a lot of trading day then",
    "start": "2385460",
    "end": "2390650"
  },
  {
    "text": "they don't need as much pew power and sometimes I only need that compute power for a very short time so they're a great example of a customer",
    "start": "2390650",
    "end": "2397700"
  },
  {
    "text": "who uses that scalability they also take advantage of spinning up a very",
    "start": "2397700",
    "end": "2402769"
  },
  {
    "text": "dedicated EMR clusters for specific purposes so they have batch batch",
    "start": "2402769",
    "end": "2408289"
  },
  {
    "text": "clusters they have query clusters etc etc so a great great example of",
    "start": "2408289",
    "end": "2413709"
  },
  {
    "text": "decoupling that storage from from computer so let's move on to real-time",
    "start": "2413709",
    "end": "2420709"
  },
  {
    "start": "2417000",
    "end": "2870000"
  },
  {
    "text": "pipeline so the types of data that are characterized in the real-time pipeline",
    "start": "2420709",
    "end": "2427339"
  },
  {
    "text": "for example things like data that's been generated from machine so you might be",
    "start": "2427339",
    "end": "2433519"
  },
  {
    "text": "getting telemetry from machines around the state that they're in etc but",
    "start": "2433519",
    "end": "2439190"
  },
  {
    "text": "obviously devices are becoming very popular as well so you might have devices that are sending data back every",
    "start": "2439190",
    "end": "2445430"
  },
  {
    "text": "every few seconds about about their state mobile it's another big driver of",
    "start": "2445430",
    "end": "2451759"
  },
  {
    "text": "the real-time pipeline so you have data coming off the mobiles it might be location data or it might be in-game",
    "start": "2451759",
    "end": "2459440"
  },
  {
    "text": "data so we have a lot of customers who create games and they actually want to get a lot of telemetry from within the",
    "start": "2459440",
    "end": "2465829"
  },
  {
    "text": "game to see you know how players are going do they need help but they getting stuck that kind of thing and then",
    "start": "2465829",
    "end": "2473569"
  },
  {
    "text": "obviously clickstream data as well is very popular so instead of getting a",
    "start": "2473569",
    "end": "2478910"
  },
  {
    "text": "weblog every hour at the end of every day that contains everyone's clicks you",
    "start": "2478910",
    "end": "2484190"
  },
  {
    "text": "actually set it up so that you get them in near real-time so as soon as someone clicks on a link you get that event",
    "start": "2484190",
    "end": "2489979"
  },
  {
    "text": "straightaway and that obviously gives you the ability to then do something about that in their real time so if",
    "start": "2489979",
    "end": "2498469"
  },
  {
    "text": "you're going to ingest this type of data you need something that can cope with potentially a lot of throughput but it",
    "start": "2498469",
    "end": "2507410"
  },
  {
    "text": "needs to be able to store these messages durably so we have a service called Amazon Kinesis and if you look at",
    "start": "2507410",
    "end": "2514579"
  },
  {
    "text": "Canisius in a little bit more detail so you've got your data sources there Kinesis similar to s3 it sits across",
    "start": "2514579",
    "end": "2521479"
  },
  {
    "text": "three availability zone so about zones are our separate data centers and",
    "start": "2521479",
    "end": "2528410"
  },
  {
    "text": "so every message that you send into Kinesis again will write that to multiple locations to make sure that we",
    "start": "2528410",
    "end": "2533730"
  },
  {
    "text": "store that durably so you create a stream within the Kinesis service and then that gives you an endpoint and then",
    "start": "2533730",
    "end": "2540570"
  },
  {
    "text": "those those sources can then push the messages into that endpoint now the idea",
    "start": "2540570",
    "end": "2546600"
  },
  {
    "text": "with the with the stream is that you then want to have multiple applications",
    "start": "2546600",
    "end": "2551760"
  },
  {
    "text": "at the backend reading from the stream and so you don't want messages to be",
    "start": "2551760",
    "end": "2557640"
  },
  {
    "text": "hidden from each of these applications you want the same message to be read multiple times so messages will stay in",
    "start": "2557640",
    "end": "2565500"
  },
  {
    "text": "the stream and till they age out so the default age and the stream is 24 hours but you can push that to seven days if",
    "start": "2565500",
    "end": "2571920"
  },
  {
    "text": "you need a bit of extra time now there's a few different ways to get data out of",
    "start": "2571920",
    "end": "2577500"
  },
  {
    "text": "Kinesis so you can for example write your own application so the KCl is the",
    "start": "2577500",
    "end": "2583710"
  },
  {
    "text": "Kinesis client library so this is a bunch of libraries that help you to write your application against Kinesis",
    "start": "2583710",
    "end": "2589590"
  },
  {
    "text": "and so that would run on on ec2 machines and then that would obviously pull",
    "start": "2589590",
    "end": "2594960"
  },
  {
    "text": "messages often do whatever you need to do with it a lot of customers though are moving",
    "start": "2594960",
    "end": "2600420"
  },
  {
    "text": "much more towards a service approach so they prefer to use AWS lambda for this so lambda is is essentially a service",
    "start": "2600420",
    "end": "2608220"
  },
  {
    "text": "framework where you just you just give us your code and then we will then run that code either in response to an event",
    "start": "2608220",
    "end": "2614370"
  },
  {
    "text": "or in this case it will actually your code will sit there and lambda will poll",
    "start": "2614370",
    "end": "2620690"
  },
  {
    "text": "Kinesis waiting for messages and then when those messages come off it will then fire off your your logic to then do",
    "start": "2620690",
    "end": "2627420"
  },
  {
    "text": "whatever you need to do with the data another popular option is actually again to use Hadoop or EMR in conjunction with",
    "start": "2627420",
    "end": "2634890"
  },
  {
    "text": "SPARC streaming so SPARC streaming is designed specifically for allowing you",
    "start": "2634890",
    "end": "2640170"
  },
  {
    "text": "to process streaming data by creating micro batches and then doing something",
    "start": "2640170",
    "end": "2647250"
  },
  {
    "text": "with that with that data so three three common approaches now what we then see",
    "start": "2647250",
    "end": "2654120"
  },
  {
    "text": "customers doing with those types of things often fall into some very broad categories so the first thing often that",
    "start": "2654120",
    "end": "2660430"
  },
  {
    "text": "people will do is they'll just create a log of every every message that comes through they just want to know that that",
    "start": "2660430",
    "end": "2665800"
  },
  {
    "text": "they've got that sitting somewhere they also want to create real-time dashboards",
    "start": "2665800",
    "end": "2671800"
  },
  {
    "text": "so they can actually kind of see what's happening at any given time they might want to do deeper and a Linux on it",
    "start": "2671800",
    "end": "2679740"
  },
  {
    "text": "another common use case is actually to do predictions on the data so based on",
    "start": "2679740",
    "end": "2685090"
  },
  {
    "text": "the data that's coming through use some kind of predictive algorithm to try and predict whether maybe if we're getting",
    "start": "2685090",
    "end": "2692470"
  },
  {
    "text": "device data is this device about to fail based on the on the telemetry data we're",
    "start": "2692470",
    "end": "2698410"
  },
  {
    "text": "getting back from it and also then we might just fire off an alert so if we see that a particular message is out of",
    "start": "2698410",
    "end": "2706330"
  },
  {
    "text": "bounds for example we might actually then trigger off an alert system which which let someone know so how does this",
    "start": "2706330",
    "end": "2713080"
  },
  {
    "text": "map back to our back to our framework so you can see there we've got Kinesis in the ingest and then in our real-time",
    "start": "2713080",
    "end": "2718480"
  },
  {
    "text": "pipeline we've got our three different options for for dealing with the data",
    "start": "2718480",
    "end": "2724240"
  },
  {
    "text": "and then for our logging for our logging",
    "start": "2724240",
    "end": "2729570"
  },
  {
    "text": "application obviously an obvious choice for that is to put that into s3 for",
    "start": "2729570",
    "end": "2735090"
  },
  {
    "text": "real-time dashboards one of the most common places for that is elastic search",
    "start": "2735090",
    "end": "2741300"
  },
  {
    "text": "so if you're not familiar with elastic search it's an open source real-time",
    "start": "2741300",
    "end": "2746470"
  },
  {
    "text": "search engine and also analytics platform that's been around for a while but very very popular very scalable and",
    "start": "2746470",
    "end": "2754590"
  },
  {
    "text": "we've found a lot of customers were saying look we really love elastic search but we really don't want to manage the cluster anymore can you come",
    "start": "2754590",
    "end": "2762220"
  },
  {
    "text": "out with a managed service so we did that so that's what Amazon elastic search is it's essentially a managed",
    "start": "2762220",
    "end": "2768540"
  },
  {
    "text": "elastic search custom I'm integrated with the the open source ingestion and",
    "start": "2768540",
    "end": "2776080"
  },
  {
    "text": "also the visualization tools such as log stash in Cabana so if you're not familiar with Cabana it's it's a great",
    "start": "2776080",
    "end": "2784420"
  },
  {
    "text": "way to create real-time dashboards very easy to use and use a lot for this type",
    "start": "2784420",
    "end": "2789610"
  },
  {
    "text": "of we just want to kind of get a quick snapshot of what's going on in your environment for the analytics use case",
    "start": "2789610",
    "end": "2799509"
  },
  {
    "text": "obviously redshift is a is an obvious choice for that for predictions we might",
    "start": "2799509",
    "end": "2805479"
  },
  {
    "text": "see again Hadoop or EMR being used with something like spike machine learning",
    "start": "2805479",
    "end": "2812140"
  },
  {
    "text": "your markets you might be you might be sick of seeing the spark logo come up it seems to pop up everywhere and but this",
    "start": "2812140",
    "end": "2817959"
  },
  {
    "text": "is the beauty of spark is that it's it's got so many different modules to it that it really is an extremely flexible",
    "start": "2817959",
    "end": "2824969"
  },
  {
    "text": "processing engine and then you might use SNS which is the simple notification",
    "start": "2824969",
    "end": "2831880"
  },
  {
    "text": "service to send off alerts so if you if you pick something up in your real-time pipeline that wasn't quite right",
    "start": "2831880",
    "end": "2837640"
  },
  {
    "text": "you can obviously send off for a look for that now I just wanted to mention Airbnb here they've just recently opened",
    "start": "2837640",
    "end": "2845140"
  },
  {
    "text": "source something called stream alert and they've built a framework on top of a lot of the services that we've talked",
    "start": "2845140",
    "end": "2850930"
  },
  {
    "text": "about here Kinesis and athena lander etc and they've open sourced that really",
    "start": "2850930",
    "end": "2857140"
  },
  {
    "text": "really interesting so if you if you came to have a bit of a read through that just do a search on Airbnb stream alert",
    "start": "2857140",
    "end": "2863440"
  },
  {
    "text": "and and have a bit of a read through that they blogged about it a couple of days ago and really nice use of the",
    "start": "2863440",
    "end": "2868869"
  },
  {
    "text": "technology so just moving to our last",
    "start": "2868869",
    "end": "2874029"
  },
  {
    "start": "2870000",
    "end": "3050000"
  },
  {
    "text": "use case so if you think about the the concept of a day like no not everyone likes the term day delay but essentially",
    "start": "2874029",
    "end": "2879900"
  },
  {
    "text": "when you look at the day lake or the data hub or whatever you prefer to call",
    "start": "2879900",
    "end": "2885459"
  },
  {
    "text": "it the them the main facets of it when you look at all the definitions and you",
    "start": "2885459",
    "end": "2890680"
  },
  {
    "text": "distill out what it's really talking about there's really two two sides to this the first is that you can put any",
    "start": "2890680",
    "end": "2896920"
  },
  {
    "text": "data in the data lake it's not just about structured data",
    "start": "2896920",
    "end": "2903339"
  },
  {
    "text": "it's about pretty much anything you have any data can go into the lake and then at the other end",
    "start": "2903339",
    "end": "2908829"
  },
  {
    "text": "it's about any analysis and so you you're allowing users to use the tools that they want to access the data",
    "start": "2908829",
    "end": "2915249"
  },
  {
    "text": "you're not saying to them you can only use SQL they can use whatever they like so that that for me is really the",
    "start": "2915249",
    "end": "2921369"
  },
  {
    "text": "essence of the data Lake is is any type of data and any type of analysis and so when we map that back to",
    "start": "2921369",
    "end": "2928780"
  },
  {
    "text": "to our kind of architecture here all this really is is essentially just all",
    "start": "2928780",
    "end": "2935380"
  },
  {
    "text": "the three things that we've talked about so it's all the different types of sources so it's databases and flat files",
    "start": "2935380",
    "end": "2941260"
  },
  {
    "text": "and and real time sources it's the different ingestion mechanisms depending",
    "start": "2941260",
    "end": "2946660"
  },
  {
    "text": "on what the and the data source is and then it's either pushing that through your through your bachelor but also",
    "start": "2946660",
    "end": "2956680"
  },
  {
    "text": "taking in your real-time feeds and then also pushing them into to be married",
    "start": "2956680",
    "end": "2962260"
  },
  {
    "text": "with the with the batch data as well as well as doing things with them in their",
    "start": "2962260",
    "end": "2967750"
  },
  {
    "text": "real-time so we often see this use case where the whole point of having the the the speed layer is that you can see that",
    "start": "2967750",
    "end": "2974080"
  },
  {
    "text": "data very quickly you can see it in real time and you can react in real time but you also then want to have that data",
    "start": "2974080",
    "end": "2981720"
  },
  {
    "text": "available for for later analysis as well and be able to then marry that back to",
    "start": "2981720",
    "end": "2987220"
  },
  {
    "text": "the other data sources that are coming through so that's very common is to see your your speed data going in two",
    "start": "2987220",
    "end": "2994360"
  },
  {
    "text": "different directions there and then the serving layer obviously we've just put",
    "start": "2994360",
    "end": "2999370"
  },
  {
    "text": "in put a few down here but we see richer for analytics EMR for the host of",
    "start": "2999370",
    "end": "3004440"
  },
  {
    "text": "different Hadoop applications fina potentially for that sequel access and then ec2 as well so obviously on ec2 you",
    "start": "3004440",
    "end": "3012660"
  },
  {
    "text": "can wrap any application that you that you like and and that gives you tremendous flexibility and then those",
    "start": "3012660",
    "end": "3020160"
  },
  {
    "text": "consumers those constituents on the far right hand side there can then access that data",
    "start": "3020160",
    "end": "3025920"
  },
  {
    "text": "in whatever form they want to they're not going to be constrained by a particular tool and then of course the",
    "start": "3025920",
    "end": "3033840"
  },
  {
    "text": "the other kind of wrappers around this that we talked about as well are going to play into the data Lakers well making",
    "start": "3033840",
    "end": "3039960"
  },
  {
    "text": "sure you've got the right controls in place the right encryption levels the",
    "start": "3039960",
    "end": "3045000"
  },
  {
    "text": "the right order trails and monitoring etc",
    "start": "3045000",
    "end": "3049970"
  },
  {
    "start": "3050000",
    "end": "3230000"
  },
  {
    "text": "so that's essentially what I wanted to take you through from an architecture perspective I just wanted to make a",
    "start": "3050210",
    "end": "3055430"
  },
  {
    "text": "quick mention of the x1 so we talked about the different instance types that",
    "start": "3055430",
    "end": "3061670"
  },
  {
    "text": "we have and obviously one of the things that comes up a lot is is how much memory some of these instances have so",
    "start": "3061670",
    "end": "3067400"
  },
  {
    "text": "the x1 is relatively new it's really designed for for in-memory applications",
    "start": "3067400",
    "end": "3072500"
  },
  {
    "text": "in a very large scale so if you look at some of the applications that that are good for it so we've talked a lot today",
    "start": "3072500",
    "end": "3077750"
  },
  {
    "text": "about Apache spark we've also talked about presto as well but also sfe HANA",
    "start": "3077750",
    "end": "3083750"
  },
  {
    "text": "as well we see that as a tied up for the x12 where you know you need that beat",
    "start": "3083750",
    "end": "3089150"
  },
  {
    "text": "that big memory footprint and Hana is a great example of you know that kind of serving layer where you you're able to",
    "start": "3089150",
    "end": "3095150"
  },
  {
    "text": "to give give your users very fast response time it's got the that the",
    "start": "3095150",
    "end": "3102380"
  },
  {
    "text": "Intel Haswell processors in there so lots of lots of great grant they're up to 2 terabytes of memory which is which",
    "start": "3102380",
    "end": "3109339"
  },
  {
    "text": "is nice and up to 128 vcp years per",
    "start": "3109339",
    "end": "3115490"
  },
  {
    "text": "instance so it's a quite a lot of power in these instances and for the right use case can be extremely extremely useful",
    "start": "3115490",
    "end": "3123609"
  },
  {
    "text": "now while we're on the subject of of Intel because as I said we were in partnership with Intel for this for this",
    "start": "3123609",
    "end": "3130460"
  },
  {
    "text": "webinar there's actually a lot of Intel smarts in in a lot of the the instance",
    "start": "3130460",
    "end": "3137420"
  },
  {
    "text": "types that we have and so they've done a huge amount of work around obviously",
    "start": "3137420",
    "end": "3143150"
  },
  {
    "text": "putting as I said more smarts into the into the processing and if you jump onto",
    "start": "3143150",
    "end": "3148220"
  },
  {
    "text": "the AWS website there's an Intel page there that's really interesting it talks a lot about these technologies but also",
    "start": "3148220",
    "end": "3154160"
  },
  {
    "text": "talks about the different the different instance types that we have on which of",
    "start": "3154160",
    "end": "3159560"
  },
  {
    "text": "them support some of these different technologies so so if that's of interest jump onto the AWS page and have a look",
    "start": "3159560",
    "end": "3166940"
  },
  {
    "text": "at that and then just finally before I go I just wanted to also mention that",
    "start": "3166940",
    "end": "3173920"
  },
  {
    "text": "well while listening to me talk is obviously extremely exciting getting",
    "start": "3173920",
    "end": "3179300"
  },
  {
    "text": "hands-on is is really where you start to to get a feel for the technology and start and start to to understand how it all",
    "start": "3179300",
    "end": "3186290"
  },
  {
    "text": "fits together so what we've done is we've we've made some some self-paced labs available for you to use and so if",
    "start": "3186290",
    "end": "3194870"
  },
  {
    "text": "you can register by jumping onto that that URL and they're gonna be they're",
    "start": "3194870",
    "end": "3201080"
  },
  {
    "text": "gonna be free until the 31st of March this year so if you want to to jump on",
    "start": "3201080",
    "end": "3207350"
  },
  {
    "text": "to get some hands-on then then feel free to do that so I shall leave that that there so you can you can follow that",
    "start": "3207350",
    "end": "3213800"
  },
  {
    "text": "link if you need to and I shall sign off so thank you very much for for your time today thanks very much for joining and",
    "start": "3213800",
    "end": "3219460"
  },
  {
    "text": "best of luck in creating your moderator architecture",
    "start": "3219460",
    "end": "3224859"
  }
]