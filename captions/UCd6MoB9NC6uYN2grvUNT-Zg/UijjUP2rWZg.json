[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "thank you so much for joining us this morning after I'm sure it was a long night at the party hope you all had",
    "start": "359",
    "end": "6799"
  },
  {
    "text": "fun so I'm Bob griffi I'm a Solutions architect for um AWS and joining me",
    "start": "6799",
    "end": "13719"
  },
  {
    "text": "today from finra we have um Brett Shriver um senior director um in Market",
    "start": "13719",
    "end": "19160"
  },
  {
    "text": "surveillance for finra and um Ricardo Portia who is a lead architect in Market",
    "start": "19160",
    "end": "26519"
  },
  {
    "start": "27000",
    "end": "27000"
  },
  {
    "text": "surveillance so today um I'm just going to basically set the table and talk",
    "start": "27679",
    "end": "33280"
  },
  {
    "text": "about uh what is spark and how do you run spark on AWS and if there's a bunch",
    "start": "33280",
    "end": "38600"
  },
  {
    "text": "of spark experts in the room I apologize this is going to be pretty high level but um I'm going to assume that some of",
    "start": "38600",
    "end": "44600"
  },
  {
    "text": "you aren't spark so I'm just going to set a baseline understanding of what spark is and how you can run it on AWS",
    "start": "44600",
    "end": "51079"
  },
  {
    "text": "and then I'm going to hand it off to um Brett who's going to take over and talk specifics about how they use spark on",
    "start": "51079",
    "end": "57440"
  },
  {
    "text": "AWS for aligning time ordered events um in their Market surveillance",
    "start": "57440",
    "end": "63679"
  },
  {
    "text": "area so spark what is it so spark is a",
    "start": "64000",
    "end": "69880"
  },
  {
    "start": "66000",
    "end": "66000"
  },
  {
    "text": "general purpose um data processing framework um it's generally good for Big",
    "start": "69880",
    "end": "77880"
  },
  {
    "text": "Data is what it's mainly used for if you think historically what you use um Hadoop for you know map produce um was",
    "start": "77880",
    "end": "85560"
  },
  {
    "text": "one of the first platforms used for Big Data um you can use spark for the same type of jobs um but the advantage of",
    "start": "85560",
    "end": "93040"
  },
  {
    "text": "using spark over something like ma ruce is you're going to get mainly the speed um Improvement so if you use um you know",
    "start": "93040",
    "end": "99920"
  },
  {
    "text": "Sparks memory capabilities U you can get about 100 times faster um results than",
    "start": "99920",
    "end": "105520"
  },
  {
    "text": "you would using map reduce um and then 10 times faster if you just use you know",
    "start": "105520",
    "end": "111079"
  },
  {
    "text": "disc based processing so sparc has a foundational layer aache spark and it",
    "start": "111079",
    "end": "117320"
  },
  {
    "text": "also comes with um a set of library iies that are um pre-made for you to use",
    "start": "117320",
    "end": "122520"
  },
  {
    "text": "there's spark SQL if you want to run standard SQL queries on top of spark there's a streaming Library there's a",
    "start": "122520",
    "end": "128959"
  },
  {
    "text": "machine learning library there's a graphing Library as you can see there um you can use Java Scola Python and R so",
    "start": "128959",
    "end": "138599"
  },
  {
    "text": "very good very popular framework for running um data processing jobs today",
    "start": "138599",
    "end": "143680"
  },
  {
    "text": "and if you want to learn more um just hit the Apache spark website up there spark. apache.org",
    "start": "143680",
    "end": "150000"
  },
  {
    "text": "and um go and poke around there's a lot of good information out there about spark and um the various libraries that",
    "start": "150000",
    "end": "155080"
  },
  {
    "text": "you can use so what about running spark on AWS can you run spark and AWS of course you",
    "start": "155080",
    "end": "162080"
  },
  {
    "start": "157000",
    "end": "157000"
  },
  {
    "text": "can we have um there's various ways you can do it so spark itself comes in a",
    "start": "162080",
    "end": "168360"
  },
  {
    "text": "standalone cluster mode so you can just spin up ec2 instances ec2 is our um",
    "start": "168360",
    "end": "175080"
  },
  {
    "text": "General compute service alasa compute Cloud I assume you all know that um",
    "start": "175080",
    "end": "180519"
  },
  {
    "text": "so you can just spin up multiple ec2 instances install spark in its native Standalone cluster mode it has its own",
    "start": "180519",
    "end": "186400"
  },
  {
    "text": "built-in job scheduler and just install it and you're ready to go just run it on",
    "start": "186400",
    "end": "191599"
  },
  {
    "text": "top of ec2 directly spark also supports um Apache mesos as the job scheduler so if you're",
    "start": "191599",
    "end": "198519"
  },
  {
    "text": "comfortable with that and you already Ed that you know how to use it again you can spin up ec2 instances on",
    "start": "198519",
    "end": "204280"
  },
  {
    "text": "AWS um install msos and install spark on top of that and then you can use use that to to run your your your jobs and",
    "start": "204280",
    "end": "212159"
  },
  {
    "text": "finally Amazon EMR is a EMR stands for elastic map reduce that is our managed",
    "start": "212159",
    "end": "217799"
  },
  {
    "text": "to dup framework um spark is a first class citizen on um EMR meaning that you",
    "start": "217799",
    "end": "224879"
  },
  {
    "text": "can just specify when you launch a cluster that you want um spark to be installed so when you launch a cluster",
    "start": "224879",
    "end": "231480"
  },
  {
    "text": "either through the console or through the CLI you can just specify you know I want as one of the applications I want",
    "start": "231480",
    "end": "236640"
  },
  {
    "text": "installed is spark and when the cluster boots and comes up SP will be installed and ready to go um and on EMR spark uses the the yarn",
    "start": "236640",
    "end": "245920"
  },
  {
    "text": "job scheduler so what about storage",
    "start": "245920",
    "end": "252599"
  },
  {
    "text": "so this is not a comprehensive list nor is it a mutually exclusive list so spark",
    "start": "252599",
    "end": "259239"
  },
  {
    "text": "can support many many types of storage um from which to do their um do your",
    "start": "259239",
    "end": "264360"
  },
  {
    "text": "processing I'm just going to focus on a few things on here because these are more AWS specific but again it's not",
    "start": "264360",
    "end": "270120"
  },
  {
    "start": "269000",
    "end": "269000"
  },
  {
    "text": "comprehensive there's many other data stores that you can use to run your spark jobs um so I'm just going to run",
    "start": "270120",
    "end": "275479"
  },
  {
    "text": "through this real quick we have um you know a traditional thing like if you're using Hadoop um Hadoop comes with hdfs",
    "start": "275479",
    "end": "282240"
  },
  {
    "text": "um um Hadoop file system and the traditional way to do that is to",
    "start": "282240",
    "end": "289520"
  },
  {
    "text": "run hdfs on ephemeral storage local instance storage um on ec2 so that is an",
    "start": "289520",
    "end": "295919"
  },
  {
    "text": "option if you want to have your data right with your compute which is a traditional way that you would run map redu jobs or HRA Pig jobs on um on EMR",
    "start": "295919",
    "end": "303960"
  },
  {
    "text": "you can also run spark um using that as your storage your primary storage you can also since you're running on ec2 you",
    "start": "303960",
    "end": "310280"
  },
  {
    "text": "can um instead choose to use EBS which is our elastic Block store and uh run your jobs on elastic",
    "start": "310280",
    "end": "318000"
  },
  {
    "text": "Block store which are attached to your ec2 instances um that's another option um Amazon S3 uh simple storage",
    "start": "318000",
    "end": "325680"
  },
  {
    "text": "service is our foundational Object Store very high",
    "start": "325680",
    "end": "331120"
  },
  {
    "text": "durability um basically unlimited scale very good um input output",
    "start": "331120",
    "end": "337560"
  },
  {
    "text": "characteristics so I'm going to have some more slides later on this and focus on S3 but S3 is a very good place to",
    "start": "337560",
    "end": "344000"
  },
  {
    "text": "store um data for your big data jobs um a lot of our customers including Fina",
    "start": "344000",
    "end": "349080"
  },
  {
    "text": "use it as a a data Lake um from which they run various types of processing jobs including spark and finally um",
    "start": "349080",
    "end": "356639"
  },
  {
    "text": "Amazon Dynamo DB Dynamo DB is our our managed no SQL",
    "start": "356639",
    "end": "362160"
  },
  {
    "text": "database and again spark can um call into that um if you've got data that",
    "start": "362160",
    "end": "368240"
  },
  {
    "text": "fits nicely into a nosql database um spark works very nicely with Dynamo DB",
    "start": "368240",
    "end": "373400"
  },
  {
    "text": "and again this is not mutually exclusive if you have data um on S3 or you want to",
    "start": "373400",
    "end": "379560"
  },
  {
    "text": "put data on hdfs and you have Damo data in Dynamo DB um that's fine you can",
    "start": "379560",
    "end": "385280"
  },
  {
    "text": "write your jobs to pull for all these data sources you don't have to pick just one so I think burner mentioned in his",
    "start": "385280",
    "end": "390479"
  },
  {
    "text": "keynote yesterday that you know we provide a full two set tool set um of applications so you want to use the",
    "start": "390479",
    "end": "396800"
  },
  {
    "text": "right tool for your specific workload and your specific data so if your data and your workload you know fits better",
    "start": "396800",
    "end": "403520"
  },
  {
    "text": "in um an S3 or Dynamo DB please use that as appropriate for your your specific",
    "start": "403520",
    "end": "408960"
  },
  {
    "text": "workload and then just write your spark queries to pull from the appropriate data",
    "start": "408960",
    "end": "414880"
  },
  {
    "start": "417000",
    "end": "417000"
  },
  {
    "text": "sets so I just wanted to touch quickly on um some other customers that are using spark obviously ther is using",
    "start": "417000",
    "end": "423039"
  },
  {
    "text": "spark and you're going to hear quite a bit from them in a few minutes about um the specifics of how they're they're",
    "start": "423039",
    "end": "428960"
  },
  {
    "text": "using it um in their workloads but if you go out to um our public website aws.amazon.com",
    "start": "428960",
    "end": "434680"
  },
  {
    "text": "and just search for spark you're going to see a number of customers up there",
    "start": "434680",
    "end": "440360"
  },
  {
    "text": "that use spark um on AWS um these are just three that I grabbed from the website and again you",
    "start": "440360",
    "end": "446560"
  },
  {
    "text": "can go um look at the case studies out there yourself please do I want to focus in a little bit on the middle one krux",
    "start": "446560",
    "end": "453199"
  },
  {
    "text": "they are doing um processing using um ephemeral uh Amazon EMR instances um",
    "start": "453199",
    "end": "461639"
  },
  {
    "text": "they're using spot capacity and they're using S3 with emrfs as our data later for Apache spark",
    "start": "461639",
    "end": "469440"
  },
  {
    "text": "and I want to point that out because I'm going to go into some best practices here um in my next few slides and this",
    "start": "469440",
    "end": "475639"
  },
  {
    "text": "is hitting um some of the best practices so um this is a really good way to use um",
    "start": "475639",
    "end": "482599"
  },
  {
    "text": "spark and and lay out your data so let me get into",
    "start": "482599",
    "end": "486919"
  },
  {
    "text": "that all right so this is not just specific to spark um for any big data",
    "start": "487720",
    "end": "493759"
  },
  {
    "start": "488000",
    "end": "488000"
  },
  {
    "text": "processing either using spark or Presto or Hive or pig or or any type of big",
    "start": "493759",
    "end": "499680"
  },
  {
    "text": "data processing you might want to do on AWS um separating your storage from",
    "start": "499680",
    "end": "505240"
  },
  {
    "text": "compute is a really really good best practice for for these reasons so the first one optimize cluster size based on",
    "start": "505240",
    "end": "512000"
  },
  {
    "text": "um compute requirements so if you don't separate your storage and your compute",
    "start": "512000",
    "end": "517159"
  },
  {
    "text": "often times you have to size your cluster based on your data set so if you have a very large data set and you want",
    "start": "517159",
    "end": "524159"
  },
  {
    "text": "to put that all in htfs for example on on on a Hadoop cluster then you have to",
    "start": "524159",
    "end": "529320"
  },
  {
    "text": "size your data set with a number of nodes that for enough disc capacity to store your storage so if you don't have",
    "start": "529320",
    "end": "534920"
  },
  {
    "text": "that many compute requirements High compute requirements for that specific workload you're you're going to be",
    "start": "534920",
    "end": "540360"
  },
  {
    "text": "oversizing your your cluster because you just need that size of a cluster to put all your data on it so and that's not",
    "start": "540360",
    "end": "546880"
  },
  {
    "text": "efficient that's not a good use of your money um it'd be much better if you can just size the cluster appropriately for",
    "start": "546880",
    "end": "552920"
  },
  {
    "text": "the the compu that you need and so in a very similar fashion if you separate wrate your storage and compute um you",
    "start": "552920",
    "end": "559800"
  },
  {
    "text": "can select your optimal ec2 instance types so it's it's kind of similar to the first one so if I'm not separating",
    "start": "559800",
    "end": "566040"
  },
  {
    "text": "storage and compute I may have to pick you know a dent storage type like an I2 instance for example or a D2 instance or",
    "start": "566040",
    "end": "573160"
  },
  {
    "text": "our older hs1 instance family that has you know dense local storage on there for your um uh your data needs but if",
    "start": "573160",
    "end": "581920"
  },
  {
    "text": "you've got a job like running spark for example and you want to take advantage of Spark's um you know memory caching",
    "start": "581920",
    "end": "588560"
  },
  {
    "text": "that it does you might want to run your jobs on R3 instances and if you separate storage",
    "start": "588560",
    "end": "595079"
  },
  {
    "text": "and compute you can do that you can select R3 instances to get the back performance out of your jobs while",
    "start": "595079",
    "end": "600399"
  },
  {
    "text": "leaving your data in S3 um and so when I do separate storage",
    "start": "600399",
    "end": "606079"
  },
  {
    "text": "from compute um what I'm not saying is um generally the best practice is use S3 as your data l so spark um especially",
    "start": "606079",
    "end": "614040"
  },
  {
    "text": "with spark with EMR EMR comes with emrfs EMR file system which allows you um very",
    "start": "614040",
    "end": "620240"
  },
  {
    "text": "easy access to S3 and it's basically treated it um like a local file system",
    "start": "620240",
    "end": "625920"
  },
  {
    "text": "and then you can optimize your your cluster requirements again you you can use R3 instances or maybe you want to",
    "start": "625920",
    "end": "631079"
  },
  {
    "text": "use C4 instances for compute intensive jobs um so you have the right type of instance type to to maximize your your",
    "start": "631079",
    "end": "638440"
  },
  {
    "text": "compute capacity for your for your job um the next good Advantage is you",
    "start": "638440",
    "end": "644200"
  },
  {
    "text": "can shut down your cluster when not in use if you have your data with your Hado cluster and you need if you want to shut",
    "start": "644200",
    "end": "651880"
  },
  {
    "text": "down the cluster to save money well that's not an easy thing to do especially if you're talking about you",
    "start": "651880",
    "end": "657000"
  },
  {
    "text": "know terabytes of data Maybe you can hydrate your your cluster from S3 but",
    "start": "657000",
    "end": "662720"
  },
  {
    "text": "you know that could take hours or maybe even days depending on how much data you have and so you're really it's really",
    "start": "662720",
    "end": "669600"
  },
  {
    "text": "painful and not in your best interest to shut down your cluster when you have to back up and rehydrate your cluster every",
    "start": "669600",
    "end": "675760"
  },
  {
    "text": "time you shut it down and bring it back up but if your data is native on S3 and",
    "start": "675760",
    "end": "680800"
  },
  {
    "text": "you're running your spark jobs on your computer instances on the data in S3 you're reading from S3 you're writing",
    "start": "680800",
    "end": "686880"
  },
  {
    "text": "your output to S3 then you don't have to worry about rehydrating your clusters so when you're done with a job you can just",
    "start": "686880",
    "end": "693000"
  },
  {
    "text": "shut down all your clusters and you're not paying for any of that your data is sitting there in S3 ready for your next",
    "start": "693000",
    "end": "699040"
  },
  {
    "text": "jobs to spin up you can spin up a cluster when you're ready point it to your data in S3 and off you go that's",
    "start": "699040",
    "end": "705360"
  },
  {
    "text": "really really good for cost savings um and a similar Advantage is you can also",
    "start": "705360",
    "end": "712440"
  },
  {
    "text": "when you do this you can share data Bunch multiple clusters so let's say you have different types of jobs um finra",
    "start": "712440",
    "end": "718760"
  },
  {
    "text": "for example they do Market surveillance and they have different things that they're looking for so they could spin up a cluster looking for one specific",
    "start": "718760",
    "end": "726040"
  },
  {
    "text": "type of activity uh in their data and they could point that data in S3 run the",
    "start": "726040",
    "end": "732720"
  },
  {
    "text": "job when it's um write the data out with S3 and shut the cluster down when they're done but they may have hundreds",
    "start": "732720",
    "end": "738880"
  },
  {
    "text": "of these type of jobs and so instead of sharing one big cluster and worrying about you know scheduling jobs and",
    "start": "738880",
    "end": "744560"
  },
  {
    "text": "getting prioritization and trying to to to map between jobs that might run hours between jobs that run you know for a few",
    "start": "744560",
    "end": "750760"
  },
  {
    "text": "minutes and try and all do that in a single cluster you can just spin up clusters for these specific workloads",
    "start": "750760",
    "end": "756279"
  },
  {
    "text": "and point it to the data and S3 so I could be running hundreds tens hundreds of clusters all using that same data in",
    "start": "756279",
    "end": "763360"
  },
  {
    "text": "S3 and optimize those clusters for the job I'm running so if I'm doing a",
    "start": "763360",
    "end": "768399"
  },
  {
    "text": "compute intensive job on that data for this specific thing I'm looking for I'm going to run those on C4 instances I'm",
    "start": "768399",
    "end": "774000"
  },
  {
    "text": "going to point it on there and then I have another job I run1 run the same time same time using the same data set",
    "start": "774000",
    "end": "779720"
  },
  {
    "text": "that might be Memory intensive so I'm going to run those on R3 instances so you can really run multiple jobs and you",
    "start": "779720",
    "end": "786120"
  },
  {
    "text": "can have your separate teams focus on doing their job the best way they can by",
    "start": "786120",
    "end": "792120"
  },
  {
    "text": "spinning up the appropriate type of cluster for their specific job type and just use that shame s data set on",
    "start": "792120",
    "end": "798279"
  },
  {
    "text": "S3 and finally fault tolerance and disaster recovery so when you spit up a hudo",
    "start": "798279",
    "end": "803839"
  },
  {
    "text": "cluster in a traditional way you normally put these in placement groups in a single a because you there's a lot of inter node communication between the",
    "start": "803839",
    "end": "810880"
  },
  {
    "text": "nodes and you're going to want that really really fast network access in the in the same a the problem is that's kind",
    "start": "810880",
    "end": "817199"
  },
  {
    "text": "of like anti Cloud right hopefully you guys have learned over the week that you know we have multiple availability zones",
    "start": "817199",
    "end": "823399"
  },
  {
    "text": "you're going to want to spread your workloads across these availability zones so that you can take advantage of the um High availability capability so",
    "start": "823399",
    "end": "830160"
  },
  {
    "text": "if we lose one for whatever reason your jobs are still up and running in um other availability zones",
    "start": "830160",
    "end": "836959"
  },
  {
    "text": "so you're still going to want to spin up a cluster in an availability Zone but if you lose that availability Zone it's",
    "start": "836959",
    "end": "842160"
  },
  {
    "text": "really easy to bring up a cluster in another one because your data is sit in an S3 again you don't have to worry about data hydration you don't have to",
    "start": "842160",
    "end": "848920"
  },
  {
    "text": "worry about losing any of your data because it's all living there on S3 and it's persisted so it makes it a lot",
    "start": "848920",
    "end": "855199"
  },
  {
    "text": "easier to deal with um the disasters and and fail over and and and and deal with",
    "start": "855199",
    "end": "861040"
  },
  {
    "text": "recovery because you can just spin up your cluster and another availability Zone finally another good practice is to",
    "start": "861040",
    "end": "868800"
  },
  {
    "start": "866000",
    "end": "866000"
  },
  {
    "text": "use spot so spot is a way to run compute on",
    "start": "868800",
    "end": "874199"
  },
  {
    "text": "uh AWS where you're basically bidding on excess ec2 capacity that Amazon has and",
    "start": "874199",
    "end": "880560"
  },
  {
    "text": "this will save you you know up to 50 to 90% on your compute and spot works very",
    "start": "880560",
    "end": "886120"
  },
  {
    "text": "very nicely with these um ephemeral compute clusters that you can start up and down because you've separated your",
    "start": "886120",
    "end": "891560"
  },
  {
    "text": "storage and compute because one of the aspects of spot is if somebody comes in and out bids you AWS could take your",
    "start": "891560",
    "end": "899079"
  },
  {
    "text": "your machines away so if you have all your data on your clusters and you're not separating storage and compute",
    "start": "899079",
    "end": "905519"
  },
  {
    "text": "that's painful you don't want your machines to go away go away if you're running on spot however if you're using",
    "start": "905519",
    "end": "912320"
  },
  {
    "text": "um S3 as your data Lake and you separated your storage compute if you do happen to lose your compute because",
    "start": "912320",
    "end": "917680"
  },
  {
    "text": "somebody out bids your spot it's okay you can just bring it up in another um availability Zone you know do another",
    "start": "917680",
    "end": "923480"
  },
  {
    "text": "spot ask and and you're off and running so those two work very very well well together the separated storage of",
    "start": "923480",
    "end": "930519"
  },
  {
    "text": "compute and then pairing that with spot to save money um to run your workloads so again this isn't specific to spark",
    "start": "930519",
    "end": "937240"
  },
  {
    "text": "but it works very very nicely on spark spark um works very well with um with S3",
    "start": "937240",
    "end": "942440"
  },
  {
    "text": "and it's a really good practice so with that I am actually going to turn it over to Brett now and",
    "start": "942440",
    "end": "947839"
  },
  {
    "text": "he's going to go into his specific use case for um their Market surveillance using",
    "start": "947839",
    "end": "953759"
  },
  {
    "text": "spark okay thank you Bob uh thank you everyone for coming out here this Friday",
    "start": "953759",
    "end": "959880"
  },
  {
    "text": "morning I know it's been a long week for everybody and many of you might have been out late last night I think there was some sort of event going on around",
    "start": "959880",
    "end": "965839"
  },
  {
    "text": "here um for all so thank you for showing up uh and I'm going to go into uh what",
    "start": "965839",
    "end": "971440"
  },
  {
    "text": "fenra has done here um and how we implemented a lot of the best practices that Bob went over uh my name is Brett",
    "start": "971440",
    "end": "978519"
  },
  {
    "text": "Shriver senior director finra specialized in Market rig surveillance uh Ricardo Portillo was our lead",
    "start": "978519",
    "end": "984480"
  },
  {
    "text": "architect on this uh specific um project hopefully we'll have some time at at the end we can go into some Q&A on more",
    "start": "984480",
    "end": "991160"
  },
  {
    "text": "detailed implementation specifics or any uh technical questions you",
    "start": "991160",
    "end": "996240"
  },
  {
    "text": "have so first off who is finra I don't know how familiar you guys are with us",
    "start": "996240",
    "end": "1002680"
  },
  {
    "text": "uh but we're the financial industry regulatory Authority we're actually an independent non-government private",
    "start": "1002680",
    "end": "1009120"
  },
  {
    "text": "sector uh company uh we have responsibility to",
    "start": "1009120",
    "end": "1014279"
  },
  {
    "text": "regulate 90% of the equities Market by volume and about 60% of the options uh",
    "start": "1014279",
    "end": "1021040"
  },
  {
    "text": "Market by volume in the United States that can account to up to 75 billion",
    "start": "1021040",
    "end": "1026558"
  },
  {
    "text": "records a day and they're coming whether we're ready to handle them or not it's like a fire hose of data coming at us we got a big data problem um and it isn't",
    "start": "1026559",
    "end": "1033798"
  },
  {
    "text": "going away it's only getting worse uh we have to keep that data online so that our end users after we produce things",
    "start": "1033799",
    "end": "1040880"
  },
  {
    "text": "that might require them to go in and investigate or build a case on some sort",
    "start": "1040880",
    "end": "1046319"
  },
  {
    "text": "of malicious activity in the market we keep it online for a couple years legal process is not instantaneous so they",
    "start": "1046319",
    "end": "1052640"
  },
  {
    "text": "need to be able to go back and retrieve that data on demand uh so that they can",
    "start": "1052640",
    "end": "1057760"
  },
  {
    "text": "build their uh evidence case or rerun stuff or do analysis to help them with what they're doing so that has driven",
    "start": "1057760",
    "end": "1064240"
  },
  {
    "text": "the need to have 20 pedabytes of data online we also have to rebuild or",
    "start": "1064240",
    "end": "1070799"
  },
  {
    "text": "reconstruct the market we have data coming in from all sorts of sources we have the exchanges sending us the",
    "start": "1070799",
    "end": "1076120"
  },
  {
    "text": "proprietary feeds from their matching engines we we have broker dealers sending us their feeds all of them are",
    "start": "1076120",
    "end": "1081840"
  },
  {
    "text": "running on slightly different clocks so we have kind of a clock drift problem uh",
    "start": "1081840",
    "end": "1087000"
  },
  {
    "text": "in order to figure out what's happened in the market we actually have to kind of replay the market um in two ways one",
    "start": "1087000",
    "end": "1094320"
  },
  {
    "text": "is in order comes into the system it may get executed right away or it may get routed it may get partially executed it",
    "start": "1094320",
    "end": "1100640"
  },
  {
    "text": "may get canceled replaced these we call it an order life cycle tend to build a a",
    "start": "1100640",
    "end": "1106520"
  },
  {
    "text": "tree a directed graph if you will that we have to Traverse all these billions of records and try and reconstruct after",
    "start": "1106520",
    "end": "1112120"
  },
  {
    "text": "the fact that has left us storing in the order of trillions of nodes and edges of",
    "start": "1112120",
    "end": "1118400"
  },
  {
    "text": "these reconstructed order life cycles online we also have to be able to on the",
    "start": "1118400",
    "end": "1124360"
  },
  {
    "text": "Fly replay the market so that end users analysts that are trying to build a case",
    "start": "1124360",
    "end": "1129520"
  },
  {
    "text": "and figure out exactly what did happen around this suspicious activity in the market can see not only the top of book",
    "start": "1129520",
    "end": "1135120"
  },
  {
    "text": "but they may to see the depth of the book so there was a th000 shares available at $58 well what's available just a notch",
    "start": "1135120",
    "end": "1142480"
  },
  {
    "text": "below that um and that's very important to make sure when we're doing these surveillances that the broker dealer",
    "start": "1142480",
    "end": "1151080"
  },
  {
    "text": "executed at the best possible price for their customer some more uh kind of by the",
    "start": "1151080",
    "end": "1157799"
  },
  {
    "text": "Numbers about what fenra does we oversee more than 3,900 Securities firms",
    "start": "1157799",
    "end": "1163960"
  },
  {
    "text": "approximately 640,000 Brokers we handle 6 billion shares traded every day in the",
    "start": "1163960",
    "end": "1170600"
  },
  {
    "text": "US and we run hundreds of surveillance patterns against every one of those trades every single day it amounts to",
    "start": "1170600",
    "end": "1177640"
  },
  {
    "text": "processing on the order of six terabytes of data every day and we don't do it just because we like to process data the",
    "start": "1177640",
    "end": "1184320"
  },
  {
    "text": "end result uh we tried and produce value for our business in 2015 we had over 800 fraud cases that",
    "start": "1184320",
    "end": "1192280"
  },
  {
    "text": "were referred we refer to the SEC and we Levy more than $191 million in fines and restitution",
    "start": "1192280",
    "end": "1199360"
  },
  {
    "text": "so that's kind of what we get out of all of our big data processing that we do now I want to kind of focus more on",
    "start": "1199360",
    "end": "1206400"
  },
  {
    "text": "my space which is the actual surveillances themselves that run across all of these billions of records and",
    "start": "1206400",
    "end": "1212000"
  },
  {
    "text": "come up with the events that the end users are interested in looking at we call these events of regulatory interest",
    "start": "1212000",
    "end": "1219000"
  },
  {
    "text": "suspicious activity they're going to be associated with various Dimensions could be a firm Dimension could be an issued",
    "start": "1219000",
    "end": "1224720"
  },
  {
    "text": "Dimension and usually a window of time we have ulation surveillances that we",
    "start": "1224720",
    "end": "1230200"
  },
  {
    "text": "run where people are actually trying to manipulate the market in order to unfairly realize a profit we also have a",
    "start": "1230200",
    "end": "1236480"
  },
  {
    "text": "whole Suite of compliance surveillances which make sure that firms out there trading on the markets adhere to the",
    "start": "1236480",
    "end": "1243000"
  },
  {
    "text": "various SEC rules and regulations and fer also has our own rules and regulations specifically the ones on",
    "start": "1243000",
    "end": "1249640"
  },
  {
    "text": "spark are very focused around time ordered events uh in order to figure out",
    "start": "1249640",
    "end": "1257640"
  },
  {
    "text": "if things are dis if orders are displayed properly to the book so that every exchange knows that this order is",
    "start": "1257640",
    "end": "1263919"
  },
  {
    "text": "out there and can be executed on we have to take all of our data feeds in from",
    "start": "1263919",
    "end": "1269720"
  },
  {
    "text": "the exchanges and the orders order them by time try and solve our fuzzy time problem and then kind of walk aate",
    "start": "1269720",
    "end": "1276720"
  },
  {
    "text": "machine as we look through the activity in all the various markets at once and ensure that orders were handled in a",
    "start": "1276720",
    "end": "1283840"
  },
  {
    "text": "fair Manner and an equable manner for the enduser with 12 markets exchanges 75",
    "start": "1283840",
    "end": "1289840"
  },
  {
    "text": "billion records that becomes a very very complex problems in terms of getting through that compute on a day-to-day",
    "start": "1289840",
    "end": "1296400"
  },
  {
    "text": "basis we have to get through it because the next day data is coming and we're obligated to to surveil every single",
    "start": "1296400",
    "end": "1301760"
  },
  {
    "text": "trade and every single order that we receive across many many different uh",
    "start": "1301760",
    "end": "1307559"
  },
  {
    "text": "scenarios so more detailed examples of the types of things we're looking for these are more familiar maybe to you",
    "start": "1307559",
    "end": "1313640"
  },
  {
    "text": "guys if you follow the markets at all front running uh again it's a very time order thing we're looking for uh",
    "start": "1313640",
    "end": "1321000"
  },
  {
    "text": "basically a a customer places a big order with a broker dealer it's an order",
    "start": "1321000",
    "end": "1326240"
  },
  {
    "text": "big enough that he thinks it might actually move the market so what he does is try and trade in front of that either",
    "start": "1326240",
    "end": "1332159"
  },
  {
    "text": "for his own Advantage maybe for one of his other clients Advantage if it's a sell order that he thinks is going to",
    "start": "1332159",
    "end": "1337320"
  },
  {
    "text": "drive the market down then maybe he'll try and execute a smaller sell order in front of it before the price drops and",
    "start": "1337320",
    "end": "1343400"
  },
  {
    "text": "then go ahead and push the sell order from the customer through and then do",
    "start": "1343400",
    "end": "1348600"
  },
  {
    "text": "whatever action he might want to do after that uh to realize profit or avoid a loss that's front running uh we also",
    "start": "1348600",
    "end": "1354919"
  },
  {
    "text": "have layering this is a very common technique that we see amongst our Market manipulation uh um surveillances they",
    "start": "1354919",
    "end": "1363600"
  },
  {
    "text": "they all follow a pretty similar model basically somebody will try and",
    "start": "1363600",
    "end": "1369080"
  },
  {
    "text": "establish an interest on one side of the market either the sell or the buy side",
    "start": "1369080",
    "end": "1374760"
  },
  {
    "text": "slightly away from the market so they'll put a large volume of orders out there to try and move the price like if",
    "start": "1374760",
    "end": "1380480"
  },
  {
    "text": "there's a lot of sell interest slightly away from the top of book then maybe the uh the buy will go down um so what they",
    "start": "1380480",
    "end": "1388120"
  },
  {
    "text": "do is they put false interest on one side of the market to try and trigger an event on the other side of the market",
    "start": "1388120",
    "end": "1393279"
  },
  {
    "text": "which is the one they actually plan on executing when that executes you'll see the entire interest that they had on the",
    "start": "1393279",
    "end": "1399799"
  },
  {
    "text": "other side of the book disappear so we have surveillances looking through all the volume across all the exchanges for",
    "start": "1399799",
    "end": "1406960"
  },
  {
    "text": "one or two firms out there trying to put some false interest orders they never actually intend to execute um and then",
    "start": "1406960",
    "end": "1414320"
  },
  {
    "text": "executing the other side of the market and then you'll see a pattern of that cancelling and the suddenly the interest goes",
    "start": "1414320",
    "end": "1420559"
  },
  {
    "text": "away another type of surveillance that we have to do across these billions of",
    "start": "1420559",
    "end": "1425679"
  },
  {
    "start": "1421000",
    "end": "1421000"
  },
  {
    "text": "Records is to make sure that when a customer places in order that it's actually executed at a fair price the",
    "start": "1425679",
    "end": "1431880"
  },
  {
    "text": "best price that's out there so we see that the uh the Top Line there that's the bid the bottom Line's the ass we",
    "start": "1431880",
    "end": "1438520"
  },
  {
    "text": "would expect that any customer order coming in should execute within those bands in this particular example we're",
    "start": "1438520",
    "end": "1445039"
  },
  {
    "text": "showing uh the diamond is the actual trade execution it's outside of those bands so for some reason the the broker",
    "start": "1445039",
    "end": "1451400"
  },
  {
    "text": "dealer executed it at a price that was not favorable to their actual customer presumably they have some way to to get",
    "start": "1451400",
    "end": "1457960"
  },
  {
    "text": "the margin and receive some benefit on their",
    "start": "1457960",
    "end": "1461679"
  },
  {
    "text": "own so another part of this whole picture is that when an exchange or or when a firm",
    "start": "1466919",
    "end": "1474240"
  },
  {
    "text": "has a a limit order it needs to go out and display at the top of the the book if it's the best price they have out",
    "start": "1474240",
    "end": "1479679"
  },
  {
    "text": "there um if they're an obligated Market maker then they have to show their full volume and the price out on the market",
    "start": "1479679",
    "end": "1487520"
  },
  {
    "text": "if that is then the best uh price across all the exchanges it should get",
    "start": "1487520",
    "end": "1493200"
  },
  {
    "text": "reflected in the Universal best bidden offer that's out there in the market and the purpose of this is just to show so",
    "start": "1493200",
    "end": "1499480"
  },
  {
    "text": "we have kind of an exchange view of the market and we have a universal view of the market and we have surveillances",
    "start": "1499480",
    "end": "1504840"
  },
  {
    "text": "trying to make sure that things are displayed so that everybody can know that that interest in that particular",
    "start": "1504840",
    "end": "1510880"
  },
  {
    "text": "stock is out there and has a fair chance to uh execute against it um and that it",
    "start": "1510880",
    "end": "1518200"
  },
  {
    "text": "it's also we need to make sure that they're handling within the exchange so what we end up with is a bunch of parallel time series events that we then",
    "start": "1518200",
    "end": "1526000"
  },
  {
    "text": "have to scan and walk kind of on a issue by isue sometimes firm by firm basis uh",
    "start": "1526000",
    "end": "1532240"
  },
  {
    "text": "in order to decide and determine if there was manipulation or if orders were",
    "start": "1532240",
    "end": "1537399"
  },
  {
    "text": "handled on a fair b or fairly for the rest of the market from the uh customer side this is",
    "start": "1537399",
    "end": "1545679"
  },
  {
    "text": "kind of an example of if they have the best bidden offer and they placed with their firm they would expect to go out",
    "start": "1545679",
    "end": "1552720"
  },
  {
    "text": "there and see it show up as the top of book in this instance they've placed a a",
    "start": "1552720",
    "end": "1558039"
  },
  {
    "text": "link order and we see that the top of the book does not reflect that so that would be a violation that we're trying",
    "start": "1558039",
    "end": "1564360"
  },
  {
    "text": "to search for so how did we used to do this we uh",
    "start": "1564360",
    "end": "1570559"
  },
  {
    "start": "1569000",
    "end": "1569000"
  },
  {
    "text": "used a primarily database Appliance Centric solution and it served us",
    "start": "1570559",
    "end": "1576760"
  },
  {
    "text": "reasonbly well for a few years um and we used to I think be on even an oracle",
    "start": "1576760",
    "end": "1582360"
  },
  {
    "text": "solution years before that we had two tiers of storage we had our",
    "start": "1582360",
    "end": "1589080"
  },
  {
    "text": "primary storage which we ran all of our surveillances against that was a database Appliance had a sliding window",
    "start": "1589080",
    "end": "1594679"
  },
  {
    "text": "of data available in time we ran on the order of 300 SQL jobs against this this",
    "start": "1594679",
    "end": "1601000"
  },
  {
    "text": "sliding window every day those were the actual surveillances that are going out and looking for the various non-compliant or Market Market",
    "start": "1601000",
    "end": "1607799"
  },
  {
    "text": "manipulative activity out there now that data would then uh go to the second tier",
    "start": "1607799",
    "end": "1614200"
  },
  {
    "text": "storage which was also uh Appliance oriented and that had a long longer retention period the primary use case",
    "start": "1614200",
    "end": "1620760"
  },
  {
    "text": "for that was end users now trying to investigate these events that our surveillance has kicked out and they",
    "start": "1620760",
    "end": "1627240"
  },
  {
    "text": "would need to go back and replay the market they would uh need to uh do audit Trails um analytics Diagnostics whatever",
    "start": "1627240",
    "end": "1634520"
  },
  {
    "text": "they may have to do to put a disposition on any particular event that our surveillances kick out that would go on",
    "start": "1634520",
    "end": "1641039"
  },
  {
    "text": "the two-year retention policy the surveillance window typically held about 20 days 20 Market days worth of data so",
    "start": "1641039",
    "end": "1647720"
  },
  {
    "text": "a month um this had a lot of pain points and then I guess aside that we had um a",
    "start": "1647720",
    "end": "1655159"
  },
  {
    "text": "disaster recovery site which we actually hosted on our Prim the production site was uh hosted in a Data Center and that",
    "start": "1655159",
    "end": "1661120"
  },
  {
    "text": "was basically a full bloom production mirror of what we had in production so",
    "start": "1661120",
    "end": "1666200"
  },
  {
    "text": "in the event of a disaster we're still obligated to run our surveillances we don't want to end up on the front page of the Wall Street Journal because we",
    "start": "1666200",
    "end": "1671760"
  },
  {
    "text": "had a disaster and we couldn't get up and running so we basically had to double pay for everything to keep mirrored infrastructures up and running",
    "start": "1671760",
    "end": "1679640"
  },
  {
    "start": "1679000",
    "end": "1679000"
  },
  {
    "text": "a lot of pain points in the Legacy uh architecture that when we chose as an",
    "start": "1679640",
    "end": "1685600"
  },
  {
    "text": "organization to migrate our Market regulation infrastructure to AWS and the",
    "start": "1685600",
    "end": "1690840"
  },
  {
    "text": "cloud we were targeting to resolve the Legacy architecture extremely expensive",
    "start": "1690840",
    "end": "1696279"
  },
  {
    "text": "to maintain it was on the uh seven digits and that's probably a little uh on the conservative side probably more",
    "start": "1696279",
    "end": "1702039"
  },
  {
    "text": "like eight digits a year to to keep that infrastructure up and running we would",
    "start": "1702039",
    "end": "1707399"
  },
  {
    "text": "have to requisition and size that in advance it was uh of the two layers of",
    "start": "1707399",
    "end": "1713360"
  },
  {
    "text": "storage one was a scale up architecture the other is had some potential to scale",
    "start": "1713360",
    "end": "1718880"
  },
  {
    "text": "out way you had to bring that Hardware in place stand it up you had the it was",
    "start": "1718880",
    "end": "1724640"
  },
  {
    "text": "a three-month process by the time you get purchase orders approved get it into your loading docks get it into your data centers get it up and running uh then",
    "start": "1724640",
    "end": "1731480"
  },
  {
    "text": "you have a not a data migration process and then over a weekend you try and flip all the applications over to this new",
    "start": "1731480",
    "end": "1738679"
  },
  {
    "text": "hardware and you cross your fingers that you're up and running on Monday course on Tuesday you don't want to be on the front page of the Wall Street Journal so",
    "start": "1738679",
    "end": "1745840"
  },
  {
    "text": "that also as Bob was talking about this same infrastructure had a tightly",
    "start": "1745840",
    "end": "1750880"
  },
  {
    "text": "coupled storage and compute so again if we think we're going to see volatility and we always see volatility in the",
    "start": "1750880",
    "end": "1757039"
  },
  {
    "text": "market where maybe a brexit happens Market volumes double from where they are today uh we have to be ready to",
    "start": "1757039",
    "end": "1763240"
  },
  {
    "text": "handle that so we're always looking out multiple years into the future trying to pre-b that infrastructure so that we're",
    "start": "1763240",
    "end": "1769600"
  },
  {
    "text": "ready for that in general we see Market events happen once or twice a year that",
    "start": "1769600",
    "end": "1774919"
  },
  {
    "text": "we literally double our previous high water marks in terms of the volume and we're on a constant uh slope upwards in",
    "start": "1774919",
    "end": "1782200"
  },
  {
    "text": "terms of the volume we get every single year because volumes are going up on the order of say 20% a year high frequency",
    "start": "1782200",
    "end": "1788679"
  },
  {
    "text": "trading more and more prevalent systems get faster and faster so it was a continuous struggle to keep up with that",
    "start": "1788679",
    "end": "1796559"
  },
  {
    "text": "demand it also didn't really support the industry movement we're slowly moving more and more towards real time in terms",
    "start": "1796559",
    "end": "1802919"
  },
  {
    "text": "of our our data feeds coming into our systems that was a very batch oriented",
    "start": "1802919",
    "end": "1808240"
  },
  {
    "text": "solution so really not going to take us to the Future um and was really kind of running out of legs in terms of",
    "start": "1808240",
    "end": "1814399"
  },
  {
    "text": "state-of-the-art infrastructure we have a lot of back processing and reprocessing request",
    "start": "1814399",
    "end": "1821360"
  },
  {
    "text": "either we get data issues that come in and require us to rerun surveillances because Upstream data was bad for some",
    "start": "1821360",
    "end": "1827320"
  },
  {
    "text": "time period or we have a a defect in one of our surveillances and we have to go run it back to make sure we didn't miss",
    "start": "1827320",
    "end": "1832919"
  },
  {
    "text": "anything these reruns can sometimes span months uh half year whatever the only",
    "start": "1832919",
    "end": "1838720"
  },
  {
    "text": "way to do that on the Legacy architecture was to provision some extra capacity and if a lot of these things",
    "start": "1838720",
    "end": "1845880"
  },
  {
    "text": "hit us at once then we would have to carve out that excess capacity and time slice stuff through the actual",
    "start": "1845880",
    "end": "1852720"
  },
  {
    "text": "surveillance platform only had 30 days of data in it so we would actually have to roll replay data from our um",
    "start": "1852720",
    "end": "1860600"
  },
  {
    "text": "long-term storage through that infrastructure and run it could take months to actually replay or rerun",
    "start": "1860600",
    "end": "1867080"
  },
  {
    "text": "surveillances in this old architecture uh that's was particularly problematic because the business wants to know",
    "start": "1867080",
    "end": "1873080"
  },
  {
    "text": "pretty much so right away if there was a problem in the data or if there's a problem in our surveillance what was the impact what did we miss that type of",
    "start": "1873080",
    "end": "1879639"
  },
  {
    "text": "thing and we also had no ability to fine tune our compute model and our storage model once we purchased it so we're",
    "start": "1879639",
    "end": "1886200"
  },
  {
    "text": "basically projecting years in advance what our compute needs in terms of do we need to be IO optimized or memory",
    "start": "1886200",
    "end": "1892760"
  },
  {
    "text": "optimized and there was certainly no way on individual batch jobs to optimize on a per job um",
    "start": "1892760",
    "end": "1901158"
  },
  {
    "start": "1901000",
    "end": "1901000"
  },
  {
    "text": "basis so obviously we landed on AWS and Spark for a handful of our most complex",
    "start": "1901519",
    "end": "1908480"
  },
  {
    "text": "surveillances uh that's what we're here talking about today the ones that are really focused on the the time ordered",
    "start": "1908480",
    "end": "1914200"
  },
  {
    "text": "events um we looked at other things too we considered Java map reduce we kind of",
    "start": "1914200",
    "end": "1920960"
  },
  {
    "text": "did not go that route because it was hard to maintain one of the things we wanted to get out of this redesign was",
    "start": "1920960",
    "end": "1927480"
  },
  {
    "text": "the ability to debug and test our code which we really struggled with in a SQL",
    "start": "1927480",
    "end": "1933200"
  },
  {
    "text": "oriented uh world when you have billions of Records you're running these jobs making a lot of temp tables and there's",
    "start": "1933200",
    "end": "1938440"
  },
  {
    "text": "something wrong it's it's hard to step through that and figure out exactly where something blew up on you uh we",
    "start": "1938440",
    "end": "1945120"
  },
  {
    "text": "looked at Apache draft to help s solve our order life cycle problem it was really immature at that particular time",
    "start": "1945120",
    "end": "1951880"
  },
  {
    "text": "so we decided that it might have some potential down the road but we didn't go that route and we landed on uh running",
    "start": "1951880",
    "end": "1958120"
  },
  {
    "text": "AWS spark on EMR we have the portfolio of our surveillances that are very um",
    "start": "1958120",
    "end": "1965000"
  },
  {
    "text": "compliant or very good fits for set-based Solutions we actually migrated them in Hive just because SQL tosql made",
    "start": "1965000",
    "end": "1972519"
  },
  {
    "text": "it less cost prohibitive to do this whole migration the ones that were particularly performant",
    "start": "1972519",
    "end": "1978480"
  },
  {
    "text": "uh had high performance needs those we targeted for spark which when we started this effort I guess it was pushing three",
    "start": "1978480",
    "end": "1984519"
  },
  {
    "text": "years ago no ways near as mature as it was now so it was kind of a bit of an R&D project for these uh these handful",
    "start": "1984519",
    "end": "1990360"
  },
  {
    "text": "of surveillances that that really had the the highest constraints on compute and",
    "start": "1990360",
    "end": "1996360"
  },
  {
    "text": "memory so this is kind of the an illustration of the problem that would really blow us up on on the",
    "start": "1996480",
    "end": "2003039"
  },
  {
    "start": "1998000",
    "end": "1998000"
  },
  {
    "text": "surveillances we targeted for spark we have a lot of quotes on a lot",
    "start": "2003039",
    "end": "2008519"
  },
  {
    "text": "of different venues and a couple of order orders of magnitude larger than the number of Trades the trades tend to",
    "start": "2008519",
    "end": "2014760"
  },
  {
    "text": "be the trigger events for something it's interested in when somebody trades that's the opportunity to make money",
    "start": "2014760",
    "end": "2020320"
  },
  {
    "text": "whether that was uh fair or by manipulation so we need to put these",
    "start": "2020320",
    "end": "2026159"
  },
  {
    "text": "build these time series events across uh firms issues large partitions",
    "start": "2026159",
    "end": "2033559"
  },
  {
    "text": "we would get cartisian products in the old SQL based solution and that we",
    "start": "2033559",
    "end": "2039039"
  },
  {
    "text": "managed to work through except that we would have hotpots on some days where there was a lot of interest in say",
    "start": "2039039",
    "end": "2044639"
  },
  {
    "text": "Microsoft and then for one day in a in a whole and actually only a small time",
    "start": "2044639",
    "end": "2049800"
  },
  {
    "text": "slice of period we couldn't actually run our surveillance so we'd have to do some sort of oneoff code drop to actually",
    "start": "2049800",
    "end": "2056599"
  },
  {
    "text": "Target uh that specific time slice to get the surveillance through on just one issue and when you're surveilling across",
    "start": "2056599",
    "end": "2063878"
  },
  {
    "text": "thousands of issues you really don't want to be doing oneoff uh code drops even if it only happen happens once or so a year so moving to a more procedural",
    "start": "2063879",
    "end": "2071480"
  },
  {
    "text": "language in spark we're able to to turn this more into a union the data sort the",
    "start": "2071480",
    "end": "2077280"
  },
  {
    "text": "data and then walk the data we could keep multiple uh states of the various",
    "start": "2077280",
    "end": "2083560"
  },
  {
    "text": "exchanges in memory at once and walk them for say a given issue and keep",
    "start": "2083560",
    "end": "2089079"
  },
  {
    "text": "track of what the market looked like uh and if it were streaming data we would do the exact same thing so we're kind of",
    "start": "2089079",
    "end": "2095200"
  },
  {
    "text": "ready for a streaming realtime world but we could keep a state of the market in memory uh and and do comparisons across",
    "start": "2095200",
    "end": "2102760"
  },
  {
    "text": "the exchanges to make sure that orders are being displayed correctly when they come in uh and that they're being",
    "start": "2102760",
    "end": "2108560"
  },
  {
    "text": "executed at a fair",
    "start": "2108560",
    "end": "2111720"
  },
  {
    "text": "price so here's an example of the architecture we used uh we feel that it",
    "start": "2114880",
    "end": "2121599"
  },
  {
    "start": "2115000",
    "end": "2115000"
  },
  {
    "text": "aderes to a lot of the best practices that Bob went over in his his part of the presentation uh Amazon S3 is the",
    "start": "2121599",
    "end": "2128440"
  },
  {
    "text": "primary storage a real GameChanger for finra in terms of the the market regulation space we now have one single",
    "start": "2128440",
    "end": "2136560"
  },
  {
    "text": "gold source of all our data and for as long as we're willing to pay we can shrink that we can grow that we can push",
    "start": "2136560",
    "end": "2142280"
  },
  {
    "text": "it off the glacier we can pull it back easily on demand we have it there it's cataloged it's versioned so if we have",
    "start": "2142280",
    "end": "2150200"
  },
  {
    "text": "something where we need to replay the data it's out there if we have an instance or an example of when we have",
    "start": "2150200",
    "end": "2155560"
  },
  {
    "text": "to actually go pull the data what did it look like at this specific instant in time because I'm building a court case",
    "start": "2155560",
    "end": "2162839"
  },
  {
    "text": "it's 2 years old the data is drifted slightly in two years we can actually go back and give our users exactly what the",
    "start": "2162839",
    "end": "2169319"
  },
  {
    "text": "data looked like when the surveillance ran so we don't get some case kicked out",
    "start": "2169319",
    "end": "2174520"
  },
  {
    "text": "of court because we can't reproduce the results and then you know somehow that that brings doubt into the",
    "start": "2174520",
    "end": "2181200"
  },
  {
    "text": "picture uh on the right we have an example of a very large surveillance",
    "start": "2181200",
    "end": "2186280"
  },
  {
    "text": "that runs has multiple steps of first one where we tend to pull the data off of S3 uh the second one where we're",
    "start": "2186280",
    "end": "2191800"
  },
  {
    "text": "they're ordering it um by time and across multiple Dimensions that tends to",
    "start": "2191800",
    "end": "2197319"
  },
  {
    "text": "be more of a a memory intensive step and then the final one we're actually crunching and doing the the hard",
    "start": "2197319",
    "end": "2203720"
  },
  {
    "text": "analytics on the data trying to find the violations that's much more compute intensive with this new architecture as",
    "start": "2203720",
    "end": "2210240"
  },
  {
    "text": "Bob was stating we can actually break down our surveillance into multiple steps and choose different cluster types",
    "start": "2210240",
    "end": "2217359"
  },
  {
    "text": "what actually spin up on EMR a different cluster type optimized for the constraints and resources of that",
    "start": "2217359",
    "end": "2223040"
  },
  {
    "text": "particular step in their surveillance um so here we're showing an example where we go IO and then we go memory and then",
    "start": "2223040",
    "end": "2230680"
  },
  {
    "text": "we move on to compute every step along the way we can",
    "start": "2230680",
    "end": "2235839"
  },
  {
    "text": "persist then back to S3 the intermediate results this has a lot of value to us for multiple reasons some of these some",
    "start": "2235839",
    "end": "2241599"
  },
  {
    "text": "of these surveillances can run for hours and hours the last thing you want to do especially if you're trying to leverage",
    "start": "2241599",
    "end": "2246880"
  },
  {
    "text": "spot price which might not be as stable um is to have to rerun because the job",
    "start": "2246880",
    "end": "2252240"
  },
  {
    "text": "crashes or something goes wrong in the middle from the beginning by persisting out to S3 the intermediate steps we have",
    "start": "2252240",
    "end": "2259440"
  },
  {
    "text": "a restart a point of failure more or less whenever we want it also brings the",
    "start": "2259440",
    "end": "2265440"
  },
  {
    "text": "the advantage of being able to do analysis on the data either uh short",
    "start": "2265440",
    "end": "2270800"
  },
  {
    "text": "term during the development cycle on the intermediate steps at a much more granular level in a much larger time",
    "start": "2270800",
    "end": "2277640"
  },
  {
    "text": "frame than we could on the old Appliance oriented architecture we use this for multiple reasons it helps us debug",
    "start": "2277640",
    "end": "2284599"
  },
  {
    "text": "things it helps us test things if the users want to uh do some sort of",
    "start": "2284599",
    "end": "2290440"
  },
  {
    "text": "enhancement we can actually go and look at some of the intermediate steps and provide them output that we couldn't do",
    "start": "2290440",
    "end": "2296640"
  },
  {
    "text": "before like if we do this enhancement it can have this impact and this is based on three months of historical data that",
    "start": "2296640",
    "end": "2301880"
  },
  {
    "text": "we're now actually able to store as long or short as we want um and that's a",
    "start": "2301880",
    "end": "2307640"
  },
  {
    "text": "provides a big benefit in terms of our development cycle to the end users and in terms of our resiliency restart at",
    "start": "2307640",
    "end": "2314319"
  },
  {
    "text": "the point of failure um and and our ability to to test things",
    "start": "2314319",
    "end": "2320680"
  },
  {
    "text": "and give to our testers here's some of the intermediate States and what we think they should look like as we go",
    "start": "2320680",
    "end": "2326599"
  },
  {
    "start": "2329000",
    "end": "2329000"
  },
  {
    "text": "along so I mentioned spot a little uh earlier so spot has the the potential to",
    "start": "2329240",
    "end": "2336760"
  },
  {
    "text": "save us a ton of money when we run this stuff um spot prices fluctuate we run a",
    "start": "2336760",
    "end": "2342839"
  },
  {
    "text": "lot of our stuff still in batch so we can actually monitor spot pricing if we want and we can go out and pick times of",
    "start": "2342839",
    "end": "2349079"
  },
  {
    "text": "the day where instance types are cheaper we can run then um as we showed in the",
    "start": "2349079",
    "end": "2355960"
  },
  {
    "text": "previous slide we can go out and pick instance types that are optimized towards the particular step in the",
    "start": "2355960",
    "end": "2363240"
  },
  {
    "text": "process that we're running and we can",
    "start": "2363240",
    "end": "2368880"
  },
  {
    "text": "also achieve uh either cheaper cost when",
    "start": "2368880",
    "end": "2374359"
  },
  {
    "text": "we run or if we have a need to make sure that we get it done more quickly and that there's no loss of spot nodes for",
    "start": "2374359",
    "end": "2381480"
  },
  {
    "text": "whatever reason we can go on demand if we want to too so we have a lot of flexibility now that we've uh gone to",
    "start": "2381480",
    "end": "2387920"
  },
  {
    "text": "AWS running these in spark in terms of what trading off what we want to pay",
    "start": "2387920",
    "end": "2393560"
  },
  {
    "text": "with how fast we need it done that type of thing um it's been a a real game Cher",
    "start": "2393560",
    "end": "2399880"
  },
  {
    "text": "with infin um for allowing us to reprocess uh keep up with Market demands",
    "start": "2399880",
    "end": "2405760"
  },
  {
    "text": "keep up with uh boost demands and Amazon continuously comes out with richer and",
    "start": "2405760",
    "end": "2412079"
  },
  {
    "text": "richer tool sets to automate more and more of this in terms of Auto configuration Etc so we've uh been able",
    "start": "2412079",
    "end": "2419520"
  },
  {
    "text": "to play with that and experiment with their tool sets uh this this also allows us to experiment wh if scenarios on De",
    "start": "2419520",
    "end": "2427040"
  },
  {
    "text": "boxes because now they they can just go out and spin up a cluster and play with scenarios during a death cycle or if",
    "start": "2427040",
    "end": "2432319"
  },
  {
    "text": "they're looking for ways to optimize on compute they can try it out um without trying to steal time from production",
    "start": "2432319",
    "end": "2439920"
  },
  {
    "text": "servers so overall what benefits did we realize from this migration effort uh",
    "start": "2441839",
    "end": "2449760"
  },
  {
    "text": "and we had a lot of them outlined earlier that we put forth in our business case and across the board I",
    "start": "2449760",
    "end": "2456359"
  },
  {
    "text": "think we were able to deliver on them we had an order of magnitude Savings in in",
    "start": "2456359",
    "end": "2462400"
  },
  {
    "text": "uh particularly on the ones that were running on spark in terms of the uh old on Prem design versus hosted AWS running",
    "start": "2462400",
    "end": "2470480"
  },
  {
    "text": "on spark our response time has dropped dramatically we're no longer trying to",
    "start": "2470480",
    "end": "2477040"
  },
  {
    "text": "do rerun request um analysis requests SEC might come in and say run this ad",
    "start": "2477040",
    "end": "2482319"
  },
  {
    "text": "hoc request we're no longer having to cue that stuff up in spare capacity that may may not be there uh depending on",
    "start": "2482319",
    "end": "2489200"
  },
  {
    "text": "Market volumes and system maintenance we can spend stuff up on demand it's now just a cost number it's a business",
    "start": "2489200",
    "end": "2494680"
  },
  {
    "text": "decision to get through through that things it used to take months to turn around and give to the business we can",
    "start": "2494680",
    "end": "2501240"
  },
  {
    "text": "now turn around in days or weeks we can actually feed them results faster than they can process it scaling I think we",
    "start": "2501240",
    "end": "2508400"
  },
  {
    "text": "all know scaling in its architecture uh it just takes that almost out of the",
    "start": "2508400",
    "end": "2513560"
  },
  {
    "text": "equation uh we can play with different methods of scaling we can scale up the as large as we want uh we can scale uh",
    "start": "2513560",
    "end": "2520520"
  },
  {
    "text": "CPU intensive nodes memory intensive all that we' managed to realize uh in and its Spades so expected future benefits I",
    "start": "2520520",
    "end": "2529839"
  },
  {
    "text": "mentioned uh particularly with the spark solution we expect to be able to support",
    "start": "2529839",
    "end": "2535040"
  },
  {
    "text": "the move towards more and more real-time data as more data starts coming in from exchanges as more data starts coming in",
    "start": "2535040",
    "end": "2541480"
  },
  {
    "text": "from the broker dealers real time we're going to be able to to adapt to that and build a more streaming uh surveillance",
    "start": "2541480",
    "end": "2547839"
  },
  {
    "text": "program in spark where we really just couldn't do that in in the SQL World running on the database appliances that",
    "start": "2547839",
    "end": "2553680"
  },
  {
    "text": "was really a batch oriented architecture I mentioned experimentation",
    "start": "2553680",
    "end": "2558760"
  },
  {
    "text": "with uh new instance types uh we can experiment with ways to save money and",
    "start": "2558760",
    "end": "2565240"
  },
  {
    "text": "we're looking at converting a lot of that high portfolio that we just pushed up to the cloud as is to spark for a few",
    "start": "2565240",
    "end": "2571800"
  },
  {
    "text": "various reasons um one is that on this slide you'll see that uh we we actually",
    "start": "2571800",
    "end": "2577359"
  },
  {
    "text": "held a competition for ways to save money on the AWS spark infrastructure",
    "start": "2577359",
    "end": "2583119"
  },
  {
    "text": "and people played with just porting the uh Hive surveillances as is straight to",
    "start": "2583119",
    "end": "2588240"
  },
  {
    "text": "spark ql with as uh few changes as possible so a lot of them looks like they might be a 2X cost savings just",
    "start": "2588240",
    "end": "2594680"
  },
  {
    "text": "doing that uh other things we're looking to do is migrate to the latest versions of",
    "start": "2594680",
    "end": "2600359"
  },
  {
    "text": "spark uh take advantage of the latest apis as they come out and I think I already mentioned a real time",
    "start": "2600359",
    "end": "2607839"
  },
  {
    "text": "um adaptation of our surveillance",
    "start": "2607839",
    "end": "2611760"
  },
  {
    "text": "portfolio so that's the high level like to thank you guys again for coming out",
    "start": "2614079",
    "end": "2619200"
  },
  {
    "text": "on this Friday morning after a long week uh we will be we're here now to take any",
    "start": "2619200",
    "end": "2625640"
  },
  {
    "text": "more technical questions or any more in-depth questions uh please remember to complete your evaluations and if you",
    "start": "2625640",
    "end": "2632040"
  },
  {
    "text": "would like to have any more information unfortunately we're at the end of the entire uh week here these",
    "start": "2632040",
    "end": "2638880"
  },
  {
    "start": "2633000",
    "end": "2633000"
  },
  {
    "text": "presentations have already been done but they should be out there on the uh reinvent site if you're looking for more",
    "start": "2638880",
    "end": "2644440"
  },
  {
    "text": "detail deep dives into some of the other relevant portions of our architecture uh",
    "start": "2644440",
    "end": "2649520"
  },
  {
    "text": "some particular ones that are interesting that we've done talks on earlier in the week building a secure",
    "start": "2649520",
    "end": "2655400"
  },
  {
    "text": "data science platform that is focused specifically around spark and we're",
    "start": "2655400",
    "end": "2660640"
  },
  {
    "text": "looking at third party uh providers to basically provide the end users whether",
    "start": "2660640",
    "end": "2665839"
  },
  {
    "text": "they be data scientist or analyst the ability to stand up clusters in the cloud and be provisioned with all the",
    "start": "2665839",
    "end": "2671760"
  },
  {
    "text": "tools they need without having to be a technologist uh so just to be able at the go to a a front end at the click of",
    "start": "2671760",
    "end": "2677480"
  },
  {
    "text": "a button stand up in provision the size of cluster to do whatever analytics and data science they want to with the tool",
    "start": "2677480",
    "end": "2683839"
  },
  {
    "text": "sets that are standardized in her company already there U and Spark being",
    "start": "2683839",
    "end": "2688920"
  },
  {
    "text": "a prime one there um with some of the machine learning stuff coming in at the click of a button so we're in the the",
    "start": "2688920",
    "end": "2695520"
  },
  {
    "text": "middle phases of rolling that out uh the finer and the cloud Big Data Enterprise is kind of an Enterprise",
    "start": "2695520",
    "end": "2702040"
  },
  {
    "text": "overview of our whole migration effort for those of you that are might be migrating to the cloud that one's worth going to see kind of Lessons Learned and",
    "start": "2702040",
    "end": "2708280"
  },
  {
    "text": "best practices for migrating a entire infrastructure to the cloud and the data Lake for big data on Amazon S3 as I",
    "start": "2708280",
    "end": "2716119"
  },
  {
    "text": "mentioned earlier a lot of the value ad here came from this gold source of data",
    "start": "2716119",
    "end": "2721200"
  },
  {
    "text": "that is those that is versioned um and we can get it at a point in time and",
    "start": "2721200",
    "end": "2726559"
  },
  {
    "text": "it's c cataloged so everybody that reads or writes to the lake the information is",
    "start": "2726559",
    "end": "2731920"
  },
  {
    "text": "catalog with a lot of metadata we have versions We can pull whatever we want any instance in time that might be a presentation you're interested relevant",
    "start": "2731920",
    "end": "2738040"
  },
  {
    "text": "to this as well",
    "start": "2738040",
    "end": "2741200"
  }
]