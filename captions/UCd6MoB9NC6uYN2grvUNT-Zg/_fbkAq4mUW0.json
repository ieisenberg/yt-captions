[
  {
    "text": "welcome everyone um my name is Derek Pi uh product manager for autoscaling",
    "start": "4000",
    "end": "9160"
  },
  {
    "text": "thanks for coming to um CPN 201 more nines for your dimes uh thanks for",
    "start": "9160",
    "end": "15559"
  },
  {
    "text": "sticking around this late in the conference I know I'm uh between hangover and lunch for you guys so uh",
    "start": "15559",
    "end": "23320"
  },
  {
    "text": "we'll try to make this interesting and worthwhile um so the uh topic of the",
    "start": "23320",
    "end": "28480"
  },
  {
    "text": "talk today um is just about how a few customers like you use autoscaling not only to scale their applications but",
    "start": "28480",
    "end": "35320"
  },
  {
    "text": "also to automate provisioning uh improve availability um and save a few bucks uh",
    "start": "35320",
    "end": "41280"
  },
  {
    "text": "so we've got uh four speakers uh four customers you can see who they are right here um so I won't waste too much of",
    "start": "41280",
    "end": "48039"
  },
  {
    "text": "your time with a uh with the sales pitch um we'll get started with Cameron from",
    "start": "48039",
    "end": "53239"
  },
  {
    "text": "The Weather [Applause]",
    "start": "53239",
    "end": "58920"
  },
  {
    "text": "Channel morning everyone uh my name is Cameron stes with the Weather Channel um before",
    "start": "58920",
    "end": "65000"
  },
  {
    "text": "I actually change slides I want to want to I have a problem with unexpected weather events we're we're pretty good",
    "start": "65000",
    "end": "71439"
  },
  {
    "text": "at forecasting um so uh yeah so um obviously most of",
    "start": "71439",
    "end": "80119"
  },
  {
    "text": "you probably know us as The Weather Channel we have our our TV station we have our weather.com website mobile apps",
    "start": "80119",
    "end": "86119"
  },
  {
    "text": "Etc um we're also uh Wonderground ground um and then lesser known are uh",
    "start": "86119",
    "end": "93200"
  },
  {
    "text": "our other companies WSI and weather central who focus more on uh business",
    "start": "93200",
    "end": "98360"
  },
  {
    "text": "services for the weather um since we all like to to talk",
    "start": "98360",
    "end": "104200"
  },
  {
    "text": "numbers um we're a top 30 web web property in the US second most viewed",
    "start": "104200",
    "end": "109880"
  },
  {
    "text": "television channel in the US uh just behind ESPN um we uh provide weather",
    "start": "109880",
    "end": "116560"
  },
  {
    "text": "forecasting and weather data to 85% of US Airlines we cover about 50,000",
    "start": "116560",
    "end": "122159"
  },
  {
    "text": "flights a day and in providing them weather uh weather data and forecasts and we're getting into a space",
    "start": "122159",
    "end": "128920"
  },
  {
    "text": "where we are providing um weather data to Major retailers so that they can forecast their spend and demand and um",
    "start": "128920",
    "end": "137040"
  },
  {
    "text": "adjust their sales and and marketing and so on based on",
    "start": "137040",
    "end": "142000"
  },
  {
    "text": "weather um and then we have an unduplicated audience of 163 million uh visitors across weather.com and the",
    "start": "143040",
    "end": "149400"
  },
  {
    "text": "Weather Chann uh television channel so before I get to the auto",
    "start": "149400",
    "end": "156200"
  },
  {
    "text": "scaling part I wanted to touch on how we how we ended up at at AWS at Amazon web services um specifically for this this",
    "start": "156200",
    "end": "162879"
  },
  {
    "text": "application case um so if we roll back about a year ago it's uh hurricane season",
    "start": "162879",
    "end": "168560"
  },
  {
    "text": "2012 um we're capacity constrained we have areas of of uh data centers we have applications that are constrained on",
    "start": "168560",
    "end": "174959"
  },
  {
    "text": "capacity and we're leading up into a busy season for us so we're looking for",
    "start": "174959",
    "end": "180120"
  },
  {
    "text": "options we're looking to see what we can do to um to get out of that",
    "start": "180120",
    "end": "186480"
  },
  {
    "text": "situation so we looked around at the applications and we identified a a mobile workload so a workload that that",
    "start": "186480",
    "end": "192959"
  },
  {
    "text": "we could easily um you know shift and lift or forklift to a new environment um",
    "start": "192959",
    "end": "198480"
  },
  {
    "text": "that ended up being the radar image application for Wonderground domcom it was a uh an an ideal um workload for",
    "start": "198480",
    "end": "206840"
  },
  {
    "text": "this scenario it was a self-contained application so the the website itself",
    "start": "206840",
    "end": "212959"
  },
  {
    "text": "was separate from the radar generation the actual image Generation Um and it",
    "start": "212959",
    "end": "218080"
  },
  {
    "text": "was easy to replicate the data store so it was very decoupled um from any of the",
    "start": "218080",
    "end": "223239"
  },
  {
    "text": "other systems within the within the data centers within our traditional data center um and easy to to pick up and",
    "start": "223239",
    "end": "231720"
  },
  {
    "text": "move um so before I get into why autoscaling is a good uh a good case for",
    "start": "233879",
    "end": "239680"
  },
  {
    "text": "for us who who all is using Autos scaling in the room okay so maybe a third",
    "start": "239680",
    "end": "248400"
  },
  {
    "text": "right so for us we have um on a short timeline the the typical typical um you",
    "start": "248400",
    "end": "255000"
  },
  {
    "text": "know trending uh trending pattern uh people wake up in the morning check the weather before they head out to see what",
    "start": "255000",
    "end": "260759"
  },
  {
    "text": "they should wear see what they're going to do that day um they you know have lunch so we get a little dip over lunch",
    "start": "260759",
    "end": "266600"
  },
  {
    "text": "and then into the evening um people stop wearing about the weather so much um again this is a pretty pretty typical",
    "start": "266600",
    "end": "273600"
  },
  {
    "text": "you know day-to-day traffic pattern works very well for Autos scaling we can scale up to to uh to meet the Peaks and",
    "start": "273600",
    "end": "279919"
  },
  {
    "text": "scale down in The Valleys on a longer timeline our um our",
    "start": "279919",
    "end": "285600"
  },
  {
    "text": "traffic pattern may look fairly flat um we have some some small blips in there some small lulls um so you know over",
    "start": "285600",
    "end": "293360"
  },
  {
    "text": "weeks and months it it can still be pretty flat until we get a weather event",
    "start": "293360",
    "end": "299440"
  },
  {
    "text": "um um and this is where obviously you know a storm something's happening that that causes um a huge surge in traffic",
    "start": "299440",
    "end": "306600"
  },
  {
    "text": "for us and for us this was Hurricane Sandy of last year on a on from one day",
    "start": "306600",
    "end": "312960"
  },
  {
    "text": "to the next this was a th% increase in traffic to our",
    "start": "312960",
    "end": "317840"
  },
  {
    "text": "platform so in moving the application to to AWS we have a pretty basic configuration",
    "start": "320240",
    "end": "327759"
  },
  {
    "text": "um we have all our data is still um or the data for this application was still in the traditional data center so we",
    "start": "327759",
    "end": "334120"
  },
  {
    "text": "needed to to push that data uh store out to uh this new environment so we have uh",
    "start": "334120",
    "end": "340720"
  },
  {
    "text": "a data Bridge a distribution queue pushing the data out to data nodes um",
    "start": "340720",
    "end": "345759"
  },
  {
    "text": "across availability zones we have the edge nodes or front end nodes that are actually doing the the um image",
    "start": "345759",
    "end": "352600"
  },
  {
    "text": "processing so actually building the images generating the images for the websites and then an elb on top of that",
    "start": "352600",
    "end": "359000"
  },
  {
    "text": "so The Edge node the the middle tier there is what's actually autoscaled um and the data node stays",
    "start": "359000",
    "end": "366800"
  },
  {
    "text": "stays static so this is a this is a chart of",
    "start": "366800",
    "end": "374120"
  },
  {
    "text": "out of cloud watch showing um CPU utilization and then host count of this environment so the top line is is CPU",
    "start": "374120",
    "end": "382160"
  },
  {
    "text": "bottom line is host count and in in moving this application out to Amazon we found that um based on",
    "start": "382160",
    "end": "389919"
  },
  {
    "text": "the traffic pattern based on uh um performance profile of the application",
    "start": "389919",
    "end": "395680"
  },
  {
    "text": "that a good good band of utilization for us to focus in or or or aim for was um a",
    "start": "395680",
    "end": "403280"
  },
  {
    "text": "a low of 65% utilization and a high of 80% um we could keep the um at the 65%",
    "start": "403280",
    "end": "411400"
  },
  {
    "text": "CPD utilization level performance was still uh was still fine was still adequate um so we could keep the cluster",
    "start": "411400",
    "end": "417759"
  },
  {
    "text": "actually running somewhat hot as we scale down and and save on costs um but",
    "start": "417759",
    "end": "423160"
  },
  {
    "text": "if we exceeded 80% we we kind of saw that as our high water mark where we needed to start to scale up either",
    "start": "423160",
    "end": "428840"
  },
  {
    "text": "performance would start to degrade or we just knew that um you know the uh",
    "start": "428840",
    "end": "434160"
  },
  {
    "text": "overall scale of the cluster would be constrained in terms of how we actually",
    "start": "434160",
    "end": "440120"
  },
  {
    "text": "scale um we scale up very quickly so we actually do a percentage based scaling",
    "start": "440120",
    "end": "446000"
  },
  {
    "text": "based on um that high threshold of 80% so if we exceed 80% utilization for a uh",
    "start": "446000",
    "end": "453919"
  },
  {
    "text": "60-second period we will actually scale up um 20% of the cluster size so adding",
    "start": "453919",
    "end": "459639"
  },
  {
    "text": "a a large set of servers um into the cluster if we drop below 65% for uh a 5",
    "start": "459639",
    "end": "468520"
  },
  {
    "text": "minute period we will start to scale down in in very small steps so um you",
    "start": "468520",
    "end": "474120"
  },
  {
    "text": "can see on the chart there those events and then as we go into the later in the day you can see it'll actually scale",
    "start": "474120",
    "end": "479159"
  },
  {
    "text": "back back up",
    "start": "479159",
    "end": "481639"
  },
  {
    "text": "again so that actually works really well for us again we we really honed in on that uh that utilization range um we get",
    "start": "484479",
    "end": "491560"
  },
  {
    "text": "good performance of the application so response time is still um still very good for uh for our end users um and",
    "start": "491560",
    "end": "499199"
  },
  {
    "text": "that really gives us good uh a good kind of price point to focus on in terms of CPU usage for the",
    "start": "499199",
    "end": "505039"
  },
  {
    "text": "cluster but we do have some challenges and you probably will to so first off your monting tools probably",
    "start": "505039",
    "end": "512399"
  },
  {
    "text": "suck um and they probably suck for a traditional kind of static environment and they're really going to",
    "start": "512399",
    "end": "518640"
  },
  {
    "text": "suck in a dynamic environment um as as nodes as hosts uh you know come up",
    "start": "518640",
    "end": "526000"
  },
  {
    "text": "dynamically as they as they are uh removed dynamically um that's challenging for um most monting tools to",
    "start": "526000",
    "end": "533519"
  },
  {
    "text": "really uh uh either do host registration or or track metrics add alerts add",
    "start": "533519",
    "end": "540000"
  },
  {
    "text": "alarms and so on for all those Dynamic uh Dynamic hosts so um leverage Cloud watch as much as",
    "start": "540000",
    "end": "546760"
  },
  {
    "text": "you can and look for uh modern ring tools that um that can adapt to uh",
    "start": "546760",
    "end": "553720"
  },
  {
    "text": "Dynamic environment rolling out application",
    "start": "553720",
    "end": "559079"
  },
  {
    "text": "updates is uh is um a bit cumbersome um there's a lot of uh pieces moving pieces",
    "start": "559079",
    "end": "566200"
  },
  {
    "text": "to um Auto scaling there's groups launch configuration alarms policies and so on",
    "start": "566200",
    "end": "571640"
  },
  {
    "text": "launch configurations are where you actually Define um you know your application data so you you define your",
    "start": "571640",
    "end": "577519"
  },
  {
    "text": "machine image um user data tags Etc um to roll out a new version you create a",
    "start": "577519",
    "end": "583200"
  },
  {
    "text": "new launch configuration with your new machine image and and um those other config points but actually changing that",
    "start": "583200",
    "end": "590640"
  },
  {
    "text": "creating a new launch configuration changing that onto your Autos scan CBE doesn't actually change your your environment um there's no uh there's no",
    "start": "590640",
    "end": "599120"
  },
  {
    "text": "you know kickoff of of rolling out your update automatically so a couple different options you can basically wait",
    "start": "599120",
    "end": "605040"
  },
  {
    "text": "for uh Auto scaling to happen and and roll in new instances with the new launch configurations and roll out the",
    "start": "605040",
    "end": "610680"
  },
  {
    "text": "old ones um that's a policy that can actually or a strategy that can actually be uh tweaked within autoscaling or you",
    "start": "610680",
    "end": "617320"
  },
  {
    "text": "can actually um you know intentionally go in and and roll out or remove old instances yourself um that's the the",
    "start": "617320",
    "end": "624160"
  },
  {
    "text": "strategy that we're currently doing so as we uh create a a you know version 4.1 of our launch config um we would",
    "start": "624160",
    "end": "632040"
  },
  {
    "text": "actually uh go out and just one by one kill off uh the old instances another strategy another",
    "start": "632040",
    "end": "638839"
  },
  {
    "text": "option that we're looking at um is actually standing up in a a completely new Autos scaling group um with that new",
    "start": "638839",
    "end": "645480"
  },
  {
    "text": "configuration scaling it up to the current Uh current capacity of the old group and then shutting down the old",
    "start": "645480",
    "end": "651480"
  },
  {
    "text": "group um we haven't quite gotten there yet this this isn't so cumbersome that we've had to put the effort into that",
    "start": "651480",
    "end": "657720"
  },
  {
    "text": "but that's the uh that's the direction we're going to be heading um",
    "start": "657720",
    "end": "664560"
  },
  {
    "text": "so this isn't uh this isn't necessarily an autoscaling issue but it is it is uh",
    "start": "664560",
    "end": "669880"
  },
  {
    "text": "autoscaling again put some um put some burden on you in in uh managing this",
    "start": "669880",
    "end": "675519"
  },
  {
    "text": "cluster managing your Fleet across a non- scaling group so for us when um when a Zone failure happens and it's not",
    "start": "675519",
    "end": "682240"
  },
  {
    "text": "necessarily an Amazon having an issue but it could be our data node having an issue um we get to this uh",
    "start": "682240",
    "end": "689800"
  },
  {
    "text": "we basically Cascade that failure into our other zones as one zone comes is is",
    "start": "689800",
    "end": "694880"
  },
  {
    "text": "essentially dead traffic will redistribute across the other ones and then takes those down with them um",
    "start": "694880",
    "end": "701360"
  },
  {
    "text": "that's really where uh getting that CPU utilization getting your your policies",
    "start": "701360",
    "end": "706480"
  },
  {
    "text": "um getting your Min andax and desired capacities right um is really key Mak",
    "start": "706480",
    "end": "712680"
  },
  {
    "text": "making sure that you have um uh over provision capacity in in the other zones to make sure that if that failover",
    "start": "712680",
    "end": "718720"
  },
  {
    "text": "happens happens you're not taking down those other uh other zones as a",
    "start": "718720",
    "end": "724160"
  },
  {
    "text": "result and then lastly there are lots of knobs and switches to get right or wrong",
    "start": "724160",
    "end": "730279"
  },
  {
    "text": "I mentioned autoscaling groups launch configurations alarms policies each of",
    "start": "730279",
    "end": "735440"
  },
  {
    "text": "those has you know many different settings many different config configuration options on themselves or",
    "start": "735440",
    "end": "741160"
  },
  {
    "text": "on those um getting the right mix of those for your application for your",
    "start": "741160",
    "end": "747120"
  },
  {
    "text": "performance uh profile that you're expecting for um a good user experience for your application is difficult it",
    "start": "747120",
    "end": "754320"
  },
  {
    "text": "takes a lot of a lot of changes it takes a lot of iterations to really get that right um you will not get it right the",
    "start": "754320",
    "end": "760560"
  },
  {
    "text": "first time plan to set it up test it try it and then make tweaks along the",
    "start": "760560",
    "end": "765800"
  },
  {
    "text": "way so with that I will turn it over to",
    "start": "765800",
    "end": "770720"
  },
  {
    "text": "[Applause]",
    "start": "770860",
    "end": "777159"
  },
  {
    "text": "Keith hi there uh I'm Keith Baker from noia",
    "start": "777839",
    "end": "782920"
  },
  {
    "text": "and also I'm in the mapping side so here here.com uh you'll see mentioned as well",
    "start": "782920",
    "end": "789320"
  },
  {
    "text": "um and it's just the group that I'm in uh so I'm going to talk about using",
    "start": "789320",
    "end": "794399"
  },
  {
    "text": "autoscaling for some things that people don't necessarily think of as Auto scaling because scaling isn't really",
    "start": "794399",
    "end": "800120"
  },
  {
    "text": "part of it um in particular uh we're we use it to maintain a non-critical",
    "start": "800120",
    "end": "806560"
  },
  {
    "text": "application um and this is our click workor annotation application uh so it's",
    "start": "806560",
    "end": "812320"
  },
  {
    "text": "used internally um it was our first step into Amazon um we have about 100 users are all hourly contractors uh downtime",
    "start": "812320",
    "end": "819959"
  },
  {
    "text": "is actually sort of acceptable um we don't want to you know really take people off by sort of you know they",
    "start": "819959",
    "end": "826079"
  },
  {
    "text": "can't get their work done uh but at the same time it's not you know customer facing uh we're paying them to be there",
    "start": "826079",
    "end": "832639"
  },
  {
    "text": "uh so uh we sort of have a captive audience we can uh let things happen um",
    "start": "832639",
    "end": "838959"
  },
  {
    "text": "we ported it from an internal application and our goal was actually to get better up time because we had had",
    "start": "838959",
    "end": "844519"
  },
  {
    "text": "some problems uh in our traditional data center um but it sort of gave us a really good way to start off in",
    "start": "844519",
    "end": "851240"
  },
  {
    "text": "Amazon uh this is the basic architecture of uh the site it's a single instance",
    "start": "851240",
    "end": "856519"
  },
  {
    "text": "running a Jengo web app with a p uh postgress database on it uh it's backed",
    "start": "856519",
    "end": "861800"
  },
  {
    "text": "by two it has two ABS volumes uh that are actually in raid one um we download",
    "start": "861800",
    "end": "868399"
  },
  {
    "text": "config fation out of S3 we push uh backups of the database into S3 and we also do snapshots um we have a single",
    "start": "868399",
    "end": "875959"
  },
  {
    "text": "elb out front uh doing SSL for us uh and Route 53 for the names uh we use a",
    "start": "875959",
    "end": "882199"
  },
  {
    "text": "little bit of cloud watch just to if the instance is down for longer than we expect it to be down uh it'll notify",
    "start": "882199",
    "end": "889040"
  },
  {
    "text": "us um so as I mentioned uh single instance single a um Auto scaling set to",
    "start": "889040",
    "end": "896560"
  },
  {
    "text": "uh maintain one instance so if it goes down a failure takes about 15 minutes for it to come back up usually uh",
    "start": "896560",
    "end": "904120"
  },
  {
    "text": "usually is sort of a bit unfair to Amazon it actually doesn't fail that often uh the most common reason that it",
    "start": "904120",
    "end": "909480"
  },
  {
    "text": "goes down for any length of time is actually back uh upgrading it um so",
    "start": "909480",
    "end": "914920"
  },
  {
    "text": "upgrades are done with plan downtime we tell people it's going to happen a week before and then we just uh run a",
    "start": "914920",
    "end": "920560"
  },
  {
    "text": "redeployment uh in general that takes the same 15 minutes uh sometimes we have",
    "start": "920560",
    "end": "925639"
  },
  {
    "text": "a database schema update uh while we're doing update and that actually executes before the site goes live again uh so",
    "start": "925639",
    "end": "933399"
  },
  {
    "text": "sometimes a little bit longer if we have to do a major update those are pretty few and far between though um oh I should mention there we",
    "start": "933399",
    "end": "941160"
  },
  {
    "text": "also use the snapshots to test the upgrade and the database schema change beforehand so we'll just take snapshots",
    "start": "941160",
    "end": "946680"
  },
  {
    "text": "of the EBS volumes and then spin up a different test uh node and let that let",
    "start": "946680",
    "end": "952880"
  },
  {
    "text": "make sure it runs perfectly before we do it um it gives us a very you know basically the database we're going to do",
    "start": "952880",
    "end": "958360"
  },
  {
    "text": "it on in production is very similar so uh that's helped us actually a lot with",
    "start": "958360",
    "end": "963560"
  },
  {
    "text": "those updates um so while the up time is flexible uh our data is actually really",
    "start": "963560",
    "end": "970279"
  },
  {
    "text": "precious we're paying people hourly to make this data um we uh we use two EBS",
    "start": "970279",
    "end": "976759"
  },
  {
    "text": "volumes in raid one um so we have a copy there that's mostly for trying to keep it up you know we'll we'll get an alarm",
    "start": "976759",
    "end": "984120"
  },
  {
    "text": "if one of the the EBS volumes goes bad uh it's never actually happened uh the only time things have gone bad is when",
    "start": "984120",
    "end": "991000"
  },
  {
    "text": "they've all gone bad um we have periodic backups of postest we just do PG dumps",
    "start": "991000",
    "end": "997399"
  },
  {
    "text": "uh we encrypt them and stick them in S3 um we used to do it uh daily uh we",
    "start": "997399",
    "end": "1004000"
  },
  {
    "text": "actually increased it to hourly uh because we ran into two of the EBS",
    "start": "1004000",
    "end": "1010560"
  },
  {
    "text": "outages um we must have really picked the wrong availability Zone um we got",
    "start": "1010560",
    "end": "1015720"
  },
  {
    "text": "two EBS outages um that took us down for sort of 11 hours and we wished we had",
    "start": "1015720",
    "end": "1021120"
  },
  {
    "text": "had the the backups we ended up getting all of our data back because it just came back but we actually at one point",
    "start": "1021120",
    "end": "1027199"
  },
  {
    "text": "had thought about uh moving it to another availability Zone and bringing it back up off of the backups but we",
    "start": "1027199",
    "end": "1032918"
  },
  {
    "text": "were going to lose 5 hours worth of work so we decided not to 5 hours worth of work by you know many taggers were",
    "start": "1032919",
    "end": "1039160"
  },
  {
    "text": "working which I think was around 10 um so we have the EBS snapshots they're manual right now we usually use them for",
    "start": "1039160",
    "end": "1046678"
  },
  {
    "text": "the testing upgrades but we can also go back to them if we needed to in general our backup is really the postrest",
    "start": "1046679",
    "end": "1054160"
  },
  {
    "text": "dump um so using autoscaling uh the one",
    "start": "1054160",
    "end": "1059400"
  },
  {
    "text": "thing that really has to be true is the system has to can't be configured by hand it actually has to recover itself",
    "start": "1059400",
    "end": "1065440"
  },
  {
    "text": "um so what we do is we use um masterless puppet uh basically we build uh puppet",
    "start": "1065440",
    "end": "1072679"
  },
  {
    "text": "tree into an RPM stick it in S3 uh and then we use cloud init to uh configure",
    "start": "1072679",
    "end": "1079760"
  },
  {
    "text": "that repository uh install that puppet configuration from the RPM and then kick",
    "start": "1079760",
    "end": "1084960"
  },
  {
    "text": "off a puppet run um puppet installs all the needed packages actually connects",
    "start": "1084960",
    "end": "1090640"
  },
  {
    "text": "the EBS volumes um at that point I don't think I knew about uh mappings for uh",
    "start": "1090640",
    "end": "1097559"
  },
  {
    "text": "instances so I didn't actually have that um the puppet code just does calls the Java tools to mount the EBS volumes uh",
    "start": "1097559",
    "end": "1104919"
  },
  {
    "text": "it then configures the raid starts postgress and Apache and your instances back up and running um we could have",
    "start": "1104919",
    "end": "1111320"
  },
  {
    "text": "done this as uh sort of building an instance and then snapshotting an Ami",
    "start": "1111320",
    "end": "1118039"
  },
  {
    "text": "and then uh letting it sort of reboot itself on that Ami we wanted all of our",
    "start": "1118039",
    "end": "1124400"
  },
  {
    "text": "configuration uh stored uh in our source repository so this we can sort of build",
    "start": "1124400",
    "end": "1130039"
  },
  {
    "text": "this from acentos Ami plus cloud in it and then everything else uh is all in",
    "start": "1130039",
    "end": "1136159"
  },
  {
    "text": "our repository so uh we can go back to zero and it it's not in someone's head",
    "start": "1136159",
    "end": "1142360"
  },
  {
    "text": "how it was configured um we could at this point",
    "start": "1142360",
    "end": "1147480"
  },
  {
    "text": "take a snap take an Ami of the configured machine uh but it happens so",
    "start": "1147480",
    "end": "1154679"
  },
  {
    "text": "quickly we don't bother um compared to the time the instance takes to just you know Auto scaling has to notice that",
    "start": "1154679",
    "end": "1160919"
  },
  {
    "text": "it's down and then it has to create a replacement and you know this is 2 minutes or something added to it because",
    "start": "1160919",
    "end": "1167600"
  },
  {
    "text": "puppet has to do more work than if it was just a disc",
    "start": "1167600",
    "end": "1172600"
  },
  {
    "text": "image um so in terms of Click work it was totally successful um we moved it to",
    "start": "1172640",
    "end": "1178760"
  },
  {
    "text": "Amazon we drastically increased our up time uh over the server in a closet uh",
    "start": "1178760",
    "end": "1183840"
  },
  {
    "text": "we had had two day outages actually um for a hurricane um we lost power uh to",
    "start": "1183840",
    "end": "1190760"
  },
  {
    "text": "our data center which was I'll use data center um so moving to Amazon basically",
    "start": "1190760",
    "end": "1198640"
  },
  {
    "text": "got us uh um much better up time which is good I",
    "start": "1198640",
    "end": "1204000"
  },
  {
    "text": "mean as I said we didn't really need it but uh not making our workers Miss time",
    "start": "1204000",
    "end": "1209400"
  },
  {
    "text": "uh was was a good thing we designed for one day outages um we've had two outages",
    "start": "1209400",
    "end": "1214559"
  },
  {
    "text": "in the past year and a little more uh sorry actually almost two years um they",
    "start": "1214559",
    "end": "1221240"
  },
  {
    "text": "were 2 hours and 9 hours uh they were both EBS related events um we could have",
    "start": "1221240",
    "end": "1226760"
  },
  {
    "text": "designed this to to avoid the ABS events we just didn't you know we didn't need to we didn't need to spend the cost um",
    "start": "1226760",
    "end": "1233960"
  },
  {
    "text": "and uh it was sort of known um we we were prepared for",
    "start": "1233960",
    "end": "1239760"
  },
  {
    "text": "that um so that got us past the sort of first deployment um we got through all",
    "start": "1239760",
    "end": "1246520"
  },
  {
    "text": "the bureaucratic hurdles we're a big company um o Amazon you know it's public Cloud um and then so we got something",
    "start": "1246520",
    "end": "1254080"
  },
  {
    "text": "more fun we got to actually move our local search application",
    "start": "1254080",
    "end": "1259360"
  },
  {
    "text": "um it was our first customer facing application um obviously it needed to have really good up time um we provide",
    "start": "1259360",
    "end": "1266480"
  },
  {
    "text": "the uh the onebox search for here.com and also for all Nokia phones um or",
    "start": "1266480",
    "end": "1273520"
  },
  {
    "text": "really any Windows Phone you load the uh here.com app",
    "start": "1273520",
    "end": "1278600"
  },
  {
    "text": "on um our architecture here we uh deploy to uh four regions um currently uh we",
    "start": "1278600",
    "end": "1287360"
  },
  {
    "text": "also deploy to multiple azs um and we have used an elb between the azs um and",
    "start": "1287360",
    "end": "1294919"
  },
  {
    "text": "depending on the region depends on how many azs we do uh interestingly sort of",
    "start": "1294919",
    "end": "1300480"
  },
  {
    "text": "going back to the whole single node um we do a lot of single node uh Autos",
    "start": "1300480",
    "end": "1306400"
  },
  {
    "text": "scaling groups um so instances with one node um in those clusters automatically",
    "start": "1306400",
    "end": "1313000"
  },
  {
    "text": "register themselves in DNS with Route 53 uh based on an Autos scaling group name",
    "start": "1313000",
    "end": "1318360"
  },
  {
    "text": "so when you reboot a node when a node dies uh it will re-register itself with the NS um the auto scaling group names",
    "start": "1318360",
    "end": "1327039"
  },
  {
    "text": "uh and DNS names are formed with a pattern based on the cluster name so cluster one front ends can find cluster",
    "start": "1327039",
    "end": "1333520"
  },
  {
    "text": "one zookeepers cluster 2 G2 keeper Etc um so we could have done some API",
    "start": "1333520",
    "end": "1340919"
  },
  {
    "text": "queries with the autoscaling groups um but we decided to use DNS because it sort of allowed us to use a whole bunch",
    "start": "1340919",
    "end": "1346440"
  },
  {
    "text": "of standard tools that look for a server via DNS name um you have to be a little",
    "start": "1346440",
    "end": "1351799"
  },
  {
    "text": "bit careful of what tool you're using and I'll get into that in a second um so",
    "start": "1351799",
    "end": "1357240"
  },
  {
    "text": "one of our primary uses for this is Zookeeper um we have three single",
    "start": "1357240",
    "end": "1362480"
  },
  {
    "text": "instance Auto scaling groups each one maintains one zookeeper server um and",
    "start": "1362480",
    "end": "1369000"
  },
  {
    "text": "the reason for this is that zookeeper actually needs uh a consistent set of servers across all nodes um if you do an",
    "start": "1369000",
    "end": "1376679"
  },
  {
    "text": "autoscaling group you you don't have a necessarily a way to order them um so we",
    "start": "1376679",
    "end": "1383559"
  },
  {
    "text": "just did zookeeper one two and three uh and they uh they all maintain themselves",
    "start": "1383559",
    "end": "1389720"
  },
  {
    "text": "if one goes away it starts back up rejoins the the group and gets an update uh and then is usually happy um so when",
    "start": "1389720",
    "end": "1399880"
  },
  {
    "text": "we launched uh and I think this has been fixed now but uh zookeeper didn't re resolve host names on the server so you",
    "start": "1399880",
    "end": "1405960"
  },
  {
    "text": "couldn't just say you know zookeeper one zookeeper 2 zookeeper 3 um we actually",
    "start": "1405960",
    "end": "1411120"
  },
  {
    "text": "used inetd and netcat to just zookeeper would connect to Local Host which would",
    "start": "1411120",
    "end": "1416520"
  },
  {
    "text": "be the inetd and that would start up uh a netcat which would look up the host name and connect um so you basically get",
    "start": "1416520",
    "end": "1423679"
  },
  {
    "text": "a DNS lookup on every connection to the server um this actually has worked",
    "start": "1423679",
    "end": "1429720"
  },
  {
    "text": "really well for us uh the they tend to um not have problems with it uh we end",
    "start": "1429720",
    "end": "1438000"
  },
  {
    "text": "up not having to use uh elastic IPS you could use elastic IPS if you wanted um",
    "start": "1438000",
    "end": "1443440"
  },
  {
    "text": "but we figured why not just use DNS um it's sort of a long proven",
    "start": "1443440",
    "end": "1449000"
  },
  {
    "text": "technology um so the clients thankfully had been fixed we didn't actually have to do any trickery with the clients we",
    "start": "1449000",
    "end": "1455039"
  },
  {
    "text": "just told all all of our clients you zookeeper uh connect to uh that name uh",
    "start": "1455039",
    "end": "1460840"
  },
  {
    "text": "and they're fine um you do have to make sure that Java isn't caching um the DNS",
    "start": "1460840",
    "end": "1466399"
  },
  {
    "text": "lookups it is a lot better than it used to be about that now um but uh it will",
    "start": "1466399",
    "end": "1473320"
  },
  {
    "text": "in some modes it will cash host names forever uh once it's looked them up we also have to keep our DNS timeouts",
    "start": "1473320",
    "end": "1479399"
  },
  {
    "text": "pretty short uh but only internal things are looking these up so it's not like we're hammering on DNS servers um but if",
    "start": "1479399",
    "end": "1485240"
  },
  {
    "text": "your TTL is too long right it's going to take a while for the node to come up change the host name and you're going to have to wait a while uh until that host",
    "start": "1485240",
    "end": "1492039"
  },
  {
    "text": "name necessarily is flushed out of caches um we considered doing a zookeeper per per um per a uh but um",
    "start": "1492039",
    "end": "1502960"
  },
  {
    "text": "some of the regions we deploy in only have two azs and if one AZ were to",
    "start": "1502960",
    "end": "1508080"
  },
  {
    "text": "fail we we might lose Quorum uh and then both azs would be down uh so we're using",
    "start": "1508080",
    "end": "1515480"
  },
  {
    "text": "uh three zookeepers and one a in a cluster um and we have lost them all at",
    "start": "1515480",
    "end": "1521039"
  },
  {
    "text": "once they go offline uh and the node the our cluster detects that it no longer",
    "start": "1521039",
    "end": "1527320"
  },
  {
    "text": "has uh zookeeper and goes unhealthy on the elb the elb goes offline and things keep",
    "start": "1527320",
    "end": "1533600"
  },
  {
    "text": "working zookeepers come back up we importantly don't have locking in Zookeeper uh so they can just reregister",
    "start": "1533600",
    "end": "1541039"
  },
  {
    "text": "and they can find each other again uh there's no actual state that we really care about there",
    "start": "1541039",
    "end": "1547960"
  },
  {
    "text": "um so logging we do the same thing with logging um we have one logging node uh",
    "start": "1547960",
    "end": "1555520"
  },
  {
    "text": "that basically buffers log for the entire cluster um each uh host pushing to that log node",
    "start": "1555520",
    "end": "1562679"
  },
  {
    "text": "also has a capability to buffer uh just in case this one log node dies uh it",
    "start": "1562679",
    "end": "1569080"
  },
  {
    "text": "just gives us a place to do compression encryption and upload it periodic uploads T3 without affecting the",
    "start": "1569080",
    "end": "1575159"
  },
  {
    "text": "performance of the other nodes so if it goes offline for a little while we don't actually care um it also in our case",
    "start": "1575159",
    "end": "1581360"
  },
  {
    "text": "forwards to a central logging system for the rest of the company um but it does provide us a single point of",
    "start": "1581360",
    "end": "1587679"
  },
  {
    "text": "investigation for the entire cluster we tag all incoming requests with an ID and",
    "start": "1587679",
    "end": "1592840"
  },
  {
    "text": "then that gets propagated to all the sub requests that we do to get data back and uh we can just GP for that ID across the",
    "start": "1592840",
    "end": "1600200"
  },
  {
    "text": "that one node and we'll find uh all of the subqueries that were created um and",
    "start": "1600200",
    "end": "1607159"
  },
  {
    "text": "uh yeah I mentioned all scribe nodes uh so updating um we update data on these",
    "start": "1607159",
    "end": "1615760"
  },
  {
    "text": "nodes um we used to have it so that every one of the nodes would sequentially go offline update its data",
    "start": "1615760",
    "end": "1622320"
  },
  {
    "text": "and then come back online the problem is that as your clusters get bigger and bigger uh that takes a really long time",
    "start": "1622320",
    "end": "1627440"
  },
  {
    "text": "to update all your data so we're moving to a system that uh will basically take",
    "start": "1627440",
    "end": "1632720"
  },
  {
    "text": "a lock in Zookeeper and then when the cluster size it will increase the cluster size and then uh that new node",
    "start": "1632720",
    "end": "1641000"
  },
  {
    "text": "will get new data and then the all the nodes will get updated at the same time",
    "start": "1641000",
    "end": "1646919"
  },
  {
    "text": "uh and they can kill they basically once they see that the one they spun up has updated data it can kill itself off and",
    "start": "1646919",
    "end": "1654200"
  },
  {
    "text": "then everything's updated um you have to be careful sort of with tracking uh to",
    "start": "1654200",
    "end": "1659240"
  },
  {
    "text": "make sure that uh the node that spins up a node",
    "start": "1659240",
    "end": "1665000"
  },
  {
    "text": "waits for that one to finish and they sort of have to coordinate with that um we're using the autoscaling uh messages",
    "start": "1665000",
    "end": "1672399"
  },
  {
    "text": "to keep track of that so uh we've been really successful",
    "start": "1672399",
    "end": "1677480"
  },
  {
    "text": "uccessful we've all of our detected health problems have been successfully replaced with Autos scaling with no",
    "start": "1677480",
    "end": "1683279"
  },
  {
    "text": "intervention um we've been paranoid so it still Pages us but it's sort of beginning to feel silly um The Zookeeper",
    "start": "1683279",
    "end": "1690279"
  },
  {
    "text": "setup has worked great for us our biggest problem has actually been undetected problems um dis failures uh",
    "start": "1690279",
    "end": "1697240"
  },
  {
    "text": "and Inter intermittent connectivity failures uh have been our our highest",
    "start": "1697240",
    "end": "1702519"
  },
  {
    "text": "problems uh the dis failures uh are something that I expected when I when",
    "start": "1702519",
    "end": "1708000"
  },
  {
    "text": "Amazon says uh sort of design for failure I thought they'd notice those and terminate nodes um as being",
    "start": "1708000",
    "end": "1715120"
  },
  {
    "text": "unhealthy uh but when you really think about it if you look at like the 24 dis instances um one no one disc going bad",
    "start": "1715120",
    "end": "1722279"
  },
  {
    "text": "is not something that your application may be tolerant of that uh so we actually wrote code to look for dis",
    "start": "1722279",
    "end": "1729159"
  },
  {
    "text": "errors and terminate the instance and uh with that I'll hand it",
    "start": "1729159",
    "end": "1735840"
  },
  {
    "text": "to Lauren from Adobe [Applause]",
    "start": "1735840",
    "end": "1745980"
  },
  {
    "text": "thanks hello everyone my name is Lawrence and I'm with Adobe and I work in a team called Cloud Ops and what we",
    "start": "1746440",
    "end": "1752919"
  },
  {
    "text": "do is we provide technical operation services on behalf of our internal clients which are internal team within",
    "start": "1752919",
    "end": "1759519"
  },
  {
    "text": "the company Adobe is moving pretty aggressively to the cloud the Creative",
    "start": "1759519",
    "end": "1764919"
  },
  {
    "text": "Cloud initiative which is initiative that start about two years two plus years ago was designed from scratch to",
    "start": "1764919",
    "end": "1771360"
  },
  {
    "text": "run in Amazon it was also designed with a backend uh common platform with the",
    "start": "1771360",
    "end": "1777320"
  },
  {
    "text": "idea was to be able to reuse that if uh other internal teams within the uh the company wanted to build additional",
    "start": "1777320",
    "end": "1783840"
  },
  {
    "text": "products and so what I'm going to be talking about today is how we use",
    "start": "1783840",
    "end": "1788880"
  },
  {
    "text": "autoscaling on this uh common platform we call that the share",
    "start": "1788880",
    "end": "1795600"
  },
  {
    "text": "Cloud so before I get get into Auto scaling I thought it would be important to recap a little bit what the goals are",
    "start": "1795720",
    "end": "1800960"
  },
  {
    "text": "when you want to use Auto scalings and what it meant for us and for us it meant really three things one was to meet",
    "start": "1800960",
    "end": "1807159"
  },
  {
    "text": "demand well that's pretty obvious you never want to be in a situation where you don't have enough capacity to meet",
    "start": "1807159",
    "end": "1813480"
  },
  {
    "text": "your user demands um but the other thing is we wanted to control costs this is the",
    "start": "1813480",
    "end": "1820039"
  },
  {
    "text": "cloud it's a pay as you go model um if you allocate resources and you don't use them it's not like you can go back to",
    "start": "1820039",
    "end": "1826360"
  },
  {
    "text": "Amazon and tell them hey I didn't use my resources because you give me the money back probably not going to work I",
    "start": "1826360",
    "end": "1831919"
  },
  {
    "text": "haven't tried it but the other thing is we want to use um Autos scaling to maintain capacity and",
    "start": "1831919",
    "end": "1839600"
  },
  {
    "text": "what that means is um you hear everywhere that ec2 is an Epal uh",
    "start": "1839600",
    "end": "1845120"
  },
  {
    "text": "service that instances can disappear Into Thin Air at any given notice that",
    "start": "1845120",
    "end": "1850200"
  },
  {
    "text": "really never happened to me um but ec2 instances can go berserk and more likely",
    "start": "1850200",
    "end": "1856240"
  },
  {
    "text": "because of the software that you install on it um and so what we did is we actually took advantage of that uh and",
    "start": "1856240",
    "end": "1863919"
  },
  {
    "text": "uh created uh one of one of our our most useful standard operating procedure",
    "start": "1863919",
    "end": "1868960"
  },
  {
    "text": "which is that if it doesn't work just kill it uh and I learned this week that there's a term for that if you attended",
    "start": "1868960",
    "end": "1875200"
  },
  {
    "text": "Chrisman session on Wednesday he calls that stonei um so our operations team knows how to do that the instance is not",
    "start": "1875200",
    "end": "1882200"
  },
  {
    "text": "behaving properly they just go ahead and kill it and we also built some automation uh without monitoring tool to",
    "start": "1882200",
    "end": "1888919"
  },
  {
    "text": "terminate uh misbehaving instances and then rely on o scaling to bring uh capacity back",
    "start": "1888919",
    "end": "1896080"
  },
  {
    "text": "up okay so quickly um the architecture of the share Cloud um it's a fairly",
    "start": "1896080",
    "end": "1901799"
  },
  {
    "text": "simple distributed as sychronous architecture uh you got a layer of web servers that take requests behind an",
    "start": "1901799",
    "end": "1908760"
  },
  {
    "text": "elb um a lot of the business logic is then delegated to a worker layer uh via",
    "start": "1908760",
    "end": "1916399"
  },
  {
    "text": "sqs so the web layers uh post jobs to sqs a worker layers consume those jobs",
    "start": "1916399",
    "end": "1923679"
  },
  {
    "text": "and do some processing and then publish the results back into another queue um",
    "start": "1923679",
    "end": "1928760"
  },
  {
    "text": "and the the worker layer is actually something that um we have uh today 30 workers our engineering team is pretty",
    "start": "1928760",
    "end": "1935159"
  },
  {
    "text": "smart they created an SDK that allows other team within the company to create new workers so the number of workers",
    "start": "1935159",
    "end": "1941880"
  },
  {
    "text": "actually growing pretty quickly and example of workers would be some trans coding activities or metadata",
    "start": "1941880",
    "end": "1947840"
  },
  {
    "text": "extractions or thumbnailing and like that and so what I'm going to be talking about today is how did we scale the web",
    "start": "1947840",
    "end": "1954080"
  },
  {
    "text": "layer and how we scaled the worker",
    "start": "1954080",
    "end": "1958638"
  },
  {
    "text": "layer so on the web layer well when the web components came to us they were actually rated engineering team told us",
    "start": "1959639",
    "end": "1966559"
  },
  {
    "text": "hey we tested this thing they can handle 900 requests per seconds so right on that was good we definitely used that",
    "start": "1966559",
    "end": "1973200"
  },
  {
    "text": "metric right away to be able to scale on that but we pretty pretty quickly realized that this wasn't enough not all",
    "start": "1973200",
    "end": "1980159"
  },
  {
    "text": "requests are the same some requests are slower some requests are faster some requests takes more CPU than others um",
    "start": "1980159",
    "end": "1988320"
  },
  {
    "text": "and some requests use a lot more Network bandwidth than others right if you upload a gigantic video file there's",
    "start": "1988320",
    "end": "1994480"
  },
  {
    "text": "going to be a lot of inbound bandwidth that's going to be used versus if you make a a request for some metadata this",
    "start": "1994480",
    "end": "2001360"
  },
  {
    "text": "is just a very small amount of information that would be outbound um so what we did is we added",
    "start": "2001360",
    "end": "2007799"
  },
  {
    "text": "additional metrics and additional uh policies to be able to scale on those",
    "start": "2007799",
    "end": "2012960"
  },
  {
    "text": "conditions um so the good news is those metrics are readily available in",
    "start": "2012960",
    "end": "2018760"
  },
  {
    "text": "cloudwatch cloudwatch provides aggregated CPU across your scaling groups uh aggregated networks uh numbers",
    "start": "2018760",
    "end": "2026559"
  },
  {
    "text": "across your scaling groups they're not always usable directly and I'll touch a",
    "start": "2026559",
    "end": "2032639"
  },
  {
    "text": "little bit on that on how we overcome that problem and what we did",
    "start": "2032639",
    "end": "2038519"
  },
  {
    "text": "on the worker layer as you saw it's all based on SQ sqs so that was pretty easy",
    "start": "2038519",
    "end": "2044639"
  },
  {
    "text": "we basically uh scale uh based on the number of messages that have not been consumed in the queue uh we try to look",
    "start": "2044639",
    "end": "2051878"
  },
  {
    "text": "at other ways to scale like for instance CPU for us it didn't work because those",
    "start": "2051879",
    "end": "2056919"
  },
  {
    "text": "workers they are configured to only handle a fixed number of job at the same time so you know the CPU is probably",
    "start": "2056919",
    "end": "2064878"
  },
  {
    "text": "going to stay static no matter how many messages I in the CU that doesn't really tell us how busy those systems were so",
    "start": "2064879",
    "end": "2070878"
  },
  {
    "text": "right now we only have one scaling U um metric that we can use for that",
    "start": "2070879",
    "end": "2077960"
  },
  {
    "text": "layer so that's great we know what to scale up this is good but how do you scale down well it turns out scaling",
    "start": "2078839",
    "end": "2085158"
  },
  {
    "text": "down um became a little more of a headache than what we anticipated there's always there's a little bit of a",
    "start": "2085159",
    "end": "2091000"
  },
  {
    "text": "Fear Factor here right what if you scale down in the middle of the day and then all of a sudden you have this surge of",
    "start": "2091000",
    "end": "2097000"
  },
  {
    "text": "uh request and you remove your capacity um and then the other thing is scaling down is not always very graceful",
    "start": "2097000",
    "end": "2104839"
  },
  {
    "text": "so I don't know if you had experience with scaling down instances that are behind erbs basically the the Der",
    "start": "2104839",
    "end": "2111079"
  },
  {
    "text": "registration is pretty abrupt all the active connections are dropped and the instance is removed and then on that",
    "start": "2111079",
    "end": "2117240"
  },
  {
    "text": "worker layer we have some workers that um take a bit of time to process we have some conversions to PDF for instance",
    "start": "2117240",
    "end": "2123800"
  },
  {
    "text": "that could take minutes if the instance where the workers running on is selected for termination that job would just",
    "start": "2123800",
    "end": "2129720"
  },
  {
    "text": "never complete um the data is now lost job goes back in a queue and new workers is going to pick it up but while we use",
    "start": "2129720",
    "end": "2136400"
  },
  {
    "text": "an asynchronous process we still want those jobs to be uh processed pretty fast uh so what we did is uh we actually",
    "start": "2136400",
    "end": "2143839"
  },
  {
    "text": "went live with no downscaling and we made a conscious decision to to do that",
    "start": "2143839",
    "end": "2149000"
  },
  {
    "text": "because we wanted to learn about what our parent was uh so that we can make an conscious decision on what numbers to",
    "start": "2149000",
    "end": "2155960"
  },
  {
    "text": "use and what met tricks to use as well okay so but if you want to do cost",
    "start": "2155960",
    "end": "2163760"
  },
  {
    "text": "control then obviously you're going to have to do downscaling so after we",
    "start": "2163760",
    "end": "2168880"
  },
  {
    "text": "analyzed our traffic when we're live um we went back and look at our scaling history and also our usage well we",
    "start": "2168880",
    "end": "2175920"
  },
  {
    "text": "learned that our pattern was pretty typical of normal uh usage people are starting to use the product in the",
    "start": "2175920",
    "end": "2181200"
  },
  {
    "text": "morning and there's a ramp down in the evening so that Leed stuff to um",
    "start": "2181200",
    "end": "2187760"
  },
  {
    "text": "allow us to uh put together some schedule scaling right just scale up in the morning and then scale down in the",
    "start": "2187760",
    "end": "2194359"
  },
  {
    "text": "evening the other nice thing about this is that you can be proactive in terms of",
    "start": "2194359",
    "end": "2200280"
  },
  {
    "text": "having the capacity that you need right away one of the issue that we have with scaling is we never scale fast enough",
    "start": "2200280",
    "end": "2207160"
  },
  {
    "text": "right if you have a sudden spike it will take some minutes some amount of time for uh instances to come up and be",
    "start": "2207160",
    "end": "2213720"
  },
  {
    "text": "configured properly before they become actually useful and so schedule scaling allow us to put some capacity ahead of",
    "start": "2213720",
    "end": "2220520"
  },
  {
    "text": "time and then in the evening uh we just scale down because we know that there's going to be less people using the",
    "start": "2220520",
    "end": "2226800"
  },
  {
    "text": "product uh during the evening and the night um and because we have Auto",
    "start": "2226800",
    "end": "2232000"
  },
  {
    "text": "scaling if there was some people that would you know all of a sudden use our products in the middle of the night then aut scaling would take care of",
    "start": "2232000",
    "end": "2238760"
  },
  {
    "text": "that um once we learn about a partn as well it allows us to create some more",
    "start": "2238760",
    "end": "2244079"
  },
  {
    "text": "policies to scale down and what the important part here um the important thing here is that you",
    "start": "2244079",
    "end": "2249560"
  },
  {
    "text": "want to scale down very slowly like if there's one thing you want to remember from this session is you want to scale",
    "start": "2249560",
    "end": "2254800"
  },
  {
    "text": "up very fast but you want to scale down very slowly uh the other thing that our team",
    "start": "2254800",
    "end": "2260040"
  },
  {
    "text": "does is we manage the stage environments and so those stage environments are used by various teams to do integration",
    "start": "2260040",
    "end": "2266800"
  },
  {
    "text": "testing and load testing and all kind of other testing and the usage is not predictable at all and so what we decid",
    "start": "2266800",
    "end": "2274520"
  },
  {
    "text": "but but it still cost money right Amazon doesn't differentiate between Dev and stage and production it's all it's all",
    "start": "2274520",
    "end": "2280760"
  },
  {
    "text": "the same stuff and so what we did here is we did uh we took an aggressive approach to basically downscale",
    "start": "2280760",
    "end": "2286640"
  },
  {
    "text": "everything to the minimum size every day I think we do that multiple times a day now so we have some Chron jobs that just",
    "start": "2286640",
    "end": "2292599"
  },
  {
    "text": "go ahead and downscale their environments uh quickly okay so you're gonna ask me how",
    "start": "2292599",
    "end": "2299440"
  },
  {
    "text": "do we do this well we firmly believe in cloud formation we use cloud formation",
    "start": "2299440",
    "end": "2305359"
  },
  {
    "text": "extensively V across all those uh workers and web layers each worker has its own cloud formation stack um there's",
    "start": "2305359",
    "end": "2312760"
  },
  {
    "text": "a colleague of mine actually that presented uh the way we do deployments at Adobe for this service uh we do ab",
    "start": "2312760",
    "end": "2319680"
  },
  {
    "text": "deployments so meaning that if you have an aside that is active then in the background we build a sides that we test",
    "start": "2319680",
    "end": "2327640"
  },
  {
    "text": "uh and then when it becomes act uh it test and pass all the the criteria for release then we move that into to be the",
    "start": "2327640",
    "end": "2334240"
  },
  {
    "text": "the uh in production so what I'm trying to show here is that you know it's not how to use C information just how easy",
    "start": "2334240",
    "end": "2340280"
  },
  {
    "text": "it is to put a policy and an alarm in your template it's a few lines of code",
    "start": "2340280",
    "end": "2347800"
  },
  {
    "text": "uh here scaling based on the sqsq um and because for us we are again",
    "start": "2347800",
    "end": "2353400"
  },
  {
    "text": "we still we're still using uh cloud formation was the right way to",
    "start": "2353400",
    "end": "2359119"
  },
  {
    "text": "go um but what if cloud formation doesn't support what you need for instance schedule so autoscaling",
    "start": "2359119",
    "end": "2364960"
  },
  {
    "text": "supports natively schedule scaling but you can't really set that up with cloudformation",
    "start": "2364960",
    "end": "2370640"
  },
  {
    "text": "today so what we did here is we created some custom scripts to uh perform those",
    "start": "2370640",
    "end": "2375960"
  },
  {
    "text": "scaling in the morning so we have uh some basically predefined configuration settings uh scale this worker to this",
    "start": "2375960",
    "end": "2382960"
  },
  {
    "text": "number of instance in the morning and then scale to this number of instance in the evening you know business hours or",
    "start": "2382960",
    "end": "2388800"
  },
  {
    "text": "weekend hours for instance um so this just simply run as a crown look at the",
    "start": "2388800",
    "end": "2394680"
  },
  {
    "text": "all the auto scaling groups that we have um and then scale them down this is the output of the log it's actually a very",
    "start": "2394680",
    "end": "2401520"
  },
  {
    "text": "simple uh script um the other thing is I mentioned",
    "start": "2401520",
    "end": "2408480"
  },
  {
    "text": "earlier uh cloudwatch has a lot of metrics uh but for us they were not necessarily usable directly so for",
    "start": "2408480",
    "end": "2414960"
  },
  {
    "text": "instance the number of requests coming through the Erb that's a very good number to know but it's not very good",
    "start": "2414960",
    "end": "2421599"
  },
  {
    "text": "for scaling because it doesn't in take it doesn't take into account your current capacity",
    "start": "2421599",
    "end": "2427200"
  },
  {
    "text": "and what that means is that when you reach a a thir thresholds and you scale either up or down you would have to",
    "start": "2427200",
    "end": "2433240"
  },
  {
    "text": "change that number and so that means updating the alarm every time there's a",
    "start": "2433240",
    "end": "2438400"
  },
  {
    "text": "scaling event so what we did is we actually consumed those um cloudwatch metrics but rewrite them as custom",
    "start": "2438400",
    "end": "2445720"
  },
  {
    "text": "metrics that makes sense to us in this case the number of requests coming to one instance and that's great if uh you",
    "start": "2445720",
    "end": "2454079"
  },
  {
    "text": "reach a certain number and you add some capacity then the the alarm will clear and then you don't have to change",
    "start": "2454079",
    "end": "2459119"
  },
  {
    "text": "anything else and so we do that on our we layer for uh number of requests and",
    "start": "2459119",
    "end": "2465119"
  },
  {
    "text": "then also the CPU the aggregate CPU across the scaling group and the network",
    "start": "2465119",
    "end": "2471400"
  },
  {
    "text": "bandwidth okay so the other thing that we realize is not all spikes are the",
    "start": "2471440",
    "end": "2477520"
  },
  {
    "text": "same right you got the very slow ramp up you have you know medium ramp up you have you know very sudden ramp up and uh",
    "start": "2477520",
    "end": "2485560"
  },
  {
    "text": "the way it's scaling works today is that it's basically one policy one metric and",
    "start": "2485560",
    "end": "2490760"
  },
  {
    "text": "so if we take the example of a worker where there's a a few messages in the queue that needs to be processed then",
    "start": "2490760",
    "end": "2495880"
  },
  {
    "text": "you may want to add you know one or two instances but what if there's a million messages to be processed one or two",
    "start": "2495880",
    "end": "2501560"
  },
  {
    "text": "instances just not going to cut it you want to add you know a bazillion senses",
    "start": "2501560",
    "end": "2506680"
  },
  {
    "text": "and so what we did is we have this tiered uh level what we called for better for worse multi-input scalings uh",
    "start": "2506680",
    "end": "2513680"
  },
  {
    "text": "which basically uh scale more less aggressively depending on the criteria and this is not perfect right if you",
    "start": "2513680",
    "end": "2520280"
  },
  {
    "text": "look at the scale up like condition number three if condition number three is true right if you have uh adding a",
    "start": "2520280",
    "end": "2528319"
  },
  {
    "text": "fixed number of instances you have more than 10,000 messager in the que for more than one minute if that condition is",
    "start": "2528319",
    "end": "2534280"
  },
  {
    "text": "true then condition number two is true then condition number one is true and so what happens right well all those policies",
    "start": "2534280",
    "end": "2542480"
  },
  {
    "text": "may be executed randomly and so what's important to do here is to try to",
    "start": "2542480",
    "end": "2548960"
  },
  {
    "text": "leverages to first of all not have too many of those so that you have more chances to use the right policy and then",
    "start": "2548960",
    "end": "2555960"
  },
  {
    "text": "the other thing would be to take advantage of the cool down periods that you can set uh because that would allow",
    "start": "2555960",
    "end": "2562319"
  },
  {
    "text": "you to uh oops um run those policies at different",
    "start": "2562319",
    "end": "2569160"
  },
  {
    "text": "times so if I have an advice uh it's my last slide uh use cloud formation I know",
    "start": "2570359",
    "end": "2577280"
  },
  {
    "text": "there's a lot of people using cloud formation there's a lot of people not using cloud formation if you've done autoscaling uh without cloud formation",
    "start": "2577280",
    "end": "2584240"
  },
  {
    "text": "you probably know how much of a pain it is there's a lot you know you got to create launch configs you got to create alarms you got to create policies you",
    "start": "2584240",
    "end": "2590480"
  },
  {
    "text": "got to create SC groups um so use cloud formation uh the other thing is know",
    "start": "2590480",
    "end": "2596160"
  },
  {
    "text": "your system this is how you're going to know what kind of metrics you're going to be able to use how you going to scale",
    "start": "2596160",
    "end": "2601960"
  },
  {
    "text": "what kind of policies do you have to uh put together what's your scaling is Amazon provides an API to describe all",
    "start": "2601960",
    "end": "2608880"
  },
  {
    "text": "the s just use it it's there for a reason um and that's more experience",
    "start": "2608880",
    "end": "2613920"
  },
  {
    "text": "scaling up is easy scaling down was not so much easy for us um and it might not",
    "start": "2613920",
    "end": "2620000"
  },
  {
    "text": "be the case for you but just remember that you know this is not an easy thing to do uh and then the other thing like I",
    "start": "2620000",
    "end": "2626240"
  },
  {
    "text": "said to remember scale up fast and scale down slow and so with that I'm going to hand",
    "start": "2626240",
    "end": "2632960"
  },
  {
    "text": "it off to Brandon",
    "start": "2632960",
    "end": "2638079"
  },
  {
    "text": "hi so uh I'm Brandon Adams and I'm from a company called dream boox learning so I'm going to tell you today",
    "start": "2641000",
    "end": "2648200"
  },
  {
    "text": "about uh how to scale with colge alarms and how to use performance tests to accurately find the points that you need",
    "start": "2648200",
    "end": "2653960"
  },
  {
    "text": "to scale at um how to use scheduled scaling and how to scale with multiple cloudwatch alarm",
    "start": "2653960",
    "end": "2659520"
  },
  {
    "text": "conditions so a little about us uh we're a rails app uh we use the Unicorn app servers and we teach kids math so what",
    "start": "2659520",
    "end": "2666720"
  },
  {
    "text": "our product does is it figures out what kids need to learn and adapts to their",
    "start": "2666720",
    "end": "2672680"
  },
  {
    "text": "needs so because we're in education um we have a workload that's really well suited to autoscaling um this is a graph",
    "start": "2672839",
    "end": "2678920"
  },
  {
    "text": "for a typical week for us you can see we scale up in a big way right in the middle of the day but for most of the",
    "start": "2678920",
    "end": "2684760"
  },
  {
    "text": "time uh we don't have all that much traffic outside of those those periods on the weekend we've got a little bump",
    "start": "2684760",
    "end": "2690559"
  },
  {
    "text": "but at night almost nothing because those kids are asleep so scale cloudwatch",
    "start": "2690559",
    "end": "2697960"
  },
  {
    "text": "alarms what's an alarm um it measures submetric and cloudwatch go above or beyond the threshold an alarm fires and",
    "start": "2697960",
    "end": "2705480"
  },
  {
    "text": "that that could be used to trigger an auto scaling action so before we get to uh creating",
    "start": "2705480",
    "end": "2712960"
  },
  {
    "text": "our alarms we need to performance test to figure out how much load our app servers can handle um we need to",
    "start": "2712960",
    "end": "2718839"
  },
  {
    "text": "discover the ideal number of worker processes that we can use on our servers because if we go too few we're wasting resources um if we go too many we've got",
    "start": "2718839",
    "end": "2726040"
  },
  {
    "text": "resource contention and that's going to cause our performance to suffer um we need to figure out the maximum load that",
    "start": "2726040",
    "end": "2731280"
  },
  {
    "text": "we can sustain per server um and we need to find U the metric that we need to scale on for us this is CP utilization",
    "start": "2731280",
    "end": "2737599"
  },
  {
    "text": "for you it could be something different so this is a typical perf test for us uh we use New Relic to monitor",
    "start": "2737599",
    "end": "2744359"
  },
  {
    "text": "application and you can see the perf test mosies right along and performance is great until we hit a point and",
    "start": "2744359",
    "end": "2749640"
  },
  {
    "text": "suddenly we go really bad so we want to we want to tune ourselves so that we scale up before that happens",
    "start": "2749640",
    "end": "2757880"
  },
  {
    "text": "so we identifi the breaking point and then we correlate in time with our cloudwatch metrics and find out how much CPU we were using per instance at that",
    "start": "2759040",
    "end": "2765839"
  },
  {
    "text": "Breaking Point um you can see it's about 80% TPU there um when we went off the",
    "start": "2765839",
    "end": "2771319"
  },
  {
    "text": "rails and our performance test framework will also tell us how many concurrent users we had um this happened at about",
    "start": "2771319",
    "end": "2777599"
  },
  {
    "text": "400 users per server it doesn't produce graphs but it has text point that tells us",
    "start": "2777599",
    "end": "2784319"
  },
  {
    "text": "that um our first first method after obtaining this per test data was to just guess and check um we provisioned uh a",
    "start": "2784319",
    "end": "2791760"
  },
  {
    "text": "static set of servers that we knew could handle the load based on the numbers were revealed to us in the perf test and",
    "start": "2791760",
    "end": "2797559"
  },
  {
    "text": "then we started to just scale up and down uh very conservatively so we'd scale up at like 35% CPU because we",
    "start": "2797559",
    "end": "2803359"
  },
  {
    "text": "wanted to make really sure um that we didn't run ourselves off the ground and this worked um we were able to find uh",
    "start": "2803359",
    "end": "2810800"
  },
  {
    "text": "the scaling points but it was really inefficient because we were over provisioned while we were figuring out where we needed to scale scale um and it",
    "start": "2810800",
    "end": "2816800"
  },
  {
    "text": "took us a lot of time to dial that in so we thought we could find uh a better way",
    "start": "2816800",
    "end": "2822240"
  },
  {
    "text": "uh by doing math so for doing math we want to identify the variables that are involved um for us uh the independent",
    "start": "2822240",
    "end": "2828640"
  },
  {
    "text": "variable in the equation is concurrent users there are number dependent variables that uh a spike up or down",
    "start": "2828640",
    "end": "2834559"
  },
  {
    "text": "concurrent users can affect um CP utilization memory utilization disio",
    "start": "2834559",
    "end": "2839760"
  },
  {
    "text": "Network iio um for assd CPU and so after establishing that we",
    "start": "2839760",
    "end": "2846920"
  },
  {
    "text": "can do some math so this graph right here is a graph of our concurrent users",
    "start": "2846920",
    "end": "2852720"
  },
  {
    "text": "uh over time and this is about an hour and you can see I drew a slope line and",
    "start": "2852720",
    "end": "2858240"
  },
  {
    "text": "it it's about 1,600 us per hour that we're adding at this point on our graph",
    "start": "2858240",
    "end": "2863319"
  },
  {
    "text": "um it's about 27 per minute we know from a previous test that we can handle about 400 years of per server and when we're",
    "start": "2863319",
    "end": "2869079"
  },
  {
    "text": "at that point we're at about 80% CPU usage which is about 2% CPU usage per",
    "start": "2869079",
    "end": "2875000"
  },
  {
    "text": "server per user user and we know from our other testing that it takes us about 5 minutes for a",
    "start": "2875000",
    "end": "2882000"
  },
  {
    "text": "new node to come online we're adding about 27 users per minute so at the bare",
    "start": "2882000",
    "end": "2887640"
  },
  {
    "text": "minimum um we need to start spinning up new nodes when we're about 135 users per node short of that Max um which is about",
    "start": "2887640",
    "end": "2895440"
  },
  {
    "text": "53% utilization so we've got an equation right there um it's a little messy but I've got it more formally stated on the",
    "start": "2895440",
    "end": "2902040"
  },
  {
    "text": "next slide so you can see our scaling point is the maximum per node minus the",
    "start": "2902040",
    "end": "2907280"
  },
  {
    "text": "users that we're adding per minute times spin up time follow the equation um you see it",
    "start": "2907280",
    "end": "2912800"
  },
  {
    "text": "works out to 265 users per node um and then on the bottom you can see we've got",
    "start": "2912800",
    "end": "2917839"
  },
  {
    "text": "scaling Point users per node times CPU per user um works out to about 53% CPU",
    "start": "2917839",
    "end": "2923839"
  },
  {
    "text": "per node and from my physics undergrad courses I know that if you take care of the units they'll take care of you and",
    "start": "2923839",
    "end": "2930760"
  },
  {
    "text": "so we can see we cross out the units and our units match up uh the units on one",
    "start": "2930760",
    "end": "2936480"
  },
  {
    "text": "side of the equation match the units that result on the other side of the",
    "start": "2936480",
    "end": "2940838"
  },
  {
    "text": "equation so how much do we need to scale up by um for us uh the lowest we can scale up by is one node per AZ because",
    "start": "2941760",
    "end": "2948799"
  },
  {
    "text": "otherwise we'd be unbalanced and some of our azs would get more traffic than the others that's just how elbs work um at",
    "start": "2948799",
    "end": "2954680"
  },
  {
    "text": "least it used to they introduced a new feature that fixes that um for us um",
    "start": "2954680",
    "end": "2959760"
  },
  {
    "text": "when we add one node per a um we're in two azs this is an extra 800 users of capacity um and we're doing that in five",
    "start": "2959760",
    "end": "2966920"
  },
  {
    "text": "minutes so that would be plenty enough to keep up with that curve which is 1, 1600 users per hour um if we're adding",
    "start": "2966920",
    "end": "2972359"
  },
  {
    "text": "800 users capacity every five minutes we don't need to worry about bumping up how many nodes we're adding until we hit",
    "start": "2972359",
    "end": "2977480"
  },
  {
    "text": "9600 additional users per hour so we've got these predictions um",
    "start": "2977480",
    "end": "2984359"
  },
  {
    "text": "from our per test framework let's evaluate them um in the real world uh we actually scale up a little bit higher than 50% because our per test framework",
    "start": "2984359",
    "end": "2991079"
  },
  {
    "text": "is a little harsher under servers uh than the real world which is a good thing um otherwise we might start at a level where uh we'd have performance",
    "start": "2991079",
    "end": "2999599"
  },
  {
    "text": "problems yeah and you can see this is a graph of uh node count plotted against",
    "start": "2999799",
    "end": "3005000"
  },
  {
    "text": "CPU and so you can see the spike UPS happen a little bit after uh the 53%",
    "start": "3005000",
    "end": "3011359"
  },
  {
    "text": "level so next up is scheduled scaling uh we scheduled scale because",
    "start": "3012319",
    "end": "3018119"
  },
  {
    "text": "the acceleration in our load is not constant um I drew some lines through some of the more linear areas on the",
    "start": "3018119",
    "end": "3024359"
  },
  {
    "text": "graph um and you can see the slope is widely different between um the three which means we can't use a single",
    "start": "3024359",
    "end": "3032720"
  },
  {
    "text": "rule to capture all the scaling that we need to do um if we used a rule that was too aggressive in scaling up uh we'd end",
    "start": "3032720",
    "end": "3039280"
  },
  {
    "text": "up over provisioning and we'd have bounciness um that's when you scale down and then you scale up before an hour is",
    "start": "3039280",
    "end": "3046200"
  },
  {
    "text": "up which results in extra charges for you because Amazon bills by the hour uh so if you get rid of a node and then",
    "start": "3046200",
    "end": "3051319"
  },
  {
    "text": "bring up a node to replace it less than an hour later you're paying twice for it um if we scaled too timidly we'd have",
    "start": "3051319",
    "end": "3057440"
  },
  {
    "text": "poor performance and we'd have outages because we'd lack the capacity to serve",
    "start": "3057440",
    "end": "3062760"
  },
  {
    "text": "demand so to take care of that uh we add scheduled scaling points to eliminate the bounciness uh we scheduled scale for",
    "start": "3062960",
    "end": "3069480"
  },
  {
    "text": "those steepest parts of the curve that I showed earlier and we let Dynamic scaling take care of the less deep parts of the curve and as SAR and dippy would",
    "start": "3069480",
    "end": "3076920"
  },
  {
    "text": "have it while I was making this presentation I found a bouncy Point um so you can see it's highlight on the",
    "start": "3076920",
    "end": "3082280"
  },
  {
    "text": "graph there on the far right um we scale down and then l an hour later we scale right back up again uh we're scaling",
    "start": "3082280",
    "end": "3088000"
  },
  {
    "text": "down too quickly there and you can see on the graph I've identified our scaling points so at each scaling point we set a",
    "start": "3088000",
    "end": "3094559"
  },
  {
    "text": "new minimum for the auto scaling group and you can see that we've set our minimum uh a little early of where we",
    "start": "3094559",
    "end": "3100119"
  },
  {
    "text": "need to set it so I corrected that problem and then here's the graph for the next day um we moved our minimum out",
    "start": "3100119",
    "end": "3106319"
  },
  {
    "text": "our our scheduled scaling minimum and we eliminated That Bouncy Point saved a few",
    "start": "3106319",
    "end": "3112318"
  },
  {
    "text": "bucks lastly uh I'm going to show you how to scale with multiple cloudwatch alarm conditions so why would you want",
    "start": "3113920",
    "end": "3119440"
  },
  {
    "text": "to do this um every now and then maybe like three to four times a month we'll get an unexpected spike in traffic that",
    "start": "3119440",
    "end": "3126400"
  },
  {
    "text": "um we can't predict based on past uh Trends uh and it's at a high enough",
    "start": "3126400",
    "end": "3132200"
  },
  {
    "text": "level that performance suffers and if it's really high we're going to have an outage because we don't have the capacity to serve demand um we thought",
    "start": "3132200",
    "end": "3139520"
  },
  {
    "text": "we could just fix this by adding uh another alarm say like you know we're scaling up to 65% We'll add another",
    "start": "3139520",
    "end": "3144599"
  },
  {
    "text": "alarm at 80% % to scale up in a really big way um that's problematic because there's no way to",
    "start": "3144599",
    "end": "3151480"
  },
  {
    "text": "choose which alarm triggers um when both conditions are met and your low scaling",
    "start": "3151480",
    "end": "3158240"
  },
  {
    "text": "Point alarm could end up squishing your high scaling Point alarm uh it's stealing the the action that it wants to",
    "start": "3158240",
    "end": "3165599"
  },
  {
    "text": "take um there's a cool down period And so as long as the cool down period has effect your other alarms can't fire uh",
    "start": "3165599",
    "end": "3172720"
  },
  {
    "text": "and can't cause you to take autosan actions so our solution was to have multiple",
    "start": "3172720",
    "end": "3178640"
  },
  {
    "text": "Auto scaling groups so you have multiple Auto scaling groups with multiple alarms um we apply that high demand alarm to",
    "start": "3178640",
    "end": "3184359"
  },
  {
    "text": "just a single autoscaling group um that high demand group normally has no instances in it uh we reach that high",
    "start": "3184359",
    "end": "3190920"
  },
  {
    "text": "demand threshold we scale up that group that normally has zero instances and we know it's going to scale up because the",
    "start": "3190920",
    "end": "3196640"
  },
  {
    "text": "alarm only applies to that group and there's no cool down period in effect and that saves us uh from",
    "start": "3196640",
    "end": "3203559"
  },
  {
    "text": "performance problems and outages uh about three to four times a month so it's it's significant so putting all",
    "start": "3203559",
    "end": "3209400"
  },
  {
    "text": "these together you end up with a demand curve that hugs your user curve so this graph",
    "start": "3209400",
    "end": "3215440"
  },
  {
    "text": "right here is our provision node Count versus our concurrent user count and you",
    "start": "3215440",
    "end": "3221200"
  },
  {
    "text": "can see they're fairly coincident um and that's a result of the scaling strategy",
    "start": "3221200",
    "end": "3226319"
  },
  {
    "text": "that we put in place and you also get a mostly flat response curve this is another Snapshot",
    "start": "3226319",
    "end": "3231920"
  },
  {
    "text": "from New Relic um this is over a week's time it would be a little flatter but you can see on the left there um I don't",
    "start": "3231920",
    "end": "3239400"
  },
  {
    "text": "know if you can read the dates from where you are that's Halloween uh so I don't think many kids were playing uh on Halloween night so we had less",
    "start": "3239400",
    "end": "3248240"
  },
  {
    "text": "Demand right that's it thank you",
    "start": "3249240",
    "end": "3254680"
  }
]