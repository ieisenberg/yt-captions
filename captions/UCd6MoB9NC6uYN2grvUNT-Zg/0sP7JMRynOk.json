[
  {
    "start": "0",
    "end": "159000"
  },
  {
    "text": "good afternoon everybody I hope you enjoyed lunch had some time to play with the puppies enjoyed the fireside chat",
    "start": "3710",
    "end": "11309"
  },
  {
    "text": "with Andy Jesse some great information in there got a little preview there's",
    "start": "11309",
    "end": "16980"
  },
  {
    "text": "all kinds of neat equipment that are starting to be set up in that room for the party at the end of the day looks",
    "start": "16980",
    "end": "24060"
  },
  {
    "text": "like we're gonna have some circus backs and things it looks kind of fun so thank",
    "start": "24060",
    "end": "29099"
  },
  {
    "text": "you for joining us this afternoon a few housekeeping items emergency exits are",
    "start": "29099",
    "end": "34380"
  },
  {
    "text": "behind me here to the left behind you straight out in the event of the emergency please do not get in an",
    "start": "34380",
    "end": "40290"
  },
  {
    "text": "elevator please go down the stairs and get out of the building quickly if you haven't already please download the app",
    "start": "40290",
    "end": "47610"
  },
  {
    "text": "there's a web-based app for Android phones and then an app available in the App Store for iPhones we kindly request",
    "start": "47610",
    "end": "55590"
  },
  {
    "text": "that you submit feedback for all of the sessions you attend we culled through that information meticulously to help us",
    "start": "55590",
    "end": "63690"
  },
  {
    "text": "better prepare for future events so please take the time to submit feedback should you need a bathroom they are out",
    "start": "63690",
    "end": "71430"
  },
  {
    "text": "end to this direction and please note that all sessions are recorded so",
    "start": "71430",
    "end": "79310"
  },
  {
    "text": "everything that you attend today is recorded slides and videos will be available about two to four weeks from",
    "start": "79310",
    "end": "85409"
  },
  {
    "text": "now you can you'll receive an email to where you can find all the information from all the sessions so I'm Kitty Nix I",
    "start": "85409",
    "end": "93900"
  },
  {
    "text": "am senior practice manager for the federal civilian practice for professional services within AWS and I",
    "start": "93900",
    "end": "101579"
  },
  {
    "text": "am thrilled to introduce gargy and Amy who are here today to speak to you about",
    "start": "101579",
    "end": "106950"
  },
  {
    "text": "implementing a data warehouse using AWS solutions and services in a hybrid",
    "start": "106950",
    "end": "112320"
  },
  {
    "text": "environment amy was kind enough to donate her time from fannie mae one of",
    "start": "112320",
    "end": "118200"
  },
  {
    "text": "our customers we love it when our customers come and share their stories so thank you please big round of",
    "start": "118200",
    "end": "123360"
  },
  {
    "text": "applause applause the gargy who's going to start us off today by safe thank you you know I don't need that",
    "start": "123360",
    "end": "131480"
  },
  {
    "text": "yeah um thank you everyone for joining",
    "start": "131480",
    "end": "140480"
  },
  {
    "text": "her us today here in this session implementing a data warehouse on AWS in a hybrid environment",
    "start": "140480",
    "end": "146300"
  },
  {
    "text": "my name is Guardi Singh I'm work as a Solutions Architect with a worldwide public sector team in the federal",
    "start": "146300",
    "end": "151790"
  },
  {
    "text": "civilian space with me Co presenting today is Amy and she's database engineering manager at Fannie Mae so",
    "start": "151790",
    "end": "158360"
  },
  {
    "text": "let's see what do we want you to take away from this session today the first thing I want to talk about today is why",
    "start": "158360",
    "end": "164780"
  },
  {
    "start": "159000",
    "end": "159000"
  },
  {
    "text": "do enterprises want a hybrid cloud what are those reasons and so some use cases where we see our enterprise customer",
    "start": "164780",
    "end": "171830"
  },
  {
    "text": "going towards a hybrid cloud strategy then I'll talk about how you can create",
    "start": "171830",
    "end": "176900"
  },
  {
    "text": "a data warehouse on AWS what are some of the data design and what are some of the",
    "start": "176900",
    "end": "182750"
  },
  {
    "text": "data design considerations that you should have when you're designing a data warehouse on top of AWS and lastly I",
    "start": "182750",
    "end": "188150"
  },
  {
    "text": "would like to invite Amy to talk about how Fannie Mae is utilizing AWS products and services to create their own data",
    "start": "188150",
    "end": "194810"
  },
  {
    "text": "warehouse on AWS so let's start by talking about why do enterprises want a",
    "start": "194810",
    "end": "200300"
  },
  {
    "text": "hybrid environment so when we talk to our customers we see that there are certain use cases where they would",
    "start": "200300",
    "end": "205760"
  },
  {
    "text": "require to run certain application on premises now these reasons could be due to lower latency where certain",
    "start": "205760",
    "end": "211489"
  },
  {
    "text": "applications have inter dependencies within one another this could be because of certain legacy applications which",
    "start": "211489",
    "end": "217730"
  },
  {
    "text": "haven't undergone a migration effort or modernization effort we also see in the",
    "start": "217730",
    "end": "223459"
  },
  {
    "text": "federal space that certain of our customers do to compliance reasons have to run applications on-premises we see",
    "start": "223459",
    "end": "231170"
  },
  {
    "text": "that our customers utilize the cloud infrastructure and services to create dr",
    "start": "231170",
    "end": "236209"
  },
  {
    "text": "strategy to make sure that they have a geological and you know geographical dr",
    "start": "236209",
    "end": "241730"
  },
  {
    "text": "strategy where they can run certain parts of their applications on-premises and keep a backup of these applications",
    "start": "241730",
    "end": "247760"
  },
  {
    "text": "in you know different regions provided by our AWS we also see that our",
    "start": "247760",
    "end": "253010"
  },
  {
    "text": "customers recognize the value and the scalability and the elasticity of the cloud but at the same time they know",
    "start": "253010",
    "end": "258709"
  },
  {
    "text": "that not all application can move once to the cloud they see that certain applications will go through certain",
    "start": "258709",
    "end": "265020"
  },
  {
    "text": "modernization efforts in a tick it might take some time to reach there so they treat this dr this hybrid cloud strategy",
    "start": "265020",
    "end": "272129"
  },
  {
    "text": "as part of their phase migration of workloads where they recognize certain applications that can start from scratch",
    "start": "272129",
    "end": "278039"
  },
  {
    "text": "or certain applications which can be migrated right now into AWS into the cloud and certain part of those",
    "start": "278039",
    "end": "284460"
  },
  {
    "text": "applications are running on premises until they go through those modernization efforts so what do",
    "start": "284460",
    "end": "290849"
  },
  {
    "start": "290000",
    "end": "290000"
  },
  {
    "text": "customers want in a hybrid cloud environment so our customers want to be able to run workloads on-premises they",
    "start": "290849",
    "end": "297599"
  },
  {
    "text": "want to be able to run workloads in the cloud and utilize cloud services they've worn a tight integration between these",
    "start": "297599",
    "end": "304229"
  },
  {
    "text": "services so they want to have the same security posture they want to use the same identity and access management which they are utilizing on-premises and",
    "start": "304229",
    "end": "311430"
  },
  {
    "text": "take these capabilities into the cloud and they also want to achieve this scale and elasticity without worrying about",
    "start": "311430",
    "end": "317789"
  },
  {
    "text": "how they are going to provision new infrastructure or how they are going to buy new hardware they want to",
    "start": "317789",
    "end": "323789"
  },
  {
    "text": "dynamically scale up based on the resources and the applications if they have a new resource intensive",
    "start": "323789",
    "end": "329699"
  },
  {
    "text": "application they want to utilize the elasticity of the cloud to scale up during that application lifecycle and",
    "start": "329699",
    "end": "335909"
  },
  {
    "text": "scale down when the application has done its job so some of the use cases that we see our customers are utilizing for a",
    "start": "335909",
    "end": "342900"
  },
  {
    "start": "340000",
    "end": "340000"
  },
  {
    "text": "hybrid cloud strategy is they want to be able to integrate the different data silos so there are different",
    "start": "342900",
    "end": "348599"
  },
  {
    "text": "applications some of them can run on premises some of them can run in the cloud and they are generating all these",
    "start": "348599",
    "end": "353909"
  },
  {
    "text": "different kinds of data they want to make sure that they have a way to integrate these different data silos at",
    "start": "353909",
    "end": "358949"
  },
  {
    "text": "a single location and then do analysis on top of them to gain meaningful insights for their business so they they",
    "start": "358949",
    "end": "364620"
  },
  {
    "text": "can improve their business processes at the same time they want an integration between the Identity and Access",
    "start": "364620",
    "end": "369990"
  },
  {
    "text": "Management so it could be the same way that they are managing their applications on-premises the same Federation the same sam'l insertions and",
    "start": "369990",
    "end": "376860"
  },
  {
    "text": "they want to bring those capabilities into the cloud so that they can have an integrated way of managing applications",
    "start": "376860",
    "end": "382529"
  },
  {
    "text": "which are running on premises as well as applications and services which are running on the cloud they also want to",
    "start": "382529",
    "end": "388740"
  },
  {
    "text": "be able to integrate resource and deployment management so they use the same tools which they are using right",
    "start": "388740",
    "end": "394559"
  },
  {
    "text": "on premises and bring those capabilities of deployment and managing the resources into the cloud with the tools they are",
    "start": "394559",
    "end": "400949"
  },
  {
    "text": "already aware of and we have seen customers use this hybrid cloud strategy as part of their cloud bursting",
    "start": "400949",
    "end": "406829"
  },
  {
    "text": "capabilities and what I mean by that is if you have a resource intensive workload which you are not able to run",
    "start": "406829",
    "end": "413009"
  },
  {
    "text": "on premises because of provisioning new hardware it might take some time to get rack and stack those hardware's and get",
    "start": "413009",
    "end": "419969"
  },
  {
    "text": "the application going they can utilize the hybrid cloud strategy to then take this workloads to the cloud and",
    "start": "419969",
    "end": "425639"
  },
  {
    "text": "elastically scale up or scale down as the workloads demand and we have also seen a customer user like utilize this",
    "start": "425639",
    "end": "432329"
  },
  {
    "text": "hybrid cloud strategy as part of their data center extension so they are treating these regions different regions",
    "start": "432329",
    "end": "439319"
  },
  {
    "text": "in the cloud as an extension to their data center so they can run certain part of their application on-premises and",
    "start": "439319",
    "end": "445019"
  },
  {
    "text": "certain part of applications in the cloud and with all these different applications which are coming up in all",
    "start": "445019",
    "end": "450329"
  },
  {
    "text": "these different strategies we see that there is a lastima there's a vast amount of data and the data is increasing this",
    "start": "450329",
    "end": "456719"
  },
  {
    "start": "452000",
    "end": "452000"
  },
  {
    "text": "data can be produced by applications and services which are running on the cloud or application or services which are",
    "start": "456719",
    "end": "462149"
  },
  {
    "text": "running on premises and our customers want to be able to integrate all these different applications and services and",
    "start": "462149",
    "end": "468599"
  },
  {
    "text": "the data that they are producing at a single location so they can analyze and gain insights from that data when we",
    "start": "468599",
    "end": "475529"
  },
  {
    "text": "talk to our customers we see that over the last five years they have seen that the data has drawn 10 into X times and",
    "start": "475529",
    "end": "481049"
  },
  {
    "text": "they want to be able to analyze all this data that is being produced when I talk to my customers we see that whenever we",
    "start": "481049",
    "end": "488399"
  },
  {
    "text": "talk about creating a data platform they tell that the data platform should at least live for 15 years and it should",
    "start": "488399",
    "end": "494969"
  },
  {
    "text": "have the capability to scale at a magnitude of thousands so as the data demand increases and as the data has",
    "start": "494969",
    "end": "500699"
  },
  {
    "text": "been produced they want to be able to analyze all this data which has been produced so when we talk about analyzing",
    "start": "500699",
    "end": "508259"
  },
  {
    "start": "507000",
    "end": "507000"
  },
  {
    "text": "this data we talk about what is the analytical pipeline for creating an application and it all starts by the",
    "start": "508259",
    "end": "515969"
  },
  {
    "text": "collect phase you recognize what applications are producing this data you see how are you going to collect the",
    "start": "515969",
    "end": "521339"
  },
  {
    "text": "data and we have a bunch of different options available on AWS that you can select from",
    "start": "521339",
    "end": "527490"
  },
  {
    "text": "starting from database migration service which helps you migrate your data bases from on-premises into the cloud if you",
    "start": "527490",
    "end": "534970"
  },
  {
    "text": "have workloads such as object storage and you have massive amounts of data that you want to move you can utilize",
    "start": "534970",
    "end": "541120"
  },
  {
    "text": "our snow family which includes AWS snowball and snowmobile if you have workloads which require streaming data",
    "start": "541120",
    "end": "547959"
  },
  {
    "text": "or real-time applications and you want to collect real-time application data you can use the kinases family so that",
    "start": "547959",
    "end": "553720"
  },
  {
    "text": "includes can assess data streams and can assess firehose and you also have the capability to have a direct physical",
    "start": "553720",
    "end": "560110"
  },
  {
    "text": "connection between your on-premise data centers and AWS using Direct Connect so",
    "start": "560110",
    "end": "565180"
  },
  {
    "text": "you can move the data over the wire so once you have recognized how which applications are producing this data are",
    "start": "565180",
    "end": "570759"
  },
  {
    "text": "the on-premises or are they in the cloud the next step is how are you going to store this data is it going to be an",
    "start": "570759",
    "end": "576459"
  },
  {
    "text": "object store or are you dealing with files or you know different kind of data types so you can utilize Amazon simple",
    "start": "576459",
    "end": "582910"
  },
  {
    "text": "storage service for your object storage needs and Amazon glacier if you have any archival leads if you deal with file",
    "start": "582910",
    "end": "589870"
  },
  {
    "text": "systems we have different offerings in that department as well starting from Amazon Elastic file system or FXX for",
    "start": "589870",
    "end": "595899"
  },
  {
    "text": "windows-based workloads if you look at you know the different database offerings that we have starting from the",
    "start": "595899",
    "end": "602050"
  },
  {
    "text": "new services or the newest services which we launched such as Amazon quantum ledger database or ql DB or time series",
    "start": "602050",
    "end": "608649"
  },
  {
    "text": "database which is time stream and if we look at you know the more traditional RDBMS systems such as amazon RDS or",
    "start": "608649",
    "end": "616089"
  },
  {
    "text": "amazon aurora we have a bunch of different option offerings to choose from in terms of how you want to move",
    "start": "616089",
    "end": "621760"
  },
  {
    "text": "this data and how you want to save it on top of AWS the next step in this pipeline is to recognize is there any",
    "start": "621760",
    "end": "628600"
  },
  {
    "text": "ETL requirements or extract transform and load so as you're bringing in the new data do you need to change the data",
    "start": "628600",
    "end": "635050"
  },
  {
    "text": "types do you need to change it to a defined schema is the data you are collecting an unstructured format are",
    "start": "635050",
    "end": "640089"
  },
  {
    "text": "you do you want to convert it into a structured format before you do further analysis on top of it so you can utilize",
    "start": "640089",
    "end": "645430"
  },
  {
    "text": "AWS glue which is a fully managed ETL solution or extract transform and load and you can also utilize the data",
    "start": "645430",
    "end": "651790"
  },
  {
    "text": "cataloging capabilities to see which applications produce what part of this data so that you can have a holistic",
    "start": "651790",
    "end": "658449"
  },
  {
    "text": "view internally to see ok this applet produce this part of data and have a catalogue for it so I can maintain the",
    "start": "658449",
    "end": "664570"
  },
  {
    "text": "access and management as well when we talk to our customers they want to consolidate the data being produced by",
    "start": "664570",
    "end": "670750"
  },
  {
    "text": "all these different applications and they want to create data warehousing solutions so you can utilize Amazon",
    "start": "670750",
    "end": "676930"
  },
  {
    "text": "redshift which is a fully managed data warehouse in the cloud and I'm going to touch upon ratio later on as we speak as",
    "start": "676930",
    "end": "682600"
  },
  {
    "text": "well but last step in this analytical pipeline is once you have done your analysis once you have generated your",
    "start": "682600",
    "end": "689200"
  },
  {
    "text": "defined schemas and you know what kind of analysis you want to do do you want to do online illogical processes or do",
    "start": "689200",
    "end": "695170"
  },
  {
    "text": "you just want to do some ad hoc queries you can utilize business intelligence tools to connect to Amazon redshift or",
    "start": "695170",
    "end": "700840"
  },
  {
    "text": "the tools that we have available such as Amazon quick site which is a fully managed business intelligence tool we",
    "start": "700840",
    "end": "707620"
  },
  {
    "text": "have also seen our customers utilizing this analysis data sets that they create",
    "start": "707620",
    "end": "712840"
  },
  {
    "text": "and feed them to machine learning workloads to create relative analysis and you can utilize Amazon machine",
    "start": "712840",
    "end": "718390"
  },
  {
    "text": "learning or sage maker to run your purity of analysis utilizing the data that you just analyzed and we also have",
    "start": "718390",
    "end": "724840"
  },
  {
    "text": "a bunch of third-party tools available on AWS marketplace that you can choose from as well so when we talk to our",
    "start": "724840",
    "end": "732310"
  },
  {
    "text": "customers about data warehousing they want to be able to consolidate all the different data sets that have been",
    "start": "732310",
    "end": "738280"
  },
  {
    "text": "produced by these different applications but at the same time they want to elastically scale up or scale down their",
    "start": "738280",
    "end": "743560"
  },
  {
    "text": "capacity based on their workload demands and requirements they want to be able to modernize the current data warehousing",
    "start": "743560",
    "end": "750280"
  },
  {
    "text": "architectures to scale up capacity as the application capacity needs or as you get more confident queries or you get",
    "start": "750280",
    "end": "756640"
  },
  {
    "text": "more number of users accessing your data warehouse so they want to be able to modernize the data warehouse but what",
    "start": "756640",
    "end": "762400"
  },
  {
    "text": "does stay of it modernization mean our customers want to be able to scale to any amount of data so they want to be",
    "start": "762400",
    "end": "769840"
  },
  {
    "text": "able to scale as new data comes in or as new applications start producing new kind of data or new types of data they",
    "start": "769840",
    "end": "777070"
  },
  {
    "text": "want to be able to work with different workloads and they want to be able to scale to any amount of users without",
    "start": "777070",
    "end": "782620"
  },
  {
    "text": "degrading the performance they want to get consistently fast performance from their data warehouses and they want to",
    "start": "782620",
    "end": "788470"
  },
  {
    "text": "do it without managing the underneath resources the 1-d utilize a service",
    "start": "788470",
    "end": "794110"
  },
  {
    "text": "which is easy-to-use and they don't want to waste time managing managing menial",
    "start": "794110",
    "end": "799840"
  },
  {
    "text": "administrative tasks of maintaining the underneath resources they want to utilize managed services and another",
    "start": "799840",
    "end": "807160"
  },
  {
    "text": "important aspect is to for our customers it's not only the capability of",
    "start": "807160",
    "end": "812620"
  },
  {
    "text": "utilizing the data warehouse but also extend these creating capabilities using the same sequel using the same sequel",
    "start": "812620",
    "end": "819490"
  },
  {
    "text": "clients that they connect to and extend these capabilities to their data rake which they are creating on top of AWS so",
    "start": "819490",
    "end": "827140"
  },
  {
    "text": "let's now talk about how you can create data warehouse on data warehouses on AWS",
    "start": "827140",
    "end": "833490"
  },
  {
    "text": "but before I go there let's talk about what are the few design considerations",
    "start": "833490",
    "end": "841660"
  },
  {
    "start": "840000",
    "end": "840000"
  },
  {
    "text": "that you should have when you are creating a cloud data warehouse so the first thing you want to think about is how are you going to prepare your data",
    "start": "841660",
    "end": "847570"
  },
  {
    "text": "sources it's the data that is coming in is it coming in batch processes is coming in real-time how are you going to",
    "start": "847570",
    "end": "854230"
  },
  {
    "text": "ingest this data which is coming in into your data warehouse do you have any extract transform and load operations",
    "start": "854230",
    "end": "859360"
  },
  {
    "text": "associated to convert your unstructured data into a defined schema what I our",
    "start": "859360",
    "end": "865300"
  },
  {
    "text": "data quality requirements how are you doing your data quality checks before this data goes into your data warehouse for further analysis we have partner to",
    "start": "865300",
    "end": "872770"
  },
  {
    "text": "partner tools available and partner solutions available on AWS marketplace that you can choose from for your data",
    "start": "872770",
    "end": "878290"
  },
  {
    "text": "quality needs and at the same time do you have any auditing requirements how are you going to manage who has access",
    "start": "878290",
    "end": "884980"
  },
  {
    "text": "to or which application has access to what part of your data so these are a few things you should consider before you start designing your data warehouse",
    "start": "884980",
    "end": "892200"
  },
  {
    "text": "do you have any nightly ETL jobs that you want to run how are you going to manage your data transformations from",
    "start": "892200",
    "end": "898810"
  },
  {
    "text": "different applications which are feeding in this data next I want to talk about",
    "start": "898810",
    "end": "904360"
  },
  {
    "start": "904000",
    "end": "904000"
  },
  {
    "text": "is what does an enterprise data warehouse workflow typically looks like so when we talk to our customers the",
    "start": "904360",
    "end": "910960"
  },
  {
    "text": "first step starts from recognizing what are your data sources are you are your data sources on-premises or are these in",
    "start": "910960",
    "end": "917260"
  },
  {
    "text": "the cloud once you recognize your data sources and the applications which are producing this data the next step is how",
    "start": "917260",
    "end": "923440"
  },
  {
    "text": "are you going to collect it we have a bunch of different options to choose from AWS database migrations",
    "start": "923440",
    "end": "928900"
  },
  {
    "text": "we have snowball or snowmobile that you can choose from a table steer a sink which automatically transfers your data",
    "start": "928900",
    "end": "934900"
  },
  {
    "text": "from your on-premise systems into AWS with automation involved in it so you don't have to go in and manually",
    "start": "934900",
    "end": "940150"
  },
  {
    "text": "transfer or copy the data from your on-premise systems into AWS or you can utilize Direct Connect which can which",
    "start": "940150",
    "end": "947110"
  },
  {
    "text": "is actual physical connection between your data centers and AWS and you can transfer the data over the line once you",
    "start": "947110",
    "end": "954160"
  },
  {
    "text": "have recognized how are you going to move this data the next step in this pipeline or in this workflow is how are",
    "start": "954160",
    "end": "960040"
  },
  {
    "text": "you going to save this data and we have seen our customers utilizing Amazon s3 as their landing zone for all the data",
    "start": "960040",
    "end": "965410"
  },
  {
    "text": "from their different applications they keep the data in both structured and unstructured format under Amazon s3 and",
    "start": "965410",
    "end": "971020"
  },
  {
    "text": "if you have any ETL requirements or you want to transform your data from one schema to another to feed into your",
    "start": "971020",
    "end": "977590"
  },
  {
    "text": "different data warehouses you can use AWS glue which is a fully managed ETL solution or you can use Amazon EMR which",
    "start": "977590",
    "end": "983920"
  },
  {
    "text": "is an elastic MapReduce service we have also seen once the once the customers",
    "start": "983920",
    "end": "989200"
  },
  {
    "text": "have done those ETL operations they have transformed the data the next thing is another staging area so you can put the",
    "start": "989200",
    "end": "996070"
  },
  {
    "text": "transform data into Amazon s3 and that acts as your structured data or the staging area where you can now feed in",
    "start": "996070",
    "end": "1002160"
  },
  {
    "text": "this information to your data warehouse and in this specific case we have Amazon redshift which is a fully managed data",
    "start": "1002160",
    "end": "1008700"
  },
  {
    "text": "warehouse in the cloud that you can utilize and the last step in this process is to visualize and we have seen",
    "start": "1008700",
    "end": "1015180"
  },
  {
    "text": "our customers you utilizing business intelligence tools such as quick site to generate those reports at the end of the",
    "start": "1015180",
    "end": "1020490"
  },
  {
    "text": "day or at the end of the month and feed this information to their business processes to improve their business",
    "start": "1020490",
    "end": "1025829"
  },
  {
    "text": "processes when I talk to our customers we also see the need to not only query",
    "start": "1025829",
    "end": "1031230"
  },
  {
    "text": "the data which is sitting on their clusters or the data warehouse but also query the data which is sitting in the",
    "start": "1031230",
    "end": "1037230"
  },
  {
    "text": "raw a poor form underneath the storage so they want to be able to extend those capabilities from the data warehouse to",
    "start": "1037230",
    "end": "1044520"
  },
  {
    "text": "their data lakes or to the underneath storage and you can do so by utilizing the Amazon redshift spectrum feature",
    "start": "1044520",
    "end": "1050280"
  },
  {
    "text": "which allows you to query directly data which is sitting on top of s3 so what is",
    "start": "1050280",
    "end": "1057450"
  },
  {
    "text": "Amazon redshift Amazon ratio is a fast simple cost-effective data warehouse in the cloud that not only gives you the capability",
    "start": "1057450",
    "end": "1063600"
  },
  {
    "text": "to create data which is sitting on that cluster itself but also utilize the spectrum feature to extend those",
    "start": "1063600",
    "end": "1070350"
  },
  {
    "text": "capabilities to your underneath data like you can utilize open formats such as parquet OCR or Avro to analyze",
    "start": "1070350",
    "end": "1077810"
  },
  {
    "text": "utilizing the same sequel tools and same sequel syntax that you are currently using what Amazon redshift gives you is",
    "start": "1077810",
    "end": "1084570"
  },
  {
    "text": "the ability to get faster time to insight for all workloads or all different kind of analytical workloads",
    "start": "1084570",
    "end": "1090810"
  },
  {
    "text": "and it is powered by machine learning column in a storage and massively parallel processing capabilities what I",
    "start": "1090810",
    "end": "1097230"
  },
  {
    "text": "mean by column in a storage is that when you load the data into Amazon redshift clusters it is going to be saved by",
    "start": "1097230",
    "end": "1103080"
  },
  {
    "text": "column by column basis and not by row row wise it also gives you the",
    "start": "1103080",
    "end": "1108690"
  },
  {
    "text": "capability to elastically provision new resources as your workload demands increases or decreases and you also have",
    "start": "1108690",
    "end": "1114660"
  },
  {
    "text": "the capabilities to extend these creating capabilities from the same sequel clients from the same sequel to",
    "start": "1114660",
    "end": "1120120"
  },
  {
    "text": "your underneath storage using the Amazon OSHA spectrum feature one of the important things I want to talk about is",
    "start": "1120120",
    "end": "1125520"
  },
  {
    "text": "that it's 1/10 the cost of the traditional DNA warehouses so what this gives you is the ability to try out new",
    "start": "1125520",
    "end": "1131310"
  },
  {
    "text": "use cases so what we have seen our customers do is since it's under the same cost and the same budget they are",
    "start": "1131310",
    "end": "1137550"
  },
  {
    "text": "able to try new use cases and bring those new use cases once they are successful into their production workloads we are not talk to the",
    "start": "1137550",
    "end": "1145680"
  },
  {
    "text": "customers the most important things to our customers the four most important things to our customers is that the data",
    "start": "1145680",
    "end": "1151140"
  },
  {
    "text": "warehouse should be should have speed simplicity scale and security they want",
    "start": "1151140",
    "end": "1156240"
  },
  {
    "text": "to get faster time to insight they want their queries to perform better at the same time they don't want to do the",
    "start": "1156240",
    "end": "1161850"
  },
  {
    "text": "undifferentiated heavy lifting of managing the underneath resources but rather aw take care of that and at the",
    "start": "1161850",
    "end": "1167940"
  },
  {
    "text": "same time as the workload requirements increases they want to elastically increase the amount of compute and",
    "start": "1167940",
    "end": "1174570"
  },
  {
    "text": "storage they have on the data warehouse and once the compare if their workload demands decreases they want to scale",
    "start": "1174570",
    "end": "1180480"
  },
  {
    "text": "down as well and at the same time they want to utilize the same security portion and the same compliance policies",
    "start": "1180480",
    "end": "1186060"
  },
  {
    "text": "which they have been utilizing so far and bring those into AWS redshift as",
    "start": "1186060",
    "end": "1191160"
  },
  {
    "text": "well now let's take a look at what does Amazon redshift system architecture",
    "start": "1191160",
    "end": "1196850"
  },
  {
    "start": "1194000",
    "end": "1194000"
  },
  {
    "text": "looks like if you take a look at this",
    "start": "1196850",
    "end": "1203809"
  },
  {
    "text": "image you will see that you are utilizing the same sequel clients and the same bi tools to connect to Amazon",
    "start": "1203809",
    "end": "1208850"
  },
  {
    "text": "redshift if your applications have ODBC and JDBC compliant drivers you can keep",
    "start": "1208850",
    "end": "1213860"
  },
  {
    "text": "utilizing those to connect to Amazon redshift and how you connect to rest shift is that you connect to that green box or the leader node what that leader",
    "start": "1213860",
    "end": "1221510"
  },
  {
    "text": "node consists is it is the sequel endpoint to which your BI tools will connect it will store the cluster",
    "start": "1221510",
    "end": "1227240"
  },
  {
    "text": "metadata so all the metadata about the cluster will be stored on the leader node and the needle node also execute",
    "start": "1227240",
    "end": "1233809"
  },
  {
    "text": "the query execution plan so it will make sure that the queries are which you run on top of redshift are executed equally",
    "start": "1233809",
    "end": "1239960"
  },
  {
    "text": "on the compute nodes underneath talking about the compute nodes these are the local column in a storage on which your",
    "start": "1239960",
    "end": "1246649"
  },
  {
    "text": "actual data is going to set so when you load the data into an array of clusters these are the compute nodes which will",
    "start": "1246649",
    "end": "1251960"
  },
  {
    "text": "actually save the data all the queries which are executed on these compute nodes Arkham are executors and panel so",
    "start": "1251960",
    "end": "1259610"
  },
  {
    "text": "that's how it helps us achieve the massively parallel processing scale for redshift each of these compute nodes",
    "start": "1259610",
    "end": "1265460"
  },
  {
    "text": "with only execute the query on the amount of data which is stored on that particular compute node so this is why",
    "start": "1265460",
    "end": "1270740"
  },
  {
    "text": "we are able to achieve a shared nothing architecture where each of the compute node is only taking care of the part of",
    "start": "1270740",
    "end": "1276590"
  },
  {
    "text": "the query that it belongs to and the data which is saved on that compute node if you look at the operations such as",
    "start": "1276590",
    "end": "1283039"
  },
  {
    "text": "lower backup/restore through either Amazon s3 loading data from dynamodb or",
    "start": "1283039",
    "end": "1288169"
  },
  {
    "text": "EMR each of these compute node will talk in parallel to the underneath service henceforth increasing the and maximizing",
    "start": "1288169",
    "end": "1295909"
  },
  {
    "text": "the throughput for your applications now let's take a look at the compute not",
    "start": "1295909",
    "end": "1301970"
  },
  {
    "start": "1300000",
    "end": "1300000"
  },
  {
    "text": "dive deeper into what a compute node is so a compute node is partitioned into either 2 or 16 slices and you can think",
    "start": "1301970",
    "end": "1308330"
  },
  {
    "text": "of each of these slices as a virtual compute node it will have its own part of memory and this space from the at the",
    "start": "1308330",
    "end": "1315409"
  },
  {
    "text": "same time each of these slices will save data independently of one another the",
    "start": "1315409",
    "end": "1320539"
  },
  {
    "text": "leader node is responsible for distributing this data on each of these slices and it will also execute the",
    "start": "1320539",
    "end": "1326480"
  },
  {
    "text": "query execution plan where the leader node will distribute the how the query is going to be executed on",
    "start": "1326480",
    "end": "1331760"
  },
  {
    "text": "each of these different slices these slices are responsible for the rest shifts symmetric multiprocessing system",
    "start": "1331760",
    "end": "1337370"
  },
  {
    "text": "which allows redshift to be a massively parallel processing system now let's",
    "start": "1337370",
    "end": "1343340"
  },
  {
    "text": "talk about security so security is built into Amazon redshift you would be able to achieve end-to-end encryption both in",
    "start": "1343340",
    "end": "1350480"
  },
  {
    "start": "1344000",
    "end": "1344000"
  },
  {
    "text": "transit and in rest you can integrate Amazon redshift with other AWS services",
    "start": "1350480",
    "end": "1355610"
  },
  {
    "text": "such as AWS key management service so you can either utilize the keys which are generated by kms and managed by kms",
    "start": "1355610",
    "end": "1362930"
  },
  {
    "text": "or you can bring in your own keys to encrypt the underneath storage you can",
    "start": "1362930",
    "end": "1369020"
  },
  {
    "text": "also take benefit of the compliance that the service has achieved such as FedRAMP",
    "start": "1369020",
    "end": "1374420"
  },
  {
    "text": "HIPAA PCI and others now let's shift gears and talk about what are a few data",
    "start": "1374420",
    "end": "1381680"
  },
  {
    "text": "design considerations that you should have when you're designing your data warehouse on top of AWS but before I go",
    "start": "1381680",
    "end": "1387680"
  },
  {
    "text": "into what are the design considerations let's talk about some of the anti patterns where red shift would not be an",
    "start": "1387680",
    "end": "1393290"
  },
  {
    "start": "1391000",
    "end": "1391000"
  },
  {
    "text": "ideal solution for you if you have datasets which are less than 100 megabytes you would not see much benefit",
    "start": "1393290",
    "end": "1399410"
  },
  {
    "text": "of utilizing Amazon redshift because it was built for scale and it's built to be a massively parallel processing system",
    "start": "1399410",
    "end": "1405070"
  },
  {
    "text": "so for workloads which are or were less than 100 gigabytes you won't see much",
    "start": "1405070",
    "end": "1410120"
  },
  {
    "text": "benefit of utilizing Amazon redshift if you have applications which deal with online transaction processing or OLTP",
    "start": "1410120",
    "end": "1416630"
  },
  {
    "text": "operations a more traditional RDBMS or a no sequel database would be effective in these kind of situations if you have",
    "start": "1416630",
    "end": "1424100"
  },
  {
    "text": "unstructured data we have seen our customer getting maximum throughput and maximum efficiency out of their ship clusters if they have a defined schema",
    "start": "1424100",
    "end": "1431030"
  },
  {
    "text": "so if you have unstructured data you can still utilize your data Lake as part of Amazon s3 to keep that unstructured data",
    "start": "1431030",
    "end": "1437480"
  },
  {
    "text": "and utilize the Amazon redshift spectrum feature to query data directly from the underneat orage rather than worrying",
    "start": "1437480",
    "end": "1443720"
  },
  {
    "text": "about loading this data into your Amazon redshift clusters so now let's talk",
    "start": "1443720",
    "end": "1449450"
  },
  {
    "start": "1449000",
    "end": "1449000"
  },
  {
    "text": "about what are a few of the design considerations that you should have when you are creating a data warehouse so the",
    "start": "1449450",
    "end": "1455990"
  },
  {
    "text": "first thing is are using the optimal data types so I using Avro Parkway or or other open",
    "start": "1455990",
    "end": "1461960"
  },
  {
    "text": "file formats and utilizing their compression and partitioning capabilities to lower your cost and",
    "start": "1461960",
    "end": "1467630"
  },
  {
    "text": "improve your efficiencies in terms of you're creating capabilities another",
    "start": "1467630",
    "end": "1472909"
  },
  {
    "text": "thing I talk to my customers is how are you loading this data what what are your strategies around loading the data entry",
    "start": "1472909",
    "end": "1478580"
  },
  {
    "text": "error shift clusters there are multiple ways that you can do load the data into the redshift cluster but one of the ways",
    "start": "1478580",
    "end": "1485299"
  },
  {
    "text": "is to use the copy command which will copy the data setting on Amazon s3 into your clusters one of the things I talked",
    "start": "1485299",
    "end": "1492260"
  },
  {
    "text": "to them about is that utilize the russiaís massively parallel processing capabilities each of the compute nodes",
    "start": "1492260",
    "end": "1498350"
  },
  {
    "text": "is divided into slices so in this particular example you see I'm using a DC 2.8 X large compute node and it has",
    "start": "1498350",
    "end": "1504710"
  },
  {
    "text": "16 slices so if you have an input file and you divide the file into 16 pieces",
    "start": "1504710",
    "end": "1510039"
  },
  {
    "text": "each of these computer each of these slices will execute the amount of data that has to be loaded by each file so",
    "start": "1510039",
    "end": "1517070"
  },
  {
    "text": "you can essentially increase the throughput and maximize your throughput by matching the number of slices that you have in the cluster to the number of",
    "start": "1517070",
    "end": "1523370"
  },
  {
    "text": "input files because each of the slice will operate in parallel and that will increase the throughput for your loading",
    "start": "1523370",
    "end": "1529490"
  },
  {
    "text": "operations or your backup operations another thing to consider is how are you",
    "start": "1529490",
    "end": "1534740"
  },
  {
    "text": "distributing the data do you have a good distribution style what kind of distribution style you are selecting",
    "start": "1534740",
    "end": "1539870"
  },
  {
    "text": "when you are moving the data into your Amazon redshift clusters there are multiple different distribution style",
    "start": "1539870",
    "end": "1545570"
  },
  {
    "text": "available one of them is the distribution keys where you select a good distribution key and what happened",
    "start": "1545570",
    "end": "1551990"
  },
  {
    "text": "underneath the covers is that we hash the value in each of those columns and the value belonging to the same hash will go on the same computer ode and on",
    "start": "1551990",
    "end": "1558620"
  },
  {
    "text": "the same slice the other distribution style is distribution style all and this",
    "start": "1558620",
    "end": "1563630"
  },
  {
    "text": "particular distribution style is good for tables which have less than 100,000 rows because each copy of the entire",
    "start": "1563630",
    "end": "1569240"
  },
  {
    "text": "table will sit on each of the compute nodes and the last distribution style is distribution style even where as the",
    "start": "1569240",
    "end": "1576049"
  },
  {
    "text": "data is coming in we are going to distribute the data in a round-robin format",
    "start": "1576049",
    "end": "1580899"
  },
  {
    "text": "so these were the things that you can do to improve the efficiencies of your a shift clusters but underneath the covers",
    "start": "1587710",
    "end": "1593960"
  },
  {
    "text": "we are also working to increase the efficiency of the service itself now let's talk about a few of the recently released features we recently released a",
    "start": "1593960",
    "end": "1601610"
  },
  {
    "text": "new computer type which is the dc2 or dense compute node 2 which increases the performance of your ad shift clusters",
    "start": "1601610",
    "end": "1607430"
  },
  {
    "text": "from the previous generation of dc1 2 into x times at the same price so you can start using even if with a small",
    "start": "1607430",
    "end": "1614120"
  },
  {
    "text": "migration from one compute node to another you would see 2 into x time benefits of your performance being",
    "start": "1614120",
    "end": "1620150"
  },
  {
    "text": "increased we have also worked on increasing the efficiency of results at Kashyap and what this does it it if you",
    "start": "1620150",
    "end": "1628280"
  },
  {
    "text": "have sub-second repeat queries we are which you are running at a frequency which you are running at a very high",
    "start": "1628280",
    "end": "1633530"
  },
  {
    "text": "frequency we will cache it the results of those queries and we will next time the query is ran we will feed the result",
    "start": "1633530",
    "end": "1639530"
  },
  {
    "text": "from these caches instead and this frees up thousands of compute hours for your redshift cluster to perform other",
    "start": "1639530",
    "end": "1646070"
  },
  {
    "text": "operations such as ETL or injection operations another feature I want to",
    "start": "1646070",
    "end": "1651080"
  },
  {
    "start": "1651000",
    "end": "1651000"
  },
  {
    "text": "talk about today is short query acceleration and what this helps you to do is it will we will use machine",
    "start": "1651080",
    "end": "1657920"
  },
  {
    "text": "learning classifier underneath the cover to see the execution time of each of the query which you are submitting to your",
    "start": "1657920",
    "end": "1663170"
  },
  {
    "text": "Amazon redshift cluster and if we see that the within your reshef queues if",
    "start": "1663170",
    "end": "1668540"
  },
  {
    "text": "there are shorter and linked queries waiting behind the longer running queries we will dynamically route these",
    "start": "1668540",
    "end": "1673550"
  },
  {
    "text": "shorter running queries to an express queue and we will provision resources for these shorter running queries to be",
    "start": "1673550",
    "end": "1678770"
  },
  {
    "text": "executed first and once the execution of these short running queries have been done you will we will go back to the",
    "start": "1678770",
    "end": "1684410"
  },
  {
    "text": "longer any queries and give all the resources for those queries to be executed this feature is now available",
    "start": "1684410",
    "end": "1690770"
  },
  {
    "text": "and you can start going to your AWS management console and start utilizing the short query acceleration by just a",
    "start": "1690770",
    "end": "1696590"
  },
  {
    "text": "single click of a button on the arrow based management console the next feature I want to talk about today is",
    "start": "1696590",
    "end": "1702110"
  },
  {
    "start": "1701000",
    "end": "1701000"
  },
  {
    "text": "Amazon redshift elastic resize what this allows you to do is scale up or scale down your Amazon redshift clusters and",
    "start": "1702110",
    "end": "1708680"
  },
  {
    "text": "add more additional compute nodes to your clusters if you see that your workloads are demanding more resources",
    "start": "1708680",
    "end": "1714800"
  },
  {
    "text": "both in terms of the compute capabilities as well as in terms of the storage so",
    "start": "1714800",
    "end": "1719960"
  },
  {
    "text": "you can add more thrusters within minutes to your Amazon redshift cluster you don't have to worry about how the",
    "start": "1719960",
    "end": "1725390"
  },
  {
    "text": "data is going to be distributed into this new configuration because we will take care of underneath the covers when",
    "start": "1725390",
    "end": "1730610"
  },
  {
    "text": "you add those new compute nodes to the receive cluster there is a minimum transition time so it usually takes up",
    "start": "1730610",
    "end": "1736430"
  },
  {
    "text": "to one to five minutes to bring up those new compute nodes and you can then utilize the query queries which are",
    "start": "1736430",
    "end": "1742340"
  },
  {
    "text": "being going on in your shipped clusters you can quickly scale up so what we have",
    "start": "1742340",
    "end": "1747380"
  },
  {
    "text": "seen our customers utilize this feature for is for example on the month end or every day at night you have a resource",
    "start": "1747380",
    "end": "1753650"
  },
  {
    "text": "intensive ETL query or you have a storage intensive ingestion job we see our customers",
    "start": "1753650",
    "end": "1758990"
  },
  {
    "text": "utilizing the elastic resize to elastically resize and increase the compute capabilities and add more nodes",
    "start": "1758990",
    "end": "1764780"
  },
  {
    "text": "to the cluster and once that job is complete they go back to the optimal cluster size which is required to run",
    "start": "1764780",
    "end": "1771410"
  },
  {
    "text": "their additional queries another feature I want to talk about today is",
    "start": "1771410",
    "end": "1777310"
  },
  {
    "start": "1777000",
    "end": "1777000"
  },
  {
    "text": "concurrency scaling for burst user activities what this feature gives you is for example on a Monday morning you",
    "start": "1777310",
    "end": "1783710"
  },
  {
    "text": "come in and all your analysts are typing in reports and they are submitting queries on their shift clusters and you",
    "start": "1783710",
    "end": "1789770"
  },
  {
    "text": "start experimenting creating time on your queues what this feature will allow you to do is we will automatically bring",
    "start": "1789770",
    "end": "1795740"
  },
  {
    "text": "up transient clusters to the to the main cluster and we will distribute these queries which are waiting in the queue",
    "start": "1795740",
    "end": "1802190"
  },
  {
    "text": "is if we see there is more than 60 second of wait time on your queues we will automatically bring up these",
    "start": "1802190",
    "end": "1807200"
  },
  {
    "text": "transient clusters and we will distribute these queries to these transient clusters once the queries have",
    "start": "1807200",
    "end": "1812270"
  },
  {
    "text": "been executed and once there are no more queries waiting in the queues we will take these transient clusters away and",
    "start": "1812270",
    "end": "1818450"
  },
  {
    "text": "you are only paying for the time the transient clusters are actually running those queries what this allows you to",
    "start": "1818450",
    "end": "1825590"
  },
  {
    "text": "get is consistently fast performance even with thousands and concurrent queries because now your clusters are",
    "start": "1825590",
    "end": "1830660"
  },
  {
    "text": "not waiting to execute these queries but rather these transient clusters come into play and we distribute the queries",
    "start": "1830660",
    "end": "1836240"
  },
  {
    "text": "so all the queries are running concurrently for you one of the important features that we have seen",
    "start": "1836240",
    "end": "1841490"
  },
  {
    "text": "when we talk to our customers as well is this feature is essentially free for 97% of our customers because for every 24",
    "start": "1841490",
    "end": "1848270"
  },
  {
    "text": "hours your main cluster isn't used you you occur one free hour of concurrency",
    "start": "1848270",
    "end": "1853400"
  },
  {
    "text": "scaling credit now let's shift gears and talk about Amazon - the ref should",
    "start": "1853400",
    "end": "1859340"
  },
  {
    "start": "1857000",
    "end": "1857000"
  },
  {
    "text": "spectrum and what are the features that you can take use of by utilizing the spectrum feature as I mentioned earlier",
    "start": "1859340",
    "end": "1866180"
  },
  {
    "text": "spectrum allows you to query the data sitting directly on top of your Amazon s3 buckets as part of your data Lake so",
    "start": "1866180",
    "end": "1872510"
  },
  {
    "text": "you are able to use the same sequel syntax and the same sequel clients which you are using in your Amazon Amazon",
    "start": "1872510",
    "end": "1878180"
  },
  {
    "text": "redshift clusters and bring those capabilities down to your data lake so you don't have to worry about loading",
    "start": "1878180",
    "end": "1883670"
  },
  {
    "text": "the data into the clusters doing ETL transformations before you can actually start reading the information from those",
    "start": "1883670",
    "end": "1889400"
  },
  {
    "text": "from that new incoming data what we have seen our customers use this feature for is if you have our kyvan needs so they",
    "start": "1889400",
    "end": "1897410"
  },
  {
    "text": "keep the archived data as part of their data Lake and they utilize the Amazon redshift clusters to keep the newest",
    "start": "1897410",
    "end": "1904490"
  },
  {
    "text": "data but for example if you want to go back and query the data which is two years old you can utilize the spectrum",
    "start": "1904490",
    "end": "1911660"
  },
  {
    "text": "feature to go back and query only that part of data and only access what is required to be accessed for making use",
    "start": "1911660",
    "end": "1918380"
  },
  {
    "text": "of one query this particular feature gives you the capability to scale your compute separately from your storage so",
    "start": "1918380",
    "end": "1925400"
  },
  {
    "text": "you can create your data data Lake and you can scale your compute separately based on how much compute capacity do",
    "start": "1925400",
    "end": "1932810"
  },
  {
    "text": "you need to run your regular workloads versus how much compute capacity you need to run a query which will run once",
    "start": "1932810",
    "end": "1938960"
  },
  {
    "text": "in two years or once in a year you have the capabilities to utilize the open",
    "start": "1938960",
    "end": "1943970"
  },
  {
    "text": "formats such as Parque OCR Avro and other file formats to query data",
    "start": "1943970",
    "end": "1949280"
  },
  {
    "text": "directly utilizing the Amazon pressure spectrum feature we have also worked to improve the performance of spectrum and",
    "start": "1949280",
    "end": "1956330"
  },
  {
    "text": "what we are going to implement soon is that the ability to not only utilize this open data source or open file",
    "start": "1956330",
    "end": "1962420"
  },
  {
    "text": "format such as Parque and Avro but the capability to unload the data from your Amazon redshift clusters on to your",
    "start": "1962420",
    "end": "1968960"
  },
  {
    "text": "underneath storage utilizing the unload to parking option and we are also working to increase the execution time",
    "start": "1968960",
    "end": "1975770"
  },
  {
    "text": "of each of the spectrum queries by working on the feature which is one release pretty soon is called spectrum request accelerator",
    "start": "1975770",
    "end": "1983320"
  },
  {
    "text": "now if I go back and wrap it up in terms of what are the different things that we are doing to improve improve improve the",
    "start": "1983320",
    "end": "1989980"
  },
  {
    "text": "performance of Amazon redshift as a service you can see the foremost thing",
    "start": "1989980",
    "end": "1995050"
  },
  {
    "text": "that value to our customers is speed and we have recently introduced new services such as elastic resize and concurrency",
    "start": "1995050",
    "end": "2002220"
  },
  {
    "text": "scaling features within Amazon redshift if you look at scale we have introduced a spectrum request accelerator and",
    "start": "2002220",
    "end": "2009600"
  },
  {
    "text": "unload two parking options and if you look at simplicity we have released new",
    "start": "2009600",
    "end": "2014610"
  },
  {
    "text": "features such as auto vacuum which will automatically run the vacuum operations on your cluster itself when we see that",
    "start": "2014610",
    "end": "2020940"
  },
  {
    "text": "the cluster utilization is less so you don't have to run those vacuum operations again and again to delete the",
    "start": "2020940",
    "end": "2026220"
  },
  {
    "text": "rows which were marked for deletion we have also introduced the auto analyze which will take a look at your cluster",
    "start": "2026220",
    "end": "2032490"
  },
  {
    "text": "automatically and generate the performance reports of how much execution time is going happening in",
    "start": "2032490",
    "end": "2038700"
  },
  {
    "text": "your clusters and how much resources of cluster is consuming and at the same time we have introduced the auto data",
    "start": "2038700",
    "end": "2044070"
  },
  {
    "text": "distribution so we will pick a good distribution style for your clusters by looking at your cluster utilization and",
    "start": "2044070",
    "end": "2050220"
  },
  {
    "text": "pick a good distribution key to automatically distribute the data on your Amazon redshift clusters when we",
    "start": "2050220",
    "end": "2056550"
  },
  {
    "text": "talk to our customers we see that not only they warn the service to be secure but they also want to have tight",
    "start": "2056550",
    "end": "2062010"
  },
  {
    "text": "integration with other area services and one of that integration that we are going to launch is with Amazon Lake",
    "start": "2062010",
    "end": "2068700"
  },
  {
    "text": "formation once the service is generally available now I'd like to invite Amy from Fannie Mae to talk about how they",
    "start": "2068700",
    "end": "2075330"
  },
  {
    "text": "are using AWS product and services and how they are creating their own data warehouse on top of AWS thank you",
    "start": "2075330",
    "end": "2082190"
  },
  {
    "text": "[Applause] put up at noon everyone my name is Jamie",
    "start": "2082190",
    "end": "2088419"
  },
  {
    "text": "says I'm a technical manager in Fannie Mae I lead database migration as well as there is engineering trade only less so",
    "start": "2088419",
    "end": "2096669"
  },
  {
    "text": "today we would like to I would like to introduce our journey data warehouse journey to AWS first let me tell you a",
    "start": "2096669",
    "end": "2105670"
  },
  {
    "start": "2104000",
    "end": "2104000"
  },
  {
    "text": "little bit about what Fannie Mae does where the leading we are the leader in",
    "start": "2105670",
    "end": "2111070"
  },
  {
    "text": "providing housing market for Housing Finance for home buyers and renters in",
    "start": "2111070",
    "end": "2116590"
  },
  {
    "text": "the United States we provide access to affordable home financing in all markets",
    "start": "2116590",
    "end": "2122080"
  },
  {
    "text": "at all time we prepare our mission by providing liquidity to the market home",
    "start": "2122080",
    "end": "2128380"
  },
  {
    "text": "ownership in the United States are at 64 percent and Fannie Mae provide 36",
    "start": "2128380",
    "end": "2133990"
  },
  {
    "text": "percent of home financing our financing market financing makes sustainable",
    "start": "2133990",
    "end": "2139960"
  },
  {
    "text": "homeownership for millions of Americans so our data warehouse journey starts",
    "start": "2139960",
    "end": "2146020"
  },
  {
    "start": "2144000",
    "end": "2144000"
  },
  {
    "text": "about four years ago our data warehouse stores the system records the data in",
    "start": "2146020",
    "end": "2153310"
  },
  {
    "text": "our data warehouses serve analytics communities for research and data science the data in these data",
    "start": "2153310",
    "end": "2160480"
  },
  {
    "text": "warehouses also used to generate generate regulatory reports and",
    "start": "2160480",
    "end": "2165850"
  },
  {
    "text": "financial reports we have several legacy data warehouses and we are consolidating",
    "start": "2165850",
    "end": "2173020"
  },
  {
    "text": "these their warehouses into enterprise data warehouse so so far we have been",
    "start": "2173020",
    "end": "2178210"
  },
  {
    "text": "very successful in our data warehouse strategy but that creates new challenges",
    "start": "2178210",
    "end": "2183310"
  },
  {
    "text": "for us now with the database data warehouse consolidation we see six times",
    "start": "2183310",
    "end": "2190180"
  },
  {
    "start": "2185000",
    "end": "2185000"
  },
  {
    "text": "to 20 times of transaction in the last 18 months with the digital transformations we see more activities",
    "start": "2190180",
    "end": "2196780"
  },
  {
    "text": "happening in our data warehouses we see three millions queries per month and then we see four times of data growth",
    "start": "2196780",
    "end": "2204360"
  },
  {
    "text": "successful a user adoption means we have more user base that come into our data",
    "start": "2204360",
    "end": "2209830"
  },
  {
    "text": "warehouse we see 100 to 1000 users in the last three month in the last three",
    "start": "2209830",
    "end": "2214960"
  },
  {
    "text": "years then from the application standpoint we have 80 applications that come into",
    "start": "2214960",
    "end": "2219969"
  },
  {
    "text": "our data warehouses and then we are planning to unboard more than 100 applications to our data warehouses so",
    "start": "2219969",
    "end": "2227380"
  },
  {
    "text": "that leads to the concurrency issues and in some other scalability issues so with",
    "start": "2227380",
    "end": "2233679"
  },
  {
    "text": "the to mitigate some of these issues we have numerous try to do numerous things",
    "start": "2233679",
    "end": "2239769"
  },
  {
    "text": "first for the hawk user queries we have to terminate their hawk sequels if you",
    "start": "2239769",
    "end": "2246909"
  },
  {
    "text": "learn to long and for the application jobs we work with application teams through prioritize the critical jobs to",
    "start": "2246909",
    "end": "2253779"
  },
  {
    "text": "make sure that we remediate and then reduce the resource contentions so we",
    "start": "2253779",
    "end": "2259029"
  },
  {
    "text": "have been concerned in doing that and that was not good enough so what we did is we moved our heart users as well as",
    "start": "2259029",
    "end": "2267009"
  },
  {
    "text": "reporting to a separate data warehouses and then we use on the nightly data restored to sync up the data to this",
    "start": "2267009",
    "end": "2274719"
  },
  {
    "text": "data reporting data warehouses so in",
    "start": "2274719",
    "end": "2279729"
  },
  {
    "text": "some cases we also need to appricate do",
    "start": "2279729",
    "end": "2284919"
  },
  {
    "text": "our plication enhancements in order to make the new data attribute available to our business users so time to market is",
    "start": "2284919",
    "end": "2292209"
  },
  {
    "text": "our concern too so what really need is we need the self-service capability",
    "start": "2292209",
    "end": "2298239"
  },
  {
    "text": "where our business users is able to create a dataset by themselves without needing the application enhancements",
    "start": "2298239",
    "end": "2305609"
  },
  {
    "text": "what we also need is as as I mentioned we have more data coming to our data",
    "start": "2305609",
    "end": "2310779"
  },
  {
    "text": "warehouses and we have ad hoc users and in applications sometimes the those",
    "start": "2310779",
    "end": "2317139"
  },
  {
    "text": "volume are not predictable so we need to have ability to dynamically scale in",
    "start": "2317139",
    "end": "2322269"
  },
  {
    "text": "order to meet those increase their volumes we also want to have the data",
    "start": "2322269",
    "end": "2327309"
  },
  {
    "text": "sharing across multiple teams so there is a single source of records from",
    "start": "2327309",
    "end": "2332349"
  },
  {
    "text": "analysis result as well as our reporting our data is Assad is made separately",
    "start": "2332349",
    "end": "2341499"
  },
  {
    "text": "maintained by different business unit the business domain and we would like to have a cross domain",
    "start": "2341499",
    "end": "2348569"
  },
  {
    "text": "so that all these data available to our business users finally as I mentioned we",
    "start": "2348569",
    "end": "2355140"
  },
  {
    "text": "have more users and a more application onboarding to our application up to our data warehouse so we need to have an",
    "start": "2355140",
    "end": "2362309"
  },
  {
    "text": "ability to scale to meet our selling spike of workload and then to our aha queries so what we did is we turn rest",
    "start": "2362309",
    "end": "2372719"
  },
  {
    "start": "2370000",
    "end": "2370000"
  },
  {
    "text": "shift to address these challenges first the rest of has a numerous features that",
    "start": "2372719",
    "end": "2378869"
  },
  {
    "text": "gargy our garden mentioned before and I would just like to mention several these features that we use to make it mitigate",
    "start": "2378869",
    "end": "2385799"
  },
  {
    "text": "our challenges with the rest of spectrum were able to query the data directly in",
    "start": "2385799",
    "end": "2392549"
  },
  {
    "text": "our s3 and using the sequel which most of our users are familiar with",
    "start": "2392549",
    "end": "2399109"
  },
  {
    "text": "and this feature cuts down the time eliminate the data loading we also able",
    "start": "2416579",
    "end": "2423420"
  },
  {
    "text": "to share the data across the teams as well as across a business unit without making a data quality data copy we can",
    "start": "2423420",
    "end": "2432329"
  },
  {
    "text": "also join the data in Russia as well as the rest of a spectrum together and",
    "start": "2432329",
    "end": "2437489"
  },
  {
    "text": "still using the rest of query optimizer even though part of the data are not in the Russia cluster",
    "start": "2437489",
    "end": "2444499"
  },
  {
    "text": "next is the concurrency scaling of features as I mentioned one of the our",
    "start": "2444499",
    "end": "2450140"
  },
  {
    "text": "issue was a concept concurrency and to this concurrency scaling feature address",
    "start": "2450140",
    "end": "2456479"
  },
  {
    "text": "our concern now with this features Russia will automatically instantiate the new transient cluster in the",
    "start": "2456479",
    "end": "2463199"
  },
  {
    "text": "background to meet our sudden spike of workload and these transient clusters",
    "start": "2463199",
    "end": "2468959"
  },
  {
    "text": "are trying to transparent to our users and these cluster are removed at once",
    "start": "2468959",
    "end": "2474539"
  },
  {
    "text": "that workload is completed now with this feature we also see there's an less",
    "start": "2474539",
    "end": "2481319"
  },
  {
    "text": "number nose needed to to complete our existing workload and then I'll show you",
    "start": "2481319",
    "end": "2487439"
  },
  {
    "text": "later what I mean by that for every 24 hours our main cluster is up we get one hour",
    "start": "2487439",
    "end": "2493709"
  },
  {
    "text": "of credit so again that is called our cost saving so we spend on numerous time",
    "start": "2493709",
    "end": "2501299"
  },
  {
    "text": "that I mentioned that we have to work with application teams to prioritize our workload we also spend time on auto",
    "start": "2501299",
    "end": "2509819"
  },
  {
    "text": "vacuum hood for the vacuum and some other maintenance job in order to make sure that our user can run our query",
    "start": "2509819",
    "end": "2517439"
  },
  {
    "text": "optimally so with the auto workload manager and in order vacuum features",
    "start": "2517439",
    "end": "2522900"
  },
  {
    "text": "that rest of providing this save our administration caught effort and then",
    "start": "2522900",
    "end": "2528449"
  },
  {
    "text": "our DBS is able to devote its time to do more database engineering there are",
    "start": "2528449",
    "end": "2534569"
  },
  {
    "text": "other two factors that we consider Russia for we're building data Lake in",
    "start": "2534569",
    "end": "2541410"
  },
  {
    "text": "AWS and Russia is part of the AWS ecosystems so what that means is enable",
    "start": "2541410",
    "end": "2549329"
  },
  {
    "text": "us to share the data within the daily lake and within our a diverse ecosystems the",
    "start": "2549329",
    "end": "2556410"
  },
  {
    "text": "other thing is the ratio consists constantly adding new features and it has a rich role map so here's a very",
    "start": "2556410",
    "end": "2569549"
  },
  {
    "text": "very high level how our existing on-premise architecture look like in our",
    "start": "2569549",
    "end": "2574589"
  },
  {
    "text": "data warehouse environment we have upstream applications which feed the data into our data warehouse and we have",
    "start": "2574589",
    "end": "2581819"
  },
  {
    "text": "a downstream application also getting the data from our data warehouse now we have a reporting that running using the",
    "start": "2581819",
    "end": "2588480"
  },
  {
    "text": "popular VI tools and those are accessing our data warehouse then we have a hack users that access our data warehouse",
    "start": "2588480",
    "end": "2595680"
  },
  {
    "text": "directly and then running a hoc queries now going to the data warehouse data",
    "start": "2595680",
    "end": "2602400"
  },
  {
    "text": "data Lake our strategy is having all these data available in s3 first so",
    "start": "2602400",
    "end": "2609359"
  },
  {
    "start": "2607000",
    "end": "2607000"
  },
  {
    "text": "there's a numerous way to hydrate these data in LW s so in this data warehouse",
    "start": "2609359",
    "end": "2615869"
  },
  {
    "text": "use case what we did is we develop our custom solutions and we extract the data",
    "start": "2615869",
    "end": "2623519"
  },
  {
    "text": "from our data warehouse putting a flat file and then we move those data over to",
    "start": "2623519",
    "end": "2629430"
  },
  {
    "text": "s3 now from s3 we can do on numerous things using numerous different services",
    "start": "2629430",
    "end": "2636210"
  },
  {
    "text": "based on the use case different use cases for the data we need to do further transformations we use EMR and for user",
    "start": "2636210",
    "end": "2644099"
  },
  {
    "text": "we really just want to do a simple query and then we want to access the raw data",
    "start": "2644099",
    "end": "2650160"
  },
  {
    "text": "doesn't they don't want to bring up the rest on their own for those users they",
    "start": "2650160",
    "end": "2656849"
  },
  {
    "text": "use Athena then our regular rest of our regular there are warehouse users there",
    "start": "2656849",
    "end": "2663029"
  },
  {
    "text": "will be when they move to cloud they will be using Russian so with this",
    "start": "2663029",
    "end": "2668579"
  },
  {
    "text": "approach we are enable our applications as well as our business users to migrate",
    "start": "2668579",
    "end": "2675869"
  },
  {
    "text": "to AWS being phases and with this approach this also relief our",
    "start": "2675869",
    "end": "2682490"
  },
  {
    "text": "scalability issues on premise so here",
    "start": "2682490",
    "end": "2687980"
  },
  {
    "start": "2687000",
    "end": "2687000"
  },
  {
    "text": "are two one of the benchmark that I would like to share with you in this benchmark what we did is we use the",
    "start": "2687980",
    "end": "2695240"
  },
  {
    "text": "concurrency scaling features and the blue bar represents a rush shift 16",
    "start": "2695240",
    "end": "2701690"
  },
  {
    "text": "notes and red bar that represent a rush of anil with concurrency scaling feature",
    "start": "2701690",
    "end": "2707510"
  },
  {
    "text": "enabled then we run our ten most executed sequels from anywhere from",
    "start": "2707510",
    "end": "2713599"
  },
  {
    "text": "simple to complex equals and we put the result side-by-side so as you can see in",
    "start": "2713599",
    "end": "2720650"
  },
  {
    "text": "this benchmark the aggressive a nose will concurrency scaling features is",
    "start": "2720650",
    "end": "2725900"
  },
  {
    "text": "actually achieving similar or better performance than rest of sixteen note",
    "start": "2725900",
    "end": "2733030"
  },
  {
    "text": "here is another benchmark that we did this time we use the same number of notes blue line represent a rush of a",
    "start": "2733089",
    "end": "2740960"
  },
  {
    "text": "note without scaling features and red line represent a rush of a nose with a",
    "start": "2740960",
    "end": "2747830"
  },
  {
    "text": "concurrency scaring features then what we did is we run one of the compressed",
    "start": "2747830",
    "end": "2753800"
  },
  {
    "text": "query that we have which does a five table joins and we run this query anywhere from 1 to 100 concurrency so as",
    "start": "2753800",
    "end": "2762290"
  },
  {
    "text": "you can see without concurrency features the execution time of these query",
    "start": "2762290",
    "end": "2767480"
  },
  {
    "text": "increases as the query concurrences increase now with the concurrency",
    "start": "2767480",
    "end": "2774170"
  },
  {
    "text": "feature concurrency scaling features enable the execution time virtue is stay flat",
    "start": "2774170",
    "end": "2780410"
  },
  {
    "text": "meaning the performance did not degree as the query concurrency increases so",
    "start": "2780410",
    "end": "2788359"
  },
  {
    "text": "here the couple lesson learned first is choose the node type wisely so",
    "start": "2788359",
    "end": "2793790"
  },
  {
    "text": "we have the dc2 then compute as well as the ten storage different know type and",
    "start": "2793790",
    "end": "2802940"
  },
  {
    "text": "for most of our workload that DC to work well for us because most of our workloads are read intensive but for",
    "start": "2802940",
    "end": "2810470"
  },
  {
    "text": "some of our usage cases where we have a right intensive heavy right some in those cases the s2",
    "start": "2810470",
    "end": "2817710"
  },
  {
    "text": "we found in those cases the s2 found a better for us and if you're using dc1",
    "start": "2817710",
    "end": "2823890"
  },
  {
    "text": "still using this you want today I really recommend you to take a look at DC 2 when we move this e 1 2 DC 2 with a",
    "start": "2823890",
    "end": "2830810"
  },
  {
    "text": "significant performance improvement second on the concurrency scaling",
    "start": "2830810",
    "end": "2836820"
  },
  {
    "text": "features currently works for real only queries so what that means is you're right transactions or your read queries",
    "start": "2836820",
    "end": "2844590"
  },
  {
    "text": "that creates temporary tables those will remain in the main cluster then your",
    "start": "2844590",
    "end": "2850830"
  },
  {
    "text": "real only queries will be distributed to the transient clusters to distribute to",
    "start": "2850830",
    "end": "2857430"
  },
  {
    "text": "continue to perform your queries and more nodes do not always feel the better",
    "start": "2857430",
    "end": "2863610"
  },
  {
    "text": "performance so what in our one of our use cases once we crossed a certain",
    "start": "2863610",
    "end": "2870120"
  },
  {
    "text": "number of knows the performance actually degrade so that's what we mean by that find your sweet spot in other case where",
    "start": "2870120",
    "end": "2878780"
  },
  {
    "text": "we have the a node initially we have a node cluster and then we increases to 12",
    "start": "2878780",
    "end": "2886080"
  },
  {
    "text": "notes right and we eat 10% more performance gain so the",
    "start": "2886080",
    "end": "2891150"
  },
  {
    "text": "question is does it worth it so people ask does redshift perform or",
    "start": "2891150",
    "end": "2898020"
  },
  {
    "text": "can Russia perform to what what I want so I would like to rephrase the",
    "start": "2898020",
    "end": "2904170"
  },
  {
    "text": "questions to how much do I want to pay for the performance I want to gain so",
    "start": "2904170",
    "end": "2909780"
  },
  {
    "text": "when you're looking for performance always look for performance plus cost and I mentioned about concurrency",
    "start": "2909780",
    "end": "2918390"
  },
  {
    "text": "scaling feature earlier so what about if I have a lot of right so for that we",
    "start": "2918390",
    "end": "2924720"
  },
  {
    "text": "have looked at elastic resize and that gives us the more compute power",
    "start": "2924720",
    "end": "2930330"
  },
  {
    "text": "tymberlee for us to be able to complete our data load faster and lastly another",
    "start": "2930330",
    "end": "2937850"
  },
  {
    "text": "things I want to mention about is the no test so make sure that you have the",
    "start": "2937850",
    "end": "2945200"
  },
  {
    "text": "meaningful data volume for your functional tasks in our early days when",
    "start": "2945200",
    "end": "2950870"
  },
  {
    "text": "we test our with our Russian with tests we have very small data sets and I did not really see the result that we wanted",
    "start": "2950870",
    "end": "2958310"
  },
  {
    "text": "and that it was because we are using to our data volume was too too small so",
    "start": "2958310",
    "end": "2965750"
  },
  {
    "text": "just make sure that you have the meaningful data volume when you're",
    "start": "2965750",
    "end": "2971420"
  },
  {
    "text": "testing your functional now in terms of a performance testing like I mentioned need to find out your",
    "start": "2971420",
    "end": "2978320"
  },
  {
    "text": "sweet spot so for that with the numerous different numerous different tests we",
    "start": "2978320",
    "end": "2985700"
  },
  {
    "text": "try a different number nodes like I mentioned and then we try with and without concurrency scaling features",
    "start": "2985700",
    "end": "2992930"
  },
  {
    "text": "right and also with we try with a different number concurrency query",
    "start": "2992930",
    "end": "3000820"
  },
  {
    "text": "concurrency so user will give us here's the number of concurrency that I want",
    "start": "3000820",
    "end": "3006160"
  },
  {
    "text": "but not always the concurrency that you have you have been running in your",
    "start": "3006160",
    "end": "3011200"
  },
  {
    "text": "current system today translator - that's what you're going to see in the restaurant so we use a different number",
    "start": "3011200",
    "end": "3017500"
  },
  {
    "text": "of the concurrency to test and make sure that we find the optimal configuration to start with now once you start your",
    "start": "3017500",
    "end": "3024250"
  },
  {
    "text": "Russian ship with the configuration you want if you need to adjust write adding more knows or I need to resize then",
    "start": "3024250",
    "end": "3032050"
  },
  {
    "text": "that's available for you at the tip of your him finger available to you there",
    "start": "3032050",
    "end": "3038670"
  },
  {
    "text": "and finally I would like to mention about the non-functional requirement",
    "start": "3038670",
    "end": "3045750"
  },
  {
    "text": "like I mentioned we are building our Dale Lake in AWS and so either",
    "start": "3045750",
    "end": "3051190"
  },
  {
    "text": "integration with various AWS services are very important to us besides performance and also we are very",
    "start": "3051190",
    "end": "3060450"
  },
  {
    "text": "cognizant about where our data goes and with the query deep we can query",
    "start": "3060450",
    "end": "3067570"
  },
  {
    "text": "directly from our s3 bucket this is a great feature for us and then also of having our data",
    "start": "3067570",
    "end": "3073990"
  },
  {
    "text": "in our account is very important to us so with the rest of Andrusha spectrum the data will stay in our account so",
    "start": "3073990",
    "end": "3083320"
  },
  {
    "text": "with that thank you very much for coming today if I can ask you to fill out the survey and let us know your feedback that would",
    "start": "3083320",
    "end": "3089260"
  },
  {
    "text": "be great",
    "start": "3089260",
    "end": "3091560"
  }
]