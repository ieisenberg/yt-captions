[
  {
    "start": "0",
    "end": "104000"
  },
  {
    "text": "hello and welcome to architecting an open derelict for enterprise my name is Pratap Pratap ramamurti I'm a",
    "start": "120",
    "end": "9300"
  },
  {
    "text": "Solutions Architect at Amazon Web Services and I will be your host and moderator for today's webinar in",
    "start": "9300",
    "end": "16320"
  },
  {
    "text": "addition to my introductory presentation on derelict and AWS today we will hear",
    "start": "16320",
    "end": "22380"
  },
  {
    "text": "from Ashwin Vishwanath director cloud product marketing at talent and Erik Anderson",
    "start": "22380",
    "end": "31250"
  },
  {
    "text": "executive director of data at Beachbody right what is today's agenda first I",
    "start": "31250",
    "end": "38219"
  },
  {
    "text": "would give an overview of AWS and AWS marketplace with an emphasis on AWS",
    "start": "38219",
    "end": "44219"
  },
  {
    "text": "daedalic solutions and talent then we'll go over the overview of talents solution",
    "start": "44219",
    "end": "50940"
  },
  {
    "text": "feature in our story we'll also discuss the challenges faced by Beachbody the",
    "start": "50940",
    "end": "58140"
  },
  {
    "text": "Beachbody success story with AWS and talent finally we will have Q&A",
    "start": "58140",
    "end": "63870"
  },
  {
    "text": "discussion all right what are our learning objectives today little glitches as you",
    "start": "63870",
    "end": "69119"
  },
  {
    "text": "can see on the screen are how to migrate a variety of structured and unstructured",
    "start": "69119",
    "end": "75299"
  },
  {
    "text": "data sources to a derelict how to",
    "start": "75299",
    "end": "80310"
  },
  {
    "text": "shorten the development cycle and testing cycles how to mitigate complex",
    "start": "80310",
    "end": "86130"
  },
  {
    "text": "deployment challenges common to real-time data and how to take advantage",
    "start": "86130",
    "end": "91259"
  },
  {
    "text": "of spark and hadoop by generating native code and the data link and Alypius let's",
    "start": "91259",
    "end": "99900"
  },
  {
    "text": "look at how we've been traditionally storing data traditionally when",
    "start": "99900",
    "end": "105930"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "businesses stored data it is usually in their warehouses or other traditional",
    "start": "105930",
    "end": "113630"
  },
  {
    "text": "relational databases these date back to the pre cloud era they are complex to",
    "start": "113630",
    "end": "121259"
  },
  {
    "text": "set up they take months to add new data sources the queries take too long and",
    "start": "121259",
    "end": "127920"
  },
  {
    "text": "require large upfront costs so what is the new paradigm so",
    "start": "127920",
    "end": "133349"
  },
  {
    "text": "the new paradigm is deadly why should I build it at a lake or what is a derelict",
    "start": "133349",
    "end": "139730"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "this is a different way of thinking about collecting data but you don't have",
    "start": "139730",
    "end": "145349"
  },
  {
    "text": "to choose the schema or try to make sense of the data when you're collecting",
    "start": "145349",
    "end": "151730"
  },
  {
    "text": "the idea is to start dumping the data into a large repository and let the data",
    "start": "151730",
    "end": "158219"
  },
  {
    "text": "guru search through the data afterwards this is very similar to trying to win",
    "start": "158219",
    "end": "164459"
  },
  {
    "text": "the lottery by buying all the tickets building a derelict on it of us you know",
    "start": "164459",
    "end": "171230"
  },
  {
    "start": "168000",
    "end": "168000"
  },
  {
    "text": "AWS has several building blocks that help you build a derelict the main",
    "start": "171230",
    "end": "177930"
  },
  {
    "text": "component of a derelict on AWS is Amazon s3 a secure cost-effective storage this",
    "start": "177930",
    "end": "186269"
  },
  {
    "text": "can store any kind of data as it's not trying to fit data into a particular",
    "start": "186269",
    "end": "192299"
  },
  {
    "text": "format or a schema in addition to not to having a storage you will need other",
    "start": "192299",
    "end": "199109"
  },
  {
    "text": "aspects of data ingestion mechanisms that lets you bring the data to F 3 then",
    "start": "199109",
    "end": "205379"
  },
  {
    "text": "catalog the data in in DynamoDB or elastic cache that make it easy to",
    "start": "205379",
    "end": "211829"
  },
  {
    "text": "search the data then you need to worry about making it easy to access the data",
    "start": "211829",
    "end": "218519"
  },
  {
    "text": "using I could be using I M or Amazon kognito the data needs to be secure and",
    "start": "218519",
    "end": "225930"
  },
  {
    "text": "you could use our Amazon security services that are available on AWS",
    "start": "225930",
    "end": "231769"
  },
  {
    "text": "finally processing or analyzing the data using EMR redshift or build predictive",
    "start": "231769",
    "end": "239040"
  },
  {
    "text": "models using Amazon machine learning is where you consume the data so a daily",
    "start": "239040",
    "end": "244079"
  },
  {
    "text": "consists of a central storage and all these other features that are required for you to be able to use the data in an",
    "start": "244079",
    "end": "251159"
  },
  {
    "text": "enterprise environment now let's rethink on how to become data driven business and how are you going to",
    "start": "251159",
    "end": "257609"
  },
  {
    "start": "254000",
    "end": "254000"
  },
  {
    "text": "use is generally the first thing is to start with business insights right start",
    "start": "257609",
    "end": "264570"
  },
  {
    "text": "with insights and actions that you want to drive as in what exactly is the business",
    "start": "264570",
    "end": "270419"
  },
  {
    "text": "outcome of building this derelict right off if building this big data pipeline",
    "start": "270419",
    "end": "276000"
  },
  {
    "text": "I'd start from there then once you have that we now start to run small",
    "start": "276000",
    "end": "282080"
  },
  {
    "text": "experiments try certain things but this is not at large scale but try it out in",
    "start": "282080",
    "end": "288060"
  },
  {
    "text": "smaller scale with smaller data sets maybe and then what you do is you you",
    "start": "288060",
    "end": "294120"
  },
  {
    "text": "find the ones that succeed and scale those processes up and you pay only for",
    "start": "294120",
    "end": "301949"
  },
  {
    "text": "the successful ones then you kill off the ones that are not successful to bear the experiments that did not come out",
    "start": "301949",
    "end": "307410"
  },
  {
    "text": "you can just kill them off and that is not going to be expensive for you because it was not a done at large scale",
    "start": "307410",
    "end": "314759"
  },
  {
    "text": "right when you do this when you try to experiment and we have a lower risk of experimentation this lets you to be it",
    "start": "314759",
    "end": "322830"
  },
  {
    "text": "helps you to be agile and rapidly innovate so this would take when you are trying to scale the successful",
    "start": "322830",
    "end": "329870"
  },
  {
    "text": "experiment or trying to experiment itself now this would take just minutes",
    "start": "329870",
    "end": "335400"
  },
  {
    "text": "to set up this infrastructure and which you usually takes months so please take",
    "start": "335400",
    "end": "341909"
  },
  {
    "text": "advantage of a rich platform of services that respond quickly to change think business needs so we talked about",
    "start": "341909",
    "end": "349260"
  },
  {
    "start": "349000",
    "end": "349000"
  },
  {
    "text": "business case so let's talk about what business cases you can really focus on a",
    "start": "349260",
    "end": "354680"
  },
  {
    "text": "non-blue process that we go through with our customers is to say hey you've got a",
    "start": "354680",
    "end": "359729"
  },
  {
    "text": "lot of great ideas but generally you have a you need to have a short list of",
    "start": "359729",
    "end": "364830"
  },
  {
    "text": "things that you think will most impact your business specifically around a",
    "start": "364830",
    "end": "370650"
  },
  {
    "text": "possibly sick space possibly around your business relationships with your",
    "start": "370650",
    "end": "376110"
  },
  {
    "text": "customers so let's start with that short list and get down to what what to start",
    "start": "376110",
    "end": "383610"
  },
  {
    "text": "with on respect to data and that will have the most impact so what aspect of this is the choice of",
    "start": "383610",
    "end": "392430"
  },
  {
    "text": "data one way to look at the kind of data is the recency old error versus new data",
    "start": "392430",
    "end": "398639"
  },
  {
    "text": "right this would this is this is coming from the type of data you're choosing to meet your business",
    "start": "398639",
    "end": "405629"
  },
  {
    "text": "requirements right processing real-time data as it arrives can let you make",
    "start": "405629",
    "end": "411240"
  },
  {
    "text": "decisions much faster and get the most value from an error but building your",
    "start": "411240",
    "end": "417869"
  },
  {
    "text": "own custom applications to process streaming data is complicated and resource intensive you need to train or",
    "start": "417869",
    "end": "426300"
  },
  {
    "text": "hire developers with the right skill sets and then wait four months for the applications to be built and fighting to",
    "start": "426300",
    "end": "432479"
  },
  {
    "text": "you operate and scale the application as business grows all of this takes a lot",
    "start": "432479",
    "end": "438629"
  },
  {
    "text": "of time money at the end of the day lots of companies just never get there and so",
    "start": "438629",
    "end": "445050"
  },
  {
    "text": "they settle for the status quo and live with the information that is hours or days old",
    "start": "445050",
    "end": "450569"
  },
  {
    "text": "alright but that is a starting point you should then look at what sources of data would be usable to automate that",
    "start": "450569",
    "end": "456809"
  },
  {
    "text": "response now in this case you could be using existing data or you could be",
    "start": "456809",
    "end": "462270"
  },
  {
    "text": "using a real-time data like ERP transactions okay what are the outcomes",
    "start": "462270",
    "end": "469050"
  },
  {
    "start": "468000",
    "end": "468000"
  },
  {
    "text": "of modern data architecture you could modernize and consolidate your business",
    "start": "469050",
    "end": "475529"
  },
  {
    "text": "applications you could innovate for new revenue streams it could use this for",
    "start": "475529",
    "end": "481469"
  },
  {
    "text": "real-time engagement with customers or you could automate for expands of beach and here's an example of a big data",
    "start": "481469",
    "end": "488939"
  },
  {
    "start": "487000",
    "end": "487000"
  },
  {
    "text": "pipeline one important thing to notice that Amazon interviews has lots of",
    "start": "488939",
    "end": "494099"
  },
  {
    "text": "building blocks and for you to use the right services for your needs here it",
    "start": "494099",
    "end": "500159"
  },
  {
    "text": "starts with Amazon history where the data is stored and we are using glue",
    "start": "500159",
    "end": "505669"
  },
  {
    "text": "crawlers to catalog and populate the new catalog and once it's the Dara's catalog",
    "start": "505669",
    "end": "513209"
  },
  {
    "text": "we don't know what kind of data is in history you could use Amazon s3 or EMR",
    "start": "513209",
    "end": "518698"
  },
  {
    "text": "or redshift spectrum to analyze the data or process the data and finally once",
    "start": "518699",
    "end": "524699"
  },
  {
    "text": "you've done that you can use a business intelligence tool like quick site right but in the center of all this is Amazon",
    "start": "524699",
    "end": "532889"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "s3 the Amazon s3 is where you store the data the nail features of f3 are exactly",
    "start": "532889",
    "end": "539400"
  },
  {
    "text": "what you want from a derelict this is nearly impossible to achieve with a",
    "start": "539400",
    "end": "545550"
  },
  {
    "text": "fixed set database cluster some of the features are its replicated across availability zones for high availability",
    "start": "545550",
    "end": "552720"
  },
  {
    "text": "and durability massively parallel and scalable it's low storage cost at less",
    "start": "552720",
    "end": "560520"
  },
  {
    "text": "than point zero to five dollars or 2.5 cents per GB stores and scales",
    "start": "560520",
    "end": "568160"
  },
  {
    "text": "independent of compute let me repeat that it stores and scales independent of",
    "start": "568160",
    "end": "574260"
  },
  {
    "text": "compute so you store you pay only for in storage now let's look at that paradigm",
    "start": "574260",
    "end": "579570"
  },
  {
    "text": "a little bit more so traditional data bearing losses were tied to Hardware",
    "start": "579570",
    "end": "584690"
  },
  {
    "text": "which is very traditional everybody's very filtration you store the data it's all usually tied to Hardware either as a",
    "start": "584690",
    "end": "591570"
  },
  {
    "text": "dependency or as a licensing mechanism this dependency creates inefficiencies",
    "start": "591570",
    "end": "596910"
  },
  {
    "text": "and restricts the growth of Dera and or increases the costs more and",
    "start": "596910",
    "end": "603480"
  },
  {
    "text": "bigger architectures does not have this dependency and storage and compute are decoupled this helps us to help both",
    "start": "603480",
    "end": "613110"
  },
  {
    "text": "storage and compute scale independently right but the day you need more data you",
    "start": "613110",
    "end": "618780"
  },
  {
    "text": "scale a storage field today I need to do more processing is when you scale the compute now let's look at how we can",
    "start": "618780",
    "end": "626850"
  },
  {
    "text": "improve their agility with talent thank you very much for thought and call",
    "start": "626850",
    "end": "633660"
  },
  {
    "text": "everyone and thank you for attending today's webinar so basically today's topic is about data Lake and the fact is",
    "start": "633660",
    "end": "640380"
  },
  {
    "start": "638000",
    "end": "638000"
  },
  {
    "text": "that stakes are ever higher when it comes to big data so for instance IDC",
    "start": "640380",
    "end": "647160"
  },
  {
    "text": "predicted just a few years ago back in 2015 that revenue from sales of big data",
    "start": "647160",
    "end": "652920"
  },
  {
    "text": "an analyst applications would really increase more than 50% from five hundred",
    "start": "652920",
    "end": "658770"
  },
  {
    "text": "twenty two billion back in 2015 to more than one hundred eighty seven billion in 2019 other research indicated that",
    "start": "658770",
    "end": "667170"
  },
  {
    "text": "over 70 percent of companies intended on increasing spending on their analytics",
    "start": "667170",
    "end": "672540"
  },
  {
    "text": "and making sure that data discovery was a very very significant part of architecture however research also",
    "start": "672540",
    "end": "680879"
  },
  {
    "text": "showed that in terms of the success of these big data project",
    "start": "680879",
    "end": "686309"
  },
  {
    "text": "half of them were expected to fail to deliver against original expectation now",
    "start": "686309",
    "end": "691949"
  },
  {
    "text": "that is a huge number considering the amount of investment in the states so beta legs were really brought in as a",
    "start": "691949",
    "end": "700259"
  },
  {
    "start": "697000",
    "end": "697000"
  },
  {
    "text": "way to ease some of the difficulty of big data and the first phase of it was",
    "start": "700259",
    "end": "706109"
  },
  {
    "text": "actually quite successful the first phase of data leaks all about capturing and storing raw data of several",
    "start": "706109",
    "end": "714899"
  },
  {
    "text": "different types so for instance this bit as great for consumer businesses because",
    "start": "714899",
    "end": "720509"
  },
  {
    "text": "it would help increase the speed and quality of a web search the efficiency of their advertising to clickstream data",
    "start": "720509",
    "end": "728869"
  },
  {
    "text": "improve customer interaction and behavior across different web channels",
    "start": "728869",
    "end": "734489"
  },
  {
    "text": "so this is really great for ingesting this these types of data and analyzing them but now the next phase of data",
    "start": "734489",
    "end": "743489"
  },
  {
    "text": "leaks a little bit more challenging because it's not only the raw types of data need to analyze but also the",
    "start": "743489",
    "end": "750179"
  },
  {
    "text": "structure type of data such as data warehouses databases and what-have-you",
    "start": "750179",
    "end": "755910"
  },
  {
    "text": "and the trouble in co-mingling these different types of data is the fact that there's a lot of cleaning and other work",
    "start": "755910",
    "end": "762360"
  },
  {
    "text": "that's required before the data is actually properly captured and waiting for mods and so a data Lake of course is",
    "start": "762360",
    "end": "769489"
  },
  {
    "text": "is a great way in order to ingest raw data as well as structured data and",
    "start": "769489",
    "end": "777029"
  },
  {
    "text": "making sure that the next phase of the project but data quality the right",
    "start": "777029",
    "end": "783329"
  },
  {
    "text": "transformations it is covered that part divided to taking care of successfully so that's what the next phase of beta",
    "start": "783329",
    "end": "790259"
  },
  {
    "text": "life is all about let's take a look at some of the reasons why daily projects",
    "start": "790259",
    "end": "795600"
  },
  {
    "start": "792000",
    "end": "792000"
  },
  {
    "text": "could fail the number one reason by far is lack of expertise now specifically when it comes to Big",
    "start": "795600",
    "end": "802240"
  },
  {
    "text": "Data technologies there's a ton of technologies out there spark Kafka but",
    "start": "802240",
    "end": "808630"
  },
  {
    "text": "multiple different paradigm and approached not only that you have data",
    "start": "808630",
    "end": "813910"
  },
  {
    "text": "engineers who typically sit in IT well you have data scientist this is the line of business so there's a lot of the",
    "start": "813910",
    "end": "820240"
  },
  {
    "text": "siloed operating model that also feeds in hand to hand what sounds lack of",
    "start": "820240",
    "end": "825250"
  },
  {
    "text": "expertise that can contras the failure for heroic project next really poor",
    "start": "825250",
    "end": "831399"
  },
  {
    "text": "architectural design and integration I mentioned earlier that the next phase of",
    "start": "831399",
    "end": "837010"
  },
  {
    "text": "data Lake projects really have to co-mingle raw data together sculpture",
    "start": "837010",
    "end": "842500"
  },
  {
    "text": "data but we don't have the right architecture set up to integrate these different data sources together that can",
    "start": "842500",
    "end": "850300"
  },
  {
    "text": "lead to your data Lake project fail then DevOps practices now give up it is very",
    "start": "850300",
    "end": "857649"
  },
  {
    "text": "very essential to data Lake because it helps to make sure that as your data volume scale that your that your big",
    "start": "857649",
    "end": "864790"
  },
  {
    "text": "data processes specific data a product's also scale accordingly because one of",
    "start": "864790",
    "end": "870880"
  },
  {
    "text": "the worst things that can happen in any survival is project is that you do all the work ingest the data that's really",
    "start": "870880",
    "end": "877870"
  },
  {
    "text": "half the bath but by the time you send it downstream other analytical",
    "start": "877870",
    "end": "883930"
  },
  {
    "text": "applications for consumption the data is already stale or out-of-date and as a",
    "start": "883930",
    "end": "889270"
  },
  {
    "text": "result whatever analytics perform on that data isn't very actionable by the",
    "start": "889270",
    "end": "895870"
  },
  {
    "text": "line of business stakeholders so that's why I get off to a very very important part of making sure the data gets from",
    "start": "895870",
    "end": "903279"
  },
  {
    "text": "the source the other intended recipients in the most timely manner possible last but not least need a government a",
    "start": "903279",
    "end": "909220"
  },
  {
    "text": "data governance is an area that a lot of companies don't really pay attention to",
    "start": "909220",
    "end": "914949"
  },
  {
    "text": "but it's extremely important in the context of data and really it's around the fact that data may be secure while",
    "start": "914949",
    "end": "923140"
  },
  {
    "text": "the same time you can be accessible by a variety different personas within your organization",
    "start": "923140",
    "end": "929570"
  },
  {
    "text": "and the way to do that is to ensure that the proper data governance procedures in place to make sure that data secure all",
    "start": "929570",
    "end": "937100"
  },
  {
    "text": "the way from the time it reaches into the data Lake making sure that the right",
    "start": "937100",
    "end": "943400"
  },
  {
    "text": "person is whether it business analyst whether it be the scientist so the engineers all have access self-service",
    "start": "943400",
    "end": "950270"
  },
  {
    "text": "access to that data but also to make sure that in sensitive information is properly masks so with that what are",
    "start": "950270",
    "end": "957710"
  },
  {
    "text": "some of the foundational elements that need to ensure as part of your data Lake project from the very beginning first",
    "start": "957710",
    "end": "964130"
  },
  {
    "start": "958000",
    "end": "958000"
  },
  {
    "text": "and foremost the ability to have self every data engine I mentioned earlier",
    "start": "964130",
    "end": "969380"
  },
  {
    "text": "that data ingest is really half of that and so as much as you can automate the",
    "start": "969380",
    "end": "975440"
  },
  {
    "text": "ingestion part of this project the faster you get the data downstream a data lineage and a data profiling are",
    "start": "975440",
    "end": "984050"
  },
  {
    "text": "very important parts to ensure that you can really track the source of the",
    "start": "984050",
    "end": "989210"
  },
  {
    "text": "various types of data in case you need to audit from later on but also that you can profile a different data types how",
    "start": "989210",
    "end": "996080"
  },
  {
    "text": "many of them are just semi structured data such as JSON how many of them are complex xml structures but those those",
    "start": "996080",
    "end": "1004540"
  },
  {
    "text": "kind of profile information can really help to optimize your downstream proxies",
    "start": "1004540",
    "end": "1011560"
  },
  {
    "text": "whether you're using Hadoop as a service clusters downstream or whether using a",
    "start": "1011560",
    "end": "1017740"
  },
  {
    "text": "cloud data warehouse downstream can really help you you're able to profile a data early on let's also not forget the",
    "start": "1017740",
    "end": "1025120"
  },
  {
    "text": "important part of data preparation a data preparation in area that's really important for line of business so with",
    "start": "1025120",
    "end": "1032770"
  },
  {
    "text": "the increasing amount of data out there it's very important that line of business personas such as business",
    "start": "1032770",
    "end": "1038170"
  },
  {
    "text": "analysts have access to the data now prior to pushing the data into a bi tool",
    "start": "1038170",
    "end": "1045819"
  },
  {
    "text": "searches Amazon quick side you really need to make sure that the data is prepared the right way a bit of",
    "start": "1045820",
    "end": "1052360"
  },
  {
    "text": "preparation really helps so let me get into some town specific features that",
    "start": "1052360",
    "end": "1058270"
  },
  {
    "start": "1054000",
    "end": "1054000"
  },
  {
    "text": "can help so first and foremost I want to touch on a very important area of deaf",
    "start": "1058270",
    "end": "1064650"
  },
  {
    "text": "so mentioned earlier why DevOps is important and how it can really help you",
    "start": "1064650",
    "end": "1070080"
  },
  {
    "text": "scale your big data jobs so the specific features of talent provide in the area",
    "start": "1070080",
    "end": "1076260"
  },
  {
    "text": "of DevOps is the ability to to really move your doop jobs seamlessly from dev",
    "start": "1076260",
    "end": "1083280"
  },
  {
    "text": "to test across now we know that in order to scale your Big Data projects across the entire organization you really need",
    "start": "1083280",
    "end": "1091680"
  },
  {
    "text": "to optimize a different role that will be within each environment we have certain developers they're dedicated to",
    "start": "1091680",
    "end": "1099000"
  },
  {
    "text": "a dev environment others are dedicated testing and yet others are dedicated of pre-production and production that each",
    "start": "1099000",
    "end": "1106310"
  },
  {
    "text": "different environment might have very specific aduke configuration data so you",
    "start": "1106310",
    "end": "1113100"
  },
  {
    "text": "want to make sure that based on the different roles other developers they can upload the correct configuration file also what you want to make sure of",
    "start": "1113100",
    "end": "1121380"
  },
  {
    "text": "is that you have you home security standards from your central IT organization deployed across every",
    "start": "1121380",
    "end": "1128310"
  },
  {
    "text": "single Big Data project every single environment and this capability is available for both spark and spark",
    "start": "1128310",
    "end": "1134340"
  },
  {
    "text": "screening jobs so this makes it very easy to really port your integration",
    "start": "1134340",
    "end": "1139530"
  },
  {
    "text": "jobs across different environment next let's talk a little bit about calm capabilities for big data matching using",
    "start": "1139530",
    "end": "1145290"
  },
  {
    "start": "1142000",
    "end": "1142000"
  },
  {
    "text": "specialist part machine learning now as your data get more and more voluminous",
    "start": "1145290",
    "end": "1150530"
  },
  {
    "text": "what tends to happen is regardless of how fast you're able to ingest the data",
    "start": "1150530",
    "end": "1156690"
  },
  {
    "text": "it will still take time to process it's just a nature of the beast as more and",
    "start": "1156690",
    "end": "1163500"
  },
  {
    "text": "more data comes in the laws of physics will eventually apply haki optimizes",
    "start": "1163500",
    "end": "1169290"
  },
  {
    "text": "even faster so one of the things we've introduced with talent if you ability to",
    "start": "1169290",
    "end": "1175170"
  },
  {
    "text": "use our data stewardship interface to just simplify this problem a lot of data",
    "start": "1175170",
    "end": "1182520"
  },
  {
    "text": "can contain duplicate data for instance and you want to save yourself time but not having to process duplicate data by",
    "start": "1182520",
    "end": "1189840"
  },
  {
    "text": "getting rid of them before we when starting your short spark process the capabilities",
    "start": "1189840",
    "end": "1195660"
  },
  {
    "text": "that talent brings to the table here there's the ability to use a certain sub",
    "start": "1195660",
    "end": "1201780"
  },
  {
    "text": "segment of the data set that actually has duplicate data and using that as a",
    "start": "1201780",
    "end": "1206940"
  },
  {
    "text": "training set to sample that data and then run a random forest as a training log for you so using that training model",
    "start": "1206940",
    "end": "1214560"
  },
  {
    "text": "we're able to predict where other potential duplicates could occur within",
    "start": "1214560",
    "end": "1220050"
  },
  {
    "text": "that large distance so what this means is that you have a lot more performance",
    "start": "1220050",
    "end": "1225590"
  },
  {
    "text": "improve performance by continuously matching data fit as when they come in",
    "start": "1225590",
    "end": "1231090"
  },
  {
    "text": "and this accelerate your time inside next as a as a integration vendor that",
    "start": "1231090",
    "end": "1237660"
  },
  {
    "text": "committed to Big Data we have several abilities with quark 2.1",
    "start": "1237660",
    "end": "1243240"
  },
  {
    "text": "and for some of you that may still not be a 2.1 we're backwards compatible we",
    "start": "1243240",
    "end": "1248310"
  },
  {
    "text": "allow natural light processing with spark data preparation products that I talked to you about earlier work to",
    "start": "1248310",
    "end": "1255420"
  },
  {
    "text": "spark streaming and we also support kerberized CAFTA so basically we want to",
    "start": "1255420",
    "end": "1261780"
  },
  {
    "text": "make sure that your Big Data projects when you use spark is really ready for",
    "start": "1261780",
    "end": "1266820"
  },
  {
    "text": "the enterprise I talked earlier about governance nobody Pacific Lee comes to big data governance it's very important",
    "start": "1266820",
    "end": "1274770"
  },
  {
    "start": "1268000",
    "end": "1268000"
  },
  {
    "text": "to have end-to-end data lineage so the reason for this is you really want to",
    "start": "1274770",
    "end": "1279930"
  },
  {
    "text": "understand more about your unstructured data and you want to harvest the data",
    "start": "1279930",
    "end": "1284970"
  },
  {
    "text": "structure in order to build your inventory within the data so anytime that data changes as and when",
    "start": "1284970",
    "end": "1291780"
  },
  {
    "text": "you data comes in you want to make sure that you have change in version control you get notified accordingly and so we",
    "start": "1291780",
    "end": "1300090"
  },
  {
    "text": "have several metadata bridges to s3 ETFs different file systems as well different",
    "start": "1300090",
    "end": "1306510"
  },
  {
    "text": "file format and this lift capability is extremely important especially when it",
    "start": "1306510",
    "end": "1312780"
  },
  {
    "text": "comes to the protection of sensitive data are ensuring that maintain regulatory compliance actually with",
    "start": "1312780",
    "end": "1320610"
  },
  {
    "text": "regulations such as GDP are coming around the corner next name",
    "start": "1320610",
    "end": "1325669"
  },
  {
    "text": "so to ensure that you're in Canada protect sensitive data maintain regulatory compliance it's very",
    "start": "1325669",
    "end": "1331519"
  },
  {
    "text": "important to ensure that big data governance prosecutors incorporated from a very big layer and with that I'm going",
    "start": "1331519",
    "end": "1338119"
  },
  {
    "text": "to pass the ball over to Mary Anderson of Beachbody hi everyone so my name is",
    "start": "1338119",
    "end": "1344209"
  },
  {
    "text": "Eric Anderson and I'm going to walk you through our big data journey at Beachbody so first let's talk a little",
    "start": "1344209",
    "end": "1349759"
  },
  {
    "start": "1348000",
    "end": "1348000"
  },
  {
    "text": "bit about Beachbody we're a leading provider of fitness nutrition and weight loss programs including Shakeology p90x",
    "start": "1349759",
    "end": "1356690"
  },
  {
    "text": "insanity 21 day fix and dozens more we've empowered over 23 million customers to live healthy and fulfilling",
    "start": "1356690",
    "end": "1363320"
  },
  {
    "text": "lives with a network of over 350,000 independent coaches and 800 employees we've also seen over 5 million monthly",
    "start": "1363320",
    "end": "1370339"
  },
  {
    "text": "unique visitors across our digital platforms including our streaming fitness platform Beachbody on demand where you can stream all of our latest",
    "start": "1370339",
    "end": "1377659"
  },
  {
    "text": "fitness content online and we achieved over a billion in gross sales back in 2015 all that sounds great but like any",
    "start": "1377659",
    "end": "1385609"
  },
  {
    "start": "1384000",
    "end": "1384000"
  },
  {
    "text": "growth company we still face challenges so let's talk about the challenge the",
    "start": "1385609",
    "end": "1390769"
  },
  {
    "text": "first was the fact that I couldn't help stakeholders to answer questions in a timely manner to the point where the",
    "start": "1390769",
    "end": "1396679"
  },
  {
    "text": "question would be irrelevant by the time it was answered and the second was our ability to answer certain business",
    "start": "1396679",
    "end": "1402649"
  },
  {
    "text": "questions very well like some of the transactional examples you see on the top here which are which are essentially",
    "start": "1402649",
    "end": "1409579"
  },
  {
    "text": "raw facts but our newest product offering Beachbody on demand our online Fitness streaming service was going to",
    "start": "1409579",
    "end": "1416179"
  },
  {
    "text": "have little to no visibility without a more scalable robust platform so we needed to move from transactions to",
    "start": "1416179",
    "end": "1422239"
  },
  {
    "text": "behavior and increase our scale dramatically we decided in order to keep up with the pace of the business it was",
    "start": "1422239",
    "end": "1428419"
  },
  {
    "text": "necessary to create a cloud-based big data capability so what solution would solve all of those challenges we started",
    "start": "1428419",
    "end": "1435709"
  },
  {
    "start": "1432000",
    "end": "1432000"
  },
  {
    "text": "with a vision of what we'd like the platform to do for us then performed a series of proof of concepts to identify",
    "start": "1435709",
    "end": "1441679"
  },
  {
    "text": "the optimal solution for each need independently we also engaged a consulting partner Saul in consulting to",
    "start": "1441679",
    "end": "1447829"
  },
  {
    "text": "help complement our team skillsets and ultimately we built the solution that was an open enterprise data platform so",
    "start": "1447829",
    "end": "1455419"
  },
  {
    "text": "so what does that mean exactly for us it means open in every sense of the it's open-source technology it's open",
    "start": "1455419",
    "end": "1462290"
  },
  {
    "text": "toolsets and it's open protected access it was decentralized data ownership to",
    "start": "1462290",
    "end": "1467810"
  },
  {
    "text": "ensure that this would be a true enterprise asset rather than simply a departmental solution and I'll speak to",
    "start": "1467810",
    "end": "1472820"
  },
  {
    "text": "that a little bit later and while data ownership was decentralized we offer centralized",
    "start": "1472820",
    "end": "1478610"
  },
  {
    "text": "people processes and tools as a shared service and optional software offerings as necessary",
    "start": "1478610",
    "end": "1484810"
  },
  {
    "text": "the platform was built to capture all data as real-time responsible and",
    "start": "1484810",
    "end": "1490060"
  },
  {
    "text": "authorized users should have access to all of the information they need including both processed and raw data",
    "start": "1490060",
    "end": "1495920"
  },
  {
    "text": "sets and the data is encrypted course per our security requirements both at rest and in motion and all of this is",
    "start": "1495920",
    "end": "1503570"
  },
  {
    "text": "done with the ultimate goal of shifting time from collecting data to analyzing that data and adding value so the",
    "start": "1503570",
    "end": "1510140"
  },
  {
    "text": "technology was key to enabling our transformation to a modern data architecture so I'll highlight that here",
    "start": "1510140",
    "end": "1516020"
  },
  {
    "start": "1515000",
    "end": "1515000"
  },
  {
    "text": "but people and processes including a strong DevOps capability we're also key and making that a success so this is an",
    "start": "1516020",
    "end": "1523280"
  },
  {
    "text": "overall solution blueprint of our data platform on the Left we have our source systems including internal external and",
    "start": "1523280",
    "end": "1529790"
  },
  {
    "text": "cloud the middle is the data platform and that consists of various components",
    "start": "1529790",
    "end": "1534800"
  },
  {
    "text": "including Hadoop Cloud Storage MPP databases app servers and more and all",
    "start": "1534800",
    "end": "1540050"
  },
  {
    "text": "of that is hosted in AWS and on the right we have our information consumers which could be analytics operations or",
    "start": "1540050",
    "end": "1546920"
  },
  {
    "text": "various applications including web messaging call centers or others this slide breaks down our s3 data Lake",
    "start": "1546920",
    "end": "1554450"
  },
  {
    "start": "1552000",
    "end": "1552000"
  },
  {
    "text": "architecture into logical grouping by business areas so there are pros and cons to business area versus source",
    "start": "1554450",
    "end": "1560840"
  },
  {
    "text": "system organization single bucket versus multi bucket but this is a decision we made in mind",
    "start": "1560840",
    "end": "1566360"
  },
  {
    "text": "really with end-users in mind so that they can more easily find the data they were looking for and again the key is",
    "start": "1566360",
    "end": "1573740"
  },
  {
    "text": "that ownership of top-level directories is decentralized so that many teams can publish as long as they're conforming to",
    "start": "1573740",
    "end": "1580820"
  },
  {
    "text": "our naming and governance standards which we've clearly defined so drilling in a little more technically this is our",
    "start": "1580820",
    "end": "1586880"
  },
  {
    "start": "1584000",
    "end": "1584000"
  },
  {
    "text": "technical architecture we're using different methods of accessing AWS but the key is that",
    "start": "1586880",
    "end": "1593670"
  },
  {
    "text": "all of those methods are secured and encrypted and this is where DevOps becomes extremely critical especially if",
    "start": "1593670",
    "end": "1600150"
  },
  {
    "text": "you want to have a seamless single sign-on environment for your users we operate various self-service tools but",
    "start": "1600150",
    "end": "1607410"
  },
  {
    "text": "we don't mandate the use of any one of them because as I said earlier we promote a bring your own tool approach to maximize the adoption of our platform",
    "start": "1607410",
    "end": "1614250"
  },
  {
    "text": "the numbers in blue aligned with the next slides that speak to the various proof of concepts that we performed and",
    "start": "1614250",
    "end": "1620910"
  },
  {
    "text": "the independent platform decisions that we made which includes storage data pipeline database compute and analytics",
    "start": "1620910",
    "end": "1629220"
  },
  {
    "text": "so the first decision we made was storage and I think this was one of our most critical decisions we chose s3 over",
    "start": "1629220",
    "end": "1636870"
  },
  {
    "start": "1630000",
    "end": "1630000"
  },
  {
    "text": "HDFS and we made that decision for a few reasons the first was eleven nines of durability",
    "start": "1636870",
    "end": "1642510"
  },
  {
    "text": "that was absolutely unmatched in HDFS it was also cheap so when I look at our AWS",
    "start": "1642510",
    "end": "1649290"
  },
  {
    "text": "invoice I can hardly see the s3 cost especially compared with the cost of any other service we also benchmark the",
    "start": "1649290",
    "end": "1656940"
  },
  {
    "text": "performance of s3 versus HDFS using various tools including hive and presto",
    "start": "1656940",
    "end": "1661980"
  },
  {
    "text": "and although s3 was slower it performed well enough especially based on the two benefits I mentioned earlier and the",
    "start": "1661980",
    "end": "1668850"
  },
  {
    "text": "final reason was for the easy connectivity to other apps we're moving to the cloud and to the other AWS",
    "start": "1668850",
    "end": "1674610"
  },
  {
    "text": "services we wanted to utilize for data pipeline we selected talent after evaluating talent informatica and AWS",
    "start": "1674610",
    "end": "1681630"
  },
  {
    "start": "1677000",
    "end": "1677000"
  },
  {
    "text": "data pipeline there there were many other players in the market although we weren't looking for a Hadoop only",
    "start": "1681630",
    "end": "1687840"
  },
  {
    "text": "solution we need something that worked across our heterogeneous environments both cloud and on-premise e database and",
    "start": "1687840",
    "end": "1695520"
  },
  {
    "text": "big data and talents built in connector state of us spark native processing and",
    "start": "1695520",
    "end": "1700680"
  },
  {
    "text": "its connectivity to our own Prem traditional solutions made it a clear winner in that group we selected",
    "start": "1700680",
    "end": "1706170"
  },
  {
    "start": "1706000",
    "end": "1706000"
  },
  {
    "text": "redshift for our analytic database which is where we copy more of our highly curated data so think of this",
    "start": "1706170",
    "end": "1713190"
  },
  {
    "text": "effectively as a speed layer for analytic consumers all data is an s3 including the data that we push to",
    "start": "1713190",
    "end": "1719790"
  },
  {
    "text": "redshift so hive is still the primary query tool for users for both raw and highly",
    "start": "1719790",
    "end": "1725520"
  },
  {
    "text": "processed data although we're currently evaluating Athena and Richard spectrum as well and business",
    "start": "1725520",
    "end": "1731940"
  },
  {
    "text": "users can connect with any sequel client they prefer or any bi tool or any",
    "start": "1731940",
    "end": "1736980"
  },
  {
    "text": "notebook again where we promote this spring eurotool approach so choosing our Hadoop cluster was a difficult decision",
    "start": "1736980",
    "end": "1743330"
  },
  {
    "start": "1741000",
    "end": "1741000"
  },
  {
    "text": "because with so many competitors in an open-source community and the offerings",
    "start": "1743330",
    "end": "1748860"
  },
  {
    "text": "were becoming commoditized we wanted to pick EMR for simplicity of administration although at the time and",
    "start": "1748860",
    "end": "1755159"
  },
  {
    "text": "this was over a year ago it wasn't ready because it was on a branch it was on a",
    "start": "1755159",
    "end": "1760320"
  },
  {
    "text": "branch during the evaluation and some of the features were one to two years delay which is not the case anymore important",
    "start": "1760320",
    "end": "1767490"
  },
  {
    "text": "works in cloud era were a virtual tie during the evaluation and we really could have picked either distribution at",
    "start": "1767490",
    "end": "1772649"
  },
  {
    "text": "the time each had pros and cons we knew our initial decision was short-term since we'd be looking to move to a",
    "start": "1772649",
    "end": "1779429"
  },
  {
    "text": "managed service eventually so our initial decision was heavily focused on training enablement and Hortonworks",
    "start": "1779429",
    "end": "1784529"
  },
  {
    "text": "scores very well in that space and our final decision was around our analytics engine we chose SPARC because of its",
    "start": "1784529",
    "end": "1791519"
  },
  {
    "text": "language flexibility and available libraries we also push many of our data integration jobs from talent to spark",
    "start": "1791519",
    "end": "1798360"
  },
  {
    "text": "for processing as well so now we can talk about some of the business benefits we achieve by implementing the",
    "start": "1798360",
    "end": "1804090"
  },
  {
    "start": "1800000",
    "end": "1800000"
  },
  {
    "text": "self-service cloud-based platform the first is we reduced our data acquisition time from about two and a half months",
    "start": "1804090",
    "end": "1810809"
  },
  {
    "text": "for typical data source to within a single two-week sprint we created machine learning models to improve the",
    "start": "1810809",
    "end": "1816299"
  },
  {
    "text": "accuracy of our marketing campaigns by incorporating both behavior and search terms we reduce the cost of external",
    "start": "1816299",
    "end": "1822779"
  },
  {
    "text": "tagging vendors like Adobe by leveraging video logs directly instead of tracking that same information in a tagging",
    "start": "1822779",
    "end": "1829470"
  },
  {
    "text": "platform we reduced our employee attrition rates from double-digit to 0% throughout the duration of this project",
    "start": "1829470",
    "end": "1836360"
  },
  {
    "text": "employees were more engaged than ever and eager to learn and we automated",
    "start": "1836360",
    "end": "1841710"
  },
  {
    "text": "operational processes such as customer online order status on our consumer website and finally we retested and",
    "start": "1841710",
    "end": "1849240"
  },
  {
    "text": "identified many web conversion opportunities related to our homepage and other critical site sections and all",
    "start": "1849240",
    "end": "1855389"
  },
  {
    "text": "of these benefits equate to a real ROI that showing value to our stakeholders now I'll pass it back to the top or QA",
    "start": "1855389",
    "end": "1863070"
  },
  {
    "text": "thank you Rick thank you Eric we have a",
    "start": "1863070",
    "end": "1869230"
  },
  {
    "text": "few questions first question this is a",
    "start": "1869230",
    "end": "1874990"
  },
  {
    "text": "question for me how do I deploy the derelict architecture that I showed in",
    "start": "1874990",
    "end": "1881170"
  },
  {
    "text": "the initial slides it obvious provides what is called a CloudFormation template",
    "start": "1881170",
    "end": "1887760"
  },
  {
    "text": "that gives you a vanilla version of the derelict solution that was shown so",
    "start": "1887760",
    "end": "1894820"
  },
  {
    "text": "alpha machine template is alw service which LCR ma deployment of infrastructures that will help you",
    "start": "1894820",
    "end": "1900840"
  },
  {
    "text": "create these extra buckets and all the other services it's attached so that is",
    "start": "1900840",
    "end": "1906310"
  },
  {
    "text": "available you can you can download it from the AWS page you can find the clot",
    "start": "1906310",
    "end": "1914260"
  },
  {
    "start": "1913000",
    "end": "1913000"
  },
  {
    "text": "formation template and the details on the first link here because it'll use that amazon.com slash big data slash",
    "start": "1914260",
    "end": "1921460"
  },
  {
    "text": "data Lake on AWS second question this question is for Eric you mentioned that",
    "start": "1921460",
    "end": "1929590"
  },
  {
    "text": "you use machine learning can you give an example of what machine learning are",
    "start": "1929590",
    "end": "1934810"
  },
  {
    "text": "used on the data in the use case please cure so or for our marketing campaigns",
    "start": "1934810",
    "end": "1943090"
  },
  {
    "text": "we leverage random forests and for our our propensity to turn models we're",
    "start": "1943090",
    "end": "1948100"
  },
  {
    "text": "leveraging extreme gradient boosted trees okay all right so another question",
    "start": "1948100",
    "end": "1954570"
  },
  {
    "text": "by room for enterprises looking to migrate from legacy data warehouses to",
    "start": "1954570",
    "end": "1962410"
  },
  {
    "text": "cloud-based data Lakes are you planning on a workload migration kit in your roadmap example each of your migration",
    "start": "1962410",
    "end": "1969550"
  },
  {
    "text": "or MDM I can answer this first and then hand it over to a shrine in abuse has something called database migration",
    "start": "1969550",
    "end": "1977860"
  },
  {
    "text": "service DMS this is a service that lets you migrate from the traditional data",
    "start": "1977860",
    "end": "1985270"
  },
  {
    "text": "warehouse like Oracle to to anubius data",
    "start": "1985270",
    "end": "1990640"
  },
  {
    "text": "warehouse like redshift we have in over 20,000 users of 20,000 there are",
    "start": "1990640",
    "end": "1999550"
  },
  {
    "text": "code data warehouses have been now using this tool about the MDM and migration",
    "start": "1999550",
    "end": "2007080"
  },
  {
    "text": "kit I'll hand over to a screen you wanna take this sure I mean I wanna well what",
    "start": "2007080",
    "end": "2012960"
  },
  {
    "text": "I want to add is in terms of talents capabilities for MDM and some of the",
    "start": "2012960",
    "end": "2019110"
  },
  {
    "text": "other data warehousing capabilities asking the questions are talent data",
    "start": "2019110",
    "end": "2025920"
  },
  {
    "text": "fabric product is the one that provides all this comprehensive capability so Big Data MDM data governance so the data",
    "start": "2025920",
    "end": "2036600"
  },
  {
    "text": "fabric is really the comprehensive platform that's tied in for all these different capabilities now in terms of",
    "start": "2036600",
    "end": "2044160"
  },
  {
    "text": "actually migration migrating from the legacy data warehouse to let's say a",
    "start": "2044160",
    "end": "2049530"
  },
  {
    "text": "cloud-based data Lake like Amazon s3 we have well over 900 connectors and",
    "start": "2049530",
    "end": "2055860"
  },
  {
    "text": "components including connectors to legacy databases and also connectors to",
    "start": "2055860",
    "end": "2061290"
  },
  {
    "text": "Amazon s3 so what you would do is that the first step in to basically take is",
    "start": "2061290",
    "end": "2066710"
  },
  {
    "text": "connecting to your legacy data warehouse and you probably want to denormalize the",
    "start": "2066710",
    "end": "2072480"
  },
  {
    "text": "data as you're moving it into Amazon s3 and we have very very specialized",
    "start": "2072480",
    "end": "2078270"
  },
  {
    "text": "components as part of our framework to help you do that ok another question by",
    "start": "2078270",
    "end": "2085980"
  },
  {
    "text": "Nora this is also for ashwin can you give some details on moving from talent to",
    "start": "2085980",
    "end": "2093360"
  },
  {
    "text": "spark why and how did you make those decisions sure I guess I probably frame",
    "start": "2093360",
    "end": "2099450"
  },
  {
    "text": "the question so it's not an either-or argument so we're basically a cloud and",
    "start": "2099450",
    "end": "2105420"
  },
  {
    "text": "big data integration provider so we actually support box there's not really a question of using talent or spark it's",
    "start": "2105420",
    "end": "2113130"
  },
  {
    "text": "the fact that talent actually supports spark natively an important part about",
    "start": "2113130",
    "end": "2118610"
  },
  {
    "text": "native support for spark is when it comes to performance so for instance",
    "start": "2118610",
    "end": "2125310"
  },
  {
    "text": "let's say you're trying to run certain jobs on top of EMR using spark",
    "start": "2125310",
    "end": "2132150"
  },
  {
    "text": "but you also want some value-added transformation using accountants we can help you do that we can make sure that",
    "start": "2132150",
    "end": "2138090"
  },
  {
    "text": "the native power of spark is used in the processing of those jobs so what that means eventually your time to process",
    "start": "2138090",
    "end": "2147060"
  },
  {
    "text": "those jobs then move those jobs downstream to other potential Amazon",
    "start": "2147060",
    "end": "2152130"
  },
  {
    "text": "services such as redshift for instance that time is greatly reduced okay this",
    "start": "2152130",
    "end": "2159690"
  },
  {
    "text": "is question four for Eric can you discuss your stratification of data to",
    "start": "2159690",
    "end": "2167040"
  },
  {
    "text": "allow parallelism did you set up descriptive analytics so sounds like a",
    "start": "2167040",
    "end": "2173400"
  },
  {
    "text": "couple of questions there from from from a parallel processing perspective you",
    "start": "2173400",
    "end": "2180600"
  },
  {
    "text": "know again we're leveraging relational Hortonworks cluster in the cloud currently running on ec2 instances and",
    "start": "2180600",
    "end": "2185670"
  },
  {
    "text": "for analytics we're leveraging SPARC primarily using PI spark for some of our",
    "start": "2185670",
    "end": "2191520"
  },
  {
    "text": "analytics capabilities although we we recently took a look at data robot and it started to use that platform as well",
    "start": "2191520",
    "end": "2197340"
  },
  {
    "text": "which also runs in AWS okay there's one more question for Eric did you consider",
    "start": "2197340",
    "end": "2203790"
  },
  {
    "text": "Amazon Athena and redshift spectrum did the fact that these required data to go",
    "start": "2203790",
    "end": "2209700"
  },
  {
    "text": "from the public network affect your decision I'm not sure if hive another system used have the same restrictions",
    "start": "2209700",
    "end": "2216420"
  },
  {
    "text": "so when we perform the evaluation spectrum and Athena weren't options",
    "start": "2216420",
    "end": "2221490"
  },
  {
    "text": "those were released after we did look at Athena although at the time we looked at",
    "start": "2221490",
    "end": "2226980"
  },
  {
    "text": "it and this was about six months ago the the inability to create structures right order to persist data was an issue for",
    "start": "2226980",
    "end": "2233310"
  },
  {
    "text": "us so as that changes over time or it may have perhaps already with AWS as roadmap eager to learn more during",
    "start": "2233310",
    "end": "2240240"
  },
  {
    "text": "rement this year it's definitely something we'll take another look at and likewise with spectrum so if either of",
    "start": "2240240",
    "end": "2247140"
  },
  {
    "text": "those tools and a work as a managed service and we can eliminate our administration you know I'd be more than",
    "start": "2247140",
    "end": "2253800"
  },
  {
    "text": "happy to do that okay there's one question for me with s3 is used as a",
    "start": "2253800",
    "end": "2259830"
  },
  {
    "text": "data storage layer or data Lake does that mean that for CSV files RB export from various operational",
    "start": "2259830",
    "end": "2267090"
  },
  {
    "text": "SQL systems and and store an s3 and then downstream data processing and analytics",
    "start": "2267090",
    "end": "2272490"
  },
  {
    "text": "pull all the data from s3 the data Lake idea is to have to dump all the data",
    "start": "2272490",
    "end": "2280200"
  },
  {
    "text": "that you receive and store it for future to processing right that's the primary",
    "start": "2280200",
    "end": "2286650"
  },
  {
    "text": "use for a storage this could be used for the CSV files that are generated that is",
    "start": "2286650",
    "end": "2292350"
  },
  {
    "text": "that could be one use case it could also be that you could be strong unstructured data like log files or other files I",
    "start": "2292350",
    "end": "2299100"
  },
  {
    "text": "know of a specific customer news case when they had stored a CSV files",
    "start": "2299100",
    "end": "2305130"
  },
  {
    "text": "exported from certain sequel or DBMS database and what happened was they were",
    "start": "2305130",
    "end": "2311730"
  },
  {
    "text": "just touring it there did not know of any use but there was a project called conflation where they had to do",
    "start": "2311730",
    "end": "2318720"
  },
  {
    "text": "approximate record match this is a machine learning problem and they did not have enough data to be able to run",
    "start": "2318720",
    "end": "2325350"
  },
  {
    "text": "this run did not have enough training data to train the models to make these",
    "start": "2325350",
    "end": "2333590"
  },
  {
    "text": "approximate record matches and somebody was looking through these these these",
    "start": "2333590",
    "end": "2339960"
  },
  {
    "text": "backups and they some guy just found that hey we have this data let's back up",
    "start": "2339960",
    "end": "2346080"
  },
  {
    "text": "from from some time ago and there had been cases where they had make changes",
    "start": "2346080",
    "end": "2351810"
  },
  {
    "text": "to individual rows in the in the tables and they were looking for those changes",
    "start": "2351810",
    "end": "2358680"
  },
  {
    "text": "right from before to after and those were the training data so the idea is that to not use not be restricted to CSV",
    "start": "2358680",
    "end": "2368130"
  },
  {
    "text": "files or in other ways but just think of s3 as your large repository to dump",
    "start": "2368130",
    "end": "2373890"
  },
  {
    "text": "whatever data that you have in case you need to use their warehouse or DBMS you",
    "start": "2373890",
    "end": "2380310"
  },
  {
    "text": "can are you going to graph DB or other forms of databases feel free to load the data from s3 and",
    "start": "2380310",
    "end": "2388380"
  },
  {
    "text": "once you're done maybe dump it back to history this is a question for",
    "start": "2388380",
    "end": "2395640"
  },
  {
    "text": "Eric what are the tools you used for data virtualization so we currently",
    "start": "2395640",
    "end": "2402930"
  },
  {
    "text": "don't leverage data virtualization technology today okay all right this is",
    "start": "2402930",
    "end": "2410160"
  },
  {
    "text": "a question for xmin what do you mean by self-service data service how its",
    "start": "2410160",
    "end": "2416250"
  },
  {
    "text": "implemented Callen ESB or something like that so actually a self-service self-service is",
    "start": "2416250",
    "end": "2424880"
  },
  {
    "text": "really a concept and it's I wouldn't say it's implemented as a data service per",
    "start": "2424880",
    "end": "2430500"
  },
  {
    "text": "se what the don't concept of self-service means you get all relevant",
    "start": "2430500",
    "end": "2436710"
  },
  {
    "text": "data into a data leak such as Amazon s3 and how quickly can you give access to",
    "start": "2436710",
    "end": "2446220"
  },
  {
    "text": "the various different data personas to that data so for instance let's assume",
    "start": "2446220",
    "end": "2451890"
  },
  {
    "text": "you have a business analyst that needs to immediately access data in s3 how the",
    "start": "2451890",
    "end": "2457440"
  },
  {
    "text": "business analyst would act as a data and still make intelligence out of it if they would use talent data preparation",
    "start": "2457440",
    "end": "2464690"
  },
  {
    "text": "product and using that they would cleanse and apply some data quality",
    "start": "2464690",
    "end": "2471180"
  },
  {
    "text": "rules to that data prior to analyzing it in something like quick site so the",
    "start": "2471180",
    "end": "2476250"
  },
  {
    "text": "concept of self-service really pertains to allowing access to whatever relevant",
    "start": "2476250",
    "end": "2483210"
  },
  {
    "text": "persona in your organization that needs access to that data it could be data engineer it could be a business analyst",
    "start": "2483210",
    "end": "2490170"
  },
  {
    "text": "designs it so it's not specifically implementing it as a service through",
    "start": "2490170",
    "end": "2496950"
  },
  {
    "text": "town ESB or anything like that okay another question for estrine while you're there for third-party data",
    "start": "2496950",
    "end": "2503730"
  },
  {
    "text": "integrations would you recommend ingesting data through the daedalic prior to moving it to an operational",
    "start": "2503730",
    "end": "2511110"
  },
  {
    "text": "data store or would you go to the operational data store first then feed",
    "start": "2511110",
    "end": "2516840"
  },
  {
    "text": "the dynamic are in parallel so this really depends I would say upon that the",
    "start": "2516840",
    "end": "2523470"
  },
  {
    "text": "use case for the data one thing that is definitely advisable is",
    "start": "2523470",
    "end": "2529080"
  },
  {
    "text": "to analytics and operational types of data within the same you know within the",
    "start": "2529080",
    "end": "2535710"
  },
  {
    "text": "same cluster if you will because no those are really different use cases so if you're using something for analytics",
    "start": "2535710",
    "end": "2542250"
  },
  {
    "text": "you really want to ingest that data into specifically an Amazon s3 data leak and",
    "start": "2542250",
    "end": "2548790"
  },
  {
    "text": "then as you move it downstream into Amazon EMR and eventually into Amazon",
    "start": "2548790",
    "end": "2554340"
  },
  {
    "text": "redshift and a quick side and there's that part of the pipeline and then the operational data store part of it that",
    "start": "2554340",
    "end": "2560490"
  },
  {
    "text": "really should be bifurcated into a separate process because the other day what you're trying to do is you're kind",
    "start": "2560490",
    "end": "2566100"
  },
  {
    "text": "of optimized for performance in getting the data from the very source of the",
    "start": "2566100",
    "end": "2572280"
  },
  {
    "text": "data leak to the various constituents downstream so the way to do that nearly is to focus on the analytical use cases",
    "start": "2572280",
    "end": "2578850"
  },
  {
    "text": "versus the operational data store uses all right I have two questions for Eric",
    "start": "2578850",
    "end": "2585050"
  },
  {
    "text": "first question did you conform all your s3 data to a standard format like raw or",
    "start": "2585050",
    "end": "2592020"
  },
  {
    "text": "JSON or did you keep them in different formats so raw data we ingest as it",
    "start": "2592020",
    "end": "2598230"
  },
  {
    "text": "exists from the source so if that's a zip file if the zip file CSV tab Avro",
    "start": "2598230",
    "end": "2604560"
  },
  {
    "text": "park' whatever it is for a process where we we have standardized on park' however",
    "start": "2604560",
    "end": "2610100"
  },
  {
    "text": "okay Parkay days all right second question once you build",
    "start": "2610100",
    "end": "2616050"
  },
  {
    "text": "the daedalic did you have to build different views which extracted data",
    "start": "2616050",
    "end": "2622440"
  },
  {
    "text": "from Delic or for different business dreams so we do the data Lake is",
    "start": "2622440",
    "end": "2628800"
  },
  {
    "text": "effectively a feeder for everything else so we land data into s3 first and from",
    "start": "2628800",
    "end": "2634740"
  },
  {
    "text": "their constituents can access that whether it the you know departments applications etc so in some cases you",
    "start": "2634740",
    "end": "2642660"
  },
  {
    "text": "know certain departments have the have the skill and the capability of extracting the data however they need to",
    "start": "2642660",
    "end": "2650070"
  },
  {
    "text": "whether that's in a raw complicated format with you know multiple joints other groups may may need a little bit",
    "start": "2650070",
    "end": "2655800"
  },
  {
    "text": "of help and in those cases we may provide or we do provide more processed layers of data for them",
    "start": "2655800",
    "end": "2662310"
  },
  {
    "text": "and additionally views of that data theory all right cool um that's all I have thank you very much today for",
    "start": "2662310",
    "end": "2669570"
  },
  {
    "text": "joining us in this webinar and thank you for attending thank you I should generate you",
    "start": "2669570",
    "end": "2678050"
  }
]