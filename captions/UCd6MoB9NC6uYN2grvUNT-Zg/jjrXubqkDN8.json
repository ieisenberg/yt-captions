[
  {
    "text": "all right welcome to the talk and it's gonna be the last one day for you so if we you can enjoy the closing reception",
    "start": "0",
    "end": "6000"
  },
  {
    "text": "afterwards so I'm Leo I'm a Solutions Architect here at Amazon Web Services",
    "start": "6000",
    "end": "11969"
  },
  {
    "text": "and before we get started just if there's anyone on the back at once a seat I see there's a bunch of empty",
    "start": "11969",
    "end": "17640"
  },
  {
    "text": "seats kind of in the middle here so feel free to go ahead and grab one so we're",
    "start": "17640",
    "end": "22859"
  },
  {
    "text": "gonna be talking about cloud watch logs so what do you do with these logs how do you get the logs and we're going to be",
    "start": "22859",
    "end": "30029"
  },
  {
    "text": "talking about it amidst lambda so how do you actually take action on what what it",
    "start": "30029",
    "end": "36329"
  },
  {
    "text": "is you that you're being alerted on by your logs so we're gonna also take a",
    "start": "36329",
    "end": "42210"
  },
  {
    "text": "look at some industry trends that we see that impact monitoring we're gonna learn",
    "start": "42210",
    "end": "48059"
  },
  {
    "text": "about cloud watching Club which logs and its features what you can do with it we're gonna talk about three different use cases and see how cloud watch and",
    "start": "48059",
    "end": "57750"
  },
  {
    "text": "nativist lambda can help you in those use cases so these are the scenarios",
    "start": "57750",
    "end": "64080"
  },
  {
    "text": "that we're going to be looking at the first one is you've got logs from your",
    "start": "64080",
    "end": "69330"
  },
  {
    "text": "load balancer so you've got a web application you've got a load balancer in front of your web servers or app",
    "start": "69330",
    "end": "76110"
  },
  {
    "text": "servers and instead of doing the logging on the web server level you're doing it",
    "start": "76110",
    "end": "81930"
  },
  {
    "text": "at the load balancer so no matter what server it hits you've got the logs in one central location so when you set that up on your load balancers that goes",
    "start": "81930",
    "end": "88799"
  },
  {
    "text": "ten s3 bucket which is great so now you have all these files in s3 but how do you actually you know search those logs",
    "start": "88799",
    "end": "94229"
  },
  {
    "text": "trigger on certain events in those logs so we're gonna talk about how to do that with lambda and cloud watch logs",
    "start": "94229",
    "end": "100680"
  },
  {
    "text": "we're gonna talk about how to customize alarms so cloud watch is a built-in alarming",
    "start": "100680",
    "end": "107579"
  },
  {
    "text": "feature that allows you to get an email or other tips of notifications when",
    "start": "107579",
    "end": "113250"
  },
  {
    "text": "certain thresholds are triggered in your metrics but how do you customize those",
    "start": "113250",
    "end": "119420"
  },
  {
    "text": "alarms to actually give you the information that you need to act on the alarms so we're going to talk about that",
    "start": "119420",
    "end": "125939"
  },
  {
    "text": "and that involves a cloud watch logs cloud watch the alarms feature lambda",
    "start": "125939",
    "end": "131879"
  },
  {
    "text": "and our simple email service and then we're going to take a look at how do you quickly spin up an elastic",
    "start": "131879",
    "end": "139020"
  },
  {
    "text": "search cluster to do some deep diving in our analysis on your logs so we have a",
    "start": "139020",
    "end": "147060"
  },
  {
    "text": "elastic search service right and we have built in functionality from CloudWatch logs to stream to service but what if",
    "start": "147060",
    "end": "153180"
  },
  {
    "text": "you don't want to you know pay for a long-running elastic search cluster what if you just want to spin one up and dump",
    "start": "153180",
    "end": "158340"
  },
  {
    "text": "some time period of logs into it analyze it and then shut it down and save a lot of money we're gonna talk about that so",
    "start": "158340",
    "end": "164690"
  },
  {
    "text": "before we do that let's talk about a scenario that I've definitely experienced back when I was a systems",
    "start": "164690",
    "end": "171870"
  },
  {
    "text": "administrator I'm sure some of you have so yeah I'm I'm sure everyone's had days",
    "start": "171870",
    "end": "177540"
  },
  {
    "text": "like this so in this scenario this is actually based on you know a",
    "start": "177540",
    "end": "184190"
  },
  {
    "text": "conglomeration of true stories that I've seen in my career I'm sure many of you have seen so you've got your you're",
    "start": "184190",
    "end": "190760"
  },
  {
    "text": "working on on a team for an important application and you get a ticket that",
    "start": "190760",
    "end": "197310"
  },
  {
    "text": "the application is down you didn't get any like alarms that were fired you",
    "start": "197310",
    "end": "202920"
  },
  {
    "text": "didn't see anything on your monitoring systems your first sense that something was wrong was when a ticket was filed by",
    "start": "202920",
    "end": "209640"
  },
  {
    "text": "a customer so John the uncle developer is paged through that ticketing system",
    "start": "209640",
    "end": "215360"
  },
  {
    "text": "he starts looking at their alarms none of them fires he just knows something is potentially wrong so this is the",
    "start": "215360",
    "end": "221880"
  },
  {
    "text": "blissful ignorance stage of this incident so then John starts looking at his",
    "start": "221880",
    "end": "228330"
  },
  {
    "text": "service dashboards and monitoring systems and he sees that there's an availability impact so he sees that in fact the app is down but he doesn't",
    "start": "228330",
    "end": "237000"
  },
  {
    "text": "really know anything else he doesn't know what's causing the outage and he doesn't know how to like start to figure",
    "start": "237000",
    "end": "243600"
  },
  {
    "text": "out what's wrong he just knows that it's down so he starts to escalate this to his manager so this is the confusion",
    "start": "243600",
    "end": "251700"
  },
  {
    "text": "stage of the incident so then you start getting more tickets pouring in a lot of",
    "start": "251700",
    "end": "258630"
  },
  {
    "text": "your customers are upset that your application is down and so the escalation manager gene joins the event",
    "start": "258630",
    "end": "264030"
  },
  {
    "text": "and starts to also try to figure out what's wrong top of that now things start getting stressful because the CTO happens to",
    "start": "264030",
    "end": "271590"
  },
  {
    "text": "notice that there's now an issue the CTO starts to message both John and Jane",
    "start": "271590",
    "end": "279030"
  },
  {
    "text": "that hey what's going on why are we down so now this is the stress stage of the incident so given that they don't have",
    "start": "279030",
    "end": "288240"
  },
  {
    "text": "any real data they start going back to previous incidents just there you know tribal knowledge and so they",
    "start": "288240",
    "end": "295740"
  },
  {
    "text": "start looking and they remember that oh there was a recent incident where",
    "start": "295740",
    "end": "301340"
  },
  {
    "text": "expensive operations caused a similar issue to this so they start diving into",
    "start": "301340",
    "end": "308009"
  },
  {
    "text": "the logs and their production hosts so again another red flag here there's no centralized logging system they have to",
    "start": "308009",
    "end": "313830"
  },
  {
    "text": "manually assess agents all the production hosts because there's no centralized logging system they fly grep",
    "start": "313830",
    "end": "320099"
  },
  {
    "text": "through the host through the log files manually and so that's pretty painful so they identify a suspect customer they",
    "start": "320099",
    "end": "326699"
  },
  {
    "text": "think some customers operations are causing this issue and they prepare a",
    "start": "326699",
    "end": "331710"
  },
  {
    "text": "change in the configuration just block that customer from the app hopefully thus restoring the application so this",
    "start": "331710",
    "end": "338340"
  },
  {
    "text": "is the false hope part of the incident so the other team that has to execute",
    "start": "338340",
    "end": "346259"
  },
  {
    "text": "that change engages and they indicate that the this didn't actually the",
    "start": "346259",
    "end": "355229"
  },
  {
    "text": "configuration changed and actually fix anything so the issue still exists the",
    "start": "355229",
    "end": "360270"
  },
  {
    "text": "custom now you have a customer blocked and your app is down so out of ideas they go back to the old well have you",
    "start": "360270",
    "end": "366330"
  },
  {
    "text": "tried rebooting it idea so they they suggest to fail over the database to the",
    "start": "366330",
    "end": "372389"
  },
  {
    "text": "standby and so that's the desperation stage of instant and so the rebooting",
    "start": "372389",
    "end": "379710"
  },
  {
    "text": "though works so the failover works the app recovers they then later on do a",
    "start": "379710",
    "end": "385229"
  },
  {
    "text": "root cause analysis and they find that there was a GD BC version introduced a while back that caused the memory leak",
    "start": "385229",
    "end": "390870"
  },
  {
    "text": "that eventually caused this issue so they fix the leak they add some new",
    "start": "390870",
    "end": "396300"
  },
  {
    "text": "alarms that in the future they know when you know they're running out of RAM and they can",
    "start": "396300",
    "end": "401879"
  },
  {
    "text": "deal with this ahead of time and they tune their service alarms so that that's their post outage actions so this is the",
    "start": "401879",
    "end": "409860"
  },
  {
    "text": "enlightened stage of incident however there's a lot more things wrong than just you know those steps that they did",
    "start": "409860",
    "end": "416249"
  },
  {
    "text": "to correct this specific issue so think we can do better than that right so just to reflect on this",
    "start": "416249",
    "end": "421860"
  },
  {
    "text": "incident other red flags some things they didn't have alarms on other things",
    "start": "421860",
    "end": "430889"
  },
  {
    "text": "the alarms weren't actionable we don't always have access to the right logs so",
    "start": "430889",
    "end": "436889"
  },
  {
    "text": "as soon as there's an incident ideally you have access to us and from those logging system you know what queries to",
    "start": "436889",
    "end": "441930"
  },
  {
    "text": "run you know where to look you know what hosts against you don't have to SSH into any production of hosts",
    "start": "441930",
    "end": "447419"
  },
  {
    "text": "so making sure you have the right logs is important and have access to those logs and you have to make sure that your",
    "start": "447419",
    "end": "454529"
  },
  {
    "text": "dashboards and your monitoring systems they actually impact that they actually",
    "start": "454529",
    "end": "461729"
  },
  {
    "text": "show customer behavior and and how your app is performing with real customers so showing like the CPU usage of a host",
    "start": "461729",
    "end": "469490"
  },
  {
    "text": "probably not that important a lot showing how many orders you're getting per second if it's you know a retail",
    "start": "469490",
    "end": "475529"
  },
  {
    "text": "website or showing you know how fast your web pages are actually loading or",
    "start": "475529",
    "end": "480750"
  },
  {
    "text": "if there's orders going on that those kind of metrics are important so figuring out jar dashboards actually",
    "start": "480750",
    "end": "487020"
  },
  {
    "text": "tell them what we want them to show us what's important so monitoring turns out",
    "start": "487020",
    "end": "492569"
  },
  {
    "text": "is really hard so let's let's take a look at some trends we see in monitoring so the first thing is the complexity is",
    "start": "492569",
    "end": "499460"
  },
  {
    "text": "increasing we see a lot of customers moving away from monolithic applications",
    "start": "499460",
    "end": "505129"
  },
  {
    "text": "to micro services so the great thing about micro services is they're small they can all be developed in parallel",
    "start": "505129",
    "end": "511199"
  },
  {
    "text": "they're loosely coupled which usually means hi there they're more highly",
    "start": "511199",
    "end": "516240"
  },
  {
    "text": "available in terms of their impact on the whole system but as a downside because there's more services that means",
    "start": "516240",
    "end": "522930"
  },
  {
    "text": "there's more logs there's more things that you need to log right so it's more complicated when you have micro services",
    "start": "522930",
    "end": "529199"
  },
  {
    "text": "each of those micro services could be written in a different language or framework because they're communicating with each",
    "start": "529199",
    "end": "534520"
  },
  {
    "text": "other through api's you can't assume that you know your whole app is written",
    "start": "534520",
    "end": "539800"
  },
  {
    "text": "in the same language or the same framework and we also see people moving",
    "start": "539800",
    "end": "546070"
  },
  {
    "text": "into containers people moving to servers compute such as lambda and so yeah and",
    "start": "546070",
    "end": "551260"
  },
  {
    "text": "those are other things where you need to think about how do I log what's going on and we see specialization the",
    "start": "551260",
    "end": "556810"
  },
  {
    "text": "persistence tier so we see customers moving to for example dynamodb or Redis",
    "start": "556810",
    "end": "563890"
  },
  {
    "text": "first for storing session State and stuff like that again just another area where you're thinking about logging and",
    "start": "563890",
    "end": "569590"
  },
  {
    "text": "monitoring applications are also becoming more dynamic so see ICD",
    "start": "569590",
    "end": "579010"
  },
  {
    "text": "processes are evolving instead of doing you know deploys like once a quarter or once a year once a month we see that",
    "start": "579010",
    "end": "586960"
  },
  {
    "text": "small changes are being continuously built tested and deployed so for example",
    "start": "586960",
    "end": "594820"
  },
  {
    "text": "inside of Amazon we we do I think we do a deployment around once every 1.5",
    "start": "594820",
    "end": "602170"
  },
  {
    "text": "seconds across the company might be it's less time than that now I know our customers doing a lot of deployments all",
    "start": "602170",
    "end": "608140"
  },
  {
    "text": "the time and so as the scale of applications is changing and oftentimes",
    "start": "608140",
    "end": "613780"
  },
  {
    "text": "increasing to me demand that infrastructure is growing again creating more things that you need to monitor we",
    "start": "613780",
    "end": "620800"
  },
  {
    "text": "see that applications are becoming more and more global and the customer behavior is unpredictable so you know if",
    "start": "620800",
    "end": "627010"
  },
  {
    "text": "you're a us-based company for example or you're just assuming you have customers of your applications in the US you know",
    "start": "627010",
    "end": "633430"
  },
  {
    "text": "let's say you're an education that means you used to be able to assume well the peak hours for my application are from 9:00 a.m. to 3:00 p.m. but as you get to",
    "start": "633430",
    "end": "641080"
  },
  {
    "text": "become global then you've got people using your application all the time there might be spikes in demand at any",
    "start": "641080",
    "end": "648160"
  },
  {
    "text": "given time and your applications have to be able to scale to that demand and so as all this is going on it means that",
    "start": "648160",
    "end": "655360"
  },
  {
    "text": "that's a forcing function for increased automation so in order to be able to scale in order to be able to increase",
    "start": "655360",
    "end": "661060"
  },
  {
    "text": "the size of your infrastructure you have to stop treating your infrastructure like",
    "start": "661060",
    "end": "666519"
  },
  {
    "text": "unique snowflakes and treat everything like cattle that's everything's automated everything to be easily replaced so and automation means again",
    "start": "666519",
    "end": "675759"
  },
  {
    "text": "you need monitoring so another trend that we see is more business impact so",
    "start": "675759",
    "end": "683009"
  },
  {
    "text": "there's an increasing role of applications and business outcomes for",
    "start": "683009",
    "end": "689170"
  },
  {
    "text": "our customers so obviously if you're a technology company your applications are what you produce that it's important but",
    "start": "689170",
    "end": "694660"
  },
  {
    "text": "now we see a customers who traditionally their technology has them in their primary business now it's becoming more",
    "start": "694660",
    "end": "701379"
  },
  {
    "text": "and more rural so your applications have to stay up or might have a negative outcome on your business we see that the",
    "start": "701379",
    "end": "707319"
  },
  {
    "text": "speed at which you innovate is directly tied to your competitive advantage and",
    "start": "707319",
    "end": "714569"
  },
  {
    "text": "we see increased expectations from customers so customers expect up time they expect your apps to be fast I",
    "start": "714569",
    "end": "721209"
  },
  {
    "text": "remember you know surely recently I've used applications where they had like you know if it was a lot of business",
    "start": "721209",
    "end": "726670"
  },
  {
    "text": "hours you couldn't use this web application so not kind of things becoming less almost common we also see",
    "start": "726670",
    "end": "732329"
  },
  {
    "text": "that monitoring isn't a standalone thing anymore it's one aspect of the lifecycle",
    "start": "732329",
    "end": "740949"
  },
  {
    "text": "of an application so you've got infrastructure provisioning",
    "start": "740949",
    "end": "747040"
  },
  {
    "text": "configuration management compliance governance resource optimization monitoring is a tool to do all of those",
    "start": "747040",
    "end": "753550"
  },
  {
    "text": "things right so we have to inside the company talk about good intentions",
    "start": "753550",
    "end": "760199"
  },
  {
    "text": "versus actual metrics right and so or",
    "start": "760199",
    "end": "765790"
  },
  {
    "text": "and mechanisms so everybody has good intentions ever and wants their application to work really well but you",
    "start": "765790",
    "end": "772300"
  },
  {
    "text": "need mechanisms to make sure that's happening so whether you're you know configuring your OS or provisioning your",
    "start": "772300",
    "end": "779139"
  },
  {
    "text": "infrastructure or optimizing how your applications work or making sure that they fit in with your compliance and",
    "start": "779139",
    "end": "785259"
  },
  {
    "text": "governance scheme did they're secure logging and monitoring is is a tool that",
    "start": "785259",
    "end": "791470"
  },
  {
    "text": "you can use to make sure all that stuff is happening so it's a mechanism to succeed so with that in mind let's talk",
    "start": "791470",
    "end": "799089"
  },
  {
    "text": "about the X tools that we're gonna we're gonna talk about today so cloud watch so cloud",
    "start": "799089",
    "end": "804520"
  },
  {
    "text": "watch has two major parts of it there's the metrics part and then there's the logging part so cloud watch gives you a",
    "start": "804520",
    "end": "811330"
  },
  {
    "text": "bunch of metrics logs and events over time to help you understand how your system behaves in terms of metrics it",
    "start": "811330",
    "end": "819010"
  },
  {
    "text": "gives you both built in metrics so for example when you spin up an ec2 instance you get metrics on average CPU usage and",
    "start": "819010",
    "end": "825570"
  },
  {
    "text": "network out stuff like that you can also do custom metrics so you can install an agent on your ec2 instance and measure",
    "start": "825570",
    "end": "833110"
  },
  {
    "text": "whatever you want like you know how many threads of a certain process you have for example so you can do custom metrics",
    "start": "833110",
    "end": "838510"
  },
  {
    "text": "as well and you can also publish logs so for example what lambda it has built-in",
    "start": "838510",
    "end": "845980"
  },
  {
    "text": "Cloud watch logs integrations so you can publish the logs of your lambda functions as they get executed to cloud",
    "start": "845980",
    "end": "852160"
  },
  {
    "text": "watch logs in terms of ec2 for example you could run an agent on your instance",
    "start": "852160",
    "end": "857620"
  },
  {
    "text": "and that agent publishes any kind of OS or application logs that you tell it to out to cloud watch logs so that's the",
    "start": "857620",
    "end": "865360"
  },
  {
    "text": "logging part of it and you can also do automatic notifications so you set",
    "start": "865360",
    "end": "871210"
  },
  {
    "text": "metrics so for example I could be which we're gonna be doing an example here for",
    "start": "871210",
    "end": "876610"
  },
  {
    "text": "my load balancer logs I can make a metric out of how many four or fours they have and then if that metric hits a",
    "start": "876610",
    "end": "882880"
  },
  {
    "text": "certain threshold over a defined time period then I can get an automatic notification that hey my threshold has",
    "start": "882880",
    "end": "889900"
  },
  {
    "text": "been violated so I'm getting a lot of four fours so not only can you get these",
    "start": "889900",
    "end": "894910"
  },
  {
    "text": "notifications you can also act on them so once you once you get this",
    "start": "894910",
    "end": "902620"
  },
  {
    "text": "notification you can actually trigger a lambda function to fix whatever the issues right so you can also inspect",
    "start": "902620",
    "end": "910720"
  },
  {
    "text": "navigate zoom and correlate across time to investigate what is going on for",
    "start": "910720",
    "end": "918340"
  },
  {
    "text": "metrics that you create from cloud watch logs you can jump directly to those logs",
    "start": "918340",
    "end": "923770"
  },
  {
    "text": "from the metrics so for example if you have a metric of the number of 404s when",
    "start": "923770",
    "end": "928930"
  },
  {
    "text": "you look at that chart in the cloud watch web console you can just click on it and",
    "start": "928930",
    "end": "934750"
  },
  {
    "text": "it'll take you to actual logs in question that's super useful I think and you can also generate additional metrics",
    "start": "934750",
    "end": "940060"
  },
  {
    "text": "from your log data so you can automatically correct issues so for",
    "start": "940060",
    "end": "946240"
  },
  {
    "text": "example if you don't have enough if your CPU such is really high across you know",
    "start": "946240",
    "end": "954040"
  },
  {
    "text": "an auto scaling group then you trigger a scale up condition so you can actually increase the size of your auto scaling",
    "start": "954040",
    "end": "960220"
  },
  {
    "text": "group or if you set up you know some sort of McLeod watch alarm off of your",
    "start": "960220",
    "end": "968740"
  },
  {
    "text": "logs where something sketchy is going on in your instance you can have a lambda function that takes that instance and",
    "start": "968740",
    "end": "974800"
  },
  {
    "text": "closes a report on it and you know puts it in a in a state where you can only inspect it for forensics it's really",
    "start": "974800",
    "end": "980800"
  },
  {
    "text": "open to anything you want to do like that and so lambda allows you to customize your remediation techniques so",
    "start": "980800",
    "end": "988390"
  },
  {
    "text": "let's talk about some recent improvements over the last year so in cloud watch so the price of custom",
    "start": "988390",
    "end": "994120"
  },
  {
    "text": "metrics has dropped we've keep adding metrics logs and events for more a degree of services",
    "start": "994120",
    "end": "999310"
  },
  {
    "text": "such as cloud trail last a beanstalk SES again you can navigate from those metrics back back to the logs that",
    "start": "999310",
    "end": "1005490"
  },
  {
    "text": "generated those metrics it used to be the metric retention there's only two",
    "start": "1005490",
    "end": "1010589"
  },
  {
    "text": "weeks it's now you can store it for up to 15 months we've really stay a collect",
    "start": "1010589",
    "end": "1018120"
  },
  {
    "text": "d plugin so it makes it easier to generate custom metrics off of your instances there's a better web console",
    "start": "1018120",
    "end": "1025558"
  },
  {
    "text": "experience you're gonna have metrics down to one second resolution time another thing that we added kind of",
    "start": "1025559",
    "end": "1031798"
  },
  {
    "text": "recently that's not on the slides is metric math so you can actually derive a new metric off of other metrics so for",
    "start": "1031799",
    "end": "1040350"
  },
  {
    "text": "example if you're doing trying to calculate like the AI ops of your AFS",
    "start": "1040350",
    "end": "1045808"
  },
  {
    "text": "file system you can take to other metrics and and do a you know a multiplication on them and get their",
    "start": "1045809",
    "end": "1051059"
  },
  {
    "text": "shop so you can you can derive metrics from other metrics so cloud watch though",
    "start": "1051059",
    "end": "1057210"
  },
  {
    "text": "isn't the whole story again monitoring is hard every customer that I work with",
    "start": "1057210",
    "end": "1062669"
  },
  {
    "text": "has different needs in terms of monitoring you different tools aside for me the BS tools and we have a really part a rich",
    "start": "1062669",
    "end": "1069480"
  },
  {
    "text": "partner ecosystem so we have partners such as spunk sumo logic many others",
    "start": "1069480",
    "end": "1075240"
  },
  {
    "text": "that can help you with logging that can help you with application performance monitoring and so we give you the tools",
    "start": "1075240",
    "end": "1082140"
  },
  {
    "text": "and flexibility to integrate with other UW services and to use whatever combo of",
    "start": "1082140",
    "end": "1087510"
  },
  {
    "text": "services you need to right so all of our services are API driven and sealion abled so you can take all the data from",
    "start": "1087510",
    "end": "1094350"
  },
  {
    "text": "cloud watch logs and cloud watch and for example put it into whatever monitoring",
    "start": "1094350",
    "end": "1099630"
  },
  {
    "text": "or logging system use so let's talk about kelabra logs so cloud watch logs is the part of cloud watch that enables",
    "start": "1099630",
    "end": "1106710"
  },
  {
    "text": "you to centralize your logs to search them to stream them to other services to",
    "start": "1106710",
    "end": "1113550"
  },
  {
    "text": "retain them and to secure them so the logs they're integrated the servicing rate with our kms or key management",
    "start": "1113550",
    "end": "1119370"
  },
  {
    "text": "system so for ingress you can take stuff from ec2 instances from cloud trail from",
    "start": "1119370",
    "end": "1126480"
  },
  {
    "text": "lambda from ECS and pump it straight into cloud watch logs you can us to do",
    "start": "1126480",
    "end": "1131520"
  },
  {
    "text": "anything custom so again there's an API for cloud watch logs so you can really",
    "start": "1131520",
    "end": "1136830"
  },
  {
    "text": "if you use that API you can pum whatever data you want from any order that you want as long as it can access the cloud watch logs API endpoint once the data is",
    "start": "1136830",
    "end": "1144480"
  },
  {
    "text": "in the cloud watch logs you can export it to s3 buckets to lambda functions to",
    "start": "1144480",
    "end": "1152250"
  },
  {
    "text": "an elastic search cluster either a manage database elastic search cluster or a soft manageable a strip search",
    "start": "1152250",
    "end": "1158130"
  },
  {
    "text": "cluster or to Kinesis so let's just do a",
    "start": "1158130",
    "end": "1166320"
  },
  {
    "text": "very quick summary of the services we're gonna be using for a little demo today so lambda is our server list compute",
    "start": "1166320",
    "end": "1173520"
  },
  {
    "text": "service so allows you to run arbitrary code without having to manage any servers or clusters or anything like",
    "start": "1173520",
    "end": "1179940"
  },
  {
    "text": "that so it supports a bunch of different languages such as go and Python and JavaScript and so you can just write up",
    "start": "1179940",
    "end": "1187200"
  },
  {
    "text": "functions and any of the languages that it supports and execute them either manually on a schedule or driven by",
    "start": "1187200",
    "end": "1193890"
  },
  {
    "text": "other services so for example when you drop a file in tennis 3 bucket you can have that trigger all a",
    "start": "1193890",
    "end": "1199090"
  },
  {
    "text": "the function there's a lot of different other ways to trigger lambda functions and so the code only runs when it needs",
    "start": "1199090",
    "end": "1205000"
  },
  {
    "text": "to and you only pay for when your lambda function is running so for example with",
    "start": "1205000",
    "end": "1211570"
  },
  {
    "text": "ec2 you're paying for when your ec2 instance is up and running with lambda you're paying only for execution time so",
    "start": "1211570",
    "end": "1218800"
  },
  {
    "text": "only when your lambda functions actually being executed and the billing is really granular it's per 100 milliseconds of",
    "start": "1218800",
    "end": "1224440"
  },
  {
    "text": "execution time so let's take a look at our first example again just to reiterate we have logs that are being",
    "start": "1224440",
    "end": "1232090"
  },
  {
    "text": "collected off of a Neil beep they're being putting us three bucket which is great so now we have those logs but we",
    "start": "1232090",
    "end": "1237280"
  },
  {
    "text": "want to be able to search them we want to be able to make metrics out of them and make alarms out of them so how do we",
    "start": "1237280",
    "end": "1243580"
  },
  {
    "text": "do that so again without without putting your",
    "start": "1243580",
    "end": "1250090"
  },
  {
    "text": "logs in the yellow B that means the logs are gonna be scattered on instance those are we don't want that so that's why we've enabled the yellow B logging and",
    "start": "1250090",
    "end": "1257260"
  },
  {
    "text": "so now we've got our logs in this three bucket so again we want to search and filter so we need to import them into",
    "start": "1257260",
    "end": "1263950"
  },
  {
    "text": "CloudWatch logs so today cloud watch provides an agent",
    "start": "1263950",
    "end": "1269560"
  },
  {
    "text": "that you can install for instance logs so you can install that agent on your",
    "start": "1269560",
    "end": "1275020"
  },
  {
    "text": "instance and that can push your logs to cloud my logs but how'd about things that you have an s3 well so what we're",
    "start": "1275020",
    "end": "1281860"
  },
  {
    "text": "gonna do here is the elbe delivers your logs in tennis three bucket we're gonna",
    "start": "1281860",
    "end": "1288190"
  },
  {
    "text": "enable a feature called s3 event notifications so anytime a new object is",
    "start": "1288190",
    "end": "1293770"
  },
  {
    "text": "dropped into this s3 bucket that is going to trigger the event notifications feature which is going to trigger a",
    "start": "1293770",
    "end": "1298960"
  },
  {
    "text": "lambda function that lambda function is going to go into my s3 bucket it is going to extract the logs from the files",
    "start": "1298960",
    "end": "1306670"
  },
  {
    "text": "on the bucket and publish those to cloud watch logs so we're gonna what we would",
    "start": "1306670",
    "end": "1312250"
  },
  {
    "text": "do here is provision three instances those instances are going to be running Apache or it doesn't really matter what",
    "start": "1312250",
    "end": "1319240"
  },
  {
    "text": "they run could be nginx we're going to create a ELB or an alb so we're going to",
    "start": "1319240",
    "end": "1325150"
  },
  {
    "text": "do a classical a bouncer an application level bouncer and so we're gonna create",
    "start": "1325150",
    "end": "1330820"
  },
  {
    "text": "that load balancer we're gonna create a target group so in this epoch in this example it's an application-level bouncer so we're gonna",
    "start": "1330820",
    "end": "1337139"
  },
  {
    "text": "create a target group that contains the three web servers that we just spun up so now we've got a little balance here",
    "start": "1337139",
    "end": "1342480"
  },
  {
    "text": "with three web servers behind it we are going to enable log delivery to s3 so",
    "start": "1342480",
    "end": "1347700"
  },
  {
    "text": "now any traffic that hits the CL be it's gonna be logged in my patchy you'll be logs s3 bucket and so now we've got that",
    "start": "1347700",
    "end": "1356759"
  },
  {
    "text": "s3 bucket we've got the logs going to it so now we're gonna trigger a lambda function on object.create",
    "start": "1356759",
    "end": "1362759"
  },
  {
    "text": "so we're going to pick this three bucket where the logs are being dropped we're gonna trigger lambda only when a new",
    "start": "1362759",
    "end": "1370379"
  },
  {
    "text": "object is created so only when new logs are dropped in there and we can specify a prefix or suffix if we want to we're",
    "start": "1370379",
    "end": "1377580"
  },
  {
    "text": "not going to do that in this case so then the actual lambda function and these are all up on github I'll have the",
    "start": "1377580",
    "end": "1384119"
  },
  {
    "text": "link later on so if you on the code you can easily get it so we're going to use",
    "start": "1384119",
    "end": "1389279"
  },
  {
    "text": "the get object API from s3 so we're going to pull the the elbe logs from the",
    "start": "1389279",
    "end": "1395909"
  },
  {
    "text": "s3 bucket and then we're going to post those logs into cloud wash logs but",
    "start": "1395909",
    "end": "1401610"
  },
  {
    "text": "using cloud blush logs put in log events API so that's pretty simple so now we're",
    "start": "1401610",
    "end": "1408690"
  },
  {
    "text": "just going to make sure this works so we'd go to our lbph it's going to hit the default Apache web page that means",
    "start": "1408690",
    "end": "1415679"
  },
  {
    "text": "that is getting to the web servers we are then going to wait a few minutes",
    "start": "1415679",
    "end": "1420869"
  },
  {
    "text": "we're going to go to my s3 bucket we're going to refresh it or we're going to make sure that the logs are being",
    "start": "1420869",
    "end": "1426869"
  },
  {
    "text": "deposited in the bucket which they are now again they get deposited Bivins in a",
    "start": "1426869",
    "end": "1433019"
  },
  {
    "text": "tarball file or zip file by default so normally searching that would be difficult means if the download them and",
    "start": "1433019",
    "end": "1439259"
  },
  {
    "text": "extract them and combine them so the but the lambda function does all that for us so it's going to post it to cloud wash",
    "start": "1439259",
    "end": "1445919"
  },
  {
    "text": "logs so then we're going to go to cloud wash logs we're going to verify that in fact the EOB logs are now there so",
    "start": "1445919",
    "end": "1454529"
  },
  {
    "text": "that's great so now we have our logs all in one place all in one log group so now",
    "start": "1454529",
    "end": "1461100"
  },
  {
    "text": "let's let's make some metrics out of it so I want to search all the HTTP",
    "start": "1461100",
    "end": "1466290"
  },
  {
    "text": "get calls from the last 24 hours so what I can do is I can search by get HTTP and",
    "start": "1466290",
    "end": "1472380"
  },
  {
    "text": "that's gonna get me all the HTTP GET calls and then you can specify what period of time you want that for I'm",
    "start": "1472380",
    "end": "1479400"
  },
  {
    "text": "then going to create a filter pattern and I want to get all the get calls that",
    "start": "1479400",
    "end": "1484890"
  },
  {
    "text": "have a latency of more than one millisecond so there's a method to doing",
    "start": "1484890",
    "end": "1490230"
  },
  {
    "text": "the filter pattern so basically I'm going to type that into the filter pattern you can do a test pattern so you",
    "start": "1490230",
    "end": "1495270"
  },
  {
    "text": "can make sure that your pattern is working so there's all tar it's actually breaking out my my results into the",
    "start": "1495270",
    "end": "1501600"
  },
  {
    "text": "proper format I can verify that and so now I'm going to define a metric filter",
    "start": "1501600",
    "end": "1506610"
  },
  {
    "text": "on that log group so I've got my filter pattern I'm going to say okay so I want",
    "start": "1506610",
    "end": "1511680"
  },
  {
    "text": "to have a metric called high latency count and every time there's a hit on more than one millisecond that's going",
    "start": "1511680",
    "end": "1518190"
  },
  {
    "text": "to be a value of one so now I can view over the last week or however long I",
    "start": "1518190",
    "end": "1525210"
  },
  {
    "text": "want my timeframe to be how many get requests I had that were over one",
    "start": "1525210",
    "end": "1531030"
  },
  {
    "text": "millisecond so the next step from that",
    "start": "1531030",
    "end": "1536220"
  },
  {
    "text": "would be I can actually you know if this starts spiking I can for example trigger auto scaling to get more webservers I",
    "start": "1536220",
    "end": "1543120"
  },
  {
    "text": "can trigger a lambda function to do something else so it's pretty flexible on what you can do from there so the key",
    "start": "1543120",
    "end": "1549420"
  },
  {
    "text": "takeaways are that you can use s3 log",
    "start": "1549420",
    "end": "1554580"
  },
  {
    "text": "data and you can you can send it to cloud wash logs using lambda and then",
    "start": "1554580",
    "end": "1560670"
  },
  {
    "text": "you can search and extract metrics from those logs in near-real-time so let's take a look at our second use",
    "start": "1560670",
    "end": "1567600"
  },
  {
    "text": "case here so we're gonna customize alarms that we get from cloud watch to",
    "start": "1567600",
    "end": "1573150"
  },
  {
    "text": "fit our specific needs to make them useful for us so we can act on them so when you get an alarm you want",
    "start": "1573150",
    "end": "1578280"
  },
  {
    "text": "information that's actionable you want information that allows you to figure out is this a lot more important and if",
    "start": "1578280",
    "end": "1583890"
  },
  {
    "text": "it is how do I fix whatever the arms about you also might want to add custom text to the alarm so for example you",
    "start": "1583890",
    "end": "1591090"
  },
  {
    "text": "want might want to add a link to your wiki or to your giro so somebody can file a ticket or to your run book for",
    "start": "1591090",
    "end": "1597870"
  },
  {
    "text": "how you handle this Pacific issue so you want to customize what the alarm text is so what we're",
    "start": "1597870",
    "end": "1603730"
  },
  {
    "text": "going to hear is Claudio watch logs generates an alarm based off of metric",
    "start": "1603730",
    "end": "1609520"
  },
  {
    "text": "that alarm what it normally does is the triggers an SMS topic so sentences are",
    "start": "1609520",
    "end": "1614740"
  },
  {
    "text": "simple notification service a topic can have different subscribers so for example my email can be a subscriber of",
    "start": "1614740",
    "end": "1621520"
  },
  {
    "text": "an SMS topic and so that means every time something you have sent a topic I get an email so the email I get by",
    "start": "1621520",
    "end": "1627910"
  },
  {
    "text": "default it's somewhat useful it tells me hey this specific alarm is now firing",
    "start": "1627910",
    "end": "1632980"
  },
  {
    "text": "but it's not suit it doesn't actually give me what I need so we're make sure that it gives me what I need so what",
    "start": "1632980",
    "end": "1638320"
  },
  {
    "text": "we're gonna do is on top of my email we're gonna add a lambda function to this S&S topic the lambda function is",
    "start": "1638320",
    "end": "1644290"
  },
  {
    "text": "then going to actually pull the logs that trigger this alarm it is going to call SES to email a snippet of those",
    "start": "1644290",
    "end": "1652179"
  },
  {
    "text": "logs and custom text as I program it so now the email I'm gonna get is going to",
    "start": "1652179",
    "end": "1657190"
  },
  {
    "text": "be a lot more useful so what will you do here is provision an ec2 instance in this case we're gonna be running Tomcat",
    "start": "1657190",
    "end": "1663730"
  },
  {
    "text": "on it we're gonna configure the cloud watch logs agent to send those Tom",
    "start": "1663730",
    "end": "1671950"
  },
  {
    "text": "account logs out to cloud watch logs I'm not gonna go too deep into how to do that but it's pretty easy you just",
    "start": "1671950",
    "end": "1677320"
  },
  {
    "text": "install the agent tell it what log files to send to club lunch logs you then",
    "start": "1677320",
    "end": "1682570"
  },
  {
    "text": "define a filter pattern so in this case we're worried about unauthorized access attempts so HTTP code for one's so we're",
    "start": "1682570",
    "end": "1690370"
  },
  {
    "text": "gonna get a filter pattern to extract all the four ones from the Tomcat logs we're going to create a metric filter",
    "start": "1690370",
    "end": "1697600"
  },
  {
    "text": "and so basically we're gonna have a count of how many unauthorized access attempts there were against my instance",
    "start": "1697600",
    "end": "1703990"
  },
  {
    "text": "with Tomcat on it and then we're going to define an alarm so if there are more than five unauthorized accesses so five",
    "start": "1703990",
    "end": "1711970"
  },
  {
    "text": "four ones in my logs for one consecutive time period in this case the time periods gonna be five minutes we are",
    "start": "1711970",
    "end": "1718600"
  },
  {
    "text": "going to trigger that alarm that alarm is going to go to an SNS topic the SNS",
    "start": "1718600",
    "end": "1726820"
  },
  {
    "text": "topic is going to trigger a lambda function so we're going to have SNS trigger lambda",
    "start": "1726820",
    "end": "1732790"
  },
  {
    "text": "it's the actual lambda function what it's gonna do is first is going to give the actual metric filter information using the described metric filters API",
    "start": "1732790",
    "end": "1741370"
  },
  {
    "text": "this is going to figure out what is actually going on with this metric it's",
    "start": "1741370",
    "end": "1747070"
  },
  {
    "text": "then going to go and look at the specific relevant log data so it's going",
    "start": "1747070",
    "end": "1752230"
  },
  {
    "text": "to look at the four oh ones and it's going to use the filter log events API",
    "start": "1752230",
    "end": "1758760"
  },
  {
    "text": "so it's gonna then use SCS which is our simple email services or servers that we",
    "start": "1758760",
    "end": "1764020"
  },
  {
    "text": "have for highly deliverable email you can use it for transactional email or",
    "start": "1764020",
    "end": "1770280"
  },
  {
    "text": "marketing email in this case it's transactional email so it's going to",
    "start": "1770280",
    "end": "1775810"
  },
  {
    "text": "compose an email and it's gonna send me a snippet of the log data it's also gonna give me what the alarm name is",
    "start": "1775810",
    "end": "1781720"
  },
  {
    "text": "which AWS account this is in which region this is in what time the alarm",
    "start": "1781720",
    "end": "1787900"
  },
  {
    "text": "happened in and a link to my run book my company is a wiki so I can actually see",
    "start": "1787900",
    "end": "1793180"
  },
  {
    "text": "how do I resolve this alarm so now we're gonna trigger some 401 so we're gonna go to our web server that we spin up we're",
    "start": "1793180",
    "end": "1799780"
  },
  {
    "text": "gonna make sure to type in the wrong username and password to trigger those four ones we're gonna do it more than five times in five minutes and so this",
    "start": "1799780",
    "end": "1809140"
  },
  {
    "text": "is the email that I get by default right when that alarm was triggered so as you can see it tells me that threshold for",
    "start": "1809140",
    "end": "1815560"
  },
  {
    "text": "the alarm has been crossed what time it happened at what they the base account is and what the metric is so that's",
    "start": "1815560",
    "end": "1822160"
  },
  {
    "text": "great but doesn't actually tell me like what worse was being accessed",
    "start": "1822160",
    "end": "1827760"
  },
  {
    "text": "what were the logs saying is this something that I need to care about or not so this is the email that's",
    "start": "1827760",
    "end": "1835690"
  },
  {
    "text": "generated by the lambda function so this is much more useful so this gives me the alarm name the account the region the",
    "start": "1835690",
    "end": "1842740"
  },
  {
    "text": "the snippet of logs that I care about and the link to my run book so that some key takeaways here so you can customize",
    "start": "1842740",
    "end": "1850600"
  },
  {
    "text": "alarms to add whatever it is that you need to add to them so then when you see",
    "start": "1850600",
    "end": "1855640"
  },
  {
    "text": "a spike on a metric you can also give the action alarms that are causing that spike and the lambda function can really",
    "start": "1855640",
    "end": "1861700"
  },
  {
    "text": "be used to customize your alarm or however you want it to so let's take a look at the last use case here",
    "start": "1861700",
    "end": "1866770"
  },
  {
    "text": "so again we have let's say a few days worth of logs in s3 we would like to",
    "start": "1866770",
    "end": "1873610"
  },
  {
    "text": "spin up an elastic search cluster do some advanced analysis on those logs and then shut it down how do we do that",
    "start": "1873610",
    "end": "1879490"
  },
  {
    "text": "so you don't always want to have a long-running elastic search cluster right you're paying for the cluster for",
    "start": "1879490",
    "end": "1887200"
  },
  {
    "text": "its run time so in this case we just want to turn a cluster on do the log analysis turn it off to save some money",
    "start": "1887200",
    "end": "1894120"
  },
  {
    "text": "so you also want to take advantage of our managed elastic search service",
    "start": "1894120",
    "end": "1899980"
  },
  {
    "text": "because you don't want to have to handle manually configuring elastic search that's undifferentiated heavy lifting",
    "start": "1899980",
    "end": "1905410"
  },
  {
    "text": "that you want to not have to worry about so you want to build an on-demand cluster to analyze historical data so",
    "start": "1905410",
    "end": "1913330"
  },
  {
    "text": "what we're going to do here is CloudWatch log supports just exporting to s3 by default I'll show you to do",
    "start": "1913330",
    "end": "1919960"
  },
  {
    "text": "that we're gonna again use the event notification feature of s3 so any time",
    "start": "1919960",
    "end": "1925570"
  },
  {
    "text": "an object gets dropped into my history bucket lambda is going to we're gonna",
    "start": "1925570",
    "end": "1930580"
  },
  {
    "text": "have a lambda function that transforms that object into a format that is easily read by elastic search and it's going to",
    "start": "1930580",
    "end": "1937510"
  },
  {
    "text": "send only filtered blogs that we care about to the elastic search service so",
    "start": "1937510",
    "end": "1943809"
  },
  {
    "text": "the first thing we're actually going to analyze here is VPC flow logs so VPC",
    "start": "1943809",
    "end": "1948880"
  },
  {
    "text": "flow logs they give you network data on your V PC so they'll give you data like",
    "start": "1948880",
    "end": "1956010"
  },
  {
    "text": "somebody was trying to hit port 22 on this instance was it accepted or",
    "start": "1956010",
    "end": "1961090"
  },
  {
    "text": "rejected what was the source and destination IP address basically gives you everything but the contents of the",
    "start": "1961090",
    "end": "1966520"
  },
  {
    "text": "packet itself so as you can imagine if you turns out and for everything that can be a lot of data it can be a lot to",
    "start": "1966520",
    "end": "1972520"
  },
  {
    "text": "actually you know filter down to something that's useful so we're gonna enable we're in enable flogs in this",
    "start": "1972520",
    "end": "1980650"
  },
  {
    "text": "case we're enable them only for a certain elastic network interfaces you can enable flow logs for a whole V PC if",
    "start": "1980650",
    "end": "1986679"
  },
  {
    "text": "you want in this case it's just a certain en I which is attached 10 instance so we're going to make sure",
    "start": "1986679",
    "end": "1992980"
  },
  {
    "text": "that that actually works so it does so in this case I'm looking at this elastic network interface and I'm seeing that",
    "start": "1992980",
    "end": "1998470"
  },
  {
    "text": "the logs are coming through so then I'm going to create a lot an elastic search cluster so this is pretty",
    "start": "1998470",
    "end": "2004600"
  },
  {
    "text": "point-and-click pretty self-explanatory I'm just gonna create a default cluster and I'm going to configure my lambda",
    "start": "2004600",
    "end": "2011680"
  },
  {
    "text": "function that is triggered by logs being dropped in my x3 bucket so flow logs",
    "start": "2011680",
    "end": "2017080"
  },
  {
    "text": "automatically or we can configure the flocs to go to s3 so it's gonna use the",
    "start": "2017080",
    "end": "2024040"
  },
  {
    "text": "get object API so whenever something drops into s3 bucket we're going to we're going to read it we're gonna have",
    "start": "2024040",
    "end": "2030280"
  },
  {
    "text": "code that transforms the flow logs from their native format inside JSON document",
    "start": "2030280",
    "end": "2035290"
  },
  {
    "text": "so you can easily send that and have it parsed by elasticsearch we're going to",
    "start": "2035290",
    "end": "2041980"
  },
  {
    "text": "ingest the logs into last six search by talking to its HTTP endpoint so as you can see there it is actually posting to",
    "start": "2041980",
    "end": "2048909"
  },
  {
    "text": "my HTTP endpoint for for my elastic search cluster and then we're going to",
    "start": "2048910",
    "end": "2055389"
  },
  {
    "text": "export a certain time period of logs so in this case we're doing weeks worth of logs we're going to export them from the",
    "start": "2055390",
    "end": "2065220"
  },
  {
    "text": "flow logs where they get dropped in in cloud watch logs into an s3 bucket and",
    "start": "2065220",
    "end": "2071440"
  },
  {
    "text": "that's going to trigger my lambda function so we see that okay the export is complete I can verify it there's data",
    "start": "2071440",
    "end": "2077679"
  },
  {
    "text": "in my s3 bucket and then I can verify it it's actually made it into my elastic",
    "start": "2077679",
    "end": "2084700"
  },
  {
    "text": "search cluster so I can go to elastic search cluster I bring up kibana and I see that the data is there not only is it there but it's in JSON so I",
    "start": "2084700",
    "end": "2091210"
  },
  {
    "text": "can it has separated all the different fields such as source address destination address port and I can now",
    "start": "2091210",
    "end": "2097720"
  },
  {
    "text": "search by those fields so once it's in the Cabana you can create different",
    "start": "2097720",
    "end": "2103630"
  },
  {
    "text": "dashboards so you can you know get data and make sure that it's useful all kinds",
    "start": "2103630",
    "end": "2112570"
  },
  {
    "text": "of and some of these if you look at our around our database labs repository we've got some pre-built dashboards as",
    "start": "2112570",
    "end": "2118840"
  },
  {
    "text": "well for Cubana that you can do so the key takeaways here are you can take a",
    "start": "2118840",
    "end": "2124540"
  },
  {
    "text": "limited subset of data you can send it to elastic search on demand you can then",
    "start": "2124540",
    "end": "2131050"
  },
  {
    "text": "shut down the cluster when you don't want to pay for it anymore where you done so this reduces with cost you just",
    "start": "2131050",
    "end": "2136980"
  },
  {
    "text": "don't have to worry about scalability of your elasticsearch cluster because we imagine that for you and you have to worry about",
    "start": "2136980",
    "end": "2143089"
  },
  {
    "text": "administrating the cluster because it's a managed service and so it makes",
    "start": "2143089",
    "end": "2149099"
  },
  {
    "text": "troubleshooting easier right so just to recap the whole talk before I go into questions",
    "start": "2149099",
    "end": "2155040"
  },
  {
    "text": "monitoring it's hard and it's also very important so we want to make that easier",
    "start": "2155040",
    "end": "2161550"
  },
  {
    "text": "for you and cloud watch logs and lambda are powerful tools that can enable you",
    "start": "2161550",
    "end": "2168540"
  },
  {
    "text": "to do the right monitoring that you need for your organization what I showed you is just three different examples but",
    "start": "2168540",
    "end": "2175250"
  },
  {
    "text": "lambda is custom code that you put in and kelabra logs will take whatever",
    "start": "2175250",
    "end": "2180990"
  },
  {
    "text": "metrics you send it so aside from the three examples I showed you you can you",
    "start": "2180990",
    "end": "2186570"
  },
  {
    "text": "can you know customize those to suit your needs so just some useful links",
    "start": "2186570",
    "end": "2193460"
  },
  {
    "text": "we have website just for cloud watch taking an overview we have really good documentation for a cloud watch we have",
    "start": "2193640",
    "end": "2199950"
  },
  {
    "text": "a blog where we talk about new service launches and also examples of what",
    "start": "2199950",
    "end": "2206070"
  },
  {
    "text": "customers have done with cloud watch and the lambda functions I used in the demo scenarios they're all up in github so",
    "start": "2206070",
    "end": "2212369"
  },
  {
    "text": "you can use the code and try it yourself so thank you for your time",
    "start": "2212369",
    "end": "2217950"
  },
  {
    "text": "please remember to complete your evaluations of the session and the app we have about 12 minutes left but also",
    "start": "2217950",
    "end": "2224280"
  },
  {
    "text": "this is the last session of the day so the closing reception is next I'd be glad to take questions I'm just going to",
    "start": "2224280",
    "end": "2230609"
  },
  {
    "text": "step off to the side here and if you have a question just come up and ask me thank you [Applause]",
    "start": "2230609",
    "end": "2241359"
  }
]