[
  {
    "text": "- Hi folks, my name's Emily. I'm a machine learning\nspecialist solutions architect at Amazon Web Services.",
    "start": "1420",
    "end": "6870"
  },
  {
    "text": "And today we are gonna learn\nabout distributed hosting on AWS and especially\nwith Amazon SageMaker.",
    "start": "6870",
    "end": "13290"
  },
  {
    "text": "So at this point in our journey, no doubt you've created a foundation model",
    "start": "13290",
    "end": "19530"
  },
  {
    "text": "or you've found a foundation model, and you're trying to host this thing. So this session is all about\ndiving deep into understanding",
    "start": "19530",
    "end": "25650"
  },
  {
    "text": "what that is, why you would wanna do it, how you can do it easily\non AWS and on SageMaker.",
    "start": "25650",
    "end": "30939"
  },
  {
    "text": "So let's dive in. All right, so first off, we're gonna actually talk about\ndifferent ways of building",
    "start": "30940",
    "end": "38430"
  },
  {
    "text": "your foundation model applications. So if you're interested in\nworking with foundation models, which I'm assuming you are,",
    "start": "38430",
    "end": "45210"
  },
  {
    "text": "then first we'll recap different ways to build your applications. The reason we care about this is because",
    "start": "45210",
    "end": "50910"
  },
  {
    "text": "we're gonna work backwards from that to figure out different\nways of hosting your models, especially on AWS.",
    "start": "50910",
    "end": "57360"
  },
  {
    "text": "So these range from things\nlike batch use cases to online use cases, serverless use cases,",
    "start": "57360",
    "end": "63899"
  },
  {
    "text": "embedded use cases, things like that. So we'll learn how to\nhost a distributed model. Like once you've narrowed\ndown your hosting choices",
    "start": "63900",
    "end": "72900"
  },
  {
    "text": "and you're ready to\njust get to brass tacks, then we're gonna learn how\nto host a distributed model.",
    "start": "72900",
    "end": "78420"
  },
  {
    "text": "So to take a model, park that model on multiple accelerators, multiple instances and\ncertainly multiple accelerators.",
    "start": "78420",
    "end": "85890"
  },
  {
    "text": "Then we're gonna look at\nsome optimization techniques. So ways to improve the\nperformance of that. Most of these are gonna center around",
    "start": "85890",
    "end": "92280"
  },
  {
    "text": "actually shrinking the\nsize of the model literally so that it runs faster, with a number of other techniques as well.",
    "start": "92280",
    "end": "99450"
  },
  {
    "text": "Then we're gonna close it out\nwith a hands-on walkthrough of large model serving on AWS,",
    "start": "99450",
    "end": "104490"
  },
  {
    "text": "in particular, we have a\nfully managed container, a large model inference container",
    "start": "104490",
    "end": "109680"
  },
  {
    "text": "that you can use on SageMaker\nto easily get up and running. So let's check it out.",
    "start": "109680",
    "end": "114710"
  },
  {
    "text": "All right, so let's say\nyou've made it this far, you've picked your foundation\nmodel, you've evaluated it,",
    "start": "116387",
    "end": "125070"
  },
  {
    "text": "you've tested it, you've run\nall the prompt engineering, you've run all the fine tuning, you've done some pre-training.",
    "start": "125070",
    "end": "133920"
  },
  {
    "text": "Maybe you created a net new\npre-trained model for fun.",
    "start": "133920",
    "end": "138153"
  },
  {
    "text": "And now what are we gonna do? So let's try and go find some customers. Let's put that foundation\nmodel of front some people.",
    "start": "139050",
    "end": "145709"
  },
  {
    "text": "So first off, let's think\nabout your customers, right, your end consumers.",
    "start": "145710",
    "end": "150810"
  },
  {
    "text": "They might be using your website, they might be using your mobile app, they might be using some other\nunknown digital experience",
    "start": "150810",
    "end": "158730"
  },
  {
    "text": "yet to be invented. And so in any case, these customers are gonna be interacting with some type",
    "start": "158730",
    "end": "164460"
  },
  {
    "text": "of client application, right? You've got some type of digital experience that you're exposing to these customers.",
    "start": "164460",
    "end": "171120"
  },
  {
    "text": "And your goal in that phase is to simplify the customer experience.",
    "start": "171120",
    "end": "175533"
  },
  {
    "text": "It's an important point, it's worth saying you wanna\nsimplify the customer experience as much as possible. You wanna minimize the amount\nof work that your customers",
    "start": "176520",
    "end": "183870"
  },
  {
    "text": "have to do to understand\nyour technical system, to use your technical system,\nto run it efficiently.",
    "start": "183870",
    "end": "190530"
  },
  {
    "text": "And so the first lifecycle here is like, how your customers are\ninteracting with your app, right?",
    "start": "190530",
    "end": "196640"
  },
  {
    "text": "That's phase one. And then the second phase is\nsort of streamlining everything",
    "start": "196640",
    "end": "202170"
  },
  {
    "text": "on the backend. So how your client\napplication, which again, can be your website, it\ncan be your mobile device,",
    "start": "202170",
    "end": "208320"
  },
  {
    "text": "how that interacts with\nthe foundation model, how that stays up to date\nwith the foundation model, how that prompts the foundation model,",
    "start": "208320",
    "end": "216360"
  },
  {
    "text": "how that stylizes the\ncontent that comes out of it, how you run health checks,\nhow you run fact checks,",
    "start": "216360",
    "end": "222690"
  },
  {
    "text": "all things like this. And so different goal here, right? 'Cause the goal is to streamline\nyour development lifecycle.",
    "start": "222690",
    "end": "229770"
  },
  {
    "text": "You wanna get super fast and super good at just shipping new changes,",
    "start": "229770",
    "end": "235410"
  },
  {
    "text": "at keeping your model nice and fresh, keeping the touch points\nbetween these systems on point",
    "start": "235410",
    "end": "242310"
  },
  {
    "text": "and really clean without technical debts so that you can innovate",
    "start": "242310",
    "end": "247440"
  },
  {
    "text": "and so that you can add new\ntechnology really quickly. So at a high level, that's\nwhat we're trying to do.",
    "start": "247440",
    "end": "253053"
  },
  {
    "text": "There are many different options for building foundation\nmodel applications, as we'll come to find out.",
    "start": "254910",
    "end": "260579"
  },
  {
    "text": "Some foundation models\nare just strictly online. Like you host a model on an endpoint",
    "start": "260580",
    "end": "267300"
  },
  {
    "text": "with real-time inferencing and people are just interacting with it. They click a button, for example,",
    "start": "267300",
    "end": "274290"
  },
  {
    "text": "by typing in their prompt\nthat invokes an API, that hits the model, sends\na response back right away.",
    "start": "274290",
    "end": "281670"
  },
  {
    "text": "We call that online, right? The model is just interacting\nwith the people in real time.",
    "start": "281670",
    "end": "286143"
  },
  {
    "text": "A different type of\napplication is offline. For example, such as running in batch",
    "start": "287160",
    "end": "292980"
  },
  {
    "text": "where maybe you store a number\nof records for something",
    "start": "292980",
    "end": "297980"
  },
  {
    "text": "and then you wanna send those records to your model, run your\nmodel on those records, and then send the responses back.",
    "start": "298590",
    "end": "304540"
  },
  {
    "text": "So that's a batch use case,\nalso called an offline use case.",
    "start": "304540",
    "end": "309153"
  },
  {
    "text": "Then there's also a queued\nuse case where maybe you have your model hosted on some\nresources, and you want to",
    "start": "310290",
    "end": "318330"
  },
  {
    "text": "park a queue in front of\nthat so that the requests sort of bundle up in this queue. And then as space becomes available,",
    "start": "318330",
    "end": "325280"
  },
  {
    "text": "then they go and they get responded to and then they come back. So queue is an option.",
    "start": "325280",
    "end": "330513"
  },
  {
    "text": "You can embed your model. So certainly embedded applications, which is similar to on-device,",
    "start": "331590",
    "end": "338250"
  },
  {
    "text": "would be much more common\nin small foundation models. So for stellar 1.3 billion parameter LLM,",
    "start": "338250",
    "end": "346080"
  },
  {
    "text": "that's like, responding\nreally well would be nice. Or yeah,",
    "start": "346080",
    "end": "352380"
  },
  {
    "text": "that's probably a great\noption for a small chat, or a small Stable Diffusion model",
    "start": "352380",
    "end": "357780"
  },
  {
    "text": "could also run nicely on-device. And so those two are ways\nthat you can actually take",
    "start": "357780",
    "end": "364020"
  },
  {
    "text": "the content that is your\nfoundation model and just put it directly embedded into\nyour application code.",
    "start": "364020",
    "end": "370980"
  },
  {
    "text": "This is recommended more for obviously low connectivity scenarios",
    "start": "370980",
    "end": "377070"
  },
  {
    "text": "where you're operating\nwithout connectivity to wifi or where you just want a better, you know,",
    "start": "377070",
    "end": "382110"
  },
  {
    "text": "a faster customer experience. And so you just actually\nkeep the model on the device. So that's an option.",
    "start": "382110",
    "end": "388746"
  },
  {
    "text": "Then another option is serverless. So you can, indeed, for\nsome smaller models,",
    "start": "388746",
    "end": "393960"
  },
  {
    "text": "compile those models. And then actually I've seen\nthem converted into C++.",
    "start": "393960",
    "end": "400440"
  },
  {
    "text": "And you can then put them\nin a Lambda function, believe it or not, to then operate at a much\nless expensive price point.",
    "start": "400440",
    "end": "410159"
  },
  {
    "text": "And so each of these\noptions have tradeoffs. And there are trade-offs around latency,",
    "start": "410160",
    "end": "416430"
  },
  {
    "text": "how quickly the responses will be. There are trade-offs around\nhow expensive they're gonna be.",
    "start": "416430",
    "end": "421530"
  },
  {
    "text": "And there are trade-offs around\nhow easy it is to lifecycle the entire model. And so you and your teams will\nneed to figure out sort of",
    "start": "421530",
    "end": "428600"
  },
  {
    "text": "which hosting option\nyou're most interested in, and then go from there.",
    "start": "428601",
    "end": "434553"
  },
  {
    "text": "So one online application\nis search, all right? So let's say I give you a search bar,",
    "start": "436020",
    "end": "442919"
  },
  {
    "text": "I give you some search browser, you type in your search terms. I'm gonna take your search\nterms, and I'm gonna use a model,",
    "start": "442920",
    "end": "451560"
  },
  {
    "text": "actually, I'm gonna use a model to convert those search terms into something that's called an embedding.",
    "start": "451560",
    "end": "457800"
  },
  {
    "text": "An embedding is a dense representation, which is a fancy way of saying a list.",
    "start": "457800",
    "end": "463711"
  },
  {
    "text": "It's a long list of numbers\nthat a model basically uses",
    "start": "463711",
    "end": "468711"
  },
  {
    "text": "to understand context, right? So when we take any type of\nmodality, be that vision,",
    "start": "469800",
    "end": "477090"
  },
  {
    "text": "be that language, first we\nconvert that into embeddings to then feed that into\nyour neural network.",
    "start": "477090",
    "end": "483090"
  },
  {
    "text": "And so we'll use a model to\nconvert those search terms into embeddings.",
    "start": "483090",
    "end": "488820"
  },
  {
    "text": "Then in step two, I'm gonna use some type\nof similarity algorithm. Just like a K nearest\nneighbors would do the job.",
    "start": "488820",
    "end": "496800"
  },
  {
    "text": "Combined with a vector store\nto actually find and retrieve my documents.",
    "start": "496800",
    "end": "502470"
  },
  {
    "text": "So the search and the\nsimilarity is gonna operate at the embedding level.",
    "start": "502470",
    "end": "508770"
  },
  {
    "text": "So I'll take my words for what's the weather like\nin Washington DC for example,",
    "start": "508770",
    "end": "516864"
  },
  {
    "text": "and I'll convert that into an embedding. And then in my similarity\ndocument store here,",
    "start": "516900",
    "end": "522209"
  },
  {
    "text": "my vector store, I'm going to use like a KNN\nalgorithm to go find a document",
    "start": "522210",
    "end": "527879"
  },
  {
    "text": "that most closely relates to that. Then I'm gonna retrieve that document. And then in step three,",
    "start": "527880",
    "end": "534330"
  },
  {
    "text": "I'll use another model to\nactually convert that answer into natural language.",
    "start": "534330",
    "end": "539940"
  },
  {
    "text": "So to take the document, extract the relative\ninformation from that document,",
    "start": "539940",
    "end": "546300"
  },
  {
    "text": "and then convert that into\na natural language answer that my consumer is gonna like,\nthat my customer will enjoy.",
    "start": "546300",
    "end": "554069"
  },
  {
    "text": "So this whole process is called retrieval augmented generation. And it's using, as you can see,",
    "start": "554070",
    "end": "560100"
  },
  {
    "text": "at least two models that\nare both hosted online. So these are online application\nexamples where I'm getting",
    "start": "560100",
    "end": "568380"
  },
  {
    "text": "sort of an immediate call\nand response to my customers, and I'm retrieving documents.",
    "start": "568380",
    "end": "574230"
  },
  {
    "text": "Retrieving documents\ngenerally is a handy way to mitigate LLM hallucinations. So if I want to have a chat\njust about a couple PDFs",
    "start": "574230",
    "end": "583319"
  },
  {
    "text": "or just about a couple documents such as all of my consumer history, then retrieval augmented generation",
    "start": "583320",
    "end": "590070"
  },
  {
    "text": "is a perfect way to do this. So that's an example\nof online applications.",
    "start": "590070",
    "end": "595443"
  },
  {
    "text": "Here's an example of\nan offline application, which could be call center summarization.",
    "start": "596850",
    "end": "603420"
  },
  {
    "text": "So in call center summarization,\nI might trigger this after the call completes.",
    "start": "603420",
    "end": "609720"
  },
  {
    "text": "So if I wanna do it offline, then I'll wait until\nafter the call is done and then I'll send out the transcripts.",
    "start": "609720",
    "end": "615750"
  },
  {
    "text": "Alternatively, if I were doing an online application\nexample for call centers,",
    "start": "615750",
    "end": "621210"
  },
  {
    "text": "I would then host the model and\nbe inferencing in real time. So just some differences there.",
    "start": "621210",
    "end": "627960"
  },
  {
    "text": "So I'm gonna trigger my job and then I'm gonna invoke a pipeline. So I'll trigger this pipeline\nsort of based on the action.",
    "start": "627960",
    "end": "637550"
  },
  {
    "text": "I'll trigger a pipeline,\nwhich could be in Lambda, or it could be in any\norchestration tooling.",
    "start": "638010",
    "end": "645660"
  },
  {
    "text": "And then I'm gonna point to\nmy call center transcripts. So maybe I uploaded those to S3.",
    "start": "645660",
    "end": "652649"
  },
  {
    "text": "And then using the upload\nto S3, I invoke a pipeline, probably the most common flow.",
    "start": "652650",
    "end": "658710"
  },
  {
    "text": "And so I'll point to my\ncall center transcripts, and then I'm gonna run a batch job. So I'll spin up some instances",
    "start": "658710",
    "end": "666519"
  },
  {
    "text": "or just point to another LLM, maybe if the LLM is already serverless. And then I'm gonna invoke the\nmodel with the transcripts.",
    "start": "667680",
    "end": "675990"
  },
  {
    "text": "So then I'll take my transcripts, pop the transcripts into\nthe prompt, and I'll say,",
    "start": "675990",
    "end": "681050"
  },
  {
    "text": "dear LLM, give me a summary. And then I'll evaluate those\nresponses and send them",
    "start": "681051",
    "end": "686580"
  },
  {
    "text": "to my analyst teams. So this is a way to implement\ncall center summarization",
    "start": "686580",
    "end": "692540"
  },
  {
    "text": "using LLMs and using foundation models, notably in an offline fashion right here.",
    "start": "693270",
    "end": "698500"
  },
  {
    "text": "We trigger this process\nafter call completes, invoking a pipeline, and sort\nof running some processes",
    "start": "698500",
    "end": "707010"
  },
  {
    "text": "to prompt a model and\nthen get responses back.",
    "start": "707010",
    "end": "709713"
  },
  {
    "text": "All right, so how do\nwe do this, you wonder? Now, I know this sounds very interesting, but literally when it\ncomes to brass tacks,",
    "start": "712470",
    "end": "719220"
  },
  {
    "text": "how am I gonna take my model\nand put my model somewhere? So SageMaker has a really\nawesome model deployment stack.",
    "start": "719220",
    "end": "728810"
  },
  {
    "text": "We've a lot of technology, a lot of capabilities that you can use",
    "start": "728859",
    "end": "734220"
  },
  {
    "text": "to host your own models. And so this view sort of lets\nyou see all of the things that",
    "start": "734220",
    "end": "740670"
  },
  {
    "text": "are possible in SageMaker model deployment across different instances,\nCPU and GPU accelerators,",
    "start": "740670",
    "end": "747740"
  },
  {
    "text": "deep learning compilers and\ndifferent model run times, a variety of model servers\nsuch as TensorFlow, TorchServe,",
    "start": "747741",
    "end": "755790"
  },
  {
    "text": "NVIDIA Triton, DJL, all sorts of things. And then frameworks on top of that.",
    "start": "755790",
    "end": "761339"
  },
  {
    "text": "So the model server is sort\nof taking the responses and integrating those into a RESTful API.",
    "start": "761340",
    "end": "768500"
  },
  {
    "text": "And then the framework is\njust implementing the model. So the framework is implementing\nthe low level model,",
    "start": "768500",
    "end": "775410"
  },
  {
    "text": "which is then running\njust the forward pass through the neural network.",
    "start": "775410",
    "end": "779730"
  },
  {
    "text": "And then SageMaker of course\nis managing the infrastructure for me, of the actual instances.",
    "start": "781110",
    "end": "787140"
  },
  {
    "text": "So for the rest of the slides here, we're just gonna look at real-time\ninferencing on SageMaker,",
    "start": "787140",
    "end": "793020"
  },
  {
    "text": "using Hugging Face and using\nthe DJL serving library.",
    "start": "793020",
    "end": "796863"
  },
  {
    "text": "So what do we need to\nhost a model on SageMaker? So really I need three things.",
    "start": "798930",
    "end": "804060"
  },
  {
    "text": "I need a model artifact, which is stored in my S3 bucket. And I'm gonna wanna tar zip that thing.",
    "start": "804060",
    "end": "811470"
  },
  {
    "text": "So I have my model artifact\nstored in my bucket. I'm going to use an inferencing image",
    "start": "811470",
    "end": "818130"
  },
  {
    "text": "that's hosted in a registry. So for example, I can bring\nmy own inference.py scripts,",
    "start": "818130",
    "end": "825170"
  },
  {
    "text": "but I might use an image\nthat's hosted in ECR",
    "start": "825210",
    "end": "830210"
  },
  {
    "text": "to easily get all of the\npackages that I need. And then of course, I'm gonna need some managed ML instances",
    "start": "830220",
    "end": "836040"
  },
  {
    "text": "to actually provision this for me. One pro tip for you,",
    "start": "836040",
    "end": "841412"
  },
  {
    "text": "the latest deep learning\ncontainers are a really great place that you can find scripts that run nicely.",
    "start": "842250",
    "end": "850680"
  },
  {
    "text": "So basically, if you point to the latest\ndeep learning container, it's much more likely to easily\nrun your inferencing script.",
    "start": "850680",
    "end": "857546"
  },
  {
    "text": "This is also true for\ntraining because we maintain really up-to-date deep\nlearning containers.",
    "start": "857546",
    "end": "864540"
  },
  {
    "text": "And so you can easily pick\na deep learning container that matches your inferencing\nframework such as PyTorch.",
    "start": "864540",
    "end": "872520"
  },
  {
    "text": "And here we're gonna look at a managed deep learning\ncontainer for model hosting.",
    "start": "872520",
    "end": "876782"
  },
  {
    "text": "So that's what we need. So the large model inference\ncontainer on SageMaker",
    "start": "878370",
    "end": "885510"
  },
  {
    "text": "starts with, again, a base Docker image. And then has the ability to\nwork on different instances.",
    "start": "885510",
    "end": "894390"
  },
  {
    "text": "So different accelerators, be that GPU, the Neuron SDK, or CPU.",
    "start": "894390",
    "end": "900540"
  },
  {
    "text": "And so these are stored in the image. And then PyTorch runs on top of that. So PyTorch is gonna point to\nthose low level frameworks,",
    "start": "900540",
    "end": "909317"
  },
  {
    "text": "and then we'll use a\ndistributed training framework on top of that. So the distributed training\nframework here is letting us",
    "start": "909317",
    "end": "917610"
  },
  {
    "text": "shard our model over multiple GPUs. Commonly, in this case,",
    "start": "917610",
    "end": "923040"
  },
  {
    "text": "we're actually only gonna use tensor and pipeline parallelism. Data parallelism is not needed\nbecause we're only doing",
    "start": "923040",
    "end": "931260"
  },
  {
    "text": "a forward pass. So because we're only doing a forward pass through the neural nets,",
    "start": "931260",
    "end": "937140"
  },
  {
    "text": "we're actually not doing any\ngradient optimizing at all, because we're not learn\ntraining the parameters,",
    "start": "937140",
    "end": "943200"
  },
  {
    "text": "we're just invoking them. So we don't need data parallel, actually, we're just using tensor\nand pipeline parallelism.",
    "start": "943200",
    "end": "950050"
  },
  {
    "text": "Great. And so we use this distributed\nframework to shard the layers",
    "start": "950050",
    "end": "955883"
  },
  {
    "text": "of the neural network across\nmultiple accelerators. And then we'll put all of this inside of a serving environment that\nlets us easily host that model",
    "start": "956370",
    "end": "966020"
  },
  {
    "text": "and host distributed models. And so the large model inference\ncontainer also includes just faster loading because\nthe models are quite large.",
    "start": "966690",
    "end": "976019"
  },
  {
    "text": "Some of the largest are\nactually multiple hundreds of gigs on disk, they're\nquite significant beasts.",
    "start": "976020",
    "end": "982290"
  },
  {
    "text": "And so we wanna use faster model loading to easily load those.",
    "start": "982290",
    "end": "987509"
  },
  {
    "text": "And so this again is a managed\ncontainer that you can use to host large models on SageMaker.",
    "start": "987510",
    "end": "993423"
  },
  {
    "text": "So let's take a look\nat some of the latest. So these are available on GitHub. It is the AWS Deep Learning\nContainer Repository",
    "start": "994770",
    "end": "1003649"
  },
  {
    "text": "on GitHub. And so here you can just see again, posted online are different frameworks.",
    "start": "1003650",
    "end": "1009230"
  },
  {
    "text": "So DJLServing with FasterTransformer, Hugging Face transformers\nand Hugging Face accelerates.",
    "start": "1009230",
    "end": "1015950"
  },
  {
    "text": "And this is the inference containers. So these are for all inferencing. And then different accelerators, right?",
    "start": "1015950",
    "end": "1021770"
  },
  {
    "text": "So GPU, Neurons, latest versions\nof PyTorch, Python rather,",
    "start": "1021770",
    "end": "1026770"
  },
  {
    "text": "more recent versions\nof Python I should say. And then the example URL. And so when you're working",
    "start": "1029090",
    "end": "1035329"
  },
  {
    "text": "with the deep learning\ncontainers, you can copy this URL. Actually this URL is global,",
    "start": "1035330",
    "end": "1043130"
  },
  {
    "text": "and so everyone is going\nto use this same address",
    "start": "1043130",
    "end": "1047827"
  },
  {
    "text": "for the ECR image. So you're just gonna literally\ncopy this and then just update it based on the\nregion that you're in.",
    "start": "1048680",
    "end": "1055760"
  },
  {
    "text": "But the account ID is the same. So it's just a net copy.",
    "start": "1055760",
    "end": "1060413"
  },
  {
    "text": "Then you can extend this thing. So once you are pointing to\nthe deep learning containers,",
    "start": "1061610",
    "end": "1067820"
  },
  {
    "text": "again, using the right\nregion that you're in and updating the container name,",
    "start": "1067820",
    "end": "1074450"
  },
  {
    "text": "you can extend this container. This is, again, both for\nhosting and for training.",
    "start": "1074450",
    "end": "1081299"
  },
  {
    "text": "And it's really helpful\nbecause you'll likely encounter some packages that aren't pip installable",
    "start": "1081300",
    "end": "1088970"
  },
  {
    "text": "that you actually need to\nwrite out some batch code for. Maybe it's a really gnarly install",
    "start": "1088970",
    "end": "1094250"
  },
  {
    "text": "and a lot of really complex\nconfigs that you need. Docker is a great way to just build",
    "start": "1094250",
    "end": "1100640"
  },
  {
    "text": "a net new image for yourself. This is true in hosting and for training for\nnotebooks as well actually.",
    "start": "1100640",
    "end": "1107090"
  },
  {
    "text": "And so this way you can easily, again, extend your prebuilt container,",
    "start": "1107090",
    "end": "1112789"
  },
  {
    "text": "package all of your programs\ninto this container, and then interact with it. So this is a nice way to\nstreamline your operations.",
    "start": "1112790",
    "end": "1121253"
  },
  {
    "text": "All right, then let's see if\nwe can understand this thing. So we've got our model artifacts,",
    "start": "1122930",
    "end": "1129200"
  },
  {
    "text": "our model weights sitting in S3. And then when we invoke this,\nwhen we spin up this instance,",
    "start": "1129200",
    "end": "1134840"
  },
  {
    "text": "we're gonna download the model from S3. So that gets downloaded to\nthe model inferencing host,",
    "start": "1134840",
    "end": "1142130"
  },
  {
    "text": "this inference endpoint\nthrough the model container, which is using Docker. This is then going to save the model",
    "start": "1142130",
    "end": "1148789"
  },
  {
    "text": "on this local instance storage, in this case, some NVMe instance storage.",
    "start": "1148790",
    "end": "1154700"
  },
  {
    "text": "Once this has been saved to disk, then we're gonna load it, this case using faster transformer,",
    "start": "1154700",
    "end": "1160100"
  },
  {
    "text": "which optimizes the model for hosting. And then you see there are\na number of options here.",
    "start": "1160100",
    "end": "1166010"
  },
  {
    "text": "So Stable Diffusion, DeepSpeed,\ngeneric python scripts, Hugging Face Accelerate,\nand Transformers NeuronX.",
    "start": "1166010",
    "end": "1173390"
  },
  {
    "text": "So a lot of different open\nsource packages that you can use. Then we're gonna load that\nmodel on multiple GPUs.",
    "start": "1173390",
    "end": "1181460"
  },
  {
    "text": "So the large model inference\ncontainer is really nice because it sort of handles a lot of the distribution for you.",
    "start": "1181460",
    "end": "1188299"
  },
  {
    "text": "Again, because you're not\ndoing any data parallelism, because we're not doing\nany back propagation,",
    "start": "1188300",
    "end": "1193853"
  },
  {
    "text": "it's easier to automate\nthe partitioning of that basically for those reasons.",
    "start": "1195961",
    "end": "1201110"
  },
  {
    "text": "And so we'll partition\nthe model using those.",
    "start": "1201110",
    "end": "1206110"
  },
  {
    "text": "And then that sort of result\ngoes into the model server, which can then handle inference requests.",
    "start": "1206510",
    "end": "1212330"
  },
  {
    "text": "So then inference requests\ninteract with the model server, which then wraps the partitioned model,",
    "start": "1212330",
    "end": "1220490"
  },
  {
    "text": "the model that's partitioned\non multiple GPUs. And there you have it,",
    "start": "1220490",
    "end": "1225710"
  },
  {
    "text": "that's distributed model\nhosting on SageMaker. But this is not the only thing.",
    "start": "1225710",
    "end": "1231740"
  },
  {
    "text": "There are a number of\noptimizations for this. So ways that we can then improve our model",
    "start": "1231740",
    "end": "1236779"
  },
  {
    "text": "to make it run faster,\nmake the model smaller, and all of that improves our runtime.",
    "start": "1236780",
    "end": "1242120"
  },
  {
    "text": "So if we start with a large model, then we'll likely use a model compilation",
    "start": "1242120",
    "end": "1247400"
  },
  {
    "text": "and or model compression technique to just actually make the\nmodel physically smaller.",
    "start": "1247400",
    "end": "1252919"
  },
  {
    "text": "So compilation will take the\nmodel and then compile it for appropriate GPUs.",
    "start": "1252920",
    "end": "1259640"
  },
  {
    "text": "So it'll make it run\nfaster and more optimally on a variety of GPUs.",
    "start": "1259640",
    "end": "1266570"
  },
  {
    "text": "And then model compression\nhas a similar effect",
    "start": "1266570",
    "end": "1271570"
  },
  {
    "text": "but operates in a couple different ways. So quantization actually\nchanges the data representation.",
    "start": "1272240",
    "end": "1278060"
  },
  {
    "text": "So you might have trained\nusing FP16 or BF16,",
    "start": "1278060",
    "end": "1283060"
  },
  {
    "text": "and then you might\nquantize down to integers. Actually you might quantize down to int8,",
    "start": "1283640",
    "end": "1289100"
  },
  {
    "text": "which has a smaller representation space. Pruning, you might just strictly\nremove some of the layers",
    "start": "1289100",
    "end": "1297500"
  },
  {
    "text": "of the neural network. And you might perform\ndistillation where you start",
    "start": "1297500",
    "end": "1302570"
  },
  {
    "text": "with a larger neural network and then you run a teacher-student\ndistillation process",
    "start": "1302570",
    "end": "1308659"
  },
  {
    "text": "to then train this much smaller model. Model partitioning, model\ndistribution we learned about.",
    "start": "1308660",
    "end": "1314840"
  },
  {
    "text": "And then prompt engineering\nwe've learned about as well. And so these are a number of\nways that you can optimize",
    "start": "1314840",
    "end": "1320180"
  },
  {
    "text": "your large models on SageMaker. And with that, let's check out the demo.",
    "start": "1320180",
    "end": "1326150"
  },
  {
    "text": "So in this notebook, we are going to deploy a\n175 billion parameter LLM.",
    "start": "1326150",
    "end": "1333040"
  },
  {
    "text": "That's correct, This is a\nextremely large language model. This is an open source\nLLM built by Hugging Face",
    "start": "1334130",
    "end": "1342260"
  },
  {
    "text": "during their big science project. This is known as Bloom. And so bloom comes in a\nfew different varieties.",
    "start": "1342260",
    "end": "1350240"
  },
  {
    "text": "This is the largest,\n175 billion parameters.",
    "start": "1350240",
    "end": "1355240"
  },
  {
    "text": "And this notebook very cleanly wraps the end-to-end deployment. And so we're gonna do this on SageMaker.",
    "start": "1355250",
    "end": "1363470"
  },
  {
    "text": "So as before, we'll use the\nSageMaker example notebooks. In particular, this is a DJL\nDeepSpeed deployment notebook.",
    "start": "1363470",
    "end": "1372700"
  },
  {
    "text": "And you have a QR code that\nyou can use to get the link. Alternatively, if you're\non a laptop or a desktop,",
    "start": "1372701",
    "end": "1381700"
  },
  {
    "text": "you're welcome to just use the Bitly. So let's check it out.",
    "start": "1382610",
    "end": "1386650"
  },
  {
    "text": "All right, so as before in\na SageMaker Studio domain.",
    "start": "1392030",
    "end": "1397030"
  },
  {
    "text": "And here I have downloaded the notebook. So the notebook, you'll see,",
    "start": "1398660",
    "end": "1404930"
  },
  {
    "text": "and let's just go check\nthis out over here.",
    "start": "1404930",
    "end": "1407393"
  },
  {
    "text": "So the notebook is nice. You know what, lemme use the bitly. So bit.ly/sm-nb-7-hosting.",
    "start": "1412490",
    "end": "1421817"
  },
  {
    "text": "There we go. So this notebook is handy because it's extremely self-contained. I like that a lot about it.",
    "start": "1424850",
    "end": "1431230"
  },
  {
    "text": "It doesn't use many other files. All of the files this notebook uses",
    "start": "1431230",
    "end": "1437270"
  },
  {
    "text": "are actually just in the notebook itself, so you don't need many other artifacts.",
    "start": "1437270",
    "end": "1443633"
  },
  {
    "text": "In addition, it also has\nthis nice local mode option. So if you are running a\nnotebook on a P4d machine,",
    "start": "1444500",
    "end": "1452710"
  },
  {
    "text": "then you're welcome to to run this here. So let's check this out. All right, so I'm in Studio.",
    "start": "1453140",
    "end": "1460520"
  },
  {
    "text": "And here I'm running on a small CPU here. So this is the m5.large instance type.",
    "start": "1460520",
    "end": "1468443"
  },
  {
    "text": "And let's take a look at this. So first we're going to just update boto3",
    "start": "1469610",
    "end": "1478540"
  },
  {
    "text": "and the AWS CLI and the\nSageMaker Python SDK. And then again,",
    "start": "1478541",
    "end": "1483823"
  },
  {
    "text": "I'm gonna go straight to the p4d hosted in the inference mode and\nnot in the local mode,",
    "start": "1483823",
    "end": "1491390"
  },
  {
    "text": "as I find that a little bit cleaner. But some people love to do locally.",
    "start": "1491390",
    "end": "1496133"
  },
  {
    "text": "Local mode, if you're not aware, is a way to develop and test\nquickly for training jobs",
    "start": "1497730",
    "end": "1505120"
  },
  {
    "text": "and for endpoints in SageMaker. And essentially you\ncan add this parameter,",
    "start": "1507170",
    "end": "1513890"
  },
  {
    "text": "setting the mode to local, and then you can do a bit\nof local development there.",
    "start": "1513890",
    "end": "1518520"
  },
  {
    "text": "All right, so we're gonna\ngrab a few artifacts here. And SageMaker likes the model\nto be in this tarball format.",
    "start": "1520400",
    "end": "1528770"
  },
  {
    "text": "And so there's gonna be\na directory called code. And then this code directory",
    "start": "1528770",
    "end": "1533900"
  },
  {
    "text": "is gonna have this model.py, and that is the inference function",
    "start": "1533900",
    "end": "1539510"
  },
  {
    "text": "that we're gonna walk through so you can see how to format your models for SageMaker inferencing.",
    "start": "1539510",
    "end": "1545990"
  },
  {
    "text": "And then there's the\nserving.properties file as well.",
    "start": "1545990",
    "end": "1549203"
  },
  {
    "text": "All right, so we'll do our\nSageMaker imports as usual,",
    "start": "1551240",
    "end": "1556240"
  },
  {
    "text": "got our execution role,\nsession, bucket, et cetera.",
    "start": "1556430",
    "end": "1560753"
  },
  {
    "text": "Then the model bucket. Example files, the code\nprefix, hf-large-model-djl.",
    "start": "1563030",
    "end": "1570610"
  },
  {
    "text": "And then the model prefix. So those are just folders in this bucket. And then the image URI.",
    "start": "1573209",
    "end": "1579740"
  },
  {
    "text": "So the image_uris, let's\nsee where that comes from.",
    "start": "1579740",
    "end": "1584543"
  },
  {
    "text": "image_uris comes from\nthe SageMaker Python SDK. And so this is an object\nthat's gonna let us",
    "start": "1585777",
    "end": "1593240"
  },
  {
    "text": "just point to the base image that this endpoint is going to use. So image_uris.retrieve,",
    "start": "1593240",
    "end": "1600100"
  },
  {
    "text": "we're gonna provide it\nthat framework name. So this is the DJL or the\nDeep Java Library framework,",
    "start": "1600100",
    "end": "1606680"
  },
  {
    "text": "which is nice for hosting large models. And in particular, this\nis a DeepSpeed version.",
    "start": "1606680",
    "end": "1612169"
  },
  {
    "text": "There are many other distributed\nframeworks you can use for hosting a large model.",
    "start": "1612170",
    "end": "1617630"
  },
  {
    "text": "This one just happens to use DeepSpeed. But you can also use\nHugging Face Accelerate,",
    "start": "1617630",
    "end": "1623660"
  },
  {
    "text": "you can use any number of new frameworks that have been developed, there's\na lot of flexibility here.",
    "start": "1623660",
    "end": "1629603"
  },
  {
    "text": "All right, so we're gonna\nmake our directory here.",
    "start": "1630740",
    "end": "1634463"
  },
  {
    "text": "So this code_bloom176. Oh, and this is the image URI, by the way.",
    "start": "1636020",
    "end": "1641270"
  },
  {
    "text": "And so this is literally the\naddress for that image in ECR.",
    "start": "1641270",
    "end": "1645353"
  },
  {
    "text": "So here is our script. So from djl_python,",
    "start": "1646670",
    "end": "1651745"
  },
  {
    "text": "importing a couple objects,\nimporting deepspeed.",
    "start": "1651745",
    "end": "1654623"
  },
  {
    "text": "And then remember, the packages for this are\nalready installed in this image.",
    "start": "1656810",
    "end": "1662380"
  },
  {
    "text": "So because we're using this\nprebuilt inferencing image",
    "start": "1663320",
    "end": "1668039"
  },
  {
    "text": "for DJL, it has these base\npackages pre-installed.",
    "start": "1668990",
    "end": "1673990"
  },
  {
    "text": "So there's no need to bring\nour own requirements.txt. If you do need to create\nyour own container for this,",
    "start": "1674100",
    "end": "1683000"
  },
  {
    "text": "then simply follow the steps\nto create a new container on SageMaker, notably by\nusing a notebook instance,",
    "start": "1683000",
    "end": "1690880"
  },
  {
    "text": "creating a new Docker container, pushing that to the\nelastic container registry.",
    "start": "1691130",
    "end": "1696470"
  },
  {
    "text": "And then you can use that\nfor your hosting image. So here's our model.py.",
    "start": "1696470",
    "end": "1702833"
  },
  {
    "text": "And we're importing\ndeepspeed, torch, math, os.",
    "start": "1703906",
    "end": "1708906"
  },
  {
    "text": "And then from the transformers\nlibrary from Hugging Face, this AutoConfig,",
    "start": "1709340",
    "end": "1714561"
  },
  {
    "text": "the AutoModel for causal\nlanguage modeling, and then the tokenizer.",
    "start": "1714561",
    "end": "1718500"
  },
  {
    "text": "And then this load model function here is looking for the number of partitions.",
    "start": "1719750",
    "end": "1725900"
  },
  {
    "text": "So remember tensor parallelism, as we learned about in the\nsession on distributed training,",
    "start": "1725900",
    "end": "1732410"
  },
  {
    "text": "is a way to take your neural net, especially a layer in a neural network,",
    "start": "1732410",
    "end": "1739370"
  },
  {
    "text": "and partition that across\nmultiple accelerators. And so that would commonly be\na multi-head attention block",
    "start": "1739370",
    "end": "1747670"
  },
  {
    "text": "that is too big to fit on a single GPU. And so you need multiple\nGPUs or accelerators",
    "start": "1748970",
    "end": "1754910"
  },
  {
    "text": "to partition that out. And so that's this parameter actually is the tensor parallel degree.",
    "start": "1754910",
    "end": "1761363"
  },
  {
    "text": "And then the model\nlocation is the directory on the hosting instance where the checkpoints will be downloaded.",
    "start": "1763850",
    "end": "1770630"
  },
  {
    "text": "So that's the model ID. And then the tokenizer, right? This should look very familiar.",
    "start": "1770630",
    "end": "1776690"
  },
  {
    "text": "So the tokenizer is going to\nlook in this model location. And so what this means is that you can,",
    "start": "1776690",
    "end": "1785060"
  },
  {
    "text": "so here what's happening\nis the name of the model",
    "start": "1785060",
    "end": "1790060"
  },
  {
    "text": "is also being used as the\nname of the local directory on the hosting instance",
    "start": "1790880",
    "end": "1798080"
  },
  {
    "text": "where the model artifact will\njust sit, it will live there. And then we're pointing to that location",
    "start": "1798080",
    "end": "1804380"
  },
  {
    "text": "from this auto tokenizer. And so we're reading from pre-trained to that model location.",
    "start": "1804380",
    "end": "1809429"
  },
  {
    "text": "And now we're gonna construct a model, interesting, apparently\nwith fake meta tensors,",
    "start": "1811460",
    "end": "1820470"
  },
  {
    "text": "which will later be replaced during the DeepSpeed\ninference checkpoint load.",
    "start": "1820470",
    "end": "1827090"
  },
  {
    "text": "Okay, interesting. So here we're gonna define\nthis false meta model,",
    "start": "1827090",
    "end": "1832940"
  },
  {
    "text": "which we'll then update. So the model is this\nfrom this auto config,",
    "start": "1832940",
    "end": "1839960"
  },
  {
    "text": "same as before, using BF16. And then the logging,",
    "start": "1839960",
    "end": "1847159"
  },
  {
    "text": "and remember this just\ngoes straight to CloudWatch so you can see what's happening. So starting DeepSpeed init",
    "start": "1847160",
    "end": "1853310"
  },
  {
    "text": "with a tensor parallel degree\nof whatever this is gonna be. And then we define the model.",
    "start": "1853310",
    "end": "1858623"
  },
  {
    "text": "And the tensor parallel presharded repos come with their own checkpoint\nconfig, which would be great.",
    "start": "1860424",
    "end": "1865460"
  },
  {
    "text": "Then we will define the model size here, in this case using int8.",
    "start": "1866600",
    "end": "1871910"
  },
  {
    "text": "So that's quite aggressively quantized. Replacing a few objects here.",
    "start": "1871910",
    "end": "1879053"
  },
  {
    "text": "And then setting the checkpoints. And then this is reading the checkpoints, which will then replace that base model.",
    "start": "1881060",
    "end": "1888173"
  },
  {
    "text": "Okay, so you've got a\nfunction, load model. So this function load model\nis doing all of this for you.",
    "start": "1889220",
    "end": "1897110"
  },
  {
    "text": "It's initializing the tokenizer,",
    "start": "1897110",
    "end": "1900437"
  },
  {
    "text": "and then is creating this\ndeep speed false model, which it then changes out by\npointing to the checkpoint.",
    "start": "1903710",
    "end": "1912560"
  },
  {
    "text": "And then just returns both\nthe model and the tokenizer. And then this run inference\nfunction takes that model,",
    "start": "1912560",
    "end": "1921260"
  },
  {
    "text": "takes the tokenizer, the new data on which you'd\nlike to run inference,",
    "start": "1921260",
    "end": "1927380"
  },
  {
    "text": "and the parameters. And then just set this. So we'll set the pad tokens,",
    "start": "1927380",
    "end": "1933563"
  },
  {
    "text": "grab the inputs by performing\ntokenization on those inputs just here, walking through those.",
    "start": "1934580",
    "end": "1942443"
  },
  {
    "text": "And if it's a valid tensor,\nsetting it to the device.",
    "start": "1943580",
    "end": "1948580"
  },
  {
    "text": "And then calling generate. So we're generating the input tokens and here we're adding that.",
    "start": "1949160",
    "end": "1955853"
  },
  {
    "text": "All right, so we've sent\nthe token to the device, which sends it to the GPU,\nthen call model.generate.",
    "start": "1961070",
    "end": "1968300"
  },
  {
    "text": "Then we get these outputs and then we're gonna run the\noutputs through the tokenizer one more time to get the natural language.",
    "start": "1968300",
    "end": "1977100"
  },
  {
    "text": "And then the last process\nhere is this handler",
    "start": "1977100",
    "end": "1981030"
  },
  {
    "text": "that does some of this work for us. So it's using the global arguments,",
    "start": "1983020",
    "end": "1991040"
  },
  {
    "text": "which is the model and the tokenizer. And then if it doesn't have those, it's gonna try to load them.",
    "start": "1991040",
    "end": "1996580"
  },
  {
    "text": "And then we will get the\ndata through the inputs",
    "start": "1999590",
    "end": "2004590"
  },
  {
    "text": "where these inputs are\ncoming from the web server. So in this case,\nessentially this is coming",
    "start": "2006220",
    "end": "2013750"
  },
  {
    "text": "from this serving.properties config. And so the web server,",
    "start": "2013750",
    "end": "2019150"
  },
  {
    "text": "which is handling the API build out, is gonna send you the data to,",
    "start": "2019150",
    "end": "2025930"
  },
  {
    "text": "basically through this\nfunction, so inputs. And then you'll just\nwrite that get_as_json",
    "start": "2025930",
    "end": "2032595"
  },
  {
    "text": "and you'll get your data. And then likely you set it as a JSON here.",
    "start": "2032595",
    "end": "2039930"
  },
  {
    "text": "So then from that we'll\nget the input sentence and then we'll get the parameters.",
    "start": "2041410",
    "end": "2049149"
  },
  {
    "text": "And then just try to run inference using everything as last\ntime, using the model,",
    "start": "2049150",
    "end": "2055240"
  },
  {
    "text": "the tokenizer, the input\nsentences and the parameters. And then you'll get\nthis output, hopefully,",
    "start": "2055240",
    "end": "2063429"
  },
  {
    "text": "and then put the outputs\nin this new object, package that up, and\nthen send it as a JSON.",
    "start": "2063430",
    "end": "2071383"
  },
  {
    "text": "All right, and so that\nis your inferencing code. If you don't want to write\nyour inferencing code,",
    "start": "2073600",
    "end": "2080829"
  },
  {
    "text": "no worries. There are of course many\nexamples that do this. And there are other sort\nof containers that have",
    "start": "2080830",
    "end": "2090369"
  },
  {
    "text": "a slightly higher level\napproach where you can just send your model ID.",
    "start": "2090370",
    "end": "2095770"
  },
  {
    "text": "So actually one of them\nis the Hugging Face,",
    "start": "2095770",
    "end": "2100770"
  },
  {
    "text": "they have a lot of\nSageMaker example notebooks that are quite good. So let's see if we can find them.",
    "start": "2102400",
    "end": "2109450"
  },
  {
    "text": "Yeah, from Phil Schmid. So these are excellent, they're a really good source of examples.",
    "start": "2109450",
    "end": "2118300"
  },
  {
    "text": "And there's one in particular\nfor deploying that I was looking at recently that, which is here.",
    "start": "2118300",
    "end": "2127000"
  },
  {
    "text": "So this uses a new deep learning container that Hugging Face built\nfor deploying large models.",
    "start": "2127000",
    "end": "2135190"
  },
  {
    "text": "And what's nice about it,\nand very straightforward, is you just pass the model ID.",
    "start": "2135190",
    "end": "2141340"
  },
  {
    "text": "So in this case you really\njust send the model ID,",
    "start": "2141340",
    "end": "2146340"
  },
  {
    "text": "such as here, to this framework, and then it's very easy to deploy.",
    "start": "2146500",
    "end": "2153020"
  },
  {
    "text": "Now this is limited to a\nsmaller number of models. So this is supported for these\nvarious model architectures.",
    "start": "2153020",
    "end": "2161580"
  },
  {
    "text": "And so if your large language\nmodel is within this list",
    "start": "2161890",
    "end": "2166890"
  },
  {
    "text": "of about 15, oh, I'm exaggerating,\nlist of about 10 models,",
    "start": "2166930",
    "end": "2171627"
  },
  {
    "text": "then it should be quite easy\nbecause you simply point to it and then say deploy.",
    "start": "2173290",
    "end": "2179980"
  },
  {
    "text": "And that's quite straightforward. It's fun too, they actually\nadded this little chat here",
    "start": "2179980",
    "end": "2185710"
  },
  {
    "text": "that lets you like, interact with it through a radio interface actually.",
    "start": "2185710",
    "end": "2190830"
  },
  {
    "text": "Yeah, which looks like this, so then you can actually like\nchat with it in your notebook",
    "start": "2190830",
    "end": "2196269"
  },
  {
    "text": "or locally, which is kind of fun. So that's nice. But let's go use our Bloom model.",
    "start": "2196270",
    "end": "2202810"
  },
  {
    "text": "So, and I picked this one\nfor you because I wanted you",
    "start": "2202810",
    "end": "2207810"
  },
  {
    "text": "to see sort of this lower level of how you can build your\nown inferencing function.",
    "start": "2208300",
    "end": "2215830"
  },
  {
    "text": "But if you don't want to do that, then the other notebooks\nare a good candidate path.",
    "start": "2215830",
    "end": "2221983"
  },
  {
    "text": "And so let's keep going. So this is that serving.properties file,",
    "start": "2223840",
    "end": "2231913"
  },
  {
    "text": "which is one way to write this. We can also just do write\nfile, but that seems fine.",
    "start": "2233020",
    "end": "2238453"
  },
  {
    "text": "And then just tarring the model archive or tarring the model\nto produce the archive.",
    "start": "2239290",
    "end": "2246819"
  },
  {
    "text": "Uploading this local file to S3. So now it's out in S3.",
    "start": "2246820",
    "end": "2252882"
  },
  {
    "text": "Optionally you can add\nyour VPC configs to this. And then we're gonna create the model.",
    "start": "2253892",
    "end": "2261070"
  },
  {
    "text": "So the model name here,\nwhich we can bring. And then we're gonna use the, again,",
    "start": "2261070",
    "end": "2267970"
  },
  {
    "text": "SageMaker Python SDK. So we'll create this model. We have this model name.",
    "start": "2267970",
    "end": "2273640"
  },
  {
    "text": "The primary image is that inferencing URI that we got earlier. And then we have the\nBloom model sitting in S3.",
    "start": "2273640",
    "end": "2283380"
  },
  {
    "text": "And then we'll say create\nmodel, and that's this. All right, and then\nstill using the SDK here,",
    "start": "2283750",
    "end": "2293263"
  },
  {
    "text": "we will give this endpoint config a name and we'll create the endpoint\nconfig using the p4d.",
    "start": "2294280",
    "end": "2301840"
  },
  {
    "text": "So a pretty large instance here that has eight NVIDIA A100's on the p4d.",
    "start": "2301840",
    "end": "2309840"
  },
  {
    "text": "And then we'll get the response. And so first we're creating this endpoint,",
    "start": "2312310",
    "end": "2317589"
  },
  {
    "text": "and the endpoint config rather. And then once the endpoint\nconfig is created, then we create the endpoint itself.",
    "start": "2317590",
    "end": "2324763"
  },
  {
    "text": "And then once that's been created, the notebook essentially walks you through",
    "start": "2326200",
    "end": "2333280"
  },
  {
    "text": "what we already looked at, which is some of the key\npoints in that script",
    "start": "2333280",
    "end": "2338440"
  },
  {
    "text": "to load the model from S3. And then there's a nice\nlittle weight function here.",
    "start": "2338440",
    "end": "2345820"
  },
  {
    "text": "And then it shows status in service. Then we'll use boto3\nactually to invoke the model.",
    "start": "2345820",
    "end": "2354310"
  },
  {
    "text": "So then we'll use boto3\nto invoke the endpoint.",
    "start": "2355480",
    "end": "2359893"
  },
  {
    "text": "And we're gonna send it this package, which is, Amazon.com is the best.",
    "start": "2360940",
    "end": "2367090"
  },
  {
    "text": "And then we have the parameters here, and then we get the results.",
    "start": "2367090",
    "end": "2371143"
  },
  {
    "text": "All right, and let's see if we can do\nthis for a different one. So maybe I'll try something like,",
    "start": "2372670",
    "end": "2379813"
  },
  {
    "text": "Today is the hottest day.",
    "start": "2380740",
    "end": "2384793"
  },
  {
    "text": "And let's see what we get.",
    "start": "2386103",
    "end": "2386936"
  },
  {
    "text": "So it takes a bit because\nit's quite a large model.",
    "start": "2394480",
    "end": "2399480"
  },
  {
    "text": "So you're invoking the\ntensor parallelism here. And then here we go.",
    "start": "2400450",
    "end": "2405553"
  },
  {
    "text": "It's quite an aggressive temperature, up to 1000 degrees Fahrenheit. (laughing)",
    "start": "2408748",
    "end": "2414103"
  },
  {
    "text": "Goodness. Wow, that's quite aggressive.",
    "start": "2416620",
    "end": "2422290"
  },
  {
    "text": "Obviously here's an example\nof hallucination for you. Clearly it's not 1000 degrees Fahrenheit.",
    "start": "2422290",
    "end": "2430000"
  },
  {
    "text": "Right now, it's about\n84 degrees Fahrenheit. 1000 degrees would be impossible for life.",
    "start": "2430000",
    "end": "2438880"
  },
  {
    "text": "But so in any case, then here's a slightly different example that uses different parameters.",
    "start": "2438880",
    "end": "2448620"
  },
  {
    "text": "All right, and so with that, we learned how to use\nSageMaker to host large models",
    "start": "2449830",
    "end": "2456600"
  },
  {
    "text": "using the deep learning\ncontainers, specifically for DJL. And then I also wanted to give\na shout out to Phil Schmid,",
    "start": "2456760",
    "end": "2465942"
  },
  {
    "text": "awesome set of SageMaker example notebooks from Hugging Face. And this is another way that\nyou can deploy large models",
    "start": "2466180",
    "end": "2474520"
  },
  {
    "text": "on SageMaker. So thank you. I hope you enjoyed this video.",
    "start": "2474520",
    "end": "2479890"
  },
  {
    "text": "And you know, there's\ngonna be more coming. So hang out with me",
    "start": "2479890",
    "end": "2485110"
  },
  {
    "text": "and we'll get you some\nmore content soon, thanks.",
    "start": "2485110",
    "end": "2488113"
  }
]