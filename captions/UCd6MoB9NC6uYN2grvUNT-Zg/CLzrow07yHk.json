[
  {
    "text": "good afternoon everybody my name is George Smith I'm a Solutions Architect",
    "start": "920",
    "end": "6390"
  },
  {
    "text": "at AWS working with financial services companies and this is fsv 3:03",
    "start": "6390",
    "end": "12389"
  },
  {
    "text": "where i'm gonna spend some time discussing with you today options for queryable archives and data leaks on AWS",
    "start": "12389",
    "end": "19880"
  },
  {
    "text": "so during this session we're going to talk about a pattern for better cheaper and faster queryable archives or I've",
    "start": "20600",
    "end": "27930"
  },
  {
    "text": "we've termed them queryable archive I've also heard them called active archives we're gonna implement that on AWS and",
    "start": "27930",
    "end": "35370"
  },
  {
    "text": "we're going to talk about why bringing dark data into the light adds value for your enterprise here's our agenda I'm",
    "start": "35370",
    "end": "43950"
  },
  {
    "text": "gonna start with the whys why are we in this position why are we archiving data we're gonna build one",
    "start": "43950",
    "end": "49590"
  },
  {
    "text": "we're gonna demo it talk about cost-benefit and then dark data and next steps so why do we archive this is sort",
    "start": "49590",
    "end": "58559"
  },
  {
    "text": "of the story of my life prior to joining AWS I managed database teams and databases for several large",
    "start": "58559",
    "end": "64799"
  },
  {
    "text": "financial institutions and this is how we got to archiving we started with the",
    "start": "64799",
    "end": "71820"
  },
  {
    "text": "transactional system so we would build the system in this case for me it was a Nasdaq desk or a listed desk or",
    "start": "71820",
    "end": "78630"
  },
  {
    "text": "derivatives trading system we would start out building these systems and day one all we're worried about is getting",
    "start": "78630",
    "end": "84900"
  },
  {
    "text": "the transactions through the system as quickly as we can and making sure they perform well and the business is able to execute trades day 90 comes along or day",
    "start": "84900",
    "end": "93630"
  },
  {
    "text": "180 comes along and now we've got historical database sitting in our day",
    "start": "93630",
    "end": "99479"
  },
  {
    "text": "to day trading database and we actually called it that we called it Nasdaq trading or listed training that was the",
    "start": "99479",
    "end": "105119"
  },
  {
    "text": "name of the database at this point we have a performance problem for whatever",
    "start": "105119",
    "end": "110189"
  },
  {
    "text": "reason we didn't know at the time what was going on things slowed down so we sat down with the developers we went",
    "start": "110189",
    "end": "116340"
  },
  {
    "text": "through an entire performance tuning exercise and came to the dramatic conclusion that we just have too much",
    "start": "116340",
    "end": "122040"
  },
  {
    "text": "data on our database so we started building archive databases in this case",
    "start": "122040",
    "end": "127049"
  },
  {
    "text": "we called them history databases so we had a Nasdaq trading database",
    "start": "127049",
    "end": "132480"
  },
  {
    "text": "we had a Nasdaq history database and we would move data on a periodic basis into our historical database so that our",
    "start": "132480",
    "end": "139290"
  },
  {
    "text": "trading database can stay small and fast and continue to perform well this became",
    "start": "139290",
    "end": "144840"
  },
  {
    "text": "a cycle we would do it once a month once a quarter however however our",
    "start": "144840",
    "end": "151890"
  },
  {
    "text": "transaction volume progressed we would have to archive based on that cycle",
    "start": "151890",
    "end": "156950"
  },
  {
    "text": "sound familiar to anybody is anybody in this audience managing trading system was there any transactional based system",
    "start": "156950",
    "end": "162989"
  },
  {
    "text": "where they have to pull data out of their transactional system in order to keep performance stable and fast is that",
    "start": "162989",
    "end": "170730"
  },
  {
    "text": "a Ferrari shirt no what is that lannister okay so yeah so this this is",
    "start": "170730",
    "end": "180599"
  },
  {
    "text": "just what we did this is exactly how we went around creating our initial historical databases the next thing that",
    "start": "180599",
    "end": "188310"
  },
  {
    "text": "happened is regulators decided that we needed to keep data for longer so we",
    "start": "188310",
    "end": "193349"
  },
  {
    "text": "started with the same cycle the only difference is that our archive databases we had to keep for five years or for",
    "start": "193349",
    "end": "199530"
  },
  {
    "text": "seven years or for some longer period of time and those databases just grew and grew and grew and what we had once this",
    "start": "199530",
    "end": "206639"
  },
  {
    "text": "happened is a very lopsided environment we had way more storage attached to the",
    "start": "206639",
    "end": "212130"
  },
  {
    "text": "compute than we originally intended we had designed for holding a couple of",
    "start": "212130",
    "end": "218160"
  },
  {
    "text": "terabytes now maybe we have a hundred terabytes and then when we need to access these databases which happened a",
    "start": "218160",
    "end": "224849"
  },
  {
    "text": "few times a year a regulator or a legal event would occur or something would happen to one of our clients where they",
    "start": "224849",
    "end": "231359"
  },
  {
    "text": "would be doing the same thing and they need to access this database client would come to us and say I need this",
    "start": "231359",
    "end": "237209"
  },
  {
    "text": "data we would go out to our archived data store get the data if the if the",
    "start": "237209",
    "end": "242489"
  },
  {
    "text": "data set was small enough it would work fine if the database if the data set wasn't foreign small enough meaning I",
    "start": "242489",
    "end": "248129"
  },
  {
    "text": "need five years worth of trades in IBM or Oracle or Microsoft or something",
    "start": "248129",
    "end": "254340"
  },
  {
    "text": "along those lines performance would degrade we already have a lopsided environment we don't have enough compute",
    "start": "254340",
    "end": "260729"
  },
  {
    "text": "now we have a crisis we have an end user who needs access to data that",
    "start": "260729",
    "end": "265990"
  },
  {
    "text": "really get it in a timely fashion so what we actually did and we did this is",
    "start": "265990",
    "end": "271270"
  },
  {
    "text": "we searched the environment or our data centers for spare compute we would build",
    "start": "271270",
    "end": "277210"
  },
  {
    "text": "a brand new database on top of that compute would restore our five years or",
    "start": "277210",
    "end": "282340"
  },
  {
    "text": "however many years of trades data that we needed and we'd rather report the end",
    "start": "282340",
    "end": "287680"
  },
  {
    "text": "user was now able to get their data and give it to whomever they needed it and then we would go back and we would tear",
    "start": "287680",
    "end": "293770"
  },
  {
    "text": "everything down this would take anywhere",
    "start": "293770",
    "end": "299229"
  },
  {
    "text": "between 5 and 10 staff days in order to build this up run the reports tear everything down again not really the",
    "start": "299229",
    "end": "306490"
  },
  {
    "text": "best use of resources and at the end of this we had an unhappy user they were",
    "start": "306490",
    "end": "313449"
  },
  {
    "text": "not all that satisfied yet the eventually they got their data but it wasn't the best experience for them so",
    "start": "313449",
    "end": "321009"
  },
  {
    "text": "what if and we ask yourself a lot at Amazon what if because we're always trying to figure out what the next thing",
    "start": "321009",
    "end": "326919"
  },
  {
    "text": "is what the new normal is gonna look at like so what if I had an archived data",
    "start": "326919",
    "end": "332289"
  },
  {
    "text": "store that was scalable and would perform regardless of the amount of data that I had in there or the amount data",
    "start": "332289",
    "end": "338530"
  },
  {
    "text": "it was accessing well initially I could store as much data as I want and not",
    "start": "338530",
    "end": "343570"
  },
  {
    "text": "worry about scaling storage and compute I can just simply store the data and I",
    "start": "343570",
    "end": "349509"
  },
  {
    "text": "wouldn't be in the fire drill business of building out new environments when my archived data store didn't perform so no",
    "start": "349509",
    "end": "355930"
  },
  {
    "text": "more fire drills so let's go build it now we're gonna build this environment",
    "start": "355930",
    "end": "362889"
  },
  {
    "text": "in my account in AWS first I'm going to show you the reference architecture and then I'm gonna show you a demo and we",
    "start": "362889",
    "end": "369159"
  },
  {
    "text": "want to build this without breaking the bank we certainly don't want to go overboard as we build this so at 10,000",
    "start": "369159",
    "end": "375580"
  },
  {
    "text": "feet we start out with all of our historical data sets that are sitting in our data centers today you'd think of",
    "start": "375580",
    "end": "381550"
  },
  {
    "text": "these as Nasdaq history it listed history derivatives history these are all sitting in the data centers when I",
    "start": "381550",
    "end": "387969"
  },
  {
    "text": "used to work at the banks and taking up data center space compute storage",
    "start": "387969",
    "end": "394560"
  },
  {
    "text": "licensing and resources I'm gonna extract that data I'm gonna transform that data so that it",
    "start": "394560",
    "end": "402280"
  },
  {
    "text": "used it works with my new storage platform and then I'm gonna load that data into the new storage platform once",
    "start": "402280",
    "end": "410260"
  },
  {
    "text": "it's loaded I'll have the ability to explore that data using a variety tools that exist today with an AWS our tech stack for",
    "start": "410260",
    "end": "418510"
  },
  {
    "text": "this solution we're gonna start with Amazon s3 our simple storage service",
    "start": "418510",
    "end": "424560"
  },
  {
    "text": "this is a secure meaning you can encrypt data at rest you can encrypt data in",
    "start": "424560",
    "end": "430660"
  },
  {
    "text": "motion you can control who accesses data and control who doesn't have access to",
    "start": "430660",
    "end": "435970"
  },
  {
    "text": "data it's a highly scalable and it is a durable cloud storage platform s3",
    "start": "435970",
    "end": "443500"
  },
  {
    "text": "regular three stores trillions of objects today and regularly scarce scales to millions of requests per",
    "start": "443500",
    "end": "448960"
  },
  {
    "text": "second and carries eleven nines of durability we're going to use AWS",
    "start": "448960",
    "end": "454840"
  },
  {
    "text": "database migration service which is a service that we have that allows you to migrate data from point A to point B and",
    "start": "454840",
    "end": "461010"
  },
  {
    "text": "understands several different data sources and targets we're gonna use a",
    "start": "461010",
    "end": "466360"
  },
  {
    "text": "dia AWS glue which is our managed ETL service that's a product that allows us",
    "start": "466360",
    "end": "472630"
  },
  {
    "text": "to transform the data from the departmental data model the NASDAQ history of illicit history into an",
    "start": "472630",
    "end": "479410"
  },
  {
    "text": "enterprise data model so you can look across the enterprise and just say show me all my equity trades or just show me",
    "start": "479410",
    "end": "485680"
  },
  {
    "text": "all my trades we're gonna use red shift and red shift spectrum red shift is a",
    "start": "485680",
    "end": "491200"
  },
  {
    "text": "petabyte scale data warehouse where you can store data locally in the red shift cluster and be able to access it and",
    "start": "491200",
    "end": "497110"
  },
  {
    "text": "then red shift spectrum extends the abilities of red shift out to s3 and",
    "start": "497110",
    "end": "502570"
  },
  {
    "text": "sizes out to exabytes so you can access a lot of the historical data that's sitting in s3 regardless of size EMR",
    "start": "502570",
    "end": "511270"
  },
  {
    "text": "which is our managed to do framework also capable of reading data from s3 and Amazon Athena which is a service query",
    "start": "511270",
    "end": "518530"
  },
  {
    "text": "framework that allows you to run sequel statements against data that is sitting in s3 so we're going to come a little",
    "start": "518530",
    "end": "526870"
  },
  {
    "text": "bit closer to the ground here and kind of see exactly what we're building so at a thousand feet we have our source",
    "start": "526870",
    "end": "532690"
  },
  {
    "text": "database which is our historical NASDAQ history listed history database we're going to extract that data and transform",
    "start": "532690",
    "end": "539710"
  },
  {
    "text": "that data using AWS database migration service and then we're going to store that data into an archive bucket in s3",
    "start": "539710",
    "end": "546910"
  },
  {
    "text": "which is our queryable archive in a compressed CSV format once we've done",
    "start": "546910",
    "end": "552820"
  },
  {
    "text": "that we have solved the query archive problem we have the data now sitting in",
    "start": "552820",
    "end": "558730"
  },
  {
    "text": "a highly scalable and available storage layer that we can run compute against but we're going to go a little bit",
    "start": "558730",
    "end": "564820"
  },
  {
    "text": "further we're going to then extract that departmental model again we're going to",
    "start": "564820",
    "end": "570520"
  },
  {
    "text": "use AWS glue to do an ETL extract transform load process against that data",
    "start": "570520",
    "end": "576940"
  },
  {
    "text": "and we're going to load it into s3 in our enterprise format and for the",
    "start": "576940",
    "end": "582700"
  },
  {
    "text": "enterprise format we've chose for the data Lake you can see the little lake inside the bucket there we've chosen",
    "start": "582700",
    "end": "588610"
  },
  {
    "text": "park' objects because a lot of our customers we notice as they're building out data leaks are choosing formats like",
    "start": "588610",
    "end": "594610"
  },
  {
    "text": "Clarke a once the data is available they're in both of those buckets you can",
    "start": "594610",
    "end": "601150"
  },
  {
    "text": "access it and explore it using a variety of services that exist in AWS today so I",
    "start": "601150",
    "end": "609430"
  },
  {
    "text": "have a few demos here where I'm going to take this reference architecture we're gonna go piece by piece and show exactly",
    "start": "609430",
    "end": "614920"
  },
  {
    "text": "how this is gonna work so first we have our archive then we have our data Lake",
    "start": "614920",
    "end": "620080"
  },
  {
    "text": "and we're going to talk about step one which is extracting the data from a historical data stores and putting them",
    "start": "620080",
    "end": "627160"
  },
  {
    "text": "into s3 this is DMS this is the DMS console I'm doing this with the console",
    "start": "627160",
    "end": "633520"
  },
  {
    "text": "in reality once this is done you'd be doing with batch jobs but they are tasks and endpoints in the console a task is a",
    "start": "633520",
    "end": "641470"
  },
  {
    "text": "batch job that moves data from point A to point B you can see I have a source end point which is Oracle and a target",
    "start": "641470",
    "end": "648700"
  },
  {
    "text": "end point which is s3 and that is also displayed in the overview section I'm",
    "start": "648700",
    "end": "655060"
  },
  {
    "text": "gonna do a restart the restart is a full load it copies the entire table from our",
    "start": "655060",
    "end": "662980"
  },
  {
    "text": "source database into s3 and it stores it into a compressed CSV format the job has been running about",
    "start": "662980",
    "end": "670670"
  },
  {
    "text": "five minutes and if we look at the statistics we can see it's moved about 3.4 million rows and it's done that",
    "start": "670670",
    "end": "677899"
  },
  {
    "text": "right here on the right we're gonna video a video edit out the boring stuff",
    "start": "677899",
    "end": "682959"
  },
  {
    "text": "come back four hours and 26 minutes later the job is done and we copied a",
    "start": "682959",
    "end": "689660"
  },
  {
    "text": "hundred and sixty-seven million rows across so this data set is a data set that I manufactured to represent orders",
    "start": "689660",
    "end": "697579"
  },
  {
    "text": "on a trading desk orders and executions at this point like I said we have our",
    "start": "697579",
    "end": "704269"
  },
  {
    "text": "data in our able archive so now we're gonna want to transform it into the",
    "start": "704269",
    "end": "709279"
  },
  {
    "text": "park' format of the file into the enterprise data lake format which is a which is an enterprise data model not a",
    "start": "709279",
    "end": "716350"
  },
  {
    "text": "departmental model so there would be in normal cases when you're doing this you're going to be you're gonna be",
    "start": "716350",
    "end": "722959"
  },
  {
    "text": "validating the data you're gonna be enriching the data you're gonna be pulling in data from other sources so",
    "start": "722959",
    "end": "728089"
  },
  {
    "text": "that you can reference the data across the entire enterprise versus just the department and you're just gonna put",
    "start": "728089",
    "end": "734300"
  },
  {
    "text": "together a data model that traditionally you might think about as data warehouses or data Mart's in this case we're",
    "start": "734300",
    "end": "741800"
  },
  {
    "text": "putting it into a data Lake for further processing we have a glue console again",
    "start": "741800",
    "end": "746959"
  },
  {
    "text": "this is going to be a batch job once it's really done we have databases here",
    "start": "746959",
    "end": "752870"
  },
  {
    "text": "I called the database for the data Lake glue catalog not a very original name",
    "start": "752870",
    "end": "758230"
  },
  {
    "text": "within the database we have tables there's only one table in here now and that's the table that we just created",
    "start": "758230",
    "end": "763970"
  },
  {
    "text": "with our DMS job in the last demo this is the CSV version of the orders table",
    "start": "763970",
    "end": "770320"
  },
  {
    "text": "so if we look at our job we can see a script gets generated by Glu Glu",
    "start": "770320",
    "end": "775880"
  },
  {
    "text": "generates Python and then you're able to go in there and edit that script if you need to so we start the job it's running",
    "start": "775880",
    "end": "783800"
  },
  {
    "text": "and then we're going to edit out the boring stuff we can see in this case I think it's running for 28 seconds and",
    "start": "783800",
    "end": "791670"
  },
  {
    "text": "edit away it's done it was four hours and I'm sorry two hours and 55 minutes",
    "start": "791670",
    "end": "797410"
  },
  {
    "text": "we loaded all 167 million rows into our data Lake so now we've got data in two",
    "start": "797410",
    "end": "805600"
  },
  {
    "text": "locations the other thing I'm going to show you and this is gonna be a shorter demo is just loading directly into",
    "start": "805600",
    "end": "811960"
  },
  {
    "text": "redshift because redshift is our petabyte scale data warehouse environment and you can go with glue",
    "start": "811960",
    "end": "817690"
  },
  {
    "text": "into a variety of sources redshift being one of them in this case we go right back to the console again this is a",
    "start": "817690",
    "end": "824650"
  },
  {
    "text": "batch job when you actually build it we can see that we added a connection here this connection is the core redshift",
    "start": "824650",
    "end": "831760"
  },
  {
    "text": "query archive connection so now glue knows about redshift and it knows that we want to load data there so we run our",
    "start": "831760",
    "end": "839200"
  },
  {
    "text": "job you have a CSV to redshift job and we do this because if you're running",
    "start": "839200",
    "end": "845080"
  },
  {
    "text": "this you can see this succeeded and it took four hours and 31 minutes but if you're actually running this in your",
    "start": "845080",
    "end": "851350"
  },
  {
    "text": "accounts you're gonna run both of the jobs I just showed back to back in parallel so you don't need to run them",
    "start": "851350",
    "end": "856980"
  },
  {
    "text": "you don't need to run them back to back you're running the parallel and that gets your data into your data Lake much",
    "start": "856980",
    "end": "864370"
  },
  {
    "text": "quicker because you're not waiting on anything three copies of the data we can",
    "start": "864370",
    "end": "870550"
  },
  {
    "text": "access from it access it from any of those any of those copies and I'm",
    "start": "870550",
    "end": "875710"
  },
  {
    "text": "actually going to show you on the query demo accessing it through a few of these a few of these services in this demo the",
    "start": "875710",
    "end": "886270"
  },
  {
    "text": "one I'm going to do is start up an EMR cluster I'm gonna access the data in",
    "start": "886270",
    "end": "891400"
  },
  {
    "text": "both environments the archive and the data Lake and then I'm gonna shut down the cluster so let's see how this goes",
    "start": "891400",
    "end": "897130"
  },
  {
    "text": "and this is going to be mostly command line driven so the first thing I do is I execute an API command it starts up the",
    "start": "897130",
    "end": "903490"
  },
  {
    "text": "cluster we can see the cluster starting up and we can see the cluster ID has been assigned and that cluster ID ends",
    "start": "903490",
    "end": "910150"
  },
  {
    "text": "in GBP I'm also passing in some configuration information so for this",
    "start": "910150",
    "end": "915370"
  },
  {
    "text": "cluster I'm telling it to use the glue catalog as its hive meta store and as its SPARC I've made a store and I'm",
    "start": "915370",
    "end": "922450"
  },
  {
    "text": "going to tell it not to give me the information which is just warning or error messages whenever I run a query going into our",
    "start": "922450",
    "end": "931820"
  },
  {
    "text": "EMR console we see there's no job there's no clusters running when i refresh that we can see the cluster that",
    "start": "931820",
    "end": "937730"
  },
  {
    "text": "we just created what that command is now starting it's been starting for about a",
    "start": "937730",
    "end": "942889"
  },
  {
    "text": "minute this is a three node cluster and",
    "start": "942889",
    "end": "947930"
  },
  {
    "text": "we can see the ID matches and it ends in G VP we're gonna refresh this and edit",
    "start": "947930",
    "end": "954920"
  },
  {
    "text": "out the boring stuff we're at twelve minutes later the clusters up it's",
    "start": "954920",
    "end": "960949"
  },
  {
    "text": "waiting it's ready to go we can see it's the same it's got a DNS endpoint and it's the same three node",
    "start": "960949",
    "end": "967130"
  },
  {
    "text": "cluster and the ID hasn't changed it still ends in GBP so going back we're",
    "start": "967130",
    "end": "976310"
  },
  {
    "text": "gonna sign into the master node just SSH in we have to answer the SSH challenge",
    "start": "976310",
    "end": "983660"
  },
  {
    "text": "because this is the first time we've ever signed into this node because it's brand-new start up hive and we're gonna",
    "start": "983660",
    "end": "990860"
  },
  {
    "text": "tell it to use the glue catalog showing our tables we can see we have our two",
    "start": "990860",
    "end": "996079"
  },
  {
    "text": "tables I'm going to select count star from the CSV orders table which is our historical our archive data store and we",
    "start": "996079",
    "end": "1003970"
  },
  {
    "text": "can still see we have one hundred and sixty-seven million rows that are still in that table I didn't tune any of these",
    "start": "1003970",
    "end": "1010660"
  },
  {
    "text": "clusters so these performance is just the performance that you're seeing here is just out of the box now I'm gonna",
    "start": "1010660",
    "end": "1017050"
  },
  {
    "text": "read it from the park' table which is our data Lake and we have the same hundred and sixty-seven million rows now",
    "start": "1017050",
    "end": "1024339"
  },
  {
    "text": "just to verify it's not just rows that came over I put together a query that does orders and executions by hour so",
    "start": "1024339",
    "end": "1031780"
  },
  {
    "text": "this is going to show me how many orders and how many executions were generated",
    "start": "1031780",
    "end": "1037058"
  },
  {
    "text": "and stored in our orders table by our of the trading day and in this case you can",
    "start": "1037059",
    "end": "1042819"
  },
  {
    "text": "see we've got our seven hours because it's a six and a half hour trading day I've got a list of orders and executions",
    "start": "1042819",
    "end": "1049720"
  },
  {
    "text": "as I said I generated this data this is not real trading data so for those of you who are in the audience who know",
    "start": "1049720",
    "end": "1055990"
  },
  {
    "text": "what trading data looks like this isn't it I go back to the console",
    "start": "1055990",
    "end": "1062530"
  },
  {
    "text": "and I terminate the cluster I do this for because I don't need it anymore the",
    "start": "1062530",
    "end": "1068470"
  },
  {
    "text": "data is sitting in s3 it's alive and well in s3 I only need the compute when",
    "start": "1068470",
    "end": "1074350"
  },
  {
    "text": "I want to process it one of the big advantages of moving my data into my s3",
    "start": "1074350",
    "end": "1079390"
  },
  {
    "text": "archive in my s3 data leak is that s3 keeps that data alive when I used to do",
    "start": "1079390",
    "end": "1085660"
  },
  {
    "text": "this on premise in the banks I used to run servers and database software in order to be able access that data I",
    "start": "1085660",
    "end": "1091540"
  },
  {
    "text": "don't have to do that here the data sitting in s3 it's alive I only bring the compute to the data when I need to",
    "start": "1091540",
    "end": "1098020"
  },
  {
    "text": "rent jobs otherwise I shut down the compute save that money save those resources and the data stays there nice",
    "start": "1098020",
    "end": "1105370"
  },
  {
    "text": "and healthy waiting until the next time I want to run a query this is red ship",
    "start": "1105370",
    "end": "1111610"
  },
  {
    "text": "actually I'm sorry this is aqua data studio what I've got open here are some windows that are going to read the",
    "start": "1111610",
    "end": "1117700"
  },
  {
    "text": "redshifts query archive so the first thing I'm going to show you is this is essentially how you access data that",
    "start": "1117700",
    "end": "1123760"
  },
  {
    "text": "sits in s3 the schema for S 3 is spectrum and if you are accessing a",
    "start": "1123760",
    "end": "1129490"
  },
  {
    "text": "table in the spectrum schema you're really accessing data in s3 so you can",
    "start": "1129490",
    "end": "1134860"
  },
  {
    "text": "see I've got my redshift cluster here I'm creating a table in the spectrum schema for orders park' which is our",
    "start": "1134860",
    "end": "1141520"
  },
  {
    "text": "data lake and I have to give it some information about the structure of this data and the location to this data in",
    "start": "1141520",
    "end": "1147640"
  },
  {
    "text": "order for redshift to actually be able to go out there and access that data you",
    "start": "1147640",
    "end": "1153370"
  },
  {
    "text": "can see on the Left I've highlighted a public schema that is the local data so",
    "start": "1153370",
    "end": "1160630"
  },
  {
    "text": "we're going to compare the two so here I'm going to select count store from the local data set because it's public 167",
    "start": "1160630",
    "end": "1166780"
  },
  {
    "text": "million rows less than a second to execute that query now I'm going to do the exact same query as orders and",
    "start": "1166780",
    "end": "1173650"
  },
  {
    "text": "executions by hour against the local data set this query ran in 4 seconds and",
    "start": "1173650",
    "end": "1179110"
  },
  {
    "text": "gave me the same results that we saw before so you see I'm using two services so far and I'm able to get the exact",
    "start": "1179110",
    "end": "1185620"
  },
  {
    "text": "same results from this same thing from the data in s3 I'm going to get the same",
    "start": "1185620",
    "end": "1190720"
  },
  {
    "text": "results it's going to run longer because it's going through spectrum not local data 167 million rows",
    "start": "1190720",
    "end": "1196480"
  },
  {
    "text": "three seconds and then orders in executions by our pulling that data out",
    "start": "1196480",
    "end": "1204429"
  },
  {
    "text": "of s3 using spectrum they cut the data comes back in about five seconds with",
    "start": "1204429",
    "end": "1209470"
  },
  {
    "text": "the exact same results we showed the EMR",
    "start": "1209470",
    "end": "1214780"
  },
  {
    "text": "cluster in the red shift cluster and just like the batch jobs you could run these both at the same time you can have",
    "start": "1214780",
    "end": "1220780"
  },
  {
    "text": "multiple compute sources accessing your data in your data Lake or your archive",
    "start": "1220780",
    "end": "1225850"
  },
  {
    "text": "at the exact same time this is bringing compute to the data and I can bring multiple compute stacks to the same data",
    "start": "1225850",
    "end": "1233380"
  },
  {
    "text": "and access that data at the exact same time and s3 will serve that data to each source the last one I'm going to show",
    "start": "1233380",
    "end": "1240429"
  },
  {
    "text": "you is Athena Athena is unique in that it's a service infrastructure I'm not",
    "start": "1240429",
    "end": "1245650"
  },
  {
    "text": "starting anything I'm not running anything I'm just going to the service so I'm going against the glue catalog",
    "start": "1245650",
    "end": "1250990"
  },
  {
    "text": "we've got that same two tables we look at the properties of the parkade table it's got the same structure it's the",
    "start": "1250990",
    "end": "1257230"
  },
  {
    "text": "same location of the data set we've been accessing all along and I'm gonna run the same two queries since Athena is a",
    "start": "1257230",
    "end": "1265120"
  },
  {
    "text": "managed service somebody else is managing the clusters performance and availability so here I got my 167 in one",
    "start": "1265120",
    "end": "1272920"
  },
  {
    "text": "point for three seconds scanning zero kilobytes now I'm gonna run the orders",
    "start": "1272920",
    "end": "1279790"
  },
  {
    "text": "and executions by our this query I think runs in one point eight nine seconds or something along those lines and scans",
    "start": "1279790",
    "end": "1286870"
  },
  {
    "text": "eighty five point three five megabytes but gets the same results the megabytes",
    "start": "1286870",
    "end": "1292690"
  },
  {
    "text": "are important to Athena they're also important to spectrum because for both",
    "start": "1292690",
    "end": "1297880"
  },
  {
    "text": "of those services you pay each byte you pay by query and you pay per terabyte",
    "start": "1297880",
    "end": "1303970"
  },
  {
    "text": "scanned so it's five dollars per terabyte to run to scan data in either",
    "start": "1303970",
    "end": "1311020"
  },
  {
    "text": "of those services so this query which ran twitch scanned eighty five point three five megabytes I calculated this",
    "start": "1311020",
    "end": "1317830"
  },
  {
    "text": "coin cost point zero zero zero for three cents so this is a very good platform as",
    "start": "1317830",
    "end": "1324250"
  },
  {
    "text": "well as spectrum for doing maybe some ad-hoc queries or if you're looking to go back in time and look at data that's that's a",
    "start": "1324250",
    "end": "1330740"
  },
  {
    "text": "bit older than what you would consider current historical data we'll talk about",
    "start": "1330740",
    "end": "1336980"
  },
  {
    "text": "that in a minute so when I was again",
    "start": "1336980",
    "end": "1344930"
  },
  {
    "text": "working in the banks I knew I had an archive data problem I knew it was a pain in the neck to go and get my",
    "start": "1344930",
    "end": "1351080"
  },
  {
    "text": "archive data and it wasn't really usable and I knew that there was a better way to do this the problem is that I could",
    "start": "1351080",
    "end": "1357290"
  },
  {
    "text": "not cost justify to my management going out and building this environment and",
    "start": "1357290",
    "end": "1363080"
  },
  {
    "text": "right now what I'm gonna do in the next couple of sections is try to cost justify to you why this solution is",
    "start": "1363080",
    "end": "1368870"
  },
  {
    "text": "going to end up costing you less money than your current solution which is what I described from my history and give you",
    "start": "1368870",
    "end": "1375890"
  },
  {
    "text": "better access to your data and provide better ability to analyze that data and",
    "start": "1375890",
    "end": "1383450"
  },
  {
    "text": "produce value for your organization first is you have a single golden source of truth instead of going out in",
    "start": "1383450",
    "end": "1392060"
  },
  {
    "text": "replicas and hero violins I'm sorry we have our archive in our data Lake here got going back to oh I did this wrong so",
    "start": "1392060",
    "end": "1399530"
  },
  {
    "text": "going back to what we're talking about we're talking about our archive in our data leaks we have a single golden",
    "start": "1399530",
    "end": "1404600"
  },
  {
    "text": "source of truth one copy of the data as far as I'm concerned sitting in s/3 s/3 really has multiple copies highly",
    "start": "1404600",
    "end": "1410930"
  },
  {
    "text": "available across the region but I'm paying for one copy when I did this on premise I had a primary database entire",
    "start": "1410930",
    "end": "1419450"
  },
  {
    "text": "stack I had a secondary database in order to be able have high availability and disaster recovery so in case anything",
    "start": "1419450",
    "end": "1426080"
  },
  {
    "text": "happened to one data center or one environment I would build it and the other environment and I had backup copies of the data in this case we're",
    "start": "1426080",
    "end": "1432980"
  },
  {
    "text": "keeping the data in s3 single copy of the data the other thing I had on",
    "start": "1432980",
    "end": "1438110"
  },
  {
    "text": "premise is I had several data warehouses or several data Mart's that would do",
    "start": "1438110",
    "end": "1443870"
  },
  {
    "text": "end-of-day processing so when my trading day was done I would send the final batch cut to call it five systems it was",
    "start": "1443870",
    "end": "1451340"
  },
  {
    "text": "really more than that they would all load that data on independent systems and start processing",
    "start": "1451340",
    "end": "1457020"
  },
  {
    "text": "in this model where we bring the compute to the data and not send the data to the compute like we did like I did before I",
    "start": "1457020",
    "end": "1463530"
  },
  {
    "text": "load this data into my data like one system and I start up five compute",
    "start": "1463530",
    "end": "1468630"
  },
  {
    "text": "stacks and I process the data that's what single golden source of truth means",
    "start": "1468630",
    "end": "1474330"
  },
  {
    "text": "in this context the X the next is right sizing compute for workload so for this",
    "start": "1474330",
    "end": "1479610"
  },
  {
    "text": "we're really talking about the compute that we bring to the data so I just mentioned I start up five compute",
    "start": "1479610",
    "end": "1485160"
  },
  {
    "text": "clusters to process my data for the overnight batch but what happens if a",
    "start": "1485160",
    "end": "1490260"
  },
  {
    "text": "Briggs had occurred I mean I worked in trading environments and trading houses whenever a brexit occurred or an",
    "start": "1490260",
    "end": "1497070"
  },
  {
    "text": "election surprise occurred or tweat went out or say it was cyber monday volumes",
    "start": "1497070",
    "end": "1503970"
  },
  {
    "text": "would go through the roof and we would have to run around the environment checking all over infrastructure making",
    "start": "1503970",
    "end": "1509160"
  },
  {
    "text": "sure we can handle that night that trading volume and that's nights over night bats within the windows that we",
    "start": "1509160",
    "end": "1514770"
  },
  {
    "text": "had I mean I don't know how many people I think there's one few people over here running trading desk I'm sure you're",
    "start": "1514770",
    "end": "1520320"
  },
  {
    "text": "doing the same thing if anything happens like that in this model say I'm running",
    "start": "1520320",
    "end": "1525809"
  },
  {
    "text": "a ten node redshift cluster and that's my normal volume redshift cluster or a ten no DMR cluster for normal volumes",
    "start": "1525809",
    "end": "1532880"
  },
  {
    "text": "Briggs it happens or an election surprise happens volumes go through the",
    "start": "1532880",
    "end": "1539309"
  },
  {
    "text": "roof I can start up a thirty node cluster today I don't need to start up another ten node cluster I just start up",
    "start": "1539309",
    "end": "1545130"
  },
  {
    "text": "a thirty node cluster as long as my application is designed for it it'll scale after 30 nodes and process the",
    "start": "1545130",
    "end": "1550800"
  },
  {
    "text": "overnight workload two weeks from now three weeks from now and volume has calmed down I can go back to a ten node",
    "start": "1550800",
    "end": "1556410"
  },
  {
    "text": "cluster I don't need to provision my infrastructure for peak anymore I can",
    "start": "1556410",
    "end": "1562770"
  },
  {
    "text": "provision my infrastructure for what I need when I need it and that's a big advantage I think in this model and the",
    "start": "1562770",
    "end": "1571920"
  },
  {
    "text": "next is availability I mentioned earlier that I used to run two stacks primary",
    "start": "1571920",
    "end": "1578610"
  },
  {
    "text": "stack in a dr stack and this is really talking about the compute at this point",
    "start": "1578610",
    "end": "1586370"
  },
  {
    "text": "in data warehousing environments it was very difficult to run two stacks I can certainly purchase both stacks but if I",
    "start": "1587090",
    "end": "1593880"
  },
  {
    "text": "had a hundred terabytes or 200 terabytes of data it was really hard to keep everything in sync it's a lot of heavy lifting and a lot of moving parts and",
    "start": "1593880",
    "end": "1602809"
  },
  {
    "text": "I'm paying for everything in AWS we build our own environments out in",
    "start": "1602809",
    "end": "1608309"
  },
  {
    "text": "regions and we build out or we build out availability zones within those regions",
    "start": "1608309",
    "end": "1614039"
  },
  {
    "text": "so we're region is a geographic location us East one that's in Northern Virginia an availability zone is a place where we",
    "start": "1614039",
    "end": "1622440"
  },
  {
    "text": "build data centers one or more data centers for availability zone and those availability zones are separated they're",
    "start": "1622440",
    "end": "1628470"
  },
  {
    "text": "on separate power grids they're in separate plug floodplains and they're typically separated by miles and miles",
    "start": "1628470",
    "end": "1633990"
  },
  {
    "text": "of distance so if anything would have happened in one availability zone it",
    "start": "1633990",
    "end": "1641190"
  },
  {
    "text": "wouldn't impact the other availability zones so as you're running your compute in AWS you start up computing your",
    "start": "1641190",
    "end": "1648419"
  },
  {
    "text": "availability zone s3 is a regional service so s3 is available in all availability zones in the region",
    "start": "1648419",
    "end": "1654419"
  },
  {
    "text": "if one availability zone goes away s3 doesn't go away s3 will be available for",
    "start": "1654419",
    "end": "1660000"
  },
  {
    "text": "the entire region if something happens to either the availability zone or your compute in",
    "start": "1660000",
    "end": "1665610"
  },
  {
    "text": "that in that AZ you select another availability zone you would have done",
    "start": "1665610",
    "end": "1672149"
  },
  {
    "text": "this beforehand you start your compute you move your compute into the other availability zone you consider Pross",
    "start": "1672149",
    "end": "1678269"
  },
  {
    "text": "continue processing this simplifies at least compared to what I used to do and we were pretty good at it in the banks",
    "start": "1678269",
    "end": "1685590"
  },
  {
    "text": "I've worked at but this simplifies compared to that exactly how I you would build and manage your disaster recovery",
    "start": "1685590",
    "end": "1691679"
  },
  {
    "text": "your high availability environments",
    "start": "1691679",
    "end": "1695510"
  },
  {
    "text": "now we don't want to break the bank we said that at the beginning so we're gonna talk about some of the efficiencies that you gain that will",
    "start": "1697930",
    "end": "1704320"
  },
  {
    "text": "ultimately net you dollars so the first is storage efficiencies when you're",
    "start": "1704320",
    "end": "1709480"
  },
  {
    "text": "storing data you pay for what storage you need to store that data whether you store it on premise or whether you store",
    "start": "1709480",
    "end": "1716470"
  },
  {
    "text": "it on AWS the difference is that on premise you're paying for way more than just the initial copy of the data and",
    "start": "1716470",
    "end": "1722620"
  },
  {
    "text": "you may not have the same compression options available are in databases for",
    "start": "1722620",
    "end": "1728050"
  },
  {
    "text": "many years Sybase db2 Oracle sequel server Tara data all of them they all",
    "start": "1728050",
    "end": "1733180"
  },
  {
    "text": "have compression options but I typically would get 30% compression 40%",
    "start": "1733180",
    "end": "1738250"
  },
  {
    "text": "compression whenever I was able to compress my data looking at the data",
    "start": "1738250",
    "end": "1743320"
  },
  {
    "text": "that we used in the demo today in Oracle that data was about 76 gigabytes the actual data because I extracted it and",
    "start": "1743320",
    "end": "1750310"
  },
  {
    "text": "put into a CSV file which is a text file in an uncompressed format was 56 gigabytes so there was 20 gigabytes of",
    "start": "1750310",
    "end": "1756790"
  },
  {
    "text": "overhead some of that was inefficiency in the way I put it out there I know if I would have done a reorg I probably",
    "start": "1756790",
    "end": "1762970"
  },
  {
    "text": "would have gained some of that back so you know you could just say instead of it being 76 maybe at 66 or something",
    "start": "1762970",
    "end": "1770260"
  },
  {
    "text": "along those lines when you actually store it and your your-your-your making sure that the data is organized",
    "start": "1770260",
    "end": "1775960"
  },
  {
    "text": "correctly putting it into redshift in a column the format it's only 17 gigabytes",
    "start": "1775960",
    "end": "1781110"
  },
  {
    "text": "but when you get it into s3 in your archive data store in your data leak",
    "start": "1781110",
    "end": "1786360"
  },
  {
    "text": "it's five and a half and four and a half gigabytes so for the price of I'm sorry",
    "start": "1786360",
    "end": "1793660"
  },
  {
    "text": "five and a half and three and a half I apologize for the price of about nine gigabytes I have a historical archive and I have data sitting in a data leak",
    "start": "1793660",
    "end": "1801720"
  },
  {
    "text": "versus my 76 gigabytes for the data sitting in Oracle so I'm gonna save in",
    "start": "1801720",
    "end": "1808060"
  },
  {
    "text": "my storage cost and then your mileage is gonna vary because some of your data sets gonna kind of press better than",
    "start": "1808060",
    "end": "1814210"
  },
  {
    "text": "others but and you might have various compression options in your databases but either way I'm betting that you're",
    "start": "1814210",
    "end": "1820210"
  },
  {
    "text": "going to save on your storage cost additionally there are compute",
    "start": "1820210",
    "end": "1825520"
  },
  {
    "text": "Confession efficiencies again I mentioned that s3 keeps the data alive for you",
    "start": "1825520",
    "end": "1830920"
  },
  {
    "text": "with Oracle a sequel server Sybase or db2 or tera data or any of those other products I've worked with I needed to",
    "start": "1830920",
    "end": "1837820"
  },
  {
    "text": "run servers and storage in order to be able to access my data so in order to keep my historical datasets alive I had",
    "start": "1837820",
    "end": "1844870"
  },
  {
    "text": "to run 24 by 7 servers and storage and database licensing and OS licensing etc",
    "start": "1844870",
    "end": "1850030"
  },
  {
    "text": "plus maintain all those environments power all those environments cool all those environments just so that my data",
    "start": "1850030",
    "end": "1856030"
  },
  {
    "text": "would stay accessible by putting it into s3 I don't have to do that and you can see the price varies dramatically I'll",
    "start": "1856030",
    "end": "1862990"
  },
  {
    "text": "point out that the Oracle price you've got two prices there one of them is to bring your own license price and the",
    "start": "1862990",
    "end": "1869470"
  },
  {
    "text": "other is if you pay for the licensing with your AWS bill but for 21 cents I",
    "start": "1869470",
    "end": "1875200"
  },
  {
    "text": "was able to put this data set 21 cents per month I'm able to put this data into s3 and granted this is a very small",
    "start": "1875200",
    "end": "1882070"
  },
  {
    "text": "amount of data and we're talking about standard s3 here so if we look at a petabyte of data these are today's",
    "start": "1882070",
    "end": "1889450"
  },
  {
    "text": "prices if I put this petabyte of data into s3 to standard s3 standard to your",
    "start": "1889450",
    "end": "1894550"
  },
  {
    "text": "s3 which is what I've been doing this entire presentation it's twenty two thousand five hundred and eighty-three",
    "start": "1894550",
    "end": "1900130"
  },
  {
    "text": "dollars this petabyte might represent five petabytes of data that's sitting on",
    "start": "1900130",
    "end": "1906610"
  },
  {
    "text": "premise today in my historical archives just because of the compression of the beginning but this is historical data I",
    "start": "1906610",
    "end": "1913770"
  },
  {
    "text": "don't need it to be as accessible as current data so I can use life cycle",
    "start": "1913770",
    "end": "1920920"
  },
  {
    "text": "policies and I can move along these tiers of services and I can move the data from standard s3 for maybe the",
    "start": "1920920",
    "end": "1927580"
  },
  {
    "text": "current years worth of data then I can move it into s3 and frequent access for anything greater than a year and then I",
    "start": "1927580",
    "end": "1934660"
  },
  {
    "text": "can move it into Amazon glacier maybe anything greater than three years and i can build a you know with s3 and s3 and",
    "start": "1934660",
    "end": "1941560"
  },
  {
    "text": "frequent access that's pretty straightforward to do with life cycle policies and the data is still",
    "start": "1941560",
    "end": "1946900"
  },
  {
    "text": "accessible so you don't have to do anything special with glacier you're moving it into a cold storage environment so if you want to access",
    "start": "1946900",
    "end": "1953680"
  },
  {
    "text": "that data you have to do a little you have to basically go out and request that data be recovered and put back in s3 for you and you could build a system",
    "start": "1953680",
    "end": "1960820"
  },
  {
    "text": "that will go out pull the data out of glacier put it into s three so you can start up some compute",
    "start": "1960820",
    "end": "1966130"
  },
  {
    "text": "run your reports and then shut down the compute and then delete that data pretty",
    "start": "1966130",
    "end": "1971320"
  },
  {
    "text": "straightforward thing to do and with glacier you can get that data back in five minutes or twelve hours depending",
    "start": "1971320",
    "end": "1977500"
  },
  {
    "text": "on how you want to design your system what you want your SLA used to be and how you want to manage your budget so",
    "start": "1977500",
    "end": "1983770"
  },
  {
    "text": "you have the ability to do something that we've always wanted to do which is",
    "start": "1983770",
    "end": "1989290"
  },
  {
    "text": "to put move colder data on to cheaper storage platforms which is a little bit harder to do unless you've got a system",
    "start": "1989290",
    "end": "1994390"
  },
  {
    "text": "to do this for you on-premise then you can here where it's all built into lifecycle policies and the like so you",
    "start": "1994390",
    "end": "2002160"
  },
  {
    "text": "can also save even more money by selecting the right storage tier for the",
    "start": "2002160",
    "end": "2008370"
  },
  {
    "text": "age of your data so let's talk a second about dark data I'll let you guys read",
    "start": "2008370",
    "end": "2014940"
  },
  {
    "text": "this while I get a drink because I know you're all reading it what this article",
    "start": "2014940",
    "end": "2023130"
  },
  {
    "text": "actually discusses and it spends a lot of time discussing is accessing data",
    "start": "2023130",
    "end": "2029130"
  },
  {
    "text": "that is unstructured but there is a section of this article which talks",
    "start": "2029130",
    "end": "2035940"
  },
  {
    "text": "about structured data that is not accessible and that structured data",
    "start": "2035940",
    "end": "2041850"
  },
  {
    "text": "that's not accessible is the data that's sitting in my historical data stores and in some cases in my data where data",
    "start": "2041850",
    "end": "2048419"
  },
  {
    "text": "warehouses and data Mart's and the reason it's not as widely as accessible we know why it doesn't work for the",
    "start": "2048419",
    "end": "2054090"
  },
  {
    "text": "historical data stores because of they're lopsided there's way more storage than compute whenever you try to",
    "start": "2054090",
    "end": "2059310"
  },
  {
    "text": "run any analytics against it it just falls over and then we have to go out and build brand new environments just to",
    "start": "2059310",
    "end": "2064378"
  },
  {
    "text": "be able to run certain queries but once we bring that data into the light and",
    "start": "2064379",
    "end": "2070550"
  },
  {
    "text": "once we make that data accessible to a larger part of the organization without",
    "start": "2070550",
    "end": "2076200"
  },
  {
    "text": "seeing the performance issues that we're seeing in the archive stores that I've worked in a lot of interesting things",
    "start": "2076200",
    "end": "2084148"
  },
  {
    "text": "start to happen you start to be able to run analytics against that data store your data scientists can go can go nuts",
    "start": "2084149",
    "end": "2090090"
  },
  {
    "text": "and actually access this access years more data than they probably have today",
    "start": "2090090",
    "end": "2095310"
  },
  {
    "text": "and be able to run the in analytics that creates insight for",
    "start": "2095310",
    "end": "2100450"
  },
  {
    "text": "your organization and unlocks value and it helps you deliver to your customers the things that they want when they want",
    "start": "2100450",
    "end": "2107349"
  },
  {
    "text": "it and make sure organization a lot more efficient so going back to our to our",
    "start": "2107349",
    "end": "2114420"
  },
  {
    "text": "architecture here we've now illuminated our two data stores we've illuminated our archived data sets",
    "start": "2114420",
    "end": "2120819"
  },
  {
    "text": "and we've illuminated further illuminated data lake because the scale at which we can start compute against",
    "start": "2120819",
    "end": "2126880"
  },
  {
    "text": "both of those data sets has increased dramatically which leads us to what we",
    "start": "2126880",
    "end": "2133569"
  },
  {
    "text": "call a flywheel for data so we start out with sources of data through this entire presentation I've been talking about",
    "start": "2133569",
    "end": "2139119"
  },
  {
    "text": "user activity and purchasing customers entering orders those orders getting executed and them end up purchasing",
    "start": "2139119",
    "end": "2146440"
  },
  {
    "text": "securities the data now is in a platform",
    "start": "2146440",
    "end": "2151750"
  },
  {
    "text": "that allows for better analytics those analytics allow for better decisions and were able to create better products and",
    "start": "2151750",
    "end": "2158349"
  },
  {
    "text": "services for our customers our customers are a lot happier we're a lot happy you",
    "start": "2158349",
    "end": "2163690"
  },
  {
    "text": "were doing things a lot more efficiently all the users in our organization are going to notice this so they want in at",
    "start": "2163690",
    "end": "2170799"
  },
  {
    "text": "this point so they're gonna bring in more data to the data link that data",
    "start": "2170799",
    "end": "2176440"
  },
  {
    "text": "describes more and more about what our organization does on a day to day basis and how we interact with our customers",
    "start": "2176440",
    "end": "2183299"
  },
  {
    "text": "the analytics available now to this data open up dramatically because it's basically all of the analytics and",
    "start": "2183299",
    "end": "2189369"
  },
  {
    "text": "reporting services that exist within AWS today so they're able to run Romo",
    "start": "2189369",
    "end": "2194380"
  },
  {
    "text": "reports and gain more insight than they can right now because all these services exist we also have artificial",
    "start": "2194380",
    "end": "2201910"
  },
  {
    "text": "intelligence deep learning neural networks machine learning services that we keep developing and keep adding to so",
    "start": "2201910",
    "end": "2208930"
  },
  {
    "text": "that is going to continue to produce better decisions better products which attracts more users and this cycle just",
    "start": "2208930",
    "end": "2215380"
  },
  {
    "text": "keeps continuing so it starts to feed on itself and we end up finding yourself in",
    "start": "2215380",
    "end": "2221140"
  },
  {
    "text": "a much better place with more opportunity for analytics and for",
    "start": "2221140",
    "end": "2226210"
  },
  {
    "text": "storage I'm sorry analytics and reporting",
    "start": "2226210",
    "end": "2230670"
  },
  {
    "text": "what can we do next I mean this is day one of reinvent so let's talk about what",
    "start": "2232220",
    "end": "2237660"
  },
  {
    "text": "we expected these are the topics we thought we were going to discuss at the beginning of this session we've gone",
    "start": "2237660",
    "end": "2243150"
  },
  {
    "text": "through all of these topics and we've discussed them in debt in detail so what",
    "start": "2243150",
    "end": "2248460"
  },
  {
    "text": "can you do now well you've got a week week of reinvent to go to it's a lot of",
    "start": "2248460",
    "end": "2254039"
  },
  {
    "text": "exciting things going on here and you're gonna learn a lot of really new cool things that we are doing and what you",
    "start": "2254039",
    "end": "2259589"
  },
  {
    "text": "can be doing you're gonna hear from other customers so for the next week you're gonna be spending it with us just",
    "start": "2259589",
    "end": "2265559"
  },
  {
    "text": "filling up filling yourself up with knowledge and looking at all the cool stuff that's going on well when you get back to work just go and look at your",
    "start": "2265559",
    "end": "2272700"
  },
  {
    "text": "archive datastores you've all on most of you have them just find out what they cost what does it",
    "start": "2272700",
    "end": "2278339"
  },
  {
    "text": "cost you to store all that historical data just look at your department maybe look at a couple of departments if some of you run multiple departments and then",
    "start": "2278339",
    "end": "2286349"
  },
  {
    "text": "look at what it would cost if you do what we just discussed if you took that data you moved it from your on-premise",
    "start": "2286349",
    "end": "2293460"
  },
  {
    "text": "database servers you put it into s3 and decommission to all of the on-premise work and continue to do your processing",
    "start": "2293460",
    "end": "2300239"
  },
  {
    "text": "from here we have a calculator we call a simple monthly calculator this is the link but if you go into a search end and",
    "start": "2300239",
    "end": "2305940"
  },
  {
    "text": "type in AWS simple monthly calculator this link will come up and you'll be able to use it determine what a cost per",
    "start": "2305940",
    "end": "2312450"
  },
  {
    "text": "month to do your historical data sets in AWS at that point you can ask yourself",
    "start": "2312450",
    "end": "2319019"
  },
  {
    "text": "whether or not this is going to save your company money or not whether or not this is something that you can sell to",
    "start": "2319019",
    "end": "2324869"
  },
  {
    "text": "your management because there really is a value proposition here and once you've",
    "start": "2324869",
    "end": "2330720"
  },
  {
    "text": "convinced yourself of that go build it it's doing a proof of concept on AWS is",
    "start": "2330720",
    "end": "2336630"
  },
  {
    "text": "very straightforward you pay for what you use and if you're not using it you don't pay for it so you can put some",
    "start": "2336630",
    "end": "2342569"
  },
  {
    "text": "some data out there run some keep them hewed against it can prove it all it works and then shut everything down it's",
    "start": "2342569",
    "end": "2348029"
  },
  {
    "text": "only gonna cost you what it would cost to run that for the short period of time of the POC and we have people here at",
    "start": "2348029",
    "end": "2355230"
  },
  {
    "text": "AWS that would love to help you guys do that so at this point I'd like to thank",
    "start": "2355230",
    "end": "2360329"
  },
  {
    "text": "everybody for spending the time with me today I will be up here for a bit more to",
    "start": "2360329",
    "end": "2365969"
  },
  {
    "text": "answer any questions there's also surveys that you guys can fill out if you could to talk about the",
    "start": "2365969",
    "end": "2372509"
  },
  {
    "text": "session and what you thought about it we want to make sure that we're providing the right kind of content and the right",
    "start": "2372509",
    "end": "2377579"
  },
  {
    "text": "kind of topics see going forward thank you very much [Applause]",
    "start": "2377579",
    "end": "2385980"
  }
]