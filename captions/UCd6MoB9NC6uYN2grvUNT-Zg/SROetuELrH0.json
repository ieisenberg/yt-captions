[
  {
    "start": "0",
    "end": "55000"
  },
  {
    "text": "and that you're ready to learn some more and then there will be time to continue",
    "start": "30",
    "end": "5940"
  },
  {
    "text": "with the labs after that welcome also to those joining us on twitch so realized",
    "start": "5940",
    "end": "12330"
  },
  {
    "text": "earlier on I was a little rude I didn't actually introduce myself properly so my",
    "start": "12330",
    "end": "17460"
  },
  {
    "text": "name is Gareth eager and I am a solution architect with AWS I work with some of",
    "start": "17460",
    "end": "23910"
  },
  {
    "text": "our enterprise customers in the New York City region to help them understand AWS",
    "start": "23910",
    "end": "30179"
  },
  {
    "text": "services how they can use them together to architect solutions for their",
    "start": "30179",
    "end": "35880"
  },
  {
    "text": "business and so my job is pretty much keeping up with the innovation and their",
    "start": "35880",
    "end": "42770"
  },
  {
    "text": "constant developments in AWS in this session we're going to look at",
    "start": "42770",
    "end": "48270"
  },
  {
    "text": "processing and analyzing your data in the data Lake so we know that that is",
    "start": "48270",
    "end": "54570"
  },
  {
    "text": "changing you've probably seen a hundred presentations already that tell you this so this is probably not new and shocking",
    "start": "54570",
    "end": "60629"
  },
  {
    "start": "55000",
    "end": "55000"
  },
  {
    "text": "news to you data is changing and as a result analytics are changing you know",
    "start": "60629",
    "end": "66770"
  },
  {
    "text": "perhaps 10 years ago so the data sets and the way of analyzing your data was",
    "start": "66770",
    "end": "72510"
  },
  {
    "text": "very different there were a lot of batch jobs it wasn't a real rush like we'll take today's data tonight will run at",
    "start": "72510",
    "end": "78840"
  },
  {
    "text": "overnight we'll have some figures in the morning everything was batch there is still a lot of batch but as things",
    "start": "78840",
    "end": "85500"
  },
  {
    "text": "accelerate as data accelerates there is this requirement from businesses to stay",
    "start": "85500",
    "end": "91290"
  },
  {
    "text": "competitive and to work more towards the use of streaming data and there's all different types of data sets now you",
    "start": "91290",
    "end": "98640"
  },
  {
    "text": "know traditionally it was a lot of database and there were maybe some Excel spreadsheets today businesses are",
    "start": "98640",
    "end": "105810"
  },
  {
    "text": "dealing and they wanting to be able to better analyze all types of different data so there could be a realty company",
    "start": "105810",
    "end": "113310"
  },
  {
    "text": "who wants to be able to take all the photos that the Realtors are taking of properties for sale and they want to",
    "start": "113310",
    "end": "119880"
  },
  {
    "text": "analyze and automatically extract information out of that that's data and that's new types of data that need new",
    "start": "119880",
    "end": "125850"
  },
  {
    "text": "types of analysis and so the AWS data",
    "start": "125850",
    "end": "131069"
  },
  {
    "start": "129000",
    "end": "129000"
  },
  {
    "text": "Lake has you've been learning today helps to Restless firstly it enables you to very",
    "start": "131069",
    "end": "137100"
  },
  {
    "text": "quickly ingest and store any type of data so you can bring your photos in you",
    "start": "137100",
    "end": "143010"
  },
  {
    "text": "can bring in your call center audio transcripts you can bring in your excel",
    "start": "143010",
    "end": "148230"
  },
  {
    "text": "files and your JSON files and all different types of data structure names",
    "start": "148230",
    "end": "153900"
  },
  {
    "text": "and structured can be very quickly ingested with various tools in AWS into",
    "start": "153900",
    "end": "159360"
  },
  {
    "text": "the data Lake and then you've got this wide variety of tools that you can use",
    "start": "159360",
    "end": "164490"
  },
  {
    "text": "so that you can pick the right tool for the right job to optimize your analytics",
    "start": "164490",
    "end": "169860"
  },
  {
    "text": "and and we're going to be looking at that today and then we also provide a",
    "start": "169860",
    "end": "175020"
  },
  {
    "text": "bunch of services and tools to help you balance getting insights from your data",
    "start": "175020",
    "end": "180709"
  },
  {
    "text": "democratizing your data and making it available to a wider audience within your corporation but balancing that",
    "start": "180709",
    "end": "187800"
  },
  {
    "text": "again still keeping the data secure still auditing the the data usage and",
    "start": "187800",
    "end": "193920"
  },
  {
    "text": "making sure that you have good data governance in place so the analytics",
    "start": "193920",
    "end": "204420"
  },
  {
    "text": "portfolio or the data Lake ecosystem around s3 is is really extensive as this",
    "start": "204420",
    "end": "211620"
  },
  {
    "text": "slide shows and the slide shows a lot of common use cases that apply for the data",
    "start": "211620",
    "end": "219480"
  },
  {
    "text": "Lake so on the right hand side you've got analytics and there's all different types of analytics from interactive to",
    "start": "219480",
    "end": "226260"
  },
  {
    "text": "Hadoop to data warehousing and then more and more we're hearing about machine",
    "start": "226260",
    "end": "232200"
  },
  {
    "text": "learning use cases and they're all different types of again machine learning and use cases from building",
    "start": "232200",
    "end": "238560"
  },
  {
    "text": "customized models for an organization from data that you've trained yourself to using services in AWS like AWS",
    "start": "238560",
    "end": "246720"
  },
  {
    "text": "recognition that is a pre-built and trained model that can extract metadata",
    "start": "246720",
    "end": "251910"
  },
  {
    "text": "out of an image to tell you if the person in the image is smiling if they're wearing sunglasses whether they",
    "start": "251910",
    "end": "259229"
  },
  {
    "text": "seem to be happy or sad or angry and so you have all these use cases and within",
    "start": "259229",
    "end": "266130"
  },
  {
    "text": "AWS you have all these tools to to help you deal with these use cases and so the",
    "start": "266130",
    "end": "274620"
  },
  {
    "text": "tools that we develop we integrate with s3 because that really it becomes the",
    "start": "274620",
    "end": "280919"
  },
  {
    "text": "heart of of your data Lake so we spoke a",
    "start": "280919",
    "end": "287850"
  },
  {
    "start": "285000",
    "end": "285000"
  },
  {
    "text": "little bit about this but not in too much detail earlier on in in the previous presentation but it's great",
    "start": "287850",
    "end": "295410"
  },
  {
    "text": "that you've got all this raw data and you can easily ingest it into your data Lake but what can happen though is you",
    "start": "295410",
    "end": "303300"
  },
  {
    "text": "can ultimately end up with a data swamp if all you do is ingest your data and",
    "start": "303300",
    "end": "308940"
  },
  {
    "text": "then think it's going to be useful to people you're gonna end up with a data swamp and a data swamp is an area where",
    "start": "308940",
    "end": "316050"
  },
  {
    "text": "you've got a lot of data but nobody's really sure what's there they don't know how to find the data that they're",
    "start": "316050",
    "end": "322020"
  },
  {
    "text": "looking for and that applies to them and then even if they do find that data they're kind of unsure about where it",
    "start": "322020",
    "end": "328380"
  },
  {
    "text": "came from and how it got there and can they trust it and is recent and is it optimized and so building a data lake is",
    "start": "328380",
    "end": "336330"
  },
  {
    "text": "more than just ingesting raw data you need to do a couple of core things and",
    "start": "336330",
    "end": "341970"
  },
  {
    "text": "we can look at a few of them here most importantly is catalogued the data so that you you have more metadata about",
    "start": "341970",
    "end": "349890"
  },
  {
    "text": "what's in your data lake and make this a searchable catalog so that the data consumers in your organization can find",
    "start": "349890",
    "end": "356070"
  },
  {
    "text": "the data that they need and understand more about the sources of that data and",
    "start": "356070",
    "end": "361110"
  },
  {
    "text": "then you also want to optimize the data for analytics it's great you can ingest",
    "start": "361110",
    "end": "366930"
  },
  {
    "text": "all these JSON files or CSV files and you can ingest hundreds of gigabytes you",
    "start": "366930",
    "end": "372090"
  },
  {
    "text": "can adjust hundreds of terabytes but those files are not really optimized for running large scale queries against",
    "start": "372090",
    "end": "379229"
  },
  {
    "text": "and so you need to do some work to compress the files and to put them into",
    "start": "379229",
    "end": "385500"
  },
  {
    "text": "a format that is better suited for analytics and then you also want to partition your data and we're gonna look",
    "start": "385500",
    "end": "391500"
  },
  {
    "text": "and explain a little bit about how you approach that and what the benefits are and then you also want to look at",
    "start": "391500",
    "end": "398490"
  },
  {
    "text": "denormalize you know flattening your dad so this is where you may join a number of tables together kind of the opposite",
    "start": "398490",
    "end": "406170"
  },
  {
    "text": "of when when you're going to a traditional relational database you're generally normalizing the data you want",
    "start": "406170",
    "end": "412800"
  },
  {
    "text": "kind of one copy of any one entity or entry and you have that link to a bunch of other tables through foreign keys",
    "start": "412800",
    "end": "419690"
  },
  {
    "text": "with data analytics you often end up denormalizing the data or flatten it and",
    "start": "419690",
    "end": "426830"
  },
  {
    "text": "as you've heard today and as you've seen when you've been doing the labs that AWS",
    "start": "426830",
    "end": "433320"
  },
  {
    "text": "glue is a service that helps you both with the data cataloging and then with developing and running your ETL jobs so",
    "start": "433320",
    "end": "441690"
  },
  {
    "text": "the caller's go out and discover your data and discover the information about the partitions in the schema and then",
    "start": "441690",
    "end": "447810"
  },
  {
    "text": "you're able to run ETL jobs to to do things like convert the format of the",
    "start": "447810",
    "end": "455610"
  },
  {
    "text": "file to compress it partition it and denormalize effect and little various other things that you may want to do",
    "start": "455610",
    "end": "462060"
  },
  {
    "text": "such as aggregating data and working out averages so as I mentioned text files",
    "start": "462060",
    "end": "471720"
  },
  {
    "start": "468000",
    "end": "468000"
  },
  {
    "text": "are not optimized for analytics text files are great if you're the one doing",
    "start": "471720",
    "end": "478140"
  },
  {
    "text": "the analysts or the analytics and you need a to be able to read the file a",
    "start": "478140",
    "end": "484410"
  },
  {
    "text": "text file you can open up in your favorite text editor but it that's",
    "start": "484410",
    "end": "489810"
  },
  {
    "text": "probably not a really scalable approach to doing big data analytics so with big",
    "start": "489810",
    "end": "495060"
  },
  {
    "text": "data analytics you want machines to do the analytics for you and the machines are much better at working with",
    "start": "495060",
    "end": "500850"
  },
  {
    "text": "compressed files and so over the last few years what has become really popular",
    "start": "500850",
    "end": "506730"
  },
  {
    "text": "is these new file formats really popular ones are parquet and RC and these",
    "start": "506730",
    "end": "513360"
  },
  {
    "text": "formats are by default they compressed they're in binary format so you're not going to be able to open them with your",
    "start": "513360",
    "end": "519690"
  },
  {
    "text": "favorite text editor but the analytics software and tools that you use will be",
    "start": "519690",
    "end": "526020"
  },
  {
    "text": "able to very effectively work with them and one of the reasons is analytics often is favors or often works",
    "start": "526020",
    "end": "534040"
  },
  {
    "text": "with columns than rows of data so let's say you get into tonight sales data from",
    "start": "534040",
    "end": "540460"
  },
  {
    "text": "around the country and you've got gigabytes of data and there's a column there that talks about the the total",
    "start": "540460",
    "end": "547600"
  },
  {
    "text": "price of every transaction very often in analytics you're going to be wanting to do something with that whether it's",
    "start": "547600",
    "end": "554050"
  },
  {
    "text": "summing it up or whether it's working out averages or doing some other kind of",
    "start": "554050",
    "end": "560320"
  },
  {
    "text": "statistics on that and so these file formats are already structured around working with columns and the way that",
    "start": "560320",
    "end": "567100"
  },
  {
    "text": "they physically written to disk even is all about to optimize running or reading",
    "start": "567100",
    "end": "573220"
  },
  {
    "text": "them on a column base these files also include integrated indexes and stats so",
    "start": "573220",
    "end": "581080"
  },
  {
    "text": "you can imagine your sales data once you convert it to parquet for example you",
    "start": "581080",
    "end": "587170"
  },
  {
    "text": "may end up with thousands and thousands of these paka files that you don't want to run your analytics against and then",
    "start": "587170",
    "end": "593470"
  },
  {
    "text": "in each file because it's keeping stats it can do things like track what is the minimum value of a column and what is",
    "start": "593470",
    "end": "601030"
  },
  {
    "text": "the maximum value so let's say for one specific file the minimum value is",
    "start": "601030",
    "end": "606610"
  },
  {
    "text": "fifty-three dollars for this one column and the maximum value is 864 dollars if",
    "start": "606610",
    "end": "612550"
  },
  {
    "text": "you want to do a query where you're saying I want to know all sales over a thousand dollars your analytics engine",
    "start": "612550",
    "end": "618880"
  },
  {
    "text": "can read this metadata or the statistics about the file and it immediately knows",
    "start": "618880",
    "end": "624460"
  },
  {
    "text": "there is no transactions in their file there are over a thousand dollars and so",
    "start": "624460",
    "end": "629680"
  },
  {
    "text": "it doesn't need to scan all the details on their file to figure it out the metadata tells it that and it can move",
    "start": "629680",
    "end": "635230"
  },
  {
    "text": "on to the next file so this is why these formats are really optimized for the",
    "start": "635230",
    "end": "640900"
  },
  {
    "text": "type of analytics that we need to run today there there is another type that's optimized in a similar way but it's row",
    "start": "640900",
    "end": "647650"
  },
  {
    "text": "based so if you're doing reads where you're reading a subset of rows but",
    "start": "647650",
    "end": "654100"
  },
  {
    "text": "you're reading all the columns then this ever is a good format for that and that",
    "start": "654100",
    "end": "659320"
  },
  {
    "text": "also is compressed as in a binary format it also includes stats and indexes but",
    "start": "659320",
    "end": "665470"
  },
  {
    "text": "that's for where you're reading subset of rows but all columns a lot of what we're going to be talking about",
    "start": "665470",
    "end": "671000"
  },
  {
    "text": "today and a lot of what we do in analytics is really focused on the columns the other way that you can",
    "start": "671000",
    "end": "678050"
  },
  {
    "start": "675000",
    "end": "675000"
  },
  {
    "text": "optimize your analytics is through data partitioning and this is where you separate your data files by any column",
    "start": "678050",
    "end": "685520"
  },
  {
    "text": "by any column of your choice and then when your analytic engine runs it only",
    "start": "685520",
    "end": "690860"
  },
  {
    "text": "has to read the partitions where where there is relevant data based on for",
    "start": "690860",
    "end": "697520"
  },
  {
    "text": "example the where clause in your sequel statement and what this does is similar",
    "start": "697520",
    "end": "702980"
  },
  {
    "text": "to using park' files in addition to that it helps you to reduce the amount of",
    "start": "702980",
    "end": "708860"
  },
  {
    "text": "data that needs to be scanned and that leads to increased performance and then if you remember athena and services a",
    "start": "708860",
    "end": "717500"
  },
  {
    "text": "creature of spectrum the pricing is based on the amount of data scan so five dollars to scan a terabyte of data if",
    "start": "717500",
    "end": "724580"
  },
  {
    "text": "you can reduce the amount of data that needs to be scanned you can significantly reduce the cost of your",
    "start": "724580",
    "end": "730370"
  },
  {
    "text": "queries and so we will actually have a look at some examples of that in a moment",
    "start": "730370",
    "end": "736300"
  },
  {
    "start": "736000",
    "end": "736000"
  },
  {
    "text": "so when you're partitioning a very common way to do the partitioning is",
    "start": "736300",
    "end": "741770"
  },
  {
    "text": "based on a date so it could be something kind of simple like this you have flight",
    "start": "741770",
    "end": "747080"
  },
  {
    "text": "data and you break it up into different years and so if you looked at this in a",
    "start": "747080",
    "end": "753170"
  },
  {
    "text": "three if you'd use something like Spock and you'd said Spock I've got all the Stata I want to write it to s3 and I",
    "start": "753170",
    "end": "760550"
  },
  {
    "text": "want to partition it by the column yeah you would look and you would see a structure something like this and you",
    "start": "760550",
    "end": "766610"
  },
  {
    "text": "may have noticed that in the labs that you were doing and so when you now run a query that says select the destination",
    "start": "766610",
    "end": "772760"
  },
  {
    "text": "origin from flights where the year equals 1991 the analytics engine only",
    "start": "772760",
    "end": "778490"
  },
  {
    "text": "has to look at that one prefix or directory you could think of that way containing files for the year equals",
    "start": "778490",
    "end": "785950"
  },
  {
    "text": "1991 and in a similar way you another",
    "start": "785950",
    "end": "791270"
  },
  {
    "text": "common pattern is to get a little more granular with that where you're now",
    "start": "791270",
    "end": "797140"
  },
  {
    "text": "doing your partitioning in multiple levels so by year by month and by day and so if you know",
    "start": "797140",
    "end": "804270"
  },
  {
    "text": "that a lot of your queries are going to be query one specific day or few specific days then during your",
    "start": "804270",
    "end": "811110"
  },
  {
    "text": "partitioning like this is it's beneficial you know I don't even have to read through all the files for an entire",
    "start": "811110",
    "end": "816750"
  },
  {
    "text": "year you can go and find the files for a specific day so a few guidelines here",
    "start": "816750",
    "end": "824340"
  },
  {
    "start": "822000",
    "end": "822000"
  },
  {
    "text": "when choosing a column that you want to partition your data on and really the",
    "start": "824340",
    "end": "829470"
  },
  {
    "text": "the first rule here is that it should be economy with relatively low cardinality and that's uniqueness throughout the",
    "start": "829470",
    "end": "837300"
  },
  {
    "text": "data set and yet also with a pretty even distribution of data for that column or",
    "start": "837300",
    "end": "842910"
  },
  {
    "text": "key so again if we're looking at the date format let's say we we did go up by year",
    "start": "842910",
    "end": "849570"
  },
  {
    "text": "a month and day you know if you've got sales data for a year and you're open every day of the year you're gonna have",
    "start": "849570",
    "end": "855240"
  },
  {
    "text": "a good pretty equal mix of data in each one of those partitions so you doing it",
    "start": "855240",
    "end": "861990"
  },
  {
    "text": "by day month and year you'll have 365 partitions per year that's not a problem at all you can have data for 10 or 20",
    "start": "861990",
    "end": "869670"
  },
  {
    "text": "years like that but if you get a bunch of IOT data in and you're getting a lot",
    "start": "869670",
    "end": "874860"
  },
  {
    "text": "of data per second and you think well maybe we partitioned by second there would be going to an extreme where",
    "start": "874860",
    "end": "880770"
  },
  {
    "text": "you're probably going to start running into performance problems because you will literally have millions of partitions for a year",
    "start": "880770",
    "end": "888230"
  },
  {
    "text": "project did so let me repeat the question quick yeah so this is for for",
    "start": "907980",
    "end": "913030"
  },
  {
    "text": "batch processing you're ingesting all this data you're doing some processing to partition it out and your and your",
    "start": "913030",
    "end": "920110"
  },
  {
    "text": "optimizing your analytics by doing that and the question was how about when you've got real-time streaming data come",
    "start": "920110",
    "end": "926740"
  },
  {
    "text": "in how do you partition or optimize your analytics there so typically you would land your data as it comes in to some s3",
    "start": "926740",
    "end": "934510"
  },
  {
    "text": "bucket and then you'd run a separate job to read that data do compaction which is combining of you know if you have a lot",
    "start": "934510",
    "end": "940390"
  },
  {
    "text": "of small files into recently sized files you'd repartition that data because you",
    "start": "940390",
    "end": "947590"
  },
  {
    "text": "know you might be getting data in a certain way that's on the received timestamp but you it might be queried in",
    "start": "947590",
    "end": "953020"
  },
  {
    "text": "a different way so you want to partition it based on the query keys",
    "start": "953020",
    "end": "958560"
  },
  {
    "text": "right so I mean typically we see customers doing micro batching and you",
    "start": "965389",
    "end": "970529"
  },
  {
    "text": "can run things triggered by either a lambda function and you could pick up a certain set of files",
    "start": "970529",
    "end": "979759"
  },
  {
    "text": "how often as in like how many times in a day or so typically you want to have",
    "start": "999230",
    "end": "1007309"
  },
  {
    "text": "enough data to make sure you have good size compaction like which is like 256 MB to about a gig file a single size",
    "start": "1007309",
    "end": "1014870"
  },
  {
    "text": "file that's the range of data size that's recommended so it really depends",
    "start": "1014870",
    "end": "1019939"
  },
  {
    "text": "on how much data you are getting and you know what is the lag that you are ok with on the queering site so if you want",
    "start": "1019939",
    "end": "1025339"
  },
  {
    "text": "as he was mentioning like you know near real-time availability of the newest state then you want to run that you know",
    "start": "1025339",
    "end": "1031970"
  },
  {
    "text": "pretty much every minute or you know even less than that yeah so it depends",
    "start": "1031970",
    "end": "1044029"
  },
  {
    "text": "on how much data you're getting at what rate and then you know how quickly you want to access that data and you just",
    "start": "1044029",
    "end": "1050029"
  },
  {
    "text": "need to kind of figure out a good balance there if you truly need data if you're truly needing data and near real-time then",
    "start": "1050029",
    "end": "1058309"
  },
  {
    "text": "you're probably going to live with that performance hit essentially on the queering side the recommendation for",
    "start": "1058309",
    "end": "1065120"
  },
  {
    "text": "most optimal querying is that size so you probably want to see if you are ok",
    "start": "1065120",
    "end": "1070250"
  },
  {
    "text": "with slightly stale data like if you're not getting enough data in a minute maybe 5 minutes or 10 minutes is a good",
    "start": "1070250",
    "end": "1076220"
  },
  {
    "text": "window to",
    "start": "1076220",
    "end": "1078700"
  },
  {
    "text": "I'm not yeah so just to make sure I understand the question you were asking",
    "start": "1091029",
    "end": "1096679"
  },
  {
    "text": "do we have customers who are streaming data into redshift so we do have customers who will use",
    "start": "1096679",
    "end": "1103879"
  },
  {
    "text": "tools like Kinesis firehose where they take streaming data and then they batch it and and one of the functions of",
    "start": "1103879",
    "end": "1110179"
  },
  {
    "text": "Kinesis firehose is to load that into targets like redshift so Drina inserts",
    "start": "1110179",
    "end": "1115610"
  },
  {
    "text": "into a chipped is relatively expensive operation so you got to play around with",
    "start": "1115610",
    "end": "1120679"
  },
  {
    "text": "the buffering to make sure you're getting the right intervals or the right data sizes but Jeff Evans on Kinesis",
    "start": "1120679",
    "end": "1126769"
  },
  {
    "text": "firehose is a tool that can help to load your streaming data directly into redshift yeah so behind the scenes is",
    "start": "1126769",
    "end": "1137539"
  },
  {
    "text": "putting it into h3 and then it's from h3 its DNA a load into redshift the reason",
    "start": "1137539",
    "end": "1142940"
  },
  {
    "text": "for that is because redshift is optimized to read from s3 it actually paralyze as the read so the",
    "start": "1142940",
    "end": "1148100"
  },
  {
    "text": "recommendation is to put data and some temporary storage in s3 and and origin",
    "start": "1148100",
    "end": "1153610"
  },
  {
    "text": "all right so to continue some guidelines here sorry if we can",
    "start": "1153610",
    "end": "1159350"
  },
  {
    "text": "is it a partitioning question because otherwise I'd rather okay let's go for it sorry when you're talking about the",
    "start": "1159350",
    "end": "1172789"
  },
  {
    "text": "date",
    "start": "1172789",
    "end": "1175000"
  },
  {
    "text": "so I mean you can you can partition by by any column so you you can partition",
    "start": "1179580",
    "end": "1187390"
  },
  {
    "text": "by by date and year so and actually I'm going to show you a couple of examples",
    "start": "1187390",
    "end": "1192760"
  },
  {
    "text": "of how that structure would would look in a moment but the the next point show",
    "start": "1192760",
    "end": "1197950"
  },
  {
    "text": "was actually to say that you can use strings as well it doesn't have to be dating your partitions so for example if",
    "start": "1197950",
    "end": "1204190"
  },
  {
    "text": "you know that you're going to be doing queries in a regular basis taking they say the sales data and you sometimes",
    "start": "1204190",
    "end": "1210160"
  },
  {
    "text": "want to query that by country and sometimes by state and sometimes by the business unit in a state then the that",
    "start": "1210160",
    "end": "1218290"
  },
  {
    "text": "would be a pretty good way to partition it country / state / business unit if it",
    "start": "1218290",
    "end": "1223870"
  },
  {
    "text": "was rather by business unit you may want to put it business unit first and then country and state and the the key here",
    "start": "1223870",
    "end": "1231310"
  },
  {
    "text": "really is you do need to give some thought when you're doing your partitioning into how do you want to",
    "start": "1231310",
    "end": "1236530"
  },
  {
    "text": "query your data and what do you want to filter out so you've got to think when you're writing sequel statements what",
    "start": "1236530",
    "end": "1242530"
  },
  {
    "text": "are you likely to be put in with the where clause and then you want to optimize your partitioning around that",
    "start": "1242530",
    "end": "1250680"
  },
  {
    "text": "so we had the slide up earlier in a previous presentation but once the shows",
    "start": "1250680",
    "end": "1256360"
  },
  {
    "text": "is all your data sources coming into s3 and their first bucket is your raw data",
    "start": "1256360",
    "end": "1261670"
  },
  {
    "text": "you catalog that with glue so that we can now have other tools like glue ETL",
    "start": "1261670",
    "end": "1269320"
  },
  {
    "text": "or EMR or athina work with that data and then you run a glue ETL process and",
    "start": "1269320",
    "end": "1275860"
  },
  {
    "text": "maybe in the first stage you're just saying I'm going to transform this file into POC a format",
    "start": "1275860",
    "end": "1281740"
  },
  {
    "text": "and I'm going to partition it by a specific yeah and so then you end up end",
    "start": "1281740",
    "end": "1286810"
  },
  {
    "text": "up with that being in maybe a separate bucket or separate prefix within your s3 bucket you run your glue jobs again to",
    "start": "1286810",
    "end": "1294130"
  },
  {
    "text": "now go and catalog the transform data and then there may even be a third phase in your data lake where you're saying",
    "start": "1294130",
    "end": "1299980"
  },
  {
    "text": "okay now I want to do some denormalizing I want to do some enrichment of the data",
    "start": "1299980",
    "end": "1305830"
  },
  {
    "text": "I have and join various tables and so there may be multiple phases",
    "start": "1305830",
    "end": "1311060"
  },
  {
    "text": "to your pipeline for preparing your data for wider analytics within your",
    "start": "1311060",
    "end": "1317390"
  },
  {
    "text": "corporation and ultimately though you're going to get to the point where you feel",
    "start": "1317390",
    "end": "1323780"
  },
  {
    "start": "1319000",
    "end": "1319000"
  },
  {
    "text": "that you have optimized your data for querying and now you have some choices about what tools you want to use to",
    "start": "1323780",
    "end": "1330020"
  },
  {
    "text": "query that data so we can look at a couple of them today Amazon Athena Amazon Emaar redshift and",
    "start": "1330020",
    "end": "1337970"
  },
  {
    "text": "Rachel ships spectrum and those are our kind of analytic engines and then we're also going to talk a little bit about",
    "start": "1337970",
    "end": "1343580"
  },
  {
    "text": "quick site which is our business intelligence visualization tool so let's",
    "start": "1343580",
    "end": "1350540"
  },
  {
    "start": "1349000",
    "end": "1349000"
  },
  {
    "text": "start off with talking about Athena so if you've been doing the labs you've already used the thinner a little bit",
    "start": "1350540",
    "end": "1356090"
  },
  {
    "text": "and so you would have realized Athena enables interactive query of data",
    "start": "1356090",
    "end": "1361670"
  },
  {
    "text": "directly in s3 or in your data Lake using standard NC sequel statements and",
    "start": "1361670",
    "end": "1367430"
  },
  {
    "text": "so your typical users share our data scientists and data analysts may be data",
    "start": "1367430",
    "end": "1372800"
  },
  {
    "text": "engineers who wanting to both maybe initially explore the data that's coming into the data Lake and then they wanting",
    "start": "1372800",
    "end": "1379580"
  },
  {
    "text": "to run sequel analytical queries on the data in the data Lake possibly even",
    "start": "1379580",
    "end": "1384740"
  },
  {
    "text": "using Athena and those queries to create new tables as part of the processing",
    "start": "1384740",
    "end": "1392810"
  },
  {
    "text": "pipeline the the great thing about Athena is that it is that there's no",
    "start": "1392810",
    "end": "1400370"
  },
  {
    "start": "1395000",
    "end": "1395000"
  },
  {
    "text": "infrastructure for you to set up or manage so once you have used glue to go",
    "start": "1400370",
    "end": "1405590"
  },
  {
    "text": "and catalog your data you can immediately go for example to the Athena console and you can write a sequel query",
    "start": "1405590",
    "end": "1412460"
  },
  {
    "text": "to go and query the data in s3 you can of course also use JDBC or ODBC",
    "start": "1412460",
    "end": "1418790"
  },
  {
    "text": "connectors with Athena so it doesn't have to be through the console using those JDBC connectors you can use",
    "start": "1418790",
    "end": "1424760"
  },
  {
    "text": "various other tools to also do sequel analytics on your s3 data and then as I",
    "start": "1424760",
    "end": "1431270"
  },
  {
    "text": "mentioned it's you paying for the amount of data scanned and that's why the partitioning and the transforming of the",
    "start": "1431270",
    "end": "1437180"
  },
  {
    "text": "files are so important and and so you'll be paying $5 per terabyte of data",
    "start": "1437180",
    "end": "1442670"
  },
  {
    "text": "scanned so let's look at some best practices and the first of these really time to what",
    "start": "1442670",
    "end": "1449190"
  },
  {
    "text": "we've been talking about but it gives you an example of what a difference partitioning can make for your data so",
    "start": "1449190",
    "end": "1457110"
  },
  {
    "text": "in that first example we're doing a select count star from Allen Adham table",
    "start": "1457110",
    "end": "1462240"
  },
  {
    "text": "and we're saying where the ship date equals a specific date 1996 - on on - -",
    "start": "1462240",
    "end": "1468120"
  },
  {
    "text": "on one so if we run that against a non partition table that query and in this",
    "start": "1468120",
    "end": "1473610"
  },
  {
    "text": "testing that was done it would take nearly ten seconds nine point seven one seconds to run and it would need to scan",
    "start": "1473610",
    "end": "1480480"
  },
  {
    "text": "the full line item data table and that's a 74 gig now by partitioning that data",
    "start": "1480480",
    "end": "1487290"
  },
  {
    "text": "by year a month and date we're able to reduce the run time down to just over two seconds and then even more",
    "start": "1487290",
    "end": "1494540"
  },
  {
    "text": "impressively we're able to reduce the amount of data scan from 74 gig down to",
    "start": "1494540",
    "end": "1499590"
  },
  {
    "text": "just under 30 Meg's of data scanned and so you can see we take the cost of the query from 36 cents to scan the 74 gig",
    "start": "1499590",
    "end": "1507780"
  },
  {
    "text": "of data the cost of the query - well below 1 cent to effectively get the same",
    "start": "1507780",
    "end": "1514140"
  },
  {
    "text": "results from your data set and and that's some of the power of partitioning at the same time it is possible you know",
    "start": "1514140",
    "end": "1522060"
  },
  {
    "text": "there's always two sides to everything you can end up over partitioning your data and this is again where you need to",
    "start": "1522060",
    "end": "1529470"
  },
  {
    "text": "give some thought into how you're going to what type of queries you're going to be running on your data if you are going",
    "start": "1529470",
    "end": "1536130"
  },
  {
    "text": "to be up and running queries that select all the columns for your data set you",
    "start": "1536130",
    "end": "1543870"
  },
  {
    "text": "may actually find that your performance will decrease slightly because s39 has",
    "start": "1543870",
    "end": "1548880"
  },
  {
    "text": "to work through the different partitions of your data so where you're still",
    "start": "1548880",
    "end": "1554520"
  },
  {
    "text": "scanning the same amount of data here partitioned or and partitioned if you do in a select star on everything but the",
    "start": "1554520",
    "end": "1561900"
  },
  {
    "text": "amount of run time is slightly longer on the partitioned data because of the overhead of dealing with the partitions",
    "start": "1561900",
    "end": "1567900"
  },
  {
    "text": "and dealing with every partition not being able to prune any of them out yeah",
    "start": "1567900",
    "end": "1574880"
  },
  {
    "text": "so the question was along with the query charges is any charge for the cost of compute and with Athena there is no",
    "start": "1579349",
    "end": "1586019"
  },
  {
    "text": "charge with the cost of computers purely based on the data that you scan for your queries so that's one of the advantages",
    "start": "1586019",
    "end": "1593759"
  },
  {
    "text": "if you're not running anything there's there's no monthly cost there's nothing to have it available you purely pay for",
    "start": "1593759",
    "end": "1600179"
  },
  {
    "text": "when your query runs based on the amount of data scanned so let's look at some",
    "start": "1600179",
    "end": "1605309"
  },
  {
    "text": "other best practices here and and these already have to do with how you format",
    "start": "1605309",
    "end": "1610379"
  },
  {
    "start": "1606000",
    "end": "1606000"
  },
  {
    "text": "your your sequel queries so here we see that if you do kind of a traditional",
    "start": "1610379",
    "end": "1616320"
  },
  {
    "text": "sequel query where you saying I'm wanting to get some data from a lana",
    "start": "1616320",
    "end": "1621839"
  },
  {
    "text": "atom table from the comment field where it includes the words tweets or regular",
    "start": "1621839",
    "end": "1627719"
  },
  {
    "text": "or Express and a few others and you do that by chaining together but a bunch of like statements that's not an optimal",
    "start": "1627719",
    "end": "1637559"
  },
  {
    "text": "way with something like an Athena the Athena analytics engine to run that",
    "start": "1637559",
    "end": "1643619"
  },
  {
    "text": "query but rather using a regular expression as in the top example you're",
    "start": "1643619",
    "end": "1648929"
  },
  {
    "text": "able to see a 17% improvement in the performance of their query to be honest",
    "start": "1648929",
    "end": "1657029"
  },
  {
    "text": "I didn't do this one so I'm going to give you a length o to the blog post where it actually has some of these best",
    "start": "1657029",
    "end": "1663330"
  },
  {
    "text": "practices and more and some of the data may be there I'm not sure you know I",
    "start": "1663330",
    "end": "1674549"
  },
  {
    "text": "think it really does depend ultimately on your data set every this was a sample",
    "start": "1674549",
    "end": "1680119"
  },
  {
    "text": "so it depends on perhaps how wide the columns are how big the data set is how",
    "start": "1680119",
    "end": "1685739"
  },
  {
    "text": "many like statements you're training together but the the idea here though is regular expressions are going to be more",
    "start": "1685739",
    "end": "1693649"
  },
  {
    "text": "performant with something like Athena where where we can optimize around that",
    "start": "1693649",
    "end": "1698759"
  },
  {
    "text": "better than we can with training like statements",
    "start": "1698759",
    "end": "1703339"
  },
  {
    "text": "yeah the cost of these two would be the same we'd still scan the same amount of data so the cost would be the same but",
    "start": "1704390",
    "end": "1710360"
  },
  {
    "text": "you'd have a performance improvement with the one and he has another really important tip because when you're doing",
    "start": "1710360",
    "end": "1716960"
  },
  {
    "text": "analytics you're going to quite often end up joining tables together and this",
    "start": "1716960",
    "end": "1722630"
  },
  {
    "text": "is seems like a really small thing but it makes a really big difference when you're doing a join you want the larger",
    "start": "1722630",
    "end": "1728270"
  },
  {
    "text": "table on the left-hand side of your query and the smaller table on the right-hand side and this really just has",
    "start": "1728270",
    "end": "1734390"
  },
  {
    "text": "to do with how under the covers the analytic engine works but over here we've got tables a pots table and that's",
    "start": "1734390",
    "end": "1741799"
  },
  {
    "text": "a really big table and then we've got a line-item table and this is from an order so this is obviously a much",
    "start": "1741799",
    "end": "1747230"
  },
  {
    "text": "smaller table and so when we're doing this this joint here if we put the part",
    "start": "1747230",
    "end": "1753400"
  },
  {
    "text": "table on the Left we see a 53% performance improvement compared to",
    "start": "1753400",
    "end": "1759890"
  },
  {
    "text": "running pretty much exactly the same query but putting the larger table on the right hand side so it's a small",
    "start": "1759890",
    "end": "1766549"
  },
  {
    "text": "thing but if you have data engineers in your organization that are starting to work with a thinner and they're new to",
    "start": "1766549",
    "end": "1773780"
  },
  {
    "text": "it like said there's going to be a a link to the full blog post in one of the",
    "start": "1773780",
    "end": "1779090"
  },
  {
    "text": "slides but it's important that your team's become aware of this as it can make a significant difference to the",
    "start": "1779090",
    "end": "1786380"
  },
  {
    "text": "performance of the queries",
    "start": "1786380",
    "end": "1789400"
  },
  {
    "text": "ya know exactly so the question is yeah do you save any money through these",
    "start": "1803530",
    "end": "1809660"
  },
  {
    "text": "performance things no this is generally just about about yeah and performance so",
    "start": "1809660",
    "end": "1815300"
  },
  {
    "text": "you want you if you think their time is money then maybe you're saving some money alright but do not saving any money as",
    "start": "1815300",
    "end": "1821990"
  },
  {
    "text": "far as your Athena cost go the thinner costs are the same but you're improving",
    "start": "1821990",
    "end": "1827030"
  },
  {
    "text": "performance and that's what you want to do and then this one as you can see makes the biggest difference and you",
    "start": "1827030",
    "end": "1833090"
  },
  {
    "text": "probably would have guessed this now through our discussions about using pakka files because they column based",
    "start": "1833090",
    "end": "1838940"
  },
  {
    "text": "files and and using partitioning to reduce amount of data scanned this is just an example where we say we're doing",
    "start": "1838940",
    "end": "1845600"
  },
  {
    "text": "a select star so we selecting all the columns in our table and another example",
    "start": "1845600",
    "end": "1852230"
  },
  {
    "text": "where we're selecting the specific columns that we know we need for our query and the query is a hundred and",
    "start": "1852230",
    "end": "1857690"
  },
  {
    "text": "forty five times faster so again this this kind of means you need to encourage your data engineers basically not to be",
    "start": "1857690",
    "end": "1865340"
  },
  {
    "text": "lazy don't do select stars it may be time teen it may be easier but don't do",
    "start": "1865340",
    "end": "1870920"
  },
  {
    "text": "select star when specially when you're writing these for production and you've got these big ETL jobs that are running",
    "start": "1870920",
    "end": "1877900"
  },
  {
    "text": "make sure that you really put thought into your queries because they're little optimizations they can make a",
    "start": "1877900",
    "end": "1883940"
  },
  {
    "text": "significant difference to the performance yes",
    "start": "1883940",
    "end": "1890350"
  },
  {
    "text": "yeah so the question is as the schema changes and that's pretty common in your",
    "start": "1908690",
    "end": "1914639"
  },
  {
    "text": "data lake you may find the schema changes if you're doing a select on specific columns there's perhaps a a",
    "start": "1914639",
    "end": "1921419"
  },
  {
    "text": "bigger chance that your your query is going to fail if if a column goes away and I think you're totally right there",
    "start": "1921419",
    "end": "1927690"
  },
  {
    "text": "and so like everything you need to balance these things out and and every organization and perhaps every table",
    "start": "1927690",
    "end": "1934019"
  },
  {
    "text": "every query you may decide on different trade-offs here so I'm sure projects it",
    "start": "1934019",
    "end": "1945419"
  },
  {
    "text": "does it user know does it assume a null value if a column goes away suddenly when Athena's do you know where the",
    "start": "1945419",
    "end": "1951990"
  },
  {
    "text": "theme is trying to read if the column is not there I'm not quite sure okay",
    "start": "1951990",
    "end": "1960799"
  },
  {
    "text": "so yeah in our example we were using those well those were in the select but",
    "start": "1975080",
    "end": "1980370"
  },
  {
    "text": "if they were in the way cause it could fail and you know this is generally I think as somebody if you're in RT if",
    "start": "1980370",
    "end": "1987270"
  },
  {
    "text": "you're in data you realize that there are trade-offs worth a lot of the decisions that you make and so sometimes",
    "start": "1987270",
    "end": "1993170"
  },
  {
    "text": "you need to put a lot of thought into it and sometimes you need to decide to change the strategy it really depends to",
    "start": "1993170",
    "end": "2000559"
  },
  {
    "text": "you if a a if you don't have a good way to track schema changes and early on we",
    "start": "2000559",
    "end": "2005660"
  },
  {
    "text": "spoke about how you can get glue to notify you of those changes through cloud watch integration but if you don't",
    "start": "2005660",
    "end": "2012260"
  },
  {
    "text": "feel that you can adequately do that and it's critical to you for job fails maybe you make a trade off and take a",
    "start": "2012260",
    "end": "2018200"
  },
  {
    "text": "performance hit in exchange for the stability but if you put in the right",
    "start": "2018200",
    "end": "2023360"
  },
  {
    "text": "procedures in place you're getting those medications from glooming schema changes and you're proactively dealing with it",
    "start": "2023360",
    "end": "2029240"
  },
  {
    "text": "you can certainly get significant performance improvements by being more selective in how you write your queries",
    "start": "2029240",
    "end": "2036710"
  },
  {
    "text": "yeah so I don't believe you can index",
    "start": "2036710",
    "end": "2043340"
  },
  {
    "text": "petitions directly but what the the file formats like okay they've got indexing",
    "start": "2043340",
    "end": "2049878"
  },
  {
    "text": "as part of the the format of the file so it's not really the the partition there's index but there is kind of this",
    "start": "2049879",
    "end": "2057020"
  },
  {
    "text": "concept of indexing and sorting with within the files store",
    "start": "2057020",
    "end": "2061960"
  },
  {
    "text": "sorry dude we recommend what so so it's",
    "start": "2071919",
    "end": "2087829"
  },
  {
    "text": "a question whether you should do a continuous crawling or su crawlers right now run on a schedule and you need to",
    "start": "2087829",
    "end": "2094128"
  },
  {
    "text": "kind of give it the s3 buckets or paths which you want to crawl so it is really",
    "start": "2094129",
    "end": "2099650"
  },
  {
    "text": "I mean it's not that it'll crawl everything in s3 for you so you need to",
    "start": "2099650",
    "end": "2105319"
  },
  {
    "text": "kind of give it enough data to know which buckets to crawl and give it the right I am credentials to go and access",
    "start": "2105319",
    "end": "2112640"
  },
  {
    "text": "that data",
    "start": "2112640",
    "end": "2115028"
  },
  {
    "text": "I mean I mean I don't know if data really changes from a schema perspective",
    "start": "2123760",
    "end": "2128950"
  },
  {
    "text": "that often typically what we see is new partitions carrying register in the catalog and crawlers can do that as well",
    "start": "2128950",
    "end": "2134790"
  },
  {
    "text": "and crawlers will only look at incremental or new data so if you have no data that has been written then the",
    "start": "2134790",
    "end": "2141460"
  },
  {
    "text": "crawler it will be a no op for the crawlers that being said I mean depending on how often you're getting",
    "start": "2141460",
    "end": "2146829"
  },
  {
    "text": "updates to your data if it's if it's like streaming updates and maybe you can run it every 10 minutes every 15 minutes",
    "start": "2146829",
    "end": "2153070"
  },
  {
    "text": "if it is a batch update and an hourly crawling usually is okay no there's no",
    "start": "2153070",
    "end": "2166599"
  },
  {
    "text": "coupling between the crawlers or the query jobs as such so yeah the question there was does caller keep query jobs",
    "start": "2166599",
    "end": "2172810"
  },
  {
    "text": "rate waiting but yeah they're they're decoupled effectively so what the caller does it updates the catalog a team is",
    "start": "2172810",
    "end": "2180760"
  },
  {
    "text": "not going to wait for those updates Athena uses the catalog as it finds at when it's running the query so moving on",
    "start": "2180760",
    "end": "2189849"
  },
  {
    "text": "to Amazon EMR which is not a different way for analyzing your data and and here",
    "start": "2189849",
    "end": "2196390"
  },
  {
    "start": "2192000",
    "end": "2192000"
  },
  {
    "text": "typical uses of data scientists and data engineers who need to process and",
    "start": "2196390",
    "end": "2202240"
  },
  {
    "text": "analyze really large amounts of data using some common open source projects",
    "start": "2202240",
    "end": "2207339"
  },
  {
    "text": "and that's typically Apache Hadoop in a patchy spot so again we keep coming back",
    "start": "2207339",
    "end": "2215950"
  },
  {
    "start": "2212000",
    "end": "2212000"
  },
  {
    "text": "to our data Lake it's built on s3 and it has all this integration to different analytical tools in this case with EMR",
    "start": "2215950",
    "end": "2223480"
  },
  {
    "text": "you get access to 19 open source tools and that is things like for for batch",
    "start": "2223480",
    "end": "2231190"
  },
  {
    "text": "processing it's tools like hive and peg for interactive processing spark or",
    "start": "2231190",
    "end": "2238270"
  },
  {
    "text": "presto from machine learning maybe spark ml and so you have this wide variety of",
    "start": "2238270",
    "end": "2243700"
  },
  {
    "text": "tools that you can use in a managed cluster with EMR and so with EMR we",
    "start": "2243700",
    "end": "2250569"
  },
  {
    "text": "manage the cluster we regularly update the releases or the availability of the",
    "start": "2250569",
    "end": "2256059"
  },
  {
    "text": "latest tools generally within 30 days after they have been released they become available within EMR and so you",
    "start": "2256059",
    "end": "2264359"
  },
  {
    "text": "using the storage in s3 and so a really important concept here to remember is",
    "start": "2264359",
    "end": "2269559"
  },
  {
    "text": "and what we keep pushing is this decoupling of compute and storage and so traditionally when you were doing who",
    "start": "2269559",
    "end": "2276339"
  },
  {
    "text": "dupe based projects if you were running this on-prem somewhere it wasn't easy to decouple compute and storage generally",
    "start": "2276339",
    "end": "2283599"
  },
  {
    "text": "each compute node or each node within your cluster would have some Associated",
    "start": "2283599",
    "end": "2288999"
  },
  {
    "text": "storage with it if you wanted to increase compute you how to increase storage and and the other way around as",
    "start": "2288999",
    "end": "2295449"
  },
  {
    "text": "well and then you couldn't shut down the cluster even if things were quiet because there was where all the data",
    "start": "2295449",
    "end": "2302049"
  },
  {
    "text": "left and so that was where the data was persisted when you decouple your Hadoop",
    "start": "2302049",
    "end": "2307349"
  },
  {
    "text": "processing and the compute from the storage at the s3 layer it enables",
    "start": "2307349",
    "end": "2313329"
  },
  {
    "text": "things like running transient clusters and running multiple clusters to work on the same data sets so often when you",
    "start": "2313329",
    "end": "2320170"
  },
  {
    "text": "were running into dupe on premise there'd be a bit of a fight between the data science and the data engineering",
    "start": "2320170",
    "end": "2325630"
  },
  {
    "text": "teams because everybody had a job that they wanted to run at the same time and the server would get busy and not be",
    "start": "2325630",
    "end": "2333880"
  },
  {
    "text": "able to handle the load always when you're running it with in with s3 as",
    "start": "2333880",
    "end": "2339789"
  },
  {
    "text": "your persistent storage base you can now spun up multiple clusters and so each team can have their own EMR cluster",
    "start": "2339789",
    "end": "2347109"
  },
  {
    "text": "configured with the tools that they want analyzing and accessing the data in s3",
    "start": "2347109",
    "end": "2352479"
  },
  {
    "text": "and then you can also have transient clusters where you don't have to have the cluster running all the time you're",
    "start": "2352479",
    "end": "2358779"
  },
  {
    "text": "able to have these transient clusters where if you know at every night at ten",
    "start": "2358779",
    "end": "2363939"
  },
  {
    "text": "o'clock forget all the cell's data in and we've got this ETL job that needs to run for three hours or so to process it",
    "start": "2363939",
    "end": "2370079"
  },
  {
    "text": "you're able to spun up a cluster just to process that and then shut down at the",
    "start": "2370079",
    "end": "2375819"
  },
  {
    "text": "end of it so I see there a couple questions but we've already got 20 minutes left and they still we're only",
    "start": "2375819",
    "end": "2381249"
  },
  {
    "text": "about halfway through so we will save questions for the end and if we don't",
    "start": "2381249",
    "end": "2386349"
  },
  {
    "text": "get to it at the end at leti come and speak to us as well so an",
    "start": "2386349",
    "end": "2392680"
  },
  {
    "text": "example of somebody who's using EMR with a s3 based on a lake is Zillow I'm sure",
    "start": "2392680",
    "end": "2398890"
  },
  {
    "text": "a lot of you will know Zillow and the app they operate a portfolio of online",
    "start": "2398890",
    "end": "2404800"
  },
  {
    "text": "real estate properties and they had this challenge where they had to analyze data",
    "start": "2404800",
    "end": "2410080"
  },
  {
    "text": "regarding 100 million homes and they had to very quickly use machine learning to work out the Zestimate they're famous",
    "start": "2410080",
    "end": "2417310"
  },
  {
    "text": "were being able to work out a real market value estimate for all the",
    "start": "2417310",
    "end": "2422470"
  },
  {
    "text": "properties that are in their database and so Zillow bolted on AWS this",
    "start": "2422470",
    "end": "2427810"
  },
  {
    "text": "pipeline where they take data from multiple sources so public property",
    "start": "2427810",
    "end": "2433030"
  },
  {
    "text": "records home tax assessments all types of different data sources they bring that in through Kinesis into Amazon s3",
    "start": "2433030",
    "end": "2440620"
  },
  {
    "text": "into the data Lake and then they use Spock on EMR to run these machine",
    "start": "2440620",
    "end": "2446230"
  },
  {
    "text": "learning jobs to to analyze and then work out estimates for that so another",
    "start": "2446230",
    "end": "2456490"
  },
  {
    "text": "type of analysis that you may need to do may involve data warehouses and this is",
    "start": "2456490",
    "end": "2461530"
  },
  {
    "text": "where you have business analysts that are that are looking to say you know we want to load the really frequently query",
    "start": "2461530",
    "end": "2468610"
  },
  {
    "text": "data into a high-speed data warehouse and this is generally for dashboards and",
    "start": "2468610",
    "end": "2474310"
  },
  {
    "text": "at the top of operational reporting and so Amazon redshift is a fast and and",
    "start": "2474310",
    "end": "2481990"
  },
  {
    "text": "simple to configure and deploy and manage data warehouse that as you'll see",
    "start": "2481990",
    "end": "2487000"
  },
  {
    "text": "in a moment can also extend your queries to the daedalic so quick side chat felt",
    "start": "2487000",
    "end": "2494110"
  },
  {
    "text": "I had to put this in because this is quite often a question that comes up",
    "start": "2494110",
    "end": "2499300"
  },
  {
    "text": "where people say so let me get it data lakes have now replaced data warehouses",
    "start": "2499300",
    "end": "2504400"
  },
  {
    "text": "or I have to choose a dead Lake or a data warehouse and and that's absolutely",
    "start": "2504400",
    "end": "2509680"
  },
  {
    "text": "not the case data lakes are complementary to data warehouses and opens up in terms the",
    "start": "2509680",
    "end": "2517780"
  },
  {
    "text": "data Lake is a source of data for the data warehouse and there's a few other things in this",
    "start": "2517780",
    "end": "2522880"
  },
  {
    "text": "table I'm not going to go through through them all now but you can see this table is kind of a good quick",
    "start": "2522880",
    "end": "2528130"
  },
  {
    "text": "highlight of some of the differences where things like with the data Lake is schema and read so it's really optimized",
    "start": "2528130",
    "end": "2533859"
  },
  {
    "text": "for quick ingestion just just bring your data in when we read it will will use will project a schema on to it where",
    "start": "2533859",
    "end": "2541420"
  },
  {
    "text": "with the data warehouse you really got to put a lot of thought upfront into exactly how you're going to structure your schema and then it is really pretty",
    "start": "2541420",
    "end": "2547660"
  },
  {
    "text": "fixed and ingesting data is a relatively expensive operation and then data lakes",
    "start": "2547660",
    "end": "2554799"
  },
  {
    "text": "are for data scientists and analysts and multiple use cases where data warehouse",
    "start": "2554799",
    "end": "2560859"
  },
  {
    "text": "is really excel at business intelligence reporting but but that is that is what they do data warehouses are sequel",
    "start": "2560859",
    "end": "2568049"
  },
  {
    "text": "queries that's what you use with the data warehouse with the data Lake you can use things like Hadoop and and hive",
    "start": "2568049",
    "end": "2575349"
  },
  {
    "text": "and spark for for your analytics so with",
    "start": "2575349",
    "end": "2581260"
  },
  {
    "text": "warehouse with with red shift you get a data warehouse that you can deploy within a matter of minutes you can",
    "start": "2581260",
    "end": "2587619"
  },
  {
    "text": "specify the size that you want your your cluster to be and if you've ever dealt with data warehouses on Prem you'll see",
    "start": "2587619",
    "end": "2595869"
  },
  {
    "text": "that the cost of redshift comparatively is is very low and then a few years ago",
    "start": "2595869",
    "end": "2603880"
  },
  {
    "start": "2601000",
    "end": "2601000"
  },
  {
    "text": "we announced rich of spectrum which really extends your data warehouse to",
    "start": "2603880",
    "end": "2609190"
  },
  {
    "text": "the data Lake and so a common pattern here is where you will load maybe the last three months of sales data into",
    "start": "2609190",
    "end": "2616630"
  },
  {
    "text": "your data warehouse because you know that the business queries data on a quarterly basis and so the last three",
    "start": "2616630",
    "end": "2622779"
  },
  {
    "text": "months of data is constantly being queried by thousands of users throughout the business and business analysts and",
    "start": "2622779",
    "end": "2629170"
  },
  {
    "text": "they really need the top performance from those queries and they need to be able to do queries at scale and and all",
    "start": "2629170",
    "end": "2636759"
  },
  {
    "text": "the time but occasionally they may need one of those queries they may need some",
    "start": "2636759",
    "end": "2642880"
  },
  {
    "text": "data that's in the data like this may be from an unrelated data set or maybe some archive data so it's not something",
    "start": "2642880",
    "end": "2649269"
  },
  {
    "text": "they're clearing all the time but it would be really useful if they could access that and",
    "start": "2649269",
    "end": "2654640"
  },
  {
    "text": "all the state of that every data analyst may want into the data warehouse will become a relatively cost costly process",
    "start": "2654640",
    "end": "2664030"
  },
  {
    "text": "and so what you do with Amazon rates of spectrum you are able to define those",
    "start": "2664030",
    "end": "2670270"
  },
  {
    "text": "tables that you have in your data Lake that have been discovered by glue so they're registered in the blue catalog",
    "start": "2670270",
    "end": "2676770"
  },
  {
    "text": "Amazon rich of spectrum is able to use those as external tables and you can",
    "start": "2676770",
    "end": "2682150"
  },
  {
    "text": "join those external tables with the internal tables that have been loaded into the data warehouse so it looks a",
    "start": "2682150",
    "end": "2689710"
  },
  {
    "text": "little bit like this way again you've got your data Lake at the bottom you use copy commands to copy data from s3 into",
    "start": "2689710",
    "end": "2697299"
  },
  {
    "text": "the rich of cluster over here we've got a three node cluster and so those",
    "start": "2697299",
    "end": "2704410"
  },
  {
    "text": "clusters all have high performance SSD disk and you're doing all your heart data queries on that and then every now",
    "start": "2704410",
    "end": "2710710"
  },
  {
    "text": "and then when you need to get some archive data or another data source in s3 for the occasional query rich of",
    "start": "2710710",
    "end": "2718059"
  },
  {
    "text": "spectrum will go out and it will use a bunch of warm compute power that that we have available and we will run their",
    "start": "2718059",
    "end": "2723730"
  },
  {
    "text": "query and then pass it back to the reg of nodes so that they can consolidate the queries and then at the top there",
    "start": "2723730",
    "end": "2731349"
  },
  {
    "text": "you have the leader node which then presents is or managers that interface between the reporting tools or whatever",
    "start": "2731349",
    "end": "2738579"
  },
  {
    "text": "you're using tomato JDBC connection and and Richard so a great example here is",
    "start": "2738579",
    "end": "2746319"
  },
  {
    "text": "one close to home here with amazon.com we had 50 petabytes of data and we had",
    "start": "2746319",
    "end": "2752470"
  },
  {
    "text": "600,000 analytic jobs a day and we really found that we were hitting the",
    "start": "2752470",
    "end": "2758890"
  },
  {
    "text": "limits of our traditional Oracle data warehouse we found was really expensive",
    "start": "2758890",
    "end": "2764049"
  },
  {
    "text": "it was difficult to maintain it took a lot of man-hours to to keep that up and running we were getting to the point who",
    "start": "2764049",
    "end": "2770619"
  },
  {
    "text": "that it was becoming really difficult to scale it so and so we set out on this process to move to a new architecture",
    "start": "2770619",
    "end": "2777970"
  },
  {
    "text": "using an s3 data Lake and then using redshift rich of spectrum and EMR I",
    "start": "2777970",
    "end": "2784359"
  },
  {
    "text": "believe was November last year that we for moved of all of our Oracle data",
    "start": "2784359",
    "end": "2790090"
  },
  {
    "text": "warehouses and we've now been able to double the amount of data store to a hundred petabytes we've lowered the cost",
    "start": "2790090",
    "end": "2796990"
  },
  {
    "text": "of running the analytics and able to gain new insights through the power of",
    "start": "2796990",
    "end": "2802090"
  },
  {
    "text": "having both redshift and rich of spectrum and making that available to the business analysts and then the the",
    "start": "2802090",
    "end": "2811450"
  },
  {
    "text": "last service I want to talk about is Amazon quick site and quick site really",
    "start": "2811450",
    "end": "2817240"
  },
  {
    "text": "is a fast cloud-based business intelligence service so if you've worked",
    "start": "2817240",
    "end": "2822370"
  },
  {
    "text": "before in the past with products like tableau and clique and some of those",
    "start": "2822370",
    "end": "2828660"
  },
  {
    "text": "quick site is our business intelligence platform and so this is generally used",
    "start": "2828660",
    "end": "2834430"
  },
  {
    "text": "by business analysts who want to be able to create rich visualizations from your data to gain new insights and then be",
    "start": "2834430",
    "end": "2841870"
  },
  {
    "text": "able to very easily share those around the organization so a couple things to",
    "start": "2841870",
    "end": "2847810"
  },
  {
    "text": "point out that make quick site different to some of the other BI tools aren't",
    "start": "2847810",
    "end": "2852820"
  },
  {
    "text": "there first is that is purely card based it's it's serverless so there's no server to go and deploy",
    "start": "2852820",
    "end": "2859450"
  },
  {
    "text": "there's no ec2 instances to manage and you'll get to in the next labs that we",
    "start": "2859450",
    "end": "2865300"
  },
  {
    "text": "do you're going to log into quick site and and you'll you'll get some hands-on experience with that and then it's",
    "start": "2865300",
    "end": "2871840"
  },
  {
    "text": "scalable you can start doing some queries today and actually you always get one free user per AWS account for",
    "start": "2871840",
    "end": "2878080"
  },
  {
    "text": "quick site so there's no charge for the first user and then you may expand to to 10 users next week and and maybe you",
    "start": "2878080",
    "end": "2884410"
  },
  {
    "text": "expand to a thousand users in a couple of months and we scale it all behind the scenes for you so it's it's there's",
    "start": "2884410",
    "end": "2891190"
  },
  {
    "text": "nothing to manage here it's all managed behind the scenes and then again along",
    "start": "2891190",
    "end": "2896200"
  },
  {
    "text": "with you you if you know Amazon and AWS you MA our pricing philosophy we want",
    "start": "2896200",
    "end": "2902320"
  },
  {
    "text": "you to pay for what you use and so as you derive value from our services you",
    "start": "2902320",
    "end": "2907510"
  },
  {
    "text": "pay for those and so you can actually have users that if they don't log into quick site and don't run any they don't",
    "start": "2907510",
    "end": "2915130"
  },
  {
    "text": "do look at any of the dashboards or run anything you're not going to pay anything",
    "start": "2915130",
    "end": "2920440"
  },
  {
    "text": "for them for the month when they don't do anything and if they log in once a week to have a look at a dashboard",
    "start": "2920440",
    "end": "2925839"
  },
  {
    "text": "there's a per session price that is charged for that so it's an incredibly",
    "start": "2925839",
    "end": "2931270"
  },
  {
    "text": "low cost tool that you can really make available to wide groups of users within an organization because you're only",
    "start": "2931270",
    "end": "2938290"
  },
  {
    "text": "paying as they use and actually work with quick site and then quick site",
    "start": "2938290",
    "end": "2944380"
  },
  {
    "text": "again is is able to work with the tools we've spoken about Athena and redshift",
    "start": "2944380",
    "end": "2951310"
  },
  {
    "text": "and EMR but quick site is also able to connect to a bunch of other data sources",
    "start": "2951310",
    "end": "2956530"
  },
  {
    "text": "that includes data sources that may be on Prem so as long as you've got the networking access setup right you can",
    "start": "2956530",
    "end": "2963700"
  },
  {
    "text": "use quick site to analyze data from on Prem databases there's a bunch of tools",
    "start": "2963700",
    "end": "2969609"
  },
  {
    "text": "here our RDS which is our managed relational database service quick site can integrate with that and then also",
    "start": "2969609",
    "end": "2976780"
  },
  {
    "text": "with a bunch of third-party applications so things like Salesforce and Square and Adobe Analytics so there's a wide",
    "start": "2976780",
    "end": "2983410"
  },
  {
    "text": "variety of data sources that you can bring together for your business analyst so that they can look for new insights",
    "start": "2983410",
    "end": "2991599"
  },
  {
    "text": "and so this is really what quick site is good at is being able to create and then",
    "start": "2991599",
    "end": "2997599"
  },
  {
    "text": "publish and share interactive dashboards so you can have a business analyst that takes data sources from various places",
    "start": "2997599",
    "end": "3004380"
  },
  {
    "text": "brings them together creates the basic dashboard and then gives business users",
    "start": "3004380",
    "end": "3010859"
  },
  {
    "text": "access and when they log in they can further filter the data they can drill down into the data as well so they can",
    "start": "3010859",
    "end": "3016530"
  },
  {
    "text": "it's not just a static dashboard they're able to interact with with the data visualization",
    "start": "3016530",
    "end": "3023900"
  },
  {
    "text": "you're even able to take the dashboards that are developed and actually embed them in your own application so you can",
    "start": "3023900",
    "end": "3030990"
  },
  {
    "text": "have possibly external users to your organization you don't want to give them direct quick site access you can embed a",
    "start": "3030990",
    "end": "3039530"
  },
  {
    "text": "QuickStart visualization through our frames into an application that you've",
    "start": "3039530",
    "end": "3045060"
  },
  {
    "text": "written and an example here is the NFL so they have a bunch of stats that they",
    "start": "3045060",
    "end": "3051660"
  },
  {
    "text": "make available to the teams through their application or through their website",
    "start": "3051660",
    "end": "3057160"
  },
  {
    "text": "these teams wouldn't even know that they were using quick site under the covers but they're able to log in they get the",
    "start": "3057160",
    "end": "3063309"
  },
  {
    "text": "data the visualizations they can interact again drill down and filter the data and in the backend it's using quick",
    "start": "3063309",
    "end": "3070690"
  },
  {
    "text": "site but it's presented as a cohesive part of your application and another",
    "start": "3070690",
    "end": "3078369"
  },
  {
    "text": "example of customer care is is Rio Tinto they're the world's largest materials and mining company and they've got",
    "start": "3078369",
    "end": "3086309"
  },
  {
    "text": "20,000 users for their critical risk management CRM system and they're a big",
    "start": "3086309",
    "end": "3092049"
  },
  {
    "text": "user of quick site to enable those users to very easily explore large data sets",
    "start": "3092049",
    "end": "3098469"
  },
  {
    "text": "with thousands of data points so I knew",
    "start": "3098469",
    "end": "3104259"
  },
  {
    "start": "3101000",
    "end": "3101000"
  },
  {
    "text": "we wouldn't have much time so I'm not really going to go very deep into this but it would be wrong if we were",
    "start": "3104259",
    "end": "3109420"
  },
  {
    "text": "speaking about dead Lakes and we're speaking about analytics if we didn't at least talk briefly about machine",
    "start": "3109420",
    "end": "3115749"
  },
  {
    "text": "learning and all the use cases that are coming out for machine learning now and",
    "start": "3115749",
    "end": "3121259"
  },
  {
    "text": "so at AWS we've taken this approach where we've got effectively three different layers of machine learning",
    "start": "3121259",
    "end": "3127539"
  },
  {
    "text": "that are often targeted at different people within an organization so you may",
    "start": "3127539",
    "end": "3134259"
  },
  {
    "text": "have data scientists the guys that have gone and been working with machine lora machine learning models for years and",
    "start": "3134259",
    "end": "3141130"
  },
  {
    "text": "have a PhD and machine learning those types of people are often wanting to",
    "start": "3141130",
    "end": "3147819"
  },
  {
    "text": "really kind of get down to the core and they will use something like our deep",
    "start": "3147819",
    "end": "3154150"
  },
  {
    "text": "learning AMI our Amazon machine image they will boot that up on a ec2 virtual machine and",
    "start": "3154150",
    "end": "3161190"
  },
  {
    "text": "that comes pre compiled with all of these different frameworks like MX NEX",
    "start": "3161190",
    "end": "3166269"
  },
  {
    "text": "and tensorflow and a bunch of others so we pre compile manage that put it together and so",
    "start": "3166269",
    "end": "3171819"
  },
  {
    "text": "you'll have your data scientists at some level may want to work at that level we",
    "start": "3171819",
    "end": "3178269"
  },
  {
    "text": "find our goal here really is to make machine learning available to much wider",
    "start": "3178269",
    "end": "3184119"
  },
  {
    "text": "variety of developers not just the the people that have have been studying it for years and they've got",
    "start": "3184119",
    "end": "3190109"
  },
  {
    "text": "their PhDs and so we've got services in the middle they at the platform they're things like Amazon sage maker and this",
    "start": "3190109",
    "end": "3197160"
  },
  {
    "text": "is a service that makes it much easier to use pre-built algorithms or develop your own it makes it easier to train",
    "start": "3197160",
    "end": "3204540"
  },
  {
    "text": "with the data that's in s3 to train your models to tune the models and then",
    "start": "3204540",
    "end": "3209579"
  },
  {
    "text": "ultimately to deploy the model to a hosted sage maker environment so that",
    "start": "3209579",
    "end": "3214770"
  },
  {
    "text": "you can run predictions or inference against those models and then at the top",
    "start": "3214770",
    "end": "3219900"
  },
  {
    "text": "we've got the AI services and this is where we've taken a model we've developed it and trained it and we don't",
    "start": "3219900",
    "end": "3226890"
  },
  {
    "text": "stop once we release the service we keep developing it and training it and improving it but you've got a bunch of",
    "start": "3226890",
    "end": "3232740"
  },
  {
    "text": "services here for things like Amazon recognition and so the example gave earlier of the realty company getting",
    "start": "3232740",
    "end": "3239280"
  },
  {
    "text": "all of these pictures from the agents of the houses they can use things like",
    "start": "3239280",
    "end": "3245130"
  },
  {
    "text": "recognition to identify is there a pool in this picture is this a kitchen is",
    "start": "3245130",
    "end": "3250170"
  },
  {
    "text": "this the bathroom or you can imagine in a dating application you can do facial",
    "start": "3250170",
    "end": "3255839"
  },
  {
    "text": "recognition to see is somebody happy are they sad so there's all types of metadata that you can now use you can",
    "start": "3255839",
    "end": "3263910"
  },
  {
    "text": "pull out of images using machine learning and we make that available with something like Amazon recognition we've",
    "start": "3263910",
    "end": "3270869"
  },
  {
    "text": "got other services like Amazon transcribe and this way you can take an audio file such as recordings of",
    "start": "3270869",
    "end": "3277309"
  },
  {
    "text": "meetings or from your contact centre you can take these audio files run it",
    "start": "3277309",
    "end": "3283140"
  },
  {
    "text": "through transcribe and you will get a transcript of the dialogue in that",
    "start": "3283140",
    "end": "3288569"
  },
  {
    "text": "meeting and then you can use something like Amazon comprehend to to gain insights",
    "start": "3288569",
    "end": "3293910"
  },
  {
    "text": "into what that text is telling you was a suppository entities that were discussed",
    "start": "3293910",
    "end": "3300329"
  },
  {
    "text": "and so on and so as you bring all these different data sources into the",
    "start": "3300329",
    "end": "3305579"
  },
  {
    "text": "centralized s3 data Lake you really are enabled with a large amount of you you",
    "start": "3305579",
    "end": "3313140"
  },
  {
    "text": "can really solve a bunch of use cases using the wide variety of tools that are",
    "start": "3313140",
    "end": "3318299"
  },
  {
    "text": "available within the ecosystem built around s3 so",
    "start": "3318299",
    "end": "3325240"
  },
  {
    "text": "I see we do still have five minutes left so let's take a couple questions so the",
    "start": "3325240",
    "end": "3340270"
  },
  {
    "text": "question was do we have ml models built within Athena at this time we I don't",
    "start": "3340270",
    "end": "3345280"
  },
  {
    "text": "think we do",
    "start": "3345280",
    "end": "3348180"
  },
  {
    "text": "okay so if I'm understanding and remembering cuz there were two questions there I think the one was how we talk",
    "start": "3401410",
    "end": "3407950"
  },
  {
    "text": "about the decoupling of storage in compute and then how does Rachel put into that are we saying you know don't",
    "start": "3407950",
    "end": "3413200"
  },
  {
    "text": "use redshift because now with Richard your storage in your compute is is coupled and I would say again there",
    "start": "3413200",
    "end": "3420730"
  },
  {
    "text": "there are trade-offs and there are different workloads and you should look at what tool you need for for each",
    "start": "3420730",
    "end": "3426520"
  },
  {
    "text": "workload what's to remember with Richard because of Rachel spectrum the cap in between computing storage is not as",
    "start": "3426520",
    "end": "3432940"
  },
  {
    "text": "tight so for the data that you're requiring on a very frequent basis and",
    "start": "3432940",
    "end": "3439440"
  },
  {
    "text": "you really need high-performance that you're not going to be able to get in the same way from s3 you need maybe sub",
    "start": "3439440",
    "end": "3446950"
  },
  {
    "text": "sub second performance on large data queries you really need kind of SSD disk",
    "start": "3446950",
    "end": "3452740"
  },
  {
    "text": "close to the compute to be able to do that so there are workloads where redshift makes sense and you are going",
    "start": "3452740",
    "end": "3459940"
  },
  {
    "text": "to very likely have a bunch of range of clusters and you can extend that so the",
    "start": "3459940",
    "end": "3466300"
  },
  {
    "text": "idea here really is just you want to put the right data in redshift and that's data that you need fast extremely fast",
    "start": "3466300",
    "end": "3473620"
  },
  {
    "text": "query performance on and data that is queried frequently if it's not queried that frequently it should probably be in",
    "start": "3473620",
    "end": "3479230"
  },
  {
    "text": "the data like and you should use range of spectrum to extend your queries to that or you should use Athena for that",
    "start": "3479230",
    "end": "3486390"
  },
  {
    "text": "alternatively as well and I think you asking the question about how Athena integrates with things like spark ml on",
    "start": "3486390",
    "end": "3493870"
  },
  {
    "text": "that so when you're using something like EMR that has a spark engine built into",
    "start": "3493870",
    "end": "3500140"
  },
  {
    "text": "it that won't use Athena to query the data and s3e Amar's got its own integration with the glue catalog that",
    "start": "3500140",
    "end": "3506740"
  },
  {
    "text": "makes the data and s3 available to spark so yeah the different use case is there",
    "start": "3506740",
    "end": "3513250"
  },
  {
    "text": "and I mean I just want to add that you so sage maker integrates with EMR as well so you can run CH maker with EMR",
    "start": "3513250",
    "end": "3519580"
  },
  {
    "text": "sage maker also integrates with a spark endpoint that glue provides if you want",
    "start": "3519580",
    "end": "3524920"
  },
  {
    "text": "to use a serval a spark endpoint so you can run sage maker or your own zeppelin notebook and connect it to the dev",
    "start": "3524920",
    "end": "3532360"
  },
  {
    "text": "endpoints and glue which are essentially a spark in point okay I think we've got one good two minutes and they see how",
    "start": "3532360",
    "end": "3539710"
  },
  {
    "text": "quick this question is yes",
    "start": "3539710",
    "end": "3547380"
  },
  {
    "text": "so I mean the question was about a sribeen eventually consistent storage",
    "start": "3559269",
    "end": "3566479"
  },
  {
    "text": "and how does that work with with EMR and transient data if I'm understanding",
    "start": "3566479",
    "end": "3571669"
  },
  {
    "text": "correctly so often what you'll do is you may load some data from s3 into your",
    "start": "3571669",
    "end": "3576949"
  },
  {
    "text": "cluster into an HDFS file system running in the cluster that you use as transient storage while the job run and then at",
    "start": "3576949",
    "end": "3584659"
  },
  {
    "text": "the end of the job you will offload data game to the persistent data store which is s3 EMR also EMR FS which actually",
    "start": "3584659",
    "end": "3593509"
  },
  {
    "text": "helps with the consistency consistency issues on s3 so if you have MRFs enabled",
    "start": "3593509",
    "end": "3599269"
  },
  {
    "text": "and you're writing to s3 you should not see the consistency",
    "start": "3599269",
    "end": "3603669"
  },
  {
    "text": "so I just gotta quickly wrap up our twitch broadcast chair we've kind of",
    "start": "3610630",
    "end": "3617229"
  },
  {
    "text": "reached up there hour for that so thank you to everybody from twitch who who joined for this as well",
    "start": "3617229",
    "end": "3623739"
  },
  {
    "text": "but we'll we'll take a couple of more questions after this thank you",
    "start": "3623739",
    "end": "3628829"
  },
  {
    "text": "[Music]",
    "start": "3631880",
    "end": "3635359"
  },
  {
    "text": "[Music]",
    "start": "3638560",
    "end": "3645599"
  }
]