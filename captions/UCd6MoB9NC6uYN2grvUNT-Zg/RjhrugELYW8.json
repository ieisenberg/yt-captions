[
  {
    "start": "0",
    "end": "19000"
  },
  {
    "text": "and so today I'm very pleased to",
    "start": "0",
    "end": "2129"
  },
  {
    "text": "announce a new feature of Amazon sage",
    "start": "2129",
    "end": "4830"
  },
  {
    "text": "maker that we call neo a deep learning",
    "start": "4830",
    "end": "7230"
  },
  {
    "text": "model compiler that lets customers train",
    "start": "7230",
    "end": "9660"
  },
  {
    "text": "models once and deploy them anywhere",
    "start": "9660",
    "end": "11940"
  },
  {
    "text": "with up to a 2x performance improvement",
    "start": "11940",
    "end": "15900"
  },
  {
    "text": "[Applause]",
    "start": "15900",
    "end": "18949"
  },
  {
    "text": "so let's take a look at how this works",
    "start": "18949",
    "end": "21289"
  },
  {
    "start": "19000",
    "end": "67000"
  },
  {
    "text": "using sage maker neo which is a fully",
    "start": "21289",
    "end": "23789"
  },
  {
    "text": "managed environment inside sage maker",
    "start": "23789",
    "end": "25560"
  },
  {
    "text": "you train your model as you would",
    "start": "25560",
    "end": "27240"
  },
  {
    "text": "normally then you pass it into our new",
    "start": "27240",
    "end": "29970"
  },
  {
    "text": "neo deep learning compiler and you",
    "start": "29970",
    "end": "32488"
  },
  {
    "text": "select your deployment target the",
    "start": "32489",
    "end": "34260"
  },
  {
    "text": "hardware specific target and the deep",
    "start": "34260",
    "end": "36870"
  },
  {
    "text": "learning compiler will take advantage of",
    "start": "36870",
    "end": "38820"
  },
  {
    "text": "everything that it knows just as you",
    "start": "38820",
    "end": "40649"
  },
  {
    "text": "would compile normal C++ code to be able",
    "start": "40649",
    "end": "43200"
  },
  {
    "text": "to take advantage of all of the",
    "start": "43200",
    "end": "44520"
  },
  {
    "text": "performance advantages of those",
    "start": "44520",
    "end": "46320"
  },
  {
    "text": "individual platforms and you can build",
    "start": "46320",
    "end": "48420"
  },
  {
    "text": "as from a single trained model multiple",
    "start": "48420",
    "end": "51000"
  },
  {
    "text": "deployment targets then you just take",
    "start": "51000",
    "end": "53190"
  },
  {
    "text": "them you can run them on ec2 you can run",
    "start": "53190",
    "end": "55739"
  },
  {
    "text": "them on Sage maker you can even run them",
    "start": "55739",
    "end": "57539"
  },
  {
    "text": "using Greengrass ml inference or you can",
    "start": "57539",
    "end": "59699"
  },
  {
    "text": "just download them and run them on your",
    "start": "59699",
    "end": "61949"
  },
  {
    "text": "devices as you go this is a material",
    "start": "61949",
    "end": "64559"
  },
  {
    "text": "improvement with just a few clicks we've",
    "start": "64559",
    "end": "68250"
  },
  {
    "start": "67000",
    "end": "152000"
  },
  {
    "text": "been working already with a number of",
    "start": "68250",
    "end": "70200"
  },
  {
    "text": "hardware vendors including Intel",
    "start": "70200",
    "end": "72630"
  },
  {
    "text": "Qualcomm arm cadence Nvidia and Xilinx",
    "start": "72630",
    "end": "75960"
  },
  {
    "text": "FPGA s and today we're also going to",
    "start": "75960",
    "end": "79049"
  },
  {
    "text": "make neo open-source what this means is",
    "start": "79049",
    "end": "86159"
  },
  {
    "text": "that other vendors can take the deep",
    "start": "86159",
    "end": "88439"
  },
  {
    "text": "learning compiler and the Associated",
    "start": "88439",
    "end": "90060"
  },
  {
    "text": "runtime and add their own optimizations",
    "start": "90060",
    "end": "92369"
  },
  {
    "text": "it means that as new neural networks",
    "start": "92369",
    "end": "94409"
  },
  {
    "text": "become available they can just start",
    "start": "94409",
    "end": "96030"
  },
  {
    "text": "getting added in real time to the",
    "start": "96030",
    "end": "97950"
  },
  {
    "text": "compiler into the runtime and it means",
    "start": "97950",
    "end": "99869"
  },
  {
    "text": "that is free to incorporate in any app",
    "start": "99869",
    "end": "102090"
  },
  {
    "text": "for any device and so we're able to move",
    "start": "102090",
    "end": "105810"
  },
  {
    "text": "more data to the algorithms for faster",
    "start": "105810",
    "end": "108299"
  },
  {
    "text": "computation both in the cloud and at the",
    "start": "108299",
    "end": "110759"
  },
  {
    "text": "edge and what that means is you no",
    "start": "110759",
    "end": "112890"
  },
  {
    "text": "longer have to go find the researchers",
    "start": "112890",
    "end": "114750"
  },
  {
    "text": "you no longer have to write the research",
    "start": "114750",
    "end": "116880"
  },
  {
    "text": "proposals you no longer have to hand",
    "start": "116880",
    "end": "118649"
  },
  {
    "text": "over those huge checks and you no longer",
    "start": "118649",
    "end": "120509"
  },
  {
    "text": "have to wait as all of that hardware is",
    "start": "120509",
    "end": "122880"
  },
  {
    "text": "delivered to your loading dock as you",
    "start": "122880",
    "end": "124770"
  },
  {
    "text": "rack stack and configure and eventually",
    "start": "124770",
    "end": "127290"
  },
  {
    "text": "build models and this will allow you to",
    "start": "127290",
    "end": "130259"
  },
  {
    "text": "build better models in less time",
    "start": "130259",
    "end": "132420"
  },
  {
    "text": "[Music]",
    "start": "132420",
    "end": "153980"
  }
]