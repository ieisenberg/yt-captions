[
  {
    "text": "thanks for joining we're going to hear some great stories from our customers today about how they're using Amazon s3",
    "start": "0",
    "end": "9480"
  },
  {
    "text": "to build big data platforms and s3 is such a fundamental part of our big data",
    "start": "9480",
    "end": "16020"
  },
  {
    "text": "ecosystem that it's great to hear these real stories and we're going to hear from red fin and Twitter about what",
    "start": "16020",
    "end": "22920"
  },
  {
    "text": "they're doing and just to set the the scene you know s3 is an object storage",
    "start": "22920",
    "end": "28199"
  },
  {
    "text": "platform it's designed for very high durability 11 nines of durability and",
    "start": "28199",
    "end": "33690"
  },
  {
    "text": "what that really means is out of ten thousand files you store them on s3 you could theoretically lose one of those",
    "start": "33690",
    "end": "39840"
  },
  {
    "text": "files in 10 million years so we would talk about durability this is a system that's really designed to hold all of",
    "start": "39840",
    "end": "45870"
  },
  {
    "text": "your data to scale automatically and to give you extreme confidence that your",
    "start": "45870",
    "end": "51480"
  },
  {
    "text": "data will be available and it's integrated with our big data services at a very low level it's also designed for",
    "start": "51480",
    "end": "58280"
  },
  {
    "text": "allowing you to manage the cost very effectively so around three cents a gig",
    "start": "58280",
    "end": "63840"
  },
  {
    "text": "for standard redundancy in s3 and then",
    "start": "63840",
    "end": "70110"
  },
  {
    "text": "we've recently launched a new storage class in frequent access again still",
    "start": "70110",
    "end": "75750"
  },
  {
    "text": "gives you 11 nines of durability significantly lowers the cost while allowing you to retain that data online",
    "start": "75750",
    "end": "81830"
  },
  {
    "text": "but for data that you don't access very frequently and then there's an additional charge if you do have to",
    "start": "81830",
    "end": "87119"
  },
  {
    "text": "bring it back and then of course there's Amazon glacier for very cold data that",
    "start": "87119",
    "end": "93210"
  },
  {
    "text": "you need to store for a very very low cost three to five hours retrieval time and s3 will take care of managing this",
    "start": "93210",
    "end": "99210"
  },
  {
    "text": "life cycle for you sitting underneath your big data platform another feature",
    "start": "99210",
    "end": "104220"
  },
  {
    "text": "of s3 is that you can use it to generate events whether that's object lost object",
    "start": "104220",
    "end": "111810"
  },
  {
    "text": "created and then consume that with AWS services like lambda API gateway SNS SQS",
    "start": "111810",
    "end": "121290"
  },
  {
    "text": "and so on and really allow the data storage system to sit at the heart of your business so some use cases that we",
    "start": "121290",
    "end": "129629"
  },
  {
    "text": "would like you to consider for your big data applications the first one would be 3 is your primary store for all things",
    "start": "129629",
    "end": "136200"
  },
  {
    "text": "data related s3 is integrated with tools like dynamo DB redshift lambda EMR data",
    "start": "136200",
    "end": "142799"
  },
  {
    "text": "pipeline kinesis machine learning quick site and really sits at the heart of",
    "start": "142799",
    "end": "148230"
  },
  {
    "text": "your data management problem one of the",
    "start": "148230",
    "end": "153269"
  },
  {
    "text": "ways that people we use this as a replacement for an HDFS environment we've significantly invested in",
    "start": "153269",
    "end": "161040"
  },
  {
    "text": "functionality within elastic MapReduce that allows you to treat s3 in the same",
    "start": "161040",
    "end": "166319"
  },
  {
    "text": "way that you would treat HDFS with significantly higher performance than you would get from the native Hadoop",
    "start": "166319",
    "end": "173510"
  },
  {
    "text": "implementations of the s3 driver and for example we offer you amazon EMR FS the",
    "start": "173510",
    "end": "179730"
  },
  {
    "text": "elastic MapReduce file system which indexes s3 data and DynamoDB to give you",
    "start": "179730",
    "end": "185010"
  },
  {
    "text": "about twice the performance you would get from using s3 raw and that's accompanied with a command line client",
    "start": "185010",
    "end": "190790"
  },
  {
    "text": "that allows you to do things like manually syncing your data creating",
    "start": "190790",
    "end": "196170"
  },
  {
    "text": "differences between the EMR FS index and so on and this pattern allows you then",
    "start": "196170",
    "end": "203220"
  },
  {
    "text": "to do some very interesting things about you know using s3 as your primary store",
    "start": "203220",
    "end": "209940"
  },
  {
    "text": "and HDFS just for temporary storage between individual jobs that you're running and this allows you then to shut",
    "start": "209940",
    "end": "218340"
  },
  {
    "text": "down clusters when they're no longer running and no longer needed and this saves you a lot of money while retaining",
    "start": "218340",
    "end": "225150"
  },
  {
    "text": "the data with 11 nines of durability you also don't have to think about scaling of your HDFS environment and how do you",
    "start": "225150",
    "end": "233879"
  },
  {
    "text": "provide enough cores and enough RAM to keep up with your throughput requirements because s3 will scale",
    "start": "233879",
    "end": "239970"
  },
  {
    "text": "automatically for you and we have customers who shared evidence of seeing",
    "start": "239970",
    "end": "244980"
  },
  {
    "text": "seventy eighty percent linear scalability serializing data at two terabytes a minute out of s3 it also",
    "start": "244980",
    "end": "254579"
  },
  {
    "text": "allows you to really simply share data between Hadoop clusters in a way that doing it with HDFS is just very",
    "start": "254579",
    "end": "260760"
  },
  {
    "text": "complicated to do and so whether you're creating one cluster that's on a cpu",
    "start": "260760",
    "end": "267310"
  },
  {
    "text": "optimized family generating some data on to s3 and then consuming that into",
    "start": "267310",
    "end": "272740"
  },
  {
    "text": "another tool you can really mix and match the compute independently of this",
    "start": "272740",
    "end": "279550"
  },
  {
    "text": "storage tier and that allows us to do another interesting use case which is",
    "start": "279550",
    "end": "285340"
  },
  {
    "text": "just for Hadoop in HDFS backups you know elastic MapReduce has a massive",
    "start": "285340",
    "end": "290410"
  },
  {
    "text": "ecosystem of tools that sit around it and a great example of this is HBase",
    "start": "290410",
    "end": "295950"
  },
  {
    "text": "HBase allows you to create incremental backups and s3 is a perfect place to put",
    "start": "295950",
    "end": "301120"
  },
  {
    "text": "those very long term durability very easy to restore those backups out and",
    "start": "301120",
    "end": "307979"
  },
  {
    "text": "quite an obvious solution really i found the clicker the other use case that",
    "start": "307979",
    "end": "316360"
  },
  {
    "text": "we're seeing more and more of is being able to treat us three as a data-centric eventbus what do we mean by that we're",
    "start": "316360",
    "end": "322810"
  },
  {
    "text": "taking advantage of these events and allowing us to drive your business workflows from the events that are",
    "start": "322810",
    "end": "329680"
  },
  {
    "text": "occurring on s3 itself so a great example that you might consider using is",
    "start": "329680",
    "end": "335280"
  },
  {
    "text": "one where you build an ec2 instance to service incoming data feeds with",
    "start": "335280",
    "end": "342940"
  },
  {
    "text": "something like SFTP that ec2 instance can use something like s three FS the s3",
    "start": "342940",
    "end": "349150"
  },
  {
    "text": "file system so that those files that are coming in through ftp are never sitting",
    "start": "349150",
    "end": "354310"
  },
  {
    "text": "on local instant storage they're just being sent directly through to s3 and",
    "start": "354310",
    "end": "361080"
  },
  {
    "text": "then we can use AWS lambda to pick up those file events and we have an example",
    "start": "361080",
    "end": "368560"
  },
  {
    "text": "of how you can use AWS lambda to load that data into redshift automatically so",
    "start": "368560",
    "end": "376060"
  },
  {
    "text": "this would be a serverless infrastructure for taking ftp input and just loading it into a database and",
    "start": "376060",
    "end": "382150"
  },
  {
    "text": "that's a really nice idea for how you could create s3 is the heart of the workflow and then build these",
    "start": "382150",
    "end": "389229"
  },
  {
    "text": "surveillance architectures that sit around it so I like to introduce our",
    "start": "389229",
    "end": "395740"
  },
  {
    "text": "first customer speaker redfin is a real estate management company and we're very pleased to invite them up",
    "start": "395740",
    "end": "402950"
  },
  {
    "text": "to talk about how they're using s3 and some of these patterns to go faster and do smarter and more interesting things",
    "start": "402950",
    "end": "409220"
  },
  {
    "text": "thank you thanks Ian so before I start I",
    "start": "409220",
    "end": "417230"
  },
  {
    "text": "just want to a general pool how many of you guys have heard about redfin all",
    "start": "417230",
    "end": "422420"
  },
  {
    "text": "right thank you for those who hadn't heard about us we are a real estate",
    "start": "422420",
    "end": "427460"
  },
  {
    "text": "broker we started the 2006 in Seattle and starting as a technology company",
    "start": "427460",
    "end": "434900"
  },
  {
    "text": "over the years we find hey where'd she need a agent so now we actually have",
    "start": "434900",
    "end": "441080"
  },
  {
    "text": "full time agents working for us try to help people buy and sell houses myself",
    "start": "441080",
    "end": "451160"
  },
  {
    "text": "actually I manage the big data analytics team we have a bunch of data scientists",
    "start": "451160",
    "end": "456260"
  },
  {
    "text": "and sovereign teen years building a base Big Data Platform bigger part for us is",
    "start": "456260",
    "end": "462920"
  },
  {
    "text": "trying to answer the questions some of the questions about what does the user",
    "start": "462920",
    "end": "468080"
  },
  {
    "text": "want right who they are and what do they need and how can we be helpful for the",
    "start": "468080",
    "end": "475220"
  },
  {
    "text": "home buying or selling it's a very emotional process when you are first",
    "start": "475220",
    "end": "481700"
  },
  {
    "text": "home first time hope I you know it's usually the largest purchase in your life you don't do it very often and you",
    "start": "481700",
    "end": "489440"
  },
  {
    "text": "are trying to figure out where is the place you're going to stay for a long period of time so we're trying to figure",
    "start": "489440",
    "end": "495470"
  },
  {
    "text": "out a way to help our users to undead if",
    "start": "495470",
    "end": "501590"
  },
  {
    "text": "I find social homes and then once they actually find out at home they like we",
    "start": "501590",
    "end": "506660"
  },
  {
    "text": "have agent to help through them to go get you into the house and tour the",
    "start": "506660",
    "end": "511700"
  },
  {
    "text": "house negotiate and the close a deal our our model is like it's different quite",
    "start": "511700",
    "end": "519830"
  },
  {
    "text": "different from the traditional brokers our agent got paid based on the customer",
    "start": "519830",
    "end": "524900"
  },
  {
    "text": "satisfaction and we don't get paid Bay sound sort of any advertising wee bit we",
    "start": "524900",
    "end": "532070"
  },
  {
    "text": "got paid one we actually close a deal the customer and now agent get paid based on the satisfaction so if you guys",
    "start": "532070",
    "end": "539480"
  },
  {
    "text": "come back to say hey close the deal with the reference agent young I come back say hey yummy really sucks you know I",
    "start": "539480",
    "end": "546140"
  },
  {
    "text": "don't want to work with you anymore I've got very little bonus but if you guys give me a very good review I got a lot",
    "start": "546140",
    "end": "552649"
  },
  {
    "text": "of mobile us so that's actually our effort trying to align this incentive of the agent with their customers so how do",
    "start": "552649",
    "end": "561050"
  },
  {
    "text": "we how do we it doesn't look at oh so the fundamental a our team is trying to",
    "start": "561050",
    "end": "568550"
  },
  {
    "text": "figure out to understand who are the users are what do they want and how can we be helpful and it really is a kind of",
    "start": "568550",
    "end": "575570"
  },
  {
    "text": "a matchmaking business we're doing here right the way we see real estate at this stage is that you need you need to know",
    "start": "575570",
    "end": "583820"
  },
  {
    "text": "about the users you need to know about the properties and also you know when together agent engaged to how close the",
    "start": "583820",
    "end": "591260"
  },
  {
    "text": "deal we have a lot of data about our users I didn't look up too well but we",
    "start": "591260",
    "end": "598550"
  },
  {
    "text": "have millions of users generate the billions of events every month and the",
    "start": "598550",
    "end": "606170"
  },
  {
    "text": "user also will have agents thousands of agents across the country we have 18",
    "start": "606170",
    "end": "611600"
  },
  {
    "text": "markets in us that taking the users to",
    "start": "611600",
    "end": "617209"
  },
  {
    "text": "see houses open houses and close deals there's a lot of the agent generated",
    "start": "617209",
    "end": "624740"
  },
  {
    "text": "data for properties we have imported the data about properties over all about 100",
    "start": "624740",
    "end": "631760"
  },
  {
    "text": "million u.s. properties and there's a ton of data about the neighborhood and",
    "start": "631760",
    "end": "637120"
  },
  {
    "text": "anything this is so history a lot of",
    "start": "637120",
    "end": "642170"
  },
  {
    "text": "things about the properties so what we're trying to do is to do the mesh making work for then you have tons of",
    "start": "642170",
    "end": "649640"
  },
  {
    "text": "data and value trying to January inside right try to understand what does people really want it so what we did is a",
    "start": "649640",
    "end": "657199"
  },
  {
    "text": "beautiful at forum to process all the data there's a fundamentally two ways of",
    "start": "657199",
    "end": "664399"
  },
  {
    "text": "doing that why is the batch processing part is the real time processing these are kind of like the lambda architecture",
    "start": "664399",
    "end": "671480"
  },
  {
    "text": "it's slightly different but I can focus",
    "start": "671480",
    "end": "676490"
  },
  {
    "text": "a little more on the batch processing aspects so start with the raw data you have data comes in we use our entire",
    "start": "676490",
    "end": "684650"
  },
  {
    "text": "platform is built on AWS we have all the",
    "start": "684650",
    "end": "691070"
  },
  {
    "text": "raw data in s3 and all the compute we're using EMR ec2 and all the front end",
    "start": "691070",
    "end": "698660"
  },
  {
    "text": "we're using dynamodb redshift or s3 so",
    "start": "698660",
    "end": "703940"
  },
  {
    "text": "the overall design it's like what am I mentioned we use s3 to store data to",
    "start": "703940",
    "end": "711410"
  },
  {
    "text": "produce the data and all the process data and we use EMR full compute but we",
    "start": "711410",
    "end": "718040"
  },
  {
    "text": "we have the design that is we don't have any persistent ec2 instances we don't",
    "start": "718040",
    "end": "724070"
  },
  {
    "text": "have any of none of our EMR clusters is persistent so we only use it as a week",
    "start": "724070",
    "end": "730630"
  },
  {
    "text": "as we needed you it's kind of reminder that the one of the principles we're",
    "start": "730630",
    "end": "736760"
  },
  {
    "text": "thinking is kind of like a databases you know for those of you guys familiar there's this asset my principles atomic",
    "start": "736760",
    "end": "743750"
  },
  {
    "text": "consistent isolation and the durable we're able to do that using as three to",
    "start": "743750",
    "end": "749780"
  },
  {
    "text": "achieve the durability piece so all the data comes in we produce it is three when we process it we save it it back to",
    "start": "749780",
    "end": "757310"
  },
  {
    "text": "s3 so that's a general pattern and every single task we do it has its only",
    "start": "757310",
    "end": "763430"
  },
  {
    "text": "environment starting from the EMR cluster and the bootstrap with the",
    "start": "763430",
    "end": "768710"
  },
  {
    "text": "software we did we would deploy on it process it once we finish we save the data back to s3 now we shut down the",
    "start": "768710",
    "end": "775550"
  },
  {
    "text": "cluster we use we use Swati instance a",
    "start": "775550",
    "end": "782710"
  },
  {
    "text": "ec2 instances so for those of you know that spot instances actually really",
    "start": "782710",
    "end": "788800"
  },
  {
    "text": "depends on how the market goes so your incident scoop you got killed at any time and that's one of the challenge we",
    "start": "788800",
    "end": "795890"
  },
  {
    "text": "are facing you know we want to go cheap many to the cost the same time we want to make sure that we get our job down the way we do",
    "start": "795890",
    "end": "804620"
  },
  {
    "text": "that is we stand oh we build a spot instance bidding strategy the idea is",
    "start": "804620",
    "end": "811430"
  },
  {
    "text": "actually I'm going to predict how likely a certain instances going to be called so the instant havoc in the class and",
    "start": "811430",
    "end": "816680"
  },
  {
    "text": "bid for that and if I got the resource I will run my job on top of it if I'm",
    "start": "816680",
    "end": "823790"
  },
  {
    "text": "lucky I'll finish my job and I sat down my cluster okay if I'm unlucky and",
    "start": "823790",
    "end": "829850"
  },
  {
    "text": "sometimes we are but in the middle of the job the ec2 instance are gone because someone else up with me then the",
    "start": "829850",
    "end": "837290"
  },
  {
    "text": "cluster will be be sat down and the task itself will rerun we're actually going",
    "start": "837290",
    "end": "843560"
  },
  {
    "text": "to ditch everything in between start from scratch right this is how we achieved sort of like a",
    "start": "843560",
    "end": "848600"
  },
  {
    "text": "transaction-based idea so it's a very simple sort of design but they also",
    "start": "848600",
    "end": "855080"
  },
  {
    "text": "empowered us two buildings very it's very easy for us to build on top of it",
    "start": "855080",
    "end": "861320"
  },
  {
    "text": "you really don't have too many any state of your processing you use s three as the checkpoint for your output for input",
    "start": "861320",
    "end": "869390"
  },
  {
    "text": "and then you use the EMR for compute so I get a white example about how we use",
    "start": "869390",
    "end": "876410"
  },
  {
    "text": "this pipeline what do we do with it in",
    "start": "876410",
    "end": "881510"
  },
  {
    "text": "general we have a couple of different services why is actually do some user",
    "start": "881510",
    "end": "888080"
  },
  {
    "text": "profiling you know understand the users the scoring what do they like or give a recommendation about the user will house",
    "start": "888080",
    "end": "893540"
  },
  {
    "text": "be like another part on the property aspect will predict how likely a home will be sold we call that hot homes so",
    "start": "893540",
    "end": "901940"
  },
  {
    "text": "this is a one of the features that we released last year the general idea is actually give any properties on the",
    "start": "901940",
    "end": "908060"
  },
  {
    "text": "market at a given price I'm trying to predict hey how likely you'll be sold",
    "start": "908060",
    "end": "913070"
  },
  {
    "text": "within academia days this essentially the one of the time I'm trying to do",
    "start": "913070",
    "end": "918380"
  },
  {
    "text": "give a little bit more insight to the buyers this is mostly a buyer feature so",
    "start": "918380",
    "end": "923450"
  },
  {
    "text": "in a hot market I think for example in the Bay Area on Seattle you're going to",
    "start": "923450",
    "end": "929150"
  },
  {
    "text": "say you know the house come on top and you really want to know why should I as you go Tory that",
    "start": "929150",
    "end": "934310"
  },
  {
    "text": "what I put in an offer so they forgive a little bit insight about how likely the home can be sold so how do we do this",
    "start": "934310",
    "end": "943660"
  },
  {
    "text": "I'll give you a little preview how we do it and don't try to read through it it's",
    "start": "943660",
    "end": "950360"
  },
  {
    "text": "not intend to be read through in general every single dot an old you see there is",
    "start": "950360",
    "end": "957380"
  },
  {
    "text": "a single task so for hot homes this is the whole workflow of the hot home",
    "start": "957380",
    "end": "963110"
  },
  {
    "text": "feature every single task starting from the left to right it's the different",
    "start": "963110",
    "end": "969279"
  },
  {
    "text": "tasks with processing data we either ingesting the raw data always rebuilding",
    "start": "969279",
    "end": "976000"
  },
  {
    "text": "features which tracking features and then we may be other machine learning models the hull home models and all we",
    "start": "976000",
    "end": "983150"
  },
  {
    "text": "actually doing a scoring or publishing the result to the front a dynamo dB those are the sort of the overall",
    "start": "983150",
    "end": "989720"
  },
  {
    "text": "workflow every single node is a single task that has is independent in a way",
    "start": "989720",
    "end": "996110"
  },
  {
    "text": "that can be retry can be redrive so the whole thing can be automated so that's",
    "start": "996110",
    "end": "1003700"
  },
  {
    "text": "the general all the motion a new feature we have this type of pipeline so some of",
    "start": "1003700",
    "end": "1009010"
  },
  {
    "text": "the real-time page cases we have a different pipeline for batch processes pipeline basically there's no not a",
    "start": "1009010",
    "end": "1014830"
  },
  {
    "text": "single ec2 instance are persistent and everything's are producing down s3 so",
    "start": "1014830",
    "end": "1024280"
  },
  {
    "text": "again just trying to go through the current path what we have for the batch processing we use s3 to staging the data",
    "start": "1024280",
    "end": "1031688"
  },
  {
    "text": "to save the data we use the EMR for compute and we use a dynamo DB for",
    "start": "1031689",
    "end": "1037000"
  },
  {
    "text": "front-end and we'll use arrested four people at the data warehouse use cases for real-time pipeline will use SQS we",
    "start": "1037000",
    "end": "1044589"
  },
  {
    "text": "use a Kinesis to store the streams there's a user stream there's a profit stream there's agent string and then we",
    "start": "1044589",
    "end": "1051490"
  },
  {
    "text": "build on top of that to enable some of the features like a be testing real-time personalization and marketing stuff so",
    "start": "1051490",
    "end": "1060760"
  },
  {
    "text": "so trying to summarize how we use you know how you use the storage for our and",
    "start": "1060760",
    "end": "1065770"
  },
  {
    "text": "another pipeline essentially EMR is too it's all temporary data way we don't",
    "start": "1065770",
    "end": "1071080"
  },
  {
    "text": "we don't use it at March and we use s3 as a permanent primary data store SQS",
    "start": "1071080",
    "end": "1078549"
  },
  {
    "text": "Kinesis is the event bus is the stream data store DynamoDB we use that as a",
    "start": "1078549",
    "end": "1084220"
  },
  {
    "text": "prime production caching layer is in essence it's not all the data being",
    "start": "1084220",
    "end": "1089740"
  },
  {
    "text": "replaces every day every hour sometime by seconds so how billions of records on",
    "start": "1089740",
    "end": "1094960"
  },
  {
    "text": "primary DynamoDB but it's issues of a caching layer provided a low latency",
    "start": "1094960",
    "end": "1101860"
  },
  {
    "text": "high availability later for us but redshift this is for people you know people who familiar with SQL sequel they",
    "start": "1101860",
    "end": "1108789"
  },
  {
    "text": "can just analyst p.m. business user they can't require the data directly on red shift so how do we use s3 we use that",
    "start": "1108789",
    "end": "1116259"
  },
  {
    "text": "for raw data staging data checkpoint and production everything need to be",
    "start": "1116259",
    "end": "1123549"
  },
  {
    "text": "republished able read rival from s3 so that's that's the fundamental design Oh what we have so why s3 actually it's",
    "start": "1123549",
    "end": "1133659"
  },
  {
    "text": "very simple and easy for us to use right and the worst scalable we don't have to",
    "start": "1133659",
    "end": "1138820"
  },
  {
    "text": "worry about skinning and the bigger part number one for us is a highly durable since we're the whole designs of build",
    "start": "1138820",
    "end": "1145779"
  },
  {
    "text": "upon that the data is there why right Wes 3i I can be you know assured that",
    "start": "1145779",
    "end": "1151360"
  },
  {
    "text": "there's very low a probability that will be lost you know 11-9 sort of durability",
    "start": "1151360",
    "end": "1157500"
  },
  {
    "text": "and another nice thing is really easier for a small team like us there's a lot",
    "start": "1157500",
    "end": "1163929"
  },
  {
    "text": "of integration building you know I don't have the connectors with the dynamo DB",
    "start": "1163929",
    "end": "1169360"
  },
  {
    "text": "with the rest shape with the canisius there's a lot of things amazon i tribute a long s3 which actually just provide a",
    "start": "1169360",
    "end": "1176740"
  },
  {
    "text": "lot of automation and just free stuff for us to use so some of the tips are",
    "start": "1176740",
    "end": "1182529"
  },
  {
    "text": "lesson learned the number one license for us and the most important message I want to say is actually use s3 as the",
    "start": "1182529",
    "end": "1189850"
  },
  {
    "text": "trusted primary physician store okay that's the number one thing we learned",
    "start": "1189850",
    "end": "1195450"
  },
  {
    "text": "other little things since we're small team and every engineer is that",
    "start": "1195450",
    "end": "1202940"
  },
  {
    "text": "they do things quick and I will by design we don't really don't have a very",
    "start": "1202940",
    "end": "1207960"
  },
  {
    "text": "rigorous process so every now then we have people to say oops young I did need some of the data that's okay what do we",
    "start": "1207960",
    "end": "1213960"
  },
  {
    "text": "do we can redrive it if it's just some small amount of data it's very easy to",
    "start": "1213960",
    "end": "1220320"
  },
  {
    "text": "drive with not a big problem what do you delete a like the past three years two years of data yes in principle you can",
    "start": "1220320",
    "end": "1228240"
  },
  {
    "text": "redrive those is it going to take a long time it's going to take us a long time",
    "start": "1228240",
    "end": "1233760"
  },
  {
    "text": "the divider to manually right so we really don't want to some of the data we",
    "start": "1233760",
    "end": "1239190"
  },
  {
    "text": "really don't want to lose it so we come out of design say hey why don't we do this just kick take a copy of the the",
    "start": "1239190",
    "end": "1246149"
  },
  {
    "text": "buckets that we really care about and sieve into another bucket so okay let's do that so we did that and then we look",
    "start": "1246149",
    "end": "1253799"
  },
  {
    "text": "at our bill there oops our three castles doubled so I guess that's by design and",
    "start": "1253799",
    "end": "1260240"
  },
  {
    "text": "then later how we learn hey there's a cross region replication we really don't have to do the whole thing ourselves but",
    "start": "1260240",
    "end": "1266970"
  },
  {
    "text": "we did that already then there are other conversation maybe we should try a three versioning that turns out to be the one",
    "start": "1266970",
    "end": "1272970"
  },
  {
    "text": "that we're using now what happened is actually for those cases people actually accidentally to do something we can",
    "start": "1272970",
    "end": "1279360"
  },
  {
    "text": "always say hey user as reverting pick it up and then recover from that luckily we",
    "start": "1279360",
    "end": "1285659"
  },
  {
    "text": "haven't done that too often so I think of the team getting a little more mature another lesson we learned it's really",
    "start": "1285659",
    "end": "1294000"
  },
  {
    "text": "how to organize your data with s3 when we started to pretty small right we just",
    "start": "1294000",
    "end": "1299360"
  },
  {
    "text": "kind of a random organized the different buckets and how we name the objects so",
    "start": "1299360",
    "end": "1306659"
  },
  {
    "text": "the bucket level really we have some of high levels or breakdowns things like hey some of the raw data we you know",
    "start": "1306659",
    "end": "1313710"
  },
  {
    "text": "someone is actually external party drop-off data to us we have dedicated bucket for that for some of the",
    "start": "1313710",
    "end": "1319409"
  },
  {
    "text": "intermediate results those machine learning sort of the processed data the models we want to produce into a",
    "start": "1319409",
    "end": "1326210"
  },
  {
    "text": "different buckets and we break it out a bucket by by feature but within",
    "start": "1326210",
    "end": "1333760"
  },
  {
    "text": "each bucket the naming convention because a lot of data we have is partitioned by day we just do something",
    "start": "1333760",
    "end": "1339940"
  },
  {
    "text": "like for example prefixes like a hot home / 2015 09 01 what we later learned",
    "start": "1339940",
    "end": "1348340"
  },
  {
    "text": "is I she says we actually that I works well until we have some tables some of",
    "start": "1348340",
    "end": "1355690"
  },
  {
    "text": "the high tables have like 50,000 100,000 partitions it takes a long time to load",
    "start": "1355690",
    "end": "1361360"
  },
  {
    "text": "the table and you can see cases that it got time out that you can even though",
    "start": "1361360",
    "end": "1366430"
  },
  {
    "text": "the table properly so those are the things that is really hard to change later so unfortunately get stuck with it",
    "start": "1366430",
    "end": "1373930"
  },
  {
    "text": "we have other ways to solve the problem so when you design your bucket and when",
    "start": "1373930",
    "end": "1379510"
  },
  {
    "text": "your name your object figure the way think think ahead of time you know as a gold as three if it had become your",
    "start": "1379510",
    "end": "1386890"
  },
  {
    "text": "primary datastore they will be more data to it and over time you're going to hate on some of the limitations so talk about",
    "start": "1386890",
    "end": "1394150"
  },
  {
    "text": "those limit we learn every single limit I guess we hit on almost the single put",
    "start": "1394150",
    "end": "1400270"
  },
  {
    "text": "is a file data limit we learn that and the single object have a file terabytes",
    "start": "1400270",
    "end": "1407920"
  },
  {
    "text": "I think the limit there's also hundred buckets that you can have four per",
    "start": "1407920",
    "end": "1413830"
  },
  {
    "text": "account I believe you can talk to s3 and ask them to increase the limit that's the kind of soft limit there's a couple",
    "start": "1413830",
    "end": "1422020"
  },
  {
    "text": "other limit with the EMR FS for example you can turn around a consistent view if",
    "start": "1422020",
    "end": "1429520"
  },
  {
    "text": "you don't turn it on there's my big cases that you may see you know eventual",
    "start": "1429520",
    "end": "1435190"
  },
  {
    "text": "consistent coming play will be say hey I write the data there how do i why it take so long for me to show to look at",
    "start": "1435190",
    "end": "1441550"
  },
  {
    "text": "my object but if you turn on the sort of even a consistent view you should be",
    "start": "1441550",
    "end": "1448900"
  },
  {
    "text": "able to gather data quickly another part we learned is actually to set our",
    "start": "1448900",
    "end": "1454210"
  },
  {
    "text": "monitors so we have a bill distrust with s3 we sort of ignored it and over time",
    "start": "1454210",
    "end": "1461560"
  },
  {
    "text": "if I all you know there's a lot of data we put there and some of the data really we don't use it the air",
    "start": "1461560",
    "end": "1467710"
  },
  {
    "text": "we just save it there so that include a lot of cost so we did is actually builds our monitors and then we actually",
    "start": "1467710",
    "end": "1474039"
  },
  {
    "text": "defined the data retention policy you can leverage let's read lifecycle management to glyceride some of the data",
    "start": "1474039",
    "end": "1480760"
  },
  {
    "text": "or delete some the data that you don't need now I should drive your costs down so that's what I have and the red fern",
    "start": "1480760",
    "end": "1488350"
  },
  {
    "text": "basically we're a technology company we were also a full-service broker so use",
    "start": "1488350",
    "end": "1496149"
  },
  {
    "text": "us if you have any questions if you're interested learning how we do things in details go find me or email me right",
    "start": "1496149",
    "end": "1503799"
  },
  {
    "text": "thank you thank you very much that's",
    "start": "1503799",
    "end": "1511480"
  },
  {
    "text": "great really some good lessons learned there and wanted to follow on from that",
    "start": "1511480",
    "end": "1517840"
  },
  {
    "text": "and talk about some best practices that we would encourage you to use with s3 and I think we're going to be touching",
    "start": "1517840",
    "end": "1524740"
  },
  {
    "text": "definitely on some of what Young was talking about so you know when you consider storing data and s3 you have to",
    "start": "1524740",
    "end": "1531159"
  },
  {
    "text": "consider that of course not all data is going to need the same sort of storage",
    "start": "1531159",
    "end": "1536789"
  },
  {
    "text": "format so unstructured data versus semi-structured data versus fully structured data and you know you should",
    "start": "1536789",
    "end": "1543610"
  },
  {
    "text": "really consider choosing a format that supports and is sympathetic to the type",
    "start": "1543610",
    "end": "1548830"
  },
  {
    "text": "of data that you're storing rather than having this idea that everything has to be turned into one and only one format a",
    "start": "1548830",
    "end": "1554039"
  },
  {
    "text": "good example would be if you choose to store some structured data in a columnar",
    "start": "1554039",
    "end": "1559960"
  },
  {
    "text": "format as soon as you just try and use that columnar format for log files it starts to get very messy we also",
    "start": "1559960",
    "end": "1565990"
  },
  {
    "text": "recommend that you store a copy of your raw input whatever that means to you whatever you receive data from an",
    "start": "1565990",
    "end": "1572740"
  },
  {
    "text": "application or a system definitely store a copy of that because it allows you as Young said that redrive your processes",
    "start": "1572740",
    "end": "1579460"
  },
  {
    "text": "from source if needed and ideally you would never do that but as you build",
    "start": "1579460",
    "end": "1585100"
  },
  {
    "text": "these workflows you absolutely could use that initial raw data set for testing",
    "start": "1585100",
    "end": "1592029"
  },
  {
    "text": "new ideas and so on the data standardization that you would want to",
    "start": "1592029",
    "end": "1598179"
  },
  {
    "text": "put in place are going to be things like common compression or for all types of CSV data you",
    "start": "1598179",
    "end": "1604809"
  },
  {
    "text": "definitely would want to standardize on a storage format for that and we would",
    "start": "1604809",
    "end": "1610509"
  },
  {
    "text": "encourage you also to consider how your data is going to evolve over time some",
    "start": "1610509",
    "end": "1616059"
  },
  {
    "text": "data formats allow you to have evolvable schemas so that you can continue to",
    "start": "1616059",
    "end": "1621669"
  },
  {
    "text": "consume data with a an updated schema definition even if all data doesn't fit that that definition which is very",
    "start": "1621669",
    "end": "1628179"
  },
  {
    "text": "difficult to do with CSV for example so for unstructured data you know your your",
    "start": "1628179",
    "end": "1634029"
  },
  {
    "text": "native log files that you're getting from applications we just encourage you uses a standard codec for compressing",
    "start": "1634029",
    "end": "1640899"
  },
  {
    "text": "that and consider for big data workloads that that should be a streaming codec like lzo snappy bzip2 for",
    "start": "1640899",
    "end": "1648999"
  },
  {
    "text": "semi-structured data Jason XML this is an area where you definitely should consider how that data is going to",
    "start": "1648999",
    "end": "1654969"
  },
  {
    "text": "change over time and potentially use a data format that will allow that change",
    "start": "1654969",
    "end": "1660429"
  },
  {
    "text": "to be easily consumed by your big data applications and of course lots of structure data is csv today and that's",
    "start": "1660429",
    "end": "1667329"
  },
  {
    "text": "how we receive it that doesn't mean that's the only way we can store it so colombier storage formats like or can",
    "start": "1667329",
    "end": "1672639"
  },
  {
    "text": "park a can give you massive performance gains but again we would absolutely",
    "start": "1672639",
    "end": "1677679"
  },
  {
    "text": "expect you keep a CSV copy of that in case you ever wanted to change from one",
    "start": "1677679",
    "end": "1682929"
  },
  {
    "text": "format to another so where you store that on s3 is very important as young touched on as well so you know s3 is a",
    "start": "1682929",
    "end": "1690399"
  },
  {
    "text": "flat key space it doesn't have the concept of a directory so if you want to rename the path for a petabyte of data",
    "start": "1690399",
    "end": "1696969"
  },
  {
    "text": "you're going to have to go through and rename every single file not going to come very expensive so thinking about",
    "start": "1696969",
    "end": "1703089"
  },
  {
    "text": "the path structure ahead of time is extremely important and we'd like you to",
    "start": "1703089",
    "end": "1708820"
  },
  {
    "text": "consider the idea that you know the natural partitioning of the data including using things like the",
    "start": "1708820",
    "end": "1715239"
  },
  {
    "text": "application that generated the data the time business unit those are all really",
    "start": "1715239",
    "end": "1720489"
  },
  {
    "text": "good things to build into the prefix rather than just having some arbitrary structure and where we can start to",
    "start": "1720489",
    "end": "1726070"
  },
  {
    "text": "think about the idea of representing s3 using a resource oriented architecture",
    "start": "1726070",
    "end": "1732099"
  },
  {
    "text": "this is the idea that on a web application for example every sort of basic business",
    "start": "1732099",
    "end": "1737559"
  },
  {
    "text": "primitive is exposed through your Web API at a path that is self-describing",
    "start": "1737559",
    "end": "1742779"
  },
  {
    "text": "and intuitive and this is a great principle to apply to your data because it lets your data then almost have a",
    "start": "1742779",
    "end": "1750070"
  },
  {
    "text": "metadata index or a catalog built in to where it's stored that will these",
    "start": "1750070",
    "end": "1755350"
  },
  {
    "text": "integration between systems and so on so I'd next like to invite our guests from",
    "start": "1755350",
    "end": "1763179"
  },
  {
    "text": "Twitter to join Joseph and he's going to talk a little bit more about how they've applied some of these best practices and",
    "start": "1763179",
    "end": "1771119"
  },
  {
    "text": "thank you very much I appreciate it and thank you young as well I actually used",
    "start": "1773249",
    "end": "1780009"
  },
  {
    "text": "redfin myself when I was looking at houses last month all right my name is",
    "start": "1780009",
    "end": "1786070"
  },
  {
    "text": "Joseph unruh I'm the tech lead for the tell apart group at Twitter you've probably never heard of tell apart but",
    "start": "1786070",
    "end": "1792490"
  },
  {
    "text": "you've almost certainly interacted with it on most sites that display advertisements we will generally get a",
    "start": "1792490",
    "end": "1799720"
  },
  {
    "text": "notification of that and get a bid request for that advertisement this means that we get around 200,000",
    "start": "1799720",
    "end": "1805960"
  },
  {
    "text": "requests per second and we're adding about 15 to 20 terabytes of data s3 every single day so I want to share some",
    "start": "1805960",
    "end": "1814960"
  },
  {
    "text": "of our experiences how we built in architecture around that scale and how",
    "start": "1814960",
    "end": "1820570"
  },
  {
    "text": "we work with us three strengths work around Esther's weaknesses and how you can use s3 at a similar scale all right",
    "start": "1820570",
    "end": "1829269"
  },
  {
    "text": "so first of all it doesn't Twitter have its own infrastructure why is Twitter on",
    "start": "1829269",
    "end": "1835749"
  },
  {
    "text": "AWS so just five short months ago tell apart was a completely independent",
    "start": "1835749",
    "end": "1841539"
  },
  {
    "text": "company we built our entire infrastructure on top of AWS and s3 and",
    "start": "1841539",
    "end": "1849960"
  },
  {
    "text": "and since the acquisition we have continued have grown on Twitter's",
    "start": "1851909",
    "end": "1857559"
  },
  {
    "text": "infrastructure but we have also continued to grown on top of AWS and we're just one piece of Twitter's AWS",
    "start": "1857559",
    "end": "1864639"
  },
  {
    "text": "usage we run two to three of the other machines at any given time and we're just a part of Twitter's",
    "start": "1864639",
    "end": "1871180"
  },
  {
    "text": "overall presence and generally acquisitions that come into Twitter are on AWS so the better that Twitter",
    "start": "1871180",
    "end": "1877750"
  },
  {
    "text": "supports this the eds use case the better it can integrate new companies into the fold all right so Twitter",
    "start": "1877750",
    "end": "1887520"
  },
  {
    "text": "generally experiences 316 million monthly active users on the website but",
    "start": "1887520",
    "end": "1892720"
  },
  {
    "text": "that's dwarfed by the number of unique visitors overall who aren't logged into",
    "start": "1892720",
    "end": "1897760"
  },
  {
    "text": "the site and even more people see the little Twitter widgets that appear",
    "start": "1897760",
    "end": "1903280"
  },
  {
    "text": "inside of the site's nearly 200 billion in every single quarter so what tell",
    "start": "1903280",
    "end": "1909190"
  },
  {
    "text": "apart does is we help drive knowledge and information about those logged out users as well as serving the off-site",
    "start": "1909190",
    "end": "1916780"
  },
  {
    "text": "advertising for Twitter as a whole we have a lot of expertise in building up cross device and cross-platform user",
    "start": "1916780",
    "end": "1924880"
  },
  {
    "text": "understanding so that we can identify a single user regardless of where they were signing in from so first off if",
    "start": "1924880",
    "end": "1936310"
  },
  {
    "text": "anybody's had the experience of browsing on a merchant website and then browsing around the internet later and seeing",
    "start": "1936310",
    "end": "1943030"
  },
  {
    "text": "those products that you've seen over and over again I do want to apologize but we",
    "start": "1943030",
    "end": "1949120"
  },
  {
    "text": "want to make this experience better we actually believe that advertising should be experienced that people want to see",
    "start": "1949120",
    "end": "1954840"
  },
  {
    "text": "because it's providing useful recommendations and useful information but this demonstrates what a hard",
    "start": "1954840",
    "end": "1961330"
  },
  {
    "text": "problem is and we solve it by making sure that we have the full user identity",
    "start": "1961330",
    "end": "1967840"
  },
  {
    "text": "information we can load the entire history of the user we load profile",
    "start": "1967840",
    "end": "1972880"
  },
  {
    "text": "attributes such as gender number of times they bought things and even when",
    "start": "1972880",
    "end": "1980800"
  },
  {
    "text": "they've returned things to the store after purchasing them online we feed all",
    "start": "1980800",
    "end": "1985870"
  },
  {
    "text": "that information into our bidding models and determine how much to bid and this",
    "start": "1985870",
    "end": "1991210"
  },
  {
    "text": "has to happen in under 100 milliseconds or else we Clips the the data timeline",
    "start": "1991210",
    "end": "1998590"
  },
  {
    "text": "for the networks and this is happening up to 200,000 times every second so we've chosen to",
    "start": "1998590",
    "end": "2005310"
  },
  {
    "text": "use a lambda style architecture to deal with this in our case data is fed from",
    "start": "2005310",
    "end": "2011640"
  },
  {
    "text": "the front end both to our speed layer and our backs layer on the back end we",
    "start": "2011640",
    "end": "2017310"
  },
  {
    "text": "are setting up the user graphs with dupe and setting up user history and we feed",
    "start": "2017310",
    "end": "2024720"
  },
  {
    "text": "all of that into horcrux which is our internal read-only database and we use a",
    "start": "2024720",
    "end": "2030390"
  },
  {
    "text": "read-only database excuse me sorry we usually read-only database because our",
    "start": "2030390",
    "end": "2035970"
  },
  {
    "text": "response time requires that the horcrux lookups happen in a single contiguous",
    "start": "2035970",
    "end": "2041940"
  },
  {
    "text": "disk read and that's really only possible with the read-only datastore in",
    "start": "2041940",
    "end": "2047280"
  },
  {
    "text": "order to make sure that our data stays fresh we also route that data through Kafka and spark where we build naive",
    "start": "2047280",
    "end": "2053908"
  },
  {
    "text": "identity linkages and incremental user history and put that into a speedy be",
    "start": "2053909",
    "end": "2059750"
  },
  {
    "text": "another in-house database it's based off Redis which keeps all that more recent data in memory so between the two of",
    "start": "2059750",
    "end": "2067110"
  },
  {
    "text": "them we get really fast access and at the front end the data gets merged together alright so I want to talk",
    "start": "2067110",
    "end": "2074760"
  },
  {
    "text": "mostly about the batch architecture that's where s three lives and it's instantly where I have the most",
    "start": "2074760",
    "end": "2079950"
  },
  {
    "text": "expertise all right so in the back side our first step is we clean up our data",
    "start": "2079950",
    "end": "2085800"
  },
  {
    "text": "that means filling in our missing fields that means pre joining on all of the",
    "start": "2085800",
    "end": "2091440"
  },
  {
    "text": "data sources that are commonly used together and that means organizing the",
    "start": "2091440",
    "end": "2097440"
  },
  {
    "text": "data in appropriate partitions by our including dealing with late data as it",
    "start": "2097440",
    "end": "2103320"
  },
  {
    "text": "comes in next we build our overall identity graph and then we build our s3",
    "start": "2103320",
    "end": "2110610"
  },
  {
    "text": "I sorry then we build our machine learning models and other recommendation",
    "start": "2110610",
    "end": "2116790"
  },
  {
    "text": "models so all this is happening on thousands of machines over hundreds of",
    "start": "2116790",
    "end": "2122550"
  },
  {
    "text": "jobs we write out 15 to 20 terabytes of new data every single day and we're",
    "start": "2122550",
    "end": "2128580"
  },
  {
    "text": "reading about 100 to 150 terabytes of data s3 every day all right that's",
    "start": "2128580",
    "end": "2137530"
  },
  {
    "text": "architecture so we operate in both the u.s. East one and US west one reason for",
    "start": "2137530",
    "end": "2144309"
  },
  {
    "text": "now in both cases the front ends reading data right up to scribe and publish that data into s3 we use Hadoop to build up",
    "start": "2144309",
    "end": "2152920"
  },
  {
    "text": "our base data sources we use Hadoop plus eight space in order to build up our identity graphs and we use spark to",
    "start": "2152920",
    "end": "2159760"
  },
  {
    "text": "build our machine learning models all that data ends up in s3 and is it",
    "start": "2159760",
    "end": "2164799"
  },
  {
    "text": "published out to horcrux in both regions all right so now I want to talk a little",
    "start": "2164799",
    "end": "2171640"
  },
  {
    "text": "bit about how we use s3 and leveraging strengths and work around some of the weaknesses if we're saying that s 3 is",
    "start": "2171640",
    "end": "2181240"
  },
  {
    "text": "our most widely used service outside of the Amazon ec2 and it is our persistent",
    "start": "2181240",
    "end": "2187089"
  },
  {
    "text": "data store the only other persistent a sore we have any significance is our my",
    "start": "2187089",
    "end": "2193059"
  },
  {
    "text": "sequel clusters and and a couple of RDS instances I've already talked about",
    "start": "2193059",
    "end": "2200069"
  },
  {
    "text": "Hadoop and spark but I also want to add",
    "start": "2200069",
    "end": "2205450"
  },
  {
    "text": "that we use a hive and sparks equal with those as well for we also start our",
    "start": "2205450",
    "end": "2212950"
  },
  {
    "text": "docker containers on top of s3 and we use our docker containers within our patio or deployment apache Aurora is in",
    "start": "2212950",
    "end": "2220450"
  },
  {
    "text": "May Zoe's framework that allows users to schedule jobs and setup services on a",
    "start": "2220450",
    "end": "2227559"
  },
  {
    "text": "common resource pool with arbitrary instances inside of it by by treating",
    "start": "2227559",
    "end": "2235000"
  },
  {
    "text": "things as resources such as CPU disk and memory it attracts the way the instance",
    "start": "2235000",
    "end": "2242799"
  },
  {
    "text": "types it's actually very similar to ET to container store I've talked about",
    "start": "2242799",
    "end": "2249520"
  },
  {
    "text": "Horcrux a little bit already we also use s3 to store our machine learning models which gets populated into our custom",
    "start": "2249520",
    "end": "2257410"
  },
  {
    "text": "green room database finally we use SFTP for storing all of our client data and",
    "start": "2257410",
    "end": "2263290"
  },
  {
    "text": "by putting that ns3 be it means we're essentially unconstrained in our space oftentimes we",
    "start": "2263290",
    "end": "2269540"
  },
  {
    "text": "served as an unofficial backup for our clients when they've lost data and we can make it available to them again like",
    "start": "2269540",
    "end": "2279500"
  },
  {
    "text": "red fin and others we run our batch infrastructure almost entirely from spot",
    "start": "2279500",
    "end": "2286010"
  },
  {
    "text": "and thus actually most of our infrastructures run on the spot this keeps our costs extra low but price",
    "start": "2286010",
    "end": "2293750"
  },
  {
    "text": "spikes mean that we can lose a single machine or even the entire cluster in a very short amount of time and this",
    "start": "2293750",
    "end": "2299510"
  },
  {
    "text": "requires an unusual resilience to errors and this is why our s3 is really",
    "start": "2299510",
    "end": "2304820"
  },
  {
    "text": "important for us because it is our truly are only truly persistent data source",
    "start": "2304820",
    "end": "2310930"
  },
  {
    "text": "we've started to migrate to spot fleet which is Amazon's new sort of general",
    "start": "2310930",
    "end": "2319820"
  },
  {
    "text": "framework for bringing up bringing up spot instances and our initial tests",
    "start": "2319820",
    "end": "2327170"
  },
  {
    "text": "show that we're going to be able to get much better price stability and overall system stability from using it because",
    "start": "2327170",
    "end": "2332420"
  },
  {
    "text": "we can start out with a mix of instance types sometimes s3 just isn't fast",
    "start": "2332420",
    "end": "2339740"
  },
  {
    "text": "enough there's two main use cases that drive our our our caching of the",
    "start": "2339740",
    "end": "2346940"
  },
  {
    "text": "underlying data the first is our querying users like our analysts and",
    "start": "2346940",
    "end": "2355480"
  },
  {
    "text": "people who want to debug underlying issues when people are acquiring the data they expect fast responsive times",
    "start": "2355480",
    "end": "2362270"
  },
  {
    "text": "otherwise they're sitting around waiting for their query result to complete and they're being unproductive you should be",
    "start": "2362270",
    "end": "2368960"
  },
  {
    "text": "able to explore our data quickly so we can extract new information the other major use case is that Amazon s3",
    "start": "2368960",
    "end": "2378550"
  },
  {
    "text": "partitions all of its data based on the key name what this often means is that a",
    "start": "2378550",
    "end": "2384740"
  },
  {
    "text": "single date partition for us gets mapped onto a single s3 key space for them",
    "start": "2384740",
    "end": "2392270"
  },
  {
    "text": "because all within a single date folder if you have many jobs reading from that same data source",
    "start": "2392270",
    "end": "2398670"
  },
  {
    "text": "it's overloading that single partition on s3 and resulting in slow down in data",
    "start": "2398670",
    "end": "2405160"
  },
  {
    "text": "for each of the jobs by keeping a cash on HDFS it ensures that that data is readily accessible the other layer of",
    "start": "2405160",
    "end": "2412960"
  },
  {
    "text": "cash that we have is tachyon currently we're running with about 7 terabytes of in-memory tachyon is another HDFS",
    "start": "2412960",
    "end": "2420460"
  },
  {
    "text": "replacement player and we found that by running sparks equal on top of tachyon",
    "start": "2420460",
    "end": "2426420"
  },
  {
    "text": "we can have query times that go from a couple minutes down to a couple of seconds so data caching is hard so we",
    "start": "2426420",
    "end": "2435970"
  },
  {
    "text": "have a system that we've developed to manage that process jobs no longer have",
    "start": "2435970",
    "end": "2441490"
  },
  {
    "text": "to be aware of the paths their reading and writing from and said they just specify the name of the path that they're reading and writing from along",
    "start": "2441490",
    "end": "2448210"
  },
  {
    "text": "with the partitions that they're interested in and then meta store will tell them where to get it from and where to write it two jobs always right to s3",
    "start": "2448210",
    "end": "2455890"
  },
  {
    "text": "so that we can ensure this persistent and sticks around and in the meta store allowed us to the job once it's complete",
    "start": "2455890",
    "end": "2463180"
  },
  {
    "text": "it will set off caching to the appropriate layers if necessary and once that caching is done it will make that",
    "start": "2463180",
    "end": "2470170"
  },
  {
    "text": "data available to other jobs as inputs the other thing that the meda store does for us is it keeps a full index of s3",
    "start": "2470170",
    "end": "2477849"
  },
  {
    "text": "pass doing list operations on s3 is very slow to the point where if sequel",
    "start": "2477849",
    "end": "2486790"
  },
  {
    "text": "queries end up touching s3 can add as much as a third to the additional run time by listing paths this makes it",
    "start": "2486790",
    "end": "2493810"
  },
  {
    "text": "nearly instantaneous in addition it updates the hive meta store so that",
    "start": "2493810",
    "end": "2499480"
  },
  {
    "text": "spark and sparks equal and hive always have up-to-date information finally we",
    "start": "2499480",
    "end": "2507640"
  },
  {
    "text": "make a lot of use of the s3 bucket policies and easy to roles when we were",
    "start": "2507640",
    "end": "2514240"
  },
  {
    "text": "acquired about five months ago or when the acquisition was announced about five",
    "start": "2514240",
    "end": "2519819"
  },
  {
    "text": "months ago we found out at the same time as everybody else and we had 30 days in",
    "start": "2519819",
    "end": "2525190"
  },
  {
    "text": "which to make sure that we were regularly regulatory compliant with sarbanes-oxley",
    "start": "2525190",
    "end": "2531210"
  },
  {
    "text": "in order for Twitter to be able to recognize our revenue so this means that no data which has revenue or financial",
    "start": "2531210",
    "end": "2540130"
  },
  {
    "text": "information can be deleted and we implied this generally to our important data sources so by using bucket policies",
    "start": "2540130",
    "end": "2546910"
  },
  {
    "text": "and ec two roles we are able to lock it down so certain data could only be written by one cluster and then lock",
    "start": "2546910",
    "end": "2552910"
  },
  {
    "text": "down that cluster so only code they've been code reviewed could be run on top of it but even if you don't use have",
    "start": "2552910",
    "end": "2562390"
  },
  {
    "text": "regulatory requirements for your data I definitely recommend using bucket policies to help control your data once",
    "start": "2562390",
    "end": "2569380"
  },
  {
    "text": "early on Intel aparts history just before I joined a developer a row script",
    "start": "2569380",
    "end": "2574930"
  },
  {
    "text": "which accidentally started deleting all of our base data sources fortunately we",
    "start": "2574930",
    "end": "2579970"
  },
  {
    "text": "followed some of IANS advice and had some backups we are able to restore it and recover some of the deleted data but",
    "start": "2579970",
    "end": "2587140"
  },
  {
    "text": "after that we implemented policies to prevent felician unless explicitly stated all right so I want to tell you",
    "start": "2587140",
    "end": "2595270"
  },
  {
    "text": "about how you can achieve this kinds of performance on s3 yourself choosing an",
    "start": "2595270",
    "end": "2601630"
  },
  {
    "text": "instance type can be a delicate balance oftentimes it's better to have a larger",
    "start": "2601630",
    "end": "2607900"
  },
  {
    "text": "instance type because it keeps the data local and it has it means to have a smaller cluster to manage however",
    "start": "2607900",
    "end": "2613990"
  },
  {
    "text": "because of ec2 bandwidth limits we've",
    "start": "2613990",
    "end": "2619359"
  },
  {
    "text": "discovered that it's often advantageous to have smaller rather than larger instance types so I ran a test with for",
    "start": "2619359",
    "end": "2625960"
  },
  {
    "text": "our 32 X larges and one are 38 x larges which in theory are equivalent in terms",
    "start": "2625960",
    "end": "2632109"
  },
  {
    "text": "of total resources and the four hour 3 2 x-large has had fifty percent better",
    "start": "2632109",
    "end": "2637680"
  },
  {
    "text": "bandwidth than a single are 38 x-large however the benefit didn't extend to our",
    "start": "2637680",
    "end": "2645940"
  },
  {
    "text": "three larges for us because at that point the s3 transfer was no longer the",
    "start": "2645940",
    "end": "2652390"
  },
  {
    "text": "bowel neck and the bottleneck was actually the inter machine communication",
    "start": "2652390",
    "end": "2658470"
  },
  {
    "text": "when reading and writing from s3 always use multi-part uploads and downloads",
    "start": "2659040",
    "end": "2664270"
  },
  {
    "text": "this means that you can start uploading as the processing completes it also",
    "start": "2664270",
    "end": "2669589"
  },
  {
    "text": "means you can start downloading start processing things as you're downloading it also means in either case that",
    "start": "2669589",
    "end": "2675380"
  },
  {
    "text": "there's a failure you can deal with it and just re-upload that one part when",
    "start": "2675380",
    "end": "2680690"
  },
  {
    "text": "you're using a dupe in spark use the newer s3 a library instead of the older s3n library it uses java's s3 library",
    "start": "2680690",
    "end": "2688910"
  },
  {
    "text": "underneath instead of the jet SVT and we found s3a is far more performant and has",
    "start": "2688910",
    "end": "2694640"
  },
  {
    "text": "better error recovery and again try to avoid list operations on s3 we aren't on",
    "start": "2694640",
    "end": "2701839"
  },
  {
    "text": "EMR because it would add significantly to the underlying swap price that we have however if you are on e.m EMR",
    "start": "2701839",
    "end": "2710210"
  },
  {
    "text": "there's EMR FS which Ian talked about which is a great operation a great option for dealing with the slowest",
    "start": "2710210",
    "end": "2718579"
  },
  {
    "text": "operations issue when running MapReduce on s3 we recommend the disabled",
    "start": "2718579",
    "end": "2724940"
  },
  {
    "text": "speculative execution for reducers and write the output directly for those that",
    "start": "2724940",
    "end": "2731750"
  },
  {
    "text": "don't know speculative execution it takes free resources on a cluster and runs duplicates of existing tasks that",
    "start": "2731750",
    "end": "2738950"
  },
  {
    "text": "are already running unfortunately this requires that it output to a temporary location and then move that data to the",
    "start": "2738950",
    "end": "2746329"
  },
  {
    "text": "final destination when it's done because of eventual consistency sometimes when",
    "start": "2746329",
    "end": "2751789"
  },
  {
    "text": "it tries to move that file isn't present in the overall process fails that being",
    "start": "2751789",
    "end": "2757010"
  },
  {
    "text": "said about two months ago s3 introduced read after a consistency on the s3",
    "start": "2757010",
    "end": "2763609"
  },
  {
    "text": "standard region so it's actually possible that this underlying problem is fixed and we haven't had a chance to",
    "start": "2763609",
    "end": "2769430"
  },
  {
    "text": "test it cut file sizes should be equal to or slightly less than HDFS block size",
    "start": "2769430",
    "end": "2774890"
  },
  {
    "text": "I know HDFS block size is a totally arbitrary sort of restriction on s3",
    "start": "2774890",
    "end": "2781670"
  },
  {
    "text": "since they don't mean anything however both spark and dupe use the block size as a guide for how to distribute data to",
    "start": "2781670",
    "end": "2788569"
  },
  {
    "text": "mappers and reducers so if you can keep your file size equal and approximate to",
    "start": "2788569",
    "end": "2794180"
  },
  {
    "text": "the block size then you're going to have a much more consistent experience reading data we use the park a column",
    "start": "2794180",
    "end": "2801569"
  },
  {
    "text": "base file format we recommend it but generally we recommend it for only base",
    "start": "2801569",
    "end": "2806640"
  },
  {
    "text": "data sources and not necessarily everything there's a little extra overhead and creating it but if it gets",
    "start": "2806640",
    "end": "2812730"
  },
  {
    "text": "red frequently it's definitely worth it",
    "start": "2812730",
    "end": "2818150"
  },
  {
    "text": "alright so in the course of building our overall officer our curt art architecture we've had many late nights",
    "start": "2818150",
    "end": "2826369"
  },
  {
    "text": "and which things went wrong and we want to share some of the lessons that we've",
    "start": "2826369",
    "end": "2831569"
  },
  {
    "text": "learned in the course of building it always learn as much as you can about AWS services before you use them it",
    "start": "2831569",
    "end": "2838410"
  },
  {
    "text": "helps you to understand what's happening underneath it allows you to write more performance more performant systems for",
    "start": "2838410",
    "end": "2845009"
  },
  {
    "text": "it it also means you have a better understanding what's going wrong when you get that 2am page and are better",
    "start": "2845009",
    "end": "2851190"
  },
  {
    "text": "able to deal with emergencies try to use Amazon libraries as much as possible pretty much across the board we found",
    "start": "2851190",
    "end": "2858239"
  },
  {
    "text": "that they're more performant and have better error handling mostly because",
    "start": "2858239",
    "end": "2863880"
  },
  {
    "text": "they have better understanding of the underlying implementations and are kept better up to date and then finally",
    "start": "2863880",
    "end": "2869579"
  },
  {
    "text": "always be ready for machines to fail we run two thousand machines in batch every",
    "start": "2869579",
    "end": "2875339"
  },
  {
    "text": "night and it's guaranteed every day one of them is going to have some sort of issue as a networking performance or bad",
    "start": "2875339",
    "end": "2882150"
  },
  {
    "text": "neighbor rather than spending time debugging issues just be ready to kill machines as soon as there's anything",
    "start": "2882150",
    "end": "2887880"
  },
  {
    "text": "wrong with them and let Amazon bring up new ones it's definitely much more expedient all right we are hiring so if",
    "start": "2887880",
    "end": "2897390"
  },
  {
    "text": "you go to the Twitter careers page under software engineering and search for tell",
    "start": "2897390",
    "end": "2902489"
  },
  {
    "text": "apart we were definitely interested oh I",
    "start": "2902489",
    "end": "2908069"
  },
  {
    "text": "think you're up first",
    "start": "2908069",
    "end": "2913069"
  },
  {
    "text": "so yeah just a quick wrap-up you know I think we've heard some great stories about how s3 can really give you this",
    "start": "2916349",
    "end": "2923489"
  },
  {
    "text": "fantastic approach to having a very high durability high-performance centralized store which allow you to syndicate into",
    "start": "2923489",
    "end": "2930969"
  },
  {
    "text": "other big data environments you know there's a lot of interfaces a lot of tools and we'll continue to build",
    "start": "2930969",
    "end": "2936579"
  },
  {
    "text": "services that are able to consume data from s3 push data back and you know",
    "start": "2936579",
    "end": "2942069"
  },
  {
    "text": "definitely please consider you know your storage approach the prefixes that",
    "start": "2942069",
    "end": "2947079"
  },
  {
    "text": "you're using so that you don't create something that isn't going to meet your needs long-term that's about the only thing that you need to kind of consider",
    "start": "2947079",
    "end": "2953650"
  },
  {
    "text": "upfront and with that we're we're out of time we're happy to take questions after",
    "start": "2953650",
    "end": "2959890"
  },
  {
    "text": "maybe at the back so the room can be freed up but thank you again to read Finn and Twitter for their time and",
    "start": "2959890",
    "end": "2967599"
  },
  {
    "text": "attention and thank you very much",
    "start": "2967599",
    "end": "2971190"
  }
]