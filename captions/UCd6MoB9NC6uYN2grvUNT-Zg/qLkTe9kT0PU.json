[
  {
    "start": "0",
    "end": "156000"
  },
  {
    "text": "hello everyone my name is rob hegarida and i'm a principal technical account manager at aws and i'm based out of",
    "start": "4319",
    "end": "9920"
  },
  {
    "text": "austin texas welcome to aws supports you where we share best practices and troubleshooting tips from aws support",
    "start": "9920",
    "end": "17199"
  },
  {
    "text": "joining me today is anish and swapnil from aws support can you give give us a quick introduction a niche and then swap",
    "start": "17199",
    "end": "23519"
  },
  {
    "text": "you yeah sure rob my name is aneesh i'm a technical account manager based in",
    "start": "23519",
    "end": "28800"
  },
  {
    "text": "dublin ireland and i work with enterprise customers to help them build a cost effective highly available and",
    "start": "28800",
    "end": "36320"
  },
  {
    "text": "low latency solutions with aws big data services such as glue emr dynamodb",
    "start": "36320",
    "end": "42719"
  },
  {
    "text": "kinesis spark and athena i am really excited to be part of this show today or to use hotmail",
    "start": "42719",
    "end": "48960"
  },
  {
    "text": "thank you anish hello everyone myself swapnil i'm a cloud support engineer and a glue subject matter expert based in",
    "start": "48960",
    "end": "56000"
  },
  {
    "text": "dublin ireland i work with customer base globally helping them and supporting them with various aws big data services",
    "start": "56000",
    "end": "63520"
  },
  {
    "text": "including blue i'm really happy to be here thank you awesome thank you both for introduction",
    "start": "63520",
    "end": "68960"
  },
  {
    "text": "so today we will be introducing the audience to aws glue will be specifically focused on",
    "start": "68960",
    "end": "75119"
  },
  {
    "text": "performance improvements by leveraging jdbc parallel reads from aws glue and some further best practices for",
    "start": "75119",
    "end": "81119"
  },
  {
    "text": "optimizing those jdbc reads but before we do go into the details of today's episode a quick note to the attendees",
    "start": "81119",
    "end": "86960"
  },
  {
    "text": "online please feel free to use the chat window on the right hand side of your screen to ask your questions throughout today's episode and really pick you know",
    "start": "86960",
    "end": "93680"
  },
  {
    "text": "swap neil alanis's brains today so we look forward to hearing from you uh anish can you go ahead and walk us through we're gonna be talking about",
    "start": "93680",
    "end": "99520"
  },
  {
    "text": "today yeah sure rob so in today's section we have broken down",
    "start": "99520",
    "end": "105680"
  },
  {
    "text": "the presentation into two sections uh in the first section i will be going through what is aws glue as a service",
    "start": "105680",
    "end": "112720"
  },
  {
    "text": "we'll have a quick look at the overall flow of glue in terms of etl jobs",
    "start": "112720",
    "end": "118159"
  },
  {
    "text": "we'll be then looking at aws glue components and then we'll have a look how glue is",
    "start": "118159",
    "end": "124000"
  },
  {
    "text": "built over spark core and be discussing about glue dynamic frame and spark data frames",
    "start": "124000",
    "end": "131120"
  },
  {
    "text": "in the next section sopnl will be talking about integration with jdbc data based sources and the",
    "start": "131120",
    "end": "137840"
  },
  {
    "text": "benefits of parallel reads he'll be then talking about how to calculate the",
    "start": "137840",
    "end": "142959"
  },
  {
    "text": "maximum parallel parallelism and in the end we'll be going through a demonstration with the sample etl jobs",
    "start": "142959",
    "end": "149040"
  },
  {
    "text": "and to figure out how we can optimize the etl job runs so let's move on to the next section",
    "start": "149040",
    "end": "156480"
  },
  {
    "start": "156000",
    "end": "224000"
  },
  {
    "text": "so what is aws glue now aws glue is a cloud optimized extract transform and",
    "start": "156480",
    "end": "162800"
  },
  {
    "text": "load service etl for short basically allows you to organize locate",
    "start": "162800",
    "end": "168720"
  },
  {
    "text": "move and transform all your data across your business so you can put them to use",
    "start": "168720",
    "end": "174720"
  },
  {
    "text": "now glue provides crawlers with automatic schema inference for your semi-structured and structured data sets",
    "start": "174720",
    "end": "181280"
  },
  {
    "text": "meaning crawlers automatically can discover all your data sets discover your file types extract the schema and",
    "start": "181280",
    "end": "188159"
  },
  {
    "text": "store all this information in a centralized metadata catalog for later querying and analysis",
    "start": "188159",
    "end": "195360"
  },
  {
    "text": "glue also automatically generates the scripts that you need to extract transform",
    "start": "195360",
    "end": "200480"
  },
  {
    "text": "and load your data from source to target so you don't have to start from scratch",
    "start": "200480",
    "end": "205680"
  },
  {
    "text": "and the best part is glue is serverless that is you simply point clue to all your etl jobs and hit run you don't need",
    "start": "205680",
    "end": "213440"
  },
  {
    "text": "to provision configure or spin up services servers and certainly you don't need to manage their life cycle",
    "start": "213440",
    "end": "221040"
  },
  {
    "text": "now let's have a look how does the overall flow looks like now in this diagram you can see on the",
    "start": "221040",
    "end": "226159"
  },
  {
    "start": "224000",
    "end": "284000"
  },
  {
    "text": "left hand side there are the data sources ranging from amazon relational database",
    "start": "226159",
    "end": "232879"
  },
  {
    "text": "uh glue catalog your in-house jdbc databases amazon s3 streaming data and",
    "start": "232879",
    "end": "239439"
  },
  {
    "text": "you can connect them uh to where using glue connector or glue crawlers",
    "start": "239439",
    "end": "245519"
  },
  {
    "text": "once the connection is established you can basically catalog into the glue catalog your structured and",
    "start": "245519",
    "end": "251280"
  },
  {
    "text": "semi-structured data and once the catalog has been done you can basically transform all your data as per your",
    "start": "251280",
    "end": "258400"
  },
  {
    "text": "business needs via glue studio deven points and aws glue data brew where you",
    "start": "258400",
    "end": "264080"
  },
  {
    "text": "don't even have to write a single piece of code and once the transformation is done you can then basically",
    "start": "264080",
    "end": "270240"
  },
  {
    "text": "uh move the data to the data location wherever the data target is and it could",
    "start": "270240",
    "end": "275440"
  },
  {
    "text": "be any of the nosql databases data warehouses or it could be your in-house jdbc databases as well",
    "start": "275440",
    "end": "282880"
  },
  {
    "text": "so let's have a look at the major glue components now the first component here",
    "start": "282880",
    "end": "288160"
  },
  {
    "start": "284000",
    "end": "381000"
  },
  {
    "text": "is the data catalog which is basically the centralized metadata store for all your analytics",
    "start": "288160",
    "end": "294320"
  },
  {
    "text": "data in aws now with data catalogs you have crawlers which as",
    "start": "294320",
    "end": "299840"
  },
  {
    "text": "explained can extract the structure and the load all of the metadata into",
    "start": "299840",
    "end": "304960"
  },
  {
    "text": "the catalog now catalog is high metastore compatible so other analytics services like emr",
    "start": "304960",
    "end": "312320"
  },
  {
    "text": "redshift and athena can integrate with the catalog and perform analytics",
    "start": "312320",
    "end": "317759"
  },
  {
    "text": "now the another major component for glue is its serverless job execution system",
    "start": "317759",
    "end": "323199"
  },
  {
    "text": "at the core of job execution system is the open platform apache spark that runs",
    "start": "323199",
    "end": "329039"
  },
  {
    "text": "all your jobs you just submit your etl jobs written in python or scala and we provision all the",
    "start": "329039",
    "end": "335440"
  },
  {
    "text": "infrastructure that is necessary to run these jobs and all you do is pay for the",
    "start": "335440",
    "end": "340880"
  },
  {
    "text": "time that job ran now if you don't have coding background that's no problem blue provides you",
    "start": "340880",
    "end": "346800"
  },
  {
    "text": "tools based on data present in your catalog that will automatically generate",
    "start": "346800",
    "end": "352000"
  },
  {
    "text": "that you need for transformation from one source to another now third and the last component is the",
    "start": "352000",
    "end": "357600"
  },
  {
    "text": "glue orchestration system it basically allows you to stitch together multiple jobs to accomplish a",
    "start": "357600",
    "end": "363840"
  },
  {
    "text": "larger task and it can be done in a linear fashion or a directed acyclic graph for jobs to put together",
    "start": "363840",
    "end": "370720"
  },
  {
    "text": "and the best part is you can monitor the status and check for failures and can also integrate with other aws services",
    "start": "370720",
    "end": "379960"
  },
  {
    "text": "but let's have a look how a glue etl environment looks like on the top left",
    "start": "380319",
    "end": "385680"
  },
  {
    "start": "381000",
    "end": "424000"
  },
  {
    "text": "hand corner you can see the different flavors of jdbc data sources like oracle",
    "start": "385680",
    "end": "390800"
  },
  {
    "text": "mysql postgres and on the bottom you can see that aws",
    "start": "390800",
    "end": "396160"
  },
  {
    "text": "native data sources like rds dynamodb and you can basically perform etl",
    "start": "396160",
    "end": "403039"
  },
  {
    "text": "on any of these data sources and can move the transform data to any of the",
    "start": "403039",
    "end": "408080"
  },
  {
    "text": "data targets which can be again your in-house jdbc data sources or the",
    "start": "408080",
    "end": "414319"
  },
  {
    "text": "native aws services and swapnil in the later section will be diving deep more into this section",
    "start": "414319",
    "end": "423240"
  },
  {
    "text": "now in order to understand what etl script is doing it is very important to actually understand how the",
    "start": "423599",
    "end": "430400"
  },
  {
    "start": "424000",
    "end": "477000"
  },
  {
    "text": "system is built so glue is actually built on top of apache spark core now at the bottom you",
    "start": "430400",
    "end": "437199"
  },
  {
    "text": "can see you get the rdds which is the resilient distributed data sets these are basically data structures that",
    "start": "437199",
    "end": "443680"
  },
  {
    "text": "allow you to put data in them and then run operations over them in a fault tolerant way",
    "start": "443680",
    "end": "449039"
  },
  {
    "text": "now apache spark itself has a built-in additional data source on top of its called data frames and",
    "start": "449039",
    "end": "455919"
  },
  {
    "text": "data frames are optimized for doing sql-like analytics",
    "start": "455919",
    "end": "460960"
  },
  {
    "text": "now in glue we have a parallel set of libraries or data structures called dynamic frame that are basically",
    "start": "460960",
    "end": "467599"
  },
  {
    "text": "optimized for etl and then bunch of transformations on top of dynamic frame that make data cleaning easy",
    "start": "467599",
    "end": "476000"
  },
  {
    "start": "477000",
    "end": "604000"
  },
  {
    "text": "now as explained in previous slide the data frames are basic core data",
    "start": "477520",
    "end": "482639"
  },
  {
    "text": "structures for spark they're like structured tables with data frames where we need schema up front and",
    "start": "482639",
    "end": "489280"
  },
  {
    "text": "each row has the same structure now when it comes to etl jobs you're looking at",
    "start": "489280",
    "end": "494720"
  },
  {
    "text": "unclean incomplete semi-structured data for example logs in average json format",
    "start": "494720",
    "end": "501520"
  },
  {
    "text": "so what dynamic frames do is they actually store the structure",
    "start": "501520",
    "end": "506720"
  },
  {
    "text": "of each record with record itself so every record can be different from the previous",
    "start": "506720",
    "end": "512640"
  },
  {
    "text": "now there's no need to have schema up front no need to pass over the data to complete the schema again you can just",
    "start": "512640",
    "end": "518719"
  },
  {
    "text": "start running operations over it and that's why it's very useful for etl",
    "start": "518719",
    "end": "523839"
  },
  {
    "text": "now it's very important to understand that data frames and dynamic frames are not competing with each other",
    "start": "523839",
    "end": "529920"
  },
  {
    "text": "they go hand-in-hand complementary now you can turn dynamic frame to data frame and vice versa and can make use of them",
    "start": "529920",
    "end": "537440"
  },
  {
    "text": "as per your use case now before i move on to the next section",
    "start": "537440",
    "end": "542480"
  },
  {
    "text": "uh rob do we have any questions from the audience hey fixation yeah we don't have any",
    "start": "542480",
    "end": "548959"
  },
  {
    "text": "questions from the audience right now so attendees please make sure to post your questions into the chat but i do have a",
    "start": "548959",
    "end": "554320"
  },
  {
    "text": "question uh you mentioned that you know glue is built onto apache spark how does spark data frames differ from the aws",
    "start": "554320",
    "end": "560399"
  },
  {
    "text": "glue dynamic frame yeah sure so as i mentioned they both",
    "start": "560399",
    "end": "567839"
  },
  {
    "text": "have their own use cases dynamic frame is more for uncleaned data or semi-structured data",
    "start": "567839",
    "end": "575680"
  },
  {
    "text": "but when you have use case where your data is structured uh you can always go for data frame now glue supports both",
    "start": "575680",
    "end": "582720"
  },
  {
    "text": "dynamic frame and data frame and you can convert them as per your use case",
    "start": "582720",
    "end": "588800"
  },
  {
    "text": "okay thanks appreciate that so i'll let you carry on through this presentation thank you perfect",
    "start": "588800",
    "end": "594640"
  },
  {
    "text": "so moving on to the next section i'll pass over to sopnel who will be talking more about the integration with jdbc data",
    "start": "594640",
    "end": "601519"
  },
  {
    "text": "sources",
    "start": "601519",
    "end": "604519"
  },
  {
    "start": "604000",
    "end": "894000"
  },
  {
    "text": "thank you so in the coming slides we'll be talking about how glue interacts with jdbc data",
    "start": "607040",
    "end": "612800"
  },
  {
    "text": "source and more importantly we'll be focusing on why parallel reads are better in terms",
    "start": "612800",
    "end": "618560"
  },
  {
    "text": "of jdb series as compared to default read and topic will be focused on spark",
    "start": "618560",
    "end": "624079"
  },
  {
    "text": "and uh also glue because glue again as i said is built on top of a party spark",
    "start": "624079",
    "end": "629920"
  },
  {
    "text": "so before i start off uh i would like to add a customer use case where uh they were using uh aws glue to read the",
    "start": "629920",
    "end": "637200"
  },
  {
    "text": "data from jdbc and right out to amazon s3 uh the use case involved to carry out",
    "start": "637200",
    "end": "642720"
  },
  {
    "text": "business analytics from the data written out to amazon s3 however since the default read are a single credit which",
    "start": "642720",
    "end": "649440"
  },
  {
    "text": "means only one executor will be used to read the entire data source the job was taking longer and eventually",
    "start": "649440",
    "end": "656160"
  },
  {
    "text": "will fail with out of memory a customer did retry the job multiple times but it",
    "start": "656160",
    "end": "661440"
  },
  {
    "text": "always failed and due to the nature of the job and the time it took",
    "start": "661440",
    "end": "666880"
  },
  {
    "text": "they were nearing to the deadline and unfortunately the issue got escalated on their end i was working with this",
    "start": "666880",
    "end": "672240"
  },
  {
    "text": "particular customer and i reviewed the apl code and saw that they were using default pdbc option uh my main task was",
    "start": "672240",
    "end": "679600"
  },
  {
    "text": "to get them out of the production page so once i reviewed the code i",
    "start": "679600",
    "end": "684800"
  },
  {
    "text": "detailed them about the parallel read parameters in glue and also in spark and",
    "start": "684800",
    "end": "689920"
  },
  {
    "text": "help them with the code and how parameters they can what parameters they need to add and with the parallel read",
    "start": "689920",
    "end": "696240"
  },
  {
    "text": "operation customer had the job running much faster unfortunately they were able to reach the deadline and carry out the",
    "start": "696240",
    "end": "702079"
  },
  {
    "text": "business analytics so as you can see uh the default jdbc reads a",
    "start": "702079",
    "end": "708320"
  },
  {
    "text": "job will make a single connection to the jdbc data store and that particular connection will do a select star query",
    "start": "708320",
    "end": "715200"
  },
  {
    "text": "to read the entire table so generally for a smaller table default read won't make any problem",
    "start": "715200",
    "end": "721519"
  },
  {
    "text": "but in today's big data world where we have a million and billions of records in a",
    "start": "721519",
    "end": "727680"
  },
  {
    "text": "jdbc data store a single query to read the entire table will definitely cause the job to run out of memory",
    "start": "727680",
    "end": "734240"
  },
  {
    "text": "so we can see in parallel jdbc readings that uh there are multiple connections made or the jdbc data store has been",
    "start": "734240",
    "end": "740880"
  },
  {
    "text": "named parallel and the select star query are when in parallel basically each of the queries will only read a",
    "start": "740880",
    "end": "748480"
  },
  {
    "text": "part of the table instead of one query during the entire table so benefits of gdbc parallel reads will",
    "start": "748480",
    "end": "755120"
  },
  {
    "text": "definitely give us a faster job runtime we'll be talking about this in the demo",
    "start": "755120",
    "end": "760399"
  },
  {
    "text": "uh obviously the job run time comes down and in blue you only pay for how long",
    "start": "760399",
    "end": "765680"
  },
  {
    "text": "the job runs so definitely uh this will reduce the cost in parallel jdbc read",
    "start": "765680",
    "end": "772240"
  },
  {
    "text": "and the memory and the cpu burden is less not only on the glue executors but",
    "start": "772240",
    "end": "777600"
  },
  {
    "text": "also on the jdbc source on the glue executor parallel executors are used to read the",
    "start": "777600",
    "end": "783920"
  },
  {
    "text": "part of the table to read the records in the table so there is no one executor reading the",
    "start": "783920",
    "end": "789600"
  },
  {
    "text": "entire table and also on the database side we don't have any single select start query to",
    "start": "789600",
    "end": "795440"
  },
  {
    "text": "read the entire table and we have smaller queries to read the data from the tables that that's why we",
    "start": "795440",
    "end": "802000"
  },
  {
    "text": "see a reduced cpu and memory load on the database so let's talk and let's continue about",
    "start": "802000",
    "end": "808320"
  },
  {
    "text": "integration with jdbc data in the next slide so just the continuation of what i said so",
    "start": "808320",
    "end": "815519"
  },
  {
    "text": "let's focus on kdbc uh with respect to aws blue here so for data sources in aws glue we can",
    "start": "815519",
    "end": "822399"
  },
  {
    "text": "have your jdbc databases so any database that can be a reachable or a jdbc",
    "start": "822399",
    "end": "828160"
  },
  {
    "text": "protocol can be used as the source for aws blue so your jdbc database can be an",
    "start": "828160",
    "end": "833920"
  },
  {
    "text": "on-premise database or a database based on other cloud offering uh amazon rds is a well-known jdbc",
    "start": "833920",
    "end": "841279"
  },
  {
    "text": "database engine so that can be used as a source as well or else a customer might have a blue catalog table which is based",
    "start": "841279",
    "end": "848560"
  },
  {
    "text": "on jdbc so these can use can be used as a source for my blue jaw",
    "start": "848560",
    "end": "854399"
  },
  {
    "text": "once i'm done with my source and i carry out the transformation uh we can focus",
    "start": "854399",
    "end": "859600"
  },
  {
    "text": "on the targets so targets similarly i can write out to on-premise and other cloud-based jdbc databases",
    "start": "859600",
    "end": "866560"
  },
  {
    "text": "i can also write out to my amazon rds and glue data catalog but now moreover i can also write out to amazon s3 amazon",
    "start": "866560",
    "end": "874000"
  },
  {
    "text": "redshift and also to my dynamodb table so this is a mode of interaction we have seen with customers where",
    "start": "874000",
    "end": "880880"
  },
  {
    "text": "what are the typical data sources and data targets they use so let's talk about this in detail",
    "start": "880880",
    "end": "887680"
  },
  {
    "text": "so let's move to the next slide",
    "start": "887680",
    "end": "891519"
  },
  {
    "start": "894000",
    "end": "994000"
  },
  {
    "text": "so before we jump in more we need to understand how we can dictate or what parameters",
    "start": "894079",
    "end": "899920"
  },
  {
    "text": "dictate upon the maximum parallelism so the dpu is the basic unit",
    "start": "899920",
    "end": "906079"
  },
  {
    "text": "and to put things in short here dpu the number of gpus that is used in the job",
    "start": "906079",
    "end": "911199"
  },
  {
    "text": "will account for the billing so blue offers three different worker types standard g1x and g2x",
    "start": "911199",
    "end": "918079"
  },
  {
    "text": "as we can see in the chart the spark driver memory and the executor memory uh changes across the worker type",
    "start": "918079",
    "end": "925199"
  },
  {
    "text": "where g2x provides the maximum of the memory we recommend customers using g2x worker",
    "start": "925199",
    "end": "930399"
  },
  {
    "text": "type for machine learning operation and we recommend them using g1x for",
    "start": "930399",
    "end": "935600"
  },
  {
    "text": "memory intensive jobs so once the customer knows uh the worker type they are using uh the",
    "start": "935600",
    "end": "942480"
  },
  {
    "text": "the spark executable course provided by each of the worker type uh will be will",
    "start": "942480",
    "end": "947680"
  },
  {
    "text": "be defined now once we know the spark executor code uh we'll talk about how many executor my",
    "start": "947680",
    "end": "953839"
  },
  {
    "text": "job is have so to continue on this let's move on to the next slide",
    "start": "953839",
    "end": "959279"
  },
  {
    "text": "so the number of maximum spark executors will again depend on my worker type and",
    "start": "959279",
    "end": "964560"
  },
  {
    "text": "also depend on the glue version i'm using so once i know these two values uh the number of spark executor and my spark",
    "start": "964560",
    "end": "971199"
  },
  {
    "text": "executor goes the multiplication of these two will give me the value of maximum parallel",
    "start": "971199",
    "end": "976880"
  },
  {
    "text": "task so this is the default formula and what what we mean by this is these many",
    "start": "976880",
    "end": "981920"
  },
  {
    "text": "number of parallel tasks my job can do as opposed to a single thread operation",
    "start": "981920",
    "end": "988560"
  },
  {
    "text": "so let's dive in to more of the parallel parameters that can be used for jdbcd",
    "start": "988560",
    "end": "995680"
  },
  {
    "start": "994000",
    "end": "1379000"
  },
  {
    "text": "so let me start with jdbc read parallel leads in aws glue dynamic frame",
    "start": "995680",
    "end": "1001199"
  },
  {
    "text": "so in blue dynamic three we have three parameters that will enable us to do a parallel read operation",
    "start": "1001199",
    "end": "1007120"
  },
  {
    "text": "they are hash expression hatch field and hatch partition a customer can either use hash",
    "start": "1007120",
    "end": "1013279"
  },
  {
    "text": "expression or hash field and you do not need to specify both of these parameters",
    "start": "1013279",
    "end": "1019440"
  },
  {
    "text": "so let me start explaining about hash expression so for hash expression you need to",
    "start": "1019440",
    "end": "1024720"
  },
  {
    "text": "simply specify a column value in my jdbc data source uh that particular column",
    "start": "1024720",
    "end": "1030959"
  },
  {
    "text": "should return a whole number in my sql like expression so that is the only new uh that is the only requirement for that",
    "start": "1030959",
    "end": "1037600"
  },
  {
    "text": "particular column a simple expression will be a name of any numeric column like a say an id column which is my",
    "start": "1037600",
    "end": "1044160"
  },
  {
    "text": "primary key incremental value something like that that is the ideal candidate to specify the hash expression",
    "start": "1044160",
    "end": "1051360"
  },
  {
    "text": "the second option is in case if a customer do not have any numeric column as such customer can use hash field",
    "start": "1051360",
    "end": "1058240"
  },
  {
    "text": "option a good thing about hash field is a column a column data type can be of",
    "start": "1058240",
    "end": "1063360"
  },
  {
    "text": "any data type basically so any column a beat of any data type can be specified as a hash field here",
    "start": "1063360",
    "end": "1070640"
  },
  {
    "text": "uh and once customer decide whether to specify hash expression or hash field",
    "start": "1070640",
    "end": "1075919"
  },
  {
    "text": "then uh the parameter comes in is hash partition so as the name suggests hash partition",
    "start": "1075919",
    "end": "1081600"
  },
  {
    "text": "will be basically the number of connections that the glue job will make to the jdbc data store",
    "start": "1081600",
    "end": "1087360"
  },
  {
    "text": "for example if hash partition is specified as 10 that means my job will make 10 connections to the database",
    "start": "1087360",
    "end": "1093679"
  },
  {
    "text": "and each of the connection will read basically a part of the table instead of a single query to read the entire table",
    "start": "1093679",
    "end": "1100720"
  },
  {
    "text": "in case if hash partition is not specified uh the default value is seven which means that again seven connections",
    "start": "1100720",
    "end": "1107200"
  },
  {
    "text": "being made to the database firing seven queries to reiterate the table in parallel",
    "start": "1107200",
    "end": "1113200"
  },
  {
    "text": "so let's talk about hash expression in more detail uh in the next slide i have a sample code over here",
    "start": "1113200",
    "end": "1121640"
  },
  {
    "text": "okay so before we dive into this code uh rob do we have any questions",
    "start": "1122960",
    "end": "1129600"
  },
  {
    "text": "hi yes we do have a couple questions of comments to be kind of covering these items here so our first question comes",
    "start": "1129600",
    "end": "1135120"
  },
  {
    "text": "in from mod ramon357 uh is there is the schema registry in glue open source",
    "start": "1135120",
    "end": "1143039"
  },
  {
    "text": "uh i'm not sure about it to be honest uh the schema registry part is actually",
    "start": "1143440",
    "end": "1149600"
  },
  {
    "text": "for more of you are focusing on the streaming part so it's it's still in preview as such",
    "start": "1149600",
    "end": "1154960"
  },
  {
    "text": "uh so it's not open source as such but it's more of uh it i i won't be i won't",
    "start": "1154960",
    "end": "1160400"
  },
  {
    "text": "be very much sure about this particular answer okay great thanks uh and then one of the questions just came in from tommy",
    "start": "1160400",
    "end": "1166240"
  },
  {
    "text": "242 which i believe you're kind of covering a little bit or going through uh can i control the number of dpos i",
    "start": "1166240",
    "end": "1171919"
  },
  {
    "text": "want to use yeah of course you can so basically uh in one of the slides",
    "start": "1171919",
    "end": "1178640"
  },
  {
    "text": "when we create a job you need to specify the worker and the number of workers you are using so let's say for the sake of",
    "start": "1178640",
    "end": "1184640"
  },
  {
    "text": "example i'm using a gynex worker type so each worker typing g1x is equal to one worker",
    "start": "1184640",
    "end": "1190960"
  },
  {
    "text": "is equal to one gpu so if i specify my job with the 10 db 10 g1 x worker that",
    "start": "1190960",
    "end": "1196880"
  },
  {
    "text": "will map to 10 gpus and job will be charged up according to the dpu rate so",
    "start": "1196880",
    "end": "1202640"
  },
  {
    "text": "obviously customer has a control over the gpu they want uh say for a job with",
    "start": "1202640",
    "end": "1207760"
  },
  {
    "text": "less memory and cpu requirement a job with five to ten dpu should work fine uh but for a job with like memory",
    "start": "1207760",
    "end": "1215600"
  },
  {
    "text": "intensive operations the dpu number will go up so it more of depends on the vertical scaling of the job",
    "start": "1215600",
    "end": "1222000"
  },
  {
    "text": "great thanks the answer is time i'll let you continue on here about your answer questions and such appreciate it thank you rob",
    "start": "1222000",
    "end": "1228400"
  },
  {
    "text": "so yeah so let's continue to hash expressions now so in hash expressions",
    "start": "1228400",
    "end": "1233840"
  },
  {
    "text": "we have sample code here in sample code i'm specifying my jdbc url my user",
    "start": "1233840",
    "end": "1239039"
  },
  {
    "text": "password and the table name so this is how the default reads happen but more importantly we can see i'm specifying my",
    "start": "1239039",
    "end": "1246000"
  },
  {
    "text": "hash expression as id and my hash partition as 10. id is my column in my jdbc data store it",
    "start": "1246000",
    "end": "1253919"
  },
  {
    "text": "is a numerical column which auto incremental value and hash partition and specifying as 10",
    "start": "1253919",
    "end": "1259919"
  },
  {
    "text": "meaning that i want my job to make 10 connection to data store and read in parallel",
    "start": "1259919",
    "end": "1265760"
  },
  {
    "text": "so with this particular code if you look at the sample queries that are fired on the data store",
    "start": "1265760",
    "end": "1272400"
  },
  {
    "text": "uh job will do a use a modulus operator and such that it returns ten different",
    "start": "1272400",
    "end": "1278640"
  },
  {
    "text": "values from zero to nine uh ten different values again because hash partitions are ten",
    "start": "1278640",
    "end": "1284559"
  },
  {
    "text": "uh so we see that one ten each query will ideally read one tenth of the table",
    "start": "1284559",
    "end": "1289600"
  },
  {
    "text": "instead of a select a single select star query reading the entire table so this is how the sample queries are",
    "start": "1289600",
    "end": "1295600"
  },
  {
    "text": "fired to read in parallel when hash expression is used in aws group dynamic",
    "start": "1295600",
    "end": "1300640"
  },
  {
    "text": "frame let's talk about hash field in the next slide",
    "start": "1300640",
    "end": "1305840"
  },
  {
    "text": "so the sample code again is pretty much same here the only parameter which is obviously has changed is hash field",
    "start": "1307520",
    "end": "1314240"
  },
  {
    "text": "so instead of hash expression and specifying hash field in my code and i'm using the title column so a title as",
    "start": "1314240",
    "end": "1321520"
  },
  {
    "text": "that any of the column data type can be used as a hash field so in my example i'm using a title column and hash field",
    "start": "1321520",
    "end": "1328799"
  },
  {
    "text": "as the name suggests will calculate the md5 hash of the records in my title column",
    "start": "1328799",
    "end": "1334320"
  },
  {
    "text": "and it will calculate such that it creates a 10 unit set of rows and i get 10 because of hash partition",
    "start": "1334320",
    "end": "1341039"
  },
  {
    "text": "being 10 so if you review the sample queries that are fired again a modulus operator is used for",
    "start": "1341039",
    "end": "1347840"
  },
  {
    "text": "each of the md5 hash amplifier hash value calculated uh returning 10",
    "start": "1347840",
    "end": "1353039"
  },
  {
    "text": "different unique values ranging from 0 to 9. so this is how we can see that there are",
    "start": "1353039",
    "end": "1358159"
  },
  {
    "text": "parallel there are multiple queries that get fired on the data store to read in parallel uh as opposed to a single query",
    "start": "1358159",
    "end": "1365440"
  },
  {
    "text": "to read in the default jdbc operation so this is how we do uh",
    "start": "1365440",
    "end": "1370960"
  },
  {
    "text": "the jdbc parallel read operation in aws glue dynamic frame let's talk about",
    "start": "1370960",
    "end": "1376480"
  },
  {
    "text": "parallel jdbc the inappropriate path data here so the parameters here obviously are",
    "start": "1376480",
    "end": "1382320"
  },
  {
    "start": "1379000",
    "end": "1645000"
  },
  {
    "text": "different the partition column is one is where you will specify the column to",
    "start": "1382320",
    "end": "1387520"
  },
  {
    "text": "dictate the partition so this is similar to what we saw has expression and hash field was",
    "start": "1387520",
    "end": "1393120"
  },
  {
    "text": "uh so in spark data frame we will specify the partition column uh the limitation on spark data frame is that",
    "start": "1393120",
    "end": "1399120"
  },
  {
    "text": "the partition column has to be a numeric date or a timestamp column so once we",
    "start": "1399120",
    "end": "1405200"
  },
  {
    "text": "have identified that column that can be specified as a partition column then customer need to specify the lower",
    "start": "1405200",
    "end": "1411440"
  },
  {
    "text": "bound and their upon bound value keep in mind these values are only strictly to calculate the size of the",
    "start": "1411440",
    "end": "1417440"
  },
  {
    "text": "partition and these do not uh dictate upon you know the specific sql like query to",
    "start": "1417440",
    "end": "1423120"
  },
  {
    "text": "read the part of the table so once i calculate once i know the lower",
    "start": "1423120",
    "end": "1429039"
  },
  {
    "text": "values and the upper value of my partition column this will dictate upon a size of the h",
    "start": "1429039",
    "end": "1434799"
  },
  {
    "text": "of the partition uh we will touch upon of the sample values of lower bound and upper bound it",
    "start": "1434799",
    "end": "1440400"
  },
  {
    "text": "will be uh interesting to know these uh in the next slide and then the last parameter will be",
    "start": "1440400",
    "end": "1446480"
  },
  {
    "text": "just to go back on the previous slide sorry yeah so the last parameter we have is",
    "start": "1446480",
    "end": "1452400"
  },
  {
    "text": "num partitions uh as the name says these are the number of partitions that can be used for parallelism and this is similar",
    "start": "1452400",
    "end": "1459279"
  },
  {
    "text": "to what we saw a hash partition in aws blue dynamically so num partition the",
    "start": "1459279",
    "end": "1464480"
  },
  {
    "text": "again it determines the number of connections that will be made to the jdbc so now let's move on to the next slide",
    "start": "1464480",
    "end": "1473559"
  },
  {
    "text": "so i'm creating a spark data frame df i'm using spark read format and specifying the default options the",
    "start": "1473840",
    "end": "1480559"
  },
  {
    "text": "required options which are my jdbc url my username password and table name so",
    "start": "1480559",
    "end": "1486000"
  },
  {
    "text": "this is how the default parallel the default lead happens in spark data frame but now we are focusing on parallel",
    "start": "1486000",
    "end": "1492640"
  },
  {
    "text": "reads so we specify the partition column which is my id column here i'll specify",
    "start": "1492640",
    "end": "1498720"
  },
  {
    "text": "the lower bound and upper bound value so my id column has the lower value of 1",
    "start": "1498720",
    "end": "1504320"
  },
  {
    "text": "and an upper value of 1000 so that is what i specify here respectively and",
    "start": "1504320",
    "end": "1509360"
  },
  {
    "text": "then my number of partition num partition is 10. so my upper bound value is 1000 and my",
    "start": "1509360",
    "end": "1515440"
  },
  {
    "text": "number of partitions are 10 so 1000 divided by 10 will give me 100. so roughly speaking each of the query will",
    "start": "1515440",
    "end": "1522400"
  },
  {
    "text": "only read 100 records up and we have 10 queries because of 10 uh num partitions",
    "start": "1522400",
    "end": "1528400"
  },
  {
    "text": "being specified so if you reviewed the sample queries that is fired on the jdbc data store uh the",
    "start": "1528400",
    "end": "1536400"
  },
  {
    "text": "id value is where clause is used against the id values so first query will read only the",
    "start": "1536400",
    "end": "1544000"
  },
  {
    "text": "id values ranging from 0 to 100 from 100 to 200 and so on until 900 to 1000 so",
    "start": "1544000",
    "end": "1550960"
  },
  {
    "text": "this is how the parallel read operation will happen in apache spark data free",
    "start": "1550960",
    "end": "1556320"
  },
  {
    "text": "moving on to the next slide now that we have discussed on how",
    "start": "1556320",
    "end": "1561360"
  },
  {
    "text": "parallel we enhances the job performance and what are the parameters and how the parallel",
    "start": "1561360",
    "end": "1566799"
  },
  {
    "text": "read operation takes place it is important to understand that we do not over run the jdbc data store",
    "start": "1566799",
    "end": "1573279"
  },
  {
    "text": "by that i mean that a customer or the jdbc data store might have a configuration which can allow only a",
    "start": "1573279",
    "end": "1579520"
  },
  {
    "text": "maximum of say 10 connections and each my parallel lead and specifying my number of partition or hash partition as",
    "start": "1579520",
    "end": "1586080"
  },
  {
    "text": "100 this is definitely uh this will definitely cause a problem and my data so so i should also know about the",
    "start": "1586080",
    "end": "1592880"
  },
  {
    "text": "maximum connection my database supports since their queries fired be it on a parallel operation or a default",
    "start": "1592880",
    "end": "1599360"
  },
  {
    "text": "operation i should also check the connection timeout value because any of the query which may",
    "start": "1599360",
    "end": "1605360"
  },
  {
    "text": "take longer can also face timeout values so it's important to review the timeout values and more importantly i should",
    "start": "1605360",
    "end": "1612240"
  },
  {
    "text": "also uh be aware of my cpu and memory performance of jdbc data stored because",
    "start": "1612240",
    "end": "1617360"
  },
  {
    "text": "any of the query uh might have a uniform data and that can run longer and",
    "start": "1617360",
    "end": "1623840"
  },
  {
    "text": "eventually cost out of memory so once i review these parameters i can i should",
    "start": "1623840",
    "end": "1628880"
  },
  {
    "text": "go with parallel read operation and in most of the cases we do see that",
    "start": "1628880",
    "end": "1634000"
  },
  {
    "text": "there is no problem as such but it's always better to review these parameters on the jdbc data store beforehand",
    "start": "1634000",
    "end": "1641279"
  },
  {
    "text": "so moving on to the next slide so i have a small demo where i will be",
    "start": "1641279",
    "end": "1647600"
  },
  {
    "start": "1645000",
    "end": "2224000"
  },
  {
    "text": "using glue dynamic frame operations for default read and parallel weeks but before i do that rob do we have any",
    "start": "1647600",
    "end": "1654080"
  },
  {
    "text": "questions i think so we don't currently have any questions from the audience all new",
    "start": "1654080",
    "end": "1659120"
  },
  {
    "text": "members please make sure to post your questions here and we can maybe answer some of those as we uh get through the demo here so thank you i'll let you go",
    "start": "1659120",
    "end": "1665679"
  },
  {
    "text": "through your demo okay thank you rob so",
    "start": "1665679",
    "end": "1671360"
  },
  {
    "text": "this is my demo where i'm using apache glue up sorry i'm using aws blue studio",
    "start": "1671360",
    "end": "1678240"
  },
  {
    "text": "to create a job this particular job basically is i have a my equal rds",
    "start": "1678240",
    "end": "1684799"
  },
  {
    "text": "and that will be used as a source and i'll compare the default feed and i'll also compare how the parallel the",
    "start": "1684799",
    "end": "1692159"
  },
  {
    "text": "execution type and the code works so i'll go with the default option in blue studio and do a create job",
    "start": "1692159",
    "end": "1700398"
  },
  {
    "text": "since i need to specify my custom jdbc url i'll go to the script option and i",
    "start": "1702080",
    "end": "1707520"
  },
  {
    "text": "will do an edit script firstly here i'll import a couple of",
    "start": "1707520",
    "end": "1713440"
  },
  {
    "text": "libraries so i'll import boto3 and the json library",
    "start": "1713440",
    "end": "1719840"
  },
  {
    "text": "we are using amazon supporter 3 is the amazon python library and we are using it to call out aws secret manager",
    "start": "1723520",
    "end": "1732559"
  },
  {
    "text": "so let me remove this part first",
    "start": "1732880",
    "end": "1737080"
  },
  {
    "text": "so i'm creating a 103 client here and with aws secrets manager i'm pulling",
    "start": "1738640",
    "end": "1744559"
  },
  {
    "text": "out the details like database uh username url password and the table name",
    "start": "1744559",
    "end": "1750240"
  },
  {
    "text": "so by this i need not to hardcode my database credentials in my code",
    "start": "1750240",
    "end": "1755360"
  },
  {
    "text": "then i'm creating a data source 0 named blue dynamic frame with mysql as my",
    "start": "1755360",
    "end": "1761360"
  },
  {
    "text": "connection type and then i pull out the the username password url and the table name",
    "start": "1761360",
    "end": "1767520"
  },
  {
    "text": "so this is the default option and how i can read from jdbc data so and i'll call a sparkaction.count",
    "start": "1767520",
    "end": "1775360"
  },
  {
    "text": "to make sure my job run so as we know for a spark job to run we need to call upon an action so dot count is my action",
    "start": "1775360",
    "end": "1782000"
  },
  {
    "text": "here and now before we run this job i need to fill in some job details here",
    "start": "1782000",
    "end": "1789760"
  },
  {
    "text": "so i will name this job now let me name is a test",
    "start": "1790080",
    "end": "1795360"
  },
  {
    "text": "i'll specify my i am load to be used here i will use glue 3.0 the latest available",
    "start": "1795360",
    "end": "1802159"
  },
  {
    "text": "version and i'll stick to the default g1x worker type and the number of worker 10",
    "start": "1802159",
    "end": "1807679"
  },
  {
    "text": "provided to us",
    "start": "1807679",
    "end": "1810480"
  },
  {
    "text": "i will make sure that i have spark ui enabled uh we'll be reviewing spark ui for each of the job run and we'll also",
    "start": "1813279",
    "end": "1819840"
  },
  {
    "text": "check up on the dag on how it looks with the default and parallel agreed",
    "start": "1819840",
    "end": "1825279"
  },
  {
    "text": "and lastly i will specify a blue connection this connection will make sure that my blue job has access to the jdbc data",
    "start": "1825440",
    "end": "1832799"
  },
  {
    "text": "store and this looks fine now so i'll save this job and now i can run the job",
    "start": "1832799",
    "end": "1841039"
  },
  {
    "text": "so in the run status i see uh the job is now running so let me jump over once this is",
    "start": "1842880",
    "end": "1848880"
  },
  {
    "text": "succeeded",
    "start": "1848880",
    "end": "1851840"
  },
  {
    "text": "okay so the run status is marked as succeeded now and important point to review here is",
    "start": "1857760",
    "end": "1863519"
  },
  {
    "text": "the execution type which is 18 minutes so the default uh read jdbc read with my",
    "start": "1863519",
    "end": "1870640"
  },
  {
    "text": "data store took 18 minutes in this case and if i jump over to the other tab and",
    "start": "1870640",
    "end": "1876399"
  },
  {
    "text": "review the spark ui so let me have a close look at the stack and we can see there are two stages and",
    "start": "1876399",
    "end": "1882640"
  },
  {
    "text": "the entire operation is single credit so this proves again that a single query a single executor is used to read the",
    "start": "1882640",
    "end": "1889760"
  },
  {
    "text": "entire table and that's and that is how it took 18 minutes for the drop to be completed",
    "start": "1889760",
    "end": "1896640"
  },
  {
    "text": "so in the script uh let me add few parameters here so include dynamic stream we talked about",
    "start": "1898159",
    "end": "1904799"
  },
  {
    "text": "hash field and hash partition parameter so let me add that particular parameters",
    "start": "1904799",
    "end": "1910399"
  },
  {
    "text": "while i create my dynamic frame data source 0 so i'm using my hash field as id column",
    "start": "1910399",
    "end": "1917039"
  },
  {
    "text": "in my data store and i'm specifying the hash partition as 36",
    "start": "1917039",
    "end": "1922960"
  },
  {
    "text": "36 uh is from the the worker type i'm using so g1x will",
    "start": "1922960",
    "end": "1929120"
  },
  {
    "text": "give me four executor codes and i'm using 10 worker type one of which is the master so i'm left",
    "start": "1929120",
    "end": "1934799"
  },
  {
    "text": "with nine nine executors so nine executor each with four",
    "start": "1934799",
    "end": "1939919"
  },
  {
    "text": "four cores will give me nine multiply by four thirty six and that's how i get a hash partition at thirty six",
    "start": "1939919",
    "end": "1948840"
  },
  {
    "text": "so with couple of parameters added hash field and hash partition let me save this talk",
    "start": "1949600",
    "end": "1956720"
  },
  {
    "text": "and i will run this job",
    "start": "1959279",
    "end": "1963080"
  },
  {
    "text": "the run status is running so let me jump over when this is completed",
    "start": "1966080",
    "end": "1971720"
  },
  {
    "text": "so now the rank status is marked has succeeded and the interesting point here is the",
    "start": "1976000",
    "end": "1981120"
  },
  {
    "text": "execution time is down to nine minutes as compared to the default feed where we saw the execution time was 18 minutes so",
    "start": "1981120",
    "end": "1988320"
  },
  {
    "text": "we can see that top is basically 50 i took basically 50 less time to be",
    "start": "1988320",
    "end": "1994000"
  },
  {
    "text": "completed so if i jump over to spark ui for this particular job",
    "start": "1994000",
    "end": "2000480"
  },
  {
    "text": "and let me review the dac here so here's something interesting in the tag i i should see 36 connection so if i",
    "start": "2000799",
    "end": "2007519"
  },
  {
    "text": "scroll all the way to the right so it ends on 36 that means i have made",
    "start": "2007519",
    "end": "2012880"
  },
  {
    "text": "my job has made basically 36 connections to the jdbc data store doing some",
    "start": "2012880",
    "end": "2018480"
  },
  {
    "text": "map operations because of the dot count i have called upon and that's why i see",
    "start": "2018480",
    "end": "2024720"
  },
  {
    "text": "the operation is faster the job is faster now that we have played around with hash",
    "start": "2024720",
    "end": "2030000"
  },
  {
    "text": "field let me go back to my script and instead of hash field parameter i",
    "start": "2030000",
    "end": "2036880"
  },
  {
    "text": "will use a hash expression parameter i will stick to the id column for this example as well",
    "start": "2036880",
    "end": "2043120"
  },
  {
    "text": "and of course i'm not changing anything so my hash partition will be 36",
    "start": "2043120",
    "end": "2049039"
  },
  {
    "text": "so let's jump over once this is completed",
    "start": "2050800",
    "end": "2055240"
  },
  {
    "text": "okay so the run status we can see now is marked as succeeded and the execution time is 12 minutes which is again faster",
    "start": "2061839",
    "end": "2069520"
  },
  {
    "text": "than 18 minutes for the default jdbcd",
    "start": "2069520",
    "end": "2074800"
  },
  {
    "text": "and then we see with hash field we saw nine minutes as the execution time and and with hash",
    "start": "2074800",
    "end": "2081118"
  },
  {
    "text": "expression the execution time is down to 12 minutes so from this we can see the execution",
    "start": "2081119",
    "end": "2086480"
  },
  {
    "text": "time differences and if i go back to the spark ui for the last job run",
    "start": "2086480",
    "end": "2093440"
  },
  {
    "text": "i see the dial is pretty much similar where my job is making 36 connections",
    "start": "2094960",
    "end": "2100240"
  },
  {
    "text": "so let me scroll down all the way to the right it's ending with 36.",
    "start": "2100240",
    "end": "2106720"
  },
  {
    "text": "so it says that there is a parallel operation that's from the dag so if i go to let me go back quickly to",
    "start": "2108160",
    "end": "2114800"
  },
  {
    "text": "the first job run which was a default jdbc read and in the tag we can see that",
    "start": "2114800",
    "end": "2120160"
  },
  {
    "text": "it was single threaded we go to the second job run with hash field option",
    "start": "2120160",
    "end": "2126000"
  },
  {
    "text": "the dag we see is 36 connections being made and same to the last object the dial is",
    "start": "2126000",
    "end": "2133119"
  },
  {
    "text": "pretty much similar so parallel connections being made showing that parallel read operation is happening and we can",
    "start": "2133119",
    "end": "2139440"
  },
  {
    "text": "clearly see uh the execution time differs so for tables with with millions and billions",
    "start": "2139440",
    "end": "2145440"
  },
  {
    "text": "of records in it with more columns in it uh the jdbc parallel operation excels well and we can see a",
    "start": "2145440",
    "end": "2152400"
  },
  {
    "text": "considerable difference uh in the job execution time so that should be it from my end over to",
    "start": "2152400",
    "end": "2158880"
  },
  {
    "text": "you rob thanks appreciate the there's definitely some uh improvements that",
    "start": "2158880",
    "end": "2164480"
  },
  {
    "text": "could be made with those jdbc parallel reads so thanks for sharing that information today uh anything else that",
    "start": "2164480",
    "end": "2170079"
  },
  {
    "text": "you'd want to share today for our audience before we go",
    "start": "2170079",
    "end": "2174480"
  },
  {
    "text": "that's all from my end thanks job thanks for having me thank you very much perfect thank you so",
    "start": "2175119",
    "end": "2181680"
  },
  {
    "text": "everyone today we looked at jdbc read performance improvements with aws glue and saw how those could definitely be",
    "start": "2181680",
    "end": "2187280"
  },
  {
    "text": "increased in the performance wise if there are any questions that were not answered today you can post your questions over on repost.aws",
    "start": "2187280",
    "end": "2194480"
  },
  {
    "text": "and email us any feedback to aws supports you at amazon.com we want to hear from you tell us what else you'd",
    "start": "2194480",
    "end": "2200320"
  },
  {
    "text": "like to see on the show thank you for joining us at aws supports you and hope you have a happy day and happy cloud",
    "start": "2200320",
    "end": "2206000"
  },
  {
    "text": "computing [Music]",
    "start": "2206000",
    "end": "2220070"
  },
  {
    "text": "you",
    "start": "2222880",
    "end": "2224960"
  }
]