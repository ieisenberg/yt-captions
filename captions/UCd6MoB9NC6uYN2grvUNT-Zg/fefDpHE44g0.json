[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "morning everyone thanks for coming uh today you're going to learn about how to make your applications go faster we're",
    "start": "1560",
    "end": "8519"
  },
  {
    "text": "going to talk about mcash D and elasticache and then you'll hear from uh khed alumi who's the SVP of",
    "start": "8519",
    "end": "16520"
  },
  {
    "text": "Technology of Health Guru about how they've scaled their applications and my name is rul Pathak and I'm the uh senior",
    "start": "16520",
    "end": "23400"
  },
  {
    "text": "product manager in database services and I've worked on elasticache and RDS um so",
    "start": "23400",
    "end": "28519"
  },
  {
    "text": "let's get started first thing I'll do is give a very quick overview of caching and then we'll talk",
    "start": "28519",
    "end": "34079"
  },
  {
    "text": "about MC D how elastic Cache can help and then I'll also talk about some of",
    "start": "34079",
    "end": "39559"
  },
  {
    "text": "the common use cases that we see and ways that we see people using mcash de",
    "start": "39559",
    "end": "44960"
  },
  {
    "text": "in elastic cache productively and some common gotos to avoid so at its core cache is a very",
    "start": "44960",
    "end": "52520"
  },
  {
    "start": "50000",
    "end": "50000"
  },
  {
    "text": "simple thing it's a way to speed up your applications by storing data in memory and it's a way to reduce load on your",
    "start": "52520",
    "end": "59480"
  },
  {
    "text": "database and you know like all of us we want our applications to go faster and we want",
    "start": "59480",
    "end": "65799"
  },
  {
    "text": "our users to have more responsive applications and typically caching provides the most benefits when we're",
    "start": "65799",
    "end": "71520"
  },
  {
    "text": "dealing with applications that are read intensive and most apps that we see today tend to follow that pattern often",
    "start": "71520",
    "end": "77640"
  },
  {
    "text": "as high as 80 to 90% read Centric and as we all know memory is orders of",
    "start": "77640",
    "end": "83000"
  },
  {
    "text": "magnitude faster we'll get responses in micros seconds instead of",
    "start": "83000",
    "end": "88280"
  },
  {
    "start": "88000",
    "end": "88000"
  },
  {
    "text": "milliseconds conceptually caches are simple you start off with a cach that's empty thinking about information that",
    "start": "88280",
    "end": "94840"
  },
  {
    "text": "doesn't change all that often so for example a phone number the first step is to look in a cache since it's empty",
    "start": "94840",
    "end": "101240"
  },
  {
    "text": "we'll have a Miss we then go to the database we update the value stored in the cach and then for subsequent",
    "start": "101240",
    "end": "107799"
  },
  {
    "text": "accesses we're just going into that cache to get the information and we save the DB hit and uh can focus on just",
    "start": "107799",
    "end": "114880"
  },
  {
    "text": "getting that information directly out of the cach so now what one of the things that",
    "start": "114880",
    "end": "120039"
  },
  {
    "text": "we run into is what happens when a number changes and so now what you have is a situation where your cash is out of",
    "start": "120039",
    "end": "125759"
  },
  {
    "text": "date and you've got to go back to the database to get the new information update your cach and then you can",
    "start": "125759",
    "end": "132120"
  },
  {
    "text": "proceed as before and so that brings us to one of the issues with caches which is that while they can provide",
    "start": "132120",
    "end": "138239"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "significant benefits in terms of speedups you have to maintain coherence of the cach and make sure it's",
    "start": "138239",
    "end": "143440"
  },
  {
    "text": "consistent with the information in your database so mcash D um if you're not",
    "start": "143440",
    "end": "149879"
  },
  {
    "start": "148000",
    "end": "148000"
  },
  {
    "text": "familar with it it's a free open open source high performance inmemory key value store and it was developed in 2003",
    "start": "149879",
    "end": "157120"
  },
  {
    "text": "by Brad Fitzpatrick for live Journal as they were scaling and it's used by many of the top sites in the world today",
    "start": "157120",
    "end": "163400"
  },
  {
    "text": "YouTube Facebook Netflix Twitter um all of them use it to provide low latency",
    "start": "163400",
    "end": "169319"
  },
  {
    "text": "responses for their customers and from an architectural perspective mcash D is has two",
    "start": "169319",
    "end": "176239"
  },
  {
    "start": "172000",
    "end": "172000"
  },
  {
    "text": "components there's a client side piece which is an API and a c client library with some pretty simple operations sets",
    "start": "176239",
    "end": "183400"
  },
  {
    "text": "gets updates the client knows the location of all of your cach nodes and the cash",
    "start": "183400",
    "end": "189599"
  },
  {
    "text": "nodes are independent so they don't talk to each other what happens is when",
    "start": "189599",
    "end": "194680"
  },
  {
    "text": "you're trying to store data in the cach the client figures out which node that data should live on and then uh data",
    "start": "194680",
    "end": "202360"
  },
  {
    "text": "stored in the cach at that place when you go to do a lookup the client can figure out which node to go to directly",
    "start": "202360",
    "end": "209000"
  },
  {
    "text": "uh there are a couple of ways to set up the hashing schemes there's the simplest one is a modulo method where the key is",
    "start": "209000",
    "end": "214760"
  },
  {
    "text": "basically rotated across each of the cache nodes um that's typically not recommended because what happens is if a",
    "start": "214760",
    "end": "221000"
  },
  {
    "text": "node goes down and you have to redistribute your cach you're going to have to have a complete reshuffling of",
    "start": "221000",
    "end": "226560"
  },
  {
    "text": "everything and so consistent hashing is the way to go and most clients support both variants um and that's something",
    "start": "226560",
    "end": "232439"
  },
  {
    "text": "that we recommend and then within within each server the cache is an associative",
    "start": "232439",
    "end": "238159"
  },
  {
    "text": "key value array and you can store sorry you can store arbitrary data in the cache but",
    "start": "238159",
    "end": "244319"
  },
  {
    "text": "essentially it's just a simple lookup table and the other part to remember is that by default mcash D is least",
    "start": "244319",
    "end": "250760"
  },
  {
    "text": "recently used so the information ages out of the cache and there's no persistence in the system and so you",
    "start": "250760",
    "end": "256320"
  },
  {
    "text": "have to keep it you have to maintain you have to just assume that that data can go away if the cache does and have it",
    "start": "256320",
    "end": "262800"
  },
  {
    "text": "persisted elsewhere you can store ARB arbitrary data in MCD it's not just about data",
    "start": "262800",
    "end": "269720"
  },
  {
    "start": "265000",
    "end": "265000"
  },
  {
    "text": "databased query results it's also about HTML Snippets couple of things to note your keys are limited to 250 bytes in",
    "start": "269720",
    "end": "276479"
  },
  {
    "text": "size and your values are limited to 1 Megabyte however you can chunk them and then use multiple um store larger",
    "start": "276479",
    "end": "283160"
  },
  {
    "text": "objects you just have to maintain splitting and rejoining within your",
    "start": "283160",
    "end": "288280"
  },
  {
    "text": "application so elastic cache elastic cache is um a fully manag web service",
    "start": "288280",
    "end": "294600"
  },
  {
    "text": "that allows you to create mcash D compatible cache clusters in the cloud",
    "start": "294600",
    "end": "299840"
  },
  {
    "text": "and it's a pay as you go system as with all AWS Services designed to allow you to scale up as you need pay as you grow",
    "start": "299840",
    "end": "307280"
  },
  {
    "text": "uh the diagram that you're seeing here is how a typical application will be deployed and so you have your users your",
    "start": "307280",
    "end": "313120"
  },
  {
    "text": "web front end is on ec2 that's set up within an ec2 Security Group and your",
    "start": "313120",
    "end": "318319"
  },
  {
    "text": "application stack is where you'll have your client library with has your API where you have your mcash node",
    "start": "318319",
    "end": "325520"
  },
  {
    "text": "configurations um you then have a cache cluster which has its own Security Group and that's uh a benefit of elasticache",
    "start": "325520",
    "end": "331600"
  },
  {
    "text": "relative to mcash D by itself is that it's by default secure and you have to allow it access uh you have to allow",
    "start": "331600",
    "end": "338479"
  },
  {
    "text": "your application tier to have access to your cach normally mcash D runs wide open and you have to deal with security",
    "start": "338479",
    "end": "344360"
  },
  {
    "text": "by yourself um the other thing that we provide as part of elasticache is um",
    "start": "344360",
    "end": "350160"
  },
  {
    "text": "metrics in Cloud watch so you can keep keep keep tabs on hit rates eviction",
    "start": "350160",
    "end": "355960"
  },
  {
    "text": "rates what's happening whether some nodes are being used more than others and it's a way to monitor your",
    "start": "355960",
    "end": "362039"
  },
  {
    "start": "362000",
    "end": "362000"
  },
  {
    "text": "system and the primary benefits around elasticache relate to manageability and",
    "start": "362039",
    "end": "367160"
  },
  {
    "text": "so it's easy to deploy uh simple to do within the AWS console or using",
    "start": "367160",
    "end": "372360"
  },
  {
    "text": "apis migration is straightforward we've designed it to be completely compatible with mcash d and so if you're already",
    "start": "372360",
    "end": "378319"
  },
  {
    "text": "using mcash in your applications all you have to do is update your configuration with the endpoints of your elasticache",
    "start": "378319",
    "end": "385000"
  },
  {
    "text": "nodes and your application should work as is um it's it's easy to administer and",
    "start": "385000",
    "end": "391080"
  },
  {
    "text": "so what you have is we have observers that monitor your cache nodes if there's ever a problem we will replace them and",
    "start": "391080",
    "end": "397960"
  },
  {
    "text": "that's also something to keep in mind is a lot of people think about um using MCD in a failover mode so for example if a",
    "start": "397960",
    "end": "404160"
  },
  {
    "text": "node goes down they have it configured to go to a different node that's typically not a recommended practice uh",
    "start": "404160",
    "end": "410080"
  },
  {
    "text": "because you don't want to have cache data being shuffled around it's normally best to wait and tolerate slightly higher latency on a small percentage of",
    "start": "410080",
    "end": "417080"
  },
  {
    "text": "your requests than to completely reshuffle everything for everybody and I'll talk a little bit more about that",
    "start": "417080",
    "end": "422199"
  },
  {
    "text": "when we get into some of the gotas um El last cach is also easy to scale it's uh trivial to add a new cache",
    "start": "422199",
    "end": "429319"
  },
  {
    "text": "node one of the challenges that we've had in the past is um when you do add or remove nodes you do have to update your",
    "start": "429319",
    "end": "435720"
  },
  {
    "text": "mcash D configuration on the client side and I'll talk a little bit about a new feature we just launched this morning",
    "start": "435720",
    "end": "441639"
  },
  {
    "text": "that actually makes that problem go away uh and should provide a lot of benefits to customers that are looking for scalable caching solutions that move up",
    "start": "441639",
    "end": "448039"
  },
  {
    "text": "and down more easily and then on the security side it's easy to secure and control access to your",
    "start": "448039",
    "end": "454720"
  },
  {
    "text": "mcash D layer and to decide uh what aspects of your application and architecture will be able to talk to it",
    "start": "454720",
    "end": "461639"
  },
  {
    "text": "another big benefit of having your caching system live in elastic cache is that all of your application endpoints",
    "start": "461639",
    "end": "468720"
  },
  {
    "text": "and your web servers can talk to the same cache and so you get the benefit of a distributed cache system uh without",
    "start": "468720",
    "end": "474280"
  },
  {
    "text": "having to maintain it on each of your individual servers so common use cases and design",
    "start": "474280",
    "end": "480639"
  },
  {
    "text": "patterns that we see um we really have elasticache customers in a range of",
    "start": "480639",
    "end": "486000"
  },
  {
    "start": "482000",
    "end": "482000"
  },
  {
    "text": "different categories social networks as you can imagine they have a large volume of users reading and writing and",
    "start": "486000",
    "end": "491360"
  },
  {
    "text": "interacting with content and for them low latency is critical on the gaming side we see elasticache used very often",
    "start": "491360",
    "end": "499080"
  },
  {
    "text": "here for providing fast responses and for maintaining transient data such as what actions people have taken scores",
    "start": "499080",
    "end": "505919"
  },
  {
    "text": "things like that media sites again these tend to be very read Centric and uh one",
    "start": "505919",
    "end": "511759"
  },
  {
    "text": "they need to provide low response times for their customers and khed will talk about their particular experience in",
    "start": "511759",
    "end": "517839"
  },
  {
    "text": "detail and dive into some of the architecture decisions that they've made portals uh again cache content is the",
    "start": "517839",
    "end": "524959"
  },
  {
    "text": "key here typically what you see in any Q&A site is a a ratio of writers people",
    "start": "524959",
    "end": "530160"
  },
  {
    "text": "that are responding to questions versus readers is very heavily skewed on the read side uh when I worked um in a past",
    "start": "530160",
    "end": "537040"
  },
  {
    "text": "life at a Q&A and review site we typically saw only 1% of people were actually generating content and the rest were consuming it and so those sites",
    "start": "537040",
    "end": "543920"
  },
  {
    "text": "benefit greatly from caching as well so recommendation engines here there are a couple of interesting use cases um we",
    "start": "543920",
    "end": "550959"
  },
  {
    "text": "typically see customers storing expensive computations and intermediate results in memory and so because you",
    "start": "550959",
    "end": "557399"
  },
  {
    "text": "don't want to have to recompute every time they store those results in a cache and then look them up as they go again",
    "start": "557399",
    "end": "563040"
  },
  {
    "text": "providing better responses and then another area where we've seen a lot of growth is with mobile apps and so again",
    "start": "563040",
    "end": "569760"
  },
  {
    "text": "here you've typically got a much lower bandwidth connection to the mobile device and people have on their machines",
    "start": "569760",
    "end": "575320"
  },
  {
    "text": "uh at home and so providing Fast Response times and low latency becomes",
    "start": "575320",
    "end": "581160"
  },
  {
    "text": "critical so here's a typical gaming deployment and it's also a way to see how you can combine elasticache with a",
    "start": "581160",
    "end": "587640"
  },
  {
    "start": "585000",
    "end": "585000"
  },
  {
    "text": "range of services that we offer at AWS and so walking through the system from the bottom left you've got users",
    "start": "587640",
    "end": "594200"
  },
  {
    "text": "coming in to a load balancer and that's connected to their frontend application layer with an",
    "start": "594200",
    "end": "600079"
  },
  {
    "text": "autoscaling group and so that can scale up and down as their traffic changes the web tier is connected to an",
    "start": "600079",
    "end": "607680"
  },
  {
    "text": "Amazon elasticache stack uh again to have their low latency response times for things like scores player actions",
    "start": "607680",
    "end": "614160"
  },
  {
    "text": "what a player's currently doing and that's typically used in front of Amazon RDS which you can see here set up in a",
    "start": "614160",
    "end": "620320"
  },
  {
    "text": "multi-az setting and multi-az is an RDS feature which allows you to have a hot standby in a second availability Zone in",
    "start": "620320",
    "end": "626959"
  },
  {
    "text": "the event of any issues and and typically what you have is dynamic uh",
    "start": "626959",
    "end": "632480"
  },
  {
    "text": "content with low latency coming out of elasticache but larger objects coming out of S3 and cloudfront and so these",
    "start": "632480",
    "end": "638440"
  },
  {
    "text": "are composed together to provide a rich gaming experience but also be able to provide high",
    "start": "638440",
    "end": "645560"
  },
  {
    "text": "responsiveness so common design patterns and use cases that we see uh one very",
    "start": "645760",
    "end": "651680"
  },
  {
    "start": "646000",
    "end": "646000"
  },
  {
    "text": "common use for Las cach is maintaining web server session State and so this allows you to keep user sessions in your",
    "start": "651680",
    "end": "657959"
  },
  {
    "text": "cache and in your database and what that means is that your front end tier can be stateless and the advantage there is",
    "start": "657959",
    "end": "663800"
  },
  {
    "text": "that it allows you to be much more scalable and responsive on the front end uh while still maintaining a low latency",
    "start": "663800",
    "end": "669279"
  },
  {
    "text": "pathway for what a user's actual session is doing without having to go to a database to get it um Again State management comes up",
    "start": "669279",
    "end": "676760"
  },
  {
    "text": "very often people are having rapid interactions and rather to rather than deal with a roundtrip latency to a",
    "start": "676760",
    "end": "682040"
  },
  {
    "text": "database every time uh operating out of the cach and that scenario is important one thing to remember is that you do",
    "start": "682040",
    "end": "688480"
  },
  {
    "text": "want to provide persistence and you do have to manage that at your application Level and so but you can do that",
    "start": "688480",
    "end": "694040"
  },
  {
    "text": "asynchronously and use the cach to get response time but still use the database for persistence and then the final part",
    "start": "694040",
    "end": "701480"
  },
  {
    "text": "another one where we see lots of usage is in um maintaining things that are expensive to compute and we see this a",
    "start": "701480",
    "end": "707480"
  },
  {
    "text": "lot in Social for example lists of most popular restaurants um those things are expensive at scale and people keep those",
    "start": "707480",
    "end": "714399"
  },
  {
    "text": "around and update them periodically and serve them out of cash picture ID is again tagged keeping",
    "start": "714399",
    "end": "721440"
  },
  {
    "text": "keeping track of a largely a large data set that's changing uh that's another thing that people do",
    "start": "721440",
    "end": "728480"
  },
  {
    "text": "often so anti patterns this is uh an area where we we talk to a lot of",
    "start": "728480",
    "end": "733959"
  },
  {
    "start": "729000",
    "end": "729000"
  },
  {
    "text": "customers and we help them troubleshoot issues that they run into and uh what we see fairly often is that people assume a",
    "start": "733959",
    "end": "740399"
  },
  {
    "text": "certain amount of latency or a certain percentage of cash hits and so design their applications in a way that end up",
    "start": "740399",
    "end": "745720"
  },
  {
    "text": "making them somewhat brittle and so what we recommend is that uh you should assume especially in a distributed",
    "start": "745720",
    "end": "751720"
  },
  {
    "text": "environment that you may need to have deal with unpredictability from time to time and build in your application logic",
    "start": "751720",
    "end": "757519"
  },
  {
    "text": "to allow people to deal with",
    "start": "757519",
    "end": "760680"
  },
  {
    "text": "that um you know it's uh you're typically dealing with complex distributed systems and there may be",
    "start": "762680",
    "end": "768199"
  },
  {
    "text": "different uh amounts of bottleneck different amounts of traffic uh different objects being requested and so",
    "start": "768199",
    "end": "774440"
  },
  {
    "text": "having the application have some graceful mode of dealing with unexpected events uh is what I'm talking about on",
    "start": "774440",
    "end": "780639"
  },
  {
    "text": "that front and we can Circle back um to come into it later another issue that people run into",
    "start": "780639",
    "end": "786839"
  },
  {
    "text": "is not really thinking about maintaining consistency how long should an object actually live in your cach and mcash d",
    "start": "786839",
    "end": "792519"
  },
  {
    "text": "and elastic cache allow you to set a time to live uh which essentially lets",
    "start": "792519",
    "end": "797720"
  },
  {
    "text": "the cash know when an object has expired and this helps you maintain consistency on the cash side because you can set a",
    "start": "797720",
    "end": "804560"
  },
  {
    "text": "time to live that's appropriate for the type of data that you're dealing with and so don't serve stale data to your",
    "start": "804560",
    "end": "810600"
  },
  {
    "text": "users and to your customers another area that we see people do um do something that's",
    "start": "810600",
    "end": "816839"
  },
  {
    "text": "suboptimal is when they uh sequentially send single operations to the cache and so what happens here is you have the",
    "start": "816839",
    "end": "822560"
  },
  {
    "text": "overhead of making a request and waiting to get a response whereas mcash and elasticache apis provide you the ability",
    "start": "822560",
    "end": "829199"
  },
  {
    "text": "to do batch operations and so you only incur the connection overhead once and get back multiple result",
    "start": "829199",
    "end": "835519"
  },
  {
    "text": "sets and then I talked briefly about this uh redist distribution for when",
    "start": "835519",
    "end": "840600"
  },
  {
    "text": "nodes go down or if they're unreachable uh generally the best practice is to set up your client so that it waits because",
    "start": "840600",
    "end": "847199"
  },
  {
    "text": "typically some of these things are transient and in the case of elasticache we monitor your nodes and we'll replace them in the inent in the event of any",
    "start": "847199",
    "end": "853839"
  },
  {
    "text": "issues and so if you have a distributed setup um waiting is best because",
    "start": "853839",
    "end": "859279"
  },
  {
    "text": "typically only request going to a particular node will be affected whereas the rest of your application can perform",
    "start": "859279",
    "end": "865320"
  },
  {
    "text": "um at a very high performance level and back off on the cash so that's something",
    "start": "865320",
    "end": "872199"
  },
  {
    "text": "that needs to be managed at the application Level um it's really up to you but it is",
    "start": "872199",
    "end": "878600"
  },
  {
    "text": "recommended and then aods again make sure that uh the application doesn't",
    "start": "878600",
    "end": "883880"
  },
  {
    "text": "frail or freeze if there's a timeout or if something happens really it's about planning for things that may not that",
    "start": "883880",
    "end": "890920"
  },
  {
    "text": "you may not have expected to occur and allowing your application to go on so that at least while some portion of your",
    "start": "890920",
    "end": "896560"
  },
  {
    "text": "users may have a suboptimal experience for the vast major majority of them things are functioning",
    "start": "896560",
    "end": "902199"
  },
  {
    "text": "well so some some things to note when you're configuring uh elasticache uh so",
    "start": "902199",
    "end": "907759"
  },
  {
    "text": "mcash D requires memory for every connection and that's something that you have to configure independently so it's",
    "start": "907759",
    "end": "913199"
  },
  {
    "text": "important to think about what your usage pattern is going to be how many connections you you expect to have open",
    "start": "913199",
    "end": "918440"
  },
  {
    "text": "at any one time and we expose a lot of the parameters that you need to configure your meach server and you can",
    "start": "918440",
    "end": "923839"
  },
  {
    "text": "do that within the console another assumption that people make often uh cash has a binary as well",
    "start": "923839",
    "end": "930680"
  },
  {
    "text": "as an asky protocol um they often assume that binary is faster and that's not often the case and a lot of that's",
    "start": "930680",
    "end": "937199"
  },
  {
    "text": "related to batch operations that we just talked about in the overhead of having to do everything sequentially so it's",
    "start": "937199",
    "end": "943160"
  },
  {
    "text": "really just important to understand your use case and to test it uh and profile your application to see what uh what",
    "start": "943160",
    "end": "949600"
  },
  {
    "text": "will perform best for you so some best practices uh one is",
    "start": "949600",
    "end": "956160"
  },
  {
    "start": "953000",
    "end": "953000"
  },
  {
    "text": "share your objects so with elasticache you have a caching lay that sits below your web layer and all of your web",
    "start": "956160",
    "end": "961920"
  },
  {
    "text": "servers can talk to the same elasticache cach cluster and so by sharing this",
    "start": "961920",
    "end": "967480"
  },
  {
    "text": "cache cluster and providing all of your web tier access to it you can actually benefit from usage patterns across all",
    "start": "967480",
    "end": "972959"
  },
  {
    "text": "of your tiers uh think about your times to live think about uh the memory you need for",
    "start": "972959",
    "end": "978440"
  },
  {
    "text": "connections the other thing is take advantage of cloud watch set alarms don't be surprised when things happen we",
    "start": "978440",
    "end": "984920"
  },
  {
    "text": "provide a lot of metrics in terms of how your cash is performing and CED will we talk about how they've used some of",
    "start": "984920",
    "end": "990240"
  },
  {
    "text": "those consistent hashing we talked about and the the main benefit here is that if",
    "start": "990240",
    "end": "995279"
  },
  {
    "text": "there is a problem with your node consistent hashing minimizes the impact on your cluster so in a modulo hashing",
    "start": "995279",
    "end": "1001600"
  },
  {
    "text": "environment if you had six nodes and one went down your modulo denominator changes and so the cache would actually",
    "start": "1001600",
    "end": "1007600"
  },
  {
    "text": "reshuffle all of the data in the cluster and that would create a massive problem on the application side because every",
    "start": "1007600",
    "end": "1013560"
  },
  {
    "text": "single user would have a NE negative experience while data is being redistributed whereas in a consistent hashing model only nodes adjacent to",
    "start": "1013560",
    "end": "1020880"
  },
  {
    "text": "that would be affected so the majority of your users wouldn't have any trouble uh and one other thing this is a",
    "start": "1020880",
    "end": "1026640"
  },
  {
    "text": "new feature that we just launched autodiscovery uh that was launched this morning and um that's something that I",
    "start": "1026640",
    "end": "1032000"
  },
  {
    "text": "highly recommend you use and I'll talk about how that works so um autodiscovery so one of the",
    "start": "1032000",
    "end": "1039280"
  },
  {
    "text": "things that we hear a lot from customers is that yes elastic cash is great so MCD but we really dislike having to update",
    "start": "1039280",
    "end": "1045959"
  },
  {
    "text": "our application configuration every time we change the node accounts in our system or every time we need to scale",
    "start": "1045959",
    "end": "1052240"
  },
  {
    "text": "and so with autodiscovery what we what we decided to do was to create a system which allowed customers to dynamically",
    "start": "1052240",
    "end": "1058919"
  },
  {
    "text": "change the number of cache nodes in their cluster without having to change their applications without having to",
    "start": "1058919",
    "end": "1064520"
  },
  {
    "text": "restart them and one of the big benefits with the way that we've done this which is to provide configuration information",
    "start": "1064520",
    "end": "1071160"
  },
  {
    "text": "on every node in the cluster itself and then provide a smart client that's able to automatically update itself is that",
    "start": "1071160",
    "end": "1077480"
  },
  {
    "text": "you get all of the benefits of a Dynamic solution you only have to provide one endpoint but you don't pay any of the",
    "start": "1077480",
    "end": "1083080"
  },
  {
    "text": "penalties associated with a proxy based solution you don't have a bottleneck or you don't have CPU and network",
    "start": "1083080",
    "end": "1088520"
  },
  {
    "text": "transaction overhead and so we're really excited about this it's currently available um in Java and with a new",
    "start": "1088520",
    "end": "1097640"
  },
  {
    "text": "mcash D version engine we are also um actively working on a PHP model and",
    "start": "1097640",
    "end": "1103360"
  },
  {
    "text": "looking for people to participate in a private beta uh of that so please let me know if you'd be interested in doing and",
    "start": "1103360",
    "end": "1109120"
  },
  {
    "text": "we plan to add support for more languages um in",
    "start": "1109120",
    "end": "1115039"
  },
  {
    "text": "2013 so with that I'm going to turn over to uh Ked alad",
    "start": "1115080",
    "end": "1121210"
  },
  {
    "text": "[Applause]",
    "start": "1121210",
    "end": "1125778"
  },
  {
    "text": "great uh hello everyone I'm going to be talking a little bit about how health Guru used uh elastic hash to solve a",
    "start": "1127200",
    "end": "1134840"
  },
  {
    "text": "problem we had with one of our verticals um Let Me Tell you a little bit about",
    "start": "1134840",
    "end": "1141520"
  },
  {
    "text": "our agenda for this particular uh presentation primarily I'm going to be",
    "start": "1141520",
    "end": "1147480"
  },
  {
    "text": "talking a little bit uh about our company and then uh about a product that we developed for syndicating our",
    "start": "1147480",
    "end": "1152720"
  },
  {
    "text": "video uh I work for a media company so there's actually only one picture in my",
    "start": "1152720",
    "end": "1158880"
  },
  {
    "text": "presentation uh my wife claimed that I will lose all of you as a result but I'm",
    "start": "1158880",
    "end": "1164039"
  },
  {
    "text": "going to kind of try and engage you with my dialogue instead uh so let me tell you tell you a little bit about",
    "start": "1164039",
    "end": "1169320"
  },
  {
    "text": "health.com we're the largest provider of health information via video on the",
    "start": "1169320",
    "end": "1175520"
  },
  {
    "text": "internet currently uh we serve about a 100 million videos a month and uh we just",
    "start": "1175520",
    "end": "1182200"
  },
  {
    "text": "started syndicating those videos at the beginning of 2011 so we've uh we've seen a rather",
    "start": "1182200",
    "end": "1189880"
  },
  {
    "text": "exponential growth in the use of our our video uh on other health sites and that's really what I'm going to talk",
    "start": "1189880",
    "end": "1196000"
  },
  {
    "text": "about um we produce all of our videos in-house so we have a library of about",
    "start": "1196000",
    "end": "1201159"
  },
  {
    "text": "3,200 videos right now uh we shoot them all we film doctors and we produce a",
    "start": "1201159",
    "end": "1206600"
  },
  {
    "text": "approximately 100 a month so it's a relatively small body of content but it's really heavily distributed uh we",
    "start": "1206600",
    "end": "1213799"
  },
  {
    "text": "also develop web products around this content to uh encourage users to find the kind of health information they're",
    "start": "1213799",
    "end": "1219720"
  },
  {
    "text": "looking for so we've built products like quizzes uh surveys slideshows video a",
    "start": "1219720",
    "end": "1225000"
  },
  {
    "text": "video Q&A product that uh helps users drill down to the video questions that they're interested in getting",
    "start": "1225000",
    "end": "1230120"
  },
  {
    "text": "answered and uh all of our content because it's Health oriented has a lot of surrounding metadata with it uh we",
    "start": "1230120",
    "end": "1236880"
  },
  {
    "text": "have to carry along a lot of medical keywords we we hold transcripts we uh",
    "start": "1236880",
    "end": "1242120"
  },
  {
    "text": "all the content has information about who the expert is and what their qualifications are so that we can uh",
    "start": "1242120",
    "end": "1248440"
  },
  {
    "text": "make sure that people are confident that the video they're watching is actually expert in",
    "start": "1248440",
    "end": "1254559"
  },
  {
    "text": "nature okay so in 2008 we developed a",
    "start": "1254559",
    "end": "1259840"
  },
  {
    "text": "platform to uh serve video on the web uh we called it it's in 2010 we completely",
    "start": "1259840",
    "end": "1267440"
  },
  {
    "start": "1260000",
    "end": "1260000"
  },
  {
    "text": "revised it and changed the way that we did it uh and at that time we also adopted Amazon as our solution uh we",
    "start": "1267440",
    "end": "1274360"
  },
  {
    "text": "were we were really growing and uh we needed some scale so Amazon was a Amazon",
    "start": "1274360",
    "end": "1279960"
  },
  {
    "text": "web services was a logical choice for us uh the whole platform the core platform is built on lamp uh we use engine X",
    "start": "1279960",
    "end": "1286640"
  },
  {
    "text": "instead of Apache and the p is PHP uh we use MySQL as our as our",
    "start": "1286640",
    "end": "1292559"
  },
  {
    "text": "database solution we use RDS uh heavily and uh our our our architecture is built",
    "start": "1292559",
    "end": "1298159"
  },
  {
    "text": "on the Zen framework which incidentally if you're not using a framework for your PHP uh Zen is is really great has a",
    "start": "1298159",
    "end": "1304880"
  },
  {
    "text": "whole bunch of class connections to Amazon uh services that uh really encapsulate a lot of what uh Amazon web",
    "start": "1304880",
    "end": "1313279"
  },
  {
    "text": "services do uh our first platform was called Chocobo uh this is because we",
    "start": "1313279",
    "end": "1319679"
  },
  {
    "text": "have a chief software architect who is a huge Final Fantasy fan and forced me to",
    "start": "1319679",
    "end": "1325279"
  },
  {
    "text": "codename all of our projects using final fantasy summonable monsters I I don't know uh so this this",
    "start": "1325279",
    "end": "1334760"
  },
  {
    "text": "core platform that we started with uh it was really a Content om uh a CMS a a",
    "start": "1334760",
    "end": "1341320"
  },
  {
    "text": "basic MBC pattern to deliver a website we had some models that encapsulated things like uh what health content is",
    "start": "1341320",
    "end": "1348320"
  },
  {
    "text": "and and what an expert is and then some basic core Services things to do imag res sizing things to uh transcode a",
    "start": "1348320",
    "end": "1355000"
  },
  {
    "text": "video on the Fly um and when we built this platform we intended it to run on a single web",
    "start": "1355000",
    "end": "1361600"
  },
  {
    "text": "server supported by a local database and supported by a local solar install and",
    "start": "1361600",
    "end": "1366960"
  },
  {
    "text": "uh and and a caching service um quick show of hands who's not using MCD at",
    "start": "1366960",
    "end": "1373520"
  },
  {
    "text": "this point in their in their web services uh so quite a few people are still running with",
    "start": "1373520",
    "end": "1380360"
  },
  {
    "text": "just application and business logic okay so MCD is super easy to get up and",
    "start": "1380360",
    "end": "1387000"
  },
  {
    "text": "running literally if you're using a if you're using a web server um if you're using any kind of web",
    "start": "1387000",
    "end": "1394000"
  },
  {
    "text": "serving platform like Apache or Eng Gen X you literally just tell it hey I have I have men cached installed look here",
    "start": "1394000",
    "end": "1400200"
  },
  {
    "text": "first whenever you make any request so MCD is very very straightforward to get to get up and running um",
    "start": "1400200",
    "end": "1408559"
  },
  {
    "start": "1408000",
    "end": "1408000"
  },
  {
    "text": "I'm going to talk a little bit about a special case where men casd was not quite enough for us uh and uh what",
    "start": "1408559",
    "end": "1416240"
  },
  {
    "text": "happened is we we have all this great health content and a lot of other companies a lot of other websites",
    "start": "1416240",
    "end": "1422200"
  },
  {
    "text": "started approaching us and saying hey we really would love to have video on our health site how can we get it uh",
    "start": "1422200",
    "end": "1428440"
  },
  {
    "text": "initially we provided an embed very much like YouTube where you just kind of get the a code snip in and it loads an if",
    "start": "1428440",
    "end": "1434799"
  },
  {
    "text": "frame and has a little code in it that tells you about the video and you you watch it on your site but we needed a much uh more Enterprise",
    "start": "1434799",
    "end": "1441720"
  },
  {
    "text": "solution so we developed uh we we we thought that we what we were really shooting for was a fully managed uh",
    "start": "1441720",
    "end": "1448159"
  },
  {
    "text": "partner Sol syndication solution uh and this platform was first described to me by our CEO in uh Q2 of 2010 and uh his",
    "start": "1448159",
    "end": "1457200"
  },
  {
    "text": "big idea his big contribution to this was that he wanted to develop a platform where you would get a single embed code",
    "start": "1457200",
    "end": "1464600"
  },
  {
    "text": "and you would only get it once and you could use it anywhere you wanted and it would intelligently uh discover what it",
    "start": "1464600",
    "end": "1471039"
  },
  {
    "text": "was supposed to put on that page primarily what he was trying to avoid was some of the other uh platform",
    "start": "1471039",
    "end": "1476799"
  },
  {
    "text": "Solutions where you have to use lots and lots of different code Snippets depending on what you want to show up on",
    "start": "1476799",
    "end": "1481840"
  },
  {
    "text": "your page and it's really hard if you're talking to Publishers who may have uh no",
    "start": "1481840",
    "end": "1487000"
  },
  {
    "text": "or very limited technical staffs to figure out how to use even things like feeds uh so what his goal was was to",
    "start": "1487000",
    "end": "1494080"
  },
  {
    "text": "have us develop a embed code that could be put onto any page and would just figure everything out for the for the",
    "start": "1494080",
    "end": "1500080"
  },
  {
    "text": "publisher that was a bit of a challenge needless to say and I thank him for",
    "start": "1500080",
    "end": "1505640"
  },
  {
    "text": "proposing that um but uh so how did we go about solving",
    "start": "1505640",
    "end": "1512760"
  },
  {
    "start": "1511000",
    "end": "1511000"
  },
  {
    "text": "this we decided that what we would do is we would build uh something called a placement a syndication placement and it",
    "start": "1512760",
    "end": "1519159"
  },
  {
    "text": "would contain three elements a Content set which would describe what the publisher wanted to show up on their",
    "start": "1519159",
    "end": "1526520"
  },
  {
    "text": "site uh it would contain widget which was our idea of a client side code that",
    "start": "1526520",
    "end": "1532600"
  },
  {
    "text": "would deliver some kind of application experience to the user and it would contain a Target set the target set's",
    "start": "1532600",
    "end": "1537960"
  },
  {
    "text": "actually the important part here it's a list of URLs that we would check uh to see if the placement matched uh what we",
    "start": "1537960",
    "end": "1545120"
  },
  {
    "text": "were being asked for so for example uh if the if the client said hey my website",
    "start": "1545120",
    "end": "1551200"
  },
  {
    "text": "is www.example.com uh and uh whenever my widget shows up on",
    "start": "1551200",
    "end": "1557039"
  },
  {
    "text": "the for/ pregancy page I want a pregnancy video they would provide that in the Target set and identify uh",
    "start": "1557039",
    "end": "1563399"
  },
  {
    "text": "content set that and contain pregnancy videos uh we also allowed wild cards in this so if you were",
    "start": "1563399",
    "end": "1570240"
  },
  {
    "text": "www.example.com starrey we would match anything that had",
    "start": "1570240",
    "end": "1576159"
  },
  {
    "text": "any content in there and ended with pregnancy with with that particular placement so and then we would build a",
    "start": "1576159",
    "end": "1584000"
  },
  {
    "text": "custom JavaScript application that we would deliver to your end user and they would see it",
    "start": "1584000",
    "end": "1589279"
  },
  {
    "text": "uh siren was what we called this it's if you don't know if you haven't played Final Fantasy recently siren is also a",
    "start": "1589279",
    "end": "1595120"
  },
  {
    "text": "summonable monster uh what it what it did it extended the chocoo platform and uh it added a",
    "start": "1595120",
    "end": "1604640"
  },
  {
    "text": "syndication CMS that included uh the ability to manage these placements and a",
    "start": "1604640",
    "end": "1610000"
  },
  {
    "text": "reporting layer it it also included a keyphrase extraction algorithm uh we borrowed this heavily from an open",
    "start": "1610000",
    "end": "1616000"
  },
  {
    "text": "source project that the University of witi I apologize them if I mispronounced that in New Zealand",
    "start": "1616000",
    "end": "1622720"
  },
  {
    "text": "produced um we added a DB andm for partners specific data and we built some",
    "start": "1622720",
    "end": "1627880"
  },
  {
    "text": "business logic and services to support this uh new kind of of vertical that we",
    "start": "1627880",
    "end": "1633679"
  },
  {
    "text": "were building the end goal was really to just deliver a piece of client side code to uh the user that that allowed for the",
    "start": "1633679",
    "end": "1641840"
  },
  {
    "text": "execution of the health content experience baked into it would be a reference to a video and uh also styling",
    "start": "1641840",
    "end": "1649520"
  },
  {
    "text": "code and JavaScript that would allow them to interact with this widget and",
    "start": "1649520",
    "end": "1654640"
  },
  {
    "text": "experience the Right video at the right time so we had a",
    "start": "1654640",
    "end": "1661440"
  },
  {
    "start": "1658000",
    "end": "1658000"
  },
  {
    "text": "great uh deal of interest in this and our bis Dev and sales peoples were able to go out and really pitch this to",
    "start": "1661440",
    "end": "1667600"
  },
  {
    "text": "people and everyone was super excited and partners were in love with this really sophisticated idea uh the reality",
    "start": "1667600",
    "end": "1674519"
  },
  {
    "text": "however was that we had a lot to do server side as a result um there's when you when you incorporate",
    "start": "1674519",
    "end": "1682320"
  },
  {
    "text": "sophistication into any application you just make everything slow and the best",
    "start": "1682320",
    "end": "1687919"
  },
  {
    "text": "way to lose people is to give them a slow experience uh clients are totally",
    "start": "1687919",
    "end": "1693320"
  },
  {
    "text": "thrilled to get something but they're not at all excited if it happens the day after they made the",
    "start": "1693320",
    "end": "1699279"
  },
  {
    "text": "request um there's a lot to do when with with our particular platform we had Network latency problems we had",
    "start": "1699279",
    "end": "1704960"
  },
  {
    "text": "extremely high CPU execution requirements uh there were a lot of database requests going back and forth",
    "start": "1704960",
    "end": "1712039"
  },
  {
    "text": "and uh as we scaled we ran into this huge failure",
    "start": "1712039",
    "end": "1717240"
  },
  {
    "text": "problem so our system has potentially a lot to do we get a request that contains",
    "start": "1717240",
    "end": "1723039"
  },
  {
    "text": "a URL and we have to figure out who it's from and who it matches uh we have to go and request the",
    "start": "1723039",
    "end": "1729519"
  },
  {
    "text": "partner's page receive it parse it uh analyze it for what it's all about and",
    "start": "1729519",
    "end": "1735440"
  },
  {
    "text": "then produce a Content set that matches it respecting rules that the partner has given us for what is appropriate and not",
    "start": "1735440",
    "end": "1741200"
  },
  {
    "text": "appropriate for their site then we have to compose a a an appropriate response strip that compiles JavaScript and CSS",
    "start": "1741200",
    "end": "1748320"
  },
  {
    "text": "together to deliver back to the end user so that they can have this awesome experience and uh that that takes a",
    "start": "1748320",
    "end": "1754279"
  },
  {
    "text": "little bit of time uh so without anything we were seeing um we we use a",
    "start": "1754279",
    "end": "1761240"
  },
  {
    "text": "metric called time to first second um which is this average time to video played of 60 plus seconds",
    "start": "1761240",
    "end": "1768640"
  },
  {
    "text": "with no cash whatsoever and uh we had systemwide collapse when we exceeded 50",
    "start": "1768640",
    "end": "1773840"
  },
  {
    "text": "requests to the system so obviously a really CPU intensive process really DB intensive process and it couldn't",
    "start": "1773840",
    "end": "1781200"
  },
  {
    "text": "couldn't work if we didn't use some other kind of solution so we anticipated this latency",
    "start": "1781200",
    "end": "1787720"
  },
  {
    "text": "and we built in a m cach layer uh essentially when we got the request we",
    "start": "1787720",
    "end": "1793960"
  },
  {
    "text": "looked at who it was coming from and what the URL was and we generated a hash key to store in the in mcash this is uh",
    "start": "1793960",
    "end": "1803480"
  },
  {
    "text": "really a great way of storing keys by the way if you have some kind of um",
    "start": "1803480",
    "end": "1809080"
  },
  {
    "text": "parameter like a URL that that you're trying to keep track of hash it we use sha one and use that as your key uh as",
    "start": "1809080",
    "end": "1816559"
  },
  {
    "text": "was mentioned before the 250 byte limit on the key actually comes into play when you start using the keys",
    "start": "1816559",
    "end": "1823679"
  },
  {
    "text": "uh in a in a dynamic fashion so hashing is is really effective here um so what we did is we generated a key",
    "start": "1823679",
    "end": "1831559"
  },
  {
    "text": "we went to our local MC see if it was there uh if it wasn't we told the server",
    "start": "1831559",
    "end": "1838039"
  },
  {
    "text": "to process uh the request and generate the appropriate script use the hash set the",
    "start": "1838039",
    "end": "1844320"
  },
  {
    "text": "key put it into memcache um if we did get a hit we just fired off the JavaScript that was stored in the in the",
    "start": "1844320",
    "end": "1851080"
  },
  {
    "text": "slab uh behind the key and sent it to the user that that process was really quick and we had used this very",
    "start": "1851080",
    "end": "1856159"
  },
  {
    "text": "effectively to speed up our website however with this particular application",
    "start": "1856159",
    "end": "1862880"
  },
  {
    "text": "we ran into immediate problems so the big problems that we the",
    "start": "1862880",
    "end": "1868639"
  },
  {
    "start": "1863000",
    "end": "1863000"
  },
  {
    "text": "the way that MCD distri distributed with this particular platform was that uh each server",
    "start": "1868639",
    "end": "1874440"
  },
  {
    "text": "contained the entire siren platform code and a locally installed MCD we instantiated all these servers as",
    "start": "1874440",
    "end": "1882159"
  },
  {
    "text": "M1 Smalls uh and they all sat behind a load balancer",
    "start": "1882159",
    "end": "1888440"
  },
  {
    "text": "this meant that requests to our uh siren server pool were arbitrarily assigned",
    "start": "1888440",
    "end": "1894120"
  },
  {
    "text": "healthy instances if you've use load balancers and ec2s this is kind of standard it just round Robbin them to",
    "start": "1894120",
    "end": "1899559"
  },
  {
    "text": "anything healthy and uh we would autoscale that pool of siren servers against average",
    "start": "1899559",
    "end": "1905760"
  },
  {
    "text": "CPU use so if CPU use climbed above 90% we would add two more servers every 5",
    "start": "1905760",
    "end": "1911559"
  },
  {
    "text": "minutes until it dropped below 90% if it dropped below 15% we would remove one server every 5 minutes until we were",
    "start": "1911559",
    "end": "1918039"
  },
  {
    "text": "back down to our base level or cpu's climbed above 15% so we tried to stay between that sweet spot of 15 and 90%",
    "start": "1918039",
    "end": "1924639"
  },
  {
    "text": "CPU usage for scaling uh again this had been a very effective solution for us",
    "start": "1924639",
    "end": "1929720"
  },
  {
    "text": "with our web servers uh and then behind all this we had an RDS Master with uh",
    "start": "1929720",
    "end": "1934880"
  },
  {
    "text": "read replicas supporting this uh this architecture seemed to us like a good",
    "start": "1934880",
    "end": "1941559"
  },
  {
    "text": "idea at the time uh but didn't really work uh what really",
    "start": "1941559",
    "end": "1948279"
  },
  {
    "start": "1948000",
    "end": "1948000"
  },
  {
    "text": "happened here is that cash missed all the time because we have this distributed architecture and because we",
    "start": "1948279",
    "end": "1954200"
  },
  {
    "text": "had local mem casd sitting on every server every request was getting arbitrarily assigned to a server and as",
    "start": "1954200",
    "end": "1961480"
  },
  {
    "text": "we if we had 20 servers worst case scenario 20 misses and the 21st finally",
    "start": "1961480",
    "end": "1967360"
  },
  {
    "text": "hits somebody who actually has your your uh a server that has your data in in a",
    "start": "1967360",
    "end": "1973639"
  },
  {
    "text": "memory um the other problem we had is that every was becoming redundant all the",
    "start": "1973639",
    "end": "1979720"
  },
  {
    "text": "memory was looking identical uh every server stored the same data over time whatever the requests were it just built",
    "start": "1979720",
    "end": "1985360"
  },
  {
    "text": "into the same kind of stack of of data and uh actually scaling this pool",
    "start": "1985360",
    "end": "1990440"
  },
  {
    "text": "introduced more problems rather than less uh usually we think of scaling as a solution here it was actually a problem",
    "start": "1990440",
    "end": "1996320"
  },
  {
    "text": "every new server we added uh increased the amount of database demand because it was introducing an empty cach um we did",
    "start": "1996320",
    "end": "2003720"
  },
  {
    "text": "play around at the when we when we started doing this with um with warming the cash but again because it was just a",
    "start": "2003720",
    "end": "2010600"
  },
  {
    "text": "redundant warm it it wasn't very effective either uh and um and as we",
    "start": "2010600",
    "end": "2016480"
  },
  {
    "text": "scaled we also increase the likelihood of a cash Miss because again traffic is being arbitrarily assigned you got 20 servers you get 20 misses if you have 40",
    "start": "2016480",
    "end": "2023840"
  },
  {
    "text": "servers well you had now you now you have 40 misses and uh slowed everything down this lack of pulled memory also",
    "start": "2023840",
    "end": "2030000"
  },
  {
    "text": "meant that we were limited to the cach size of any one given server so with M1",
    "start": "2030000",
    "end": "2035519"
  },
  {
    "text": "Smalls I think we have uh one gig of memory available we have to allocate some of that to the CPU uh and PHP the",
    "start": "2035519",
    "end": "2043080"
  },
  {
    "text": "rest of it gets to go to mcast so we were finding that we had a memory limit of about uh 600 Megs of cash Bas that's",
    "start": "2043080",
    "end": "2051079"
  },
  {
    "text": "that's pretty tiny and we filled that up really quickly and then we didn't know what to",
    "start": "2051079",
    "end": "2056240"
  },
  {
    "text": "do um and as soon as you hit as soon as you fill up cash and you start trying to shuffle new things into it you get into",
    "start": "2056240",
    "end": "2061919"
  },
  {
    "text": "an eviction uh problem if you've if you've used MCD this is what happens",
    "start": "2061919",
    "end": "2067520"
  },
  {
    "text": "when you have a full cache uh men casd starts to look through your your table and find out what the oldest piece of",
    "start": "2067520",
    "end": "2074158"
  },
  {
    "text": "data is in there and evict it and adds your new piece of data to the to memory",
    "start": "2074159",
    "end": "2081358"
  },
  {
    "text": "uh the you really don't want to have evictions happening because it means that content that shouldn't have expired",
    "start": "2081359",
    "end": "2086638"
  },
  {
    "text": "is being kicked out and we had relatively short expiration times on our on our M casd units I think they were",
    "start": "2086639",
    "end": "2093760"
  },
  {
    "text": "set to just 6 days or something which isn't which isn't horribly long for for cash but um yeah we we just couldn't",
    "start": "2093760",
    "end": "2101880"
  },
  {
    "text": "handle the eviction so we knew we had big problems very",
    "start": "2101880",
    "end": "2108880"
  },
  {
    "text": "quickly we knew we had big problems and uh we started looking for Solutions at",
    "start": "2108880",
    "end": "2113960"
  },
  {
    "text": "the time the premium solution was something called Mease it's a",
    "start": "2113960",
    "end": "2119000"
  },
  {
    "text": "couchdb uh kind of operation that lets you take a bunch of servers and sit",
    "start": "2119000",
    "end": "2124400"
  },
  {
    "text": "couchdb in front of them to manage mcache to cross a bunch of servers and use the nosql database as your as your",
    "start": "2124400",
    "end": "2132119"
  },
  {
    "text": "uh workflow controller we uh we we were going to",
    "start": "2132119",
    "end": "2138560"
  },
  {
    "text": "actually incorporate menase and uh but we realized it was going to require a lot of work there was a lot of code to write to get men base up and running and",
    "start": "2138560",
    "end": "2145320"
  },
  {
    "text": "to uh incorporate couchdb and as we were deciding that you",
    "start": "2145320",
    "end": "2151079"
  },
  {
    "text": "know this is what we've got to do a last cach arrived uh Amazon has this bad habit of coming up with exactly the",
    "start": "2151079",
    "end": "2157079"
  },
  {
    "text": "service we needed as soon as we kind of realized we have a problem so uh last",
    "start": "2157079",
    "end": "2163599"
  },
  {
    "text": "cash arrived and we we uh implemented it uh we we experimented it with with it first and realized um as soon as we we",
    "start": "2163599",
    "end": "2170680"
  },
  {
    "text": "have a small cluster of Dev servers that were running uh siren cocation and uh we",
    "start": "2170680",
    "end": "2177040"
  },
  {
    "text": "we built a small elas cach node and",
    "start": "2177040",
    "end": "2182440"
  },
  {
    "text": "attached this Dev server pool to it and we didn't actually require any changes at all uh no code changes no",
    "start": "2182440",
    "end": "2188400"
  },
  {
    "text": "configuration changes really we just pointed uh the mcash uh service to this elasticache",
    "start": "2188400",
    "end": "2195400"
  },
  {
    "text": "node and everything worked um and spinning up the elasticache node was super easy easier than an ec2 instance",
    "start": "2195400",
    "end": "2203079"
  },
  {
    "text": "yes so just a question so using Zen framework so I'm assuming you were using Zen and were instantiating using men in",
    "start": "2203079",
    "end": "2210960"
  },
  {
    "text": "there how how did you get that to go to were frame it was it was completely",
    "start": "2210960",
    "end": "2219119"
  },
  {
    "text": "opaque so the the Amazon uh the Zen framework is using uh Zen cache and uh",
    "start": "2219119",
    "end": "2225800"
  },
  {
    "text": "the references to it actually were uh we didn't change them at all all we had to do was we were using engine X and that",
    "start": "2225800",
    "end": "2232200"
  },
  {
    "text": "engine X points to the location of our mcash layer all we did was change that parameter to point to the elasticache",
    "start": "2232200",
    "end": "2238480"
  },
  {
    "text": "location it elasticache has a uh elasticache each node gives you a unique uh URL for its location and we just",
    "start": "2238480",
    "end": "2245800"
  },
  {
    "text": "pluged that in and everything worked worked um all of it really was transparent I mean it was was opaque I",
    "start": "2245800",
    "end": "2252560"
  },
  {
    "text": "mean there wasn't anything that we needed to do differently except change that one configuration setting in engine",
    "start": "2252560",
    "end": "2257760"
  },
  {
    "text": "x uh and literally all it was was pointing to a different server location I'll talk about a little bit of the",
    "start": "2257760",
    "end": "2263280"
  },
  {
    "text": "problems we have with scaling it but um but it was that easy you know zen zen framework just worked right along with",
    "start": "2263280",
    "end": "2269920"
  },
  {
    "text": "it um and like I said the spin up was super easy you literally click two buttons and you have an elasticache node",
    "start": "2269920",
    "end": "2276920"
  },
  {
    "text": "uh you just tell how many you want and how big you want them to be and the pricing on this was super super",
    "start": "2276920",
    "end": "2282640"
  },
  {
    "text": "compelling as well uh unlike ec2 instances the elastic cast nodes are",
    "start": "2282640",
    "end": "2288119"
  },
  {
    "text": "spec are they're also ec2 instances but they're designed specifically for memory so they have more memory they have",
    "start": "2288119",
    "end": "2294800"
  },
  {
    "text": "smaller CPUs they don't load the Ami loads nothing else but a but M cached so",
    "start": "2294800",
    "end": "2300560"
  },
  {
    "text": "you're not running anything extraneous you get almost the entire uh memory space to use so uh the small flash cash",
    "start": "2300560",
    "end": "2308200"
  },
  {
    "text": "note I think is like 3.2 gigs and we were seeing uh pretty much all of that 3",
    "start": "2308200",
    "end": "2314319"
  },
  {
    "text": "gigs from that uh available from mcash which is huge compared to the 500 or 600 we were seeing on an individual ec2",
    "start": "2314319",
    "end": "2320839"
  },
  {
    "text": "instance um also the fact that there were metrics associated with with cloudwatch metrics associated with us",
    "start": "2320839",
    "end": "2326359"
  },
  {
    "text": "was awesome um we could see exactly what was going on and with mcast it's sometimes difficult uh there are",
    "start": "2326359",
    "end": "2332599"
  },
  {
    "text": "programs that you can use like PHP mcash admin to help you but when you have 20",
    "start": "2332599",
    "end": "2337760"
  },
  {
    "text": "or 30 servers it's really kind of difficult to see the big picture and uh cloudwatch was really",
    "start": "2337760",
    "end": "2344800"
  },
  {
    "text": "helpful with that so what did it take really we spun up a cluster of four",
    "start": "2344800",
    "end": "2350720"
  },
  {
    "start": "2349000",
    "end": "2349000"
  },
  {
    "text": "nodes we uh installed Moxy as a client on all of our application servers and uh",
    "start": "2350720",
    "end": "2358000"
  },
  {
    "text": "I and basically all we did was modify our custom Ami to have Moxy be part of it and pointed uh Moxy to the",
    "start": "2358000",
    "end": "2365920"
  },
  {
    "text": "elasticache nodes uh we installed PHP mcash admin and",
    "start": "2365920",
    "end": "2371359"
  },
  {
    "text": "pointed we had had it installed for our mcash D installs but uh we just pointed to the",
    "start": "2371359",
    "end": "2377000"
  },
  {
    "text": "elasticache uh nodes and it worked exactly the same and other than that",
    "start": "2377000",
    "end": "2382160"
  },
  {
    "text": "there was really nothing to do uh all of our infrastructure remained the same no code changes no application",
    "start": "2382160",
    "end": "2388760"
  },
  {
    "text": "configuration changes very very modest changes to our uh web",
    "start": "2388760",
    "end": "2394720"
  },
  {
    "text": "server and it was it really I mean it literally took us less than a week to install last toache um truthfully it",
    "start": "2394720",
    "end": "2402640"
  },
  {
    "start": "2397000",
    "end": "2397000"
  },
  {
    "text": "probably would have taken us a day if I had a more active Chief software",
    "start": "2402640",
    "end": "2410000"
  },
  {
    "text": "architect's probably playing Final Fantasy that day uh but we saw immediate performance",
    "start": "2410000",
    "end": "2418680"
  },
  {
    "text": "improvements um the the elastic cast servers like I said are built to handle memory",
    "start": "2418680",
    "end": "2425480"
  },
  {
    "text": "operations so they're an immediate an improvement over your existing ec2s and uh the uniform calls were",
    "start": "2425480",
    "end": "2432440"
  },
  {
    "text": "tremendous for us the fact that now we had a pool of memory that was reliably",
    "start": "2432440",
    "end": "2438079"
  },
  {
    "text": "going to return back uh a true miss or a true hit was",
    "start": "2438079",
    "end": "2445200"
  },
  {
    "text": "gigantic also the scaling improvements were were clear now uh one of the nice",
    "start": "2445400",
    "end": "2451119"
  },
  {
    "start": "2446000",
    "end": "2446000"
  },
  {
    "text": "things about uh elasticache is that you can scale these servers vertically very easily and that's actually how we we use",
    "start": "2451119",
    "end": "2457839"
  },
  {
    "text": "them to start the small sizes are three Megs the largest sizes are I think upwards of 60 so or so sorry gigs I mean",
    "start": "2457839",
    "end": "2466520"
  },
  {
    "text": "so the small ones are three gigs the largest ones are upwards of 60 and without any configuration changes at all",
    "start": "2466520",
    "end": "2471560"
  },
  {
    "text": "you can swap the instances upwards or downwards as you see fit so if you",
    "start": "2471560",
    "end": "2477160"
  },
  {
    "text": "suddenly hit your memory cap and you've got four nodes configured you can scale them all up as big as you like and",
    "start": "2477160",
    "end": "2483040"
  },
  {
    "text": "nothing needs to change so if you've got if you got a problem that needs mean you pay more obviously but uh if you've got",
    "start": "2483040",
    "end": "2489920"
  },
  {
    "text": "a problem you can scale upwards very easily uh removing mcast from our application layer allowed us to now",
    "start": "2489920",
    "end": "2495720"
  },
  {
    "text": "scale it with no problem whatsoever uh our application servers started doing what they were supposed to do which is",
    "start": "2495720",
    "end": "2501920"
  },
  {
    "text": "handle requests performance CPU processing and uh pass that on to memory",
    "start": "2501920",
    "end": "2507640"
  },
  {
    "text": "where as needed not handle memory uh hit misc conditions so the application servers",
    "start": "2507640",
    "end": "2514359"
  },
  {
    "text": "started only getting called when they actually needed to be and uh the the actual memory pool actually scales",
    "start": "2514359",
    "end": "2520200"
  },
  {
    "text": "horizontally fairly fairly easily as well um we're really excited about this",
    "start": "2520200",
    "end": "2525560"
  },
  {
    "text": "autodiscovery honestly but uh because the the biggest problem is scaling upwards not downwards you can scale",
    "start": "2525560",
    "end": "2531000"
  },
  {
    "text": "downwards very easily you just remove a node and it populates the rest of the nodes with what it used to have uh but",
    "start": "2531000",
    "end": "2537079"
  },
  {
    "text": "scaling upwards is a bit more challenging uh the the solution that we had actually to",
    "start": "2537079",
    "end": "2544760"
  },
  {
    "start": "2540000",
    "end": "2540000"
  },
  {
    "text": "the scaling upwards problem was to use Route 53 which is also an Amazon product for DNS uh",
    "start": "2544760",
    "end": "2552480"
  },
  {
    "text": "management um there are two big advantages to using R 53 one of them is",
    "start": "2552480",
    "end": "2557680"
  },
  {
    "text": "that you reduce latency between calls to your domain name server and uh back into",
    "start": "2557680",
    "end": "2563000"
  },
  {
    "text": "last cach so if you have an external domain name server you have to call out to that it resolves your M cach then",
    "start": "2563000",
    "end": "2568040"
  },
  {
    "text": "calls back into Amazon if use rout 53 that's all gone it's all in within the data center um and also it allows us to",
    "start": "2568040",
    "end": "2574880"
  },
  {
    "text": "produce uniform naming conventions for our uh our elastic cache nodes so one of the",
    "start": "2574880",
    "end": "2580800"
  },
  {
    "text": "things we did was build a small script that just iterated the the number at the end of the uh elasticache node name and",
    "start": "2580800",
    "end": "2588920"
  },
  {
    "text": "when we needed to scale we still had to redeploy um Moxy but it was all automated through",
    "start": "2588920",
    "end": "2595440"
  },
  {
    "text": "scripting um elasticache also provides uniform access which means you can manage it with a database very easily uh",
    "start": "2595440",
    "end": "2602480"
  },
  {
    "text": "if you need to invalidate or if you need to prevent content dding you can build a small database application that'll or",
    "start": "2602480",
    "end": "2610319"
  },
  {
    "text": "build a small database table that monitors what's in El last cache and then you can uh use your application to manage it uh you can also manage you can",
    "start": "2610319",
    "end": "2618280"
  },
  {
    "text": "also manage misses very effectively as well the way we handle this is we just have a default that we load into elastic",
    "start": "2618280",
    "end": "2624520"
  },
  {
    "text": "cache and uh if we miss we just serve that instead and uh we also learned that",
    "start": "2624520",
    "end": "2631119"
  },
  {
    "text": "deploying elastic cash across availability zones works fine uh you can deploy it across as many availability",
    "start": "2631119",
    "end": "2636800"
  },
  {
    "text": "zones as you want and it'll continue to work as long as you have Moxy pointing to them this allows you some degree of",
    "start": "2636800",
    "end": "2643280"
  },
  {
    "text": "um protection from failure for any given availability zone so it's really uh",
    "start": "2643280",
    "end": "2650480"
  },
  {
    "text": "really nice to figure that out as well I'm going to give you a quick case study for a company called daily RX they",
    "start": "2650480",
    "end": "2656520"
  },
  {
    "text": "adopted our platform early uh they had huge problems with our initial deployment so when they rolled us out",
    "start": "2656520",
    "end": "2663800"
  },
  {
    "text": "across their whole site their our our application stopped working all together",
    "start": "2663800",
    "end": "2669480"
  },
  {
    "text": "uh they loved the the idea of this product but they were going uh they",
    "start": "2669480",
    "end": "2675319"
  },
  {
    "text": "really were considering abandoning us because of poor performance we installed the last cach and average response time",
    "start": "2675319",
    "end": "2680520"
  },
  {
    "text": "dropped from 60 plus seconds to under two We Now record uh response times of",
    "start": "2680520",
    "end": "2685680"
  },
  {
    "text": "less than two seconds for all of our script calls uh I think recently when I",
    "start": "2685680",
    "end": "2691400"
  },
  {
    "text": "last looked at this and we expanded the last cach a little bit we're seeing average response times of 746",
    "start": "2691400",
    "end": "2696440"
  },
  {
    "text": "milliseconds over a 30-day period uh which we're really proud of considering that previous this the average was",
    "start": "2696440",
    "end": "2702400"
  },
  {
    "text": "something like 32 um and we had an incident early with",
    "start": "2702400",
    "end": "2708400"
  },
  {
    "text": "them where they were filling up our cash really quickly and we were able to scale our cash vertically uh to prevent an",
    "start": "2708400",
    "end": "2714880"
  },
  {
    "text": "outage uh and we later realized they had used a marketing partner that was changing the URL regularly but uh we",
    "start": "2714880",
    "end": "2722000"
  },
  {
    "text": "were able to resolve that future plans and ideas we'd like to implement Auto scaling on elastic cach and aut",
    "start": "2722000",
    "end": "2727319"
  },
  {
    "text": "Discovery will allow that now to happen we've thought about using M cach and elastic cach across multiple regions to",
    "start": "2727319",
    "end": "2735000"
  },
  {
    "text": "create something like a memcache CDN uh and we also want to try and break up these these Services into common",
    "start": "2735000",
    "end": "2742280"
  },
  {
    "text": "components um and try and assemble things out of me out of elasticache instead of delivering an exact hit we'll",
    "start": "2742280",
    "end": "2749800"
  },
  {
    "text": "see if that pans out into anything uh questions",
    "start": "2749800",
    "end": "2755839"
  }
]