[
  {
    "start": "0",
    "end": "126000"
  },
  {
    "text": "[Music] hello my name is rob hickorita and i'm a",
    "start": "360",
    "end": "6399"
  },
  {
    "text": "principal technical account manager at aws and i'm located in austin texas welcome to aws supports you where aws",
    "start": "6399",
    "end": "13280"
  },
  {
    "text": "support experts provide tips to optimize performance in the cloud lower cost and provide you with best practices and",
    "start": "13280",
    "end": "18880"
  },
  {
    "text": "design considerations and joining me today i have tom and melanie from aws support can you give us a quick",
    "start": "18880",
    "end": "24320"
  },
  {
    "text": "introduction tom and then melanie sure it's a pleasure to be here i'm tom coombs and i'm a technical account",
    "start": "24320",
    "end": "29519"
  },
  {
    "text": "manager from zurich in switzerland hey everyone my name is melanie lee and",
    "start": "29519",
    "end": "34559"
  },
  {
    "text": "i'm an amr specialist technical account manager at adabs based out of sydney",
    "start": "34559",
    "end": "39760"
  },
  {
    "text": "great thank you both for joining us especially given the wide variety of time zones you're joining us from so",
    "start": "39760",
    "end": "44879"
  },
  {
    "text": "really appreciate that um today audience we're going to be introducing you to using aws hardware for machine learning",
    "start": "44879",
    "end": "51120"
  },
  {
    "text": "and we'll be specifically focusing on aws infraredia but before we go into the details a quick note to all the",
    "start": "51120",
    "end": "56960"
  },
  {
    "text": "attendees online please make sure to use the chat window on the right hand side of your screen to let us know where you're joining us from today and also",
    "start": "56960",
    "end": "63520"
  },
  {
    "text": "share your thoughts and ask questions today of tom and melanie as they're going through our content we really do look forward to hearing from you",
    "start": "63520",
    "end": "70080"
  },
  {
    "text": "we also will be providing a link to a survey over in the chat so you can let us know how we did today uh share your",
    "start": "70080",
    "end": "75119"
  },
  {
    "text": "feedback as well about today's episode uh tom with that i'm gonna turn it over to you to uh start us on what we're",
    "start": "75119",
    "end": "81600"
  },
  {
    "text": "gonna be talking about today sure okay we're gonna give a brief brief introduction to machine learning and",
    "start": "81600",
    "end": "87920"
  },
  {
    "text": "what aws is seeing our customers doing with it uh we're then going to give an introduction to inferentia on new",
    "start": "87920",
    "end": "94159"
  },
  {
    "text": "hardware which customers can use we're going to see how customers are using it and then we're going to give some details uh that you need to know uh",
    "start": "94159",
    "end": "101119"
  },
  {
    "text": "for a demo uh we're going to use a neuron for that when you're an sdk we're going to do some comparisons to see how customers",
    "start": "101119",
    "end": "107520"
  },
  {
    "text": "can choose this hardware in furniture hardware versus our other options we're",
    "start": "107520",
    "end": "112799"
  },
  {
    "text": "then going to see how to get started with inferentia and then we're going to do a demo of inferential melanie uh is",
    "start": "112799",
    "end": "118000"
  },
  {
    "text": "going to do that then we'll talk a little bit about training and some additional examples as the way customers are using this",
    "start": "118000",
    "end": "124560"
  },
  {
    "text": "so let's get started machine learning went from being an inspirational technology to a mainstream",
    "start": "124560",
    "end": "130959"
  },
  {
    "start": "126000",
    "end": "456000"
  },
  {
    "text": "really fast for a long time though it was a technology which was limited to few a",
    "start": "130959",
    "end": "136239"
  },
  {
    "text": "few major tech companies and hardcore academic researchers but things came to change when cloud",
    "start": "136239",
    "end": "142319"
  },
  {
    "text": "computing entered the mainstream cloud power and data became more available and literally machine learning",
    "start": "142319",
    "end": "148959"
  },
  {
    "text": "is now making impact across all industries fashion retail real estate healthcare we",
    "start": "148959",
    "end": "154400"
  },
  {
    "text": "see it in many many sectors it's moving from the periphery now as a part of core business and",
    "start": "154400",
    "end": "160000"
  },
  {
    "text": "everyday everyday industry that's great to see and this is in line with aws's vision to put ml in the hands",
    "start": "160000",
    "end": "166800"
  },
  {
    "text": "of every developer well let's have a look at these uh ec2",
    "start": "166800",
    "end": "173519"
  },
  {
    "text": "uh inform instances they feature um the new hardware which is uh built specifically by amazon aws",
    "start": "173519",
    "end": "181440"
  },
  {
    "text": "inferential that's what we're going to talk about today it's 80 lower cost than gpu instances",
    "start": "181440",
    "end": "189200"
  },
  {
    "text": "for running deep learning um inference it seamlessly integrates with key ml",
    "start": "189200",
    "end": "196319"
  },
  {
    "text": "frameworks such as tensorflow pytorch mxnet really quickly we're going to show you how to do that later on",
    "start": "196319",
    "end": "203280"
  },
  {
    "text": "this is available from a hosting perspective in vms containers",
    "start": "203280",
    "end": "208319"
  },
  {
    "text": "kubernetes and also sagemaker and we'll be demoing sagemaker later",
    "start": "208319",
    "end": "214959"
  },
  {
    "text": "inform instances are optimized to serve a variety of machine learning inference use cases everything from classification",
    "start": "216159",
    "end": "223280"
  },
  {
    "text": "object detection natural language processing nlp speech recognition search ranking and",
    "start": "223280",
    "end": "229760"
  },
  {
    "text": "fraud detection they're useful to customers who are looking to reduce latency as well",
    "start": "229760",
    "end": "234959"
  },
  {
    "text": "because of the efficiency of these chips and we see many customers doing that we fully expect that lowering the cost",
    "start": "234959",
    "end": "240720"
  },
  {
    "text": "of ml will be a major drive in having more applications adopt ml capabilities",
    "start": "240720",
    "end": "247040"
  },
  {
    "text": "and provide overall better experience for end users it also gives customers the ability to be more sustainable which",
    "start": "247040",
    "end": "253680"
  },
  {
    "text": "is something we hear from many customers inform instances have been available for",
    "start": "253680",
    "end": "260079"
  },
  {
    "text": "about just over two years now they've been adopted in by a growing number of customers we're glad to say",
    "start": "260079",
    "end": "266960"
  },
  {
    "text": "our customers have been able to accomplish some amazing things with inferentia customers like autodesk and",
    "start": "266960",
    "end": "272000"
  },
  {
    "text": "sprinkler have made significant gains to their nlp models while reducing their",
    "start": "272000",
    "end": "277120"
  },
  {
    "text": "costs amazon services like robotics advertising and prime video we're able",
    "start": "277120",
    "end": "282400"
  },
  {
    "text": "to use inferentia to reduce costs by 80 while reducing latency",
    "start": "282400",
    "end": "288479"
  },
  {
    "text": "we take a look a closer look at some of these customers a bit later on but now let's dive into",
    "start": "288479",
    "end": "293520"
  },
  {
    "text": "interim furniture itself the ec2 inform instances feature the aws",
    "start": "293520",
    "end": "301360"
  },
  {
    "text": "inferential chip as well as aws neuron a new software product which natively integrates into popular machine learning",
    "start": "301360",
    "end": "308479"
  },
  {
    "text": "frameworks and optimizes performance for inferential chips import instances are built on the aws",
    "start": "308479",
    "end": "315039"
  },
  {
    "text": "nitro system which we'll talk a bit about later on as well as on the custom second generation xeon scalable",
    "start": "315039",
    "end": "321919"
  },
  {
    "text": "processors and those have up to 100 giga gigabytes of networking to enable high",
    "start": "321919",
    "end": "327919"
  },
  {
    "text": "throughput influence which is required by many customers",
    "start": "327919",
    "end": "332960"
  },
  {
    "text": "what are your actual choices when you're actually booting one of these things up import instance is available in four",
    "start": "333520",
    "end": "339280"
  },
  {
    "text": "four flavors or sizes um one xl two xl six xl and twenty four",
    "start": "339280",
    "end": "345280"
  },
  {
    "text": "xl and this follows our normal conventions around um at times two being doubly as powerful as",
    "start": "345280",
    "end": "351520"
  },
  {
    "text": "the x large the x large and two times x large are",
    "start": "351520",
    "end": "356560"
  },
  {
    "text": "single chip instances and the six times and 24 times are multi-chip varieties",
    "start": "356560",
    "end": "361919"
  },
  {
    "text": "inform instances will also support also supported in aws managed services",
    "start": "361919",
    "end": "367680"
  },
  {
    "text": "such as sagemaker eks and ecs if you ever need help to choose which",
    "start": "367680",
    "end": "373520"
  },
  {
    "text": "one and how to optimize the loads going on that do contact your technical account manager it's something we love",
    "start": "373520",
    "end": "378960"
  },
  {
    "text": "doing helping our customers save money and deliver faster",
    "start": "378960",
    "end": "383600"
  },
  {
    "text": "let's take a closer look at the inferentia chip each inferential chip is providing more",
    "start": "384400",
    "end": "389680"
  },
  {
    "text": "than 100 teraf terror operations per second at low power which includes which allows us to pack",
    "start": "389680",
    "end": "396880"
  },
  {
    "text": "16 of them into a single server providing more than two peta operations per second that's more than double the",
    "start": "396880",
    "end": "403759"
  },
  {
    "text": "performance of our biggest nvidia gpu gp3 machines",
    "start": "403759",
    "end": "409199"
  },
  {
    "text": "each inference chip has four neuron cores and euron core is a high performance",
    "start": "409199",
    "end": "415680"
  },
  {
    "text": "matrix manipulation engine and there are certain inference workloads which run very very well on this and again we can",
    "start": "415680",
    "end": "422319"
  },
  {
    "text": "help you profile your and migrate your workloads to this do",
    "start": "422319",
    "end": "428160"
  },
  {
    "text": "contact your aws support representative or your account team to help each neuron core has a two-stage memory",
    "start": "428160",
    "end": "434960"
  },
  {
    "text": "architecture including external commodity dram really makes things move fast",
    "start": "434960",
    "end": "441199"
  },
  {
    "text": "small batch operations improve performance because we have a big",
    "start": "441199",
    "end": "446240"
  },
  {
    "text": "higher sustained cache on the chip this will enable neuron performance over",
    "start": "446240",
    "end": "452240"
  },
  {
    "text": "time",
    "start": "452240",
    "end": "454479"
  },
  {
    "start": "456000",
    "end": "791000"
  },
  {
    "text": "let's talk about something which we need to know uh which we're going to use within the demo later on the first thing i want to talk about is aws neuron aids",
    "start": "457280",
    "end": "465199"
  },
  {
    "text": "neuron is the sdk providing tools and drivers and allows us to use the inferentia",
    "start": "465199",
    "end": "472160"
  },
  {
    "text": "accelerated hardware it's designed from the ground up to allow maximum scalability in optimizing",
    "start": "472160",
    "end": "479120"
  },
  {
    "text": "both for the highest throughput and the lowest latency neuron has three components",
    "start": "479120",
    "end": "486319"
  },
  {
    "text": "an ahead of time compiler a runtime driver and profiling tools neuron is pre-integrated into popular",
    "start": "486319",
    "end": "492960"
  },
  {
    "text": "machine learning frameworks that we all use such as tensorflow mxnet and pytorch",
    "start": "492960",
    "end": "500080"
  },
  {
    "text": "if you're already running applications on these frameworks either on gpu or cpu you'll find a similar environment to",
    "start": "500080",
    "end": "506800"
  },
  {
    "text": "accelerate your ml application to neuron neuron is pre-installed on many of our",
    "start": "506800",
    "end": "512240"
  },
  {
    "text": "deep on all of our aws deep learning amis or you can install install it on your",
    "start": "512240",
    "end": "517919"
  },
  {
    "text": "own mile mi image by just doing a pip install and",
    "start": "517919",
    "end": "523440"
  },
  {
    "text": "there are instructions on how to do this on our github repositories with really good demos",
    "start": "523440",
    "end": "528560"
  },
  {
    "text": "we'll be talking you through a demo later on in most cases we see customers change in just a few lines of code",
    "start": "528560",
    "end": "535279"
  },
  {
    "text": "um and we'll have an example of that now here is an example of the effort",
    "start": "535279",
    "end": "540560"
  },
  {
    "text": "required to deploy your model with inferentia and neuron as you can see developers just need to change one line",
    "start": "540560",
    "end": "546640"
  },
  {
    "text": "of code replacing torch trace commands in your neuron with the neuron version",
    "start": "546640",
    "end": "552800"
  },
  {
    "text": "which happens to be called similarly torch neuron trace",
    "start": "552800",
    "end": "557839"
  },
  {
    "text": "when you run this neuron will be called under the hood and compile and optimize the model for inferential and then",
    "start": "557839",
    "end": "563120"
  },
  {
    "text": "execute compiling the model is only necessary once it can be done offline",
    "start": "563120",
    "end": "569680"
  },
  {
    "text": "enterprise support has experts to help you with these migration strategies where they may become more complex",
    "start": "569680",
    "end": "576560"
  },
  {
    "text": "please do reach out i'd like to cover a few important",
    "start": "576560",
    "end": "582160"
  },
  {
    "text": "capabilities that are designed to allow developers to focus on optimizing their applications",
    "start": "582160",
    "end": "588880"
  },
  {
    "text": "and scale inference workloads as seamlessly as possible the first part is smart partitioning",
    "start": "588880",
    "end": "595440"
  },
  {
    "text": "smart partitioning automatically optimizes neural net compute and decides",
    "start": "595440",
    "end": "600640"
  },
  {
    "text": "what parts of the graph model will run on inferential chips which parts will be executed by the framework",
    "start": "600640",
    "end": "606880"
  },
  {
    "text": "aws neuron is aware of system resources isn't and is able to partition the neural net",
    "start": "606880",
    "end": "612640"
  },
  {
    "text": "graph intelligently so that most the most demanding compute operations or an inferential while the",
    "start": "612640",
    "end": "618959"
  },
  {
    "text": "operation other operations that run more efficiently on cpu end up partitioned to the framework",
    "start": "618959",
    "end": "624880"
  },
  {
    "text": "this system capability is a pragmatic approach to running inference workloads in the cloud and allows developers to",
    "start": "624880",
    "end": "630880"
  },
  {
    "text": "utilize the system and increase overall performance the second part i want to tell you about",
    "start": "630880",
    "end": "637839"
  },
  {
    "text": "many developers told us that although training models in floating point 32 fp32 is time consuming and expensive",
    "start": "637839",
    "end": "646480"
  },
  {
    "text": "the advantage is that it provides better accuracy but 32-bit computers expensive",
    "start": "646480",
    "end": "651600"
  },
  {
    "text": "and high-power in high power and consumption it's really hard to move to lower 16-bit floating point or integers",
    "start": "651600",
    "end": "658959"
  },
  {
    "text": "to optimize cost the problem solved with aws neurons capability",
    "start": "658959",
    "end": "664000"
  },
  {
    "text": "we call floating point 32 automatic casting take some of that heavy lifting away from you",
    "start": "664000",
    "end": "669680"
  },
  {
    "text": "it enables you to to do some of the more interesting in their work edurash neuron and inferentia are the",
    "start": "669680",
    "end": "675600"
  },
  {
    "text": "first ml accelerators in aws that take trained thirsty two 32-bit models and",
    "start": "675600",
    "end": "681440"
  },
  {
    "text": "we're gonna run them at the speed of 16-bit using the uh the the",
    "start": "681440",
    "end": "686880"
  },
  {
    "text": "16-bit data types the conversion happens automatically in hardware and is enabled by default it",
    "start": "686880",
    "end": "693440"
  },
  {
    "text": "enables you to keep high precision trained models and seamlessly run high throughput inference enjoying the speed",
    "start": "693440",
    "end": "699920"
  },
  {
    "text": "and the low cost of 16-bit data types let's talk about another interesting",
    "start": "699920",
    "end": "705360"
  },
  {
    "text": "part here where we have the uh the neuron core pipeline customers that are running real-time",
    "start": "705360",
    "end": "711760"
  },
  {
    "text": "inference applications like search and voice assistance are constantly looking for ways to increase throughput",
    "start": "711760",
    "end": "718880"
  },
  {
    "text": "but also keep latency of each call low they need to maintain the",
    "start": "718880",
    "end": "723920"
  },
  {
    "text": "interactiveness of the application to users while cpu and gpu",
    "start": "723920",
    "end": "729760"
  },
  {
    "text": "with cpu and gpu this usually means that they're forced to run inference one at a time which typically solves the latency",
    "start": "729760",
    "end": "737680"
  },
  {
    "text": "constraint but it does come at a high cost because running one inference call at a",
    "start": "737680",
    "end": "743040"
  },
  {
    "text": "time um has low low utilization of expensive hardware this is not um the",
    "start": "743040",
    "end": "748959"
  },
  {
    "text": "way we like our customers to think of the cloud and use the cloud with neuron pipeline the entire models",
    "start": "748959",
    "end": "754880"
  },
  {
    "text": "can be stored in the cache of the memory attached to each neuron neuron core the neuron cache memory is about five",
    "start": "754880",
    "end": "761519"
  },
  {
    "text": "times faster than the latest versions you'll typically see within gpus",
    "start": "761519",
    "end": "767519"
  },
  {
    "text": "the final part i want to tell you about a neuron core groups that enable application builds can currently deploy",
    "start": "768240",
    "end": "773839"
  },
  {
    "text": "multiple models onto our chips it's very flexible and allows additional",
    "start": "773839",
    "end": "778959"
  },
  {
    "text": "additional speed increases for example when using different neural networks you can run as a pipeline of a",
    "start": "778959",
    "end": "785279"
  },
  {
    "text": "given input really really helps",
    "start": "785279",
    "end": "789600"
  },
  {
    "text": "let's have a look at actually some of the the numbers and the comparisons here",
    "start": "790880",
    "end": "796880"
  },
  {
    "start": "791000",
    "end": "1388000"
  },
  {
    "text": "on how info one performs when compared and being used with",
    "start": "796880",
    "end": "802240"
  },
  {
    "text": "common deep learning models we see an object detection example we see a natural language",
    "start": "802240",
    "end": "808800"
  },
  {
    "text": "example we'll be seeing an example of natural language later on actually running in the demo",
    "start": "808800",
    "end": "815279"
  },
  {
    "text": "we also see image classification and you'll see that when compared when comparing inf1 to",
    "start": "815279",
    "end": "821600"
  },
  {
    "text": "g4 you get a two point uh or just under three times higher throughput",
    "start": "821600",
    "end": "827360"
  },
  {
    "text": "which means that your cost of inference is 76 lower that's when using image detection we have similar figures there",
    "start": "827360",
    "end": "834160"
  },
  {
    "text": "within the other the other types of inference",
    "start": "834160",
    "end": "838800"
  },
  {
    "text": "summarizing that using bert base to compare we see 24 higher throughput than",
    "start": "841680",
    "end": "847600"
  },
  {
    "text": "g5 and 68 lower cost than g5",
    "start": "847600",
    "end": "852880"
  },
  {
    "text": "this can really save cost and reduce latency for our customers and that's what makes us really excited to talk",
    "start": "852880",
    "end": "858160"
  },
  {
    "text": "about this kind of thing speaking of being excited about what our customers are doing we have an example",
    "start": "858160",
    "end": "864240"
  },
  {
    "text": "here in real life airbnb who i use quite regularly millions of guests are using uh their",
    "start": "864240",
    "end": "870800"
  },
  {
    "text": "chat bots they saw a two times improvement in throughput over out of the box gpu-based instances for their pi",
    "start": "870800",
    "end": "878000"
  },
  {
    "text": "torch-based burp models another example of the way people are",
    "start": "878000",
    "end": "883839"
  },
  {
    "text": "using inferentia is actually to reduce training costs we have other ways of reducing training costs and we'll be",
    "start": "883839",
    "end": "889440"
  },
  {
    "text": "talking about training but when you're talking about um getting",
    "start": "889440",
    "end": "895760"
  },
  {
    "text": "data to train um to train car models",
    "start": "895760",
    "end": "900880"
  },
  {
    "text": "you can see that you need to have labeled data to feed into those you need to remove duplicates you need to",
    "start": "900880",
    "end": "906800"
  },
  {
    "text": "identify unique images you need to select diverse driving scenarios and that you can do as an",
    "start": "906800",
    "end": "913360"
  },
  {
    "text": "inference task and you can then speed up your uh your inputs inferential can",
    "start": "913360",
    "end": "918560"
  },
  {
    "text": "achieve 2.85 times higher throughput and 76 percent higher influence cost than g4dn",
    "start": "918560",
    "end": "925279"
  },
  {
    "text": "instances in this particular example",
    "start": "925279",
    "end": "929199"
  },
  {
    "text": "customers are moving from digital transformation right now to sustainable transformation it's",
    "start": "930480",
    "end": "935759"
  },
  {
    "text": "increasingly important something very important to aws amazon as a whole and",
    "start": "935759",
    "end": "941600"
  },
  {
    "text": "and a lot of our customers here's a good example of a two of two times high performance per watt on a",
    "start": "941600",
    "end": "948399"
  },
  {
    "text": "large workload meaning a massive co2 save and your tam can help you visualize",
    "start": "948399",
    "end": "953440"
  },
  {
    "text": "this and help plan for sustainability you may have heard of our new well-architected pillar for",
    "start": "953440",
    "end": "959040"
  },
  {
    "text": "sustainability how do you actually get started with",
    "start": "959040",
    "end": "964320"
  },
  {
    "text": "inf1 and inferentia one of the ways which we demon later is with sagemaker this is the easiest and",
    "start": "964320",
    "end": "970560"
  },
  {
    "text": "the quickest way amazon sage makers fully manage service for building training and deploying ml",
    "start": "970560",
    "end": "976240"
  },
  {
    "text": "models quickly inf1 instances and new one are integrated into sagemaker and provide a",
    "start": "976240",
    "end": "982480"
  },
  {
    "text": "one click deployment of models into inf1 you can also use eks you can also use",
    "start": "982480",
    "end": "989199"
  },
  {
    "text": "ecs you could also use our amis and you could",
    "start": "989199",
    "end": "994480"
  },
  {
    "text": "use that as well or your own ami as we discussed earlier",
    "start": "994480",
    "end": "999680"
  },
  {
    "text": "informs available in the majority of our regions again give us a call if you need help getting started",
    "start": "1000959",
    "end": "1008160"
  },
  {
    "text": "in the demo later melanie will be showing how to use important sagemaker and there's just a few more things that",
    "start": "1009120",
    "end": "1015279"
  },
  {
    "text": "i want you to know uh going into that demo so that i feel that you could follow it efficiently",
    "start": "1015279",
    "end": "1021040"
  },
  {
    "text": "um one of the concepts that she'll be using will be sagemaker neo",
    "start": "1021040",
    "end": "1026558"
  },
  {
    "text": "so deployments of ml models is really really complex you have to build you have to choose between building your own",
    "start": "1026559",
    "end": "1032558"
  },
  {
    "text": "algorithms or choosing aws's built-in algorithms then you have to train using various frameworks and you have to",
    "start": "1032559",
    "end": "1039600"
  },
  {
    "text": "optimize models you have to do it again and again and again you have to optimize for multiple targets",
    "start": "1039600",
    "end": "1047038"
  },
  {
    "text": "not all models are optimized for size versus performance as well you can see here in this graph that this is",
    "start": "1048079",
    "end": "1053440"
  },
  {
    "text": "sometimes a problem problem two is that not all targets are easy and this is really the main pro the",
    "start": "1053440",
    "end": "1059120"
  },
  {
    "text": "main problem but before i move on from that i've been talking for a long time are there any",
    "start": "1059120",
    "end": "1064559"
  },
  {
    "text": "questions coming in hey tom thanks for uh covering what you've been going over so far today uh",
    "start": "1064559",
    "end": "1070960"
  },
  {
    "text": "just to the point where you mentioned the sustainability side of things with choosing the implant instances and reducing kind of the carbon emissions",
    "start": "1070960",
    "end": "1076880"
  },
  {
    "text": "there do you want to make a note that next month actually all of our aws support episodes uh are going to be",
    "start": "1076880",
    "end": "1082400"
  },
  {
    "text": "rotating around uh that sustainability pillar so check those out for sure if they want to",
    "start": "1082400",
    "end": "1087679"
  },
  {
    "text": "learn more about not just in the inf ones but in other aspects that we're working on sustainability but i did have",
    "start": "1087679",
    "end": "1094000"
  },
  {
    "text": "a question uh from a user in the chat here from jibjab o6 um you were going",
    "start": "1094000",
    "end": "1100000"
  },
  {
    "text": "over kind of some you know user stories as far as you know how they can use it for the benefit but they really want to",
    "start": "1100000",
    "end": "1106160"
  },
  {
    "text": "know if you could cover some specifics on um or you can share some ways you've guided users to adopt and fund instances",
    "start": "1106160",
    "end": "1112640"
  },
  {
    "text": "over other instances yeah sure well it's always about always",
    "start": "1112640",
    "end": "1118240"
  },
  {
    "text": "thinking about workload by workflow to see where imp can be can be used first think about how you're going to run that",
    "start": "1118240",
    "end": "1124160"
  },
  {
    "text": "model think about if you're going to do it in a in a vm a container or within sagemaker",
    "start": "1124160",
    "end": "1131280"
  },
  {
    "text": "i think if uh if the model is appropriate to be run on inferential we",
    "start": "1131280",
    "end": "1136400"
  },
  {
    "text": "think that a majority can but remember that they are particularly good at matrix manipulation",
    "start": "1136400",
    "end": "1141919"
  },
  {
    "text": "and matrix calculation so that tends to help do reach out to your tam and your account team to talk through",
    "start": "1141919",
    "end": "1149039"
  },
  {
    "text": "how to profile that and we have various tools within sagemaker too to do that",
    "start": "1149039",
    "end": "1154799"
  },
  {
    "text": "and actually within the demo later on we'll be showing uh some of the some of the uh",
    "start": "1154799",
    "end": "1160960"
  },
  {
    "text": "some of the stages that that we always go through so i hope that that answers your question and if it didn't feel free",
    "start": "1160960",
    "end": "1166720"
  },
  {
    "text": "to ask an additional one great thanks tom and melanie just want to see did you have anything she wanted to add to you as well today",
    "start": "1166720",
    "end": "1173200"
  },
  {
    "text": "yeah we do have a customer actually is already in kind of using inference in their production environment especially",
    "start": "1173200",
    "end": "1179600"
  },
  {
    "text": "if they are like processing large amount of image data like we talk when we say large amount we're talking about",
    "start": "1179600",
    "end": "1185679"
  },
  {
    "text": "millions of images and each of the image is pretty big and inferencing is definitely one of the",
    "start": "1185679",
    "end": "1191520"
  },
  {
    "text": "instance type that we would suggest them especially from the cost optimization perspective and also as tom mentioned",
    "start": "1191520",
    "end": "1197679"
  },
  {
    "text": "that uh like cost of foundation is one of the big pillars that we generally help our customer when they are on like",
    "start": "1197679",
    "end": "1204799"
  },
  {
    "text": "a on their aws journey like especially for me i'm focusing on aml so we do run",
    "start": "1204799",
    "end": "1210720"
  },
  {
    "text": "uh multiple one-to-many like um our customization sessions so we're during the sessions that we're also kind of",
    "start": "1210720",
    "end": "1216720"
  },
  {
    "text": "guiding customers on what are the best ways that can help them when they are especially looking at deployment",
    "start": "1216720",
    "end": "1223120"
  },
  {
    "text": "real-time deployment because the instance is basically the endpoint also run 24 7. so using a efficient chip",
    "start": "1223120",
    "end": "1229600"
  },
  {
    "text": "like instance type would definitely help them significantly on their cost optimization site",
    "start": "1229600",
    "end": "1235760"
  },
  {
    "text": "yeah we generally see a lot of customers using uh inference uh informs for uh",
    "start": "1235760",
    "end": "1240799"
  },
  {
    "text": "computer vision and nlp models uh they they tend to rely on this this matrix",
    "start": "1240799",
    "end": "1246240"
  },
  {
    "text": "manipulation and calculation great all right well i appreciate the",
    "start": "1246240",
    "end": "1251919"
  },
  {
    "text": "extra information and audience please feel free to ask your questions here moving forward but i'll turn it back over to you to continue on to the rest",
    "start": "1251919",
    "end": "1257520"
  },
  {
    "text": "of the content great okay so we're just going into uh",
    "start": "1257520",
    "end": "1263760"
  },
  {
    "text": "more details on on neo um so not all targets are easy you need to",
    "start": "1263760",
    "end": "1269919"
  },
  {
    "text": "we talked about compilation of models and compiling for a",
    "start": "1269919",
    "end": "1275120"
  },
  {
    "text": "um a specific hardware and that's where things get hard and that's where neo steps in",
    "start": "1275120",
    "end": "1280880"
  },
  {
    "text": "um you have all of these uh you have all of these these variables cloud native or embedded",
    "start": "1280880",
    "end": "1287840"
  },
  {
    "text": "you have various frameworks you have cloud servers or maybe edge devices or maybe",
    "start": "1287840",
    "end": "1293039"
  },
  {
    "text": "both you have different architectures you have x86 risk inferential",
    "start": "1293039",
    "end": "1300720"
  },
  {
    "text": "you have multiple paths as i said you may have many you may have multiple frameworks you may have multiple",
    "start": "1301120",
    "end": "1306159"
  },
  {
    "text": "destinations of hardware so introducing sagemaker neo in a",
    "start": "1306159",
    "end": "1312960"
  },
  {
    "text": "nutshell is that you can gain an accurate model with high performance automatic optimization",
    "start": "1312960",
    "end": "1319919"
  },
  {
    "text": "we talked through some of the examples of optimization optimization which you've done earlier on",
    "start": "1319919",
    "end": "1324960"
  },
  {
    "text": "we're talking about a broad framework support and we're talking about broad hardware support so you don't need to think about all of those the cross",
    "start": "1324960",
    "end": "1331440"
  },
  {
    "text": "combination of all of those",
    "start": "1331440",
    "end": "1334720"
  },
  {
    "text": "so to summarize on that multiple deployment options you have multiple hosting options you have the ability to",
    "start": "1337840",
    "end": "1343679"
  },
  {
    "text": "do batch transforms you can use elastic imprints you have the option to run anywhere with",
    "start": "1343679",
    "end": "1349520"
  },
  {
    "text": "with neo when it comes to multiple types of hardware including deep ratio racer which is really fun",
    "start": "1349520",
    "end": "1355360"
  },
  {
    "text": "and also deep lens which is our solution for embedding machine learning into camera hardware",
    "start": "1355360",
    "end": "1362720"
  },
  {
    "text": "so with that let's actually see how this works with customers let's do a demo uh for that i'm going to hand over to",
    "start": "1362960",
    "end": "1368799"
  },
  {
    "text": "melanie do we have any questions before that does anything else come in or shall we go straight to the demo",
    "start": "1368799",
    "end": "1374960"
  },
  {
    "text": "i think we can go straight to the demo no questions currently at this time so appreciate that over to you melanie",
    "start": "1374960",
    "end": "1381760"
  },
  {
    "text": "thanks tom and really good presentation and really great information that has shared so now",
    "start": "1381760",
    "end": "1388720"
  },
  {
    "start": "1388000",
    "end": "2418000"
  },
  {
    "text": "let's move on to the demo to see how we can actually practice on using neo and",
    "start": "1388720",
    "end": "1395280"
  },
  {
    "text": "of deploying the model in on influencer so this demo basically is first",
    "start": "1395280",
    "end": "1400880"
  },
  {
    "text": "like develop and also first run by our san francisco aws submit so now we just reuse the demo",
    "start": "1400880",
    "end": "1407440"
  },
  {
    "text": "and change a little bit to just the providing uh some tailored content for this session so now the content of today's demo",
    "start": "1407440",
    "end": "1415120"
  },
  {
    "text": "basically we will go through the setting up of the environment first and then we will get the model from the pre-trend",
    "start": "1415120",
    "end": "1421760"
  },
  {
    "text": "hanging phase model zoo to get download the tokenizer and then download the model and then save it in a format that",
    "start": "1421760",
    "end": "1427520"
  },
  {
    "text": "can be used directly in sagemaker and we will default and we will deploy the model first to a cpu based endpoint",
    "start": "1427520",
    "end": "1435279"
  },
  {
    "text": "and also then after that we will perform some like inference and also we will",
    "start": "1435279",
    "end": "1440640"
  },
  {
    "text": "compile use sagemaker nil to compile the uh the preacher model and deploy the model to an inference in uh like",
    "start": "1440640",
    "end": "1447120"
  },
  {
    "text": "influencer in instance so you will be able to compare like how easy it is uh",
    "start": "1447120",
    "end": "1452240"
  },
  {
    "text": "from deploying to a traditional like cpu based instance and to a influencer",
    "start": "1452240",
    "end": "1457360"
  },
  {
    "text": "instance and also using nil to do modal compilation and after that we will do a",
    "start": "1457360",
    "end": "1462640"
  },
  {
    "text": "benchmark comparison and compare the two performance of these two endpoint and then",
    "start": "1462640",
    "end": "1469760"
  },
  {
    "text": "end the demo so now let's first have a look at as we have already introduced uh",
    "start": "1470159",
    "end": "1476080"
  },
  {
    "text": "aws inferencia uh neuron and also sagemaker neo i will not take too much time in this section but then basically",
    "start": "1476080",
    "end": "1483600"
  },
  {
    "text": "we want to deploy into two endpoints one is cpu-based or one is inferencia so we",
    "start": "1483600",
    "end": "1490080"
  },
  {
    "text": "will be able to compare the performance of these two endpoint type",
    "start": "1490080",
    "end": "1495600"
  },
  {
    "text": "so a little bit more information like if you want to read a bit here about inferencia and neuron and which",
    "start": "1495600",
    "end": "1503200"
  },
  {
    "text": "are like i think tom has already covered most of the information that you will",
    "start": "1503200",
    "end": "1508240"
  },
  {
    "text": "need to to understand how these different terminology and for the demo first like we need to",
    "start": "1508240",
    "end": "1515440"
  },
  {
    "text": "set up the environment basically we just um perform some imports and also do pip",
    "start": "1515440",
    "end": "1520480"
  },
  {
    "text": "install to upgrade the packages that we use so upgrade the transformer sagemaker and pytorch and now we import these",
    "start": "1520480",
    "end": "1528080"
  },
  {
    "text": "packages to the notebook uh in addition in this notebook you can see that we're using uh",
    "start": "1528080",
    "end": "1534880"
  },
  {
    "text": "like pi torch 1.3 python kernel and that is running on a",
    "start": "1534880",
    "end": "1541200"
  },
  {
    "text": "cpu instance that's two cpu with eight a gigabyte of memory",
    "start": "1541200",
    "end": "1546240"
  },
  {
    "text": "so once we set up the environment and we will first load the model from uh hugging phase model hub so this model is",
    "start": "1546240",
    "end": "1553760"
  },
  {
    "text": "pre-trained to basically perform a binary classification uh if you put",
    "start": "1553760",
    "end": "1559679"
  },
  {
    "text": "like a like string of paragraph and it will tell you whether this input is a paragraph or not a paragraph",
    "start": "1559679",
    "end": "1566640"
  },
  {
    "text": "so now like we will first get the tokenizer so the tokenizer basically um",
    "start": "1566640",
    "end": "1572720"
  },
  {
    "text": "like it's helping to uh split the the strings into a smaller unit to for to",
    "start": "1572720",
    "end": "1579919"
  },
  {
    "text": "prepare the input for the model to perform transformation and then the model basically that we get",
    "start": "1579919",
    "end": "1587279"
  },
  {
    "text": "is directly from the pre-trend model and then we are one important note is that",
    "start": "1587279",
    "end": "1592400"
  },
  {
    "text": "we need to set the return dictionary to false to be able to use without a compilation and now like",
    "start": "1592400",
    "end": "1599200"
  },
  {
    "text": "once we downloaded the model we need to prepare the model uh to be used for sagemaker as if you",
    "start": "1599200",
    "end": "1605520"
  },
  {
    "text": "have already used sagemaker hosting this is one of the typical examples that we",
    "start": "1605520",
    "end": "1610720"
  },
  {
    "text": "we say it's bring your own model to stage marker hosting which means like in sagemaker you don't really necessarily",
    "start": "1610720",
    "end": "1617200"
  },
  {
    "text": "need to do all the training everything inside the stage maker to be able to use host so you can directly bring your",
    "start": "1617200",
    "end": "1623360"
  },
  {
    "text": "model that's maybe trend elsewhere or pre-trend by the the the commonly used",
    "start": "1623360",
    "end": "1628799"
  },
  {
    "text": "pre-trim models available so to directly bring them into sagemaker so",
    "start": "1628799",
    "end": "1634720"
  },
  {
    "text": "to prepare the model to be used in sagemaker you need to make sure that the model is saved in the table file in",
    "start": "1634720",
    "end": "1641279"
  },
  {
    "text": "turbo format and saves on s3 so when you deploy the model the stagemaker service will handle downloading the model from",
    "start": "1641279",
    "end": "1647679"
  },
  {
    "text": "s3 and until the model and then store the model artifact into the past as",
    "start": "1647679",
    "end": "1653440"
  },
  {
    "text": "a slash opt slash ml slash model pass and then here we also like to perform",
    "start": "1653440",
    "end": "1660080"
  },
  {
    "text": "uh like one of the model is saved as the normal model directly and then we save another trace model uh in the uh other",
    "start": "1660080",
    "end": "1667520"
  },
  {
    "text": "model paths that will be used for the compilation so now you can see that we are actually uh saved the model as a",
    "start": "1667520",
    "end": "1674640"
  },
  {
    "text": "table file so you can see that in when using sagemaker we're expecting the",
    "start": "1674640",
    "end": "1679760"
  },
  {
    "text": "model that pi torch like we using pi towards like a prebuilt container we're",
    "start": "1679760",
    "end": "1685039"
  },
  {
    "text": "expecting the model format to be either dot pt or pth format so once we have saved the model into a",
    "start": "1685039",
    "end": "1691919"
  },
  {
    "text": "table file we upload the model in the next cell to s3 so you will see the",
    "start": "1691919",
    "end": "1697200"
  },
  {
    "text": "the prefix of the s3 model location",
    "start": "1697200",
    "end": "1702000"
  },
  {
    "text": "so this will once we saved the model onto s3 and then we will have the two model",
    "start": "1702320",
    "end": "1709120"
  },
  {
    "text": "locations we will first deploy the model to a cpu-based endpoint",
    "start": "1709120",
    "end": "1715600"
  },
  {
    "text": "so when we deploy the model into a cpu-based endpoint the the strategy that we use here is using like a",
    "start": "1715600",
    "end": "1723440"
  },
  {
    "text": "a script mode stage maker script mode so some like simple uh like a background",
    "start": "1723440",
    "end": "1729279"
  },
  {
    "text": "about sagemaker script mode is that when you use the um common framework like pytorch tensorflow",
    "start": "1729279",
    "end": "1736720"
  },
  {
    "text": "mxnet like we support screen mode which means customer can providing an entry point script so inside that entry point",
    "start": "1736720",
    "end": "1743600"
  },
  {
    "text": "script in this example that we see is inference.pi so in that inference.pi file you will be able to define your",
    "start": "1743600",
    "end": "1750640"
  },
  {
    "text": "input function model function predict function and output function so all these functions are customer custom",
    "start": "1750640",
    "end": "1757520"
  },
  {
    "text": "functions where you will be able to specify how you would like to load model how you would like to load the input and",
    "start": "1757520",
    "end": "1764640"
  },
  {
    "text": "also like perform predictions so everything is customized so in this example you can see in the",
    "start": "1764640",
    "end": "1770480"
  },
  {
    "text": "model function we actually depend down the model type for example if we see",
    "start": "1770480",
    "end": "1775520"
  },
  {
    "text": "there is a model underscore neuron.pt file and we we know that this is a compile model so then we will we will",
    "start": "1775520",
    "end": "1782640"
  },
  {
    "text": "choose the compile model uh like pass to basically read the model into memory load the model into memory",
    "start": "1782640",
    "end": "1789279"
  },
  {
    "text": "or else if just normal model we use the normal way of loading a pi torch model",
    "start": "1789279",
    "end": "1795360"
  },
  {
    "text": "into memory so then in the input function basically specifies how we would like to treat the input data or",
    "start": "1795360",
    "end": "1801279"
  },
  {
    "text": "what type of data that we're expecting in this case we're expecting the json format of data so we will use json load",
    "start": "1801279",
    "end": "1807679"
  },
  {
    "text": "to read in the payload and then in the predict function as we have the already know the input data and",
    "start": "1807679",
    "end": "1814480"
  },
  {
    "text": "motor load into memory so we will specify our own ways of how we would like to perform predictions",
    "start": "1814480",
    "end": "1821279"
  },
  {
    "text": "using the model against the input data and also you can specify the output function so you see in the input",
    "start": "1821279",
    "end": "1828000"
  },
  {
    "text": "function in output string that we specify a a specific string say birth",
    "start": "1828000",
    "end": "1833360"
  },
  {
    "text": "predicts that something r whether it's a phrase or not phrase but this is customizable so this is uh like in this",
    "start": "1833360",
    "end": "1840559"
  },
  {
    "text": "example that we just choose to make it more clear of the output string",
    "start": "1840559",
    "end": "1846080"
  },
  {
    "text": "but you can choose any way that you want when you want to perform prediction so",
    "start": "1846080",
    "end": "1851760"
  },
  {
    "text": "now the next step is that we are creating a uh like a hugging phase model so we just use sagemaker sdk stagemaker",
    "start": "1851760",
    "end": "1859279"
  },
  {
    "text": "sdk is our high level api which abstracts a lot of the uh",
    "start": "1859279",
    "end": "1864399"
  },
  {
    "text": "parameters that when you want to make the api calls to create a model or create endpoint so",
    "start": "1864399",
    "end": "1870880"
  },
  {
    "text": "this api basically uh in this class the hacking phase model class you will be",
    "start": "1870880",
    "end": "1876320"
  },
  {
    "text": "able to specify for example the the transformer version pi torch version the",
    "start": "1876320",
    "end": "1881519"
  },
  {
    "text": "entry point script and then the source stir so important note here like",
    "start": "1881519",
    "end": "1886799"
  },
  {
    "text": "basically when you provide a solster the sagemaker service or the sagemaker sdk",
    "start": "1886799",
    "end": "1892880"
  },
  {
    "text": "will basically target everything in that folder in this case it's the code folder and then put",
    "start": "1892880",
    "end": "1899279"
  },
  {
    "text": "that everything into a sourceter.gz that will be available when you deploy",
    "start": "1899279",
    "end": "1904480"
  },
  {
    "text": "the model to an endpoint that source third auto gz file that is stored on",
    "start": "1904480",
    "end": "1910000"
  },
  {
    "text": "xray will be downloaded to your container like the hosting container and installed the under a slash opt ml",
    "start": "1910000",
    "end": "1918159"
  },
  {
    "text": "code folder that will be made available so if you have additional files or for",
    "start": "1918159",
    "end": "1923360"
  },
  {
    "text": "example if you want to pip install additional packages you can basically put a requirements.txt file",
    "start": "1923360",
    "end": "1930000"
  },
  {
    "text": "there so when the service like when we the sagemaker service runs the container",
    "start": "1930000",
    "end": "1935120"
  },
  {
    "text": "and like notice there's a requirements.txt file in the code directory it will automatically pip",
    "start": "1935120",
    "end": "1940960"
  },
  {
    "text": "install all the necessary packages that you put there so this is just additional information regarding how to use sagemaker",
    "start": "1940960",
    "end": "1948640"
  },
  {
    "text": "python sdk and also perform modal deployment so this model function",
    "start": "1948640",
    "end": "1953760"
  },
  {
    "text": "will return a modal object so that model object basically has a deploy function",
    "start": "1953760",
    "end": "1959120"
  },
  {
    "text": "in you can directly use the deploy function here so what this deploy function does is that it will make three",
    "start": "1959120",
    "end": "1964720"
  },
  {
    "text": "ap calls it will make a straight model api call and then that will",
    "start": "1964720",
    "end": "1969760"
  },
  {
    "text": "create the actual uh stage micro model and then create an endpoint configuration and at the end based on",
    "start": "1969760",
    "end": "1976159"
  },
  {
    "text": "endpoint configuration create the endpoint now we can see here in the endpoint configuration we're using a c5",
    "start": "1976159",
    "end": "1981840"
  },
  {
    "text": "x large which has uh two cpu i think it's two cpu eight",
    "start": "1981840",
    "end": "1987039"
  },
  {
    "text": "gig of memory so which is equivalent to the cpu uh uh like and memory setup as infusion",
    "start": "1987039",
    "end": "1995039"
  },
  {
    "text": "and uh like once we deploy the model you will be able to see okay it takes about",
    "start": "1995039",
    "end": "2000720"
  },
  {
    "text": "four minutes to a deploy model to an endpoint now we just perform an inference and you can see that based on",
    "start": "2000720",
    "end": "2007360"
  },
  {
    "text": "the input and then the model just predicted this is a paragraph so now let's let's continue to basically",
    "start": "2007360",
    "end": "2015840"
  },
  {
    "text": "perform a compile a compilation of the patreon model and then deploy to inference influencer instance so in the",
    "start": "2015840",
    "end": "2023279"
  },
  {
    "text": "first section that similarly as the previously done with the cpu instance we perform",
    "start": "2023279",
    "end": "2029519"
  },
  {
    "text": "we create a model object so we similarly you can see the code is almost identical",
    "start": "2029519",
    "end": "2034799"
  },
  {
    "text": "to the ones that we had when we need uh when we deploy um to us cpu instance",
    "start": "2034799",
    "end": "2041519"
  },
  {
    "text": "and here um we only just like that returned like model object is",
    "start": "2041519",
    "end": "2048000"
  },
  {
    "text": "uh specify a different name called compiled in model and this modal object",
    "start": "2048000",
    "end": "2053040"
  },
  {
    "text": "like directly has a compile function so this compound function basically handles the model compilation so some keynote",
    "start": "2053040",
    "end": "2060158"
  },
  {
    "text": "here is that they they the input shape is basically specifies the name and",
    "start": "2060159",
    "end": "2066000"
  },
  {
    "text": "shape of the expected inputs for your trend model in json dictionary and then like you see like we specify",
    "start": "2066000",
    "end": "2073520"
  },
  {
    "text": "the framework and then also the output path is basically the compiled model path that is",
    "start": "2073520",
    "end": "2079760"
  },
  {
    "text": "on s3 and also the compile option like here like we have put a note here in plugin",
    "start": "2079760",
    "end": "2087358"
  },
  {
    "text": "phase model should be compiled to d type in 64. so that's what what we set up here",
    "start": "2087359",
    "end": "2094560"
  },
  {
    "text": "and then the the option need to be kind of provided as a json format and then you can see the compilation like takes a",
    "start": "2094560",
    "end": "2101040"
  },
  {
    "text": "bit time and here we have already cut off the the time that they're taking so overall it takes about 13 minutes or",
    "start": "2101040",
    "end": "2108880"
  },
  {
    "text": "to to perform the model compilation and once the model is compiled you can see that automatically uploaded to s3",
    "start": "2108880",
    "end": "2115440"
  },
  {
    "text": "location so the good thing about using python sdk is that when you actually um",
    "start": "2115440",
    "end": "2121760"
  },
  {
    "text": "you significantly reduce the amount of code that you need when you compare to use portal 3 and you can see that you can",
    "start": "2121760",
    "end": "2128560"
  },
  {
    "text": "directly use the same mode object to call the deploy function where the deploy function can",
    "start": "2128560",
    "end": "2134079"
  },
  {
    "text": "automatically identify where the compiled model path is and then use that model we need to deploy to endpoint and",
    "start": "2134079",
    "end": "2141440"
  },
  {
    "text": "you can see that here we have changed instance type to inf1 x launch and nothing else",
    "start": "2141440",
    "end": "2148079"
  },
  {
    "text": "like has changed so it is identical to the one that we when we are deploying trying to deploy",
    "start": "2148079",
    "end": "2154560"
  },
  {
    "text": "to a cpu instance so the from the code perspective like you don't really need to change anything only the instance",
    "start": "2154560",
    "end": "2160400"
  },
  {
    "text": "type that is different and here we set the weight to true you can set the weight to false which means",
    "start": "2160400",
    "end": "2166400"
  },
  {
    "text": "like once you make the api call once you like uh run the cell of code and it will",
    "start": "2166400",
    "end": "2172560"
  },
  {
    "text": "directly finish but if you set way to true so the cell basically wait for the endpoint to be deployed",
    "start": "2172560",
    "end": "2179040"
  },
  {
    "text": "so when we are deploying when you are deploying an endpoint you can basically switch to",
    "start": "2179040",
    "end": "2184560"
  },
  {
    "text": "stagemaker console to see that uh to monitor how the endpoint deploy so you",
    "start": "2184560",
    "end": "2190000"
  },
  {
    "text": "can see that when the endpoint is uh kind of being created you will see the status as create so once the endpoint is",
    "start": "2190000",
    "end": "2197440"
  },
  {
    "text": "ready and it will show as the endpoint in service status so basically in the",
    "start": "2197440",
    "end": "2202720"
  },
  {
    "text": "back end where like the sagemaker server is basically we need to spin up the instance download the container and then",
    "start": "2202720",
    "end": "2209359"
  },
  {
    "text": "like once the container is up and running everything is running and then we will do some ping health check to making sure that the endpoint is healthy",
    "start": "2209359",
    "end": "2216079"
  },
  {
    "text": "and once the endpoint passed the ping health check you will be able to see the endpoint becomes in service status now",
    "start": "2216079",
    "end": "2222960"
  },
  {
    "text": "as the endpoint becomes in service you will be able to see that the endpoint is ready",
    "start": "2222960",
    "end": "2228960"
  },
  {
    "text": "and then and then now the next step we will perform the inference against endpoint uh",
    "start": "2228960",
    "end": "2234720"
  },
  {
    "text": "i mean basically it's the same same code that we used before and you can see that the model predicts this",
    "start": "2234720",
    "end": "2241520"
  },
  {
    "text": "input as a paragraph so now we are performing some benchmark and comparison so here we're basically ascending uh",
    "start": "2241520",
    "end": "2248880"
  },
  {
    "text": "using a multi-thread to basically sending concurrent invocations against the",
    "start": "2248880",
    "end": "2254560"
  },
  {
    "text": "endpoint the first endpoint that we're gonna test is the cpu endpoint and we're gonna send like each thread we stand uh",
    "start": "2254560",
    "end": "2261200"
  },
  {
    "text": "100 invocations and uh concurrently and in five threads",
    "start": "2261200",
    "end": "2266720"
  },
  {
    "text": "and then once the invocation is done you can see that this is the distribution of",
    "start": "2266720",
    "end": "2272160"
  },
  {
    "text": "the latency so as as it indicates here you can see like",
    "start": "2272160",
    "end": "2277359"
  },
  {
    "text": "59 of the time the request takes less than three seconds which is uh",
    "start": "2277359",
    "end": "2283280"
  },
  {
    "text": "three minutes second at 3000 millisecond and then the rough request throughput is two requests per",
    "start": "2283280",
    "end": "2290720"
  },
  {
    "text": "second so the throughput is like two tps so now let's move on to the same similar",
    "start": "2290720",
    "end": "2296880"
  },
  {
    "text": "benchmarking for the inferencial instance so again we do the five thread to send like each thread 100 invocations",
    "start": "2296880",
    "end": "2304160"
  },
  {
    "text": "against inferential endpoint and also like once the the like inference is done",
    "start": "2304160",
    "end": "2310400"
  },
  {
    "text": "we will perform the uh like upload the histogram and we can see that 95 of the request",
    "start": "2310400",
    "end": "2318240"
  },
  {
    "text": "takes less than 35 millisecond and then the throughput is 62 a 162 tps so",
    "start": "2318240",
    "end": "2324960"
  },
  {
    "text": "overall using influencer instance that we can see here the summary it dropped",
    "start": "2324960",
    "end": "2330880"
  },
  {
    "text": "the latency to 35 millisecond from three seconds on a cpu instance and the average",
    "start": "2330880",
    "end": "2337760"
  },
  {
    "text": "throughput increased 262 tps from just two tps on the cpu so you can see a",
    "start": "2337760",
    "end": "2345200"
  },
  {
    "text": "significant improvement on the latency and also like um and also",
    "start": "2345200",
    "end": "2350640"
  },
  {
    "text": "improve on the overall throughput so now after this is done and we just clean up",
    "start": "2350640",
    "end": "2358720"
  },
  {
    "text": "and that basically ends the demo i will hand it back to tom to continue with the",
    "start": "2358720",
    "end": "2364400"
  },
  {
    "text": "presentation thank you very much melanie um great to see the uh the large uh",
    "start": "2364400",
    "end": "2371359"
  },
  {
    "text": "well the decreases in uh latency they're really really big uh big changes",
    "start": "2371359",
    "end": "2376560"
  },
  {
    "text": "uh and my next slide says a very similar thing we talked earlier somebody asked the",
    "start": "2376560",
    "end": "2382079"
  },
  {
    "text": "question about what inferentia um we see customers using inferential for and we",
    "start": "2382079",
    "end": "2387520"
  },
  {
    "text": "mentioned that inferential is great for computer vision and nlp computer vision like uh yolos and birds are a great fit",
    "start": "2387520",
    "end": "2395440"
  },
  {
    "text": "here's an example um using bert you can see that impron delivers",
    "start": "2395440",
    "end": "2400560"
  },
  {
    "text": "2.47 times higher throughput than g4 and 24 higher throughput than g5",
    "start": "2400560",
    "end": "2406560"
  },
  {
    "text": "it delivers 84 lower cost than g4 and uh 68 lower",
    "start": "2406560",
    "end": "2412400"
  },
  {
    "text": "cost than g5 so really really kind of backs up what you were saying so that's good",
    "start": "2412400",
    "end": "2419359"
  },
  {
    "start": "2418000",
    "end": "2669000"
  },
  {
    "text": "we've talked generally about inference today because we're using our inferential chips",
    "start": "2419359",
    "end": "2425119"
  },
  {
    "text": "uh just around the corner we will be um releasing our trainium chips which are uh purpose-built chips for",
    "start": "2425119",
    "end": "2432319"
  },
  {
    "text": "training which is a subtly different um machine learning thing and requires",
    "start": "2432319",
    "end": "2437520"
  },
  {
    "text": "to be that efficient requires a specific uh specific hardware",
    "start": "2437520",
    "end": "2442560"
  },
  {
    "text": "our trn-1 enables the most cost-effective training in the cloud for a wide range of deep learning",
    "start": "2442560",
    "end": "2448000"
  },
  {
    "text": "workloads address is generating across chip servers and the data center connectivity to give our customers",
    "start": "2448000",
    "end": "2453920"
  },
  {
    "text": "cutting-edge hardware and this is the example of that we have these in we should have these",
    "start": "2453920",
    "end": "2459359"
  },
  {
    "text": "generally available within the first half of this year",
    "start": "2459359",
    "end": "2464078"
  },
  {
    "text": "let's just talk through a few examples a few people um asked earlier on about",
    "start": "2464640",
    "end": "2471040"
  },
  {
    "text": "how customers are actually using this um we have one example internal to amazon using aws they manage",
    "start": "2471040",
    "end": "2478640"
  },
  {
    "text": "to lower their latency by 30 percent and save cost by 71 percent the alexa team always very interesting",
    "start": "2478640",
    "end": "2485280"
  },
  {
    "text": "for me to being an avid alexa user i won't be able to turn my lights on and off without it the alexa team managed to migrate their",
    "start": "2485280",
    "end": "2492160"
  },
  {
    "text": "highly complex text-to-speech model that generates human-like speech to inferentia eighty",
    "start": "2492160",
    "end": "2497599"
  },
  {
    "text": "percent of alexa voice responses are now synthesized on inf1 leading to a 30 cost",
    "start": "2497599",
    "end": "2503200"
  },
  {
    "text": "saving inferential provides a 25 reduction in the latency which is really important",
    "start": "2503200",
    "end": "2509680"
  },
  {
    "text": "when you're waiting for your lights to come on that could be a critical situation",
    "start": "2509680",
    "end": "2514960"
  },
  {
    "text": "we already talked about airbnb um sprinklers ai driven",
    "start": "2514960",
    "end": "2520240"
  },
  {
    "text": "unified customer experience platform enables companies to gather and translate real-time customer feedback",
    "start": "2520240",
    "end": "2526319"
  },
  {
    "text": "across multiple channels using imp1 showed a significant performance",
    "start": "2526319",
    "end": "2532480"
  },
  {
    "text": "increase at lower cost autodesk answers over a hundred thousand customer",
    "start": "2532480",
    "end": "2537760"
  },
  {
    "text": "questions per month by applying natural natural language um and deep learning techniques",
    "start": "2537760",
    "end": "2544640"
  },
  {
    "text": "they managed to um gain a four point nine percent higher throughput over four put uh g4",
    "start": "2544640",
    "end": "2550560"
  },
  {
    "text": "dn anthem's application automates the generation of actionable insights for",
    "start": "2550560",
    "end": "2555920"
  },
  {
    "text": "customer opinion options using deep learning natural language models",
    "start": "2555920",
    "end": "2561839"
  },
  {
    "text": "they deployed their deep learning workload to inf1 the results were two times",
    "start": "2561839",
    "end": "2568480"
  },
  {
    "text": "throughput increase so another great example thank you very much",
    "start": "2568480",
    "end": "2574480"
  },
  {
    "text": "i've had a lot of fun here i hope you have any more questions",
    "start": "2574480",
    "end": "2579680"
  },
  {
    "text": "melanie thank you very much for the information you provided today and showing us how you know customers can really adopt these info instances and",
    "start": "2579680",
    "end": "2585839"
  },
  {
    "text": "get some cost savings and how we ourselves right amazon and aws have enabled uh some of those cost reductions",
    "start": "2585839",
    "end": "2591839"
  },
  {
    "text": "and efficiencies by leveraging this so no questions from the audience though um so",
    "start": "2591839",
    "end": "2597520"
  },
  {
    "text": "everyone today we look at some aws custom hardware and how it can be uh easily used to save money uh have faster",
    "start": "2597520",
    "end": "2604240"
  },
  {
    "text": "inference latency and be more sustainable overall again make sure you check out uh next month's sustainability",
    "start": "2604240",
    "end": "2610160"
  },
  {
    "text": "pillars that we'll be going over in our episodes if there are any questions that were not answered today or you think of",
    "start": "2610160",
    "end": "2615280"
  },
  {
    "text": "any questions later on please head on over to repost.ews for one of our experts can provide you with an answer",
    "start": "2615280",
    "end": "2621119"
  },
  {
    "text": "on your question and it could even become a topic for one of our future shows so",
    "start": "2621119",
    "end": "2626560"
  },
  {
    "text": "if you have feedback for us please check the chat box on the right i'll post a link to the survey here shortly or you",
    "start": "2626560",
    "end": "2632079"
  },
  {
    "text": "can email us aws supports you at amazon.com because we really do want to hear from you and uh hear what you'd",
    "start": "2632079",
    "end": "2638000"
  },
  {
    "text": "like to see on the show thank you for joining us at aws supports you happy cloud computing",
    "start": "2638000",
    "end": "2644000"
  },
  {
    "text": "also don't forget to stay tuned thank you",
    "start": "2644000",
    "end": "2650000"
  },
  {
    "text": "thank you everyone bye-bye thanks bye [Music]",
    "start": "2650000",
    "end": "2665369"
  },
  {
    "text": "you",
    "start": "2668480",
    "end": "2670560"
  }
]