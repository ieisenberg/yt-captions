[
  {
    "text": "hi everyone this is kaval jit KMI",
    "start": "4440",
    "end": "6000"
  },
  {
    "text": "working as principal AML specialist",
    "start": "6000",
    "end": "7680"
  },
  {
    "text": "Solutions architect and in this session",
    "start": "7680",
    "end": "9599"
  },
  {
    "text": "we will learn how you can use Amazon",
    "start": "9599",
    "end": "11120"
  },
  {
    "text": "sagemaker to deploy and inference Lama 2",
    "start": "11120",
    "end": "13759"
  },
  {
    "text": "models to build chatboard like",
    "start": "13759",
    "end": "15920"
  },
  {
    "text": "applications so let us see a quick",
    "start": "15920",
    "end": "17920"
  },
  {
    "text": "overview of sagemaker model serving",
    "start": "17920",
    "end": "20320"
  },
  {
    "text": "stack now sagemaker provides a very deep",
    "start": "20320",
    "end": "23359"
  },
  {
    "text": "set of machine learning tools and",
    "start": "23359",
    "end": "25279"
  },
  {
    "text": "capabilities uh which makes your journey",
    "start": "25279",
    "end": "27199"
  },
  {
    "text": "of deploying your models very customized",
    "start": "27199",
    "end": "29679"
  },
  {
    "text": "perform and managed if you look at this",
    "start": "29679",
    "end": "32000"
  },
  {
    "text": "tag at the bottom tier is where you have",
    "start": "32000",
    "end": "34040"
  },
  {
    "text": "an option to choose from CPUs gpus or",
    "start": "34040",
    "end": "36320"
  },
  {
    "text": "going completely serverless with your",
    "start": "36320",
    "end": "38079"
  },
  {
    "text": "model deployments in the layer above is",
    "start": "38079",
    "end": "40719"
  },
  {
    "text": "where sagemaker offers pre-built",
    "start": "40719",
    "end": "43200"
  },
  {
    "text": "optimized deep learning containers where",
    "start": "43200",
    "end": "45039"
  },
  {
    "text": "you have an option to pick up py toch",
    "start": "45039",
    "end": "47000"
  },
  {
    "text": "mxnet uh transfer flow kind of",
    "start": "47000",
    "end": "49039"
  },
  {
    "text": "Frameworks and the model serving stacks",
    "start": "49039",
    "end": "51440"
  },
  {
    "text": "and on top of these uh Saker offers",
    "start": "51440",
    "end": "53600"
  },
  {
    "text": "capabilities where you can choose from",
    "start": "53600",
    "end": "55520"
  },
  {
    "text": "multiple different inference options you",
    "start": "55520",
    "end": "57359"
  },
  {
    "text": "can choose from Real Time inference",
    "start": "57359",
    "end": "59160"
  },
  {
    "text": "async inference",
    "start": "59160",
    "end": "60960"
  },
  {
    "text": "serverless now if you look at this",
    "start": "60960",
    "end": "62760"
  },
  {
    "text": "entire stack the choice of compute uh",
    "start": "62760",
    "end": "65198"
  },
  {
    "text": "the choice of picking up a container or",
    "start": "65199",
    "end": "67680"
  },
  {
    "text": "model serving stack and the inference",
    "start": "67680",
    "end": "70080"
  },
  {
    "text": "option uh which provides you with a very",
    "start": "70080",
    "end": "72240"
  },
  {
    "text": "high performance at a lower cost is what",
    "start": "72240",
    "end": "74680"
  },
  {
    "text": "makes Hemer a very good choice to deploy",
    "start": "74680",
    "end": "77360"
  },
  {
    "text": "your models so with with that uh let us",
    "start": "77360",
    "end": "80040"
  },
  {
    "text": "quickly see how you can deploy your",
    "start": "80040",
    "end": "82119"
  },
  {
    "text": "models on stagemaker with three simple",
    "start": "82119",
    "end": "86119"
  },
  {
    "text": "steps so step one here uh we will uh",
    "start": "86159",
    "end": "89479"
  },
  {
    "text": "first point to a location where a model",
    "start": "89479",
    "end": "91600"
  },
  {
    "text": "artifacts um are are located it could be",
    "start": "91600",
    "end": "94799"
  },
  {
    "text": "an S3 location it could be hugging face",
    "start": "94799",
    "end": "96600"
  },
  {
    "text": "uh model ID as well uh Second Step we",
    "start": "96600",
    "end": "99119"
  },
  {
    "text": "Define a container image uh you can uh",
    "start": "99119",
    "end": "102360"
  },
  {
    "text": "pick up from the one of the managed uh",
    "start": "102360",
    "end": "105119"
  },
  {
    "text": "sagemaker containers that we talked",
    "start": "105119",
    "end": "106719"
  },
  {
    "text": "about in the previous slide or you can",
    "start": "106719",
    "end": "108000"
  },
  {
    "text": "bring in your own containers as well and",
    "start": "108000",
    "end": "109960"
  },
  {
    "text": "then you can specify an IM Ro which",
    "start": "109960",
    "end": "111479"
  },
  {
    "text": "handles the identity uh access and",
    "start": "111479",
    "end": "113759"
  },
  {
    "text": "management at a second step uh you will",
    "start": "113759",
    "end": "116399"
  },
  {
    "text": "Define your endpoint configuration using",
    "start": "116399",
    "end": "118320"
  },
  {
    "text": "Create endpoint config AP",
    "start": "118320",
    "end": "120439"
  },
  {
    "text": "this is where you define the instance",
    "start": "120439",
    "end": "122560"
  },
  {
    "text": "types that you want to use to host your",
    "start": "122560",
    "end": "124520"
  },
  {
    "text": "model and the number of instances that",
    "start": "124520",
    "end": "126119"
  },
  {
    "text": "you want to use uh you can specify an",
    "start": "126119",
    "end": "128360"
  },
  {
    "text": "autoscaling policy uh which will scale",
    "start": "128360",
    "end": "130720"
  },
  {
    "text": "up or scale down your instances as your",
    "start": "130720",
    "end": "132640"
  },
  {
    "text": "traffic pattern will change you can also",
    "start": "132640",
    "end": "134800"
  },
  {
    "text": "specify model serving stack and",
    "start": "134800",
    "end": "136360"
  },
  {
    "text": "deployment strategies uh at this step as",
    "start": "136360",
    "end": "139400"
  },
  {
    "text": "well and finally you will create an",
    "start": "139400",
    "end": "141760"
  },
  {
    "text": "endpoint using Create endpoint API in",
    "start": "141760",
    "end": "144519"
  },
  {
    "text": "this uh specific case we are using real",
    "start": "144519",
    "end": "147120"
  },
  {
    "text": "time endpoint where a client will send a",
    "start": "147120",
    "end": "149440"
  },
  {
    "text": "request to those end points and we'll",
    "start": "149440",
    "end": "151000"
  },
  {
    "text": "wait for the response though there are",
    "start": "151000",
    "end": "153239"
  },
  {
    "text": "multiple other options that we have",
    "start": "153239",
    "end": "154920"
  },
  {
    "text": "talked about in the previous slide uh",
    "start": "154920",
    "end": "156360"
  },
  {
    "text": "like asyn inference seress inference and",
    "start": "156360",
    "end": "158200"
  },
  {
    "text": "others uh which you can also choose",
    "start": "158200",
    "end": "160120"
  },
  {
    "text": "we're going to Deep dive on those",
    "start": "160120",
    "end": "161440"
  },
  {
    "text": "inference options in the next video in",
    "start": "161440",
    "end": "163879"
  },
  {
    "text": "this in the series but for now let us",
    "start": "163879",
    "end": "166480"
  },
  {
    "text": "quickly look at a demo where we will see",
    "start": "166480",
    "end": "168519"
  },
  {
    "text": "how you can create the real-time",
    "start": "168519",
    "end": "169840"
  },
  {
    "text": "endpoints and deploy L models on",
    "start": "169840",
    "end": "173360"
  },
  {
    "text": "stagemaker so we are switching on to our",
    "start": "173360",
    "end": "176480"
  },
  {
    "text": "Jupiter notebook here so in this",
    "start": "176480",
    "end": "178599"
  },
  {
    "text": "notebook we will deploy llama 2 7",
    "start": "178599",
    "end": "180640"
  },
  {
    "text": "billion uh model on stagemaker endpoint",
    "start": "180640",
    "end": "183680"
  },
  {
    "text": "now llama 2 is uh in the family of llama",
    "start": "183680",
    "end": "186400"
  },
  {
    "text": "large language model which is created or",
    "start": "186400",
    "end": "188159"
  },
  {
    "text": "released by meta team and we'll use a",
    "start": "188159",
    "end": "191680"
  },
  {
    "text": "large model inference container to",
    "start": "191680",
    "end": "193760"
  },
  {
    "text": "deploy this model now large model",
    "start": "193760",
    "end": "195959"
  },
  {
    "text": "inference also called LMI is the um",
    "start": "195959",
    "end": "199159"
  },
  {
    "text": "Advanced model serving technology uh",
    "start": "199159",
    "end": "201840"
  },
  {
    "text": "which sageer offers through a",
    "start": "201840",
    "end": "203159"
  },
  {
    "text": "specialized container uh this is powered",
    "start": "203159",
    "end": "205080"
  },
  {
    "text": "by djl serving and offers lot of",
    "start": "205080",
    "end": "206879"
  },
  {
    "text": "different um Advanced optimizations like",
    "start": "206879",
    "end": "208959"
  },
  {
    "text": "quantization parallelism continuous",
    "start": "208959",
    "end": "211560"
  },
  {
    "text": "batching and others now this also gives",
    "start": "211560",
    "end": "214159"
  },
  {
    "text": "you an option to work with a high",
    "start": "214159",
    "end": "215799"
  },
  {
    "text": "performant uh model serving libraries",
    "start": "215799",
    "end": "218360"
  },
  {
    "text": "like VM tensor RTM uh deep speed and",
    "start": "218360",
    "end": "223159"
  },
  {
    "text": "others and again uh you have options to",
    "start": "223159",
    "end": "225840"
  },
  {
    "text": "pick up uh from other model serving",
    "start": "225840",
    "end": "228040"
  },
  {
    "text": "Stacks like to serve and video Tron",
    "start": "228040",
    "end": "230599"
  },
  {
    "text": "tensorflow serving",
    "start": "230599",
    "end": "232400"
  },
  {
    "text": "too so um to start with the code we will",
    "start": "232400",
    "end": "235560"
  },
  {
    "text": "uh import the libraries uh and",
    "start": "235560",
    "end": "237200"
  },
  {
    "text": "dependencies that we need to uh run our",
    "start": "237200",
    "end": "239400"
  },
  {
    "text": "code",
    "start": "239400",
    "end": "240760"
  },
  {
    "text": "we will use sagemaker Python SDK and",
    "start": "240760",
    "end": "243120"
  },
  {
    "text": "sagemaker session to interact with",
    "start": "243120",
    "end": "244640"
  },
  {
    "text": "sagemaker apis and we'll create",
    "start": "244640",
    "end": "247760"
  },
  {
    "text": "variables to point to um the B3 clients",
    "start": "247760",
    "end": "250879"
  },
  {
    "text": "for S3 sagemaker",
    "start": "250879",
    "end": "254879"
  },
  {
    "text": "Services as the next step uh we will uh",
    "start": "255040",
    "end": "257639"
  },
  {
    "text": "pull in the uh deep learning container",
    "start": "257639",
    "end": "260000"
  },
  {
    "text": "um in this case we are using uh djl",
    "start": "260000",
    "end": "262479"
  },
  {
    "text": "serving container powered by Deep speed",
    "start": "262479",
    "end": "264680"
  },
  {
    "text": "and this is a specific version that you",
    "start": "264680",
    "end": "266440"
  },
  {
    "text": "would uh use uh for the container image",
    "start": "266440",
    "end": "270280"
  },
  {
    "text": "and then you will Define the environment",
    "start": "270280",
    "end": "271759"
  },
  {
    "text": "variables which will uh which will",
    "start": "271759",
    "end": "273960"
  },
  {
    "text": "Define the behavior of this container if",
    "start": "273960",
    "end": "275680"
  },
  {
    "text": "you see here we are mentioning that our",
    "start": "275680",
    "end": "277960"
  },
  {
    "text": "model is in hugging phase and this is",
    "start": "277960",
    "end": "279919"
  },
  {
    "text": "the model ID uh which we want to",
    "start": "279919",
    "end": "282199"
  },
  {
    "text": "download um from hugging phase we have",
    "start": "282199",
    "end": "285440"
  },
  {
    "text": "specified tensor parallel degree here",
    "start": "285440",
    "end": "287199"
  },
  {
    "text": "which is Max which means uh we want to",
    "start": "287199",
    "end": "289639"
  },
  {
    "text": "use deep speed um and partition or model",
    "start": "289639",
    "end": "292520"
  },
  {
    "text": "across uh maximum number of gpus that a",
    "start": "292520",
    "end": "295120"
  },
  {
    "text": "particular instance will have and when",
    "start": "295120",
    "end": "297680"
  },
  {
    "text": "it comes to Rolling badge uh this is the",
    "start": "297680",
    "end": "299840"
  },
  {
    "text": "batching technique which could be",
    "start": "299840",
    "end": "301840"
  },
  {
    "text": "continuous or iteration level batching",
    "start": "301840",
    "end": "304520"
  },
  {
    "text": "uh in this case we are using VM which is",
    "start": "304520",
    "end": "307039"
  },
  {
    "text": "a high perform and open source model",
    "start": "307039",
    "end": "308800"
  },
  {
    "text": "serving framework um especially in the",
    "start": "308800",
    "end": "311520"
  },
  {
    "text": "cases where we have the open-ended um",
    "start": "311520",
    "end": "315680"
  },
  {
    "text": "text generation or chatboard like",
    "start": "315680",
    "end": "317440"
  },
  {
    "text": "application VM has proven to provide a",
    "start": "317440",
    "end": "319560"
  },
  {
    "text": "very high throughput so we're using VM",
    "start": "319560",
    "end": "321720"
  },
  {
    "text": "here to kind of um do continuous uh",
    "start": "321720",
    "end": "325600"
  },
  {
    "text": "batching uh where it will merge concr",
    "start": "325600",
    "end": "327960"
  },
  {
    "text": "Quest coming in for in",
    "start": "327960",
    "end": "330479"
  },
  {
    "text": "and we have also specified a batch size",
    "start": "330479",
    "end": "332400"
  },
  {
    "text": "of 32 which means that U 32 concurent",
    "start": "332400",
    "end": "336000"
  },
  {
    "text": "requests could be batched together any",
    "start": "336000",
    "end": "338160"
  },
  {
    "text": "request which comes beyond that will be",
    "start": "338160",
    "end": "339919"
  },
  {
    "text": "queed",
    "start": "339919",
    "end": "341280"
  },
  {
    "text": "up and there are other environment",
    "start": "341280",
    "end": "343520"
  },
  {
    "text": "variables as well which you can um",
    "start": "343520",
    "end": "345440"
  },
  {
    "text": "mention and refer through our LMI",
    "start": "345440",
    "end": "349840"
  },
  {
    "text": "documentation okay now the three steps",
    "start": "349919",
    "end": "352039"
  },
  {
    "text": "that we talked about uh we will uh",
    "start": "352039",
    "end": "354039"
  },
  {
    "text": "implementing those in our code uh Step",
    "start": "354039",
    "end": "356479"
  },
  {
    "text": "One is we will create a model using",
    "start": "356479",
    "end": "358240"
  },
  {
    "text": "Create model API uh this this is where",
    "start": "358240",
    "end": "360039"
  },
  {
    "text": "we Define a model name uh and you pass",
    "start": "360039",
    "end": "362759"
  },
  {
    "text": "in your inference image that we have",
    "start": "362759",
    "end": "365000"
  },
  {
    "text": "just um initialized uh in the previous",
    "start": "365000",
    "end": "368080"
  },
  {
    "text": "cell and the environment variable which",
    "start": "368080",
    "end": "370400"
  },
  {
    "text": "will have uh the hugging face model ID",
    "start": "370400",
    "end": "373000"
  },
  {
    "text": "from where the model needs to be",
    "start": "373000",
    "end": "374319"
  },
  {
    "text": "downloaded remember we our model could",
    "start": "374319",
    "end": "376599"
  },
  {
    "text": "be in S3 location as well if it would",
    "start": "376599",
    "end": "378360"
  },
  {
    "text": "have been in S3 we could have used uh S5",
    "start": "378360",
    "end": "380800"
  },
  {
    "text": "CMD optimization that LMI uh container",
    "start": "380800",
    "end": "383479"
  },
  {
    "text": "offers uh which is very optimized we to",
    "start": "383479",
    "end": "385639"
  },
  {
    "text": "download a model from S3 over to the",
    "start": "385639",
    "end": "388479"
  },
  {
    "text": "stagemaker um point and then um you know",
    "start": "388479",
    "end": "391960"
  },
  {
    "text": "the model is created as a second step",
    "start": "391960",
    "end": "394479"
  },
  {
    "text": "we'll create the endpoint config using",
    "start": "394479",
    "end": "396280"
  },
  {
    "text": "the create endpoint config API now in",
    "start": "396280",
    "end": "399039"
  },
  {
    "text": "this case uh we are specifying the",
    "start": "399039",
    "end": "401560"
  },
  {
    "text": "instance count as count as one and we're",
    "start": "401560",
    "end": "403800"
  },
  {
    "text": "using G5 2x large instance now G5 is a",
    "start": "403800",
    "end": "408000"
  },
  {
    "text": "a1g uh tens GPU and it has a one GPU um",
    "start": "408000",
    "end": "413160"
  },
  {
    "text": "uh in that instance type right and we",
    "start": "413160",
    "end": "415759"
  },
  {
    "text": "have defined routing strategy at this",
    "start": "415759",
    "end": "417400"
  },
  {
    "text": "step as well uh which is the least",
    "start": "417400",
    "end": "418800"
  },
  {
    "text": "outsanding request this means that once",
    "start": "418800",
    "end": "421199"
  },
  {
    "text": "the model is deployed uh and if you have",
    "start": "421199",
    "end": "423440"
  },
  {
    "text": "multiple instances behind an endpoint uh",
    "start": "423440",
    "end": "425879"
  },
  {
    "text": "the routing strategy it will route the",
    "start": "425879",
    "end": "427720"
  },
  {
    "text": "request to the instance which have the",
    "start": "427720",
    "end": "429280"
  },
  {
    "text": "least number of outstanding requests and",
    "start": "429280",
    "end": "431960"
  },
  {
    "text": "along with this you can define a model",
    "start": "431960",
    "end": "433599"
  },
  {
    "text": "serving stack and uh other deployment",
    "start": "433599",
    "end": "435800"
  },
  {
    "text": "strategies like Shadow deployment or AB",
    "start": "435800",
    "end": "437759"
  },
  {
    "text": "testing at this particular",
    "start": "437759",
    "end": "440240"
  },
  {
    "text": "step now as a third step uh is where we",
    "start": "440240",
    "end": "442879"
  },
  {
    "text": "will actually create our endpoint right",
    "start": "442879",
    "end": "444960"
  },
  {
    "text": "so here we will pass on the endpoint",
    "start": "444960",
    "end": "446639"
  },
  {
    "text": "configuration that we have uh created in",
    "start": "446639",
    "end": "448840"
  },
  {
    "text": "the last step and then give it endpoint",
    "start": "448840",
    "end": "450599"
  },
  {
    "text": "name and then we'll uh we'll create our",
    "start": "450599",
    "end": "453360"
  },
  {
    "text": "endpoint using this API so it",
    "start": "453360",
    "end": "455759"
  },
  {
    "text": "approximately take around 15 minutes uh",
    "start": "455759",
    "end": "457599"
  },
  {
    "text": "or longer so we have already created",
    "start": "457599",
    "end": "459280"
  },
  {
    "text": "that but uh when you will replicate this",
    "start": "459280",
    "end": "461759"
  },
  {
    "text": "example in your machines uh you can use",
    "start": "461759",
    "end": "464000"
  },
  {
    "text": "this code to track the progress of that",
    "start": "464000",
    "end": "466360"
  },
  {
    "text": "model endpoint right and once the",
    "start": "466360",
    "end": "468520"
  },
  {
    "text": "endpoint is up and running uh you will",
    "start": "468520",
    "end": "470440"
  },
  {
    "text": "see the status as in",
    "start": "470440",
    "end": "472680"
  },
  {
    "text": "service now there are three options uh",
    "start": "472680",
    "end": "475039"
  },
  {
    "text": "to invoke that endpoint the first option",
    "start": "475039",
    "end": "476800"
  },
  {
    "text": "is to use stagemaker Python SDK now this",
    "start": "476800",
    "end": "479639"
  },
  {
    "text": "is the high level abstraction API and",
    "start": "479639",
    "end": "481599"
  },
  {
    "text": "data scientists love to work with it and",
    "start": "481599",
    "end": "483840"
  },
  {
    "text": "the way to use sagemaker Python SDK is",
    "start": "483840",
    "end": "485759"
  },
  {
    "text": "to use sagemaker do predictor API and",
    "start": "485759",
    "end": "488240"
  },
  {
    "text": "then initialize that with the endpoint",
    "start": "488240",
    "end": "490159"
  },
  {
    "text": "name that we have used to create our",
    "start": "490159",
    "end": "492039"
  },
  {
    "text": "endpoint and pass in a sagemaker session",
    "start": "492039",
    "end": "494720"
  },
  {
    "text": "and the Json serializer and then once",
    "start": "494720",
    "end": "497360"
  },
  {
    "text": "you have the predictor um variable you",
    "start": "497360",
    "end": "499599"
  },
  {
    "text": "can use that to predict uh your an",
    "start": "499599",
    "end": "503240"
  },
  {
    "text": "invoke your endpoint uh with the input",
    "start": "503240",
    "end": "505360"
  },
  {
    "text": "prompt that we have specified here as",
    "start": "505360",
    "end": "507840"
  },
  {
    "text": "what does Amazon offers and the",
    "start": "507840",
    "end": "509800"
  },
  {
    "text": "parameters where you can Define uh the",
    "start": "509800",
    "end": "512360"
  },
  {
    "text": "number of new tokens that you want to",
    "start": "512360",
    "end": "514080"
  },
  {
    "text": "generate and whether you want to uh use",
    "start": "514080",
    "end": "516680"
  },
  {
    "text": "any kind of sampling so in this case we",
    "start": "516680",
    "end": "518240"
  },
  {
    "text": "will be generating the new tokens um",
    "start": "518240",
    "end": "520240"
  },
  {
    "text": "without any sampling right so if I run",
    "start": "520240",
    "end": "522760"
  },
  {
    "text": "this uh cell right",
    "start": "522760",
    "end": "525120"
  },
  {
    "text": "now uh this will um give me a result uh",
    "start": "525120",
    "end": "529279"
  },
  {
    "text": "which will specify what Amazon com",
    "start": "529279",
    "end": "532480"
  },
  {
    "text": "offers the second week to invoke uh this",
    "start": "532480",
    "end": "535000"
  },
  {
    "text": "endpoint is to use PTO 3 now we have",
    "start": "535000",
    "end": "537440"
  },
  {
    "text": "seen mlops Engineers uh use uh this apis",
    "start": "537440",
    "end": "540560"
  },
  {
    "text": "extensively U now this API offers a",
    "start": "540560",
    "end": "542800"
  },
  {
    "text": "lowlevel constructs and gives you more",
    "start": "542800",
    "end": "544720"
  },
  {
    "text": "flexibility in providing the input",
    "start": "544720",
    "end": "546720"
  },
  {
    "text": "variables and to uh work with the",
    "start": "546720",
    "end": "549200"
  },
  {
    "text": "response that you get from the endpoint",
    "start": "549200",
    "end": "551560"
  },
  {
    "text": "right here you get a adjacent object",
    "start": "551560",
    "end": "553800"
  },
  {
    "text": "with the same input prompt and the",
    "start": "553800",
    "end": "555440"
  },
  {
    "text": "parameters and then you invoke the",
    "start": "555440",
    "end": "557920"
  },
  {
    "text": "endpoint using sagemaker B3 client and",
    "start": "557920",
    "end": "560800"
  },
  {
    "text": "once you get a response then you can",
    "start": "560800",
    "end": "562240"
  },
  {
    "text": "decode that response and uh can get the",
    "start": "562240",
    "end": "566240"
  },
  {
    "text": "output right now the Third Way is",
    "start": "566560",
    "end": "569880"
  },
  {
    "text": "how the application developers like to",
    "start": "569880",
    "end": "571800"
  },
  {
    "text": "invoke the sagemaker endpoint and that",
    "start": "571800",
    "end": "573360"
  },
  {
    "text": "is through Postman so I'll bring up this",
    "start": "573360",
    "end": "576519"
  },
  {
    "text": "interface right so you define the uh",
    "start": "576519",
    "end": "579519"
  },
  {
    "text": "sagemaker endpoint URL you can get this",
    "start": "579519",
    "end": "581320"
  },
  {
    "text": "endpoint URL either from sagemaker",
    "start": "581320",
    "end": "583079"
  },
  {
    "text": "Studio or from sagemaker console and you",
    "start": "583079",
    "end": "587040"
  },
  {
    "text": "specify in the body the same Json object",
    "start": "587040",
    "end": "589320"
  },
  {
    "text": "where you pass in your input prompt and",
    "start": "589320",
    "end": "591760"
  },
  {
    "text": "you pass in your parameter on how many",
    "start": "591760",
    "end": "593720"
  },
  {
    "text": "tokens you want to generate and whether",
    "start": "593720",
    "end": "594959"
  },
  {
    "text": "you want to use sampling and other",
    "start": "594959",
    "end": "596279"
  },
  {
    "text": "parameters that Lama to model or",
    "start": "596279",
    "end": "598880"
  },
  {
    "text": "particular model serving stack offers",
    "start": "598880",
    "end": "601079"
  },
  {
    "text": "and then in the headers you just Define",
    "start": "601079",
    "end": "602680"
  },
  {
    "text": "the content type uh that um you are",
    "start": "602680",
    "end": "605440"
  },
  {
    "text": "using and in the authorization tab this",
    "start": "605440",
    "end": "607640"
  },
  {
    "text": "is where you kind of pass in your user",
    "start": "607640",
    "end": "610680"
  },
  {
    "text": "security ID and access key so that you",
    "start": "610680",
    "end": "612519"
  },
  {
    "text": "can authenticate this request uh with",
    "start": "612519",
    "end": "614560"
  },
  {
    "text": "the stagemaker I am and once you send",
    "start": "614560",
    "end": "617519"
  },
  {
    "text": "this request this this",
    "start": "617519",
    "end": "620079"
  },
  {
    "text": "endpoint this is where you get the",
    "start": "620079",
    "end": "624240"
  },
  {
    "text": "response so key takeaways here there are",
    "start": "626160",
    "end": "628920"
  },
  {
    "text": "three simple apis that you could use uh",
    "start": "628920",
    "end": "631040"
  },
  {
    "text": "to deploy your models to stagemaker",
    "start": "631040",
    "end": "632640"
  },
  {
    "text": "endpoint uh which is create model which",
    "start": "632640",
    "end": "634480"
  },
  {
    "text": "will point to the location where your",
    "start": "634480",
    "end": "636519"
  },
  {
    "text": "model is located uh create endpoint",
    "start": "636519",
    "end": "638639"
  },
  {
    "text": "config which will uh help you uh",
    "start": "638639",
    "end": "641639"
  },
  {
    "text": "configure uh the kind of infrastructure",
    "start": "641639",
    "end": "643680"
  },
  {
    "text": "that you want to deploy a model on the",
    "start": "643680",
    "end": "645079"
  },
  {
    "text": "deployment strategy the kind of model",
    "start": "645079",
    "end": "646519"
  },
  {
    "text": "serving stack you want to use routing",
    "start": "646519",
    "end": "648519"
  },
  {
    "text": "strategy and others and finally you once",
    "start": "648519",
    "end": "650440"
  },
  {
    "text": "you have created that endpoint config",
    "start": "650440",
    "end": "652519"
  },
  {
    "text": "you will use the create endpoint API to",
    "start": "652519",
    "end": "654720"
  },
  {
    "text": "create and bring your endpoint",
    "start": "654720",
    "end": "657480"
  },
  {
    "text": "up so this concludes demo you can refer",
    "start": "657480",
    "end": "660240"
  },
  {
    "text": "to these links to dive deeper into the",
    "start": "660240",
    "end": "661880"
  },
  {
    "text": "example that we have just covered and to",
    "start": "661880",
    "end": "664160"
  },
  {
    "text": "look into the sagemaker uh model",
    "start": "664160",
    "end": "666000"
  },
  {
    "text": "deployment documentation uh I hope you",
    "start": "666000",
    "end": "668480"
  },
  {
    "text": "have found this session helpful uh we'll",
    "start": "668480",
    "end": "670040"
  },
  {
    "text": "see you in the next video of this model",
    "start": "670040",
    "end": "672839"
  },
  {
    "text": "deployment series thank you",
    "start": "672839",
    "end": "677079"
  }
]