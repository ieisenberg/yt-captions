[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "okay thanks for joining us everyone we're going to talk about best practices",
    "start": "359",
    "end": "7319"
  },
  {
    "text": "for building a data Lake on AWS and glacier today and we've got a few guest",
    "start": "7319",
    "end": "13410"
  },
  {
    "text": "stars from viber and Airbnb who are going to share their stories so let's",
    "start": "13410",
    "end": "19289"
  },
  {
    "start": "18000",
    "end": "49000"
  },
  {
    "text": "get started what are we going to cover today essentially we're gonna talk about what a data Lake is on s3 and glacier",
    "start": "19289",
    "end": "26340"
  },
  {
    "text": "and some of the common characteristics and why we choose these architectures we're gonna focus a bit on data",
    "start": "26340",
    "end": "33180"
  },
  {
    "text": "cataloging since that's an essential part security performance and analytics are absolutely the core of doing this",
    "start": "33180",
    "end": "39960"
  },
  {
    "text": "successfully and then finally you're gonna hear from viber and air B&B about what they've done and you know they're",
    "start": "39960",
    "end": "47399"
  },
  {
    "text": "gonna share their best practices so what is a data like let's start off by defining it because like a lot of other",
    "start": "47399",
    "end": "54899"
  },
  {
    "start": "49000",
    "end": "168000"
  },
  {
    "text": "ubiquitous over use terms like cloud or big data data likes one of those things",
    "start": "54899",
    "end": "61079"
  },
  {
    "text": "that people have been talking about for several years now but you know it could mean anything from a data warehouse to",
    "start": "61079",
    "end": "67820"
  },
  {
    "text": "just plain dumb storage so when we talk about building a data Lake on AWS what",
    "start": "67820",
    "end": "74310"
  },
  {
    "text": "are we really talking about essentially it's a platform to consolidate all of",
    "start": "74310",
    "end": "80580"
  },
  {
    "text": "your data assets in a single location where you can then start to characterize them transform them analyze them and",
    "start": "80580",
    "end": "88530"
  },
  {
    "text": "ultimately share and distribute the results and so some of the common defining attributes that we ascribe to a",
    "start": "88530",
    "end": "96060"
  },
  {
    "text": "data Lake are you want to be able to decouple storage and compute for a",
    "start": "96060",
    "end": "102060"
  },
  {
    "text": "couple of reasons number one for cost optimization so you're only paying for",
    "start": "102060",
    "end": "107850"
  },
  {
    "text": "what you use of each and also for elasticity so that you can grow and shrink on demand as meet your business",
    "start": "107850",
    "end": "115350"
  },
  {
    "text": "needs rapid ingestion transformation is a key attribute particularly when we",
    "start": "115350",
    "end": "120780"
  },
  {
    "text": "start to think about more and more use cases going streaming and closer and closer to real-time secure multi-tenancy",
    "start": "120780",
    "end": "129149"
  },
  {
    "text": "is at the core of it I mean ultimately one of the problems with tradition analytic architectures was fragmentation",
    "start": "129149",
    "end": "136020"
  },
  {
    "text": "of many different silos so you really want to be able to consolidate those silos into a single platform get the",
    "start": "136020",
    "end": "143130"
  },
  {
    "text": "data in one place bring all the users and lines of business and consumers to",
    "start": "143130",
    "end": "148500"
  },
  {
    "text": "the data where it lives and process it in place and query it in place and then ultimately you want to innovate rapidly",
    "start": "148500",
    "end": "155730"
  },
  {
    "text": "and try new ideas and fail fast so it's got to have scheme on read' so obviously",
    "start": "155730",
    "end": "161760"
  },
  {
    "text": "a data Lake can be a lot more than this but these are kind of the core defining attributes that we're gonna talk about",
    "start": "161760",
    "end": "166770"
  },
  {
    "text": "today so what can you do with the data like many many use cases once you get",
    "start": "166770",
    "end": "174450"
  },
  {
    "start": "168000",
    "end": "212000"
  },
  {
    "text": "the data on a centralized platform like s3 you know the first is obviously the",
    "start": "174450",
    "end": "179550"
  },
  {
    "text": "bread and butter of you know where analytics came out of batch processing so if you want to just do ad hoc data",
    "start": "179550",
    "end": "186660"
  },
  {
    "text": "exploration visualization you can put the data in s3 and use our managed",
    "start": "186660",
    "end": "193500"
  },
  {
    "text": "services or even our service some services like redshift spectrum Athena",
    "start": "193500",
    "end": "198900"
  },
  {
    "text": "EMR and a whole host of BI tools to you know just start to do ad hoc exploration",
    "start": "198900",
    "end": "205350"
  },
  {
    "text": "a data batch processing and large scale ad hoc reporting or even structured",
    "start": "205350",
    "end": "211620"
  },
  {
    "text": "reporting but ultimately you know where the meat of it is today is really around",
    "start": "211620",
    "end": "217050"
  },
  {
    "start": "212000",
    "end": "250000"
  },
  {
    "text": "streaming and real-time analytics and so you can ingest data in process it as",
    "start": "217050",
    "end": "223320"
  },
  {
    "text": "you're ingesting it in stored so you have historical records and can do you know a lot of batch background",
    "start": "223320",
    "end": "229140"
  },
  {
    "text": "processing but there's a whole host of services you can bring to the data to do",
    "start": "229140",
    "end": "234390"
  },
  {
    "text": "streaming and real-time analysis for a whole array of use cases like telematics",
    "start": "234390",
    "end": "240300"
  },
  {
    "text": "has been a big topic of conversation obviously predictive maintenance you",
    "start": "240300",
    "end": "245730"
  },
  {
    "text": "know you name it you can do it with streaming and analytics and then finally",
    "start": "245730",
    "end": "251340"
  },
  {
    "start": "250000",
    "end": "305000"
  },
  {
    "text": "as we've heard a lot about several announcements at reinvent a IIM machine",
    "start": "251340",
    "end": "257130"
  },
  {
    "text": "learning is really big so you know when you get the data on the platform a whole host of services to speed how you can",
    "start": "257130",
    "end": "266889"
  },
  {
    "text": "implement a io machine learning so pick your use case we've got you covered because at the end of the day most of",
    "start": "266889",
    "end": "274719"
  },
  {
    "text": "you aren't going to do one of these use cases it's not going to be the tyranny of or where it's like you get to choose",
    "start": "274719",
    "end": "280389"
  },
  {
    "text": "a or B or C it's how do you do all of the above and that's really the power the data lake is consolidate your data",
    "start": "280389",
    "end": "288039"
  },
  {
    "text": "assets bring a whole array of tools to the data so that no matter what your use",
    "start": "288039",
    "end": "293560"
  },
  {
    "text": "cases no matter how broad and varied your business you've got this central platform to build on and transform",
    "start": "293560",
    "end": "300189"
  },
  {
    "text": "around as your levels of sophistication grow and your use cases evolve so why do",
    "start": "300189",
    "end": "306250"
  },
  {
    "start": "305000",
    "end": "411000"
  },
  {
    "text": "we choose s3 for a data lake let's start with the core here first is if you're gonna put all your data assets on a",
    "start": "306250",
    "end": "312310"
  },
  {
    "text": "single platform it's got to have rock-solid durability you know so that's built into s3 from the get-go designed",
    "start": "312310",
    "end": "319960"
  },
  {
    "text": "for 11/9 of durability virtually unlimited scalability of both",
    "start": "319960",
    "end": "325029"
  },
  {
    "text": "performance and capacity so it gives you that foundation that you're never gonna outgrow and your data is going to be you",
    "start": "325029",
    "end": "332830"
  },
  {
    "text": "know secure and well protected because security compliance an audit is another",
    "start": "332830",
    "end": "337930"
  },
  {
    "text": "core foundation that needs to be there I mean when you look at all of the you know major banks insurance companies you",
    "start": "337930",
    "end": "345370"
  },
  {
    "text": "know people that are healthcare that are in compliant industries you can't compromise on any of these attributes so",
    "start": "345370",
    "end": "351969"
  },
  {
    "text": "you know it's got all those core capabilities built in another is scalability of management because you",
    "start": "351969",
    "end": "360250"
  },
  {
    "text": "may start and you can build the data like people think big when they think data leak but you can start a data Lake with 10 terabytes of data you know scale",
    "start": "360250",
    "end": "367449"
  },
  {
    "text": "to petabytes but what you can't do is scale the manpower and effort required to manage all that data so we've got",
    "start": "367449",
    "end": "374259"
  },
  {
    "text": "object level controls to do that for you and then obviously you know we've got a",
    "start": "374259",
    "end": "380020"
  },
  {
    "text": "huge number of partner integrations because we announced more and more services as you've heard about you know",
    "start": "380020",
    "end": "387219"
  },
  {
    "text": "here at reinvent but there's a whole broad echo system of people you want to use that are well integrated as well",
    "start": "387219",
    "end": "393659"
  },
  {
    "text": "data ingest is key because data is going to be coming from a lot of sources and then ultimately",
    "start": "393659",
    "end": "399790"
  },
  {
    "text": "you're gonna want to give business insights to your data and accelerate those time to insights so we're",
    "start": "399790",
    "end": "405190"
  },
  {
    "text": "constantly innovating with things like s3 select and glacier select that you heard about so another key factor is",
    "start": "405190",
    "end": "413140"
  },
  {
    "text": "obviously optimizing cost and when you build a data Lake on s3 and glacier you",
    "start": "413140",
    "end": "420280"
  },
  {
    "text": "don't have to have all your data on s3 and glacier you can really do intelligent tearing so that you're",
    "start": "420280",
    "end": "426610"
  },
  {
    "text": "fitting your workload and data to where it should best reside for optimal",
    "start": "426610",
    "end": "432970"
  },
  {
    "text": "performance optimal costs long-term retention and so you can really integrate this whole range of native",
    "start": "432970",
    "end": "439840"
  },
  {
    "text": "Hadoop on local storage s3 standard s3",
    "start": "439840",
    "end": "445060"
  },
  {
    "text": "and frequent access for colder data and ultimately glacier for archive data and",
    "start": "445060",
    "end": "450310"
  },
  {
    "text": "tie it all together with storage analytics to help you understand how to do this and ultimately write policies to",
    "start": "450310",
    "end": "457360"
  },
  {
    "text": "do it automatically to once again you know scale management another key factor",
    "start": "457360",
    "end": "465190"
  },
  {
    "start": "463000",
    "end": "523000"
  },
  {
    "text": "obviously is very rich data sources are going to come into the data lake so how",
    "start": "465190",
    "end": "470200"
  },
  {
    "text": "do you integrate them all natively and easily and we've got a whole host of",
    "start": "470200",
    "end": "475450"
  },
  {
    "text": "ingestion methods here whether it's integrating legacy file apps like mainstream mainframe computers you know",
    "start": "475450",
    "end": "483670"
  },
  {
    "text": "we've got file gateways to help you do that if you want to lift and shift from an existing data Lake we have things",
    "start": "483670",
    "end": "489370"
  },
  {
    "text": "like snowball and snowmobile to do bulk lifting firehose as the foundation for",
    "start": "489370",
    "end": "495670"
  },
  {
    "text": "streaming data and obviously a whole host of native and ISV connectors so that your applications and you know",
    "start": "495670",
    "end": "504010"
  },
  {
    "text": "consumers and brokers of data can write directly to s3 and then if you're",
    "start": "504010",
    "end": "510220"
  },
  {
    "text": "integrating long-haul data we've got things like transfer acceleration or if you're building hybrid environments",
    "start": "510220",
    "end": "515800"
  },
  {
    "text": "Direct Connect so once again it's not one method it's many methods that you're",
    "start": "515800",
    "end": "520870"
  },
  {
    "text": "gonna use and integrate once you get the data in obviously cataloging it is key",
    "start": "520870",
    "end": "526180"
  },
  {
    "start": "523000",
    "end": "575000"
  },
  {
    "text": "because you've got to know what you have and there's a couple different ways to catalog if you're going to use your data",
    "start": "526180",
    "end": "531190"
  },
  {
    "text": "like for more than analytics you're probably going to want to build a master catalog and we've seen",
    "start": "531190",
    "end": "536889"
  },
  {
    "text": "a number of customers do this using lambda and triggers on you know s3",
    "start": "536889",
    "end": "543600"
  },
  {
    "text": "operations to populate a dynamodb meta",
    "start": "543600",
    "end": "548889"
  },
  {
    "text": "story index and if you look at you know EMR FS or where Apache Hadoop is going",
    "start": "548889",
    "end": "556149"
  },
  {
    "text": "this is how they you know build a master catalog and address some of the eventual",
    "start": "556149",
    "end": "561939"
  },
  {
    "text": "consistency design of s3 but you can also populate an elastic catalog so that",
    "start": "561939",
    "end": "570309"
  },
  {
    "text": "you can start to do very intelligent searches for data and data attributes but ultimately you're going to want to",
    "start": "570309",
    "end": "577209"
  },
  {
    "start": "575000",
    "end": "633000"
  },
  {
    "text": "have an analytic specific catalog that all of your common apps like Athena redshift redshift spectrum EMR you know",
    "start": "577209",
    "end": "585819"
  },
  {
    "text": "Apache Hadoop can all integrate into and once again it's about automating this",
    "start": "585819",
    "end": "591339"
  },
  {
    "text": "process and so we have glue which essentially is a you know integrated",
    "start": "591339",
    "end": "596920"
  },
  {
    "text": "service that runs in s3 service that as you ingest data it will both catalog it",
    "start": "596920",
    "end": "603429"
  },
  {
    "text": "and then ultimately down the road transform it in ETL the data and so that gives you a centralized glue data",
    "start": "603429",
    "end": "610449"
  },
  {
    "text": "catalog that's integrated with all common analytics apps that can use a",
    "start": "610449",
    "end": "616089"
  },
  {
    "text": "hive meta store and so you know it essentially is hive meta store compatible and if you've got an existing",
    "start": "616089",
    "end": "622209"
  },
  {
    "text": "hive catalog you can migrate it into glue we've got tools to help you do that but we've added some extensions to make",
    "start": "622209",
    "end": "628569"
  },
  {
    "text": "it easier to do data discovery data lineage and classification and the",
    "start": "628569",
    "end": "634059"
  },
  {
    "start": "633000",
    "end": "664000"
  },
  {
    "text": "really cool and interesting thing about glue is we've got crawlers so when you",
    "start": "634059",
    "end": "639339"
  },
  {
    "text": "ingest new data in you can either schedule or use things like lambda",
    "start": "639339",
    "end": "644709"
  },
  {
    "text": "triggers to fire off these crawlers and they'll go and automatically discover data and not just the raw data assets",
    "start": "644709",
    "end": "651429"
  },
  {
    "text": "would even schema the data and populate this catalog automatically so it's all",
    "start": "651429",
    "end": "657220"
  },
  {
    "text": "about reducing the heavy lifting of doing all these manual processes automating it and then scaling it so",
    "start": "657220",
    "end": "664720"
  },
  {
    "start": "664000",
    "end": "752000"
  },
  {
    "text": "with I'm gonna turn over to my colleague PD and he's gonna go deeper into some of the other attributes so now that you've",
    "start": "664720",
    "end": "673060"
  },
  {
    "text": "decided that Amazon s trendy sure is foundational for your data Lake it is imperative that you focus on data",
    "start": "673060",
    "end": "678910"
  },
  {
    "text": "security optimizing performance and optimizing analytics so let's start with",
    "start": "678910",
    "end": "684250"
  },
  {
    "text": "security there are many ways that you can secure your data in your data leaked right from Identity and Access",
    "start": "684250",
    "end": "690400"
  },
  {
    "text": "Management to encryption to adhering to any compliance or regulatory compliance",
    "start": "690400",
    "end": "695520"
  },
  {
    "text": "so for example Amazon may see you automatically classifies and identifies",
    "start": "695520",
    "end": "701110"
  },
  {
    "text": "within an object if you have business sensitive data such as personal identification numbers and then gives",
    "start": "701110",
    "end": "707050"
  },
  {
    "text": "you the visibility so that you can make changes to it the combination of bucket",
    "start": "707050",
    "end": "712270"
  },
  {
    "text": "policies object Ackles and bucket ackles gives you a very fine-grain control on your buckets as",
    "start": "712270",
    "end": "719080"
  },
  {
    "text": "well as the objects within them and so that you get the desired outcome from it we came up with a set of managed config",
    "start": "719080",
    "end": "726550"
  },
  {
    "text": "rules to secure your s3 buckets now using that same underlying technology now if you go to the s3 console you can",
    "start": "726550",
    "end": "733450"
  },
  {
    "text": "identify that which of your buckets are have public access then go and make those changes confidently",
    "start": "733450",
    "end": "739830"
  },
  {
    "text": "similarly we made some changes where with a bucket policy and changing the bucket configuration you can mandate",
    "start": "739830",
    "end": "746890"
  },
  {
    "text": "that all the objects going into your data leak is encrypted by default so our",
    "start": "746890",
    "end": "752440"
  },
  {
    "start": "752000",
    "end": "781000"
  },
  {
    "text": "customers use a server-side encryption and AWS employs strong multi-factor",
    "start": "752440",
    "end": "757590"
  },
  {
    "text": "encryption which is called envelope encryption so you can manage access controls by monitoring who has access to",
    "start": "757590",
    "end": "764800"
  },
  {
    "text": "your encryption keys and hence your data so for example you can use iam to create",
    "start": "764800",
    "end": "770200"
  },
  {
    "text": "users with temporary credentials so given user for a certain period of time can access the encryption keys and hence",
    "start": "770200",
    "end": "776440"
  },
  {
    "text": "your data beyond that time they won't have access to that data so some of our",
    "start": "776440",
    "end": "782230"
  },
  {
    "start": "781000",
    "end": "848000"
  },
  {
    "text": "customers use server-side encryption either SSE s3 or SSE kms if you're using",
    "start": "782230",
    "end": "787930"
  },
  {
    "text": "SS es3 every individual object is encrypted by a unique data key and that",
    "start": "787930",
    "end": "794470"
  },
  {
    "text": "data key additionally is encrypted by a master key that we automatically rotate with s3 now it's important to note that this",
    "start": "794470",
    "end": "801740"
  },
  {
    "text": "is data encryption at rest so an authorized request from an anonymous user with through an SDK or CLI if they",
    "start": "801740",
    "end": "809540"
  },
  {
    "text": "do a get request once it leaves the s3 subsystem you'll be able to read the data if you want additional safeguards",
    "start": "809540",
    "end": "816320"
  },
  {
    "text": "and controls a lot of our customers use AWS kms or key management service in",
    "start": "816320",
    "end": "821480"
  },
  {
    "text": "that case the master keys are kept and they don't leave the kms service the key",
    "start": "821480",
    "end": "827060"
  },
  {
    "text": "data keys are generated and sent to s3 over a secure connection and they individually encrypt the objects there",
    "start": "827060",
    "end": "833800"
  },
  {
    "text": "the master key has to have X extra extra key usage policies so you need I am",
    "start": "833800",
    "end": "839779"
  },
  {
    "text": "policies and key usage policies to be validated before you have access to the encryption keys and hence your data so",
    "start": "839779",
    "end": "848089"
  },
  {
    "start": "848000",
    "end": "933000"
  },
  {
    "text": "security entitlements are integral part of your data Lake so here are the things that we recommend by default an s3 usage",
    "start": "848089",
    "end": "857329"
  },
  {
    "text": "or resource is private so the bucket owner or the resource owner owns the access to it but here's what you should",
    "start": "857329",
    "end": "864170"
  },
  {
    "text": "keep in mind always follow the I am best practices such as creating users and",
    "start": "864170",
    "end": "869180"
  },
  {
    "text": "creating permission groups and assigning permissions and not using the root credentials if you're using AWS SDK or",
    "start": "869180",
    "end": "875990"
  },
  {
    "text": "CLI by default those HTTP requests are sent over a secure connection but then",
    "start": "875990",
    "end": "881630"
  },
  {
    "text": "if you're using something custom we recommend that you sign it with zigge before and then send it through an ssl",
    "start": "881630",
    "end": "886790"
  },
  {
    "text": "connection secondly use server-side encryption we are doing the undifferentiated heavy lifting for you",
    "start": "886790",
    "end": "893079"
  },
  {
    "text": "so you can use sse KMS or s sec or sse s3 and then you can mandate that every",
    "start": "893079",
    "end": "900740"
  },
  {
    "text": "object that goes into that bucket is default I encrypted by default and finally if you want to protect your data",
    "start": "900740",
    "end": "908240"
  },
  {
    "text": "for from accidental overrides or application failures use versioning if",
    "start": "908240",
    "end": "913850"
  },
  {
    "text": "you use versioning we keep on updating the version a new version as you write the object with the same key name if by",
    "start": "913850",
    "end": "922060"
  },
  {
    "text": "accident it gets deleted we just add a delete marker to it and then you can get the object but it does not get deleted",
    "start": "922060",
    "end": "928279"
  },
  {
    "text": "and can be restored from previous versions so let's shift gears and talk about how",
    "start": "928279",
    "end": "934819"
  },
  {
    "start": "933000",
    "end": "972000"
  },
  {
    "text": "can you optimize performance s3 by default scales to thousands of requests",
    "start": "934819",
    "end": "942139"
  },
  {
    "text": "per second for steady-state traffic per prefix if there is a peak or if there is",
    "start": "942139",
    "end": "948169"
  },
  {
    "text": "a burst then st will adjust quickly now this is good for thousand requests per second or two thousand requests per",
    "start": "948169",
    "end": "954319"
  },
  {
    "text": "second but if you're looking at hundreds of thousands of requests per second we recommend that you add an alphanumeric",
    "start": "954319",
    "end": "960319"
  },
  {
    "text": "three four character hash before the date and if you want it to be a little",
    "start": "960319",
    "end": "965419"
  },
  {
    "text": "more list friendly there is an example right here where you have a prefix right before the hash so putting it all",
    "start": "965419",
    "end": "972829"
  },
  {
    "start": "972000",
    "end": "1050000"
  },
  {
    "text": "together in terms of a data Lake we recommend that you aggregate small files",
    "start": "972829",
    "end": "978019"
  },
  {
    "text": "and create a large file as you put it into s3 your goal should be 128",
    "start": "978019",
    "end": "983239"
  },
  {
    "text": "megabytes or higher having larger files has three benefits you have faster",
    "start": "983239",
    "end": "988910"
  },
  {
    "text": "listing that is less metadata lookup and then request cost goes down with EMR the",
    "start": "988910",
    "end": "995540"
  },
  {
    "text": "s3 disk CP command helps you do so or if you have a click stream or IOT type of",
    "start": "995540",
    "end": "1001179"
  },
  {
    "text": "workload you can stream the data those small files with Kinesis fire hose and use kinases firehose to consolidate the",
    "start": "1001179",
    "end": "1007689"
  },
  {
    "text": "data and then put it on to s3 secondly if you have if you want to efficiently",
    "start": "1007689",
    "end": "1014049"
  },
  {
    "text": "query ITAR objects within the objects data within the objects you can use s3",
    "start": "1014049",
    "end": "1019989"
  },
  {
    "text": "select and I'm going to go a little deeper when I talk about analytics in place and then Park a or o RC which are",
    "start": "1019989",
    "end": "1028750"
  },
  {
    "text": "columnar data formats often helps improve query performance if you're",
    "start": "1028750",
    "end": "1034089"
  },
  {
    "text": "using EMR and you use consistent view list delete and read after write",
    "start": "1034089",
    "end": "1039279"
  },
  {
    "text": "consistencies for multiple MapReduce jobs happening on the same data set can be minimized so you can have a list",
    "start": "1039279",
    "end": "1045370"
  },
  {
    "text": "consistency with EMR consistent view so",
    "start": "1045370",
    "end": "1050559"
  },
  {
    "start": "1050000",
    "end": "1083000"
  },
  {
    "text": "let's talk about analytics and query in place Amazon s3 is the only cloud",
    "start": "1050559",
    "end": "1056139"
  },
  {
    "text": "storage provider that helps you run sophisticated analytics in place with",
    "start": "1056139",
    "end": "1061330"
  },
  {
    "text": "just sequel queries so you can have Athena running sequel queries on unstructured data spectrum running see comparison",
    "start": "1061330",
    "end": "1068830"
  },
  {
    "text": "structure data in this slide you see an end-to-end architecture of persistently",
    "start": "1068830",
    "end": "1074260"
  },
  {
    "text": "storing data on Amazon s3 and then analyzing with different analytics engine and the visualizing it with quick",
    "start": "1074260",
    "end": "1080290"
  },
  {
    "text": "cite natively within s3 you have s3 select it's a powerful API that goes",
    "start": "1080290",
    "end": "1088150"
  },
  {
    "start": "1083000",
    "end": "1116000"
  },
  {
    "text": "within the object and retrieves data with just sequel select from where clauses so think of a use case where you",
    "start": "1088150",
    "end": "1095590"
  },
  {
    "text": "have 200 stores putting weekly sales data into a single large object and you",
    "start": "1095590",
    "end": "1100810"
  },
  {
    "text": "want the data for only one store for that week your application now can call",
    "start": "1100810",
    "end": "1106330"
  },
  {
    "text": "s3 select API it will go within the object and just get that data thereby reducing cost and optimizing performance",
    "start": "1106330",
    "end": "1112540"
  },
  {
    "text": "to a great deal if you are working with the demand highly distributed processing",
    "start": "1112540",
    "end": "1118930"
  },
  {
    "start": "1116000",
    "end": "1176000"
  },
  {
    "text": "frameworks EMR is the way to go with Amazon EMR storage and compute is",
    "start": "1118930",
    "end": "1124870"
  },
  {
    "text": "decoupled so you're optimizing cost improving performance there is elasticity you can run multiple EMR",
    "start": "1124870",
    "end": "1130450"
  },
  {
    "text": "clusters right size them shut them down without impacting anything on s3 if",
    "start": "1130450",
    "end": "1136090"
  },
  {
    "text": "you're using AMR it is a good idea to compress those datasets using compression algorithms such as gzip B's",
    "start": "1136090",
    "end": "1142270"
  },
  {
    "text": "if l co snappy it depends on your use case whether you want faster compression",
    "start": "1142270",
    "end": "1147340"
  },
  {
    "text": "or you want more compression and these are the pros and cons of these algorithms park' format or over C format it does",
    "start": "1147340",
    "end": "1155140"
  },
  {
    "text": "not matter which analytics engine you are using or tool you're using a columnar format will always benefit",
    "start": "1155140",
    "end": "1160840"
  },
  {
    "text": "querying your data and ultimately aggregating small files with s3 DCP",
    "start": "1160840",
    "end": "1166540"
  },
  {
    "text": "gloop right across and achieving the optimal file size will always help you with reducing requests cost and faster",
    "start": "1166540",
    "end": "1173380"
  },
  {
    "text": "query performance if you have cases",
    "start": "1173380",
    "end": "1178780"
  },
  {
    "start": "1176000",
    "end": "1247000"
  },
  {
    "text": "where you are using structured data with large tables complex queries with large joints or you are already having use",
    "start": "1178780",
    "end": "1185710"
  },
  {
    "text": "case we're using redshift you can now without doing ETL use redshift spectrum",
    "start": "1185710",
    "end": "1191260"
  },
  {
    "text": "and do those complex queries so restoring data from data warehouses and",
    "start": "1191260",
    "end": "1196330"
  },
  {
    "text": "if you're thinking hundreds of terabytes of data that takes time that in dueces latency so now what you can do is",
    "start": "1196330",
    "end": "1202420"
  },
  {
    "text": "push those large tables on to s3 use redshift spectrum and then spin up read-only clusters when you have a spike",
    "start": "1202420",
    "end": "1208960"
  },
  {
    "text": "and then do those analytics retro spectrum also charges by the data that",
    "start": "1208960",
    "end": "1214390"
  },
  {
    "text": "you scan and it pulls the columns on the s3 tables so if you have columnar",
    "start": "1214390",
    "end": "1219400"
  },
  {
    "text": "formats it will eliminate all the unneeded columns that are not required and if you know which columns you are",
    "start": "1219400",
    "end": "1226090"
  },
  {
    "text": "frequently using you can partition them and then that will improve your query as well and there are certain sequel",
    "start": "1226090",
    "end": "1232810"
  },
  {
    "text": "operations such as some the aggregate functions some average min max or the",
    "start": "1232810",
    "end": "1238120"
  },
  {
    "text": "group by clauses that those predicates you can push down to the spectrum layer and that will improve your query",
    "start": "1238120",
    "end": "1243250"
  },
  {
    "text": "performance much higher finally if you",
    "start": "1243250",
    "end": "1248860"
  },
  {
    "start": "1247000",
    "end": "1296000"
  },
  {
    "text": "want to run ad-hoc queries on unstructured data you want to use something which is server less that is",
    "start": "1248860",
    "end": "1253930"
  },
  {
    "text": "fully managed so you're not managing any infrastructure you want to do schema and read then Athena is a very good tool to",
    "start": "1253930",
    "end": "1260170"
  },
  {
    "text": "do so simply define the schema on the console pointed to the s3 data and then start querying we recommend that you use",
    "start": "1260170",
    "end": "1269260"
  },
  {
    "text": "parquet because it automatically compresses the data and then splits it splitting that data will introduce",
    "start": "1269260",
    "end": "1276310"
  },
  {
    "text": "parallelism and then multiple Athena clients can access that data and ultimately to reduce any overhead of",
    "start": "1276310",
    "end": "1283660"
  },
  {
    "text": "doing that query optimizing the file size so putting on larger file sizes 120",
    "start": "1283660",
    "end": "1289480"
  },
  {
    "text": "megabytes or higher will always be better in terms of reducing costs and optimizing performance so we took a",
    "start": "1289480",
    "end": "1296740"
  },
  {
    "text": "customer data set and we did an example where we put it in a CSV or txt format and compared it to a parquet format",
    "start": "1296740",
    "end": "1304170"
  },
  {
    "text": "whether it's the size of storage on s3 the query run time or the data scan or",
    "start": "1304170",
    "end": "1309910"
  },
  {
    "text": "the cost there were multiple benefits that we saw when we ran it in park' format so it's highly recommended that",
    "start": "1309910",
    "end": "1315790"
  },
  {
    "text": "with any query in place that you do parquet is going to be much better now",
    "start": "1315790",
    "end": "1321690"
  },
  {
    "text": "that we have set the context I would like to invite our invite our first special guest Amir is Shalom to talk",
    "start": "1321690",
    "end": "1328720"
  },
  {
    "start": "1325000",
    "end": "1342000"
  },
  {
    "text": "about viber is the chief architect at viber",
    "start": "1328720",
    "end": "1332940"
  },
  {
    "text": "hi everybody I've been with viber for something like 6 years which is almost",
    "start": "1335190",
    "end": "1341020"
  },
  {
    "text": "from the beginning well in the u.s. it's a bit less popular in many countries",
    "start": "1341020",
    "end": "1346060"
  },
  {
    "start": "1342000",
    "end": "1460000"
  },
  {
    "text": "around the world viber is the most popular instant messaging application sometimes even more than Facebook in",
    "start": "1346060",
    "end": "1354870"
  },
  {
    "text": "contrast to most instant messaging applications we end-to-end encrypt are",
    "start": "1354870",
    "end": "1361240"
  },
  {
    "text": "data and all the messages and media formats that we're doing and have full multi device support in 2017 Rakatan",
    "start": "1361240",
    "end": "1370180"
  },
  {
    "text": "which is our parent company became the official sponsor of xev Barcelona and the NBA Golden State Warriors I received",
    "start": "1370180",
    "end": "1378370"
  },
  {
    "text": "also the jersey of FC B but unfortunately not the Golden State Warriors yet still waiting for that",
    "start": "1378370",
    "end": "1384930"
  },
  {
    "text": "these teams are using viber to be their official messaging and communications",
    "start": "1384930",
    "end": "1391240"
  },
  {
    "text": "platforms and they're using our public account and they have millions of followers on that they also have chat",
    "start": "1391240",
    "end": "1398260"
  },
  {
    "text": "BOTS which users can use to basically choose their MVP players and share",
    "start": "1398260",
    "end": "1403930"
  },
  {
    "text": "experiences so it's a lot more encompassing experience with viber",
    "start": "1403930",
    "end": "1409650"
  },
  {
    "text": "but let's talk a little bit about data I want to show you a short video showing what happens in viber in just 60 seconds",
    "start": "1409650",
    "end": "1416410"
  },
  {
    "text": "so you get a feel of what we do [Music]",
    "start": "1416410",
    "end": "1431689"
  },
  {
    "text": "[Music]",
    "start": "1441960",
    "end": "1450369"
  },
  {
    "text": "[Music]",
    "start": "1452670",
    "end": "1459240"
  },
  {
    "text": "so vibra is what's called the planetary scale application we're in almost every",
    "start": "1459980",
    "end": "1466200"
  },
  {
    "start": "1460000",
    "end": "1498000"
  },
  {
    "text": "country around the world with close to a billion users we have 10 to 15 billion",
    "start": "1466200",
    "end": "1471600"
  },
  {
    "text": "events daily with peaking at over 300 thousand events per second we store 5",
    "start": "1471600",
    "end": "1477539"
  },
  {
    "text": "petabytes of data on s3 and glacier and our production no sequel database called couch based is doing over 2 million",
    "start": "1477539",
    "end": "1484919"
  },
  {
    "text": "transactions per second on 20 terabytes of data and 35 billion keys so we're",
    "start": "1484919",
    "end": "1490710"
  },
  {
    "text": "working quite a lot of data now in order to handle that amount of data the architecture that we use is this we",
    "start": "1490710",
    "end": "1500009"
  },
  {
    "start": "1498000",
    "end": "1585000"
  },
  {
    "text": "basically have from our production database systems we send events to our real-time data pipeline which is Kinesis",
    "start": "1500009",
    "end": "1506580"
  },
  {
    "text": "this is fanned out into to consumers 1 which back us up the raw data which is",
    "start": "1506580",
    "end": "1512249"
  },
  {
    "text": "Kinesis firehose using lambda transformations and we have a real-time data processor which is apache storm",
    "start": "1512249",
    "end": "1518759"
  },
  {
    "text": "which handles the real-time event handling this both cleans the events",
    "start": "1518759",
    "end": "1524070"
  },
  {
    "text": "verifies them and sends them into real time processing that's required such as spam handling and updating our profile",
    "start": "1524070",
    "end": "1531690"
  },
  {
    "text": "database which is again Couchbase then it writes down it fans down the different events we have over 300 event",
    "start": "1531690",
    "end": "1538169"
  },
  {
    "text": "types and it fans them out into different directories on our s3 data lake we have many different ETL jobs",
    "start": "1538169",
    "end": "1545100"
  },
  {
    "text": "which do all kinds of analytics and aggregations we have different spark jobs presto pig even lambda functions and all",
    "start": "1545100",
    "end": "1553320"
  },
  {
    "text": "the data is basically returned back to our data Lake after processing some of",
    "start": "1553320",
    "end": "1559679"
  },
  {
    "text": "the data is also loaded into data warehouses such as redshift or standard databases like Aurora or my sequel then",
    "start": "1559679",
    "end": "1567570"
  },
  {
    "text": "we have our query engines which we use Athena presto and others to query those data and finally going into our",
    "start": "1567570",
    "end": "1573989"
  },
  {
    "text": "reporting tools which we use to blow for most of our business users and we also have some self-service reporting done",
    "start": "1573989",
    "end": "1581340"
  },
  {
    "text": "with read ash and Zeppelin and others I'd like to take you through a few of",
    "start": "1581340",
    "end": "1587639"
  },
  {
    "start": "1585000",
    "end": "1601000"
  },
  {
    "text": "our challenges that we had with our like the first is a few s3 performance issues then data access rights encrypted",
    "start": "1587639",
    "end": "1596429"
  },
  {
    "text": "data storage and how do you store data from third parties so regarding s3",
    "start": "1596429",
    "end": "1602640"
  },
  {
    "start": "1601000",
    "end": "1672000"
  },
  {
    "text": "performance we have as I mentioned before over 300 different event types these are all ingested into our Kinesis",
    "start": "1602640",
    "end": "1608850"
  },
  {
    "text": "stream to Apache storm and the problem is that some of these events are very high throughput they can reach over",
    "start": "1608850",
    "end": "1614940"
  },
  {
    "text": "50,000 events per second while others are very slow they can come on in let's",
    "start": "1614940",
    "end": "1621059"
  },
  {
    "text": "say once every few minutes and what happens is that because it's written into a hive directory format by the day",
    "start": "1621059",
    "end": "1630240"
  },
  {
    "text": "of the month day and hour basically the what happens is the the slower events",
    "start": "1630240",
    "end": "1636870"
  },
  {
    "text": "become very very small files and then when we have let's say a big presto",
    "start": "1636870",
    "end": "1642210"
  },
  {
    "text": "cluster which is running on a very large range of dates let's say a few months on one of these events it could run through",
    "start": "1642210",
    "end": "1649169"
  },
  {
    "text": "the the files very very fast and we're reaching peaks of over 15,000 TPS on a",
    "start": "1649169",
    "end": "1654240"
  },
  {
    "text": "single s3 bucket which cause throttling the problem is it's not only throttling this request is throttling everything on",
    "start": "1654240",
    "end": "1661440"
  },
  {
    "text": "our data Lake so that means that all of our ETL jobs are storm processor all of these are basically getting operational",
    "start": "1661440",
    "end": "1668940"
  },
  {
    "text": "pains so we had to find some kind of solution for that so what we did is we had two parts one",
    "start": "1668940",
    "end": "1674730"
  },
  {
    "start": "1672000",
    "end": "1737000"
  },
  {
    "text": "is to concatenate the smaller files into a larger ones optimally over a hundred megabytes we do this using the tool that",
    "start": "1674730",
    "end": "1682110"
  },
  {
    "text": "they mentioned before which is s3 disk CP and the second part is to use a",
    "start": "1682110",
    "end": "1687179"
  },
  {
    "text": "column or data format which in our case we use park' but as they said it doesn't",
    "start": "1687179",
    "end": "1692640"
  },
  {
    "text": "really matter or C as good as well and me it was an EMR spark cluster in order to do that we also want to further",
    "start": "1692640",
    "end": "1699659"
  },
  {
    "text": "optimize this by basically doing those two in a single stage in order to remove the latency involved and we're thinking",
    "start": "1699659",
    "end": "1706230"
  },
  {
    "text": "of doing that either with a spark job or maybe using the managed ETL called glue",
    "start": "1706230",
    "end": "1711480"
  },
  {
    "text": "and Amazon another to optimizations we're looking into is to use reverse high partitioning which basically means",
    "start": "1711480",
    "end": "1718350"
  },
  {
    "text": "to reverse the year month day hour so we have better partitioning over",
    "start": "1718350",
    "end": "1725500"
  },
  {
    "text": "different s3 physical partitions and finally use even larger files and 100",
    "start": "1725500",
    "end": "1730820"
  },
  {
    "text": "megabytes for the larger throughput events which could further improve performance the next use case is data",
    "start": "1730820",
    "end": "1738890"
  },
  {
    "start": "1737000",
    "end": "1776000"
  },
  {
    "text": "access rights these days you've probably heard a lot of the gdpr and all of these",
    "start": "1738890",
    "end": "1744260"
  },
  {
    "text": "data privacy concerns which are of course very valid and we always strive",
    "start": "1744260",
    "end": "1749870"
  },
  {
    "text": "to keep users privacy and we want to our events sometimes contain sensitive data",
    "start": "1749870",
    "end": "1756590"
  },
  {
    "text": "such as phone numbers they're only containing metadata just to be clear no messages or things like that inside the",
    "start": "1756590",
    "end": "1763310"
  },
  {
    "text": "events and even if there were they're into and encrypted so we can read them anyway so we want to be able to work",
    "start": "1763310",
    "end": "1769400"
  },
  {
    "text": "with these events but limit our employees and access to the personal",
    "start": "1769400",
    "end": "1774980"
  },
  {
    "text": "data so we have two ways of doing that the first is to have a redacted version",
    "start": "1774980",
    "end": "1781730"
  },
  {
    "start": "1776000",
    "end": "1828000"
  },
  {
    "text": "of the data which is basically means that we're going to make certain fields disappear all of our event data beta",
    "start": "1781730",
    "end": "1788680"
  },
  {
    "text": "event data is stored in Aurora so it contains for each different events of",
    "start": "1788680",
    "end": "1793970"
  },
  {
    "text": "the fields and for each field we can specify if it should be redacted or not so let's say the phone number I",
    "start": "1793970",
    "end": "1799280"
  },
  {
    "text": "mentioned before we would mark it as 2 to be redacted then we have an automated process which would take the event data",
    "start": "1799280",
    "end": "1806420"
  },
  {
    "text": "and create two different hive meta stores one with the full access data and one with those fields redacted which",
    "start": "1806420",
    "end": "1812540"
  },
  {
    "text": "basically makes them disappear then for let's say we have a reporting tool the user would try to login using an LDAP",
    "start": "1812540",
    "end": "1820010"
  },
  {
    "text": "Active Directory and then we would map for that user if he has the full access or the redacted access to the data the",
    "start": "1820010",
    "end": "1829160"
  },
  {
    "start": "1828000",
    "end": "1883000"
  },
  {
    "text": "section the second option that we have is to anonymize the data what we do is that we have in our storm real time",
    "start": "1829160",
    "end": "1836210"
  },
  {
    "text": "processor getting the events before they're actually written to s3 we basically check if that field should be",
    "start": "1836210",
    "end": "1842960"
  },
  {
    "text": "anonymized anonymizing data allows us to leave the the full the full event intact",
    "start": "1842960",
    "end": "1850250"
  },
  {
    "text": "but still allow the event to be unique according to user and things like that",
    "start": "1850250",
    "end": "1856920"
  },
  {
    "text": "I'm for it and then it's written to ask three anonymized unfortunately we can do",
    "start": "1856920",
    "end": "1861940"
  },
  {
    "text": "that for all of our fields because we have all kinds of legal requirements to keep some of that data and also it",
    "start": "1861940",
    "end": "1868240"
  },
  {
    "text": "limits some of our data science capabilities so we basically have some",
    "start": "1868240",
    "end": "1873490"
  },
  {
    "text": "kind of hybrid solution so for funks some events or in some fields we use the redacted version and for others we use",
    "start": "1873490",
    "end": "1879370"
  },
  {
    "text": "the anonymization capabilities the third",
    "start": "1879370",
    "end": "1884380"
  },
  {
    "start": "1883000",
    "end": "1938000"
  },
  {
    "text": "use case I'd like to talk about is encrypting data storage the one I want",
    "start": "1884380",
    "end": "1890110"
  },
  {
    "text": "to talk about is that we back up our production database servers the Couchbase service that was talking about",
    "start": "1890110",
    "end": "1896530"
  },
  {
    "text": "before we back them up daily to s3 this data has very sensitive data and very",
    "start": "1896530",
    "end": "1903400"
  },
  {
    "text": "important data so we want them to be encrypted we want to have strict access control over who can access this data we",
    "start": "1903400",
    "end": "1910840"
  },
  {
    "text": "want to be able to replicate this data over to another region and we want to",
    "start": "1910840",
    "end": "1916480"
  },
  {
    "text": "have a rather complex data retention this is very large data so in order to",
    "start": "1916480",
    "end": "1921910"
  },
  {
    "text": "save costs what we want to do is have for the first number of days of the backup we want to store the monastry and",
    "start": "1921910",
    "end": "1928150"
  },
  {
    "text": "weekly backups up to let's say a month we want to move them into infrequent access for saving costs so the way that",
    "start": "1928150",
    "end": "1935830"
  },
  {
    "text": "we do that there we go is we have our",
    "start": "1935830",
    "end": "1941140"
  },
  {
    "start": "1938000",
    "end": "2018000"
  },
  {
    "text": "database servers there backing up to the local region s3 bucket and we use",
    "start": "1941140",
    "end": "1946330"
  },
  {
    "text": "server-side encryption kms for that to encrypt the data this also gives us better access control because even",
    "start": "1946330",
    "end": "1952900"
  },
  {
    "text": "though you let's say some users may have access to that bucket or certain processes could have like s3 asterisks",
    "start": "1952900",
    "end": "1959230"
  },
  {
    "text": "or whatever to all the buckets if they don't have specific access to the KMS key they won't be able to access that",
    "start": "1959230",
    "end": "1965770"
  },
  {
    "text": "data so that's very important to us regarding the lifecycle policies what we",
    "start": "1965770",
    "end": "1971680"
  },
  {
    "text": "do is that we use a rather new feature called object level tagging which allows",
    "start": "1971680",
    "end": "1976690"
  },
  {
    "text": "us to tag each specific file with a specific tag and then we tag the ones that we want to stay in s3 with let's",
    "start": "1976690",
    "end": "1982990"
  },
  {
    "text": "say an s3 tag and the ones that we want to move to si a we tag them with an SI and then we can set the lifecycle policy",
    "start": "1982990",
    "end": "1990350"
  },
  {
    "text": "to set them to the correct lifecycle policy according to the tag and finally",
    "start": "1990350",
    "end": "1995690"
  },
  {
    "text": "we have we use a very new feature that was just released a few weeks ago which is the across region replication kms to",
    "start": "1995690",
    "end": "2002100"
  },
  {
    "text": "replicate the files from the local s3 bucket to another region while it's",
    "start": "2002100",
    "end": "2008409"
  },
  {
    "text": "encrypted both in transit and in rest in the final destination so that's been",
    "start": "2008409",
    "end": "2013600"
  },
  {
    "text": "very useful that new feature thanks guys final use case I'd like to talk about is",
    "start": "2013600",
    "end": "2020649"
  },
  {
    "start": "2018000",
    "end": "2069000"
  },
  {
    "text": "storage of data that we received from third parties into our data like we",
    "start": "2020649",
    "end": "2025889"
  },
  {
    "text": "don't want to give them direct access to our data like we want to also validate",
    "start": "2025889",
    "end": "2031059"
  },
  {
    "text": "the data and allow optional data transformation on it so what we do is we",
    "start": "2031059",
    "end": "2037389"
  },
  {
    "text": "have some kind of DMZ s3 bucket so it's a separate s3 bucket from our data like",
    "start": "2037389",
    "end": "2042429"
  },
  {
    "text": "that we provide access keys to our third parties and they can write data using",
    "start": "2042429",
    "end": "2047980"
  },
  {
    "text": "that to that bucket then we have a lambda function which is triggered for each file that's written there then that",
    "start": "2047980",
    "end": "2055030"
  },
  {
    "text": "lambda function could do those data validations transformations and copy the files to our s3 data lake securely and",
    "start": "2055030",
    "end": "2063368"
  },
  {
    "text": "safely and also optionally encrypt them if it's necessary so just to summarize",
    "start": "2063369",
    "end": "2070378"
  },
  {
    "start": "2069000",
    "end": "2113000"
  },
  {
    "text": "viber stores all of its data and events in a centralized data Lake this provides",
    "start": "2070379",
    "end": "2077408"
  },
  {
    "text": "us access by a large number of query tools and ETL jobs very easily to get",
    "start": "2077409",
    "end": "2084460"
  },
  {
    "text": "better performance always use large files and column or data formats for",
    "start": "2084460",
    "end": "2090638"
  },
  {
    "text": "users privacy use either redacted or anonymization in order to make PII data",
    "start": "2090639",
    "end": "2097390"
  },
  {
    "text": "more safe from your employees and finally for security use kms for",
    "start": "2097390",
    "end": "2103180"
  },
  {
    "text": "encryption and for data from third parties you can use an s3 DMZ with",
    "start": "2103180",
    "end": "2108970"
  },
  {
    "text": "lambda functions to copy them to your data Lake so thank you very much",
    "start": "2108970",
    "end": "2114690"
  },
  {
    "start": "2113000",
    "end": "2146000"
  },
  {
    "text": "[Applause] now we would like to invite our second",
    "start": "2114690",
    "end": "2121910"
  },
  {
    "text": "special guest Hong Bao Zheng who's a software engineer at Airbnb and who's going to talk about how they are building their debt lake thanks Petey",
    "start": "2121910",
    "end": "2133540"
  },
  {
    "text": "hello everyone I'm a humble engineer from that they write infrastructure team",
    "start": "2133540",
    "end": "2138890"
  },
  {
    "text": "at Airbnb today I'm going to talk about the theorists storage system in our data",
    "start": "2138890",
    "end": "2144530"
  },
  {
    "text": "warehouse I'm going to cover the storage challenges we are facing in our data",
    "start": "2144530",
    "end": "2150590"
  },
  {
    "start": "2146000",
    "end": "2166000"
  },
  {
    "text": "warehouse the Thira storages system that we'll be able to solve the to solve the",
    "start": "2150590",
    "end": "2156500"
  },
  {
    "text": "issues and there's some optimizations authority to improve the s3 access",
    "start": "2156500",
    "end": "2163160"
  },
  {
    "text": "performance an Airbnb the amount of data",
    "start": "2163160",
    "end": "2169250"
  },
  {
    "start": "2166000",
    "end": "2217000"
  },
  {
    "text": "grows tremendously we're seeing over three times over 3 times a year over",
    "start": "2169250",
    "end": "2175400"
  },
  {
    "text": "year growth rate and we have tens of a petabytes of data in our data warehouse",
    "start": "2175400",
    "end": "2181150"
  },
  {
    "text": "we used to put all our data English in HDFS but it didn't take too long for us",
    "start": "2181150",
    "end": "2188450"
  },
  {
    "text": "to generate too many fire blocks for them no to handle or so the bill is not",
    "start": "2188450",
    "end": "2195650"
  },
  {
    "text": "small to pay for the powerful ec2 instances that we used to build the HDFS",
    "start": "2195650",
    "end": "2201230"
  },
  {
    "text": "we wanted to move massive amount of data to s3 but there are a couple of things",
    "start": "2201230",
    "end": "2206720"
  },
  {
    "text": "we want we want to address for instance the invention consistency the data",
    "start": "2206720",
    "end": "2213800"
  },
  {
    "text": "access performance so the solution is a",
    "start": "2213800",
    "end": "2220360"
  },
  {
    "start": "2217000",
    "end": "2313000"
  },
  {
    "text": "tiered storage system we want to keep the hard data on HDFS where I will move",
    "start": "2220360",
    "end": "2226610"
  },
  {
    "text": "the warm and code data to s3 so such that we bring the best of the best",
    "start": "2226610",
    "end": "2232460"
  },
  {
    "text": "together we have good performance to to access a recently generated data which",
    "start": "2232460",
    "end": "2240920"
  },
  {
    "text": "we queried the most by leveraging average s3 we have Infernus inference",
    "start": "2240920",
    "end": "2250020"
  },
  {
    "text": "and a lower cost as you can see from the chart that people use various clients to",
    "start": "2250020",
    "end": "2256920"
  },
  {
    "text": "write them read and write data to HDFS and the way poor degree move the old",
    "start": "2256920",
    "end": "2262980"
  },
  {
    "text": "data to s3 and the people can read data from s3 later on so this is how we do it",
    "start": "2262980",
    "end": "2271530"
  },
  {
    "text": "we have pipelines to collect h FS image of HDFS and also down to the heavy metal",
    "start": "2271530",
    "end": "2279090"
  },
  {
    "text": "storm to our data warehouse the storage processor takes in configurable archive",
    "start": "2279090",
    "end": "2284790"
  },
  {
    "text": "policy and the retention policy on to decide which are which data set so we",
    "start": "2284790",
    "end": "2291390"
  },
  {
    "text": "want to delete and which data sets to to move to s3 we get we because we know",
    "start": "2291390",
    "end": "2299070"
  },
  {
    "text": "like what data are generated every day from the pipeline's so in the next two",
    "start": "2299070",
    "end": "2304440"
  },
  {
    "text": "slides I'm going to focus on how we back up data to s3 and how we have data from",
    "start": "2304440",
    "end": "2310260"
  },
  {
    "text": "a shared AFS so I'm going to use one",
    "start": "2310260",
    "end": "2316140"
  },
  {
    "start": "2313000",
    "end": "2382000"
  },
  {
    "text": "example like just like one high partition to show you how we backup the",
    "start": "2316140",
    "end": "2322710"
  },
  {
    "text": "data to s3 since we backup tons of data to s3 every day so parallelism is the",
    "start": "2322710",
    "end": "2329850"
  },
  {
    "text": "key here there are a couple of MapReduce jobs involved in this process so firstly",
    "start": "2329850",
    "end": "2336170"
  },
  {
    "text": "MapReduce job is to look for all the files within this partition and generate",
    "start": "2336170",
    "end": "2342960"
  },
  {
    "text": "the destination s3 paths for each of the file in the in the partition so based on",
    "start": "2342960",
    "end": "2350910"
  },
  {
    "text": "the mapping between the source file and the destination path we have another",
    "start": "2350910",
    "end": "2356180"
  },
  {
    "text": "MapReduce job to do the copy from HDFS to s3 to ensure the integrity of the",
    "start": "2356180",
    "end": "2363390"
  },
  {
    "text": "data we generate and compare the CRC of the files the last step in the backup",
    "start": "2363390",
    "end": "2370140"
  },
  {
    "text": "process is to update the chimera store of the information that we have our",
    "start": "2370140",
    "end": "2376650"
  },
  {
    "text": "successful backup for this partition",
    "start": "2376650",
    "end": "2381589"
  },
  {
    "start": "2382000",
    "end": "2426000"
  },
  {
    "text": "so in the archive staff so we need to verify if we have if the backup user is",
    "start": "2382140",
    "end": "2389410"
  },
  {
    "text": "successful valid and up-to-date because it's possible that after we back up a",
    "start": "2389410",
    "end": "2395470"
  },
  {
    "text": "partition people regenerate data for this partition and then we do some",
    "start": "2395470",
    "end": "2401740"
  },
  {
    "text": "lightweight data validation such as comparing the number of files and the",
    "start": "2401740",
    "end": "2406840"
  },
  {
    "text": "size of the files because we already did this CRC the last important step is to",
    "start": "2406840",
    "end": "2412930"
  },
  {
    "text": "update the locations of the partitions such that for future queries they are",
    "start": "2412930",
    "end": "2418600"
  },
  {
    "text": "going to query the data from s3 instead of which DFS this slide shows the",
    "start": "2418600",
    "end": "2428200"
  },
  {
    "start": "2426000",
    "end": "2463000"
  },
  {
    "text": "timeline of how we handle a partition for for example we generates that our",
    "start": "2428200",
    "end": "2437110"
  },
  {
    "text": "partition today and by the end of the day we are going to back the back up the partition to s3 after the data kind of",
    "start": "2437110",
    "end": "2445390"
  },
  {
    "text": "goes off let's say one month later we are going to archival data and the",
    "start": "2445390",
    "end": "2451630"
  },
  {
    "text": "deleted data from HD HDFS by that time we will get our consistent view for the",
    "start": "2451630",
    "end": "2459430"
  },
  {
    "text": "data on s3 so all the challenges solved",
    "start": "2459430",
    "end": "2467130"
  },
  {
    "start": "2463000",
    "end": "2479000"
  },
  {
    "text": "by this tearless storage system well",
    "start": "2467130",
    "end": "2472600"
  },
  {
    "text": "partly because we still want to improve the performance of s3 we type the deep",
    "start": "2472600",
    "end": "2480400"
  },
  {
    "start": "2479000",
    "end": "2510000"
  },
  {
    "text": "into s3 a file system which is a hive client to access s3 open sourced by Hana",
    "start": "2480400",
    "end": "2487810"
  },
  {
    "text": "works with either a couple of optimizations and we ended up having s3",
    "start": "2487810",
    "end": "2493180"
  },
  {
    "text": "a plus file system so these are the optimizations with it including metadata",
    "start": "2493180",
    "end": "2500230"
  },
  {
    "text": "cache leveraging multi-part API in read",
    "start": "2500230",
    "end": "2505570"
  },
  {
    "text": "and the read read prefetch so getting",
    "start": "2505570",
    "end": "2511420"
  },
  {
    "start": "2510000",
    "end": "2566000"
  },
  {
    "text": "metadata from s3 is not as performant as HD so it takes tens of milliseconds or even",
    "start": "2511420",
    "end": "2520150"
  },
  {
    "text": "like 100 millisecond over 100 milliseconds to get the metadata metadata for one s3 object",
    "start": "2520150",
    "end": "2526990"
  },
  {
    "text": "so we've seen 1/2 queries that stores for 40 minutes just the William for all",
    "start": "2526990",
    "end": "2533860"
  },
  {
    "text": "the metadata - to return from s3 before doing any real job so we put the",
    "start": "2533860",
    "end": "2541300"
  },
  {
    "text": "metadata into RDS database to accelerate",
    "start": "2541300",
    "end": "2546310"
  },
  {
    "text": "the retrieval this schema here is pretty simple we need to identify for s3 passes",
    "start": "2546310",
    "end": "2552520"
  },
  {
    "text": "a direct user directory or not and also the lens for each of the object with",
    "start": "2552520",
    "end": "2559120"
  },
  {
    "text": "this we achieve the study time speed up for metadata retrieval",
    "start": "2559120",
    "end": "2565860"
  },
  {
    "start": "2566000",
    "end": "2584000"
  },
  {
    "text": "s3 multi-part API provides our couple of other advantages including improved",
    "start": "2567660",
    "end": "2574450"
  },
  {
    "text": "throughput quick recovery from any network issues here we focus on the",
    "start": "2574450",
    "end": "2581470"
  },
  {
    "text": "throughput first we evaluate the",
    "start": "2581470",
    "end": "2587220"
  },
  {
    "start": "2584000",
    "end": "2610000"
  },
  {
    "text": "pennyways the right the right throughput by leveraging multiple API we experiment",
    "start": "2587220",
    "end": "2595540"
  },
  {
    "text": "with various parts eyes and number of threats as you can see here we achieved",
    "start": "2595540",
    "end": "2603220"
  },
  {
    "text": "like more than three times improvement over the baseline another important",
    "start": "2603220",
    "end": "2612280"
  },
  {
    "start": "2610000",
    "end": "2650000"
  },
  {
    "text": "optimization with it is read prefetch we proactively initiate the fetching for",
    "start": "2612280",
    "end": "2618700"
  },
  {
    "text": "the next couple of parts when we read data it not only reduced the latency for",
    "start": "2618700",
    "end": "2624700"
  },
  {
    "text": "data fetching and also improved the read perform reads through pool so with like",
    "start": "2624700",
    "end": "2632590"
  },
  {
    "text": "the decent with a small number of professions threads and the different",
    "start": "2632590",
    "end": "2640260"
  },
  {
    "text": "parts eyes which are cheap like over two times like rid of race through food",
    "start": "2640260",
    "end": "2646690"
  },
  {
    "text": "improvement we use their couple for real high",
    "start": "2646690",
    "end": "2652750"
  },
  {
    "start": "2650000",
    "end": "2688000"
  },
  {
    "text": "priests to test the performance of our optimization so the latency improvement",
    "start": "2652750",
    "end": "2659920"
  },
  {
    "text": "ranges from 10 percent to 10 times depends on depending on how many three",
    "start": "2659920",
    "end": "2666730"
  },
  {
    "text": "objects that are one query reads and also depending like the proportion",
    "start": "2666730",
    "end": "2672340"
  },
  {
    "text": "proportion of time that use one chorus spend down the spend on reading data",
    "start": "2672340",
    "end": "2679000"
  },
  {
    "text": "versus computation so this wraps up my talk and thank you guys okay great",
    "start": "2679000",
    "end": "2693820"
  },
  {
    "text": "so you know you've heard a couple of great examples of how to build a data",
    "start": "2693820",
    "end": "2699280"
  },
  {
    "text": "Lake on AWS s3 and glacier and some best practices for that so to wrap it all up",
    "start": "2699280",
    "end": "2705250"
  },
  {
    "text": "and get to Q&A so you can ask good probing questions of our special guest",
    "start": "2705250",
    "end": "2711820"
  },
  {
    "text": "what did we learn you know always store a raw copy a copy of your raw input data",
    "start": "2711820",
    "end": "2717490"
  },
  {
    "text": "because you never know what you're gonna want to do with it down the road you know use lambda and s3 events to start",
    "start": "2717490",
    "end": "2724690"
  },
  {
    "text": "to trigger and automate the full spectrum of workload security obviously is the foundational so implement the",
    "start": "2724690",
    "end": "2731710"
  },
  {
    "text": "right security methods and controls be mindful of your data format for",
    "start": "2731710",
    "end": "2736890"
  },
  {
    "text": "efficiency and performance partitioned data improve performance and compression",
    "start": "2736890",
    "end": "2743620"
  },
  {
    "text": "to save space and reduce cost and improve performance so you know these",
    "start": "2743620",
    "end": "2750940"
  },
  {
    "text": "are a few of the best practices and you can certainly ask questions of our panel about other best practices they'd",
    "start": "2750940",
    "end": "2756850"
  },
  {
    "text": "recommend so finally to wrap it up you know we've talked about how storage is foundational for building a data Lake so",
    "start": "2756850",
    "end": "2763630"
  },
  {
    "start": "2758000",
    "end": "2788000"
  },
  {
    "text": "we have a whole series of educational courses if you're intrigued by the idea of building a data Lake don't know a lot",
    "start": "2763630",
    "end": "2770560"
  },
  {
    "text": "about s3 AWS storage services and how to put them to use as well as all of our",
    "start": "2770560",
    "end": "2775990"
  },
  {
    "text": "other storages that may go into other tools you'd build around to data Lake so the education is out there so thanks",
    "start": "2775990",
    "end": "2783000"
  },
  {
    "text": "again for joining us and with that let's open it up for Q&A there is a microphone",
    "start": "2783000",
    "end": "2800550"
  },
  {
    "text": "here yeah hello Mike hello yeah a question on the storm usage did you guys",
    "start": "2800550",
    "end": "2807720"
  },
  {
    "text": "consider using lambda instead of storm and if you did not choose to use lambda",
    "start": "2807720",
    "end": "2813960"
  },
  {
    "text": "and why yeah we did think of using",
    "start": "2813960",
    "end": "2819570"
  },
  {
    "start": "2816000",
    "end": "2875000"
  },
  {
    "text": "lambda instead of storm but there's a few issues that we were afraid of using",
    "start": "2819570",
    "end": "2826050"
  },
  {
    "text": "lambda because lambda is I think it's mapped to a single shard in a Kinesis",
    "start": "2826050",
    "end": "2833460"
  },
  {
    "text": "stream the amount of concurrent lambdas that you can run and because we're running such a high throughput number of",
    "start": "2833460",
    "end": "2839520"
  },
  {
    "text": "events we were afraid that it wouldn't keep up with the speed and also we wanted to work with we wanted to do",
    "start": "2839520",
    "end": "2845430"
  },
  {
    "text": "quite a lot of things like work with our no sequel database and it does look quite a lot of processing so we were in",
    "start": "2845430",
    "end": "2852030"
  },
  {
    "text": "did it wouldn't be robust enough and didn't wouldn't give us a lot enough flexibility but if four simple things",
    "start": "2852030",
    "end": "2857460"
  },
  {
    "text": "that might be good it probably would be good enough Thanks no problem hi one",
    "start": "2857460",
    "end": "2864660"
  },
  {
    "text": "question you talked about converting your data sets into columnar format yeah",
    "start": "2864660",
    "end": "2870030"
  },
  {
    "text": "do you do any other quality checks or standardizations on the data yeah we do",
    "start": "2870030",
    "end": "2876420"
  },
  {
    "start": "2875000",
    "end": "2937000"
  },
  {
    "text": "the the data cleansing we do actually in storm right now so we have we hold in Aurora we hold for each event we hold",
    "start": "2876420",
    "end": "2884190"
  },
  {
    "text": "for each field exactly what it should hold let's say this should be a string or this should be certain data",
    "start": "2884190",
    "end": "2889680"
  },
  {
    "text": "validations and if it's not valid we basically throw it into an invalid bucket and then we have all kinds of",
    "start": "2889680",
    "end": "2895080"
  },
  {
    "text": "statistics about what's valid what's not valid so when it's written to s3 it's already valid and then like 99.9% of the",
    "start": "2895080",
    "end": "2903570"
  },
  {
    "text": "data works the the change of the format works fine because it's already cleaned do you do any remediation on the invalid data",
    "start": "2903570",
    "end": "2910620"
  },
  {
    "text": "or do you just kind of throw it out we store it for some time for a limited",
    "start": "2910620",
    "end": "2916200"
  },
  {
    "text": "period of time and we have events if it goes over a certain percentage of the amount of the per event and then we",
    "start": "2916200",
    "end": "2922079"
  },
  {
    "text": "manually look at it because it shouldn't happen two months so we have a certain threshold that we allow a certain amount",
    "start": "2922079",
    "end": "2929430"
  },
  {
    "text": "of invalid events but if it goes over that we manually check it thank you no problem this is a question",
    "start": "2929430",
    "end": "2939059"
  },
  {
    "start": "2937000",
    "end": "3009000"
  },
  {
    "text": "to Airbnb thanks for sharing your best practices regarding you know the",
    "start": "2939059",
    "end": "2944519"
  },
  {
    "text": "metadata caching where you're storing it on my sequel are you running into any",
    "start": "2944519",
    "end": "2950700"
  },
  {
    "text": "scalability issues as the number of objects and all that you eventually store in s3 increase yeah that's a good",
    "start": "2950700",
    "end": "2957150"
  },
  {
    "text": "question so for like so right now so we put the data into RDS instance so with",
    "start": "2957150",
    "end": "2964950"
  },
  {
    "text": "heaven grow into an a scalability issue but there's a kind of issue that because",
    "start": "2964950",
    "end": "2970559"
  },
  {
    "text": "all basically all the mappers or reducers gonna to contact my sicko for",
    "start": "2970559",
    "end": "2977099"
  },
  {
    "text": "the metadata so like the number of connections or bottleneck but we have a",
    "start": "2977099",
    "end": "2983099"
  },
  {
    "text": "mature solution inside Airbnb that we",
    "start": "2983099",
    "end": "2988200"
  },
  {
    "text": "said basically we set up the TP proxy to",
    "start": "2988200",
    "end": "2993509"
  },
  {
    "text": "our justice issue but later on we may consider like using more scalable TB",
    "start": "2993509",
    "end": "3000200"
  },
  {
    "text": "such as racket animal to Kalama DB to save this data um hi we have relatively",
    "start": "3000200",
    "end": "3011569"
  },
  {
    "text": "unstructured data living in s3 and have been using Athena to query it so if we",
    "start": "3011569",
    "end": "3017210"
  },
  {
    "text": "move to redshift spectrum can I expect improve performance if we you know added some structure so are you converting it",
    "start": "3017210",
    "end": "3027799"
  },
  {
    "text": "to any formats or it's just a text so you know it isn't parquet okay so we",
    "start": "3027799",
    "end": "3036440"
  },
  {
    "text": "have to take a look on what is your use case specifically so Athena is very good",
    "start": "3036440",
    "end": "3041869"
  },
  {
    "text": "for unstructured data so it are you physically issues currently with your parka format when",
    "start": "3041869",
    "end": "3048360"
  },
  {
    "text": "you do your query with well we do have certain queries that go past 30 minutes and athina times I see okay",
    "start": "3048360",
    "end": "3053850"
  },
  {
    "text": "so the best place if you could beat with me after this and we can talk one-on-one and we will see your use case and we'll decide whether we can check whether",
    "start": "3053850",
    "end": "3060060"
  },
  {
    "text": "structured format would be better than an unstructured format okay great thank you I have a micro question about the",
    "start": "3060060",
    "end": "3068730"
  },
  {
    "text": "use of s3 notifications we've always been cautious about this due to the fact that if you're running",
    "start": "3068730",
    "end": "3074670"
  },
  {
    "text": "up to tens of millions hundreds of millions of events even with however many nines there might be some missed",
    "start": "3074670",
    "end": "3081240"
  },
  {
    "text": "messages as have been discussion or concerned whoever's using that architecture okay so are you using",
    "start": "3081240",
    "end": "3092070"
  },
  {
    "start": "3089000",
    "end": "3145000"
  },
  {
    "text": "events to do something like populate a catalog like dynamo avoided doing that",
    "start": "3092070",
    "end": "3099210"
  },
  {
    "text": "because of some concerns and I wondered how you know others have dealt with that",
    "start": "3099210",
    "end": "3104360"
  },
  {
    "text": "yeah I mean oftentimes what people will do is you know because you're right event delivery isn't a hundred percent",
    "start": "3104360",
    "end": "3111990"
  },
  {
    "text": "guaranteed it's you know pretty close to that so we often see people you know",
    "start": "3111990",
    "end": "3117300"
  },
  {
    "text": "essentially doing a remediation okay I have the catalog potentially using something like object inventory report",
    "start": "3117300",
    "end": "3124290"
  },
  {
    "text": "once a day and then remediating that with the catalog and calling out",
    "start": "3124290",
    "end": "3130320"
  },
  {
    "text": "differences is probably a common practice there alright thank you it's a",
    "start": "3130320",
    "end": "3138810"
  },
  {
    "text": "team unless suited for structured data compared to racial spectrum so if you",
    "start": "3138810",
    "end": "3146220"
  },
  {
    "text": "what we typically recommend is if you have structured data with multiple",
    "start": "3146220",
    "end": "3152400"
  },
  {
    "text": "joints and large tables it's better to use redshift spectrum there and then if you have unstructured data and doing",
    "start": "3152400",
    "end": "3157890"
  },
  {
    "text": "ad-hoc queries that's where Athena does a better job again it depends totally on",
    "start": "3157890",
    "end": "3163620"
  },
  {
    "text": "your use case so if you have an essay on your account and we can work together to it's your use case and see what's",
    "start": "3163620",
    "end": "3168780"
  },
  {
    "text": "working better for you right so joins are better served with joins large table",
    "start": "3168780",
    "end": "3174450"
  },
  {
    "text": "yeah and scale is this exabytes of data similarly yeah large complex queries",
    "start": "3174450",
    "end": "3179670"
  },
  {
    "text": "often times redshift is a better choice okay the great thing is you know you are",
    "start": "3179670",
    "end": "3187890"
  },
  {
    "text": "using it against the same data sets and so you can you know do a trial you can",
    "start": "3187890",
    "end": "3193650"
  },
  {
    "text": "you know essentially with spectrum it allows you to do it in place so with in place data set so you can start there",
    "start": "3193650",
    "end": "3200760"
  },
  {
    "text": "and then load into redshift if you think that's going to give you a net benefit",
    "start": "3200760",
    "end": "3206220"
  },
  {
    "text": "right make sense thank you yes just a follow-up question to his thing like and",
    "start": "3206220",
    "end": "3213000"
  },
  {
    "text": "use the Athena versus s3 selected so these two are complementary products s3",
    "start": "3213000",
    "end": "3219240"
  },
  {
    "start": "3216000",
    "end": "3242000"
  },
  {
    "text": "select which is in preview right now is a way to efficiently retrieve a portion of the object that you have today Athena",
    "start": "3219240",
    "end": "3226410"
  },
  {
    "text": "I mean it just select from where whereas Athena has all the big data analytics engines and I've been querying",
    "start": "3226410",
    "end": "3233490"
  },
  {
    "text": "capabilities that are built-in so I would say these are complementary products and these are not replacing",
    "start": "3233490",
    "end": "3238950"
  },
  {
    "text": "each other yeah I mean yeah right so the",
    "start": "3238950",
    "end": "3244350"
  },
  {
    "start": "3242000",
    "end": "3381000"
  },
  {
    "text": "integration of Athena and select is going to come in the future yeah yeah no so to elaborate on that I mean",
    "start": "3244350",
    "end": "3250800"
  },
  {
    "text": "essentially Athena is a new API for accessing data where one at the at the",
    "start": "3250800",
    "end": "3256770"
  },
  {
    "text": "one select query corresponds to one object and so you need a higher-level",
    "start": "3256770",
    "end": "3262530"
  },
  {
    "text": "query engine to really take advantage of that but then when you have the plugins",
    "start": "3262530",
    "end": "3267720"
  },
  {
    "text": "to Athena which spectrum and or to select which spectrum and Athena will",
    "start": "3267720",
    "end": "3272970"
  },
  {
    "text": "use it will both accelerate the performance because s3 is only doing the",
    "start": "3272970",
    "end": "3278880"
  },
  {
    "text": "pre filtering and then only returning the results to the higher-level query engine and reduce cost because you're",
    "start": "3278880",
    "end": "3285750"
  },
  {
    "text": "processing less data",
    "start": "3285750",
    "end": "3289340"
  },
  {
    "text": "so the question was about how as three select in 2018 would play into reporting",
    "start": "3315010",
    "end": "3322810"
  },
  {
    "text": "you know so once again Athena is not a standalone tool it's essentially an API",
    "start": "3322810",
    "end": "3328240"
  },
  {
    "text": "that is doing essentially predicate push down to filter the data and only return",
    "start": "3328240",
    "end": "3333940"
  },
  {
    "text": "the you know filtered results to things like red shift register a phenol we have",
    "start": "3333940",
    "end": "3340600"
  },
  {
    "text": "seen particularly with more unstructured data types you know four hundred percent",
    "start": "3340600",
    "end": "3346210"
  },
  {
    "text": "improvement in performance in you know kind of the early preview benchmarking that we've done so it will definitely",
    "start": "3346210",
    "end": "3353080"
  },
  {
    "text": "improve query performance but remains to be seen you know how this will play into Athena",
    "start": "3353080",
    "end": "3360130"
  },
  {
    "text": "versus red shift versus red shift spectrum so John you meant select is not a standalone tool select sorry so if you",
    "start": "3360130",
    "end": "3369310"
  },
  {
    "text": "have so if you have files like an s3 you want to get them into red shift when would you copy them interrupt chief",
    "start": "3369310",
    "end": "3376030"
  },
  {
    "text": "versus using spectrum instead of copying them directly in red shift so one of the",
    "start": "3376030",
    "end": "3382480"
  },
  {
    "text": "use case you can think of is say you have hundreds of terabytes of data and you want to do the restore that restore",
    "start": "3382480",
    "end": "3388780"
  },
  {
    "text": "is introducing Layton sees right what you do is put those large tables in s3 and then you can spin up read-only",
    "start": "3388780",
    "end": "3395170"
  },
  {
    "text": "clusters in red shift so if you have a peak at that time you just pull data from the s3 from s3 to red shift and",
    "start": "3395170",
    "end": "3403990"
  },
  {
    "text": "then run your applications so that's one way one of the use cases there if you want to learn about more use cases we",
    "start": "3403990",
    "end": "3410290"
  },
  {
    "text": "can talk one-on-one right after this yeah oh yeah my questions for Viper I",
    "start": "3410290",
    "end": "3416080"
  },
  {
    "start": "3413000",
    "end": "3460000"
  },
  {
    "text": "have two questions for you so one is when you say you are fit scale at your data did you use any third-party tools and",
    "start": "3416080",
    "end": "3423220"
  },
  {
    "text": "then to do you have a way to convert the after scale it they drive back to clear no we didn't use any third-party tools",
    "start": "3423220",
    "end": "3430540"
  },
  {
    "text": "we just use our own hashing algorithm and the whole idea of using the",
    "start": "3430540",
    "end": "3437320"
  },
  {
    "text": "anonymization is so it can't be turned back otherwise it's not properly anonymized so it's a",
    "start": "3437320",
    "end": "3443470"
  },
  {
    "text": "one-way option okay thank you but yeah if you want to have a discussion of",
    "start": "3443470",
    "end": "3450100"
  },
  {
    "text": "tokenization it's obviously a hot topic of these days so we'd like to hear what your requirements are and you know so",
    "start": "3450100",
    "end": "3457630"
  },
  {
    "text": "feel free to reach out this is a question for everybody whose kiss so in",
    "start": "3457630",
    "end": "3464080"
  },
  {
    "start": "3460000",
    "end": "3524000"
  },
  {
    "text": "third party when you're bringing the third party and you said lambda allow you're triggering the lambda to validate",
    "start": "3464080",
    "end": "3469150"
  },
  {
    "text": "so row by row you're validating so all the data like that or were the the data",
    "start": "3469150",
    "end": "3476590"
  },
  {
    "text": "that were in the use case of the third party that is third party that we are partnering with or for example and",
    "start": "3476590",
    "end": "3482380"
  },
  {
    "text": "things like that so we are it should be sort of a struct structure data that we know that it should be of a certain",
    "start": "3482380",
    "end": "3488470"
  },
  {
    "text": "format so we are expecting this data so we know how to validate it so it's the",
    "start": "3488470",
    "end": "3495460"
  },
  {
    "text": "kushma is you're using Aurora storing the events so so you use all the data or",
    "start": "3495460",
    "end": "3501850"
  },
  {
    "text": "a hot date or say say you have s3 data Lake right you're you're asking it for",
    "start": "3501850",
    "end": "3507970"
  },
  {
    "text": "the from the third parties are we storing all the data coming in one part",
    "start": "3507970",
    "end": "3513700"
  },
  {
    "text": "it goes to Aurora and then remaining part going to s3 bucket sv we're moving",
    "start": "3513700",
    "end": "3518770"
  },
  {
    "text": "all the data in all the data in yeah so Aurora is using four what is the purpose",
    "start": "3518770",
    "end": "3524530"
  },
  {
    "text": "of Aurora there what is that sorry I didn't get that was purpose of using Aurora the the purpose of using the",
    "start": "3524530",
    "end": "3532030"
  },
  {
    "text": "lambda function I don't uh Oh rora the Aurora we don't use it for",
    "start": "3532030",
    "end": "3537190"
  },
  {
    "text": "the for the third parties we use it it's basically so we know the structure of",
    "start": "3537190",
    "end": "3542230"
  },
  {
    "text": "the data in Aurora we have all the fields so we don't necessarily if it's JSON then it's easier but we get also",
    "start": "3542230",
    "end": "3549100"
  },
  {
    "text": "let's say TSV format where we don't know it's not inside the fields themselves the the structure so we need somewhere",
    "start": "3549100",
    "end": "3555940"
  },
  {
    "text": "to have the the data on the data the metadata we use that for that Aurora for that tink no problem yes",
    "start": "3555940",
    "end": "3569220"
  },
  {
    "text": "pardon yeah",
    "start": "3576700",
    "end": "3581810"
  },
  {
    "text": "glacier select so so essentially glacier select you know uses the same sequel",
    "start": "3581810",
    "end": "3590210"
  },
  {
    "text": "semantics as s3 select but the big difference is with glacier obviously",
    "start": "3590210",
    "end": "3595880"
  },
  {
    "text": "this is cold archive data that traditionally you would have had to restore a dataset you know and then load",
    "start": "3595880",
    "end": "3602750"
  },
  {
    "text": "it into s3 and then be able to query it what glacier select allows you to do is",
    "start": "3602750",
    "end": "3608120"
  },
  {
    "text": "to essentially write sequel statements to data that's in glacier using the",
    "start": "3608120",
    "end": "3614330"
  },
  {
    "text": "glacier select API and then only the data that meets those statements the",
    "start": "3614330",
    "end": "3621950"
  },
  {
    "text": "objects that meet those statements will you know be able to be restored and",
    "start": "3621950",
    "end": "3627740"
  },
  {
    "text": "you'll do a glacier restore still selecting which glacier retrievals here",
    "start": "3627740",
    "end": "3633500"
  },
  {
    "text": "you want to use depending on how quickly you need the data but then what gets restored will just be the portions of",
    "start": "3633500",
    "end": "3640010"
  },
  {
    "text": "the objects that meet those glacier select query statements and then that will be you know loaded into your",
    "start": "3640010",
    "end": "3646220"
  },
  {
    "text": "glacier restore location and you can do whatever else you want to do with that",
    "start": "3646220",
    "end": "3651860"
  },
  {
    "text": "data so it'll essentially filter the data in glacier and then when you restore it it's only restoring the filtered results",
    "start": "3651860",
    "end": "3658400"
  },
  {
    "text": "but it's still asynchronous in the same manner the glacier works you can use",
    "start": "3658400",
    "end": "3669260"
  },
  {
    "text": "standard sequel select statements like from where yes so but s3 select and",
    "start": "3669260",
    "end": "3676400"
  },
  {
    "text": "glacier select will support the same sequel semantics so we are out of time",
    "start": "3676400",
    "end": "3682070"
  },
  {
    "text": "but we'll be hanging out here or outside if you have questions just come see us there",
    "start": "3682070",
    "end": "3688540"
  }
]