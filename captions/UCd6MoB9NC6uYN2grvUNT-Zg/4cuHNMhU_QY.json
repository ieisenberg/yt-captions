[
  {
    "text": "- All right.",
    "start": "300",
    "end": "1133"
  },
  {
    "text": "Hey, folks, Emily here.",
    "start": "1133",
    "end": "2370"
  },
  {
    "text": "Today, you are gonna get started",
    "start": "2370",
    "end": "3840"
  },
  {
    "text": "with pre-training your own\nfoundation models on SageMaker.",
    "start": "3840",
    "end": "7560"
  },
  {
    "text": "Let's jump in.",
    "start": "7560",
    "end": "9000"
  },
  {
    "text": "So you'll see here I\nhave SageMaker Studio.",
    "start": "9000",
    "end": "11340"
  },
  {
    "text": "I'm gonna open my SageMaker\nStudio lab environment,",
    "start": "11340",
    "end": "15390"
  },
  {
    "text": "which is right here.",
    "start": "15390",
    "end": "17190"
  },
  {
    "text": "In this example, I am\ngoing to be pre-training",
    "start": "17190",
    "end": "21600"
  },
  {
    "text": "a Llama 2 70-billion parameter\nmodel using SageMaker",
    "start": "21600",
    "end": "26490"
  },
  {
    "text": "and running on Trainium instances.",
    "start": "26490",
    "end": "28650"
  },
  {
    "text": "The example for this\nis available publicly.",
    "start": "28650",
    "end": "31439"
  },
  {
    "text": "So again, Arun, kudos to\nyou for developing this",
    "start": "31440",
    "end": "35190"
  },
  {
    "text": "and all the other Amazonians\nwho helped put this together.",
    "start": "35190",
    "end": "38519"
  },
  {
    "text": "Thank you.",
    "start": "38520",
    "end": "39540"
  },
  {
    "text": "So we're gonna use this notebook.",
    "start": "39540",
    "end": "41370"
  },
  {
    "text": "Fortunately, I already have it cloned.",
    "start": "41370",
    "end": "43440"
  },
  {
    "text": "Let's check it out.",
    "start": "43440",
    "end": "45000"
  },
  {
    "text": "So, again, this is Jupyter Lab",
    "start": "45000",
    "end": "47850"
  },
  {
    "text": "that's running inside of SageMaker Studio.",
    "start": "47850",
    "end": "50730"
  },
  {
    "text": "You'll see I have this\ndirectory here, pre-train llama,",
    "start": "50730",
    "end": "55140"
  },
  {
    "text": "with my dataset, my\nscripts, and my notebook.",
    "start": "55140",
    "end": "59010"
  },
  {
    "text": "Let's check out the notebook.",
    "start": "59010",
    "end": "61589"
  },
  {
    "text": "So in the notebook,",
    "start": "61590",
    "end": "62640"
  },
  {
    "text": "first you're gonna pip\ninstall your packages,",
    "start": "62640",
    "end": "66330"
  },
  {
    "text": "again locally to the notebook.",
    "start": "66330",
    "end": "68580"
  },
  {
    "text": "And then we're gonna download the dataset.",
    "start": "68580",
    "end": "72030"
  },
  {
    "text": "So this is the wiki corpus,",
    "start": "72030",
    "end": "74310"
  },
  {
    "text": "which we're gonna download,",
    "start": "74310",
    "end": "75720"
  },
  {
    "text": "and then actually tokenize locally.",
    "start": "75720",
    "end": "78300"
  },
  {
    "text": "So when you are tokenizing this locally,",
    "start": "78300",
    "end": "80580"
  },
  {
    "text": "just make sure you have\na large enough instance.",
    "start": "80580",
    "end": "83550"
  },
  {
    "text": "I'm running a C5.18 XL\nin Jupyter Lab right now",
    "start": "83550",
    "end": "88550"
  },
  {
    "text": "to give me enough power\nto tokenize those files.",
    "start": "89670",
    "end": "93540"
  },
  {
    "text": "So we do the tokenizing locally.",
    "start": "93540",
    "end": "96090"
  },
  {
    "text": "Once the tokenizing is done,",
    "start": "96090",
    "end": "98009"
  },
  {
    "text": "you're gonna upload\nthat to your S3 bucket.",
    "start": "98010",
    "end": "101520"
  },
  {
    "text": "Once that's completed,",
    "start": "101520",
    "end": "103079"
  },
  {
    "text": "then you can just run the training job.",
    "start": "103080",
    "end": "105420"
  },
  {
    "text": "You'll see I have all the necessary",
    "start": "105420",
    "end": "107549"
  },
  {
    "text": "hyperparameters right here,",
    "start": "107550",
    "end": "109080"
  },
  {
    "text": "including my overall world size,",
    "start": "109080",
    "end": "110790"
  },
  {
    "text": "processes per node,",
    "start": "110790",
    "end": "112530"
  },
  {
    "text": "data, tensor, and\npipeline parallel degrees.",
    "start": "112530",
    "end": "115979"
  },
  {
    "text": "You can bring other\nhyperparameters here as well",
    "start": "115980",
    "end": "119020"
  },
  {
    "text": "in order to set this, as necessary.",
    "start": "120180",
    "end": "123090"
  },
  {
    "text": "The base docker image is\na deep learning container,",
    "start": "123090",
    "end": "126719"
  },
  {
    "text": "so that's an AWS-managed\ndeep learning container",
    "start": "126720",
    "end": "130110"
  },
  {
    "text": "for PyTorch training using NeuronX,",
    "start": "130110",
    "end": "133470"
  },
  {
    "text": "which is what we're gonna leverage",
    "start": "133470",
    "end": "134760"
  },
  {
    "text": "in order to train on Trainium.",
    "start": "134760",
    "end": "137430"
  },
  {
    "text": "And so here is my PyTorch estimator.",
    "start": "137430",
    "end": "139920"
  },
  {
    "text": "Again, I'm calling the\nscript run_llama_nxd.",
    "start": "139920",
    "end": "143310"
  },
  {
    "text": "You can download that\nscript from the GitHub repo",
    "start": "143310",
    "end": "146010"
  },
  {
    "text": "and do this with me.",
    "start": "146010",
    "end": "147209"
  },
  {
    "text": "Again, I'm using a Trainium 32 XL machine,",
    "start": "147210",
    "end": "151590"
  },
  {
    "text": "so that's quite a few accelerator chips",
    "start": "151590",
    "end": "154200"
  },
  {
    "text": "running on this instance here.",
    "start": "154200",
    "end": "156150"
  },
  {
    "text": "Again, the training script is local.",
    "start": "156150",
    "end": "158939"
  },
  {
    "text": "And then when we call a\nmodel.fit here, estimator.fit,",
    "start": "158940",
    "end": "163530"
  },
  {
    "text": "this spins up a set of new machines",
    "start": "163530",
    "end": "165870"
  },
  {
    "text": "using SageMaker Training,",
    "start": "165870",
    "end": "167879"
  },
  {
    "text": "invokes the training\nscript on those machines,",
    "start": "167880",
    "end": "171420"
  },
  {
    "text": "and then runs the job.",
    "start": "171420",
    "end": "173550"
  },
  {
    "text": "You'll see I have the logs for this job",
    "start": "173550",
    "end": "176730"
  },
  {
    "text": "available in the console.",
    "start": "176730",
    "end": "178860"
  },
  {
    "text": "Also available in SageMaker Studio.",
    "start": "178860",
    "end": "181200"
  },
  {
    "text": "You can view the job metadata,",
    "start": "181200",
    "end": "183090"
  },
  {
    "text": "you can view the hyperparameters,",
    "start": "183090",
    "end": "185700"
  },
  {
    "text": "you can view the logs for this.",
    "start": "185700",
    "end": "187890"
  },
  {
    "text": "And then, again, this is running",
    "start": "187890",
    "end": "189390"
  },
  {
    "text": "using the SageMaker distributed\ntraining capabilities.",
    "start": "189390",
    "end": "192630"
  },
  {
    "text": "So the node is healthy, it's\nactive, it's fun to use.",
    "start": "192630",
    "end": "197630"
  },
  {
    "text": "And so that is getting started",
    "start": "197670",
    "end": "199920"
  },
  {
    "text": "with pre-training foundation\nmodels on SageMaker.",
    "start": "199920",
    "end": "202410"
  },
  {
    "text": "Hope you enjoyed it.",
    "start": "202410",
    "end": "203243"
  },
  {
    "text": "In the next one, we're gonna get started",
    "start": "203243",
    "end": "204750"
  },
  {
    "text": "with customizing and fine tuning\nyour own foundation models.",
    "start": "204750",
    "end": "208470"
  },
  {
    "text": "Thanks.",
    "start": "208470",
    "end": "209303"
  }
]