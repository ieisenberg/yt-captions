[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "hello all uh we are here to present uh about the GoPro how GoPro collects data",
    "start": "1599",
    "end": "7520"
  },
  {
    "text": "and uh a GoPro data Big Data platform the talk has been sponsored by Tableau",
    "start": "7520",
    "end": "13639"
  },
  {
    "text": "and how this talk will be structured is we'll start with uh what GoPro does what",
    "start": "13639",
    "end": "19760"
  },
  {
    "text": "has been a data Journey then we'll talk uh in detail about gopro's data",
    "start": "19760",
    "end": "24800"
  },
  {
    "text": "architecture David will take you guys through that and finally Christian is going to talk about about some",
    "start": "24800",
    "end": "30480"
  },
  {
    "text": "visualizations and some uh brief demo to talk about how we use visualizations we",
    "start": "30480",
    "end": "36600"
  },
  {
    "text": "use both Tableau and AWS in a way a bit different way than others and we will take you guys through that in the end",
    "start": "36600",
    "end": "43480"
  },
  {
    "text": "there will be a Q&A and we will uh take any questions that you may",
    "start": "43480",
    "end": "50079"
  },
  {
    "text": "have so this is just a brief slide about us and what we do at",
    "start": "50320",
    "end": "56199"
  },
  {
    "text": "GoPro so so before we go get started about",
    "start": "56199",
    "end": "62199"
  },
  {
    "start": "58000",
    "end": "69000"
  },
  {
    "text": "data at GoPro and what has been a background and how the company got",
    "start": "62199",
    "end": "67520"
  },
  {
    "text": "started so when we got at GoPro GoPro was mainly a camera company and now we",
    "start": "67520",
    "end": "74880"
  },
  {
    "start": "69000",
    "end": "124000"
  },
  {
    "text": "are slowly transitioning to be a software and a platform company and how",
    "start": "74880",
    "end": "80280"
  },
  {
    "text": "a data analytics was done like how was the initial cameras developed was based",
    "start": "80280",
    "end": "85360"
  },
  {
    "text": "a lot on Word of Mouth so as soon as somebody will build a new prototype of a camera we will give to the GoPro",
    "start": "85360",
    "end": "92360"
  },
  {
    "text": "athletes they will go surf with the cameras try to figure out how people are",
    "start": "92360",
    "end": "97399"
  },
  {
    "text": "using the camera what is working what is not working go and iterate a lot of the",
    "start": "97399",
    "end": "102520"
  },
  {
    "text": "data analytics was based on Word of Mouth and our founder Nick Woodman and but over the last couple of years we are",
    "start": "102520",
    "end": "109320"
  },
  {
    "text": "slowly transitioning to become a more a software driven company a more data driven company and we will talk about",
    "start": "109320",
    "end": "115920"
  },
  {
    "text": "all the different products that we have and how we manage the data that is collected by all those",
    "start": "115920",
    "end": "124360"
  },
  {
    "start": "124000",
    "end": "211000"
  },
  {
    "text": "products this is some of the different products that we have right now a lot of",
    "start": "124360",
    "end": "129520"
  },
  {
    "text": "you may not know but GoPro now has a lot of software products we have if you buy",
    "start": "129520",
    "end": "135319"
  },
  {
    "text": "a GoPro camera you you basically definitely may have known about the capture app which helps you connect to",
    "start": "135319",
    "end": "140800"
  },
  {
    "text": "the camera and uh basically capture photos and see stuff we also have a",
    "start": "140800",
    "end": "147040"
  },
  {
    "text": "couple of editing apps uh which allow you to create automated videos and",
    "start": "147040",
    "end": "152280"
  },
  {
    "text": "automated editing in a minute or so it's called quick for mobile or quick for desktop and finally we have our VR app",
    "start": "152280",
    "end": "159080"
  },
  {
    "text": "also which allows you to capture and view VR experiences and definitely what we are",
    "start": "159080",
    "end": "165280"
  },
  {
    "text": "known for our cameras our spherical units and as you can see like this is a",
    "start": "165280",
    "end": "170560"
  },
  {
    "text": "truly Internet of Things play a lot of these apps and a lot of these cameras are do connect to the internet now the",
    "start": "170560",
    "end": "177879"
  },
  {
    "text": "latest Hero 5 camera also uploads your data to the cloud directly for example so what we have here is we had a growing",
    "start": "177879",
    "end": "185920"
  },
  {
    "text": "data needs we actually have a Internet of Things play here and we collect all the data and it is several billions of",
    "start": "185920",
    "end": "192640"
  },
  {
    "text": "events every day and we'll talk about how what are the challenges and just uh",
    "start": "192640",
    "end": "198440"
  },
  {
    "text": "to allay any concerns we anonymize all data we only capture data after we get",
    "start": "198440",
    "end": "203959"
  },
  {
    "text": "permission from you",
    "start": "203959",
    "end": "207799"
  },
  {
    "start": "211000",
    "end": "294000"
  },
  {
    "text": "so what are the unique challenges for dated GoPro and this a lot of these things will be covered again when uh",
    "start": "211920",
    "end": "218519"
  },
  {
    "text": "David and Christian cover the different uh architecture details so we have a",
    "start": "218519",
    "end": "224239"
  },
  {
    "text": "variety of data as we saw in our last slide both hardware and software products and our challenge is to",
    "start": "224239",
    "end": "229519"
  },
  {
    "text": "basically mesh the data basically collect all the data and bring it in one place so that we can understand how our",
    "start": "229519",
    "end": "235400"
  },
  {
    "text": "products are doing and how our customers are using them we have a variety of dat injection mechanisms both real time as",
    "start": "235400",
    "end": "242959"
  },
  {
    "text": "well a batch Pipeline and we'll talk about that and I think the next two",
    "start": "242959",
    "end": "248439"
  },
  {
    "text": "point points are something more important and people don't really talk about that in the Big Data space about",
    "start": "248439",
    "end": "254480"
  },
  {
    "text": "complex Transformations it is very important for all of us to bring the data in one place and also uh aggregate",
    "start": "254480",
    "end": "262280"
  },
  {
    "text": "it and structure it and transform it so that it makes sense for our analytics team to analyze the data and and we will",
    "start": "262280",
    "end": "270960"
  },
  {
    "text": "talk a bit about that in the next slide and uh as you can see we have a lot of different products we all have to bring",
    "start": "270960",
    "end": "276080"
  },
  {
    "text": "the data together in one place and blend it in a way that it is available for",
    "start": "276080",
    "end": "281120"
  },
  {
    "text": "analytics and also handle privacy and an anonymization uh according to the",
    "start": "281120",
    "end": "286360"
  },
  {
    "text": "different laws which are previl in different",
    "start": "286360",
    "end": "290360"
  },
  {
    "text": "countries so how does AWS help us so we",
    "start": "293800",
    "end": "299160"
  },
  {
    "text": "mainly and we'll talk about the different Technologies is like definitely faster lead times uh when",
    "start": "299160",
    "end": "304720"
  },
  {
    "text": "GoPro is an all AWS environment we started on AWS and it definitely helps",
    "start": "304720",
    "end": "310199"
  },
  {
    "text": "us procure Hardware faster we can uh try new technologies months versus days and",
    "start": "310199",
    "end": "317080"
  },
  {
    "text": "also uh as you may know like just like any consumer electronics company we have Peak demands times in Christmas and",
    "start": "317080",
    "end": "324240"
  },
  {
    "text": "holidays so it helps us scale up and scale down also very quickly and David",
    "start": "324240",
    "end": "329280"
  },
  {
    "text": "is going to cover some details there and uh we'll talk about how Tableau helps us",
    "start": "329280",
    "end": "335919"
  },
  {
    "start": "332000",
    "end": "429000"
  },
  {
    "text": "uh chrisan is going through go through some demos about that uh these are some",
    "start": "335919",
    "end": "341000"
  },
  {
    "text": "of the things that we have noticed in how Tableau helps us become a more data",
    "start": "341000",
    "end": "346039"
  },
  {
    "text": "driven company definitely helps us go deep with data uh we definitely Data",
    "start": "346039",
    "end": "353199"
  },
  {
    "text": "Insights need to be visual a lot of our users are business users they're not very technology oriented so so Tableau",
    "start": "353199",
    "end": "360160"
  },
  {
    "text": "allows us to visually see how the data how people are using the products and",
    "start": "360160",
    "end": "366080"
  },
  {
    "text": "figure out patterns and uh do analysis the other point which I briefly talked",
    "start": "366080",
    "end": "371400"
  },
  {
    "text": "about data is context so that is something which at GoPro we have done a lot of uh we have a standard event",
    "start": "371400",
    "end": "378199"
  },
  {
    "text": "tonomy across all our products and we try to transform all the data that comes in according to that uniform even",
    "start": "378199",
    "end": "384960"
  },
  {
    "text": "taxonomy so that we can uh have a common definition and a common understanding of",
    "start": "384960",
    "end": "390560"
  },
  {
    "text": "different events and different user flows inside the company and one extra",
    "start": "390560",
    "end": "397039"
  },
  {
    "text": "thing that we do which is maybe different from other companies is we use Tableau a lot for data monitoring and",
    "start": "397039",
    "end": "404039"
  },
  {
    "text": "data QA also uh we because as insights are visual uh rather than QA people",
    "start": "404039",
    "end": "411680"
  },
  {
    "text": "going and uh really testing the data out like whether the a certain event is",
    "start": "411680",
    "end": "417240"
  },
  {
    "text": "coming through or not we basically use T to tell us build charts to see how the",
    "start": "417240",
    "end": "424240"
  },
  {
    "text": "data pipelines are working and David is going to talk about that and that's it",
    "start": "424240",
    "end": "431599"
  },
  {
    "start": "429000",
    "end": "453000"
  },
  {
    "text": "uh this is our Moto uh go is definitely a very cool company to work for if you",
    "start": "431599",
    "end": "436720"
  },
  {
    "text": "want to come and work for us and I will hand it over to David to talk about a",
    "start": "436720",
    "end": "442400"
  },
  {
    "text": "data architecture and we will uh take question in the",
    "start": "442400",
    "end": "447680"
  },
  {
    "text": "end great uh thank you Goro for getting us started I'm going to let here Jump",
    "start": "448120",
    "end": "453199"
  },
  {
    "text": "Ahead here I'm not going try that clicker I really screw things up I'm sure if I use that um so Dave wyers um",
    "start": "453199",
    "end": "459440"
  },
  {
    "text": "Big Data architect within the data science and engineering team at GoPro um just to give you a little bit of my",
    "start": "459440",
    "end": "464720"
  },
  {
    "text": "background um I spent actually guess two decades building um backend systems um",
    "start": "464720",
    "end": "470840"
  },
  {
    "text": "everything from e-commerce backends to search relevance and ranking engines um",
    "start": "470840",
    "end": "476440"
  },
  {
    "text": "and then always we're always dealing with data processing data warehouse housing and so that's what I'll be talking about today is about our",
    "start": "476440",
    "end": "482599"
  },
  {
    "text": "architecture so we're going to take a little bit of a deeper technical Deep dive here so we'll go ahead and kind of",
    "start": "482599",
    "end": "488280"
  },
  {
    "start": "483000",
    "end": "879000"
  },
  {
    "text": "Jump Right In Here one of the first things I wanted to to mention if you take a look at our high level architecture diagram on the screens here",
    "start": "488280",
    "end": "495240"
  },
  {
    "text": "is our data sources um so we have uh both streaming data sources and batch",
    "start": "495240",
    "end": "501199"
  },
  {
    "text": "data sources it's what people today tend to call lamb architecture um the the formats of the",
    "start": "501199",
    "end": "508240"
  },
  {
    "text": "data are wildly different um when you look at that in the upper leftand corner that streaming ingestion cluster that is",
    "start": "508240",
    "end": "515360"
  },
  {
    "text": "coming from devices that's as gor was referring to typical iot data it's",
    "start": "515360",
    "end": "520640"
  },
  {
    "text": "coming from cameras um actual the GoPro cameras and so forth such as our session here um and it also comes from from our",
    "start": "520640",
    "end": "528000"
  },
  {
    "text": "drone uh Karma and then also from our software applications so one of the things that people often times don't",
    "start": "528000",
    "end": "533880"
  },
  {
    "text": "realize with GoPro is we're not just a a manufacturing a hardware Manufacturing Company where we're actually have a huge",
    "start": "533880",
    "end": "539839"
  },
  {
    "text": "very large software uh division within it and we we produce a lot of software and the best part is most of it's",
    "start": "539839",
    "end": "545880"
  },
  {
    "text": "consumer facing and it's completely free so it's actually a good price um so it allows folks to basically take the",
    "start": "545880",
    "end": "551959"
  },
  {
    "text": "content off of their cameras people like myself who don't know anything about video editing I can write all kinds of",
    "start": "551959",
    "end": "558480"
  },
  {
    "text": "uh spark streaming jobs to to um process data in real time but I can't make a video edit for the life of me but with a",
    "start": "558480",
    "end": "565360"
  },
  {
    "text": "lot our software you can and all all of those applications are constantly sent us data so some of them um they just",
    "start": "565360",
    "end": "572040"
  },
  {
    "text": "rapid fire as events as buttons are clicked and so forth they're rapid fire sending us events to a an endpoint that",
    "start": "572040",
    "end": "578360"
  },
  {
    "text": "we have and I'll go into a little bit deeper in a minute here another slide on that some of them batch up that data it",
    "start": "578360",
    "end": "584160"
  },
  {
    "text": "can be Json data it can be gzipped um it can be batched it can be to single events and then when you're dealing with",
    "start": "584160",
    "end": "590839"
  },
  {
    "text": "these devices these have very limited processing power and storage capabilities obviously and so we use a",
    "start": "590839",
    "end": "597440"
  },
  {
    "text": "proprietary binary format um which makes things very interesting on the ingestion side and they're very very sparse events",
    "start": "597440",
    "end": "603440"
  },
  {
    "text": "they don't have any state information which you have to rebuild so we deal with side on the batch side and that's",
    "start": "603440",
    "end": "610040"
  },
  {
    "text": "in that um lower leftand corner on your screens there that is basically third party data so that's your Erp data",
    "start": "610040",
    "end": "617320"
  },
  {
    "text": "that's your CRM data web analytics social media um all of those which are",
    "start": "617320",
    "end": "623760"
  },
  {
    "text": "thirdparty companies and so we're at the mercy of them when they provide us data",
    "start": "623760",
    "end": "629079"
  },
  {
    "text": "um and have to work on their schedule and generally speaking most of that data it's available maybe once a day",
    "start": "629079",
    "end": "634320"
  },
  {
    "text": "sometimes twice a day there are some of the services that I'll give it to you every hour so but the Cadence is very different for batch oriented whereas on",
    "start": "634320",
    "end": "641000"
  },
  {
    "text": "the the streaming stuff on the upper le- hand side that's like every two anywhere from every two seconds to every five",
    "start": "641000",
    "end": "646399"
  },
  {
    "text": "minutes it's much quicker so I just want to kind of talk about data sources there a bit the next thing I wanted to kind of",
    "start": "646399",
    "end": "653040"
  },
  {
    "text": "show you on this uh diagram is really the evolution of our architecture um and that's where AWS has helped us a ton um",
    "start": "653040",
    "end": "660680"
  },
  {
    "text": "you can see like the dotted line where it says original cluster when we started it was I'd say your typical data Lake",
    "start": "660680",
    "end": "666959"
  },
  {
    "text": "architecture which means you're doing the way I break it down is there's three different workloads you're doing data",
    "start": "666959",
    "end": "672440"
  },
  {
    "text": "ingestion um so you've got streaming data sources you got batch data sources you want to ingest that data as fast as",
    "start": "672440",
    "end": "677639"
  },
  {
    "text": "possible you're doing ETL right because you got to transform that data into a common format you want to join together",
    "start": "677639",
    "end": "684560"
  },
  {
    "text": "data as much as possible so that the end user queries don't have to do all kinds of joining you will flatten things out you want to summarize them you want to",
    "start": "684560",
    "end": "690600"
  },
  {
    "text": "aggregate them so we call that ETL so that's our second function main function and then the third one is lifting that",
    "start": "690600",
    "end": "695800"
  },
  {
    "text": "data essentially off of disk right got a big distributed system and getting it to your analytics applications in this case",
    "start": "695800",
    "end": "702040"
  },
  {
    "text": "it's mostly Tableau for folks that are doing ad hoc um exploratory stuff it might be Hue which is the hudo UI um and",
    "start": "702040",
    "end": "709639"
  },
  {
    "text": "then for some data scientists it might be just straight up Python and R code so those are kind of the three workloads",
    "start": "709639",
    "end": "715160"
  },
  {
    "text": "when we began we had that one cluster and you can imagine what would happen analysts would come in hey let's take",
    "start": "715160",
    "end": "721959"
  },
  {
    "text": "this 100 billion Road table take this other 100 billion Road table and do a cartisian product something they",
    "start": "721959",
    "end": "727279"
  },
  {
    "text": "shouldn't do but they did it and instantly kills your htfs kills your IO",
    "start": "727279",
    "end": "732880"
  },
  {
    "text": "can even consume CPU and then your ingestion starts to fall behind now you can you can use yarn for those that are",
    "start": "732880",
    "end": "739399"
  },
  {
    "text": "familiar with Hadoop you can use Linux c groups and some things like that and basically uh govern the io resources but",
    "start": "739399",
    "end": "747199"
  },
  {
    "text": "the nice thing with AWS is it's light easier just to actually separate those workloads with physical clusters so",
    "start": "747199",
    "end": "753320"
  },
  {
    "text": "that's what we did and so when you take a look at this uh architecture diagram again you'll see we've got three",
    "start": "753320",
    "end": "758639"
  },
  {
    "text": "clusters up our left hand side is our streaming ingestion cluster um in the middle we've got our ETL cluster and",
    "start": "758639",
    "end": "765480"
  },
  {
    "text": "then uh towards the right there is our secure data Mark and so we do streaming ingestion we use spark streaming um and",
    "start": "765480",
    "end": "772920"
  },
  {
    "text": "I'll go a little bit deeper in another slide here and talk about the rationalization of the the technology Stacks here but we use spark streaming",
    "start": "772920",
    "end": "778800"
  },
  {
    "text": "we use kofka which is a distributed Pub sub system use hbas to do some on the-fly aggregation and then of course",
    "start": "778800",
    "end": "785680"
  },
  {
    "text": "we have a restful um endpoint that sits in front of that for all of those U streaming events that come in from our",
    "start": "785680",
    "end": "791760"
  },
  {
    "text": "iot devices um on the Lower left-and Side that's our batch induction framework it is just a good old Java",
    "start": "791760",
    "end": "799399"
  },
  {
    "text": "application framework ingest data directly into our ETL cluster the etail cluster honestly that's really just good",
    "start": "799399",
    "end": "805800"
  },
  {
    "text": "old vanilla like Hive uses a map reduce execution engine you're talking a Cadence there about the fastest it runs",
    "start": "805800",
    "end": "812760"
  },
  {
    "text": "is about once an hour or so and honestly a lot of that stuff is more like once every 24 hours because again we're at",
    "start": "812760",
    "end": "818240"
  },
  {
    "text": "the mercy of those third party providers when they make that data available but at that point we're bringing together",
    "start": "818240",
    "end": "823600"
  },
  {
    "text": "all of the the streaming and the batch data sources we're doing the aggregations we're doing the joins and",
    "start": "823600",
    "end": "828800"
  },
  {
    "text": "then more importantly we take um the the Json which we've converted everything to Jason at that point um the stuff that we",
    "start": "828800",
    "end": "835759"
  },
  {
    "text": "get before yeah can be anything it can be proprietary binary formats it can be",
    "start": "835759",
    "end": "840800"
  },
  {
    "text": "a very nested Json format it can be XML it can be CSV we flattened it all out",
    "start": "840800",
    "end": "846839"
  },
  {
    "text": "into nice flat Json uh files to that point and one of the first things we do with the the ETL clusters is we actually",
    "start": "846839",
    "end": "853160"
  },
  {
    "text": "convert those to a column in a format called par um and we we'll take a deeper dive in a minute here into that and then",
    "start": "853160",
    "end": "859240"
  },
  {
    "text": "the secure data Mart is where our end users generally speaking our lingua franka is SQL that's what pretty much",
    "start": "859240",
    "end": "865480"
  },
  {
    "text": "everybody uses if you're using Tableau um most of all of our t analysts are are definitely um SQL junkies um same thing",
    "start": "865480",
    "end": "873639"
  },
  {
    "text": "with Hue and if you're writing python on R code again you got to know SQL to to pull data out of our",
    "start": "873639",
    "end": "879000"
  },
  {
    "start": "879000",
    "end": "1039000"
  },
  {
    "text": "platform so let's just start to take a little bit of a deeper dive here so kind of go down another layer of the onion",
    "start": "879000",
    "end": "884320"
  },
  {
    "text": "here so again three clusters data ingestion ETL data delivery we're going to start by talking about our our data",
    "start": "884320",
    "end": "891600"
  },
  {
    "text": "induction cluster here so architecture here is uh again uses an elb elastic",
    "start": "891600",
    "end": "897720"
  },
  {
    "text": "load balancer which is provided by AWS um the magic that it does is basically",
    "start": "897720",
    "end": "903639"
  },
  {
    "text": "you can write your wrestful service we happen to be a Java and schol Shop so we write um JV servlets have it run in a",
    "start": "903639",
    "end": "910720"
  },
  {
    "text": "tomcat container you virtualize it as an Ami you set up um an auto scale group",
    "start": "910720",
    "end": "917600"
  },
  {
    "text": "and Amazon takes care of the rest for you you give it maybe a minimum number of nodes that you want to start off with",
    "start": "917600",
    "end": "923480"
  },
  {
    "text": "and we have a very cyclical um curve to our load during the week which is sort",
    "start": "923480",
    "end": "928759"
  },
  {
    "text": "of I'd say a shifted e-commerce curve for those who've worked in the e-commerce business for us basically we",
    "start": "928759",
    "end": "935279"
  },
  {
    "text": "start to get heavy uh load on our systems on the weekends which makes sense people are out with their GoPros",
    "start": "935279",
    "end": "941839"
  },
  {
    "text": "maybe they're taking some videos while they're surfing maybe they with their kids going skiing they plug in the",
    "start": "941839",
    "end": "947480"
  },
  {
    "text": "computer plug in this either to a computer or connected to a mobile device on Friday nights Saturday nights Sunday",
    "start": "947480",
    "end": "953240"
  },
  {
    "text": "mornings and it will give us fully anonymized logs um and so we start to see a spike there uh AWS will",
    "start": "953240",
    "end": "960000"
  },
  {
    "text": "automatically scale up our um via the auto scale group number of Tomcat servers and then we're good to go um",
    "start": "960000",
    "end": "967560"
  },
  {
    "text": "we've got kofka sitting um behind our Tomcat servers what we use kofka for is",
    "start": "967560",
    "end": "972959"
  },
  {
    "text": "basically we we shove all those logs in there as fast as we can um next slide",
    "start": "972959",
    "end": "978240"
  },
  {
    "text": "we'll talk a little little bit deeper about kofin some of those things but in a nutshell for for folks who aren't that",
    "start": "978240",
    "end": "984040"
  },
  {
    "text": "familiar with it it has just amazing right through put take about three or four commodity nodes um you can get a",
    "start": "984040",
    "end": "990880"
  },
  {
    "text": "million rights per second which is which is pretty insane um probably about 10 times what you get with a traditional",
    "start": "990880",
    "end": "996199"
  },
  {
    "text": "messaging system um very strong ordering of policy for the messages which means",
    "start": "996199",
    "end": "1001319"
  },
  {
    "text": "uh closer to exactly once processing so we don't get duplication um shove those logs directly into kofka and then we",
    "start": "1001319",
    "end": "1008160"
  },
  {
    "text": "kind of go through this um roundabout of spark streaming jobs that are pulling logs off of kofka um maybe doing some",
    "start": "1008160",
    "end": "1015519"
  },
  {
    "text": "flattening of the various events they stick them back into kofka and then other spark streaming jobs we'll pull",
    "start": "1015519",
    "end": "1021560"
  },
  {
    "text": "them off continue to do processing until we have gotten the uh actual events nice",
    "start": "1021560",
    "end": "1027199"
  },
  {
    "text": "and flattened out and we write them to hdfs um and then we have a schedule job",
    "start": "1027199",
    "end": "1032880"
  },
  {
    "text": "that about every 30 minutes we'll start to copy those to the ETL cluster so let",
    "start": "1032880",
    "end": "1037959"
  },
  {
    "text": "I want to talk a little bit more about um basically that interaction between kofka and Spark there so you saw that",
    "start": "1037959",
    "end": "1043600"
  },
  {
    "start": "1039000",
    "end": "1137000"
  },
  {
    "text": "there was kind of that that circular pattern spark jobs pulling logs off of kofka then sticking them back into kofka",
    "start": "1043600",
    "end": "1049480"
  },
  {
    "text": "the reason why we do that is is basically described here in this diagram and and what we're doing here is we have",
    "start": "1049480",
    "end": "1055200"
  },
  {
    "text": "a reusable set of spark streaming jobs at the very kind of root of that tree of of spark jobs um you'll have very",
    "start": "1055200",
    "end": "1063120"
  },
  {
    "text": "generic jobs it'll do things like we take pii very seriously at GoPro um we're aware of all the international",
    "start": "1063120",
    "end": "1068720"
  },
  {
    "text": "laws and so forth with those things and so the first thing we do is we take care of all the pii we remove we rehash we do",
    "start": "1068720",
    "end": "1075120"
  },
  {
    "text": "whatever we have to do to make sure that we're in compliant and so that's a very generic job all of our logs go through that um as you kind of work your way",
    "start": "1075120",
    "end": "1082159"
  },
  {
    "text": "down that tree of spark jobs they'll become more specific we'll start to basically um route the logs based on the",
    "start": "1082159",
    "end": "1089120"
  },
  {
    "text": "type of data they are if it's a camera log it'll go into a sub of topics for cameras if it's a a mobile log it'll go",
    "start": "1089120",
    "end": "1095120"
  },
  {
    "text": "to another one desktop log and so forth and then at the very end kind of those Leaf nodes those last spark jobs right",
    "start": "1095120",
    "end": "1101480"
  },
  {
    "text": "before those hdfs folders is where we handle what we call the impedance mismatch between kofka and hdfl",
    "start": "1101480",
    "end": "1109640"
  },
  {
    "text": "and the the challenge there is that kofka loves small little messages like 1K messages um hdfs wants like one gigabyte",
    "start": "1109640",
    "end": "1118320"
  },
  {
    "text": "files so you're off by you know six orders of magnitude there about a million times and so what we actually do",
    "start": "1118320",
    "end": "1124000"
  },
  {
    "text": "there is that's where we actually queue up the data we we wait until we have enough data and we write do a nice big",
    "start": "1124000",
    "end": "1129120"
  },
  {
    "text": "write hdfs loves that um and then the files are all ready to basically um do",
    "start": "1129120",
    "end": "1134320"
  },
  {
    "text": "our ETL processing much faster so trust me it gets a little easier",
    "start": "1134320",
    "end": "1140440"
  },
  {
    "start": "1137000",
    "end": "1290000"
  },
  {
    "text": "there the the real time streaming stuff is by far most complicated stuff um now",
    "start": "1140440",
    "end": "1146200"
  },
  {
    "text": "it actually as we go from left to right on our three clusters we're going to move to ETL gets a lot simpler at this",
    "start": "1146200",
    "end": "1151720"
  },
  {
    "text": "point we have a merge basically so we bring in from the top of this diagram you'll see um the streaming data bring",
    "start": "1151720",
    "end": "1158559"
  },
  {
    "text": "that into htfs within our ETL cluster um then we have our batch induction framework this is our good old Java",
    "start": "1158559",
    "end": "1164320"
  },
  {
    "text": "application framework which is just customed to you know what we wrote it's got a a SCH mod that we use um some of",
    "start": "1164320",
    "end": "1170720"
  },
  {
    "text": "these jobs run you know once every 12 hours some once every 24 hours there's a few that run a little bit faster than",
    "start": "1170720",
    "end": "1176240"
  },
  {
    "text": "that but they basically have adapters that we've written that know how to pull down extracts from uh third parties such",
    "start": "1176240",
    "end": "1183640"
  },
  {
    "text": "as Salesforce and Google analytics and YouTube and um Nets suite and all kinds",
    "start": "1183640",
    "end": "1188840"
  },
  {
    "text": "of different Services they pull those down um it's a uh a wide variety of",
    "start": "1188840",
    "end": "1195559"
  },
  {
    "text": "different uh data formats again Json text files CSV XML nested Json flatten",
    "start": "1195559",
    "end": "1202440"
  },
  {
    "text": "that all out so what you end up with again is Json so the the ETL cluster for",
    "start": "1202440",
    "end": "1208480"
  },
  {
    "text": "its input this nice flatten Json the first thing we do is we we convert that to paret so park is a colum or format",
    "start": "1208480",
    "end": "1216960"
  },
  {
    "text": "it's great means it's great scans um performance great scan speed it's great for analytical queries it also means it",
    "start": "1216960",
    "end": "1223280"
  },
  {
    "text": "compresses very well we flatten out our data a lot and so what that means is you have a lot of repetitive value and if",
    "start": "1223280",
    "end": "1229360"
  },
  {
    "text": "you use a compression algorithm especially something like a gzip it compresses down insanely well about 100x",
    "start": "1229360",
    "end": "1235000"
  },
  {
    "text": "is usually what we see so one of the the best ways to improve the performance of",
    "start": "1235000",
    "end": "1240520"
  },
  {
    "text": "of a a big data platform is proper compression um because if you use Json",
    "start": "1240520",
    "end": "1245640"
  },
  {
    "text": "it's the data is just enormous so as soon as we convert it to par we're ready to basically start to do our joins and",
    "start": "1245640",
    "end": "1251159"
  },
  {
    "text": "our aggregations so we start to our joints and aggregations using Hive map ruce um it's not the speediest thing but",
    "start": "1251159",
    "end": "1258000"
  },
  {
    "text": "the thing with map produces that it always almost always finishes um and so this isn't like uh super um you know in",
    "start": "1258000",
    "end": "1265919"
  },
  {
    "text": "a rush data and so it can run for a little bit we make sure it's it's correct again we save it as parquet",
    "start": "1265919",
    "end": "1271840"
  },
  {
    "text": "files we add the the schema on top of it at this point so we've taken it from ra",
    "start": "1271840",
    "end": "1276880"
  },
  {
    "text": "Jason which you know has you no schema assigned to it and now we've given it an actual schema and we put that into the",
    "start": "1276880",
    "end": "1283000"
  },
  {
    "text": "hive metastore so essentially we have relational tables at this point so the next thing is now we're ready to make",
    "start": "1283000",
    "end": "1289080"
  },
  {
    "text": "this data available for the analysts so we do a distributed copy to our what we",
    "start": "1289080",
    "end": "1294200"
  },
  {
    "start": "1290000",
    "end": "1366000"
  },
  {
    "text": "call our secure data Mark and again this this cluster is actually getting uh what happens here is a bit simpler from the",
    "start": "1294200",
    "end": "1300520"
  },
  {
    "text": "previous two clusters the biggest challenge here is the security uh part of it so at GoPro we're very concerned",
    "start": "1300520",
    "end": "1306440"
  },
  {
    "text": "about security we want to make sure that people are looking at only the data that're allowed to look at that they can't see any other data so we use",
    "start": "1306440",
    "end": "1312279"
  },
  {
    "text": "something called Apache Sentry which basically allows you to create access control lists based on Active Directory",
    "start": "1312279",
    "end": "1318240"
  },
  {
    "text": "Group uses curos for the authentication and then we lock down all the data not only at the the relational table level",
    "start": "1318240",
    "end": "1325520"
  },
  {
    "text": "but also at the hdfs level so even if somebody tried to circumvent and say oh I'm smarter than you I'm just going to",
    "start": "1325520",
    "end": "1330919"
  },
  {
    "text": "circumvent Hive and go straight for hdfs we still have them locked out so we make sure that people can only see the data",
    "start": "1330919",
    "end": "1336559"
  },
  {
    "text": "that they're allowed to see we use Impala which is our in-memory uh SQL execution engine uh basically you can",
    "start": "1336559",
    "end": "1343919"
  },
  {
    "text": "think of it as it's about 100 times faster than using Hive with the map reduce execution engine and then the",
    "start": "1343919",
    "end": "1349640"
  },
  {
    "text": "applications that are connecting to it are primarily Tableau for all the viz and and dashboards and Christian will",
    "start": "1349640",
    "end": "1355679"
  },
  {
    "text": "walk us through some some pretty cool dashboards that we have again we use c h which is the Hadoop UI for more",
    "start": "1355679",
    "end": "1361760"
  },
  {
    "text": "interactive querying and then good old fashioned python for a lot of",
    "start": "1361760",
    "end": "1367039"
  },
  {
    "start": "1366000",
    "end": "1445000"
  },
  {
    "text": "folks so you wanted to talk to us a little bit about so you know making that Improvement that we did from kind of our",
    "start": "1367039",
    "end": "1373159"
  },
  {
    "text": "version 1.0 platform to version 2.0 which is where we went from one cluster",
    "start": "1373159",
    "end": "1378559"
  },
  {
    "text": "to three clusters separating data ingestion from utl and then from data delivery you get the good things are you",
    "start": "1378559",
    "end": "1384520"
  },
  {
    "text": "get isolation workloads which is great you get fast ingestion you get the security with um Apache Sentry you get",
    "start": "1384520",
    "end": "1390440"
  },
  {
    "text": "fast delivery of queries which is you using Impala they're Loosely coupled clusters which we love because if you",
    "start": "1390440",
    "end": "1396640"
  },
  {
    "text": "get um a new application inhouse and they need a different version of Impala you can upgrade Impala without affecting",
    "start": "1396640",
    "end": "1403360"
  },
  {
    "text": "the data ingestion so we love that but there's always areas that you can improve right and so this is with the",
    "start": "1403360",
    "end": "1408679"
  },
  {
    "text": "the 3.0 architecture that we're currently working on um the the things we want to improve multiple copies of",
    "start": "1408679",
    "end": "1414640"
  },
  {
    "text": "data if the data was streamed in we may have up to three copies of the data and it kind of comes back to well where's",
    "start": "1414640",
    "end": "1420080"
  },
  {
    "text": "the system of record here you know which data should we believe um tightly coupled storage and compute lack of elasticity we definitely have that",
    "start": "1420080",
    "end": "1426400"
  },
  {
    "text": "challenge today three static clusters we know we need to double or triple resources during the um the holiday",
    "start": "1426400",
    "end": "1433320"
  },
  {
    "text": "season growing hdfs clusters and shrinking them is a very very painful",
    "start": "1433320",
    "end": "1438520"
  },
  {
    "text": "task not something that we look forward to and then there's just the operational overhead multiple clusters that you have",
    "start": "1438520",
    "end": "1443679"
  },
  {
    "text": "to maintain is not fun so this is this um some of this is future architecture",
    "start": "1443679",
    "end": "1448840"
  },
  {
    "start": "1445000",
    "end": "1583000"
  },
  {
    "text": "some of this is actually already in place so the the first thing I would call your attention to is right there in the middle and you'll see that we have",
    "start": "1448840",
    "end": "1455360"
  },
  {
    "text": "one Hive metastore versus we had three previously one for each cluster today uh",
    "start": "1455360",
    "end": "1460480"
  },
  {
    "text": "we're working on having just one and then we have one Amazon S3 bucket that houses all the data and the what what",
    "start": "1460480",
    "end": "1466480"
  },
  {
    "text": "this allows us to do is precisely you see on the right side of the uh slide there and on the top there suddenly you",
    "start": "1466480",
    "end": "1472840"
  },
  {
    "text": "have ephemeral clusters it means clusters you start up and you shut down so instead of getting into the situation",
    "start": "1472840",
    "end": "1478679"
  },
  {
    "text": "where we have a cluster out there secure data Mark everything's all set up and an analyst comes in and they're like yep we",
    "start": "1478679",
    "end": "1484960"
  },
  {
    "text": "want to know something Dave we got to take that 150 billion row table we got to join it with another 150 billion row table we want to do this and that and",
    "start": "1484960",
    "end": "1491279"
  },
  {
    "text": "you're doing some calculations you're like you going to need 100 nodes for that that's crazy well we can actually do that now um You you spend it up you",
    "start": "1491279",
    "end": "1499320"
  },
  {
    "text": "run it for a few hours maybe a few days and you spin it back down you don't have to maintain these clusters 24x7 365 and",
    "start": "1499320",
    "end": "1506799"
  },
  {
    "text": "storing data in the AWS S3 buckets allows us to do that um same thing for that ETL cluster sometimes that ETL",
    "start": "1506799",
    "end": "1513399"
  },
  {
    "text": "cluster is used for several hours and then it's not used for another 10 hours or so before another job kicks off you",
    "start": "1513399",
    "end": "1518679"
  },
  {
    "text": "know in that 12-hour Cadence so why we why are we paying the bill basically for to sit there for 10 hours same thing",
    "start": "1518679",
    "end": "1524640"
  },
  {
    "text": "storing the data in S3 buckets allows us to do that um and then the the other thing which is um more on the software",
    "start": "1524640",
    "end": "1530039"
  },
  {
    "text": "engineering side which we've done is taking that batch induction framework and previously you saw that it was like",
    "start": "1530039",
    "end": "1535399"
  },
  {
    "text": "writing directly into that ETL cluster we're actually redirecting that directly into our streaming ingestion cluster we",
    "start": "1535399",
    "end": "1541080"
  },
  {
    "text": "actually take that data and we shove it into Kafka what that allows us to do is basically go from two code lines to one",
    "start": "1541080",
    "end": "1546159"
  },
  {
    "text": "code line um so whenever you reduce the number of lines of code that you have to maintain high high efficiency there and",
    "start": "1546159",
    "end": "1552679"
  },
  {
    "text": "the second part that we're doing there is you'll see that the output from the streaming ingestion cluster is no longer Json it's par and it has the ddl so what",
    "start": "1552679",
    "end": "1560440"
  },
  {
    "text": "it means is we have dynamically defined the schema of that data and then we've put it into a highly performant format",
    "start": "1560440",
    "end": "1566799"
  },
  {
    "text": "par which is highly compressed 100 times smaller than what we're dealing with the Json and we're ready to go in fact some",
    "start": "1566799",
    "end": "1573399"
  },
  {
    "text": "of our analysts who are more advanced and they don't even want to wait around for us to do all this you know heavy ETL processing they can hit that data in a",
    "start": "1573399",
    "end": "1580360"
  },
  {
    "text": "more realtime fashion so with that I'll I'll hand it over to Christian so he can show us some",
    "start": "1580360",
    "end": "1586799"
  },
  {
    "text": "his badass Tableau dashboards and bues everyone I'm Christian I'm a analytics",
    "start": "1586799",
    "end": "1592440"
  },
  {
    "text": "engineer at GoPro and um I'm also heavily involved in the administration of Tableau server so first I want to",
    "start": "1592440",
    "end": "1600159"
  },
  {
    "text": "briefly go over um our Tableau server setup um so uh compared to a lot of",
    "start": "1600159",
    "end": "1605720"
  },
  {
    "text": "other companies it's not that big um so we do have about 76 um extracts being",
    "start": "1605720",
    "end": "1611000"
  },
  {
    "text": "refreshed throughout the day and uh about maybe a fifth of them are in intervals um in less than an hour",
    "start": "1611000",
    "end": "1618799"
  },
  {
    "text": "um 120 active users but um we really only see concurrency of about um three",
    "start": "1618799",
    "end": "1624360"
  },
  {
    "text": "at a time and uh so um our setup is we have one primary um one primary machine",
    "start": "1624360",
    "end": "1632240"
  },
  {
    "text": "and that has all of that runs all of the processes except for the backgrounder and uh so the worker handles the",
    "start": "1632240",
    "end": "1638799"
  },
  {
    "text": "backgrounder and uh what the backgrounder does is basically all it does is it refreshes the extracts um u",
    "start": "1638799",
    "end": "1645919"
  },
  {
    "text": "in Tableau server and so we find that um for us particularly I know that um at",
    "start": "1645919",
    "end": "1650960"
  },
  {
    "text": "other companies um it it might be different but our taable server workload is um compute intensive so um we go with",
    "start": "1650960",
    "end": "1658520"
  },
  {
    "text": "the uh C4 instances um so the great thing about um having taable server on",
    "start": "1658520",
    "end": "1664000"
  },
  {
    "text": "AWS is that you're able to um experiment with different sizes and um you know",
    "start": "1664000",
    "end": "1669080"
  },
  {
    "text": "when we first started out um we found that you know just having a C4 2x large",
    "start": "1669080",
    "end": "1675360"
  },
  {
    "text": "for the primary um was big enough and a C4 large was good enough for the backgrounder and um so right now since",
    "start": "1675360",
    "end": "1682960"
  },
  {
    "text": "we um have a large number of extracts um are refreshing throughout the day we",
    "start": "1682960",
    "end": "1688720"
  },
  {
    "text": "since then have moved on to um C4 adex large for the primary and uh C4 4X large",
    "start": "1688720",
    "end": "1694919"
  },
  {
    "text": "for a worker and so um I like to uh C put um",
    "start": "1694919",
    "end": "1701760"
  },
  {
    "text": "the tabl visualizations in three different categories so we have our preil which is um for data discovery",
    "start": "1701760",
    "end": "1708440"
  },
  {
    "text": "prototyping and QA um operations which is um just monitoring the health of the data stream and um analytics which um",
    "start": "1708440",
    "end": "1716679"
  },
  {
    "text": "you know all of the dashboards that are created by the analyst teams uh which Drive",
    "start": "1716679",
    "end": "1722840"
  },
  {
    "start": "1723000",
    "end": "1741000"
  },
  {
    "text": "insights so first I would like to go over um operations so",
    "start": "1723120",
    "end": "1728840"
  },
  {
    "text": "um the using um Tableau for operations allows us to uh monitor data as it",
    "start": "1728840",
    "end": "1735279"
  },
  {
    "text": "reaches essentially what is the final Landing point um the visualization and",
    "start": "1735279",
    "end": "1740799"
  },
  {
    "text": "so um here's an example of a um simple",
    "start": "1740799",
    "end": "1746080"
  },
  {
    "start": "1741000",
    "end": "1765000"
  },
  {
    "text": "operations dashboard um and so can see here um you know these are just a count",
    "start": "1746080",
    "end": "1751159"
  },
  {
    "text": "of records and you can see an anomaly that happened on you know one particular day say oh uh the data might have been",
    "start": "1751159",
    "end": "1757240"
  },
  {
    "text": "fine you know throughout the whole pipeline right up until this point and uh we could use tblo to identify",
    "start": "1757240",
    "end": "1765080"
  },
  {
    "start": "1765000",
    "end": "1773000"
  },
  {
    "text": "that so next is um the an analytics dashboards these are all the dashboards that the analysts build um so um as um",
    "start": "1765080",
    "end": "1775320"
  },
  {
    "start": "1773000",
    "end": "1848000"
  },
  {
    "text": "garv mentioned earlier um our cameras log events we have the Hero 5 which has a feature we call cameras a hub which",
    "start": "1775320",
    "end": "1782320"
  },
  {
    "text": "can um automatically upload your media up to um your your cloud account and so",
    "start": "1782320",
    "end": "1788840"
  },
  {
    "text": "um first off since this is essentially you know iot data that we're dealing with it's comes in in super large",
    "start": "1788840",
    "end": "1795880"
  },
  {
    "text": "amounts so first off aggregates are very important for the successful adoption of",
    "start": "1795880",
    "end": "1800960"
  },
  {
    "text": "Tableau um so it's up to um analytics Engineers such as myself to engage with",
    "start": "1800960",
    "end": "1806919"
  },
  {
    "text": "the analysts to figure out um what exactly they want to do with the data so that we could take you know these 100 um",
    "start": "1806919",
    "end": "1815080"
  },
  {
    "text": "or close to a billion events that we receive every day you know crunch it down to something more",
    "start": "1815080",
    "end": "1820240"
  },
  {
    "text": "manageable and so they would use these insights to for example with um camera",
    "start": "1820240",
    "end": "1825880"
  },
  {
    "text": "as a hub um to um see you know what is",
    "start": "1825880",
    "end": "1832120"
  },
  {
    "text": "the optimal um compression for the video that it's uploading so that we could kind of strike that balance between um",
    "start": "1832120",
    "end": "1839919"
  },
  {
    "text": "you know uh good video quality and um upload time that the camera",
    "start": "1839919",
    "end": "1846760"
  },
  {
    "start": "1848000",
    "end": "1930000"
  },
  {
    "text": "performs um next up is um so we do do some real time for analytics and so an",
    "start": "1848440",
    "end": "1854200"
  },
  {
    "text": "example of that is um for a user sign up for a cloud service um and so in order",
    "start": "1854200",
    "end": "1861399"
  },
  {
    "text": "to achieve this we um we use U Hive external tables that read data um that",
    "start": "1861399",
    "end": "1867960"
  },
  {
    "text": "read files straight off of the U spark cluster um and it just reads files right",
    "start": "1867960",
    "end": "1874000"
  },
  {
    "text": "away right as the files are trickling and and uh we could see them in a dashboard now it's um you know something",
    "start": "1874000",
    "end": "1880320"
  },
  {
    "text": "like user signups is a great use case for that because it's a very simple calculation you're just seeing how many",
    "start": "1880320",
    "end": "1885919"
  },
  {
    "text": "people are getting added to the service so um things like that are uh great in",
    "start": "1885919",
    "end": "1893840"
  },
  {
    "text": "spark and so um next off is um preal so",
    "start": "1894600",
    "end": "1899960"
  },
  {
    "text": "uh for preal I want to um look at um how we use it for QA um which I find pretty",
    "start": "1899960",
    "end": "1907960"
  },
  {
    "text": "interesting um it's kind of a atypical way to use Tableau but basically um uh",
    "start": "1907960",
    "end": "1914120"
  },
  {
    "text": "for example we have our desktop application and um it logs events and",
    "start": "1914120",
    "end": "1919279"
  },
  {
    "text": "the developers want to make sure that you know the it's sending events the way they expect them to come in and so we",
    "start": "1919279",
    "end": "1925919"
  },
  {
    "text": "use um uh Hive external tables for that and so this is kind of a overview of the",
    "start": "1925919",
    "end": "1933000"
  },
  {
    "start": "1930000",
    "end": "1966000"
  },
  {
    "text": "process so you have you know any flat file um as Dave said Json is our file",
    "start": "1933000",
    "end": "1938120"
  },
  {
    "text": "format of choice so we use uh Json CDE and uh we use it to read the files right",
    "start": "1938120",
    "end": "1945279"
  },
  {
    "text": "off of the directory and so as soon as um a QA tester uh fires off events",
    "start": "1945279",
    "end": "1952080"
  },
  {
    "text": "they'll be able to see their events come into the dashboard within um a few",
    "start": "1952080",
    "end": "1957279"
  },
  {
    "text": "minutes um right now it's at 2 minutes so you know it goes into hi external",
    "start": "1957279",
    "end": "1962880"
  },
  {
    "text": "table and then to visualization and so um real quickly I'll show a demo of um a",
    "start": "1962880",
    "end": "1969279"
  },
  {
    "start": "1966000",
    "end": "1980000"
  },
  {
    "text": "v of a dashboard that they use for QA and uh here it is and uh",
    "start": "1969279",
    "end": "1979320"
  },
  {
    "start": "1980000",
    "end": "2099000"
  },
  {
    "text": "and so uh this as I said is for QA so it's really ugly and thankfully no one's",
    "start": "1980720",
    "end": "1987919"
  },
  {
    "text": "trying to use this to drive insights um but um so you can see here is um we have",
    "start": "1987919",
    "end": "1994360"
  },
  {
    "text": "this tester tracker tab where a tester will go in fire off some events and then",
    "start": "1994360",
    "end": "1999679"
  },
  {
    "text": "within minutes they'd be able to see their name pop up here say like okay well I fired off this many events at",
    "start": "1999679",
    "end": "2006279"
  },
  {
    "text": "this time just to this is just kind of for them to confirm that um the events went through the pipe and then they can",
    "start": "2006279",
    "end": "2013039"
  },
  {
    "text": "go and dig deeper into um the stats for the particular events and so um they're able to look at",
    "start": "2013039",
    "end": "2021480"
  },
  {
    "text": "this and just quickly within two minutes of firing off the events um see that it comes in and it comes in um you know in",
    "start": "2021480",
    "end": "2028480"
  },
  {
    "text": "whatever format is expected and so also want to go through um how um in the context of Tableau how",
    "start": "2028480",
    "end": "2035639"
  },
  {
    "text": "we make this possible so um for those of you um that don't know a Json Ser is",
    "start": "2035639",
    "end": "2041440"
  },
  {
    "text": "just uh it's just a jar file that you load up um into using initial SQL and in",
    "start": "2041440",
    "end": "2048320"
  },
  {
    "text": "Tableau and uh it allows you to um Define the schema so in this case it's",
    "start": "2048320",
    "end": "2053358"
  },
  {
    "text": "Json and so um here you know we have this top level key EP meta and then",
    "start": "2053359",
    "end": "2059440"
  },
  {
    "text": "these this is all of this is a nested key and you know here you see there's a key that's three levels deep and so you",
    "start": "2059440",
    "end": "2066000"
  },
  {
    "text": "just use this to define the schema and it's makes it really easy and so after you've defined the schema it's just a",
    "start": "2066000",
    "end": "2071679"
  },
  {
    "text": "typical select query uh to select um each key in in in the Json file and so",
    "start": "2071679",
    "end": "2078720"
  },
  {
    "text": "here it's do notated so here you can see it's two levels deep here so you have events exploded do event so that's just",
    "start": "2078720",
    "end": "2084839"
  },
  {
    "text": "top level key events exploded and then um whatever nested key is in there which is event and then um finally the uh the",
    "start": "2084839",
    "end": "2091960"
  },
  {
    "text": "actual value of that event and uh so that is it for my part",
    "start": "2091960",
    "end": "2098280"
  },
  {
    "text": "thank you",
    "start": "2098280",
    "end": "2101440"
  }
]