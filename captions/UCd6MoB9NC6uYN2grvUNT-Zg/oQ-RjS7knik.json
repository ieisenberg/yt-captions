[
  {
    "start": "0",
    "end": "259000"
  },
  {
    "text": "foreign",
    "start": "480",
    "end": "2719"
  },
  {
    "text": "account manager here at AWS and I'm based out of Delhi India",
    "start": "7820",
    "end": "13500"
  },
  {
    "text": "welcome to AWS supports you where AWS support experts provide tips to optimize",
    "start": "13500",
    "end": "18840"
  },
  {
    "text": "performance in the cloud lower cost and provides you with the best practices and",
    "start": "18840",
    "end": "24000"
  },
  {
    "text": "design considerations joining me today is Kunal from the AWS Professional Services team Kunal can you",
    "start": "24000",
    "end": "31619"
  },
  {
    "text": "give us a quick introduction please thanks Tanya sorry hey good morning all",
    "start": "31619",
    "end": "38940"
  },
  {
    "text": "my name is Kunal Gautam working as a senior Big Data consultant for Amazon Professional Services been in the domain",
    "start": "38940",
    "end": "45420"
  },
  {
    "text": "for 13 plus years and have worked across 30 plus Enterprises across the globe to",
    "start": "45420",
    "end": "50520"
  },
  {
    "text": "bring in the cultural and technological changes required to have a positive business impact leveraging AWS big data",
    "start": "50520",
    "end": "57420"
  },
  {
    "text": "services and machine learning and AI over to you Tanya",
    "start": "57420",
    "end": "62579"
  },
  {
    "text": "thank you Kunal for your introduction so today we will be covering around the",
    "start": "62579",
    "end": "68100"
  },
  {
    "text": "recapping of the reinvent launches basically in the big data and analytics domain but before we get into the",
    "start": "68100",
    "end": "75720"
  },
  {
    "text": "details a quick note to the attendees online use the chat window on the right hand side of your screen to let us know",
    "start": "75720",
    "end": "82439"
  },
  {
    "text": "where you are joining us from today and also share your thoughts and questions throughout the episode we really look",
    "start": "82439",
    "end": "89520"
  },
  {
    "text": "forward to hearing from you we will also provide a link to our survey so if you",
    "start": "89520",
    "end": "95040"
  },
  {
    "text": "know you would like to let us know how we did during the show and if there are any questions in feedback just leave",
    "start": "95040",
    "end": "101100"
  },
  {
    "text": "your feedback there with that uh Kunal can you help us walk us through what all that we would be",
    "start": "101100",
    "end": "107220"
  },
  {
    "text": "covering up today thanks Tanya let's go ahead with the agenda so we'll",
    "start": "107220",
    "end": "114180"
  },
  {
    "text": "start with Amazon data Zone AWS clean rooms AWS glue Amazon redshift Amazon",
    "start": "114180",
    "end": "121020"
  },
  {
    "text": "Aurora zero ETL Amazon Athena for Apaches Park and Amazon quick site for",
    "start": "121020",
    "end": "126060"
  },
  {
    "text": "passionated reports so even before we start and go for a",
    "start": "126060",
    "end": "131879"
  },
  {
    "text": "deep dive let's look at the end to end data Journey so if you look at any data",
    "start": "131879",
    "end": "137220"
  },
  {
    "text": "product or any data pipeline so overall it's divided into four stages",
    "start": "137220",
    "end": "142739"
  },
  {
    "text": "the first stage being the data ingestion the second stage being the data storage",
    "start": "142739",
    "end": "148099"
  },
  {
    "text": "the third one data transformation and the last one data consumption",
    "start": "148099",
    "end": "153540"
  },
  {
    "text": "so if you look at ingestion your data assets could be on on premises it could be on AWS it could it could be cut",
    "start": "153540",
    "end": "160920"
  },
  {
    "text": "across your databases your SAS application your third-party data media data streaming data",
    "start": "160920",
    "end": "167040"
  },
  {
    "text": "and they have to be ingested all the data goes and gets streamed or gets",
    "start": "167040",
    "end": "172620"
  },
  {
    "text": "copied over to Amazon S3 so Amazon S3 allows you to store data data at",
    "start": "172620",
    "end": "178200"
  },
  {
    "text": "petabyte scales in a safe and secure Manner and that's what we pretty much call a data Lake",
    "start": "178200",
    "end": "184319"
  },
  {
    "text": "so once you have your data on a data Lake you can have your data warehouse Solutions like redshift which can allow",
    "start": "184319",
    "end": "190080"
  },
  {
    "text": "you to query your data you can also have nosql stores like dynamodb you can have databases like",
    "start": "190080",
    "end": "196019"
  },
  {
    "text": "Aurora RDS which can also help you to store data and have analytical results on top of it",
    "start": "196019",
    "end": "202980"
  },
  {
    "text": "for transformation at a distributed scale we use services like AWS glue",
    "start": "202980",
    "end": "208019"
  },
  {
    "text": "Amazon EMR to process your data at scale and last but not the least once you have",
    "start": "208019",
    "end": "213720"
  },
  {
    "text": "your data we can leverage Amazon quicksite to go ahead and consume consume this data",
    "start": "213720",
    "end": "219900"
  },
  {
    "text": "so now if you correlate between the agenda and the data Journey what we can see is the Amazon data Zone uh AWS glue",
    "start": "219900",
    "end": "228239"
  },
  {
    "text": "clean rooms they lie at the center of of this diagram",
    "start": "228239",
    "end": "233340"
  },
  {
    "text": "redshift Athena glue they all are used for data processing and ad hoc query",
    "start": "233340",
    "end": "239879"
  },
  {
    "text": "last but not the least the quick site is being leveraged for the data consumption part",
    "start": "239879",
    "end": "245760"
  },
  {
    "text": "let's go ahead and do a deep dive into each of these new services and all the",
    "start": "245760",
    "end": "250980"
  },
  {
    "text": "cool and amazing features that has been announced",
    "start": "250980",
    "end": "256280"
  },
  {
    "start": "259000",
    "end": "773000"
  },
  {
    "text": "let's get started with Amazon data Zone so folks here's a problem statement",
    "start": "259320",
    "end": "265440"
  },
  {
    "text": "there's an Enterprise and in the Enterprise you have manufacturing units all the way from",
    "start": "265440",
    "end": "271620"
  },
  {
    "text": "Berlin New York Tokyo and you have",
    "start": "271620",
    "end": "277820"
  },
  {
    "text": "the ID being served from Bangalore and the head offices are spread across uh",
    "start": "277820",
    "end": "283919"
  },
  {
    "text": "let's say Tel Aviv is it possible",
    "start": "283919",
    "end": "289139"
  },
  {
    "text": "that we can go ahead and figure out what are my data assets that are spread across the different",
    "start": "289139",
    "end": "296040"
  },
  {
    "text": "GEOS how is it possible that I can go ahead and discover this data seamlessly so",
    "start": "296040",
    "end": "302699"
  },
  {
    "text": "let's say somebody at a business level sitting in New York wants to figure out hey in my Tokyo plant what is the data",
    "start": "302699",
    "end": "310500"
  },
  {
    "text": "that's available there can I have access to the data",
    "start": "310500",
    "end": "315960"
  },
  {
    "text": "what are the governance rule is it possible that I set a governance Rule and the governance rule will be applied",
    "start": "315960",
    "end": "321540"
  },
  {
    "text": "through the data across all the manufacturing plans out there",
    "start": "321540",
    "end": "328259"
  },
  {
    "text": "last but not last but not the least how easy is it to use tools and services to process my",
    "start": "328259",
    "end": "336300"
  },
  {
    "text": "data and get insights from that so that I can make data driven decisions as fast",
    "start": "336300",
    "end": "342180"
  },
  {
    "text": "as possible if one tries to solve this problem it",
    "start": "342180",
    "end": "347639"
  },
  {
    "text": "will take months and years all together to build this solution so forget about getting all the beautiful insights from",
    "start": "347639",
    "end": "354900"
  },
  {
    "text": "this data or the amazing amazing insights from this data just to build this platform will take a huge amount of",
    "start": "354900",
    "end": "361919"
  },
  {
    "text": "time and that's where Amazon data Zone comes",
    "start": "361919",
    "end": "367500"
  },
  {
    "text": "into being so Amazon data Zone helps you to search share discover data at scale across",
    "start": "367500",
    "end": "375900"
  },
  {
    "text": "organizational boundaries it allows you to collaborate on data",
    "start": "375900",
    "end": "381240"
  },
  {
    "text": "projects through a unified data analytical portal",
    "start": "381240",
    "end": "386660"
  },
  {
    "text": "there are two main tenants one is the producer and one is the consumer so all",
    "start": "387000",
    "end": "392460"
  },
  {
    "text": "it is trying to do is it is reducing the distance between them and when I say",
    "start": "392460",
    "end": "398580"
  },
  {
    "text": "distance it could be in terms of tools it could be in terms of services it could be in terms of the geographical",
    "start": "398580",
    "end": "405000"
  },
  {
    "text": "distribution between them how easy is it that all the different stakeholders",
    "start": "405000",
    "end": "411600"
  },
  {
    "text": "for the data they get all together and they are able to share their data they",
    "start": "411600",
    "end": "416940"
  },
  {
    "text": "are able to Leverage The Power of their data irrespective of where they are located on this planet",
    "start": "416940",
    "end": "425300"
  },
  {
    "text": "so that's what your the Amazon data portal provides what it allows you to do is your data assets can be sitting on on",
    "start": "425759",
    "end": "433979"
  },
  {
    "text": "premises it can be an AWS or it can be any other SAS application",
    "start": "433979",
    "end": "439319"
  },
  {
    "text": "it allows you to browse understand connect search and collaborate",
    "start": "439319",
    "end": "446099"
  },
  {
    "text": "these data assets across the different stakeholders by leveraging the",
    "start": "446099",
    "end": "451259"
  },
  {
    "text": "difference different AWS tools out there to process your data and gain insight",
    "start": "451259",
    "end": "457560"
  },
  {
    "text": "so holistically if you look at it look at it it is providing you three important",
    "start": "457560",
    "end": "463199"
  },
  {
    "text": "features first is it provides you a 360 degree view of all your trusted business",
    "start": "463199",
    "end": "470099"
  },
  {
    "text": "data spread across the different GEOS and AWS regions",
    "start": "470099",
    "end": "475560"
  },
  {
    "text": "secondly it provides it provides you the",
    "start": "475560",
    "end": "480720"
  },
  {
    "text": "the ability to access your data in accordance to the compliance and security",
    "start": "480720",
    "end": "487400"
  },
  {
    "text": "regulations of your organization because different Enterprises depending upon",
    "start": "487400",
    "end": "492780"
  },
  {
    "text": "what kind of vertical they are working around the regulations requirements are different",
    "start": "492780",
    "end": "497940"
  },
  {
    "text": "so it is quite possible that you have a central governance layer and this governance layer ensures that that all",
    "start": "497940",
    "end": "503280"
  },
  {
    "text": "your data layers or data assets are adhered to that each and every stakeholder has to adhere to this adhere",
    "start": "503280",
    "end": "511020"
  },
  {
    "text": "to this this requirement last but not the least because now you",
    "start": "511020",
    "end": "516360"
  },
  {
    "text": "are you are able to democratize your data sitting across the different geolocations you increase the",
    "start": "516360",
    "end": "522120"
  },
  {
    "text": "productivity by inviting the different stakeholders or members for on your data",
    "start": "522120",
    "end": "527519"
  },
  {
    "text": "projects by not only providing them the data access but also providing them data governance and the right tooling to go",
    "start": "527519",
    "end": "535260"
  },
  {
    "text": "ahead and gather access to your data",
    "start": "535260",
    "end": "539720"
  },
  {
    "text": "so let's look at the two most important tenets of your data Zone",
    "start": "541680",
    "end": "546899"
  },
  {
    "text": "the first being the producer and the second being the consumer because for the data somebody is producing it and",
    "start": "546899",
    "end": "553920"
  },
  {
    "text": "somebody wants to consume that so let's look at",
    "start": "553920",
    "end": "558959"
  },
  {
    "text": "what a publisher can do in the data zone so a publisher is the one who is",
    "start": "558959",
    "end": "564540"
  },
  {
    "text": "generating the data or producing the data they are able to decide where will I",
    "start": "564540",
    "end": "571560"
  },
  {
    "text": "produce my data who should be able to access my data what kind of taxonomy I can fix on my",
    "start": "571560",
    "end": "578640"
  },
  {
    "text": "data I can tag my data saying that hey this column is a pii this column belongs to this specific Department",
    "start": "578640",
    "end": "586040"
  },
  {
    "text": "last but not the least if I am the producer I would also love to see who",
    "start": "586140",
    "end": "591300"
  },
  {
    "text": "are able to access my data I should get a log somewhere or or a metric saying that these are the folks who are",
    "start": "591300",
    "end": "597600"
  },
  {
    "text": "leveraging my data so that I can also understand is my data really useful are people",
    "start": "597600",
    "end": "603420"
  },
  {
    "text": "really able to first of all figure out my data and secondly is this data useful is it repetitive or is it only something",
    "start": "603420",
    "end": "609420"
  },
  {
    "text": "that only one business unit is using or is it something it's so important maybe 10 business units are using it across",
    "start": "609420",
    "end": "615360"
  },
  {
    "text": "the globe from the subscriber part first of all",
    "start": "615360",
    "end": "620700"
  },
  {
    "text": "the subscriber gets the ability to search your data because the data has been cataloged not only leveraging",
    "start": "620700",
    "end": "626580"
  },
  {
    "text": "technical taxonomy but also business taxonomy so I'm able to search the data once I'm able to search the data",
    "start": "626580",
    "end": "634080"
  },
  {
    "text": "am I able to send a request because the owners or let's say the ownership of",
    "start": "634080",
    "end": "639899"
  },
  {
    "text": "the data lies with the producer so can I go ahead and access the data request for",
    "start": "639899",
    "end": "645180"
  },
  {
    "text": "this data once the producer of the data provides",
    "start": "645180",
    "end": "650339"
  },
  {
    "text": "me the request I get the subscription over the data and I go ahead and start leveraging it",
    "start": "650339",
    "end": "656700"
  },
  {
    "text": "and it doesn't matter and it all happens through the single portal this amazing single portal that's out there",
    "start": "656700",
    "end": "664399"
  },
  {
    "text": "the most important feature which allows you to make your data discoverable that",
    "start": "666899",
    "end": "671940"
  },
  {
    "text": "allows you to allows the subscribers of the data to easily go and figure out who",
    "start": "671940",
    "end": "676980"
  },
  {
    "text": "is the owner what kind of data lies what kind of data asset this belongs to it is",
    "start": "676980",
    "end": "682260"
  },
  {
    "text": "all coming up because data Zone not only provides the ability just like AWS glue data catalog",
    "start": "682260",
    "end": "689279"
  },
  {
    "text": "the not only technical taxonomy is there that this is a table this is having problems and this is the description of",
    "start": "689279",
    "end": "694920"
  },
  {
    "text": "a column rather it also provides you the ability to go ahead and create organizational domain so the",
    "start": "694920",
    "end": "701760"
  },
  {
    "text": "organization domain pretty much correlate to your org hierarchy so in an orc there will be different business",
    "start": "701760",
    "end": "706860"
  },
  {
    "text": "unit and that's what you see in the catalog structure you have a corporate and then you have different business units there r d manufacturing",
    "start": "706860",
    "end": "712740"
  },
  {
    "text": "procurement you have a metadata template so each business unit can own their data asset",
    "start": "712740",
    "end": "718079"
  },
  {
    "text": "and they can go ahead and create a metadata template which allows them to create hey how am I go am I gonna go and",
    "start": "718079",
    "end": "725579"
  },
  {
    "text": "do uh and create a business taxonomy on top of my data so I already have a technical taxonomy can I create a",
    "start": "725579",
    "end": "731820"
  },
  {
    "text": "business taxonomy once I am able to create the metadata this this asset is published and in the portal it pretty",
    "start": "731820",
    "end": "738720"
  },
  {
    "text": "much looks like a Marketplace that you have all the data from the different business unit who owns that you can go",
    "start": "738720",
    "end": "744360"
  },
  {
    "text": "search subscribe to it and start leveraging it so if you look at Amazon data Zone",
    "start": "744360",
    "end": "750420"
  },
  {
    "text": "holistically in terms of Big Data it provides you the ability to create jio distributed data lakes with the ability",
    "start": "750420",
    "end": "758279"
  },
  {
    "text": "to search catalog your data and have a central governance",
    "start": "758279",
    "end": "764399"
  },
  {
    "text": "to which all the stakeholders data assets and tools comply",
    "start": "764399",
    "end": "770899"
  },
  {
    "start": "773000",
    "end": "1107000"
  },
  {
    "text": "with this we move on to AWS clean rooms",
    "start": "773639",
    "end": "780860"
  },
  {
    "text": "so let's look at another problem statement you have a media at tech company",
    "start": "787320",
    "end": "794040"
  },
  {
    "text": "you have multiple stakeholders you will have folks from customer care you will have campaign managers you will",
    "start": "794040",
    "end": "801540"
  },
  {
    "text": "have different ad agencies you will have Partners who are going to enrich your data",
    "start": "801540",
    "end": "807860"
  },
  {
    "text": "all this data will come to a central place and the challenge is each and everyone",
    "start": "807899",
    "end": "814019"
  },
  {
    "text": "wants to share a piece of the data with the other stakeholder let's say for",
    "start": "814019",
    "end": "820440"
  },
  {
    "text": "example the Ad Agency folks wants the ACT Tech to use certain portions or",
    "start": "820440",
    "end": "825660"
  },
  {
    "text": "certain Columns of their data not all of their data similarly",
    "start": "825660",
    "end": "830820"
  },
  {
    "text": "the the campaign managers would like to see certain aspects of the attack data",
    "start": "830820",
    "end": "836339"
  },
  {
    "text": "how easy is it to build such a system so if you look at this problem if you go",
    "start": "836339",
    "end": "842519"
  },
  {
    "text": "ahead and figure out what is the best mechanism soon you will realize the only",
    "start": "842519",
    "end": "848100"
  },
  {
    "text": "way out there would be to maintain multiple copies of the data by the different stakeholder which also implies",
    "start": "848100",
    "end": "854279"
  },
  {
    "text": "that every time a refresh happens all those copies have to be refreshed",
    "start": "854279",
    "end": "860880"
  },
  {
    "text": "secondly if you go ahead building the system this will this is a pretty much complicated task and it takes months of",
    "start": "860880",
    "end": "867360"
  },
  {
    "text": "development time last but not the least the Privacy control will require",
    "start": "867360",
    "end": "875760"
  },
  {
    "text": "quite a complicated mechanism to implement that so to solve this problem",
    "start": "875760",
    "end": "882899"
  },
  {
    "text": "AWS has come up with AWS clean rooms so AWS clean rooms helps customer and their",
    "start": "882899",
    "end": "890519"
  },
  {
    "text": "partners to more easily and securely match analyze and collaborate on their",
    "start": "890519",
    "end": "897779"
  },
  {
    "text": "combined data sets without the need for any sharing or",
    "start": "897779",
    "end": "903779"
  },
  {
    "text": "revealing their underlying data so the benefits of that first of the",
    "start": "903779",
    "end": "910019"
  },
  {
    "text": "first benefit is there is no data movement at all you just create a single copy and you are allowed",
    "start": "910019",
    "end": "916740"
  },
  {
    "text": "to create rules on who can consume this data what portion of this data will be used for analysis what portion of this",
    "start": "916740",
    "end": "923220"
  },
  {
    "text": "data will be used by consumption by the different partners all you can do is go to a clean room",
    "start": "923220",
    "end": "930720"
  },
  {
    "text": "you and you add participants who can collaborate on this single data",
    "start": "930720",
    "end": "936959"
  },
  {
    "text": "copy there is no need at all to reveal your",
    "start": "936959",
    "end": "942300"
  },
  {
    "text": "underlying data and that is done by using a construct known as Associated",
    "start": "942300",
    "end": "947699"
  },
  {
    "text": "tables you are able to protect your protect your underlying data with a broad set of",
    "start": "947699",
    "end": "954360"
  },
  {
    "text": "configuration configuration access pattern controls so these are much more fine-grained control which can say that",
    "start": "954360",
    "end": "961139"
  },
  {
    "text": "which column which participant can see so although you have a single copy",
    "start": "961139",
    "end": "966899"
  },
  {
    "text": "still you can go ahead and you can Define the control for each and every participant",
    "start": "966899",
    "end": "972480"
  },
  {
    "text": "and you are able to run queries adhering to these rules so in a the in the clean rooms we have",
    "start": "972480",
    "end": "979800"
  },
  {
    "text": "the concept of an analytical rules and output constraint which allows what",
    "start": "979800",
    "end": "986279"
  },
  {
    "text": "participant can see what and said last but not the least for analysis what will be the output like how what part of the",
    "start": "986279",
    "end": "992579"
  },
  {
    "text": "output should be constrained it should be not visible or you are not allowed to access that these kind of definitions",
    "start": "992579",
    "end": "998579"
  },
  {
    "text": "are stored there so with this I take a pause hit Tanya do",
    "start": "998579",
    "end": "1005480"
  },
  {
    "text": "you have any questions thank you Kunal for uh taking us over uh",
    "start": "1005480",
    "end": "1011839"
  },
  {
    "text": "you know these services and giving us a quick recap just one question uh on behalf of the audience can you name some",
    "start": "1011839",
    "end": "1017899"
  },
  {
    "text": "domains that will actually benefit directly from these services that you've just talked about",
    "start": "1017899",
    "end": "1024638"
  },
  {
    "text": "right so as I mentioned uh both for data Zone as well as the clean rooms media ad Tech",
    "start": "1024679",
    "end": "1032480"
  },
  {
    "text": "is one of the biggest player uh autonomous driving at our systems so",
    "start": "1032480",
    "end": "1039020"
  },
  {
    "text": "there are a lot of companies working in this domain they will benefit from this amazingly given that for ingestion",
    "start": "1039020",
    "end": "1046339"
  },
  {
    "text": "storage consumption there are different partners that they work along with along with given that the data isn't like",
    "start": "1046339",
    "end": "1052820"
  },
  {
    "text": "isn't like petabytes of scale you do ingestion and terabyte scales there so that's another domain that will immensely benefit another is the fintech",
    "start": "1052820",
    "end": "1060860"
  },
  {
    "text": "because the same problem statement you are getting data from multiple sources enriching that and then finally you use",
    "start": "1060860",
    "end": "1066500"
  },
  {
    "text": "in the fintech to figure out the propensity of a user doing something so that's another domain so if I look at it",
    "start": "1066500",
    "end": "1072620"
  },
  {
    "text": "any domain which is involving Partners who is Geo distributed who wants to",
    "start": "1072620",
    "end": "1077900"
  },
  {
    "text": "share their data ensuring that the governing rules are applied it applies to pretty much all those domains out",
    "start": "1077900",
    "end": "1084500"
  },
  {
    "text": "there and I have just called out few of them got it thank you Kunal I hope that",
    "start": "1084500",
    "end": "1091220"
  },
  {
    "text": "answers the question from one of our audience it was from jibjab06 thanks uh Kunal for answering on that over to you",
    "start": "1091220",
    "end": "1098539"
  },
  {
    "text": "and I think uh we'll hear you out more talking about AWS glow in the upcoming slides thank you",
    "start": "1098539",
    "end": "1105559"
  },
  {
    "text": "thanks Tanya so let's moving on ahead folks let's focus on AWS glue which is the",
    "start": "1105559",
    "end": "1112700"
  },
  {
    "start": "1107000",
    "end": "1312000"
  },
  {
    "text": "transformation layer if you remember the first diagram the transformation layer in the between wherein your data have",
    "start": "1112700",
    "end": "1118160"
  },
  {
    "text": "been ingested onto S3 and you start you you want to process the data that's where AWS glue comes into beam so what",
    "start": "1118160",
    "end": "1125539"
  },
  {
    "text": "is AWS blue AWS glue is a serverless data integration service that makes it",
    "start": "1125539",
    "end": "1131780"
  },
  {
    "text": "easier to discover prepare move and integrate data for analytics and machine",
    "start": "1131780",
    "end": "1139460"
  },
  {
    "text": "learning purposes blue can be used both for ETL elt",
    "start": "1139460",
    "end": "1145480"
  },
  {
    "text": "streaming and batch processing glue not only provides the ability to",
    "start": "1145480",
    "end": "1151340"
  },
  {
    "text": "process but you can also build your pipelines you can go ahead and",
    "start": "1151340",
    "end": "1157100"
  },
  {
    "text": "schedule them based based on your input pattern or your requirement",
    "start": "1157100",
    "end": "1163160"
  },
  {
    "text": "so glue as a whole provides you all this ability inside a single portal",
    "start": "1163160",
    "end": "1170380"
  },
  {
    "text": "so glue came out with the first the glue 4.0 version that came",
    "start": "1171020",
    "end": "1178220"
  },
  {
    "text": "out with it has started supporting spark version 3.3.0 as compared to 3.1.0",
    "start": "1178220",
    "end": "1185539"
  },
  {
    "text": "and python 3.10 is also being subtited now so it has become much more performant",
    "start": "1185539",
    "end": "1190820"
  },
  {
    "text": "you have more pushed on predicates that has been enabled out there the most important aspect",
    "start": "1190820",
    "end": "1199340"
  },
  {
    "text": "that you have your data and you want to change just a part of the record let's say you're having you are having one",
    "start": "1199340",
    "end": "1205160"
  },
  {
    "text": "terabytes of data and you wanted to change like one ten ten records out of that so previously we had to read this",
    "start": "1205160",
    "end": "1211160"
  },
  {
    "text": "one terabyte of data make the one couple of record changes and then save",
    "start": "1211160",
    "end": "1216260"
  },
  {
    "text": "back this one terabyte of data once again so blue 4.0 has introduced",
    "start": "1216260",
    "end": "1221780"
  },
  {
    "text": "the formats of Apache hudi Apache Iceberg and Delta Lake formats which are",
    "start": "1221780",
    "end": "1228020"
  },
  {
    "text": "not now native to AWS glue so you just enable a parameter and you are ready to",
    "start": "1228020",
    "end": "1233240"
  },
  {
    "text": "use any of these formats which allows for cost as well as job",
    "start": "1233240",
    "end": "1238700"
  },
  {
    "text": "performance optimization glue4 4.0 also enables you to process",
    "start": "1238700",
    "end": "1244820"
  },
  {
    "text": "your data at scale by leveraging the cloud Shuffle storage plugin on S3",
    "start": "1244820",
    "end": "1251240"
  },
  {
    "text": "because blue 4.0 3.0 and prior they were pretty much Limited",
    "start": "1251240",
    "end": "1256760"
  },
  {
    "text": "by this instance stored type storage type for all the shuffle to be taken care of in case your Shuffle was",
    "start": "1256760",
    "end": "1263780"
  },
  {
    "text": "increasing more than that either job used to fail or you had to scale that out like you had to horizontally scale that out so blue 4.0 allows you to use",
    "start": "1263780",
    "end": "1272240"
  },
  {
    "text": "leverage S3 to handle your spark shuffling it has also upgraded connectors for RDS",
    "start": "1272240",
    "end": "1279140"
  },
  {
    "text": "MySQL and SQL Server to ensure that jobs are much more performant and efficient",
    "start": "1279140",
    "end": "1284919"
  },
  {
    "text": "last but not the least the blue 4.0 as you write your uh Pi",
    "start": "1284919",
    "end": "1291679"
  },
  {
    "text": "spark script it looks at those scripts it dynamically optimizes your",
    "start": "1291679",
    "end": "1300020"
  },
  {
    "text": "query to make your Pi spark much more performant",
    "start": "1300020",
    "end": "1305080"
  },
  {
    "text": "glue has another important",
    "start": "1305419",
    "end": "1309620"
  },
  {
    "text": "service it has come up with another uh in engine known as AWS glue for Ray",
    "start": "1310640",
    "end": "1318919"
  },
  {
    "start": "1312000",
    "end": "1354000"
  },
  {
    "text": "which allows you to scale your python workload",
    "start": "1318919",
    "end": "1323980"
  },
  {
    "text": "just like because this is another engine type so all the cool features of glue",
    "start": "1324140",
    "end": "1329960"
  },
  {
    "text": "being serverless you not have to taking care of the infra product",
    "start": "1329960",
    "end": "1335299"
  },
  {
    "text": "Auto scaling it's all taken care automatically it is extremely cost effective because",
    "start": "1335299",
    "end": "1342260"
  },
  {
    "text": "it goes by the pay as you grow billing model",
    "start": "1342260",
    "end": "1347320"
  },
  {
    "text": "so moving on from our ETL onto our data warehouse path which is redshift",
    "start": "1348500",
    "end": "1355100"
  },
  {
    "start": "1354000",
    "end": "1583000"
  },
  {
    "text": "so let's look at redshift before if you wanted to get data out of redshift it",
    "start": "1355100",
    "end": "1360440"
  },
  {
    "text": "was quite a cumbersome process so redshift Amazon redshift integration of up for Apaches Park",
    "start": "1360440",
    "end": "1368179"
  },
  {
    "text": "has come up which opens up the redshift data warehouse solution for",
    "start": "1368179",
    "end": "1374179"
  },
  {
    "text": "for a broader set of AWS analysis analytics and machine learning use cases",
    "start": "1374179",
    "end": "1380720"
  },
  {
    "text": "you can pretty much write your Pi spark jobs either in Java Scala or python",
    "start": "1380720",
    "end": "1387679"
  },
  {
    "text": "and get access to the data which is sitting inside redshift",
    "start": "1387679",
    "end": "1392620"
  },
  {
    "text": "the the redshift Pi spark plugin has been built on existing open source connector",
    "start": "1392840",
    "end": "1400580"
  },
  {
    "text": "and enhances its performance and security",
    "start": "1400580",
    "end": "1405740"
  },
  {
    "text": "and it is also 10x faster application performance",
    "start": "1405740",
    "end": "1411260"
  },
  {
    "text": "so this Pi spark application you can pretty much go ahead and you can write that either in EMR glue or even in your",
    "start": "1411260",
    "end": "1420020"
  },
  {
    "text": "Amazon sagemaker notebook so you just go ahead Leverage The connectors and Bam",
    "start": "1420020",
    "end": "1425480"
  },
  {
    "text": "you are good to go by integrating any of these services to leverage to get the",
    "start": "1425480",
    "end": "1431419"
  },
  {
    "text": "data out of redshift and mix and match transform",
    "start": "1431419",
    "end": "1439640"
  },
  {
    "text": "with the data sitting on S3 or dynamodb or Aurora and get deeper and analytical",
    "start": "1439640",
    "end": "1446419"
  },
  {
    "text": "Insight onto the data data sitting on that shift the next important feature",
    "start": "1446419",
    "end": "1453380"
  },
  {
    "text": "for redshift is the streaming ingestion so let's look at a problem statement",
    "start": "1453380",
    "end": "1458780"
  },
  {
    "text": "before before this service was announced so you had real-time data streaming in",
    "start": "1458780",
    "end": "1464120"
  },
  {
    "text": "from Amazon Kinesis or Amazon streaming Manis Kafka as this data was streaming in you had to",
    "start": "1464120",
    "end": "1471679"
  },
  {
    "text": "push the data onto a fire hose then it used to stage at S3 and then finally a",
    "start": "1471679",
    "end": "1477500"
  },
  {
    "text": "copy command was being used and the data was being copied onto redshift",
    "start": "1477500",
    "end": "1482720"
  },
  {
    "text": "on a staging table and finally the staging table was getting merged into the final table",
    "start": "1482720",
    "end": "1489679"
  },
  {
    "text": "you had to create this pipeline this pipeline had to be maintained last but not the least if there were",
    "start": "1489679",
    "end": "1495740"
  },
  {
    "text": "errors you had to really go ahead and figure out what's happening if there were some errors maybe I have some missed out my data you had to do this",
    "start": "1495740",
    "end": "1504020"
  },
  {
    "text": "Plumbing tasked manually to ensure that you are not losing your data",
    "start": "1504020",
    "end": "1510158"
  },
  {
    "text": "so Amazon redshift streaming ingestion allows you to ingest hundreds of",
    "start": "1510320",
    "end": "1516620"
  },
  {
    "text": "megabytes of data per second which allows you to query your data in",
    "start": "1516620",
    "end": "1522020"
  },
  {
    "text": "in near real time you can connect multiple Amazon Genesis",
    "start": "1522020",
    "end": "1527360"
  },
  {
    "text": "streams or Amazon managed streaming for Apache Kafka that is msk",
    "start": "1527360",
    "end": "1532580"
  },
  {
    "text": "and pull data directly to Amazon redshift so in this case Amazon redshift",
    "start": "1532580",
    "end": "1537740"
  },
  {
    "text": "itself becomes the consumer you don't have to Stage your data anywhere because",
    "start": "1537740",
    "end": "1543440"
  },
  {
    "text": "Amazon redshift and when I say Amazon redshift it also goes for the serverless part yeah both the flavors of Amazon redshift",
    "start": "1543440",
    "end": "1549559"
  },
  {
    "text": "so they themselves became the consumers they are consuming data directly from your",
    "start": "1549559",
    "end": "1555140"
  },
  {
    "text": "streaming Solutions of Kinesis and Kafka and allowing you to create materialized",
    "start": "1555140",
    "end": "1560600"
  },
  {
    "text": "views so as and when your data is getting pushed your data is available in the fastest",
    "start": "1560600",
    "end": "1567080"
  },
  {
    "text": "possible amount of time in redshift so that the downside consumers or the consumer of this data can start running",
    "start": "1567080",
    "end": "1573500"
  },
  {
    "text": "analytical queries on top of it gather data inside and make data driven business decisions",
    "start": "1573500",
    "end": "1580658"
  },
  {
    "start": "1583000",
    "end": "2277000"
  },
  {
    "text": "moving on to one of the most important features or let's say something that's",
    "start": "1583100",
    "end": "1589279"
  },
  {
    "text": "that's that's that's really Gonna Change how people have been using their Aurora databases",
    "start": "1589279",
    "end": "1595220"
  },
  {
    "text": "Amazon Aurora zero ETL to Amazon redshift so let's look at a problem statement that initially in an",
    "start": "1595220",
    "end": "1602240"
  },
  {
    "text": "organization there were different business unit so there was a finance department there was a manufacturing Department there was an HR department",
    "start": "1602240",
    "end": "1609580"
  },
  {
    "text": "there was a department uh to maintain metadata I wanted to",
    "start": "1609580",
    "end": "1616820"
  },
  {
    "text": "go ahead and ensure that there are no data silos so what was the mechanism the",
    "start": "1616820",
    "end": "1622640"
  },
  {
    "text": "mechanism was get the data dumped out from all these Amazon Aurora instances",
    "start": "1622640",
    "end": "1627740"
  },
  {
    "text": "push it onto S3 create this ETL pipe data pipelines which will keep doing",
    "start": "1627740",
    "end": "1633320"
  },
  {
    "text": "this activity what it means is it goes ahead on a daily minutely or RV frequency you had to pull out data push",
    "start": "1633320",
    "end": "1641179"
  },
  {
    "text": "it onto S3 create a data Lake have a Lake formation on top of it for the governance and that's how you were able",
    "start": "1641179",
    "end": "1647600"
  },
  {
    "text": "to break the data silos in your organization but to build this you really had to have",
    "start": "1647600",
    "end": "1653840"
  },
  {
    "text": "a Workforce of data engineers and data scientists to build your data pipelines maintain that maintain them and it was",
    "start": "1653840",
    "end": "1660200"
  },
  {
    "text": "pretty much costly as well as the job used to take time the refresh rate was not like seconds or milliseconds right",
    "start": "1660200",
    "end": "1666559"
  },
  {
    "text": "because it's it's quite a time consuming process to write this ETL as well as for",
    "start": "1666559",
    "end": "1672140"
  },
  {
    "text": "the ETL to execute what Amazon Aurora now supports is zero",
    "start": "1672140",
    "end": "1678500"
  },
  {
    "text": "ETL integration with Amazon redshift which allows",
    "start": "1678500",
    "end": "1684020"
  },
  {
    "text": "you to have a real-time analytics and machine run machine learning use cases",
    "start": "1684020",
    "end": "1690020"
  },
  {
    "text": "using Amazon redshift on petabytes of transactional data what it means is as",
    "start": "1690020",
    "end": "1695419"
  },
  {
    "text": "and when the transaction is happening you go ahead and you can start leveraging this data to be processed to",
    "start": "1695419",
    "end": "1703820"
  },
  {
    "text": "be transformed for the article queries to be run on redshift",
    "start": "1703820",
    "end": "1709520"
  },
  {
    "text": "so as and when your data transaction data is coming up coming up on Amazon",
    "start": "1709520",
    "end": "1714679"
  },
  {
    "text": "Aurora that data is copied onto redshift",
    "start": "1714679",
    "end": "1720440"
  },
  {
    "text": "you don't have to build any ETL pipelines now because this copy is completely similarly seamless",
    "start": "1720440",
    "end": "1728380"
  },
  {
    "text": "and it's happening all under the hood",
    "start": "1728419",
    "end": "1732580"
  },
  {
    "text": "so all you have to worry about is hey I have 10 Aurora databases I consolidate",
    "start": "1734000",
    "end": "1739700"
  },
  {
    "text": "or I just connect this 10 Aurora databases to my Amazon redshift cluster and my Amazon redshift cluster in my",
    "start": "1739700",
    "end": "1747200"
  },
  {
    "text": "Amazon redshift cluster I can go ahead and start running my analytical queries without maintaining any ETL pipelines to",
    "start": "1747200",
    "end": "1754400"
  },
  {
    "text": "move the data from Aurora to Amazon drag shift",
    "start": "1754400",
    "end": "1759880"
  },
  {
    "text": "let's look underneath what's happening under the hood so first of all Amazon Aurora has its",
    "start": "1760580",
    "end": "1767240"
  },
  {
    "text": "own storage and as and when the transactions are happening the data is",
    "start": "1767240",
    "end": "1772399"
  },
  {
    "text": "copied from Amazon Aurora storage to Red strip storage",
    "start": "1772399",
    "end": "1777440"
  },
  {
    "text": "just keep in mind we are not talking about one is to one mapping rather you can have multiple Aurora instances",
    "start": "1777440",
    "end": "1783500"
  },
  {
    "text": "writing to the same redshift instance so you can consolidate Aurora so this is",
    "start": "1783500",
    "end": "1789140"
  },
  {
    "text": "pretty much this is pretty much thinking like of a data Lake without writing any ETL in between",
    "start": "1789140",
    "end": "1798158"
  },
  {
    "text": "the next beautiful feature is that Aurora because it's it's a schema",
    "start": "1798860",
    "end": "1804200"
  },
  {
    "text": "evolving solution if the schema evolves redshift will also ensure that it",
    "start": "1804200",
    "end": "1810559"
  },
  {
    "text": "understands this is schema Evolution and it is also available for the consumers",
    "start": "1810559",
    "end": "1815600"
  },
  {
    "text": "of redshift and you can leverage all the cool",
    "start": "1815600",
    "end": "1821960"
  },
  {
    "text": "features of redshift such as materialized views data sharing Federated access to multiple data",
    "start": "1821960",
    "end": "1828799"
  },
  {
    "text": "sources and data Lakes so this combines the power of Aurora",
    "start": "1828799",
    "end": "1833960"
  },
  {
    "text": "Gathering the data and also enabling for the end users to go ahead and query this",
    "start": "1833960",
    "end": "1839720"
  },
  {
    "text": "data all within all by by leveraging just a single click",
    "start": "1839720",
    "end": "1846580"
  },
  {
    "text": "do we have any questions till now",
    "start": "1849580",
    "end": "1854200"
  },
  {
    "text": "uh thanks Kunal uh I think there's one question that we have from one of uh the",
    "start": "1854659",
    "end": "1860000"
  },
  {
    "text": "audience on the chat hacker pbk and I think this is one of the very",
    "start": "1860000",
    "end": "1865460"
  },
  {
    "text": "frequent questions right uh which which we kind of see from the audience what is the difference between blue",
    "start": "1865460",
    "end": "1871340"
  },
  {
    "text": "and Athena so would you like to answer that for the audience please",
    "start": "1871340",
    "end": "1877399"
  },
  {
    "text": "right so let me come up with an analogy and that will help you to appreciate the",
    "start": "1877399",
    "end": "1884179"
  },
  {
    "text": "fact why do we have different services so when humans started Counting",
    "start": "1884179",
    "end": "1889520"
  },
  {
    "text": "we started using Abacus right like we were very primitive we started using abacus",
    "start": "1889520",
    "end": "1895340"
  },
  {
    "text": "time progress and we went ahead creating semiconductors and calculators came into",
    "start": "1895340",
    "end": "1901279"
  },
  {
    "text": "being Prime progress further we had computers",
    "start": "1901279",
    "end": "1906320"
  },
  {
    "text": "or desktop or laptops in our home and we started using Excel time further progress and now we have",
    "start": "1906320",
    "end": "1912919"
  },
  {
    "text": "data coming at petabyte scale and we are able to still consume the data and figure out hey uh who is doing well in",
    "start": "1912919",
    "end": "1919279"
  },
  {
    "text": "the elections or uh what is trending on Twitter just look at the scale billions of events are coming and you're still",
    "start": "1919279",
    "end": "1924559"
  },
  {
    "text": "able to find the scale so what changed in this in in this hierarchy the only",
    "start": "1924559",
    "end": "1932539"
  },
  {
    "text": "thing that changed was how you are processing your data and at what speed you can process that it means the",
    "start": "1932539",
    "end": "1938960"
  },
  {
    "text": "underlying engine the underlying engine for Abacus was human brain the underlying engine for calculator was the",
    "start": "1938960",
    "end": "1945919"
  },
  {
    "text": "semiconductors for Excel it was the CPU power right and for uh this Trends and",
    "start": "1945919",
    "end": "1951620"
  },
  {
    "text": "everything you have the big data processing system like spark or Presto or mapreduce so now coming to this question",
    "start": "1951620",
    "end": "1958820"
  },
  {
    "text": "even in the Big Data solution the first engine that came into being was known as mapreduce",
    "start": "1958820",
    "end": "1965059"
  },
  {
    "text": "but mapreduce had certain problems which was fixed and optimized and then the",
    "start": "1965059",
    "end": "1970640"
  },
  {
    "text": "engine that came was known as phase then came Presto and then came spark",
    "start": "1970640",
    "end": "1976279"
  },
  {
    "text": "spark and Presto came almost at the same time but spark being much more performant",
    "start": "1976279",
    "end": "1981320"
  },
  {
    "text": "so now coming back to Athena is based on Apache presto",
    "start": "1981320",
    "end": "1987200"
  },
  {
    "text": "spark ETL distributed processing because spark glue also sorry blue also Pro glue",
    "start": "1987200",
    "end": "1992960"
  },
  {
    "text": "provides uh Apache spark ETL and python ETL right so I'm I'm focusing on the uh",
    "start": "1992960",
    "end": "1999440"
  },
  {
    "text": "spark ETL part here so Athena is completely based upon presto",
    "start": "1999440",
    "end": "2005380"
  },
  {
    "text": "you cannot write code there you can just have SQL like you can just connect with",
    "start": "2005380",
    "end": "2011080"
  },
  {
    "text": "Athena using SQL queries so what it does is anybody who has no understanding of",
    "start": "2011080",
    "end": "2017260"
  },
  {
    "text": "programming can still go ahead just go to this dashboard of Athena or the portal of Athena",
    "start": "2017260",
    "end": "2023559"
  },
  {
    "text": "look at the data create tables on top of it and using SQL they can process their",
    "start": "2023559",
    "end": "2028779"
  },
  {
    "text": "data and get insights to it but let's say somebody who knows coding",
    "start": "2028779",
    "end": "2035140"
  },
  {
    "text": "and they want to Leverage The Power of Apache spark which is which is much more performant as compared to presto right",
    "start": "2035140",
    "end": "2041080"
  },
  {
    "text": "they can go ahead write code interact with it interact with their data and go",
    "start": "2041080",
    "end": "2046539"
  },
  {
    "text": "ahead and process the data glue is is like from the service perspective AWS",
    "start": "2046539",
    "end": "2052118"
  },
  {
    "text": "glue is more towards writing data Pipelines Athena is more towards doing ad hoc",
    "start": "2052119",
    "end": "2058118"
  },
  {
    "text": "queries you are not expected to write pipelines using Athena Athena is just ensuring you are given a",
    "start": "2058119",
    "end": "2064358"
  },
  {
    "text": "portal you write SQL queries you don't worry about the infra like what's happening in the background and still you're able to process in glue you need",
    "start": "2064359",
    "end": "2071320"
  },
  {
    "text": "to have some level of understanding how spark works you need to have some coding understanding how to code that write the",
    "start": "2071320",
    "end": "2076720"
  },
  {
    "text": "ETL pipeline which are repetitive so just to keep the answer very simple Athena is more for ad hoc queries",
    "start": "2076720",
    "end": "2082898"
  },
  {
    "text": "running on Presto glow is for running ETL pipelines uh mostly taken care by",
    "start": "2082899",
    "end": "2089260"
  },
  {
    "text": "data engineers and you are able to run this pipeline again and again to keep processing your data",
    "start": "2089260",
    "end": "2096158"
  },
  {
    "text": "hope that answers his question thank you Canal I think that that was",
    "start": "2096159",
    "end": "2101260"
  },
  {
    "text": "really an elaborative uh explanation on to what is the really the difference between glue and Athena so I think just",
    "start": "2101260",
    "end": "2108339"
  },
  {
    "text": "to summarize the key differences Athena is primarily which is used more on a",
    "start": "2108339",
    "end": "2113560"
  },
  {
    "text": "tool for the analytics while as Kunal explained clue is more of a transformation and a data movement tool",
    "start": "2113560",
    "end": "2119680"
  },
  {
    "text": "I hope that answers the question for our audience and then moving to a next question that we have Kunal from one of",
    "start": "2119680",
    "end": "2125740"
  },
  {
    "text": "the other audience that we have uh today with us on live are there any other",
    "start": "2125740",
    "end": "2131920"
  },
  {
    "text": "services other than glue that can be actually used for distributed data processing can you shed some light there",
    "start": "2131920",
    "end": "2138520"
  },
  {
    "text": "as well of course so uh glue is as I mentioned for the",
    "start": "2138520",
    "end": "2144760"
  },
  {
    "text": "data transformation you have multiple other services blue is one of them and glue is based on Apaches part we have",
    "start": "2144760",
    "end": "2151720"
  },
  {
    "text": "another service known as Amazon EMR elastic map reduce which can pretty much",
    "start": "2151720",
    "end": "2157359"
  },
  {
    "text": "do the same task the only thing is the interface is different and Amazon EMR is much more let's say complicated to to",
    "start": "2157359",
    "end": "2164560"
  },
  {
    "text": "handle because you need to have a better level of understanding in order to operate it",
    "start": "2164560",
    "end": "2170020"
  },
  {
    "text": "in glue the the infra part is completely invisible you just focus on writing the",
    "start": "2170020",
    "end": "2175540"
  },
  {
    "text": "script you need to have some level of understanding but you just need to write your script you don't you don't have to worry about what's happening in the",
    "start": "2175540",
    "end": "2181300"
  },
  {
    "text": "background how is the infra being spawned how the infra is being taken teared down you don't have to worry about that in case of Amazon EMR uh EMR",
    "start": "2181300",
    "end": "2189520"
  },
  {
    "text": "just before reinvent both the flavors has come out so we have EMR cluster",
    "start": "2189520",
    "end": "2195700"
  },
  {
    "text": "wherein you have to have a very good understanding of the cluster and the beauty of AMR is it allows you to write",
    "start": "2195700",
    "end": "2202300"
  },
  {
    "text": "your code in any of the analytical engines so what I mean by that is you can write your code in spark you can",
    "start": "2202300",
    "end": "2209079"
  },
  {
    "text": "write your code in Presto you can write your code in Flink you can write your code in mapreduce so all this analytical",
    "start": "2209079",
    "end": "2216760"
  },
  {
    "text": "engines that we can think of it's all part of a single package",
    "start": "2216760",
    "end": "2221920"
  },
  {
    "text": "and in EMR serverless you don't have to worry about the infra you just go ahead and you write your hive jobs or you go",
    "start": "2221920",
    "end": "2229420"
  },
  {
    "text": "ahead and write your spark jobs and submit it uh leveraging an API the infra",
    "start": "2229420",
    "end": "2235359"
  },
  {
    "text": "part is automatically taken care by AWS so it's pretty much moving on the lines of blue now that you just worry about",
    "start": "2235359",
    "end": "2241900"
  },
  {
    "text": "writing the business code that helps your business to grow and not to worry about the infra so these two like Amazon",
    "start": "2241900",
    "end": "2248380"
  },
  {
    "text": "EMR and EMR serverless is the closest one to glue that you can think of",
    "start": "2248380",
    "end": "2254140"
  },
  {
    "text": "foreign was really a nice explanation so I hope",
    "start": "2254140",
    "end": "2261640"
  },
  {
    "text": "that answered the question for the audience I think back to you uh to cover the next upcoming slides and the next",
    "start": "2261640",
    "end": "2268359"
  },
  {
    "text": "launches that we have thank you great",
    "start": "2268359",
    "end": "2274380"
  },
  {
    "text": "thanks a lot Tanya so let's move ahead with Amazon Athena for Apache Spark",
    "start": "2276339",
    "end": "2282400"
  },
  {
    "start": "2277000",
    "end": "2506000"
  },
  {
    "text": "so folks as you know Amazon Athena was supporting Presto with a SQL",
    "start": "2282400",
    "end": "2288339"
  },
  {
    "text": "interface so you go ahead write your you create a table on on top of your data set you use SQL and you can run your ad",
    "start": "2288339",
    "end": "2295359"
  },
  {
    "text": "hoc queries for doing any data sanity you want to figure out what's happening within your data uh unique user account",
    "start": "2295359",
    "end": "2301240"
  },
  {
    "text": "unique number of rows all these operations you were able to do and the beauty of that was Amazon Athena is",
    "start": "2301240",
    "end": "2307599"
  },
  {
    "text": "completely serverless so you're never worried about the infra you just went ahead created a table and started doing",
    "start": "2307599",
    "end": "2314020"
  },
  {
    "text": "analytics on top of it so let's let's look let's go for a little bit of Deep dive of what's Amazon Athena like in a",
    "start": "2314020",
    "end": "2320380"
  },
  {
    "text": "much more structured fashion and then we'll stick on to what is what is Apache spark how is how is the feature of",
    "start": "2320380",
    "end": "2326020"
  },
  {
    "text": "Apache spark that's that's gonna ensure that more users will be leveraging this",
    "start": "2326020",
    "end": "2331420"
  },
  {
    "text": "feature to process the data because now only now it supports both Presto as well",
    "start": "2331420",
    "end": "2336579"
  },
  {
    "text": "as part so first of all Amazon Athena it allows you to run interactive analytics in",
    "start": "2336579",
    "end": "2343660"
  },
  {
    "text": "under a second because it's all serverless to analyze better bytes of data",
    "start": "2343660",
    "end": "2349180"
  },
  {
    "text": "the Apache spark feature that has come up it's an interactive Spark",
    "start": "2349180",
    "end": "2354700"
  },
  {
    "text": "application or notebook that starts instantaneously runs faster because Amazon has provided",
    "start": "2354700",
    "end": "2362859"
  },
  {
    "text": "the spark optimized runtime so you write your code in the notebook you don't have",
    "start": "2362859",
    "end": "2368020"
  },
  {
    "text": "to worry about the infra it looks at the code optimizes it and executes that on",
    "start": "2368020",
    "end": "2373180"
  },
  {
    "text": "your behalf it allows the user because everything has been taken care of apart from the",
    "start": "2373180",
    "end": "2379119"
  },
  {
    "text": "code that definitely that you have to write so you can spend more time on the insights",
    "start": "2379119",
    "end": "2385060"
  },
  {
    "text": "on doing ensuring that the data driven decisions can be made faster and not",
    "start": "2385060",
    "end": "2390160"
  },
  {
    "text": "managing the infra that's running in the background the solution is completely serverless",
    "start": "2390160",
    "end": "2397660"
  },
  {
    "text": "fully managed and you don't have you don't have to provision maintain or configure any",
    "start": "2397660",
    "end": "2404320"
  },
  {
    "text": "resources while using this feature",
    "start": "2404320",
    "end": "2409380"
  },
  {
    "text": "last but not the least I would request everybody to start start using it because this is one of the",
    "start": "2409540",
    "end": "2416740"
  },
  {
    "text": "super cool tools that you will leverage to do any kind of data sanity test to do",
    "start": "2416740",
    "end": "2422380"
  },
  {
    "text": "like any kind of ad hoc analysis leveraging the power of spark so pretty",
    "start": "2422380",
    "end": "2427839"
  },
  {
    "text": "much before what used to happen was you go ahead you're building an application in spark you go back to Athena for your",
    "start": "2427839",
    "end": "2433480"
  },
  {
    "text": "data scientists for example I know that hey in my input I have 100 rows in my output data set I'm expecting 10 rows",
    "start": "2433480",
    "end": "2439780"
  },
  {
    "text": "and the ETL pipeline you are leveraging EMR or glue to do that and in and then what happens hey I want",
    "start": "2439780",
    "end": "2446980"
  },
  {
    "text": "to see what my output is is my data correct or not so I go to Amazon Athena do I ad hack analysis on my output data",
    "start": "2446980",
    "end": "2452500"
  },
  {
    "text": "set and figure out this is my expected output and that's how you were able to close the close the loop saying that my",
    "start": "2452500",
    "end": "2459460"
  },
  {
    "text": "ETL has run fine and this is the expectation the challenge here was as a data engineer I had to learn Pi spark I",
    "start": "2459460",
    "end": "2465820"
  },
  {
    "text": "had to learn SQL which is expected but I'm just saying you have to switch now what's happening is as a developer if I",
    "start": "2465820",
    "end": "2473200"
  },
  {
    "text": "just know spark I can run my ETL in glue or EMR using pi spark and same code I",
    "start": "2473200",
    "end": "2480700"
  },
  {
    "text": "can just leverage in Athena to go ahead and do my ad hoc data scientist or ad",
    "start": "2480700",
    "end": "2486579"
  },
  {
    "text": "hoc queries and Spark being much more efficient as compared to presto your queries will be",
    "start": "2486579",
    "end": "2493780"
  },
  {
    "text": "rendering much more faster you will be paying a lot less without the need to maintain or manage any infra",
    "start": "2493780",
    "end": "2502320"
  },
  {
    "text": "right so with this we move on to quicksite",
    "start": "2503260",
    "end": "2509020"
  },
  {
    "start": "2506000",
    "end": "2606000"
  },
  {
    "text": "so if you remember our first Slide the data Journey quick side comes at the very end which is the business",
    "start": "2509020",
    "end": "2515260"
  },
  {
    "text": "intelligence tool for the consumption so quick site has come up with a very",
    "start": "2515260",
    "end": "2520960"
  },
  {
    "text": "cool feature of paginated reports which allows you to create schedule and share",
    "start": "2520960",
    "end": "2527859"
  },
  {
    "text": "reports and data as a single fully managed",
    "start": "2527859",
    "end": "2533520"
  },
  {
    "text": "cloud-based bi solution so the reports that you get now it is",
    "start": "2533520",
    "end": "2539740"
  },
  {
    "text": "almost like a storyline why is that because you get it like a document through which you can paginate you can",
    "start": "2539740",
    "end": "2546760"
  },
  {
    "text": "create headers and Footers you can create line breaks you can go ahead create the different graphs the widgets",
    "start": "2546760",
    "end": "2553240"
  },
  {
    "text": "you can add all that as part of your report this report can be shared by",
    "start": "2553240",
    "end": "2558359"
  },
  {
    "text": "thousands of users who can leverage it to take data-driven",
    "start": "2558359",
    "end": "2565839"
  },
  {
    "text": "decisions as fast as possible so you don't worry about you you did not have to again this is a serverless solution",
    "start": "2565839",
    "end": "2572079"
  },
  {
    "text": "you need not worry about the infra you need not worry about how the analytics",
    "start": "2572079",
    "end": "2578020"
  },
  {
    "text": "has been done or how the transformation has been done you just worry about just dragging and dropping creating cool",
    "start": "2578020",
    "end": "2584640"
  },
  {
    "text": "paginated reports and sharing this report with the business owners and stakeholders to go ahead and",
    "start": "2584640",
    "end": "2592000"
  },
  {
    "text": "make data-driven decisions with a single click on their laptop or",
    "start": "2592000",
    "end": "2597700"
  },
  {
    "text": "desktop",
    "start": "2597700",
    "end": "2600060"
  },
  {
    "start": "2606000",
    "end": "2751000"
  },
  {
    "text": "so Tanya that's all from today's session thanks Kunal uh I think that was really",
    "start": "2606640",
    "end": "2614319"
  },
  {
    "text": "uh you know great insights by you uh into the you know the launches and what",
    "start": "2614319",
    "end": "2620619"
  },
  {
    "text": "all new things that has you know come on the recently invent on the big data and analytics side",
    "start": "2620619",
    "end": "2626020"
  },
  {
    "text": "I think lot of lot of uh info that we kind of discussed today I do not see uh",
    "start": "2626020",
    "end": "2631780"
  },
  {
    "text": "any questions from the audience further so I think we are good for today and I really appreciate you know you going",
    "start": "2631780",
    "end": "2638380"
  },
  {
    "text": "through all the uh you know big lunches and basically on the big data and analytics side so I hope our audience",
    "start": "2638380",
    "end": "2644079"
  },
  {
    "text": "have found them really helpful uh with that I think uh today we kind of",
    "start": "2644079",
    "end": "2649540"
  },
  {
    "text": "already closed on recapping on the reinvent launches uh basically on the big data and analytics site so if there",
    "start": "2649540",
    "end": "2655900"
  },
  {
    "text": "are any questions that you all have uh that you know we were not answered that we were not able to answer today you can",
    "start": "2655900",
    "end": "2662740"
  },
  {
    "text": "post your questions on repost dot AWS where one of our exports can provide you",
    "start": "2662740",
    "end": "2669160"
  },
  {
    "text": "with an answer or to your question or probably you know your question could become a topic of one of our future",
    "start": "2669160",
    "end": "2675760"
  },
  {
    "text": "shows and if you have any feedback for us for any of the next shows if you",
    "start": "2675760",
    "end": "2680980"
  },
  {
    "text": "would like to hear us uh anything that you would like to you know include as a part of the show please check the chat",
    "start": "2680980",
    "end": "2686980"
  },
  {
    "text": "box on the right hand side for a link to our survey or you can also email us at",
    "start": "2686980",
    "end": "2692740"
  },
  {
    "text": "AWS supports you at amazon.com we would want to hear from you tell us what else",
    "start": "2692740",
    "end": "2698800"
  },
  {
    "text": "you would like to see on these shows and with that I would also like to call",
    "start": "2698800",
    "end": "2703900"
  },
  {
    "text": "out uh please join us again in two weeks on channel 25th at 10 A.M IST where our",
    "start": "2703900",
    "end": "2710319"
  },
  {
    "text": "experts will also be going ahead and talking about AWS Health aware servicenow integration and I hope that",
    "start": "2710319",
    "end": "2717520"
  },
  {
    "text": "that is going to be helpful as well so again thank you Kunal for joining us and thanks to everybody who has joined",
    "start": "2717520",
    "end": "2724119"
  },
  {
    "text": "us today uh really really appreciate all your time and thank you again Happy Cloud",
    "start": "2724119",
    "end": "2729700"
  },
  {
    "text": "Computing see you next show thank you thanks all amazing day ahead",
    "start": "2729700",
    "end": "2737160"
  },
  {
    "text": "[Music]",
    "start": "2739580",
    "end": "2743880"
  }
]