[
  {
    "text": "hi there I'm Aruna govindaraju open site",
    "start": "1319",
    "end": "4380"
  },
  {
    "text": "specialist architect and today we will",
    "start": "4380",
    "end": "6540"
  },
  {
    "text": "see how open search service is used for",
    "start": "6540",
    "end": "8880"
  },
  {
    "text": "Vector search",
    "start": "8880",
    "end": "11219"
  },
  {
    "text": "open search is a Community Driven open",
    "start": "11219",
    "end": "13500"
  },
  {
    "text": "sourcefultech search engine built on top",
    "start": "13500",
    "end": "15480"
  },
  {
    "text": "of Apache leucine open search service is",
    "start": "15480",
    "end": "18300"
  },
  {
    "text": "a managed offering on the AWS Cloud that",
    "start": "18300",
    "end": "20640"
  },
  {
    "text": "allows you to build such applications at",
    "start": "20640",
    "end": "23039"
  },
  {
    "text": "scale",
    "start": "23039",
    "end": "23939"
  },
  {
    "text": "the core search engine enables text",
    "start": "23939",
    "end": "25920"
  },
  {
    "text": "analytics to do full tick search",
    "start": "25920",
    "end": "27840"
  },
  {
    "text": "aggregation Boolean search and uses",
    "start": "27840",
    "end": "30420"
  },
  {
    "text": "bm-25 to score and rank the results",
    "start": "30420",
    "end": "34140"
  },
  {
    "text": "various ml Integrations have matured",
    "start": "34140",
    "end": "36300"
  },
  {
    "text": "over the past few years allow you to",
    "start": "36300",
    "end": "38219"
  },
  {
    "text": "generate and store Vector embedding",
    "start": "38219",
    "end": "39960"
  },
  {
    "text": "build semantic and predictive search",
    "start": "39960",
    "end": "41879"
  },
  {
    "text": "perform ml analytics through anomaly",
    "start": "41879",
    "end": "44640"
  },
  {
    "text": "detection fraud detection and model",
    "start": "44640",
    "end": "46620"
  },
  {
    "text": "driven inference",
    "start": "46620",
    "end": "49520"
  },
  {
    "text": "let's look at the various plugin that",
    "start": "49559",
    "end": "51719"
  },
  {
    "text": "enable ml integration at a high level",
    "start": "51719",
    "end": "54180"
  },
  {
    "text": "the ml Integrations can be classified",
    "start": "54180",
    "end": "56160"
  },
  {
    "text": "into three categories the k n and the",
    "start": "56160",
    "end": "59280"
  },
  {
    "text": "neural plugin enable Vector embedding",
    "start": "59280",
    "end": "61800"
  },
  {
    "text": "storage and such the LTR or the learning",
    "start": "61800",
    "end": "64978"
  },
  {
    "text": "tool rank plugin allows you to rank",
    "start": "64979",
    "end": "67380"
  },
  {
    "text": "rescore the search results at query time",
    "start": "67380",
    "end": "71340"
  },
  {
    "text": "the ml comments allows you to bring your",
    "start": "71340",
    "end": "73560"
  },
  {
    "text": "own model to build business specific",
    "start": "73560",
    "end": "75600"
  },
  {
    "text": "applications such as recommendations",
    "start": "75600",
    "end": "78240"
  },
  {
    "text": "anomaly detection and predictive search",
    "start": "78240",
    "end": "82259"
  },
  {
    "text": "here are five reasons as to why today",
    "start": "82259",
    "end": "84299"
  },
  {
    "text": "open search is also used as a vector",
    "start": "84299",
    "end": "86759"
  },
  {
    "text": "database open search allows you to build",
    "start": "86759",
    "end": "89520"
  },
  {
    "text": "store search manage and scale Vector",
    "start": "89520",
    "end": "92220"
  },
  {
    "text": "embedding",
    "start": "92220",
    "end": "93840"
  },
  {
    "text": "today for the demo I will be showing how",
    "start": "93840",
    "end": "96240"
  },
  {
    "text": "to associate a pre-trained model to an",
    "start": "96240",
    "end": "98400"
  },
  {
    "text": "index in your cluster and also compare",
    "start": "98400",
    "end": "101220"
  },
  {
    "text": "the vanilla search results to semantic",
    "start": "101220",
    "end": "103200"
  },
  {
    "text": "search using the open search dashboard",
    "start": "103200",
    "end": "106920"
  },
  {
    "text": "behind the scene we will leverage the",
    "start": "106920",
    "end": "109740"
  },
  {
    "text": "model serving framework the framework",
    "start": "109740",
    "end": "111960"
  },
  {
    "text": "aims to make it easier to integrate and",
    "start": "111960",
    "end": "115380"
  },
  {
    "text": "operationalize ml models on open search",
    "start": "115380",
    "end": "117840"
  },
  {
    "text": "to support a variety of ml use cases the",
    "start": "117840",
    "end": "121079"
  },
  {
    "text": "neural plugin released in version 2.5 is",
    "start": "121079",
    "end": "124619"
  },
  {
    "text": "an experimental feature for you to try",
    "start": "124619",
    "end": "126899"
  },
  {
    "text": "out different models to build improved",
    "start": "126899",
    "end": "129000"
  },
  {
    "text": "search experience",
    "start": "129000",
    "end": "131280"
  },
  {
    "text": "for the demo I will upload a pre-trained",
    "start": "131280",
    "end": "133860"
  },
  {
    "text": "model to the open search service cluster",
    "start": "133860",
    "end": "136080"
  },
  {
    "text": "create an ingestion pipeline associated",
    "start": "136080",
    "end": "139379"
  },
  {
    "text": "with the model Define a set of fields",
    "start": "139379",
    "end": "142260"
  },
  {
    "text": "that needs to be converted into Vector",
    "start": "142260",
    "end": "143879"
  },
  {
    "text": "create an index mapping with the same",
    "start": "143879",
    "end": "145980"
  },
  {
    "text": "and associate with a pipeline create an",
    "start": "145980",
    "end": "148620"
  },
  {
    "text": "index using the new mapping",
    "start": "148620",
    "end": "150599"
  },
  {
    "text": "at the time of ingesting the retail",
    "start": "150599",
    "end": "152819"
  },
  {
    "text": "product catalog today you can see how",
    "start": "152819",
    "end": "155520"
  },
  {
    "text": "the predefined fields get converted to",
    "start": "155520",
    "end": "157440"
  },
  {
    "text": "Vector embedding using the neural plugin",
    "start": "157440",
    "end": "160200"
  },
  {
    "text": "once the index is created with the",
    "start": "160200",
    "end": "162120"
  },
  {
    "text": "vector embedding we will perform a",
    "start": "162120",
    "end": "164519"
  },
  {
    "text": "neural query search the neural plugin",
    "start": "164519",
    "end": "166920"
  },
  {
    "text": "will translate user provided text into a",
    "start": "166920",
    "end": "170459"
  },
  {
    "text": "KNL Vector query using the same provided",
    "start": "170459",
    "end": "173819"
  },
  {
    "text": "model ID let's move on to the demo we",
    "start": "173819",
    "end": "177120"
  },
  {
    "text": "are here in devtools as part of Step 1",
    "start": "177120",
    "end": "179700"
  },
  {
    "text": "we will first reset the default plugin",
    "start": "179700",
    "end": "182160"
  },
  {
    "text": "ml comments dot only run on ML node to",
    "start": "182160",
    "end": "185940"
  },
  {
    "text": "set to false the neural search plugin is",
    "start": "185940",
    "end": "188819"
  },
  {
    "text": "an experimental feature and does not",
    "start": "188819",
    "end": "190319"
  },
  {
    "text": "support ml nodes in the GPU instances",
    "start": "190319",
    "end": "192360"
  },
  {
    "text": "just yet so we will run our model on our",
    "start": "192360",
    "end": "195659"
  },
  {
    "text": "data node by setting this parameter to",
    "start": "195659",
    "end": "199379"
  },
  {
    "text": "false once ml nodes are launched on the",
    "start": "199379",
    "end": "202140"
  },
  {
    "text": "open search service you can take",
    "start": "202140",
    "end": "203819"
  },
  {
    "text": "advantage of GPU acceleration on your ml",
    "start": "203819",
    "end": "206940"
  },
  {
    "text": "node you can validate whether the",
    "start": "206940",
    "end": "209580"
  },
  {
    "text": "setting is successful by checking your",
    "start": "209580",
    "end": "212700"
  },
  {
    "text": "cluster settings",
    "start": "212700",
    "end": "215239"
  },
  {
    "text": "Step 2 upload your model to your open",
    "start": "215519",
    "end": "218940"
  },
  {
    "text": "search cluster I have used the",
    "start": "218940",
    "end": "220980"
  },
  {
    "text": "paraphrase multilingual mini LM model",
    "start": "220980",
    "end": "223440"
  },
  {
    "text": "available in the open search",
    "start": "223440",
    "end": "224879"
  },
  {
    "text": "documentation here is a quick reference",
    "start": "224879",
    "end": "227340"
  },
  {
    "text": "for you under pre-trained models the",
    "start": "227340",
    "end": "230519"
  },
  {
    "text": "framework that we saw earlier supports",
    "start": "230519",
    "end": "232980"
  },
  {
    "text": "only text embedding as of the version",
    "start": "232980",
    "end": "234959"
  },
  {
    "text": "2.5 open search only supports the",
    "start": "234959",
    "end": "237900"
  },
  {
    "text": "thought scripts and the Onyx format the",
    "start": "237900",
    "end": "240659"
  },
  {
    "text": "model size is also an important",
    "start": "240659",
    "end": "242640"
  },
  {
    "text": "parameter that we need to specify most",
    "start": "242640",
    "end": "244860"
  },
  {
    "text": "deep learning models are more than 100",
    "start": "244860",
    "end": "246959"
  },
  {
    "text": "MB for this reason open search splits",
    "start": "246959",
    "end": "249720"
  },
  {
    "text": "the model file into smaller chunks to be",
    "start": "249720",
    "end": "252000"
  },
  {
    "text": "stored in the model index so make sure",
    "start": "252000",
    "end": "254459"
  },
  {
    "text": "you correctly size your nodes so that",
    "start": "254459",
    "end": "256500"
  },
  {
    "text": "you have enough memory when making",
    "start": "256500",
    "end": "258239"
  },
  {
    "text": "inference for this demo I have used two",
    "start": "258239",
    "end": "260880"
  },
  {
    "text": "R6 gd4x large nodes but this really",
    "start": "260880",
    "end": "264060"
  },
  {
    "text": "depends on the size of the data set and",
    "start": "264060",
    "end": "266280"
  },
  {
    "text": "the model that you plan to upload the",
    "start": "266280",
    "end": "268080"
  },
  {
    "text": "model configuration includes the model",
    "start": "268080",
    "end": "269940"
  },
  {
    "text": "type embedding dimensions and framework",
    "start": "269940",
    "end": "272580"
  },
  {
    "text": "type the all configuration field is used",
    "start": "272580",
    "end": "275220"
  },
  {
    "text": "for reference purpose you can specify",
    "start": "275220",
    "end": "277620"
  },
  {
    "text": "all model configuration in this field",
    "start": "277620",
    "end": "279360"
  },
  {
    "text": "once the model is uploaded you can use",
    "start": "279360",
    "end": "281520"
  },
  {
    "text": "the get model API generation to get the",
    "start": "281520",
    "end": "284400"
  },
  {
    "text": "all the model configuration stored in",
    "start": "284400",
    "end": "286620"
  },
  {
    "text": "this field the URL is the URL to the",
    "start": "286620",
    "end": "289560"
  },
  {
    "text": "model file and this model file must be",
    "start": "289560",
    "end": "291900"
  },
  {
    "text": "saved as a zip file before you do the",
    "start": "291900",
    "end": "293759"
  },
  {
    "text": "upload now let's execute this command",
    "start": "293759",
    "end": "296639"
  },
  {
    "text": "here in your response you have a task ID",
    "start": "296639",
    "end": "299580"
  },
  {
    "text": "that you need to capture move on to the",
    "start": "299580",
    "end": "302100"
  },
  {
    "text": "next step",
    "start": "302100",
    "end": "304320"
  },
  {
    "text": "use the task ID to get the model ID",
    "start": "304320",
    "end": "307500"
  },
  {
    "text": "and here's the model that is generated I",
    "start": "307500",
    "end": "310199"
  },
  {
    "text": "have the model ID metadata here let's",
    "start": "310199",
    "end": "313080"
  },
  {
    "text": "capture the model ID",
    "start": "313080",
    "end": "315419"
  },
  {
    "text": "and we would move on to step three where",
    "start": "315419",
    "end": "318540"
  },
  {
    "text": "I need to load this model the load model",
    "start": "318540",
    "end": "320940"
  },
  {
    "text": "operation that we see here reads the",
    "start": "320940",
    "end": "323699"
  },
  {
    "text": "model chunks from the model index and",
    "start": "323699",
    "end": "325680"
  },
  {
    "text": "then creates an instance of the model to",
    "start": "325680",
    "end": "328020"
  },
  {
    "text": "load into the memory the bigger the",
    "start": "328020",
    "end": "330060"
  },
  {
    "text": "model more will be the number of chunks",
    "start": "330060",
    "end": "332220"
  },
  {
    "text": "generated and more the number of chunks",
    "start": "332220",
    "end": "334620"
  },
  {
    "text": "longer it would take to load the model",
    "start": "334620",
    "end": "337380"
  },
  {
    "text": "into the memory let's execute this",
    "start": "337380",
    "end": "339539"
  },
  {
    "text": "command on step 3. on your response you",
    "start": "339539",
    "end": "343139"
  },
  {
    "text": "have a task ID capture the task ID so",
    "start": "343139",
    "end": "346440"
  },
  {
    "text": "that we can track the status of this",
    "start": "346440",
    "end": "349199"
  },
  {
    "text": "task",
    "start": "349199",
    "end": "351000"
  },
  {
    "text": "so let's check the status of the load as",
    "start": "351000",
    "end": "354840"
  },
  {
    "text": "you can see the response of the task",
    "start": "354840",
    "end": "357120"
  },
  {
    "text": "status will include a model ID if the",
    "start": "357120",
    "end": "360000"
  },
  {
    "text": "state is changed to completed now make",
    "start": "360000",
    "end": "362699"
  },
  {
    "text": "sure you capture this model ID so that",
    "start": "362699",
    "end": "365160"
  },
  {
    "text": "we can use in all the following steps",
    "start": "365160",
    "end": "367979"
  },
  {
    "text": "let's move on to step 4 where we would",
    "start": "367979",
    "end": "370620"
  },
  {
    "text": "create the neural pipeline here let's",
    "start": "370620",
    "end": "372840"
  },
  {
    "text": "create a neural pipeline which is a",
    "start": "372840",
    "end": "375120"
  },
  {
    "text": "ninjas Pipeline and it is associated",
    "start": "375120",
    "end": "377280"
  },
  {
    "text": "with the model that you've just created",
    "start": "377280",
    "end": "379139"
  },
  {
    "text": "and all the fields in your index that",
    "start": "379139",
    "end": "381600"
  },
  {
    "text": "needs to be vectorized or specified here",
    "start": "381600",
    "end": "384300"
  },
  {
    "text": "now let's quickly create the neural",
    "start": "384300",
    "end": "386340"
  },
  {
    "text": "pipeline once that is completed let's",
    "start": "386340",
    "end": "388620"
  },
  {
    "text": "move on and the step 5 to create your",
    "start": "388620",
    "end": "391620"
  },
  {
    "text": "new index the new index semantic demo",
    "start": "391620",
    "end": "394800"
  },
  {
    "text": "store will be associated with a neural",
    "start": "394800",
    "end": "397139"
  },
  {
    "text": "pipeline that you just created and also",
    "start": "397139",
    "end": "399300"
  },
  {
    "text": "because the index maps to the KN and",
    "start": "399300",
    "end": "401580"
  },
  {
    "text": "Vector field the index setting field",
    "start": "401580",
    "end": "404400"
  },
  {
    "text": "index KNN will be set to True also for",
    "start": "404400",
    "end": "407699"
  },
  {
    "text": "those fields that are defined in the",
    "start": "407699",
    "end": "409740"
  },
  {
    "text": "pipeline as the fields that needs to be",
    "start": "409740",
    "end": "411720"
  },
  {
    "text": "vectorized use the KNN method definition",
    "start": "411720",
    "end": "414419"
  },
  {
    "text": "to specify and declare the type",
    "start": "414419",
    "end": "417240"
  },
  {
    "text": "Dimension and Method let's quickly",
    "start": "417240",
    "end": "419639"
  },
  {
    "text": "create the index",
    "start": "419639",
    "end": "421800"
  },
  {
    "text": "the index is created and let's move on",
    "start": "421800",
    "end": "424800"
  },
  {
    "text": "to ingestion step 6 ingest the required",
    "start": "424800",
    "end": "428400"
  },
  {
    "text": "retail demo store or the product catalog",
    "start": "428400",
    "end": "430919"
  },
  {
    "text": "into your semantic demo store there are",
    "start": "430919",
    "end": "433680"
  },
  {
    "text": "two ways to ingest into the semantic",
    "start": "433680",
    "end": "435660"
  },
  {
    "text": "demo store you can either re-index from",
    "start": "435660",
    "end": "438660"
  },
  {
    "text": "the existing product catalog or you can",
    "start": "438660",
    "end": "441479"
  },
  {
    "text": "do a bulk ingest from your source of",
    "start": "441479",
    "end": "444539"
  },
  {
    "text": "truth I'm going to be re-indexed from",
    "start": "444539",
    "end": "446759"
  },
  {
    "text": "the existing catalog and let's do that",
    "start": "446759",
    "end": "448560"
  },
  {
    "text": "now we are done the semantic demo store",
    "start": "448560",
    "end": "451199"
  },
  {
    "text": "is ingested with",
    "start": "451199",
    "end": "454220"
  },
  {
    "text": "2465 products and let's quickly see if",
    "start": "454220",
    "end": "457800"
  },
  {
    "text": "your vector embeddings are generated",
    "start": "457800",
    "end": "460259"
  },
  {
    "text": "so your vectors the description",
    "start": "460259",
    "end": "462000"
  },
  {
    "text": "underscore V and the name underscore V",
    "start": "462000",
    "end": "465360"
  },
  {
    "text": "are generated by your neural plugin now",
    "start": "465360",
    "end": "468780"
  },
  {
    "text": "let's move on and look at neural search",
    "start": "468780",
    "end": "471240"
  },
  {
    "text": "we are back at the open search dashboard",
    "start": "471240",
    "end": "473340"
  },
  {
    "text": "and we will be using the search",
    "start": "473340",
    "end": "475139"
  },
  {
    "text": "relevance plugin to compare the results",
    "start": "475139",
    "end": "477599"
  },
  {
    "text": "between vanilla search and a neural",
    "start": "477599",
    "end": "479819"
  },
  {
    "text": "search on your left side is a simple",
    "start": "479819",
    "end": "481979"
  },
  {
    "text": "vanilla search that I am going to search",
    "start": "481979",
    "end": "483780"
  },
  {
    "text": "against the semantic demo store index",
    "start": "483780",
    "end": "486060"
  },
  {
    "text": "that we just created and also on your",
    "start": "486060",
    "end": "488759"
  },
  {
    "text": "right side is the neural query that we",
    "start": "488759",
    "end": "491460"
  },
  {
    "text": "will search against all your vector",
    "start": "491460",
    "end": "493560"
  },
  {
    "text": "embedding so let's quickly do red shirt",
    "start": "493560",
    "end": "497330"
  },
  {
    "text": "[Music]",
    "start": "497330",
    "end": "498720"
  },
  {
    "text": "and compare the results now let's",
    "start": "498720",
    "end": "501180"
  },
  {
    "text": "compare the results between the two",
    "start": "501180",
    "end": "503520"
  },
  {
    "text": "here for the left side on a vanilla",
    "start": "503520",
    "end": "506039"
  },
  {
    "text": "search it's doing a plain text search",
    "start": "506039",
    "end": "507780"
  },
  {
    "text": "full text search and the first result is",
    "start": "507780",
    "end": "511800"
  },
  {
    "text": "good and it matches what you just",
    "start": "511800",
    "end": "513599"
  },
  {
    "text": "searched for and that is uh the behavior",
    "start": "513599",
    "end": "516360"
  },
  {
    "text": "of a bn25 it gives gets the closest",
    "start": "516360",
    "end": "519419"
  },
  {
    "text": "match to the Top by default and so you",
    "start": "519419",
    "end": "522360"
  },
  {
    "text": "got the right results but when you",
    "start": "522360",
    "end": "524279"
  },
  {
    "text": "review the rest of the results you can",
    "start": "524279",
    "end": "525839"
  },
  {
    "text": "see red backpack red handbag over shirt",
    "start": "525839",
    "end": "528540"
  },
  {
    "text": "and red shoes now let's look at the",
    "start": "528540",
    "end": "531240"
  },
  {
    "text": "neural search again we have not",
    "start": "531240",
    "end": "532860"
  },
  {
    "text": "fine-tuned the semantic demo store index",
    "start": "532860",
    "end": "536100"
  },
  {
    "text": "so this is all out of the box the neural",
    "start": "536100",
    "end": "538860"
  },
  {
    "text": "search has an ability to understand the",
    "start": "538860",
    "end": "541320"
  },
  {
    "text": "semantics of your catalog and as you can",
    "start": "541320",
    "end": "544140"
  },
  {
    "text": "see it surfaced red shirt red over shirt",
    "start": "544140",
    "end": "548279"
  },
  {
    "text": "and then any color which is similar to",
    "start": "548279",
    "end": "551700"
  },
  {
    "text": "Red so it is able to group pink and",
    "start": "551700",
    "end": "554519"
  },
  {
    "text": "Crimson along with red so the top four",
    "start": "554519",
    "end": "557700"
  },
  {
    "text": "results are much more relevant than what",
    "start": "557700",
    "end": "560339"
  },
  {
    "text": "you would see on a vanilla search better",
    "start": "560339",
    "end": "562980"
  },
  {
    "text": "yet let me change the search term to",
    "start": "562980",
    "end": "565800"
  },
  {
    "text": "Something in French",
    "start": "565800",
    "end": "568019"
  },
  {
    "text": "now that's a black shirt or a black",
    "start": "568019",
    "end": "570300"
  },
  {
    "text": "dress so as you can see on my left side",
    "start": "570300",
    "end": "572940"
  },
  {
    "text": "on a vanilla search there is no",
    "start": "572940",
    "end": "575100"
  },
  {
    "text": "understanding of the language and the",
    "start": "575100",
    "end": "577500"
  },
  {
    "text": "terms chemise and Noir does not exist in",
    "start": "577500",
    "end": "580260"
  },
  {
    "text": "my catalog so I get zero results on the",
    "start": "580260",
    "end": "582959"
  },
  {
    "text": "other hand when you're doing uh a neural",
    "start": "582959",
    "end": "585600"
  },
  {
    "text": "search I have used the sentence",
    "start": "585600",
    "end": "587820"
  },
  {
    "text": "Transformer which is multilingual so",
    "start": "587820",
    "end": "590399"
  },
  {
    "text": "what happens is the the shamis Noir is",
    "start": "590399",
    "end": "594600"
  },
  {
    "text": "translated into respective Vector",
    "start": "594600",
    "end": "596700"
  },
  {
    "text": "embedding by the neural plugin and all",
    "start": "596700",
    "end": "599040"
  },
  {
    "text": "the documents that matches like black",
    "start": "599040",
    "end": "601620"
  },
  {
    "text": "coat and black shirt black jacket or",
    "start": "601620",
    "end": "604440"
  },
  {
    "text": "Surface to the top thereby showing you",
    "start": "604440",
    "end": "607140"
  },
  {
    "text": "that we can do a multilingual search",
    "start": "607140",
    "end": "609360"
  },
  {
    "text": "without having to use any tokenizers or",
    "start": "609360",
    "end": "613260"
  },
  {
    "text": "filters this brings us to the end of the",
    "start": "613260",
    "end": "615360"
  },
  {
    "text": "demo reach out to the open search",
    "start": "615360",
    "end": "616920"
  },
  {
    "text": "service team now to enable the neural",
    "start": "616920",
    "end": "619019"
  },
  {
    "text": "search plugin on the open search service",
    "start": "619019",
    "end": "620880"
  },
  {
    "text": "cluster hope you will have fun exploring",
    "start": "620880",
    "end": "623279"
  },
  {
    "text": "the neural plugin and the neural search",
    "start": "623279",
    "end": "624959"
  },
  {
    "text": "thank you",
    "start": "624959",
    "end": "627740"
  }
]