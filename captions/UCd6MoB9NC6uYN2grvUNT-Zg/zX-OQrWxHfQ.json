[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "In this video, you’ll see how to join data\n from Change Data Capture (CDC) and",
    "start": "0",
    "end": "4537"
  },
  {
    "text": "streaming sources to enhance analytics.",
    "start": "4537",
    "end": "6597"
  },
  {
    "text": "Using Amazon Kinesis Data Analytics\n for Apache Flink, you can create a",
    "start": "7350",
    "end": "11391"
  },
  {
    "text": "Studio notebook for interactive data \nanalytics, query CDC data from a",
    "start": "11391",
    "end": "15632"
  },
  {
    "text": "relational database, and use JOIN queries \nto enrich CDC data with streaming data.",
    "start": "15632",
    "end": "20568"
  },
  {
    "text": "This table illustrates the performance \nand complexity trade-offs between",
    "start": "21909",
    "end": "25150"
  },
  {
    "start": "22000",
    "end": "34000"
  },
  {
    "text": "several streaming data \nenrichment patterns in Apache Flink.",
    "start": "25150",
    "end": "28078"
  },
  {
    "text": "For this video, we’ll demonstrate \njoins using the Table/SQL API pattern.",
    "start": "28784",
    "end": "32857"
  },
  {
    "text": "We’ll be using Amazon Kinesis Data \nAnalytics for Apache Flink to read and",
    "start": "34398",
    "end": "38203"
  },
  {
    "text": "join data from two sources:",
    "start": "38203",
    "end": "40304"
  },
  {
    "text": "an AmazonAurora MySQL relational \ndatabase and an Amazon Kinesis Data Stream.",
    "start": "40304",
    "end": "44894"
  },
  {
    "text": "A CDC connector allows Apache Flink \nto ingest the initial data from Amazon",
    "start": "45540",
    "end": "49430"
  },
  {
    "text": "Aurora MySQL, and thereafter \ncapture only the changed records.",
    "start": "49430",
    "end": "53222"
  },
  {
    "text": "Apache Flink also ingests events \nfrom an Amazon Kinesis Data Stream.",
    "start": "54000",
    "end": "57695"
  },
  {
    "text": "Enriched data resulting from transformations \nor aggregation within Amazon Kinesis Data",
    "start": "58777",
    "end": "63312"
  },
  {
    "text": "Analytics for Apache Flink is then \nsent to Amazon Simple Storage",
    "start": "63312",
    "end": "67024"
  },
  {
    "text": "Service (Amazon S3) for storage \nand use in further analytics.",
    "start": "67024",
    "end": "70775"
  },
  {
    "start": "72000",
    "end": "171000"
  },
  {
    "text": "Let’s start by navigating to Amazon \nKinesis Data Analytics Studio.",
    "start": "73021",
    "end": "76507"
  },
  {
    "text": "Studio notebook allows us to query \ndata streams in real-time using SQL,",
    "start": "79459",
    "end": "83041"
  },
  {
    "text": "Python or Scala, to build a \nstream processing application.",
    "start": "83041",
    "end": "86319"
  },
  {
    "text": "Let’s create a new Studio notebook.",
    "start": "86789",
    "end": "88528"
  },
  {
    "text": "We’ll choose to create our \nnotebook with custom settings.",
    "start": "90951",
    "end": "93313"
  },
  {
    "text": "Let's give our application a name.",
    "start": "94408",
    "end": "95895"
  },
  {
    "text": "We’ll leave the default \nApache Flink runtime selected.",
    "start": "98435",
    "end": "101079"
  },
  {
    "text": "The notebook will create an AWS Identity\nand Access Management (IAM) role for us.",
    "start": "103021",
    "end": "107776"
  },
  {
    "text": "We'll need to specify an AWS Glue \ndatabase that defines metadata for",
    "start": "108870",
    "end": "112440"
  },
  {
    "text": "our sources and destinations.",
    "start": "112440",
    "end": "114164"
  },
  {
    "text": "We’ll select the default.",
    "start": "114376",
    "end": "115515"
  },
  {
    "text": "Let’s proceed to the next step.",
    "start": "116680",
    "end": "118185"
  },
  {
    "text": "We’ll retain the default Scaling and \nLogging and monitoring settings",
    "start": "121529",
    "end": "124848"
  },
  {
    "text": "and move our focus to Networking.",
    "start": "124848",
    "end": "126497"
  },
  {
    "text": "The first option, Custom VPC configuration, \nrequires a private routable network path from",
    "start": "127650",
    "end": "132692"
  },
  {
    "text": "the Studio notebook to the database VPC.",
    "start": "132692",
    "end": "135075"
  },
  {
    "text": "To demo what it would look like if your \nsource of streaming was Amazon MSK,",
    "start": "136286",
    "end": "139924"
  },
  {
    "text": "we’ll select VPC configuration based on\n Amazon MSK cluster, which places the",
    "start": "139924",
    "end": "145073"
  },
  {
    "text": "Studio notebook in the same VPC as \nthe Amazon MSK and source databases.",
    "start": "145074",
    "end": "149513"
  },
  {
    "text": "Let’s specify the Amazon MSK cluster.",
    "start": "150619",
    "end": "152928"
  },
  {
    "text": "The cluster we selected has three subnets.",
    "start": "155809",
    "end": "158000"
  },
  {
    "text": "Several AWS-supported connectors \nare added by default out of the box.",
    "start": "159106",
    "end": "163022"
  },
  {
    "text": "In addition, we’ll be adding a custom \nconnector for capturing CDC data from",
    "start": "164163",
    "end": "167691"
  },
  {
    "text": "our Amazon Aurora MySQL database.",
    "start": "167691",
    "end": "170000"
  },
  {
    "start": "171000",
    "end": "244000"
  },
  {
    "text": "This GitHub page provides a variety \nof CDC Connectors for Apache Flink.",
    "start": "171882",
    "end": "175754"
  },
  {
    "text": "You can access the connectors \nand their documentation here.",
    "start": "176342",
    "end": "178805"
  },
  {
    "text": "For the purposes of this demonstration, \nwe have already downloaded the mysql-cdc",
    "start": "180075",
    "end": "184039"
  },
  {
    "text": "connector and placed it \nin an Amazon S3 bucket.",
    "start": "184039",
    "end": "186867"
  },
  {
    "text": "Here’s the connector.",
    "start": "188000",
    "end": "188842"
  },
  {
    "text": "Let’s copy the bucket’s S3 URI so we can\n add this connector to our Studio notebook.",
    "start": "189253",
    "end": "193651"
  },
  {
    "text": "Now, we’ll add our custom connector.",
    "start": "196685",
    "end": "198295"
  },
  {
    "text": "First, we’ll choose the Amazon \nS3 bucket of our connector.",
    "start": "199530",
    "end": "202345"
  },
  {
    "text": "We’ll paste in the path to the S3 object.",
    "start": "204203",
    "end": "206390"
  },
  {
    "text": "We can remove the bucket information, \nwhich is already specified in the box above.",
    "start": "207495",
    "end": "211064"
  },
  {
    "text": "Let’s save these changes.",
    "start": "212240",
    "end": "213521"
  },
  {
    "text": "The custom connector has been \nadded to the notebook configurations.",
    "start": "217249",
    "end": "220081"
  },
  {
    "text": "Let’s continue.",
    "start": "220082",
    "end": "220913"
  },
  {
    "text": "Here we’ll quickly review our chosen \nsettings and create the Studio notebook.",
    "start": "222736",
    "end": "226209"
  },
  {
    "text": "Our Studio notebook has been created.",
    "start": "227927",
    "end": "229773"
  },
  {
    "text": "It takes a few minutes to create \nthe underlying Apache Flink cluster.",
    "start": "230220",
    "end": "233347"
  },
  {
    "text": "For our purposes, we’ll proceed with an existing\n Studio notebook that has the same settings.",
    "start": "234546",
    "end": "238527"
  },
  {
    "text": "Let’s open the notebook in Apache Zeppelin.",
    "start": "239667",
    "end": "241471"
  },
  {
    "start": "244000",
    "end": "455000"
  },
  {
    "text": "The notebook is already populated with the \ncode we need to interact with our source data.",
    "start": "244352",
    "end": "248000"
  },
  {
    "text": "In the first code block, we’re enabling once \nin a minute checkpointing for the application.",
    "start": "248447",
    "end": "252553"
  },
  {
    "text": "In the next code block, we’ll instantiate a \nMySQL client using the mysql-connector-python",
    "start": "253741",
    "end": "258768"
  },
  {
    "text": "library in order to interact with \nour database from the notebook.",
    "start": "258768",
    "end": "262054"
  },
  {
    "text": "The next two statements will be \nused later to insert a record in our",
    "start": "263206",
    "end": "266041"
  },
  {
    "text": "source database to verify CDC.",
    "start": "266041",
    "end": "268261"
  },
  {
    "text": "The next code block has SQL statements \nwe can use to create tables that correspond",
    "start": "270308",
    "end": "274005"
  },
  {
    "text": "to our two different data sources.",
    "start": "274005",
    "end": "275676"
  },
  {
    "text": "The customer table connects to the MySQL \ndatabase using the mysql-cdc connector.",
    "start": "276064",
    "end": "280563"
  },
  {
    "text": "The order table corresponds to \nour Kinesis data stream using the",
    "start": "281315",
    "end": "284423"
  },
  {
    "text": "built-in kinesis connector for Flink.",
    "start": "284423",
    "end": "286246"
  },
  {
    "text": "The table columns correspond to the \nschema of each event in our stream.",
    "start": "286916",
    "end": "290095"
  },
  {
    "text": "Our new tables have been created and have\n been added to our AWS Glue Data Catalog.",
    "start": "292366",
    "end": "296669"
  },
  {
    "text": "We can now use select queries \nto read from our data sources.",
    "start": "297763",
    "end": "300732"
  },
  {
    "text": "First, we’ll verify the CDC data collector by \nreading from our MySQL customer table.",
    "start": "301920",
    "end": "306580"
  },
  {
    "text": "Let’s filter for customer IDs greater than 1998.",
    "start": "307180",
    "end": "310266"
  },
  {
    "text": "For each query, the Studio notebook \nsubmits a job to the Apache Flink cluster,",
    "start": "311536",
    "end": "315451"
  },
  {
    "text": "which then emits the results to our notebook.",
    "start": "315451",
    "end": "317496"
  },
  {
    "text": "We have selected customers \nwith an ID greater than 1998.",
    "start": "318591",
    "end": "321729"
  },
  {
    "text": "Let’s insert a record with an ID \nof 2002 and see what happens.",
    "start": "322281",
    "end": "325638"
  },
  {
    "text": "The new record immediately \nappears in the results.",
    "start": "333577",
    "end": "335837"
  },
  {
    "text": "The Flink job is ingesting all the \nCDC data and constantly reading",
    "start": "336000",
    "end": "339425"
  },
  {
    "text": "the underlying database changes.",
    "start": "339425",
    "end": "341112"
  },
  {
    "text": "Let’s see what happens \nwhen we update a record.",
    "start": "342206",
    "end": "344085"
  },
  {
    "text": "Let’s update the record that we just \ninserted with by changing the name",
    "start": "345273",
    "end": "348169"
  },
  {
    "text": "from “Customer 2002” to \n“Customer Name 2002.”",
    "start": "348169",
    "end": "351415"
  },
  {
    "text": "As expected, the results were updated.",
    "start": "354120",
    "end": "355912"
  },
  {
    "text": "Next, let’s verify the kinesis connector.",
    "start": "357029",
    "end": "359315"
  },
  {
    "text": "To do this, we’ll select from the order \ntable we created, which reads from our",
    "start": "359704",
    "end": "363301"
  },
  {
    "text": "Amazon Kinesis Data Steam.",
    "start": "363301",
    "end": "364883"
  },
  {
    "text": "Each data event has a customer ID, item\n name, order amount, and an order time.",
    "start": "366106",
    "end": "370421"
  },
  {
    "text": "Every time we run this job, we should see \nall the data events that exist in this stream.",
    "start": "370891",
    "end": "374852"
  },
  {
    "text": "The next block of code enriches these\n data events with the customer information",
    "start": "376059",
    "end": "379612"
  },
  {
    "text": "so we can run real-time analytics to see \nthe total order amount per market segment.",
    "start": "379612",
    "end": "383559"
  },
  {
    "text": "In this query, the select statement joins \nthe customer table with the order table on",
    "start": "384676",
    "end": "389044"
  },
  {
    "text": "the customer_id field, even though the \ntables are coming from different sources.",
    "start": "389044",
    "end": "393100"
  },
  {
    "text": "We also calculate the summation \nof order amounts per customer.",
    "start": "393441",
    "end": "396509"
  },
  {
    "text": "Our results group the totals by customer\n ID, customer name, and market segment.",
    "start": "397685",
    "end": "401786"
  },
  {
    "text": "To see the sum of the order amounts \nper market segment, we can switch",
    "start": "402880",
    "end": "405915"
  },
  {
    "text": "from the table view to a bar chart \nwith market segment as the key field.",
    "start": "405915",
    "end": "409593"
  },
  {
    "text": "We’re can now see order \ntotals for each market segment.",
    "start": "410734",
    "end": "413243"
  },
  {
    "text": "Presently, our stream producer isn’t running,",
    "start": "415383",
    "end": "417561"
  },
  {
    "text": "so we don't see any changes \nfrom events in the data stream.",
    "start": "417561",
    "end": "420244"
  },
  {
    "text": "Now let’s start our stream producer.",
    "start": "421008",
    "end": "422772"
  },
  {
    "text": "As data is received by the Kinesis Data \nStream, it becomes available to Apache Flink.",
    "start": "424077",
    "end": "428467"
  },
  {
    "text": "As more data streams in, the results \nof the calculation update in real time.",
    "start": "428984",
    "end": "432771"
  },
  {
    "text": "You can now add code to send \nthese results to Amazon S3 using",
    "start": "433500",
    "end": "436502"
  },
  {
    "text": "the Flink FileSystem connector.",
    "start": "436502",
    "end": "438279"
  },
  {
    "text": "You’ve just seen how to join data from CDC\nand streaming sources to enhance analytics.",
    "start": "441019",
    "end": "445600"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "446882",
    "end": "450133"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "450521",
    "end": "452906"
  }
]