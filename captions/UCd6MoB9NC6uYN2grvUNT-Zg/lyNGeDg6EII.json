[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "The Amazon Relational Database Service (RDS)\nmakes it easy to set up, operate, and scale a",
    "start": "4233",
    "end": "9733"
  },
  {
    "text": "relational database in the cloud. It provides cost-efficient\nand resizable capacity while automating time-consuming",
    "start": "9733",
    "end": "15733"
  },
  {
    "text": "administration tasks such as hardware provisioning,\ndatabase setup, patching, and backups.",
    "start": "15733",
    "end": "21932"
  },
  {
    "text": "When we think about backups, we often think about\ndisaster recovery or data migration scenarios, but",
    "start": "21933",
    "end": "27233"
  },
  {
    "text": "these aren’t the only use cases.",
    "start": "27233",
    "end": "29033"
  },
  {
    "text": "Let’s consider another common one. Suppose we\nhave an example database called “bank-demo”",
    "start": "29033",
    "end": "34133"
  },
  {
    "text": "running on a db.r5.large instance and taking automated\nnightly snapshots.",
    "start": "34133",
    "end": "39333"
  },
  {
    "text": "Our internal analytics team wants to be able to run a\nhigh volume of intense queries on the data whenever",
    "start": "39333",
    "end": "43933"
  },
  {
    "text": "they want. Can we make this possible for them in a\nsafe, reliable, and cost-effective way?",
    "start": "43933",
    "end": "49532"
  },
  {
    "text": "The answer is “yes!”",
    "start": "49533",
    "end": "51633"
  },
  {
    "text": "In this demo, we’re going to export an Amazon RDS\nsnapshot from this example RDS database to S3, and",
    "start": "51633",
    "end": "57633"
  },
  {
    "text": "then, with the help of AWS Glue, use Amazon Athena\nto perform queries on the exported snapshot.",
    "start": "57633",
    "end": "63533"
  },
  {
    "text": "This will enable us to generate reports, perform\ncomputationally expensive analytical queries,",
    "start": "63533",
    "end": "68533"
  },
  {
    "text": "combine our data from RDS with other sources\nto build data lakes, comply with long-term data ",
    "start": "68533",
    "end": "73632"
  },
  {
    "text": "retention requirements, and much more - all without\nimpacting the performance or availability of our database.",
    "start": "73633",
    "end": "79633"
  },
  {
    "text": "To get started, let's head over to the RDS console, and\nopen the list of available snapshots for our sample",
    "start": "79633",
    "end": "84933"
  },
  {
    "start": "80000",
    "end": "200000"
  },
  {
    "text": "database. We’ll choose the most recent automated \nsystem snapshot, but we could also select any",
    "start": "84933",
    "end": "89833"
  },
  {
    "text": "snapshots we’ve taken manually, or even create a\nbrand new snapshot just for this exercise by using",
    "start": "89833",
    "end": "95133"
  },
  {
    "text": "the “Take snapshot” button. Next, we’ll select\n“Export to Amazon S3” from the “Actions”",
    "start": "95133",
    "end": "100333"
  },
  {
    "text": "dropdown menu button and start configuring\nour export. Let’s call it “bank-demo-export-1”.",
    "start": "100333",
    "end": "107333"
  },
  {
    "text": "Now we need to indicate how much data we’d\nlike to export. We can either export the entire",
    "start": "107333",
    "end": "111933"
  },
  {
    "text": "database, or we can specify only a few tables by\nlisting the schema and name for each table that",
    "start": "111933",
    "end": "116933"
  },
  {
    "text": "we want to include. Let’s go ahead and export\neverything.",
    "start": "116933",
    "end": "120632"
  },
  {
    "text": "We need to choose a destination on S3 for the export,\nbut since we don’t have one yet, let’s go create it.",
    "start": "120633",
    "end": "126633"
  },
  {
    "text": "We’ll call it “bank-demo-exports”, leave\nthe default options alone, and as a general",
    "start": "126633",
    "end": "132533"
  },
  {
    "text": "best practice, leave the “Block all public access”\nsetting checked. Once we’ve done that, we can",
    "start": "132533",
    "end": "139033"
  },
  {
    "text": "go back to the snapshot export form, refresh the\n“S3 Destination” section, and select our new bucket.",
    "start": "139033",
    "end": "145333"
  },
  {
    "text": "Now we need to choose an IAM Role that the\nsnapshot export process can use to access",
    "start": "145333",
    "end": "149933"
  },
  {
    "text": "our S3 bucket. We don’t have one yet, so let’s create\na new role called “bank-demo-exports”.",
    "start": "149933",
    "end": "155833"
  },
  {
    "text": "The role will automatically have this IAM Policy shown\nhere, and that will allow it to read, write, and manage",
    "start": "155833",
    "end": "163032"
  },
  {
    "text": "the snapshot exports for us. To protect\nour exported data, we must also provide ",
    "start": "163033",
    "end": "168533"
  },
  {
    "text": "a customer-managed encryption key from the\nAmazon Key Management Service (KMS).",
    "start": "168533",
    "end": "173033"
  },
  {
    "text": "We can do this by going to the KMS console and\npressing the Create Key button.",
    "start": "173033",
    "end": "178533"
  },
  {
    "text": "We’ll give the key a logical name and description,\nbut since there are no other users in this demo",
    "start": "178533",
    "end": "182733"
  },
  {
    "text": "account, we don’t need any separate key\nadministrators or key users at this time, so",
    "start": "182733",
    "end": "187133"
  },
  {
    "text": "we can leave those options alone. We’ll click\n“Finish” to create the key, but before leaving",
    "start": "187133",
    "end": "191733"
  },
  {
    "text": "KMS, let’s grab a copy of the key ARN, and then\npaste it into the Encryption section back in RDS.",
    "start": "191733",
    "end": "199333"
  },
  {
    "text": "Now we can start the Export.",
    "start": "199333",
    "end": "201333"
  },
  {
    "start": "200000",
    "end": "220000"
  },
  {
    "text": "The amount of time it’ll take to finish will depend\non the size of the snapshot itself, but we can look",
    "start": "201333",
    "end": "205732"
  },
  {
    "text": "in the RDS Console to view the current status. For more\ngranular progress, the `aws rds describe-export-tasks`",
    "start": "205733",
    "end": "212933"
  },
  {
    "text": "command in the AWS CLI will tell us how much data\nhas been extracted and what percentage of the",
    "start": "212933",
    "end": "217833"
  },
  {
    "text": "overall process has been completed thus far.",
    "start": "217833",
    "end": "220933"
  },
  {
    "start": "220000",
    "end": "285000"
  },
  {
    "text": "Now that the export is complete, let’s take a\nlook at what ended up in our S3 bucket.",
    "start": "220933",
    "end": "226233"
  },
  {
    "text": "We have two export info JSON files, and a folder\nwith the same name as our database. The first",
    "start": "226233",
    "end": "231733"
  },
  {
    "text": "info file is the final report of the export task, and\nthe second one breaks the status down for us by",
    "start": "231733",
    "end": "237333"
  },
  {
    "text": "individual table, including overall size and the\ndatatype mappings. Within the “bank_demo”",
    "start": "237333",
    "end": "243032"
  },
  {
    "text": "folder we have separate folders for each individual\ntable that was exported, and under each of those",
    "start": "243033",
    "end": "248233"
  },
  {
    "text": "we’ll see one or more folders corresponding to how\nmany partitions were created during the export",
    "start": "248233",
    "end": "252733"
  },
  {
    "text": "process. “transactions”, being the largest of the\nthree tables, created several hundred partitions,",
    "start": "252733",
    "end": "258532"
  },
  {
    "text": "while the “accounts” and “customers” tables only\nrequired one each. Finally, at the deepest level,",
    "start": "258533",
    "end": "264533"
  },
  {
    "text": "we’ll find the exported data in\nApache Parquet format.",
    "start": "264533",
    "end": "268733"
  },
  {
    "text": "The Parquet format is up to 2x faster to\nexport and consumes up to 6x less",
    "start": "268733",
    "end": "273433"
  },
  {
    "text": "storage in Amazon S3, compared to text\nformats, and we can analyze the exported",
    "start": "273433",
    "end": "278033"
  },
  {
    "text": "data with other AWS services like Amazon Athena,\nAmazon EMR, and Amazon SageMaker.",
    "start": "278033",
    "end": "284432"
  },
  {
    "text": "Let’s set up Athena for that right now.",
    "start": "284433",
    "end": "286833"
  },
  {
    "start": "285000",
    "end": "360000"
  },
  {
    "text": "First, we need to tell Athena where to find the\ndata and what it looks like. Thanks to AWS Glue,",
    "start": "286833",
    "end": "291833"
  },
  {
    "text": "we don’t need to give Athena the details on every\ntable and its properties ourselves though - we can",
    "start": "291833",
    "end": "296233"
  },
  {
    "text": "set up a crawler to go discover that for us.\nLet’s call the crawler “bank_demo” and tell",
    "start": "296233",
    "end": "301233"
  },
  {
    "text": "it that it’s going to crawl through an S3 bucket\nin our account under this path. Next we’ll ask it",
    "start": "301233",
    "end": "306333"
  },
  {
    "text": "to create the appropriate IAM Role for us, and, for\nthe time being, tell it to only crawl our data when we",
    "start": "306333",
    "end": "312133"
  },
  {
    "text": "ask it to. And finally, we’ll ask it to organize the schema\ninformation it discovers under the name “bank_demo”",
    "start": "312133",
    "end": "318633"
  },
  {
    "text": "With that, the crawler is ready to go, but there’s one\nmore important task that we need to complete before",
    "start": "318633",
    "end": "323533"
  },
  {
    "text": "running it. Remember earlier when we configured\nthe KMS key policy, but didn’t need to give anybody",
    "start": "323533",
    "end": "328633"
  },
  {
    "text": "else permission to use the key? That’s no longer\nthe case. This crawler is going to need to crawl",
    "start": "328633",
    "end": "333533"
  },
  {
    "text": "through our exported snapshot data in S3,\nwhich is encrypted, so the crawler, or more",
    "start": "333533",
    "end": "338332"
  },
  {
    "text": "specifically the crawler’s IAM Role, is going\nto need access to use that key. To grant it",
    "start": "338333",
    "end": "343733"
  },
  {
    "text": "access, we just go back to KMS and add the new\nIAM Role as a key user. Now let’s run the crawler!",
    "start": "343733",
    "end": "351033"
  },
  {
    "text": "This will take a few minutes to complete depending\non the size of your data, but you can expect it to",
    "start": "351033",
    "end": "355533"
  },
  {
    "text": "finish considerably faster than the original\nexport to S3 did.",
    "start": "355533",
    "end": "360233"
  },
  {
    "start": "360000",
    "end": "410000"
  },
  {
    "text": "Now let’s go back to Athena\nand run some queries!",
    "start": "360233",
    "end": "363233"
  },
  {
    "text": "Amazon Athena supports a subset of SQL, and\nyou can refer to the Athena documentation for",
    "start": "363233",
    "end": "367733"
  },
  {
    "text": "a full reference. If you’ve ever worked with SQL\nbefore though, you’ll find the query editor fairly",
    "start": "367733",
    "end": "373133"
  },
  {
    "text": "straightforward. As you can see, we’ve just counted\nup all of the accounts, found a random customer,",
    "start": "373133",
    "end": "379733"
  },
  {
    "text": "and looked up the ten most recent transactions\nin all of their accounts using the same general",
    "start": "379733",
    "end": "384233"
  },
  {
    "text": "query structure that we’re accustomed to.",
    "start": "384233",
    "end": "387233"
  },
  {
    "text": "The best part is that all of these queries we’re running\nare being executed against the exported data that we",
    "start": "387233",
    "end": "391233"
  },
  {
    "text": "have in S3. Whether we need to run a high volume\nof analytical queries, or even just a handful of very",
    "start": "391233",
    "end": "396433"
  },
  {
    "text": "slow queries involving columns that aren’t optimized\nfor search, we know that there’s going to be absolutely",
    "start": "396433",
    "end": "401633"
  },
  {
    "text": "no impact to the database itself.",
    "start": "401633",
    "end": "404733"
  },
  {
    "text": "That’s all great, but we don’t want to have to be\nexporting RDS snapshots to S3 manually every time.",
    "start": "404733",
    "end": "409533"
  },
  {
    "text": "Let’s automate this!",
    "start": "409533",
    "end": "411432"
  },
  {
    "start": "410000",
    "end": "465000"
  },
  {
    "text": "Amazon RDS uses the Amazon Simple Notification\nService (SNS) to publish notifications when certain",
    "start": "411433",
    "end": "417533"
  },
  {
    "text": "RDS events occur. These notifications can\ntrigger an AWS Lambda function to start a",
    "start": "417533",
    "end": "423033"
  },
  {
    "text": "snapshot export, and we can then use an\nAWS Glue crawler to make the snapshot",
    "start": "423033",
    "end": "427233"
  },
  {
    "text": "data available for querying with Amazon Athena.",
    "start": "427233",
    "end": "430433"
  },
  {
    "text": "We’ll use the code provided in the \n`aws-samples/rds-snapshot-export-to-s3-pipeline`",
    "start": "430433",
    "end": "436833"
  },
  {
    "text": "repository to deploy an example of this setup.",
    "start": "436833",
    "end": "439833"
  },
  {
    "text": "After giving it the name of our database and the\nname of an S3 bucket to create for the exports,",
    "start": "439833",
    "end": "445833"
  },
  {
    "text": "the Amazon Cloud Development Kit (CDK) will\nhandle deploying everything.",
    "start": "445833",
    "end": "450533"
  },
  {
    "text": "Step-by-step instructions on how to do this are\nprovided in the repository, so we won’t walk",
    "start": "450533",
    "end": "455633"
  },
  {
    "text": "through all of that here, but we can see that\nafter a few minutes, CDK has created",
    "start": "455633",
    "end": "459733"
  },
  {
    "text": "everything we need to automatically export the\n“bank-demo” database’s automated snapshots to S3.",
    "start": "459733",
    "end": "465733"
  },
  {
    "text": "Now let’s wait a few days and see what happens...",
    "start": "465733",
    "end": "469733"
  },
  {
    "text": "... OK, great!",
    "start": "469733",
    "end": "471133"
  },
  {
    "text": "Our Lambda function has been exporting all\nof the snapshots that took place after the",
    "start": "471133",
    "end": "474633"
  },
  {
    "text": "pipeline was set up, and we can now run our\nGlue crawler to make this data available to us",
    "start": "474633",
    "end": "479133"
  },
  {
    "text": "in Athena. That’s going to take a little bit longer\nthis time, since the crawler has more data to go",
    "start": "479133",
    "end": "483633"
  },
  {
    "text": "through, but to avoid this in the future, we could\nschedule the Glue crawler to run automatically",
    "start": "483633",
    "end": "488633"
  },
  {
    "text": "each morning shortly after we expect our snapshot\nexports to complete. Our “On Demand” crawler just",
    "start": "488633",
    "end": "494333"
  },
  {
    "text": "finished, though, so let’s see what’s\navailable now in Athena.",
    "start": "494333",
    "end": "497833"
  },
  {
    "text": "Unlike in the previous example, where we had one\ntable in the Glue data catalog corresponding to",
    "start": "497833",
    "end": "502633"
  },
  {
    "text": "each table that we had in the “bank-demo” database,\nwe now have several instances of each of those tables,",
    "start": "502633",
    "end": "508432"
  },
  {
    "text": "which correspond to the different days’ snapshots.\nThis means that we can run queries on not just the",
    "start": "508433",
    "end": "514133"
  },
  {
    "text": "latest snapshot, but also on the earlier versions\nof our database in the older snapshots.",
    "start": "514133",
    "end": "520533"
  },
  {
    "text": "These snapshot exports in S3 and the tables in the\nAWS Glue data catalog will build up over time though,",
    "start": "520533",
    "end": "526733"
  },
  {
    "text": "but you can clean them up automatically by using\nAmazon S3 object lifecycle policies, which will",
    "start": "526733",
    "end": "532433"
  },
  {
    "text": "transition your snapshot exports through the different\ntiers of storage available such Infrequent Access,",
    "start": "532433",
    "end": "537933"
  },
  {
    "text": "Glacier, or Deep Archive automatically as they age.",
    "start": "537933",
    "end": "542532"
  },
  {
    "text": "For more information on the S3 lifecycle policies,\nthe RDS Snapshot Export to S3 feature, or any of",
    "start": "542533",
    "end": "549233"
  },
  {
    "text": "the other services mentioned in this demo, please\nvisit the links in the video description below.",
    "start": "549233",
    "end": "554233"
  },
  {
    "text": "Thanks for watching!",
    "start": "554233",
    "end": "555633"
  }
]