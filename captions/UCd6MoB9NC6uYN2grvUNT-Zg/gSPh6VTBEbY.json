[
  {
    "text": "hello and welcome to today's AWS master class about elastic MapReduce my name is",
    "start": "1490",
    "end": "7830"
  },
  {
    "text": "ryan Shuttleworth and I'm a Technical Evangelist here at AWS and I'm gonna be your host today on this webinar so",
    "start": "7830",
    "end": "13650"
  },
  {
    "text": "today's webinar is a masterclass on elastic MapReduce and the master class series that we're doing is really about",
    "start": "13650",
    "end": "19740"
  },
  {
    "text": "doing a technical deep dive beyond the basics of a high-level service overview to help you educate you on how to get",
    "start": "19740",
    "end": "26400"
  },
  {
    "text": "the best from AWS technologies or sets of technologies and show you how things work and how to get things done and in",
    "start": "26400",
    "end": "32369"
  },
  {
    "text": "45 minutes were hopefully broaden your knowledge and give you enough to then go look for more information online and do",
    "start": "32369",
    "end": "38250"
  },
  {
    "text": "some of the tutorials that we have as well and show you some of the sample applications so today's topic is elastic",
    "start": "38250",
    "end": "45090"
  },
  {
    "text": "MapReduce and it's a key tool in the toolbox to help with big data challenges",
    "start": "45090",
    "end": "50219"
  },
  {
    "text": "and it makes possible processes and analytics in a timely fashion that previously weren't feasible because they",
    "start": "50219",
    "end": "56070"
  },
  {
    "text": "maybe took too long or they were too expensive to build clusters yourself in your own environments to achieve things",
    "start": "56070",
    "end": "62070"
  },
  {
    "text": "in a cost effective and timely fashion and elastic MapReduce is is cost effective when leverage with ec2 and the",
    "start": "62070",
    "end": "68520"
  },
  {
    "text": "spot market and that's one of the things we're going to talk about today and there's a broader ecosystem of tools to",
    "start": "68520",
    "end": "73979"
  },
  {
    "text": "handle specific use cases and we're going to dive into a couple of those today now elastic MapReduce is a pretty",
    "start": "73979",
    "end": "79799"
  },
  {
    "text": "expensive topic so in 45 minutes we're not going to touch everything but hopefully this will whet your appetite",
    "start": "79799",
    "end": "85350"
  },
  {
    "text": "and give you enough to get going get set up using the console or some of the command-line tools and use some of the",
    "start": "85350",
    "end": "91170"
  },
  {
    "text": "technologies like hive on top of MapReduce to do interactive query so",
    "start": "91170",
    "end": "96990"
  },
  {
    "text": "what is earmark elastic MapReduce well fundamentally it's Hadoop as a service",
    "start": "96990",
    "end": "102079"
  },
  {
    "text": "and Hadoop is a an open source framework for paralyzation of data and we'll come",
    "start": "102079",
    "end": "107130"
  },
  {
    "text": "to a sort of an overview at high level of what that's all about fundamentally it's this thing called MapReduce and it's a MapReduce engine",
    "start": "107130",
    "end": "113490"
  },
  {
    "text": "under the covers where you can map data in an aggregate data in the simplest fashion and the two-step process it's",
    "start": "113490",
    "end": "119880"
  },
  {
    "text": "integrated with tools and they will use queries type languages on top very large data sets and because it's a massively",
    "start": "119880",
    "end": "126990"
  },
  {
    "text": "parallel engine under the covers you can achieve near linear scalability the more resources you put uh something",
    "start": "126990",
    "end": "133260"
  },
  {
    "text": "your data analytics jobs will go and it's integrated into a double your services like like ec2 like s3 like",
    "start": "133260",
    "end": "140730"
  },
  {
    "text": "dynamodb and it's a cost-effective wrapper if you like around that open source framework to allow you to do big",
    "start": "140730",
    "end": "146970"
  },
  {
    "text": "data analytics in a timely fashion so EMR self and hadoop if you like is a",
    "start": "146970",
    "end": "154650"
  },
  {
    "text": "framework that splits data into lots of pieces let's some processing and curl and gathers the results in a simplest",
    "start": "154650",
    "end": "160920"
  },
  {
    "text": "use case that's what it's all about and because of that in the Alinea scalability the more horsepower you",
    "start": "160920",
    "end": "165989"
  },
  {
    "text": "throw at that processing the quicker you can get results out of it in a really",
    "start": "165989",
    "end": "171150"
  },
  {
    "text": "common use case would be doing something like click log analysis or Apache log analysis so let's say you have a very",
    "start": "171150",
    "end": "177810"
  },
  {
    "text": "large click log maybe terabytes in size and inside there there's lots of actions by a user",
    "start": "177810",
    "end": "183030"
  },
  {
    "text": "maybe these are actions on a web application maybe these are ad impressions maybe on a particular site",
    "start": "183030",
    "end": "188099"
  },
  {
    "text": "and you can save lots of actions contained in a lot of lot of data an EMR",
    "start": "188099",
    "end": "193799"
  },
  {
    "text": "lets you split that log into many small pieces and it does that as part of the framework and process them in a cluster",
    "start": "193799",
    "end": "200280"
  },
  {
    "text": "of ec2 instances and then aggregate the results from some sort of process and",
    "start": "200280",
    "end": "205470"
  },
  {
    "text": "you've done across all those little chunks of data and combine them together to ultimately find out what John Smith",
    "start": "205470",
    "end": "210959"
  },
  {
    "text": "did in this case and because you can spray horsepower at it you can get to that answer in a fraction of the time",
    "start": "210959",
    "end": "216389"
  },
  {
    "text": "that you might do otherwise with a small cluster or a single machine and it's all",
    "start": "216389",
    "end": "222120"
  },
  {
    "text": "about this thing called Hadoop and that's the Hadoop logo there the yellow elephant and there's two sort of fundamental pieces to Hadoop there's",
    "start": "222120",
    "end": "229199"
  },
  {
    "text": "what we call HDFS which is a file system underneath the covers which is a cluster file system for reliable storage of data",
    "start": "229199",
    "end": "236430"
  },
  {
    "text": "across a process in cluster and then there's the MapReduce element which enables you to do data analysis it",
    "start": "236430",
    "end": "242819"
  },
  {
    "text": "enables you to do some sort of processing on a day sir and then maybe aggregate those data across all the",
    "start": "242819",
    "end": "248069"
  },
  {
    "text": "nodes that you've processed across now the two fundamental pieces that will aggress in turn over this presentation",
    "start": "248069",
    "end": "253609"
  },
  {
    "text": "but first off you know big data is not Hadoop big data is an expansive topic",
    "start": "253609",
    "end": "259320"
  },
  {
    "text": "and big deep data is defined in a number of ways but there's a very common one there which is three V's it's about data",
    "start": "259320",
    "end": "265889"
  },
  {
    "text": "with very high volley data petabyte scale potentially data",
    "start": "265889",
    "end": "271260"
  },
  {
    "text": "that may be changing over very high velocities such as aggregating logs off many application servers on a high",
    "start": "271260",
    "end": "277650"
  },
  {
    "text": "availability and a high scale application on the web and data with a high degree of variety now there are",
    "start": "277650",
    "end": "284400"
  },
  {
    "text": "other definitions some organizations might talk about data we have particularly low densities such as maybe",
    "start": "284400",
    "end": "290370"
  },
  {
    "text": "a Twitter stream or something like that where there's very small pieces of information containing a very large",
    "start": "290370",
    "end": "296040"
  },
  {
    "text": "volume that are relevant to you and indeed some people add a fourth V which is validity to try and answer that sort",
    "start": "296040",
    "end": "303060"
  },
  {
    "text": "of angle about what data is relevant to you or not but big data is fundamentally not to dupe big data is when you start",
    "start": "303060",
    "end": "310500"
  },
  {
    "text": "to need to innovate to collect store analyze and manage your data when data gets to certain scale that you have to",
    "start": "310500",
    "end": "316350"
  },
  {
    "text": "be clever about how you get data into an environment where you store it that's one platform to use for that how you",
    "start": "316350",
    "end": "322260"
  },
  {
    "text": "analyze it and there's a subsequent management of your data sets now today we're going to focus on that analyze",
    "start": "322260",
    "end": "327419"
  },
  {
    "text": "stage with EMR but I will touch upon how you get your data and store your data as",
    "start": "327419",
    "end": "332430"
  },
  {
    "text": "well just to frame big data and put EMR in that context so how do you get your",
    "start": "332430",
    "end": "337919"
  },
  {
    "text": "slice of big data there's a whole load of services you can use but some really common ones talking about very large",
    "start": "337919",
    "end": "343110"
  },
  {
    "text": "data sets you might have in your environment and not in the cloud customers will use things like AWS",
    "start": "343110",
    "end": "348780"
  },
  {
    "text": "Direct Connect putting a high-bandwidth low-latency pipe between their environment and the AWS cloud and they",
    "start": "348780",
    "end": "355440"
  },
  {
    "text": "will need to push data up in very high volumes and high speeds into the cloud to do things like EMR analysis we also",
    "start": "355440",
    "end": "362280"
  },
  {
    "text": "have a service where you can actually ship physical disk drives to us called AWS import/export so putting them on a cardboard box on the back of a FedEx van",
    "start": "362280",
    "end": "369150"
  },
  {
    "text": "is extremely high bandwidth and you can get very large data sets seeded into the",
    "start": "369150",
    "end": "374220"
  },
  {
    "text": "cloud so you can start doing analytics that way some customers will use queueing like sqs because it's also a",
    "start": "374220",
    "end": "380130"
  },
  {
    "text": "highly scalable event buffering system so they can throw stuff from their application servers into sqs and then",
    "start": "380130",
    "end": "386370"
  },
  {
    "text": "slowly process them off those queues into storage and then do analysis and all our customers will use things like",
    "start": "386370",
    "end": "391800"
  },
  {
    "text": "storage gateway to synchronize local data sets into the cloud so they can spin up clusters like EMR clusters that",
    "start": "391800",
    "end": "397770"
  },
  {
    "text": "will show you how to do later there are other storage homages as well an s3 on the far right",
    "start": "397770",
    "end": "403090"
  },
  {
    "text": "is one that will come out a lot in this webinar because s3 is a fundamental sort",
    "start": "403090",
    "end": "408520"
  },
  {
    "text": "of backbone to EMR there's an object data store you can have as many objects as you like up to five terabytes per",
    "start": "408520",
    "end": "414340"
  },
  {
    "text": "object stored with very high durability so a hugely scalable web storage platform and then there are other things",
    "start": "414340",
    "end": "420340"
  },
  {
    "text": "like RDS relational database on the left and then some no sequel stores in the middle there with DynamoDB being our",
    "start": "420340",
    "end": "426280"
  },
  {
    "text": "very high performance provision throughput databases now all of these technologies come into play in some",
    "start": "426280",
    "end": "431440"
  },
  {
    "text": "shape or form when we talk about EMR because once you have data in the cloud such as putting it into s3 it becomes a",
    "start": "431440",
    "end": "438009"
  },
  {
    "text": "source and an output destination for EMR results likewise DynamoDB can be used in",
    "start": "438009",
    "end": "443889"
  },
  {
    "text": "that context and then moving data out once you've analyzed them into things like RDS will rather stand a sequel at",
    "start": "443889",
    "end": "449979"
  },
  {
    "text": "the top of as well as a very common use case so that's where you put your data once you've got it in the cloud so let's",
    "start": "449979",
    "end": "456759"
  },
  {
    "text": "get on with elastic MapReduce then and talk about how you get started with elastic MapReduce some of the tools and",
    "start": "456759",
    "end": "462430"
  },
  {
    "text": "the operating modes that under the covers of elastic MapReduce so astin",
    "start": "462430",
    "end": "467949"
  },
  {
    "text": "MapReduce at a high level technically is a framework that you can start at the",
    "start": "467949",
    "end": "473320"
  },
  {
    "text": "end of a web service call or from the console so you can log into the AWS management console go to the EMR tab and",
    "start": "473320",
    "end": "479919"
  },
  {
    "text": "you can start a cluster you can also do that via the CLI tools and there's other Ruby tools that are very very effective",
    "start": "479919",
    "end": "485889"
  },
  {
    "text": "that you can download onto your local machine or indeed you can install that on ec2 instance which you're using to",
    "start": "485889",
    "end": "491050"
  },
  {
    "text": "control these things from and ultimately you're poking the AWS end point for EMR to initiate a cluster start and there's",
    "start": "491050",
    "end": "498909"
  },
  {
    "text": "some things that get created under the covers when we started EMR cluster and these are the moving parts there's what we call a master instance group and",
    "start": "498909",
    "end": "505419"
  },
  {
    "text": "there's an instance and ec2 instance they get started within that group and this controls everything that goes on",
    "start": "505419",
    "end": "511449"
  },
  {
    "text": "inside an EMR cluster so it coordinates the tasks that other nodes will do and",
    "start": "511449",
    "end": "516940"
  },
  {
    "text": "ultimately it runs my sequel under the covers and there's a self-contained installation that happens as part of creating the EMR cluster then you have",
    "start": "516940",
    "end": "525010"
  },
  {
    "text": "two other types of instance groups the first of those is what we call the core instance group and this is a group of",
    "start": "525010",
    "end": "532180"
  },
  {
    "text": "ec2 instances and there can be four from 1 to n instances there are running",
    "start": "532180",
    "end": "539290"
  },
  {
    "text": "for the lifetime of the cluster and these instances have the HDFS file",
    "start": "539290",
    "end": "544870"
  },
  {
    "text": "system underneath the covers so in EMR speak or Hadoop speak they run what we",
    "start": "544870",
    "end": "550300"
  },
  {
    "text": "call the data nodes they can store days for an HDFS that's made available to all there are instances and the core",
    "start": "550300",
    "end": "555640"
  },
  {
    "text": "instance group and they also run this thing called a task tracker daemon which actually does the job that we're asking",
    "start": "555640",
    "end": "561610"
  },
  {
    "text": "it to do you can also define under EMR an optional task group called a task",
    "start": "561610",
    "end": "567730"
  },
  {
    "text": "instance group and these are nodes that you can add to or subtract they can start at 0 or they can scale out to many",
    "start": "567730",
    "end": "574120"
  },
  {
    "text": "and they can scale back down again and you can do that as many times as you like throughout the lifetime of a",
    "start": "574120",
    "end": "579730"
  },
  {
    "text": "cluster but these are just running that task instance so they're just there to take work from the master instance group",
    "start": "579730",
    "end": "587920"
  },
  {
    "text": "coordinating them and doing the job and in passing results back and where that data needs to be stored it will be",
    "start": "587920",
    "end": "593470"
  },
  {
    "text": "stored on HDFS typically in the core instance group so the master node will",
    "start": "593470",
    "end": "598960"
  },
  {
    "text": "coordinate things and then typically you have say Amazon s3 under the covers and",
    "start": "598960",
    "end": "604330"
  },
  {
    "text": "what will happen is that the master instance group will be coordinating a job when you start a job across those",
    "start": "604330",
    "end": "609490"
  },
  {
    "text": "instance groups and data will be pulled in and out of s3 as required and",
    "start": "609490",
    "end": "615240"
  },
  {
    "text": "typically a very simple job will be to read data from s3 in the first place move it on to some core instance groups",
    "start": "615240",
    "end": "622450"
  },
  {
    "text": "do some processing and then push the results back into Amazon s3 once the",
    "start": "622450",
    "end": "627670"
  },
  {
    "text": "MapReduce job has completed now I take you for a very simple example of a an",
    "start": "627670",
    "end": "634420"
  },
  {
    "text": "EMR job where we take a simple set of data from s3 and then push it back to s3 to illustrate how EMR works and how you",
    "start": "634420",
    "end": "641170"
  },
  {
    "text": "can use some of those tools now one of the covers what is actually happening in",
    "start": "641170",
    "end": "646330"
  },
  {
    "text": "a MapReduce and if you look at single ec2 instance here we've got some sort of input data on the left and we're going",
    "start": "646330",
    "end": "653260"
  },
  {
    "text": "to produce some sort of output data on the right now that input file there would be a piece of the data let's say",
    "start": "653260",
    "end": "659230"
  },
  {
    "text": "that's come from s3 as provided by the MapReduce framework and coordinated by",
    "start": "659230",
    "end": "664690"
  },
  {
    "text": "that master node so take that input file as a chunk may we have an application server log file",
    "start": "664690",
    "end": "669940"
  },
  {
    "text": "and it's passed into a map function that you can define that will do some sort of",
    "start": "669940",
    "end": "675340"
  },
  {
    "text": "analysis or processing on that data in this case pulling out different types of records and showing they're on different",
    "start": "675340",
    "end": "681580"
  },
  {
    "text": "colors then the reduced phase will then take a particular type of Records aggregate them up and produce an output",
    "start": "681580",
    "end": "688180"
  },
  {
    "text": "file so when you expand the number of nodes that you have in the amar cluster this is the sort of thing that's",
    "start": "688180",
    "end": "694030"
  },
  {
    "text": "happening so we can see I've got three colors of Records that I'm looking to extract a master node we'll have",
    "start": "694030",
    "end": "699340"
  },
  {
    "text": "distributed a chunk of my input file across each of my instances and then my",
    "start": "699340",
    "end": "704380"
  },
  {
    "text": "map processor would run and I would subsequently run reduce processes that would aggregate all the Greens all the",
    "start": "704380",
    "end": "710110"
  },
  {
    "text": "Reds all the yellows and produce them into files on the output now the output is always going to be a series of files",
    "start": "710110",
    "end": "715840"
  },
  {
    "text": "which you can then post process into a single continuous file and one of the sample applications I'll look at is a",
    "start": "715840",
    "end": "722050"
  },
  {
    "text": "word count a very simple example to illustrate how this works and that's a tutorial that you can get going with",
    "start": "722050",
    "end": "727270"
  },
  {
    "text": "really easily from within the console EMR also integrates I mentioned at the",
    "start": "727270",
    "end": "733660"
  },
  {
    "text": "start to various different tools so if you simplify that view of what EMR looks like there's a series of nodes there the",
    "start": "733660",
    "end": "738730"
  },
  {
    "text": "large square being the master node and then having task nodes wrapped around it and HDFS is being an underlying file",
    "start": "738730",
    "end": "745180"
  },
  {
    "text": "system you can pull data in and out of s3 integrates with dynamodb and a rather",
    "start": "745180",
    "end": "750880"
  },
  {
    "text": "by simple DB can be used as well so these are very fundamental data sets that and input output points that you",
    "start": "750880",
    "end": "757240"
  },
  {
    "text": "can use you can also then use open source tools such as flume which is",
    "start": "757240",
    "end": "762850"
  },
  {
    "text": "distributed log collections so you can use flume to pull logs into lina marra cluster from many different application",
    "start": "762850",
    "end": "769030"
  },
  {
    "text": "servers for example or you can use a patchy HBase as an input or indeed an output which is an in-memory key value",
    "start": "769030",
    "end": "775450"
  },
  {
    "text": "store similar I guess to dynamo DB and in that sentence and a key value high-performance store so you can use",
    "start": "775450",
    "end": "781630"
  },
  {
    "text": "data management tools and there are other tools that you can integrate with EMR and you can use analytics languages",
    "start": "781630",
    "end": "787180"
  },
  {
    "text": "across the top so hive is a an SQL like language will compare to relational",
    "start": "787180",
    "end": "792940"
  },
  {
    "text": "databases later in this webinar and pig is a more prescriptive analytics framework and a language is uses",
    "start": "792940",
    "end": "799330"
  },
  {
    "text": "something called Pig Latin to define sort the queries or scripts that you might wish to perform across data data that's",
    "start": "799330",
    "end": "806560"
  },
  {
    "text": "fundamentally spread across many different nodes or as resident and s3 as",
    "start": "806560",
    "end": "811600"
  },
  {
    "text": "a file system once processing has occurred there are tools like scoop that",
    "start": "811600",
    "end": "816699"
  },
  {
    "text": "enable you to pull data out of an EMR cluster and lo the relational database such as under RDS this relational",
    "start": "816699",
    "end": "824230"
  },
  {
    "text": "database service like my sequel database and then you can use tools like AWS data pipeline to pull all that data wherever",
    "start": "824230",
    "end": "831819"
  },
  {
    "text": "it might reside and move it into something like redshift which is our high performance data warehouse so then",
    "start": "831819",
    "end": "838209"
  },
  {
    "text": "all your popular dashboarding or query tools that you have in your business might sit across the top so a typical",
    "start": "838209",
    "end": "843730"
  },
  {
    "text": "process here might be the we're using EMR we've data pulled in from different",
    "start": "843730",
    "end": "848980"
  },
  {
    "text": "application servers using flume ear Mars executing a hive script to do some sort",
    "start": "848980",
    "end": "855490"
  },
  {
    "text": "of transformation or query across the top and data then might reside then in s3 as an output and then pipeline would",
    "start": "855490",
    "end": "862870"
  },
  {
    "text": "push it into redshift and transform it into a Kalama database format for",
    "start": "862870",
    "end": "868389"
  },
  {
    "text": "high-performance data warehousing queries and processes like that can continuously operate on very large data",
    "start": "868389",
    "end": "874480"
  },
  {
    "text": "sets and that's what many customers are doing with EMR to help them drive insight into what's going on in their",
    "start": "874480",
    "end": "880269"
  },
  {
    "text": "applications or maybe drive different sort of proactive processes for their business based upon the insight that you",
    "start": "880269",
    "end": "886180"
  },
  {
    "text": "can gain from what people are really doing in their real time so EMR itself",
    "start": "886180",
    "end": "892660"
  },
  {
    "text": "under the covers what you define is a job flow so this is your interaction",
    "start": "892660",
    "end": "898360"
  },
  {
    "text": "point with a cluster so a job flow when you created it one in EMR and you create one in the console you created the job",
    "start": "898360",
    "end": "904870"
  },
  {
    "text": "flow and the cluster was created and then will create the master nodes the core instances and any tasks instances",
    "start": "904870",
    "end": "911110"
  },
  {
    "text": "that you define you can then define a set of processing steps and you have n of these steps before a job flow ends on",
    "start": "911110",
    "end": "918910"
  },
  {
    "text": "the far right so you might execute a script such as a piece of Java compiled it could be hive script it could be",
    "start": "918910",
    "end": "925089"
  },
  {
    "text": "Python it could be bash we support a whole lot of different processes and I could drop into those later you don't can chain these execution",
    "start": "925089",
    "end": "931540"
  },
  {
    "text": "steps using data from our previous steps and then you can also die define new steps as long as the pro job",
    "start": "931540",
    "end": "938200"
  },
  {
    "text": "flow is running so and then when you're done with the cluster you can terminate it and all the instances will disappear",
    "start": "938200",
    "end": "943990"
  },
  {
    "text": "and that's the fundamental of of what you can do now I'm going to step in this very simple two-step process and show",
    "start": "943990",
    "end": "950800"
  },
  {
    "text": "you how that happens and show you behind the scenes how to interact with that with some of the tools so Hadoop in this",
    "start": "950800",
    "end": "957730"
  },
  {
    "text": "job flow breaks the data set as we've seen in the previous diagrams into multiple sets if the data is set it's",
    "start": "957730",
    "end": "963220"
  },
  {
    "text": "too large to process quickly or a single note so in that previous example add three ec2 instances add three chunks of",
    "start": "963220",
    "end": "968830"
  },
  {
    "text": "files Hadoop will automatically split a data set that you give into it into processing units a step that we've seen",
    "start": "968830",
    "end": "976240"
  },
  {
    "text": "and there can be compiled Java for example or it could be a script to be executed by something called the Hadoop",
    "start": "976240",
    "end": "982090"
  },
  {
    "text": "streaming which we'll have a look at and then typically the data will be passed between steps using HDFS or s3 and then",
    "start": "982090",
    "end": "989080"
  },
  {
    "text": "you generally output onto s3 for subsequent use by other technologies and that's a job flow on the ear model",
    "start": "989080",
    "end": "996930"
  },
  {
    "text": "so in the console if you go to the EMR tab you will see that you can create a",
    "start": "997380",
    "end": "1005400"
  },
  {
    "text": "job flow using the console and it will take you through a set of steps in the wizard just like creating an ec2",
    "start": "1005400",
    "end": "1011760"
  },
  {
    "text": "instance in order for you to create an EMR cluster and the first step we will",
    "start": "1011760",
    "end": "1018120"
  },
  {
    "text": "be naming a job flow and if you look in the console you can start a sample application and I'm going to run the",
    "start": "1018120",
    "end": "1024150"
  },
  {
    "text": "sample application here for a streaming job flow and I'll talk about the different types of job flows later the",
    "start": "1024150",
    "end": "1029670"
  },
  {
    "text": "streaming job flow will enable us to essentially use a utility that comes with mere two distributions so Hadoop is",
    "start": "1029670",
    "end": "1036480"
  },
  {
    "text": "a Java framework under the cover and the streamer will allow us to run and in the arbitrary executable on the machines on",
    "start": "1036480",
    "end": "1044130"
  },
  {
    "text": "the nodes in order to do a job and it allows us to create a simple MapReduce with any sort of executable scripts such",
    "start": "1044130",
    "end": "1049800"
  },
  {
    "text": "as bash Python and so on and the way it works is it reads the input from standard input and then the reducer",
    "start": "1049800",
    "end": "1055920"
  },
  {
    "text": "outputs through standard output and by default we're talking about lines of records that represent data as key value",
    "start": "1055920",
    "end": "1063420"
  },
  {
    "text": "pairs so pretty common format in terms of logs and outputs from different types of applications",
    "start": "1063420",
    "end": "1069380"
  },
  {
    "text": "so using the console starting the streamer stamp sample application",
    "start": "1069380",
    "end": "1074400"
  },
  {
    "text": "there's a number of things to bear in mind here and to illustrate the first is the input location of some data now in",
    "start": "1074400",
    "end": "1081240"
  },
  {
    "text": "this sample application this is an s3 bucket the weep which will provide so in the EU West here there's a bucket called",
    "start": "1081240",
    "end": "1087210"
  },
  {
    "text": "elastic MapReduce that's open to everyone and there's a samples directory and I've got some input data in there",
    "start": "1087210",
    "end": "1092220"
  },
  {
    "text": "and a word count so this input data for this example is basically a set of words",
    "start": "1092220",
    "end": "1097559"
  },
  {
    "text": "in files so and we're going to count the number of instances of individual types of words the output location in this",
    "start": "1097559",
    "end": "1106590"
  },
  {
    "text": "case is my s3 bucket so I've set this up as rjs EMR as - three buckets and it's",
    "start": "1106590",
    "end": "1112950"
  },
  {
    "text": "going to output a path object there with that date in order to output my chunk files that come out the other end and",
    "start": "1112950",
    "end": "1119190"
  },
  {
    "text": "then the mapper I'm going to use the sample mapper and this is actually a Python script called word splitter so this is the location and you can push",
    "start": "1119190",
    "end": "1126600"
  },
  {
    "text": "scripts into s3 and then define where those are for when the EMR runs and the",
    "start": "1126600",
    "end": "1131730"
  },
  {
    "text": "scripts in this case is a simple Python script and it's worth just looking at what this does for a streamer well this",
    "start": "1131730",
    "end": "1137730"
  },
  {
    "text": "is a very basic sense running from standard in to standard out now there's two pieces of this one is that reads",
    "start": "1137730",
    "end": "1144000"
  },
  {
    "text": "words from standard in line by line so for each line and the standard in is going to extract a word using that red X",
    "start": "1144000",
    "end": "1150360"
  },
  {
    "text": "and then it's going to for each word it's going to output into standard out a line in this case it's going to output",
    "start": "1150360",
    "end": "1157020"
  },
  {
    "text": "long values son : and incest is the word might be abacus tab 1 so this is a tab",
    "start": "1157020",
    "end": "1162510"
  },
  {
    "text": "delimited key value pair so it's going to output that for every word that comes out so laws be a 1 after each word even",
    "start": "1162510",
    "end": "1170130"
  },
  {
    "text": "though there are a hundred words there'll be abacus 1 times 100 then we're using the standard Hadoop",
    "start": "1170130",
    "end": "1177540"
  },
  {
    "text": "streaming command here and this is an aggregate and now Hadoop comes with a whole range of standards functions that",
    "start": "1177540",
    "end": "1184650"
  },
  {
    "text": "you can run but of course you could run your own script just like pushing an s3 bucket location for my Python script you",
    "start": "1184650",
    "end": "1190500"
  },
  {
    "text": "could run your own Python script here as well so aggregates going to reduce if you like sort and reduce all those",
    "start": "1190500",
    "end": "1197309"
  },
  {
    "text": "different words across the nodes so I'll sort them so all the abacuses appear in together and",
    "start": "1197309",
    "end": "1202650"
  },
  {
    "text": "how'd those instances up and out put a single rose so abacus one abacus one will become because to say sort the",
    "start": "1202650",
    "end": "1209520"
  },
  {
    "text": "input and add up the totals so a really simple example of counting words once",
    "start": "1209520",
    "end": "1215850"
  },
  {
    "text": "you then go to next in the console you will be asked to find the cluster size so here we have those master instances",
    "start": "1215850",
    "end": "1222360"
  },
  {
    "text": "the core instances and the task instances so I'm going to start my master on a small and I'm going to have",
    "start": "1222360",
    "end": "1227730"
  },
  {
    "text": "two core instances that are running my HDFS filesystem and they can take tasks",
    "start": "1227730",
    "end": "1233610"
  },
  {
    "text": "and do the jobs that I've asked but I could add optional their instances for my task instance group that could expand",
    "start": "1233610",
    "end": "1240210"
  },
  {
    "text": "and contract and they effectively take on the load so let's just look at those instance groups in a bit more detail",
    "start": "1240210",
    "end": "1245670"
  },
  {
    "text": "just to reiterate master instance group that manages the job flow so it coordinates the distribution of the work",
    "start": "1245670",
    "end": "1251190"
  },
  {
    "text": "across those tasks instance groups it also tracks the status so it knows if any have gone wrong you can also SSH on",
    "start": "1251190",
    "end": "1259020"
  },
  {
    "text": "to the master node so if you want to have a look at what's going on at the logs will directly access the user",
    "start": "1259020",
    "end": "1264180"
  },
  {
    "text": "interface for a Depot command-line tools of a dupe you can SSH onto that master node and work from there the core",
    "start": "1264180",
    "end": "1272220"
  },
  {
    "text": "instance groups then and these are the ones that do the fundamental part of the job and run HDFS so you need to define",
    "start": "1272220",
    "end": "1278280"
  },
  {
    "text": "this capacity upfront and it will be fixed for the entire Java or fixed at a minimum you can actually add core",
    "start": "1278280",
    "end": "1284400"
  },
  {
    "text": "instances as you run a job flow if you need to add more but you can't take them away so core instances run the data node",
    "start": "1284400",
    "end": "1291150"
  },
  {
    "text": "they store the data and they do tasks using the task tracker deep daemon and then lastly these tasks instance",
    "start": "1291150",
    "end": "1298290"
  },
  {
    "text": "groups now these just do tasks they pass get data passed into them and they give data back to the coordinating master",
    "start": "1298290",
    "end": "1305520"
  },
  {
    "text": "node and you can add or subtract as many of these tasks instances as you like",
    "start": "1305520",
    "end": "1310530"
  },
  {
    "text": "throughout the lifetime of a job so you can expand a cluster to do a particular",
    "start": "1310530",
    "end": "1315930"
  },
  {
    "text": "type of processing at a particular point in the day and then subtract it afterwards so you can use elasticity in",
    "start": "1315930",
    "end": "1321690"
  },
  {
    "text": "the cloud to achieve your workflows or your job flows and the more timely fashion so creating a new job flow the",
    "start": "1321690",
    "end": "1329790"
  },
  {
    "text": "next stage will be some settings here so if you want to SSH onto a master node you need to provide",
    "start": "1329790",
    "end": "1336410"
  },
  {
    "text": "keep her so a standard ec2 key pair that allow you to SSH on to that node if you",
    "start": "1336410",
    "end": "1342050"
  },
  {
    "text": "want to keep the job flow running once it's processing is completed so you can",
    "start": "1342050",
    "end": "1347090"
  },
  {
    "text": "dynamically add more steps and carry on processing the data you can keep it alive you can turn on logging and push",
    "start": "1347090",
    "end": "1353420"
  },
  {
    "text": "logs out into s3 which also means that you can turn on ear Mardy bugging and",
    "start": "1353420",
    "end": "1358580"
  },
  {
    "text": "view the logs in the console and have a look at what's going on and various other different settings if you want to run this into a V PC for example as well",
    "start": "1358580",
    "end": "1366490"
  },
  {
    "text": "now the s3 output will be a series of files in this case that looked like this so these are the words with their",
    "start": "1366490",
    "end": "1373250"
  },
  {
    "text": "associated counts that are tab delimited and depending upon the number of nodes that I chose to run in that cluster",
    "start": "1373250",
    "end": "1379070"
  },
  {
    "text": "they'll be more or less of those files as parts than the output bucket that I've defined see if I ran - I might have",
    "start": "1379070",
    "end": "1386480"
  },
  {
    "text": "6 files if I ran 9 there might be 1314 the number of reducer processes will",
    "start": "1386480",
    "end": "1392690"
  },
  {
    "text": "increase or decrease depending upon the capacity or the data that I have and that's what I'll get I'll get an input",
    "start": "1392690",
    "end": "1398060"
  },
  {
    "text": "file in s3 that will be split up automatically passed across my cluster my map process will run to extract that",
    "start": "1398060",
    "end": "1405260"
  },
  {
    "text": "data and then my reduce will abrogate the totals and output part files that",
    "start": "1405260",
    "end": "1410390"
  },
  {
    "text": "are all unique counts of the words within those parts now you can do that",
    "start": "1410390",
    "end": "1417770"
  },
  {
    "text": "same job at the command line so here is the Ruby tools at the CLI so this is a",
    "start": "1417770",
    "end": "1423740"
  },
  {
    "text": "really simple install that you can W get from our website onto an instance or your local machine there's long as",
    "start": "1423740",
    "end": "1429470"
  },
  {
    "text": "you've got Ruby installed you can run this you set up some credential files typically as you do with all of our",
    "start": "1429470",
    "end": "1435380"
  },
  {
    "text": "command-line tools your secret access keys to authenticate against your account and you're essentially defining",
    "start": "1435380",
    "end": "1440900"
  },
  {
    "text": "exactly the same things so a mapper in this case getting my word splits and py from s3 my input data set where the par",
    "start": "1440900",
    "end": "1448670"
  },
  {
    "text": "for the bucket path that I'm going to get my data from where I'm going to output my data and in this case using a",
    "start": "1448670",
    "end": "1455270"
  },
  {
    "text": "standard reducer function aggregate so",
    "start": "1455270",
    "end": "1460460"
  },
  {
    "text": "by default if you took that sort of command at the command line that will launch a Java flow on a single ec2 note",
    "start": "1460460",
    "end": "1467330"
  },
  {
    "text": "so the run in a single cluster on ec2 em 1 small instance and when you are steps are running correctly what you'd",
    "start": "1467330",
    "end": "1473570"
  },
  {
    "text": "normally do then is on a larger set of data to expand that cluster across multiple notes it starts small by",
    "start": "1473570",
    "end": "1479600"
  },
  {
    "text": "default but you can specify at the command line using num instances and instance type parameters the size of",
    "start": "1479600",
    "end": "1485690"
  },
  {
    "text": "cluster and those that you want to run so elastic MapReduce command at the command line might look like this where",
    "start": "1485690",
    "end": "1491390"
  },
  {
    "text": "I've specified a key pair so I can SSH on to it my region my name of my job flow on five instances of type m2 for",
    "start": "1491390",
    "end": "1499190"
  },
  {
    "text": "extra-large I want us to keep alive and I want my log bucket to be in that location in s3 so this is a classic sort",
    "start": "1499190",
    "end": "1506900"
  },
  {
    "text": "of setup that gives you all the controls of the console and a simple command line called the instance height count they're",
    "start": "1506900",
    "end": "1513740"
  },
  {
    "text": "shown to expand the cluster and I've got to keep a live flag set up here so I can",
    "start": "1513740",
    "end": "1519320"
  },
  {
    "text": "keep adding steps to this process which is a useful thing as you are doing iterative developments if you started on",
    "start": "1519320",
    "end": "1524810"
  },
  {
    "text": "a cluster you defined a particular script you want to push it up there you don't want to add another step to that",
    "start": "1524810",
    "end": "1530600"
  },
  {
    "text": "job flow to process data in a different way and you can do that over a period of time build up your job flows and then",
    "start": "1530600",
    "end": "1536090"
  },
  {
    "text": "pull all that together into a final production workflow job flow when you're done using the command line you can also",
    "start": "1536090",
    "end": "1543260"
  },
  {
    "text": "add some of these tools that I've spoken off so simple command line flags will enable you to add things like pig hive",
    "start": "1543260",
    "end": "1550490"
  },
  {
    "text": "and HBase and we'll look at HBase and pig and hive and a little bit more detail subsequently you can also",
    "start": "1550490",
    "end": "1558860"
  },
  {
    "text": "bootstrap so just as you bootstrap ec2 instances to configure them in a particular way when you might be",
    "start": "1558860",
    "end": "1565340"
  },
  {
    "text": "installing some software or adding some some code from s3 to run a web app for example when you start on ec2 instance",
    "start": "1565340",
    "end": "1571490"
  },
  {
    "text": "you can use bootstrapping actions on an EMR job flow and there's a whole host of",
    "start": "1571490",
    "end": "1576740"
  },
  {
    "text": "these bootstrapping actions you can run custom ones but you can run some standard ones too so he has shown some",
    "start": "1576740",
    "end": "1581840"
  },
  {
    "text": "configure in Hadoop action with some arguments there about the block size on filesystem and the actions that you can",
    "start": "1581840",
    "end": "1589370"
  },
  {
    "text": "perform at bootstrapping are specified here so that you can have up to 16 job bootstrap actions per job floating",
    "start": "1589370",
    "end": "1595430"
  },
  {
    "text": "probably providing multiple what we call bootstrap action parameters as you just seen now these are all defined s3",
    "start": "1595430",
    "end": "1601730"
  },
  {
    "text": "scripts so you can see on the left that I've got configured demons as one of the scripts configure deep memory",
    "start": "1601730",
    "end": "1607550"
  },
  {
    "text": "intensive so configure daemons allows you to configure the JVM property so on the right there is an example of pulling",
    "start": "1607550",
    "end": "1614510"
  },
  {
    "text": "in that's that script and passing in the arguments to change the heap size of Java and the garbage collect time to",
    "start": "1614510",
    "end": "1620780"
  },
  {
    "text": "change the performance profile of the cluster configure Hadoop will do things around the fundamental Hadoop",
    "start": "1620780",
    "end": "1626600"
  },
  {
    "text": "configuration of the framework memory intensive will be a configuration enables you to use smaller instances in",
    "start": "1626600",
    "end": "1633050"
  },
  {
    "text": "a more memory efficient way you can also have conditional run ifs so shown there",
    "start": "1633050",
    "end": "1639020"
  },
  {
    "text": "as a simple example of echoing out that this is running on a master node so you can do conditional processing and then",
    "start": "1639020",
    "end": "1646130"
  },
  {
    "text": "you can fire in a bunch of scripts in a bucket in a directory that will run when",
    "start": "1646130",
    "end": "1652310"
  },
  {
    "text": "the shutdown action or the job flow is complete and all these scripts will run in parallel but they must finish in 60",
    "start": "1652310",
    "end": "1658070"
  },
  {
    "text": "seconds or you can run a custom action so here is showing downloading a shell script from s3 that performs some sort",
    "start": "1658070",
    "end": "1666680"
  },
  {
    "text": "of download of data or or configuration of assets on that cluster so as you start a job flow you can ensure that",
    "start": "1666680",
    "end": "1672260"
  },
  {
    "text": "it's fully automated to get it into a state that you want to run in you can do",
    "start": "1672260",
    "end": "1679970"
  },
  {
    "text": "that in the console too so shown here I've got to configure Hadoop action so when you run through the console there's a simple wizard that will interface with",
    "start": "1679970",
    "end": "1686420"
  },
  {
    "text": "those scripts and you can see exactly here that s3 location of that script and then the arguments that you're firing in",
    "start": "1686420",
    "end": "1692180"
  },
  {
    "text": "so you can configure any of those and you can stack up those 16 actions ok so",
    "start": "1692180",
    "end": "1700700"
  },
  {
    "text": "that's a job flow and that's integrating bootstrapping actions to it defining the",
    "start": "1700700",
    "end": "1707420"
  },
  {
    "text": "size of the cluster and a very simple example of what we call a streamer where it's taking standard in and piping out",
    "start": "1707420",
    "end": "1713450"
  },
  {
    "text": "the standard out some processing of some sort of log file but there's an integrated tools so you need to know",
    "start": "1713450",
    "end": "1719210"
  },
  {
    "text": "what sort of tools you might want to be using or the purpose you're going to be doing like a deep cluster for to know which tool to use",
    "start": "1719210",
    "end": "1724910"
  },
  {
    "text": "so her deep streaming that I've just shown you is really structure are really suitable for unstructured dates with a",
    "start": "1724910",
    "end": "1731030"
  },
  {
    "text": "wide variety of formats so there's a high flexibility on a language you can use to process those formats so you",
    "start": "1731030",
    "end": "1737300"
  },
  {
    "text": "might define a script in Python you might use Perl bash Ruby might be compiled Java whatever you want to work",
    "start": "1737300",
    "end": "1744200"
  },
  {
    "text": "in as long as you can execute it on that Linux instance that's running the recovers the Hadoop stream or executable",
    "start": "1744200",
    "end": "1750440"
  },
  {
    "text": "jar will allow you to call out to that a new standard in standard out so it's the mapper the reducer the inputs and the",
    "start": "1750440",
    "end": "1756350"
  },
  {
    "text": "outputs are all supplied by a job flow and you can scale that from one instance to an unlimited number of processes on",
    "start": "1756350",
    "end": "1762560"
  },
  {
    "text": "the other sides if you like doing data analytics in that way any sort of scale",
    "start": "1762560",
    "end": "1768350"
  },
  {
    "text": "or complexity is generally wrong because you're actually having to code your data analystics and you have a simple two-stage data flow which is quite rigid",
    "start": "1768350",
    "end": "1775370"
  },
  {
    "text": "you have the map and the reduced stage so if you're doing complex joins across data typically going to be hard w",
    "start": "1775370",
    "end": "1781280"
  },
  {
    "text": "error-prone and then if you're using things like java for this language then",
    "start": "1781280",
    "end": "1786290"
  },
  {
    "text": "you're prototyping requires you to recompile some jars and push those up to you in my cluster so okay for small",
    "start": "1786290",
    "end": "1792650"
  },
  {
    "text": "scale projects and for getting started and exploring a deep but generally you're going to want to go beyond that",
    "start": "1792650",
    "end": "1797660"
  },
  {
    "text": "and that's where the other tools that are integrated to come in so high is a very popular one and they believe to do",
    "start": "1797660",
    "end": "1804020"
  },
  {
    "text": "structured data analysis but it's at Emily a higher latency so you've got latency requirements of fast data",
    "start": "1804020",
    "end": "1811220"
  },
  {
    "text": "operations and hide might not be for you the way you want to do SQL like operations hive is great Pig improves",
    "start": "1811220",
    "end": "1818990"
  },
  {
    "text": "the sort of performance sort of end but it gives you a semi structured data that can be mined using set operations and",
    "start": "1818990",
    "end": "1825290"
  },
  {
    "text": "the language under the covers called Pig Latin which is a very different sort of language from an SQL type language",
    "start": "1825290",
    "end": "1831440"
  },
  {
    "text": "and then you can integrate with HBase as well so you can start HBase and have a near real-time very high performance key",
    "start": "1831440",
    "end": "1837740"
  },
  {
    "text": "value store the structured data you can then query on top off it's just as you",
    "start": "1837740",
    "end": "1842960"
  },
  {
    "text": "might query dynamo for example so let's have a look at hive integration and high",
    "start": "1842960",
    "end": "1848419"
  },
  {
    "text": "integration really is all about producing a schema on read so rather than having a schema pre-populated with",
    "start": "1848419",
    "end": "1854240"
  },
  {
    "text": "data we define a mapping if you like of your data that you have held across this distributed EMR cluster and the schema",
    "start": "1854240",
    "end": "1862070"
  },
  {
    "text": "is created on read and that gives you some benefits so it's a simple way to use her deep because it's an sql-like",
    "start": "1862070",
    "end": "1868250"
  },
  {
    "text": "interface on top of the dates that contained into it so for example a create table statement actually",
    "start": "1868250",
    "end": "1874549"
  },
  {
    "text": "references a data location on s3 under EMR and hive the language is called hive",
    "start": "1874549",
    "end": "1880340"
  },
  {
    "text": "QL which is very similar to SQL so a query might be select count one from some table where that table is actually",
    "start": "1880340",
    "end": "1886309"
  },
  {
    "text": "data that's held in s3 and distributed across EMR as a point of processing to get that data back so it requires some",
    "start": "1886309",
    "end": "1893570"
  },
  {
    "text": "sort of mapping to the input data so you need to know what that data structure is and it uses des to make different input",
    "start": "1893570",
    "end": "1900950"
  },
  {
    "text": "formats queryable and it's got some quite powerful data types and there's like a raised and map so she would make it a powerful language above SQL so if",
    "start": "1900950",
    "end": "1910159"
  },
  {
    "text": "you look at a comparison between SQL and hive ql' SQL allows you to do input in",
    "start": "1910159",
    "end": "1915379"
  },
  {
    "text": "update insert Denise it supports transactions it has indexes it's very low latency there are hundreds of",
    "start": "1915379",
    "end": "1922429"
  },
  {
    "text": "functions built into the language but things like multi table inserts aren't supported and things like create a table",
    "start": "1922429",
    "end": "1928940"
  },
  {
    "text": "from a select generally not valid SQL 92 some databases have it that most don't",
    "start": "1928940",
    "end": "1934399"
  },
  {
    "text": "on the other side they have hive QL where you have the equivalent insert and overwrite table the things like",
    "start": "1934399",
    "end": "1940639"
  },
  {
    "text": "transactions indexes and so on are not supported it's generally a much higher",
    "start": "1940639",
    "end": "1945710"
  },
  {
    "text": "latency minutes to get results back out of this distributed data sets and there's a reduced set of functions but",
    "start": "1945710",
    "end": "1952370"
  },
  {
    "text": "things like multi table inserts are supported and you can do a simple select statement and create a new table to help",
    "start": "1952370",
    "end": "1958490"
  },
  {
    "text": "your analytics so it creates a very fluid environment for processing these very large data sets across",
    "start": "1958490",
    "end": "1964600"
  },
  {
    "text": "stirrers you can interface with hive using JDBC so there's a JDBC listener",
    "start": "1964600",
    "end": "1972700"
  },
  {
    "text": "started by default on EMR the ports there are open for hive and then you go",
    "start": "1972700",
    "end": "1977770"
  },
  {
    "text": "down under the covers anti-do we were different set of ports and there's a high of JDBC driver that you can download and to get that working on your",
    "start": "1977770",
    "end": "1985000"
  },
  {
    "text": "EMR cluster you need to open a port on a security group on the master node on the master group and then you can create an",
    "start": "1985000",
    "end": "1991570"
  },
  {
    "text": "SSH tunnel for your JDBC connection to operate across so just like opening up",
    "start": "1991570",
    "end": "1996760"
  },
  {
    "text": "port for SQL access on an instance in ec2 you're running a database on exactly",
    "start": "1996760",
    "end": "2002070"
  },
  {
    "text": "the same process the security groups under the covers that get built with VM are you just need to open up and then",
    "start": "2002070",
    "end": "2008130"
  },
  {
    "text": "you can access them using a JDBC connection so here we've got a CLI",
    "start": "2008130",
    "end": "2015800"
  },
  {
    "text": "example of using hive in a script fashion so here I've got a job flow",
    "start": "2015800",
    "end": "2021510"
  },
  {
    "text": "that's being created my name is hive job flow and I've defined the property called hive script so just as I",
    "start": "2021510",
    "end": "2027840"
  },
  {
    "text": "previously was using a streamer and I was using a Python script to process my",
    "start": "2027840",
    "end": "2035100"
  },
  {
    "text": "data here I'm saying this is a hive script workflow and I'm going to pass in a location of my query key a 5 ql file",
    "start": "2035100",
    "end": "2043770"
  },
  {
    "text": "and I've got some inputs there of my input bucket and my output bucket that's",
    "start": "2043770",
    "end": "2048840"
  },
  {
    "text": "gonna be in s3 that this hive query is going to run across so this will create a hive EMR job and depending upon the",
    "start": "2048840",
    "end": "2057840"
  },
  {
    "text": "number of instances that I might also pass into this query the size of that cluster will vary and then I can process",
    "start": "2057840",
    "end": "2063510"
  },
  {
    "text": "that hive script and the output data will be chucked into my output bucket so they're a very you know it succinct way",
    "start": "2063510",
    "end": "2069570"
  },
  {
    "text": "of defining a hive processing in query to offload in EMR you can also start a",
    "start": "2069570",
    "end": "2077340"
  },
  {
    "text": "cluster and turn on what we call hive interactive so if you do this you can now drop into the hive prompts within",
    "start": "2077340",
    "end": "2084060"
  },
  {
    "text": "your master node or across a connection that you've SSH tunneled in and start",
    "start": "2084060",
    "end": "2089340"
  },
  {
    "text": "using an interactive five session I start interacting with that data across the EMR cluster because I've kept it",
    "start": "2089340",
    "end": "2095250"
  },
  {
    "text": "alive you can see I've got my alive flag I've got five instances big ones and one largers and i'm hive",
    "start": "2095250",
    "end": "2101380"
  },
  {
    "text": "interactive so right past in my args where i had an input set in s3 for example i can now start querying that",
    "start": "2101380",
    "end": "2108760"
  },
  {
    "text": "input set using hive interactive tools so some of the things that i might",
    "start": "2108760",
    "end": "2114070"
  },
  {
    "text": "better do here and just step you through this is like i'm defining a an external table in hive so this is hive ql and",
    "start": "2114070",
    "end": "2120400"
  },
  {
    "text": "this is where I'm gonna get the data from and it's table created as a structure it's just a mapping to my data",
    "start": "2120400",
    "end": "2127690"
  },
  {
    "text": "that's held actually in s3 so a log file an s3 in this case this is impressions",
    "start": "2127690",
    "end": "2132970"
  },
  {
    "text": "for ad-serving from an application server a log file so it's just a mapping to the source so",
    "start": "2132970",
    "end": "2138370"
  },
  {
    "text": "when I create a table that's really fast happens nearly instantly and then I've got there you go the source data that is",
    "start": "2138370",
    "end": "2144280"
  },
  {
    "text": "coming from the location is in s3 and there's various different other things in there defining the mapping the row",
    "start": "2144280",
    "end": "2149800"
  },
  {
    "text": "format for example because this data file is in JSON I'm using a JSON sir",
    "start": "2149800",
    "end": "2155770"
  },
  {
    "text": "they file their jar that's going to be downloaded and they want me to process",
    "start": "2155770",
    "end": "2161020"
  },
  {
    "text": "that data okay so we provide you with those helper things to let need to process non-flat files once you've done",
    "start": "2161020",
    "end": "2169000"
  },
  {
    "text": "something like that at the hive interactive prompt you can then select star from impressions and just grab five",
    "start": "2169000",
    "end": "2174010"
  },
  {
    "text": "as a for example so that's selecting directly from s3 via 5 cluster instances",
    "start": "2174010",
    "end": "2181150"
  },
  {
    "text": "on EMR to give me a high-performance processing across what could be an extremely large data set that single",
    "start": "2181150",
    "end": "2188650"
  },
  {
    "text": "file for example could be five terabytes in size or it could be multiple files that I have mapped in in the EMR job",
    "start": "2188650",
    "end": "2194650"
  },
  {
    "text": "flow so if you look at some of the",
    "start": "2194650",
    "end": "2199960"
  },
  {
    "text": "different types of tools that you can use streaming was the first example I gave so good for unstructured data you",
    "start": "2199960",
    "end": "2205600"
  },
  {
    "text": "can support any language pretty much as long as you can execute it on that Linux instance unlimited volume and medium",
    "start": "2205600",
    "end": "2212020"
  },
  {
    "text": "latency you because it's piping standard understand about do you got - I've just looked at with hql again unlimited",
    "start": "2212020",
    "end": "2218980"
  },
  {
    "text": "volume it gives you an SQL like language across some structured data and then we move into Pig with pig latin as a",
    "start": "2218980",
    "end": "2224710"
  },
  {
    "text": "language and then we have things like dynamo DB as mechanisms you can use",
    "start": "2224710",
    "end": "2230140"
  },
  {
    "text": "because you can push data out of the EMR cluster in dynamo and then use",
    "start": "2230140",
    "end": "2236380"
  },
  {
    "text": "dynamo as a very high-performance very low latency store of that data so ultra-low latency but in the context of",
    "start": "2236380",
    "end": "2243700"
  },
  {
    "text": "big data whether it's could be petabytes this might be used for more relatively low volume two sets of data in that",
    "start": "2243700",
    "end": "2249070"
  },
  {
    "text": "context and then we have a redshift which is our data warehousing technology",
    "start": "2249070",
    "end": "2254110"
  },
  {
    "text": "so you can push out into redshift and then use all your common popular BI tools and dashboards slice and dice",
    "start": "2254110",
    "end": "2260590"
  },
  {
    "text": "tools on top of the data that you process through an EMR cluster so generally you'll be using a combination",
    "start": "2260590",
    "end": "2266770"
  },
  {
    "text": "of these things so it might be some hive scripts pushing data out into s3 which",
    "start": "2266770",
    "end": "2272050"
  },
  {
    "text": "then gets processed into redshift for example now there are some cost",
    "start": "2272050",
    "end": "2277150"
  },
  {
    "text": "considerations to think about when running EMR and about making the most of ec2 resources because it's all based",
    "start": "2277150",
    "end": "2282910"
  },
  {
    "text": "upon this kind of fundamental equation so one instance for a hundred hours equals a hundred instances for one hour",
    "start": "2282910",
    "end": "2289060"
  },
  {
    "text": "and that's sort of linear scalability plays into cost argument because a small",
    "start": "2289060",
    "end": "2294370"
  },
  {
    "text": "instances for 100 of them for one hour cost you eight dollars whereas a thousand instances you can run for one",
    "start": "2294370",
    "end": "2301270"
  },
  {
    "text": "hour and you'll achieve and they're linear scalability and you'll only spend $80 so you know a thousand instances",
    "start": "2301270",
    "end": "2307450"
  },
  {
    "text": "running in the EMR cluster for one hour and it'll cost you 80 bucks on a small instance now playing into that is the",
    "start": "2307450",
    "end": "2314950"
  },
  {
    "text": "different types of a c2 instance purchase plans that we have if you look at our capacity planning within ec2 we",
    "start": "2314950",
    "end": "2323320"
  },
  {
    "text": "have these things called reserved instances so everything anyone has bought a reserved instance has a reserve",
    "start": "2323320",
    "end": "2328480"
  },
  {
    "text": "capacity allocation so as a proportion of the cloud that's reserved for people that are paying up front of an upfront",
    "start": "2328480",
    "end": "2335380"
  },
  {
    "text": "fee to reserve that particular instance type and that particular availability zone for example we don't have the",
    "start": "2335380",
    "end": "2341050"
  },
  {
    "text": "on-demand instances that if you're standing starting a standard EMR cluster you would be pulling stuff from the",
    "start": "2341050",
    "end": "2347590"
  },
  {
    "text": "on-demand pool and then we have the excess on top and this is an illustrative diagram and we have spot",
    "start": "2347590",
    "end": "2354430"
  },
  {
    "text": "instances so the excess capacity we sell then at a variable price and that's called our spot instance market now if",
    "start": "2354430",
    "end": "2361660"
  },
  {
    "text": "you go into the console you can see spot instance pricing history tip is much much less than the on-demand",
    "start": "2361660",
    "end": "2368339"
  },
  {
    "text": "price but occasionally a spike so you can see here as a chart around micro instance in all zones in US east now sat",
    "start": "2368339",
    "end": "2376319"
  },
  {
    "text": "down there at less than half a cent normally but then it spikes up sometimes to two cents there above the sort of",
    "start": "2376319",
    "end": "2383279"
  },
  {
    "text": "standard price so generally you can access resources at a much lower price level using spot now EMR integrates with",
    "start": "2383279",
    "end": "2392069"
  },
  {
    "text": "spot and if you go through the console you'll see you can choose any of those and their instance groups and you can start them on spot instances and then",
    "start": "2392069",
    "end": "2398849"
  },
  {
    "text": "define the price you're willing to pay for them so what people do is they'll start their master notes on demand or",
    "start": "2398849",
    "end": "2404579"
  },
  {
    "text": "maybe if they're running EMR all the time and I've been running those on reserved then the core instances will be",
    "start": "2404579",
    "end": "2410010"
  },
  {
    "text": "on demand or reserved and then tasks instances will generally be in spawn spots because you can do something like",
    "start": "2410010",
    "end": "2416579"
  },
  {
    "text": "this see if you look at EMR a job where I've got four instances and let's say",
    "start": "2416579",
    "end": "2421680"
  },
  {
    "text": "this particular job takes 14 hours to complete on those four instances and for sake of argument I've got 50 cents per",
    "start": "2421680",
    "end": "2429000"
  },
  {
    "text": "hour is the price for that particular instance type that I'm using so for instances for 14 hours at $0.50 is $28",
    "start": "2429000",
    "end": "2436589"
  },
  {
    "text": "to run that EMR job over 14 hours now taking advantage of that near linear",
    "start": "2436589",
    "end": "2442680"
  },
  {
    "text": "scalability if I were to add five spot instances to their anion doubling the capacity I could pretty much half the",
    "start": "2442680",
    "end": "2449039"
  },
  {
    "text": "generation of that job so by adding twice as much I'm going to take half the",
    "start": "2449039",
    "end": "2454140"
  },
  {
    "text": "time to complete this particular EMR job flow now if you look at the pricing of that I've got my four instances running",
    "start": "2454140",
    "end": "2460529"
  },
  {
    "text": "before at $0.50 but they're running for half the time so that price is now $14",
    "start": "2460529",
    "end": "2466740"
  },
  {
    "text": "but my five additional instance is again for the sake of argument let's say they're half the price of the on-demand",
    "start": "2466740",
    "end": "2472230"
  },
  {
    "text": "price for that particular instance time for 25 cents they run for seven hours cost me a dollar seventy five so in",
    "start": "2472230",
    "end": "2478829"
  },
  {
    "text": "total I'm paying $22 seventy five to do the same job in half the time so I send",
    "start": "2478829",
    "end": "2484619"
  },
  {
    "text": "a cost saving of twenty two percent and I do things and a time saving of 50 percent so by adding more resources on",
    "start": "2484619",
    "end": "2490859"
  },
  {
    "text": "the spot market I get done things done quicker and I get things done cheaper with EMR so this is a really cool",
    "start": "2490859",
    "end": "2498060"
  },
  {
    "text": "or thing you should bear in mind if you're running serious production EMR workflows is the instance types you use",
    "start": "2498060",
    "end": "2504030"
  },
  {
    "text": "and how you mix and match them so you might have a proportion of tasks instances that are on-demand and the",
    "start": "2504030",
    "end": "2510240"
  },
  {
    "text": "proportion that are on the spot market and that way you can expand and contract",
    "start": "2510240",
    "end": "2515970"
  },
  {
    "text": "and you can take advantage of that very load price point for the instances and get your jobs done quicker now there's",
    "start": "2515970",
    "end": "2523530"
  },
  {
    "text": "some other things to bear in mind some hints and tips so there's the dynamic",
    "start": "2523530",
    "end": "2528630"
  },
  {
    "text": "creation of clusters for processing jobs so think about the CLI tools and the automation that you can achieve so only",
    "start": "2528630",
    "end": "2536400"
  },
  {
    "text": "spin up clusters when you need them and then use them for that hour because everything is a billable hour so you",
    "start": "2536400",
    "end": "2542040"
  },
  {
    "text": "think about the points in time of the day when you want to process jobs and mainly think about time where sort of",
    "start": "2542040",
    "end": "2548040"
  },
  {
    "text": "cost aware architectures so maybe running your analytics overnight on a spot market will be more cost effective",
    "start": "2548040",
    "end": "2555240"
  },
  {
    "text": "than doing it in pieces during the day because the price might be cheaper overnight using spots I think a bog data",
    "start": "2555240",
    "end": "2561540"
  },
  {
    "text": "stored in s3 so this is the best place to put it and you can also use reduce",
    "start": "2561540",
    "end": "2567120"
  },
  {
    "text": "redundancy storage in s3 because by default we support eleven nines or design for eleven lines of data",
    "start": "2567120",
    "end": "2573240"
  },
  {
    "text": "durability in s3 but if that data is something transient you can use reduce",
    "start": "2573240",
    "end": "2578520"
  },
  {
    "text": "redundancy storage and you'll actually pay less for storing that data for an s3 I think about DynamoDB for",
    "start": "2578520",
    "end": "2584220"
  },
  {
    "text": "high-performance data access see if you've got some jobs that really need low latency query results then think",
    "start": "2584220",
    "end": "2589590"
  },
  {
    "text": "about piping the core parts of your analytics data into dynamo DB and using that as a source or destination and then",
    "start": "2589590",
    "end": "2595560"
  },
  {
    "text": "when you're done with data migrate that data to glassier so if you're using s3",
    "start": "2595560",
    "end": "2600570"
  },
  {
    "text": "you can set lifecycle policies and when your data is done with you can push it into glassier maybe a single copy in",
    "start": "2600570",
    "end": "2606810"
  },
  {
    "text": "cold storage a very cheap price point for archive and then only if you really need to have to go get it back for some",
    "start": "2606810",
    "end": "2613290"
  },
  {
    "text": "retrospective analysis you can pull it back out and then your integrates with VPC so if you're running a V PC use that",
    "start": "2613290",
    "end": "2620940"
  },
  {
    "text": "because it will give a secure environment for your EMR clusters and you can set I am Identity and Access Management roles",
    "start": "2620940",
    "end": "2627030"
  },
  {
    "text": "setup network security you can set up access from a user perspective about who can do what",
    "start": "2627030",
    "end": "2632069"
  },
  {
    "text": "in your EMR setup so you can secure this hold environment another thing to think",
    "start": "2632069",
    "end": "2638489"
  },
  {
    "text": "about and there's a blog post that you can go look out for this is that there's a framework called shark that sits on",
    "start": "2638489",
    "end": "2645029"
  },
  {
    "text": "top of hive and spark so rather than using Hadoop was an underlying framework in EMR you can use spark and this will",
    "start": "2645029",
    "end": "2651809"
  },
  {
    "text": "typically you know run a lot lot faster than a Hadoop job so you can simply use a bootstrapping action that I've showing",
    "start": "2651809",
    "end": "2657900"
  },
  {
    "text": "you there to install spark on the nodes and then you'll start running spark jobs",
    "start": "2657900",
    "end": "2663119"
  },
  {
    "text": "on top hive and you know up to 30 times quicker for running a particular job so a really good way of boosting the",
    "start": "2663119",
    "end": "2668940"
  },
  {
    "text": "performance of some of these big data analytics drops on the EMR CMR is not just a dupe and it's expanding all the",
    "start": "2668940",
    "end": "2675390"
  },
  {
    "text": "time and you can subscribe to my colleague Geoff's blog in the States at AWS TypePad com get an RSS feed from",
    "start": "2675390",
    "end": "2682769"
  },
  {
    "text": "there and each time any of these new features come out he talks about them in that blog so in summary just to wrap up",
    "start": "2682769",
    "end": "2689910"
  },
  {
    "text": "my 45 minutes EMR is hadoop as a service essentially integrates with all sorts of",
    "start": "2689910",
    "end": "2695369"
  },
  {
    "text": "told you expecting that open source ecosystems such as hi peg and so on and allows you to do MapReduce type",
    "start": "2695369",
    "end": "2701190"
  },
  {
    "text": "processes it allows you to scale out massively parallel so split data up into many different pieces to achieve",
    "start": "2701190",
    "end": "2707930"
  },
  {
    "text": "analytics in a timeframe that wouldn't normally be achievable the resources you might have on your desktop and they",
    "start": "2707930",
    "end": "2713430"
  },
  {
    "text": "integrates into AWS services like s3 dynamo and so on and it's cost-effective because of the operation again seems",
    "start": "2713430",
    "end": "2720599"
  },
  {
    "text": "like the spot market you can get dumb things done quicker for cheaper under an EMR using spot instances so there's a",
    "start": "2720599",
    "end": "2728249"
  },
  {
    "text": "really good tutorial that you can try which actually talks about using hive and it talks about the ad impressions",
    "start": "2728249",
    "end": "2735359"
  },
  {
    "text": "example that I gave you so if you go to that and I'll leave this on the screen aws.amazon.com slash articles two eight",
    "start": "2735359",
    "end": "2741329"
  },
  {
    "text": "two five five there's a great tutorial there's also some fantastic videos on YouTube that talk about getting started",
    "start": "2741329",
    "end": "2747719"
  },
  {
    "text": "with all these different steps you can find out more about that at the Big Data section of the Amazon website and any",
    "start": "2747719",
    "end": "2755190"
  },
  {
    "text": "questions you've asked today I'll follow up with offline but thanks again for listening that's 45 minutes on EMR",
    "start": "2755190",
    "end": "2761339"
  },
  {
    "text": "hopefully you found that useful and whetted your appetite you",
    "start": "2761339",
    "end": "2767869"
  }
]