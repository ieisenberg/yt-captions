[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "good afternoon my name is Paul Sears and I'm a sushis architect with AWS on the",
    "start": "199",
    "end": "7020"
  },
  {
    "text": "partner organization I support big data partners and today",
    "start": "7020",
    "end": "12389"
  },
  {
    "text": "I'm joined with Alex Fong who's the lead architect Experion and we're going to be basically",
    "start": "12389",
    "end": "20070"
  },
  {
    "text": "listening or turning about Experian leverages Amazon ec2 Amazon s3 Amazon",
    "start": "20070",
    "end": "27420"
  },
  {
    "text": "EBS with cloud era Hadoop and for their big data analytics I'm also gonna do a",
    "start": "27420",
    "end": "33450"
  },
  {
    "text": "little bit of a talk around EBS itself and kind of give you a little preview on Hadoop so let's go ahead and get started",
    "start": "33450",
    "end": "41120"
  },
  {
    "text": "there we go so under big analytics we have about five general workload categories and if",
    "start": "42980",
    "end": "52530"
  },
  {
    "start": "44000",
    "end": "44000"
  },
  {
    "text": "you were using Amazon services Amazon EMR redshift Kinesis or dynamo DB for",
    "start": "52530",
    "end": "59910"
  },
  {
    "text": "example you don't need to worry too much about managing your big data solutions in terms of your storage and your",
    "start": "59910",
    "end": "67500"
  },
  {
    "text": "capacity as is all managed for you but for today we're going to be focusing more on cloud era and doing cloud era on",
    "start": "67500",
    "end": "75180"
  },
  {
    "text": "ec2 so we're learning about how to optimize your storage choose to write instant types and the right volume types",
    "start": "75180",
    "end": "82500"
  },
  {
    "text": "for your storage for cloud era so back in when Hadoop was first came around the",
    "start": "82500",
    "end": "93150"
  },
  {
    "start": "86000",
    "end": "86000"
  },
  {
    "text": "idea was that we had a lot of expensive network storage and a very expensive",
    "start": "93150",
    "end": "98159"
  },
  {
    "text": "dreck attached store devious sans and a lot of expensive servers so to in order",
    "start": "98159",
    "end": "103979"
  },
  {
    "text": "to do big data where storage was expensive Hadoop was designed with a file system",
    "start": "103979",
    "end": "110040"
  },
  {
    "text": "in mind that leveraged commodity hardware commodity disks so it was very",
    "start": "110040",
    "end": "115259"
  },
  {
    "text": "inexpensive and we they replicated the disks and servers multiple times to give",
    "start": "115259",
    "end": "122490"
  },
  {
    "text": "you better durability better availability of your services but we have we look in the cloud actually all",
    "start": "122490",
    "end": "130440"
  },
  {
    "text": "changes because with EBS being durable and persistent we can take some",
    "start": "130440",
    "end": "136320"
  },
  {
    "text": "advantages that we didn't have before so we have basically network-attached storage but allows you to decouple your",
    "start": "136320",
    "end": "142740"
  },
  {
    "text": "compute from the storage allows you to scale each component as needed for your",
    "start": "142740",
    "end": "148920"
  },
  {
    "text": "Big Data solution and optimize your costs while you're doing it we also offer an instant store which is a",
    "start": "148920",
    "end": "155520"
  },
  {
    "text": "ephemeral storage for certain use cases as well so to to remind you of some of",
    "start": "155520",
    "end": "166140"
  },
  {
    "start": "163000",
    "end": "163000"
  },
  {
    "text": "the building blocks we have with our server our storage services we have our object storage which is Amazon s3 and",
    "start": "166140",
    "end": "172950"
  },
  {
    "text": "Mazon glacier this provides eleven lines of durability and it's you know it's",
    "start": "172950",
    "end": "178770"
  },
  {
    "text": "basically around file objects we also have Amazon EFS switches are highly",
    "start": "178770",
    "end": "186060"
  },
  {
    "text": "scalable and available of elastic file service which is basically NFS that's",
    "start": "186060",
    "end": "192300"
  },
  {
    "text": "being served from a fully managed solution and then we have block storage which is the raw discs you would",
    "start": "192300",
    "end": "197400"
  },
  {
    "text": "normally see on a server and and we're going to be focusing on that today with Amazon EBS so we have two kinds of block",
    "start": "197400",
    "end": "208050"
  },
  {
    "start": "205000",
    "end": "205000"
  },
  {
    "text": "storage offerings you'll get ephemeral or instant store with ec2 which is the actual storage",
    "start": "208050",
    "end": "213870"
  },
  {
    "text": "that's attached to the physical hosts that your instance run on and he's",
    "start": "213870",
    "end": "219180"
  },
  {
    "text": "coming either sdd or HDD offerings we have EBS SSD back volumes and these come",
    "start": "219180",
    "end": "228630"
  },
  {
    "text": "in gp2 and i/o one offerings and then we have eb abs HDD back volumes which come",
    "start": "228630",
    "end": "237420"
  },
  {
    "text": "in st one and sc1 volumes so amazon ec2",
    "start": "237420",
    "end": "242640"
  },
  {
    "start": "240000",
    "end": "240000"
  },
  {
    "text": "instance store as i mentioned earlier is non persistent it's ephemeral it's local to the hosts is where your instance is",
    "start": "242640",
    "end": "250049"
  },
  {
    "text": "running so the data isn't persistent and if you terminate your instance or",
    "start": "250049",
    "end": "255209"
  },
  {
    "text": "there's an issue with the host that access that data is no longer available so you lose that data the data is not",
    "start": "255209",
    "end": "261780"
  },
  {
    "text": "replicated anywhere so it's basically what you have on that instant host itself there's",
    "start": "261780",
    "end": "267210"
  },
  {
    "text": "concept of snapshots so if you want to copy data you're doing all manually you",
    "start": "267210",
    "end": "272490"
  },
  {
    "text": "can get into the store in either SSD or HDD instance types and so now what is",
    "start": "272490",
    "end": "279539"
  },
  {
    "start": "278000",
    "end": "278000"
  },
  {
    "text": "EBS EBS is our block storage as a service that offers five nines of",
    "start": "279539",
    "end": "285090"
  },
  {
    "text": "availability and there's an API that allows you to create your volumes and",
    "start": "285090",
    "end": "291120"
  },
  {
    "text": "then you can attach and detach your volumes to your ec2 instances and we'll see how this flexibility can really be",
    "start": "291120",
    "end": "298710"
  },
  {
    "text": "an advantage for how you design your big data applications this is a network attached storage through where you",
    "start": "298710",
    "end": "306900"
  },
  {
    "text": "connect those volumes to your instance however you're not getting a dedicated",
    "start": "306900",
    "end": "312599"
  },
  {
    "text": "disk you're getting portion of volumes create comprised of many many disks that",
    "start": "312599",
    "end": "318750"
  },
  {
    "text": "are shared so as I mentioned the volumes",
    "start": "318750",
    "end": "325050"
  },
  {
    "text": "persist independent ec2 you can detach and reattach as you need to you can",
    "start": "325050",
    "end": "330270"
  },
  {
    "text": "separate your compute from your storage so if you're using a ephemeral storage",
    "start": "330270",
    "end": "335430"
  },
  {
    "text": "instance stores you're very limited in how you can scale your storage there's a fine line capacity on those instance",
    "start": "335430",
    "end": "342180"
  },
  {
    "text": "sizes and if you want to add more storage you need to add more instances with EBS you can actually just scale",
    "start": "342180",
    "end": "352409"
  },
  {
    "text": "appropriately you can add more volumes and attach them to your instance as you need to you can attach and detach",
    "start": "352409",
    "end": "359240"
  },
  {
    "text": "between instances within an availability zones so you actually can take an instance down or detach your volumes",
    "start": "359240",
    "end": "366300"
  },
  {
    "text": "shutting this down make a new one and reattach your volumes this allows you to play around experiment allows you to",
    "start": "366300",
    "end": "373009"
  },
  {
    "text": "right-size the instance for your need so if you need more compute you can change your instance size and then reattach",
    "start": "373009",
    "end": "379139"
  },
  {
    "text": "your volumes I have shown here basically",
    "start": "379139",
    "end": "384930"
  },
  {
    "text": "how you just reattach from one to another and it really is that simple so",
    "start": "384930",
    "end": "391139"
  },
  {
    "text": "we have a lot as we have a lot of many offerings of volumes how do you really",
    "start": "391139",
    "end": "397800"
  },
  {
    "text": "choose make the right decision what you're going to use so in Gen roll oh my animation is gone so in",
    "start": "397800",
    "end": "405010"
  },
  {
    "text": "general gp2 is the volume that's our general-purpose work for it works for",
    "start": "405010",
    "end": "412300"
  },
  {
    "start": "406000",
    "end": "406000"
  },
  {
    "text": "most use cases it's it's a lot supports random i/o it works well what's a contra",
    "start": "412300",
    "end": "418240"
  },
  {
    "text": "oh it's the general volume so if you really don't care what volume you want to use gp2 is a good choice if you have",
    "start": "418240",
    "end": "424090"
  },
  {
    "text": "a need for dedicated consistent latency and higher up higher I ups then you can",
    "start": "424090",
    "end": "431260"
  },
  {
    "text": "use il-1 and this will give you a higher consist of latency and up to a lot higher ups if you really need an",
    "start": "431260",
    "end": "438100"
  },
  {
    "text": "extremely on I up so you can use I instances I three for example which have nvme SSDs in get to three million I ops",
    "start": "438100",
    "end": "446410"
  },
  {
    "text": "but again that's going to be ephemeral or temporary storage it won't be anything permanent if you're looking at",
    "start": "446410",
    "end": "453430"
  },
  {
    "text": "throughput and costs are in our consideration then you can use our HDD",
    "start": "453430",
    "end": "459130"
  },
  {
    "text": "volumes which will be like SC one or st one SC one is a little lower throughput than our st one volumes and so you can",
    "start": "459130",
    "end": "467380"
  },
  {
    "text": "balance your costs and performance with that and then we also have a instant",
    "start": "467380",
    "end": "472870"
  },
  {
    "text": "store ephemeral type with a d2 as well so abs also offers encryption and this",
    "start": "472870",
    "end": "480700"
  },
  {
    "start": "478000",
    "end": "478000"
  },
  {
    "text": "is it's it's very very simple use you just enable it when you're creating your",
    "start": "480700",
    "end": "486460"
  },
  {
    "text": "volume it's that simple to do you can encrypt your you can attach you can",
    "start": "486460",
    "end": "492820"
  },
  {
    "text": "actually attach both encrypted and decrypted volumes to any of our instances as you need and there's no",
    "start": "492820",
    "end": "498910"
  },
  {
    "text": "import no performance impact at all use encryption for this it and when you make",
    "start": "498910",
    "end": "505600"
  },
  {
    "text": "snapshots of these volumes they're also encrypt it as well well my amination my",
    "start": "505600",
    "end": "514390"
  },
  {
    "text": "animation is a little off here sorry okay so we also have a concept of EBS",
    "start": "514390",
    "end": "520599"
  },
  {
    "start": "518000",
    "end": "518000"
  },
  {
    "text": "optimized instances and this is where if you normally would use EBS on certain",
    "start": "520600",
    "end": "526990"
  },
  {
    "text": "instances the bandwidth to talk to the storage it's also shared with the bandwidth to do everything else so it's",
    "start": "526990",
    "end": "533170"
  },
  {
    "text": "a shared bandwidth in this example here c-32 x-large has a hunt 25 megabits of",
    "start": "533170",
    "end": "538420"
  },
  {
    "text": "bandwidth that shared between EBS and your other your other services like your",
    "start": "538420",
    "end": "543940"
  },
  {
    "text": "other network services s3 other instances the databases and such so when",
    "start": "543940",
    "end": "552490"
  },
  {
    "text": "you have an optimized instance for example a you can actually dedicate or",
    "start": "552490",
    "end": "558280"
  },
  {
    "text": "in this case you can enable EBS optimized and it gives you a dedicated set of bandwidth for your storage that",
    "start": "558280",
    "end": "565180"
  },
  {
    "text": "is separate from the rest of your network so you get better performance better throughput without sharing that bandwidth with other network services so",
    "start": "565180",
    "end": "578790"
  },
  {
    "text": "it's an ABS optimized is now enabled by default on most of our newer instance",
    "start": "578790",
    "end": "584200"
  },
  {
    "text": "types such as a C 4 and C 5 instance family and also when it's enabled by",
    "start": "584200",
    "end": "589960"
  },
  {
    "text": "default there's no additional cost it's just it is you just get it there are some instances where it's not enabled by",
    "start": "589960",
    "end": "595840"
  },
  {
    "text": "default but you can't enable it and there's a little bit extra cross for that but you do get dedicated bandwidth",
    "start": "595840",
    "end": "600940"
  },
  {
    "text": "in those situations there are also some instance types that do not support it right now such as a C 3/8 X large and by",
    "start": "600940",
    "end": "609880"
  },
  {
    "text": "animation together ok so we're getting into a little bit of",
    "start": "609880",
    "end": "616840"
  },
  {
    "start": "615000",
    "end": "615000"
  },
  {
    "text": "Hadoop here this is kind of giving you a background for what we're gonna be learning with experience experience with",
    "start": "616840",
    "end": "624220"
  },
  {
    "text": "big data analytics and Hadoop is is uses an HDFS file system that is very",
    "start": "624220",
    "end": "632650"
  },
  {
    "text": "sequential i/o based that does read and write to sacral in 64 megabytes or larger blocks it doesn't do any random",
    "start": "632650",
    "end": "639400"
  },
  {
    "text": "i/o in terms of terms of the actual file system itself it replicates a data three times by default so you have three",
    "start": "639400",
    "end": "646780"
  },
  {
    "text": "copies your data across your Hadoop cluster so there also you can also layer",
    "start": "646780",
    "end": "653590"
  },
  {
    "text": "different file systems and make them compatible HDFS by using an API calls",
    "start": "653590",
    "end": "659320"
  },
  {
    "text": "for example with EMR FS which is the file system we use with Amazon EMR it",
    "start": "659320",
    "end": "664750"
  },
  {
    "text": "uses an API call the translate or build a bridge between Amazon s3 API calls and Hadoop",
    "start": "664750",
    "end": "671920"
  },
  {
    "text": "HDFS I API calls so we actually can run Amazon EMR on s3 so with Amazon s3 HDFS",
    "start": "671920",
    "end": "683010"
  },
  {
    "start": "678000",
    "end": "678000"
  },
  {
    "text": "might be a good choice to run on s3 there's some advantages with that for example you can scale out horizontally",
    "start": "683010",
    "end": "689550"
  },
  {
    "text": "without any data distribution you can just keep growing your data and keep growing your clusters well I haven't do",
    "start": "689550",
    "end": "695380"
  },
  {
    "text": "anything because the data is all available automatically for you there's no need to do any disaster recovery",
    "start": "695380",
    "end": "701080"
  },
  {
    "text": "because Amazon s3 is extremely durable with eleven nines of durability so you",
    "start": "701080",
    "end": "706540"
  },
  {
    "text": "don't need to have multiple copies around you still should have probably two but you don't need to do snapshots",
    "start": "706540",
    "end": "712330"
  },
  {
    "text": "or backup sort of those kind of things for a availability aspect you may still",
    "start": "712330",
    "end": "717940"
  },
  {
    "text": "want to do that for data integrity and you also can use transient clusters now",
    "start": "717940",
    "end": "723580"
  },
  {
    "text": "if you're using s3 so you can actually have clusters that you run only when you want to do your your analysis or run",
    "start": "723580",
    "end": "730960"
  },
  {
    "text": "your jobs and then you can turn them off and your data is still available in Amazon s3 for other Apple applications",
    "start": "730960",
    "end": "736420"
  },
  {
    "text": "or consumers but there are some disadvantages as well Amazon s3 it does",
    "start": "736420",
    "end": "742870"
  },
  {
    "text": "it doesn't support immediate consistency so you have some consistency issues for",
    "start": "742870",
    "end": "748000"
  },
  {
    "text": "example if you do a rename operation on HDFS your renaming the actual data structure the the node name inside HDFS",
    "start": "748000",
    "end": "756280"
  },
  {
    "text": "and it happens in place well the Amazon s3 agree me a copy command is gonna copy the data from one",
    "start": "756280",
    "end": "763030"
  },
  {
    "text": "file to another to do the rename so it's a eventually consistent you can there",
    "start": "763030",
    "end": "768070"
  },
  {
    "text": "are some mechanisms in place to address this the latest Apache Hadoop previews",
    "start": "768070",
    "end": "774970"
  },
  {
    "text": "and support of s3 and s3 a s3 guard kind of addressed some these consistency",
    "start": "774970",
    "end": "781030"
  },
  {
    "text": "issues Amazon EMR FS will use dynamo DB in the backend to do metadata keep track",
    "start": "781030",
    "end": "788230"
  },
  {
    "text": "of metadata so I can keep tracking consistency issues for you security has various amounts of support right now",
    "start": "788230",
    "end": "794050"
  },
  {
    "text": "it's getting better all the time and there may be some kinda building issues with some certain distributions or",
    "start": "794050",
    "end": "799540"
  },
  {
    "text": "certain engines that you might be using so when should you use HDFS on EBS really comes down to",
    "start": "799540",
    "end": "808200"
  },
  {
    "start": "803000",
    "end": "803000"
  },
  {
    "text": "when you want really high I ops and high performance so abs is going to be much",
    "start": "808200",
    "end": "813330"
  },
  {
    "text": "more performant for your storage than using Amazon s3 if you have long-running",
    "start": "813330",
    "end": "819510"
  },
  {
    "text": "clusters for example if you're running a map power distribution you're running a map RFS and you have a global namespace",
    "start": "819510",
    "end": "825360"
  },
  {
    "text": "most clusters are always gonna run so nets and that in that case running it on EBS makes a lot of sense if you're doing",
    "start": "825360",
    "end": "833130"
  },
  {
    "text": "your own distribution like we're doing discussing cloud era here or Hortonworks format bar then you probably want to use",
    "start": "833130",
    "end": "839120"
  },
  {
    "text": "HDFS on EBS so if you're using HDS HDFS",
    "start": "839120",
    "end": "846060"
  },
  {
    "text": "on EBS you can actually optimize your choice of storage depending on the nature of your cluster and how you're",
    "start": "846060",
    "end": "853500"
  },
  {
    "text": "handling your jobs and how you're processing things so you have a choice of Amazon ec2 instance store which is",
    "start": "853500",
    "end": "861870"
  },
  {
    "text": "the ephemeral storage and a d2 instance can get about three gigabytes a second a",
    "start": "861870",
    "end": "867000"
  },
  {
    "text": "throughput overall but you're you're actually an imitation is really more than Network and compute than it is",
    "start": "867000",
    "end": "874200"
  },
  {
    "text": "going to be the actual instance storage size so it's very common that you need to scale your computer higher to get to",
    "start": "874200",
    "end": "879960"
  },
  {
    "text": "throughput so in this case Amazon EBS st-1 volumes or a really good choice for",
    "start": "879960",
    "end": "885660"
  },
  {
    "text": "Hadoop clusters because of this crucial i/o and the throughput 5 up to 500",
    "start": "885660",
    "end": "891930"
  },
  {
    "text": "megabytes per second with bursts and the baseline of 40 megabytes is works very",
    "start": "891930",
    "end": "897930"
  },
  {
    "text": "well for Hadoop workloads recommending that you use at least two terabyte volume sizes to get the most out of your",
    "start": "897930",
    "end": "904800"
  },
  {
    "text": "buckets manual your burst performance and also because Amazon EFS I'm sorry",
    "start": "904800",
    "end": "910410"
  },
  {
    "text": "EBS replicates automatically within the availability zone you can route you can",
    "start": "910410",
    "end": "916650"
  },
  {
    "text": "reduce your replication factor Hadoop from 2 from 3 to 2 3 is a default and",
    "start": "916650",
    "end": "921720"
  },
  {
    "text": "you can actually save some storage cost as well because you don't need to have the extra data around for that durability",
    "start": "921720",
    "end": "928640"
  },
  {
    "text": "ok so now we're going to be moving into experience story and I'm going to turn over to Alex who's gonna share his",
    "start": "932320",
    "end": "938450"
  },
  {
    "text": "experiences thanks Paul for that",
    "start": "938450",
    "end": "943850"
  },
  {
    "text": "terrific overview so I'm Alex Vaughn I'm the architect for the Bastet group that",
    "start": "943850",
    "end": "950810"
  },
  {
    "text": "Experian I come to experiment from cloud era where I you know deployed CDH on",
    "start": "950810",
    "end": "957910"
  },
  {
    "text": "ISIL on IBM or exi or metal AWS so to",
    "start": "957910",
    "end": "964190"
  },
  {
    "text": "give you some context of what we're doing at Experian you know we have a large legacy infrastructure db2",
    "start": "964190",
    "end": "972550"
  },
  {
    "text": "mainframe lots of data not a mountain of data and what we've done is we've we've",
    "start": "972550",
    "end": "978830"
  },
  {
    "text": "built a analytical platform to sort of prove out the migration into the cloud",
    "start": "978830",
    "end": "984730"
  },
  {
    "text": "that's the animal of analytical sandbox that I'll be talking about it's basically a hybrid cloud platform and in",
    "start": "984730",
    "end": "991670"
  },
  {
    "text": "its in its long-term future it will be integrating batch and real-time data we've done this for about a year only a",
    "start": "991670",
    "end": "998750"
  },
  {
    "text": "little over a year we have 3ch clusters 800 terabytes of EBS st-1 1.1 petabytes",
    "start": "998750",
    "end": "1007810"
  },
  {
    "text": "of s3 and it's probably going to double in the next lesson year and 70 terabytes",
    "start": "1007810",
    "end": "1014440"
  },
  {
    "text": "of EBS GPT those numbers are actually kind of an indicative of this entire",
    "start": "1014440",
    "end": "1019540"
  },
  {
    "text": "talk of how we use storage so how did we manage to go into the cloud well this is",
    "start": "1019540",
    "end": "1026470"
  },
  {
    "start": "1022000",
    "end": "1022000"
  },
  {
    "text": "a diagram that shows a very rough idea of the architecture the data flow the",
    "start": "1026470",
    "end": "1034449"
  },
  {
    "text": "bottom box is our data centers one of our data centers customers go through a",
    "start": "1034449",
    "end": "1039600"
  },
  {
    "text": "deep Citrix cluster and everything goes",
    "start": "1039600",
    "end": "1045850"
  },
  {
    "text": "through a VPN we're replacing that eventually with Direct Connect for performance and whatnot and the data",
    "start": "1045850",
    "end": "1052960"
  },
  {
    "text": "itself gets exported from mainframe db2 depersonalized and everything gets stored in s3",
    "start": "1052960",
    "end": "1059830"
  },
  {
    "text": "initially and then processed and stored in EBS and back to s3 so that's the general dataflow we only",
    "start": "1059830",
    "end": "1067070"
  },
  {
    "text": "use PCI DSS services that's extremely important for our internal security governance the ones in green are the",
    "start": "1067070",
    "end": "1073880"
  },
  {
    "text": "ones that are very important to us you can see EMR there it's it's also we do a",
    "start": "1073880",
    "end": "1079610"
  },
  {
    "text": "lot of processing with EMR because EMR has the s3 file system which you know",
    "start": "1079610",
    "end": "1086600"
  },
  {
    "text": "avoids the eventual consistency issue okay so what's s3 what can I say about",
    "start": "1086600",
    "end": "1094850"
  },
  {
    "start": "1092000",
    "end": "1092000"
  },
  {
    "text": "su it's it's everywhere right like you feel it when you go to work let me go to church you know when you pay your taxes",
    "start": "1094850",
    "end": "1102370"
  },
  {
    "text": "remember that would be great if Laurence Fishburne talked about s3 so it's kind",
    "start": "1102370",
    "end": "1108950"
  },
  {
    "text": "of like the matrix it's really the backbone for the ascend and for Experian you know there's a 11 9s of durability",
    "start": "1108950",
    "end": "1116720"
  },
  {
    "text": "like Paul said so it serves as a really good record of store and if you're using",
    "start": "1116720",
    "end": "1123890"
  },
  {
    "text": "CDH 510 and below you're going to have problems with s3 if you're doing",
    "start": "1123890",
    "end": "1130730"
  },
  {
    "text": "transactional processing because of the eventual consistency you know there's third-party systems like data bricks is",
    "start": "1130730",
    "end": "1139490"
  },
  {
    "text": "DB i/o SD card which hunk comes with from",
    "start": "1139490",
    "end": "1145100"
  },
  {
    "text": "Cloudera CDH 511 plus so those those can be used safely and Emaar of course is s3",
    "start": "1145100",
    "end": "1152990"
  },
  {
    "text": "safe for transactional writes so the way we also use s3 is that you know we we",
    "start": "1152990",
    "end": "1160700"
  },
  {
    "text": "share data across clusters and across the AWS accounts we use it as a configuration store for initializing or",
    "start": "1160700",
    "end": "1167720"
  },
  {
    "text": "servers for CloudFormation SSN user data and logs and backup that's",
    "start": "1167720",
    "end": "1173750"
  },
  {
    "text": "pretty much since this is more like an EBS talk I would just leave it at that",
    "start": "1173750",
    "end": "1180610"
  },
  {
    "start": "1180000",
    "end": "1180000"
  },
  {
    "text": "what we can do though is also enable some s3 a optimizations that are",
    "start": "1181000",
    "end": "1188090"
  },
  {
    "text": "available in like more modern versions of CDH you know the FAFSA upload is is",
    "start": "1188090",
    "end": "1193640"
  },
  {
    "text": "very important step back to true defaults false the maximum number of connections 1500",
    "start": "1193640",
    "end": "1200070"
  },
  {
    "text": "should be a minimum really but you can know you can max it out from a total cluster cores so by default the fast",
    "start": "1200070",
    "end": "1206730"
  },
  {
    "text": "upload buffer is on disk which is EBS which is kind of slow it's going back",
    "start": "1206730",
    "end": "1211950"
  },
  {
    "text": "and forth in network so doing it as a set is it a memory array and I set the",
    "start": "1211950",
    "end": "1217559"
  },
  {
    "text": "most apart size to be equivalent to the DFS block size which should be around",
    "start": "1217559",
    "end": "1222750"
  },
  {
    "text": "128 megabytes or 256 if you're like you know using Impala a lot and the total",
    "start": "1222750",
    "end": "1229710"
  },
  {
    "text": "amount of buffer size is really a calculation of the multi part size plus",
    "start": "1229710",
    "end": "1235799"
  },
  {
    "text": "the number of active blocks that are transferring block size for our s3 data",
    "start": "1235799",
    "end": "1241620"
  },
  {
    "text": "is I set the same as the same as our HDFS block size and the s3 endpoint",
    "start": "1241620",
    "end": "1247650"
  },
  {
    "text": "region is very good because all our traffic goes through an s3 endpoint that for security it doesn't leave the V PC",
    "start": "1247650",
    "end": "1253850"
  },
  {
    "text": "and the threads max should be set to a number of available cores that yarn can",
    "start": "1253850",
    "end": "1262200"
  },
  {
    "text": "use for that particular node manager that particular node worker so how do we",
    "start": "1262200",
    "end": "1268260"
  },
  {
    "start": "1267000",
    "end": "1267000"
  },
  {
    "text": "plan for high availability so on a you know classic bare metal or whatnot you",
    "start": "1268260",
    "end": "1275309"
  },
  {
    "text": "know there are three components one is sort of like the racks like how you distribute the workers there's different",
    "start": "1275309",
    "end": "1282179"
  },
  {
    "text": "types of roles you know there's the master utility server role gateway role",
    "start": "1282179",
    "end": "1287340"
  },
  {
    "text": "worker role worker role contains you know the Impala daemon the node manager",
    "start": "1287340",
    "end": "1293630"
  },
  {
    "text": "the data node which is for the HDFS storage so we can simulate racks via a",
    "start": "1293630",
    "end": "1300840"
  },
  {
    "text": "Z's so what I show here is that you know our V PC consists of five AZ's and we",
    "start": "1300840",
    "end": "1308429"
  },
  {
    "text": "distribute the cluster across two or more a Z's we have a SAS cluster that we",
    "start": "1308429",
    "end": "1314790"
  },
  {
    "text": "distribute the same way we have Hugh instances that are specific for each",
    "start": "1314790",
    "end": "1320220"
  },
  {
    "text": "client because he was kind of a you know a security hole there's a lot of features that are exposed with you so",
    "start": "1320220",
    "end": "1326700"
  },
  {
    "text": "every every multitec this is a multi tenant cluster every client gets its own hue database we have our",
    "start": "1326700",
    "end": "1333960"
  },
  {
    "text": "studio across multiple zones and a tableau servers as well another major",
    "start": "1333960",
    "end": "1341960"
  },
  {
    "text": "component of high availability for cloud era for Hadoop in general is the",
    "start": "1341960",
    "end": "1347730"
  },
  {
    "text": "metadata you know that's where cloud era manager keeps its data that's where the hive Menace story is so you're backing",
    "start": "1347730",
    "end": "1354510"
  },
  {
    "text": "database really should be a cluster of some sort we use our yes RDS is a deploy",
    "start": "1354510",
    "end": "1360390"
  },
  {
    "text": "in a highly available production layout daily snapshots whatnot so with a with a",
    "start": "1360390",
    "end": "1367350"
  },
  {
    "text": "ZZZ we have the ability to have rack awareness and placement groups so just",
    "start": "1367350",
    "end": "1373710"
  },
  {
    "start": "1373000",
    "end": "1373000"
  },
  {
    "text": "to get into you know what you know Aziz and rax just to just to give you more",
    "start": "1373710",
    "end": "1379080"
  },
  {
    "text": "info you know the default replication is 3 a file is broken into blocks you know",
    "start": "1379080",
    "end": "1384929"
  },
  {
    "text": "you see yellow green purple the first block is written to Iraq whether that RAC be a bare-metal or in AZ and the",
    "start": "1384929",
    "end": "1392280"
  },
  {
    "text": "second and third blocks are in to another rack or a Z and because you know EBS has five nines of the bill",
    "start": "1392280",
    "end": "1398220"
  },
  {
    "text": "availability remember it EBS Lamia is not a hard drive it's a service and if you go if",
    "start": "1398220",
    "end": "1406260"
  },
  {
    "text": "you recall from the Google study on hard drives you know two percent of all drives the fail and with first year nine",
    "start": "1406260",
    "end": "1412260"
  },
  {
    "text": "percent fail within three years so this is much better so what we've done is to reduce costs",
    "start": "1412260",
    "end": "1418470"
  },
  {
    "text": "but we've we've lower the default replication to two and that's a 33",
    "start": "1418470",
    "end": "1424169"
  },
  {
    "text": "percent drop in SEO and storage so",
    "start": "1424169",
    "end": "1429299"
  },
  {
    "start": "1429000",
    "end": "1429000"
  },
  {
    "text": "placement groups are also very important they provide the lowest latency among nodes highest network throughput they're",
    "start": "1429299",
    "end": "1437010"
  },
  {
    "text": "essentially a cluster you know they the only issues is that you've got a pre pre",
    "start": "1437010",
    "end": "1447540"
  },
  {
    "text": "plan your cluster because adding addition knows within a placement group isn't always guaranteed so what we do is",
    "start": "1447540",
    "end": "1454980"
  },
  {
    "text": "we actually sort of like deploy a pool of worker nodes more than we need pretty",
    "start": "1454980",
    "end": "1462690"
  },
  {
    "text": "much all considered and the lowest possible that the least expensive instance type which is an M",
    "start": "1462690",
    "end": "1468200"
  },
  {
    "text": "for large and when we need it we bump it",
    "start": "1468200",
    "end": "1473330"
  },
  {
    "text": "up to the the size that we we need and I'll get into that in a bit so some of",
    "start": "1473330",
    "end": "1479270"
  },
  {
    "text": "these placement groups can be enabled for certain ec2 types here the MCI our",
    "start": "1479270",
    "end": "1484850"
  },
  {
    "text": "XP f-series smallest type is a large and really they they do best in ten gigabit",
    "start": "1484850",
    "end": "1493900"
  },
  {
    "text": "Network and higher so the second part of like deploying a highly available",
    "start": "1493900",
    "end": "1500300"
  },
  {
    "start": "1496000",
    "end": "1496000"
  },
  {
    "text": "cluster Hadoop cluster is of course to distribute the services across the zones",
    "start": "1500300",
    "end": "1505310"
  },
  {
    "text": "so here we have you know two racks and two AZ's and each rack or each AZ",
    "start": "1505310",
    "end": "1511220"
  },
  {
    "text": "contains a pretty much a exact copy of the other rack if two services",
    "start": "1511220",
    "end": "1517730"
  },
  {
    "text": "everything is they say H a unless it's it's century so it's CDH v dot thirteen",
    "start": "1517730",
    "end": "1525800"
  },
  {
    "text": "which is what's just released about two three weeks ago clothes the single point",
    "start": "1525800",
    "end": "1530990"
  },
  {
    "text": "of failure for century and the hive Metis for service there now we can deploy every single service in an H a",
    "start": "1530990",
    "end": "1538850"
  },
  {
    "text": "way manner things that require memory",
    "start": "1538850",
    "end": "1544600"
  },
  {
    "text": "our name nodes memories memory intensive service rules include name nodes hive",
    "start": "1544600",
    "end": "1550310"
  },
  {
    "text": "server to Uzis so in choosing your instance type remember to reserve like",
    "start": "1550310",
    "end": "1557750"
  },
  {
    "text": "memory for the system processes as well that that ensures a more stable deployment so easy to instance ec2",
    "start": "1557750",
    "end": "1567710"
  },
  {
    "start": "1564000",
    "end": "1564000"
  },
  {
    "text": "instance types to for a secure ado in",
    "start": "1567710",
    "end": "1573490"
  },
  {
    "text": "choosing the particular instance types I go through four different parameter storage network whether it's never",
    "start": "1573490",
    "end": "1580580"
  },
  {
    "text": "optimizable the total memory and the memory to vcore or Core ratio you know",
    "start": "1580580",
    "end": "1586390"
  },
  {
    "text": "so we stay away from actual like d2s because everything is aes-256 encrypted",
    "start": "1586390",
    "end": "1594830"
  },
  {
    "text": "so EBS provides that the there's EBS optimized instances and it's",
    "start": "1594830",
    "end": "1601240"
  },
  {
    "text": "flexible like Paul said to add and remove drives so ec2 types that are",
    "start": "1601240",
    "end": "1609960"
  },
  {
    "start": "1606000",
    "end": "1606000"
  },
  {
    "text": "appropriate for the master utility server role are often compute and memory",
    "start": "1609960",
    "end": "1615160"
  },
  {
    "text": "types you know these include the m4 for Excel C for extra larges are for two",
    "start": "1615160",
    "end": "1621010"
  },
  {
    "text": "excels are for four excels and you can see some of the specs that I used to",
    "start": "1621010",
    "end": "1626740"
  },
  {
    "text": "consider what's appropriate you know the number of cores the total memory the throughput of each in ec2 type is very",
    "start": "1626740",
    "end": "1634660"
  },
  {
    "text": "important because this isn't a limited I'm sorry bandwidth that goes through an",
    "start": "1634660",
    "end": "1640930"
  },
  {
    "text": "ec2 there's only so much the pipe is only so",
    "start": "1640930",
    "end": "1646330"
  },
  {
    "text": "large so some of these have different types of throughput and we also",
    "start": "1646330",
    "end": "1651820"
  },
  {
    "text": "associate costs of course so if you",
    "start": "1651820",
    "end": "1657130"
  },
  {
    "text": "recall like a name node for each block that each HDFS block consumes about 300",
    "start": "1657130",
    "end": "1665110"
  },
  {
    "text": "bytes of data there's two components there's a file pointer and some other",
    "start": "1665110",
    "end": "1671710"
  },
  {
    "text": "metadata for each block and that's what the name node keeps in memory so we",
    "start": "1671710",
    "end": "1677740"
  },
  {
    "text": "usually use like eight gigabytes of heap for that particular service in hive server 2 now in my experience hive",
    "start": "1677740",
    "end": "1684250"
  },
  {
    "text": "server - if you're not using g1g see you're using CMS likely right and CMS",
    "start": "1684250",
    "end": "1690370"
  },
  {
    "text": "doesn't work well with heap sizes there are 14 gigs or higher you just have these top of the world type of garbage",
    "start": "1690370",
    "end": "1697180"
  },
  {
    "text": "collections so we've actually converted all our hive server instances to use G 1",
    "start": "1697180",
    "end": "1705100"
  },
  {
    "text": "GC that enables us to use heap sizes there were 30 40 50 gigs and hive server",
    "start": "1705100",
    "end": "1710770"
  },
  {
    "text": "- if you recall is the proxy for queries you know all the SQL that goes to your hive instance gets gets pushed through a",
    "start": "1710770",
    "end": "1717760"
  },
  {
    "text": "hive server - so you can imagine this like cluster where there are a lot of SAS clients making queries a lot of and",
    "start": "1717760",
    "end": "1723850"
  },
  {
    "text": "that's that could be a quite a bottleneck so what do I mean by memory",
    "start": "1723850",
    "end": "1731020"
  },
  {
    "start": "1729000",
    "end": "1729000"
  },
  {
    "text": "memory core ratio so let's let's say for this instance you know we want to use an",
    "start": "1731020",
    "end": "1736900"
  },
  {
    "text": "m4 10 Excel as a worker node versus an r4 8 Excel right so you see the memory",
    "start": "1736900",
    "end": "1743740"
  },
  {
    "text": "they're comparable the ratio for the",
    "start": "1743740",
    "end": "1749650"
  },
  {
    "text": "number of amount of memory - core reserving - or a course for the system",
    "start": "1749650",
    "end": "1755500"
  },
  {
    "text": "would be 4 gigabytes per core for the m4 10 Excel and you know seven point eight",
    "start": "1755500",
    "end": "1761020"
  },
  {
    "text": "gigabytes per core for the r4 eight Excel and in sort of configuring the",
    "start": "1761020",
    "end": "1767560"
  },
  {
    "text": "SPARC yarn executors parameters you you",
    "start": "1767560",
    "end": "1774400"
  },
  {
    "text": "know it's recommended that if you don't really understand your workload you use three to four time so three to five",
    "start": "1774400",
    "end": "1780880"
  },
  {
    "text": "cores per executors you know that would amount to about 12 16 and 20 gigabytes",
    "start": "1780880",
    "end": "1786130"
  },
  {
    "text": "per executors and that's actually too small for our type of workload large you",
    "start": "1786130",
    "end": "1793600"
  },
  {
    "text": "know big data type of workloads like you know things like hto any sort of",
    "start": "1793600",
    "end": "1798910"
  },
  {
    "text": "analysis machine learning requires it's much better to use fewer executors with",
    "start": "1798910",
    "end": "1803940"
  },
  {
    "text": "much larger memory so in our for 8x lr44 excel our for 16 excel those memory to",
    "start": "1803940",
    "end": "1811090"
  },
  {
    "text": "core ratios are you know are linear so",
    "start": "1811090",
    "end": "1818100"
  },
  {
    "text": "in the r4 8 Excel we have we can start off with you know 4 cores per executors",
    "start": "1818100",
    "end": "1824020"
  },
  {
    "text": "and have an executor of like 31 gigs that works really well to start off with",
    "start": "1824020",
    "end": "1831060"
  },
  {
    "start": "1831000",
    "end": "1831000"
  },
  {
    "text": "so in choosing you know easy to type instance types for worker",
    "start": "1831060",
    "end": "1837810"
  },
  {
    "text": "server roles we can pick between storage roles or memory types you know storage",
    "start": "1837810",
    "end": "1843970"
  },
  {
    "text": "roles are the t2s now here I just really the d-28 excels in our experience are",
    "start": "1843970",
    "end": "1849670"
  },
  {
    "text": "the only storage optimized ec2 types that we would consider it's the only one",
    "start": "1849670",
    "end": "1855970"
  },
  {
    "text": "that has 10 Gigabit Ethernet that's 48 terabytes of instance HDD very fast HDD",
    "start": "1855970",
    "end": "1861700"
  },
  {
    "text": "but it's ephemeral so it's not those those drives aren't encrypted now if you're you",
    "start": "1861700",
    "end": "1868090"
  },
  {
    "text": "you're using cloud era navigator or nav encrypt you can use nav encrypt to Lux",
    "start": "1868090",
    "end": "1875350"
  },
  {
    "text": "encrypt every single one of these volumes so that's you know 48 volumes",
    "start": "1875350",
    "end": "1880780"
  },
  {
    "text": "that need to be Lux encrypted and in cloud era navigator app nav encrypt actually sort of automates that there's",
    "start": "1880780",
    "end": "1886600"
  },
  {
    "text": "a Z tab file under it see that that handles it handles the mounting and decryption of these these block devices",
    "start": "1886600",
    "end": "1893230"
  },
  {
    "text": "when the when the server role I'm sorry when the server restarts but it's very",
    "start": "1893230",
    "end": "1899409"
  },
  {
    "text": "complex to set up very complex to manage and for in some ways it's too dense",
    "start": "1899409",
    "end": "1906460"
  },
  {
    "text": "because it's it's ephemeral as well so the mistakes happen you know the",
    "start": "1906460",
    "end": "1912520"
  },
  {
    "text": "instance could be stopped it could be a hardware failure so we would have to so",
    "start": "1912520",
    "end": "1918100"
  },
  {
    "text": "when when this happens all that data is lost and then when it comes back on there's a replication storm the blocks",
    "start": "1918100",
    "end": "1923919"
  },
  {
    "text": "have to be replicated across the cluster so you have reduced performance so we we",
    "start": "1923919",
    "end": "1929740"
  },
  {
    "text": "tend to avoid that sort of ephemeral storage and we go with ER for eight",
    "start": "1929740",
    "end": "1936880"
  },
  {
    "text": "excel 16 excels as the primary worker instances for our production clusters",
    "start": "1936880",
    "end": "1942270"
  },
  {
    "text": "these are ten gigabit twenty gigabit ethernet respectively they exceed",
    "start": "1942270",
    "end": "1950110"
  },
  {
    "text": "throughput the instance throughput for the D 280 excels EBS encryption out of",
    "start": "1950110",
    "end": "1956830"
  },
  {
    "text": "the box really seamless this envelope encryption is great with key rotation and whatnot and you have custom kernel",
    "start": "1956830",
    "end": "1964720"
  },
  {
    "text": "options for the ADAC cells and sixteen exhales as well and I like I said before great memory to vcore ratio and very",
    "start": "1964720",
    "end": "1972610"
  },
  {
    "text": "flexible storage strategy strategy what do I mean by that so as we grow this out",
    "start": "1972610",
    "end": "1977950"
  },
  {
    "text": "you know we often need more compute than storage so we may attach for just two",
    "start": "1977950",
    "end": "1983640"
  },
  {
    "text": "st1 volumes as HDFS and then just later on attached to more and I'll show you",
    "start": "1983640",
    "end": "1989440"
  },
  {
    "text": "why we use those numbers in a second so if you're ever wondering how like",
    "start": "1989440",
    "end": "1996550"
  },
  {
    "start": "1992000",
    "end": "1992000"
  },
  {
    "text": "what resources are consumed by cloudier manager you can go to the hosts tab and",
    "start": "1996550",
    "end": "2001560"
  },
  {
    "text": "click resources and you can see what cloudera manager thinks the resources",
    "start": "2001560",
    "end": "2007590"
  },
  {
    "text": "are being are needed for that particular server role it really is a function of",
    "start": "2007590",
    "end": "2013050"
  },
  {
    "text": "the the service roles that you deployed on that particular ec2 so that's a good",
    "start": "2013050",
    "end": "2021900"
  },
  {
    "text": "way to check things so the the talks",
    "start": "2021900",
    "end": "2027930"
  },
  {
    "start": "2024000",
    "end": "2024000"
  },
  {
    "text": "really come down to really like our for instance instances and EBS st-1 for workers you know so here I'm trying to",
    "start": "2027930",
    "end": "2036690"
  },
  {
    "text": "match the volume bursting baseline throughout throughput with the instance",
    "start": "2036690",
    "end": "2042480"
  },
  {
    "text": "type because only so much information can flow through an ec2 if you have for",
    "start": "2042480",
    "end": "2049379"
  },
  {
    "text": "instance two terabyte st once you're going to get the full 500 megabytes per",
    "start": "2049380",
    "end": "2054960"
  },
  {
    "text": "second of birth but only two of those drives will actually two of those rise active drives will consume like the",
    "start": "2054960",
    "end": "2061860"
  },
  {
    "text": "entire throughput for that ec2 and so really the way we size or just our dis",
    "start": "2061860",
    "end": "2069179"
  },
  {
    "text": "is not is a function of both complexity and ease of maintenance but also of",
    "start": "2069179",
    "end": "2074639"
  },
  {
    "text": "capacity you know how often are we going to deploy a 50 node cluster where every",
    "start": "2074640",
    "end": "2080100"
  },
  {
    "text": "worker you know has only 4 terabytes of st-1 storage unlikely so instead of like",
    "start": "2080100",
    "end": "2088350"
  },
  {
    "text": "say 22 terabyte drives we will use 4 10 terabyte drives for an r4 16 XL or two",
    "start": "2088350",
    "end": "2096179"
  },
  {
    "text": "for an AR for a to excel and and I I threw on the d-28 Excel there for",
    "start": "2096179",
    "end": "2101400"
  },
  {
    "text": "comparison you see the max throughput for each one hour for 16x cells are 20",
    "start": "2101400",
    "end": "2106440"
  },
  {
    "text": "to 20 Gigabit Ethernet so that's a function than network everything is",
    "start": "2106440",
    "end": "2112110"
  },
  {
    "text": "pretty much a function of network and having large fewer large sd1 volumes has",
    "start": "2112110",
    "end": "2118500"
  },
  {
    "text": "a benefit for EBS affinity you know I like I said I've worked with the amore X",
    "start": "2118500",
    "end": "2124680"
  },
  {
    "text": "I before and it just came as a sort of a precognition that that would be a good",
    "start": "2124680",
    "end": "2130380"
  },
  {
    "text": "idea and the EBS team basically said you know fewer large drives much better than",
    "start": "2130380",
    "end": "2136230"
  },
  {
    "text": "you know it's also much better for maintenance right who wants to deploy you know for a petabyte cluster that",
    "start": "2136230",
    "end": "2143310"
  },
  {
    "text": "would be like like 500 volumes so like I",
    "start": "2143310",
    "end": "2151530"
  },
  {
    "text": "said with the r48 excels and 16x sells we can do really we can do some low-level tuning you know there's C",
    "start": "2151530",
    "end": "2157260"
  },
  {
    "text": "stage C states are an idle power saving feature for most common most most",
    "start": "2157260",
    "end": "2164720"
  },
  {
    "text": "current Intel kernels and for an r4 16 Excel we have you know five CCS zero one",
    "start": "2164720",
    "end": "2173640"
  },
  {
    "text": "three six and seven you can actually cat that on the sis Sofia to see what you",
    "start": "2173640",
    "end": "2179819"
  },
  {
    "text": "five but and what I've shown there is that for C state zero there's like zero",
    "start": "2179819",
    "end": "2186060"
  },
  {
    "text": "of course zero micro seconds of latency to 4c 110 for C 344 C 6 and C 6 seems to",
    "start": "2186060",
    "end": "2195569"
  },
  {
    "text": "be the default idle State and I'll show you how we can tune that",
    "start": "2195569",
    "end": "2202349"
  },
  {
    "text": "so for under Etsy boot grub config you",
    "start": "2202349",
    "end": "2208349"
  },
  {
    "text": "can set Intel idle max CC 8 to equal one this means that this this coral the",
    "start": "2208349",
    "end": "2213930"
  },
  {
    "text": "kernel will keep the core at sea state 1 and no higher as you can see from the",
    "start": "2213930",
    "end": "2219510"
  },
  {
    "text": "left screen the default is C 6 95% of but most of all the cores are on this",
    "start": "2219510",
    "end": "2224550"
  },
  {
    "text": "idle inning the system is in sees a stick when you set that state o 95% will",
    "start": "2224550",
    "end": "2231630"
  },
  {
    "text": "be stuck in C C state 1 which is how does that pertain to performance or EBS",
    "start": "2231630",
    "end": "2237270"
  },
  {
    "text": "performance so in writing like a 1 gigabyte file dest e 1 we're using the",
    "start": "2237270",
    "end": "2242819"
  },
  {
    "text": "same are for 16 Excel same su onedrive complete system idle like this is not a",
    "start": "2242819",
    "end": "2249079"
  },
  {
    "text": "node that's being worked on but default you can default c6 state it takes 2",
    "start": "2249079",
    "end": "2258150"
  },
  {
    "text": "point 3 7 to 8 seconds you know and we set that idle it'll",
    "start": "2258150",
    "end": "2263920"
  },
  {
    "text": "write it in two point two eight seven seconds which is a p-value of point zero zero two which is you know significance",
    "start": "2263920",
    "end": "2270730"
  },
  {
    "text": "so there is some performance improvement now some of this stuff is very low level and the reason why we do this is because",
    "start": "2270730",
    "end": "2277690"
  },
  {
    "text": "as an RM prize we can't really use the AWS AMI and that is something you know",
    "start": "2277690",
    "end": "2283660"
  },
  {
    "text": "if you can get away with it you you should look into a lot of this stuff is",
    "start": "2283660",
    "end": "2290619"
  },
  {
    "text": "already baked into that ami so another thing we could do is is set the read",
    "start": "2290619",
    "end": "2296680"
  },
  {
    "start": "2294000",
    "end": "2294000"
  },
  {
    "text": "ahead buffer now this read ahead buffer so the read ahead buffer allows us to",
    "start": "2296680",
    "end": "2305010"
  },
  {
    "text": "write sequential workloads I have had better disk performance and throughput on sequential workloads it's very",
    "start": "2305010",
    "end": "2311589"
  },
  {
    "text": "important that you verify that your workload is sequential and that your writing large files because changing",
    "start": "2311589",
    "end": "2316660"
  },
  {
    "text": "this from default will actually negatively impact your performance otherwise so the recommended are a",
    "start": "2316660",
    "end": "2325300"
  },
  {
    "text": "buffer is one megabyte so you would set you know RA to 2048 KB and times",
    "start": "2325300",
    "end": "2333490"
  },
  {
    "text": "I'm sorry 2048 bytes times the 512 bytes sector equals one megabyte so I've",
    "start": "2333490",
    "end": "2340869"
  },
  {
    "text": "actually changed this to eight 192 for our our 460 excels and it's something",
    "start": "2340869",
    "end": "2347170"
  },
  {
    "text": "you can play with and it's very good that when you make these changes it's",
    "start": "2347170",
    "end": "2352599"
  },
  {
    "text": "you really should just test it and it's very workload dependent so that's very",
    "start": "2352599",
    "end": "2358450"
  },
  {
    "text": "much an experience setting but in general for sequential i/o a one",
    "start": "2358450",
    "end": "2366730"
  },
  {
    "text": "megabyte read RA which should be sufficient we also change it to deadline the deadline Iowa schedule by default at",
    "start": "2366730",
    "end": "2374710"
  },
  {
    "text": "cfq and there's a lot of things you could do with cfq but you know it's",
    "start": "2374710",
    "end": "2379839"
  },
  {
    "text": "intuition tells me deadline is better mainly because it prioritizes reads over writes reads tend to block it batches",
    "start": "2379839",
    "end": "2387700"
  },
  {
    "text": "i/o and latency is important for us",
    "start": "2387700",
    "end": "2393599"
  },
  {
    "start": "2394000",
    "end": "2394000"
  },
  {
    "text": "so I used Phyo to benchmark EBS performance here's an example of the",
    "start": "2394020",
    "end": "2400720"
  },
  {
    "text": "file file it's very very easy to use this this writes a 1 gigabyte file and",
    "start": "2400720",
    "end": "2408990"
  },
  {
    "text": "there's I use this to test some settings",
    "start": "2408990",
    "end": "2415240"
  },
  {
    "text": "that I'll show you in a second so how does for a for Tara for 10 terabyte sd-1",
    "start": "2415240",
    "end": "2423670"
  },
  {
    "text": "on a are for 16 XL how does it how does it perform so if we set the file job to",
    "start": "2423670",
    "end": "2431290"
  },
  {
    "text": "run for 4 reads verses for writes you know you could see the aggregate",
    "start": "2431290",
    "end": "2436510"
  },
  {
    "text": "throughput for each one and the 1783 is actually very important cuz that that",
    "start": "2436510",
    "end": "2442330"
  },
  {
    "text": "shows you that it maxes out the the ec2 throughput and but really you know often",
    "start": "2442330",
    "end": "2451150"
  },
  {
    "text": "the workloads mix you get mixed reads and mixed writes so you can set the file config to do that where two disks are",
    "start": "2451150",
    "end": "2458980"
  },
  {
    "text": "writing to this or reading and you can see the throughput there it seems like",
    "start": "2458980",
    "end": "2464170"
  },
  {
    "text": "you know the aggregate throughput is much higher like you know some 100 times 1,500 I mean I'm sorry I've plus 50",
    "start": "2464170",
    "end": "2470680"
  },
  {
    "text": "Niners it's like 20 up but the entire workload takes longer to run by about a couple hundred milliseconds that goes to",
    "start": "2470680",
    "end": "2479410"
  },
  {
    "text": "show you that we actually never you know that it's a really important week we don't we don't exceed the ec2 throughput",
    "start": "2479410",
    "end": "2486010"
  },
  {
    "text": "no matter what like I could put 10 10 terabyte st ones here on this particular",
    "start": "2486010",
    "end": "2492670"
  },
  {
    "text": "ec2 and we wouldn't get more than 17 hundred megabytes per second okay so we",
    "start": "2492670",
    "end": "2499870"
  },
  {
    "start": "2499000",
    "end": "2499000"
  },
  {
    "text": "also use a hybrid EBS layout now this is very different than say bare metal where",
    "start": "2499870",
    "end": "2505150"
  },
  {
    "text": "in bare metal we have one disk and it holds HDFS it holds yarn local data hive",
    "start": "2505150",
    "end": "2510850"
  },
  {
    "text": "local data and Palo local data so what I did here was we have a GB to boot volume",
    "start": "2510850",
    "end": "2517680"
  },
  {
    "text": "encrypted we have a data volume that's gp2 where we",
    "start": "2517680",
    "end": "2523620"
  },
  {
    "text": "you know we add the zookeeper data we separate zookeeper logs from zookeeper",
    "start": "2523620",
    "end": "2528810"
  },
  {
    "text": "data so we're basically bifurcating a flow for max performance since you keeper yarn local data is kept on the",
    "start": "2528810",
    "end": "2535620"
  },
  {
    "text": "SSD GP 2 volume hive local data as well and the cloud era manager hosts and",
    "start": "2535620",
    "end": "2541890"
  },
  {
    "text": "service data those used to be in a database if you recall but now they're just written to log files directly on",
    "start": "2541890",
    "end": "2547500"
  },
  {
    "text": "disks so those are written to those are random read/write workloads so they they",
    "start": "2547500",
    "end": "2553350"
  },
  {
    "text": "go well with st1 some of the other use cases we have for our SAS foundation",
    "start": "2553350",
    "end": "2560820"
  },
  {
    "text": "server we use i3 i3 16x cells we with ephemeral storage that's the recommended",
    "start": "2560820",
    "end": "2568790"
  },
  {
    "text": "using an ephemeral storage is recommended by SAS so we use LVM and",
    "start": "2568790",
    "end": "2575730"
  },
  {
    "text": "stripe those 8 terabyte 8 2 terabyte drives to make one 16 care bite work space SAS workspace for a ephemeral SAS",
    "start": "2575730",
    "end": "2584190"
  },
  {
    "text": "data and SD one drives for SAS data for the the persistent data okay so okay so",
    "start": "2584190",
    "end": "2595260"
  },
  {
    "text": "how does st-1 work with like I said se1 is really should be matched you should",
    "start": "2595260",
    "end": "2600420"
  },
  {
    "text": "use the EBS storage types that natural particular workload here files I show",
    "start": "2600420",
    "end": "2608120"
  },
  {
    "text": "performance or throughput of certain file sizes on st-1 so you can use DD or",
    "start": "2608120",
    "end": "2614220"
  },
  {
    "text": "you can use file and they both give similar results so for writing one KB",
    "start": "2614220",
    "end": "2620070"
  },
  {
    "text": "size files the bandwidth is like two to ten megabytes per second where you know",
    "start": "2620070",
    "end": "2626520"
  },
  {
    "text": "a Hadoop workload is you know 64 megabytes or higher and once you've hit",
    "start": "2626520",
    "end": "2634500"
  },
  {
    "text": "around 16 you can realize the full potential of the sd1 drive and there",
    "start": "2634500",
    "end": "2640260"
  },
  {
    "text": "we're exceeding the 400 megabytes per second because we're basically using some of the birth rates so it's my",
    "start": "2640260",
    "end": "2650730"
  },
  {
    "start": "2648000",
    "end": "2648000"
  },
  {
    "text": "assumption correct for like splitting or putting the yarn local data",
    "start": "2650730",
    "end": "2656330"
  },
  {
    "text": "on two gp2 well if you look at the yarn local data files on one particular node",
    "start": "2656330",
    "end": "2662210"
  },
  {
    "text": "that I just picked out at random you can see that you know so the yarn node manager dot locals dirt is the parameter",
    "start": "2662210",
    "end": "2669710"
  },
  {
    "text": "in a dupe that specifies where yarn keeps its local data non HDFS data and there are three directories in their",
    "start": "2669710",
    "end": "2675860"
  },
  {
    "text": "file cache user cash and the nm private one so you can see the sizes the size",
    "start": "2675860",
    "end": "2682250"
  },
  {
    "text": "range for the files and the number of files so if the user cache contains you know tens of thousands of small files so",
    "start": "2682250",
    "end": "2690440"
  },
  {
    "text": "it does make sense to put this type of data onto gp2 and not st-1 and the added",
    "start": "2690440",
    "end": "2699230"
  },
  {
    "text": "benefit is actually if you're using HDFS encryption like us we can just Lux",
    "start": "2699230",
    "end": "2704450"
  },
  {
    "text": "encrypt that entire GB to 200 gigabyte volume instead of the ten terabyte HDD",
    "start": "2704450",
    "end": "2709910"
  },
  {
    "text": "volume because this data really should be encrypted as well and because if you",
    "start": "2709910",
    "end": "2716180"
  },
  {
    "text": "were to have Lux encrypt the SD one volume you're adding you know Lux encryption over EBS encryption plus the",
    "start": "2716180",
    "end": "2723230"
  },
  {
    "text": "HDI HDFS encryption on itself and that's kind of a low performing recipe so",
    "start": "2723230",
    "end": "2735610"
  },
  {
    "start": "2734000",
    "end": "2734000"
  },
  {
    "text": "here's an example of a script that I use to just GPT partition and align these",
    "start": "2735610",
    "end": "2742730"
  },
  {
    "text": "large GPS drives so this is an excerpt from a larger script there's no warranty",
    "start": "2742730",
    "end": "2747950"
  },
  {
    "text": "on it so use it as you may the important point here is that when you're deploying",
    "start": "2747950",
    "end": "2755060"
  },
  {
    "text": "htif a cluster you know you can specify the location of HDFS yarn impala just",
    "start": "2755060",
    "end": "2764630"
  },
  {
    "text": "don't put them in under the same like directory like put them under usually",
    "start": "2764630",
    "end": "2769730"
  },
  {
    "text": "it's under HDFS and and and here the nomenclature is one for the first st",
    "start": "2769730",
    "end": "2775130"
  },
  {
    "text": "drive to four and then we have the DFS folder for HDFS yarn for yarn data",
    "start": "2775130",
    "end": "2781340"
  },
  {
    "text": "Impala if you were to put that yarn impala data in DFS when you reboot the",
    "start": "2781340",
    "end": "2786500"
  },
  {
    "text": "system like first of all you're gonna have to change the parameter Shinzon or for yarn to react because it's owned by HDFS and that's this kind",
    "start": "2786500",
    "end": "2793820"
  },
  {
    "text": "of a security violation and then when you reboot the system it's going to change back because the cloud era manager agent keeps all that in sync and",
    "start": "2793820",
    "end": "2801230"
  },
  {
    "text": "you're just all of a sudden have HDFS data but not be able to run jobs and whatnot and of course anything over two",
    "start": "2801230",
    "end": "2809000"
  },
  {
    "text": "terabytes has to be GPT partitioned align it on the 2048 byte",
    "start": "2809000",
    "end": "2815180"
  },
  {
    "text": "okay so improperly so these are virtual drives if you're not aligning the the",
    "start": "2815180",
    "end": "2824000"
  },
  {
    "text": "volumes then you'll have lower disk performance and with this script I just",
    "start": "2824000",
    "end": "2829820"
  },
  {
    "text": "automatically detect and partition and",
    "start": "2829820",
    "end": "2835000"
  },
  {
    "text": "set the FS tab just automatically this should be run as user data and this",
    "start": "2835000",
    "end": "2841220"
  },
  {
    "text": "could be pulled as part of a cloud init script in fact I actually configure everything well as much as I can by",
    "start": "2841220",
    "end": "2848240"
  },
  {
    "text": "convention because it's just easier that way what do I mean by that for instance we have one HDFS tab that governs all",
    "start": "2848240",
    "end": "2856040"
  },
  {
    "text": "these instances whether it's an R Co server or a worker node or a master node",
    "start": "2856040",
    "end": "2861620"
  },
  {
    "text": "or a SAS foundation server or Gateway and it's just the way I label it that",
    "start": "2861620",
    "end": "2867830"
  },
  {
    "text": "you can sort of realize and we'll get into that a bit so putting it all",
    "start": "2867830",
    "end": "2873290"
  },
  {
    "text": "together I have a sort of a it's non-scientific as there's no it was hard",
    "start": "2873290",
    "end": "2879590"
  },
  {
    "text": "to get the data but we have an old cluster it runs RHEL six you know there's no hybrid SC one GPT layout",
    "start": "2879590",
    "end": "2886040"
  },
  {
    "text": "there's no kernel see se tuning you still use the CMS garbage collector and",
    "start": "2886040",
    "end": "2891620"
  },
  {
    "text": "then we have a more modern Red Hat seven so you need a 310 kernel or higher we",
    "start": "2891620",
    "end": "2899480"
  },
  {
    "text": "use a hybrid SC 1 GP 2 layout we've got CC tuning we used to you once you see",
    "start": "2899480",
    "end": "2905240"
  },
  {
    "text": "and you know even though actually I'd left this up but system be actually a",
    "start": "2905240",
    "end": "2910580"
  },
  {
    "text": "hat is a smaller instance type but that's not really all that important because like I said our 4/4 xl's 8xl 16x",
    "start": "2910580",
    "end": "2917720"
  },
  {
    "text": "else they scale linearly as long as you when you run the jobs you keep the number of executors the same should run",
    "start": "2917720",
    "end": "2922870"
  },
  {
    "text": "about the same networking is a bit slower though the system B actually runs two to three times faster and garbage",
    "start": "2922870",
    "end": "2928450"
  },
  {
    "text": "collection occurs much less so I'm gonna",
    "start": "2928450",
    "end": "2935080"
  },
  {
    "text": "leave you with just a simple well one of",
    "start": "2935080",
    "end": "2940240"
  },
  {
    "text": "the major things that that we have to do is to expand the cluster now this",
    "start": "2940240",
    "end": "2946390"
  },
  {
    "text": "cluster is curve rise it's got TLS one two and if you are",
    "start": "2946390",
    "end": "2952150"
  },
  {
    "text": "familiar with cloud era doop you know adding nodes you know TLS one two cluster where both the clutter manager",
    "start": "2952150",
    "end": "2959080"
  },
  {
    "text": "and the agents are checking certs requires you to sort of disable TLS one two that's not that's not gonna happen",
    "start": "2959080",
    "end": "2965230"
  },
  {
    "text": "we can't do that and we can't reboot cm because then",
    "start": "2965230",
    "end": "2971260"
  },
  {
    "text": "we'll cut out on monitoring and in our environment because we're still",
    "start": "2971260",
    "end": "2977770"
  },
  {
    "text": "transitioning into the cloud we don't use route 53 or any of the directory services and they give us so",
    "start": "2977770",
    "end": "2984070"
  },
  {
    "text": "all these nodes all this carburization occurs via you know ad clusters on Prem",
    "start": "2984070",
    "end": "2989730"
  },
  {
    "start": "2989000",
    "end": "2989000"
  },
  {
    "text": "so what we do is we pre can pool all the nodes and we get them all set up so that",
    "start": "2989730",
    "end": "2995620"
  },
  {
    "text": "they are kerberized and and joined and",
    "start": "2995620",
    "end": "3001800"
  },
  {
    "text": "whatnot you pre generate the SSL certs so we're using a sort of a private it's",
    "start": "3001800",
    "end": "3009090"
  },
  {
    "text": "it's a private hybrid cloud system so you can pre generate the key stores and",
    "start": "3009090",
    "end": "3014730"
  },
  {
    "text": "the search for every single server even if they don't exist and then load up the java key stores so that cloud rare",
    "start": "3014730",
    "end": "3021540"
  },
  {
    "text": "manager wanted when it boots will have all these certs in memory we use",
    "start": "3021540",
    "end": "3027860"
  },
  {
    "text": "convention to name the certs there's no specific like host name that's in any of",
    "start": "3027860",
    "end": "3033720"
  },
  {
    "text": "these conflicts and we put the config file in s3 as part of our init process",
    "start": "3033720",
    "end": "3041660"
  },
  {
    "text": "so if you're familiar with cloud era Hadoop there is an agent running and the",
    "start": "3041660",
    "end": "3048450"
  },
  {
    "text": "agent uses a config file under Etsy cloud or SEM agent called config in E",
    "start": "3048450",
    "end": "3054150"
  },
  {
    "text": "and in it in order to have a you kill us cluster there are these four",
    "start": "3054150",
    "end": "3059869"
  },
  {
    "text": "parameters that need to be specified one is the private server key public server",
    "start": "3059869",
    "end": "3065339"
  },
  {
    "text": "cert use TLS equals one and because we are you know security conscious we don't",
    "start": "3065339",
    "end": "3070920"
  },
  {
    "text": "generate server certs that are password with us so cloud era manager cloud your agent has ability to use a command like",
    "start": "3070920",
    "end": "3077700"
  },
  {
    "text": "Kat to read the password for that particular server key and that key that",
    "start": "3077700",
    "end": "3084299"
  },
  {
    "text": "password file sits on our servers its root readable only so basically if you you're on the server and your root",
    "start": "3084299",
    "end": "3090660"
  },
  {
    "text": "you're pretty much on the box you know so it's you know change POSIX questions 400 so you can use this command and",
    "start": "3090660",
    "end": "3098039"
  },
  {
    "text": "because the agent runs as root you can you can cat the password in order to initialize the agent or started and so",
    "start": "3098039",
    "end": "3107819"
  },
  {
    "text": "what we do is we just we use user data or CloudFormation to install a date the",
    "start": "3107819",
    "end": "3113490"
  },
  {
    "text": "Damons or agents RPM and what this does so the RPM for from cloud error actually over writes that config me instead of",
    "start": "3113490",
    "end": "3120450"
  },
  {
    "text": "saving the new config as a RPM - new so",
    "start": "3120450",
    "end": "3126930"
  },
  {
    "text": "that's kind of an issue so what we do is after that is we pull down the config ami from s3 and then we just restart the",
    "start": "3126930",
    "end": "3133559"
  },
  {
    "text": "cloud era SCM agent and what that does is immediately the agent communicates with cloud era manager and it's already",
    "start": "3133559",
    "end": "3140579"
  },
  {
    "text": "become a managed box and then what you can do is you can use the CM resi API or",
    "start": "3140579",
    "end": "3146640"
  },
  {
    "text": "go directly to the GUI and apply a config role a config role is just",
    "start": "3146640",
    "end": "3151799"
  },
  {
    "text": "basically as a set of services that you've you know defined as the Gateway the work or whatnot and that initializes",
    "start": "3151799",
    "end": "3158160"
  },
  {
    "text": "the the node and so it works for us it",
    "start": "3158160",
    "end": "3166170"
  },
  {
    "start": "3163000",
    "end": "3163000"
  },
  {
    "text": "works really well we were able to expand a cluster from my 20 nodes to dozens and",
    "start": "3166170",
    "end": "3172109"
  },
  {
    "text": "dozens within 24 hours you know sort of like an enterprise environment like if we were able to do this with you know ad",
    "start": "3172109",
    "end": "3180539"
  },
  {
    "text": "like a tus ad or route 53 it would be",
    "start": "3180539",
    "end": "3186010"
  },
  {
    "text": "you know much better it would be faster",
    "start": "3186010",
    "end": "3191760"
  },
  {
    "text": "but we have to work with what we have so in summary I think the take-home",
    "start": "3191760",
    "end": "3198100"
  },
  {
    "text": "points are we really have to understand the workload and match its storage",
    "start": "3198100",
    "end": "3203410"
  },
  {
    "text": "performance pricing you know for HDFS you know had we used GP - we didn't paid",
    "start": "3203410",
    "end": "3210160"
  },
  {
    "text": "eighty thousand per month instead of thirty six based upon the numbers I showed in the beginning had we not had",
    "start": "3210160",
    "end": "3217000"
  },
  {
    "text": "we use all se ones instead of s T so s3",
    "start": "3217000",
    "end": "3222040"
  },
  {
    "text": "I didn't talk too much about it but it is sort of a backbone for a lot of our",
    "start": "3222040",
    "end": "3228160"
  },
  {
    "text": "data noe so we keep a lot of the write once read many time data in s3 and that",
    "start": "3228160",
    "end": "3234760"
  },
  {
    "text": "saves us you know extra twenty five thousand a month we use optimal you see two types we turn our ami or use if you",
    "start": "3234760",
    "end": "3247240"
  },
  {
    "text": "can the AWS ami we use s3 to share and distribute the data and if you keep up",
    "start": "3247240",
    "end": "3254859"
  },
  {
    "text": "with CDH you'll get the benefits of you know the improvement things like fast",
    "start": "3254859",
    "end": "3261400"
  },
  {
    "text": "upload for s3s regard who provides the DynamoDB back consistency check for",
    "start": "3261400",
    "end": "3268830"
  },
  {
    "text": "actually enabling transactional workloads on s3 via cloud era if like I",
    "start": "3268830",
    "end": "3275109"
  },
  {
    "text": "said you need to be on CDH 511 and above for that and thank you all for attending",
    "start": "3275109",
    "end": "3281440"
  },
  {
    "text": "I hope this was helpful",
    "start": "3281440",
    "end": "3285420"
  },
  {
    "text": "so if you would like to learn more about EBS we have a number of EBS sessions",
    "start": "3289350",
    "end": "3295660"
  },
  {
    "start": "3291000",
    "end": "3291000"
  },
  {
    "text": "this week all different types there are choc talks and deep dives so there's",
    "start": "3295660",
    "end": "3301720"
  },
  {
    "text": "just a sample here also in available in the app to find the ones that may",
    "start": "3301720",
    "end": "3306940"
  },
  {
    "text": "interest you on leveraging EBS so with that I would like to thank Alex and",
    "start": "3306940",
    "end": "3313930"
  },
  {
    "text": "Experian for the time and sharing their story with us very interesting how they leverage",
    "start": "3313930",
    "end": "3319980"
  },
  {
    "text": "different types of EBS storage and how they built in their clusters and how they get the right performance and cost",
    "start": "3319980",
    "end": "3326830"
  },
  {
    "text": "benefits out of that so thank you everybody really appreciate your time and hope you enjoy the session",
    "start": "3326830",
    "end": "3332920"
  },
  {
    "text": "[Applause]",
    "start": "3332920",
    "end": "3336260"
  }
]