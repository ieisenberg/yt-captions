[
  {
    "start": "0",
    "end": "98000"
  },
  {
    "text": "hey everybody happy Thursday thanks so much for joining us our stage maker",
    "start": "3080",
    "end": "8160"
  },
  {
    "text": "webinar today is going to cover deep AR forecasting and I know that Chris Granick is fired up to get started",
    "start": "8160",
    "end": "14459"
  },
  {
    "text": "I am Tsui Wiggin I'm the AWS global partner marketing manager for AI and machine learning you guys in the",
    "start": "14459",
    "end": "21390"
  },
  {
    "text": "audience everybody attending the webinar belongs to one of the twelve consulting",
    "start": "21390",
    "end": "26400"
  },
  {
    "text": "partners who have achieved the AWS machine learning competencies so thank you all for being here this is a private",
    "start": "26400",
    "end": "33300"
  },
  {
    "text": "webinar for you guys and we want to make sure that you can ask any and all questions that you have so feel free to",
    "start": "33300",
    "end": "39000"
  },
  {
    "text": "pop those into the there's a right nav that's part of the go-to webinar",
    "start": "39000",
    "end": "44430"
  },
  {
    "text": "platform and you should see a questions section if you don't see questions feel free to pop those in chat Chris burns",
    "start": "44430",
    "end": "52110"
  },
  {
    "text": "Chris with the C burns is going to be kind of monitoring that section and and trying to get those questions answered",
    "start": "52110",
    "end": "58230"
  },
  {
    "text": "while Chris with a case Granick presents for you today so Chris Granick is a",
    "start": "58230",
    "end": "64378"
  },
  {
    "text": "partner solution architect for AWS he leads the machine learning segment globally and Chris Burns is also a PSA",
    "start": "64379",
    "end": "71130"
  },
  {
    "text": "for AWS who's focused on AI and machine learning so let's jump into this really cool topic and let Chris take it away",
    "start": "71130",
    "end": "80270"
  },
  {
    "text": "well hello everybody I'm speaking to you from New York City communication capital of the world so",
    "start": "80270",
    "end": "87479"
  },
  {
    "text": "this series provides our partner network with an in-depth look at the built-in algorithms of sage maker and so I'm not",
    "start": "87479",
    "end": "96719"
  },
  {
    "text": "late I'd like to first talk about how do we choose the algorithms that we're",
    "start": "96719",
    "end": "103170"
  },
  {
    "start": "98000",
    "end": "98000"
  },
  {
    "text": "going to build in of course there's a lot of choices right the open source community we have scikit-learn Syfy",
    "start": "103170",
    "end": "111799"
  },
  {
    "text": "numpy you can build stuff of course then there's all the frameworks the frameworks such as tensorflow MMX net PI",
    "start": "111799",
    "end": "119369"
  },
  {
    "text": "torch they all come with algorithms so how do we choose the ones who that we're going to build and present first of all",
    "start": "119369",
    "end": "127079"
  },
  {
    "text": "they should be popular they should be algorithms that are in demand and we see them everyday in our workplace or that",
    "start": "127079",
    "end": "133980"
  },
  {
    "text": "we use them here at and we seek to provide 10 times or better performance on each of these algorithms",
    "start": "133980",
    "end": "141160"
  },
  {
    "text": "so you know how do we do that because that's a pretty high bar first of all we",
    "start": "141160",
    "end": "146470"
  },
  {
    "text": "stream whenever possible so basically we can use almost anything that uses to",
    "start": "146470",
    "end": "152020"
  },
  {
    "text": "cast a gradient descent we seek to create algorithms that want to see data",
    "start": "152020",
    "end": "158260"
  },
  {
    "text": "once and once only and of course that means we're going to have to create a shared state especially when we branch",
    "start": "158260",
    "end": "164980"
  },
  {
    "text": "out our training to clusters so we have built an infrastructure of shared state",
    "start": "164980",
    "end": "170200"
  },
  {
    "text": "that enables us to do that we seek to use GPUs whenever possible now a few of",
    "start": "170200",
    "end": "175900"
  },
  {
    "text": "the algorithm algorithms don't k-means Lda and XG boost notably or CPU only but",
    "start": "175900",
    "end": "183100"
  },
  {
    "text": "wherever possible we we seek to use our GPUs especially the powerful Voltas on",
    "start": "183100",
    "end": "189280"
  },
  {
    "text": "the p3 class we seek to create algorithms that are distributed that run",
    "start": "189280",
    "end": "195130"
  },
  {
    "text": "efficiently in a distributed environment so this of course is a challenge that we",
    "start": "195130",
    "end": "200310"
  },
  {
    "text": "overcame when we created this shared state mechanism so in training you know",
    "start": "200310",
    "end": "207580"
  },
  {
    "text": "we have a set of parameters and statistics that are generated on the fly and they can be shared we have some some",
    "start": "207580",
    "end": "218500"
  },
  {
    "text": "techniques for sharing and generating these on the fly and you can create many",
    "start": "218500",
    "end": "223900"
  },
  {
    "text": "models while you're actually doing the training we've sort of solved the",
    "start": "223900",
    "end": "229720"
  },
  {
    "text": "situation in the past where if you wanted to optimize your model you had to do hyper parameter optimization and run",
    "start": "229720",
    "end": "235959"
  },
  {
    "text": "the same job with different parameters so this is really a breakthrough of sage",
    "start": "235959",
    "end": "240970"
  },
  {
    "text": "maker and use it in we seek to use it in all of our built-in models abstraction",
    "start": "240970",
    "end": "248560"
  },
  {
    "text": "and containerization is another key principle as you use sage maker more and more you're going to discover that we",
    "start": "248560",
    "end": "256720"
  },
  {
    "text": "are built on a foundation of containers there are four ways to use sage maker",
    "start": "256720",
    "end": "263100"
  },
  {
    "text": "through the console through a command-line interface SPARC and of course with",
    "start": "263100",
    "end": "268880"
  },
  {
    "text": "notebooks that we're gonna look at today last but not least the least we seek to",
    "start": "268880",
    "end": "275140"
  },
  {
    "text": "efficiently access our data from s3 so when you're doing any kind of training",
    "start": "275140",
    "end": "281960"
  },
  {
    "text": "and most work in sage maker it's best to do your planning and use the facilities",
    "start": "281960",
    "end": "288020"
  },
  {
    "text": "that s3 implements so those are some key principles it's something we really",
    "start": "288020",
    "end": "293300"
  },
  {
    "text": "don't address too frequently but we have a product manager here that really just",
    "start": "293300",
    "end": "299600"
  },
  {
    "text": "focuses on nothing but what are the appropriate algorithms and that of course is the the subject of this series",
    "start": "299600",
    "end": "306650"
  },
  {
    "text": "um you know last but not least We strongly favor record i/o protobuf for our file format there's a couple",
    "start": "306650",
    "end": "314690"
  },
  {
    "text": "reasons for that when you're storing images for example JPEG records you greatly reduce the size of the data set",
    "start": "314690",
    "end": "321920"
  },
  {
    "text": "on disk and packing data together allows for continuous reading which in improves",
    "start": "321920",
    "end": "327650"
  },
  {
    "text": "performance and record i/o is a simple way to partition and simplify a",
    "start": "327650",
    "end": "333590"
  },
  {
    "text": "distributed setting all right so timeseriesforecasting you know it's very popular almost every",
    "start": "333590",
    "end": "340010"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "company can use it the some of the most popular use cases are improving supply",
    "start": "340010",
    "end": "346880"
  },
  {
    "text": "chain efficiencies by doing demand prediction avoiding outages allocating",
    "start": "346880",
    "end": "353660"
  },
  {
    "text": "computing resources more effectively by predicting webserver traffic even saving",
    "start": "353660",
    "end": "359480"
  },
  {
    "text": "lives by forecasting patient visits and staffing hospitals to meet patient needs",
    "start": "359480",
    "end": "364600"
  },
  {
    "text": "any kind of prediction for urban planning housing prices traffic in",
    "start": "364600",
    "end": "370520"
  },
  {
    "text": "cities these are pretty much you know many of the common use cases for time",
    "start": "370520",
    "end": "376130"
  },
  {
    "text": "series within Amazon we use forecasting to predict product and labor labor",
    "start": "376130",
    "end": "383510"
  },
  {
    "text": "demand in our fulfillment centers in particular for key dates like prime day Black Friday Cyber Monday are making",
    "start": "383510",
    "end": "390980"
  },
  {
    "text": "sure that we can elastically scale AWS computing and storage capacity for all of our",
    "start": "390980",
    "end": "396590"
  },
  {
    "text": "customers now there are numerous statistical methods that have been developed to",
    "start": "396590",
    "end": "402169"
  },
  {
    "text": "forecast time-series data but still the process for developing forecasts tends to be a mix of objective statistics and",
    "start": "402169",
    "end": "410060"
  },
  {
    "text": "subjective interpretations properly modeling time series data takes a great",
    "start": "410060",
    "end": "415520"
  },
  {
    "text": "deal of care what's the right level for example to aggregate the model at if you're too",
    "start": "415520",
    "end": "421490"
  },
  {
    "text": "granular the signal would get lost in the noise and if you're to aggregate important variations are missed also",
    "start": "421490",
    "end": "428870"
  },
  {
    "text": "what's the rights acclivity daily weekly monthly you know are there holiday peaks",
    "start": "428870",
    "end": "433999"
  },
  {
    "text": "how do we weigh this versus the overall trends linear regression with appropriate controls for trend",
    "start": "433999",
    "end": "440210"
  },
  {
    "text": "seasonality and recent behavior remains a common method for forecasting but we",
    "start": "440210",
    "end": "446629"
  },
  {
    "text": "have now entered the era of deep neural networks and if we have multiple time",
    "start": "446629",
    "end": "452569"
  },
  {
    "text": "related related time series we would want to use sage makers deep AR",
    "start": "452569",
    "end": "457849"
  },
  {
    "text": "algorithm which is specifically designed for forecasting so just a few",
    "start": "457849",
    "end": "464210"
  },
  {
    "start": "462000",
    "end": "462000"
  },
  {
    "text": "definitions recurrent neural network this that's a new term for you is a type",
    "start": "464210",
    "end": "469580"
  },
  {
    "text": "of deep learning neural network where connections between nodes form a directed graph along a sequence this",
    "start": "469580",
    "end": "476810"
  },
  {
    "text": "allows them to exhibit temporal dynamic behavior for a time sequence so unlike",
    "start": "476810",
    "end": "482930"
  },
  {
    "text": "feed-forward neural networks RNs can use their internal state to process sequences of inputs an important type of",
    "start": "482930",
    "end": "491599"
  },
  {
    "text": "ayran M is an LST M which stands for a horrible combination of words long",
    "start": "491599",
    "end": "498110"
  },
  {
    "text": "short-term memory but in Ellis TM cell can process data sequentially and keep its state hidden through time a common",
    "start": "498110",
    "end": "505159"
  },
  {
    "text": "Ellis TN unit is composed of a cell an input and output and a forget gate the",
    "start": "505159",
    "end": "511969"
  },
  {
    "text": "cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into",
    "start": "511969",
    "end": "518448"
  },
  {
    "text": "and out of the cell when predicting for multiple data points one step ahead forecasts update the",
    "start": "518449",
    "end": "525680"
  },
  {
    "text": "history to the with the correct known value these are common and easy to",
    "start": "525680",
    "end": "531079"
  },
  {
    "text": "produce and can give us some intuition of whether our model performing as expected however they can",
    "start": "531079",
    "end": "538370"
  },
  {
    "text": "also present an unnecessarily optimistic evaluation of the forecast in horizon",
    "start": "538370",
    "end": "544520"
  },
  {
    "text": "Horizon models when forecasting out-of-sample each forecast builds off",
    "start": "544520",
    "end": "549560"
  },
  {
    "text": "of the forecasted periods that precede it so errors early on in the test data",
    "start": "549560",
    "end": "554900"
  },
  {
    "text": "can't compound to create large deviations for observations late in the test data although this is more",
    "start": "554900",
    "end": "561200"
  },
  {
    "text": "realistic it can be difficult to create the forecasts particularly as the model",
    "start": "561200",
    "end": "567470"
  },
  {
    "text": "complexity increases a cold start scenario occurs when we want to generate",
    "start": "567470",
    "end": "572779"
  },
  {
    "text": "a forecast for a time series with little or no historical data this occurs",
    "start": "572779",
    "end": "578690"
  },
  {
    "text": "frequently in practice such as in scenarios where new products are introduced or a new AWS region is",
    "start": "578690",
    "end": "584720"
  },
  {
    "text": "launched traditional methods here such as ARIMA relies solely on the historical",
    "start": "584720",
    "end": "591320"
  },
  {
    "text": "data of an individual time series and as such they are typically less accurate in",
    "start": "591320",
    "end": "596510"
  },
  {
    "text": "the cold start case now probabilistic forecasts are used in deep AR and they",
    "start": "596510",
    "end": "603440"
  },
  {
    "text": "produce both point forecasts and probably and probabilistic forecasts so",
    "start": "603440",
    "end": "608630"
  },
  {
    "text": "for example the amount of sneakers sold in a week is between x and y with ze",
    "start": "608630",
    "end": "614210"
  },
  {
    "text": "probability the later forecasts are particularly well-suited for business",
    "start": "614210",
    "end": "619220"
  },
  {
    "text": "applications such as capacity planning where specific forecast quantities are",
    "start": "619220",
    "end": "624440"
  },
  {
    "text": "more important than the most likely outcome so I want to cover another",
    "start": "624440",
    "end": "631760"
  },
  {
    "text": "really important topic which is what kind of data can you use alright so",
    "start": "631760",
    "end": "637940"
  },
  {
    "text": "there's a notion if you're not coming to this from data science data science and I know most of you aren't most are",
    "start": "637940",
    "end": "643010"
  },
  {
    "text": "coming from development you may not know this term of a stationary series so I'd",
    "start": "643010",
    "end": "648110"
  },
  {
    "text": "like to make sure everyone is really clear on what this is so the true",
    "start": "648110",
    "end": "653330"
  },
  {
    "text": "definition is it's a stochastic process where the conditional joint probability distribution oops well you can read it I",
    "start": "653330",
    "end": "667040"
  },
  {
    "text": "won't read it for you but and I don't want to get to any of the terms wrong but basically you can see on",
    "start": "667040",
    "end": "672590"
  },
  {
    "text": "the Left we have essentially a modulation within a fixed range and on",
    "start": "672590",
    "end": "678800"
  },
  {
    "text": "the right we have an upward trend the stationary series is characterized by",
    "start": "678800",
    "end": "684350"
  },
  {
    "text": "being fixed within that modulated range",
    "start": "684350",
    "end": "688509"
  },
  {
    "start": "690000",
    "end": "690000"
  },
  {
    "text": "can change the coil there we go so that's one definition or one pretty",
    "start": "690130",
    "end": "696890"
  },
  {
    "text": "clear sign of the stationary C series the other you can see on the left here",
    "start": "696890",
    "end": "702650"
  },
  {
    "text": "we have the plain sine wave as opposed to one that's varying greatly and one",
    "start": "702650",
    "end": "708170"
  },
  {
    "text": "that varies in in cadence or periodicity",
    "start": "708170",
    "end": "714430"
  },
  {
    "text": "I had hoc meeting seems to behind okay",
    "start": "718600",
    "end": "724310"
  },
  {
    "text": "so these are important concepts and one of the most common misconceptions is",
    "start": "724310",
    "end": "731210"
  },
  {
    "start": "726000",
    "end": "726000"
  },
  {
    "text": "that we can just go straight into timeseriesforecasting and start predicting stock prices I mean why don't",
    "start": "731210",
    "end": "738410"
  },
  {
    "text": "we all just quit our day job and do this right at the end of the seminar well here's the sp500 I just picked it",
    "start": "738410",
    "end": "745760"
  },
  {
    "text": "up this morning given as a line series with a little bit of data there's volume data on the bottom and I would just ask",
    "start": "745760",
    "end": "752960"
  },
  {
    "text": "you to say to yourself right now is is this a stationary series well of course",
    "start": "752960",
    "end": "759950"
  },
  {
    "start": "759000",
    "end": "759000"
  },
  {
    "text": "it's not so how do you standardize this well the way we do that typically is",
    "start": "759950",
    "end": "766880"
  },
  {
    "text": "through technical analysis and I'm just going to annotate my screen here just a",
    "start": "766880",
    "end": "773960"
  },
  {
    "text": "little bit to point out a few things that I learned working on Wall Street of",
    "start": "773960",
    "end": "780080"
  },
  {
    "text": "where you can actually begin this kind of analysis so you see here we have two",
    "start": "780080",
    "end": "787780"
  },
  {
    "text": "indicators that were drawn right into the chart of prices of the chart",
    "start": "787780",
    "end": "794480"
  },
  {
    "text": "themselves so when we have an exponential moving average at 13 days a",
    "start": "794480",
    "end": "799580"
  },
  {
    "text": "period of 13 and an e/m a of 50 so crossovers in that and the",
    "start": "799580",
    "end": "804860"
  },
  {
    "text": "Delta between these are an indicator of trim they're an indicator of the",
    "start": "804860",
    "end": "810019"
  },
  {
    "text": "strength of an up train this strength of a down trend down below we have an even",
    "start": "810019",
    "end": "815839"
  },
  {
    "text": "more powerful indicator it's called MACD which is moving average Convergence divergence which will combine these",
    "start": "815839",
    "end": "823790"
  },
  {
    "text": "exponential moving averages and create a histogram and the dista gram is is indicated by the blue bars here so now",
    "start": "823790",
    "end": "831589"
  },
  {
    "text": "we actually have gotten this into a stationary series so these are really",
    "start": "831589",
    "end": "836899"
  },
  {
    "text": "powerful trend indicators and in fact you can see where we have a divergence",
    "start": "836899",
    "end": "841910"
  },
  {
    "text": "where we cross at the center line on the bottom here we actually have what looks like a buying opportunity we have",
    "start": "841910",
    "end": "849379"
  },
  {
    "text": "another one here you could look at that as a reinforcement signal and so on and so these are techniques on Wall Street",
    "start": "849379",
    "end": "857000"
  },
  {
    "text": "to steer stationary eyes our data now I'll just add one more data point here",
    "start": "857000",
    "end": "864620"
  },
  {
    "text": "which is that stock prices on Wall Street move through human behavior it's not like capturing you know the physics",
    "start": "864620",
    "end": "872509"
  },
  {
    "text": "of water flow or something and that's what this chart on the top is really indicating but this chart on the top is",
    "start": "872509",
    "end": "879829"
  },
  {
    "text": "indicating is buying pressure so as prices we average out where prices close",
    "start": "879829",
    "end": "885110"
  },
  {
    "text": "at the end of the day and so right here we would indicate that there was a lot of bullish sentiment and the Bulls are",
    "start": "885110",
    "end": "891709"
  },
  {
    "text": "getting tired so maybe you're into a bearish mode here now it's the Bears",
    "start": "891709",
    "end": "898250"
  },
  {
    "text": "have gotten exhausted so you know and once again this is a stationary eyes",
    "start": "898250",
    "end": "904130"
  },
  {
    "text": "data and you can begin to use it in your analysis with some of the techniques",
    "start": "904130",
    "end": "910189"
  },
  {
    "text": "that we're about to cover right now all right so let's take a look at some",
    "start": "910189",
    "end": "916309"
  },
  {
    "text": "traditional forecasting methods you know pop over to first of all stage maker I",
    "start": "916309",
    "end": "923059"
  },
  {
    "text": "have an instance running here that is running first of all a linear time",
    "start": "923059",
    "end": "928430"
  },
  {
    "text": "series now as you may know stage maker comes with a number of built in examples",
    "start": "928430",
    "end": "934360"
  },
  {
    "text": "when you open up a sage maker Jupiter notebook there's a tab that says sage",
    "start": "934360",
    "end": "939760"
  },
  {
    "text": "maitreya examples and that's what I'm using here I'm using a time series forecaster using the built in algorithm",
    "start": "939760",
    "end": "946120"
  },
  {
    "text": "linear learner which Chris Burns covered last week so let's just walk through this briefly first of all we have to",
    "start": "946120",
    "end": "954040"
  },
  {
    "text": "create our own bucket to begin to store the data that we're gonna download for",
    "start": "954040",
    "end": "959500"
  },
  {
    "text": "this particular instance we have to setup sage maker and get our iam role to give us the permissions that we need to",
    "start": "959500",
    "end": "967029"
  },
  {
    "text": "run this data I should cover what our objective here is so in this particular",
    "start": "967029",
    "end": "972220"
  },
  {
    "text": "model we're going to be downloading its",
    "start": "972220",
    "end": "977829"
  },
  {
    "text": "gasoline prices where is the data set yeah gasoline prices and looking at",
    "start": "977829",
    "end": "984190"
  },
  {
    "text": "trends and seeing if we can begin to predict them using a linear learner",
    "start": "984190",
    "end": "989200"
  },
  {
    "text": "model now you can run this notebook yourself and you can dig in and take",
    "start": "989200",
    "end": "994480"
  },
  {
    "text": "your time with this and that's why I like to choose sort of our built in notebooks whenever possible",
    "start": "994480",
    "end": "1000660"
  },
  {
    "text": "so the first thing we do is download this data set it's a publicly accessible data set the first step in any machine",
    "start": "1000660",
    "end": "1008430"
  },
  {
    "text": "learning process is to begin to understand your data and the best way to begin to understand your data is to",
    "start": "1008430",
    "end": "1014040"
  },
  {
    "text": "visualize it now if you are notebook doesn't look exactly the same as mine don't worry I've made a few",
    "start": "1014040",
    "end": "1020100"
  },
  {
    "text": "modifications in this notebook to make it read a little bit better for the demo but essentially this is what we're doing",
    "start": "1020100",
    "end": "1026730"
  },
  {
    "text": "here we're plotting these gas prices over time and actually I think this is",
    "start": "1026730",
    "end": "1032280"
  },
  {
    "text": "not prices but volume volume of consumption over time I see thousands of",
    "start": "1032280",
    "end": "1037980"
  },
  {
    "text": "barrels here in the data so the next step in the process is to break this",
    "start": "1037980",
    "end": "1043079"
  },
  {
    "text": "into some rap with some series of segments and that's what we're doing",
    "start": "1043079",
    "end": "1050130"
  },
  {
    "text": "right here in this so all we're doing is sort of breaking up the the data set so",
    "start": "1050130",
    "end": "1056940"
  },
  {
    "text": "that it's easier to just in terms of periodicity next we're going to take our",
    "start": "1056940",
    "end": "1062610"
  },
  {
    "text": "data set and divide it into a training a validation and a test set so",
    "start": "1062610",
    "end": "1068480"
  },
  {
    "text": "we have to take special care whenever you're taking a time-series data set when you're doing your training and",
    "start": "1068480",
    "end": "1074330"
  },
  {
    "text": "validation and testing because unlike many other data sets the order in which",
    "start": "1074330",
    "end": "1079760"
  },
  {
    "text": "the time series is tested is really important simply you want to use the end",
    "start": "1079760",
    "end": "1086360"
  },
  {
    "text": "of your data set for testing and validation and that makes sense now I mention the importance of converting our",
    "start": "1086360",
    "end": "1093320"
  },
  {
    "text": "data to record IO format that's what we're doing here and then we're loading that data up to s3 and then we do that",
    "start": "1093320",
    "end": "1101360"
  },
  {
    "text": "again with our test set and validation okay now we're going to train the model",
    "start": "1101360",
    "end": "1106400"
  },
  {
    "text": "so here we are getting the container as I mentioned stage makers built on this",
    "start": "1106400",
    "end": "1111410"
  },
  {
    "text": "foundation of containers and that's all we're doing here we have broken up our containers now they're a lot lighter so",
    "start": "1111410",
    "end": "1118640"
  },
  {
    "text": "that you get them by algorithm name that may be due to you if you've been working with stage maker for a while then we're",
    "start": "1118640",
    "end": "1124820"
  },
  {
    "text": "gonna set our hyper parameters we only have a few here first of all we're gonna create a number of separate models so",
    "start": "1124820",
    "end": "1132290"
  },
  {
    "text": "that we can optimize and that's the numata 'ls parameter here so in this",
    "start": "1132290",
    "end": "1137360"
  },
  {
    "text": "case we're taking the maximum at least the current maximum which happens to be 32 so when we run this training job",
    "start": "1137360",
    "end": "1144340"
  },
  {
    "text": "we're gonna use our job queue which is separate from the notebook and we're",
    "start": "1144340",
    "end": "1151730"
  },
  {
    "text": "gonna run this on a c4 X large and it's going to create 32 separate models and",
    "start": "1151730",
    "end": "1158179"
  },
  {
    "text": "do hyper parameter optimization for us and return to us the best performing",
    "start": "1158179",
    "end": "1164299"
  },
  {
    "text": "model using f1 so and other and other",
    "start": "1164299",
    "end": "1169540"
  },
  {
    "text": "measures alright so you know this is a pre-baked system I'm not going to run this right now",
    "start": "1169540",
    "end": "1175400"
  },
  {
    "text": "it takes a couple minutes but once that model is ready and we can see our models when we go back to the console you just",
    "start": "1175400",
    "end": "1181520"
  },
  {
    "text": "click on models and I guess it's down here the linear demo model we then",
    "start": "1181520",
    "end": "1190130"
  },
  {
    "text": "deploy that model to a production endpoint so next up we're going to do",
    "start": "1190130",
    "end": "1196460"
  },
  {
    "text": "some forecasting against our endpoint there's a couple of ways that we can judge the accuracy",
    "start": "1196460",
    "end": "1202490"
  },
  {
    "text": "our model by far the most common and is the root mean square error and that's",
    "start": "1202490",
    "end": "1208460"
  },
  {
    "text": "what we're going to be using here so as we go down below here and we begin to",
    "start": "1208460",
    "end": "1215920"
  },
  {
    "text": "plot our results first of all we have a",
    "start": "1215920",
    "end": "1221450"
  },
  {
    "text": "model in I'm sorry I said we were going to use root meet root mean square we're",
    "start": "1221450",
    "end": "1228770"
  },
  {
    "text": "actually using mean absolute percent error so in the first test we're using a",
    "start": "1228770",
    "end": "1236900"
  },
  {
    "text": "one step ahead fourth one step ahead forecast and we could see the accuracy",
    "start": "1236900",
    "end": "1243980"
  },
  {
    "text": "of this model is relatively good the next model are one step ahead model I'm",
    "start": "1243980",
    "end": "1249620"
  },
  {
    "text": "sorry I'm getting ahead of myself is actually a little bit better better and what I'm looking at here of course is",
    "start": "1249620",
    "end": "1255230"
  },
  {
    "text": "this indicator right here which gives us the actual accuracy here as point zero",
    "start": "1255230",
    "end": "1260900"
  },
  {
    "text": "three six here we've got 0.001 nine and then finally our last forecast is giving",
    "start": "1260900",
    "end": "1268670"
  },
  {
    "text": "us a point to one point zero to one so this particular example and I actually",
    "start": "1268670",
    "end": "1277280"
  },
  {
    "text": "do recommend using the the simplest models possible for your particular",
    "start": "1277280",
    "end": "1282920"
  },
  {
    "text": "problem is showing us that depending on the approach that we're taking to",
    "start": "1282920",
    "end": "1289000"
  },
  {
    "text": "actually do our projections we can get some some very different results and you",
    "start": "1289000",
    "end": "1294860"
  },
  {
    "text": "may not always need to use something as complex and robust don't be the right",
    "start": "1294860",
    "end": "1301820"
  },
  {
    "text": "word as a deep AR now deep AR does produce really impressive results when",
    "start": "1301820",
    "end": "1308300"
  },
  {
    "text": "you have a lot of data so that brings it",
    "start": "1308300",
    "end": "1313429"
  },
  {
    "start": "1310000",
    "end": "1310000"
  },
  {
    "text": "up the next point here which is when do we use deep neural networks for this this kind of situation well it begins",
    "start": "1313429",
    "end": "1321080"
  },
  {
    "text": "with the most important question which is what kind of problem are we solving so basically when you take a look at",
    "start": "1321080",
    "end": "1329240"
  },
  {
    "text": "very very big data so just the obvious example would be looking at",
    "start": "1329240",
    "end": "1335680"
  },
  {
    "text": "the number of items that accompany cells based off of the number of cells for those that sales for those items and you",
    "start": "1335680",
    "end": "1342310"
  },
  {
    "text": "can imagine Amazon with a catalogue like we have we have a lot of insight into this problem we could see that",
    "start": "1342310",
    "end": "1348790"
  },
  {
    "text": "magnitudes of time-series differ widely and that the distribution of those magnitudes are strongly skewed so some",
    "start": "1348790",
    "end": "1358090"
  },
  {
    "text": "time ago some folks started using a couple of deep learning methods to",
    "start": "1358090",
    "end": "1363820"
  },
  {
    "text": "create an architecture that would do a couple of things including incorporating",
    "start": "1363820",
    "end": "1369370"
  },
  {
    "text": "a negative binomial likelihood for count data and also special treatment for the",
    "start": "1369370",
    "end": "1375640"
  },
  {
    "text": "case where magnitudes vary widely all of this is spelled out in detail in the amazon white paper on deep AR so i gave",
    "start": "1375640",
    "end": "1385690"
  },
  {
    "start": "1381000",
    "end": "1381000"
  },
  {
    "text": "a big warning up front i'm sure everybody heard you can't just throw any data at this kind of problem first of",
    "start": "1385690",
    "end": "1392980"
  },
  {
    "text": "all it must be stationary so if you need to massage your data if you need to do some feature engineering up front that",
    "start": "1392980",
    "end": "1400560"
  },
  {
    "text": "may be the biggest part of the job you're going to want to put that into either gzip gzipped json lines format",
    "start": "1400560",
    "end": "1409930"
  },
  {
    "text": "file or we do also process Parkay so what is the contents of this file going",
    "start": "1409930",
    "end": "1415420"
  },
  {
    "text": "to look like before a gzipping of course first of all you're going to have a start with a string in the format of",
    "start": "1415420",
    "end": "1421360"
  },
  {
    "text": "year-month-day hours minutes seconds you're going to have a target this is going to be integers or floats that",
    "start": "1421360",
    "end": "1428410"
  },
  {
    "text": "represent your time series values and there's some optional features so one is",
    "start": "1428410",
    "end": "1434950"
  },
  {
    "text": "to dynamic feature assessment I'm going to discuss that hyper parameter just a little bit later and then second",
    "start": "1434950",
    "end": "1441550"
  },
  {
    "text": "categories one of the principal reasons to use deep AR is that you have a very",
    "start": "1441550",
    "end": "1448960"
  },
  {
    "text": "large set of categories that of categorical data that relate to each",
    "start": "1448960",
    "end": "1454480"
  },
  {
    "text": "other so imagine if you will you know you're selling a lot of different shoes you may have categories of sneakers",
    "start": "1454480",
    "end": "1460690"
  },
  {
    "text": "versus boots etc well you can begin to do prediction with these separate",
    "start": "1460690",
    "end": "1466990"
  },
  {
    "text": "categories with very little data or the data you have um one other question is what",
    "start": "1466990",
    "end": "1472780"
  },
  {
    "start": "1468000",
    "end": "1468000"
  },
  {
    "text": "is the AR stand for her in deep AR well it stands for auto regressive and auto",
    "start": "1472780",
    "end": "1478179"
  },
  {
    "text": "regressive has more meaning when you compare it to the moving average model now I certainly pointed to a number of",
    "start": "1478179",
    "end": "1483970"
  },
  {
    "text": "moving averages when I showed you that stock spring and I think you know what that is but to put it in very very",
    "start": "1483970",
    "end": "1490059"
  },
  {
    "text": "simple terms an auto regressive model builds into it the fact that in this",
    "start": "1490059",
    "end": "1496240"
  },
  {
    "text": "particular graph on the top here let's say you were selling popsicles and it",
    "start": "1496240",
    "end": "1502179"
  },
  {
    "text": "was really cold it was like thirty degrees and then all of a sudden you had a really really warm day well you know",
    "start": "1502179",
    "end": "1509470"
  },
  {
    "text": "ninety hundred degrees Fahrenheit of course if you're not in the u.s. so",
    "start": "1509470",
    "end": "1514570"
  },
  {
    "text": "popsicle sales just shoot through the roof right then the temperatures kind of go back to normal and there is this kind",
    "start": "1514570",
    "end": "1521770"
  },
  {
    "text": "of behavior where people buy fifty percent less than the day before so it kind of moves out now this can also",
    "start": "1521770",
    "end": "1528370"
  },
  {
    "text": "describe a situation where you run out of inventory on a product or many other kinds of spikey situations a moving",
    "start": "1528370",
    "end": "1536320"
  },
  {
    "text": "average model simply does not include that kind of regressive approach so what",
    "start": "1536320",
    "end": "1544419"
  },
  {
    "start": "1542000",
    "end": "1542000"
  },
  {
    "text": "are the advantages of deep AR well first of all there is minimal feature",
    "start": "1544419",
    "end": "1551860"
  },
  {
    "text": "engineering that's required to begin to use deep AR because of the fact that the",
    "start": "1551860",
    "end": "1560679"
  },
  {
    "text": "deep neural network models are doing some of that feature engineering for you",
    "start": "1560679",
    "end": "1567570"
  },
  {
    "text": "when if you take a look at these graphs they showcase sort of the forecasting",
    "start": "1567570",
    "end": "1575049"
  },
  {
    "text": "scenarios using example demand forecast produced by deep AR for products sold at",
    "start": "1575049",
    "end": "1581679"
  },
  {
    "text": "Amazon the first figure shows a cold start scenario since the model shares",
    "start": "1581679",
    "end": "1587260"
  },
  {
    "text": "information across items predictions are reasonable even within a limited with limited historical data the second and",
    "start": "1587260",
    "end": "1594280"
  },
  {
    "text": "third figures show that deep AR can produce probabilistic forecasts for products with different magnitudes by",
    "start": "1594280",
    "end": "1601240"
  },
  {
    "text": "using a probe likelihood functions for the setting which in this particular case happened",
    "start": "1601240",
    "end": "1606309"
  },
  {
    "text": "to be a negative binomial there's a lot",
    "start": "1606309",
    "end": "1611980"
  },
  {
    "start": "1609000",
    "end": "1609000"
  },
  {
    "text": "more advantages we provide support for different types of time series real",
    "start": "1611980",
    "end": "1618639"
  },
  {
    "text": "numbers scalars etc we provide a modicum attic evaluation of model accuracy it is",
    "start": "1618639",
    "end": "1626440"
  },
  {
    "text": "engineered to use either GP or a CPU Hardware to train the LS TM and and the",
    "start": "1626440",
    "end": "1634600"
  },
  {
    "text": "RN n quickly and flexibly it scales up two data sets comprising more than a",
    "start": "1634600",
    "end": "1640570"
  },
  {
    "text": "hundred thousand items in time series and there is support for training data",
    "start": "1640570",
    "end": "1646179"
  },
  {
    "text": "and JSON lines of parquet format also importantly we support missing values",
    "start": "1646179",
    "end": "1652440"
  },
  {
    "text": "categorical and time series features and generalized frequencies here's a quick",
    "start": "1652440",
    "end": "1660669"
  },
  {
    "start": "1657000",
    "end": "1657000"
  },
  {
    "text": "look at the architecture and I will not go into much depth here because",
    "start": "1660669",
    "end": "1665710"
  },
  {
    "text": "certainly the papers do but I do want to point out that the integration of both",
    "start": "1665710",
    "end": "1672700"
  },
  {
    "text": "the sequence to sequence algorithm and the LST M algorithm so in training we",
    "start": "1672700",
    "end": "1679899"
  },
  {
    "text": "see what looks like a typical LS TM on rather in a in encoding rather where",
    "start": "1679899",
    "end": "1690460"
  },
  {
    "text": "we're taking our values and encoding them up to samples based on a certain",
    "start": "1690460",
    "end": "1696700"
  },
  {
    "text": "function of likelihood then we're using an encoder decoder model to begin to use",
    "start": "1696700",
    "end": "1704080"
  },
  {
    "text": "our predictions so we can look ahead or look backwards actually with the LS TM",
    "start": "1704080",
    "end": "1711879"
  },
  {
    "text": "architecture in our prediction phase of the model and that's how we're able to",
    "start": "1711879",
    "end": "1717999"
  },
  {
    "text": "create more accuracy and also predict and also create the windows into which",
    "start": "1717999",
    "end": "1724840"
  },
  {
    "text": "its best to predict so once again I don't kind of want to get into the data",
    "start": "1724840",
    "end": "1729850"
  },
  {
    "text": "science piece of that this and I encourage you if you are a data scientist to look at the white paper",
    "start": "1729850",
    "end": "1736330"
  },
  {
    "text": "at archive.org but I did want to cover the integration of these two deep",
    "start": "1736330",
    "end": "1741730"
  },
  {
    "text": "learning algorithms so what are the results well once again if you have",
    "start": "1741730",
    "end": "1748210"
  },
  {
    "start": "1742000",
    "end": "1742000"
  },
  {
    "text": "chosen the right data set if it is principally a very large data set with a",
    "start": "1748210",
    "end": "1754269"
  },
  {
    "text": "large number of related time series data the results are exceptional first of all",
    "start": "1754269",
    "end": "1761620"
  },
  {
    "text": "you get all the advantages of running on essentially infinitely scalable hardware at AWS both in terms of CPU and s3",
    "start": "1761620",
    "end": "1770190"
  },
  {
    "text": "storage capability but also you we're beating essentially all the other",
    "start": "1770190",
    "end": "1775659"
  },
  {
    "text": "numbers that are typically predicted for this kind of timeseriesforecasting so I",
    "start": "1775659",
    "end": "1782440"
  },
  {
    "start": "1779000",
    "end": "1779000"
  },
  {
    "text": "do want to talk about the hyper parameters just a little bit first of",
    "start": "1782440",
    "end": "1787570"
  },
  {
    "text": "all it is obvious that you have to choose some sort of context length that",
    "start": "1787570",
    "end": "1793570"
  },
  {
    "text": "is going to give you the number of time points that the model gets to see before making a prediction the value of that",
    "start": "1793570",
    "end": "1801070"
  },
  {
    "text": "parameter should be about the same length as the prediction length and the prediction length is the number of time",
    "start": "1801070",
    "end": "1806380"
  },
  {
    "text": "steps that the model is trained to predict that's also called the forecast",
    "start": "1806380",
    "end": "1811389"
  },
  {
    "text": "horizon we want to take a look at the number of cells the number of layers the typical",
    "start": "1811389",
    "end": "1817840"
  },
  {
    "text": "ranges from 1 to 4 now the likely choosing a likelihood model I would",
    "start": "1817840",
    "end": "1823779"
  },
  {
    "text": "refer you to the documentation for certain but the choices here are",
    "start": "1823779",
    "end": "1829480"
  },
  {
    "text": "Gaussian beta student tau and deterministic l1 the default is student",
    "start": "1829480",
    "end": "1837010"
  },
  {
    "text": "tau so once again this is a really sort of an important but nuanced aspect of",
    "start": "1837010",
    "end": "1843909"
  },
  {
    "text": "training the model and I'll refer you to the documentation for more detail there",
    "start": "1843909",
    "end": "1849370"
  },
  {
    "text": "a number of epochs of course that's the number of passes that we go over the training data we always want to split",
    "start": "1849370",
    "end": "1855519"
  },
  {
    "text": "things into mini batches to speed up the the training process we need to choose a",
    "start": "1855519",
    "end": "1862750"
  },
  {
    "text": "training rate we also want to choose a bit of dropout for improved",
    "start": "1862750",
    "end": "1867820"
  },
  {
    "text": "regularization drop out if you're not familiar with that actually we'll go into the neural",
    "start": "1867820",
    "end": "1873240"
  },
  {
    "text": "network and randomly delete a few nodes and that actually helps improve",
    "start": "1873240",
    "end": "1878250"
  },
  {
    "text": "regularization and prevent overfitting last but not least is early stopping",
    "start": "1878250",
    "end": "1883350"
  },
  {
    "text": "patience if that parameter is set training stops when no progress is made",
    "start": "1883350",
    "end": "1888540"
  },
  {
    "text": "within a specified number of epochs all right let's take a look at this we have",
    "start": "1888540",
    "end": "1895560"
  },
  {
    "text": "two great notebooks that are built into Sage maker 4 for this purpose I'm gonna",
    "start": "1895560",
    "end": "1902340"
  },
  {
    "text": "switch first to the notebook that covers and trying to increase the size here",
    "start": "1902340",
    "end": "1908250"
  },
  {
    "text": "there we go that uses synthetic or toy data now toy data you know has benefits",
    "start": "1908250",
    "end": "1915090"
  },
  {
    "text": "and flaws but it certainly makes understanding the capabilities of teeth",
    "start": "1915090",
    "end": "1921600"
  },
  {
    "text": "they are really really clear mostly because you can get the data that you know a time series in this model are",
    "start": "1921600",
    "end": "1931230"
  },
  {
    "text": "best suited for and you'll see at the end how that kind of hands out so let's",
    "start": "1931230",
    "end": "1936510"
  },
  {
    "text": "start at the top here first of all I ran this a few times and notice that I had to upgrade numpy to Rena repeatedly we",
    "start": "1936510",
    "end": "1944610"
  },
  {
    "text": "initialize our data set with math clot live and pandas we install a special",
    "start": "1944610",
    "end": "1951180"
  },
  {
    "text": "module here called s 3f s and now we actually start so we do our sage maker",
    "start": "1951180",
    "end": "1959250"
  },
  {
    "text": "setup we set up our execution role for I am privileges we set up our s3 paths now",
    "start": "1959250",
    "end": "1966090"
  },
  {
    "text": "we're ready to go here again we're calling the image the container image",
    "start": "1966090",
    "end": "1971700"
  },
  {
    "text": "that has deep forecasting AR and now we're going to generate our toy data a",
    "start": "1971700",
    "end": "1979470"
  },
  {
    "text": "note here that when we're generating our toy data we know in the very beginning that our frequency periodicity is going",
    "start": "1979470",
    "end": "1986490"
  },
  {
    "text": "to be ours and our prediction length is going to be 48 so we have this can we have this luxury because we're producing",
    "start": "1986490",
    "end": "1992790"
  },
  {
    "text": "the data that we're going to operate on we can also set our context link our t0",
    "start": "1992790",
    "end": "1998150"
  },
  {
    "text": "data length number of periods etc so here we have I'll just skip down here",
    "start": "1998150",
    "end": "2005749"
  },
  {
    "text": "and like I said if your notebook looks a little bit different I just reformatted things a bit so that it would look better on this webinar but",
    "start": "2005749",
    "end": "2013339"
  },
  {
    "text": "here we have a plot of our toy data so",
    "start": "2013339",
    "end": "2018649"
  },
  {
    "text": "next we're going to divide our data into a test and training set as mentioned",
    "start": "2018649",
    "end": "2026419"
  },
  {
    "text": "here the test and training set are differentiated in the sense that they",
    "start": "2026419",
    "end": "2032479"
  },
  {
    "text": "can't be chosen just at random here they they do when when you choose your test set it does have to come at the end of",
    "start": "2032479",
    "end": "2040129"
  },
  {
    "text": "your series so we're going to use a special characteristic of pandas series",
    "start": "2040129",
    "end": "2045999"
  },
  {
    "text": "I've mentioned before that we are using JSON strings format and when use panda",
    "start": "2045999",
    "end": "2052460"
  },
  {
    "text": "series there's a very convenient function that helps write out our training data in exactly the format that",
    "start": "2052460",
    "end": "2059179"
  },
  {
    "text": "we need now we're gonna run this through an estimator and this is our wrapper for that estimator and here are the hyper",
    "start": "2059179",
    "end": "2066289"
  },
  {
    "text": "parameters that we're going to use for training notably let's see here we have",
    "start": "2066289",
    "end": "2071599"
  },
  {
    "text": "this early stopping patience of ten it's really the only one that we didn't sort of set up for ourselves in advance we",
    "start": "2071599",
    "end": "2079309"
  },
  {
    "text": "refer to our input for test and training as d2 channels here they are formatted",
    "start": "2079309",
    "end": "2085339"
  },
  {
    "text": "here and then I ran the training job for you before we got on this call let me",
    "start": "2085339",
    "end": "2091069"
  },
  {
    "text": "just go to the very bottom and indicate here that we had a total billable seconds of a hundred and set 67 on this",
    "start": "2091069",
    "end": "2099140"
  },
  {
    "text": "particular notebook all right now we're going to create our production endpoint and start doing predictions here we've",
    "start": "2099140",
    "end": "2106039"
  },
  {
    "text": "created a class that's simply going to help us take advantage of that panda",
    "start": "2106039",
    "end": "2111470"
  },
  {
    "text": "series convenience function to help us create to to format our prediction call",
    "start": "2111470",
    "end": "2118400"
  },
  {
    "text": "so this is nice cut and paste code that I encourage you to borrow when you're",
    "start": "2118400",
    "end": "2123650"
  },
  {
    "text": "creating your own notebooks so now we create the actual prediction endpoint",
    "start": "2123650",
    "end": "2131230"
  },
  {
    "text": "and this code I wrote to just to make this next section a little bit more readable",
    "start": "2131230",
    "end": "2137640"
  },
  {
    "text": "all right so here we are here's the results of our tests now you can see in",
    "start": "2137640",
    "end": "2144390"
  },
  {
    "text": "the what I call that a mustard colored line that's giving us our 80% confidence",
    "start": "2144390",
    "end": "2150470"
  },
  {
    "text": "interval which is one of the key characteristics of deep a are the ability to do probabilistic forecasting",
    "start": "2150470",
    "end": "2157920"
  },
  {
    "text": "the orange line if he wanted a hard number is the hard number it is the",
    "start": "2157920",
    "end": "2163349"
  },
  {
    "text": "actual predicted number and what we're displaying here with these five different graphs and you could tell when",
    "start": "2163349",
    "end": "2169710"
  },
  {
    "text": "you look at the left-hand margin just at the range here we have three to five four to eight four two six so these are",
    "start": "2169710",
    "end": "2177060"
  },
  {
    "text": "clearly different categories of data that have been trained with the data set",
    "start": "2177060",
    "end": "2182190"
  },
  {
    "text": "but the next notebook that we use well this will be a little bit more explicit because we're going to use real data",
    "start": "2182190",
    "end": "2188849"
  },
  {
    "text": "from an electrical power plant forecasting site but what you can see if",
    "start": "2188849",
    "end": "2194910"
  },
  {
    "text": "you imagine for example the first one that has boots the Nexxus sneakers the next is maybe a brand new product that",
    "start": "2194910",
    "end": "2200700"
  },
  {
    "text": "we never introduced before some kind of you know slipper or something that we",
    "start": "2200700",
    "end": "2207119"
  },
  {
    "text": "have pretty accurate models in fact you can only see a little bit of target data",
    "start": "2207119",
    "end": "2213440"
  },
  {
    "text": "exceeding our 80 percent boundaries it does happen but you got to be a stickler",
    "start": "2213440",
    "end": "2218609"
  },
  {
    "text": "for detail to see that so that's a pretty successful model one other point",
    "start": "2218609",
    "end": "2223619"
  },
  {
    "text": "I encourage you to run these sample notebooks dig into them copy and paste",
    "start": "2223619",
    "end": "2228869"
  },
  {
    "text": "the code all of this code is in github you can actually do pull requests on this code I wish you would but when",
    "start": "2228869",
    "end": "2235740"
  },
  {
    "text": "you're running it I commented out this last command here because I wanted to preserve my endpoint for this demo but",
    "start": "2235740",
    "end": "2242490"
  },
  {
    "text": "always delete your endpoints if you're not going to use those in production because they can be costly alright next",
    "start": "2242490",
    "end": "2249030"
  },
  {
    "text": "up we have real data and just to be",
    "start": "2249030",
    "end": "2254520"
  },
  {
    "text": "super clear at the end of this presentation in just a few moments I have links to all of the information",
    "start": "2254520",
    "end": "2260010"
  },
  {
    "text": "that I've covered here including a few open source references on the data",
    "start": "2260010",
    "end": "2266010"
  },
  {
    "text": "science behind forecasting and also links to these",
    "start": "2266010",
    "end": "2271589"
  },
  {
    "text": "specific notebooks in github so in this",
    "start": "2271589",
    "end": "2276780"
  },
  {
    "text": "notebook right here we're gonna actually use some real data that's available from",
    "start": "2276780",
    "end": "2282359"
  },
  {
    "text": "UCI so we do the usual setup I won't dwell on that usual things we need to do",
    "start": "2282359",
    "end": "2288599"
  },
  {
    "text": "set up our buckets and here we are with the UCI data set we're gonna download",
    "start": "2288599",
    "end": "2294150"
  },
  {
    "text": "that from the UCI data set repository by the way I have a great collection of",
    "start": "2294150",
    "end": "2300000"
  },
  {
    "text": "public data sets if anybody wants one I hope you'll just send me any email and",
    "start": "2300000",
    "end": "2305130"
  },
  {
    "text": "request that data is the food of machine learning and if you don't have enough food you starve so so we do have a few",
    "start": "2305130",
    "end": "2314369"
  },
  {
    "text": "tricks in this particular notebook where to explore the data and I'll just scroll",
    "start": "2314369",
    "end": "2319440"
  },
  {
    "text": "down because here we're preparing our data set and now we're examining the",
    "start": "2319440",
    "end": "2324780"
  },
  {
    "text": "data we can actually change some of the predictive parameters you'll see that in a moment so here we're taking a couple",
    "start": "2324780",
    "end": "2330720"
  },
  {
    "text": "of time intervals as I said we're taking related data sets here let me get rid of a little bit of stuff up there there we",
    "start": "2330720",
    "end": "2338190"
  },
  {
    "text": "go probably get rid of this too there we go make this a little bit easier to see so on the Left we have kilowatts",
    "start": "2338190",
    "end": "2344940"
  },
  {
    "text": "consumption for different periods of time different times of days these are all different data sets that are going",
    "start": "2344940",
    "end": "2351390"
  },
  {
    "text": "to be correlated to do predictive analysis based on perhaps these exact",
    "start": "2351390",
    "end": "2357780"
  },
  {
    "text": "conditions in future so next up we train we split the train and test data sets we",
    "start": "2357780",
    "end": "2366119"
  },
  {
    "text": "can see that we have set a frequency here of two hours we've calculated a",
    "start": "2366119",
    "end": "2371180"
  },
  {
    "text": "prediction length for seven days and a context length is identical we begin our",
    "start": "2371180",
    "end": "2379650"
  },
  {
    "text": "training data set at this particular day which we know from looking at the data and as we move down just a little bit",
    "start": "2379650",
    "end": "2386520"
  },
  {
    "text": "here we're doing more of the same of getting that data prepared and set into",
    "start": "2386520",
    "end": "2392130"
  },
  {
    "text": "our s3 bucket now ready to train the model so once again",
    "start": "2392130",
    "end": "2400090"
  },
  {
    "text": "we're gonna use our estimator rapper as we've done here we're gonna use a c' for extra-large we just give the job a name",
    "start": "2400090",
    "end": "2406800"
  },
  {
    "text": "and here are the hyper parameters that we're going to use we're gonna do four hundred epics early stopping at forty",
    "start": "2406800",
    "end": "2414280"
  },
  {
    "text": "mini-batch sixty four learning rate with five zeros and then we have our context",
    "start": "2414280",
    "end": "2420760"
  },
  {
    "text": "length and prediction which were set above which we simply translate all right there we go so then we train the job and I'm gonna",
    "start": "2420760",
    "end": "2428260"
  },
  {
    "text": "scroll to the bottom here just to see how long this particular one took help everyone out there",
    "start": "2428260",
    "end": "2433690"
  },
  {
    "text": "notice the wall time here was eleven seconds billable seconds was five",
    "start": "2433690",
    "end": "2438970"
  },
  {
    "text": "hundred and sixty three the delta there is usually the amount of time it takes",
    "start": "2438970",
    "end": "2444250"
  },
  {
    "text": "to copy the image image out of s3 you know to just get your place in the",
    "start": "2444250",
    "end": "2449590"
  },
  {
    "text": "queue and and grab that image so there's a little delta there but typically when",
    "start": "2449590",
    "end": "2454770"
  },
  {
    "text": "jobs take you know let's just say more than five minutes to run that startup",
    "start": "2454770",
    "end": "2461740"
  },
  {
    "text": "time is is not significant now we're going to create our end points once again we're using a convenience function",
    "start": "2461740",
    "end": "2469120"
  },
  {
    "text": "to help us prepare our data to actually test with and now we're going to make",
    "start": "2469120",
    "end": "2474880"
  },
  {
    "text": "some predictions so we're setting up our predictor here and we're taking a look",
    "start": "2474880",
    "end": "2480850"
  },
  {
    "text": "at some of that data from our very first prediction and if I have this correct",
    "start": "2480850",
    "end": "2487810"
  },
  {
    "text": "here we go so here is where we've built into this notebook excuse me the ability",
    "start": "2487810",
    "end": "2494590"
  },
  {
    "text": "to take our predictions and modify them a little bit and this is a really nice built-in feature so all you have to do",
    "start": "2494590",
    "end": "2501340"
  },
  {
    "text": "is start with the default values and click run interact now that's going to",
    "start": "2501340",
    "end": "2506410"
  },
  {
    "text": "generate a model for us and we can see as we did on the other notebook that we",
    "start": "2506410",
    "end": "2514810"
  },
  {
    "text": "we generally do pretty well now this is unreal data so that makes it a little bit more interesting if we change some",
    "start": "2514810",
    "end": "2521050"
  },
  {
    "text": "of these parameters a little bit let's lower our confidence level and this time maybe I'll show samples and we've run",
    "start": "2521050",
    "end": "2527500"
  },
  {
    "text": "interact again we can generate more data on the fly that is a really",
    "start": "2527500",
    "end": "2533110"
  },
  {
    "text": "nice tool I'm gonna turn off that show samples just so it's a little bit more clear it looked a little too ghosty that",
    "start": "2533110",
    "end": "2539170"
  },
  {
    "text": "way so this is a nice little tool to test your predictions and yeah it's just",
    "start": "2539170",
    "end": "2545830"
  },
  {
    "text": "built right into these notebooks here so I encourage you to use that we do a",
    "start": "2545830",
    "end": "2551530"
  },
  {
    "text": "second test here which uses the dynamic feature field and here we have our data",
    "start": "2551530",
    "end": "2557920"
  },
  {
    "text": "the feature here is of course something like a peak period maybe when you're",
    "start": "2557920",
    "end": "2565720"
  },
  {
    "text": "looking at electricity demand it would be something like a lot of air conditioning is required on that day or",
    "start": "2565720",
    "end": "2572020"
  },
  {
    "text": "some industrial park as you know reserved a lot of data or you can also have an outage and you can see here that",
    "start": "2572020",
    "end": "2579010"
  },
  {
    "text": "we have some missing data right so there is missing data I'm gonna just go right",
    "start": "2579010",
    "end": "2585430"
  },
  {
    "text": "to the predictive card again at the end I'm building a new model and I want to",
    "start": "2585430",
    "end": "2590890"
  },
  {
    "text": "see how long this one took to Train good",
    "start": "2590890",
    "end": "2598180"
  },
  {
    "text": "this one took 24 minutes so planning your lunch accordingly but this is a",
    "start": "2598180",
    "end": "2604330"
  },
  {
    "text": "worthwhile build once again we see some of our top level data and then once",
    "start": "2604330",
    "end": "2610090"
  },
  {
    "text": "again we also get the ability to play with these predictions a little bit notice that because data was missing in",
    "start": "2610090",
    "end": "2617590"
  },
  {
    "text": "our sample in our training data that it's missing in the charts as well so",
    "start": "2617590",
    "end": "2625800"
  },
  {
    "text": "that's good stuff that is that is deep AR it's a beautiful thing I have a",
    "start": "2625800",
    "end": "2634420"
  },
  {
    "start": "2632000",
    "end": "2632000"
  },
  {
    "text": "number of links that are available to everybody that I think are very helpful",
    "start": "2634420",
    "end": "2639670"
  },
  {
    "text": "I don't have anything here on the stock market I gave you all my stock market information just in at the beginning but",
    "start": "2639670",
    "end": "2647580"
  },
  {
    "text": "there this this book right here are forecasting principles in practice that's a free online book highly",
    "start": "2647580",
    "end": "2654640"
  },
  {
    "text": "recommend it for anyone interested in in this as a field and then also all of the",
    "start": "2654640",
    "end": "2661570"
  },
  {
    "text": "other materials that in one way or another supported creating this presentation",
    "start": "2661570",
    "end": "2666630"
  },
  {
    "text": "finally I have links not only to all of the notebooks that were used in this",
    "start": "2666630",
    "end": "2672670"
  },
  {
    "text": "presentation but also to this great series by one of our engineers in-house",
    "start": "2672670",
    "end": "2678430"
  },
  {
    "text": "Sunil Mallya I'll just bring it up it's a great resource it's up on github you",
    "start": "2678430",
    "end": "2685329"
  },
  {
    "text": "can download it and the way he's done this is he walks through pretty much",
    "start": "2685329",
    "end": "2690819"
  },
  {
    "text": "everything you need to do univariate multivariate in time series forecasting",
    "start": "2690819",
    "end": "2696299"
  },
  {
    "text": "it's a really nice tutorial series so I encourage everyone to take a look at",
    "start": "2696299",
    "end": "2703089"
  },
  {
    "text": "that as well now I went a little bit past the presentation time I wanted 15",
    "start": "2703089",
    "end": "2709480"
  },
  {
    "text": "minutes we only have ten but I'm happy to take a few questions",
    "start": "2709480",
    "end": "2716460"
  },
  {
    "text": "hi Chris this is Chris burns I've been answering a couple questions here in the background more on logistics rather than",
    "start": "2719840",
    "end": "2728120"
  },
  {
    "text": "rather than the specifics about the DBR algorithm so if anybody has any questions now as Chris mentioned be a",
    "start": "2728120",
    "end": "2734670"
  },
  {
    "text": "perfect time to either write them in the box here or put them in the chat window so I'll just mention while we're we're",
    "start": "2734670",
    "end": "2745860"
  },
  {
    "text": "waiting for our questions to come in and Chris just so you know I can't for some",
    "start": "2745860",
    "end": "2751710"
  },
  {
    "text": "reason read the questions so I'll only do two yeah I did have one and just just",
    "start": "2751710",
    "end": "2758340"
  },
  {
    "text": "popped up Chris okay and the question is could you go into a little more detail",
    "start": "2758340",
    "end": "2764240"
  },
  {
    "text": "yeah sorry I was put on that there it says that to a little more detail how",
    "start": "2766510",
    "end": "2773590"
  },
  {
    "text": "deep a our treats categorical variables is it generating a separate model for",
    "start": "2773590",
    "end": "2778840"
  },
  {
    "text": "each value of a category that is well because of well first of all I refer you",
    "start": "2778840",
    "end": "2786910"
  },
  {
    "text": "to the documentation we did see an example of the outcomes in the in both",
    "start": "2786910",
    "end": "2796300"
  },
  {
    "text": "the toy and the electrical model for our handling that it's really a byproduct of",
    "start": "2796300",
    "end": "2802540"
  },
  {
    "text": "the way that we're do using LST m and",
    "start": "2802540",
    "end": "2808390"
  },
  {
    "text": "the sequence to sequence so I'll just point out that sequence to sequence is",
    "start": "2808390",
    "end": "2814869"
  },
  {
    "text": "an encoder decoder model that has its roots in language processing so you know",
    "start": "2814869",
    "end": "2820720"
  },
  {
    "text": "the way you take a french expression translate it to English and use it as a",
    "start": "2820720",
    "end": "2826200"
  },
  {
    "text": "encoder decoder create some characteristics that when you when",
    "start": "2826200",
    "end": "2831580"
  },
  {
    "text": "applied to this particular problem that create windows for the RNN to operate on",
    "start": "2831580",
    "end": "2838560"
  },
  {
    "text": "now I will just tell you straight up I don't know if it's creating a separate model but because of the virtues of",
    "start": "2838560",
    "end": "2845410"
  },
  {
    "text": "those particular algorithms that wouldn't be my guess so rather than say something put my foot",
    "start": "2845410",
    "end": "2852490"
  },
  {
    "text": "in my mouth I say please refer to the documentation and I hope that gave you at least a little start running start",
    "start": "2852490",
    "end": "2860250"
  },
  {
    "text": "great thanks Chris and we do have a another question here when the algorithm",
    "start": "2860820",
    "end": "2866530"
  },
  {
    "text": "makes the prediction using the dynamic features you are required to provide a value for each dynamic feature across",
    "start": "2866530",
    "end": "2873070"
  },
  {
    "text": "the prediction period and so the question is is the typical approach to train a deep AR model on each of the",
    "start": "2873070",
    "end": "2879520"
  },
  {
    "text": "dynamic features well that's an easy one no because that is what we're doing for",
    "start": "2879520",
    "end": "2885970"
  },
  {
    "text": "you in the background once again for details refer you to the documentation",
    "start": "2885970",
    "end": "2894930"
  },
  {
    "text": "it's very well documented in the PDF that we have",
    "start": "2894930",
    "end": "2900250"
  },
  {
    "text": "stage maker documentation page all right",
    "start": "2900250",
    "end": "2909040"
  },
  {
    "text": "just another one just popped up perfect timing can you use deep AR to do",
    "start": "2909040",
    "end": "2915190"
  },
  {
    "text": "failure forecasting without knowing when a failure happened so the short answer",
    "start": "2915190",
    "end": "2922480"
  },
  {
    "text": "to that would be within the probability range yes you know failure is a special",
    "start": "2922480",
    "end": "2929830"
  },
  {
    "text": "condition and we saw how those special conditions were handled in the second notebook but you know if you've got an",
    "start": "2929830",
    "end": "2938980"
  },
  {
    "text": "80% range of probability you have a 20% range of in probability so you know I",
    "start": "2938980",
    "end": "2947290"
  },
  {
    "text": "would say with your specific instance that you know if you know that data well",
    "start": "2947290",
    "end": "2953890"
  },
  {
    "text": "enough you have to qualify this thing a hundred times then you should be able to",
    "start": "2953890",
    "end": "2958900"
  },
  {
    "text": "at least you know put your team on alert say if you know that an adages is well",
    "start": "2958900",
    "end": "2966580"
  },
  {
    "text": "predicted all right thanks Chris are we",
    "start": "2966580",
    "end": "2973240"
  },
  {
    "text": "good and we'll just have one more bit of color to that too one way I've seen a lot of these models chained together is in supply chains",
    "start": "2973240",
    "end": "2980860"
  },
  {
    "text": "right so if you have you know 50 parts that you need to create a particular OEM",
    "start": "2980860",
    "end": "2989560"
  },
  {
    "text": "product and you either control or have access to the production characteristics",
    "start": "2989560",
    "end": "2998830"
  },
  {
    "text": "of those of that supply chain this is this is an instance where you can really",
    "start": "2998830",
    "end": "3004170"
  },
  {
    "text": "do some rewarding well not failure necessarily but outage",
    "start": "3004170",
    "end": "3010440"
  },
  {
    "text": "analysis and you know get ahead of being out of a product that you want to sell",
    "start": "3010440",
    "end": "3017660"
  },
  {
    "text": "all right Chris we have up we have a participant that's going to ask the question verbally here it's a little bit",
    "start": "3017660",
    "end": "3023490"
  },
  {
    "text": "difficult to have to touch I'm gonna put it I'm gonna take them off of mute yep",
    "start": "3023490",
    "end": "3029720"
  },
  {
    "text": "and we should resign so yeah this is Brian gross like we met before I was",
    "start": "3032050",
    "end": "3039950"
  },
  {
    "text": "asking a question I think teachers before us what to clarify so when we're",
    "start": "3039950",
    "end": "3045109"
  },
  {
    "text": "specifying the actual prediction request it seems to fail if we don't actually",
    "start": "3045109",
    "end": "3051830"
  },
  {
    "text": "provide some value for each of the dynamic variables into the future in the time period that we're trying to predict",
    "start": "3051830",
    "end": "3059170"
  },
  {
    "text": "is that your understanding of the way it would work so are you well you know we accept Nan",
    "start": "3059170",
    "end": "3067310"
  },
  {
    "text": "values are you leaving them empty or are you putting maybe so yeah that your",
    "start": "3067310",
    "end": "3073040"
  },
  {
    "text": "wavelength is the issue if we just put in not a number it will just put in and get that feature that you were talking",
    "start": "3073040",
    "end": "3078380"
  },
  {
    "text": "about earlier yep that is good to know all right I mean that's maybe a side effect of the encoder yet sequence to",
    "start": "3078380",
    "end": "3087020"
  },
  {
    "text": "sequencing code it just requires the exact help thank you all right well um",
    "start": "3087020",
    "end": "3101180"
  },
  {
    "text": "if we're running down on questions here that's fine it's we're pretty much in",
    "start": "3101180",
    "end": "3106420"
  },
  {
    "text": "I'm grateful for the opportunity to share this information with you contact information is here Chris and I you know",
    "start": "3106420",
    "end": "3114380"
  },
  {
    "text": "we're the PSAs here to support you so please reach out and as you begin to",
    "start": "3114380",
    "end": "3121430"
  },
  {
    "text": "play with this an experiment you're going to hit blockers and give us a call when that and then happy day comes so",
    "start": "3121430",
    "end": "3128240"
  },
  {
    "text": "thank you very much all right thanks Chris and sue I'm gonna jump in and just reiterate there what",
    "start": "3128240",
    "end": "3133490"
  },
  {
    "text": "Chris said is if you put a questionnaire we didn't quite get to that know that you're you were part of the ML",
    "start": "3133490",
    "end": "3139460"
  },
  {
    "text": "competency and Chris and I are here to enable you across the entire AWS ml stack so in the event that you did get a",
    "start": "3139460",
    "end": "3146510"
  },
  {
    "text": "question answer today or a question pops into your mind an hour from now reach out to us by email or you if you want to",
    "start": "3146510",
    "end": "3152900"
  },
  {
    "text": "call us you give you Chris's phone number you can call him directly otherwise we're here by email 24/7",
    "start": "3152900",
    "end": "3160088"
  },
  {
    "text": "awesome thanks everybody Chris that was fantastic and Chris Burns thanks for helping us get through those questions",
    "start": "3160250",
    "end": "3166170"
  },
  {
    "text": "I've got one more question to ask everybody it's philosophical question if a webinar happened and there wasn't a",
    "start": "3166170",
    "end": "3173069"
  },
  {
    "text": "survey at the end did it really even happen I mean this is a philosophical experiment we are clearly trying to",
    "start": "3173069",
    "end": "3179220"
  },
  {
    "text": "answer here at AWS so yeah you could help us out we're gonna send you a follow-up email I'm gonna have a link to",
    "start": "3179220",
    "end": "3184650"
  },
  {
    "text": "a super short survey for questions it's gonna have a link to the replay of today's call so if you want to dig back",
    "start": "3184650",
    "end": "3190799"
  },
  {
    "text": "through you can do that and it also also going to have the link to our next webinar which is going to be too extreme",
    "start": "3190799",
    "end": "3195990"
  },
  {
    "text": "today it's gonna cover blazing text so with that we will end the call and I wish everybody a fantastic rest of your",
    "start": "3195990",
    "end": "3202619"
  },
  {
    "text": "day",
    "start": "3202619",
    "end": "3204740"
  }
]