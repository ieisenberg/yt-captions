[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "good afternoon everybody uh welcome to the big data beyond hadoop discussion",
    "start": "1120",
    "end": "6640"
  },
  {
    "text": "uh my name is nikolai bakarchev solutions architect with aws worldwide public sector it's my distinct pleasure",
    "start": "6640",
    "end": "13120"
  },
  {
    "text": "to be with you today and also to share the stage uh with marco marins from iko",
    "start": "13120",
    "end": "19359"
  },
  {
    "text": "which is organization a part of the united nations and marco will be giving a specific",
    "start": "19359",
    "end": "25199"
  },
  {
    "text": "example of using big data beyond hadoop to solve some real-world",
    "start": "25199",
    "end": "31519"
  },
  {
    "text": "aviation safety statistics problems",
    "start": "31519",
    "end": "36399"
  },
  {
    "start": "36000",
    "end": "108000"
  },
  {
    "text": "so what are we going to cover today will big data scale to infinity",
    "start": "36640",
    "end": "43200"
  },
  {
    "text": "and uh i suspect you you you kind of can guess the answer here",
    "start": "43200",
    "end": "48399"
  },
  {
    "text": "and then how does aws remove the constraints towards that big data growth",
    "start": "48399",
    "end": "53680"
  },
  {
    "text": "hadoop in the cloud advantages strengths and limitations and we'll look at the hadoop ecosystem",
    "start": "53680",
    "end": "60960"
  },
  {
    "text": "amazon redshift for real-time analytics and our partners on the aws marketplace",
    "start": "60960",
    "end": "66880"
  },
  {
    "text": "and finally aws big data workflow automation and of course we have the best for last where marco will share",
    "start": "66880",
    "end": "73600"
  },
  {
    "text": "some specific examples so before we dive in uh may i ask uh who",
    "start": "73600",
    "end": "79680"
  },
  {
    "text": "is using emr today here in the audience okay wonderful wonderful i think about half of the audience and how about",
    "start": "79680",
    "end": "86479"
  },
  {
    "text": "redshift terrific terrific so hopefully after this discussion we'll have even even",
    "start": "86479",
    "end": "92079"
  },
  {
    "text": "more users and i certainly encourage you all uh to present here next year your successes with big data on aws one last",
    "start": "92079",
    "end": "100640"
  },
  {
    "text": "question who is ready for the replay party tonight come on everybody okay so let's get",
    "start": "100640",
    "end": "106799"
  },
  {
    "text": "started quickly we all experience big data in our daily",
    "start": "106799",
    "end": "112399"
  },
  {
    "start": "108000",
    "end": "169000"
  },
  {
    "text": "life no question about it we're part of the explosion of big data from social media to email overloads to actually",
    "start": "112399",
    "end": "119280"
  },
  {
    "text": "collecting data with our health personal health monitors but the impact is even greater on the",
    "start": "119280",
    "end": "125680"
  },
  {
    "text": "enterprise and this is one of our customers the international center for radio astronomy research is based in",
    "start": "125680",
    "end": "131520"
  },
  {
    "text": "australia and it's part of an international consortium building the square kilometer array",
    "start": "131520",
    "end": "137920"
  },
  {
    "text": "the biggest radio telescope in the world at least in the known universe so one day once they go live into",
    "start": "137920",
    "end": "145120"
  },
  {
    "text": "production in a couple years sk the the square kilometer array will be",
    "start": "145120",
    "end": "150959"
  },
  {
    "text": "collecting more data every single day than the entire world is collecting today think",
    "start": "150959",
    "end": "156879"
  },
  {
    "text": "about it this is zettabytes of data every day and they will be using aws and",
    "start": "156879",
    "end": "162160"
  },
  {
    "text": "crowdsource cpus to analyze 400 to 500 galaxies simultaneously talk about big",
    "start": "162160",
    "end": "168160"
  },
  {
    "text": "data our customers range from mobile cable",
    "start": "168160",
    "end": "173680"
  },
  {
    "start": "169000",
    "end": "208000"
  },
  {
    "text": "telecom oil gas and infrastructure industrial manufacturing to retail no",
    "start": "173680",
    "end": "178720"
  },
  {
    "text": "surprise there amazon.com and many others life sciences as we heard in the",
    "start": "178720",
    "end": "183920"
  },
  {
    "text": "keynote uh discussions yesterday financial services uh publishing and media and last but not",
    "start": "183920",
    "end": "191360"
  },
  {
    "text": "least online media social networking and gaming if your organization is not listed here chances are whether it's an",
    "start": "191360",
    "end": "198720"
  },
  {
    "text": "academic institution government department or a corporation you are still also dealing with big data both",
    "start": "198720",
    "end": "205680"
  },
  {
    "text": "the challenges and the opportunities so will it really scale to infinity",
    "start": "205680",
    "end": "212480"
  },
  {
    "start": "208000",
    "end": "245000"
  },
  {
    "text": "well it seems both gartner and idc agree and they've estimated the growth of",
    "start": "212480",
    "end": "219120"
  },
  {
    "text": "big data to be at 62 compound annual growth before this presentation i was checking our big data",
    "start": "219120",
    "end": "226560"
  },
  {
    "text": "presentations from two years ago and back then we reported 40 percent growth today we're reporting 62 growth in other",
    "start": "226560",
    "end": "234159"
  },
  {
    "text": "words the growth is only accelerating so what's the challenge well the challenge is this growing gap between",
    "start": "234159",
    "end": "240959"
  },
  {
    "text": "the generated data and data available for analysis",
    "start": "240959",
    "end": "246159"
  },
  {
    "start": "245000",
    "end": "374000"
  },
  {
    "text": "and that's where we believe aws can really help you remove constraints",
    "start": "246159",
    "end": "253040"
  },
  {
    "text": "we love scalability uh in almost every session you hear about that you can easily scale to thousands of",
    "start": "253040",
    "end": "259359"
  },
  {
    "text": "instances run them for an hour or two and shut them down obviously much more cost effective than investing in fixed",
    "start": "259359",
    "end": "266240"
  },
  {
    "text": "infrastructure that you you're committed to run for long periods of time so there's zero upfront capital all",
    "start": "266240",
    "end": "272160"
  },
  {
    "text": "services are on demand elastic and very scalable all the way to thousands and",
    "start": "272160",
    "end": "277440"
  },
  {
    "text": "tens of thousands of machines as you've heard here at the reinvent and you pay only for your to what you do what you",
    "start": "277440",
    "end": "284000"
  },
  {
    "text": "use so how does that apply specifically to the big data constraints",
    "start": "284000",
    "end": "289919"
  },
  {
    "text": "the famous three v's uh the volume variety and velocity of big data and uh i would focus on",
    "start": "289919",
    "end": "297040"
  },
  {
    "text": "specifically on the velocity here this is not only the accumulation of big data the rate of accumulation but also the",
    "start": "297040",
    "end": "303840"
  },
  {
    "text": "rate of change the formats of data change the sources of data change our analytic choices and",
    "start": "303840",
    "end": "312320"
  },
  {
    "text": "tools to process that data change both from the open source community and the commercial software community and",
    "start": "312320",
    "end": "318400"
  },
  {
    "text": "finally the important thing about big data is what matters is really the final results",
    "start": "318400",
    "end": "323759"
  },
  {
    "text": "the time to final results until you can benefit from your big data insights it's",
    "start": "323759",
    "end": "328800"
  },
  {
    "text": "not really the performance of any single tool in the big data pipeline",
    "start": "328800",
    "end": "333919"
  },
  {
    "text": "and that's where aws comes in to help address these constraints virtually unlimited resources",
    "start": "333919",
    "end": "341039"
  },
  {
    "text": "i'm sure you heard a lot about that here to invent a variety of compute solutions",
    "start": "341039",
    "end": "346080"
  },
  {
    "text": "and we'll review some of them including hadoop and redshift iterative experimental deployment of",
    "start": "346080",
    "end": "352639"
  },
  {
    "text": "your infrastructure to make to match the character of your big data tasks",
    "start": "352639",
    "end": "358080"
  },
  {
    "text": "and last but not least getting you to faster results by",
    "start": "358080",
    "end": "363360"
  },
  {
    "text": "deploying only the infrastructure that you need in a flexible parallel environment where multiple projects can",
    "start": "363360",
    "end": "369919"
  },
  {
    "text": "go in parallel and when they're down you can just shut them down",
    "start": "369919",
    "end": "375199"
  },
  {
    "start": "374000",
    "end": "402000"
  },
  {
    "text": "so what is the tool that helps you do that we believe there is no one tool to roll them all it's really a variety of",
    "start": "375360",
    "end": "381440"
  },
  {
    "text": "tools aws has released over 40 tools over our history even here at reinvent",
    "start": "381440",
    "end": "387600"
  },
  {
    "text": "we released several today our focus is really on the analytics tools specifically around hadoop and data",
    "start": "387600",
    "end": "394240"
  },
  {
    "text": "warehousing being redshift and of course our foundation services compute storage security and networking",
    "start": "394240",
    "end": "402800"
  },
  {
    "start": "402000",
    "end": "598000"
  },
  {
    "text": "so how do we offer hadoop in the cloud we call it amazon emr or amazon elastic",
    "start": "403199",
    "end": "410880"
  },
  {
    "text": "mapreduce and what it is is a fully managed hadoop",
    "start": "410880",
    "end": "416240"
  },
  {
    "text": "service it's based on apache hadoop it runs the hadoop distributed file system",
    "start": "416240",
    "end": "422240"
  },
  {
    "text": "it runs the latest version of hadoop 2 running on yarn and it runs the",
    "start": "422240",
    "end": "427599"
  },
  {
    "text": "mapreduce parallel processing framework now in addition to that we offer you special cases of using s3 instead or in",
    "start": "427599",
    "end": "436240"
  },
  {
    "text": "addition to hdfs and other tools which i'll be discussing a bit later",
    "start": "436240",
    "end": "442080"
  },
  {
    "text": "courtesy to our customers and partners at netflix this is from their blog this is the foundation infrastructure of",
    "start": "442080",
    "end": "448639"
  },
  {
    "text": "their hadoop on on the cloud deployment and actually for those of you who",
    "start": "448639",
    "end": "454080"
  },
  {
    "text": "saw yesterday's presentation i'm glad to say this is still the core of their environment on emr",
    "start": "454080",
    "end": "460319"
  },
  {
    "text": "and they continue to add various components to it open source components etc",
    "start": "460319",
    "end": "466160"
  },
  {
    "text": "the key points here the foundation is really s3 so s3 is",
    "start": "466160",
    "end": "471440"
  },
  {
    "text": "your data hub in the cloud highly resilient highly durable",
    "start": "471440",
    "end": "476479"
  },
  {
    "text": "and also allowing optimal parallel i o on top of that you can start as many",
    "start": "476479",
    "end": "483759"
  },
  {
    "text": "hadoop clusters or emr clusters as you as you wish or as you",
    "start": "483759",
    "end": "489039"
  },
  {
    "text": "need for your processing and then on top of that you have the hadoop ecosystem of tools including hive pig or general",
    "start": "489039",
    "end": "495919"
  },
  {
    "text": "programming languages like java and python and then netflix like many other organizations develop their own",
    "start": "495919",
    "end": "502560"
  },
  {
    "text": "bi tools on top or or administrative and management tools which they happen to",
    "start": "502560",
    "end": "508720"
  },
  {
    "text": "make available to the community as open source tools so i certainly encourage you to visit the netflix",
    "start": "508720",
    "end": "514880"
  },
  {
    "text": "blogs on this topic so how does emr work well it all starts with s3 so you",
    "start": "514880",
    "end": "521680"
  },
  {
    "text": "typically upload your data in s3 somehow uh you can choose to copy that data into",
    "start": "521680",
    "end": "527200"
  },
  {
    "text": "hdfs as well or not and we'll talk a bit more about it later then the next critical step is you",
    "start": "527200",
    "end": "532800"
  },
  {
    "text": "choose your distribution it's you have many options it's not just the latest version but several older versions of",
    "start": "532800",
    "end": "538720"
  },
  {
    "text": "hadoop they're maybe they're compatible with your existing code you choose your number and type of nodes",
    "start": "538720",
    "end": "544800"
  },
  {
    "text": "custom configuration tools like hive pick and many others and most importantly and we'll talk more about it",
    "start": "544800",
    "end": "551680"
  },
  {
    "text": "you can choose bootstrap actions which are custom ways to enhance your uh",
    "start": "551680",
    "end": "557680"
  },
  {
    "text": "cluster environment you can launch the environment literally in minutes",
    "start": "557680",
    "end": "563839"
  },
  {
    "text": "and then typically the output is also in s3 or you keep it in hdfs and there are",
    "start": "564720",
    "end": "570480"
  },
  {
    "text": "some trade-offs uh according to netflix presentation yesterday you have a little bit performance",
    "start": "570480",
    "end": "576720"
  },
  {
    "text": "degradation five to ten percent when you use s3 but you gain the durability and",
    "start": "576720",
    "end": "582640"
  },
  {
    "text": "also the high parallelism of potentially use several clusters uh with the same s3 data",
    "start": "582640",
    "end": "589360"
  },
  {
    "text": "and then finally you can easily scale out or scale back down your cluster with",
    "start": "589360",
    "end": "595680"
  },
  {
    "text": "processing nodes as needed i want to highlight only a couple of",
    "start": "595680",
    "end": "600720"
  },
  {
    "text": "configuration points on emr and this is the",
    "start": "600720",
    "end": "606240"
  },
  {
    "text": "management console interface you have access to all these tools through the command line or through the apis",
    "start": "606240",
    "end": "612160"
  },
  {
    "text": "number one the variety of distributions not only of apache hadoop which come",
    "start": "612160",
    "end": "617920"
  },
  {
    "text": "from amazon or aws but also map our distributions and hopefully in the",
    "start": "617920",
    "end": "623519"
  },
  {
    "text": "future we'll have other distributions available at emr in addition",
    "start": "623519",
    "end": "629040"
  },
  {
    "text": "with a single button click you can select applications like hive pick hbase",
    "start": "629040",
    "end": "634320"
  },
  {
    "text": "impala ganglia today we heard during the keynote from splunk",
    "start": "634320",
    "end": "640480"
  },
  {
    "text": "about the hunk application which you'll be able to deploy and also hugh was",
    "start": "640480",
    "end": "645760"
  },
  {
    "text": "recently deployed also an application available to you on emr you have general purpose programming",
    "start": "645760",
    "end": "651920"
  },
  {
    "text": "languages you have languages like myhealth and r and we'll dive deeper into that",
    "start": "651920",
    "end": "658240"
  },
  {
    "text": "one point on the emr file system another recent capability of emr",
    "start": "658240",
    "end": "664320"
  },
  {
    "text": "the key is that you can manage your const consistent view of your s3 objects",
    "start": "664320",
    "end": "670560"
  },
  {
    "text": "uh because we manage metadata of your s3 objects in dynamodb and of course you",
    "start": "670560",
    "end": "676079"
  },
  {
    "text": "can also do encryption and you learned about our new encryption services which you can benefit from on emr and then",
    "start": "676079",
    "end": "682880"
  },
  {
    "text": "finally the bootstrap actions which can give you a lot of flexibility what you",
    "start": "682880",
    "end": "688160"
  },
  {
    "text": "can do on emr so what is hadoop really well suited for",
    "start": "688160",
    "end": "694880"
  },
  {
    "text": "it's really the best tool for for basic statistics in other words counting items",
    "start": "694880",
    "end": "701279"
  },
  {
    "text": "summaries averages means etc even in this area though which is highly",
    "start": "701279",
    "end": "707360"
  },
  {
    "text": "uh paralyzed hadoop does not have by itself a package",
    "start": "707360",
    "end": "712720"
  },
  {
    "text": "for statistical computation so that's where we'll talk about the potential extension with r and how you can",
    "start": "712720",
    "end": "718800"
  },
  {
    "text": "leverage the capabilities of r together with the parallelism of hadoop",
    "start": "718800",
    "end": "724000"
  },
  {
    "text": "now what are the areas where hadoop is not really that that good at or not that's suitable number one",
    "start": "724000",
    "end": "731519"
  },
  {
    "text": "is when this parallel data slip splits when they have inherent dependencies",
    "start": "731519",
    "end": "738000"
  },
  {
    "text": "then all of a sudden you cannot benefit so well from this parallelism of hadoop",
    "start": "738000",
    "end": "743519"
  },
  {
    "text": "and typical examples are machine learning problems and that's where we'll give you the example about",
    "start": "743519",
    "end": "749440"
  },
  {
    "text": "one of many options that you have to explore with hadoop and then finally",
    "start": "749440",
    "end": "754639"
  },
  {
    "text": "iterative computation by definition mapreduce is a batch processing job consisting of",
    "start": "754639",
    "end": "761519"
  },
  {
    "text": "a mapper and a reducer and then after that data gets stored on disk so",
    "start": "761519",
    "end": "767120"
  },
  {
    "text": "clearly if you have multiple iterations they all have to go through disks which slows down the process significantly",
    "start": "767120",
    "end": "773920"
  },
  {
    "text": "graph computing is is one area where this would affect performance and that's where we'll talk about giraffe",
    "start": "773920",
    "end": "781519"
  },
  {
    "start": "780000",
    "end": "924000"
  },
  {
    "text": "so let's briefly look at the apache hadoop ecosystem which has evolved over the years to complement the",
    "start": "782160",
    "end": "789120"
  },
  {
    "text": "core map produce process this is by no means as an exhaustive view i certainly",
    "start": "789120",
    "end": "794480"
  },
  {
    "text": "encourage you to keep exploring various tools but just to give you a flavor of the environment number one these are the",
    "start": "794480",
    "end": "800880"
  },
  {
    "text": "core components of hadoop i included notch here as the original web",
    "start": "800880",
    "end": "806079"
  },
  {
    "text": "crawler project from which hadoop was a spinoff and then of course hdfs mapreduce",
    "start": "806079",
    "end": "812720"
  },
  {
    "text": "pig and hive as the analytics tool strip and cascading for building interfaces",
    "start": "812720",
    "end": "818160"
  },
  {
    "text": "you have a rich tool set of streaming tools like avro or sql interfaces with",
    "start": "818160",
    "end": "824720"
  },
  {
    "text": "scoop or data collection engines like flume or chukwa a very active open source community",
    "start": "824720",
    "end": "832079"
  },
  {
    "text": "around hadoop is nosql databases hbase accumula they run on hadoop in",
    "start": "832079",
    "end": "838480"
  },
  {
    "text": "fact you can launch them on emr cassandra is not per se part of the",
    "start": "838480",
    "end": "844399"
  },
  {
    "text": "hadoop ecosystem but is very often used together and in parallel with hadoop and then finally zookeeper",
    "start": "844399",
    "end": "851600"
  },
  {
    "text": "is a very popular tool used by",
    "start": "851600",
    "end": "856800"
  },
  {
    "text": "nosql databases used also by by giraffe to coordinate distributed systems",
    "start": "856800",
    "end": "864480"
  },
  {
    "text": "spark and test two very new technologies i certainly encourage you to explore further they were mentioned at some of",
    "start": "864480",
    "end": "870480"
  },
  {
    "text": "the presentations here in reinvent in fact netflix was experimenting with with tess these are alternatives to mapreduce",
    "start": "870480",
    "end": "879279"
  },
  {
    "text": "they actually execute a directed aciclic graph instead of the standard mapper",
    "start": "879279",
    "end": "886079"
  },
  {
    "text": "reducer then you can define a variety of execution modes and therefore run them",
    "start": "886079",
    "end": "891760"
  },
  {
    "text": "in memory and then be more efficient and then finally",
    "start": "891760",
    "end": "897040"
  },
  {
    "text": "the topic of today's discussion the extension of hadoop with r for statistical computing with my health for",
    "start": "897040",
    "end": "903760"
  },
  {
    "text": "machine learning and with giraffe from for graph processing",
    "start": "903760",
    "end": "910399"
  },
  {
    "text": "all of these projects just want to highlight almost all of them are part of the apache",
    "start": "910399",
    "end": "916320"
  },
  {
    "text": "ecosystem a couple of them are open source but part of different ecosystems like cascading and r",
    "start": "916320",
    "end": "924480"
  },
  {
    "start": "924000",
    "end": "1080000"
  },
  {
    "text": "so let's talk first about uh statistical processing with r how many of you are familiar with r or maybe",
    "start": "925440",
    "end": "931440"
  },
  {
    "text": "using it today wonderful wonderful terrific so uh really a majority of the audience it is the most popular",
    "start": "931440",
    "end": "938320"
  },
  {
    "text": "statistical processing tool uh and it's uh it uses a mixture of",
    "start": "938320",
    "end": "944480"
  },
  {
    "text": "paradigms a very powerful tool the challenge with r is that originally",
    "start": "944480",
    "end": "950079"
  },
  {
    "text": "it was designed based on statistical languages from the 70s it",
    "start": "950079",
    "end": "955120"
  },
  {
    "text": "was really designed to work on a sim on a single machine so analysts were limited to what their",
    "start": "955120",
    "end": "960480"
  },
  {
    "text": "machine can can handle in terms of processing power memory etc",
    "start": "960480",
    "end": "965519"
  },
  {
    "text": "hadoop gives you a path to run r in parallel and therefore increase the size of the",
    "start": "965519",
    "end": "972480"
  },
  {
    "text": "task you can process so how can you do that",
    "start": "972480",
    "end": "977440"
  },
  {
    "text": "in fact r is already installed on every single emr node so it's already there",
    "start": "977839",
    "end": "985199"
  },
  {
    "text": "now how can you run it in parallel then the easiest way",
    "start": "985199",
    "end": "990399"
  },
  {
    "text": "is to leverage the hadoop streaming library and many of you are probably familiar with it in which all you need to do is",
    "start": "990399",
    "end": "997839"
  },
  {
    "text": "to write a mapper and write a reducer code in r and then",
    "start": "997839",
    "end": "1003600"
  },
  {
    "text": "submit them as parameters to your hadoop streaming request and then they will run in",
    "start": "1003600",
    "end": "1009519"
  },
  {
    "text": "parallel and produce your data back to you we have by the way this described in our",
    "start": "1009519",
    "end": "1016800"
  },
  {
    "text": "big data blog and i'll be coming back to this back data block again in the presentation certainly",
    "start": "1016800",
    "end": "1022959"
  },
  {
    "text": "encourage you to visit aws big data blog a slight upgrade on this approach is",
    "start": "1022959",
    "end": "1029678"
  },
  {
    "text": "still based on on hadoop streaming but it's in a more",
    "start": "1029679",
    "end": "1035520"
  },
  {
    "text": "r user-friendly format are the libraries or the project packages by revolution",
    "start": "1035520",
    "end": "1041120"
  },
  {
    "text": "analytics as part of our hadoop to run our hadoop you would need to",
    "start": "1041120",
    "end": "1047120"
  },
  {
    "text": "download the packages install them on your emr cluster and then this will allow you to define your",
    "start": "1047120",
    "end": "1053440"
  },
  {
    "text": "entire hadoop job more in our terms instead of writing it directly in hadoop streaming",
    "start": "1053440",
    "end": "1060000"
  },
  {
    "text": "we have that also well described in great detail in our big data blog i certainly encourage you to visit that",
    "start": "1060000",
    "end": "1066320"
  },
  {
    "text": "it's certainly another capability for running our own on emr available to you and in addition",
    "start": "1066320",
    "end": "1072400"
  },
  {
    "text": "you can install uh the rstudio server uh for interactive",
    "start": "1072400",
    "end": "1077760"
  },
  {
    "text": "visualization and manipulation next let's talk about machine learning",
    "start": "1077760",
    "end": "1086000"
  },
  {
    "text": "as distinct from r machaut was developed as a hadoop",
    "start": "1087919",
    "end": "1093520"
  },
  {
    "text": "java application or java library for machine learning to begin with so it's even easier to run it on emr",
    "start": "1093520",
    "end": "1101919"
  },
  {
    "text": "the library is already installed on the emr nodes",
    "start": "1101919",
    "end": "1107039"
  },
  {
    "text": "you have actually algorithms provided for uh collaborative filtering and we're all familiar with",
    "start": "1107039",
    "end": "1113679"
  },
  {
    "text": "that if we buy online books or watch movies etc we always get recommendations",
    "start": "1113679",
    "end": "1119520"
  },
  {
    "text": "and that's typically based on collaborative filtering libraries for clustering or unsupervised",
    "start": "1119520",
    "end": "1126320"
  },
  {
    "text": "learning and typically that's about grouping data let's say grouping new stories",
    "start": "1126320",
    "end": "1131760"
  },
  {
    "text": "or similar other unsupervised statistics based on carbon parameters and then primary finally",
    "start": "1131760",
    "end": "1138400"
  },
  {
    "text": "classification or supervised learning and again we're all probably familiar with that in terms of spam filtering",
    "start": "1138400",
    "end": "1145440"
  },
  {
    "text": "we know those spam filtering engines they typically are trained on on what is considered",
    "start": "1145440",
    "end": "1150880"
  },
  {
    "text": "spam and what is not and they identify certain phrases or capitalization or other features and then they can",
    "start": "1150880",
    "end": "1157280"
  },
  {
    "text": "recognize what is valid email and what is spam",
    "start": "1157280",
    "end": "1163760"
  },
  {
    "text": "so how do you do uh how do you run mac out on emr very easy all you need to do",
    "start": "1163760",
    "end": "1168799"
  },
  {
    "text": "is really upload your data uh typically in s3 or any custom",
    "start": "1168799",
    "end": "1173919"
  },
  {
    "text": "application that you built in java based on based on the macauld library and then",
    "start": "1173919",
    "end": "1179120"
  },
  {
    "text": "you can really trigger uh the required machaut algorithms in emr",
    "start": "1179120",
    "end": "1184799"
  },
  {
    "text": "as you would run any any any jar on emr and these are some examples both",
    "start": "1184799",
    "end": "1190160"
  },
  {
    "text": "from our blog i certainly encourage you to visit i believe this blog was posted from our partners by accenture and i certainly",
    "start": "1190160",
    "end": "1197360"
  },
  {
    "text": "encourage you to submit any of your successes with aws technology in big",
    "start": "1197360",
    "end": "1202559"
  },
  {
    "text": "data to our blog as well or you can visit also the example from",
    "start": "1202559",
    "end": "1208080"
  },
  {
    "text": "apache mahout on on the news groups filtering finally",
    "start": "1208080",
    "end": "1214240"
  },
  {
    "start": "1212000",
    "end": "1356000"
  },
  {
    "text": "apache giraffe i kept this one for last it's probably the most interesting",
    "start": "1214240",
    "end": "1219280"
  },
  {
    "text": "application just given the size if nothing else it was developed by yahoo as an open",
    "start": "1219280",
    "end": "1225520"
  },
  {
    "text": "source project it was based on google pragal article years ago which google leveraged",
    "start": "1225520",
    "end": "1232159"
  },
  {
    "text": "for page rank so right there this is probably can catch your attention",
    "start": "1232159",
    "end": "1237760"
  },
  {
    "text": "facebook has publicized their use of giraffe over their entire",
    "start": "1237760",
    "end": "1242880"
  },
  {
    "text": "friendship graph so we're talking upwards of a billion vertices here and upwards of a hundred",
    "start": "1242880",
    "end": "1249919"
  },
  {
    "text": "billion edges i certainly encourage you to explore that link it's here on the on",
    "start": "1249919",
    "end": "1254960"
  },
  {
    "text": "the presentation and you'll have access to it so what's the essence the essence is really",
    "start": "1254960",
    "end": "1261200"
  },
  {
    "text": "a bulk synchronous parallel machine which is really based on the algorithms",
    "start": "1261200",
    "end": "1266559"
  },
  {
    "text": "from the point of view of a single vertex and and how they receive data signals from other vertices and then pass it",
    "start": "1266559",
    "end": "1272960"
  },
  {
    "text": "through to to the next layer and then zoo keeper enforces uh these atomic farrier",
    "start": "1272960",
    "end": "1279200"
  },
  {
    "text": "barriers between execution and i have a a little uh animation here to kind of",
    "start": "1279200",
    "end": "1285280"
  },
  {
    "text": "shows you how in parallel all these vertices are calculated and the entire state moves very fast it's executed in",
    "start": "1285280",
    "end": "1292240"
  },
  {
    "text": "memory typically so right off the bat you can see how mapreduce is probably not the best fit here",
    "start": "1292240",
    "end": "1298720"
  },
  {
    "text": "so how did the giraffe developers leverage hadoop to execute",
    "start": "1298720",
    "end": "1306240"
  },
  {
    "text": "giraffe well really they first developed it for",
    "start": "1306240",
    "end": "1311679"
  },
  {
    "text": "hadoop one and then for hadoop too on hadoop one they actually implemented this as one",
    "start": "1311679",
    "end": "1316880"
  },
  {
    "text": "single mapper so the entire processing was happening in one single mapper with hadoop 2 or",
    "start": "1316880",
    "end": "1323440"
  },
  {
    "text": "yarn actually it's a custom implementation that leverages resource management from",
    "start": "1323440",
    "end": "1328880"
  },
  {
    "text": "yarn so basically you launch your cluster you configure apache zookeeper and then you",
    "start": "1328880",
    "end": "1336320"
  },
  {
    "text": "download one of the versions of of graph i i picked and i've tried this one you certainly can explore uh",
    "start": "1336320",
    "end": "1343600"
  },
  {
    "text": "the giraffe apache website for that and then you run the giraffe jar on amazon emr again",
    "start": "1343600",
    "end": "1352159"
  },
  {
    "text": "very similar to macaud except you would have to install it",
    "start": "1352159",
    "end": "1356640"
  },
  {
    "text": "so this is just a an overview of a few technologies that complement hadoop in",
    "start": "1357679",
    "end": "1364159"
  },
  {
    "text": "areas where the hadoop is not necessarily the strongest",
    "start": "1364159",
    "end": "1369440"
  },
  {
    "text": "framework to use this is not to exclude tools like tess",
    "start": "1369440",
    "end": "1374480"
  },
  {
    "text": "or spark or shark on spark or a variety of other tools which you're welcome to explore in",
    "start": "1374480",
    "end": "1380480"
  },
  {
    "text": "the ecosystem uh the purpose is really to to highlight the capability of cmr to",
    "start": "1380480",
    "end": "1386480"
  },
  {
    "text": "run all these tools so where does redshift and and business intelligence fit in here",
    "start": "1386480",
    "end": "1392799"
  },
  {
    "text": "well imagine you produce your statistics uh and and you want to distribute the data you want to do some real-time",
    "start": "1392799",
    "end": "1399120"
  },
  {
    "text": "analysis r maybe gives you some single user tools for that and and that's terrific but if",
    "start": "1399120",
    "end": "1404960"
  },
  {
    "text": "you want to distribute it to their enterprise share with the with the weather community you probably need",
    "start": "1404960",
    "end": "1411280"
  },
  {
    "text": "real-time analytics with your familiar bi tools and that's where redshift comes in",
    "start": "1411280",
    "end": "1417760"
  },
  {
    "start": "1415000",
    "end": "1454000"
  },
  {
    "text": "and we had great presentations here at trainvent about redshift i want to highlight just two points",
    "start": "1417760",
    "end": "1423919"
  },
  {
    "text": "first of all is the leader node and the leader node speaks postgresql",
    "start": "1423919",
    "end": "1429679"
  },
  {
    "text": "it's it's not complete language but it's very close to complete and then it manages your metadata and",
    "start": "1429679",
    "end": "1436320"
  },
  {
    "text": "and coordinates your queries and then you have the compute nodes which are highly distributed all the way to 128",
    "start": "1436320",
    "end": "1442960"
  },
  {
    "text": "nodes and those nodes they took they can input data and store data in parallel",
    "start": "1442960",
    "end": "1448960"
  },
  {
    "text": "2s3 from s3 they can interact with dynamodb emr kinesis",
    "start": "1448960",
    "end": "1455200"
  },
  {
    "start": "1454000",
    "end": "1482000"
  },
  {
    "text": "so how do you typically and how do our customers typically leverage uh",
    "start": "1455200",
    "end": "1460240"
  },
  {
    "text": "redshift well once you have your achieve cluster with your data which is easily uploadable",
    "start": "1460240",
    "end": "1466720"
  },
  {
    "text": "from emr or from s3 you can appoint all your current analytic tools and",
    "start": "1466720",
    "end": "1473279"
  },
  {
    "text": "that's just a subset of them we keep adding new partners that support redshift",
    "start": "1473279",
    "end": "1478559"
  },
  {
    "text": "via the jdbc or odbc connector what is the best tool to get those to",
    "start": "1478559",
    "end": "1485279"
  },
  {
    "start": "1482000",
    "end": "1518000"
  },
  {
    "text": "what's the best way to get those tools well we have the aws marketplace how many of you are using marketplace today",
    "start": "1485279",
    "end": "1492000"
  },
  {
    "text": "i hope many uh it's a very established service we have",
    "start": "1492000",
    "end": "1497520"
  },
  {
    "text": "almost 2000 products there not surprisingly bi tools are some of the most popular along with security and",
    "start": "1497520",
    "end": "1504240"
  },
  {
    "text": "networking tools and uh basically you can start an ec2 instance with the tool",
    "start": "1504240",
    "end": "1510000"
  },
  {
    "text": "of choice and licensing is either bring your own license or or by the hour and",
    "start": "1510000",
    "end": "1515679"
  },
  {
    "text": "you can start doing analytics right away as an example and as a segue to marco's presentation",
    "start": "1515679",
    "end": "1522640"
  },
  {
    "start": "1518000",
    "end": "1549000"
  },
  {
    "text": "mongodb is available in the marketplace so let's say you may want to use a nosql database",
    "start": "1522640",
    "end": "1530720"
  },
  {
    "text": "which is not hbase or it's not dynamodb and you like  for its document management",
    "start": "1530720",
    "end": "1536000"
  },
  {
    "text": "features you can easily start it from the marketplace look at the reference architecture that",
    "start": "1536000",
    "end": "1542480"
  },
  {
    "text": "we offer even launch the aws cloud front uh to implement it immediately in aws",
    "start": "1542480",
    "end": "1549840"
  },
  {
    "start": "1549000",
    "end": "1581000"
  },
  {
    "text": "uh final product example cloudera enterprise data hub we often get the",
    "start": "1549840",
    "end": "1555440"
  },
  {
    "text": "question hey what about what if i want to like run hadoop for the cloudera version the cloudera distribution you",
    "start": "1555440",
    "end": "1562159"
  },
  {
    "text": "can absolutely certainly do that on aws in fact we have a cloud formation quick",
    "start": "1562159",
    "end": "1567200"
  },
  {
    "text": "start for you to enable you cloudera fully supports enterprise data hub on aws including",
    "start": "1567200",
    "end": "1574320"
  },
  {
    "text": "cloudera director and caldera manager and they provide also reference architectures and best practices",
    "start": "1574320",
    "end": "1581840"
  },
  {
    "start": "1581000",
    "end": "1627000"
  },
  {
    "text": "so finally uh big data workflow so typically after you get your data to",
    "start": "1581840",
    "end": "1588720"
  },
  {
    "text": "s3 and that could be either by shipping hard drives or using s3 uploads or",
    "start": "1588720",
    "end": "1595520"
  },
  {
    "text": "through our networking products like vpn or direct connect",
    "start": "1595520",
    "end": "1601840"
  },
  {
    "text": "you may have also some aws sources of data like rds glacier archive data or live",
    "start": "1601840",
    "end": "1609440"
  },
  {
    "text": "streaming data from kinesis the point is typically all of this data has to go",
    "start": "1609440",
    "end": "1615120"
  },
  {
    "text": "into s3 as your data hub and then from there you can leverage gmr you can",
    "start": "1615120",
    "end": "1620320"
  },
  {
    "text": "leverage other custom ec2 implementations or dynamodb and final analysis is typically",
    "start": "1620320",
    "end": "1626799"
  },
  {
    "text": "in redshift so real quickly how does this end-to-end workflow look like",
    "start": "1626799",
    "end": "1632559"
  },
  {
    "text": "the sources could be your data center they could be in the aws cloud or they",
    "start": "1632559",
    "end": "1637840"
  },
  {
    "text": "could be public sources like marco will be discussing in in the aviation space once you collect your data in s3",
    "start": "1637840",
    "end": "1645200"
  },
  {
    "text": "you can do some etl work in emr you can do analytical and bi processing",
    "start": "1645200",
    "end": "1651919"
  },
  {
    "text": "in in redshift or or dynamodb as you need to we provide the automation tools of",
    "start": "1651919",
    "end": "1658000"
  },
  {
    "text": "simple workflow and data pipeline and i'm not going to go in detail but we had presentations on",
    "start": "1658000",
    "end": "1664240"
  },
  {
    "text": "those topics suffice to say a simple workflow gives you deep",
    "start": "1664240",
    "end": "1669279"
  },
  {
    "text": "programmatic capabilities for parallel workflows with coordination and rollbacks and data",
    "start": "1669279",
    "end": "1675279"
  },
  {
    "text": "pipeline is is optimized it's based on on simple workflow capabilities but it's",
    "start": "1675279",
    "end": "1680559"
  },
  {
    "text": "optimized for moving data and it's more ui driven as opposed to programmatic",
    "start": "1680559",
    "end": "1685600"
  },
  {
    "text": "and then finally you can leverage odbc jdbc to redshift",
    "start": "1685600",
    "end": "1690799"
  },
  {
    "text": "or through hive or or shark over emr clusters to visualize data",
    "start": "1690799",
    "end": "1697120"
  },
  {
    "text": "within your data centers or outside within aws you can certainly replace some of the",
    "start": "1697120",
    "end": "1703600"
  },
  {
    "text": "components with open source tools we discussed mongodb or or",
    "start": "1703600",
    "end": "1708880"
  },
  {
    "text": "cloudera distribution and actually this leads me to the architecture very similar to what marco has used",
    "start": "1708880",
    "end": "1715360"
  },
  {
    "text": "which consists of data collection from public data into s3 leveraging emr and",
    "start": "1715360",
    "end": "1720880"
  },
  {
    "text": "mongodb and node.js the conclusion of my part big data is",
    "start": "1720880",
    "end": "1726480"
  },
  {
    "start": "1723000",
    "end": "1751000"
  },
  {
    "text": "really accelerating exponentially aws helps you address the three v's",
    "start": "1726480",
    "end": "1731840"
  },
  {
    "text": "it's easy to extend emrs hadoop in the cloud and redchif gives you",
    "start": "1731840",
    "end": "1737279"
  },
  {
    "text": "actionable real-time analytics and aws provides an agile big data platform thank you",
    "start": "1737279",
    "end": "1744159"
  },
  {
    "text": "thanks sir thank you thank you nikolai so uh",
    "start": "1744880",
    "end": "1752399"
  },
  {
    "start": "1751000",
    "end": "1823000"
  },
  {
    "text": "yeah so i'm uh i'm marco mars i work for for icao it's a aviation civil aviation",
    "start": "1752399",
    "end": "1757840"
  },
  {
    "text": "organization of the un so uh i work there in the safety and efficiency business unit so",
    "start": "1757840",
    "end": "1764159"
  },
  {
    "text": "we were pretty busy this year like with these malaysian accidents you know these things",
    "start": "1764159",
    "end": "1769279"
  },
  {
    "text": "uh everybody's saying oh how it's possible you lose a plane you know there's all the data which is there so you see already the the big data thing",
    "start": "1769279",
    "end": "1776240"
  },
  {
    "text": "uh behind okay and there's also the ebola for some ebola virus which goes through the network of the planes you",
    "start": "1776240",
    "end": "1782960"
  },
  {
    "text": "know so everything is around this um data analysis so how are you how are you using the cloud right now",
    "start": "1782960",
    "end": "1789679"
  },
  {
    "text": "it's actually we still try to figure out our policy but for a principal we think that you know everything we can get from the",
    "start": "1789679",
    "end": "1795520"
  },
  {
    "text": "cloud we can still leave it in the cloud you know this seems obvious but i've seen so many times you know people they",
    "start": "1795520",
    "end": "1800960"
  },
  {
    "text": "send something from the cloud and then they put it somewhere in a vault down in the basement you know so what's the",
    "start": "1800960",
    "end": "1806159"
  },
  {
    "text": "point when we produce something in-house we would say okay either it is for public",
    "start": "1806159",
    "end": "1811600"
  },
  {
    "text": "use then we put it in the cloud as a synchronization or if it's not like for some financial data or human resources",
    "start": "1811600",
    "end": "1819279"
  },
  {
    "text": "thing for our staff so that we would typically keep for things we produce in-house",
    "start": "1819279",
    "end": "1826480"
  },
  {
    "text": "we would say okay we have a simple simple interface where people entering data and we are establishing some",
    "start": "1826480",
    "end": "1832080"
  },
  {
    "text": "synchronizations you know with the cloud so that we can leverage the the power of the cloud meaning",
    "start": "1832080",
    "end": "1838480"
  },
  {
    "text": "distributing the data to thousands of people and on the other side having something",
    "start": "1838480",
    "end": "1844080"
  },
  {
    "text": "more like basic for the people who enter enter data which can be which kindly knows",
    "start": "1844080",
    "end": "1849360"
  },
  {
    "text": "so with emr i would just give you an example so nikolai asked me to show you",
    "start": "1849360",
    "end": "1854399"
  },
  {
    "start": "1850000",
    "end": "1885000"
  },
  {
    "text": "a little bit of what we do and i give you an example it's related to accident statistics okay for aviation safety is",
    "start": "1854399",
    "end": "1860159"
  },
  {
    "text": "basically about accidents so if you count them you can trend them you can calculate",
    "start": "1860159",
    "end": "1865840"
  },
  {
    "text": "rates and typically what we did until now was we did that at the end of the year okay",
    "start": "1865840",
    "end": "1871120"
  },
  {
    "text": "because we took us a year to to combine all these things well there are sources on the web and even our um",
    "start": "1871120",
    "end": "1878240"
  },
  {
    "text": "we buy data from suppliers which is updated daily so we could actually on a daily basis provide",
    "start": "1878240",
    "end": "1885679"
  },
  {
    "text": "um provide real-time real-time data on accidents so this is something we want to build so",
    "start": "1885679",
    "end": "1891919"
  },
  {
    "text": "this is our actually a snapshot of our website where i give this is an example of an accident so it's in uh september",
    "start": "1891919",
    "end": "1899279"
  },
  {
    "text": "september this year in somalia focus 50 on a landing accident so this information",
    "start": "1899279",
    "end": "1905120"
  },
  {
    "text": "is kind of it's kind of unique but then you see i have different uh stories or what we",
    "start": "1905120",
    "end": "1910720"
  },
  {
    "text": "call narratives and you see there i've i have four different sources so four different sources telling me something",
    "start": "1910720",
    "end": "1917200"
  },
  {
    "text": "about this event some of these sources they tell me a little others they tell me a lot okay and they do not",
    "start": "1917200",
    "end": "1923440"
  },
  {
    "text": "necessarily tell me the same thing okay some may say okay this happened in somalia and the other one may say yeah but no it happened actually the country",
    "start": "1923440",
    "end": "1930880"
  },
  {
    "text": "next to it okay so when we combine this so when i collect this and i'm mapping it",
    "start": "1930880",
    "end": "1937200"
  },
  {
    "text": "uh i'm mapping for my sources um i have to define a key when you see what i'm using as a key i'm using the",
    "start": "1937200",
    "end": "1943519"
  },
  {
    "text": "date and the registration number of the plane which is kind of for me the unique thing and then i'm reducing this and to",
    "start": "1943519",
    "end": "1949760"
  },
  {
    "text": "reduce it i have to set up priorities and i will show you how i'm doing that because if i have four sources telling",
    "start": "1949760",
    "end": "1954799"
  },
  {
    "text": "me something and they don't tell me the same thing i have to decide at one point what do i take and there's a notion of",
    "start": "1954799",
    "end": "1960559"
  },
  {
    "text": "priority and i'm have put that in the reducer and then we publish this on the website and looks like this",
    "start": "1960559",
    "end": "1967519"
  },
  {
    "start": "1967000",
    "end": "1994000"
  },
  {
    "text": "so typically i have problems with input formats okay so i'm sure you have the",
    "start": "1967519",
    "end": "1972559"
  },
  {
    "text": "same thing i have some of my sources they send me xml okay xml which i get like this and other sources they send me",
    "start": "1972559",
    "end": "1979440"
  },
  {
    "text": "csv files okay so everything has to go at the end of the day as to go into emr you know and",
    "start": "1979440",
    "end": "1985279"
  },
  {
    "text": "erma typically likes the thing when it's like the data is like line by line okay and xml is not line by line okay so",
    "start": "1985279",
    "end": "1993200"
  },
  {
    "text": "we have to kind of pre-treat this so we do that when we do our so in the collection process i have an",
    "start": "1993200",
    "end": "1999760"
  },
  {
    "start": "1994000",
    "end": "2048000"
  },
  {
    "text": "ec2 instance which runs some chrome jobs and so every day i kind of do some w",
    "start": "1999760",
    "end": "2005600"
  },
  {
    "text": "gets there from my source and these wgets they so they get whatever",
    "start": "2005600",
    "end": "2011200"
  },
  {
    "text": "the source provides and we have figured out that",
    "start": "2011200",
    "end": "2017279"
  },
  {
    "text": "we need to some some processing afterwards just these tr things so i just wanted to show that",
    "start": "2017279",
    "end": "2023279"
  },
  {
    "text": "there are some trrs and stds here this is just to have the data in line afterwards because as i said emr likes",
    "start": "2023279",
    "end": "2029600"
  },
  {
    "text": "when the data is in line and if you have an xml you have to get rid of the line breaks and then you have in each line you have",
    "start": "2029600",
    "end": "2036080"
  },
  {
    "text": "an one element so at the end of this i have something in s3 which is line by",
    "start": "2036080",
    "end": "2041600"
  },
  {
    "text": "line it can be csv or json or xml doesn't matter but it's it's line online",
    "start": "2041600",
    "end": "2049360"
  },
  {
    "start": "2048000",
    "end": "2102000"
  },
  {
    "text": "and then i'm launching every day once i have collected these things i'm launching my my ermr command line so",
    "start": "2049919",
    "end": "2056638"
  },
  {
    "text": "as you see i'm using node i'm not using r when you're using no but i i'm kind of a javascript guy i like that so uh",
    "start": "2056639",
    "end": "2063200"
  },
  {
    "text": "and i i don't like learning new new things necessarily if i don't need to already learning emr was not actually",
    "start": "2063200",
    "end": "2068560"
  },
  {
    "text": "not very easy so i was happy to do that so i was sticking to javascript it works pretty",
    "start": "2068560",
    "end": "2073760"
  },
  {
    "text": "fine and so because i know i like javascript i also like mongodb and so i'm putting",
    "start": "2073760",
    "end": "2080720"
  },
  {
    "text": "afterwards my results in mongodb and to do that well i need to put my key so i also highlight this here because that's",
    "start": "2080720",
    "end": "2086638"
  },
  {
    "text": "maybe some problems you also encounter sometime that your cluster cannot communicate to your servers well it's",
    "start": "2086639",
    "end": "2092398"
  },
  {
    "text": "because there's no key on it so when i launch it i'm actually putting my key okay",
    "start": "2092399",
    "end": "2098240"
  },
  {
    "text": "so well that took me some time to figure out but just showing",
    "start": "2098240",
    "end": "2104160"
  },
  {
    "start": "2102000",
    "end": "2130000"
  },
  {
    "text": "so this is my configuration of the of the emr and yeah basically i'm i'm just running a",
    "start": "2104400",
    "end": "2111599"
  },
  {
    "text": "map mapreduce here that's my my my step and then i have another step which",
    "start": "2111599",
    "end": "2117359"
  },
  {
    "text": "which i then have created a script which takes my result my emr output and puts",
    "start": "2117359",
    "end": "2122720"
  },
  {
    "text": "it in my my database and because i do this i need to have the key",
    "start": "2122720",
    "end": "2128839"
  },
  {
    "text": "okay um so for the for the mapper",
    "start": "2128839",
    "end": "2133920"
  },
  {
    "text": "for the mapper you saw that i had to define a key and my key is not unique i mean i have i'm getting it an accident",
    "start": "2133920",
    "end": "2139520"
  },
  {
    "text": "information and it has a date it has models of planes operators a lot of stuff but my key basically is the",
    "start": "2139520",
    "end": "2145839"
  },
  {
    "text": "combination of the date and the registration arm of the plane so actually i'm building uh so",
    "start": "2145839",
    "end": "2152160"
  },
  {
    "text": "i'm actually building a key was an if they find a certain rule where it was in with a hash in the middle so",
    "start": "2152160",
    "end": "2157920"
  },
  {
    "text": "that becomes that becomes my key and when i'm mapping i'm actually only",
    "start": "2157920",
    "end": "2163119"
  },
  {
    "text": "interested let's say here's an example where i'm taking the date the date the registration number the model of the",
    "start": "2163119",
    "end": "2168480"
  },
  {
    "text": "plane the source so which sources at one of my four sources and then the priority i have given to",
    "start": "2168480",
    "end": "2174400"
  },
  {
    "text": "that source okay and so i have built my data and i'm i'm doing",
    "start": "2174400",
    "end": "2180800"
  },
  {
    "text": "my output so i'm using the key plus in a stringified json of this of this element",
    "start": "2180800",
    "end": "2188160"
  },
  {
    "text": "so in this mapper here i have actually per source i have a function so if it's xml",
    "start": "2188160",
    "end": "2195280"
  },
  {
    "text": "it takes the source mapper of the xml thing if it's csv takes the csv and",
    "start": "2195280",
    "end": "2201440"
  },
  {
    "text": "so that's actually in the in the mapper so after that i have my things mapped so",
    "start": "2201440",
    "end": "2207520"
  },
  {
    "text": "i have i got rid of my xml my csv my json i have everything now and a key and then plus a json",
    "start": "2207520",
    "end": "2214240"
  },
  {
    "text": "stringified and of course that emr what takes this and",
    "start": "2214240",
    "end": "2220000"
  },
  {
    "text": "sorts it and then sends it back to my to my reducer well my reducer it's actually not a very",
    "start": "2220000",
    "end": "2226160"
  },
  {
    "text": "straightforward thing okay because normally you could say okay i have my keys and then they are if it's the same",
    "start": "2226160",
    "end": "2232240"
  },
  {
    "text": "key um then i'm just i'm just reducing it you know or",
    "start": "2232240",
    "end": "2237440"
  },
  {
    "text": "but here it's a little bit different because i have the different priorities inside you know so i have to collect",
    "start": "2237440",
    "end": "2243280"
  },
  {
    "text": "everything which is the same key meaning the same plane and date so i maybe have four like i have four",
    "start": "2243280",
    "end": "2248880"
  },
  {
    "text": "sources maybe each of one gives tells me something so i have four things so i'm just collecting them in the reducer i'm putting that in an array and",
    "start": "2248880",
    "end": "2256480"
  },
  {
    "text": "once i have i have everything like the the key has changed",
    "start": "2256480",
    "end": "2261520"
  },
  {
    "text": "so i have everything then i'm kind of sorting this sorting is by priority so i'm getting priority one two three four",
    "start": "2261520",
    "end": "2268640"
  },
  {
    "text": "in that order and then i'm filling up i'm filling up my",
    "start": "2268640",
    "end": "2273760"
  },
  {
    "text": "my let's say my combined result okay so it's actually my source",
    "start": "2273760",
    "end": "2278880"
  },
  {
    "text": "number one which will take ownership of maybe this the date or the state of state of uh registry",
    "start": "2278880",
    "end": "2285440"
  },
  {
    "text": "if the source number one gives me that it will be taken the other ones they will be in yours so that's that's why it has to be a",
    "start": "2285440",
    "end": "2291520"
  },
  {
    "text": "little bit different so i'm using this priority and sorting inside",
    "start": "2291520",
    "end": "2297320"
  },
  {
    "text": "so how how do i do this at the end of the day well i have my ec tools which then uh getting my data from from",
    "start": "2299839",
    "end": "2306320"
  },
  {
    "text": "the web of different places it puts in s3 i'm running my emr i have my result uh",
    "start": "2306320",
    "end": "2314000"
  },
  {
    "text": "in in mongodb and i have written an again with node uh written a web server",
    "start": "2314000",
    "end": "2320640"
  },
  {
    "text": "and the web server then provides these kind of examples yeah",
    "start": "2320640",
    "end": "2326160"
  },
  {
    "text": "so the last uh the last one is typically an accident a daily accident rate chart",
    "start": "2326160",
    "end": "2332000"
  },
  {
    "text": "which we were able to generate so every day we were able to give the number of accidents we had this year with a",
    "start": "2332000",
    "end": "2338320"
  },
  {
    "text": "projection to the end of the year and we can compare to last year and we're able to to give indicators like if",
    "start": "2338320",
    "end": "2344880"
  },
  {
    "text": "it's getting better or if it's getting better or worse compared to last year like this ones",
    "start": "2344880",
    "end": "2350480"
  },
  {
    "text": "so these are real-time real-time statistics so this is just an example of",
    "start": "2350480",
    "end": "2356400"
  },
  {
    "text": "of other processes uh other processes we have this is for example audit results will be doing the same thing we have",
    "start": "2356400",
    "end": "2362640"
  },
  {
    "text": "audit results which are stored somewhere safety audits where we are going and auditing the whole world and this is a",
    "start": "2362640",
    "end": "2368640"
  },
  {
    "text": "view of an uh safety audit of a country this is the same thing we do fit traffic",
    "start": "2368640",
    "end": "2373760"
  },
  {
    "text": "information this is like flights going from a country to another",
    "start": "2373760",
    "end": "2379119"
  },
  {
    "text": "country and this is for example the same thing on uh looking at data we're getting from",
    "start": "2379119",
    "end": "2385839"
  },
  {
    "text": "another source on uh maybe you know flight radar 24 or radar box 24 it's",
    "start": "2385839",
    "end": "2391359"
  },
  {
    "text": "where you see live planes flying flying around so you can connect actually to that data and",
    "start": "2391359",
    "end": "2397040"
  },
  {
    "text": "and we also do that the same way uh same way like here so we put it in an emr and then the result in this case is",
    "start": "2397040",
    "end": "2403599"
  },
  {
    "text": "an uh flow like this is the airport of uh of london heathrow uh over 48 hours so it's uh tracks from",
    "start": "2403599",
    "end": "2411920"
  },
  {
    "text": "planes you see that they they're coming from different directions and what we're looking at is if there's some sort of",
    "start": "2411920",
    "end": "2417520"
  },
  {
    "text": "nodes like this you know it's like holding patterns so that's what we're looking looking for so again this is kind of almost real-time real-time",
    "start": "2417520",
    "end": "2424000"
  },
  {
    "text": "things we can we can do and it's clear that without without the cloud and without emr we",
    "start": "2424000",
    "end": "2431200"
  },
  {
    "text": "would not be able to to to process uh process these things you know and um so this is for us really just uh",
    "start": "2431200",
    "end": "2438400"
  },
  {
    "text": "just the beginning the beginning of uh our journey here you know so",
    "start": "2438400",
    "end": "2443520"
  },
  {
    "text": "i give the flow back to nikolai please join me in congratulating",
    "start": "2443520",
    "end": "2450240"
  },
  {
    "start": "2444000",
    "end": "2532000"
  },
  {
    "text": "thank you marco in congratulating marco in the in this",
    "start": "2450240",
    "end": "2455680"
  },
  {
    "text": "uh terrific and uh uh long-reaching research into aviation safety by leveraging aws tools uh in closure i",
    "start": "2455680",
    "end": "2464319"
  },
  {
    "text": "just would like to highlight our big data blog just google it uh it or search it online",
    "start": "2464319",
    "end": "2470480"
  },
  {
    "text": "with any engine you like aws data blog and a lot of actionable very detailed",
    "start": "2470480",
    "end": "2476880"
  },
  {
    "text": "technical information which you can put to use and certainly welcome your submissions to this blog",
    "start": "2476880",
    "end": "2482960"
  },
  {
    "text": "to share with the big data community i would like to highlight also a couple",
    "start": "2482960",
    "end": "2488000"
  },
  {
    "text": "of the presentations here in reinvent they are already passed we're one of the",
    "start": "2488000",
    "end": "2493920"
  },
  {
    "text": "later sessions so please check out the materials later online",
    "start": "2493920",
    "end": "2498960"
  },
  {
    "text": "both slideshare and youtube and you can launch an application you can",
    "start": "2498960",
    "end": "2504400"
  },
  {
    "text": "see how to do that in big data 205 uh you can see the",
    "start": "2504400",
    "end": "2509440"
  },
  {
    "text": "lessons learned both from cloudera and emr in big data 305 and then last but not least the best",
    "start": "2509440",
    "end": "2517280"
  },
  {
    "text": "practices at netflix we all can learn from that so with that i would like to conclude",
    "start": "2517280",
    "end": "2522480"
  },
  {
    "text": "and thank you and please give us your feedback we look forward to it big data 302 thank you",
    "start": "2522480",
    "end": "2529119"
  },
  {
    "text": "thanks",
    "start": "2529119",
    "end": "2532119"
  }
]