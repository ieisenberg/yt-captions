[
  {
    "start": "0",
    "end": "59000"
  },
  {
    "text": "hello welcome thanks for coming seat right down in the front you",
    "start": "2719",
    "end": "8960"
  },
  {
    "text": "just see get front row my name is Rob Alexander I'm a Solutions architect based out of Seattle GNA be talking for",
    "start": "8960",
    "end": "16118"
  },
  {
    "text": "the next solid hour about elastic Block store and uh covering everything from",
    "start": "16119",
    "end": "22800"
  },
  {
    "text": "what makes up the service all the characteristics of it performance best",
    "start": "22800",
    "end": "28320"
  },
  {
    "text": "practices around reliability ility security and performance um I'm going to try to save",
    "start": "28320",
    "end": "35399"
  },
  {
    "text": "some time at the end for questions if I don't I promise I'll stay in the back uh hang out in the hallway and answer all",
    "start": "35399",
    "end": "41280"
  },
  {
    "text": "your questions that you have so let's uh let's get into it uh real quick a piece of housekeeping uh the folks that",
    "start": "41280",
    "end": "47840"
  },
  {
    "text": "actually build our St services our storage services are uh having an office hours details are up",
    "start": "47840",
    "end": "55039"
  },
  {
    "text": "there please go check that out if you want to talk to the engineers and first I wanted to kind of",
    "start": "55039",
    "end": "60960"
  },
  {
    "start": "59000",
    "end": "59000"
  },
  {
    "text": "Orient ourselves in in the world of storage so so we know what space we're playing with with block storage so um",
    "start": "60960",
    "end": "68000"
  },
  {
    "text": "three large categories of of storage service options so the first and what we're talking about today is block",
    "start": "68000",
    "end": "74159"
  },
  {
    "text": "storage where you know an operating system interacts with a device at a bite",
    "start": "74159",
    "end": "80840"
  },
  {
    "text": "level and transfers data on fixed size",
    "start": "80840",
    "end": "86000"
  },
  {
    "text": "block so that is best represented by Amazon elastic Block store which is what",
    "start": "86000",
    "end": "91119"
  },
  {
    "text": "we're talking about today file based storage system something like Amazon elastic file",
    "start": "91119",
    "end": "96840"
  },
  {
    "text": "system moves up a step in the abstraction and instead of direct uh",
    "start": "96840",
    "end": "103960"
  },
  {
    "text": "bite level interaction with a storage device you're now talking file system semantics so that's something abstracted",
    "start": "103960",
    "end": "109920"
  },
  {
    "text": "by a network file system like NFS and the best representation at AWS is the",
    "start": "109920",
    "end": "115600"
  },
  {
    "text": "EFS and finally object storage where you move up yet another degree an abstraction where your application is",
    "start": "115600",
    "end": "121520"
  },
  {
    "text": "actually talking at preferably with a restful interface HTTP to a web service",
    "start": "121520",
    "end": "128679"
  },
  {
    "text": "where you're storing uh objects with apis very simple API calls get put delete objects and obviously Amazon's S3",
    "start": "128679",
    "end": "136280"
  },
  {
    "text": "is our key service offering here with Glacier also being one of our Object Store",
    "start": "136280",
    "end": "141440"
  },
  {
    "text": "offerings So within the the block storage world at AWS we kind of divide it into three specific block storage",
    "start": "141440",
    "end": "148879"
  },
  {
    "text": "offerings the first of those being uh what was our first block store offering which",
    "start": "148879",
    "end": "154280"
  },
  {
    "text": "launched with ec2 back in 2006 over 10 years ago which is ec2 instance",
    "start": "154280",
    "end": "161040"
  },
  {
    "start": "161000",
    "end": "161000"
  },
  {
    "text": "store an instance store is the actual physical discs that are local to the",
    "start": "161040",
    "end": "166239"
  },
  {
    "text": "physical host hosting your instance so the key characteristics of",
    "start": "166239",
    "end": "172680"
  },
  {
    "text": "ec2 instance store is that it's non-persistent it it only exists for the life of the instance so if you stop the",
    "start": "172680",
    "end": "179400"
  },
  {
    "text": "instance or you terminate the instance whatever data is on that local store is is gone um it will obviously endure a",
    "start": "179400",
    "end": "186280"
  },
  {
    "text": "reboot but any API commands that that that do away with the instance the data goes with it the data is not replicated",
    "start": "186280",
    "end": "192560"
  },
  {
    "text": "by default you can of course uh do some replication schemes on the data itself uh between instances but by default",
    "start": "192560",
    "end": "199200"
  },
  {
    "text": "there's no replication service offered with instance store and same with snapshot support there's no outof thebox",
    "start": "199200",
    "end": "205239"
  },
  {
    "text": "offering for backups that's also something you will have to do for instance store and there is SSD and HDD",
    "start": "205239",
    "end": "212040"
  },
  {
    "text": "offerings for uh instance store and then we get into the into the",
    "start": "212040",
    "end": "218360"
  },
  {
    "text": "E two categories of EBS so we divide that into our SSD back so solid state drive backed volume types and those are",
    "start": "218360",
    "end": "226040"
  },
  {
    "text": "the general purpose 2 and the provisioned iops volume types and then the hddd so the magnetic",
    "start": "226040",
    "end": "234159"
  },
  {
    "text": "spinning disc backed volumes which are the throughput optimized the ST1",
    "start": "234159",
    "end": "240400"
  },
  {
    "text": "and the cold HDD the SC1 so you can see the three three strata there of of block",
    "start": "240400",
    "end": "246439"
  },
  {
    "text": "service offerings so what is EBS EBS is block",
    "start": "246439",
    "end": "252480"
  },
  {
    "text": "storage as a service so with an API call you're able to configure a set amount of",
    "start": "252480",
    "end": "259079"
  },
  {
    "text": "of volume blocks with certain performance characteristics and then attach those to an ec2",
    "start": "259079",
    "end": "267000"
  },
  {
    "text": "instance so at a as a storage service it's obvious you're accessing these",
    "start": "267880",
    "end": "273199"
  },
  {
    "text": "blocks over the network and we'll talk about a lot of those characteristics that make that an important thing to consider with",
    "start": "273199",
    "end": "279199"
  },
  {
    "text": "EBS and it's also very important to note that that EBS is not a volume is not a",
    "start": "279199",
    "end": "284479"
  },
  {
    "text": "specific hard drive if you call an a the API call it create a volume we don't go tag a hard drive that's yours um EBS is",
    "start": "284479",
    "end": "292919"
  },
  {
    "text": "a massive distributed system and uh as a massive distributed",
    "start": "292919",
    "end": "298560"
  },
  {
    "text": "system your volume is a logical volume and it's it's comprised of of blocks",
    "start": "298560",
    "end": "304080"
  },
  {
    "text": "that are distributed across many many physical",
    "start": "304080",
    "end": "309120"
  },
  {
    "text": "devices so that leads to a lot of the performance uh reliability and availability characteristics we'll uh",
    "start": "309320",
    "end": "316039"
  },
  {
    "text": "talk about uh volumes are specific to an availability zone so they need to be uh",
    "start": "316039",
    "end": "323000"
  },
  {
    "text": "accessed by an instance that's in that same availability Zone they can persist independent of the instance itself so",
    "start": "323000",
    "end": "329319"
  },
  {
    "text": "you can attach and detach um you need to unmount those file systems from your",
    "start": "329319",
    "end": "334440"
  },
  {
    "text": "instances first to have a good experience attaching and detaching from the operating systems perspective um but",
    "start": "334440",
    "end": "340560"
  },
  {
    "text": "they you know and and root obviously you're not going going to go detaching that one but um other than those two",
    "start": "340560",
    "end": "347120"
  },
  {
    "text": "caveats uh the volumes themselves are independent of the ec2",
    "start": "347120",
    "end": "352680"
  },
  {
    "text": "instances and you know EBS volume can only be attached to one one instance at a time but you can have many volumes",
    "start": "355960",
    "end": "362080"
  },
  {
    "text": "attached to a single instance so it's very common and we all we recommend to separate out Boot and data volumes and",
    "start": "362080",
    "end": "369639"
  },
  {
    "text": "that's for a lot of reasons one I just mentioned you know you keep the boot nice clean and small and then your data",
    "start": "369639",
    "end": "374960"
  },
  {
    "text": "volumes you can move around very flexibly uh it also allows you to divide up exactly what workloads are running",
    "start": "374960",
    "end": "381919"
  },
  {
    "text": "against which volume",
    "start": "381919",
    "end": "384840"
  },
  {
    "text": "types so so each block that comprises your your EBS logical volume is also replicated within the same availability",
    "start": "388199",
    "end": "397479"
  },
  {
    "text": "Zone and it's important to note that you're only paying for the storage you've allocated for the volume you're",
    "start": "397479",
    "end": "403599"
  },
  {
    "text": "not paying for the replicated data but this leads to some very interesting uh service characteristics",
    "start": "403599",
    "end": "410039"
  },
  {
    "text": "so uh as an availability Target five nines of service availability so access",
    "start": "410039",
    "end": "415639"
  },
  {
    "text": "to your volumes and for durability so so the durability of the data that's on your uh",
    "start": "415639",
    "end": "421840"
  },
  {
    "text": "volumes 0.1 to 0.2 AFR annual failure rate so that's about it's about 20 times",
    "start": "421840",
    "end": "427440"
  },
  {
    "text": "greater than your average you know Enterprise class uh hard dis Drive which is about 4% so that means if you run day",
    "start": "427440",
    "end": "435199"
  },
  {
    "text": "and day out about a thousand EBS volumes for a year you can expect to lose one or",
    "start": "435199",
    "end": "441960"
  },
  {
    "text": "two which is why we also have the snapshot service so these are point in",
    "start": "441960",
    "end": "447000"
  },
  {
    "text": "time backups of the data on your volumes that are stored in S3 which is a regional service it's not tied to a",
    "start": "447000",
    "end": "453400"
  },
  {
    "text": "specific availability Zone and it obviously has a greater magnitude with",
    "start": "453400",
    "end": "458680"
  },
  {
    "text": "11 nines of durability with S3 so you have a much greater magnitude of data",
    "start": "458680",
    "end": "463800"
  },
  {
    "text": "durability uh for your snapshots so a snapshot itself how does",
    "start": "463800",
    "end": "469879"
  },
  {
    "text": "it work so the first time you take a snapshot it's going to copy uh every modified block on your volume to S3 uh",
    "start": "469879",
    "end": "477599"
  },
  {
    "text": "any subsequent snapshots to the same volume we're going to be incremental so they're only going to evaluate what's changed since that first one and move up",
    "start": "477599",
    "end": "484720"
  },
  {
    "text": "those change blocks and deleting snapshots within that volume sequence of of snapshots it",
    "start": "484720",
    "end": "491199"
  },
  {
    "text": "will only remove data that's exclusive to that",
    "start": "491199",
    "end": "495879"
  },
  {
    "text": "snapshot so what can you do with a snapshot so with a snapshot you can create an Amazon machine image so a",
    "start": "498240",
    "end": "504840"
  },
  {
    "start": "499000",
    "end": "499000"
  },
  {
    "text": "blueprint for launching more instances and obviously when you launch those instances they'll have whatever",
    "start": "504840",
    "end": "510039"
  },
  {
    "text": "data and characteristics were part of that snapshot you can uh create new volumes",
    "start": "510039",
    "end": "516839"
  },
  {
    "text": "either in the same availability zone or different availability Zone and you can also create a new size of volume so if",
    "start": "516839",
    "end": "522760"
  },
  {
    "text": "you wanted to take a snapshot and then create a larger volume size you could do that with a",
    "start": "522760",
    "end": "529160"
  },
  {
    "text": "snapshot and then you can copy them to other regions copy them to other accounts this is a very common deal our",
    "start": "532480",
    "end": "539480"
  },
  {
    "text": "strategy to keep you know golden images of all your uh key applications and move them around to other regions for",
    "start": "539480",
    "end": "545640"
  },
  {
    "text": "disaster recovery and you can also share snapshots so you can either share them",
    "start": "545640",
    "end": "552200"
  },
  {
    "text": "with other internal accounts that you have within your company or you can share them publicly and I like to point",
    "start": "552200",
    "end": "557640"
  },
  {
    "text": "out a very good example of this is there's a whole wealth of public data sets that are available as snapshots so",
    "start": "557640",
    "end": "564320"
  },
  {
    "text": "you can literally go and look them up and launch a snapshot that will will load up with data whether that's Census",
    "start": "564320",
    "end": "570360"
  },
  {
    "text": "Data genomic data weather data Transportation data and and it's all snapshot based and obviously all of the",
    "start": "570360",
    "end": "576839"
  },
  {
    "text": "market place amies that you're launching whether that's Amazon Linux or or or or Windows those are all based on and",
    "start": "576839",
    "end": "583560"
  },
  {
    "text": "backed by EBS snapshots so another characteristics of",
    "start": "583560",
    "end": "589079"
  },
  {
    "start": "586000",
    "end": "586000"
  },
  {
    "text": "of the EBS service is EBS",
    "start": "589079",
    "end": "593639"
  },
  {
    "text": "optimized and EBS optimized is about the network bandwidth to to EBS so by",
    "start": "594279",
    "end": "600560"
  },
  {
    "text": "default if I fire up a C3 2x large and I use my favorite bandwidth testing tool",
    "start": "600560",
    "end": "607600"
  },
  {
    "text": "whether that's I perf or net pipe or pick your pick your tool you'll find that a C3 2xa large has about 125",
    "start": "607600",
    "end": "614800"
  },
  {
    "text": "megabytes of throughput of network bandwidth",
    "start": "614800",
    "end": "620000"
  },
  {
    "text": "allocated to it but that bandwidth is shared not just",
    "start": "620000",
    "end": "625600"
  },
  {
    "text": "to the network communication to your EBS volumes but everything else that you might be doing with that instance so",
    "start": "625600",
    "end": "631240"
  },
  {
    "text": "whether that's other ec2 instances or accessing the internet S3 databases that",
    "start": "631240",
    "end": "637639"
  },
  {
    "text": "120 megabytes a second is for everything EBS optimized though if you",
    "start": "637639",
    "end": "643399"
  },
  {
    "text": "enable EBS optimized gives you dedicated bandwidth to",
    "start": "643399",
    "end": "649200"
  },
  {
    "text": "EBS so I basically effectively double my bandwidth there and I get a dedicated 125 megabytes a second specifically to",
    "start": "649200",
    "end": "658040"
  },
  {
    "text": "EBS so this is enabled by default in a lot of our our newer instance types C4s d2s",
    "start": "658040",
    "end": "663880"
  },
  {
    "text": "m4s p2s which means it comes at no extra cost um some of the other uh generation",
    "start": "663880",
    "end": "669480"
  },
  {
    "text": "instances that do not have it enabled by default you can enable it for a nominal cost it can be enabled at instance",
    "start": "669480",
    "end": "675959"
  },
  {
    "text": "launch or you can do it later uh it's just a stop start of the instance to enable it and it's not an option on a",
    "start": "675959",
    "end": "681920"
  },
  {
    "text": "few instance types uh the the top end of the C3 R3 and i2s the 8X LS uh those",
    "start": "681920",
    "end": "688399"
  },
  {
    "text": "have the full 10 gig that was available to the physical host so it's kind of up to you to do what you want with the 10",
    "start": "688399",
    "end": "694920"
  },
  {
    "text": "gig and if you want to know on a per instance type basis is exactly how much bandwidth is allocated to each one for",
    "start": "694920",
    "end": "700639"
  },
  {
    "text": "EBS optimized that's in that link down there at the bottom and the last characteristic of",
    "start": "700639",
    "end": "706440"
  },
  {
    "start": "704000",
    "end": "704000"
  },
  {
    "text": "the service that I like to point out is encryption so encryption with EBS is is just dead simple it's literally a check",
    "start": "706440",
    "end": "713880"
  },
  {
    "text": "and it uh boot or data volumes can be encrypted you can attach both encrypted",
    "start": "713880",
    "end": "719959"
  },
  {
    "text": "and unencrypted volumes to the same instance type it's supported by any of our current generation instance types",
    "start": "719959",
    "end": "725440"
  },
  {
    "text": "there's no volume performance impact um snapshots are encrypted data",
    "start": "725440",
    "end": "730560"
  },
  {
    "text": "at rest is encrypted data at flight is encrypted any snapshot you create from an encrypted snapshot is also encrypted",
    "start": "730560",
    "end": "737639"
  },
  {
    "text": "and there's no extra cost and it literally is a checkbox you see there I'm creating my volume and I'm",
    "start": "737639",
    "end": "743279"
  },
  {
    "text": "going check the encryption off you go so now we covered all the",
    "start": "743279",
    "end": "749880"
  },
  {
    "text": "characteristics of the service let's let's dive into the actual volume types that we mentioned in the",
    "start": "749880",
    "end": "756240"
  },
  {
    "text": "beginning so I I mentioned we have two platforms one based on solid state drives and one based on hard dis drives",
    "start": "756240",
    "end": "762720"
  },
  {
    "text": "and uh you know you might ask why we did that and they have very different uh performance characteristics and very",
    "start": "762720",
    "end": "769160"
  },
  {
    "text": "different physics related to them so solid state discs obviously are Ram base there's no moving Parts um and the Flash",
    "start": "769160",
    "end": "777600"
  },
  {
    "text": "you know the more Banks you stack in in there the more parallel IO you can drive to an SSD and all points on the dis are",
    "start": "777600",
    "end": "784199"
  },
  {
    "text": "equally accessible so they're fantastic for you know random IO you know you're not penalized for any random whereas on",
    "start": "784199",
    "end": "790880"
  },
  {
    "text": "a spinning hard disk you know you have to get that head exactly to where you",
    "start": "790880",
    "end": "796519"
  },
  {
    "text": "need to read and that can take you know seek times or some of the most performance impacting uh remaining",
    "start": "796519",
    "end": "802519"
  },
  {
    "text": "performance blockers in in modern Computing so but if you get that head in",
    "start": "802519",
    "end": "807639"
  },
  {
    "text": "the right place hard dis drives can deliver some serious",
    "start": "807639",
    "end": "812680"
  },
  {
    "text": "sequential performance a sequential throughput and U at a very compelling",
    "start": "812680",
    "end": "817920"
  },
  {
    "text": "price point so there's still a lot of room for the hard dis Drive platforms and that's why you know we still C you",
    "start": "817920",
    "end": "825800"
  },
  {
    "text": "know just this year we came out with two new volume types that are based on spinning hard drives so again we have",
    "start": "825800",
    "end": "832240"
  },
  {
    "text": "the two the gp2 and the i1 for SSD and we have the two for hard dis D ST1 and",
    "start": "832240",
    "end": "839399"
  },
  {
    "text": "one so how do you decide like how you should be using these how do you should you think about these volume types so uh",
    "start": "839399",
    "end": "845839"
  },
  {
    "text": "the question I would pose to you is what do you consider more important for your workload is it",
    "start": "845839",
    "end": "852680"
  },
  {
    "text": "iops or is it throughput or do you not know yet or do",
    "start": "852680",
    "end": "858519"
  },
  {
    "text": "you not really care and that's an easy answer so gp2 is our jack of all trades so gp2 is",
    "start": "858519",
    "end": "867320"
  },
  {
    "text": "uh has a dead simple provision model you tell us how much storage you want and for every gigabyte you provision you get",
    "start": "867320",
    "end": "874600"
  },
  {
    "text": "three iops it has a burst model so up to a terabyte in size you can burst up to",
    "start": "874600",
    "end": "880399"
  },
  {
    "text": "3,000 iops no matter what you provision so you have your Baseline of 1 to three and then you'll have 3,000 to burst over",
    "start": "880399",
    "end": "886800"
  },
  {
    "text": "a terabyte in size you you will get what you have provisioned in Baseline a volume can go up to 160",
    "start": "886800",
    "end": "893240"
  },
  {
    "text": "megabytes a second in throughput single digigit millisecond latencies obviously because we're",
    "start": "893240",
    "end": "898720"
  },
  {
    "text": "dealing with ssds here and the capacity is up to 16 terabytes for single volume uh I will point out all these uh storage",
    "start": "898720",
    "end": "907639"
  },
  {
    "text": "numbers that I am uh giving you are even though I'm saying gigabytes it's GB",
    "start": "907639",
    "end": "913759"
  },
  {
    "text": "bytes and I'm just not going to say that over and over but we provision everything at EBS uh base two so it's",
    "start": "913759",
    "end": "919880"
  },
  {
    "text": "not a typical storage world of base 10 everything you see is base two which means it's you're actually getting about",
    "start": "919880",
    "end": "925560"
  },
  {
    "text": "7% more than what you might think you're getting um and when I talk about iops",
    "start": "925560",
    "end": "930639"
  },
  {
    "text": "with EBS uh we Benchmark everything and and measure everything against a 16 kabyte that's the last time I'm saying",
    "start": "930639",
    "end": "937000"
  },
  {
    "text": "that 16 KB uh block size so when I say uh up to 10,000 iops on a gp2 volume",
    "start": "937000",
    "end": "944199"
  },
  {
    "text": "that's at 16k you can do some quick math 10,000 time 16k is your 160 megabytes a",
    "start": "944199",
    "end": "949759"
  },
  {
    "text": "second of maximum throughput volume so these are great for just about",
    "start": "949759",
    "end": "955440"
  },
  {
    "text": "everything you know they're not our absolute fastest and they're not our absolute cheapest but they fit the",
    "start": "955440",
    "end": "961199"
  },
  {
    "text": "widest range of workloads so they're are default for all of our boot volumes they're great for bursty workloads uh",
    "start": "961199",
    "end": "967759"
  },
  {
    "text": "and they're great if you just you don't know exactly how much iops you need you don't you're not able to say on a very",
    "start": "967759",
    "end": "973319"
  },
  {
    "text": "consistent basis exactly what you need so here's just a a quick diagram of",
    "start": "973319",
    "end": "980920"
  },
  {
    "start": "977000",
    "end": "977000"
  },
  {
    "text": "of how the burst and base works so 100 iops is the minimum so even if you provision a one gig uh volume you're not",
    "start": "980920",
    "end": "987040"
  },
  {
    "text": "going to get three iops to that uh that would not be good 100 iops uh Baseline",
    "start": "987040",
    "end": "992680"
  },
  {
    "text": "uh it ramps up to you see just over three terabytes you get to the max of 10,000 and then that's the burst up to",
    "start": "992680",
    "end": "1000000"
  },
  {
    "text": "3,000 for volume sizes up to a terabyte so you see there at a 300 gig I've got a",
    "start": "1000000",
    "end": "1005959"
  },
  {
    "text": "th000 iops and I can burst to 3,000 and so how do how does this burst model actually work it's based on a",
    "start": "1005959",
    "end": "1013639"
  },
  {
    "start": "1009000",
    "end": "1009000"
  },
  {
    "text": "token bucket and for gp2 you're always is accumulating iops",
    "start": "1013639",
    "end": "1019759"
  },
  {
    "text": "constantly at three iops per every gig gigabyte you've provision into this bucket the bucket can go up to 5 four",
    "start": "1019759",
    "end": "1027038"
  },
  {
    "text": "million credits and it actually starts so all the volumes you create start with a full token",
    "start": "1027039",
    "end": "1033720"
  },
  {
    "text": "bucket and then you can spin that at 3,000 3,000 iops per second which is the",
    "start": "1033720",
    "end": "1040798"
  },
  {
    "text": "burst so burst like um seems like that's a very transient thing it might be think",
    "start": "1042839",
    "end": "1048558"
  },
  {
    "text": "measured in seconds you might be able to get out but it actually is a very uh significant amount of time that you can",
    "start": "1048559",
    "end": "1054360"
  },
  {
    "text": "actually burst so for example a 300 gig volume",
    "start": "1054360",
    "end": "1060640"
  },
  {
    "text": "you can burst solid 3,000 iops for 43 minutes I think burst is actually kind",
    "start": "1060640",
    "end": "1067320"
  },
  {
    "text": "of a misleading term but I'm not in marketing um 500 is an hour and then you know closer you get to",
    "start": "1067320",
    "end": "1074720"
  },
  {
    "text": "terabyte the closer you get to Infinity for the burst so that you get up to to 10 hours of solid burst if you're up on",
    "start": "1074720",
    "end": "1080240"
  },
  {
    "text": "the 900 gig so how do you how do you watch your burst um there's a",
    "start": "1080240",
    "end": "1086200"
  },
  {
    "text": "metric for burst balance and gp2 you can see here I bursted for a solid hour you",
    "start": "1086200",
    "end": "1092360"
  },
  {
    "text": "can see my plateau and that's where I was at 900,000 R right ey Ops over five minutes",
    "start": "1092360",
    "end": "1098039"
  },
  {
    "text": "so these cloudwatch metrics for gp2 and five minute increments so you got to do a little math so that's 3,000 iops and",
    "start": "1098039",
    "end": "1104960"
  },
  {
    "text": "you can see I ran out of my burst right there and floored it and then it dropped down to my Baseline which was half of that 450,000",
    "start": "1104960",
    "end": "1113000"
  },
  {
    "text": "rops so back to our question we cover gp2 we'll stick with iops for",
    "start": "1114760",
    "end": "1123640"
  },
  {
    "start": "1115000",
    "end": "1115000"
  },
  {
    "text": "now so the first question to asks if iops is the most important thing for your workload is how many iops do you",
    "start": "1124760",
    "end": "1130440"
  },
  {
    "text": "need so if the answer to that is greater than",
    "start": "1130440",
    "end": "1136799"
  },
  {
    "text": "65,000 then the opt options are I2 of course this slide is now outdated because we announced I3 this morning um",
    "start": "1136840",
    "end": "1145799"
  },
  {
    "text": "so why 65,000 that is the maximum amount if you do 65,000 time 16k that's 10 gig of",
    "start": "1145799",
    "end": "1153960"
  },
  {
    "text": "bandwidth so 10 gig is currently as of today the most EBS bandwidth you can get",
    "start": "1153960",
    "end": "1159000"
  },
  {
    "text": "to single instance so if you're if you're driving more than 10 gig of of IO",
    "start": "1159000",
    "end": "1164280"
  },
  {
    "text": "traffic then you need to consider something like I2 which is which is our IO you know specialized the io2 itself",
    "start": "1164280",
    "end": "1171919"
  },
  {
    "text": "does over 300,000 uh random reads and wrs the I3 that we announced today is",
    "start": "1171919",
    "end": "1177240"
  },
  {
    "text": "nine times that so up to three million so it's a different uh category of number of iops but if you're in the",
    "start": "1177240",
    "end": "1182919"
  },
  {
    "text": "range of 65,000 and Below then the next question is what are",
    "start": "1182919",
    "end": "1188600"
  },
  {
    "text": "your latency requirements so if you if you're latency requirements are in mics we're talking",
    "start": "1188600",
    "end": "1194600"
  },
  {
    "text": "microseconds you're back to the I2 again so the I2 I3 is specialized ized for again local instance store local ssds",
    "start": "1194600",
    "end": "1202360"
  },
  {
    "text": "the the lowest latency you can get but if you're in the S single digit millisecond category then what's more",
    "start": "1202360",
    "end": "1210679"
  },
  {
    "text": "important cost of performance we talked about gp2 that's the cost model for the",
    "start": "1210679",
    "end": "1216360"
  },
  {
    "text": "iops based work workloads it's the most uh cost efficient volume type but if performance is your main",
    "start": "1216360",
    "end": "1224840"
  },
  {
    "text": "driver for your workload then we're back to provision iops so the io1 volume",
    "start": "1224840",
    "end": "1230280"
  },
  {
    "text": "type so when I say performance what do I mean and it's really a couple of things so first of all you see the numbers",
    "start": "1230280",
    "end": "1237120"
  },
  {
    "text": "everything's doubled from gp2 so instead of 10,000 uh iops as the as the top end",
    "start": "1237120",
    "end": "1243440"
  },
  {
    "text": "it's 20,000 throughput also doubled instead of 160 is 320 but it's also the",
    "start": "1243440",
    "end": "1248760"
  },
  {
    "text": "consistency of performance that's most important with provision iops so provisioned iops you set exactly how",
    "start": "1248760",
    "end": "1255880"
  },
  {
    "text": "much you want and we guarantee to deliver that within 10% of that number",
    "start": "1255880",
    "end": "1261120"
  },
  {
    "text": "99.9% of the time gp2 is 99% of the time so you get another nine of consistency",
    "start": "1261120",
    "end": "1268159"
  },
  {
    "text": "of iops delivery so provisioned iops is really ideal for Mission critical workloads where you have a consistent",
    "start": "1268159",
    "end": "1275440"
  },
  {
    "text": "level of iops and you know what that is and you can set that and you you have a high degree of guarantee to meet that",
    "start": "1275440",
    "end": "1281679"
  },
  {
    "text": "Target so it's ideal for uh critical applications databases with sustained",
    "start": "1281679",
    "end": "1287039"
  },
  {
    "text": "iops and the provisioning model is a little",
    "start": "1287039",
    "end": "1292960"
  },
  {
    "start": "1290000",
    "end": "1290000"
  },
  {
    "text": "different you can you can scale up much faster so it's not storage based you can turn the dial at a 50 to one ratio so at",
    "start": "1292960",
    "end": "1300720"
  },
  {
    "text": "a 400 gig size you can have 20,000 iops to a volume so very significant for like",
    "start": "1300720",
    "end": "1307880"
  },
  {
    "text": "small hot data sets provision",
    "start": "1307880",
    "end": "1313000"
  },
  {
    "text": "iops so what if throughput is uh the defining performance characteristic for your",
    "start": "1315919",
    "end": "1321400"
  },
  {
    "text": "workload again we start with the first question what's more important small",
    "start": "1321400",
    "end": "1326880"
  },
  {
    "text": "random IO or large sequential IO and if you're doing small random IO you're back",
    "start": "1326880",
    "end": "1333200"
  },
  {
    "text": "over to SSD side of the house as I mentioned the characteristics of what makes hard dis drives good is",
    "start": "1333200",
    "end": "1340080"
  },
  {
    "text": "they're good at sequential throughput large block sequential throughput so again aggregate throughput",
    "start": "1340080",
    "end": "1348200"
  },
  {
    "text": "this goes back to the 10 gig again except on the throughput side of the house do you need more than 1,250",
    "start": "1348200",
    "end": "1354320"
  },
  {
    "text": "megabytes a second which is 10 gig of throughput if you do need",
    "start": "1354320",
    "end": "1359520"
  },
  {
    "text": "more and I recommend you check out our D2 which our is our Dent storage instance type which has up to 48",
    "start": "1359520",
    "end": "1366880"
  },
  {
    "text": "terabytes of local spinning hard disk and it can do upwards of three gigabytes",
    "start": "1366880",
    "end": "1372600"
  },
  {
    "text": "a second of sequential throughput if you're throughput needs",
    "start": "1372600",
    "end": "1378520"
  },
  {
    "text": "are less than 10 gigabits so less than 1,250 megabytes a second then again what",
    "start": "1378520",
    "end": "1385039"
  },
  {
    "text": "what's more important cost or performance and if it's performance then",
    "start": "1385039",
    "end": "1390400"
  },
  {
    "text": "ST1 is the volume type you want to look at so ST1 is the first of our throughput",
    "start": "1390400",
    "end": "1396279"
  },
  {
    "start": "1394000",
    "end": "1394000"
  },
  {
    "text": "Provisions so instead of iops now we're talking about megabytes a second it's",
    "start": "1396279",
    "end": "1401600"
  },
  {
    "text": "similar to gp2 and that you dial in amount of storage and you get a certain amount of throughput so the Baseline is",
    "start": "1401600",
    "end": "1408000"
  },
  {
    "text": "40 megabytes a second per per terabyte that you provision up to a Max of 500 per volume and it also has a burst model",
    "start": "1408000",
    "end": "1415320"
  },
  {
    "text": "so every terabyte you provision you get 250 megabytes of burst up to 500 and the",
    "start": "1415320",
    "end": "1422080"
  },
  {
    "text": "capacity model is a little different the smallest volume size is 500 gig and it goes up to 16 terabytes too so 500 gig",
    "start": "1422080",
    "end": "1429360"
  },
  {
    "text": "these are not designed to be boot volumes actually they can't be boot volumes so these are very much designed to be data for data only",
    "start": "1429360",
    "end": "1438760"
  },
  {
    "text": "so again ideal for large block High throughput sequential",
    "start": "1438760",
    "end": "1443960"
  },
  {
    "start": "1443000",
    "end": "1443000"
  },
  {
    "text": "workloads so here's a quick look at the burst and base model for ST1 you can see very quickly ramp up to",
    "start": "1443960",
    "end": "1451120"
  },
  {
    "text": "the burst so at 2 terabyte volume size you're already at Max burst so as long as you have burst",
    "start": "1451120",
    "end": "1458080"
  },
  {
    "text": "credits in your bucket a 2 terabyte ST1 volume is equivalent in performance to a",
    "start": "1458080",
    "end": "1465039"
  },
  {
    "text": "13 tbte volume because they both have the max maximum 500 megabytes of",
    "start": "1465039",
    "end": "1473159"
  },
  {
    "text": "burst and there's a quick example of of an 8 terab kind of halfway there in the middle 320 Baseline and obviously the",
    "start": "1473399",
    "end": "1481000"
  },
  {
    "text": "burst at 500 so the burst buckets again so a little different with the the throughput",
    "start": "1481000",
    "end": "1487960"
  },
  {
    "text": "volume types in that you're you're still ACC accumulating the ster this Full Bucket that you get when you create the",
    "start": "1487960",
    "end": "1495240"
  },
  {
    "text": "volume but the volume scale I mean the bucket scales with the volume so the bigger the volume the bigger the",
    "start": "1495240",
    "end": "1502760"
  },
  {
    "text": "bucket so again spending at the burst but if I have an 8 terabyte",
    "start": "1503240",
    "end": "1511399"
  },
  {
    "start": "1507000",
    "end": "1507000"
  },
  {
    "text": "volume that bucket is now 8 terabytes in credit and the idea is that when you",
    "start": "1512000",
    "end": "1518240"
  },
  {
    "text": "create a volume it comes with the burst credits that will allow you to do a full volume scan of that",
    "start": "1518240",
    "end": "1525480"
  },
  {
    "text": "volume so last but not",
    "start": "1526679",
    "end": "1532080"
  },
  {
    "text": "least on the throughput side of the house for your workload if the most important is",
    "start": "1532080",
    "end": "1539000"
  },
  {
    "text": "cost cold HDD so that's our SC1 volume type and this is based on the same",
    "start": "1539799",
    "end": "1546880"
  },
  {
    "text": "platform as ST1 uh it's just the the performance characteristics are a little bit more modest so TR you know you trade",
    "start": "1546880",
    "end": "1555080"
  },
  {
    "text": "Baseline and burst numbers for uh you know half the price basically I mean a very compelling cost price so instead",
    "start": "1555080",
    "end": "1563000"
  },
  {
    "text": "of 40 we're at 12 per terabyte up to a Max of 192 and the burst is also half so",
    "start": "1563000",
    "end": "1568159"
  },
  {
    "text": "instead of 500 your max burst is now 250 so these are ideal for things again",
    "start": "1568159",
    "end": "1574440"
  },
  {
    "text": "large sequential uh workloads but maybe not something you're going to be full scanning multiple times a day maybe",
    "start": "1574440",
    "end": "1581840"
  },
  {
    "text": "you're only doing it one time a day so whether that's logging or archiving or backups uh but customers are finding",
    "start": "1581840",
    "end": "1588039"
  },
  {
    "text": "some very interesting use cases cuz um as we'll see in a second the the cost is",
    "start": "1588039",
    "end": "1593840"
  },
  {
    "text": "very compelling and the throughput characteristics and the burst is still very very good much better than you",
    "start": "1593840",
    "end": "1600880"
  },
  {
    "text": "would get from any commodity hard drive so real quick the burst and base",
    "start": "1600880",
    "end": "1607200"
  },
  {
    "text": "here and again you can see at the max volume type you don't ever get to the actual uh Max burst for the volume size",
    "start": "1607799",
    "end": "1614440"
  },
  {
    "text": "so even at 16t same idea bucket is size to the to the",
    "start": "1614440",
    "end": "1624278"
  },
  {
    "text": "volume and you can spend it at the burst 80 so there's the kind of the map of the",
    "start": "1624679",
    "end": "1630840"
  },
  {
    "text": "volume types there's some questions to ask yourself when you're looking at how you should judge these volumes how you should choose them and use them but",
    "start": "1630840",
    "end": "1637320"
  },
  {
    "text": "what's important is that this is not a uh All or Nothing decision so we'll get to a few use cases on what that means",
    "start": "1637320",
    "end": "1644399"
  },
  {
    "text": "but real quick prices so gp2 10 cents gig that's it you don't pay for iio or anything you just uh pay for the storage",
    "start": "1644399",
    "end": "1651200"
  },
  {
    "text": "of provision and you get the the 3 to1 iops ratio i1's a little different you pay for both the storage and the amount",
    "start": "1651200",
    "end": "1658200"
  },
  {
    "text": "of I opsy provision ST1 is you know four and a",
    "start": "1658200",
    "end": "1664399"
  },
  {
    "text": "half cents a gig for three provisioned and then SC1 is half of that so two and",
    "start": "1664399",
    "end": "1670840"
  },
  {
    "text": "a half cents per gig so two and a half cents a gig that that's um you know that's cheaper than S3 so it opens up",
    "start": "1670840",
    "end": "1676880"
  },
  {
    "text": "some really interesting in use cases when instead of an object store",
    "start": "1676880",
    "end": "1682440"
  },
  {
    "text": "you need like a posix file system to be available to you in an EBS volume that you can attach and have you know data",
    "start": "1682440",
    "end": "1688840"
  },
  {
    "text": "sets that can roam around and attach to different instances and then finally snapshot",
    "start": "1688840",
    "end": "1695519"
  },
  {
    "text": "storage itself is 5 cents a gig a month so I mentioned it's not an all1",
    "start": "1695519",
    "end": "1700720"
  },
  {
    "start": "1698000",
    "end": "1698000"
  },
  {
    "text": "decision so uh you know the volume types uh the flexibility of EBS as a service really starts to exercise itself when",
    "start": "1700720",
    "end": "1707600"
  },
  {
    "text": "you can choose multiple hybrid volumes for a workload so you don't have to choose all ST1 or or all gp2 they mix",
    "start": "1707600",
    "end": "1715480"
  },
  {
    "text": "and match even on the the same instance for different workload characteristics so we'll go through a few use cases for",
    "start": "1715480",
    "end": "1722039"
  },
  {
    "text": "these hybrid volumes so uh Le which does U monitoring and metrics",
    "start": "1722039",
    "end": "1727799"
  },
  {
    "text": "for the cloud uh they're doing a talk about uh their experience migrating to",
    "start": "1727799",
    "end": "1733120"
  },
  {
    "text": "EBS for their Cassandra workloads which uh stores all their Time series data and",
    "start": "1733120",
    "end": "1738440"
  },
  {
    "text": "they were running on I2 and they migrated to C4 with EBS and uh they were",
    "start": "1738440",
    "end": "1745720"
  },
  {
    "text": "able so they store the actual data files themselves so the the cassander SS temp",
    "start": "1745720",
    "end": "1751200"
  },
  {
    "text": "SS tables um so you know very intensive uh small random workloads dedicated gp2",
    "start": "1751200",
    "end": "1759000"
  },
  {
    "text": "and then the streaming log so the transaction log the commit log for Cassandra goes off to ST1 so each",
    "start": "1759000",
    "end": "1764640"
  },
  {
    "text": "Cassandra node has two different volume types mounted to it for very different uh workloads and you know with this they",
    "start": "1764640",
    "end": "1771640"
  },
  {
    "text": "significantly Roost their mean time to recovery because they're not having to hydrate a whole I2 instance again if an",
    "start": "1771640",
    "end": "1776799"
  },
  {
    "text": "I2 fails uh and they saved a significant uh 35% in",
    "start": "1776799",
    "end": "1784480"
  },
  {
    "text": "cost and here's another one zindex zindex with their elk stack so they uh",
    "start": "1785480",
    "end": "1790960"
  },
  {
    "text": "did something very interesting with tiering their data they were able to uh not only reduce their cost by 60% % but",
    "start": "1790960",
    "end": "1797919"
  },
  {
    "text": "increased the amount of data they were storing by three times in their elastic search cluster by tearing out to",
    "start": "1797919",
    "end": "1804279"
  },
  {
    "text": "different instance types so for the really hot stuff they put it on gp2 so that's for the first week of data and",
    "start": "1804279",
    "end": "1810279"
  },
  {
    "text": "then for you know the warm stuff they went out to sequential to ST1 for 8 to",
    "start": "1810279",
    "end": "1815919"
  },
  {
    "text": "30 days and then and then 30 to 60 days out they used the",
    "start": "1815919",
    "end": "1822200"
  },
  {
    "text": "s1s and then in for has a case study out there so they uh still run their actual",
    "start": "1825320",
    "end": "1831799"
  },
  {
    "text": "SQL Server database on i2s but they do their backups using ST1",
    "start": "1831799",
    "end": "1836880"
  },
  {
    "text": "so they have ST1 uh volumes attached to their i2s and they do different kinds of",
    "start": "1836880",
    "end": "1844120"
  },
  {
    "text": "backups to different volumes and then they actually snapshot those",
    "start": "1844120",
    "end": "1849399"
  },
  {
    "text": "volumes so they found that their backups are actually significantly Faster by offloading that to the volumes",
    "start": "1849919",
    "end": "1855360"
  },
  {
    "text": "themselves 30% faster and then EMR so our our managed to dup",
    "start": "1855360",
    "end": "1864399"
  },
  {
    "text": "framework is also supports EBS and has some very interesting ways to mix hybrid",
    "start": "1864399",
    "end": "1870880"
  },
  {
    "text": "with uh EMR so EMR again has very different uh workload characteristic",
    "start": "1870880",
    "end": "1876080"
  },
  {
    "text": "depending what you're doing uh but a very Comm common pattern we see is customers using gp2 for the actual uh",
    "start": "1876080",
    "end": "1883600"
  },
  {
    "text": "yarn workload so the really small stuff and uh all Shuffle spill temp Ops are very random small IO doeses very well in",
    "start": "1883600",
    "end": "1891080"
  },
  {
    "text": "gp2 and then the actual hdfs for the storage of Hadoop is very large",
    "start": "1891080",
    "end": "1897360"
  },
  {
    "text": "sequential IO it's very consistent um so that's dedicated to HD DFS with",
    "start": "1897360",
    "end": "1904399"
  },
  {
    "text": "ST1 and then mounting multiple H ST1 volumes because Hado is really good at",
    "start": "1904399",
    "end": "1910880"
  },
  {
    "text": "going against um parallel Mount points so it can dedicate cores and tasks to",
    "start": "1910880",
    "end": "1916720"
  },
  {
    "text": "each mount Mount point so lots of s1s and then the gp2 off to the side for the",
    "start": "1916720",
    "end": "1922799"
  },
  {
    "text": "yarn stuff and that'll be in the deck later you can see this is what you would feed EMR to do exactly what I said so",
    "start": "1922799",
    "end": "1929519"
  },
  {
    "text": "dedicate certain uh sites to to different uh volume",
    "start": "1929519",
    "end": "1935120"
  },
  {
    "text": "types all right so let's dive into some very specific stuff on performance so we talked about all these",
    "start": "1936559",
    "end": "1944360"
  },
  {
    "start": "1941000",
    "end": "1941000"
  },
  {
    "text": "burst buckets and how we credit you and how ios's are or work how do we actually count that as a service like how do we",
    "start": "1944360",
    "end": "1950960"
  },
  {
    "text": "say you're using a certain am of iops to subtract from your bucket so with with counting iops for gp2 and io1 uh we",
    "start": "1950960",
    "end": "1959039"
  },
  {
    "text": "merge sequential ios's up to a Max size of 256 KB and that's both to uh minimize",
    "start": "1959039",
    "end": "1965679"
  },
  {
    "text": "the io charges on io1 so if you're doing sequential workload you'll get charged a lot less IO and maximize burst for gp2",
    "start": "1965679",
    "end": "1974320"
  },
  {
    "text": "and keep in mind this is all logical merging we're not doing anything physical I know that it's going to matter to some of you",
    "start": "1974320",
    "end": "1981120"
  },
  {
    "text": "um and I I have this box car so the box car is representing the the maximum",
    "start": "1981120",
    "end": "1986360"
  },
  {
    "text": "amount of of throughput you can put to EBS so each one of these is 256k for gp2",
    "start": "1986360",
    "end": "1994120"
  },
  {
    "text": "and i1 and as an example sending down four random iOS of size",
    "start": "1994120",
    "end": "2000000"
  },
  {
    "text": "64k they're not sequential so they're not going to be able to pack those into one box car so we're going to",
    "start": "2000000",
    "end": "2005960"
  },
  {
    "text": "count four iOS against you even though all that capacity was still",
    "start": "2005960",
    "end": "2013960"
  },
  {
    "text": "there but if you send those 64k down as sequential you'll be able to recognize",
    "start": "2014000",
    "end": "2019279"
  },
  {
    "text": "that logically merge those ios's and even though it was four iOS to you we only credit you",
    "start": "2019279",
    "end": "2026000"
  },
  {
    "text": "one against your burst credits and similarly if you send down a",
    "start": "2026000",
    "end": "2032799"
  },
  {
    "text": "very large one that's obviously larger than 256k we're GNA have to chop that up so if you send down a Meg IO for gp2 and",
    "start": "2032799",
    "end": "2040000"
  },
  {
    "text": "i1 we're going to chop that up into 256 KBS and charge you",
    "start": "2040000",
    "end": "2045919"
  },
  {
    "text": "four what about ST1 and SC1 so they're a little bit different um in those case we",
    "start": "2047119",
    "end": "2053040"
  },
  {
    "text": "merge up to a Meg instead of 256k um as they're obviously designed for large sequential",
    "start": "2053040",
    "end": "2060480"
  },
  {
    "text": "iOS and so again if you send four random iOS down to us on ST1 or SC1 volume at",
    "start": "2060480",
    "end": "2068839"
  },
  {
    "text": "64k we're not going to be able to merge them it's going to take four",
    "start": "2068839",
    "end": "2073919"
  },
  {
    "text": "units so you get charged for four megabytes of",
    "start": "2073919",
    "end": "2081520"
  },
  {
    "text": "burst even though you only sent you know much less data than",
    "start": "2081520",
    "end": "2087320"
  },
  {
    "text": "that so here's more ideal so you send down four sequential iOS each IO is a full Meg you're taking advantage of of",
    "start": "2087320",
    "end": "2094878"
  },
  {
    "text": "the full capacity of the of the box car and you get charged four ios's and you",
    "start": "2094879",
    "end": "2101560"
  },
  {
    "text": "transferred four Megs so perfect and here's where things get",
    "start": "2101560",
    "end": "2107000"
  },
  {
    "text": "interesting uh when when you have mixed workloads against ST1 and SC1 because in some cases you know the",
    "start": "2107000",
    "end": "2114839"
  },
  {
    "text": "sequential iOS they'll be able to be counted as a full IO but the randoms are going to still",
    "start": "2114839",
    "end": "2120400"
  },
  {
    "text": "take up a full unit of capacity a full one Meg so you'll end up again being charged for Megs but",
    "start": "2120400",
    "end": "2128040"
  },
  {
    "text": "you've only transferred about 1.4",
    "start": "2128040",
    "end": "2131720"
  },
  {
    "start": "2134000",
    "end": "2134000"
  },
  {
    "text": "Megs so how does this look when you actually look at your burst B balance when you're transferring one Meg so this",
    "start": "2134280",
    "end": "2140720"
  },
  {
    "text": "is five this is full out for three hours against the burst credit you can see I was able to burst for about three hours",
    "start": "2140720",
    "end": "2147400"
  },
  {
    "text": "on a 4 terabyte 4 terabyte volume if I'm doing random it's actually",
    "start": "2147400",
    "end": "2154160"
  },
  {
    "text": "going to drain at the same time because uh every single random uh or small block is going to count and use the same",
    "start": "2154160",
    "end": "2160079"
  },
  {
    "text": "amount of burst I'm just not going to be transferring a lot so I'm not going to be getting what's on the",
    "start": "2160079",
    "end": "2166240"
  },
  {
    "text": "box and to represent that even more graphically is if it was 500 megabytes for 3",
    "start": "2167800",
    "end": "2176200"
  },
  {
    "text": "hours that's 5.4 terabytes of data transferred but if I did that in 16k",
    "start": "2176200",
    "end": "2182240"
  },
  {
    "text": "random I would only transfer 87 gig so it just illustrates that",
    "start": "2182240",
    "end": "2187359"
  },
  {
    "text": "uh you know what the model is for these volume types is large block sequential",
    "start": "2187359",
    "end": "2193039"
  },
  {
    "text": "throughput so how do you verify that you're doing the right thing with these volume types um with Linux a good start",
    "start": "2193720",
    "end": "2199720"
  },
  {
    "start": "2194000",
    "end": "2194000"
  },
  {
    "text": "is iostat you look at your average request size that's in 512k sectors uh",
    "start": "2199720",
    "end": "2205280"
  },
  {
    "text": "of 512kb so here you can see that your operating system is telling you you're doing a good job you're transferring a",
    "start": "2205280",
    "end": "2211520"
  },
  {
    "text": "full Meg of your average request size Peron for Windows will tell you the same thing thing you can look at average uh",
    "start": "2211520",
    "end": "2218240"
  },
  {
    "text": "request size and then cloudwatch so cloudwatch has volume specific metrics you can go",
    "start": "2218240",
    "end": "2225319"
  },
  {
    "start": "2221000",
    "end": "2221000"
  },
  {
    "text": "into the volume Management console you can go in the monitoring Tab and uh we have per volume graphs set up for you to",
    "start": "2225319",
    "end": "2232359"
  },
  {
    "text": "to see average right size and average read size so here you see we're at",
    "start": "2232359",
    "end": "2238000"
  },
  {
    "text": "128 uh KB which is actually ideal this is what you want to see this is actually",
    "start": "2238000",
    "end": "2243359"
  },
  {
    "text": "the maximum you'll see uh even if your operator system is telling you you're transferring a Meg uh and that's",
    "start": "2243359",
    "end": "2250880"
  },
  {
    "text": "because this comes from the hypervisor perspective this is not from your instance type so um we will work on",
    "start": "2250880",
    "end": "2258720"
  },
  {
    "text": "getting this to be um more meaningful but in the meantime know that 128k is",
    "start": "2258720",
    "end": "2264240"
  },
  {
    "text": "the max you will s see as as your average right size and that means you're in a good place if you're seeing less",
    "start": "2264240",
    "end": "2269720"
  },
  {
    "text": "than that so if you're seeing something around 64 you're most likely interspersing some randoms in there or",
    "start": "2269720",
    "end": "2275680"
  },
  {
    "text": "some smaller block size sizes and getting your average",
    "start": "2275680",
    "end": "2279839"
  },
  {
    "text": "down and if you're seeing even less than that so 44 this means one of two things",
    "start": "2281640",
    "end": "2289240"
  },
  {
    "text": "you're either at a very old Linux kernel so 38 or less which did not support a",
    "start": "2289240",
    "end": "2294480"
  },
  {
    "text": "few uh features that we'll talk about or you're running",
    "start": "2294480",
    "end": "2299119"
  },
  {
    "text": "Windows sorry uh the Windows driver also does not um uh does not support",
    "start": "2300760",
    "end": "2306560"
  },
  {
    "text": "assistant Grant so uh these are features of the uh Zin device driver that we currently use",
    "start": "2306560",
    "end": "2313960"
  },
  {
    "text": "today in AWS so that uh shows itself in the instance itself so that's uh block",
    "start": "2313960",
    "end": "2320680"
  },
  {
    "text": "front for the instance and block back other drivers in the hypervisor so we'll talk a little bit about how that works",
    "start": "2320680",
    "end": "2326359"
  },
  {
    "text": "so you understand why uh kernel uh version is so",
    "start": "2326359",
    "end": "2331839"
  },
  {
    "text": "important so user space process sends down some request to your Colonel your scheduler",
    "start": "2331839",
    "end": "2338920"
  },
  {
    "text": "running um you know that by noop or deadline or cfq is going to do some things with those uh you might ask you",
    "start": "2338920",
    "end": "2345359"
  },
  {
    "text": "know what scheduler should I be thinking about any SSD volume type no up that's",
    "start": "2345359",
    "end": "2350400"
  },
  {
    "text": "the default that's a no-brainer uh HDD scheduler types uh for EBS um as my",
    "start": "2350400",
    "end": "2357640"
  },
  {
    "text": "favorite EBS engineer says does it make it does it make a difference in my scheduler and and he always says",
    "start": "2357640",
    "end": "2364079"
  },
  {
    "text": "definitely maybe uh the fact is the performance difference",
    "start": "2364079",
    "end": "2370040"
  },
  {
    "text": "you you'll see playing with schedulers is not uh is going to be very workload dependent it's not going to be an",
    "start": "2370040",
    "end": "2375760"
  },
  {
    "text": "extreme difference between one or the other I would recommend no op or deadline uh play with your workload and",
    "start": "2375760",
    "end": "2381000"
  },
  {
    "text": "see uh what the performance differences are on your specific workload but anyways the the colel and the schedule",
    "start": "2381000",
    "end": "2386280"
  },
  {
    "text": "is going to do some things with the iops coming down whether that's merging them or rearranging",
    "start": "2386280",
    "end": "2391920"
  },
  {
    "text": "them and then it's going to send it into the to the request CU so this is a ring ring buffer for the Zen device driver",
    "start": "2391920",
    "end": "2399480"
  },
  {
    "text": "that uh interrupts between the instance itself and the hypervisor and it's made up of 32",
    "start": "2399480",
    "end": "2407200"
  },
  {
    "text": "requests can be in the ring at any point in time in this buffer so pre 38 the max size of any",
    "start": "2407200",
    "end": "2414319"
  },
  {
    "text": "request was 44 KB that was flat so uh with some uh later enhancements to the",
    "start": "2414319",
    "end": "2420319"
  },
  {
    "text": "Linux kernel uh you're able to the default now is 128 so anything over uh",
    "start": "2420319",
    "end": "2426359"
  },
  {
    "text": "kernel version of 3.8 the default will be 128 KB for a request and you're able to tune that up to a full Meg uh and",
    "start": "2426359",
    "end": "2433400"
  },
  {
    "text": "that's what I meant Windows does not supports this so Windows is is stuck at",
    "start": "2433400",
    "end": "2438160"
  },
  {
    "text": "44 and on to the actual uh hypervisor and then on to EBS and actually the",
    "start": "2439640",
    "end": "2445720"
  },
  {
    "text": "train that all the train stuff and the counting and The Logical merging we discussed that happens back in the actual service so that's where that",
    "start": "2445720",
    "end": "2454280"
  },
  {
    "text": "is now another change happened in Linux 42 so uh Zen adopted the block mq model",
    "start": "2454599",
    "end": "2462400"
  },
  {
    "text": "in 42 afterwards so blocking mq did away with the old schedulers and most of",
    "start": "2462400",
    "end": "2467760"
  },
  {
    "text": "those schedulers were very much designed for optimizing workloads on hard dis drives uh and introduce block mq which",
    "start": "2467760",
    "end": "2474880"
  },
  {
    "text": "does a uh Q request per core which is great for ssds right because you can",
    "start": "2474880",
    "end": "2481599"
  },
  {
    "text": "each core has its own request cue you can you know send down a lot of parallel parall ios's but what's good for ssds is",
    "start": "2481599",
    "end": "2489400"
  },
  {
    "text": "is not so great for hard dis drives so uh this is why we recommend that if",
    "start": "2489400",
    "end": "2496160"
  },
  {
    "start": "2493000",
    "end": "2493000"
  },
  {
    "text": "you're running on a 4 t or later kernel that you crank up the maximum Quest size to to the full amount the full one Meg",
    "start": "2496160",
    "end": "2503240"
  },
  {
    "text": "and you might say well why is that and it's because if you stick at 128 and you're sending down large block ios's so",
    "start": "2503240",
    "end": "2511280"
  },
  {
    "text": "say you send uh three 1meg iOS uh and you're at the default 128k those are",
    "start": "2511280",
    "end": "2517440"
  },
  {
    "text": "going to get chopped up so that's uh 24 128k sections that might end up on",
    "start": "2517440",
    "end": "2523280"
  },
  {
    "text": "different core cues and by the time it gets to EBS that looks random you know",
    "start": "2523280",
    "end": "2529079"
  },
  {
    "text": "you started with three one megabyte ios's by the time EBS sees it it could be completely mixed up so cranking that",
    "start": "2529079",
    "end": "2536640"
  },
  {
    "text": "up to a full Meg means those three iOS stay as one unit in the request CU so",
    "start": "2536640",
    "end": "2541880"
  },
  {
    "text": "helps with the throughput so that's that's recommended in general but definitely after 42 when block mq comes",
    "start": "2541880",
    "end": "2547880"
  },
  {
    "text": "into the scene uh keep in mind the memory is allocated per device so be careful um if",
    "start": "2547880",
    "end": "2553760"
  },
  {
    "text": "you're doing one Megs that's 32 Megs of of ram per",
    "start": "2553760",
    "end": "2559040"
  },
  {
    "text": "device and here's the command to actually enable it and crank it up so it's a it's a boot level",
    "start": "2559079",
    "end": "2566599"
  },
  {
    "text": "command uh a second Performance Tuning we do recommend with ST1 and SC1 is to crank up the rad uh rad buffer so this",
    "start": "2566599",
    "end": "2574440"
  },
  {
    "text": "is recommended for any High through but re workloads it's per device so it's per actual volume the default is",
    "start": "2574440",
    "end": "2581880"
  },
  {
    "text": "128 uh play with this this setting will take you up to a Meg we've seen really",
    "start": "2581880",
    "end": "2587480"
  },
  {
    "text": "great performance with two Meg fourm it all depends on your workload but it is very important for ST1 and uh for high",
    "start": "2587480",
    "end": "2594400"
  },
  {
    "text": "throughput re workloads to to work with the uh the reead",
    "start": "2594400",
    "end": "2600960"
  },
  {
    "start": "2601000",
    "end": "2601000"
  },
  {
    "text": "buffer so hopefully by now it's it's kind of apparent where the balance is between um throughput versus iops so the",
    "start": "2602079",
    "end": "2609440"
  },
  {
    "text": "example here is an io1 volume provision iops at 20,000 so it all depends on the",
    "start": "2609440",
    "end": "2614599"
  },
  {
    "text": "actual uh block size that you're sending what you can do so on the far left side",
    "start": "2614599",
    "end": "2619640"
  },
  {
    "text": "which is the the smallest block so 16k we can send the full 20,000 and get",
    "start": "2619640",
    "end": "2624920"
  },
  {
    "text": "the full throughput if we half that 10,000 iops but send double the size as far as",
    "start": "2624920",
    "end": "2633240"
  },
  {
    "text": "request size in iOS we can still do that what we can't do is send 10,000 64k that",
    "start": "2633240",
    "end": "2640040"
  },
  {
    "text": "would obviously be 640 megabytes to the volume that would exceed the volume's throughput characteristics and we couldn't do that",
    "start": "2640040",
    "end": "2647559"
  },
  {
    "text": "but we can do is send the largest amount the largest block size for the i1",
    "start": "2647559",
    "end": "2654319"
  },
  {
    "text": "256k at 12 1,250 iops and that would get you to the full throughput so it's",
    "start": "2654319",
    "end": "2660400"
  },
  {
    "text": "always a spectrum between iops and throughput",
    "start": "2660400",
    "end": "2668000"
  },
  {
    "start": "2665000",
    "end": "2665000"
  },
  {
    "text": "which means when we talk about EBS optimized bandwidth is really important bandwidth matters that you know how much",
    "start": "2669040",
    "end": "2675480"
  },
  {
    "text": "bandwidth you have to your EBS volume and your expectations are for that workload and how much you want to drive to the volume so here we are with a C4",
    "start": "2675480",
    "end": "2682960"
  },
  {
    "text": "large which has 500 megabytes megabits excuse me uh of bandwidth dedicated to",
    "start": "2682960",
    "end": "2689319"
  },
  {
    "text": "EBS and we've attached a 2 terb gp2",
    "start": "2689319",
    "end": "2694559"
  },
  {
    "text": "volume obviously that volume can do a lot more than the bandwidth that you have to the volume so it's really not a",
    "start": "2694960",
    "end": "2701720"
  },
  {
    "text": "good match if you jump up one size to a C4 too extra",
    "start": "2701720",
    "end": "2709240"
  },
  {
    "text": "large same volume size now we've got to double the bandwidth so we can do 125 megabytes a",
    "start": "2709240",
    "end": "2716720"
  },
  {
    "text": "second much better match for that volume type you know we can actually get to where we can almost push full throughput",
    "start": "2716720",
    "end": "2723480"
  },
  {
    "text": "to that volume what it can take so pay attention to what you want to push to your volumes and how much actual Network",
    "start": "2723480",
    "end": "2729920"
  },
  {
    "text": "bandwidth you",
    "start": "2729920",
    "end": "2732480"
  },
  {
    "text": "have so here we see a full tin so an m46x large has an 10 full gigs of EBS",
    "start": "2735319",
    "end": "2743640"
  },
  {
    "text": "bandwidth available to it so you can push 1,250 megabytes a second of data to EBS so if you just put one 8 terby ST1",
    "start": "2743640",
    "end": "2752000"
  },
  {
    "text": "volume that does a Max burst of 500 you got a lot of bandwidth left over so",
    "start": "2752000",
    "end": "2758119"
  },
  {
    "text": "that's where striping starts to come into play and raiding raate zero where",
    "start": "2758119",
    "end": "2763160"
  },
  {
    "text": "you're going to attach multiple volumes and be able to push against all of them and and get a collective uh uh",
    "start": "2763160",
    "end": "2769960"
  },
  {
    "text": "throughput amount for all the volumes so when should you consider",
    "start": "2769960",
    "end": "2777040"
  },
  {
    "start": "2774000",
    "end": "2774000"
  },
  {
    "text": "rating when the storage requirement is greater than 16 terabytes that's obviously the max size for a single volume when the throughput requirements",
    "start": "2777040",
    "end": "2784200"
  },
  {
    "text": "are greater than 500 the that's again the max for the ST1 for its uh",
    "start": "2784200",
    "end": "2789359"
  },
  {
    "text": "throughput or iops on the iops end if your iops requirements are greater than 20,000 at 16k you're going to need more",
    "start": "2789359",
    "end": "2795839"
  },
  {
    "text": "than one volume but what we don't recommend is",
    "start": "2795839",
    "end": "2802920"
  },
  {
    "text": "rating for redundancy so here we see sending down",
    "start": "2802920",
    "end": "2808319"
  },
  {
    "text": "to a raate zero set we talked about the replicas you're basically emulating what a RAID 10 would do right you you have a",
    "start": "2808319",
    "end": "2815480"
  },
  {
    "text": "a a a replicated copy of every block that you're sending down to your raid zero but you're not paying for the two",
    "start": "2815480",
    "end": "2822480"
  },
  {
    "text": "times the storage which is what you would do with a RAID 10 so that's why we say we avoid raid for",
    "start": "2822480",
    "end": "2829680"
  },
  {
    "text": "redundancy data is already replicated if you're if you're doing a raid one you're having the available EBS",
    "start": "2831119",
    "end": "2838200"
  },
  {
    "text": "bandwidth available to your volume because you're sending everything down twice and same with something like a raid five or six all that parody data is",
    "start": "2838200",
    "end": "2844359"
  },
  {
    "text": "taking up your iops taking up your network",
    "start": "2844359",
    "end": "2848838"
  },
  {
    "text": "bandwidth a few things on reliability so instance failure if an instance fails",
    "start": "2852200",
    "end": "2857960"
  },
  {
    "start": "2854000",
    "end": "2854000"
  },
  {
    "text": "and your EBS volume is attached to it your volume's persistent still endures",
    "start": "2857960",
    "end": "2863440"
  },
  {
    "text": "outside the life of the instance you can obviously attach it to another instance and get access to your",
    "start": "2863440",
    "end": "2868559"
  },
  {
    "text": "data but there's also a feature called ec2 Auto Recovery that is enabled by E",
    "start": "2868559",
    "end": "2874200"
  },
  {
    "start": "2869000",
    "end": "2869000"
  },
  {
    "text": "EBS which is a much better option for recovering instance failures so you have",
    "start": "2874200",
    "end": "2879760"
  },
  {
    "text": "a cloud watch metrics on a per instance basis so it's called status check failed system and this is a rollup of all kinds",
    "start": "2879760",
    "end": "2887000"
  },
  {
    "text": "of different health checks that ec2 is doing on your behalf to validate the health of both the system and your",
    "start": "2887000",
    "end": "2893720"
  },
  {
    "text": "instance and if the system fails and this health check fails you can choose a recovery action within cloudwatch so on",
    "start": "2893720",
    "end": "2901640"
  },
  {
    "text": "that alarm what action would you like to trigger it's called recover and we will migrate that instance to new hardware",
    "start": "2901640",
    "end": "2909920"
  },
  {
    "text": "automatically recover that instance and it will have all the characteristics that that instance has so whether that's",
    "start": "2909920",
    "end": "2915760"
  },
  {
    "text": "the IP addresses the instance ID the volume mounts everything will be the same and that's supported on uh any of",
    "start": "2915760",
    "end": "2922760"
  },
  {
    "text": "our modern generation instance types that are EBS only",
    "start": "2922760",
    "end": "2927839"
  },
  {
    "start": "2928000",
    "end": "2928000"
  },
  {
    "text": "storage and what about if you you terminate your instance what happens to your volume that completely depends upon",
    "start": "2929559",
    "end": "2935960"
  },
  {
    "text": "the delete on termination flag that you set on either instance launch or volume",
    "start": "2935960",
    "end": "2941280"
  },
  {
    "text": "creation so if if you create a volume outside of an instance launch the delete",
    "start": "2941280",
    "end": "2946520"
  },
  {
    "text": "on termination flag is set to false which means if the instance that it's attached to is terminated the EBS volume",
    "start": "2946520",
    "end": "2955599"
  },
  {
    "text": "endures if however that flag is set to True which is the default for any volume",
    "start": "2955599",
    "end": "2961119"
  },
  {
    "text": "that you launch with an instance so boot volumes or if you attach a bunch of data volumes to an instance when you actually",
    "start": "2961119",
    "end": "2966880"
  },
  {
    "text": "launch it the flag by default will be set to True those volumes will go with the",
    "start": "2966880",
    "end": "2972920"
  },
  {
    "text": "instance itself which is actually a good point of housekeeping because I see a lot of customers who launch volumes",
    "start": "2972920",
    "end": "2978680"
  },
  {
    "text": "outside of instance creations and they have a lot of volumes just sitting out there that aren't doing anything and",
    "start": "2978680",
    "end": "2983799"
  },
  {
    "text": "aren't attached um and a lot of it is because they haven't set these flags so it's a good point of housekeeping to pay",
    "start": "2983799",
    "end": "2989440"
  },
  {
    "text": "attention and make sure that the volumes that are out there and unattached are are ones that you actually want to be",
    "start": "2989440",
    "end": "2994760"
  },
  {
    "text": "out there what about uh taking snapshots so a few",
    "start": "2994760",
    "end": "3000240"
  },
  {
    "start": "2996000",
    "end": "2996000"
  },
  {
    "text": "best practices about snapshots um and this is this is really the difference",
    "start": "3000240",
    "end": "3006079"
  },
  {
    "text": "between crash consistency versus application consistency so you know by default if you pull the virtual plug on",
    "start": "3006079",
    "end": "3014119"
  },
  {
    "text": "your instances with EBS you will still have crash consens toy as long as you're using a journaled file system which I",
    "start": "3014119",
    "end": "3019960"
  },
  {
    "text": "hope most of you are um so this is how you get these are best practices for",
    "start": "3019960",
    "end": "3025040"
  },
  {
    "text": "getting application consistency so this is all the caches everything in the file system and the application that's not",
    "start": "3025040",
    "end": "3031680"
  },
  {
    "text": "yet committed to disk that you would lose if the plug was pulled so for example on a database you flush you lock",
    "start": "3031680",
    "end": "3038440"
  },
  {
    "text": "the tables you clear out the database caches on a file system you sync and Fs freeze it all do all these things before",
    "start": "3038440",
    "end": "3045760"
  },
  {
    "text": "you actually take a snapshot so that you guaranteed that everything that's in Cache in state and",
    "start": "3045760",
    "end": "3051839"
  },
  {
    "text": "memory has been flushed to dis before you take your snapshot and when you when you issue that create snapshot",
    "start": "3051839",
    "end": "3058359"
  },
  {
    "text": "API you'll get an answer back within a few seconds so you only have to FS freeze for literally just a second or",
    "start": "3058359",
    "end": "3063799"
  },
  {
    "text": "two before you get back and okay from the API call you're good to go from then uh you don't you don't have to wait for",
    "start": "3063799",
    "end": "3069640"
  },
  {
    "text": "the actual transfer of data to S3 to complete the the the pending it will stay in a pending State the snapshot but",
    "start": "3069640",
    "end": "3076160"
  },
  {
    "text": "you you can go ahead and keep using the volume as soon as that creat snapshot API returns a 200",
    "start": "3076160",
    "end": "3082680"
  },
  {
    "text": "success Windows is a little different because in TFS does not have an equivalent uh file system freeze so it",
    "start": "3083839",
    "end": "3091040"
  },
  {
    "start": "3084000",
    "end": "3084000"
  },
  {
    "text": "does have a sync that's available out there in CIS inter internals you can download a sync equivalent um but really",
    "start": "3091040",
    "end": "3096680"
  },
  {
    "text": "it's it's all about vsss so volume Shadow Copy Service is is is microsofts",
    "start": "3096680",
    "end": "3101760"
  },
  {
    "text": "technology for doing their own brand of snapshots so what we recommend is is",
    "start": "3101760",
    "end": "3107480"
  },
  {
    "text": "what you saw with the inf4 uh case study is attaching dedicated backup volumes",
    "start": "3107480",
    "end": "3113880"
  },
  {
    "text": "for your windows backups uh all of Windows you know whether it's exchange or SQL Server they all support",
    "start": "3113880",
    "end": "3120640"
  },
  {
    "text": "vsss based backups natively so the idea is that you you use those native Windows",
    "start": "3120640",
    "end": "3126280"
  },
  {
    "text": "backup utilities to create your backups you store those backups on an EBS volume",
    "start": "3126280",
    "end": "3131520"
  },
  {
    "text": "and then you snapshot that back up EBS volume so here you see here we've",
    "start": "3131520",
    "end": "3137880"
  },
  {
    "text": "dedicated uh an EBS volume just for backups backups are sent to that volume",
    "start": "3137880",
    "end": "3144119"
  },
  {
    "text": "and that's what we snapshot so what about initializing a",
    "start": "3144119",
    "end": "3150359"
  },
  {
    "text": "volume so a new OBS volume you just attach it and it's ready to go the",
    "start": "3150359",
    "end": "3155400"
  },
  {
    "text": "performance characteristics there's nothing you have to do to get to the full volume performance it's ready to go",
    "start": "3155400",
    "end": "3160760"
  },
  {
    "text": "out of the box but if you're creating a volume from a snapshot we obviously have to get that",
    "start": "3160760",
    "end": "3166680"
  },
  {
    "text": "data from S3 to the new volume so there's going to be a latency impact",
    "start": "3166680",
    "end": "3172440"
  },
  {
    "text": "there because you might be hitting blocks that haven't arrived yet so there is a lazy load process where if you do",
    "start": "3172440",
    "end": "3179119"
  },
  {
    "text": "try to access a block on the device that's not there yet it'll get queued to the front but you're obviously still",
    "start": "3179119",
    "end": "3184599"
  },
  {
    "text": "going to get a latency impact before it actually arrives so we do recommend if",
    "start": "3184599",
    "end": "3190000"
  },
  {
    "text": "you're generating a new volume from a snapshot and you do want full performance out of that volume as fast",
    "start": "3190000",
    "end": "3195960"
  },
  {
    "text": "as possible to initialize first so how do you actually",
    "start": "3195960",
    "end": "3202599"
  },
  {
    "text": "initialize so we recommend a random read across volumes and here's",
    "start": "3202599",
    "end": "3207760"
  },
  {
    "text": "the file that we recommend so we recommend file over something like DD f",
    "start": "3207760",
    "end": "3213760"
  },
  {
    "start": "3209000",
    "end": "3209000"
  },
  {
    "text": "is multi-threaded it's also a lot more tunable So Random going back to the you",
    "start": "3213760",
    "end": "3219319"
  },
  {
    "text": "know the the uh the split logical volume across a lot of physical um physical",
    "start": "3219319",
    "end": "3224680"
  },
  {
    "text": "blocks across many devices if you randomize that you can you can bring that parallel nature of the volume",
    "start": "3224680",
    "end": "3230839"
  },
  {
    "text": "itself to Bear if you're running a sequential initialization you're just going to hit hit each of those along the",
    "start": "3230839",
    "end": "3236160"
  },
  {
    "text": "line you're not going to take advantage of the fact that this is a distributed system and all that stuff can come at you in parallel fashion so you can play",
    "start": "3236160",
    "end": "3243240"
  },
  {
    "text": "with the block size 128 is kind of a compromise uh between a file running a",
    "start": "3243240",
    "end": "3248839"
  },
  {
    "text": "really long time versus your data already being there so what I always recommend is watch the latency volume",
    "start": "3248839",
    "end": "3255200"
  },
  {
    "text": "characteristics the the cloudwatch latency curve for the volume while you're Rec while you're running your file so you don't necessarily need to",
    "start": "3255200",
    "end": "3262280"
  },
  {
    "text": "run your F to completion if you're watching the late curve and you see that it's starting to Flatline which means",
    "start": "3262280",
    "end": "3268599"
  },
  {
    "text": "everything uh all your data is starting is there on the disc so you're not having to wait for the latency impact of",
    "start": "3268599",
    "end": "3274640"
  },
  {
    "text": "a new block to come down from S3 then stop the file you're done um and you can",
    "start": "3274640",
    "end": "3280160"
  },
  {
    "text": "play with that block size to see what your optimal initialization uh block size might be for your for your",
    "start": "3280160",
    "end": "3287240"
  },
  {
    "start": "3288000",
    "end": "3288000"
  },
  {
    "text": "volume okay what about automating snapshots uh obviously customers that",
    "start": "3288480",
    "end": "3293720"
  },
  {
    "text": "generate thousands and thousands of of snapshots you know uh managing them uh",
    "start": "3293720",
    "end": "3298960"
  },
  {
    "text": "life cycle of them expiring them keeping track of them can be a hassle so here's a quick",
    "start": "3298960",
    "end": "3305640"
  },
  {
    "text": "uh idea of how to take a number of AWS tools to create a framework for life",
    "start": "3305640",
    "end": "3311720"
  },
  {
    "text": "cycle management of snapshots so uh it's based off of a few uh key ingredients so",
    "start": "3311720",
    "end": "3316839"
  },
  {
    "text": "Lambda our ECT run command which is uh a way to distribute system commands to all",
    "start": "3316839",
    "end": "3323119"
  },
  {
    "text": "your instances so whether that's B scripts that you want to run Powershell scripts it allows a centralized method",
    "start": "3323119",
    "end": "3329480"
  },
  {
    "text": "of sending out those commands to all your instances if you haven't heard of run command there's a link for it and then",
    "start": "3329480",
    "end": "3336359"
  },
  {
    "text": "robust use of tagging to actually drive all the logic of this workflow so we start with the instance",
    "start": "3336359",
    "end": "3343520"
  },
  {
    "text": "and and we put a couple of tag tags on the instance itself one back back me up this is backup worthy instance and two",
    "start": "3343520",
    "end": "3351640"
  },
  {
    "text": "any snapshots you take of my volumes here's the retention I would like for those",
    "start": "3351640",
    "end": "3357280"
  },
  {
    "text": "volumes we have a scheduled Lambda function that's going to run every",
    "start": "3357280",
    "end": "3362920"
  },
  {
    "text": "day it's going to search for all those instances that are tagged back up it's going to use ec2 run to send all",
    "start": "3362920",
    "end": "3371680"
  },
  {
    "text": "of those good best practice snapshotting commands that I mentioned earlier to Qui the file system make sure it's ready and",
    "start": "3371680",
    "end": "3378280"
  },
  {
    "text": "take good snapshots and then obviously snapshot all the volumes and in the process it's",
    "start": "3378280",
    "end": "3385319"
  },
  {
    "text": "going to compute the expiration date of those volumes and slap that tag onto the",
    "start": "3385319",
    "end": "3391079"
  },
  {
    "text": "Vol onto the snapshot itself so the snapshot will now have a tag that says expire me on a certain",
    "start": "3391079",
    "end": "3396319"
  },
  {
    "text": "date then on the opposite end for the actual life cycle expiration",
    "start": "3396319",
    "end": "3402359"
  },
  {
    "start": "3397000",
    "end": "3397000"
  },
  {
    "text": "we have a separate Lambda function that's going to go out and look every day for tags of expiration for",
    "start": "3402359",
    "end": "3410640"
  },
  {
    "text": "that day and it's going to delete those",
    "start": "3410640",
    "end": "3415720"
  },
  {
    "text": "so very simple two Lambda functions and a few tags to do some some very robust",
    "start": "3415720",
    "end": "3420760"
  },
  {
    "text": "house cleaning of your snapshots so hope that sounds useful",
    "start": "3420760",
    "end": "3426400"
  },
  {
    "text": "yeah there it is um we have my team um put up a prototype check it out let us",
    "start": "3426400",
    "end": "3432720"
  },
  {
    "text": "know what you",
    "start": "3432720",
    "end": "3435078"
  },
  {
    "text": "think all right last but not least uh quick best practice around encryption so",
    "start": "3441920",
    "end": "3447839"
  },
  {
    "start": "3444000",
    "end": "3444000"
  },
  {
    "text": "I mentioned encryption is checkbox um very easy to do on your volume uh",
    "start": "3447839",
    "end": "3453039"
  },
  {
    "text": "there's one thing that I would recommend though and that is not going with the",
    "start": "3453039",
    "end": "3458400"
  },
  {
    "text": "default which is to use um the default AWS EBS master key we always recommend",
    "start": "3458400",
    "end": "3464359"
  },
  {
    "text": "to create your own customer managed key instead of of using the default key it gives you a lot more control and why is",
    "start": "3464359",
    "end": "3471640"
  },
  {
    "text": "that so this is real quick how do you create your own customer managed key",
    "start": "3471640",
    "end": "3477599"
  },
  {
    "text": "just go into the IM am console identity and access management encryption Keys create a new KMS master key name it",
    "start": "3477599",
    "end": "3486200"
  },
  {
    "text": "whatever you want I name this one EBS Master because that's what I'm going to use it for and by using your own key you get a",
    "start": "3486200",
    "end": "3494280"
  },
  {
    "text": "lot more control about how the key is used so you get to define the rotation policy for that key you get to enable uh",
    "start": "3494280",
    "end": "3502000"
  },
  {
    "text": "Cloud watch auditing so you can tell who's using it what they're using it for and you can control who can use it",
    "start": "3502000",
    "end": "3508920"
  },
  {
    "text": "and who can use it for what you know uh you might want one team to use it for encryption and one team to be able to",
    "start": "3508920",
    "end": "3513960"
  },
  {
    "text": "use it for decryption and obviously who can actually administer the key so these",
    "start": "3513960",
    "end": "3521119"
  },
  {
    "text": "options would not be possible if you went with just the default key so here you see we're back at the check boox",
    "start": "3521119",
    "end": "3527720"
  },
  {
    "text": "encryption instead I've changed the master key to be the one that I just created for",
    "start": "3527720",
    "end": "3533200"
  },
  {
    "text": "myself and how does this actually work so we use a process called envelope encryption so this is a hierarchy of",
    "start": "3533720",
    "end": "3540039"
  },
  {
    "start": "3534000",
    "end": "3534000"
  },
  {
    "text": "encryption for uh EBS so the master key that you just created is stored in our",
    "start": "3540039",
    "end": "3545880"
  },
  {
    "text": "key management system it never leaves the key key management system but what it does do is allow you to individually",
    "start": "3545880",
    "end": "3553319"
  },
  {
    "text": "encrypt one data key per volume so every volume has its own unique data key that",
    "start": "3553319",
    "end": "3558680"
  },
  {
    "text": "is uh envelope encrypted by the master key and then stored as metadata in the",
    "start": "3558680",
    "end": "3565200"
  },
  {
    "text": "volume itself so that's double encrypted data key in the metadata of the volume",
    "start": "3565200",
    "end": "3571319"
  },
  {
    "text": "when you actually want to mount the volume and use it the KMS service is called again to decrypt that key from",
    "start": "3571319",
    "end": "3578160"
  },
  {
    "text": "metadata and then that decrypted key is stored in the memory of the actual instance you're trying to mount the volume",
    "start": "3578160",
    "end": "3585359"
  },
  {
    "text": "on so now you're actually able to access and use the volume decrypt and encrypt",
    "start": "3586079",
    "end": "3591520"
  },
  {
    "text": "the data on the volume with the key and now why do why would we do this",
    "start": "3591520",
    "end": "3597319"
  },
  {
    "text": "obviously it limits the exposure risk so if there's any key that's for some reason compromised it's it's contained",
    "start": "3597319",
    "end": "3603920"
  },
  {
    "text": "the blast radius is a single volume it's not your entire uh EBS inventory the",
    "start": "3603920",
    "end": "3610200"
  },
  {
    "text": "performance is obviously a huge win because the encryption is being done in memory in the instance itself where the",
    "start": "3610200",
    "end": "3616079"
  },
  {
    "text": "where the data is you're not shipping data across the wire back and forth the KMS to encrypt bulk data and it",
    "start": "3616079",
    "end": "3621440"
  },
  {
    "text": "simplifies your key management um you have one mastery key that that can encrypt any number of data",
    "start": "3621440",
    "end": "3629119"
  },
  {
    "text": "keys and that's it thank you very much I really appreciate",
    "start": "3629400",
    "end": "3636520"
  },
  {
    "text": "time",
    "start": "3639559",
    "end": "3642559"
  }
]