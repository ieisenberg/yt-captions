[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "In this video, you’ll see an example of\nstreaming ETL with Amazon Managed",
    "start": "0",
    "end": "3958"
  },
  {
    "text": "Streaming for Apache Kafka (Amazon \nMSK) and Kinesis Data Analytics.",
    "start": "3958",
    "end": "8627"
  },
  {
    "text": "With this solution, you can run Apache \nKafka clusters on AWS, build data",
    "start": "9176",
    "end": "13893"
  },
  {
    "text": "processing logic for streaming \napplications, and consume data in",
    "start": "13893",
    "end": "17132"
  },
  {
    "text": "near-real time to gain faster insights.",
    "start": "17132",
    "end": "19386"
  },
  {
    "start": "20000",
    "end": "40000"
  },
  {
    "text": "For our example, we’ll generate some \nsample forklift data and stream it to an",
    "start": "20210",
    "end": "23835"
  },
  {
    "text": "Apache Kafka cluster running in Amazon MSK.",
    "start": "23835",
    "end": "26820"
  },
  {
    "text": "We’ll then use Kinesis Data Analytics \nto perform real-time data processing",
    "start": "27547",
    "end": "31450"
  },
  {
    "text": "and send the processed data to Amazon\nOpenSearch Service, where we can view",
    "start": "31450",
    "end": "35345"
  },
  {
    "text": "and query a dashboard of \nresults, even from a mobile app.",
    "start": "35345",
    "end": "38340"
  },
  {
    "start": "40000",
    "end": "88000"
  },
  {
    "text": "Amazon MSK is a fully managed service \nfor running Apache Kafka clusters on AWS.",
    "start": "40505",
    "end": "45796"
  },
  {
    "text": "To get started, let’s see how\n to create a Kafka cluster.",
    "start": "46313",
    "end": "49154"
  },
  {
    "text": "We’ll use the Quick create option, which \ndefaults to the recommended settings.",
    "start": "50252",
    "end": "54000"
  },
  {
    "text": "For the cluster type, we can \nchoose Serverless or Provisioned.",
    "start": "56084",
    "end": "59260"
  },
  {
    "text": "The Serverless option automatically \nscales with application load.",
    "start": "59600",
    "end": "62795"
  },
  {
    "text": "The Provisioned option allows us to \ncustomize the broker settings for the cluster.",
    "start": "63119",
    "end": "66850"
  },
  {
    "text": "We can also customize \nthese cluster settings.",
    "start": "69000",
    "end": "71176"
  },
  {
    "text": "For the purposes of this video, \nwe already created a cluster,",
    "start": "73195",
    "end": "76140"
  },
  {
    "text": "so let’s cancel out here \nand go to the Clusters page.",
    "start": "76140",
    "end": "78950"
  },
  {
    "text": "Here’s our cluster.",
    "start": "81325",
    "end": "82323"
  },
  {
    "text": "Let’s look at it.",
    "start": "82323",
    "end": "83125"
  },
  {
    "text": "The cluster is active and ready to receive data.",
    "start": "84158",
    "end": "86545"
  },
  {
    "start": "88000",
    "end": "212000"
  },
  {
    "text": "We’ll now run two background processes to \nstart sending continuous data to our cluster.",
    "start": "88823",
    "end": "93068"
  },
  {
    "text": "This dummy data stream represents real- \ntime operations data for a fleet of forklifts.",
    "start": "93375",
    "end": "97688"
  },
  {
    "text": "The operations data we are sending \nincludes jobs assigned to forklifts,",
    "start": "98480",
    "end": "101917"
  },
  {
    "text": "along with forklift \nperformance and health metrics.",
    "start": "101917",
    "end": "104275"
  },
  {
    "text": "Now, we'll go to Amazon Kinesis \nto perform data analytics processing",
    "start": "106036",
    "end": "109555"
  },
  {
    "text": "on our streaming Kafka data.",
    "start": "109555",
    "end": "111067"
  },
  {
    "text": "On this tab, we can see \nour streaming applications.",
    "start": "113280",
    "end": "115715"
  },
  {
    "text": "Let’s select the Studio tab.",
    "start": "116314",
    "end": "117978"
  },
  {
    "text": "From here, we can create a Studio \nnotebook to build our application.",
    "start": "119173",
    "end": "122328"
  },
  {
    "text": "Let’s create this notebook \nwith custom settings.",
    "start": "123476",
    "end": "125702"
  },
  {
    "text": "We’ll give the notebook a name \nand continue to the next step.",
    "start": "128901",
    "end": "131620"
  },
  {
    "text": "We’ll leave the default AWS Identity and \nAccess Management (IAM) permissions.",
    "start": "134139",
    "end": "138862"
  },
  {
    "text": "Next, we can specify or create the AWS \nGlue database that stores the metadata",
    "start": "139944",
    "end": "144417"
  },
  {
    "text": "for the source and destination \nreferenced from our notebook.",
    "start": "144417",
    "end": "147164"
  },
  {
    "text": "In this case, we’ll choose \nan existing database.",
    "start": "147567",
    "end": "149641"
  },
  {
    "text": "Let’s proceed to the next screen.",
    "start": "152113",
    "end": "153702"
  },
  {
    "text": "Let’s retain the configurations for \nscaling and for logging and monitoring.",
    "start": "156851",
    "end": "160661"
  },
  {
    "text": "Let’s update the VPC connectivity setting.",
    "start": "162147",
    "end": "164311"
  },
  {
    "text": "As our Amazon MSK cluster is running \nwithin the same AWS Region and",
    "start": "165538",
    "end": "169375"
  },
  {
    "text": "account as our Amazon Kinesis application,",
    "start": "169375",
    "end": "171588"
  },
  {
    "text": "we can select VPC configuration \nbased on Amazon MSK cluster.",
    "start": "171588",
    "end": "175917"
  },
  {
    "text": "When we specify our MSK cluster, the \nnetwork settings are automatically populated.",
    "start": "177015",
    "end": "181026"
  },
  {
    "text": "Studio notebooks also add default connectors.",
    "start": "183724",
    "end": "186346"
  },
  {
    "text": "Connectors are required for connecting \nto and querying Amazon MSK data.",
    "start": "186733",
    "end": "190819"
  },
  {
    "text": "We’ll leave the defaults for the other \nsections and continue to the last step.",
    "start": "192000",
    "end": "195478"
  },
  {
    "text": "Now we can review and \ncreate the Studio notebook.",
    "start": "197026",
    "end": "199504"
  },
  {
    "text": "For the purposes of this example, \nwe already have a notebook created,",
    "start": "200118",
    "end": "203030"
  },
  {
    "text": "so we’ll cancel this one.",
    "start": "203031",
    "end": "204242"
  },
  {
    "text": "Here’s the notebook, which has the \nsame configurations we just saw.",
    "start": "205486",
    "end": "208536"
  },
  {
    "text": "Let’s open it in Apache Zeppelin.",
    "start": "209683",
    "end": "211386"
  },
  {
    "start": "212000",
    "end": "327000"
  },
  {
    "text": "With Apache Zeppelin we can write \nprocessing queries in SQL, Python,",
    "start": "212582",
    "end": "216105"
  },
  {
    "text": "or Scala, and test our analysis in real time.",
    "start": "216105",
    "end": "218776"
  },
  {
    "text": "Here’s the notebook we \ncreated for this demonstration.",
    "start": "219955",
    "end": "222285"
  },
  {
    "text": "We’re using Flink SQL to create two \ntables to connect with the two Kafka",
    "start": "223431",
    "end": "226764"
  },
  {
    "text": "topics that are streaming our forklift data.",
    "start": "226764",
    "end": "228962"
  },
  {
    "text": "We’ll run our SQL code to create our tables.",
    "start": "229430",
    "end": "231724"
  },
  {
    "text": "The tables have been created \nand are ready to be queried.",
    "start": "236343",
    "end": "238664"
  },
  {
    "text": "Behind the scenes, forklift data is still being \ncontinuously streamed to our Kafka topics.",
    "start": "240942",
    "end": "245412"
  },
  {
    "text": "Let’s run a query to select our forklift job table.",
    "start": "247577",
    "end": "250375"
  },
  {
    "text": "The incoming data on the Kafka \ntopic is streaming to this table.",
    "start": "252266",
    "end": "255325"
  },
  {
    "text": "This table is showing all forklift job data:\nthe forklift ID, job ID, when it has started,",
    "start": "256553",
    "end": "261996"
  },
  {
    "text": "when it is ending, who \n is driving it, and so on.",
    "start": "261996",
    "end": "264771"
  },
  {
    "text": "Next, let's run an aggregation query on \nour forkliftjob table to see in real-time",
    "start": "265821",
    "end": "270070"
  },
  {
    "text": "how many machines are \nrunning, faulty, or ideal.",
    "start": "270070",
    "end": "272513"
  },
  {
    "text": "Studio notebook allows many \nvisualization formats for query results.",
    "start": "274758",
    "end": "278510"
  },
  {
    "text": "We’ve chosen the bar chart \nfor our machine status results.",
    "start": "279000",
    "end": "281731"
  },
  {
    "text": "We can also change the settings to display \ndifferent columns from our database table.",
    "start": "282780",
    "end": "286453"
  },
  {
    "text": "The chart changes in real time \nas the streaming data arrives.",
    "start": "287584",
    "end": "290538"
  },
  {
    "text": "Our forkliftstatus table provides deeper \ninsight into forklift machine status.",
    "start": "291620",
    "end": "296139"
  },
  {
    "text": "This table includes details such as \ncoolant, battery, and shock levels.",
    "start": "296593",
    "end": "300307"
  },
  {
    "text": "Next, we’ll run a query to see real-time \naggregation about max shock level,",
    "start": "301374",
    "end": "305173"
  },
  {
    "text": "max battery level, max coolant level, \nand machine status at a given time.",
    "start": "305174",
    "end": "309066"
  },
  {
    "text": "Our query retrieves aggregate battery, \ncoolant, and shock data every 10 seconds.",
    "start": "310181",
    "end": "314377"
  },
  {
    "text": "Now that we’re happy with our data \nprocessing logic, we will create an",
    "start": "316460",
    "end": "319518"
  },
  {
    "text": "OpenSearch based off our SQL code.",
    "start": "319518",
    "end": "321601"
  },
  {
    "text": "We’ll create another sink table that \nwill be populated from our view data.",
    "start": "327578",
    "end": "330797"
  },
  {
    "text": "An Amazon OpenSearch Service cluster \nis connected to this table by an index.",
    "start": "331362",
    "end": "335254"
  },
  {
    "text": "Let’s quickly switch over to Amazon \nOpenSearch Service to see where",
    "start": "336384",
    "end": "339363"
  },
  {
    "text": "we retrieved this index from.",
    "start": "339363",
    "end": "341155"
  },
  {
    "text": "We have a previously created \nforklift cluster running here.",
    "start": "342414",
    "end": "345286"
  },
  {
    "text": "The index settings from this cluster are used \nin Apache Zeppelin in the sink table creation.",
    "start": "346401",
    "end": "350623"
  },
  {
    "text": "The sink table is now created.",
    "start": "357327",
    "end": "358921"
  },
  {
    "text": "Let’s switch to OpenSearch.",
    "start": "358921",
    "end": "360358"
  },
  {
    "text": "On the OpenSearch dashboard, we \ncan see the data we’re consuming.",
    "start": "361440",
    "end": "364387"
  },
  {
    "text": "We can filter by specific \nfields to drill down further.",
    "start": "366309",
    "end": "369000"
  },
  {
    "text": "We can also look at metadata \nfor a specific machine.",
    "start": "380903",
    "end": "383390"
  },
  {
    "start": "388000",
    "end": "433000"
  },
  {
    "text": "Studio notebooks are useful for creating\n ad-hoc queries and building data",
    "start": "388397",
    "end": "391897"
  },
  {
    "text": "processing logic, but aren’t \ndesigned to be run continuously.",
    "start": "391897",
    "end": "395015"
  },
  {
    "text": "Ultimately, data processing logic should \nbe deployed as an Apache Flink application.",
    "start": "395645",
    "end": "399899"
  },
  {
    "text": "We can build the Flink application \nright from here and then deploy it as",
    "start": "401030",
    "end": "404041"
  },
  {
    "text": "a Kinesis Data Analytics application.",
    "start": "404041",
    "end": "406130"
  },
  {
    "text": "Once created, the streaming application can \nbe accessed within Amazon Kinesis Analytics.",
    "start": "407358",
    "end": "411790"
  },
  {
    "text": "You’ve just seen how an example \nof streaming ETL with Amazon",
    "start": "418687",
    "end": "421360"
  },
  {
    "text": "MSK and Kinesis Data Analytics.",
    "start": "421360",
    "end": "423771"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "425047",
    "end": "428248"
  },
  {
    "text": "Thanks for watching. Now it's your turn to try.",
    "start": "428410",
    "end": "430360"
  }
]