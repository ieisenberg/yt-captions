[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "I wanted to talk a little bit about model quality tuning and and sage maker this is something that I think is one of",
    "start": "30",
    "end": "8220"
  },
  {
    "text": "the strengths of sage maker and is something that is not only a strength of",
    "start": "8220",
    "end": "13440"
  },
  {
    "text": "sage maker but is something that is very difficult to build on premises hyper",
    "start": "13440",
    "end": "18990"
  },
  {
    "text": "parameter tuning is requires a large amount of resources for a short amount",
    "start": "18990",
    "end": "24630"
  },
  {
    "text": "of time there are there's there's at least one customer that I know of that prior to",
    "start": "24630",
    "end": "30720"
  },
  {
    "text": "sage maker bought a hundred nvidia dgx ones in order to do hyper parameter",
    "start": "30720",
    "end": "37590"
  },
  {
    "text": "tuning that's that's a very expensive proposition and not something that we",
    "start": "37590",
    "end": "43789"
  },
  {
    "text": "expect every customer to do so I'm gonna",
    "start": "43789",
    "end": "49649"
  },
  {
    "text": "briefly talk about sage maker since most of you were here for my left the last talk I won't go into as much depth and",
    "start": "49649",
    "end": "56940"
  },
  {
    "text": "then we'll dive into what hyper parameter tuning is why you would want to do it",
    "start": "56940",
    "end": "62190"
  },
  {
    "text": "why sage maker is a great place to do that so it's skip over here skip over",
    "start": "62190",
    "end": "68790"
  },
  {
    "start": "65000",
    "end": "134000"
  },
  {
    "text": "the stock because you've probably all seen it by now and then skip to to the the customer",
    "start": "68790",
    "end": "77040"
  },
  {
    "text": "story so another customer that we've seen that is very hoppy with sage maker",
    "start": "77040",
    "end": "83520"
  },
  {
    "text": "is hotels comm which is part of Expedia and they are interested in ways to move",
    "start": "83520",
    "end": "91530"
  },
  {
    "text": "faster they have a lot of machine learning problems and and Matt frier who's was an amazing achieve data",
    "start": "91530",
    "end": "99390"
  },
  {
    "text": "scientist over there he's very interested in making sure that they're able to not only do very sophisticated",
    "start": "99390",
    "end": "108030"
  },
  {
    "text": "data science but able to do it at the scale that hotels.com operates it and that's part of the",
    "start": "108030",
    "end": "116009"
  },
  {
    "text": "reason why they've been looking at sage maker and and matt has been pushing us to create a set of features that allows",
    "start": "116009",
    "end": "123659"
  },
  {
    "text": "them to do things like hyper parameter tuning and much more sophisticated data science techniques in a way that's",
    "start": "123659",
    "end": "130649"
  },
  {
    "text": "compatible with with their operations so what is hyper",
    "start": "130649",
    "end": "136610"
  },
  {
    "start": "134000",
    "end": "213000"
  },
  {
    "text": "parameter tuning essentially you have this set of parameters that dictates the",
    "start": "136610",
    "end": "143780"
  },
  {
    "text": "way that machine learning algorithms work it's things like learning rate and",
    "start": "143780",
    "end": "149900"
  },
  {
    "text": "so on and those have a trade-off between the speed of training and the accuracy",
    "start": "149900",
    "end": "156470"
  },
  {
    "text": "of training and in in some sense this is truly important because if you select if",
    "start": "156470",
    "end": "164900"
  },
  {
    "text": "you buy us all the way towards accuracy for instance you could have a model that ends up never converging as a result of",
    "start": "164900",
    "end": "171349"
  },
  {
    "text": "that or if you end up saying well I really want results really quickly and",
    "start": "171349",
    "end": "177200"
  },
  {
    "text": "I'm gonna select a go all the other way for for speed you may end up with",
    "start": "177200",
    "end": "184329"
  },
  {
    "text": "oscillations and a and a very unstable model result that doesn't have the",
    "start": "184329",
    "end": "189519"
  },
  {
    "text": "accuracy to meet business goals so essentially what you want to do is you",
    "start": "189519",
    "end": "195049"
  },
  {
    "text": "want to select this this tuple of hyper parameters that is able to make the",
    "start": "195049",
    "end": "200959"
  },
  {
    "text": "right trade-offs of in order to minimize the loss function in the amount of time",
    "start": "200959",
    "end": "206419"
  },
  {
    "text": "that is appropriate for for training your machine learning model and is able",
    "start": "206419",
    "end": "212090"
  },
  {
    "text": "to meet your business goals so how do you actually find those tuples one way",
    "start": "212090",
    "end": "218840"
  },
  {
    "start": "213000",
    "end": "276000"
  },
  {
    "text": "that people do it is they do grid search they say well you know what I can't",
    "start": "218840",
    "end": "224239"
  },
  {
    "text": "think of a better way to do it so let me just explore the entire hyper parameter space and this this",
    "start": "224239",
    "end": "231410"
  },
  {
    "text": "illustration has two dimensions it's usually more than that for for instance",
    "start": "231410",
    "end": "236569"
  },
  {
    "text": "for extra boost there's there's nine parameters that people tend to adjust which means that you end up with a nine",
    "start": "236569",
    "end": "244730"
  },
  {
    "text": "dimensional space in which you're exploring all of these parameters which",
    "start": "244730",
    "end": "250190"
  },
  {
    "text": "if you think about the cost of that that makes a $1 training job end up costing",
    "start": "250190",
    "end": "256370"
  },
  {
    "text": "two million dollars which is crazy this is an almost untenable way to do hyper",
    "start": "256370",
    "end": "263570"
  },
  {
    "text": "parameter tuning unless you have a data scientist that's really picking and picking specific",
    "start": "263570",
    "end": "271940"
  },
  {
    "text": "regions of the hyper parameter space in which to to explore a second way that",
    "start": "271940",
    "end": "279139"
  },
  {
    "start": "276000",
    "end": "342000"
  },
  {
    "text": "people have have tended to look at hyper parameter tuning is they they said well",
    "start": "279139",
    "end": "284300"
  },
  {
    "text": "you know what grid search is a is a little strange and and is probably not",
    "start": "284300",
    "end": "290540"
  },
  {
    "text": "the best way to make sure that we have adequate coverage so let's try random",
    "start": "290540",
    "end": "295820"
  },
  {
    "text": "search and random searches one that has ended up being a little more efficient",
    "start": "295820",
    "end": "302060"
  },
  {
    "text": "than grid search at covering the space because there's no sanctity about those grid points and you end up having",
    "start": "302060",
    "end": "308169"
  },
  {
    "text": "additional coverage in certain areas which helped the data scientist find the",
    "start": "308169",
    "end": "314150"
  },
  {
    "text": "regions of interest and you can usually do a little better than grid if you if you do that but you still need a large",
    "start": "314150",
    "end": "321830"
  },
  {
    "text": "number of tuples in order to do to do well so you more or less haven't really",
    "start": "321830",
    "end": "327520"
  },
  {
    "text": "erased that 2 million dollar problem you've gotten and so that that 2 million",
    "start": "327520",
    "end": "332690"
  },
  {
    "text": "dollars has better results so how do we make it so that we're actually addressing that $1 training becomes 2",
    "start": "332690",
    "end": "340010"
  },
  {
    "text": "million dollars how do we address that problem and the appropri have looked at and are using in sage",
    "start": "340010",
    "end": "347840"
  },
  {
    "start": "342000",
    "end": "519000"
  },
  {
    "text": "maker is something called Bayesian optimization where we basically view the",
    "start": "347840",
    "end": "353000"
  },
  {
    "text": "the objective function that you're trying to minimize in the case of a loss function or or maximize in other cases",
    "start": "353000",
    "end": "360490"
  },
  {
    "text": "you're essentially trying to optimize that and you're assuming a surrogate",
    "start": "360490",
    "end": "366289"
  },
  {
    "text": "function that is based on the hyper parameters so based on that assumption",
    "start": "366289",
    "end": "371450"
  },
  {
    "text": "you say I think this is what it looks like in the two-dimensional space and",
    "start": "371450",
    "end": "377090"
  },
  {
    "text": "you provide these surrogate functions and then you'll say well I'm gonna run a set of training jobs and test the",
    "start": "377090",
    "end": "383479"
  },
  {
    "text": "hypothesis and then come up with a hypothesis that fits the data that I have and then repeat that process over",
    "start": "383479",
    "end": "390919"
  },
  {
    "text": "and over so what does that look like basically you start out with with something that looks like this where",
    "start": "390919",
    "end": "396349"
  },
  {
    "text": "you've got in this case you've got two data and you say well I know I know the value",
    "start": "396349",
    "end": "403130"
  },
  {
    "text": "is in these two data points and I think that the objective function lies between",
    "start": "403130",
    "end": "408860"
  },
  {
    "text": "these limits so let me figure out based on that where I should explore and then",
    "start": "408860",
    "end": "414919"
  },
  {
    "text": "you say well I'm gonna explore that that point there because I think that's where",
    "start": "414919",
    "end": "421190"
  },
  {
    "text": "the objective function might be maximized and then you take that and you run the training job and you say okay",
    "start": "421190",
    "end": "427790"
  },
  {
    "text": "well now I know exactly what that value is so I don't need to guess what the",
    "start": "427790",
    "end": "432919"
  },
  {
    "text": "value is at that point now I have a better estimation of the objective function in the in the hyper",
    "start": "432919",
    "end": "439639"
  },
  {
    "text": "parameter space so now I can figure out well what's the next best thing that I",
    "start": "439639",
    "end": "445010"
  },
  {
    "text": "can test at and then you pick that point and then you do the same thing and over",
    "start": "445010",
    "end": "450979"
  },
  {
    "text": "the course of running these training jobs you eventually have a better picture of what that objective function",
    "start": "450979",
    "end": "457940"
  },
  {
    "text": "looks like in the hyper parameter space and you're able to optimize over time the the other thing that you do as part",
    "start": "457940",
    "end": "465140"
  },
  {
    "text": "of this process is you you actually you don't do a grid search in your in in way",
    "start": "465140",
    "end": "473300"
  },
  {
    "text": "where you're running you actually do a random search as part of this so you pick a random set of points and that",
    "start": "473300",
    "end": "480440"
  },
  {
    "text": "random set of points feeds into your estimation of the objective function and then the next time instead of picking",
    "start": "480440",
    "end": "487660"
  },
  {
    "text": "exactly one point to run you you pick a set of random points around that region",
    "start": "487660",
    "end": "493280"
  },
  {
    "text": "and you say I'm gonna explore the parameter space around there because I think that's a good region of interest",
    "start": "493280",
    "end": "499130"
  },
  {
    "text": "so you essentially have this stepwise process of narrowing in on a region that",
    "start": "499130",
    "end": "505789"
  },
  {
    "text": "you think is interesting and because there's a random component to it and you're not just picking one point that",
    "start": "505789",
    "end": "510950"
  },
  {
    "text": "means that you don't get you aren't just likely to get stuck in local maxima and",
    "start": "510950",
    "end": "516529"
  },
  {
    "text": "minimize you otherwise would can we do better than bayesian optimization",
    "start": "516529",
    "end": "523570"
  },
  {
    "start": "519000",
    "end": "573000"
  },
  {
    "text": "possibly possibly this is an area of continuing research we we actually the",
    "start": "523570",
    "end": "530780"
  },
  {
    "text": "the Amazon team that that worked on hyper parameter the the hyper parameter tuning system",
    "start": "530780",
    "end": "538040"
  },
  {
    "text": "that we have in Sage maker actually published a paper on adaptive adaptive",
    "start": "538040",
    "end": "544310"
  },
  {
    "text": "Bayesian linear regression so that was published last last week I think it was",
    "start": "544310",
    "end": "549639"
  },
  {
    "text": "so this is an area of continuing research there's also some really exotic",
    "start": "549639",
    "end": "556240"
  },
  {
    "text": "approaches that people use like genetic algorithms and so on so maybe there's a",
    "start": "556240",
    "end": "561800"
  },
  {
    "text": "possibility that you can do better but in practice it seems like most of the approaches that are out there right now",
    "start": "561800",
    "end": "567709"
  },
  {
    "text": "that have shown to be really good are based on Bayesian optimization so",
    "start": "567709",
    "end": "575889"
  },
  {
    "start": "573000",
    "end": "676000"
  },
  {
    "text": "because of the time I'm gonna kind of really quickly go over the demo and show",
    "start": "575889",
    "end": "582649"
  },
  {
    "text": "you what it looks like to do to do hyper parameter optimization in and sage maker",
    "start": "582649",
    "end": "590559"
  },
  {
    "text": "essentially what you have essentially what you have here is I'm setting up the",
    "start": "590559",
    "end": "596180"
  },
  {
    "text": "training job and then when you set up the training job instead of skipping",
    "start": "596180",
    "end": "603230"
  },
  {
    "text": "over kind of skipping over where you would explore the data as part of the",
    "start": "603230",
    "end": "608569"
  },
  {
    "text": "process of setting up the training job and then you would instead of setting up",
    "start": "608569",
    "end": "614930"
  },
  {
    "text": "the the estimator as as a single set of",
    "start": "614930",
    "end": "621050"
  },
  {
    "text": "hyper parameters you would actually provide ranges for those those hyper",
    "start": "621050",
    "end": "626509"
  },
  {
    "text": "parameters and then let's see if we can find this is essentially the ranges that",
    "start": "626509",
    "end": "632839"
  },
  {
    "text": "you would provide so there are certain hyper parameters that you would fix and",
    "start": "632839",
    "end": "637879"
  },
  {
    "text": "then others that you would provide as as continuous or discrete ranges and the",
    "start": "637879",
    "end": "644959"
  },
  {
    "text": "hyper parameter tuning will actually optimize on the basis of that and this",
    "start": "644959",
    "end": "650240"
  },
  {
    "text": "this example notebook for doing hyper parameter tuning is actually available if you just start up a notebook",
    "start": "650240",
    "end": "657079"
  },
  {
    "text": "environment so that's something that if you look at the the brain on the left",
    "start": "657079",
    "end": "662980"
  },
  {
    "text": "there's a set of examples and hyper parameter tuning is one of those examples so if you would like to",
    "start": "662980",
    "end": "668990"
  },
  {
    "text": "explore it in detail this is something that you can you can essentially run",
    "start": "668990",
    "end": "674060"
  },
  {
    "text": "yourself through your your account but the result of that is that you have this",
    "start": "674060",
    "end": "680450"
  },
  {
    "start": "676000",
    "end": "733000"
  },
  {
    "text": "distribution once you run it you have this distribution of what it looks like",
    "start": "680450",
    "end": "686260"
  },
  {
    "text": "the the objective function looks like in the hyper parameter space so this is a",
    "start": "686260",
    "end": "692060"
  },
  {
    "text": "graph of max depth which is one of the hyper parameters that we explored and you can see that there's maybe some",
    "start": "692060",
    "end": "699649"
  },
  {
    "text": "effect of of max depth on the objective function and then versus ADA is is maybe",
    "start": "699649",
    "end": "706610"
  },
  {
    "text": "flat not necessarily a fruitful area",
    "start": "706610",
    "end": "711820"
  },
  {
    "text": "minimum child weight is probably not but essentially you end up with this graph",
    "start": "711820",
    "end": "718040"
  },
  {
    "text": "of areas in the hyper parameter space that Bayesian optimization looked up and",
    "start": "718040",
    "end": "723380"
  },
  {
    "text": "thought was interesting and you have the results of each of those training jobs that you can go back and look at and and",
    "start": "723380",
    "end": "730660"
  },
  {
    "text": "guide further exploration so how does how's the model tuning and in Sage Maker",
    "start": "730660",
    "end": "737390"
  },
  {
    "start": "733000",
    "end": "888000"
  },
  {
    "text": "work I kind of briefly glossed over that demo essentially it works very similarly",
    "start": "737390",
    "end": "745490"
  },
  {
    "text": "to safe maker training jobs so instead of providing specific values for hyper",
    "start": "745490",
    "end": "751310"
  },
  {
    "text": "parameters you provide these ranges of hyper parameters that you want the algorithm to explore through you have",
    "start": "751310",
    "end": "759019"
  },
  {
    "text": "full full control over the entire process you specify the objective function you specified the hyper",
    "start": "759019",
    "end": "765140"
  },
  {
    "text": "parameter ranges but we're providing you with tools that allow you to automate",
    "start": "765140",
    "end": "770660"
  },
  {
    "text": "the process of spawning out these training jobs you can also minimize the cost by saying I want the training jobs",
    "start": "770660",
    "end": "778010"
  },
  {
    "text": "to only run this certain amount of time I only want this many training jobs I don't want you know you can specify",
    "start": "778010",
    "end": "785120"
  },
  {
    "text": "anywhere between you know 10 or or a hundred or 500 training jobs it's all up",
    "start": "785120",
    "end": "792260"
  },
  {
    "text": "to you and you can choose the level of parallelization also and probably the",
    "start": "792260",
    "end": "797779"
  },
  {
    "text": "most important thing here is that this will work across all of the frameworks across all of the",
    "start": "797779",
    "end": "803390"
  },
  {
    "text": "algorithms that we we provide in sage maker as well as the algorithms that you",
    "start": "803390",
    "end": "809570"
  },
  {
    "text": "would bring to Sage Maker so as long as you're omitting the metrics for the",
    "start": "809570",
    "end": "815090"
  },
  {
    "text": "objective function into log lines you provide a regular expression as part of",
    "start": "815090",
    "end": "820250"
  },
  {
    "text": "hyper parameter tuning that allows us to identify which metric that we need to",
    "start": "820250",
    "end": "826310"
  },
  {
    "text": "optimize on and we'll do basing an optimization on that which means that you can bring your own IP and you can do",
    "start": "826310",
    "end": "832850"
  },
  {
    "text": "hyper hyper parameter tuning on your own algorithms which is something that most systems don't really allow and because",
    "start": "832850",
    "end": "840530"
  },
  {
    "text": "it uses Bayesian optimization it's pretty it's pretty cost efficient within",
    "start": "840530",
    "end": "846440"
  },
  {
    "text": "the last couple of days we actually announced early stopping so early",
    "start": "846440",
    "end": "852380"
  },
  {
    "text": "stopping is another feature that allows you to cut short training jobs that",
    "start": "852380",
    "end": "857390"
  },
  {
    "text": "weren't proving out to be very fruitful in their in their progression so that you don't pay for the full run of",
    "start": "857390",
    "end": "865010"
  },
  {
    "text": "training jobs where it's not useful we also provide incremental tuning which",
    "start": "865010",
    "end": "870260"
  },
  {
    "text": "means that if you have past results that you used if this is something that's of your tuning as part of a regular process",
    "start": "870260",
    "end": "878270"
  },
  {
    "text": "you can use the past results in order to inform future runs of hyper parameter",
    "start": "878270",
    "end": "883520"
  },
  {
    "text": "tuning as well which also allows you to minimize the cost really the power of",
    "start": "883520",
    "end": "892400"
  },
  {
    "start": "888000",
    "end": "948000"
  },
  {
    "text": "being able to allow you to tune any machine learning algorithm is something that I don't think there are many other",
    "start": "892400",
    "end": "899290"
  },
  {
    "text": "hyper parameter tuning systems out there that that allow not only does it allow",
    "start": "899290",
    "end": "905090"
  },
  {
    "text": "you to essentially have control over the entire process and and run a lot of jobs",
    "start": "905090",
    "end": "910340"
  },
  {
    "text": "in parallel but it lets you do it with your own your own algorithms and because",
    "start": "910340",
    "end": "917000"
  },
  {
    "text": "of that I think this is the only one that allows you to do it with with PI",
    "start": "917000",
    "end": "922520"
  },
  {
    "text": "torch as well so we were able to announce even with the pre-release Candidate of Pi torch 1.0 and we were",
    "start": "922520",
    "end": "929810"
  },
  {
    "text": "able to announce hyper parameter to on high torture 1.0 models so this is",
    "start": "929810",
    "end": "937369"
  },
  {
    "text": "something that allows us whether it's built in algorithms whether it's marketplace algorithms or any of these",
    "start": "937369",
    "end": "944540"
  },
  {
    "text": "frameworks or bring your own you can do a hyper parameter tuning on that see we",
    "start": "944540",
    "end": "951379"
  },
  {
    "start": "948000",
    "end": "986000"
  },
  {
    "text": "talked very briefly about that and the the workflow that we follow through with",
    "start": "951379",
    "end": "957709"
  },
  {
    "text": "here is that once we identify the objective metrics and the tuning strategy the hypo parameters system",
    "start": "957709",
    "end": "965089"
  },
  {
    "text": "spins off a set of parallel tuning jobs based on the parameters that you specify",
    "start": "965089",
    "end": "970549"
  },
  {
    "text": "supposing you specified for training jobs in parallel you would be able to run those for training jobs in parallel",
    "start": "970549",
    "end": "977059"
  },
  {
    "text": "and use that the results of that run in order to inform the next set of runs so",
    "start": "977059",
    "end": "983119"
  },
  {
    "text": "that's essentially the workflow that we follow through in general there are some",
    "start": "983119",
    "end": "988879"
  },
  {
    "start": "986000",
    "end": "1065000"
  },
  {
    "text": "some tips when it comes to model tuning categorical parameters are hard for",
    "start": "988879",
    "end": "995569"
  },
  {
    "text": "model tuning to accommodate in general keep the search ranges reasonable it's",
    "start": "995569",
    "end": "1002399"
  },
  {
    "text": "it's very tempting because of how well hyper parameter joining works to just",
    "start": "1002399",
    "end": "1007929"
  },
  {
    "text": "give it really really big ranges if you do that there's a couple of problems one",
    "start": "1007929",
    "end": "1014589"
  },
  {
    "text": "is you you may end up spending money in nonsensical regions where it doesn't",
    "start": "1014589",
    "end": "1021129"
  },
  {
    "text": "really make sense for you to explore and the the the other thing is you increase",
    "start": "1021129",
    "end": "1026889"
  },
  {
    "text": "the chances that depending on your degree of parallelization that you would end up stuck in a in a local minimum or",
    "start": "1026889",
    "end": "1033819"
  },
  {
    "text": "maximum so it's in general better that you view this as a tool by which you can",
    "start": "1033819",
    "end": "1041350"
  },
  {
    "text": "as an educated consumer of this so you can use hyper parameter tuning it's not",
    "start": "1041350",
    "end": "1048130"
  },
  {
    "text": "a complete black box you throw anything you want at it and it'll come up with good results and overall there's a",
    "start": "1048130",
    "end": "1054880"
  },
  {
    "text": "quality versus speed trade-off you want to be able to control that and make sure",
    "start": "1054880",
    "end": "1060010"
  },
  {
    "text": "that your balancing the business needs so I'm",
    "start": "1060010",
    "end": "1066900"
  },
  {
    "start": "1065000",
    "end": "1328000"
  },
  {
    "text": "gonna skip over this because of because of time and skip over two questions",
    "start": "1066900",
    "end": "1072660"
  },
  {
    "text": "which I'm sure you have go ahead",
    "start": "1072660",
    "end": "1078830"
  },
  {
    "text": "have I ever seen what Network unlearn as",
    "start": "1080060",
    "end": "1085920"
  },
  {
    "text": "you throw more data at it I haven't seen that no it's it's possible for it",
    "start": "1085920",
    "end": "1093900"
  },
  {
    "text": "depending on how how clean your data or how biased the the data set was at the",
    "start": "1093900",
    "end": "1099780"
  },
  {
    "text": "beginning it may be that I'd actually learned a bias at the beginning so if",
    "start": "1099780",
    "end": "1105870"
  },
  {
    "text": "you actually feed it data afterwards in that it would unlearn some of the biases",
    "start": "1105870",
    "end": "1113790"
  },
  {
    "text": "that it learned before it's a little harder for it to do that which is why most practitioners will do things like",
    "start": "1113790",
    "end": "1120210"
  },
  {
    "text": "shuffle in order to make sure or shuffle between epochs in order to make sure it",
    "start": "1120210",
    "end": "1125700"
  },
  {
    "text": "doesn't learn these the the order of things neural networks are finicky do",
    "start": "1125700",
    "end": "1136410"
  },
  {
    "text": "you have control of the breadth and depth of the network absolutely",
    "start": "1136410",
    "end": "1141540"
  },
  {
    "text": "but because we don't provide a generic neural network as as an algorithm what",
    "start": "1141540",
    "end": "1147750"
  },
  {
    "text": "we actually provide is is multiple deep learning frameworks in which you can can construct the neural network",
    "start": "1147750",
    "end": "1154920"
  },
  {
    "text": "caris for instance if you use Karis tensorflow or Karis MX net it's really easy to to build the network exactly how",
    "start": "1154920",
    "end": "1162330"
  },
  {
    "text": "you want so you have control over everything breadth depth everything",
    "start": "1162330",
    "end": "1167570"
  },
  {
    "text": "other question go ahead please ah that's a great question is is tuning",
    "start": "1167570",
    "end": "1177610"
  },
  {
    "text": "available in local mode the answer is no unfortunately tuning is one of those",
    "start": "1177610",
    "end": "1183250"
  },
  {
    "text": "things that you really really need to do in the cloud on a with with the full",
    "start": "1183250",
    "end": "1189760"
  },
  {
    "text": "breadth of instances available so unfortunately the answer is no I'm",
    "start": "1189760",
    "end": "1196560"
  },
  {
    "text": "looking at ways that we can potentially give some of the benefits of that but",
    "start": "1196560",
    "end": "1202660"
  },
  {
    "text": "it's really hard because you essentially expect that you have this vast pool of",
    "start": "1202660",
    "end": "1208690"
  },
  {
    "text": "instances that you can hop into in order to take advantage of that like supposing",
    "start": "1208690",
    "end": "1215500"
  },
  {
    "text": "supposing my training job needs it's relatively small",
    "start": "1215500",
    "end": "1221200"
  },
  {
    "text": "it's it means a little bit of parallelization I'm running three instances in parallel but I really want",
    "start": "1221200",
    "end": "1227860"
  },
  {
    "text": "to do hyper parameter tuning and I want to let's say run 20 20 jobs total 5 in",
    "start": "1227860",
    "end": "1236680"
  },
  {
    "text": "parallel essentially I'm asking for 15 instances in parallel through the entire",
    "start": "1236680",
    "end": "1242590"
  },
  {
    "text": "training process I mean this is really the kind of workload for which the cloud",
    "start": "1242590",
    "end": "1247720"
  },
  {
    "text": "was created because you want the instances for the period of time when you're hyper parameter tuning and then",
    "start": "1247720",
    "end": "1253930"
  },
  {
    "text": "after that you don't want them so it's it's really it's really a workload that",
    "start": "1253930",
    "end": "1259900"
  },
  {
    "text": "is well suited for the cloud but I you know given the question I will do my",
    "start": "1259900",
    "end": "1267430"
  },
  {
    "text": "best to figure out a way if we can provide some of the value in local mode as well",
    "start": "1267430",
    "end": "1273210"
  },
  {
    "text": "[Music]",
    "start": "1274610",
    "end": "1277720"
  },
  {
    "text": "yeah I mean it's certainly yeah it's",
    "start": "1280160",
    "end": "1286770"
  },
  {
    "text": "certainly something that I look into I know a lot of customers are interested in using local mode in various ways and",
    "start": "1286770",
    "end": "1293520"
  },
  {
    "text": "and that's one that that I'll take that feedback to to try and figure out what",
    "start": "1293520",
    "end": "1299250"
  },
  {
    "text": "we can do there go ahead yes the tuning",
    "start": "1299250",
    "end": "1305670"
  },
  {
    "text": "is available in all regions that we've launched in it in general I try and make sure that all sage maker features are",
    "start": "1305670",
    "end": "1312210"
  },
  {
    "text": "available everywhere it's only a few of the newest ones elastic inference",
    "start": "1312210",
    "end": "1318410"
  },
  {
    "text": "elastic inference ground truth neo I think are the three that are not available everywhere everything else",
    "start": "1318410",
    "end": "1324840"
  },
  {
    "text": "should be available everywhere go ahead",
    "start": "1324840",
    "end": "1329299"
  },
  {
    "start": "1328000",
    "end": "1589000"
  },
  {
    "text": "yeah",
    "start": "1330740",
    "end": "1333740"
  },
  {
    "text": "got it so the question was how do you determine the hyper parameter and ranges",
    "start": "1348800",
    "end": "1354390"
  },
  {
    "text": "for things like deep neural networks essentially III think that's part of why",
    "start": "1354390",
    "end": "1359550"
  },
  {
    "text": "I was cautioning that it's not a black box that you kind of throw stuff at it",
    "start": "1359550",
    "end": "1365490"
  },
  {
    "text": "it's still it's still something that I would characterize as a tool for experts",
    "start": "1365490",
    "end": "1371060"
  },
  {
    "text": "machine learning is pretty hard and I don't I don't think we're at the point",
    "start": "1371060",
    "end": "1376470"
  },
  {
    "text": "yet where we can say everything will work magically if you just you know kind of",
    "start": "1376470",
    "end": "1381690"
  },
  {
    "text": "throw things at it this is this is actually something that takes experience and it takes a deep understanding of the",
    "start": "1381690",
    "end": "1390420"
  },
  {
    "text": "data as well as the methods in order to to do that so this is more like the way",
    "start": "1390420",
    "end": "1395760"
  },
  {
    "text": "I would characterize this is it's more a way that a data scientist can be much",
    "start": "1395760",
    "end": "1401070"
  },
  {
    "text": "more productive then then something that would enable someone who doesn't know",
    "start": "1401070",
    "end": "1407340"
  },
  {
    "text": "data science to be able to do this",
    "start": "1407340",
    "end": "1410720"
  },
  {
    "text": "got it got it so the the question is more around the process of freezing",
    "start": "1429650",
    "end": "1435800"
  },
  {
    "text": "certain layers and and generalizing and and tuning on the last few layers you",
    "start": "1435800",
    "end": "1441680"
  },
  {
    "text": "can certainly do that you can certainly do that but what I'll say is it's not a",
    "start": "1441680",
    "end": "1447010"
  },
  {
    "text": "process by which it it's not a generic process so you would actually have to do",
    "start": "1447010",
    "end": "1453290"
  },
  {
    "text": "that through training the first few layers using the framework of your choice will say tensorflow because it is",
    "start": "1453290",
    "end": "1461210"
  },
  {
    "text": "for most people so you train the first few layers that you change and train the generalized layers first and then you",
    "start": "1461210",
    "end": "1468830"
  },
  {
    "text": "freeze them and you then you run another set of training jobs with the last few",
    "start": "1468830",
    "end": "1475340"
  },
  {
    "text": "layers and then you can hyper parameter tune on that but essentially it's the",
    "start": "1475340",
    "end": "1480679"
  },
  {
    "text": "the flipside - it's under your control is you have to do it and that's",
    "start": "1480679",
    "end": "1487640"
  },
  {
    "text": "something that we're we're trying to find a good balance between it's under your control and you have to do it and",
    "start": "1487640",
    "end": "1495910"
  },
  {
    "text": "until now or up to now we've viewed this more as a something that data scientists",
    "start": "1495910",
    "end": "1503809"
  },
  {
    "text": "need to have the right tools that they can use in order to do this but it's",
    "start": "1503809",
    "end": "1510410"
  },
  {
    "text": "certainly an area that we're exploring and we're trying to figure out what the toolset that we need in order to make",
    "start": "1510410",
    "end": "1515570"
  },
  {
    "text": "that kind of process easier it's not an area that we have spent a whole lot of",
    "start": "1515570",
    "end": "1520880"
  },
  {
    "text": "time on optimizing the deep neural networks themselves",
    "start": "1520880",
    "end": "1526690"
  },
  {
    "text": "hurt me",
    "start": "1531940",
    "end": "1534929"
  },
  {
    "text": "well you could you could you could use the normal training jobs I mean there's nothing you can still use Hyper",
    "start": "1538870",
    "end": "1546310"
  },
  {
    "text": "parameter tuning to do exactly those same things it's just that it you have",
    "start": "1546310",
    "end": "1552310"
  },
  {
    "text": "to configure things in order to take advantage of hyper parameter tuning in that way so it's it's not something",
    "start": "1552310",
    "end": "1559390"
  },
  {
    "text": "that's automatic it's not like you can create this neural network in in will",
    "start": "1559390",
    "end": "1565930"
  },
  {
    "text": "say Karis right like you create the neural network in Carris and then you say these layers are the ones that I",
    "start": "1565930",
    "end": "1572740"
  },
  {
    "text": "want to tune that's not something that's built into Sage maker Sage maker basically says okay you tell me what you",
    "start": "1572740",
    "end": "1580990"
  },
  {
    "text": "want it tuned and I'll tune that it doesn't choose what to tune at this point",
    "start": "1580990",
    "end": "1588090"
  },
  {
    "start": "1589000",
    "end": "1748000"
  },
  {
    "text": "it's not it's not even cascading to a different models it's more than your your cuss doing to different training",
    "start": "1590329",
    "end": "1597589"
  },
  {
    "text": "jobs in order to optimize different parts of the model and if you were to operationalize that the way that you",
    "start": "1597589",
    "end": "1603709"
  },
  {
    "text": "would do that you could do that is through something like step functions where you run the training job on the",
    "start": "1603709",
    "end": "1609529"
  },
  {
    "text": "first part of the model and then take that state and pass it to the second part and then run the training job on",
    "start": "1609529",
    "end": "1614599"
  },
  {
    "text": "the second one and then do hyper parameter tuning on on on that second one once you you have it fixed and it is",
    "start": "1614599",
    "end": "1620899"
  },
  {
    "text": "something that you you could do and and it's probably the way that most people",
    "start": "1620899",
    "end": "1626389"
  },
  {
    "text": "would want to do it at this point frankly the if you think about the state",
    "start": "1626389",
    "end": "1635179"
  },
  {
    "text": "at which machine learning is right now the way that you could solve that is you could say okay yes I I will I'll tune it",
    "start": "1635179",
    "end": "1643820"
  },
  {
    "text": "for you I'll do all of this for you and I'll fix those layers and and do it but",
    "start": "1643820",
    "end": "1648829"
  },
  {
    "text": "the the result of a naive approach to it is it becomes more expensive and I the",
    "start": "1648829",
    "end": "1657259"
  },
  {
    "text": "the reason that we built Bayesian optimization we built Bayesian optimization on purpose and we we we",
    "start": "1657259",
    "end": "1664339"
  },
  {
    "text": "went out there with Bayesian optimization and then customer said but what about grid and what we've shown to",
    "start": "1664339",
    "end": "1671690"
  },
  {
    "text": "customers that say but what about grid is that the cost of Bayesian optimization is actually lower we're",
    "start": "1671690",
    "end": "1678229"
  },
  {
    "text": "trying to be careful about what we build because machine learning is one of those",
    "start": "1678229",
    "end": "1683629"
  },
  {
    "text": "things that can get really expensive really really fast and if he gets that expensive no one's",
    "start": "1683629",
    "end": "1690169"
  },
  {
    "text": "happy I mean that's not the way that that we want to make money we want to make money by providing services to you",
    "start": "1690169",
    "end": "1698690"
  },
  {
    "text": "in the same way that we would provide services to ourselves we're naturally",
    "start": "1698690",
    "end": "1704059"
  },
  {
    "text": "looking at ways that we can be frugal and provide low-cost ways that you can",
    "start": "1704059",
    "end": "1709519"
  },
  {
    "text": "come up with the models so probably a longer answer than you are looking for",
    "start": "1709519",
    "end": "1714979"
  },
  {
    "text": "but by essentially we're trying to find a good solution that allows you to do these things very frequently",
    "start": "1714979",
    "end": "1722440"
  },
  {
    "text": "it doesn't provide the recommended parameters no no not yet but that that is that is something that I would I",
    "start": "1730980",
    "end": "1737040"
  },
  {
    "text": "would really like to Oh does it provide the recommended parameters at the end of",
    "start": "1737040",
    "end": "1742350"
  },
  {
    "text": "of hyper parameter G yes absolutely in fact um let me okay I mean let me",
    "start": "1742350",
    "end": "1750570"
  },
  {
    "text": "flip over briefly to that because that's that's actually a really good point it",
    "start": "1750570",
    "end": "1757170"
  },
  {
    "text": "provides all of the information so it provides oops somehow I'm not on the",
    "start": "1757170",
    "end": "1765480"
  },
  {
    "text": "screen so so you essentially have all of",
    "start": "1765480",
    "end": "1773190"
  },
  {
    "text": "the all of the data for all of the runs it has the objective values it has all of the parameters that had explored I",
    "start": "1773190",
    "end": "1780500"
  },
  {
    "text": "kind of briefly ran through the demo but essentially it has all of the all of the",
    "start": "1780500",
    "end": "1785730"
  },
  {
    "text": "data available so you can create a leaderboard of based on on that and you",
    "start": "1785730",
    "end": "1793500"
  },
  {
    "text": "can even look at past results so not just of this hyper parameter tuning run",
    "start": "1793500",
    "end": "1799260"
  },
  {
    "text": "but if you run it weekly you can see what the progression of it looks like week over week and and so on all of that",
    "start": "1799260",
    "end": "1807390"
  },
  {
    "text": "information is available go ahead",
    "start": "1807390",
    "end": "1811880"
  },
  {
    "start": "1810000",
    "end": "2171000"
  },
  {
    "text": "got it um so the the feedback mechanisms once it's deployed into production at",
    "start": "1822480",
    "end": "1829980"
  },
  {
    "text": "least at this point you're you're building it yourself because the the the",
    "start": "1829980",
    "end": "1837539"
  },
  {
    "text": "truth of it is something that we don't have a good understanding of what the",
    "start": "1837539",
    "end": "1843299"
  },
  {
    "text": "truth of it is that truth you can use the ground truth service in order to",
    "start": "1843299",
    "end": "1848519"
  },
  {
    "text": "provide human-based labels if that's something that you have there are some systems that inherently have the truth",
    "start": "1848519",
    "end": "1854549"
  },
  {
    "text": "in them like ad clicks those systems if someone clicked on an ad you know the truth right there or recommender systems",
    "start": "1854549",
    "end": "1861090"
  },
  {
    "text": "if someone took a recommendation you knew you know the truth so those those truth mechanisms exist we also provide a",
    "start": "1861090",
    "end": "1869820"
  },
  {
    "text": "file format called augmented manifest that allows you to take that truth and",
    "start": "1869820",
    "end": "1875730"
  },
  {
    "text": "build it back into training data but the full loop of going from truth to back to",
    "start": "1875730",
    "end": "1883200"
  },
  {
    "text": "training is something that is a little bit more manual than I would like and that's something that we'll be working",
    "start": "1883200",
    "end": "1890340"
  },
  {
    "text": "on trying to trying to figure out I mean that's something I'd like to make it a",
    "start": "1890340",
    "end": "1896759"
  },
  {
    "text": "little easier than it is right now because there are more advanced things that I'd like to do there and that's an",
    "start": "1896759",
    "end": "1902580"
  },
  {
    "text": "area that I would love feedback on for how we should do it because how we make",
    "start": "1902580",
    "end": "1909059"
  },
  {
    "text": "it so that you can give sage maker the truth that it uses in order to train further is is an essential part of",
    "start": "1909059",
    "end": "1915749"
  },
  {
    "text": "completing that loop mm-hmm",
    "start": "1915749",
    "end": "1921730"
  },
  {
    "text": "no no III I totally agree with you there's a lot of value there it's one of the reasons we built ground truth is",
    "start": "1930070",
    "end": "1936340"
  },
  {
    "text": "because for certain problems we providing a human based workflow that",
    "start": "1936340",
    "end": "1942730"
  },
  {
    "text": "allows them to identify truth is is appropriate but there isn't a uniform",
    "start": "1942730",
    "end": "1949180"
  },
  {
    "text": "way that we have to inject truth into the into the system yet good",
    "start": "1949180",
    "end": "1958410"
  },
  {
    "text": "how do you how do you decide to do but how do you decide the initial number of training job is it you as an individual",
    "start": "1961790",
    "end": "1969680"
  },
  {
    "text": "or sage maker the system",
    "start": "1969680",
    "end": "1973270"
  },
  {
    "text": "[Music] mm-hmm that's correct",
    "start": "1975610",
    "end": "1982840"
  },
  {
    "text": "so so the the number of jobs the parallelization is actually determined by it's one of the parameters provided",
    "start": "1985679",
    "end": "1993489"
  },
  {
    "text": "when you call it so sage maker doesn't determine the number of parallel jobs",
    "start": "1993489",
    "end": "1998979"
  },
  {
    "text": "but that's something that you would provide as a parameter",
    "start": "1998979",
    "end": "2003739"
  },
  {
    "text": "[Music] that absolutely so it's very it it's it",
    "start": "2006240",
    "end": "2014050"
  },
  {
    "text": "is sensitive to that parameter and that that's another thing that relies on the",
    "start": "2014050",
    "end": "2020890"
  },
  {
    "text": "data scientist expertise so depending on madan weather accuracy or speed is a",
    "start": "2020890",
    "end": "2028230"
  },
  {
    "text": "trade off for you depending on where on that trade off you can either choose to have more jobs in parallel and get the",
    "start": "2028230",
    "end": "2034060"
  },
  {
    "text": "results faster or you can choose to have fewer jobs in parallel and get more accurate results for the the same number",
    "start": "2034060",
    "end": "2039850"
  },
  {
    "text": "of training jobs but that's essentially their trade off one last question and",
    "start": "2039850",
    "end": "2046360"
  },
  {
    "text": "then I probably have two to go but I'm happy to answer further questions outside go ahead",
    "start": "2046360",
    "end": "2053760"
  },
  {
    "text": "[Music] it required what",
    "start": "2056679",
    "end": "2064408"
  },
  {
    "text": "[Music] mm-hmm",
    "start": "2066190",
    "end": "2071510"
  },
  {
    "text": "[Music] so can you use custom code to get",
    "start": "2071510",
    "end": "2078190"
  },
  {
    "text": "predictions absolutely it doesn't require the estimator or API to get predictions no it actually",
    "start": "2078190",
    "end": "2086800"
  },
  {
    "text": "doesn't that's that's the examples so we have a set of examples that use the estimator API the the estimator API",
    "start": "2086800",
    "end": "2093940"
  },
  {
    "text": "itself is actually we built that in order to make it easy it doesn't require",
    "start": "2093940",
    "end": "2099160"
  },
  {
    "text": "that there there's actually a lower level API that's the moto moto 3 API",
    "start": "2099160",
    "end": "2106530"
  },
  {
    "text": "that you can use from Python or you can use anything in the AWS SDK or if you",
    "start": "2106530",
    "end": "2111910"
  },
  {
    "text": "want to write your own it's it's essentially a rest endpoint so as long as you can handled off the auth stuff",
    "start": "2111910",
    "end": "2118600"
  },
  {
    "text": "you can just address it as a rest endpoint I one point I actually I",
    "start": "2118600",
    "end": "2125740"
  },
  {
    "text": "actually wrote like AppleScript and AppleScript application the in order to access those and there's no SDK there",
    "start": "2125740",
    "end": "2132670"
  },
  {
    "text": "there's nothing I just accessed it directly so this is it's it's totally",
    "start": "2132670",
    "end": "2138820"
  },
  {
    "text": "possible to do there are some examples that describe the low-level API so you",
    "start": "2138820",
    "end": "2145600"
  },
  {
    "text": "should be able to find those examples in the in the notebooks if you look or if",
    "start": "2145600",
    "end": "2150790"
  },
  {
    "text": "you want in the github repository that has sage maker examples whichever is convenient awesome well thank you all",
    "start": "2150790",
    "end": "2159490"
  },
  {
    "text": "for sticking with me and I appreciate the time I'll be outside to answer questions if you if you have more and if",
    "start": "2159490",
    "end": "2166210"
  },
  {
    "text": "you have other things that you'd like to to discuss thank you",
    "start": "2166210",
    "end": "2172410"
  }
]