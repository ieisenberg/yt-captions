[
  {
    "start": "0",
    "end": "159000"
  },
  {
    "text": "hello everyone and welcome my name is Thor Nolan and I am a Solutions architect working in the AWS partner",
    "start": "3360",
    "end": "10080"
  },
  {
    "text": "ecosystem and before we listen to Netflix talk about the optimiz",
    "start": "10080",
    "end": "15120"
  },
  {
    "text": "optimization and performance of their Amazon Linux or excuse me of their Linux Amis wanted to spend a few moments and",
    "start": "15120",
    "end": "22119"
  },
  {
    "text": "talk about a few emerging considerations that you need to think about When selecting your Linux Ami in the AWS",
    "start": "22119",
    "end": "29439"
  },
  {
    "text": "ecosystem",
    "start": "29439",
    "end": "31960"
  },
  {
    "text": "the Linux ecosystem at Amazon ec2 is really around customer choice and ease of use regardless of the Linux",
    "start": "34920",
    "end": "41600"
  },
  {
    "text": "distribution or Linux version that you're looking to use we want to make sure that the ease of deployment is",
    "start": "41600",
    "end": "47719"
  },
  {
    "text": "better in the AWS cc2 environment that than it is in any other type of deployment Arena that you could be",
    "start": "47719",
    "end": "55718"
  },
  {
    "text": "using on the retail side amazon.com has the concept of frustrating ation free",
    "start": "55920",
    "end": "61000"
  },
  {
    "text": "packaging and we want to ensure with Amazon ec2 that we're offering a frustration free experience with the",
    "start": "61000",
    "end": "67240"
  },
  {
    "text": "deployment of your Linux am Amis we want to believe that it should be very easy",
    "start": "67240",
    "end": "72960"
  },
  {
    "text": "for you to deploy Linux but also to be able to reproduce that environment to continue to update and manage that",
    "start": "72960",
    "end": "79200"
  },
  {
    "text": "environment and to do so in a high performing fashion so when you deploy a solution in",
    "start": "79200",
    "end": "87320"
  },
  {
    "text": "AWS you probably first decide on the operating system inversion that you're going to use probably based upon",
    "start": "87320",
    "end": "93200"
  },
  {
    "text": "application requirements and uh historic selection historic preference probably",
    "start": "93200",
    "end": "98960"
  },
  {
    "text": "the second consideration you're going to make is the actual ec2 instance type and this is one of the areas where we'd like",
    "start": "98960",
    "end": "104920"
  },
  {
    "text": "to emphasize that you need to think Beyond just the initial choice but also consider that this is something that you",
    "start": "104920",
    "end": "111399"
  },
  {
    "text": "should evaluate and in an ongoing fashion revisit and make sure you've selected the correct",
    "start": "111399",
    "end": "118600"
  },
  {
    "text": "one it's it's common to choose that initial uh ec2 instance based upon",
    "start": "118600",
    "end": "124240"
  },
  {
    "text": "already established requirements for your applications or perhaps you are going to translate CPU and memory",
    "start": "124240",
    "end": "129959"
  },
  {
    "text": "profile from an existing native install or an existing virtual deployment um",
    "start": "129959",
    "end": "135400"
  },
  {
    "text": "sometimes we even see the primary consideration be the cost of the particular instance type involved but",
    "start": "135400",
    "end": "140519"
  },
  {
    "text": "what we see all too often is that once that initial decision on an instance type is made that the customers tend to",
    "start": "140519",
    "end": "147080"
  },
  {
    "text": "stick with it even when evidence sort of is there that maybe they should think beyond",
    "start": "147080",
    "end": "153920"
  },
  {
    "text": "that so often enough customers have this initial instance type that they've selected and they are running on the",
    "start": "153920",
    "end": "161239"
  },
  {
    "start": "159000",
    "end": "159000"
  },
  {
    "text": "edge of the available resources that that particular instant Ty instance type makes available to them and so that is",
    "start": "161239",
    "end": "168159"
  },
  {
    "text": "fine probably for 90% of their workloads 90% of the time but during critical Peak",
    "start": "168159",
    "end": "173840"
  },
  {
    "text": "periods where performance is a requirement you can start to see issues occur",
    "start": "173840",
    "end": "180440"
  },
  {
    "text": "the you can take and enable best practices with cloudwatch with autoscaling the best practices",
    "start": "180440",
    "end": "185640"
  },
  {
    "text": "associated with elasticity but there is also a best practice associated with understanding how to performance tune",
    "start": "185640",
    "end": "192319"
  },
  {
    "text": "your Linux Ami and then how to monitor measure that and continue to revisit",
    "start": "192319",
    "end": "197680"
  },
  {
    "text": "this in an ongoing fashion to ensure that you have selected an instance type that doesn't meet your needs 90% of the",
    "start": "197680",
    "end": "204040"
  },
  {
    "text": "time but meets them 100% of the time so the Amazon ec2 team continues to",
    "start": "204040",
    "end": "211439"
  },
  {
    "text": "develop new instance types we've launched a few of them this month a few of them just yesterday uh until just",
    "start": "211439",
    "end": "217080"
  },
  {
    "start": "217000",
    "end": "217000"
  },
  {
    "text": "over a year ago that instance type decided which version of of the Ami pools that you had a selection from",
    "start": "217080",
    "end": "224080"
  },
  {
    "text": "because that instance type was either going to support hvm or a hardware virtual machine or it would support PV",
    "start": "224080",
    "end": "230840"
  },
  {
    "text": "or a paravirtual machine and there were no no instance types that actually supported",
    "start": "230840",
    "end": "237079"
  },
  {
    "text": "both with the instance types that we have now released we have M3 hi1 I2 the",
    "start": "237120",
    "end": "244439"
  },
  {
    "text": "hs1 and the C3 so now we have five instance types that support both PV and",
    "start": "244439",
    "end": "251040"
  },
  {
    "text": "hvm amies so now we have an emerging Choice emerging consideration that",
    "start": "251040",
    "end": "256280"
  },
  {
    "text": "wasn't always present before so when it comes to PV or par",
    "start": "256280",
    "end": "263080"
  },
  {
    "text": "virtualization this is a simple environment where the operating system itself has the introduction of drivers",
    "start": "263080",
    "end": "270039"
  },
  {
    "start": "269000",
    "end": "269000"
  },
  {
    "text": "which makes it essentially aware that it is virtualized the lucky thing is in the",
    "start": "270039",
    "end": "275919"
  },
  {
    "text": "Linux ecosystem almost all stock Linux kernels today come with these PA of virtualization drivers already in the",
    "start": "275919",
    "end": "282720"
  },
  {
    "text": "kernel existing with hardware virtualization you're actually leveraging the",
    "start": "282720",
    "end": "289639"
  },
  {
    "text": "virtualization support that's built into the CPU so you can use unmodified operating systems however we do have",
    "start": "289639",
    "end": "296440"
  },
  {
    "text": "this capability Within hvm the PA virtual and drivers as I said are",
    "start": "296440",
    "end": "302280"
  },
  {
    "text": "built into the Linux kernel and actually in an HM Linux Ami you can make use of these and you get better performance out",
    "start": "302280",
    "end": "308560"
  },
  {
    "text": "of network and storage so if by chance you are using a Linux Ami that doesn't have pvm PV",
    "start": "308560",
    "end": "315919"
  },
  {
    "text": "drivers built into it and you want to use it in an hvm arena it is highly recommended that you introduce those",
    "start": "315919",
    "end": "322319"
  },
  {
    "text": "drivers so with our I2 and C3 instance types recently released we have also the",
    "start": "322319",
    "end": "329000"
  },
  {
    "text": "capability to offer enhanced networking now this requires",
    "start": "329000",
    "end": "334360"
  },
  {
    "text": "introduction of Intel drivers and it is in the hbm only type of Arena although those drivers are completely harmless",
    "start": "334360",
    "end": "342039"
  },
  {
    "start": "339000",
    "end": "339000"
  },
  {
    "text": "and non-functional if they're used in a PV type of environment however the Intel drivers",
    "start": "342039",
    "end": "347560"
  },
  {
    "text": "are not commonly available on all Linux distributions right now and so you will have to download compile install those",
    "start": "347560",
    "end": "353280"
  },
  {
    "text": "drivers and we do provide instructions for all of this",
    "start": "353280",
    "end": "358479"
  },
  {
    "text": "online so which is it are we suggesting PV or hvm or hvm with PV",
    "start": "358479",
    "end": "364960"
  },
  {
    "text": "drivers even if you have experience with Zen environments in other areas in other",
    "start": "364960",
    "end": "370599"
  },
  {
    "text": "environments outside of AWS you want to consider that there are going to be some differences that you will see in the way",
    "start": "370599",
    "end": "376400"
  },
  {
    "text": "that PV and hvm actually work and react here the only way that we're really",
    "start": "376400",
    "end": "383759"
  },
  {
    "text": "recommending that you determine your selection between PV or hvm is to determine some metrics that you can",
    "start": "383759",
    "end": "389919"
  },
  {
    "text": "measure you can test and then you can evaluate and make that choice ultimately",
    "start": "389919",
    "end": "396360"
  },
  {
    "text": "the application and the workload testing that you do is going to be the only thing that can really functionally guide",
    "start": "396360",
    "end": "401680"
  },
  {
    "text": "you to make the best determination for the virtualization model that that best suits your",
    "start": "401680",
    "end": "408800"
  },
  {
    "text": "needs on the partner ecosystem side we're spending a lot of time with our",
    "start": "408800",
    "end": "413880"
  },
  {
    "start": "410000",
    "end": "410000"
  },
  {
    "text": "infrastructure Partners to ensure that we have a larger selection of hvm amies today the catalog is substantially",
    "start": "413880",
    "end": "421080"
  },
  {
    "text": "smaller for hm amies than it is for PV amies and so that's something that we're working with our partners to ensure that",
    "start": "421080",
    "end": "426400"
  },
  {
    "text": "we're expanding that functionality and and we want to make sure that uh we're offering that to our customers as with",
    "start": "426400",
    "end": "433840"
  },
  {
    "text": "most things that we do that requires customer feedback so whether it be right here whether it be in the forums please",
    "start": "433840",
    "end": "439759"
  },
  {
    "text": "do make known your requirements what you need from a a version as well as a distribution for hvm use as you start to",
    "start": "439759",
    "end": "447039"
  },
  {
    "text": "take advantage of these these new instance types so that is basically the uh the input",
    "start": "447039",
    "end": "453400"
  },
  {
    "text": "that we wanted to put out there I think right now what we need to do is pass it over to Coburn Watson from Netflix who",
    "start": "453400",
    "end": "459759"
  },
  {
    "text": "can talk about Linux optimization and",
    "start": "459759",
    "end": "464199"
  },
  {
    "text": "performance all right thanks Thor cool welcome on the last day here",
    "start": "468199",
    "end": "474479"
  },
  {
    "text": "at the conference hope you can hear me okay um so I guess we're good to we go",
    "start": "474479",
    "end": "480120"
  },
  {
    "text": "so yeah I'm here to talk to you about uh tuning your Linux Ami um and how we look at it so I'm cobr Watson I manage the",
    "start": "480120",
    "end": "485879"
  },
  {
    "text": "cloud performance team at Netflix um if you haven't heard of Netflix this might be your first session at the conference",
    "start": "485879",
    "end": "491919"
  },
  {
    "text": "but uh uh we've had a few so we have a lot of customers we stream a lot of movies um and we have some notables the",
    "start": "491919",
    "end": "498879"
  },
  {
    "text": "uh our Originals catalog is really has been growing so hopefully Everybody's Got High anticipation about all of our",
    "start": "498879",
    "end": "504599"
  },
  {
    "text": "second season coming out I know I know I am so one thing I wanted to do is set",
    "start": "504599",
    "end": "510000"
  },
  {
    "text": "some context around uh how we view Ami performance at Netflix right we have a lot of instances running out there and",
    "start": "510000",
    "end": "516560"
  },
  {
    "text": "why is it something that's important for us to address so you've already heard that we um you know we have tens of thousands of",
    "start": "516560",
    "end": "522919"
  },
  {
    "start": "519000",
    "end": "519000"
  },
  {
    "text": "instances running globally for Netflix uh in a given day right and when you start operating at that bit of a higher",
    "start": "522919",
    "end": "529080"
  },
  {
    "text": "scale you have the sort of rising tide lifts All Ships models right so in the organization itself most service teams",
    "start": "529080",
    "end": "535560"
  },
  {
    "text": "we have about 30 engineering teams that push code to production independently uh they push on top I'm going to talk about",
    "start": "535560",
    "end": "540920"
  },
  {
    "text": "our base Ami um in a second but basically the performance team is responsible for a holistic view of the",
    "start": "540920",
    "end": "546839"
  },
  {
    "text": "environment and making sure that we're using our resources efficiently we have a large range of workloads in our",
    "start": "546839",
    "end": "552959"
  },
  {
    "text": "environment probably not uncommon for most of you um I use the term oldp a lot I'm told it's becoming pretty old and I",
    "start": "552959",
    "end": "558480"
  },
  {
    "text": "probably shouldn't use it but you know think of um you know user facing applications and other services which tend to be pretty latency sensitive and",
    "start": "558480",
    "end": "565680"
  },
  {
    "text": "then we have batch or pre-compute so you know Netflix has this luxury of very large catalog of titles many many tens",
    "start": "565680",
    "end": "571360"
  },
  {
    "text": "of thousands and when you come to visit us I can only put 30 or so pictures on your screen to show which ones I think",
    "start": "571360",
    "end": "576560"
  },
  {
    "text": "you should play and so we recently have moved to a batch or pre-compute model so just about every day we revisit what",
    "start": "576560",
    "end": "583279"
  },
  {
    "text": "you've seen you know what's your preferences are what other people look at and we update your recommendations",
    "start": "583279",
    "end": "589160"
  },
  {
    "text": "each day and hope it's we've seen it improve some of our viewership um and we've gotten good feedback on it so hopefully you guys are seeing that as",
    "start": "589160",
    "end": "594839"
  },
  {
    "text": "well but from a workload perspective it's very different from the majority of our other services uh and then Cassandra e cach so we have",
    "start": "594839",
    "end": "601640"
  },
  {
    "text": "our data storage tiers and they themselves have very distinct workloads you know Cassandra tends to be very IO intensive in our case uh depending on",
    "start": "601640",
    "end": "608640"
  },
  {
    "text": "the cluster we have about 50 of them and then EV cach is mostly uh you know memory and network heavy",
    "start": "608640",
    "end": "615240"
  },
  {
    "text": "systems and I put this out here this isn't anything like a dig on AWS but you know whether it's virtualization in",
    "start": "615240",
    "end": "621040"
  },
  {
    "text": "general or it's running on the cloud you know I think we're all aware that we'll have more inherent variability in the environment right it's just it's it's",
    "start": "621040",
    "end": "627480"
  },
  {
    "text": "the way of the world right we have an incredible flexibility to deploy whatever we want whenever we want um and",
    "start": "627480",
    "end": "632680"
  },
  {
    "text": "for that we're giving up a little bit of control about performance I do think a lot of the new instance types you know talking about the Intel optimized",
    "start": "632680",
    "end": "638800"
  },
  {
    "text": "drivers I mean the bit rates I've heard and the latency reductions it's going to be amazing what it provides in general",
    "start": "638800",
    "end": "644079"
  },
  {
    "text": "in terms of the quality of the system and the fact that they're now exposing the CPU you're on you have a much better",
    "start": "644079",
    "end": "649600"
  },
  {
    "text": "understanding of what you're actually running on the environment which is awesome um and then in our case you know",
    "start": "649600",
    "end": "655519"
  },
  {
    "text": "we have so many thousands of instances if we had to individually configure them to G gain performance benefits it would be a real pain in the ass so we have an",
    "start": "655519",
    "end": "662279"
  },
  {
    "text": "open source project called amator that you guys might have heard of it's really our Bakery pipeline it's it's on the Netflix uh GitHub site and what it does",
    "start": "662279",
    "end": "670040"
  },
  {
    "start": "667000",
    "end": "667000"
  },
  {
    "text": "is it allows us to create a pipeline for prepping application Amis I was in a bpc",
    "start": "670040",
    "end": "675240"
  },
  {
    "text": "presentation a couple days ago and one of the best practices they talked about was when you're bootstrapping your Ami",
    "start": "675240",
    "end": "680560"
  },
  {
    "text": "that's when maybe you would pull in some packages you have greater flexibility um you know the downside is",
    "start": "680560",
    "end": "685800"
  },
  {
    "text": "it uh increases your startup time right so we really count afford that in our environment so what we do is you",
    "start": "685800",
    "end": "692000"
  },
  {
    "text": "basically will take like a distribution Ami you'll pull it into your account um and then you'll create what's called a foundation Ami because then you can",
    "start": "692000",
    "end": "697959"
  },
  {
    "text": "create snapshots off that EBS root volume and then on top of that we have what we call a Bas Ami and so we have an",
    "start": "697959",
    "end": "703360"
  },
  {
    "text": "engineering tools team they work on the monkeys that you've heard about um they maintain the Bas Ami they're actually looking for someone who wants to um own",
    "start": "703360",
    "end": "710519"
  },
  {
    "text": "the B Ami so if you guys are interested in that let me know um and then once you create That Base Ami we change that less",
    "start": "710519",
    "end": "716519"
  },
  {
    "text": "frequently that includes things on top of the foundation Ami like the version of java uh Tom Cat aache system",
    "start": "716519",
    "end": "722600"
  },
  {
    "text": "monitoring utilities and so as an engineering team in Netflix when you build your application through Jenkins",
    "start": "722600",
    "end": "727800"
  },
  {
    "text": "it basically starts with the base Ami as a template and Pops your package on top of that and then registers on Amazon as",
    "start": "727800",
    "end": "733880"
  },
  {
    "text": "a distinct uh Amy for your application so when your group is auto scaling up or down you're creating new asgs it's all",
    "start": "733880",
    "end": "740040"
  },
  {
    "text": "ready to go there's no additional configuration that happens at bootstrap time and so by having this level of",
    "start": "740040",
    "end": "745399"
  },
  {
    "text": "customization available to us we can sneak in a bunch of little things that we want to right I mean a lot of teams",
    "start": "745399",
    "end": "750639"
  },
  {
    "text": "don't want to worry about tuning various things at the network or the kernel layer uh related to that so we can put",
    "start": "750639",
    "end": "756560"
  },
  {
    "text": "in aache and Tomcat configurations on the Bas Ami things that are tweaked for various workloads are just best",
    "start": "756560",
    "end": "761680"
  },
  {
    "text": "practices in general um and then if we want workload specific applications and I will be completely honest this is",
    "start": "761680",
    "end": "767920"
  },
  {
    "text": "something that we've just started looking at heavily this year so thinking about the concept of roles and really we",
    "start": "767920",
    "end": "773519"
  },
  {
    "text": "had the Bas Ami and we have the application which is from more generalized to completely specific in its nature",
    "start": "773519",
    "end": "779519"
  },
  {
    "text": "and we're now playing around with how we're going to incorporate these various roles like you're a pre-compute application you're an ulp application",
    "start": "779519",
    "end": "785760"
  },
  {
    "text": "and sort of shuffling along the changes with them but that's something we're currently working on and maybe that'll",
    "start": "785760",
    "end": "790800"
  },
  {
    "text": "make its way into amator as an additional sort of configuration Dimension so tuning the Linux kernel um",
    "start": "790800",
    "end": "798600"
  },
  {
    "text": "some of these are probably self-evident but uh I think to Thor's point when you",
    "start": "798600",
    "end": "803680"
  },
  {
    "text": "select an instance we all have different parameters on which we select instances you know in some cases Amazon we have so",
    "start": "803680",
    "end": "809680"
  },
  {
    "text": "many instances that we um we maintain few relatively large instance pools right so if you were to look at M22 XLS",
    "start": "809680",
    "end": "816279"
  },
  {
    "text": "maybe they're 70% of our footprint right and a lot of teams might be running on an instance that's not ideal for their",
    "start": "816279",
    "end": "821959"
  },
  {
    "text": "workload it's typically a little bit bigger but given we're operating with mostly reserved instances it's required right we can't say hey here's 50",
    "start": "821959",
    "end": "828120"
  },
  {
    "text": "instances just pick and pull what you want because tomorrow we could be on 500 Cc tws or something right that we don't",
    "start": "828120",
    "end": "833240"
  },
  {
    "text": "have reservations for um but we still evaluate what instance class and I think the new models that that came out or new",
    "start": "833240",
    "end": "839480"
  },
  {
    "text": "instance classes um families that were announced are G going to give us a lot more flexibility on that but in general",
    "start": "839480",
    "end": "846240"
  },
  {
    "text": "when you go out and look at your environment um and you go to an instance you see people autoscaling or sizing their asgs the assumption is is that",
    "start": "846240",
    "end": "853040"
  },
  {
    "text": "they're sizing it for a specific reason they're pushing some dimension on that instance you know we have a lot of movie data so we have a lot of instances that",
    "start": "853040",
    "end": "859199"
  },
  {
    "text": "require a lot of ram because it has to be served from in memory and so you need to evaluate that over time things change",
    "start": "859199",
    "end": "865720"
  },
  {
    "text": "a lot in Netflix someone refactors the movie library and suddenly the heat footprint goes down quite a bit and it opens up a whole new set of categories",
    "start": "865720",
    "end": "872279"
  },
  {
    "text": "of instances they can move to and we allow that to happen um it can happen all the time but for large movements we",
    "start": "872279",
    "end": "877519"
  },
  {
    "text": "tend to uh Focus that a little bit um at the scale we're at and I pointed that out before with the rising",
    "start": "877519",
    "end": "883360"
  },
  {
    "text": "tide lift cell ships when we start talking about improving efficiencies at scale if I get another 2 to 5% out of you know 5,000 instances it's a pretty",
    "start": "883360",
    "end": "890040"
  },
  {
    "text": "big gain for us and it's there's really no neason not not to do it um and going through the tuning process I think gets",
    "start": "890040",
    "end": "896199"
  },
  {
    "text": "you more in touch with what your application is doing on your Ami a lot of people say well here's my throughput or my performance sucks or my response",
    "start": "896199",
    "end": "902399"
  },
  {
    "text": "time just went in the hole you know and that's great but really what you want to do is in our environment it's pretty",
    "start": "902399",
    "end": "907959"
  },
  {
    "text": "cool because all Engineers basically have route access to all the systems in production at any time so if I want to go look at the performance characteristics of a box I can just go",
    "start": "907959",
    "end": "914240"
  },
  {
    "text": "on to it and do packet captures whatever I want it's great um but you have the",
    "start": "914240",
    "end": "919440"
  },
  {
    "text": "ability to look at the performance of your application and its demands on the systems and that's the next two points",
    "start": "919440",
    "end": "925079"
  },
  {
    "text": "um I think the performance tooling around Linux has been improving significantly probably more in the past decade but you",
    "start": "925079",
    "end": "931399"
  },
  {
    "text": "have a ton of tools available to you you know if you're not lucky enough to be running on a on a platform that perhaps supports detrace um there's still some",
    "start": "931399",
    "end": "937959"
  },
  {
    "text": "really good other Solutions like perf and system tap that run on Linux and they continue to mature and perf and",
    "start": "937959",
    "end": "943079"
  },
  {
    "text": "system tap are some of the ones that we use pretty heavily um and doing the tuning gets you",
    "start": "943079",
    "end": "948120"
  },
  {
    "text": "an understanding of what your application demands of your operating system and then you can also just do a",
    "start": "948120",
    "end": "953199"
  },
  {
    "text": "bottomup analysis like I don't even know what the application's doing I just want to go look at the resource utilization of my system system and determine what",
    "start": "953199",
    "end": "959680"
  },
  {
    "text": "the point of contention is and then I can go work with the other team but by getting in touch with your performance at the colonel level I think it's it's",
    "start": "959680",
    "end": "965720"
  },
  {
    "text": "really eye openening uh for most people now there are a lot of trade-offs so well I have three here but um you know",
    "start": "965720",
    "end": "972440"
  },
  {
    "start": "971000",
    "end": "971000"
  },
  {
    "text": "the kernel systems are interdependent right and I've seen people I was a sis sadman a long time ago in my life um and",
    "start": "972440",
    "end": "979199"
  },
  {
    "text": "so I've seen people tweak a lot of things where they didn't necessarily validate the benefit or make sure that it was appropriate for all workloads",
    "start": "979199",
    "end": "985000"
  },
  {
    "text": "right so we're extremely careful in our tuning we're applying something that could be inherited by hundreds of services deployed within",
    "start": "985000",
    "end": "990839"
  },
  {
    "text": "days across tens of thousands of instances and so we take it pretty heavily in terms of if we change this",
    "start": "990839",
    "end": "995880"
  },
  {
    "text": "what does it mean for our other workloads so we sort of start by with the role the role model and then the 8020 rule so I don't know if anybody's",
    "start": "995880",
    "end": "1003319"
  },
  {
    "text": "ever worked on applications that use like an rdms or rdbms or a database but you know you'll have a performance",
    "start": "1003319",
    "end": "1008839"
  },
  {
    "text": "problem someone will say this this response time is slow and you typically don't want to go off and start looking at the cernal configuration of the",
    "start": "1008839",
    "end": "1014959"
  },
  {
    "text": "database server and say well maybe I need to tune IO right because what you'll probably find is there's some query missing an index that's doing like",
    "start": "1014959",
    "end": "1021920"
  },
  {
    "text": "10 gazillion reads when it could be doing like 10 and so the benefits you'll get in terms of tuning your application",
    "start": "1021920",
    "end": "1027959"
  },
  {
    "text": "versus your uh your operating system you know there there's a pretty big spread there so the assumption is like us we've",
    "start": "1027959",
    "end": "1034160"
  },
  {
    "text": "already gone through a fairly iterative process of tuning our applications we have a bunch of very senior Engineers that have you know Decades of experience",
    "start": "1034160",
    "end": "1040480"
  },
  {
    "text": "on distributed systems and our assumption is they're applying the best practices we circulate they tune their application stack and we're now at a",
    "start": "1040480",
    "end": "1047280"
  },
  {
    "text": "point where we're actually going to start at the Ami and this is something we did a lot uh this year and then back to the first point once you tailor your",
    "start": "1047280",
    "end": "1054600"
  },
  {
    "text": "Ami for a certain workload it doesn't work for everybody right so you could actually cause more problem um more harm",
    "start": "1054600",
    "end": "1060559"
  },
  {
    "text": "than good I guess would be the term um and this was from uh someone on",
    "start": "1060559",
    "end": "1066600"
  },
  {
    "text": "my team so you really just want to align your system resources to what your application needs of the system over time and if you switch instance types",
    "start": "1066600",
    "end": "1072919"
  },
  {
    "text": "you'll obviously have to revisit that again but it should be a somewhat infrequent thing you have to do um we're",
    "start": "1072919",
    "end": "1078440"
  },
  {
    "text": "actually working on building some scripts and wrappers around perf and system Taps so that we can deploy it on",
    "start": "1078440",
    "end": "1083840"
  },
  {
    "text": "all of our Amis and proactively sort of on the FL enable the type of in instrumentation to determine where",
    "start": "1083840",
    "end": "1089200"
  },
  {
    "text": "contention is so Performance Tools I was going to talk about this real quick um this could",
    "start": "1089200",
    "end": "1094840"
  },
  {
    "text": "be a whole day discussion on itself we look at the primary dimensions for performance just like I'm sure you guys",
    "start": "1094840",
    "end": "1100559"
  },
  {
    "start": "1100000",
    "end": "1100000"
  },
  {
    "text": "do as well CPU memory Network iio block IO and then scalability is thrown in there right in production we have a",
    "start": "1100559",
    "end": "1106000"
  },
  {
    "text": "problem where we tend to see the workload going up on a system or the request rate we see performance start to",
    "start": "1106000",
    "end": "1111840"
  },
  {
    "text": "not look ideal but we look at the system level and there's almost no resource that we're typically used to seeing be",
    "start": "1111840",
    "end": "1117000"
  },
  {
    "text": "saturated be saturated most of the time this will drive us back into application tuning but if it doesn't then we just",
    "start": "1117000",
    "end": "1123320"
  },
  {
    "text": "keep diving down going down the rabbit hole on where the time is spent on the system so there's all the basic tools",
    "start": "1123320",
    "end": "1129440"
  },
  {
    "start": "1129000",
    "end": "1129000"
  },
  {
    "text": "right everybody's familiar with all these tools like I get on the system where what's happening and you do a VM stat and you're like look my CPU is",
    "start": "1129440",
    "end": "1135159"
  },
  {
    "text": "completely idle um I have a real high level of context switches you know very relative term but these are the ones you",
    "start": "1135159",
    "end": "1141480"
  },
  {
    "text": "would use just at a broad sweep to say well what's happening at my system at a very high level and then once you've",
    "start": "1141480",
    "end": "1147760"
  },
  {
    "text": "actually determined hopefully these tools will lead you down the path of um what you sort of want to look at and",
    "start": "1147760",
    "end": "1153120"
  },
  {
    "text": "there's tools and there's also you know proc um file system structures that provide some of this data some of them",
    "start": "1153120",
    "end": "1158919"
  },
  {
    "text": "Network some of them CPU but it we all use these I think as an entry point but sometimes it can be frustrating like oh",
    "start": "1158919",
    "end": "1164679"
  },
  {
    "text": "I run I ran it and I can see that I'm really hammering the CPU but I seem to I have a real high system CPU load right",
    "start": "1164679",
    "end": "1170919"
  },
  {
    "text": "why is my why is my application putting so much load on the Kernel uh system side and so then you start using your",
    "start": "1170919",
    "end": "1176559"
  },
  {
    "text": "Advanced tools right where you start some of them are optimized like block trace for looking at your IO patterns",
    "start": "1176559",
    "end": "1182120"
  },
  {
    "text": "slab tops for kernel memory um I think system tap and perer here on the next page but you know wire",
    "start": "1182120",
    "end": "1188840"
  },
  {
    "text": "shark PCP dump you really start digging into things so the advanced tools are where you've got a high level sense of",
    "start": "1188840",
    "end": "1193960"
  },
  {
    "text": "where you might have some inefficiencies um but then what you want to do is you want to dive a little bit deeper",
    "start": "1193960",
    "end": "1199760"
  },
  {
    "text": "get some water here and then perf and system tap we did have drace on here before but um you know if you're aren't",
    "start": "1199760",
    "end": "1205559"
  },
  {
    "text": "running on an operating system that has a dra trce which we currently are not on um I",
    "start": "1205559",
    "end": "1211720"
  },
  {
    "text": "didn't really want to call it out uh but yeah there's just a variety of tooling so now I'm going to go on the",
    "start": "1211720",
    "end": "1218600"
  },
  {
    "text": "actual Ami tuning so I'm sort of a proxy here today just so you know the Linux a tuning expert on my team was unable to",
    "start": "1218600",
    "end": "1223919"
  },
  {
    "text": "make it so he generated this I think I'm pretty up to date on it but if there's really hardcore questions that come up",
    "start": "1223919",
    "end": "1229480"
  },
  {
    "text": "later I'm going to have to defer to him ammer was not able to make it here today he actually teaches classes on Linux system performance tuning and kernel",
    "start": "1229480",
    "end": "1235679"
  },
  {
    "text": "developments so I am not ammer but I will I will make a best best effort attempt here so when we started looking",
    "start": "1235679",
    "end": "1242840"
  },
  {
    "text": "at where you know now assume that prior to this year there was really no focused Ami tuning effort going on at Netflix",
    "start": "1242840",
    "end": "1248720"
  },
  {
    "text": "and we finally got to a point where you know this is a pretty big rock that we haven't um turned over yet right so I",
    "start": "1248720",
    "end": "1254440"
  },
  {
    "text": "got someone on my team who's a specialist in this area and we started looking at at in the different categories of tunables at the Linux",
    "start": "1254440",
    "end": "1260520"
  },
  {
    "text": "operating system level or kernel level what could bring us the biggest value um and as I talked about we had differentiated workloads so a real uh",
    "start": "1260520",
    "end": "1267799"
  },
  {
    "text": "low-hanging one to go after was our batch or pre-compute workload right they they tend to consume many thousands of",
    "start": "1267799",
    "end": "1273080"
  },
  {
    "text": "instances at a time running at probably 95 or above percent utilization on CPU",
    "start": "1273080",
    "end": "1278120"
  },
  {
    "text": "just crunching through a bunch of data and so you know the um completely Fair scheduler which came in I think around",
    "start": "1278120",
    "end": "1284720"
  },
  {
    "text": "2623 somewhere in that time frame it was a pretty radical change to the CPU scheduling capabilities on the Linux uh",
    "start": "1284720",
    "end": "1291720"
  },
  {
    "text": "system or Linux kernel and it really focuses on being fair as it says in the",
    "start": "1291720",
    "end": "1296880"
  },
  {
    "text": "name right so applications I think it tends to favor primarily interactive based applications everybody gets a fair",
    "start": "1296880",
    "end": "1303200"
  },
  {
    "text": "share of CPU time it changed how the CPU time was a portioned it's purely based on like nanc allocation versus some",
    "start": "1303200",
    "end": "1309400"
  },
  {
    "text": "other mechanisms that sort of required some jiggery at the the OS level so we took the the batch application which",
    "start": "1309400",
    "end": "1315320"
  },
  {
    "text": "we're running at high CP utilization everywhere and we evaluated what it would do how we could get more time on",
    "start": "1315320",
    "end": "1321240"
  },
  {
    "text": "the CPU for the batch application itself right so if you don't tune CFS at all and you happen to be a batch-based",
    "start": "1321240",
    "end": "1326919"
  },
  {
    "text": "application that's very CPU intensive in this case you end up getting preempted a lot and there's a lot of involuntary",
    "start": "1326919",
    "end": "1332799"
  },
  {
    "text": "context which is where the the scheduler basically says okay you've used your very tiny slice of CPU now I'm going to",
    "start": "1332799",
    "end": "1338240"
  },
  {
    "text": "kick you off and then you're waiting you're trying to get back on so there's a lot more context switching going on than necessary and the tunables that are",
    "start": "1338240",
    "end": "1344880"
  },
  {
    "text": "listed here those are the defaults that are calculated based on the number of cores in your system so it's dynamic in",
    "start": "1344880",
    "end": "1350799"
  },
  {
    "text": "nature so you have two basic approaches you can take you can basically change the scheduling priority of the process",
    "start": "1350799",
    "end": "1357200"
  },
  {
    "text": "um to be batch or you can actually change it at a systemwide level and that affects all processes on your system",
    "start": "1357200",
    "end": "1362760"
  },
  {
    "text": "right so the first one there determines when you actually get on CPU um how much time you're allocated to",
    "start": "1362760",
    "end": "1370840"
  },
  {
    "text": "consume right and I think it's it says 18 milliseconds that's incorrect but we",
    "start": "1370840",
    "end": "1376559"
  },
  {
    "text": "actually double that it actually defines you know the amount of time you get when you're on CPU and then additionally it",
    "start": "1376559",
    "end": "1381880"
  },
  {
    "text": "def the second one defines a Time window over which the fair scheduler is going try to make sure that all processes in",
    "start": "1381880",
    "end": "1387919"
  },
  {
    "text": "the queue waiting for CPU will actually get to the CPU so you can sort of simulate the batch type scheduling model",
    "start": "1387919",
    "end": "1393400"
  },
  {
    "text": "if you set your priority of your process uh to batch what happens is um you get",
    "start": "1393400",
    "end": "1398600"
  },
  {
    "text": "preempted less frequently you get more time when you're on CPU you know you get on there you have to warm your CPU caches you've got all your data you just",
    "start": "1398600",
    "end": "1404919"
  },
  {
    "text": "start doing some work and then you get kicked out the door right which isn't great for for throughput overall um and",
    "start": "1404919",
    "end": "1410360"
  },
  {
    "text": "then you can set it manually as I said at a broad system level we're still playing around with how we want to apply",
    "start": "1410360",
    "end": "1415960"
  },
  {
    "text": "this change if we want to actually apply it to the process itself the Java process or we want to affect scheduling",
    "start": "1415960",
    "end": "1421000"
  },
  {
    "text": "of all processes on the system in terms of how we're going to incorporate that in there um our services tend to be",
    "start": "1421000",
    "end": "1426480"
  },
  {
    "text": "pretty single tenants so a batch application is a b batch application and there really shouldn't be a lot of interactive activity going on on there",
    "start": "1426480",
    "end": "1433559"
  },
  {
    "text": "um you can use tools like uh PID stat so this is a case where maybe you Iden identify that you have like um a high",
    "start": "1433559",
    "end": "1439919"
  },
  {
    "text": "Contex switch rate and you're trying to determine if your application can get more efficient use of CPU and so I think",
    "start": "1439919",
    "end": "1445840"
  },
  {
    "text": "the third column are the general the context switch is what your voluntary right our application obviously has to",
    "start": "1445840",
    "end": "1450960"
  },
  {
    "text": "get on there and go get some data makes a network or a disc you know request and it gets kicked off the CPU voluntarily",
    "start": "1450960",
    "end": "1457200"
  },
  {
    "text": "and then you have the involuntary context switches and those are basically you being kicked off of the CPU by someone else needing to get on the CPU",
    "start": "1457200",
    "end": "1463720"
  },
  {
    "text": "on the schedu or feeling like you've had enough of your your slice so after applying this change on our pre compute",
    "start": "1463720",
    "end": "1469840"
  },
  {
    "text": "workload you can see that our involuntary context switch has dropped significantly on the batch application for all the Java process threads um and",
    "start": "1469840",
    "end": "1477120"
  },
  {
    "text": "we got about a 2 to 5% increase in throughput you know there was some variability there you know our data is",
    "start": "1477120",
    "end": "1482279"
  },
  {
    "text": "very uh has very different dimensions so it isn't completely Point time and we're doing this in production so we actually",
    "start": "1482279",
    "end": "1487440"
  },
  {
    "text": "had real data going through which was nice we just picked one instance out of the the Thousand or so but this is where",
    "start": "1487440",
    "end": "1492520"
  },
  {
    "text": "you use some of the additional Advanced tooling to get a sense of you know where is that process spending it time its time why is it getting kicked off CPU so",
    "start": "1492520",
    "end": "1499360"
  },
  {
    "text": "much so one thing I didn't mention is this is all mileage may vary you know don't go",
    "start": "1499360",
    "end": "1504399"
  },
  {
    "text": "home and like apply a bunch of this stuff um and then find that it breaks things um it does come back to what Thor",
    "start": "1504399",
    "end": "1510200"
  },
  {
    "text": "said where you really have to understand what it does to your your application system so that's that's the only thing we've done in the domain of looking at",
    "start": "1510200",
    "end": "1516039"
  },
  {
    "text": "our CPU scheding algorithm and we feel like right now for our batch tier that's really the only one that's going to benefit from it but we're going to be",
    "start": "1516039",
    "end": "1522000"
  },
  {
    "text": "applying that in general to the majority of our um our pre-compute workload so the next one is Page fash",
    "start": "1522000",
    "end": "1527960"
  },
  {
    "text": "tuning and we came we came to look at this one um as a result of doing performance analysis on Cassandra right",
    "start": "1527960",
    "end": "1533880"
  },
  {
    "start": "1531000",
    "end": "1531000"
  },
  {
    "text": "so as we go from these various workloads we tried to identify the subset of uh kernel parameters which could benefit us",
    "start": "1533880",
    "end": "1539720"
  },
  {
    "text": "for that specific workload based on their behavior um and so am identified some that we were going to look at for",
    "start": "1539720",
    "end": "1545440"
  },
  {
    "text": "Cassandra because it has an extremely high IO rate you know we're we're one of the larger consumers probably of the",
    "start": "1545440",
    "end": "1551320"
  },
  {
    "text": "high one for xlarge instances for SSD purposes you know a lot of people are watching movies um we're storing their",
    "start": "1551320",
    "end": "1557799"
  },
  {
    "text": "bookmarks there's a lot of data coming back to us I think of the 30 or so perent of bandwidth bandwidth we consume",
    "start": "1557799",
    "end": "1564760"
  },
  {
    "text": "on the internet each night probably about 3 to 4% of that is actually inbound that's data coming to Netflix",
    "start": "1564760",
    "end": "1569799"
  },
  {
    "text": "around your bookmarks your concurrency your streams your recommendations so um Cassandra has to write all of that out",
    "start": "1569799",
    "end": "1575960"
  },
  {
    "text": "the good news is Cassandra is very good at writing stuff out um and accepting inserts right it's really I think it was",
    "start": "1575960",
    "end": "1581200"
  },
  {
    "text": "built to support comments at Twitter or something not my of expertise uh Christos from our team uh the CD team",
    "start": "1581200",
    "end": "1587880"
  },
  {
    "text": "knows a lot about that but what you want to do is if you're in a situation um where there's an application writing",
    "start": "1587880",
    "end": "1592960"
  },
  {
    "text": "very heavily through your page cache you know if you're an application like Oracle you're probably calling a sync and you're flushing all your data down",
    "start": "1592960",
    "end": "1598480"
  },
  {
    "text": "to that redo log immediately a lot of you know more current applications like say Cassandra or Services they don't",
    "start": "1598480",
    "end": "1604200"
  },
  {
    "text": "necessarily Force synchronous rights every right right they're replicating to other nodes in the cluster you have a",
    "start": "1604200",
    "end": "1609240"
  },
  {
    "text": "certain level of you know Quorum you you feel comfortable that you can avoid a small loss of data for the purpose of",
    "start": "1609240",
    "end": "1614640"
  },
  {
    "text": "performance um and availability and so Cassandra will be right into the page cache and then it will do um a flush uh",
    "start": "1614640",
    "end": "1621679"
  },
  {
    "text": "periodically so the default configuration of the page cache is non ideal for a bunch of workloads that have",
    "start": "1621679",
    "end": "1627200"
  },
  {
    "text": "heavy right characteristics and you'll have very bursty IO Behavior so one of the settings is dirty ratio and I didn't",
    "start": "1627200",
    "end": "1633880"
  },
  {
    "text": "mean to qualify these aren't these actually aren't the fully qualified tunables right to fit them on a slide we cut all that off so this might be like",
    "start": "1633880",
    "end": "1640520"
  },
  {
    "text": "um you know VM memory whatever something like that so you have to go look those up yourself but the dirty ratio specifi",
    "start": "1640520",
    "end": "1648640"
  },
  {
    "text": "um the percentage of the page cache which is allowed to have dirty Pages before your process or a process that's",
    "start": "1648640",
    "end": "1654840"
  },
  {
    "text": "trying to write into the page cast is responsible for flushing its own buffers and so you can have an application that's just bursting ahead and the",
    "start": "1654840",
    "end": "1660880"
  },
  {
    "text": "defaults 10 right so once you get to 10% of your dirty pages on a lot of distributions um your process will have",
    "start": "1660880",
    "end": "1666399"
  },
  {
    "text": "to stop doing the work it wants to do like writing to the buffer cache it'll have start pushing its buffers down to the file system um itself and so it's",
    "start": "1666399",
    "end": "1673600"
  },
  {
    "text": "like a hiccup you don't really need so what we do is we say you know let us have a larger amount of dirty Pages um",
    "start": "1673600",
    "end": "1680000"
  },
  {
    "text": "in memory and then to overcome the limitation of that you know you think well that's great if I'm adding all this",
    "start": "1680000",
    "end": "1685279"
  },
  {
    "text": "additional space to write data to but eventually it's going to catch up to me and kill me right because at some point I'm going to have to write it out so",
    "start": "1685279",
    "end": "1691240"
  },
  {
    "text": "again you have to understand your workload but the uh you know the PD flush or the kernel flusher threads there's pretty much one per device",
    "start": "1691240",
    "end": "1697840"
  },
  {
    "text": "they're responsible for waking up looking at what's in the page cach and writing the dirty Pages out for you so",
    "start": "1697840",
    "end": "1703000"
  },
  {
    "text": "what we do to offset the fact that there's probably going to be more dirty pages over time is we basically tell the",
    "start": "1703000",
    "end": "1708799"
  },
  {
    "text": "flush demon you need to wake up sooner I think the default is 10 um and what we say is when you know 5% of the pages the",
    "start": "1708799",
    "end": "1716200"
  },
  {
    "text": "areas allocated for dirty pages is is is dirty wake up and start doing work right so you sort of making the flush team and",
    "start": "1716200",
    "end": "1722000"
  },
  {
    "text": "be more proactive now you guys are probably familiar with like operational analysis and steady state Behavior and",
    "start": "1722000",
    "end": "1727720"
  },
  {
    "text": "the assumption is if you're overrunning your IO charact you know your IO capabilities all the time you're not",
    "start": "1727720",
    "end": "1732799"
  },
  {
    "text": "going to get ahead right we're not building an infinite Q here so this does really well for high iio bursty",
    "start": "1732799",
    "end": "1738039"
  },
  {
    "text": "applications where your iio sub subsystem for the most part can keep up over time you don't want to just be on",
    "start": "1738039",
    "end": "1743880"
  },
  {
    "text": "something that you can't you can't get ahead on so we're telling the the flesh demon to wake up earlier um and then we",
    "start": "1743880",
    "end": "1749840"
  },
  {
    "text": "also set this uh dirty expire sesex and um so that's 5 minutes the default",
    "start": "1749840",
    "end": "1755640"
  },
  {
    "text": "configuration I believe is 30 seconds which basically says I do not want any dirty pages to be sitting in the page",
    "start": "1755640",
    "end": "1762600"
  },
  {
    "text": "cach for more than 30 seconds and so when the flush demon wakes up it makes a rush and it has to start writing out all",
    "start": "1762600",
    "end": "1768200"
  },
  {
    "text": "these dirty pages and it does it sort of periodic I think it becomes alive and then it sleeps shortly but what ends up",
    "start": "1768200",
    "end": "1773399"
  },
  {
    "text": "happening is it's you're putting a lot of pressure on the flesh demon to try to write much more aggressively and so you",
    "start": "1773399",
    "end": "1778440"
  },
  {
    "text": "can actually have a much bigger impact on your system so if you open it up to 5 minutes then the flesh demon can run a",
    "start": "1778440",
    "end": "1784240"
  },
  {
    "text": "little more in a healthy fashion and you don't see as much burst eio and it takes into account that there's going to be more data to write right we said 60% of",
    "start": "1784240",
    "end": "1791240"
  },
  {
    "text": "the the page Cache can be dirty Pages um and those tend to balance each other out and these are all pretty low-level",
    "start": "1791240",
    "end": "1796440"
  },
  {
    "text": "tunables so you'll want want to look at your characteristics but the advanced tools will show you I think um fincor is",
    "start": "1796440",
    "end": "1801840"
  },
  {
    "text": "the command which lets you look at your um the entities in the page cach and you can determine the various States so",
    "start": "1801840",
    "end": "1807120"
  },
  {
    "text": "there's there's tools out there to help evaluate all of this um then the last one swappiness zero we set that because",
    "start": "1807120",
    "end": "1814039"
  },
  {
    "text": "we don't have we don't allocate any swap space on our systems right they're very single tenant we don't plan on overloading them um most of them are",
    "start": "1814039",
    "end": "1820200"
  },
  {
    "text": "just Java apps running with like tomcat and so the colel doesn't need to waste",
    "start": "1820200",
    "end": "1825279"
  },
  {
    "text": "any time trying to think about if it should even swap somebody out because you don't have any swap space so if you don't use swap space just set this and",
    "start": "1825279",
    "end": "1831360"
  },
  {
    "text": "then your Colonel doesn't have to spend time thinking about it I don't have any hard numbers to share on this we're still doing some tuning on Cassandra",
    "start": "1831360",
    "end": "1837720"
  },
  {
    "text": "under various conditions but hopefully um we'll get it out in a tech blog or something so people can see some some",
    "start": "1837720",
    "end": "1842799"
  },
  {
    "text": "real data around that okay block layer tuning um how am I",
    "start": "1842799",
    "end": "1847840"
  },
  {
    "text": "on time by the way gu 40ish we're like 40-ish minutes into it okay speed up a",
    "start": "1847840",
    "end": "1853039"
  },
  {
    "start": "1852000",
    "end": "1852000"
  },
  {
    "text": "little bit so the block tuning layer so um this has three or four things that I was going to mention so one is the",
    "start": "1853039",
    "end": "1859120"
  },
  {
    "text": "scheduler uh you know when you're running on AWS and you're in a Zen container you're not really determining",
    "start": "1859120",
    "end": "1864639"
  },
  {
    "text": "your right pattern down at the file system level or down at the device level right the uh the Zen layer the Dom zero",
    "start": "1864639",
    "end": "1870399"
  },
  {
    "text": "and others are actually handling your right scheduling for you and so in most cases you run with the noop schedule",
    "start": "1870399",
    "end": "1875760"
  },
  {
    "text": "which is just a fifo right model like I'm just going to jam my data into the into the underlying system they're going",
    "start": "1875760",
    "end": "1881399"
  },
  {
    "text": "to determine out the best W it because most likely if I spend a bunch of time trying to reorder the right request to make them efficient at the disc level",
    "start": "1881399",
    "end": "1887840"
  },
  {
    "text": "it's all going to be torn abart by what other lever what other layers are down there so the noop algorithm is really",
    "start": "1887840",
    "end": "1893399"
  },
  {
    "text": "good especially for SSD based systems and then the number of requests you can have in the pipe so basically when",
    "start": "1893399",
    "end": "1898559"
  },
  {
    "text": "you're pushing data down um into the file system there are buffers that maintain at at each device level um how",
    "start": "1898559",
    "end": "1905200"
  },
  {
    "text": "many requests are allowed to be queued up and if you ever do ioat and you see the average Q size um I think the",
    "start": "1905200",
    "end": "1910919"
  },
  {
    "text": "default is 128 and so once you get above 128 you sort of have to back off and um",
    "start": "1910919",
    "end": "1918039"
  },
  {
    "text": "the block layer will actually sort of do some sorting on that and try to merge certain things as it goes down to the device system this has a more",
    "start": "1918039",
    "end": "1924679"
  },
  {
    "text": "significant impact on um spinning disc system versus SSD because you're actually a giving more to data to the",
    "start": "1924679",
    "end": "1930639"
  },
  {
    "text": "block layer that lets it do a more efficient ordering of rights which translates usually into better efficiency at your IO level um and then",
    "start": "1930639",
    "end": "1936519"
  },
  {
    "text": "read ahead basically says if you're going out to get me data go ahead and get more of it if anybody's worked with Oracle and has ever heard of like you",
    "start": "1936519",
    "end": "1942919"
  },
  {
    "text": "know B multiblock file IO read count or something like when you get your time on your disc you really want to use it so",
    "start": "1942919",
    "end": "1948480"
  },
  {
    "text": "you want to fetch more data um and the last one RQ Affinity this really deals with steering of soft interrupts so when",
    "start": "1948480",
    "end": "1956639"
  },
  {
    "text": "you issue a request from your process and it goes down through the block layer makes it to the device driver layer um",
    "start": "1956639",
    "end": "1961840"
  },
  {
    "text": "and your device is bound to a certain CPU on your system and you'll probably see the irq rate on that CPU if you're",
    "start": "1961840",
    "end": "1968639"
  },
  {
    "text": "heavy on iO just always getting hammered well on the round trip when the request is coming back up from the device driver",
    "start": "1968639",
    "end": "1974200"
  },
  {
    "text": "layer and it's dispatched to the block layer for queuing the default behavior is that that return irq the soft irq be",
    "start": "1974200",
    "end": "1980720"
  },
  {
    "text": "handled on the same processor the io was scheduled on for the device um this is another one of the cool parameters I",
    "start": "1980720",
    "end": "1986399"
  },
  {
    "text": "believe this was uh added by Google um into the kernel a lot of these aren't enabled by default but what it basically",
    "start": "1986399",
    "end": "1992760"
  },
  {
    "text": "uh maintains like a hash table and when the request comes back it actually dispatches it back to the process the",
    "start": "1992760",
    "end": "1999760"
  },
  {
    "text": "the core on which the original IO request was issued and so you end up getting better CPU cache usage um",
    "start": "1999760",
    "end": "2006279"
  },
  {
    "text": "mileage May but as the processor speed is sort of slowed down and its rate of increase but the number of cores has",
    "start": "2006279",
    "end": "2012039"
  },
  {
    "text": "gone up if you're on a system where you see a large amount of irq processing load heavy you know hard or soft this",
    "start": "2012039",
    "end": "2018279"
  },
  {
    "text": "type of parameter can really spread out your distribution of that and you can see some big performance",
    "start": "2018279",
    "end": "2023880"
  },
  {
    "text": "gains memory allocation tuning this one's this is more of a reliability one um by default Linux will over allocate",
    "start": "2023880",
    "end": "2030600"
  },
  {
    "start": "2030000",
    "end": "2030000"
  },
  {
    "text": "memory people will say give me 20 gigs and it'll say great there's 20 gigs it doesn't actually care if it has 20 gigs",
    "start": "2030600",
    "end": "2035840"
  },
  {
    "text": "in the future for you you might go and get to the point where you actually need 20 gigs and then there's some my understanding is there's a mechanism",
    "start": "2035840",
    "end": "2041639"
  },
  {
    "text": "where they'll go around and start killing some processes to make sure you get your memory so if you're in our environment we are running you know we",
    "start": "2041639",
    "end": "2047519"
  },
  {
    "text": "have a large Heap um uh Java process in most cases we want it to have all of its",
    "start": "2047519",
    "end": "2053000"
  },
  {
    "text": "memory allocated to it all the time and we don't want to get into a situation where we can't get the allocation we need this basically says be very strict",
    "start": "2053000",
    "end": "2060118"
  },
  {
    "text": "about the allocation of memory when somebody asks for it and make sure that it exists um and it's a combination of",
    "start": "2060119",
    "end": "2066118"
  },
  {
    "text": "uh what's available Swap and what's in your physical memory and we don't run with swap so it's really what's available in",
    "start": "2066119",
    "end": "2071240"
  },
  {
    "text": "physical although in order to prevent someone from doing like a DDOS or overrunning your system there is a",
    "start": "2071240",
    "end": "2076679"
  },
  {
    "text": "parameter which says okay I'll give you a lot of memory and I'll guarantee it but it can't be more than a certain percentage of physical memory because",
    "start": "2076679",
    "end": "2082520"
  },
  {
    "text": "you're probably trying to do something nasty on our system like forc all the other processes out through the same behavior and so if you set the ratio to",
    "start": "2082520",
    "end": "2089079"
  },
  {
    "text": "80 that means that at any time a given process could actually request up to 80% of physical memory for itself and it",
    "start": "2089079",
    "end": "2095240"
  },
  {
    "text": "will mark it out um you might hit that if you set it to the strict sort of the strict model and then you don't raise",
    "start": "2095240",
    "end": "2101040"
  },
  {
    "text": "that ratio I think the default is like 40 not you have to check on your system but it just it just adds more",
    "start": "2101040",
    "end": "2106119"
  },
  {
    "text": "reliability you aren't going to have this out of memories uh problems so Network stack tuning um this",
    "start": "2106119",
    "end": "2112920"
  },
  {
    "text": "is something we're working on quite a bit and this this first parameter TCP slow start after idle are people",
    "start": "2112920",
    "end": "2118960"
  },
  {
    "text": "familiar with that parameter okay it's sort of been a thorn in a lot of people's side for some time um Scott on",
    "start": "2118960",
    "end": "2125000"
  },
  {
    "start": "2121000",
    "end": "2121000"
  },
  {
    "text": "my team here did some research uh when we were splitting our traffic between West and East and we were seeing some",
    "start": "2125000",
    "end": "2131320"
  },
  {
    "text": "behaviors that we weren't expecting because we use a lot of connection pooling we have an open source product called or open source uh component",
    "start": "2131320",
    "end": "2138040"
  },
  {
    "text": "called ribbon it's a lot of our IPC maintains connection pools you know does uh retries and so you know the latency I",
    "start": "2138040",
    "end": "2145079"
  },
  {
    "text": "think was about the round trip was about 80 milliseconds for us what we were seeing was occasionally we'd have these spikes and they would be like 170 to 200",
    "start": "2145079",
    "end": "2152200"
  },
  {
    "text": "milliseconds and we're like what you know what the hell is going on um and so we really dug into it Scott knows all",
    "start": "2152200",
    "end": "2157760"
  },
  {
    "text": "the details I don't know all the details but basically what happens if you have a keep Al life connection that's over a",
    "start": "2157760",
    "end": "2163240"
  },
  {
    "text": "wide area network and it's not necessarily busy that often when I say busy it isn't um it isn't like it has to",
    "start": "2163240",
    "end": "2169280"
  },
  {
    "text": "be idle for a second it has to be idle for like one or two round trips like maybe 80 milliseconds and when that happens it kicks the connection back",
    "start": "2169280",
    "end": "2175839"
  },
  {
    "text": "into slow start mode and you have to ramp back up again so what was happening is we had these connection pools and under periods of low load the connection",
    "start": "2175839",
    "end": "2183000"
  },
  {
    "text": "only has to be idle for a couple hundred milliseconds then it would get kicked back into slow star more and we slow start mode and we would incorporate more",
    "start": "2183000",
    "end": "2189440"
  },
  {
    "text": "round trips just to warm it up again this basically says don't do that just keep the congestion window it whatever",
    "start": "2189440",
    "end": "2194880"
  },
  {
    "text": "it was and don't kick me back to a lower level I think Google a bunch of other massive you know web presence people",
    "start": "2194880",
    "end": "2200480"
  },
  {
    "text": "they just set this by default um but it wasn't it wasn't pushed down the TCP fin timeout so if you have",
    "start": "2200480",
    "end": "2206800"
  },
  {
    "text": "an application that has a high rate of socket churn from your clients and your server itself or your application is in",
    "start": "2206800",
    "end": "2212720"
  },
  {
    "text": "charge of tearing down the connection um once you close that connection on on the client it goes into a certain State I",
    "start": "2212720",
    "end": "2218440"
  },
  {
    "text": "think fin weight this parameter says that um uh the amount of time that",
    "start": "2218440",
    "end": "2223839"
  },
  {
    "text": "that's allowed to wait weight in that state so you don't run out you know and you set it lower so you don't run out of sockets that can be a problem where",
    "start": "2223839",
    "end": "2228920"
  },
  {
    "text": "you're having this High connection churn rate and you run out of sockets so that's something we said a little bit",
    "start": "2228920",
    "end": "2234400"
  },
  {
    "text": "lower uh TCP early transmit so this is based on a a Google paper paper and",
    "start": "2234400",
    "end": "2240359"
  },
  {
    "text": "Google did a bunch of extensions in this area there's actually a lot of really cool uh Linux tunables that came from",
    "start": "2240359",
    "end": "2245720"
  },
  {
    "text": "Google which are not enable by default right because just like we serve a few movies they serve a few web pages um but",
    "start": "2245720",
    "end": "2252400"
  },
  {
    "text": "probably a little scale above us as well and so the early retransmits by default um you know when you're when you're",
    "start": "2252400",
    "end": "2259079"
  },
  {
    "text": "communicating with someone over a TP conversation if you receive a duplicate a um the default TCP behavior is to try",
    "start": "2259079",
    "end": "2266359"
  },
  {
    "text": "to determine if that behavior is actually a result of out of order packet delivery or if there's actual loss and",
    "start": "2266359",
    "end": "2272119"
  },
  {
    "text": "so normally what it does is it waits around and it waits for like three of them to arrive and it's like okay now I know what the problem is and it like",
    "start": "2272119",
    "end": "2277160"
  },
  {
    "text": "sends a packet or takes whatever necessary recovery um and Google uh did some studies especially in parts of the",
    "start": "2277160",
    "end": "2283720"
  },
  {
    "text": "world where they had lower network uh performance and stability and they determined that their applications were",
    "start": "2283720",
    "end": "2288800"
  },
  {
    "text": "spending a lot of time waiting on either packet loss or out of order delivery um when they should have just automatically",
    "start": "2288800",
    "end": "2294520"
  },
  {
    "text": "sent another packet back to the client to get it going and so this actually lowers it and says instead of waiting for three uh act you know um yeah three",
    "start": "2294520",
    "end": "2303240"
  },
  {
    "text": "acts on the same packet uh we just wait for two so you're actually doing more an early retransmit um and it reduces some",
    "start": "2303240",
    "end": "2308760"
  },
  {
    "text": "of the weight time you get on the other side the rest of these I don't think I'm going to have time to go into much depth",
    "start": "2308760",
    "end": "2313960"
  },
  {
    "text": "on um the one that I do want to talk about is basically the last two on the",
    "start": "2313960",
    "end": "2319800"
  },
  {
    "text": "right or at least the last one so the other ones have to do with how much you want to put in your network queue another one determines how much Network",
    "start": "2319800",
    "end": "2326240"
  },
  {
    "text": "can be taken off the wire and queued up for you to service before you start rejecting that so if you're on a you",
    "start": "2326240",
    "end": "2331560"
  },
  {
    "text": "know a 10 gbit uh system like you're running on a high1 4X large or on a cc2 a lot of the ones especially with the",
    "start": "2331560",
    "end": "2337119"
  },
  {
    "text": "new instances coming out with really awesome Network characteristics around throughput you might want to revisit if you're a network intensive application",
    "start": "2337119",
    "end": "2343800"
  },
  {
    "text": "that you're qer sized appropriately for handling that workload um and then the last one just like I talked about with",
    "start": "2343800",
    "end": "2350160"
  },
  {
    "text": "the irq handling how you can distribute it across multiple cores the same mechanism exists um for Network traffic",
    "start": "2350160",
    "end": "2356040"
  },
  {
    "text": "and I believe this again was probably introduced by Google I could be wrong but basically if you have a very Network",
    "start": "2356040",
    "end": "2361119"
  },
  {
    "text": "intensive application when the packets are coming back it makes an effort to actually service that interrupt back on",
    "start": "2361119",
    "end": "2366920"
  },
  {
    "text": "the core on which the request was previously initiated so if you're just doing a huge amount of network traffic",
    "start": "2366920",
    "end": "2372119"
  },
  {
    "text": "this can have some big value if you're on a larger core system I mean if you're running on like a T1 micro it's probably not a big deal but if you're running on",
    "start": "2372119",
    "end": "2378160"
  },
  {
    "text": "like you know a cc2 or you know high1 and you have a lot of ecus it'll probably bring you more value so mileage",
    "start": "2378160",
    "end": "2384400"
  },
  {
    "text": "may vary you know measure it the other ones have to do with the amount of you know the buffer sizes around TCP handling but they're all tweakable and I",
    "start": "2384400",
    "end": "2391079"
  },
  {
    "text": "think now that we're getting more members in the instance classes you should evaluate them independently and make sure they're not a bottom neck for",
    "start": "2391079",
    "end": "2396920"
  },
  {
    "text": "you um okay tuning road map so as I indicated I would say that Netflix is",
    "start": "2396920",
    "end": "2403359"
  },
  {
    "text": "more at the beginning than the middle or the end of its Ami tuning exercise um we've just started it this year and",
    "start": "2403359",
    "end": "2408520"
  },
  {
    "text": "there's a lot of different workloads we're currently undergoing a move from a Centos based image to more of an auntu",
    "start": "2408520",
    "end": "2413560"
  },
  {
    "text": "based model um we don't have a lot of the utilities available we would like because of how we've configured certain",
    "start": "2413560",
    "end": "2418720"
  },
  {
    "text": "things so um over the next say 6 to 12 months I think there's going to be a lot more data we're pushing out about how",
    "start": "2418720",
    "end": "2424240"
  },
  {
    "text": "we've optimized for various workloads we're probably start working with the encoding team and some other teams as well to see how we can tune those",
    "start": "2424240",
    "end": "2429960"
  },
  {
    "text": "workloads that really push the push the instance type so future tuning activity um I",
    "start": "2429960",
    "end": "2436280"
  },
  {
    "text": "think this was a real key point that Thor brought up before you know we've had the perennial argument at Netflix of H hvm versus PV and it's very difficult",
    "start": "2436280",
    "end": "2444359"
  },
  {
    "start": "2439000",
    "end": "2439000"
  },
  {
    "text": "to get an Apples to Apples comparison because so few instance types will support both types of the VM well now",
    "start": "2444359",
    "end": "2450040"
  },
  {
    "text": "you have this flexibility where let's say you're running mcash and you're on a C3 or you're on an M probably wouldn't",
    "start": "2450040",
    "end": "2455280"
  },
  {
    "text": "be good but say you're on like a an M3 of some sort um you can actually run hvm with PV drivers and PV and actually do",
    "start": "2455280",
    "end": "2462119"
  },
  {
    "text": "apples to apples and determine which one's best for your characteristics I know Intel talked yesterday about instance type selection and certain I",
    "start": "2462119",
    "end": "2469119"
  },
  {
    "text": "think it was SSL or some other security related uh embedded capability in the processor and I believe that might have",
    "start": "2469119",
    "end": "2474400"
  },
  {
    "text": "only really been available if you're on hvm so you have to you have to do your trade-offs but the fact that the PB drivers are available for hvm eliminates",
    "start": "2474400",
    "end": "2481880"
  },
  {
    "text": "some of the limitations on being hvm around optimized IO Behavior Uh so we're going to be looking at all these things",
    "start": "2481880",
    "end": "2487760"
  },
  {
    "text": "we're going to study do some more Cassandra studies we're going to tune the block schedulers pretty much everything I uh talked to you about we",
    "start": "2487760",
    "end": "2494720"
  },
  {
    "text": "don't have a lot of file system intensive applications I mean Cassandra is pretty pretty heavy but you know with",
    "start": "2494720",
    "end": "2500000"
  },
  {
    "text": "butter FS coming out um it's really a top-notch file system but I think there's some mixed opinion over whether",
    "start": "2500000",
    "end": "2505560"
  },
  {
    "text": "it's stable enough for most people's production environments but you know it's on the way um uh the next one in",
    "start": "2505560",
    "end": "2511680"
  },
  {
    "text": "terms of the network performance we're going to evaluate that at our different tiers and that's yeah proportion rate reduction and that's the name of the",
    "start": "2511680",
    "end": "2517720"
  },
  {
    "text": "paper from Google if you want to search for it but it does a really great study on how this early retransmit Behavior",
    "start": "2517720",
    "end": "2523240"
  },
  {
    "text": "just buys them a lot um and then we're just going to capture more data ammer",
    "start": "2523240",
    "end": "2528880"
  },
  {
    "text": "wrote that line so we're going to capture a lot more data um which which is not unusual for us so that's really",
    "start": "2528880",
    "end": "2534119"
  },
  {
    "text": "it now there's a gigantic appendant in this that's probably about 50 pages where it talks about detailed tool usage",
    "start": "2534119",
    "end": "2540960"
  },
  {
    "text": "and goes into a little more depth on um on some of the settings I talked about so it couldn't cover today but more",
    "start": "2540960",
    "end": "2547160"
  },
  {
    "text": "detail you want go ahead and hit that and if you find anything that you think's inaccurate or there's more another data point that'd be valuable",
    "start": "2547160",
    "end": "2552720"
  },
  {
    "text": "for us please let us know you know I don't want to waste time looking at things with the wrong setting um but I",
    "start": "2552720",
    "end": "2558000"
  },
  {
    "text": "think we have about 10 minutes for questions so Thor do you want to come",
    "start": "2558000",
    "end": "2564119"
  },
  {
    "text": "up I'll share the bullets with you um oh one thing I forgot to mention",
    "start": "2564119",
    "end": "2570079"
  },
  {
    "text": "I'm hiring is anybody else hiring here so my performance team needs some performance Engineers so if you're",
    "start": "2570079",
    "end": "2575319"
  },
  {
    "text": "interested in working on really big scary stuff let me know",
    "start": "2575319",
    "end": "2581440"
  }
]