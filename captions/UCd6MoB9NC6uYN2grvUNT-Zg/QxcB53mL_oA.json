[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "hello everyone and welcome to reinvent I hope you guys are having a great time I name is Michael Labib a principal",
    "start": "0",
    "end": "5879"
  },
  {
    "text": "solutions architect here at AWS and I'm super super excited to be talking to you guys about Amazon ElastiCache so today",
    "start": "5879",
    "end": "12630"
  },
  {
    "text": "what we're gonna do is we're gonna dive into the service we're also going to look at various usage patterns that you can use with kind of injecting in-memory",
    "start": "12630",
    "end": "19650"
  },
  {
    "text": "data stores with your architectures so for our agenda today we're going to",
    "start": "19650",
    "end": "25109"
  },
  {
    "start": "23000",
    "end": "44000"
  },
  {
    "text": "start with what's new then we'll dive into the service itself we'll review some of those common architecture",
    "start": "25109",
    "end": "31050"
  },
  {
    "text": "patterns will dive into different caching strategies a lot of people ask how do you cache data using rightists or",
    "start": "31050",
    "end": "38460"
  },
  {
    "text": "memcache T and then we'll conclude with best practices so gone are the days",
    "start": "38460",
    "end": "46200"
  },
  {
    "start": "44000",
    "end": "91000"
  },
  {
    "text": "where you have one monolith database serving all your data needs right and for good reason with the diversity of",
    "start": "46200",
    "end": "53520"
  },
  {
    "text": "usage patterns and data access patterns essentially you really need a database",
    "start": "53520",
    "end": "58980"
  },
  {
    "text": "that's that's optimized and purpose-built for a particular use case and in the case of ElastiCache sort of",
    "start": "58980",
    "end": "66150"
  },
  {
    "text": "fits in the in-memory space partly because the data is in memory and because it's a memory it's incredibly",
    "start": "66150",
    "end": "73020"
  },
  {
    "text": "fast orders of magnitude faster than retrieving data from disk and in",
    "start": "73020",
    "end": "78060"
  },
  {
    "text": "addition to the speed you you have specialized data structures that can really augment your applications and",
    "start": "78060",
    "end": "85080"
  },
  {
    "text": "architectures and really speed up things with respect to those whose cases",
    "start": "85080",
    "end": "91490"
  },
  {
    "start": "91000",
    "end": "125000"
  },
  {
    "text": "speaking of speed you know when you're building an application typically you know speed is one of the things that",
    "start": "91490",
    "end": "97560"
  },
  {
    "text": "should be when you're your top design principles right so you'd be hard-pressed to think of a use case that",
    "start": "97560",
    "end": "103710"
  },
  {
    "text": "you want to build a slow system and you know the opposite is true and and that's partly why we see such a demand for a",
    "start": "103710",
    "end": "111030"
  },
  {
    "text": "memory data source and it's not really a sort of a binary you know do I choose in-memory or do I choose a different",
    "start": "111030",
    "end": "116490"
  },
  {
    "text": "type of database it's really to augment your architectures and we're gonna really dive into that when we look at",
    "start": "116490",
    "end": "121979"
  },
  {
    "text": "the architecture patterns so what's new a lot is new with the reticent M cache D",
    "start": "121979",
    "end": "129479"
  },
  {
    "start": "125000",
    "end": "170000"
  },
  {
    "text": "engines Redis 5.0 was out huge huge announcement so run as streams really being the",
    "start": "129479",
    "end": "135670"
  },
  {
    "text": "highlight of Redis five rhetta's five is supported with ElastiCache the other",
    "start": "135670",
    "end": "140950"
  },
  {
    "text": "thing that's new with Redis is sort of Seth sort of gotten got married with lists so if you're familiar with both of",
    "start": "140950",
    "end": "146980"
  },
  {
    "text": "those data structures so what it says can now do pop and block capabilities which is pretty awesome",
    "start": "146980",
    "end": "152920"
  },
  {
    "text": "memcache be one point five ten is out also supported with elastication they're really between both of these engines a",
    "start": "152920",
    "end": "159400"
  },
  {
    "text": "lot of optimizations enhancements algorithms things of that nature we're not going to have time to dive into each",
    "start": "159400",
    "end": "165159"
  },
  {
    "text": "one of these but if you have any questions afterward please you know let me know so since streams are such a big",
    "start": "165159",
    "end": "172900"
  },
  {
    "start": "170000",
    "end": "192000"
  },
  {
    "text": "big topic I want to spend a few minutes talking about the value of streams before we can get into that it's",
    "start": "172900",
    "end": "179530"
  },
  {
    "text": "important to level set on how time series data was sort of built and use",
    "start": "179530",
    "end": "185290"
  },
  {
    "text": "with Redis so there was generally a few different approaches that you would take",
    "start": "185290",
    "end": "190450"
  },
  {
    "text": "with with Redis with respect to time series the first one is you'd leverage a sorted set this is probably the most",
    "start": "190450",
    "end": "196480"
  },
  {
    "text": "common approach and so the way you would do it is essentially a sorted set is you",
    "start": "196480",
    "end": "202449"
  },
  {
    "text": "have a key you have a collection of members and each member is a has a value and a score and the score essentially",
    "start": "202449",
    "end": "209109"
  },
  {
    "text": "allows you to rank the data and the score for time series would be say a",
    "start": "209109",
    "end": "214690"
  },
  {
    "text": "timestamp and that would give you the capability of you know sort of sorting the data either by sending a descending",
    "start": "214690",
    "end": "220780"
  },
  {
    "text": "order the problem with using a sorted set is that you know number one the values have to be unique right in the",
    "start": "220780",
    "end": "227590"
  },
  {
    "text": "time series use case you know you might have duplicate values right and so having a constraint on the uniqueness is",
    "start": "227590",
    "end": "234129"
  },
  {
    "text": "it could be problematic the second one is the scores themselves can be modified",
    "start": "234129",
    "end": "239500"
  },
  {
    "text": "right so they're mutable and that also can be a problem so it's not truly a time series data structure another",
    "start": "239500",
    "end": "247359"
  },
  {
    "start": "246000",
    "end": "288000"
  },
  {
    "text": "approach is using a list so a list is a obviously key that's mapped to a",
    "start": "247359",
    "end": "252849"
  },
  {
    "text": "collection of members and those members allow you to push in pop values in the",
    "start": "252849",
    "end": "259989"
  },
  {
    "text": "the order of insertion now the issue with using a list is essentially you you",
    "start": "259989",
    "end": "265300"
  },
  {
    "text": "typically have one can and that the consumer could be a blocking consumer that's just popping",
    "start": "265300",
    "end": "271160"
  },
  {
    "text": "elements off the list so there's no fan-out capabilities with a list and there's also no way to recover from you",
    "start": "271160",
    "end": "278450"
  },
  {
    "text": "know say you popped a value and say that the message and get processed properly doesn't really have that true capability",
    "start": "278450",
    "end": "285770"
  },
  {
    "text": "in terms of recovering from a message another data structure that people would",
    "start": "285770",
    "end": "291470"
  },
  {
    "start": "288000",
    "end": "334000"
  },
  {
    "text": "use is pops up and so pops up is not really a data structure but it's a capability it's embedded in Redis and",
    "start": "291470",
    "end": "297980"
  },
  {
    "text": "the thing with pub/sub is it does support fan-out right so you have a channel and then you're you define a",
    "start": "297980",
    "end": "304400"
  },
  {
    "text": "channel then you have a you publish messages to the channel then you could have subscribers subscribing to that",
    "start": "304400",
    "end": "310880"
  },
  {
    "text": "channel and as our subscribing you know basically you're fanning out those messages the problem is is the data is",
    "start": "310880",
    "end": "317570"
  },
  {
    "text": "not being persisted in a structure in a data structure so if a client is not listening to the channel it's gonna miss",
    "start": "317570",
    "end": "323630"
  },
  {
    "text": "the message right and so it's not really ideal for you know critical data or",
    "start": "323630",
    "end": "330230"
  },
  {
    "text": "truly you know message driven data or time series data so the lack of you know",
    "start": "330230",
    "end": "336470"
  },
  {
    "start": "334000",
    "end": "445000"
  },
  {
    "text": "a true time series a data structure is really what birth the need of a reddish stream so I'm gonna kind of dive into",
    "start": "336470",
    "end": "342580"
  },
  {
    "text": "the anatomy quite a bit and we'll kind of break it down to some degree um so",
    "start": "342580",
    "end": "349580"
  },
  {
    "text": "you have three concepts right do you have a producer you have the stream itself which is the data structure and then you have a consumer so from a",
    "start": "349580",
    "end": "355970"
  },
  {
    "text": "producer standpoint you can add a value to the stream using the X add command",
    "start": "355970",
    "end": "361690"
  },
  {
    "text": "basically syntactically would look like this right you define the name of the stream in this case is called my stream",
    "start": "361690",
    "end": "367880"
  },
  {
    "text": "the star is basically saying you know Redis hey generate an ID for me a unique",
    "start": "367880",
    "end": "374270"
  },
  {
    "text": "ID in this case the idea would always be a time with the sequence so it's",
    "start": "374270",
    "end": "379970"
  },
  {
    "text": "guaranteed to be unique and then the message is sort of a pair with the message and the value so as soon as I",
    "start": "379970",
    "end": "386660"
  },
  {
    "text": "persist that into the stream Redis returns back that unique ID that ID",
    "start": "386660",
    "end": "392090"
  },
  {
    "text": "would be used for me to know what was the last message that I persisted so I can use that to iterate over the mess",
    "start": "392090",
    "end": "398630"
  },
  {
    "text": "is in the stream now the stream itself is append-only right so the data is immutable right and",
    "start": "398630",
    "end": "404840"
  },
  {
    "text": "so it's ideal for you know capturing the true sequence of events and in addition",
    "start": "404840",
    "end": "410780"
  },
  {
    "text": "to that you know there's a variety of different ways that you can consume that message and this in this example we're",
    "start": "410780",
    "end": "416210"
  },
  {
    "text": "just using an X range I'm saying give me all the messages with the min Max and so in this case I got my message back but",
    "start": "416210",
    "end": "423680"
  },
  {
    "text": "there's a variety of other ways and even more interestingly as you can define something called a consumer group and so",
    "start": "423680",
    "end": "429830"
  },
  {
    "text": "a consumer is consumer group is really a collection of consumers that are all participating in retrieving data from",
    "start": "429830",
    "end": "437030"
  },
  {
    "text": "the stream itself right so incredibly powerful in each one of these consumers is really just getting a unique value",
    "start": "437030",
    "end": "443330"
  },
  {
    "text": "out of the stream so in addition to the Redis updates a lot of ElastiCache",
    "start": "443330",
    "end": "449930"
  },
  {
    "start": "445000",
    "end": "480000"
  },
  {
    "text": "updates since last year I'll kind of touch on some of the bigger ones so in",
    "start": "449930",
    "end": "455060"
  },
  {
    "text": "place version upgrades say you're on Redis 3.2 or Redis for and you're using",
    "start": "455060",
    "end": "460640"
  },
  {
    "text": "cluster mode enabled or or not you can upgrade the version right and so you can",
    "start": "460640",
    "end": "466730"
  },
  {
    "text": "do this without any manual steps or application changes and basically what we'll do is we'll upgrade your replicas",
    "start": "466730",
    "end": "474200"
  },
  {
    "text": "and it will just fail over to those replicas so we'll essentially we'll take care of that operation for you the other",
    "start": "474200",
    "end": "481430"
  },
  {
    "start": "480000",
    "end": "536000"
  },
  {
    "text": "one is if you remember last year we talked a lot about the our fours well now m5s and our 5s around much",
    "start": "481430",
    "end": "488420"
  },
  {
    "text": "greater performance from the new version that the new version over the previous",
    "start": "488420",
    "end": "494540"
  },
  {
    "text": "version in terms of performance CPU and on top of that what we've done and",
    "start": "494540",
    "end": "501020"
  },
  {
    "text": "that's really the gray bar is we added additional optimizations on those instance types so just another you know",
    "start": "501020",
    "end": "508010"
  },
  {
    "text": "I guess case for using ElastiCache over running Redis on you know in the vanilla",
    "start": "508010",
    "end": "513919"
  },
  {
    "text": "ec2 instance but generally what you can see out of this graph if you compare like the blue which is the previous gen",
    "start": "513919",
    "end": "521090"
  },
  {
    "text": "our fours the gray would be the optimized on for ElastiCache m5r 5s you",
    "start": "521090",
    "end": "528710"
  },
  {
    "text": "really see a hundred and forty-four throughput additional throughput that you can achieve which is massive another",
    "start": "528710",
    "end": "537160"
  },
  {
    "start": "536000",
    "end": "598000"
  },
  {
    "text": "big announcement is you know previously we still you know the standard",
    "start": "537160",
    "end": "542200"
  },
  {
    "text": "configuration is 15 shards and a shard again is made up of a primary and up to",
    "start": "542200",
    "end": "548140"
  },
  {
    "text": "5 replicas which make up a max of 90 nodes we're changing that right if so if",
    "start": "548140",
    "end": "554260"
  },
  {
    "text": "you really need have the need to have much more maybe larger data or a much",
    "start": "554260",
    "end": "559870"
  },
  {
    "text": "larger cluster use case the 250 nodes support is available today if you want",
    "start": "559870",
    "end": "565390"
  },
  {
    "text": "to white get white listed and so if you can see do the math depending on how many primaries shards that you have or",
    "start": "565390",
    "end": "572830"
  },
  {
    "text": "how many replicas associated to that shard you can have upwards to 170 terabytes of data in your cluster that's",
    "start": "572830",
    "end": "579760"
  },
  {
    "text": "massive right so you know it's that might sound crazy but maybe not right",
    "start": "579760",
    "end": "584920"
  },
  {
    "text": "because people aren't just caching database data anymore right it's not just database it's not just session data",
    "start": "584920",
    "end": "590710"
  },
  {
    "text": "it's a lot of other types of data we'll talk through that when we get into the application architectures and in",
    "start": "590710",
    "end": "599350"
  },
  {
    "start": "598000",
    "end": "634000"
  },
  {
    "text": "addition to what I just mentioned a lot expect a lot more optimizations that are going to occur on the actual the",
    "start": "599350",
    "end": "606910"
  },
  {
    "text": "instances that we we we host for you and we're also gonna support the rename",
    "start": "606910",
    "end": "612160"
  },
  {
    "text": "command a lot of you have shared that requests that you want to be able to hide a specific commands maybe like the",
    "start": "612160",
    "end": "618280"
  },
  {
    "text": "keys command that prevents your developers from you know bringing down the the cluster and then self servicing",
    "start": "618280",
    "end": "624520"
  },
  {
    "text": "self-service patching so we're gonna give you the capability of really selecting you know within a certain",
    "start": "624520",
    "end": "629800"
  },
  {
    "text": "window when you want those patches to take place all right so let's kind of",
    "start": "629800",
    "end": "635560"
  },
  {
    "start": "634000",
    "end": "642000"
  },
  {
    "text": "level set we'll dive into the service and kind of just break it down make sure that we're all on the same page so what",
    "start": "635560",
    "end": "643930"
  },
  {
    "start": "642000",
    "end": "761000"
  },
  {
    "text": "is Redis Redis is the most popular in memory key value store in the market and",
    "start": "643930",
    "end": "650440"
  },
  {
    "text": "you can check that you know just check maybe DB engines or other sites and you'll see the rankings and what people",
    "start": "650440",
    "end": "657370"
  },
  {
    "text": "are really saying about Redis and they say that and it's so popular for a lot of reasons",
    "start": "657370",
    "end": "663350"
  },
  {
    "text": "and really you know in addition to the speed and you know you can see here sub-millisecond performance on average",
    "start": "663350",
    "end": "670370"
  },
  {
    "text": "we see somewhere in a neighborhood of 400 to 500 microseconds in terms of speed but you have additional things",
    "start": "670370",
    "end": "677480"
  },
  {
    "text": "would say like memcache T doesn't have which is really the H a right so you have H a you also can do backup and",
    "start": "677480",
    "end": "684080"
  },
  {
    "text": "restore you also have atomic operations and in where it gets really interesting is the",
    "start": "684080",
    "end": "689770"
  },
  {
    "text": "additional data structures that you have so if you're a developer and as you're",
    "start": "689770",
    "end": "695570"
  },
  {
    "text": "building your applications and you're thinking about the different objects and you know collection classes that you eat",
    "start": "695570",
    "end": "702380"
  },
  {
    "text": "that you use a lot of these are familiar to you right so whether you're using a list or a hash map a set a sorted set a",
    "start": "702380",
    "end": "709460"
  },
  {
    "text": "lot of these are things that you're currently working with so you no longer have to think about what type of how do",
    "start": "709460",
    "end": "715970"
  },
  {
    "text": "i serialize the data structure that I'm using in my current application you can actually just persist that structure",
    "start": "715970",
    "end": "722630"
  },
  {
    "text": "into Redis and operate on that structure right so it actually eliminates a lot of lines of code with respect to you know",
    "start": "722630",
    "end": "731630"
  },
  {
    "text": "what's supported and then the cool thing about having backup and restore is you can essentially up to 20 times per day",
    "start": "731630",
    "end": "738800"
  },
  {
    "text": "as a soft limit you can take snapshots right create an RPO of the data that's",
    "start": "738800",
    "end": "744140"
  },
  {
    "text": "in memory as well and from a syntactical standpoint incredibly powerful the api's",
    "start": "744140",
    "end": "751130"
  },
  {
    "text": "are very rich with respect to sorting ranking and a lot of different ways that you can manipulate data and retrieve",
    "start": "751130",
    "end": "758450"
  },
  {
    "text": "data in Redis so additional background",
    "start": "758450",
    "end": "764750"
  },
  {
    "start": "761000",
    "end": "843000"
  },
  {
    "text": "here so like if you were to stand up say open source Redis on your own maybe in a dev a POC environment you might find it",
    "start": "764750",
    "end": "772220"
  },
  {
    "text": "to be you know relatively straightforward but if you want to productionize that there's a lot that",
    "start": "772220",
    "end": "777530"
  },
  {
    "text": "goes into production izing Rutter's number one is how do you make sure that this thing is truly AJ right especially",
    "start": "777530",
    "end": "784940"
  },
  {
    "text": "if the data is say critical data or really you know it's enterprise level",
    "start": "784940",
    "end": "791510"
  },
  {
    "text": "application it's also difficult to scale right because you know if you want to change the",
    "start": "791510",
    "end": "797780"
  },
  {
    "text": "sharding whether you want to add shards and remove shards there's some limitations with open-source writers I'm gonna talk a little bit about that in a",
    "start": "797780",
    "end": "804880"
  },
  {
    "text": "slide and then from a from an investment standpoint you know how much resources",
    "start": "804880",
    "end": "810770"
  },
  {
    "text": "you want to invest in putting people behind managing Redis and when you do the TCO on that with comparing that with",
    "start": "810770",
    "end": "818060"
  },
  {
    "text": "a managed service doesn't might not make sense right I'll give you one reason is if you are hosting it on your own you're",
    "start": "818060",
    "end": "825080"
  },
  {
    "text": "paying for say data out between the ACS so if they add the data out charge plus",
    "start": "825080",
    "end": "830750"
  },
  {
    "text": "the ec2 charge and then compare that with just the managed service which doesn't charge you for that data out",
    "start": "830750",
    "end": "837290"
  },
  {
    "text": "communication right and so generally you'll see the price that'd be pretty similar right",
    "start": "837290",
    "end": "842360"
  },
  {
    "text": "and so with Amazon ElastiCache in addition to us hosting Redis we're very",
    "start": "842360",
    "end": "848450"
  },
  {
    "start": "843000",
    "end": "901000"
  },
  {
    "text": "much invested into the engine we have developers who are contributing we",
    "start": "848450",
    "end": "854000"
  },
  {
    "text": "actually just last year we contributed a encryption in transit to the Redis community along with other bugs and",
    "start": "854000",
    "end": "860960"
  },
  {
    "text": "fixes and things of that nature and we have a lot if you think about you know the our platform we have so many",
    "start": "860960",
    "end": "868000"
  },
  {
    "text": "clusters that we manage and so we see this at massive scale and we've really",
    "start": "868000",
    "end": "873350"
  },
  {
    "text": "tuned ElastiCache that fit a lot of these needs right so one thing which",
    "start": "873350",
    "end": "879410"
  },
  {
    "text": "we're going to talk about is you know how we char data and that's coming up in a next slide but the other thing is",
    "start": "879410",
    "end": "886310"
  },
  {
    "text": "really removing the headaches with respects and making sure that you know the data is encrypted the manage the manageability of the data",
    "start": "886310",
    "end": "894020"
  },
  {
    "text": "making sure that you're patched and interact and all that stuff sort of just goes away in a managed service so use",
    "start": "894020",
    "end": "902330"
  },
  {
    "start": "901000",
    "end": "1000000"
  },
  {
    "text": "cases if you're new to Redis you generally start with caching right but",
    "start": "902330",
    "end": "907790"
  },
  {
    "text": "as you expose those data structures and you think about the different ways that you can utilize Redis well we typically",
    "start": "907790",
    "end": "913880"
  },
  {
    "text": "see as caching becomes you know maybe the second you know most common use case and people start using Redis - whether it be a",
    "start": "913880",
    "end": "920900"
  },
  {
    "text": "buffer behind adjusting data a fast and just layer or maybe they want to do a",
    "start": "920900",
    "end": "926450"
  },
  {
    "text": "leaderboard and so in this slide it says leaderboard but the reality is is you",
    "start": "926450",
    "end": "932189"
  },
  {
    "text": "can have a Sales Leader board you can have the leaderboard that's really just ranking various types of data maybe most",
    "start": "932189",
    "end": "937799"
  },
  {
    "text": "popular activities to your your website or products could be Twitter feeds so",
    "start": "937799",
    "end": "945059"
  },
  {
    "text": "really the leader board is just leveraging a sort of stuff there's also geospatial capabilities",
    "start": "945059",
    "end": "950309"
  },
  {
    "text": "we'll talk about that but essentially Redis allows you to to you know pass in",
    "start": "950309",
    "end": "955350"
  },
  {
    "text": "a longitude and latitude and do like a jido radius a command and really identify values or members that belong",
    "start": "955350",
    "end": "962579"
  },
  {
    "text": "to a particular geography we see a lot of media media streaming really just",
    "start": "962579",
    "end": "969329"
  },
  {
    "text": "whether you're putting portions of the media into you know maybe a byte array into Redis or whether you have links to",
    "start": "969329",
    "end": "976649"
  },
  {
    "text": "other media and contents session stores chat applications now just became a lot",
    "start": "976649",
    "end": "982019"
  },
  {
    "text": "easier with redish streaming the same with message queues people use lists for",
    "start": "982019",
    "end": "987569"
  },
  {
    "text": "that and we're seeing a lot of machine learning use cases as well and so this side of the use cases make a lot of",
    "start": "987569",
    "end": "993899"
  },
  {
    "text": "sense anytime that you really need to augment an architecture and make something run a lot faster and so this",
    "start": "993899",
    "end": "1001549"
  },
  {
    "start": "1000000",
    "end": "1019000"
  },
  {
    "text": "is this slide is really just giving you an idea the different types of organizations that we have using",
    "start": "1001549",
    "end": "1007039"
  },
  {
    "text": "ElastiCache and if you just think about the verticals you can imagine a different types of use cases that they",
    "start": "1007039",
    "end": "1012919"
  },
  {
    "text": "might be using it forward a lot of them have to do with the previous slide that I was just discussing all right so with",
    "start": "1012919",
    "end": "1021019"
  },
  {
    "start": "1019000",
    "end": "1089000"
  },
  {
    "text": "Redis I'm gonna level set on a different types of topologies that you have just so you can understand what's available",
    "start": "1021019",
    "end": "1027079"
  },
  {
    "text": "the vertically scale topology some people refer this as kind of the classic mode vertically scaled what it really",
    "start": "1027079",
    "end": "1034220"
  },
  {
    "text": "means is all your data resides into one node right and so you have a primary node and for every primary you can have",
    "start": "1034220",
    "end": "1042649"
  },
  {
    "text": "0 to 5 replicas right and so the largest size cluster that you can have is",
    "start": "1042649",
    "end": "1049309"
  },
  {
    "text": "whatever data you can fit in the largest instance type now in this type it's",
    "start": "1049309",
    "end": "1054350"
  },
  {
    "text": "apology you would have an application that would would connect to your primary endpoint and then you might have another",
    "start": "1054350",
    "end": "1061279"
  },
  {
    "text": "connection from your applications is that's connected to your replica end points",
    "start": "1061279",
    "end": "1066340"
  },
  {
    "text": "right so you would scale your reads off your replica end points and you have your primary for your rights and you",
    "start": "1066340",
    "end": "1074590"
  },
  {
    "text": "know generally what happens is depending on you the way your client works you know there would be various options with",
    "start": "1074590",
    "end": "1081040"
  },
  {
    "text": "you know how you want to do connection pooling and how you want to discover those replicas because the replica",
    "start": "1081040",
    "end": "1086560"
  },
  {
    "text": "endpoints are not persistent now the other topology and this is where",
    "start": "1086560",
    "end": "1092290"
  },
  {
    "start": "1089000",
    "end": "1310000"
  },
  {
    "text": "we're gonna spend most of the time because most organizations are actually moving into this topology is the",
    "start": "1092290",
    "end": "1098040"
  },
  {
    "text": "horizontally scale topology and one of the reasons why makes a lot of sense to",
    "start": "1098040",
    "end": "1103750"
  },
  {
    "text": "move into this is because you can have a one shard cluster mode enabled topology",
    "start": "1103750",
    "end": "1110650"
  },
  {
    "text": "which really is the equivalent of having one primary with you know the same amount of replicas in the vertical scale",
    "start": "1110650",
    "end": "1116980"
  },
  {
    "text": "topology the added benefit though is that you can change the amount of shards",
    "start": "1116980",
    "end": "1122530"
  },
  {
    "text": "that you want in this topology with zero downtime and I'm going to talk about that in a in a later later slide but",
    "start": "1122530",
    "end": "1129610"
  },
  {
    "text": "what horizontally scales apologies really mean is that rather than all your data that entire key space belonging on",
    "start": "1129610",
    "end": "1137230"
  },
  {
    "text": "one one node you're dividing that key space into shards right so let's assume",
    "start": "1137230",
    "end": "1143560"
  },
  {
    "text": "that you have four shards on the concept of a slot is essentially there's 16,000",
    "start": "1143560",
    "end": "1149310"
  },
  {
    "text": "384 slots and then that slot would be used to decide what range of what past",
    "start": "1149310",
    "end": "1156370"
  },
  {
    "text": "lock range would belong to each shard and in the anatomy of a shard essentially is the same thing you have a",
    "start": "1156370",
    "end": "1163360"
  },
  {
    "text": "primary and then you have a replica that that is that corresponds to the same hash slot range for that particular",
    "start": "1163360",
    "end": "1170440"
  },
  {
    "text": "shard now where this is different in addition to the partitioning is also on",
    "start": "1170440",
    "end": "1175630"
  },
  {
    "text": "how you connect to Redis and how you talk to Redis so what happens is if you're using a cluster or where a client",
    "start": "1175630",
    "end": "1182970"
  },
  {
    "text": "the client itself you can map it to talk to the configuration endpoints so we",
    "start": "1182970",
    "end": "1188320"
  },
  {
    "text": "exposed that rather than a primary endpoint and in the configuration and point when you talk through that the",
    "start": "1188320",
    "end": "1194320"
  },
  {
    "text": "client depending on the implementation of the client is essentially gonna do a cluster slots info command behind the scenes it's",
    "start": "1194320",
    "end": "1201610"
  },
  {
    "text": "gonna get a map from Redis what the topology looks like and it's apology is",
    "start": "1201610",
    "end": "1206770"
  },
  {
    "text": "what are all the shards and what are the hash slot ranges associated to each shard and it's also gonna have like a",
    "start": "1206770",
    "end": "1213640"
  },
  {
    "text": "flag on which one would be a primary which one would be a replica now once it",
    "start": "1213640",
    "end": "1219250"
  },
  {
    "text": "has that math that map is stored locally in the client and the client then knows",
    "start": "1219250",
    "end": "1224890"
  },
  {
    "text": "that if I were to do a get set type of operation the first thing that it's going to do is gonna execute a circle 16",
    "start": "1224890",
    "end": "1231669"
  },
  {
    "text": "mod function and it's gonna dis based on the output of that function it's gonna know where to send the the key right",
    "start": "1231669",
    "end": "1238299"
  },
  {
    "text": "what's shard does that key belong to and if for whatever reason that's apology",
    "start": "1238299",
    "end": "1243970"
  },
  {
    "text": "changed after the after the cached map the the the client would generally",
    "start": "1243970",
    "end": "1251200"
  },
  {
    "text": "refresh its map right so that's an implementation detail of the the client and some clients also will frequently",
    "start": "1251200",
    "end": "1257320"
  },
  {
    "text": "ping Redis in the background so the takeaway here is that when you build a",
    "start": "1257320",
    "end": "1262809"
  },
  {
    "text": "custom mode enabled cluster you're basically deciding how many shards you",
    "start": "1262809",
    "end": "1267850"
  },
  {
    "text": "want how many partitions do you want of your data and then you can also by default",
    "start": "1267850",
    "end": "1273370"
  },
  {
    "text": "it's equally distributing the hash slot ranges on that cluster but you can also",
    "start": "1273370",
    "end": "1278890"
  },
  {
    "text": "do a custom distribution if you wanted to and so you can say I want just this",
    "start": "1278890",
    "end": "1284020"
  },
  {
    "text": "range on shard 1 and maybe that range on start a char - and you might want to do",
    "start": "1284020",
    "end": "1289120"
  },
  {
    "text": "that if you had if you wanted to isolate some keys on to a particular shark and then you can always check when you do",
    "start": "1289120",
    "end": "1296049"
  },
  {
    "text": "this you're essentially you should see and depending on the distribution you pick you can see the current items in",
    "start": "1296049",
    "end": "1302770"
  },
  {
    "text": "each one of those shards to see the using the CloudWatch metric that we",
    "start": "1302770",
    "end": "1307870"
  },
  {
    "text": "expose alright so just kind of a quick recap with this slide in the next the",
    "start": "1307870",
    "end": "1314980"
  },
  {
    "text": "value that you get with cluster mode enabled number one is you have additional nodes right and so additional",
    "start": "1314980",
    "end": "1322929"
  },
  {
    "text": "nodes what that really means is you're able to add more memory you're also able",
    "start": "1322929",
    "end": "1328299"
  },
  {
    "text": "to scale your rights you're also able to scale your reads much more effectively you",
    "start": "1328299",
    "end": "1335470"
  },
  {
    "text": "also have more connections if you really needed to support a lot more connections",
    "start": "1335470",
    "end": "1340860"
  },
  {
    "text": "they both support the open source client there's nothing unique about ElastiCache with respect to connecting to it and the",
    "start": "1340860",
    "end": "1350320"
  },
  {
    "text": "other added benefit is the the failover time so I mentioned earlier that if you",
    "start": "1350320",
    "end": "1355960"
  },
  {
    "text": "were a cluster mode disabled you get a primary endpoint and that's a persistent endpoint with cluster mode enabled",
    "start": "1355960",
    "end": "1362080"
  },
  {
    "text": "you're dealing with a map right and so what that means is there's no dns updates or propagation that needs to",
    "start": "1362080",
    "end": "1368710"
  },
  {
    "text": "take place so failover it takes a lot faster and then the other thing that's great is that if your data is",
    "start": "1368710",
    "end": "1374800"
  },
  {
    "text": "partitioned let's say you had 10 shards and one of the shards had a failure only 10% of your rights are impacted I mean",
    "start": "1374800",
    "end": "1382180"
  },
  {
    "text": "you could still read because if you're assuming you have a replica but then until that failover takes place which",
    "start": "1382180",
    "end": "1388540"
  },
  {
    "text": "could be you know up to 15 and 30 seconds and then you can start writing again but it's much faster than that",
    "start": "1388540",
    "end": "1395050"
  },
  {
    "text": "vertically scale topology which also needs to do the record set changes and then from a price standpoint they could",
    "start": "1395050",
    "end": "1401650"
  },
  {
    "text": "be very similar right so it's kind of you know you have to do the math but",
    "start": "1401650",
    "end": "1406780"
  },
  {
    "text": "essentially what happens is you know where you would normally scale up now here's this choosing the smallest",
    "start": "1406780",
    "end": "1412600"
  },
  {
    "text": "instance type that supports the network bandwidth that makes sense for you I would highly recommend a new are fives",
    "start": "1412600",
    "end": "1419380"
  },
  {
    "text": "and fives and pick the pic maybe a smaller size that makes sense to have multiples right and again you can do the",
    "start": "1419380",
    "end": "1426040"
  },
  {
    "text": "retarding capabilities with online recharging all right so just the kind of",
    "start": "1426040",
    "end": "1432640"
  },
  {
    "start": "1431000",
    "end": "1504000"
  },
  {
    "text": "breakdown to anatomy even further so a shard I mentioned earlier by default you",
    "start": "1432640",
    "end": "1437860"
  },
  {
    "text": "can have up to 15 this is the assuming you're not white listed so 15 shards and",
    "start": "1437860",
    "end": "1443590"
  },
  {
    "text": "again a shard is made up of a primary and a replica we're doing a synchronous replication for you if you want to see",
    "start": "1443590",
    "end": "1449860"
  },
  {
    "text": "what the replication lag is that's another CloudWatch metric and in the replicas is optional you can have either",
    "start": "1449860",
    "end": "1456100"
  },
  {
    "text": "no replicas or you could have up to five per each shard and so assume that a",
    "start": "1456100",
    "end": "1461980"
  },
  {
    "text": "failure happens we're managing this for you there's no sense at all there's no additional tools that you need will detect the failure",
    "start": "1461980",
    "end": "1469000"
  },
  {
    "text": "and it will well essentially we will elect one of the replicas with the least",
    "start": "1469000",
    "end": "1476560"
  },
  {
    "text": "replication lag to be the new primary and it will send off a few SMS notifications in case you want to",
    "start": "1476560",
    "end": "1482470"
  },
  {
    "text": "consume that and maybe do some reporting and again this takes place in roughly 15",
    "start": "1482470",
    "end": "1487750"
  },
  {
    "text": "seconds and then you can still read from your replicas right and so that's a another thing to keep in mind and also",
    "start": "1487750",
    "end": "1494500"
  },
  {
    "text": "to look at when you're choosing what client you want to use because some clients make this really easy where you can annotate and say hey you know semi",
    "start": "1494500",
    "end": "1501310"
  },
  {
    "text": "reads they're my rights they're now with online we sharding and basically what",
    "start": "1501310",
    "end": "1507700"
  },
  {
    "start": "1504000",
    "end": "1642000"
  },
  {
    "text": "this slide to show it is that you started with three and you wanted to say you wanted to move to five this is a",
    "start": "1507700",
    "end": "1513760"
  },
  {
    "text": "simple API call and in this particular case I'm scaling out right and so where",
    "start": "1513760",
    "end": "1519250"
  },
  {
    "text": "this is different with ElastiCache is that we do a slot by slot migration now",
    "start": "1519250",
    "end": "1526000"
  },
  {
    "text": "the open the ownership of the request is still at the source and it's just until",
    "start": "1526000",
    "end": "1531880"
  },
  {
    "text": "the entire slot was been migrated to the destination then we we started sending",
    "start": "1531880",
    "end": "1537760"
  },
  {
    "text": "the traffic essentially to the the new shard now what this gives you because",
    "start": "1537760",
    "end": "1543160"
  },
  {
    "text": "you're doing so we're doing slot by slot is we give you the ability to still execute you know your Lua commands your",
    "start": "1543160",
    "end": "1549520"
  },
  {
    "text": "M gets on that particular shard whereas with open source it might suffer from that because there's that split",
    "start": "1549520",
    "end": "1556210"
  },
  {
    "text": "slop scenario right and so that's one the second advantage is that this is a",
    "start": "1556210",
    "end": "1562690"
  },
  {
    "text": "much easier more reliable way to char data and so it's easier to recover from",
    "start": "1562690",
    "end": "1568740"
  },
  {
    "text": "there's no manual intervention if during the restarting capability something went wrong that could be problematic if you",
    "start": "1568740",
    "end": "1576190"
  },
  {
    "text": "were doing this on your own and then the other added benefit here is and there's no application changes right so this is",
    "start": "1576190",
    "end": "1582550"
  },
  {
    "text": "happening behind the hood you're not seeing this your application is still connecting to and writing and reading",
    "start": "1582550",
    "end": "1588910"
  },
  {
    "text": "from the cluster the cluster is still knows which is the primaries and replicas and",
    "start": "1588910",
    "end": "1594550"
  },
  {
    "text": "once the slot changes take place it's gonna modify its map and then your your application will know where to direct",
    "start": "1594550",
    "end": "1600550"
  },
  {
    "text": "the traffic now the only thing that I will mention is that this might add a",
    "start": "1600550",
    "end": "1605620"
  },
  {
    "text": "little bit of latency up to 20% latency but again we're talking about an in-memory data store so where you're",
    "start": "1605620",
    "end": "1612090"
  },
  {
    "text": "initially at 400 microseconds maybe now you're at 600 microseconds and then the",
    "start": "1612090",
    "end": "1618550"
  },
  {
    "text": "same thing is true for scaling in right so you have the ability to change how many shards that you want whether it's",
    "start": "1618550",
    "end": "1623800"
  },
  {
    "text": "out or in and this is great especially if you don't know how many shards you need or if you know that there's an",
    "start": "1623800",
    "end": "1629440"
  },
  {
    "text": "event happening maybe you're a retailer and you want to prepare for the holidays or you know Black Friday just passed or",
    "start": "1629440",
    "end": "1635470"
  },
  {
    "text": "Cyber Monday you have that ability to plan in advance right very easily with no downtime now because you can do this",
    "start": "1635470",
    "end": "1644380"
  },
  {
    "start": "1642000",
    "end": "1711000"
  },
  {
    "text": "right there's no reason why you can't automate a process right and said I'm always about automation I always talk",
    "start": "1644380",
    "end": "1650800"
  },
  {
    "text": "about automation especially with CloudWatch metrics that you should be concerned with in this particular case",
    "start": "1650800",
    "end": "1657100"
  },
  {
    "text": "say for example you were watching a particular metric maybe was memory maybe",
    "start": "1657100",
    "end": "1663520"
  },
  {
    "text": "you is engine level CPU utilization if it met your threshold kick out an SNS",
    "start": "1663520",
    "end": "1669700"
  },
  {
    "text": "notification trigger a lambda function and that lambda function can execute a",
    "start": "1669700",
    "end": "1674830"
  },
  {
    "text": "shard recharging command and essentially modify the cluster with whatever",
    "start": "1674830",
    "end": "1680680"
  },
  {
    "text": "additional shards make sense for your topology now this is not going to happen",
    "start": "1680680",
    "end": "1685930"
  },
  {
    "text": "instantaneously because there's a lot of you know data being moved around and you know operations moving around but at",
    "start": "1685930",
    "end": "1691720"
  },
  {
    "text": "least you know it's happening durably and you know just account for that you",
    "start": "1691720",
    "end": "1696940"
  },
  {
    "text": "know account for the timing so you might want to be a little bit more conservative with respect to when that threshold is met all right so let's kind",
    "start": "1696940",
    "end": "1706030"
  },
  {
    "text": "of dive into some architecture patterns that we see at least common ones first",
    "start": "1706030",
    "end": "1712480"
  },
  {
    "start": "1711000",
    "end": "1802000"
  },
  {
    "text": "one is caching right and so this caching slide is obviously showing you a lot of",
    "start": "1712480",
    "end": "1717580"
  },
  {
    "text": "different ways and back-end data stores that you can cache and I'm showing this because a lot of times people are just",
    "start": "1717580",
    "end": "1723940"
  },
  {
    "text": "so stuck on you know databases well the reality is is you can cache anything that can be",
    "start": "1723940",
    "end": "1731390"
  },
  {
    "text": "deduced or you know brought down to a data structure or even bytes right so",
    "start": "1731390",
    "end": "1737930"
  },
  {
    "text": "right us is binary safe and so what you can do is really put Redis in front of",
    "start": "1737930",
    "end": "1744350"
  },
  {
    "text": "anything that you want to reduce pressure from or anything that you just",
    "start": "1744350",
    "end": "1749510"
  },
  {
    "text": "want to speed up maybe it's a web application maybe it's a you know some kind of experience maybe it's a buffer",
    "start": "1749510",
    "end": "1755300"
  },
  {
    "text": "maybe it's something else that you want to augment and inject performance right and so I'm gonna highlight in the",
    "start": "1755300",
    "end": "1762020"
  },
  {
    "text": "caching portion of this talk s3 and in a relational database but if you have",
    "start": "1762020",
    "end": "1767900"
  },
  {
    "text": "questions about how do you cache any of these other data sources let me know but the most important thing to be aware of",
    "start": "1767900",
    "end": "1774110"
  },
  {
    "text": "is that you know you always want to make sure that your cache and the validity of",
    "start": "1774110",
    "end": "1781670"
  },
  {
    "text": "the data that's in the cache corresponds to you know if a freshness factor that",
    "start": "1781670",
    "end": "1787160"
  },
  {
    "text": "makes sense for your for your workload right so knowing what the frequency of change of the underlying data and in",
    "start": "1787160",
    "end": "1793250"
  },
  {
    "text": "applying TTL that corresponds to that data and make a lot of sense so we'll talk about that as well during the the",
    "start": "1793250",
    "end": "1799550"
  },
  {
    "text": "caching portion of the talk another use case that we see is sentiment analysis",
    "start": "1799550",
    "end": "1806210"
  },
  {
    "start": "1802000",
    "end": "1896000"
  },
  {
    "text": "so imagine you're and you're consuming a lot of fast moving data whether it's click stream or Twitter feeds or",
    "start": "1806210",
    "end": "1812180"
  },
  {
    "text": "whatever it is you could have a lot of different ways to you know consume so in this case I'm just showing a few",
    "start": "1812180",
    "end": "1818870"
  },
  {
    "text": "different options but at some point you want a data layer that's gonna persist",
    "start": "1818870",
    "end": "1824300"
  },
  {
    "text": "that information or buff but be a buffer to the data that's being ingested a lot of people use Redis for this right",
    "start": "1824300",
    "end": "1830420"
  },
  {
    "text": "because Redis can support incredibly high throughput low latency and you don't pay for throughput costs you don't",
    "start": "1830420",
    "end": "1838070"
  },
  {
    "text": "pay for request rates basically what you're paying for is the instance of pricing and so what you can have is you",
    "start": "1838070",
    "end": "1846530"
  },
  {
    "text": "know consume that information whether it's in a list or reddish streams which we just talked about and then you can",
    "start": "1846530",
    "end": "1851540"
  },
  {
    "text": "have consumers or multiple consumers peeling records off of the of the stream",
    "start": "1851540",
    "end": "1856550"
  },
  {
    "text": "and then sending those records at whatever frequency that you want to other places",
    "start": "1856550",
    "end": "1861919"
  },
  {
    "text": "maybe to do sentiment analysis with comprehend maybe to do a leaderboard as you're seeing these tweets or the click",
    "start": "1861919",
    "end": "1868970"
  },
  {
    "text": "streams taking place you want to see what the activity of the ranking of those items are in the in the in the",
    "start": "1868970",
    "end": "1875600"
  },
  {
    "text": "feed and another option is you know maybe sending that data to your primary database somewhere else right so that's",
    "start": "1875600",
    "end": "1882559"
  },
  {
    "text": "the another that's where DynamoDB is sort of fits in this kind of a strange sound here that's what that is",
    "start": "1882559",
    "end": "1891610"
  },
  {
    "text": "all right I'm just gonna ignore it so somebody shuts it off alright so and",
    "start": "1891610",
    "end": "1897380"
  },
  {
    "start": "1896000",
    "end": "1950000"
  },
  {
    "text": "then the other one is using IOT data so we get this quite a bit with IOT data",
    "start": "1897380",
    "end": "1904490"
  },
  {
    "text": "you know you you obviously have these sensors you're consuming data you know from various places",
    "start": "1904490",
    "end": "1910279"
  },
  {
    "text": "AWS IOT core makes it incredibly easy to do and as you're consuming this data you",
    "start": "1910279",
    "end": "1916070"
  },
  {
    "text": "might have a rule that's saying saying you know for for a specific type of information I want to send that data to",
    "start": "1916070",
    "end": "1922279"
  },
  {
    "text": "Redis and in maybe the other data the raw data I want to have a historical",
    "start": "1922279",
    "end": "1927409"
  },
  {
    "text": "view of all that information and shove that into my data like may be s3 and so we talked about the different ways you",
    "start": "1927409",
    "end": "1933740"
  },
  {
    "text": "can deal with time-series data with Redis again reddish streams here makes a lot of sense and in sort of sets would",
    "start": "1933740",
    "end": "1940580"
  },
  {
    "text": "be the other option all right I'm gonna",
    "start": "1940580",
    "end": "1947750"
  },
  {
    "text": "ignore that all right so the next one is real-time Kinesis filtering so we get",
    "start": "1947750",
    "end": "1954740"
  },
  {
    "start": "1950000",
    "end": "2011000"
  },
  {
    "text": "this a bit whether it's Kinesis whether it's Kafka it doesn't really matter so assume that you're grabbing or you're processing a",
    "start": "1954740",
    "end": "1961549"
  },
  {
    "text": "lot of streaming data fast moving data but as you're processing that information you want to see if maybe",
    "start": "1961549",
    "end": "1969740"
  },
  {
    "text": "similar information was already persisted right into into Redis because",
    "start": "1969740",
    "end": "1975980"
  },
  {
    "text": "maybe you want to decorate that information right you see a particular customer or a particular tweet query the",
    "start": "1975980",
    "end": "1981529"
  },
  {
    "text": "cache the in-memory cache oh I see some relevant information there what I'm",
    "start": "1981529",
    "end": "1986659"
  },
  {
    "text": "going to do is decorate the information and presented into a process maybe a cleanse stream another way is deduping",
    "start": "1986659",
    "end": "1994470"
  },
  {
    "text": "information or making sure that you just have unique information there or maybe",
    "start": "1994470",
    "end": "1999690"
  },
  {
    "text": "some other counters that you're taking place Redis makes a lot of sense again from the cost perspective the speed",
    "start": "1999690",
    "end": "2005480"
  },
  {
    "text": "perspective and the variety of data structures that are supported mobile so",
    "start": "2005480",
    "end": "2012590"
  },
  {
    "start": "2011000",
    "end": "2079000"
  },
  {
    "text": "mobile especially with the geospatial capabilities in a caching as well so you",
    "start": "2012590",
    "end": "2017900"
  },
  {
    "text": "have a mobile application maybe you want to build a recommendation engine so you know I'm a user I'm on my mobile phone",
    "start": "2017900",
    "end": "2024470"
  },
  {
    "text": "you can pass up my longitude latitude that will hit maybe your API API gateway",
    "start": "2024470",
    "end": "2030530"
  },
  {
    "text": "will trigger a lambda integration hit Redis semi eight my longitude latitude a",
    "start": "2030530",
    "end": "2035780"
  },
  {
    "text": "Redis execute a Geo a radius command and then have Redis sends you all the points",
    "start": "2035780",
    "end": "2041930"
  },
  {
    "text": "of interest back to your user right so what are the places within a mile radius",
    "start": "2041930",
    "end": "2047360"
  },
  {
    "text": "of my current position makes a whole lot of sense now in this particular case",
    "start": "2047360",
    "end": "2053060"
  },
  {
    "text": "Redis would be augmenting the backend data store which in this case would be dynamo and then you can have a nice",
    "start": "2053060",
    "end": "2059720"
  },
  {
    "text": "right back pattern where you're constantly refreshing or updating the cache by just triggering off of a dynamo",
    "start": "2059720",
    "end": "2066290"
  },
  {
    "text": "DB streams a lambda function right so it makes a nice little process to keep your",
    "start": "2066290",
    "end": "2071570"
  },
  {
    "text": "cache fresh and so this would be another case where you're showing Redis sort of",
    "start": "2071570",
    "end": "2076669"
  },
  {
    "text": "augment another data store and in rate-limiting so this is one that is",
    "start": "2076669",
    "end": "2082220"
  },
  {
    "text": "kind of counterintuitive at first but then it makes a lot of sense right so especially when you're in a cloud architecture where you could you know",
    "start": "2082220",
    "end": "2088760"
  },
  {
    "text": "scale your back-end or your cloud environments whatever the load is or",
    "start": "2088760",
    "end": "2093830"
  },
  {
    "text": "scale in in some cases you might not want to do that right especially if it's cost prohibitive or maybe you have a",
    "start": "2093830",
    "end": "2099440"
  },
  {
    "text": "product where you're selling you know silver gold platinum depending on the requests per second and so what you",
    "start": "2099440",
    "end": "2107180"
  },
  {
    "text": "would do is Redis also supports accountant a counter is essentially the integer representation stored in a",
    "start": "2107180",
    "end": "2113720"
  },
  {
    "text": "string you could increment or decrement that value and so as the requests are",
    "start": "2113720",
    "end": "2119630"
  },
  {
    "text": "coming in you can check against that counter if you know if your your threshold",
    "start": "2119630",
    "end": "2124880"
  },
  {
    "text": "wasn't met then you can allow the API request to go through if the threshold",
    "start": "2124880",
    "end": "2129980"
  },
  {
    "text": "was met then you would just send a response back to your user hey you exceeded the amount of requests that you",
    "start": "2129980",
    "end": "2136849"
  },
  {
    "text": "can do please upgrade to the next package or whatever makes sense for your environment and we see a lot of other",
    "start": "2136849",
    "end": "2145609"
  },
  {
    "start": "2143000",
    "end": "2274000"
  },
  {
    "text": "sort of you know integration patterns graph search or just another two common",
    "start": "2145609",
    "end": "2152750"
  },
  {
    "text": "ones I'll talk about graph so with graph you know imagine you have this highly",
    "start": "2152750",
    "end": "2158060"
  },
  {
    "text": "connected data this again I'll use the restaurants use case and you want to figure out you know based on maybe some",
    "start": "2158060",
    "end": "2164930"
  },
  {
    "text": "other person's liking what recommendations I should recommend to to",
    "start": "2164930",
    "end": "2170240"
  },
  {
    "text": "myself me being a target person so you would check my you know mine and that",
    "start": "2170240",
    "end": "2176359"
  },
  {
    "text": "person that's similar to me and see what that person likes and then grab that those vertices or those restaurants and",
    "start": "2176359",
    "end": "2183380"
  },
  {
    "text": "then send them over to me but as you send them over to me you might want to also send out to Redis and do a",
    "start": "2183380",
    "end": "2188869"
  },
  {
    "text": "geospatial look up and say are based on those restaurants and maybe there's attributes associated to those objects",
    "start": "2188869",
    "end": "2196010"
  },
  {
    "text": "that say where the the location of those restaurants are you might want to do another geospatial query another one",
    "start": "2196010",
    "end": "2202790"
  },
  {
    "text": "might be you want to see or you want to aggregate similar likings of of people",
    "start": "2202790",
    "end": "2208730"
  },
  {
    "text": "and you want to aggregate that in one of the collections maybe a hash map or a set in Redis you can easily do that",
    "start": "2208730",
    "end": "2216170"
  },
  {
    "text": "right so they sort of make a lot of sense with respect to use cases and the",
    "start": "2216170",
    "end": "2221839"
  },
  {
    "text": "other thing is you can obviously cache right so if you have a query that you're constantly querying you can always a",
    "start": "2221839",
    "end": "2228319"
  },
  {
    "text": "cache that information as well right and in the other use case you know would be",
    "start": "2228319",
    "end": "2234170"
  },
  {
    "text": "search so again like imagine you have queries that you're hitting your your superior search engine you can cache the",
    "start": "2234170",
    "end": "2240560"
  },
  {
    "text": "actual query so the entire query string that you're you're you're you're hitting the back end with and in the response",
    "start": "2240560",
    "end": "2246650"
  },
  {
    "text": "that came out of the the search engine you can cache that another way that we commonly see people using Redis with is",
    "start": "2246650",
    "end": "2254589"
  },
  {
    "text": "as a buffer so imagine you have a lot of fast moving data you're consuming a lot of data and then you want to process",
    "start": "2254589",
    "end": "2260559"
  },
  {
    "text": "that information and peel those records off and sort of you know whether you're using log stash and just output that",
    "start": "2260559",
    "end": "2267519"
  },
  {
    "text": "information into the search engine so basically having Redis serve as a buffer makes sense there as well so I'm going",
    "start": "2267519",
    "end": "2275319"
  },
  {
    "start": "2274000",
    "end": "2284000"
  },
  {
    "text": "to dive into some of the caching strategies we get this question a lot from people especially if you're new to",
    "start": "2275319",
    "end": "2281410"
  },
  {
    "text": "write us how do you cache data so the two common patterns basically lazy",
    "start": "2281410",
    "end": "2287259"
  },
  {
    "start": "2284000",
    "end": "2399000"
  },
  {
    "text": "loading and in an write back so we're lazy loading you always assume from your",
    "start": "2287259",
    "end": "2292390"
  },
  {
    "text": "application that the data is stored in the cache whether it's stored there or not there's",
    "start": "2292390",
    "end": "2298749"
  },
  {
    "text": "a solution on how to put it there but you start with assuming it's there and if it's there great that's called a hit",
    "start": "2298749",
    "end": "2304690"
  },
  {
    "text": "there's a CloudWatch metric that will tell you whether the data was found in Redis you can check the hit the the hits",
    "start": "2304690",
    "end": "2311380"
  },
  {
    "text": "the hit count in cloud watch now in this particular case we're using we're",
    "start": "2311380",
    "end": "2316930"
  },
  {
    "text": "assuming that a result set object was stored in Redis so results that object",
    "start": "2316930",
    "end": "2322960"
  },
  {
    "text": "is what you'd get at a relational database so I'm gonna pass a key the key would be save my query my sequel query",
    "start": "2322960",
    "end": "2329049"
  },
  {
    "text": "or something similar to my sequel query and if I get back some bytes and I'm",
    "start": "2329049",
    "end": "2334839"
  },
  {
    "text": "going to assume that that was the the byte or the serialize interpretation of that results that object now what I'm",
    "start": "2334839",
    "end": "2341499"
  },
  {
    "text": "gonna do is I'm going to convert those bytes into a result set then I'm gonna return that back to the application",
    "start": "2341499",
    "end": "2347529"
  },
  {
    "text": "right and so this is really nice because you can add a nice dowel pattern pattern right in front of the your application",
    "start": "2347529",
    "end": "2354690"
  },
  {
    "text": "if the data is not there you query your back-end database just like you would normally do and then what you do is",
    "start": "2354690",
    "end": "2361299"
  },
  {
    "text": "after that you shove that result set object right back into the cache so you just serialize those bytes back down",
    "start": "2361299",
    "end": "2369190"
  },
  {
    "text": "basically you convert your your cached or your result set object into a byte",
    "start": "2369190",
    "end": "2374380"
  },
  {
    "text": "array and then just shove that into run us and then for the key the key can be",
    "start": "2374380",
    "end": "2380680"
  },
  {
    "text": "the sequel statement or a similar query statement that sense to retrieve the data and",
    "start": "2380680",
    "end": "2387000"
  },
  {
    "text": "optionally you always have the TTL so you can say for that particular query I want it to be true for the next hour or",
    "start": "2387000",
    "end": "2394410"
  },
  {
    "text": "day or 30 seconds it's completely up to you now what other people do",
    "start": "2394410",
    "end": "2401130"
  },
  {
    "start": "2399000",
    "end": "2465000"
  },
  {
    "text": "while the first approach makes sense if you want to offload pressure from the back end you still have to do the wall",
    "start": "2401130",
    "end": "2406769"
  },
  {
    "text": "RS next and it kind of you know iterate over the iterate over the the object in",
    "start": "2406769",
    "end": "2415230"
  },
  {
    "text": "the first approach and this approach as you're doing the initial iteration of the result set object you can convert",
    "start": "2415230",
    "end": "2421049"
  },
  {
    "text": "that result set object into a hash map now the value with converting it to a",
    "start": "2421049",
    "end": "2426450"
  },
  {
    "text": "hash map is then you can persist the hash map into Redis and and what that gives you is the ability when the when",
    "start": "2426450",
    "end": "2432660"
  },
  {
    "text": "your say your API you want to request particular properties of that row you",
    "start": "2432660",
    "end": "2438510"
  },
  {
    "text": "can say for that particular customer just give me the first name last name and address right because now you've",
    "start": "2438510",
    "end": "2444359"
  },
  {
    "text": "you've reduced that that that row into a map and now you can retrieve individual",
    "start": "2444359",
    "end": "2450180"
  },
  {
    "text": "properties associated to the map so in addition to speeding up the backend now you've you're speeding up your",
    "start": "2450180",
    "end": "2456210"
  },
  {
    "text": "application logic because now you're not dealing with iterating over a result set object each time that you're grabbing",
    "start": "2456210",
    "end": "2462750"
  },
  {
    "text": "the data from cache and so from the s3 perspective same concept right imagine",
    "start": "2462750",
    "end": "2469200"
  },
  {
    "text": "you assume that the data is in cache for this particular example I'm going to assume that the data that's stored in s3",
    "start": "2469200",
    "end": "2475950"
  },
  {
    "text": "was a string and so I defined a a a naming convention for my key name which",
    "start": "2475950",
    "end": "2482849"
  },
  {
    "text": "is the bucket : the key which is the object name and in s3 if I get a value",
    "start": "2482849",
    "end": "2488640"
  },
  {
    "text": "back great I'm gonna return that back and I'm gonna assume that was the cash value nest 3 the value wasn't there I'm",
    "start": "2488640",
    "end": "2494670"
  },
  {
    "text": "gonna query s3 like I typically do maybe I'll get back a s jekt content",
    "start": "2494670",
    "end": "2502099"
  },
  {
    "text": "input stream and I'll convert that input stream essentially to a buffer a string",
    "start": "2502099",
    "end": "2507990"
  },
  {
    "text": "buffer and then again into a string and then shove that into Redis now obviously",
    "start": "2507990",
    "end": "2514289"
  },
  {
    "text": "there's other data structures in different ways that you can do this and this particular I'm gonna take whatever value was in",
    "start": "2514289",
    "end": "2520080"
  },
  {
    "text": "that file or that object and then make that a string and shove that in my value",
    "start": "2520080",
    "end": "2526140"
  },
  {
    "text": "into Redis now with Redis each value can have a max size of 512 megabytes now you",
    "start": "2526140",
    "end": "2534060"
  },
  {
    "text": "never want to store 512 megabytes as a single value but you could write technically you could but generally you",
    "start": "2534060",
    "end": "2540240"
  },
  {
    "text": "would want to have it in a managed manageable size for each individual value now the other thing that's cool",
    "start": "2540240",
    "end": "2547800"
  },
  {
    "start": "2546000",
    "end": "2660000"
  },
  {
    "text": "about us 3 is you can define this sort of right back pattern that's gonna constantly keep your data in memory",
    "start": "2547800",
    "end": "2554250"
  },
  {
    "text": "fresh so assume you you're constantly you're caching us 3 and you're writing",
    "start": "2554250",
    "end": "2559530"
  },
  {
    "text": "to s3 on those inserts you can trigger a lambda function which is gonna shove the",
    "start": "2559530",
    "end": "2564630"
  },
  {
    "text": "data right back into Redis so you begin when you query the data in s3 out of the",
    "start": "2564630",
    "end": "2570000"
  },
  {
    "text": "at a Redis or and you're also hydrating proactively hydrating the data back into",
    "start": "2570000",
    "end": "2575190"
  },
  {
    "text": "Redis all right and so one other thing I'll mention about caching is that with",
    "start": "2575190",
    "end": "2581940"
  },
  {
    "text": "lazy loading if you're just doing lazy loading that might make sense much like",
    "start": "2581940",
    "end": "2587430"
  },
  {
    "text": "it might be a good cost strategy because you're only caching data that's been",
    "start": "2587430",
    "end": "2592530"
  },
  {
    "text": "queried but if the data is not there you always have that ended that first hit",
    "start": "2592530",
    "end": "2597990"
  },
  {
    "text": "where the data wasn't there and then you have to sort of cache it a little bit of",
    "start": "2597990",
    "end": "2603210"
  },
  {
    "text": "over overhead that you have to deal with if you are if you're doing this right",
    "start": "2603210",
    "end": "2608670"
  },
  {
    "text": "back and proactively hydrating the cache what you might be doing is you might be putting data that is never going to be",
    "start": "2608670",
    "end": "2616830"
  },
  {
    "text": "queried right but at least you're pro what you're increasing the probability that on the road whenever when the",
    "start": "2616830",
    "end": "2623280"
  },
  {
    "text": "request comes in that it's going to be found right and so in practice what you",
    "start": "2623280",
    "end": "2628530"
  },
  {
    "text": "want to do is you want to do a combination of both so as you're proactively hydrating the cache you want to apply it a",
    "start": "2628530",
    "end": "2634290"
  },
  {
    "text": "conservative TTL that will make sense that if the data is never queried then",
    "start": "2634290",
    "end": "2640230"
  },
  {
    "text": "just expire it and then when if the the request finally comes in then what you do is then just the lazy load will catch",
    "start": "2640230",
    "end": "2647369"
  },
  {
    "text": "it and throw it into the and then you want to play with that and make sure that you understand you know",
    "start": "2647369",
    "end": "2652560"
  },
  {
    "text": "what exactly is an appropriate TTL value and then what is the type of data you want to cache and that would be a good",
    "start": "2652560",
    "end": "2658980"
  },
  {
    "text": "strategy now as we're talking about data and throwing data in some memory you",
    "start": "2658980",
    "end": "2664770"
  },
  {
    "text": "know you always want to make sure that you sized your cluster to a good size that makes sense that we're not over",
    "start": "2664770",
    "end": "2671430"
  },
  {
    "text": "filling memory but if you ever did overfill memory Redis is sort of polite with respect to adhering to one of your",
    "start": "2671430",
    "end": "2679609"
  },
  {
    "text": "policies your eviction policies or max memory policies to the best of its ability and so you have a variety of",
    "start": "2679609",
    "end": "2685770"
  },
  {
    "text": "different policies that choose from and I'll give you a use case where this makes sense so imagine you're putting in",
    "start": "2685770",
    "end": "2691410"
  },
  {
    "text": "a cache data which is cache queries and if for those queries you're applying a TTL value and you're also storing maybe",
    "start": "2691410",
    "end": "2698940"
  },
  {
    "text": "a metadata that you're not you don't have a TTL because that metadata doesn't expire maybe four months or you know",
    "start": "2698940",
    "end": "2705510"
  },
  {
    "text": "some other you know longer duration of time within that particular case if you",
    "start": "2705510",
    "end": "2711359"
  },
  {
    "text": "ever hit an eviction you probably do not want to use like an all keys LRU because",
    "start": "2711359",
    "end": "2717270"
  },
  {
    "text": "in all keys that are you is going to ignore the the key the TTL value",
    "start": "2717270",
    "end": "2722690"
  },
  {
    "text": "associated to akita it's just going to look for the least recently used key to evict and the scenario are described",
    "start": "2722690",
    "end": "2729780"
  },
  {
    "text": "maybe the the volatile LRU would make better sense because when it does evict a key it's going to choose a key that",
    "start": "2729780",
    "end": "2736200"
  },
  {
    "text": "already has a TTL so leave that metadata values alone all right so cluster size",
    "start": "2736200",
    "end": "2744210"
  },
  {
    "start": "2742000",
    "end": "2809000"
  },
  {
    "text": "best practices when you're first kind of thinking about what is the proper size",
    "start": "2744210",
    "end": "2749250"
  },
  {
    "text": "of this cluster the first thing you think about is storage and when you think about storage like let's assume",
    "start": "2749250",
    "end": "2755250"
  },
  {
    "text": "you know you need a hundred gigs of storage the first thing to keep in mind is that Redis itself needs about 25 gigs",
    "start": "2755250",
    "end": "2762569"
  },
  {
    "text": "of memory sorry 25 percent of memory and so by default what we're gonna do is we're gonna reserve that for you so at that",
    "start": "2762569",
    "end": "2769260"
  },
  {
    "text": "point your total cluster size now you need about a hundred and 25 gigs I'm just giving making up an example and in",
    "start": "2769260",
    "end": "2777000"
  },
  {
    "text": "assuming that you're just guesstimating you're not actually sure how much data you need you",
    "start": "2777000",
    "end": "2782109"
  },
  {
    "text": "might want to add a little buffer there and then on top of that if you're the DevOps person you also want to make sure",
    "start": "2782109",
    "end": "2787839"
  },
  {
    "text": "that people are using TTLs because at the end of the day right you want to make sure that the data is fresh in a",
    "start": "2787839",
    "end": "2793270"
  },
  {
    "text": "cache and doesn't affect you know your your experience or your customer experience and you also want to make",
    "start": "2793270",
    "end": "2801190"
  },
  {
    "text": "sure that your youuuu know how to react to the to a situation where you ever",
    "start": "2801190",
    "end": "2806440"
  },
  {
    "text": "need to scale that memory so that's your first plan then the second plan is identifying what your load is so Redis",
    "start": "2806440",
    "end": "2813790"
  },
  {
    "start": "2809000",
    "end": "2927000"
  },
  {
    "text": "can support incredibly high throughput for each individual node I mean it's",
    "start": "2813790",
    "end": "2819310"
  },
  {
    "text": "incredibly high amounts of operations per second but even with that you still want to have a good idea of you know",
    "start": "2819310",
    "end": "2825640"
  },
  {
    "text": "what's the balance between reads and writes you really need you know maybe a how many read replicas you actually need",
    "start": "2825640",
    "end": "2833170"
  },
  {
    "text": "what's your percentage of reads versus writes so you want to size that appropriately and with writes as you as",
    "start": "2833170",
    "end": "2840820"
  },
  {
    "text": "you need additional writes you'll you'll want to partition add more partitions to increase or add more shards increase",
    "start": "2840820",
    "end": "2847570"
  },
  {
    "text": "your write throughput and then the other thing is always selecting an instance type that supports the proper network",
    "start": "2847570",
    "end": "2854589"
  },
  {
    "text": "bandwidth that you need and then you know lastly that I'll talk about is you",
    "start": "2854589",
    "end": "2861010"
  },
  {
    "text": "know as we we kind of talked briefly about the blast radius that the the",
    "start": "2861010",
    "end": "2866020"
  },
  {
    "text": "nice.you value that you have with partitioning data because only a portion of your rights are affected this if you",
    "start": "2866020",
    "end": "2872859"
  },
  {
    "text": "kind of extrapolate that that logic and think about the different use cases that you might have in your your system if",
    "start": "2872859",
    "end": "2880180"
  },
  {
    "text": "you wanted to sort of isolate maybe you're caching workloads with your high",
    "start": "2880180",
    "end": "2886240"
  },
  {
    "text": "ingest buffer workloads or key workloads along with you know it may be a",
    "start": "2886240",
    "end": "2891670"
  },
  {
    "text": "different workload that might make sense that further sort of protects your environment doesn't affect the blast",
    "start": "2891670",
    "end": "2897520"
  },
  {
    "text": "radius if a particular failure occurred now the more isolation you do obviously there's more clusters that you have",
    "start": "2897520",
    "end": "2903970"
  },
  {
    "text": "maybe additional cost what we generally see people do is is isolate by purpose",
    "start": "2903970",
    "end": "2910780"
  },
  {
    "text": "so all your caching workload in a cash kind of cluster and so on what",
    "start": "2910780",
    "end": "2916590"
  },
  {
    "text": "I would never recommend is just have one cluster and just do everything on that one cluster right because there's just",
    "start": "2916590",
    "end": "2922620"
  },
  {
    "text": "too much happening in one space now I",
    "start": "2922620",
    "end": "2927780"
  },
  {
    "start": "2927000",
    "end": "3028000"
  },
  {
    "text": "talked about cloud watch so these are some of the metrics that you want to be",
    "start": "2927780",
    "end": "2932940"
  },
  {
    "text": "aware of I'll highlight the you know the ones that you I would highly recommend putting a alarm on",
    "start": "2932940",
    "end": "2938820"
  },
  {
    "text": "so bytes used for cash make sure you understand what you know what limit or threshold you you want to be aware of",
    "start": "2938820",
    "end": "2945960"
  },
  {
    "text": "how much data is actually being stored in your in and Redis and the cache hits",
    "start": "2945960",
    "end": "2951510"
  },
  {
    "text": "and this missus is incredibly important because at the end of the day it's all about making sure you have a hit in",
    "start": "2951510",
    "end": "2957600"
  },
  {
    "text": "Redis right so you can just check that check your counts and if the ratio is you know anything less and that's 8090",
    "start": "2957600",
    "end": "2964230"
  },
  {
    "text": "percent and that's a problem that's there's room for optimization there right and that's really making sure that TTL values are appropriate",
    "start": "2964230",
    "end": "2972140"
  },
  {
    "text": "engine level engine CPU utilization so that tells you how much CPU is actually",
    "start": "2972140",
    "end": "2977820"
  },
  {
    "text": "used for Redis and I would make sure that for that one I would have probably multiple alarms kick off maybe one at",
    "start": "2977820",
    "end": "2985410"
  },
  {
    "text": "forty percent you know maybe one at fifteen sixty percent and it had maybe at seventy percent maybe trigger that",
    "start": "2985410",
    "end": "2992250"
  },
  {
    "text": "had that alarm to rashard add more shards to would reduce some of the that",
    "start": "2992250",
    "end": "2997770"
  },
  {
    "text": "pressure or could be replicas depending on what type of requests are causing that problem evictions",
    "start": "2997770",
    "end": "3005090"
  },
  {
    "text": "so again evictions means that you're over filling Redis so in that particular case scale out scale up you definitely",
    "start": "3005090",
    "end": "3011900"
  },
  {
    "text": "don't want to see evictions take place the same thing with swap you shouldn't see any swap I mean this is an in-memory",
    "start": "3011900",
    "end": "3017960"
  },
  {
    "text": "system in memory datastore so so as soon as you see some swap you know that",
    "start": "3017960",
    "end": "3023450"
  },
  {
    "text": "essentially means that you you need more memory and so we have a couple other",
    "start": "3023450",
    "end": "3030470"
  },
  {
    "start": "3028000",
    "end": "3053000"
  },
  {
    "text": "talks we had a reddish stream talk actually uh I just earlier today we also",
    "start": "3030470",
    "end": "3035600"
  },
  {
    "text": "have a workshop if you want to get your hands on some Redis Friday if you're here on Friday please come by we'll also",
    "start": "3035600",
    "end": "3041630"
  },
  {
    "text": "I have some reddish streams taking place in that workshop and again hey thank you very much I hope",
    "start": "3041630",
    "end": "3047840"
  },
  {
    "text": "you learn something new and have a great day today [Applause]",
    "start": "3047840",
    "end": "3054890"
  }
]