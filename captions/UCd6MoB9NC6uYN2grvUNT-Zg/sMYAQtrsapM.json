[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "all right guys hi there my name is Juan via I'm a Solutions Architect here at a",
    "start": "1490",
    "end": "6919"
  },
  {
    "text": "AWS in today's talk we're gonna talk about real time data processing with AWS",
    "start": "6919",
    "end": "12900"
  },
  {
    "text": "lambda we're gonna kind of do it with a focus on security we're gonna look at a real world security application where we",
    "start": "12900",
    "end": "19320"
  },
  {
    "text": "take real time security audit events and we do some kind of compute some kind of logic monitoring around that in some",
    "start": "19320",
    "end": "25949"
  },
  {
    "text": "alerting right so we're gonna look at something very very very practical so I'm part of the team enable is called",
    "start": "25949",
    "end": "33300"
  },
  {
    "text": "the partner team I'm a partner Solutions Architect and more specifically than that I focus on IOT server less and",
    "start": "33300",
    "end": "40559"
  },
  {
    "text": "machine learning mostly I also do some sass here at AWS I mostly like to just",
    "start": "40559",
    "end": "46260"
  },
  {
    "text": "build cool software products I used to be a software developer about two years ago I still technically am just not by",
    "start": "46260",
    "end": "52949"
  },
  {
    "text": "title so I just build a lot of cool tools and I build stuff for customers and partners everyday so yeah so Before",
    "start": "52949",
    "end": "60899"
  },
  {
    "start": "60000",
    "end": "60000"
  },
  {
    "text": "we jump into looking at this practical real time data pipeline let's just talk",
    "start": "60899",
    "end": "66270"
  },
  {
    "text": "real quick about what a real time or batch processing data pipeline looks like right so those are the two options",
    "start": "66270",
    "end": "72390"
  },
  {
    "text": "that we've got you can process events in real time or near real time which we're going to talk about timing a little bit",
    "start": "72390",
    "end": "77880"
  },
  {
    "text": "or you can do is called batch processing when you collect events and you process them and some kind of schedule right so",
    "start": "77880",
    "end": "84420"
  },
  {
    "text": "let's look at batch processing first right so in a batch processing scenario you have some kind of data source in any",
    "start": "84420",
    "end": "91170"
  },
  {
    "text": "data processing pipeline you have a data source which is the data that you're going to be processing in this case this",
    "start": "91170",
    "end": "96689"
  },
  {
    "text": "data could come from it could come from sensors it could come from from customer data from your website it could be like",
    "start": "96689",
    "end": "103590"
  },
  {
    "text": "a click stream feed from your e-commerce Lake application for examples you can see what your customers click and where",
    "start": "103590",
    "end": "110430"
  },
  {
    "text": "they go where they're looking at a product for example right and typically what we see customers and partners doing",
    "start": "110430",
    "end": "116219"
  },
  {
    "text": "with this immense amount of data because it truly really is a mess when you think of an e-commerce application like you",
    "start": "116219",
    "end": "121619"
  },
  {
    "text": "know let's go for the big one like amazon.com but even if you go for a smaller ecommerce application when you",
    "start": "121619",
    "end": "127079"
  },
  {
    "text": "have thousands of customers and you collecting clickstream data for example that's a lot of data when you're looking at",
    "start": "127079",
    "end": "132909"
  },
  {
    "text": "security audit events in an actual production AWS account there's gonna be a lot of events right so what we",
    "start": "132909",
    "end": "138879"
  },
  {
    "text": "typically see customers do is store this data in s3 they'll store it in some kind of s3 bucket it's a very low cost",
    "start": "138879",
    "end": "145780"
  },
  {
    "text": "you pay for the storage per gig per month and you believe it's sitting there but then you need to do something with",
    "start": "145780",
    "end": "151510"
  },
  {
    "text": "that data right you got to turn it into something actionable you have to derive some kind of insight from it right so",
    "start": "151510",
    "end": "158349"
  },
  {
    "text": "this is where the compute layer comes in right so we've talked about the data source and where you put your data and now you're going to pull that data and",
    "start": "158349",
    "end": "164379"
  },
  {
    "text": "do something with it now in this example I'm using AWS lambda as my little icon",
    "start": "164379",
    "end": "169930"
  },
  {
    "text": "there for compute but it could really be any compute solution here right this could be ec2 this could be Kinesis",
    "start": "169930",
    "end": "175420"
  },
  {
    "text": "analytics this could be a partner solution but for this presentation we're going to focus on lambda and talk about",
    "start": "175420",
    "end": "180790"
  },
  {
    "text": "some of the benefits of why use lambda for real-time processing of information and once you do some kind of computation",
    "start": "180790",
    "end": "188019"
  },
  {
    "text": "generally you don't just compute and and and send results into the ether you do something with it you store that data",
    "start": "188019",
    "end": "194139"
  },
  {
    "text": "you do some kind of action on it like trigger a page your duty message you trigger a text message to go out to",
    "start": "194139",
    "end": "199810"
  },
  {
    "text": "someone or you trigger some kind of like recovery action you know it could really be truly anything and it could be one or",
    "start": "199810",
    "end": "206230"
  },
  {
    "text": "more actions as well right so when we look at a batch processing workload",
    "start": "206230",
    "end": "211239"
  },
  {
    "text": "though we're looking at a workload where data is stored and then on some kind of periodic schedule you pull the data you",
    "start": "211239",
    "end": "217540"
  },
  {
    "text": "process it and you do something with it right so you're not doing this in real time you're doing this maybe like every night or maybe you know on Friday nights",
    "start": "217540",
    "end": "225750"
  },
  {
    "text": "and you're not triggering actions immediately right and in some of in some",
    "start": "225750",
    "end": "231220"
  },
  {
    "text": "cases that's more than enough you know that's really good enough but when you're talking about security and",
    "start": "231220",
    "end": "236260"
  },
  {
    "text": "security audit events you don't want to wait till like the next day to process those to find out that you've had some",
    "start": "236260",
    "end": "241479"
  },
  {
    "text": "anomalous access patterns in your web application you want to pick those up immediately so you can do something",
    "start": "241479",
    "end": "247659"
  },
  {
    "text": "about it immediately so when we talk about batch processing we're talking typically 15 or more minute response",
    "start": "247659",
    "end": "253000"
  },
  {
    "text": "times that's generally what we're talking about usually for our customers that's about a day is generally what",
    "start": "253000",
    "end": "259120"
  },
  {
    "text": "their what their what their interval is for batch processing when we look at real-time",
    "start": "259120",
    "end": "264760"
  },
  {
    "text": "or event-driven data pipelines it looks very similar to the previous one except",
    "start": "264760",
    "end": "271030"
  },
  {
    "text": "instead of storing in s3 the events that are coming in we're gonna actually process those as they come in as quickly",
    "start": "271030",
    "end": "277810"
  },
  {
    "text": "as we can before putting it in some kind of durable storage right so s3 was the",
    "start": "277810",
    "end": "283090"
  },
  {
    "text": "durable storage that we were using previously and we can keep it stuff in there forever in this model we are gonna",
    "start": "283090",
    "end": "289810"
  },
  {
    "text": "process the message immediately as quickly as we can before putting it in durable storage it doesn't mean we can't put in durable storage in fact most of",
    "start": "289810",
    "end": "296140"
  },
  {
    "text": "times you want to do both you want to put it in some kind of durable storage as it comes in but you also want to process the message in real time right",
    "start": "296140",
    "end": "303070"
  },
  {
    "text": "so we're gonna use lambda here as well we're getting some kind of logic and then we're in a trigger and action and generally when we're talking real-time",
    "start": "303070",
    "end": "310090"
  },
  {
    "text": "data pipelines we are talking in the order of zero seconds so basically near-instant all the way through to 15",
    "start": "310090",
    "end": "317500"
  },
  {
    "text": "minutes depending on your background you know real-time has a different definition for everyone for some folks",
    "start": "317500",
    "end": "323860"
  },
  {
    "text": "real like real time is microseconds for other folks that means a few minutes so I like to use the term near real time",
    "start": "323860",
    "end": "330270"
  },
  {
    "text": "for when I talk about like real time data processing because it takes a",
    "start": "330270",
    "end": "335500"
  },
  {
    "text": "little while for events to trickle down it might take about you know 30 seconds for you to react to some event that's",
    "start": "335500",
    "end": "340600"
  },
  {
    "text": "still considered real time or near real time as I as I like to call it so at the",
    "start": "340600",
    "end": "346540"
  },
  {
    "start": "345000",
    "end": "345000"
  },
  {
    "text": "center of all of this compute that we were just talking about is AWS lambda and most of you're probably very",
    "start": "346540",
    "end": "352120"
  },
  {
    "text": "familiar with AWS lambda already but I want to call out some of the very important things about a device that",
    "start": "352120",
    "end": "357160"
  },
  {
    "text": "lambda that makes it so suitable for real-time data processing right so the biggest of which in my opinion really is",
    "start": "357160",
    "end": "363850"
  },
  {
    "text": "its ability to scale that's the most important thing to me because that means you don't have to manage infrastructure",
    "start": "363850",
    "end": "370030"
  },
  {
    "text": "as your data streams change in the frequency at which you receive the data",
    "start": "370030",
    "end": "375670"
  },
  {
    "text": "right if you have a cyber monday say like today for example and you're recording click streams you're gonna get",
    "start": "375670",
    "end": "382120"
  },
  {
    "text": "a lot more customers and Cyber Monday than any other day you don't wanna have to predict what that loads gonna be and you don't want to fall behind in your",
    "start": "382120",
    "end": "387670"
  },
  {
    "text": "real-time data processing especially if that real-time data processing has something to do with security because",
    "start": "387670",
    "end": "393310"
  },
  {
    "text": "then you're falling behind on detecting anomalous patterns on a very busy day like cyber monday right so that",
    "start": "393310",
    "end": "398590"
  },
  {
    "text": "that's a very good reason to use lambda and the other one really is simplicity you know like with a deal based on where",
    "start": "398590",
    "end": "404410"
  },
  {
    "text": "you are not managing hardware you're not managing like an operating system you're not patching it you're simply writing",
    "start": "404410",
    "end": "411010"
  },
  {
    "text": "your business logic and whatever language that we currently support like no Java Python and c-sharp and you're",
    "start": "411010",
    "end": "417610"
  },
  {
    "text": "just defining this little bit of business logic you have any number of lambda functions and you start",
    "start": "417610",
    "end": "422650"
  },
  {
    "text": "connecting them in intertwining them into a real-time data processing pipeline right and we're going to talk",
    "start": "422650",
    "end": "428410"
  },
  {
    "text": "about events here very very shortly which is another one of its big benefits is the fact that you can tie it to very",
    "start": "428410",
    "end": "433840"
  },
  {
    "text": "many AWS events and build this like server less real time processing",
    "start": "433840",
    "end": "438970"
  },
  {
    "text": "pipeline with very little middleware you don't have to write it it's kind of there already in AWS and it's also very",
    "start": "438970",
    "end": "445960"
  },
  {
    "text": "cost effective you're not paying per hour you're paying for how many times you execute it you know if it's if you guys are not really busy that day",
    "start": "445960",
    "end": "451479"
  },
  {
    "text": "e-commerce application then you don't pay much for it because you only invoke it a few times on cyber monday you pay a",
    "start": "451479",
    "end": "456760"
  },
  {
    "text": "little bit more because you know it's running a little bit more so the kinds",
    "start": "456760",
    "end": "462130"
  },
  {
    "start": "460000",
    "end": "460000"
  },
  {
    "text": "of data sources right that that you can process with lambda I'm gonna go through",
    "start": "462130",
    "end": "467560"
  },
  {
    "text": "a few examples of these data sources so you guys can get sort of an idea of what you can use as your data source in in",
    "start": "467560",
    "end": "474460"
  },
  {
    "text": "AWS to build a real-time data processing pipeline right so with a focus on security it could be things like for",
    "start": "474460",
    "end": "481360"
  },
  {
    "text": "example parsing your access logs on your load balancers right you could parse those as they get populated looking for",
    "start": "481360",
    "end": "487960"
  },
  {
    "text": "some kind of anomalous pattern right you could try to figure out if all of a sudden you know you mostly have customers in the US then all of a sudden",
    "start": "487960",
    "end": "494260"
  },
  {
    "text": "you have a lot of customers in China you know for a store that doesn't ship to China you know like what's going on",
    "start": "494260",
    "end": "500740"
  },
  {
    "text": "there you might want to investigate that you might want to parse your VPC flow logs for example and apply some machine",
    "start": "500740",
    "end": "508030"
  },
  {
    "text": "learning application to it you know train some kind of machine learning algorithm to figure out what's normal in",
    "start": "508030",
    "end": "513190"
  },
  {
    "text": "terms of flow within your network and then figure out when something not normal is happening in your actual VPC",
    "start": "513190",
    "end": "520810"
  },
  {
    "text": "when two instances that shouldn't be communicating are all of a sudden communicating that could be symptomatic",
    "start": "520810",
    "end": "525970"
  },
  {
    "text": "it could be a basically an indicator letting you know that a machine has been compromised and somebody's using it to",
    "start": "525970",
    "end": "531130"
  },
  {
    "text": "discover resources within your VPC right so this is the importance of early detection and real-time processing you could use it to",
    "start": "531130",
    "end": "537910"
  },
  {
    "text": "a parse the clock trail locks right so this is the big one this is the one that we're gonna focus on on the next few slides this using cloud trail events",
    "start": "537910",
    "end": "544930"
  },
  {
    "text": "processing them in real time and doing something actionable with it something significant right you could also use",
    "start": "544930",
    "end": "552820"
  },
  {
    "text": "within the cloud watch family there's something called cloud watch logs which you guys are familiar with but also",
    "start": "552820",
    "end": "557920"
  },
  {
    "text": "cloud watch events which had a lot of people are not very familiar with actually and with cloud much events you",
    "start": "557920",
    "end": "563200"
  },
  {
    "text": "can set up rules that basically say hey you know when there's a cod watch event of a cultural event of this kind it",
    "start": "563200",
    "end": "569680"
  },
  {
    "text": "matches this filter I want you to take that and throw it into a Kinesis stream and that's what we're gonna talk about",
    "start": "569680",
    "end": "575530"
  },
  {
    "text": "today is how you can use that as an event source to pull cloud trail data",
    "start": "575530",
    "end": "580750"
  },
  {
    "text": "which is going to be our actual event source and cloud watch events is going to be what takes the event source and triggers and actually inserts it into",
    "start": "580750",
    "end": "588460"
  },
  {
    "text": "Kinesis and then we build our real-time pipeline from there but additional two security applications you can also use",
    "start": "588460",
    "end": "595510"
  },
  {
    "text": "it to process that MLD be changes to tables like items get created items get deleted s3 events objects get you know",
    "start": "595510",
    "end": "602710"
  },
  {
    "text": "inserted into a bucket they get a put in the bucket or they get deleted or things like that you can trigger on that SNS",
    "start": "602710",
    "end": "608800"
  },
  {
    "text": "messages IOT and then finally Kinesis like processing messages out of a Kinesis stream right so I could go on",
    "start": "608800",
    "end": "615340"
  },
  {
    "text": "and on and on the realities there's a lot a lot of possible data sources in a Tobias that you can use with very little",
    "start": "615340",
    "end": "621550"
  },
  {
    "text": "effort right you don't have to write code to make any of the stuff that I mention happen you go to the respective",
    "start": "621550",
    "end": "627010"
  },
  {
    "text": "services and you simply connect them you say oh when an object arrives at s3 I want you to trigger this lambda function",
    "start": "627010",
    "end": "633220"
  },
  {
    "text": "and then you write your code that does something with that event to do whatever you want with it like send an email send",
    "start": "633220",
    "end": "638560"
  },
  {
    "text": "a send a text message or you could use your own application as a source to a real-time data processing pipeline you",
    "start": "638560",
    "end": "644650"
  },
  {
    "text": "can invoke the lambda function yourself from your application right so as we",
    "start": "644650",
    "end": "650050"
  },
  {
    "start": "648000",
    "end": "648000"
  },
  {
    "text": "talk about these events sources and lambda functions I need to dive a little bit deeper into how this invocation",
    "start": "650050",
    "end": "657610"
  },
  {
    "text": "happens how does Amazon s3 connect to lambda so you can do something relevant",
    "start": "657610",
    "end": "663970"
  },
  {
    "text": "right so there are two kinds of what we call models or triggering models for",
    "start": "663970",
    "end": "671410"
  },
  {
    "text": "alanda there is a push model and there's a pull model a push model is basically saying that a service is going to",
    "start": "671410",
    "end": "679089"
  },
  {
    "text": "actually invoke lambda so SNS is going to call your lambda function in the pull",
    "start": "679089",
    "end": "685089"
  },
  {
    "text": "model scenario you actually have a lambda pull the data from your event source and then trigger a lambda",
    "start": "685089",
    "end": "691089"
  },
  {
    "text": "function right so that's the difference between push and pull within your push models you have synchronous pushes and",
    "start": "691089",
    "end": "697300"
  },
  {
    "text": "asynchronous pushes asynchronous wishes are the most common those are the fire-and-forget once those are hey here's a payload lambda tribulus",
    "start": "697300",
    "end": "705009"
  },
  {
    "text": "function with this payload and just do it I don't care I'm gonna keep on like doing my own thing over here right so",
    "start": "705009",
    "end": "710529"
  },
  {
    "text": "it's not gonna keep track of that execution Arn it's not gonna wait for a result if it fails it fails right hence",
    "start": "710529",
    "end": "716319"
  },
  {
    "text": "fire-and-forget a synchronous invocation on the other hand is the opposite of",
    "start": "716319",
    "end": "721750"
  },
  {
    "text": "that you're gonna invoke it and you're gonna wait for a result right so for example Alexa right so Alexa uses lambda",
    "start": "721750",
    "end": "728529"
  },
  {
    "text": "is it's a very common to see Alexa and lambda together you don't have to use lambda to write Alexa skills but if you",
    "start": "728529",
    "end": "733899"
  },
  {
    "text": "write one then your what your lambda function is doing it's as getting that input from the user and then it's gonna",
    "start": "733899",
    "end": "740559"
  },
  {
    "text": "do some logic and figure out what the next question to ask is or what to",
    "start": "740559",
    "end": "745600"
  },
  {
    "text": "respond if right so the Alexa service has to wait for that lambda function to finish invoking to get that result back",
    "start": "745600",
    "end": "751509"
  },
  {
    "text": "it can't just fire and forget because it's waiting on it to give it something back right so that's what that",
    "start": "751509",
    "end": "756819"
  },
  {
    "text": "synchronous invocation is and in the pool model where the lambda service",
    "start": "756819",
    "end": "763059"
  },
  {
    "text": "itself is the one pulling data you tend to see there the synchronous invocation",
    "start": "763059",
    "end": "768459"
  },
  {
    "text": "model and that's what we're going to talk about today is actually the pull model with synchronous invocation using Amazon Kinesis so we'll talk a little",
    "start": "768459",
    "end": "776259"
  },
  {
    "text": "bit more in detail about about Kinesis after we talk through the pipeline that we're gonna essentially demo throughout",
    "start": "776259",
    "end": "782410"
  },
  {
    "start": "777000",
    "end": "777000"
  },
  {
    "text": "throughout throughout this presentation right so for this example for this security pipeline that we're going to",
    "start": "782410",
    "end": "788110"
  },
  {
    "text": "sort of build here as a demo we're going to use a cloud trail like I mentioned as our data source right so this is this",
    "start": "788110",
    "end": "795939"
  },
  {
    "text": "serve contains an audit trail of all the AWS API calls that have been made on your",
    "start": "795939",
    "end": "801460"
  },
  {
    "text": "AWS account right if you're not familiar with cloud trail it's it's one of those things that the first time you boot up",
    "start": "801460",
    "end": "808000"
  },
  {
    "text": "and and either this account is the first thing that you should enable is cloud trail and tell it to start dumping stuff",
    "start": "808000",
    "end": "813370"
  },
  {
    "text": "in s3 it's always good to have that audit trail because if anything happens look there right and see what happens so",
    "start": "813370",
    "end": "819430"
  },
  {
    "text": "we're gonna take cloud trail and we're gonna have it monitor all of our API calls specifically what we're looking",
    "start": "819430",
    "end": "824980"
  },
  {
    "text": "for is to see who terminated a production easy to instance right so we're gonna have these ec2 instances",
    "start": "824980",
    "end": "830980"
  },
  {
    "text": "that are going to be tagged and they're gonna be tagged whether there are dev QA or production instances really best",
    "start": "830980",
    "end": "837430"
  },
  {
    "text": "practice AWS accounts should be dead prog QA so everything in a Prada a W second should be prod but in this",
    "start": "837430",
    "end": "843640"
  },
  {
    "text": "example we're gonna assume we have a mixture of a w of like ec2 instance in different environments within the same",
    "start": "843640",
    "end": "849040"
  },
  {
    "text": "account so we have a product count and it gets terminated I want to know the moment that that happens as quickly as",
    "start": "849040",
    "end": "854410"
  },
  {
    "text": "possible I want to know who terminated and I want to know the when and I and I want to know what instance was",
    "start": "854410",
    "end": "859720"
  },
  {
    "text": "terminated right so that's what this pipeline is is going to accomplish so in the event source on the Left we have",
    "start": "859720",
    "end": "865750"
  },
  {
    "text": "Claude trail and then we're going to use copper events with the rule that I",
    "start": "865750",
    "end": "871000"
  },
  {
    "text": "mentioned where you can set up a rule and basically say hey I want you to use cloud trail as a source when there's an",
    "start": "871000",
    "end": "876520"
  },
  {
    "text": "instance terminate a vent I want you to take that message and put it inside an",
    "start": "876520",
    "end": "881589"
  },
  {
    "text": "Amazon Kinesis tree that's what I want you to do with it right so that's what caught watch events though so commerce events is sort of like the glue right",
    "start": "881589",
    "end": "888070"
  },
  {
    "text": "there between the data source and where it's going to go to next right and so",
    "start": "888070",
    "end": "893589"
  },
  {
    "text": "essentially like the like the orchestrator of the message and the event target here is going to be the",
    "start": "893589",
    "end": "898839"
  },
  {
    "text": "Kinesis stream and we're going to talk about cases in detail in the next line so and then and then from the Kinesis",
    "start": "898839",
    "end": "906370"
  },
  {
    "text": "stream I'm gonna have to separate consumers right we're gonna have a consumer that's a lambda function that's just going to be consuming data from the",
    "start": "906370",
    "end": "912459"
  },
  {
    "text": "Kinesis stream it's going to be doing that checking of them of the instance and it's going to say hey this was",
    "start": "912459",
    "end": "917860"
  },
  {
    "text": "terminated let me check if it was prod yes it was prod let me send a text message to someone that should probably",
    "start": "917860",
    "end": "923170"
  },
  {
    "text": "know about this and then we have another path another consumer of the stream that's gonna be getting the exact same",
    "start": "923170",
    "end": "928780"
  },
  {
    "text": "messages but all this consumer is gonna do is gonna take those messages and is gonna dump them into elasticsearch right",
    "start": "928780",
    "end": "936040"
  },
  {
    "text": "and with elastic search you can run kibana on top of it you can not well",
    "start": "936040",
    "end": "942310"
  },
  {
    "text": "store elastic search is not database it's really a search index but it's really good at that right it's really",
    "start": "942310",
    "end": "948250"
  },
  {
    "text": "good at combing through millions and millions of messages looking for some particular events so you can correlate",
    "start": "948250",
    "end": "953560"
  },
  {
    "text": "it in the event of some kind of security incident right so these are the two things we're gonna do with the data that's that's gonna actually come in",
    "start": "953560",
    "end": "960040"
  },
  {
    "text": "right so the first thing we need to do is set up that security that that that",
    "start": "960040",
    "end": "966250"
  },
  {
    "start": "961000",
    "end": "961000"
  },
  {
    "text": "actual event and Claude watch events right so I mentioned this thing called terminate instances event with cloud",
    "start": "966250",
    "end": "973030"
  },
  {
    "text": "watch events you can set up cloud trail as your event source and then you can set up terminate instances as your event",
    "start": "973030",
    "end": "979720"
  },
  {
    "text": "to filter on right because cloud trail monitors all API actions not just terminate instances it's a trick it",
    "start": "979720",
    "end": "985810"
  },
  {
    "text": "monitors creations it monitors when kms Keys are created and monitors when KMS keys are used to decrypt messages right",
    "start": "985810",
    "end": "992980"
  },
  {
    "text": "so you could monitor everything about it but in this case we're focused on terminate instance event and once you",
    "start": "992980",
    "end": "1000150"
  },
  {
    "text": "filter on it you can just have it trigger anything that you want to as as an actual target it could be one or more",
    "start": "1000150",
    "end": "1005370"
  },
  {
    "text": "targets right so in this case for in this screenshot just as a demonstration I'm showing how you can configure two",
    "start": "1005370",
    "end": "1011160"
  },
  {
    "text": "targets one Kinesis so take the message from clock trail put it in Kinesis and the other one is",
    "start": "1011160",
    "end": "1016530"
  },
  {
    "text": "take the message from cloud trail and put it in SNS right so SNS just is your like messaging broker in case you don't want to use",
    "start": "1016530",
    "end": "1022709"
  },
  {
    "text": "Kinesis or for some other application just the demonstrate that you can't have more than one target right and then once",
    "start": "1022709",
    "end": "1029069"
  },
  {
    "text": "you've done those two things what would you essentially end up with is with that is with the glue that takes the event",
    "start": "1029069",
    "end": "1035579"
  },
  {
    "text": "source and gives it to Kinesis and gives it to SNS right so you've now have that middleware implemented by AWS all server",
    "start": "1035579",
    "end": "1042329"
  },
  {
    "text": "unless you're not running instances to do this and all at scale you don't have to worry about how many messages come in it's all taken care of by AWS so when",
    "start": "1042329",
    "end": "1050370"
  },
  {
    "text": "you look at this you know expression that basically says hey only messages",
    "start": "1050370",
    "end": "1056790"
  },
  {
    "text": "from ec2 that are terminated instance those are the ones I want you to give to Kinesis that",
    "start": "1056790",
    "end": "1062100"
  },
  {
    "text": "actually expressed in JSON the GUI doesn't make it super clear that it's actually all JSON but all their services are consumer will be a via an API so you",
    "start": "1062100",
    "end": "1069150"
  },
  {
    "text": "can automate this via an API and this is what the event source pattern looks like right when you've expressed it in JSON",
    "start": "1069150",
    "end": "1075780"
  },
  {
    "text": "so it's a very simple event source and you can filter by anything including a wild-card so you can just get every",
    "start": "1075780",
    "end": "1082559"
  },
  {
    "text": "single cloud trail message as its generated immediately sent to something like Kinesis right it generally takes",
    "start": "1082559",
    "end": "1089640"
  },
  {
    "start": "1088000",
    "end": "1088000"
  },
  {
    "text": "about one to two minutes from the moment a termination happens to the moment that that that audit trail event triggers and",
    "start": "1089640",
    "end": "1096870"
  },
  {
    "text": "this is what that message looks like right before we can do anything with this message we gotta know what it looks like you know what's in it do we have",
    "start": "1096870",
    "end": "1102720"
  },
  {
    "text": "all the information we need inside that message right there's a lot of information in there specifically we",
    "start": "1102720",
    "end": "1108480"
  },
  {
    "text": "have the most important things in there we have the I am user that terminated the instance inside there we have which",
    "start": "1108480",
    "end": "1114059"
  },
  {
    "text": "instance was terminated we have the IP address of the user at the time that the instance was terminated and we have the",
    "start": "1114059",
    "end": "1121530"
  },
  {
    "text": "idea of the instance that was terminated right so we have all the information that we originally wanted is contained",
    "start": "1121530",
    "end": "1126900"
  },
  {
    "text": "inside this message except for one thing we don't know if it's a prod instance a",
    "start": "1126900",
    "end": "1132750"
  },
  {
    "text": "QA instance a dev instance because it doesn't have the tags for the instance in here so we're gonna have to query",
    "start": "1132750",
    "end": "1138690"
  },
  {
    "text": "that separately inside our lambda function right this is what the lambda function looks like this is what we're",
    "start": "1138690",
    "end": "1144750"
  },
  {
    "text": "invoking right so once we send that message into Kinesis and lambda and consumes messages from Kinesis this is",
    "start": "1144750",
    "end": "1152070"
  },
  {
    "text": "what it's going to do it's going to go through all the messages that come in and it's going to parse the Kinesis",
    "start": "1152070",
    "end": "1158370"
  },
  {
    "text": "record it's gonna then extract the actual like username that invoked the",
    "start": "1158370",
    "end": "1164039"
  },
  {
    "text": "termination and it's going to extract the instance ID and then we're gonna make a call with boto that we import",
    "start": "1164039",
    "end": "1170250"
  },
  {
    "text": "above to the ec2 service to query the the actual like metadata tags that are",
    "start": "1170250",
    "end": "1177270"
  },
  {
    "text": "associated to the instance and we're going to check to see if it has a production flag that's what tells us",
    "start": "1177270",
    "end": "1182280"
  },
  {
    "text": "this instance is production and if that is true if it is a production instance then we want to send an SMS and we want",
    "start": "1182280",
    "end": "1189090"
  },
  {
    "text": "to notify an administrator for example that the instance has been terminated you don't have to use",
    "start": "1189090",
    "end": "1195010"
  },
  {
    "text": "sms I'm just using SMS cos is a simplest thing to do with us and that's here but you could you know like integrate with",
    "start": "1195010",
    "end": "1201250"
  },
  {
    "text": "page duty you could integrate with pusher with any other kind of monitoring service that you have you can do more",
    "start": "1201250",
    "end": "1207370"
  },
  {
    "text": "than one action as well right so there's definitely quite a few things that you can do there so once you've",
    "start": "1207370",
    "end": "1213400"
  },
  {
    "text": "parsed the message and done your triggering then you finally will get your text message right so if we were to",
    "start": "1213400",
    "end": "1219550"
  },
  {
    "text": "terminate an ec2 instance you know which for this example I did as my username in my account I will get a text message",
    "start": "1219550",
    "end": "1226150"
  },
  {
    "text": "that looks like that right the lambda code that I showed you the little bit of snippet of code I showed you will do",
    "start": "1226150",
    "end": "1231160"
  },
  {
    "text": "that will parse that message trigger on that message and send you that message letting you know that that instance hasn't been terminated right so this is",
    "start": "1231160",
    "end": "1237460"
  },
  {
    "start": "1237000",
    "end": "1237000"
  },
  {
    "text": "a very simplistic look at what you could do with security the reality is you're",
    "start": "1237460",
    "end": "1244360"
  },
  {
    "text": "gonna probably do something a little bit more complex than this sometimes something this simple is very useful to",
    "start": "1244360",
    "end": "1249730"
  },
  {
    "text": "get started with but you probably are gonna do something a little bit more complicated with it you know you might even look into some machine learning",
    "start": "1249730",
    "end": "1255670"
  },
  {
    "text": "applications that some of our partners have available you might want to try to roll something yourself you know to try",
    "start": "1255670",
    "end": "1261580"
  },
  {
    "text": "to figure out you know what an anomalous pattern looks like for example if you",
    "start": "1261580",
    "end": "1266920"
  },
  {
    "text": "have you know someone that successfully logged into your AWS account from a country that you have no employees in",
    "start": "1266920",
    "end": "1273430"
  },
  {
    "text": "like that should probably raise a red flag right and you can detect that with",
    "start": "1273430",
    "end": "1279030"
  },
  {
    "text": "cloud trail and this pipeline that I mentioned because you have the IP address of the user and you know when",
    "start": "1279030",
    "end": "1284200"
  },
  {
    "text": "they logged in right so you can figure all that stuff out there's a lot of things that you can do there so at the",
    "start": "1284200",
    "end": "1289930"
  },
  {
    "text": "center of all there's something I have not talked about very much is Kinesis like why did I choose Kinesis why do I",
    "start": "1289930",
    "end": "1296440"
  },
  {
    "text": "not just invoke lamda directly from from the cod watch event which you can by the",
    "start": "1296440",
    "end": "1302530"
  },
  {
    "text": "way and completely skip Kinesis the reason I put Kinesis in there is because Kinesis is pretty much at the heart of",
    "start": "1302530",
    "end": "1308710"
  },
  {
    "text": "every real-time data processing solution that I've seen out there from all of our customers from all of our partners and",
    "start": "1308710",
    "end": "1314940"
  },
  {
    "text": "the reason for it is because of its versatility at managing and distributing",
    "start": "1314940",
    "end": "1320560"
  },
  {
    "text": "messages intended for real time consumption so it does some things that are very very useful right so to explain",
    "start": "1320560",
    "end": "1326950"
  },
  {
    "text": "Kinesis I like to tell people that it's like a timeline right you have you have this timeline that by",
    "start": "1326950",
    "end": "1332710"
  },
  {
    "text": "default is going to be a period of 24 hours and but this timeline could be as long as 7 days and when a record is",
    "start": "1332710",
    "end": "1339670"
  },
  {
    "text": "inserted into a Kinesis stream you're essentially inserting it in one end of",
    "start": "1339670",
    "end": "1344830"
  },
  {
    "text": "the timeline and the moment it gets inserted it gets time-stamped so it gets a time on it right it was inserted today",
    "start": "1344830",
    "end": "1350440"
  },
  {
    "text": "at 10:00 in the morning and then as time passes that actual record is gonna age",
    "start": "1350440",
    "end": "1355720"
  },
  {
    "text": "its gonna get older and older and older right two hours later it's gonna be two hours old so the moment that it ages all",
    "start": "1355720",
    "end": "1363100"
  },
  {
    "text": "the way to the other end of this timeline of the timeless 24 hours the moment it ages all the way to the end of",
    "start": "1363100",
    "end": "1369160"
  },
  {
    "text": "the timeline it essentially reaches what's called the trim horizon that's a very fancy term for a cliff where the",
    "start": "1369160",
    "end": "1374620"
  },
  {
    "text": "message Falls often dies like that's where it's too old it expires essentially and that's called the trim",
    "start": "1374620",
    "end": "1380770"
  },
  {
    "text": "horizon that is that that represents your oldest oldest oldest message and is even certain more messages they all aged",
    "start": "1380770",
    "end": "1387460"
  },
  {
    "text": "together all the way through to the trim horizon right so some benefits of that is if you have a lot of messages coming",
    "start": "1387460",
    "end": "1394480"
  },
  {
    "text": "in and you're doing some kind of real-time processing of messages but you find out that there's a bug in your code",
    "start": "1394480",
    "end": "1400800"
  },
  {
    "text": "after two hours of deploying this change to production so you've had two hours worth of events that you've already",
    "start": "1400800",
    "end": "1406900"
  },
  {
    "text": "processed if you had not used the Kinesis if you had directly invoked your",
    "start": "1406900",
    "end": "1412360"
  },
  {
    "text": "lambda function in a fire-and-forget fashion then you would have lost those messages like how do you reprocess those",
    "start": "1412360",
    "end": "1417940"
  },
  {
    "text": "two hours of messages that you lost because or you didn't process correct because of a bug in the code right with",
    "start": "1417940",
    "end": "1424480"
  },
  {
    "text": "Kinesis you have the ability to set your cursor your your your actual like",
    "start": "1424480",
    "end": "1430210"
  },
  {
    "text": "position within the Kinesis stream anywhere you want to you can put it all the way at the end of the trim horizon",
    "start": "1430210",
    "end": "1435880"
  },
  {
    "text": "and reprocess every message if the duration of your Kinesis stream is seven",
    "start": "1435880",
    "end": "1441040"
  },
  {
    "text": "days you've got seven days worth of messages just sitting in there you know as new ones come in and old ones come",
    "start": "1441040",
    "end": "1446680"
  },
  {
    "text": "out you have a seven day window essentially always of all your messages that are coming in so that makes it very",
    "start": "1446680",
    "end": "1452680"
  },
  {
    "text": "very very versatile for that kind of processing and you also have the ability to scale this you can take this Kinesis",
    "start": "1452680",
    "end": "1459610"
  },
  {
    "text": "stream and add were called shards to it and it's very easy you just duplicate the number shards to go from 1 to 2 to 4",
    "start": "1459610",
    "end": "1466540"
  },
  {
    "text": "and the more shards you put on it the more capacity it has to insert messages and to read messages from it right and",
    "start": "1466540",
    "end": "1473290"
  },
  {
    "text": "it's very easy to use it's server less you don't have to spin up a compute instance you don't have to manage",
    "start": "1473290",
    "end": "1478600"
  },
  {
    "text": "patches or anything like that so it's a very popular choice for real-time data processing for processing real-time",
    "start": "1478600",
    "end": "1484330"
  },
  {
    "text": "messages of any kind so what I talked about specifically is called the Kinesis stream it's part of the Kinesis family",
    "start": "1484330",
    "end": "1490120"
  },
  {
    "text": "and within the cases family there are two other services there's fire hosts and Kinesis analytics as well we're not",
    "start": "1490120",
    "end": "1496240"
  },
  {
    "text": "going to really dive too deep into those two services but I just want to talk about them really quickly so cases fire",
    "start": "1496240",
    "end": "1502150"
  },
  {
    "text": "host is a way of just loading a whole bunch of data very very very very quickly into some kind of data source",
    "start": "1502150",
    "end": "1507640"
  },
  {
    "text": "like for example into s3 you can open char data through Kinesis firehose into",
    "start": "1507640",
    "end": "1513160"
  },
  {
    "text": "s3 or into elasticsearch which has a direct integration with elastic search in Kinesis and analytics allows you to",
    "start": "1513160",
    "end": "1521770"
  },
  {
    "text": "write like sequel queries that get evaluated in real time against data coming into the pipeline and you can do",
    "start": "1521770",
    "end": "1528640"
  },
  {
    "text": "some really cool stuff with that actually you can do like sliding windows of time and do like sliding averages of",
    "start": "1528640",
    "end": "1534430"
  },
  {
    "text": "data that comes in like you can do some really fancy stuff with Kinesis analytics but for this focus on lambda",
    "start": "1534430",
    "end": "1541780"
  },
  {
    "text": "as our actual compute layer but I totally encourage you guys to look into those two especially if the analytic",
    "start": "1541780",
    "end": "1547720"
  },
  {
    "text": "component really really interests you guys and ultimately very important this",
    "start": "1547720",
    "end": "1553240"
  },
  {
    "text": "is real time right so you have in this case for our demo we have a lambda consumer a lambda function where the",
    "start": "1553240",
    "end": "1560680"
  },
  {
    "text": "lambda service is actually the thing consuming from Kinesis the lambda function is going to have an iterator",
    "start": "1560680",
    "end": "1566770"
  },
  {
    "text": "either set to latest or to trim horizon if it's set the latest that is just gonna get any message that was received",
    "start": "1566770",
    "end": "1572770"
  },
  {
    "text": "after it was configured if it set the trim horizon it's gonna process everything from the beginning of time",
    "start": "1572770",
    "end": "1577920"
  },
  {
    "text": "for the duration of you know if it feels 24 hours then it's back as far back as",
    "start": "1577920",
    "end": "1583030"
  },
  {
    "text": "24 hours and process all that data so you can choose between those you know two different locations if you're",
    "start": "1583030",
    "end": "1589450"
  },
  {
    "text": "writing your own consumer and ec2 or you're writing it to run locally on your machine and Python",
    "start": "1589450",
    "end": "1595140"
  },
  {
    "text": "you use what's called the Kinesis client library and you can set your iterator wherever you want to it doesn't have to",
    "start": "1595140",
    "end": "1600630"
  },
  {
    "text": "be at the end it doesn't have to be the beginning it could be in the middle you could process an hour's worth of data pause report where you left off continue",
    "start": "1600630",
    "end": "1607350"
  },
  {
    "text": "later and and more importantly you can have multiple consumers so you can have",
    "start": "1607350",
    "end": "1612480"
  },
  {
    "text": "one producer inserting a lot of messages into the stream and multiple consumers consuming messages from the stream but",
    "start": "1612480",
    "end": "1618750"
  },
  {
    "start": "1617000",
    "end": "1617000"
  },
  {
    "text": "there are limits obviously right like you scale this with shards so when you look at how many shards you need you",
    "start": "1618750",
    "end": "1625140"
  },
  {
    "text": "have to think about the fact that each shard will give you the ability to - -",
    "start": "1625140",
    "end": "1630180"
  },
  {
    "text": "to actually like ingest at a rate of 1 megabyte per second or right at Omega at a rate of 2 megabytes per second and",
    "start": "1630180",
    "end": "1636690"
  },
  {
    "text": "this actually from the other perspective so what really happens is you can write into the kenya stream at 1 megabyte per",
    "start": "1636690",
    "end": "1642720"
  },
  {
    "text": "second so the cod watch events could write that fast into a single shard and you can have consumers read at 2",
    "start": "1642720",
    "end": "1649200"
  },
  {
    "text": "megabytes per second so what that means is if you're writing at 1 megabyte per second you probably want to read at 1 megabyte per second otherwise you fall",
    "start": "1649200",
    "end": "1654840"
  },
  {
    "text": "behind right if it's real time you got to read as fast as you write which means we can have up to 2 consumers reading in",
    "start": "1654840",
    "end": "1660510"
  },
  {
    "text": "real time from this from this one shard but if you need more capacity you increase the number of shards that's the",
    "start": "1660510",
    "end": "1666780"
  },
  {
    "text": "beauty of Kinesis stream for scaling you can just set it to 10 and then you can do 10 and 20 megabytes per second",
    "start": "1666780",
    "end": "1672810"
  },
  {
    "text": "respectively each shard is going to support 5 read transactions per second that's not 5 messages read is 5 read",
    "start": "1672810",
    "end": "1679770"
  },
  {
    "text": "transactions and 1 read transaction can read a whole bunch of messages in bulk it's limited by by actually size in",
    "start": "1679770",
    "end": "1685770"
  },
  {
    "text": "megabytes and you can configure like I said the duration could be 24 hours could be 7 days for the duration of the",
    "start": "1685770",
    "end": "1692340"
  },
  {
    "text": "entire like stream itself and in best practice when you're deciding like how",
    "start": "1692340",
    "end": "1698610"
  },
  {
    "text": "much you need to scale it start start with one shard and monitor to see how it really works",
    "start": "1698610",
    "end": "1704040"
  },
  {
    "text": "you can use there's a little link in the console that lets you figure out how many shards you need it'll it'll ask you",
    "start": "1704040",
    "end": "1709710"
  },
  {
    "text": "questions about the kind of data you're getting and how fast the data comes in and how fast you need to read it and it's gonna compute how many shards you",
    "start": "1709710",
    "end": "1715770"
  },
  {
    "text": "need the formula it's gonna use is this one that I have listed down there it looks really complicated but really you",
    "start": "1715770",
    "end": "1722850"
  },
  {
    "text": "just need to think about how quick them at the the messages are coming in if messages are coming in at 10 megabytes per second",
    "start": "1722850",
    "end": "1728340"
  },
  {
    "text": "you need 10 shards if they're coming in at five megabytes per second you need five shards right that's really what it",
    "start": "1728340",
    "end": "1733919"
  },
  {
    "text": "boils down to and if you have a lot of consumers more than you have producers",
    "start": "1733919",
    "end": "1739289"
  },
  {
    "text": "then you might have to increase the number of shards to keep up with the consumers as opposed to the producers so",
    "start": "1739289",
    "end": "1745529"
  },
  {
    "start": "1745000",
    "end": "1745000"
  },
  {
    "text": "you have to balance those two things out but it's fairly simple because those are the only two things you have to think about right and when you create the",
    "start": "1745529",
    "end": "1752159"
  },
  {
    "text": "lambda function which is this next part right we had the Kinesis stream that was getting the messages and then we have the lambda function that is invoking",
    "start": "1752159",
    "end": "1759090"
  },
  {
    "text": "based on messages coming into the keaney's the stream when you think of the lambda function there are some tuning parameters there as well you tune",
    "start": "1759090",
    "end": "1766169"
  },
  {
    "text": "memory and by tuning the memory you give a lambda function you actually also tune its CPU compute capability right so",
    "start": "1766169",
    "end": "1773490"
  },
  {
    "text": "those two things are actually tied together the more memory you give a lambda function the more compute capacity it also has and the best way to",
    "start": "1773490",
    "end": "1781230"
  },
  {
    "text": "figure out what you need is by testing and benchmarking for pretty much 99.99%",
    "start": "1781230",
    "end": "1786600"
  },
  {
    "text": "of applications that's the way you do it for other applications you do it because you know how much RAM you really need",
    "start": "1786600",
    "end": "1791880"
  },
  {
    "text": "because you might be doing something in memory but for most applications you pick something that is cost-effective for you so start with the lowest",
    "start": "1791880",
    "end": "1798840"
  },
  {
    "text": "possible amount of memory see if it works for your application and if it does keep it if it if if it doesn't",
    "start": "1798840",
    "end": "1804360"
  },
  {
    "text": "scale-up benchmark try again right that's the way that you figure this out that's the way that you really get",
    "start": "1804360",
    "end": "1809580"
  },
  {
    "text": "something that's truly suited to your workload and the other thing to think about aside from memory for a lambda",
    "start": "1809580",
    "end": "1815909"
  },
  {
    "text": "function is the time out for a lambda function lambda functions by default you have to specify a time out they can't",
    "start": "1815909",
    "end": "1821730"
  },
  {
    "text": "run forever right so you consider a time out of 30 seconds you can set a time out of a minute you can sit a time out of two minutes when you're dealing with",
    "start": "1821730",
    "end": "1828570"
  },
  {
    "text": "real-time data processing applications try to aim for a low time out whatever",
    "start": "1828570",
    "end": "1834000"
  },
  {
    "text": "you do inside that lambda function think about the fact that you do not want to be doing it for a long period of time you know don't try to make like you know",
    "start": "1834000",
    "end": "1842730"
  },
  {
    "text": "40 relational didn't like database cost of three databases one of which is on-prem over VPN like you can do that",
    "start": "1842730",
    "end": "1850610"
  },
  {
    "text": "but that's going to result in your lambda function running for a very long period of time and it's not going to scale very well and if you think about",
    "start": "1850610",
    "end": "1857460"
  },
  {
    "text": "it your compute layer is in the middle of your of your real time data Crossing pipeline so what happens when",
    "start": "1857460",
    "end": "1863670"
  },
  {
    "text": "your computer is slow it can't keep up with the data and if it can't keep up with the data you're gonna fall behind",
    "start": "1863670",
    "end": "1868950"
  },
  {
    "text": "and if you fall behind continuously continuously continuously once messages start reaching that that trim horizon",
    "start": "1868950",
    "end": "1875130"
  },
  {
    "text": "there's nothing you can do about it like anymore at that point you are losing messages that you can't get back right",
    "start": "1875130",
    "end": "1881070"
  },
  {
    "text": "so you want to make sure that you can process messages fast enough and we're going to look into what happens when you",
    "start": "1881070",
    "end": "1886740"
  },
  {
    "text": "can't process fast enough and and and I'm gonna show you some charts of what it looks like and what you do to remedy",
    "start": "1886740",
    "end": "1892530"
  },
  {
    "text": "that right because there are things you can do to scale and sharp the key of all of it is those Kinesis charts is what's",
    "start": "1892530",
    "end": "1899640"
  },
  {
    "text": "going to be the the thing we're gonna use to solve all these problems also lambda functions have a built-in retry",
    "start": "1899640",
    "end": "1907410"
  },
  {
    "text": "mechanism for some data sources specifically for Kinesis when lambda",
    "start": "1907410",
    "end": "1912570"
  },
  {
    "text": "functions when the lambda service is the one that subscribes to the Kinesis stream as the consumer to to actually",
    "start": "1912570",
    "end": "1920400"
  },
  {
    "text": "consume messages and then invoke a lambda function it will retry in the event the lambda function fails right",
    "start": "1920400",
    "end": "1926820"
  },
  {
    "text": "because the guaranty that lambda makes to you when it uses Kinesis as an event source is that it's gonna process",
    "start": "1926820",
    "end": "1933150"
  },
  {
    "text": "messages in order within that shard it's not gonna process a message that's ten",
    "start": "1933150",
    "end": "1938160"
  },
  {
    "text": "minutes older or rather 10 minutes newer before it processes like a message",
    "start": "1938160",
    "end": "1943170"
  },
  {
    "text": "that's ten minutes older it's not gonna do that if for some reason it gets to a message it can't process because there's",
    "start": "1943170",
    "end": "1948900"
  },
  {
    "text": "a bug in the code processing that particular payload or because of something that's more ephemeral like you",
    "start": "1948900",
    "end": "1955080"
  },
  {
    "text": "know a time out reaching a database because of some network blip of some kind then the lambda function is going to retry the execution the lambda",
    "start": "1955080",
    "end": "1961920"
  },
  {
    "text": "service is going to retry the execution of the lambda function and it's gonna do so with exponential back-off just like",
    "start": "1961920",
    "end": "1967410"
  },
  {
    "text": "the AWS sdk does by convention if you guys are familiar with it and have used it so the first time it retries the",
    "start": "1967410",
    "end": "1973200"
  },
  {
    "text": "second times gonna be like double that the third time is gonna be double that all the way to 60 seconds is actually",
    "start": "1973200",
    "end": "1978390"
  },
  {
    "text": "how long it's gonna it's it's gonna retry and then it's gonna be try every 60 seconds forever until that message no",
    "start": "1978390",
    "end": "1985830"
  },
  {
    "text": "longer exists right so you want to monitor your pipeline to figure out whether it's a code bug that's kind of",
    "start": "1985830",
    "end": "1992280"
  },
  {
    "start": "1989000",
    "end": "1989000"
  },
  {
    "text": "continuously fail or it was a network blip that is going to just recover from automatically and you don't have to do anything right so when you",
    "start": "1992280",
    "end": "2000320"
  },
  {
    "text": "have your Kinesis stream and you have your lambda function the thing that makes the lambda function consume the konista stream is this thing called the",
    "start": "2000320",
    "end": "2006890"
  },
  {
    "text": "event source or the event trigger and you actually do this within the lambda service because like I said in the case",
    "start": "2006890",
    "end": "2013310"
  },
  {
    "text": "of Kinesis lambda is actually doing a pull model where it is the consumer and",
    "start": "2013310",
    "end": "2018650"
  },
  {
    "text": "it is subscribing to this can use a string to get messages from the Kenney's stream and when you do so you can spend",
    "start": "2018650",
    "end": "2025310"
  },
  {
    "text": "what's called a batch size so you can basically say look I don't want my lambda function to process more than 128",
    "start": "2025310",
    "end": "2031580"
  },
  {
    "text": "messages at a time so you set a batch size of 128 now there is no guarantee you're gonna get 128 you might get two",
    "start": "2031580",
    "end": "2037940"
  },
  {
    "text": "messages you might get one message but you're gonna always get somewhere between 1 and 128 but no more than 128",
    "start": "2037940",
    "end": "2044480"
  },
  {
    "text": "and that's important because if you benchmark your application and you know exactly how long it takes to run then you can set your timeout appropriately",
    "start": "2044480",
    "end": "2050840"
  },
  {
    "text": "right you know what the latencies are everywhere so you can set the timeout to something reasonable and you can rely on",
    "start": "2050840",
    "end": "2056810"
  },
  {
    "text": "that constant performance now behind the scenes though the land of service is",
    "start": "2056810",
    "end": "2062240"
  },
  {
    "text": "actually a little bit smarter and will consume messages from your Kinesis stream in batches larger than the batch",
    "start": "2062240",
    "end": "2068840"
  },
  {
    "text": "that you selected but it's gonna keep it in memory before serving it to your to your next lambda function this is all",
    "start": "2068840",
    "end": "2074990"
  },
  {
    "text": "behind the scenes implementations of how lambda functions work but it does so for performance to try to get that message",
    "start": "2074990",
    "end": "2081020"
  },
  {
    "text": "to you as quickly as possible and ultimately what decides what the batch",
    "start": "2081020",
    "end": "2086179"
  },
  {
    "text": "size is going to be right if you set it to 150 you could get 100 messages and what decides how many messages you get",
    "start": "2086179",
    "end": "2092480"
  },
  {
    "text": "is how many messages it could pull from the screen in 250 milliseconds that's actually the condition it uses if it got",
    "start": "2092480",
    "end": "2098810"
  },
  {
    "text": "10 messages at time 0 and then in 100 milliseconds I got 10 more messages and then at 250 I got 10 more messages then",
    "start": "2098810",
    "end": "2105170"
  },
  {
    "text": "you're gonna get 30 messages instead of 125 or 150 or however many you could you configured for your actual batch size",
    "start": "2105170",
    "end": "2111620"
  },
  {
    "text": "and that's just the way that the consumer works that's how the lambda function works right so and then like I",
    "start": "2111620",
    "end": "2118490"
  },
  {
    "text": "mentioned you can choose to start at the trim horizon which is your oldest oldest oldest message which is then gonna",
    "start": "2118490",
    "end": "2124190"
  },
  {
    "text": "process the entire stream all over all the way to the end or to the front sorry in this case or you can make it start at the latest",
    "start": "2124190",
    "end": "2130550"
  },
  {
    "text": "mark which is the very latest message which basically means the moment you configure the event at that point you",
    "start": "2130550",
    "end": "2136250"
  },
  {
    "text": "start processing messages that arrive right so when you put it all together this is what you really end up with you",
    "start": "2136250",
    "end": "2143420"
  },
  {
    "text": "end up with can you assume on the left on the function all the way on the right you end up with the lambda service in",
    "start": "2143420",
    "end": "2149420"
  },
  {
    "text": "the middle that's doing the consumption of the messages from the Kinesis stream and creating a batch of those messages",
    "start": "2149420",
    "end": "2155780"
  },
  {
    "text": "no larger than the one that you decided that you wanted and then it's synchronously invoking this lambda",
    "start": "2155780",
    "end": "2160940"
  },
  {
    "text": "function and it's synchronous because it wants to make sure it finishes before it gives it the next batch right this is",
    "start": "2160940",
    "end": "2167300"
  },
  {
    "text": "the whole guaranteeing order thing it cannot fire-and-forget because how does it know the last batch was processed it doesn't it has to wait",
    "start": "2167300",
    "end": "2173570"
  },
  {
    "text": "so that you get all the messages in order to process them in order right and then and there's that little pulling",
    "start": "2173570",
    "end": "2179270"
  },
  {
    "text": "loop there that's 250 milliseconds max that's just under the covers implementation details of how the Landis",
    "start": "2179270",
    "end": "2185960"
  },
  {
    "text": "service works when consuming from the from the Kinesis stream so so how do you",
    "start": "2185960",
    "end": "2191810"
  },
  {
    "text": "tune this why are shards so relevant to this like what happens when you have one shard versus four versus eight like how",
    "start": "2191810",
    "end": "2197840"
  },
  {
    "text": "does lamb to handle it how do you scale your compute like if whatever you're doing is very CPU intensive how do you",
    "start": "2197840",
    "end": "2204740"
  },
  {
    "text": "increase the amount of lambda functions that you invoke in parallel the process to be able to process more messages the",
    "start": "2204740",
    "end": "2211790"
  },
  {
    "text": "secret to this and it's not really secret but it's a strategy that's called is the shards right every shard that you",
    "start": "2211790",
    "end": "2218990"
  },
  {
    "text": "have maps to one concurrent invocation of a lambda function so if you have four",
    "start": "2218990",
    "end": "2224450"
  },
  {
    "text": "shards in your Kinesis stream when lenda when when lambda subscribes to the Kinesis dream to process the messages",
    "start": "2224450",
    "end": "2231080"
  },
  {
    "text": "that are coming in it's gonna have four lambda functions concurrently in other words at the same time invoking and",
    "start": "2231080",
    "end": "2238490"
  },
  {
    "text": "processing each one of those shards so each one of those lambda functions is going to be in charge of one shard and in Kinesis world what this what",
    "start": "2238490",
    "end": "2246859"
  },
  {
    "text": "determines what shard a message goes into is some kind of key and it's just",
    "start": "2246859",
    "end": "2252380"
  },
  {
    "text": "gonna hash it it's basically using a partition scheme very very similar to what you find in databases and like",
    "start": "2252380",
    "end": "2259340"
  },
  {
    "text": "relational databases when you partition on a key so you can distribute horizontally right it's",
    "start": "2259340",
    "end": "2264630"
  },
  {
    "text": "gonna look at something like an ID something that is that has a lot of entropy to it that changes often like",
    "start": "2264630",
    "end": "2270390"
  },
  {
    "text": "some kind of unique identifier like a UUID for example right so it's using",
    "start": "2270390",
    "end": "2275490"
  },
  {
    "text": "that hashing it and coming up with a number between 1 and however many shards you have and it's putting it in that",
    "start": "2275490",
    "end": "2280530"
  },
  {
    "text": "respective chart so if you have something that has a lot of entropy and your messages like a key like a UUID for",
    "start": "2280530",
    "end": "2286500"
  },
  {
    "text": "example like some kind of unique idea that you generated then it's gonna spread all those messages evenly over",
    "start": "2286500",
    "end": "2292109"
  },
  {
    "text": "all your shards right so if you have eight shards sixteen shards thirty-three shards it's gonna spread them evenly over all the shards so you might not",
    "start": "2292109",
    "end": "2300300"
  },
  {
    "text": "have a lot of messages coming in but if your compute is very intense and you want to keep up you increase the number",
    "start": "2300300",
    "end": "2305700"
  },
  {
    "text": "of shards because that means the number of lambda functions that you have running concurrently goes up with the",
    "start": "2305700",
    "end": "2311430"
  },
  {
    "text": "number of shards so you can lower your batch size from let's say 150 to 75 and",
    "start": "2311430",
    "end": "2316829"
  },
  {
    "text": "double the amount of shards and that means you have more compute capacity",
    "start": "2316829",
    "end": "2322010"
  },
  {
    "text": "because you're computing in parallel now right so the func the formulas that have",
    "start": "2322010",
    "end": "2327599"
  },
  {
    "text": "listed down there are just forms that you can use to calculate what your maximum theoretical throughput sar going to be by increasing the number of shards",
    "start": "2327599",
    "end": "2334650"
  },
  {
    "text": "right so I so I mentioned that you can read at a rate of up to two megabytes",
    "start": "2334650",
    "end": "2340290"
  },
  {
    "text": "per second from a Kinesis shard and I mentioned that that every lambda",
    "start": "2340290",
    "end": "2346619"
  },
  {
    "text": "function that exists concurrently goes up with a number of shards so you basically multiply the number of shards",
    "start": "2346619",
    "end": "2352170"
  },
  {
    "text": "by two megabytes per second and divided by how long it takes your lambda function to run right so if it takes a",
    "start": "2352170",
    "end": "2358020"
  },
  {
    "text": "second to run then you can do like you know two shots in two times two makers per second for megabytes per second or",
    "start": "2358020",
    "end": "2363599"
  },
  {
    "text": "theoretical throughput right that's how it's going to work and then below is the effective throughput calculation I'm not",
    "start": "2363599",
    "end": "2370319"
  },
  {
    "text": "gonna really bore you guys with all the math like behind all this stuff I just want to show you guys for reference and this is going to be published online",
    "start": "2370319",
    "end": "2375900"
  },
  {
    "text": "it's gonna be on SlideShare so you will have access to this stuff so you can reference these formulas if you actually want to right so another thing that I've",
    "start": "2375900",
    "end": "2384510"
  },
  {
    "start": "2380000",
    "end": "2380000"
  },
  {
    "text": "talked about earlier which is retries and i'm gonna talk about it now and in more detail is actually very important",
    "start": "2384510",
    "end": "2390960"
  },
  {
    "text": "because in real world nothing works perfectly it really doesn't stuff is gonna break right like someone's going to",
    "start": "2390960",
    "end": "2396839"
  },
  {
    "text": "reduce some new kind of payload into your Kinesis stream and you're gonna go like wait what you didn't tell me about this new payload and you're not gonna",
    "start": "2396839",
    "end": "2403410"
  },
  {
    "text": "either process it or you're gonna have some kind of if statement some kind of like you know code that you've written",
    "start": "2403410",
    "end": "2409079"
  },
  {
    "text": "that's not gonna take to it kindly and it might crash it might fail so what happens when your lambda functions start",
    "start": "2409079",
    "end": "2415289"
  },
  {
    "text": "failing in the middle of a real-time data processing pipeline like do you do you lose messages what happens to you to",
    "start": "2415289",
    "end": "2420900"
  },
  {
    "text": "your throughput right so the answer to it is going back to what s mentioned originally these functions are",
    "start": "2420900",
    "end": "2426749"
  },
  {
    "text": "synchronously invoked there's a guarantee of order so if a function fails it's not going to move forward",
    "start": "2426749",
    "end": "2433499"
  },
  {
    "text": "it's not gonna process newer messages until those older messages are successfully processed which means that",
    "start": "2433499",
    "end": "2439979"
  },
  {
    "text": "you're basically halted your compute of your real-time pipeline so you're now",
    "start": "2439979",
    "end": "2445349"
  },
  {
    "text": "falling behind but we talked about Kinesis being versatile right the beauty",
    "start": "2445349",
    "end": "2450359"
  },
  {
    "text": "the benefit of what you get there now is that it's not fire-and-forget the lambda",
    "start": "2450359",
    "end": "2455640"
  },
  {
    "text": "service is going to say I can't process past this message this thing is breaking continuously it's failing I'm trying",
    "start": "2455640",
    "end": "2462029"
  },
  {
    "text": "like but it keeps on failing so you're gonna fall behind and fall behind let's say it takes you two hours to figure",
    "start": "2462029",
    "end": "2467400"
  },
  {
    "text": "this out which hopefully you have some good monitoring in place and it doesn't take two hours but let's say it does you",
    "start": "2467400",
    "end": "2472769"
  },
  {
    "text": "have time to go in there fix the problem and reprocess all the messages and catch up to real-time and just be back in",
    "start": "2472769",
    "end": "2478979"
  },
  {
    "text": "business like nothing ever happened right that's the benefit of Kinesis combined with lambda in your data",
    "start": "2478979",
    "end": "2486299"
  },
  {
    "text": "processing pipeline so it really is the heart of the pipeline so and like I",
    "start": "2486299",
    "end": "2492210"
  },
  {
    "text": "mentioned the way that the lambda function does this is with exponential back-off s' so it's gonna try it's gonna",
    "start": "2492210",
    "end": "2498539"
  },
  {
    "text": "retry I believe in it retries in like 100 milliseconds and then 200 and 400 and then 800 then 1.2 seconds and so on",
    "start": "2498539",
    "end": "2505829"
  },
  {
    "text": "so forth until it reaches 60 seconds and then it just does it every 60 seconds until it can process the message or",
    "start": "2505829",
    "end": "2511920"
  },
  {
    "text": "until the message expires at which point there's nothing you can do except try the next message because message is gone",
    "start": "2511920",
    "end": "2516930"
  },
  {
    "text": "right so but if you retry let's say that it's caused by a network blip the thing",
    "start": "2516930",
    "end": "2522900"
  },
  {
    "text": "that doesn't happen continuously it's not guaranteed to error forever until you fix it right",
    "start": "2522900",
    "end": "2529510"
  },
  {
    "text": "because you're failing your compute throughput is gonna go down which means you might start falling behind if",
    "start": "2529510",
    "end": "2535450"
  },
  {
    "text": "functions start randomly failing if you have some kind of DNS issue in your network and every other call is failing",
    "start": "2535450",
    "end": "2541660"
  },
  {
    "text": "to resolve and every other call is trying to resolve a MySQL node for example then that means every other",
    "start": "2541660",
    "end": "2547390"
  },
  {
    "text": "lambda invocation is gonna fail and if that happens your compute capacity is half you now process at half the speed",
    "start": "2547390",
    "end": "2554380"
  },
  {
    "text": "which means you can start falling behind so how do you monitor all this stuff I",
    "start": "2554380",
    "end": "2559450"
  },
  {
    "start": "2558000",
    "end": "2558000"
  },
  {
    "text": "mean like how do you can stay constantly monitor your real-time data processing",
    "start": "2559450",
    "end": "2564820"
  },
  {
    "text": "pipeline to make sure it's processing the way it's supposed to to make sure that you're keeping up with all these",
    "start": "2564820",
    "end": "2570160"
  },
  {
    "text": "messages that you're not falling behind that lambda functions are not failing right if this is the heart of your of",
    "start": "2570160",
    "end": "2575859"
  },
  {
    "text": "like some kind of workload that you have if this is being used to process like security events you want to be able to",
    "start": "2575859",
    "end": "2581890"
  },
  {
    "text": "monitor this right and to do this you use card watch which is really no big surprise here but the cool thing about",
    "start": "2581890",
    "end": "2589390"
  },
  {
    "text": "it is that we have this metric that we expose for for Kinesis streams that's",
    "start": "2589390",
    "end": "2596350"
  },
  {
    "text": "exposed actually from both the lambda consumer perspective and from Kinesis and it's called the shard iterator age",
    "start": "2596350",
    "end": "2601780"
  },
  {
    "text": "right so the iterator is basically your cursor inside this big Kinesis stream it",
    "start": "2601780",
    "end": "2608830"
  },
  {
    "text": "tells you where in the stream you are and it's expressed in time where time is",
    "start": "2608830",
    "end": "2613960"
  },
  {
    "text": "how old the cursor is if it is zero it is at the very very very beginning of",
    "start": "2613960",
    "end": "2619300"
  },
  {
    "text": "the stream it's processing everything really well it's going super fast as that number goes up it means you're",
    "start": "2619300",
    "end": "2625359"
  },
  {
    "text": "falling behind the age of that cursor or that iterator is getting older you do not want that number to get bigger and",
    "start": "2625359",
    "end": "2633040"
  },
  {
    "text": "bigger and bigger but in the real world it's gonna go up and down right like it's it's gonna be a little bit latency",
    "start": "2633040",
    "end": "2638950"
  },
  {
    "text": "here and then it's gonna catch up a little bit latency there and it's gonna catch up and you're gonna see this little spiky pattern like that right so",
    "start": "2638950",
    "end": "2644410"
  },
  {
    "text": "that little spiky pattern is a good spiky pattern as long as it doesn't go like really high right so you can",
    "start": "2644410",
    "end": "2649750"
  },
  {
    "text": "measure it in seconds you can set an alert and basically say hey look in my real-time data processing pipeline seems",
    "start": "2649750",
    "end": "2655600"
  },
  {
    "text": "to be the sharded Raiders over 30 seconds then I want to know about it right you sent an alert and alert",
    "start": "2655600",
    "end": "2661210"
  },
  {
    "text": "yourself to see what could be go that's delaying it right so that is your main main thing that you're gonna be",
    "start": "2661210",
    "end": "2667290"
  },
  {
    "text": "focusing on but you can also look at how successful lambda functions have been at invoking by looking at the land of",
    "start": "2667290",
    "end": "2673800"
  },
  {
    "text": "service right you can just look at the lambda console you can look at lambda metrics from within carwash and figure",
    "start": "2673800",
    "end": "2679860"
  },
  {
    "text": "out the number times lambda functions invoked and how many of those times it failed it is normal for lambda functions",
    "start": "2679860",
    "end": "2685320"
  },
  {
    "text": "to fail especially if you're making Network calls inside a lambda function right even when making Network cost AWS",
    "start": "2685320",
    "end": "2690780"
  },
  {
    "text": "services it could really fail always plan for failure right like everyone strives for 100% uptime that never",
    "start": "2690780",
    "end": "2697170"
  },
  {
    "text": "really does happen so you have to have a way of of managing that inevitable and",
    "start": "2697170",
    "end": "2704160"
  },
  {
    "text": "so you set some kind of you set some kind of ceiling on it you say hey I don't want more than this number of",
    "start": "2704160",
    "end": "2709440"
  },
  {
    "text": "invocation errors if you see a big spike like that picture that's right there and then we're in rotation errors that's not",
    "start": "2709440",
    "end": "2715290"
  },
  {
    "text": "good something's going on right like it's it's worthy of Investigation and generally when you see a spike in",
    "start": "2715290",
    "end": "2720540"
  },
  {
    "text": "invocation errors you will see a spike as well in the actual like shard",
    "start": "2720540",
    "end": "2727200"
  },
  {
    "text": "iterator age you will see it go up so so in order to dive into like what bad",
    "start": "2727200",
    "end": "2734670"
  },
  {
    "start": "2731000",
    "end": "2731000"
  },
  {
    "text": "looks like I actually intentionally made lambda functions fail in my account just so I could show you guys what's really",
    "start": "2734670",
    "end": "2741240"
  },
  {
    "text": "happening underneath the covers when the lambda functions fail to do this I'm using x-ray which is really freaking",
    "start": "2741240",
    "end": "2746610"
  },
  {
    "text": "cool and it's gonna look at that all lambda invocation and it's gonna tell me actually for that specific like lambda",
    "start": "2746610",
    "end": "2753690"
  },
  {
    "text": "invocation how many times it failed and how many times had retried and as you can tell there's like a little exponential curve right there that's the",
    "start": "2753690",
    "end": "2760290"
  },
  {
    "text": "exponential back-off right like it it tried like immediately and then it waited a little bit more in tried again",
    "start": "2760290",
    "end": "2766200"
  },
  {
    "text": "and so on so forth until then until the function finally succeeded in in actually executing right so and the way",
    "start": "2766200",
    "end": "2772230"
  },
  {
    "text": "that I did that in my function by the way is I just put a little random calculation in there and I said you know if this random value is less than this",
    "start": "2772230",
    "end": "2778560"
  },
  {
    "text": "then throw an exception I just intentionally synthetically made like made it fail just to show what what",
    "start": "2778560",
    "end": "2784800"
  },
  {
    "text": "would actually happen right so if my lambda functions all behaved this way where they fail the first four times",
    "start": "2784800",
    "end": "2791100"
  },
  {
    "text": "before succeeding the fifth time then my actual throughput is going to be a hunt like 1600",
    "start": "2791100",
    "end": "2796380"
  },
  {
    "text": "these six records per second in this in this setup that I have if I were to fix",
    "start": "2796380",
    "end": "2801839"
  },
  {
    "text": "that problem fix the problem that causes it to retry then there's a two point seventy two time difference right it's going to process 4545 messages per",
    "start": "2801839",
    "end": "2809369"
  },
  {
    "text": "second significantly more messages right so if I'm falling behind and I look at",
    "start": "2809369",
    "end": "2814920"
  },
  {
    "start": "2811000",
    "end": "2811000"
  },
  {
    "text": "my shard iterator age this is what's it's gonna look like it's gonna get older and older and older and older",
    "start": "2814920",
    "end": "2821279"
  },
  {
    "text": "right in this setup I have a producer that's inserting a hundred thousand records per minute into two shards but",
    "start": "2821279",
    "end": "2828690"
  },
  {
    "text": "my lambda function is only consuming at a thousand one hundred and seventy-six records per second which is about 70",
    "start": "2828690",
    "end": "2834660"
  },
  {
    "text": "thousand records per minute so I'm thirty thousand records per minute slower than what I should be processing at right I'm behind in compute Kinesis",
    "start": "2834660",
    "end": "2842009"
  },
  {
    "text": "keeping up with it with with the producer well but but but but my consumer is not keeping up so with time",
    "start": "2842009",
    "end": "2849150"
  },
  {
    "text": "you know as the minutes pass by you see that going up and up and up and up so how do I solve that right in this case",
    "start": "2849150",
    "end": "2855900"
  },
  {
    "text": "the issue was that my computer was taking longer than I wanted it to and the way I did that was also",
    "start": "2855900",
    "end": "2862230"
  },
  {
    "text": "synthetically I added asleep in there but in the real world you might be doing to see two calls to a relational",
    "start": "2862230",
    "end": "2868650"
  },
  {
    "text": "database and that might be taking some time so what so what I did here was I just doubled the number of shard so I",
    "start": "2868650",
    "end": "2875009"
  },
  {
    "text": "just said hey you know instead of having two shards I'm gonna have four shards that's all I did would just double the number of shots and all of a sudden I",
    "start": "2875009",
    "end": "2882420"
  },
  {
    "text": "have twice the throughput because like I mentioned right now instead of having two concurrent lambda invocations you have for concurrent lambda invocations",
    "start": "2882420",
    "end": "2888269"
  },
  {
    "text": "and because those messages coming in at a rate of a hundred thousand messages per second have an actual like key that",
    "start": "2888269",
    "end": "2895980"
  },
  {
    "text": "I'm that I'm passionate I am partitioning by that it actually has a lot of entropy it gets spread over all the shards and I'm essentially doubling",
    "start": "2895980",
    "end": "2902339"
  },
  {
    "text": "my capacity by doing that so now I'm keeping up with real-time so you can see that the shard iterator which is in blue",
    "start": "2902339",
    "end": "2909420"
  },
  {
    "text": "peaked at 220 mm and is now on the dive that means I'm catching up so when I'm",
    "start": "2909420",
    "end": "2916049"
  },
  {
    "text": "on the dive I'm catching up once I dive all the way down to zero it's going to go back to that spiky pattern around zero with very low numbers which is",
    "start": "2916049",
    "end": "2923130"
  },
  {
    "start": "2923000",
    "end": "2923000"
  },
  {
    "text": "exactly what you want right so that's that's really a a relatively quick",
    "start": "2923130",
    "end": "2929790"
  },
  {
    "text": "walk through just for this for one-hour session of a real-time processing pipeline with a focus on security and",
    "start": "2929790",
    "end": "2936540"
  },
  {
    "text": "then some deep dive into Kinesis and lambda which are really the heart of real-time data processing whether you're",
    "start": "2936540",
    "end": "2942300"
  },
  {
    "text": "processing for security processing for for add you know process for ad",
    "start": "2942300",
    "end": "2948060"
  },
  {
    "text": "campaigns because you're in click streams or for analytics or for market research whatever it is you're doing right Kinesis is almost always at the",
    "start": "2948060",
    "end": "2954660"
  },
  {
    "text": "heart of every single real-world production customer platform that out that like",
    "start": "2954660",
    "end": "2960720"
  },
  {
    "text": "I've worked with right so one example of this is Netflix which is a pretty common",
    "start": "2960720",
    "end": "2966060"
  },
  {
    "text": "example that comes up in a lot of these sessions but this one really is truly impressive so they so what they wanted",
    "start": "2966060",
    "end": "2973650"
  },
  {
    "text": "to be able to do is take all the log data that they were generating from all their different services and they wanted",
    "start": "2973650",
    "end": "2979080"
  },
  {
    "text": "to be able to action on it in a very quick real-time fashion to bring results",
    "start": "2979080",
    "end": "2984360"
  },
  {
    "text": "to customers quicker to be able to drive some kind of decision that they're gonna make that day or to drive some kind of",
    "start": "2984360",
    "end": "2989520"
  },
  {
    "text": "automated platform or email messaging or whatever it is they need to do that they need to do real-time as opposed to",
    "start": "2989520",
    "end": "2995460"
  },
  {
    "text": "waiting a day to do it right and to do this they actually use Amazon kinases",
    "start": "2995460",
    "end": "3000680"
  },
  {
    "text": "and they're processing terabytes of locked data every single day with Kinesis so they have a whole bunch of",
    "start": "3000680",
    "end": "3007400"
  },
  {
    "text": "producers are just dumping log data into into like Kinesis streams and then they're consuming it and they're set up",
    "start": "3007400",
    "end": "3013700"
  },
  {
    "text": "is more complex than one I showed right it wasn't just one can easy stream you can have multiple Kinesis streams you",
    "start": "3013700",
    "end": "3019520"
  },
  {
    "text": "can have each one do something different and pass on to the next stream you know your streams could look like one big",
    "start": "3019520",
    "end": "3024980"
  },
  {
    "text": "series where it could look like you know some kind of complicated web of Kinesis streams all sending from one to the",
    "start": "3024980",
    "end": "3030380"
  },
  {
    "text": "other you know where every single you know self-contained Kinesis and lambda function has it has its own function and",
    "start": "3030380",
    "end": "3035870"
  },
  {
    "text": "purpose right so this is something that Netflix did and they were very successful at doing on AWS and they",
    "start": "3035870",
    "end": "3041420"
  },
  {
    "text": "didn't have to manage infrastructure to do it which is something that they were really looking for and they also did",
    "start": "3041420",
    "end": "3046940"
  },
  {
    "text": "this for vbc flow logs so going to that security application that I mentioned they did this for a PC full locks as",
    "start": "3046940",
    "end": "3052100"
  },
  {
    "text": "well just to monitor for security and anomalous patterns and things like that right so I did add these few slides here",
    "start": "3052100",
    "end": "3060980"
  },
  {
    "start": "3056000",
    "end": "3056000"
  },
  {
    "text": "at the very end because I didn't want to leave you guys just with the idea of lambda functions and Kinesis for real-time processing on",
    "start": "3060980",
    "end": "3067059"
  },
  {
    "text": "AWS there are more services that you can consume more services that you can use to do real-time data processing on AWS",
    "start": "3067059",
    "end": "3074729"
  },
  {
    "text": "one of which is our elastic MapReduce service or EMR service which is also",
    "start": "3074729",
    "end": "3081039"
  },
  {
    "text": "versatile Big Data service lets you launch a variety of platform configurations for big data processing",
    "start": "3081039",
    "end": "3087150"
  },
  {
    "text": "my favorite of those is the Apache spark configuration and you can use that as",
    "start": "3087150",
    "end": "3093729"
  },
  {
    "text": "your compute layer as opposed to lambda you can use them together actually if you want to as well but in this example",
    "start": "3093729",
    "end": "3101109"
  },
  {
    "text": "I'm showing basically where you can just put your EMR like spark cluster right in the middle and then you spark you know",
    "start": "3101109",
    "end": "3107440"
  },
  {
    "text": "use EMR with spark so but you're gonna probably ask me what like why should I use that over lambda for example right",
    "start": "3107440",
    "end": "3114519"
  },
  {
    "text": "and there's some very good compelling reasons lambda functions is code that you write yourself right there's a lot",
    "start": "3114519",
    "end": "3121509"
  },
  {
    "text": "of applications that are very simple it's just the you know a quick if condition here a dynamo check over here",
    "start": "3121509",
    "end": "3127390"
  },
  {
    "text": "you know like maybe a query of like of like aurora over there something like",
    "start": "3127390",
    "end": "3132489"
  },
  {
    "text": "relatively simple and then you do some kind of condition and then you move on right sometimes it's that simple and",
    "start": "3132489",
    "end": "3137829"
  },
  {
    "text": "when it's that simple highly recommend using lambda that's exactly what it's for it's completely managing you'll have",
    "start": "3137829",
    "end": "3143680"
  },
  {
    "text": "to worry about it but but what if you have a more complex use case what if you have a use case where you want to do",
    "start": "3143680",
    "end": "3149319"
  },
  {
    "text": "like sliding or tumbling windows where you're getting like you know 100",
    "start": "3149319",
    "end": "3154420"
  },
  {
    "text": "messages per second from a sensor and you want to remove outliers from it because there's some bad like",
    "start": "3154420",
    "end": "3159910"
  },
  {
    "text": "temperature readings in there you want to average them before dumping them into another kenny's stream or you want to",
    "start": "3159910",
    "end": "3165369"
  },
  {
    "text": "run some machine learning against it and you want to do it and you know in sort of a real-time application to make real-time like decisions on it and",
    "start": "3165369",
    "end": "3171789"
  },
  {
    "text": "whatnot you might find EMR and apache spark to be more suitable for your",
    "start": "3171789",
    "end": "3176859"
  },
  {
    "text": "workload it has a lot of very mature library functionalities and feature sets",
    "start": "3176859",
    "end": "3182019"
  },
  {
    "text": "included in it that run exceptionally well on top of AWS and we actually have a lot of partners in the IOT space where",
    "start": "3182019",
    "end": "3189009"
  },
  {
    "text": "I work a lot with using Apache spark everyday to do things like predictive analytics and predictive maintenance",
    "start": "3189009",
    "end": "3195249"
  },
  {
    "text": "this right so they're looking at a whole bunch of sensor data that's coming in from the field in real time they're",
    "start": "3195249",
    "end": "3202730"
  },
  {
    "text": "training machine learning algorithms every night and then they're using those terrains and machine learning algorithms",
    "start": "3202730",
    "end": "3208880"
  },
  {
    "text": "to make decisions in real time every day so they're using it to figure out you know if this truck needs to go into",
    "start": "3208880",
    "end": "3214820"
  },
  {
    "text": "maintenance because it's about to fail in the next 24 hours right because it's pulling a whole bunch of sensor either",
    "start": "3214820",
    "end": "3219830"
  },
  {
    "text": "from the truck and it knows other that that the last 143 trucks that went in for service had a very similar like kind",
    "start": "3219830",
    "end": "3227210"
  },
  {
    "text": "of look to the data and behavior of the data as the current truck that that's",
    "start": "3227210",
    "end": "3232370"
  },
  {
    "text": "that's that's on the road right now right so I've worked with three IOT partners that use Apache spark in the",
    "start": "3232370",
    "end": "3237830"
  },
  {
    "text": "heart of their solution for their real-time data processing pipeline and they use it both to train their machine learning models with spark ml and then",
    "start": "3237830",
    "end": "3244670"
  },
  {
    "text": "they use it to evaluate with the Trane machine learning model you can interface",
    "start": "3244670",
    "end": "3250370"
  },
  {
    "text": "with it in different languages as well you can use Scala Java Python and there's actually a whole bunch of other",
    "start": "3250370",
    "end": "3255560"
  },
  {
    "text": "languages that you can use as well and like I mentioned we do have customers with public references that have used",
    "start": "3255560",
    "end": "3263170"
  },
  {
    "start": "3262000",
    "end": "3262000"
  },
  {
    "text": "Apache spark and EMR as part of the real-time data processing pipeline so",
    "start": "3263170",
    "end": "3268640"
  },
  {
    "text": "one example of such customers actually is Zillow Zillow actually uses a spark",
    "start": "3268640",
    "end": "3274550"
  },
  {
    "text": "ml and uses machine learning applications to actually compute theirs",
    "start": "3274550",
    "end": "3279740"
  },
  {
    "text": "estimate in real time as all the real estate conditions and metrics and time",
    "start": "3279740",
    "end": "3284930"
  },
  {
    "text": "and everything changes they have a proprietary algorithm they use obviously to calculate what the worth what what",
    "start": "3284930",
    "end": "3292520"
  },
  {
    "text": "the values and piece of real estate is right and there's tons of people on their website constantly looking at",
    "start": "3292520",
    "end": "3297680"
  },
  {
    "text": "properties wanting to know how much this cost so much that cost houses that are not for sale just just to get an idea of",
    "start": "3297680",
    "end": "3303320"
  },
  {
    "text": "what's the you know house cost in a particular market and they use a combination of Kinesis stream with spark",
    "start": "3303320",
    "end": "3311330"
  },
  {
    "text": "on Amazon to do this and those two things go very hand-in-hand and you're gonna see Kinesis almost at the heart of",
    "start": "3311330",
    "end": "3316880"
  },
  {
    "text": "every real-time data processing solution on top of AWS which is why I spent a little bit of time deep diving into it",
    "start": "3316880",
    "end": "3322820"
  },
  {
    "text": "is to leave you guys with some good knowledge on Kinesis because that's going to be 80% of your solution right",
    "start": "3322820",
    "end": "3328370"
  },
  {
    "text": "there right you can you Kinesis by inserting messages into it and consuming it with spark you can then",
    "start": "3328370",
    "end": "3334430"
  },
  {
    "text": "use spark if you want to do tumbling windows and running averages and push into another Kinesis stream that might",
    "start": "3334430",
    "end": "3340279"
  },
  {
    "text": "then have a lambda function at the other end right so that's where I mentioned you can combine these things right so",
    "start": "3340279",
    "end": "3346839"
  },
  {
    "text": "there's there's a lot of ways that you can build a real-time data processing pipeline right we looked at serverless",
    "start": "3346839",
    "end": "3353329"
  },
  {
    "text": "one with lambda and Kinesis and with a security focus by reading cloud trail logs but there's a lot more than you can",
    "start": "3353329",
    "end": "3360049"
  },
  {
    "start": "3359000",
    "end": "3359000"
  },
  {
    "text": "do like a lot more than I could possibly cover in a single one-hour session so I highly encourage you guys to look at the",
    "start": "3360049",
    "end": "3366739"
  },
  {
    "text": "these next steps right here to check out our server Alice page on alias on",
    "start": "3366739",
    "end": "3372170"
  },
  {
    "text": "amazon.com slash serverless so a lot of really good resources in there that talk about obviously the benefits of server lists you guys have probably heard it on",
    "start": "3372170",
    "end": "3378470"
  },
  {
    "text": "and on and on and also some real-world applications of it right there's nothing like seeing someone doing something real with it to",
    "start": "3378470",
    "end": "3385249"
  },
  {
    "text": "realize it's real true benefits also check out the lambda reference",
    "start": "3385249",
    "end": "3390799"
  },
  {
    "text": "architecture on github there's actually like an entire project in there that has",
    "start": "3390799",
    "end": "3395930"
  },
  {
    "text": "like some examples on how to do like real time data processing practical real time data processing with lambda and",
    "start": "3395930",
    "end": "3402920"
  },
  {
    "text": "also another one that I didn't have time to dive into which is like how you do distributed computing with lambda and",
    "start": "3402920",
    "end": "3408980"
  },
  {
    "text": "this is essentially like a spaghetti of lambda functions where a lambda function like will read message from s3 then then",
    "start": "3408980",
    "end": "3416749"
  },
  {
    "text": "actually turn them into batches and then invoke other lambda functions so you can think of it like MapReduce with lambda",
    "start": "3416749",
    "end": "3423730"
  },
  {
    "text": "where the functions like keep on calling each other and then eventually all reduce their results down to something a",
    "start": "3423730",
    "end": "3430670"
  },
  {
    "text": "cool thing about that server lists you don't have to worry about scaling it it just scales you know it goes as fast as",
    "start": "3430670",
    "end": "3436579"
  },
  {
    "text": "it possibly can go and you're gonna pay the same amount whether it took 10 hours to run or it took 10 minutes because",
    "start": "3436579",
    "end": "3441849"
  },
  {
    "text": "lambdas build and a 100 millisecond per 128 megabyte in Turville so whether you",
    "start": "3441849",
    "end": "3449359"
  },
  {
    "text": "consume all that in 10 seconds or 10 days is gonna be the same cost and also",
    "start": "3449359",
    "end": "3455210"
  },
  {
    "text": "encourage you to look a lot more into konista streams like I mentioned like probably way too many times already so I",
    "start": "3455210",
    "end": "3461329"
  },
  {
    "text": "apologize for that but cane stream is very important into that the Kinesis console is pretty powerful it lets you configure streams",
    "start": "3461329",
    "end": "3467800"
  },
  {
    "text": "to do pretty much anything that you want to there and you can also look at the",
    "start": "3467800",
    "end": "3473150"
  },
  {
    "text": "documentation on how you can consume it with your custom app like applications as well we have something called the",
    "start": "3473150",
    "end": "3478310"
  },
  {
    "text": "Kinesis client library or KCl for short as a lot of people call it that you can",
    "start": "3478310",
    "end": "3483500"
  },
  {
    "text": "use to consume from Kinesis streams with your own application so if your familiarity is with Java and you write",
    "start": "3483500",
    "end": "3490100"
  },
  {
    "text": "spring boot applications then you can use KCl within Java if you're a Python",
    "start": "3490100",
    "end": "3495350"
  },
  {
    "text": "developer you know and you're writing things in Python then you can use KCl as",
    "start": "3495350",
    "end": "3500480"
  },
  {
    "text": "well to consume messages like there's a lot of different ways that you can consume from Kinesis you don't have to",
    "start": "3500480",
    "end": "3506330"
  },
  {
    "text": "do it completely serverless from AWS it's the way I encourage you guys to look at but I totally understand it",
    "start": "3506330",
    "end": "3511820"
  },
  {
    "text": "takes a little while to get there but you can start with Kinesis consuming the way that you're familiar with and then",
    "start": "3511820",
    "end": "3517760"
  },
  {
    "text": "take the dive into server less look into how you once you understand kinases and you're very comfortable with it look at",
    "start": "3517760",
    "end": "3524090"
  },
  {
    "text": "how you can take your compute and go server less and finally read like the",
    "start": "3524090",
    "end": "3529520"
  },
  {
    "text": "the like developer guide for Kinesis there's a lot of really good information in there and lots of good information on our lambda forums and Kinesis forums and",
    "start": "3529520",
    "end": "3536690"
  },
  {
    "text": "just all over the web in general lots of good resources but anyways with that I",
    "start": "3536690",
    "end": "3542720"
  },
  {
    "text": "say thank you guys thank you very much for your time for attention hopefully I did not bore your guests too much and",
    "start": "3542720",
    "end": "3547850"
  },
  {
    "text": "didn't say Kinesis streams too many times and hope you enjoy the rest of your sessions thank you",
    "start": "3547850",
    "end": "3555400"
  }
]