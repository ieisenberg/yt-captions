[
  {
    "start": "0",
    "end": "133000"
  },
  {
    "text": "welcome uh my name is John vyi I work with a company called c3iot and I'm excited to be uh uh introducing best",
    "start": "520",
    "end": "8040"
  },
  {
    "text": "practices using big data on AWS so before I introduce Bob and ranga I just want to tell you quickly about um C3 iot",
    "start": "8040",
    "end": "15679"
  },
  {
    "text": "who we are what we do uh we've been built on Amazon uh from day one 2009",
    "start": "15679",
    "end": "21480"
  },
  {
    "text": "when we were founded and what we do is we provide a platform as a service a software platform for Big Data",
    "start": "21480",
    "end": "26960"
  },
  {
    "text": "Predictive Analytics and iot applications and helping you Leverage the power of the AWS infrastructure and",
    "start": "26960",
    "end": "33000"
  },
  {
    "text": "services to actually solve real world business problems uh not just identify",
    "start": "33000",
    "end": "38120"
  },
  {
    "text": "insights using machine learning but also operationalize those insights into front-end user",
    "start": "38120",
    "end": "43520"
  },
  {
    "text": "applications so the way we do that is uh we have a uh actually started selling",
    "start": "43520",
    "end": "49680"
  },
  {
    "text": "our own applications which you can see on the kind of the top left corner uh predictive maintenance fraud detection",
    "start": "49680",
    "end": "55600"
  },
  {
    "text": "sensor Health CRM um but quickly our customers started",
    "start": "55600",
    "end": "61120"
  },
  {
    "text": "wanting to use our platform services to actually build custom applications that did things like um inventory",
    "start": "61120",
    "end": "67920"
  },
  {
    "text": "optimization uh supply network risk connected home so we've uh We've exposed",
    "start": "67920",
    "end": "73960"
  },
  {
    "text": "our platform as a service which includes everything from data integration all the way through storage processing metrics",
    "start": "73960",
    "end": "80439"
  },
  {
    "text": "machine learning tools full set of apis uh application logic UI Services",
    "start": "80439",
    "end": "86200"
  },
  {
    "text": "deployment management and monitoring services so our customer can build these next Generation applications which apply",
    "start": "86200",
    "end": "92439"
  },
  {
    "text": "machine learning at scale and in order to do that we leverage a lot of the great um work that AWS does uh",
    "start": "92439",
    "end": "100159"
  },
  {
    "text": "everything from S3 uh Dynamo DB RDS red shift um to ec2 uh EMR Kinesis AWS iot",
    "start": "100159",
    "end": "108759"
  },
  {
    "text": "Green Grass and many of the other services that are in the pipeline so um I'm excited uh to be introducing Bob",
    "start": "108759",
    "end": "115759"
  },
  {
    "text": "here so Bob we're good I was supposed to take five minutes but let's just cut it cut right to the chase",
    "start": "115759",
    "end": "122520"
  },
  {
    "text": "here great thank you thanks",
    "start": "124000",
    "end": "128200"
  },
  {
    "start": "133000",
    "end": "133000"
  },
  {
    "text": "John hi everyone my name is Bob Griffith I'm a Solutions architect manager for public sector and",
    "start": "133440",
    "end": "139400"
  },
  {
    "text": "AWS uh in this session I'm going to start out by giving an overview of the AWS big um data services I'm going to go",
    "start": "139400",
    "end": "146920"
  },
  {
    "text": "relatively quickly um this is just going to be a survey of the services for awareness it's not a deep dive I'm going",
    "start": "146920",
    "end": "153760"
  },
  {
    "text": "to go relatively quickly because I wanted to hand it off over to ranga who's joining us from finra and he's",
    "start": "153760",
    "end": "158959"
  },
  {
    "text": "going to give an overview of um um how they're doing their Market regulation on",
    "start": "158959",
    "end": "164319"
  },
  {
    "text": "top of um AWS so specifically their data storage and management S3 um their use",
    "start": "164319",
    "end": "169800"
  },
  {
    "text": "of EMR and their use of Lambda for data validation so again I'm just going to go through a very quick overview we will",
    "start": "169800",
    "end": "175480"
  },
  {
    "text": "try and make time for questions after so if we can get through all the content there a lot of cont content so we we go",
    "start": "175480",
    "end": "180519"
  },
  {
    "text": "into the slides relatively quickly because I think you guys will probably be very interested in what ranga has to",
    "start": "180519",
    "end": "185560"
  },
  {
    "text": "present so we're going to give try and set aside time for uh for questions if",
    "start": "185560",
    "end": "190720"
  },
  {
    "text": "we run out of time we'll also be around we'll stand outside the room so the next session can get ready we go outside the",
    "start": "190720",
    "end": "196640"
  },
  {
    "text": "room outside the door there and we can take follow-up questions out there as well all right so let's start out with",
    "start": "196640",
    "end": "202519"
  },
  {
    "text": "just the basic question is what is Big Data there's lots of definitions for this so I like this one uh when your",
    "start": "202519",
    "end": "209040"
  },
  {
    "text": "data sets become so large and complex you have to start innovating around how to collect store process analyze and",
    "start": "209040",
    "end": "214840"
  },
  {
    "text": "share them um I like this definition because it doesn't say anything about um big data is 10 pedabytes or 500",
    "start": "214840",
    "end": "222120"
  },
  {
    "text": "gigabytes or 3 exabytes or whatever um it could be different for your business",
    "start": "222120",
    "end": "228120"
  },
  {
    "text": "um it's just whenever you get enough data that you have to start innovating and thinking about these things to",
    "start": "228120",
    "end": "234239"
  },
  {
    "text": "manage your data sets so I want us to use that as a rough um Big Data",
    "start": "234239",
    "end": "239720"
  },
  {
    "text": "definition for this all right so this is a bit of an ey",
    "start": "239720",
    "end": "246519"
  },
  {
    "start": "243000",
    "end": "243000"
  },
  {
    "text": "chart and it's going to become more of an ey chart as time goes on because we keep um innovating and adding new",
    "start": "246519",
    "end": "253239"
  },
  {
    "text": "services but there's one point I want to make I'm going to make this point a couple times throughout my part of the",
    "start": "253239",
    "end": "258400"
  },
  {
    "text": "presentation and that is don't be overwhelmed by this chart or just",
    "start": "258400",
    "end": "264479"
  },
  {
    "text": "generally speaking don't be overwhelmed when you go into the AWS Management console and you see like 90 things on that front page page uh if you're new to",
    "start": "264479",
    "end": "271039"
  },
  {
    "text": "the cloud that can be very very overwhelming I know when I first started working for AWS and I was looking at",
    "start": "271039",
    "end": "276680"
  },
  {
    "text": "that I was just taking the back by how much stuff there was to learn so you might look at this and say oh look at",
    "start": "276680",
    "end": "282080"
  },
  {
    "text": "all these Services we have to collect store and process and analyze Big Data that's a lot it's overwhelming I don't",
    "start": "282080",
    "end": "287759"
  },
  {
    "text": "want to worry about it but I want you to take away the message at least from my part of the presentation",
    "start": "287759",
    "end": "292880"
  },
  {
    "text": "is you don't have to start with all this you can start with just one or two of these core services and actually to",
    "start": "292880",
    "end": "299639"
  },
  {
    "text": "start doing foundational fundamental big data work in your organization so we provide this whole Suite of tool",
    "start": "299639",
    "end": "307199"
  },
  {
    "text": "set to give you the right tools for the right job and you can pick the right tools for the right job but start small",
    "start": "307199",
    "end": "315560"
  },
  {
    "text": "start with one or two services to get started and then enhance and add on to that so I'm not going to go through this",
    "start": "315560",
    "end": "321479"
  },
  {
    "text": "slide I'm going to um have the following slides we'll go into some of these in a little bit more detail but this is just",
    "start": "321479",
    "end": "327680"
  },
  {
    "text": "to giv you an idea of the types of tools we have have to collect the data to store the data and then to process and",
    "start": "327680",
    "end": "332960"
  },
  {
    "text": "analyze the data so again if you take nothing else away from this presentation",
    "start": "332960",
    "end": "339039"
  },
  {
    "text": "just remember that you don't have to do all of this to get started pick one",
    "start": "339039",
    "end": "344880"
  },
  {
    "text": "service add on another service and then build on top of that and you can even grow that one service that you you pick",
    "start": "344880",
    "end": "351680"
  },
  {
    "text": "um so if you don't remember anything else remember that all right so let's start talking about S3 there's a reason",
    "start": "351680",
    "end": "358440"
  },
  {
    "start": "356000",
    "end": "356000"
  },
  {
    "text": "why we're starting with this one because when you talk about big data in AWS it's pretty much going to be guaranteed to be",
    "start": "358440",
    "end": "364680"
  },
  {
    "text": "using S3 this is where you should probably put your data so in S3 you can store anything it's object storage it's",
    "start": "364680",
    "end": "370880"
  },
  {
    "text": "highly scalable it's designed for 11 nines of durability I'm not going to count the nines which should be 11 up",
    "start": "370880",
    "end": "377960"
  },
  {
    "text": "there this should be the foundation of your big data in AWS so when I say pick one or two",
    "start": "377960",
    "end": "384319"
  },
  {
    "text": "services this is one of the ones you should pick part of data collection um this is",
    "start": "384319",
    "end": "390919"
  },
  {
    "text": "one of the most popular services this is one of the very few services that we have that you can actually physically touch you know the AWS cloud is kind of",
    "start": "390919",
    "end": "397440"
  },
  {
    "text": "nebulous it's all this kind of stuff sitting out there in these far away data centers and Us East and Sydney and US",
    "start": "397440",
    "end": "403880"
  },
  {
    "text": "West and govcloud and you know it's kind of out there but you can't touch or feel it this is a very popular service",
    "start": "403880",
    "end": "409400"
  },
  {
    "text": "because it's one of the few things you can actually order and go ship to you you can actually touch it and put data",
    "start": "409400",
    "end": "414840"
  },
  {
    "text": "on it so snowball um is a service to allow [Music]",
    "start": "414840",
    "end": "420599"
  },
  {
    "text": "copying of data onto this physical device sending it to S3 and then we will offload that data into I mean sending it",
    "start": "420599",
    "end": "426520"
  },
  {
    "text": "to AWS and we'll offload that data into S3 so for very large data sets that",
    "start": "426520",
    "end": "432479"
  },
  {
    "text": "would take very long weeks months years to copy over um an internet connection",
    "start": "432479",
    "end": "438440"
  },
  {
    "text": "or even a direct connect connection U this is a very good opport um option",
    "start": "438440",
    "end": "443639"
  },
  {
    "text": "to ingest that data much more quickly you can order multiple of these load",
    "start": "443639",
    "end": "448720"
  },
  {
    "text": "them all up in the AWS and we'll load them in your S3 buckets and you're good to go it's ruggedized it has an e Inc",
    "start": "448720",
    "end": "455560"
  },
  {
    "text": "shipping level buil into it and it can do 50 or 80 terabyte versions and um 10",
    "start": "455560",
    "end": "461840"
  },
  {
    "text": "gig networking on it all right next one red shift this is our uh data warehouse",
    "start": "461840",
    "end": "469280"
  },
  {
    "text": "service it's a relational massively paralleled database it's colum or storage pedig paby scale it's fully",
    "start": "469280",
    "end": "476479"
  },
  {
    "text": "managed you can do both hard spinning disc and SSD varieties and it starts out",
    "start": "476479",
    "end": "482240"
  },
  {
    "text": "at 1,000 terabytes per year U $1,000 per terabyte per year",
    "start": "482240",
    "end": "487759"
  },
  {
    "text": "sorry so data warehousing again I'm going very quickly this is just an",
    "start": "487759",
    "end": "493199"
  },
  {
    "start": "493000",
    "end": "493000"
  },
  {
    "text": "overview uh next EMR so just one quick side note in EMR I don't like the name",
    "start": "493199",
    "end": "499560"
  },
  {
    "text": "back when this was named it stands for elastic map produce that's back when Hadoop was basically map produced there",
    "start": "499560",
    "end": "506360"
  },
  {
    "text": "wasn't else much on the platform but today Hadoop is much more than just map ruce it's spark Presto Flink hbas Hive",
    "start": "506360",
    "end": "515200"
  },
  {
    "text": "tez Uzi all kinds of things you can run on Hadoop so EMR is really a managed",
    "start": "515200",
    "end": "521518"
  },
  {
    "text": "Hadoop service so it's called elastic map produce but you can think of as managed Hadoop and anything that you can",
    "start": "521519",
    "end": "527519"
  },
  {
    "text": "run on your on-prem Hadoop clusters you're going to be able to run on EMR but the advantage with EMR is it's easy",
    "start": "527519",
    "end": "533720"
  },
  {
    "text": "to use fully managed you can script it or just click a few buttons in the Management console and get a very large",
    "start": "533720",
    "end": "539519"
  },
  {
    "text": "EMR cluster um up and running in 10 15 minutes something like that um it supports on demand and spot",
    "start": "539519",
    "end": "546519"
  },
  {
    "text": "pricing and it also supports importantly S3 as a file system of course it supports hdfs which is the standard",
    "start": "546519",
    "end": "553920"
  },
  {
    "text": "Hadoop file system but we also support S3 um access directly from EMR which",
    "start": "553920",
    "end": "561680"
  },
  {
    "text": "goes back to my point you should putting your data on S3",
    "start": "561680",
    "end": "567640"
  },
  {
    "start": "565000",
    "end": "565000"
  },
  {
    "text": "Athena Athena is seress semi-structured unstructured query so it's similar to",
    "start": "567640",
    "end": "573560"
  },
  {
    "text": "hudo in that it's a cluster of servers",
    "start": "573560",
    "end": "578640"
  },
  {
    "text": "behind the scenes actually querying data living on S3 which you could also do with EMR however Athena is seress so",
    "start": "578640",
    "end": "585160"
  },
  {
    "text": "instead of having to spin up your own EMR cluster to do this you can just use Athena so if your data lives on S3 again",
    "start": "585160",
    "end": "590880"
  },
  {
    "text": "what you should probably be doing you can use Athena zero",
    "start": "590880",
    "end": "596000"
  },
  {
    "text": "infrastructure zero Administration totally servess you can use standard ANC SQL standard ANC SQL to query data",
    "start": "596000",
    "end": "604720"
  },
  {
    "text": "living on S3 using",
    "start": "604720",
    "end": "608800"
  },
  {
    "text": "Athena elastic search excuse",
    "start": "610800",
    "end": "615720"
  },
  {
    "text": "me Amazon elastic search service is a managed elastic search um service based",
    "start": "620760",
    "end": "627959"
  },
  {
    "text": "on the open source elastic search um project so if you're already using",
    "start": "627959",
    "end": "633120"
  },
  {
    "text": "elastic search on Prem and you have it up and you have it manag in that system and you're using the apis those same",
    "start": "633120",
    "end": "640880"
  },
  {
    "text": "apis work with this manag surface it's the same product just run in a managed environment so it's very good for",
    "start": "640880",
    "end": "647519"
  },
  {
    "text": "distributed search and analytics um our version also comes prepackaged with um",
    "start": "647519",
    "end": "655279"
  },
  {
    "text": "log stash and Cabana built-in um log stash integration and Cabana in the",
    "start": "655279",
    "end": "661880"
  },
  {
    "text": "console so you get that full elk stack elastic search log stash and Cabana as a",
    "start": "661880",
    "end": "667200"
  },
  {
    "text": "managed service so it's very good for log analytics fulltech Search application monitoring and",
    "start": "667200",
    "end": "673480"
  },
  {
    "start": "674000",
    "end": "674000"
  },
  {
    "text": "more all right Lambda this may not be an obvious one uh Lambda may not be the first thing",
    "start": "674040",
    "end": "679440"
  },
  {
    "text": "you think of when you think of big data on AWS you're probably more thinking of red shift and EMR and S3 and that's",
    "start": "679440",
    "end": "685360"
  },
  {
    "text": "traditionally what it's been but don't forget about Lambda Lambda is our serverless compute project product and",
    "start": "685360",
    "end": "692320"
  },
  {
    "text": "there's probably other Deep dive sessions hopefully you guys will be able to attend that go more into detail on Lambda in fact uh Fanny May had a",
    "start": "692320",
    "end": "699160"
  },
  {
    "text": "presentation somewhere the past couple days that talked about their use of Lambda for Big Data they replaced an on",
    "start": "699160",
    "end": "707079"
  },
  {
    "text": "Prem HPC environment with Lambda to do thousands of simultaneous mon Carlo",
    "start": "707079",
    "end": "715160"
  },
  {
    "text": "simulations and the nice thing about Lambda is it scales out for you and you",
    "start": "715160",
    "end": "721200"
  },
  {
    "text": "don't have to worry about managing anything so you can just start throwing jobs at it and it will just automatically scale out and just you can",
    "start": "721200",
    "end": "729399"
  },
  {
    "text": "even chain lambdas together so you can have one Lambda and another one of my customers finr is doing this they have a",
    "start": "729399",
    "end": "736560"
  },
  {
    "text": "validation process where they have one Lambda gets the initial file from S3 it",
    "start": "736560",
    "end": "741920"
  },
  {
    "text": "chunks it up into smaller bits and then calls subsequent lambdas and Farms it out and all those lambdas fan out and",
    "start": "741920",
    "end": "747720"
  },
  {
    "text": "then process the file do the valid then bring it all together at the end using Dynamo DB for state so anyway very",
    "start": "747720",
    "end": "753279"
  },
  {
    "text": "powerful serverless um big data service that you",
    "start": "753279",
    "end": "759240"
  },
  {
    "text": "should think about doing that if you have a use case that you can chunk up into small parts and then massively",
    "start": "759240",
    "end": "765399"
  },
  {
    "text": "paralyze it Lambda may be the way to go um again it's very cost efficient you pay only for the request served so if",
    "start": "765399",
    "end": "772320"
  },
  {
    "text": "you have like for example these validation jobs these may not be running 24/7 you have data that comes in in a",
    "start": "772320",
    "end": "779120"
  },
  {
    "text": "specific time window and maybe it comes in a you know within a two three hour window per day and then the lambas will",
    "start": "779120",
    "end": "785680"
  },
  {
    "text": "spin up and process it but the rest of those you know 21 hours in the day you don't have any infrastructure up and",
    "start": "785680",
    "end": "791440"
  },
  {
    "text": "running to sit in their idle waiting for data to come in so while they just sitting there and nothing's happening",
    "start": "791440",
    "end": "797040"
  },
  {
    "text": "you're not paying a dime for your Lambda functions you're only paying for the executions when the the the data",
    "start": "797040",
    "end": "802600"
  },
  {
    "text": "actually comes in and you need to process it all right Amazon",
    "start": "802600",
    "end": "809519"
  },
  {
    "text": "Kinesis so Kinesis lets you collect process and analyze real-time streaming data so",
    "start": "809519",
    "end": "817040"
  },
  {
    "text": "another takea away from this is oftentimes we think of Big Data as kind of a batch process you get a bunch of",
    "start": "817040",
    "end": "822680"
  },
  {
    "text": "data in you you churn on it you do some ETL or some kind of analytics on it you spit answers out and you visualize",
    "start": "822680",
    "end": "829199"
  },
  {
    "text": "it but big data could also be coming from other sources like you know maybe",
    "start": "829199",
    "end": "836759"
  },
  {
    "text": "you're consuming Twitter maybe not this audience but um maybe iot devices you",
    "start": "836759",
    "end": "841800"
  },
  {
    "text": "maybe have a bunch of iot devices out in the field you want to collect sensor data from that um that could very easily",
    "start": "841800",
    "end": "847880"
  },
  {
    "text": "turn into big data and so you can use Kinesis to bring that data in real time",
    "start": "847880",
    "end": "853160"
  },
  {
    "text": "um either do some basic an analytics on it in kesis itself or stream it off to S3 for further",
    "start": "853160",
    "end": "860000"
  },
  {
    "text": "analysis again fully manageable and",
    "start": "860000",
    "end": "864199"
  },
  {
    "text": "scalable quick site quick site is a um a",
    "start": "865759",
    "end": "871560"
  },
  {
    "text": "bi solution it has a um engine called spice which is um an inmemory engine to",
    "start": "871560",
    "end": "878720"
  },
  {
    "text": "store your data so you get really really fast um query",
    "start": "878720",
    "end": "884800"
  },
  {
    "text": "returns you can use Quick site to set up queries and share those those stories",
    "start": "884800",
    "end": "892079"
  },
  {
    "text": "that you set up in quick site with other people so you can set up an analysis and then share that with other people in",
    "start": "892079",
    "end": "897199"
  },
  {
    "text": "your organization it's about on10th the cost of traditional uh BI Solutions and",
    "start": "897199",
    "end": "902720"
  },
  {
    "text": "it integrates with many other um AWS Services listed here so you can see some of them S3 red shift RDS the various",
    "start": "902720",
    "end": "910800"
  },
  {
    "text": "flavors of RDS um including Aurora so again if you store your data",
    "start": "910800",
    "end": "917360"
  },
  {
    "text": "um in your results in S3 for example you can use Quick site to pull that out and visualize your your",
    "start": "917360",
    "end": "924800"
  },
  {
    "text": "results glue so data cloud and ETL data catalog and ETL this was announced at",
    "start": "924959",
    "end": "930800"
  },
  {
    "start": "925000",
    "end": "925000"
  },
  {
    "text": "reinvent last year verer Vogal or um CTO during his keynote presentation um",
    "start": "930800",
    "end": "936720"
  },
  {
    "text": "announced this this is in preview it's not GA yet um it's in preview and you",
    "start": "936720",
    "end": "941839"
  },
  {
    "text": "can go ahead on on the console and um sign up for the preview if you want to play with it this is a fully managed",
    "start": "941839",
    "end": "948240"
  },
  {
    "text": "data catalog and ETL service so again it's part of removing",
    "start": "948240",
    "end": "953920"
  },
  {
    "text": "the undifferentiated heavy lifting that um is around doing your big data analytics it will actually help you",
    "start": "953920",
    "end": "960639"
  },
  {
    "text": "build your data catalog for you it can actually generate and um edit Transformations as part of the ETL and",
    "start": "960639",
    "end": "966880"
  },
  {
    "text": "you can actually also use it to schedule and run your jobs so there's multiple pieces to this components to this to",
    "start": "966880",
    "end": "972759"
  },
  {
    "text": "help you do your big data analytics and manage that um I'm actually very excited about this so I'm looking forward to it",
    "start": "972759",
    "end": "977920"
  },
  {
    "text": "coming out as GA all right so it's a segue and then the next part of my talk",
    "start": "977920",
    "end": "983319"
  },
  {
    "text": "just some questions about the volume variety and velocity at which data is being generated in organizations today",
    "start": "983319",
    "end": "989839"
  },
  {
    "text": "I'm not going to read this whole thing to you but the the last two why is data distributed many locations where is a",
    "start": "989839",
    "end": "996360"
  },
  {
    "text": "single source of Truth and is there a way I can apply multiple analytics and processing",
    "start": "996360",
    "end": "1001519"
  },
  {
    "text": "Frameworks to the same data so one of the problems we have in organizations you've got silos of data and you have",
    "start": "1001519",
    "end": "1008519"
  },
  {
    "text": "analytics run on this set of data and this analytics set run on this set of data wouldn't it be nice if we had all",
    "start": "1008519",
    "end": "1013680"
  },
  {
    "text": "of our data in one place and then we can run multiple different types of analytics on that one big set of data so",
    "start": "1013680",
    "end": "1019600"
  },
  {
    "text": "that goes into an S3 data L this term has been very popular and there's different definitions of this as well",
    "start": "1019600",
    "end": "1026280"
  },
  {
    "text": "but this is the concept to basically store and analyze all your data structured and",
    "start": "1026280",
    "end": "1031959"
  },
  {
    "text": "unstructured in one one place centralized location at low cost and obviously I'm going to tell you that",
    "start": "1031959",
    "end": "1037438"
  },
  {
    "text": "probably makes sense to be S3 so quickly in ingest your data without needing to force it into pred defied schema you",
    "start": "1037439",
    "end": "1043558"
  },
  {
    "text": "basically apply the schema at the time of analysis when you're reading the data off of S3 and separating storage a",
    "start": "1043559",
    "end": "1049360"
  },
  {
    "text": "compute is a very good best practice all right so what does that look like real quick so it starts with",
    "start": "1049360",
    "end": "1055160"
  },
  {
    "text": "S3 you put all your data in buckets in S3 and then using these ingestion tools",
    "start": "1055160",
    "end": "1060440"
  },
  {
    "text": "that I mentioned so we got Kinesis fire hose Direct Connect snowball even maybe databased migration service or just the",
    "start": "1060440",
    "end": "1067480"
  },
  {
    "text": "straight S3 apis and clis and sdks wh however you want to get the data in S3 we have many tools to do",
    "start": "1067480",
    "end": "1073760"
  },
  {
    "text": "that and then once the data is an S3 that's when you can analyze it so you can use Athena EMR quick site red shift",
    "start": "1073760",
    "end": "1080520"
  },
  {
    "text": "all these tools I just mentioned and again you don't have to use all these if you just want to start with Athena",
    "start": "1080520",
    "end": "1085919"
  },
  {
    "text": "serverless just put your data in S3 somehow either through Direct Connect or fire hose or whatever and then just",
    "start": "1085919",
    "end": "1093840"
  },
  {
    "text": "start quering it with Athena you can do that very simply not a lot of cost there's no upfront cost just pay for the",
    "start": "1093840",
    "end": "1100600"
  },
  {
    "text": "queries that you do in Thea and the storage you're doing in S3 very simple to get started you can set up Dynamo and",
    "start": "1100600",
    "end": "1106440"
  },
  {
    "text": "elastic search to catalog and search the dat you have an S3 you can write programs on top of it using API Gateway",
    "start": "1106440",
    "end": "1112559"
  },
  {
    "text": "and directory service and Cognito you can write your own apis to give your",
    "start": "1112559",
    "end": "1118520"
  },
  {
    "text": "analyst access to the data in custom applications if You' like again hitting that same source of Truth in S3 and",
    "start": "1118520",
    "end": "1124960"
  },
  {
    "text": "finally of course you want to protect and secure that data so using KMS and Cloud watch and cloud trail and again",
    "start": "1124960",
    "end": "1130919"
  },
  {
    "text": "there's other sessions to go into depth and all these tools but basically use these full Suite of tools and so it looks like a rather complicated picture",
    "start": "1130919",
    "end": "1137200"
  },
  {
    "text": "again um when you have your full data Lake up and you're using all these tools but I do want to reiterate again if you take nothing else away you can start",
    "start": "1137200",
    "end": "1144039"
  },
  {
    "text": "with something very simple S3 quick site S3 Athena just get started with that very",
    "start": "1144039",
    "end": "1150480"
  },
  {
    "text": "simple pay for what you use and then grow as your needs grow and you get more sophisticated I'm not g to spend too",
    "start": "1150480",
    "end": "1156520"
  },
  {
    "start": "1155000",
    "end": "1155000"
  },
  {
    "text": "much time in this um this is just pointing out that if you start small",
    "start": "1156520",
    "end": "1162080"
  },
  {
    "text": "remember on AWS you only pay for what you use you don't have to buy anything up front so",
    "start": "1162080",
    "end": "1167520"
  },
  {
    "text": "there's very little penalty to starting small so if you know that you want to start with maybe a terabyte of data but",
    "start": "1167520",
    "end": "1174559"
  },
  {
    "text": "maybe you're going to grow that to 20 pedabytes of data over the next three years don't size anything for 20 pedabytes of data start with that one",
    "start": "1174559",
    "end": "1181640"
  },
  {
    "text": "terabyte that you have today and then grow into it because for example if you want to add 500 terabytes of storage you",
    "start": "1181640",
    "end": "1188200"
  },
  {
    "text": "could do that instantly on EBS or very quickly on on red shift for example whereas if you're trying to do that on",
    "start": "1188200",
    "end": "1193840"
  },
  {
    "text": "Prem that might take a couple months in government you're probably talking six months realis istically so one really",
    "start": "1193840",
    "end": "1201039"
  },
  {
    "text": "nice thing about this the whole point of this slide is start small because there's very little cost to Growing",
    "start": "1201039",
    "end": "1208520"
  },
  {
    "text": "after the fact it's very easy to grow very very different from your on Prem environments where you got to do a lots",
    "start": "1208520",
    "end": "1214360"
  },
  {
    "text": "of multiple months of planning and justification in order to do some of these things to grow into very large",
    "start": "1214360",
    "end": "1220080"
  },
  {
    "text": "data sets it's very simple to do an AWS start small and then grow into",
    "start": "1220080",
    "end": "1225159"
  },
  {
    "text": "it okay so to reiterate before I hand it off to Danga use only the services you need scale only when you need to scale",
    "start": "1225159",
    "end": "1233000"
  },
  {
    "text": "pay only for what you use you can use reserved instances in spot to save",
    "start": "1233000",
    "end": "1238679"
  },
  {
    "text": "money security use our security tools there's plenty of tools you can use with cloud trail and Cloud watch and even",
    "start": "1238679",
    "end": "1245520"
  },
  {
    "text": "with config for visibility and control into your apis and retrievals do encryption at rest we have KMS and just",
    "start": "1245520",
    "end": "1251799"
  },
  {
    "text": "button clicks to ENC encrypt data at rest on S3 um you can scale very fast up",
    "start": "1251799",
    "end": "1257120"
  },
  {
    "text": "to exabytes of data on S 3 um again Big Data doesn't mean dis",
    "start": "1257120",
    "end": "1262360"
  },
  {
    "text": "batch and you can mix and match the second Point down here you can do both on premises and Cloud you can do some of",
    "start": "1262360",
    "end": "1269080"
  },
  {
    "text": "your jobs on on Prem other jobs out in the cloud you can actually burst",
    "start": "1269080",
    "end": "1274679"
  },
  {
    "text": "capacity from on-prem into the cloud so you don't have to just pick up everything we in the cloud at once using",
    "start": "1274679",
    "end": "1280640"
  },
  {
    "text": "our Direct Connect service and some other features that we have with VPC and some of our other tooling you can actually do hybrid environments you",
    "start": "1280640",
    "end": "1287720"
  },
  {
    "text": "don't have have to do um a big lift and shift and do it all at once and my final",
    "start": "1287720",
    "end": "1293559"
  },
  {
    "start": "1291000",
    "end": "1291000"
  },
  {
    "text": "slide I haven't going to all the native AWS Services I also wanted to point out",
    "start": "1293559",
    "end": "1298720"
  },
  {
    "text": "that we have a very big ecosystem of Partners and vendors that also have tools that you may be using today that",
    "start": "1298720",
    "end": "1304760"
  },
  {
    "text": "also run just fine on our platform so C3 iot who introduced a session um they were born on AWS and they have a whole",
    "start": "1304760",
    "end": "1311400"
  },
  {
    "text": "Suite of tools that you can use from their platform that runs in AWS but if you use for example I don't know Tableau",
    "start": "1311400",
    "end": "1317679"
  },
  {
    "text": "or or sap or SAS or any of these things or",
    "start": "1317679",
    "end": "1322799"
  },
  {
    "text": "S sap Hannah all that runs in AWS too so you have all of the AWS build-in native",
    "start": "1322799",
    "end": "1329159"
  },
  {
    "text": "services but if you have other tools that you already using today you're welcome to use those they all run",
    "start": "1329159",
    "end": "1334720"
  },
  {
    "text": "wonderful and they integrate with S3 and these other tools that we have so with that I am going to hand it off to F and",
    "start": "1334720",
    "end": "1341440"
  },
  {
    "text": "he's wrong is going to go into details of how they set up and do their big data processing on AWS go",
    "start": "1341440",
    "end": "1349960"
  },
  {
    "text": "about hello everyone my name is rangar Raj cal um",
    "start": "1351640",
    "end": "1357080"
  },
  {
    "text": "I'm a senior director responsible for Enterprise data platforms my group",
    "start": "1357080",
    "end": "1362600"
  },
  {
    "text": "primarily um deals with bringing data into finra ingesting it processing",
    "start": "1362600",
    "end": "1370400"
  },
  {
    "text": "etling and Publishing it to other consumers within findra in this session",
    "start": "1370400",
    "end": "1377200"
  },
  {
    "text": "I'm primarily going to speak more about the Big Data injust and",
    "start": "1377200",
    "end": "1383278"
  },
  {
    "text": "validation so I will provide we'll talk about how we use uh S3 for storage and management",
    "start": "1387520",
    "end": "1396080"
  },
  {
    "text": "and talk more about how we use EMR on S3 for all our ETL and data processing and",
    "start": "1396080",
    "end": "1402840"
  },
  {
    "text": "also uh few slides about uh how we use Lambda for our serverless uh data",
    "start": "1402840",
    "end": "1411320"
  },
  {
    "text": "validation for those of you who doesn't know who finra is finra is a financial",
    "start": "1411760",
    "end": "1418960"
  },
  {
    "text": "industry regulatory Authority uh we are responsible for uh regulating we are non",
    "start": "1418960",
    "end": "1425600"
  },
  {
    "text": "not for-profit organization responsible for uh regulating about 650,000 plus",
    "start": "1425600",
    "end": "1432080"
  },
  {
    "text": "brokers in U us and about 3,800 Plus",
    "start": "1432080",
    "end": "1437600"
  },
  {
    "text": "brok brokerage firms in us to do our regulation for the uh stock market we",
    "start": "1437600",
    "end": "1444279"
  },
  {
    "text": "monitor about 99% of uh Equity exchanges data and also more than 2/3 of uh",
    "start": "1444279",
    "end": "1452520"
  },
  {
    "text": "options exchange data we reconstruct the market based on the data that we receive",
    "start": "1452520",
    "end": "1457919"
  },
  {
    "text": "and then try to find out what happened in the stock market and that's what we do as part of our Market regulation um",
    "start": "1457919",
    "end": "1465240"
  },
  {
    "text": "surveillance our mission is investor protection and Market Integrity again to do our Market",
    "start": "1465240",
    "end": "1471919"
  },
  {
    "text": "surveillance we bring in a lot of data so our Peak data ingest is about we get",
    "start": "1471919",
    "end": "1477360"
  },
  {
    "text": "about 75 billion transactions into finra every day and all the data is stored in",
    "start": "1477360",
    "end": "1483320"
  },
  {
    "text": "S3 and uh we have about 20 paby of storage between S3 and Glacier in our",
    "start": "1483320",
    "end": "1490799"
  },
  {
    "text": "AWS ecosystem briefly I want to tell you about um how things were before the",
    "start": "1490799",
    "end": "1497440"
  },
  {
    "start": "1492000",
    "end": "1492000"
  },
  {
    "text": "cloud so storage and compute were shared in a single set of data warehouse",
    "start": "1497440",
    "end": "1504720"
  },
  {
    "text": "appliances storage was fixed so anytime we have to deal with um doing any kinds",
    "start": "1504720",
    "end": "1511120"
  },
  {
    "text": "of massive reprocessing or any kind of historical data retrieval storage is",
    "start": "1511120",
    "end": "1516600"
  },
  {
    "text": "fixed so we have to take the data that's currently in our Appliance move it to a tape restore the one from the tape into",
    "start": "1516600",
    "end": "1522960"
  },
  {
    "text": "our Appliance do some processing and then you know put the put them back on the tape AP bring it back so lot of",
    "start": "1522960",
    "end": "1530000"
  },
  {
    "text": "operational type work uh capacity was always you know like monitored very",
    "start": "1530000",
    "end": "1535159"
  },
  {
    "text": "closely so technology was you know like really getting in the way of uh",
    "start": "1535159",
    "end": "1541159"
  },
  {
    "text": "analytics so before getting into the AWS architecture I want to just give you",
    "start": "1541159",
    "end": "1547399"
  },
  {
    "text": "like couple of real examples like on a benefit that we faced when we moved to the cloud about 6 to 9 months into our",
    "start": "1547399",
    "end": "1555240"
  },
  {
    "text": "Cloud project we finished a big uh data enhancement project and then the",
    "start": "1555240",
    "end": "1560880"
  },
  {
    "text": "business came and asked us hey now that the enhancement is looking good can you do this for the last two years worth of",
    "start": "1560880",
    "end": "1568600"
  },
  {
    "text": "data and um we were just thinking about it hey if we were not in the cloud what",
    "start": "1568600",
    "end": "1574279"
  },
  {
    "text": "should have happened it would have probably taken us maybe 3 to 4 months to",
    "start": "1574279",
    "end": "1580360"
  },
  {
    "text": "do the work and in addition to that we'd have to continuously you know like work",
    "start": "1580360",
    "end": "1585640"
  },
  {
    "text": "on our capacity like you know the production jobs are running so we need to basically you know like put them on hold get the historical data processed",
    "start": "1585640",
    "end": "1593080"
  },
  {
    "text": "so a lot of operational uh headache would have been there for the entire uh",
    "start": "1593080",
    "end": "1599640"
  },
  {
    "text": "2 years worth of data which is quite large we finished the entire processing",
    "start": "1599640",
    "end": "1605080"
  },
  {
    "text": "in 48 hours just started on a weekend it was available Monday morning and we",
    "start": "1605080",
    "end": "1610880"
  },
  {
    "text": "probably spent less than $20,000 in the whole processing so it was a immediate",
    "start": "1610880",
    "end": "1616760"
  },
  {
    "text": "benefit that we saw when it comes to doing horizontal scaling you know like um using EMR the other one other other",
    "start": "1616760",
    "end": "1624799"
  },
  {
    "text": "interesting example is um uh we had a data set which roughly is about U close",
    "start": "1624799",
    "end": "1631679"
  },
  {
    "text": "to 10 billion records every day uh which is a pretty big data object on our side",
    "start": "1631679",
    "end": "1637080"
  },
  {
    "text": "and in our on Prime environment uh we were able to only keep like maybe a week or two weeks sometimes",
    "start": "1637080",
    "end": "1643840"
  },
  {
    "text": "maximum one month of data and uh anytime a business comes and asks hey can I get",
    "start": "1643840",
    "end": "1649600"
  },
  {
    "text": "a one month of this data set for like two months back we have to take the current one month data out put it there",
    "start": "1649600",
    "end": "1655399"
  },
  {
    "text": "and then give them give the give business the access to do their data analytics so same object right now we",
    "start": "1655399",
    "end": "1662600"
  },
  {
    "text": "have about two years worth of data stored in our S3 so again you know like capacity is unlimited for us in",
    "start": "1662600",
    "end": "1669880"
  },
  {
    "start": "1669000",
    "end": "1669000"
  },
  {
    "text": "S3 so when we move to um AWS what was our primary principles we wanted to make",
    "start": "1669880",
    "end": "1676360"
  },
  {
    "text": "sure that we register and track all the data that we are keeping in S3 everything from the start to the end and",
    "start": "1676360",
    "end": "1684200"
  },
  {
    "text": "then we wanted to make sure that we have an archal copy uh more like a CSV type copy that is kept in Glacier or like in",
    "start": "1684200",
    "end": "1692399"
  },
  {
    "text": "the west zone so that uh when we if we want in case of a need as a disaster or",
    "start": "1692399",
    "end": "1697519"
  },
  {
    "text": "something we can actually take the arch copy and restore our processing and then we wanted to keep all the versions of",
    "start": "1697519",
    "end": "1703919"
  },
  {
    "text": "our data so which was not possible in our on pram environment in the on-prem environment we only kept the latest",
    "start": "1703919",
    "end": "1709559"
  },
  {
    "text": "version and all the prior versions of the data we had to go back to the tapes so here you know in the Big Data AWS we",
    "start": "1709559",
    "end": "1717880"
  },
  {
    "text": "wanted to keep all the versions anytime our business asks us hey what was there two months back and what changed we'll",
    "start": "1717880",
    "end": "1724360"
  },
  {
    "text": "be able to do a very easy you know like diff processing and other things and give them the answer and we wanted to secure and",
    "start": "1724360",
    "end": "1731240"
  },
  {
    "text": "protect the data and we wanted to partition the data in the right way so that performance you know like helps",
    "start": "1731240",
    "end": "1739398"
  },
  {
    "text": "so I'll go a little bit deeper into the architecture",
    "start": "1739600",
    "end": "1744679"
  },
  {
    "start": "1742000",
    "end": "1742000"
  },
  {
    "text": "um on the whatever you see on the bottom here majority of the data that comes from outside our finra it comes through",
    "start": "1744679",
    "end": "1751960"
  },
  {
    "text": "FTP and comes into our FTP servers we also have few customers when we move to",
    "start": "1751960",
    "end": "1757360"
  },
  {
    "text": "the cloud we ask them to directly send data directly to the cloud we developed an API which gives a temporary token",
    "start": "1757360",
    "end": "1764440"
  },
  {
    "text": "back to the customers most of these customers that send directly data to our Cloud they have uh AWS CLI running on",
    "start": "1764440",
    "end": "1771760"
  },
  {
    "text": "the a side and then they just push data directly to S3 and and some customers still send it to the FTP servers and we",
    "start": "1771760",
    "end": "1779360"
  },
  {
    "text": "have a process which you see in the middle of the picture there which moves the file from our FTP servers into S3",
    "start": "1779360",
    "end": "1786720"
  },
  {
    "text": "and that Central layer there data management which is our primary area for managing the data that coming inside and",
    "start": "1786720",
    "end": "1794960"
  },
  {
    "text": "making it available to our applications so we use a lot of EMR um S3 cers protected we run a lot of",
    "start": "1794960",
    "end": "1803640"
  },
  {
    "text": "validation normalization like ETL type jobs and graph processing all those",
    "start": "1803640",
    "end": "1809919"
  },
  {
    "text": "things are running using EMR architecture and then uh data management",
    "start": "1809919",
    "end": "1814960"
  },
  {
    "text": "is our centralized um data management catalog system which cataloges all the",
    "start": "1814960",
    "end": "1820440"
  },
  {
    "text": "data keeps the lineage between the data keeps information about who is using the data which consumer used what version",
    "start": "1820440",
    "end": "1829000"
  },
  {
    "text": "and then we the data management it's more like a metadata processing makes it available to the um consumer systems the",
    "start": "1829000",
    "end": "1837200"
  },
  {
    "text": "consumer systems are again mostly using some kind of an EMR based processing to do more advanced batch analytics or",
    "start": "1837200",
    "end": "1845120"
  },
  {
    "text": "sometimes they're using red shift to do more uh analytics uh you know like more",
    "start": "1845120",
    "end": "1850159"
  },
  {
    "text": "like browser based analytics and or using Presto for some ad hoc data",
    "start": "1850159",
    "end": "1856000"
  },
  {
    "text": "querying by data anal ANS and power users um use hbas to for doing some",
    "start": "1856000",
    "end": "1861880"
  },
  {
    "text": "graph processing or Hive or spark to do some Hive jobs or map reduce jobs or",
    "start": "1861880",
    "end": "1867960"
  },
  {
    "text": "some spark jobs for uh you know ETL so that's our current um AWS",
    "start": "1867960",
    "end": "1875159"
  },
  {
    "text": "architecture and where is our data so all data that we have everything is in S3 S3 is our one source of Truth uh we",
    "start": "1875159",
    "end": "1884519"
  },
  {
    "text": "keep all our S3 data data is in east region we constantly replicate to the west region and based on our retention",
    "start": "1884519",
    "end": "1891720"
  },
  {
    "text": "policy we also push it to Glacier and if we need like a data from Glacier we take",
    "start": "1891720",
    "end": "1897639"
  },
  {
    "text": "it back and put it in S3 and use it again um so everything S3 storage is",
    "start": "1897639",
    "end": "1903639"
  },
  {
    "text": "separated from compute so how do we access the data",
    "start": "1903639",
    "end": "1908760"
  },
  {
    "start": "1906000",
    "end": "1906000"
  },
  {
    "text": "that is in S3 so we have multiple architectures here we have graph",
    "start": "1908760",
    "end": "1914039"
  },
  {
    "text": "processing which is actually recreating the market any anytime an order is placed in the market it goes to the",
    "start": "1914039",
    "end": "1921039"
  },
  {
    "text": "exchange and then end of the day all those orders are actually are sent to finra and findra is responsible to",
    "start": "1921039",
    "end": "1928159"
  },
  {
    "text": "recreate the market so we have a lot of uh graph processing happening there and",
    "start": "1928159",
    "end": "1933720"
  },
  {
    "text": "that uses hbas on S3 hbas EMR on S3 we",
    "start": "1933720",
    "end": "1938919"
  },
  {
    "text": "have a lot of ETL running uh which is like lot of batch processing using Hive",
    "start": "1938919",
    "end": "1944039"
  },
  {
    "text": "and Spark um that's again using EMR targeted queries using",
    "start": "1944039",
    "end": "1950279"
  },
  {
    "text": "Presto lot of batch analytics surveillance type applications using EMR",
    "start": "1950279",
    "end": "1955480"
  },
  {
    "text": "and uh machine learning again you know using our EMR platforms so that's",
    "start": "1955480",
    "end": "1960799"
  },
  {
    "text": "everything is in S3 and all these different kinds of uh graph processing ETL everything is running their",
    "start": "1960799",
    "end": "1967120"
  },
  {
    "text": "applications on top of the data that is in sitting in S3 so we have lot of data so always the",
    "start": "1967120",
    "end": "1975240"
  },
  {
    "text": "question that comes is like okay where is the data how many versions of each data we have um you know like where did",
    "start": "1975240",
    "end": "1983080"
  },
  {
    "text": "it come from who Senter it to us what date they sent it to us did they send multiple copies you know like how long",
    "start": "1983080",
    "end": "1990120"
  },
  {
    "text": "do we have to keep this data so these questions keeps coming to our uh architecture team and that's when uh",
    "start": "1990120",
    "end": "1998399"
  },
  {
    "start": "1996000",
    "end": "1996000"
  },
  {
    "text": "during our Cloud migration Journey we created the centralized data management",
    "start": "1998399",
    "end": "2004159"
  },
  {
    "text": "platform which is uh open sourced so when we started our Cloud Journey we",
    "start": "2004159",
    "end": "2010240"
  },
  {
    "text": "used a lot of Open Source Products in AWS and we also wanted to contribute back to the open source so our data",
    "start": "2010240",
    "end": "2016720"
  },
  {
    "text": "management platform Services is open- sourced you can go to the finra GitHub",
    "start": "2016720",
    "end": "2023320"
  },
  {
    "text": "it's named her and uh you can actually download it and use it and definitely",
    "start": "2023320",
    "end": "2028840"
  },
  {
    "text": "give us a feedback you know if you like it happy to hear about it so the data management really keeps like a catalog",
    "start": "2028840",
    "end": "2035320"
  },
  {
    "text": "of all the data that we have which is behind the scenes it's an S3 and then we keep the schema of the data that we have",
    "start": "2035320",
    "end": "2042600"
  },
  {
    "text": "we keep the versions of the object we keep what kind of compression that we are using on this object storage",
    "start": "2042600",
    "end": "2049760"
  },
  {
    "text": "policies retentions who sent us the data who is all using the data which version",
    "start": "2049760",
    "end": "2055638"
  },
  {
    "text": "they used and it also gives us an ability to uh keep archival copies and",
    "start": "2055639",
    "end": "2062280"
  },
  {
    "text": "also actually have a shared metast store where we can directly run SQL on top of",
    "start": "2062280",
    "end": "2067520"
  },
  {
    "text": "the the shared meta store so majority of the times our SQL jobs that are using Hive spark or any other cql type engine",
    "start": "2067520",
    "end": "2077280"
  },
  {
    "text": "uh they can just directly uh you know like Connect using the metast store and uh run queries on",
    "start": "2077280",
    "end": "2084040"
  },
  {
    "text": "them same thing about uh the data management again everything is stored on S3 Bob covered about the 119 durability",
    "start": "2084040",
    "end": "2092158"
  },
  {
    "text": "for S3 so uh all of our business definitions is kept in the data like",
    "start": "2092159",
    "end": "2098119"
  },
  {
    "text": "every object that we have we keep things like trade date receive date which is",
    "start": "2098119",
    "end": "2103800"
  },
  {
    "text": "the firm ID uh all kinds of attributes that normally you think about in a",
    "start": "2103800",
    "end": "2108960"
  },
  {
    "text": "typical table it is actually stored as part of our data management and um we",
    "start": "2108960",
    "end": "2114000"
  },
  {
    "text": "also keep the partition information uh so we try to keep our partitions to maximum like two to three levels",
    "start": "2114000",
    "end": "2121040"
  },
  {
    "text": "depending on how we want to perform it so we all the partition keys are actually defined in our data management",
    "start": "2121040",
    "end": "2126400"
  },
  {
    "text": "catalog and then we also have like a ddl service where if a client application wants to",
    "start": "2126400",
    "end": "2132880"
  },
  {
    "text": "get the ddl so they can just make a get ddl call which is rest based and then it'll give the ddl back and then we can",
    "start": "2132880",
    "end": "2139480"
  },
  {
    "text": "actually go to our meta store and do a create external table or like just do an alter table at partition and uh just run",
    "start": "2139480",
    "end": "2146920"
  },
  {
    "text": "queries on top of those we primarily use bzip for our compression mainly because",
    "start": "2146920",
    "end": "2152640"
  },
  {
    "text": "it is splitable so we also had some gzip files in the past but but since gzip",
    "start": "2152640",
    "end": "2158040"
  },
  {
    "text": "files are not splitable it'll be just running you know like as many mappers actually as as many gzip files uh you",
    "start": "2158040",
    "end": "2164480"
  },
  {
    "text": "have so um we mostly are leaning towards using bip for our U compression all of",
    "start": "2164480",
    "end": "2171280"
  },
  {
    "text": "our archival copies stored actually as a CSV so that if if our technology changes",
    "start": "2171280",
    "end": "2177680"
  },
  {
    "text": "or something changes in the future we should be able to easily restore the data and use it in our future technology whatever it is so that we don't have any",
    "start": "2177680",
    "end": "2184680"
  },
  {
    "text": "propriatary format for our backup data so from our optimization so all our",
    "start": "2184680",
    "end": "2193119"
  },
  {
    "start": "2187000",
    "end": "2187000"
  },
  {
    "text": "as I said you know all the data is kept in bip so majority of the time it performs quite well wherever the",
    "start": "2193119",
    "end": "2199119"
  },
  {
    "text": "performance is something required uh we convert them into an arc file and Arc is used for all our uh you",
    "start": "2199119",
    "end": "2207839"
  },
  {
    "text": "know performance-based queries and bip and golden copy used where performance is not like uh really key and again we",
    "start": "2207839",
    "end": "2215560"
  },
  {
    "text": "partition the data in such a way that uh you know performance is uh uh improved",
    "start": "2215560",
    "end": "2221440"
  },
  {
    "text": "uh we are using Arc uh for our hi queries and uh prto queries and Spark",
    "start": "2221440",
    "end": "2227440"
  },
  {
    "text": "SQL queries but uh if you're using some other columnar format it should work too but we desired to use uh",
    "start": "2227440",
    "end": "2236000"
  },
  {
    "start": "2235000",
    "end": "2235000"
  },
  {
    "text": "Arc again going back to the same we keep the data in S3 for processing so we",
    "start": "2236000",
    "end": "2241079"
  },
  {
    "text": "directly connect our EMR backed on S3 so when you run an EMR job job we start up",
    "start": "2241079",
    "end": "2248119"
  },
  {
    "text": "and then we just create an external table and then we do a select on top of that and then uh we do an alter table at",
    "start": "2248119",
    "end": "2255079"
  },
  {
    "text": "partition to add how many other partitions that we want to read if I want to read like one month worth of data we just do an alter table at",
    "start": "2255079",
    "end": "2262079"
  },
  {
    "text": "partition for one month worth of data and then uh just keep running them we",
    "start": "2262079",
    "end": "2267560"
  },
  {
    "text": "use a lot of spot pricing uh for EMR um so for those of you that are not",
    "start": "2267560",
    "end": "2274040"
  },
  {
    "text": "familiar with the spot pricing you can bid for for how much money that you want to pay for your EMR clusters and we",
    "start": "2274040",
    "end": "2282119"
  },
  {
    "text": "start bidding from 152 cents per uh cluster per per e to instance on the",
    "start": "2282119",
    "end": "2288079"
  },
  {
    "text": "cluster and it could go up to like $2 or something like that but majority of the times we what we have found is we are",
    "start": "2288079",
    "end": "2295040"
  },
  {
    "text": "able to very easily run on the spot pricing without paying too much as I said earlier in the beginning of the my",
    "start": "2295040",
    "end": "2302920"
  },
  {
    "text": "conversation one of the big jobs that we ran in the entire weekend for $20,000",
    "start": "2302920",
    "end": "2308000"
  },
  {
    "text": "which I think we probably ran like maybe some, 1,500 ec2 instances in parallel",
    "start": "2308000",
    "end": "2313319"
  },
  {
    "text": "for the entire uh uh weekend and probably we probably bid about maybe 10",
    "start": "2313319",
    "end": "2319240"
  },
  {
    "text": "to 20 cents uh uh per E2 instance during that time so we use a lot of spot",
    "start": "2319240",
    "end": "2324680"
  },
  {
    "text": "pricing so definitely we are saving a lot of money by U doing",
    "start": "2324680",
    "end": "2330079"
  },
  {
    "text": "that we have a persistent meta store which again every time uh data is",
    "start": "2330079",
    "end": "2335839"
  },
  {
    "text": "registered The Meta store automatically keeps track of the object names table names column names data types so that",
    "start": "2335839",
    "end": "2343240"
  },
  {
    "text": "our querying systems that are using EMR Spar spark Presto And Hive um can query",
    "start": "2343240",
    "end": "2349160"
  },
  {
    "text": "them so that none of the systems they they don't need to know what is the column name or attributes and everything",
    "start": "2349160",
    "end": "2355160"
  },
  {
    "text": "is coming from a centralized uh meta store all our data is protected at rest",
    "start": "2355160",
    "end": "2361880"
  },
  {
    "start": "2358000",
    "end": "2358000"
  },
  {
    "text": "using either AWS KMS or SS standard server side encryption on S3 um and",
    "start": "2361880",
    "end": "2368680"
  },
  {
    "text": "everything is HTTP all ink Transit data is protected using https uh we use lot of uh we we have",
    "start": "2368680",
    "end": "2376319"
  },
  {
    "text": "versioning in our own data management system on top of that if we need we also enable the versioning in S3 so that we",
    "start": "2376319",
    "end": "2382000"
  },
  {
    "text": "can use the S3 versioning and as I mentioned earlier we back up to the west region and Glacier uh for uh recovery",
    "start": "2382000",
    "end": "2390240"
  },
  {
    "text": "type reasons cluster sizing so we actually try to size our",
    "start": "2390240",
    "end": "2397680"
  },
  {
    "text": "cluster Bas of based on our job complexity and EMR is normally charged",
    "start": "2397680",
    "end": "2402800"
  },
  {
    "text": "at an hourly um basis so if our job finishes in 45 50 minutes we try to make",
    "start": "2402800",
    "end": "2407920"
  },
  {
    "text": "sure that we can put some other smaller job in the same cluster so that we are very efficiently using the uh spot",
    "start": "2407920",
    "end": "2414440"
  },
  {
    "text": "pricing and um you can also you know like do other things like I mean again you can do a lot of things in EMR uh for",
    "start": "2414440",
    "end": "2421839"
  },
  {
    "text": "more like interactive analytics we don't do spot because a user is coming into the browser and looking at the",
    "start": "2421839",
    "end": "2427680"
  },
  {
    "text": "application there actually we actually plan our sizing based on some on demand U um on demand",
    "start": "2427680",
    "end": "2435560"
  },
  {
    "start": "2435000",
    "end": "2435000"
  },
  {
    "text": "instances why did we choose Hive uh and Spark SQL all of our Legacy applications",
    "start": "2435560",
    "end": "2441480"
  },
  {
    "text": "that we were using before moving to the cloud were all mostly SQL based so from a skill set perspective it was very easy",
    "start": "2441480",
    "end": "2447599"
  },
  {
    "text": "to do hive spark so we directly started using Hive Sparks equl and we were able",
    "start": "2447599",
    "end": "2454960"
  },
  {
    "text": "to just quite easily migrate all of our code that's running on a nonrem environment to the cloud environment",
    "start": "2454960",
    "end": "2460319"
  },
  {
    "text": "without much of a skill set issue and other kinds of issues so we were able to easily manage",
    "start": "2460319",
    "end": "2465800"
  },
  {
    "start": "2465000",
    "end": "2465000"
  },
  {
    "text": "it I'm not going to go deep into the benefits of data L I think Bob covered it in his H3 data leg slide I think uh",
    "start": "2465800",
    "end": "2473960"
  },
  {
    "text": "the same stuff here our usage statistics on ews uh AWS",
    "start": "2473960",
    "end": "2481440"
  },
  {
    "start": "2477000",
    "end": "2477000"
  },
  {
    "text": "so this is an stats that actually I pulled uh for our something that we in",
    "start": "2481440",
    "end": "2486520"
  },
  {
    "text": "the may may one of the weeks in May uh we are actually starting about 33,000",
    "start": "2486520",
    "end": "2492200"
  },
  {
    "text": "ec2 nodes per day and about 90% of our uh E2 usage is EMR based and we have 20",
    "start": "2492200",
    "end": "2500480"
  },
  {
    "text": "pabes of storage in S3 and Glacier all of our use is uh primarily in PR",
    "start": "2500480",
    "end": "2506440"
  },
  {
    "text": "environment 60 plus 60% plus is in PR environment rest is in our test environment and U Dev environment we try",
    "start": "2506440",
    "end": "2514680"
  },
  {
    "text": "to keep all our EMR based jobs jobs to run under 2 hours and there are certain jobs where actually like you know it",
    "start": "2514680",
    "end": "2520240"
  },
  {
    "text": "runs more than uh two to two hours the primary reason we wanted to keep it under two hours is like if you are",
    "start": "2520240",
    "end": "2526119"
  },
  {
    "text": "bidding on a spot price and if your job fails you have to start overall again so I think that's why we try to keep it",
    "start": "2526119",
    "end": "2532880"
  },
  {
    "text": "that way um this is one of the stats uh from our processing um as I",
    "start": "2532880",
    "end": "2540319"
  },
  {
    "start": "2533000",
    "end": "2533000"
  },
  {
    "text": "said like you know my team is primarily responsible for doing ingesting validating and normalizing the data and",
    "start": "2540319",
    "end": "2546480"
  },
  {
    "text": "you can see there we have about 1,000 plus jobs running on the injust 150 Plus",
    "start": "2546480",
    "end": "2552040"
  },
  {
    "text": "on validate so a lot of data files that we are processing um billions and",
    "start": "2552040",
    "end": "2557359"
  },
  {
    "text": "trillions of uh transactions that are being processed lot of storage this is a monthly data volume and you can see that",
    "start": "2557359",
    "end": "2564960"
  },
  {
    "text": "um we are using close to 150,000 plus ec2 instances for our normalization for",
    "start": "2564960",
    "end": "2570319"
  },
  {
    "text": "our inest we are getting 10,000 plus so lot of uh E2 EMR used by US and you can",
    "start": "2570319",
    "end": "2576960"
  },
  {
    "text": "see the compute hours that has been running and that's most of I would say about 90% of those compute time is spot",
    "start": "2576960",
    "end": "2585480"
  },
  {
    "text": "based I want to quickly talk about how we perform uh validations uh using Lambda so before we",
    "start": "2585520",
    "end": "2594079"
  },
  {
    "text": "moved to the cloud we had an on-prem validation uh service which the files",
    "start": "2594079",
    "end": "2599160"
  },
  {
    "text": "used to come and then they are queued for validation and these are actually",
    "start": "2599160",
    "end": "2604720"
  },
  {
    "text": "about I think 3 to four ion transactions that are coming in every day uh and our",
    "start": "2604720",
    "end": "2610559"
  },
  {
    "text": "slas are very tight the jobs are running 24x 5 and uh every any anytime there's a",
    "start": "2610559",
    "end": "2616760"
  },
  {
    "text": "backup or something else is happening on the infrastructure the jobs are slowed down so lot of problems maintaining the",
    "start": "2616760",
    "end": "2623480"
  },
  {
    "text": "on-prem solution so we moved to a Lambda based aw solution Files come into FTP",
    "start": "2623480",
    "end": "2630319"
  },
  {
    "start": "2626000",
    "end": "2626000"
  },
  {
    "text": "servers uh and then they are copied to S3 and there's a small EC to instant that runs our uh controller and it keeps",
    "start": "2630319",
    "end": "2638599"
  },
  {
    "text": "calling lamb it keeps starting Lambda for each of the files and our goal was",
    "start": "2638599",
    "end": "2644160"
  },
  {
    "text": "to finish our Lambda job in less than 1 minute and it horizontally scales you",
    "start": "2644160",
    "end": "2650280"
  },
  {
    "text": "know like beyond what we can think of so we don't run any more 24x 5 anytime the",
    "start": "2650280",
    "end": "2655359"
  },
  {
    "text": "file comes in it goes Lambda is started the file is validated and given back so",
    "start": "2655359",
    "end": "2660920"
  },
  {
    "text": "it was a huge Advantage for us uh for this whole validation Service uh that",
    "start": "2660920",
    "end": "2666319"
  },
  {
    "text": "which we used to pretty much Run 24x 5 I think nowadays it probably runs only like four or 5 hours in a day depending",
    "start": "2666319",
    "end": "2672640"
  },
  {
    "text": "on when the file is arriving into us so again our results there uh you know like",
    "start": "2672640",
    "end": "2678920"
  },
  {
    "text": "reduced cost processing times have reduced again we were able to give feedback about the errors back to our",
    "start": "2678920",
    "end": "2685839"
  },
  {
    "text": "providers in a more granular way because of our on-prem solution we were not able",
    "start": "2685839",
    "end": "2690920"
  },
  {
    "text": "to scale we didn't do validation at the every column level but in the case of our um Cloud implementation we were able",
    "start": "2690920",
    "end": "2698599"
  },
  {
    "text": "to validate at all the every single column it's about 3 billion records a day horizontally about 200 columns for",
    "start": "2698599",
    "end": "2705960"
  },
  {
    "text": "each of those rows so a lot of validations were running so the scaling horizontal scaling was you know like",
    "start": "2705960",
    "end": "2712760"
  },
  {
    "text": "pretty amazing um so when we moved to Lambda it",
    "start": "2712760",
    "end": "2717880"
  },
  {
    "text": "was quite easy for us because our original validation program was running in Java so we were able to easily Port",
    "start": "2717880",
    "end": "2723800"
  },
  {
    "text": "it to our Lambda based architecture so where are we going to go from here uh we're trying to move all of",
    "start": "2723800",
    "end": "2731240"
  },
  {
    "start": "2726000",
    "end": "2726000"
  },
  {
    "text": "our high based ETL jobs into spark we started about three years back so lot of Hive jobs at the time so spark is more",
    "start": "2731240",
    "end": "2738240"
  },
  {
    "text": "matured now so we are moving all of our hi ETL jobs into spark EMR actually has",
    "start": "2738240",
    "end": "2744280"
  },
  {
    "text": "some new Concepts called instance fleets where you can actually better pay for the spot pricing so that you have less",
    "start": "2744280",
    "end": "2750119"
  },
  {
    "text": "risk of uh losing a uh easy to instance when you when when a critical SLA job",
    "start": "2750119",
    "end": "2755880"
  },
  {
    "text": "was running and then within our finra we wanted to move our ETL services to other",
    "start": "2755880",
    "end": "2761559"
  },
  {
    "text": "areas of finra where it's not just big data even if it is a small data processing or other kinds of data",
    "start": "2761559",
    "end": "2766839"
  },
  {
    "text": "processing we wanted to use our ETL framework there so that's our future",
    "start": "2766839",
    "end": "2772240"
  },
  {
    "text": "plans to use thank",
    "start": "2772240",
    "end": "2778040"
  },
  {
    "text": "you want to take questions here we have three minutes if you want to take some questions and we'll also be outside",
    "start": "2781400",
    "end": "2788040"
  },
  {
    "text": "after but we can take a couple right now and Mark's got a",
    "start": "2788040",
    "end": "2791839"
  },
  {
    "text": "microphone so when you have an SLA you need to meet it sounds like you don't really turn the dials on which instance",
    "start": "2793079",
    "end": "2798960"
  },
  {
    "text": "you're going to run on you just put it in Lambda and it way it goes to uh complete really quickly is that accurate",
    "start": "2798960",
    "end": "2805040"
  },
  {
    "text": "yeah so when I say when I me an SLA we we were supposed to give feedback within two hours to our Center center of the",
    "start": "2805040",
    "end": "2811920"
  },
  {
    "text": "data so if the jobs are waiting sometimes we Miss SLS because we are not able to send the feedback within 2 hours",
    "start": "2811920",
    "end": "2818760"
  },
  {
    "text": "but when once we move to Lambda as an file comes in the validation is kicked off and within 10 minutes the validation",
    "start": "2818760",
    "end": "2825800"
  },
  {
    "text": "is normally the feedback is given back to the center of the data okay one more question so finra you know protects",
    "start": "2825800",
    "end": "2831440"
  },
  {
    "text": "investors basically you guys recently released a report uh that talked about the liquidity of securitized uh uh",
    "start": "2831440",
    "end": "2840000"
  },
  {
    "text": "transactions did you have to uh dive into a lot of your your data analytis",
    "start": "2840000",
    "end": "2845359"
  },
  {
    "text": "analys assist to determine what the liquidity is of these securitized packages is that one example of some of",
    "start": "2845359",
    "end": "2851119"
  },
  {
    "text": "the things you do more easier for me to answer more like Tech type questions I think there are somebody from our",
    "start": "2851119",
    "end": "2856960"
  },
  {
    "text": "corpcom to more answer those kind of questions I probably have to redirect to them them so if it's a more techy",
    "start": "2856960",
    "end": "2862240"
  },
  {
    "text": "question I can answer",
    "start": "2862240",
    "end": "2868280"
  },
  {
    "text": "sorry hey uh did you consider red shave for any of your purpose and is there a",
    "start": "2872640",
    "end": "2879119"
  },
  {
    "text": "reason for selecting EMR so we are using we use red shift",
    "start": "2879119",
    "end": "2884359"
  },
  {
    "text": "also in finra actually there was a presentation yesterday for red shift by finra EMR is mostly used for all of the",
    "start": "2884359",
    "end": "2891440"
  },
  {
    "text": "ETL processing those kind of things but red shift is used for our interactive querying where you know like we need to",
    "start": "2891440",
    "end": "2897920"
  },
  {
    "text": "give feed pretty fast feedback to the end users so we are using red shift for those kind of uh use cases okay thank",
    "start": "2897920",
    "end": "2904640"
  },
  {
    "text": "you",
    "start": "2904640",
    "end": "2907640"
  },
  {
    "text": "okay okay we'll cut it off for now we'll stand outside the door for a few minutes if there's other additional questions thank you thank you he",
    "start": "2910520",
    "end": "2918960"
  }
]