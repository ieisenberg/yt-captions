[
  {
    "start": "0",
    "end": "51000"
  },
  {
    "text": "okay 10:15 so day three morning after",
    "start": "2389",
    "end": "9960"
  },
  {
    "text": "the party you guys feeling good yeah are your minds just like blue lots - lots to",
    "start": "9960",
    "end": "17760"
  },
  {
    "text": "take back so this is scaling MongoDB on Amazon Web Services I really probably",
    "start": "17760",
    "end": "24689"
  },
  {
    "text": "should have called this like a pragmatic approach to scaling MongoDB on on Amazon Web Services what I want to do is spend",
    "start": "24689",
    "end": "33260"
  },
  {
    "text": "45 minutes hour taking you through what we've learned about scaling out our",
    "start": "33260",
    "end": "38610"
  },
  {
    "text": "cluster what if we were to start over today how we would go about it sort of",
    "start": "38610",
    "end": "43950"
  },
  {
    "text": "an optimal way so before I do that quick",
    "start": "43950",
    "end": "52800"
  },
  {
    "start": "51000",
    "end": "51000"
  },
  {
    "text": "introductions my name is Mike sabbath's I'm the CTO and co-founder of apptentive we're a seed stage startup in Seattle",
    "start": "52800",
    "end": "59219"
  },
  {
    "text": "we're actually surrounded on four sides by Amazon there in South Lake Union so um",
    "start": "59219",
    "end": "64408"
  },
  {
    "text": "lots of familiar faces here today I kind of really we set out to be the easiest",
    "start": "64409",
    "end": "71070"
  },
  {
    "text": "way for anyone with an app to talk with their customers and before I get into",
    "start": "71070",
    "end": "76710"
  },
  {
    "text": "that I'll provide a kind of a very very brief overview of what we do because the the Schill if you will so I don't get in",
    "start": "76710",
    "end": "82170"
  },
  {
    "text": "trouble with the CEO this is the introduction of me I kind of want to do introductions with you so how many folks",
    "start": "82170",
    "end": "87930"
  },
  {
    "text": "are running MongoDB in production today suite and and of that like I assume most",
    "start": "87930",
    "end": "95820"
  },
  {
    "text": "of you are on Amazon Web Services for that how many folks are on Amazon Web Services with ok slightly fewer",
    "start": "95820",
    "end": "101340"
  },
  {
    "text": "hands how many are running a as part of your production a replica set ok",
    "start": "101340",
    "end": "109140"
  },
  {
    "text": "cool that's that's good and how many have taken the next step and have shard it out cool ok progressive kind of decay",
    "start": "109140",
    "end": "117990"
  },
  {
    "text": "in enhance that'll be good so the folks that that sort of raised your hands and",
    "start": "117990",
    "end": "123390"
  },
  {
    "text": "had them up for each for each call for questions you guys can help keep me honest you can help sort of backfill the",
    "start": "123390",
    "end": "129869"
  },
  {
    "text": "knowledge as well so apptentive and amazon web services so",
    "start": "129869",
    "end": "137560"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "briefly about what we do so as I said earlier we'll make it really easy for",
    "start": "137560",
    "end": "142970"
  },
  {
    "text": "app publishers to have conversations with their customers with power in a prating prompts we power in app feedback",
    "start": "142970",
    "end": "151030"
  },
  {
    "text": "whole conversation directly within your app in in-app surveys rendered natively",
    "start": "151030",
    "end": "158650"
  },
  {
    "text": "we we run entirely on Amazon Web Services and one of the things I sort of asked me to do was give it just a quick",
    "start": "158709",
    "end": "164989"
  },
  {
    "text": "overview of how we're using Amazon Web Services we're about two and a half years old in it been an AWS the entire",
    "start": "164989",
    "end": "172940"
  },
  {
    "text": "time we produce SDKs for iOS and Android so that's the sort of little screenshots",
    "start": "172940",
    "end": "179450"
  },
  {
    "text": "there on the left we of course do all of our DNS with route 53 our clients talk to our API",
    "start": "179450",
    "end": "187720"
  },
  {
    "text": "which sits behind a elastic load balancer the slide is actually out of",
    "start": "187720",
    "end": "193340"
  },
  {
    "text": "date as of last week we're now on three C one extra larges instead of those",
    "start": "193340",
    "end": "200030"
  },
  {
    "text": "those six c1 mediums those are our web servers that powers both our API surface and our front-end experience as well we",
    "start": "200030",
    "end": "207410"
  },
  {
    "text": "run in a virtual private cloud our web servers of course do all their assets",
    "start": "207410",
    "end": "213350"
  },
  {
    "text": "off cloud from all the static assets off cloud front we allow users to upload",
    "start": "213350",
    "end": "219220"
  },
  {
    "text": "various files predominantly screenshots so that all of course gets stored in s3",
    "start": "219220",
    "end": "224560"
  },
  {
    "text": "of course the MongoDB cluster and I talked about today learning nine instances we use Redis as well",
    "start": "224560",
    "end": "232340"
  },
  {
    "text": "particularly for for background processing state tracking their roll-up of statistics cloud watch for a lot of",
    "start": "232340",
    "end": "239959"
  },
  {
    "text": "our monitoring which I'll touch on but we run a fair bit of internal infrastructure for statistics and",
    "start": "239959",
    "end": "245870"
  },
  {
    "text": "logging most recently using elastic search logstash kibana stats d graphite and then we have",
    "start": "245870",
    "end": "255440"
  },
  {
    "text": "have the orchestration for it also jenkins and chef we have our toe in the",
    "start": "255440",
    "end": "262849"
  },
  {
    "text": "water with i am we're fairly small team is twelve and so we're not super locked down we we",
    "start": "262849",
    "end": "268010"
  },
  {
    "text": "trust people implicitly and they know once it's further than about ten feet from me and so you know we we we all log",
    "start": "268010",
    "end": "275000"
  },
  {
    "text": "in with our own individual accounts but would basically have sort of global roles we have a VPN server to connect in",
    "start": "275000",
    "end": "281660"
  },
  {
    "text": "to the the private cloud there we run elastic MapReduce for roll ups and",
    "start": "281660",
    "end": "287750"
  },
  {
    "text": "analysis of our data and finally of course we have our blog which runs WordPress but we wanted it in a in a",
    "start": "287750",
    "end": "295280"
  },
  {
    "text": "subdirectory there and so that we want off elastic Beanstalk we powered with RDS and we we run a reverse proxy there",
    "start": "295280",
    "end": "301190"
  },
  {
    "text": "to the subdirectory so that's our that's our Amazon Web Services footprint we're",
    "start": "301190",
    "end": "306500"
  },
  {
    "text": "super excited about some of the new services especially a Kinesis that was announced there because we're definitely",
    "start": "306500",
    "end": "312080"
  },
  {
    "text": "doing more and more data processing one of the things that you may sort of be inferring from especially the instant",
    "start": "312080",
    "end": "319490"
  },
  {
    "text": "selection here we run really lean we try to be very frugal partly because we're like I mentioned earlier seed stage",
    "start": "319490",
    "end": "325940"
  },
  {
    "text": "shard up and so we don't have much choice but partly from from a cultural perspective we're doing about a hundred",
    "start": "325940",
    "end": "333050"
  },
  {
    "text": "million API requests a week off this infrastructure our total spend is about five thousand dollars a month so that's",
    "start": "333050",
    "end": "340400"
  },
  {
    "text": "that's us in Amazon Web Services very very happy to be there especially as we",
    "start": "340400",
    "end": "346340"
  },
  {
    "text": "scale we scale and step functions and so we will double in a overnight basically",
    "start": "346340",
    "end": "351380"
  },
  {
    "text": "as new customers come online so the agenda I want to quickly go through why",
    "start": "351380",
    "end": "357230"
  },
  {
    "text": "scale MongoDB on Amazon Web Services and then I really think of this as as three",
    "start": "357230",
    "end": "362600"
  },
  {
    "text": "parts there's planning which is a big chunk of it there's the actual sort of act of deploying and getting it all set",
    "start": "362600",
    "end": "368180"
  },
  {
    "text": "up and then you've got to maintain the thing so y-scale MongoDB on Amazon Web",
    "start": "368180",
    "end": "374150"
  },
  {
    "text": "Services so first it's easy right from a",
    "start": "374150",
    "end": "379640"
  },
  {
    "text": "system administration perspective perspective both MongoDB is an incredibly easy system to administer AWS",
    "start": "379640",
    "end": "387020"
  },
  {
    "text": "is very easy as well MongoDB itself is got very friendly in",
    "start": "387020",
    "end": "392750"
  },
  {
    "text": "query syntax both systems are are very well documented MongoDB documentation especially in the",
    "start": "392750",
    "end": "399030"
  },
  {
    "text": "last nine months or so has has improved by leaps and bounds and and the AWS documentation as well next it's flexible",
    "start": "399030",
    "end": "407820"
  },
  {
    "text": "and this is each one of these is true of both both pieces of the puzzle so AWS of course allows you to rapidly scan scale",
    "start": "407820",
    "end": "414270"
  },
  {
    "text": "on demand both systems have broad language support both support a very",
    "start": "414270",
    "end": "420240"
  },
  {
    "text": "very diverse set of scenarios and so especially for a start-up like us where we don't know what's happening next week",
    "start": "420240",
    "end": "426000"
  },
  {
    "text": "let alone next month and we have new things coming up at us all the time MongoDB has allowed us to adapt to that",
    "start": "426000",
    "end": "433560"
  },
  {
    "text": "very very quickly and then finally it's it's cost-effective and you know there's a lot of a back and forth on this you",
    "start": "433560",
    "end": "442320"
  },
  {
    "text": "know everyone's got their opinion you know the best the best story I can relate is is that step function that we",
    "start": "442320",
    "end": "448800"
  },
  {
    "text": "go up we had a new customer came out come online very high volume we doubled",
    "start": "448800",
    "end": "454110"
  },
  {
    "text": "our requests per second overnight went to bed Friday night and they woke up Saturday morning we were at twice what",
    "start": "454110",
    "end": "460200"
  },
  {
    "text": "we had been 12 hours before the ability to of course turn that knob and I know I'm preaching the choir on this one but",
    "start": "460200",
    "end": "466680"
  },
  {
    "text": "the ability to turn that knob and to scale out and not not lose my Saturday",
    "start": "466680",
    "end": "471800"
  },
  {
    "text": "took me about half an hour to double our capacity and of course you've got that",
    "start": "471800",
    "end": "476970"
  },
  {
    "text": "that fine-grained control over price and performance and something as I mentioned being a freewheel startup we spend a lot of time really making sure that we're",
    "start": "476970",
    "end": "483090"
  },
  {
    "text": "we're optimizing for every last penny so why not scale MongoDB on Amazon Web",
    "start": "483090",
    "end": "489390"
  },
  {
    "start": "487000",
    "end": "487000"
  },
  {
    "text": "Services this is sort of a as equally important and I should wrap my brain I",
    "start": "489390",
    "end": "494820"
  },
  {
    "text": "came up with two reasons why you shouldn't so the first is a MongoDB is a",
    "start": "494820",
    "end": "502080"
  },
  {
    "text": "is a document oriented database if you if your data is predominantly relational",
    "start": "502080",
    "end": "507330"
  },
  {
    "text": "store I mean if your if your approach is get you to move over from my sequel or Postgres and you're mapping one to one",
    "start": "507330",
    "end": "514590"
  },
  {
    "text": "you're going to be hitting pain it is fundamentally a different system and has",
    "start": "514590",
    "end": "519659"
  },
  {
    "text": "different characteristics it has different use cases that it's very very good at and so that's that's the sort of",
    "start": "519660",
    "end": "524910"
  },
  {
    "text": "the first reason I came up with and I think a lot of folks jump into it's sort of the",
    "start": "524910",
    "end": "530070"
  },
  {
    "text": "you know the new hotness and they don't keep this in mind they come in straight from a relational model and they have",
    "start": "530070",
    "end": "536280"
  },
  {
    "text": "that mindset and of course you know RTS is you know if you fall in this camp RTS is a great solution we is that we're",
    "start": "536280",
    "end": "543000"
  },
  {
    "text": "really really excited about the Postgres RTS stuff as it seems like most of the folks are and then the second is you",
    "start": "543000",
    "end": "550470"
  },
  {
    "text": "don't want to incur the administrative costs there are administrative costs of course - doing yourself and and doing",
    "start": "550470",
    "end": "556050"
  },
  {
    "text": "this on we spend a lot of time optimizing for IO we spend a lot of time thinking about the characteristics of",
    "start": "556050",
    "end": "561660"
  },
  {
    "text": "the cluster and of course every time we hit a steady-state we scale out and we have to adjust some and while it's",
    "start": "561660",
    "end": "568340"
  },
  {
    "text": "comparable with with other systems we've administered and - the first pull point",
    "start": "568340",
    "end": "573360"
  },
  {
    "text": "about it being easy to administer the administration costs are very low I would say our hour and a half a week on",
    "start": "573360",
    "end": "581880"
  },
  {
    "text": "average over the past year if I sort of had to backfill that time if that",
    "start": "581880",
    "end": "587700"
  },
  {
    "text": "doesn't sound like fun probably scaling MongoDB on Amazon directly is not for you and of course",
    "start": "587700",
    "end": "593040"
  },
  {
    "text": "there are hosted alternatives where they can they can help you with this and there's dynamo DV as well dynamodb",
    "start": "593040",
    "end": "599040"
  },
  {
    "text": "didn't exist when we got started as a no sequel alternative I know actually very little about it but it certainly looks",
    "start": "599040",
    "end": "605610"
  },
  {
    "text": "like an interesting offering so with that sort of out of the way and and knowing that uh hopefully I don't have",
    "start": "605610",
    "end": "612060"
  },
  {
    "text": "to sell you too much on on given the the show of hands earlier let's jump into planning so there's really sort of",
    "start": "612060",
    "end": "620460"
  },
  {
    "text": "this planning checklist and there's really three areas that we need to go through so the first is is talking",
    "start": "620460",
    "end": "625680"
  },
  {
    "text": "through some of the topological considerations and this is both from perspective of MongoDB and how long",
    "start": "625680",
    "end": "631800"
  },
  {
    "text": "those scales out and then mapping that sort of into Amazon and availability zones and regions this is the issue of",
    "start": "631800",
    "end": "639000"
  },
  {
    "text": "instant selection an increasingly complex choice given the more and more instances coming online instance types",
    "start": "639000",
    "end": "645120"
  },
  {
    "text": "and then finally once you've got that stuff locked down you probably have some of the most critical decisions to make",
    "start": "645120",
    "end": "650760"
  },
  {
    "text": "in terms of what your storage is gonna look like how you're gonna to model out and what sort of i/o you need",
    "start": "650760",
    "end": "656720"
  },
  {
    "text": "so that's apologies so very the the simplest MongoDB topology of course is a",
    "start": "656720",
    "end": "662790"
  },
  {
    "start": "658000",
    "end": "658000"
  },
  {
    "text": "single sir for D process I think you know a huge part of mongos success and",
    "start": "662790",
    "end": "670920"
  },
  {
    "text": "attraction is that it's incredibly easy on your own personal laptop to get",
    "start": "670920",
    "end": "675990"
  },
  {
    "text": "started with this right it's it's six characters return and you're up and running with Mamdouh of course when you",
    "start": "675990",
    "end": "684780"
  },
  {
    "start": "684000",
    "end": "684000"
  },
  {
    "text": "start looking at production you want to think about high availability you want to think about performance and so very",
    "start": "684780",
    "end": "690660"
  },
  {
    "text": "very quickly you're looking at a replica set replica set at a minimum is going to",
    "start": "690660",
    "end": "696420"
  },
  {
    "text": "consists of three nodes you're gonna have your primary and you're gonna have at least one secondary and then either an arbiter or another secondary what",
    "start": "696420",
    "end": "704220"
  },
  {
    "text": "you're getting out of this is automatic failover so if you're if there's a hiccup if your primary goes down automatically fails over the the client",
    "start": "704220",
    "end": "711600"
  },
  {
    "text": "drivers everyone we've worked with everyone I'm aware of it's smart enough to reconnect and you you go on your way",
    "start": "711600",
    "end": "717210"
  },
  {
    "text": "it's it's really amazing we failover fairly infrequently I would I had to",
    "start": "717210",
    "end": "722670"
  },
  {
    "text": "sort of put a number on I would say maybe once or twice a quarter and typically actually probably half of that",
    "start": "722670",
    "end": "729000"
  },
  {
    "text": "is us doing doing maintenance upgrading that sort of thing a lot of times we won't know until a couple days later so",
    "start": "729000",
    "end": "738680"
  },
  {
    "text": "when you're starting out if you're if you're small like us if you're frugal like us and you want to save a little",
    "start": "738680",
    "end": "745200"
  },
  {
    "text": "bit of money you can look at doing an arbiter so an arbiter basically is a kind of us almost think of it as a sort",
    "start": "745200",
    "end": "752640"
  },
  {
    "text": "of a silent partner it's exists to vote so when when a when your primary goes down a new primary",
    "start": "752640",
    "end": "758820"
  },
  {
    "text": "needs to be elected and in the arbor sits steps in to help with that election",
    "start": "758820",
    "end": "764040"
  },
  {
    "text": "to make sure that you have a majority there your secondary is of course by",
    "start": "764040",
    "end": "770910"
  },
  {
    "text": "virtue of being ready for automatic failover they contain a full copy or data you can direct your reads to this",
    "start": "770910",
    "end": "776790"
  },
  {
    "text": "if you so choose and so you can take some of the load off your your primary and because the the arbiter is doing so",
    "start": "776790",
    "end": "785310"
  },
  {
    "text": "very little fact it's doing basically nothing the vast majority of the time runs great on a t1 micro we've been",
    "start": "785310",
    "end": "790710"
  },
  {
    "text": "running on to you and micros for two years no hiccup it's it's been",
    "start": "790710",
    "end": "796440"
  },
  {
    "text": "rock solid there before the arbiter sort of the next step up then is replacing",
    "start": "796440",
    "end": "802500"
  },
  {
    "start": "801000",
    "end": "801000"
  },
  {
    "text": "your arbiter with it with a full-fledged secondary and really the the I think",
    "start": "802500",
    "end": "807810"
  },
  {
    "text": "there are two primary reasons why you'd want to do this we made this switch last week to kind of give you an indication",
    "start": "807810",
    "end": "813660"
  },
  {
    "text": "of scale so even having gone out and sharded even at approaching a terabyte",
    "start": "813660",
    "end": "819780"
  },
  {
    "text": "of data in thousands of operations exactly we were still running arbiters",
    "start": "819780",
    "end": "825230"
  },
  {
    "text": "the primary reason for this is if you are doing maintenance if something happens to your to one of the other",
    "start": "825230",
    "end": "832050"
  },
  {
    "text": "nodes if you decide to do a full resync which a lot of times is the easiest way to sort of get compaction and and kind",
    "start": "832050",
    "end": "839040"
  },
  {
    "text": "of the benefits from that you just are running out without a safety net for that period of time a full resync for us",
    "start": "839040",
    "end": "845190"
  },
  {
    "text": "takes about 36 hours last time we did it and so that's 36 hours where you don't have that automatic failover safety net",
    "start": "845190",
    "end": "851550"
  },
  {
    "text": "and for us we got to that threshold where I became sufficiently nervous that it was time for another secondary of",
    "start": "851550",
    "end": "858570"
  },
  {
    "text": "course adding that secondary as well also helped to distribute some of the read mode so as I've mentioned as a",
    "start": "858570",
    "end": "866430"
  },
  {
    "text": "hopefully aware when you've got then a replica set your data is replicated within the replica set right each node",
    "start": "866430",
    "end": "873120"
  },
  {
    "text": "in the replica set provided it's not arbiter has a full copy of it and if",
    "start": "873120",
    "end": "879300"
  },
  {
    "text": "this is where you're at and if there's no need for sharding and you want to scale being on Amazon Web Services it's",
    "start": "879300",
    "end": "884340"
  },
  {
    "text": "really easy to scale across instance types we're at at m1 larges right now",
    "start": "884340",
    "end": "889500"
  },
  {
    "text": "it's very easy you scale up and you go to the extra larges and the quadruple extra larges and the extra larges with",
    "start": "889500",
    "end": "895260"
  },
  {
    "text": "SSDs and the extra larges with SSDs with whipped cream and cherry on top and and",
    "start": "895260",
    "end": "901200"
  },
  {
    "text": "you have that that flexibility to scale out like that at some point however you",
    "start": "901200",
    "end": "906660"
  },
  {
    "text": "will start to look at sharding and we'll get into the specific a couple bullet points later on when it's time to shard",
    "start": "906660",
    "end": "913880"
  },
  {
    "start": "908000",
    "end": "908000"
  },
  {
    "text": "but at a high level sharding is basically partitioning right so you've",
    "start": "913880",
    "end": "920130"
  },
  {
    "text": "got replica sets you've got your data replicated within a replica set and then you have a partition to cross the shards",
    "start": "920130",
    "end": "925500"
  },
  {
    "text": "and you can stamp out as many of these shards as you want if you're at the the",
    "start": "925500",
    "end": "931740"
  },
  {
    "text": "petabyte for 200 bucks talk they were at Achilles shards right we're",
    "start": "931740",
    "end": "938730"
  },
  {
    "text": "running to so um so once you have your data partition of course you need to try",
    "start": "938730",
    "end": "944490"
  },
  {
    "text": "to keep track of which data lives where that's the job of the config servers great thing again about D is that",
    "start": "944490",
    "end": "951750"
  },
  {
    "text": "it's simple your config servers are just running D right it's the same demon slightly different configuration",
    "start": "951750",
    "end": "957470"
  },
  {
    "text": "but not a lot of moving parts here config servers are are not doing too",
    "start": "957470",
    "end": "965460"
  },
  {
    "text": "much they're keeping track of where the data is we run them on micros I keep looking to see if if we're gonna be",
    "start": "965460",
    "end": "970680"
  },
  {
    "text": "exceeding that I keep watching and the graphs and yeah at our size micros are",
    "start": "970680",
    "end": "975930"
  },
  {
    "text": "still performing really well for us for the config servers and of course you",
    "start": "975930",
    "end": "980970"
  },
  {
    "text": "have your App servers and the App servers run the only other process type that there is for long ago which is",
    "start": "980970",
    "end": "987180"
  },
  {
    "text": " s that is basically a little routing daemon it's very very lightweight it's very easy to have it up",
    "start": "987180",
    "end": "992460"
  },
  {
    "text": "and running in parallel with your App servers so we just run one as as is recommended just on every app server you",
    "start": "992460",
    "end": "997890"
  },
  {
    "text": "have our apt connect effectively to localhost for and then s is doing all the routing as awareness of",
    "start": "997890",
    "end": "1004250"
  },
  {
    "text": "the config servers and and this is sort of the the ultimate in terms of scaling",
    "start": "1004250",
    "end": "1011750"
  },
  {
    "text": "out DB if you need to grow larger if you need to get to that petabyte scale range you just stamp out more and",
    "start": "1011750",
    "end": "1017900"
  },
  {
    "text": "more of these these shards and of course you can you can scale individual shards and replica sets up along the instance",
    "start": "1017900",
    "end": "1024260"
  },
  {
    "text": "types as well so let's talk briefly about which one of these is right for you as I mentioned earlier single server not",
    "start": "1024260",
    "end": "1032030"
  },
  {
    "start": "1025000",
    "end": "1025000"
  },
  {
    "text": "for production unless you don't care about your data",
    "start": "1032030",
    "end": "1036940"
  },
  {
    "text": "don't shard prematurely I mean you know if you go back way back into the bullet point about you don't want to incur the",
    "start": "1037040",
    "end": "1042949"
  },
  {
    "text": "administrative cost there is a a step function around sharding it is it is more complex there are more moving parts",
    "start": "1042950",
    "end": "1049280"
  },
  {
    "text": "than a straight replica set and so this is not something that unless you know your data's going to grow very quickly",
    "start": "1049280",
    "end": "1054950"
  },
  {
    "text": "unless you know for sure you're going to on a new database be at a point where",
    "start": "1054950",
    "end": "1060260"
  },
  {
    "text": "you need it it's something you'd want to start out out of the gate and find yourself six weeks later with ten gigs of data in MongoDB",
    "start": "1060260",
    "end": "1066740"
  },
  {
    "text": "and you've got a sharted cluster and all the overhead that comes with it replica sets can take you surprisingly far especially if you scale along",
    "start": "1066740",
    "end": "1073340"
  },
  {
    "text": "instance types but don't wait until it's too long you shard there's this sort of",
    "start": "1073340",
    "end": "1080810"
  },
  {
    "text": "ominous note in the docs collections over 256 gigs may have issues migrating",
    "start": "1080810",
    "end": "1086990"
  },
  {
    "text": "shards that of course if you've got a production system and you're reading something like that that's sort of like a scary like footnote and the docs I go",
    "start": "1086990",
    "end": "1093710"
  },
  {
    "text": "may have problems like I would prefer to not have problems and so keep an eye on",
    "start": "1093710",
    "end": "1099380"
  },
  {
    "text": "that one we started out our largest collection was around 180 gigs when we started out",
    "start": "1099380",
    "end": "1106190"
  },
  {
    "text": "and and we hit the second point rebalancing of course consumes i/o you",
    "start": "1106190",
    "end": "1111800"
  },
  {
    "text": "are contending for the lock so if you're writing as you're rebalancing and if you especially if you're writing very high volumes which you will probably are if",
    "start": "1111800",
    "end": "1119660"
  },
  {
    "text": "you need to shard the collection and it's growing quickly like we were it can be very very slow rebalancing that 180",
    "start": "1119660",
    "end": "1126470"
  },
  {
    "text": "gigs in our sort of frugal mindset let's spend as little as we can on this it took us a few weeks to get everything",
    "start": "1126470",
    "end": "1132740"
  },
  {
    "text": "fully rebalanced and then you know as",
    "start": "1132740",
    "end": "1137870"
  },
  {
    "text": "i've mentioned you need to pick the right instance type for your topology",
    "start": "1137870",
    "end": "1143560"
  },
  {
    "text": "unless you're hitting back that collection limit size charting out a",
    "start": "1143560",
    "end": "1149030"
  },
  {
    "text": "bunch of m1 smalls probably feels pretty silly and and we're gonna get to kind of",
    "start": "1149030",
    "end": "1154580"
  },
  {
    "text": "pick me right instance in just one second let's talk really briefly about availability zones and regions and",
    "start": "1154580",
    "end": "1159710"
  },
  {
    "start": "1157000",
    "end": "1157000"
  },
  {
    "text": "mapping that infrastructure to them so here's the obvious one right distribute",
    "start": "1159710",
    "end": "1164720"
  },
  {
    "text": "across availability zones in a region there's no single point of failure when you do so you know make sure you config",
    "start": "1164720",
    "end": "1171080"
  },
  {
    "text": "servers each part of the infrastructure is distributor your app servers or config servers each member of a replica",
    "start": "1171080",
    "end": "1177170"
  },
  {
    "text": "set so your replica sets are distributed I racked my brain a little bit to try and come up with like a caveat or don't",
    "start": "1177170",
    "end": "1183980"
  },
  {
    "text": "do this if such-and-such applies to you I couldn't come up with one we distribute across the veil of a zones in",
    "start": "1183980",
    "end": "1189920"
  },
  {
    "text": "a region distributing across regions there's a little bit of a harder choice to make and especially once you",
    "start": "1189920",
    "end": "1197659"
  },
  {
    "text": "move to Sharda cluster it adds another level of complexity because then you have to decide are you distributing such",
    "start": "1197659",
    "end": "1204140"
  },
  {
    "text": "the each shard maps to a different region or are you distributing such that your shards are in multiple regions and",
    "start": "1204140",
    "end": "1210350"
  },
  {
    "text": "use your replica sets that have members in different regions and we actually started out when we were not charted and",
    "start": "1210350",
    "end": "1216679"
  },
  {
    "text": "with this ladder scenario we had two members of our replica set out in u.s.",
    "start": "1216679",
    "end": "1222590"
  },
  {
    "text": "East and we had a member in US West too and we were seeing about a hundred millisecond impact so we had the the",
    "start": "1222590",
    "end": "1231470"
  },
  {
    "text": "right concern such a set such that any rights had to be propagated to the secondaries and if we would take our",
    "start": "1231470",
    "end": "1238309"
  },
  {
    "text": "secondaries down our end response time to our customers would drop by 100 milliseconds just by that that sort of",
    "start": "1238309",
    "end": "1244909"
  },
  {
    "text": "cross trip and that back there so that's the first sort of consideration there is is the replication late latency if",
    "start": "1244909",
    "end": "1251240"
  },
  {
    "text": "you're distributing a given replica set across I should footnote this you do",
    "start": "1251240",
    "end": "1256490"
  },
  {
    "text": "have the ability Mangal gives you a lot of flexibility around having different priorities around having hidden members",
    "start": "1256490",
    "end": "1261980"
  },
  {
    "text": "the replica set and so you can very effectively have a third of your",
    "start": "1261980",
    "end": "1267260"
  },
  {
    "text": "infrastructure in another region sitting dormant with a lower priority and not incur that cost that's not something",
    "start": "1267260",
    "end": "1273049"
  },
  {
    "text": "that we did but that if you're if you're really concerned about losing a region that is definitely one way to go of",
    "start": "1273049",
    "end": "1278779"
  },
  {
    "text": "course you have data current transfer considerations if you're if you're crossing region boundary you're gonna be paying for the transfer there you have",
    "start": "1278779",
    "end": "1284899"
  },
  {
    "text": "additional administrative costs you know the the adage that was sort of asserted to me when we were getting started with",
    "start": "1284899",
    "end": "1290809"
  },
  {
    "text": "Amazon Web Services is it's not really the Amazon Cloud Fama's on clouds either",
    "start": "1290809",
    "end": "1297380"
  },
  {
    "text": "tools are getting better they're way better today than when we started in terms of working with resources there are multiple regions but it is of course",
    "start": "1297380",
    "end": "1304000"
  },
  {
    "text": "still more work a lot of the tools are still scope contextual II to a given region and you have to sort of manage",
    "start": "1304000",
    "end": "1309620"
  },
  {
    "text": "that region switching yourself the flip side is if you are doing a law if you're",
    "start": "1309620",
    "end": "1315320"
  },
  {
    "text": "driving a lot of traffic from different regions and you're not centralized you can use geo based tag aware sharding and",
    "start": "1315320",
    "end": "1322850"
  },
  {
    "text": "basically route your traffic intelligently to two shards in different regions and so that the the latencies are lower",
    "start": "1322850",
    "end": "1330260"
  },
  {
    "text": "because they're traveling less distance and so while on the whole you know I",
    "start": "1330260",
    "end": "1335720"
  },
  {
    "text": "think distributing across regions sort of gives me a little bit of pause this is the one area where where I think",
    "start": "1335720",
    "end": "1341330"
  },
  {
    "text": "there's sort of a clear win there so from the topologies let's jump in talk",
    "start": "1341330",
    "end": "1348230"
  },
  {
    "start": "1345000",
    "end": "1345000"
  },
  {
    "text": "through quickly selecting an instance you really have four sort of decision points to make you've got compute memory",
    "start": "1348230",
    "end": "1354260"
  },
  {
    "text": "of course those things are bound together in an instance you have whether or not you're gonna run EBS optimized and then of course there's gonna be a",
    "start": "1354260",
    "end": "1360620"
  },
  {
    "text": "cost associated with your decisions if you haven't seen it yet this site ec2",
    "start": "1360620",
    "end": "1367250"
  },
  {
    "text": "instances dot info it's fantastic how many folks have seen are you guys familiar with this site about half and a",
    "start": "1367250",
    "end": "1374030"
  },
  {
    "text": "half if you're not you're probably familiar with going to Amazon's web page",
    "start": "1374030",
    "end": "1379520"
  },
  {
    "text": "and swapping back and forth between like two tabs to try and map like this is the instance characteristics this is the",
    "start": "1379520",
    "end": "1384679"
  },
  {
    "text": "cost or weight what were the instance characteristics okay this was the cost pulls it all together in a simple table sortable you can filter it it's it's a",
    "start": "1384679",
    "end": "1392540"
  },
  {
    "text": "github based thing so if things get out of date it's very easy to commit to contribute to it so this may be like the",
    "start": "1392540",
    "end": "1399679"
  },
  {
    "text": "most time-saving tip in the talk for those of you that they don't know about that",
    "start": "1399679",
    "end": "1406090"
  },
  {
    "text": "so let's talk first about compute it's most likely not going to be a",
    "start": "1406090",
    "end": "1412250"
  },
  {
    "start": "1407000",
    "end": "1407000"
  },
  {
    "text": "significant factor like I mentioned we're running m1 larges are there exceptions in this if you're doing a lot of long those MapReduce if you're",
    "start": "1412250",
    "end": "1420350"
  },
  {
    "text": "running the aggregation framework if you're running prior to two to four",
    "start": "1420350",
    "end": "1427030"
  },
  {
    "text": "either the concurrency came with v8 in in two four and just as a kind of",
    "start": "1427030",
    "end": "1434390"
  },
  {
    "text": "real-world data point this is us I guess in September you can actually see there on the left a failover that happens make",
    "start": "1434390",
    "end": "1441530"
  },
  {
    "text": "sure to get both sides in the picture which is kind of cool as the primary there on the the left post part of the",
    "start": "1441530",
    "end": "1446929"
  },
  {
    "text": "Left chart if you add up those that are that are kind of overlaid somewhere on the order of thirteen fourteen hundred",
    "start": "1446929",
    "end": "1453440"
  },
  {
    "text": "operations a second when you switch over and we start doing replication at the far right they're peeking out around",
    "start": "1453440",
    "end": "1459230"
  },
  {
    "text": "four hundred um not a whole lot of CPU usage right we spiked at one point to maybe about 3035",
    "start": "1459230",
    "end": "1466969"
  },
  {
    "text": "percent there most of it's actually just sitting in IO 8 and we've been doing a bunch since then to beef up our i/o",
    "start": "1466969",
    "end": "1473690"
  },
  {
    "text": "capacity one one sort of important footnote I think this is probably less important day by day but only run in",
    "start": "1473690",
    "end": "1480169"
  },
  {
    "text": "64-bit among these is memory map files and if you're on 32-bit you will cap out at your your database total storage",
    "start": "1480169",
    "end": "1488239"
  },
  {
    "text": "potential to gigabytes in which case sharding is not an interesting topic",
    "start": "1488239",
    "end": "1493399"
  },
  {
    "text": "probably even replica sets is less interesting topic so 64-bit that's an easy one so then let's talk a little bit",
    "start": "1493399",
    "end": "1501469"
  },
  {
    "start": "1499000",
    "end": "1499000"
  },
  {
    "text": "about memory some memory here you need to start by kind of estimating your",
    "start": "1501469",
    "end": "1506899"
  },
  {
    "text": "working set and this is this is a hard thing to do especially if it's a new clustering you don't have past performance to look at so for us right",
    "start": "1506899",
    "end": "1514909"
  },
  {
    "text": "where you know let's round up a little bit where it a terabyte or working sets not a terabyte our working set is the",
    "start": "1514909",
    "end": "1521359"
  },
  {
    "text": "portion of the indices and the data that we sort of need readily available and need in memory at any one time for us",
    "start": "1521359",
    "end": "1529099"
  },
  {
    "text": "it's actually very small I was actually quite surprised that m1 larges are still",
    "start": "1529099",
    "end": "1534769"
  },
  {
    "text": "doing it for us has out of this",
    "start": "1534769",
    "end": "1540440"
  },
  {
    "text": "feature it makes a little bit easier to estimate this you can run this this command to get server status and get an",
    "start": "1540440",
    "end": "1546169"
  },
  {
    "text": "estimate from of what your working set requirements are tell you how many pages are currently in memory those are",
    "start": "1546169",
    "end": "1552349"
  },
  {
    "text": "for K pages and you can do the math and figure out how close you are to your available Ram it will tell you the",
    "start": "1552349",
    "end": "1558139"
  },
  {
    "text": "measurement period as well and so if the measurement period is decreasing or if it's a very small measurement period",
    "start": "1558139",
    "end": "1564289"
  },
  {
    "text": "that's also a good indication that you need to increase the the RAM available",
    "start": "1564289",
    "end": "1569570"
  },
  {
    "text": "for your working set and of course you can run DB stats as well and get a feel for the total size of your data that",
    "start": "1569570",
    "end": "1575239"
  },
  {
    "text": "total size of your indices and then do kind of back of the napkin math on your",
    "start": "1575239",
    "end": "1581419"
  },
  {
    "text": "access patterns and so forth so the tools to sort of other than trial and",
    "start": "1581419",
    "end": "1587179"
  },
  {
    "text": "error estimate your working set and that's very easy even though the ECA 2 instances dot info and you can go right",
    "start": "1587179",
    "end": "1593089"
  },
  {
    "text": "down the and find the the instance or the class of instances that have the right memory",
    "start": "1593089",
    "end": "1598660"
  },
  {
    "text": "requirements to do that there's very little additional overhead you know say",
    "start": "1598660",
    "end": "1604570"
  },
  {
    "text": "maybe 10% maybe even less depending of course on how much 200 gram you have and so you can assume that the vast majority",
    "start": "1604570",
    "end": "1611280"
  },
  {
    "text": " only run on that instance and then monitor on M&S I'll talk a",
    "start": "1611280",
    "end": "1618430"
  },
  {
    "text": "little bit more about MMS at the very end of the talk when we get to maintaining the cluster but while we're",
    "start": "1618430",
    "end": "1623830"
  },
  {
    "text": "here there's sort of three interesting things look at we'll show you the page faults I find this to be a really",
    "start": "1623830",
    "end": "1628870"
  },
  {
    "text": "abstract number it's like okay great we've got 100 page faults a second is that good is that bad you can monitor",
    "start": "1628870",
    "end": "1634120"
  },
  {
    "text": "your queues I find this to be a little bit less of an abstract number a little easier to wrap my mind around but ultimately what what at the end of the I",
    "start": "1634120",
    "end": "1640420"
  },
  {
    "text": "care the very most about one of the response times were delivering to our customers and and so we start at the",
    "start": "1640420",
    "end": "1645700"
  },
  {
    "text": "stop of the stack you know we have a goal of all api's in under 100 milliseconds when we start getting close",
    "start": "1645700",
    "end": "1652780"
  },
  {
    "text": "to that we start exceeding that then we go investigating so talk really quickly",
    "start": "1652780",
    "end": "1659590"
  },
  {
    "text": "about EBS optimization run it when it's available on the instance types especially if you're using provision I",
    "start": "1659590",
    "end": "1665380"
  },
  {
    "text": "ops which we'll talk about in a second the thing that we found is that your volume configuration and this is you",
    "start": "1665380",
    "end": "1671380"
  },
  {
    "text": "know as with all things in life this is to a limit for us the volume configuration impacts I hope are way",
    "start": "1671380",
    "end": "1678400"
  },
  {
    "text": "more than the instant selection we spend a lot more time tuning at the EBS level than we do tuning at the instance level",
    "start": "1678400",
    "end": "1686340"
  },
  {
    "text": "and and so let's talk a little bit about",
    "start": "1686340",
    "end": "1691360"
  },
  {
    "start": "1688000",
    "end": "1688000"
  },
  {
    "text": "storage so you've got three choices right you've got the instant storage",
    "start": "1691360",
    "end": "1696700"
  },
  {
    "text": "it's non durable it's fast but inconsistent and you can't use it for snapshot based backups which as you get",
    "start": "1696700",
    "end": "1703420"
  },
  {
    "text": "larger is actually a real considerable limitation you've got the standard EBS",
    "start": "1703420",
    "end": "1709740"
  },
  {
    "text": "it's slower there's higher variability performance here's our version of the parse charts showing sort of our latency",
    "start": "1709740",
    "end": "1718360"
  },
  {
    "text": "here this is real latency on standard EBS probably hard to see the numbers a little bit it's actually hard for me to",
    "start": "1718360",
    "end": "1724270"
  },
  {
    "text": "as well sort of an average base time of around seven milliseconds but those spikes are going up to 80 90 milliseconds and so",
    "start": "1724270",
    "end": "1732340"
  },
  {
    "text": "now fortunately we have provision I ops you get consistent performance and so you see a very very tight banding the",
    "start": "1732340",
    "end": "1739030"
  },
  {
    "text": "baseline here is much much lower you see very tight banding around about 1 and 1/2 milliseconds with our spike at 2 and",
    "start": "1739030",
    "end": "1746289"
  },
  {
    "text": "1/2 milliseconds there so this has been a clear win for us one pain point here",
    "start": "1746289",
    "end": "1754570"
  },
  {
    "text": "don't under provisions your provision I ops right you are buying a certain number a certain amount of i/o per second if you use your provisional apps",
    "start": "1754570",
    "end": "1762250"
  },
  {
    "text": "if you pay for a thousand and you use a thousand in the first 100 milliseconds",
    "start": "1762250",
    "end": "1767409"
  },
  {
    "text": "you're gonna sit waiting for for the balance of the time this was really",
    "start": "1767409",
    "end": "1773470"
  },
  {
    "text": "painful for us when we first switched over the the metrics around this you don't get a number that says oh by the",
    "start": "1773470",
    "end": "1778750"
  },
  {
    "text": "way you consume this many AI ops you sort of get queue lengths and you get latencies and you have to read the tea",
    "start": "1778750",
    "end": "1784000"
  },
  {
    "text": "leaves a little bit so watch for that don't know under provision especially as a frugal startup this is hard right",
    "start": "1784000",
    "end": "1790539"
  },
  {
    "text": "because we want to make sure that we're not out there with way more capacity than we need we've actually run in the",
    "start": "1790539",
    "end": "1798580"
  },
  {
    "text": "last 18 months on all different combinations we've run standard EBS prior to PI ops being announced we switched over to",
    "start": "1798580",
    "end": "1806200"
  },
  {
    "text": "provisioned I ops we under provisioned we had a huge amount of pain and especially for the price we're very very",
    "start": "1806200",
    "end": "1812080"
  },
  {
    "text": "frustrated we didn't know what we were doing quite frankly we switched to instant storage instant storage",
    "start": "1812080",
    "end": "1818140"
  },
  {
    "text": "immediately was like a glass of cold water moves fantastic it was fast and",
    "start": "1818140",
    "end": "1824080"
  },
  {
    "text": "then all of a sudden there were random spikes there's sort of a neighbor there was contention you know and and and",
    "start": "1824080",
    "end": "1831250"
  },
  {
    "text": "those were really really painful for us right there's nothing worse the middle of the day being in the zone coding away and suddenly your system slow and right",
    "start": "1831250",
    "end": "1838030"
  },
  {
    "text": "as you dig in to try and figure out what's going on it speeds right back up again and it's like what's going on here",
    "start": "1838030",
    "end": "1843360"
  },
  {
    "text": "and so ultimately we we moved about nine months ago back to provision I ops we",
    "start": "1843360",
    "end": "1849010"
  },
  {
    "text": "provision the proper number and it sits in sort of happiness ever since so",
    "start": "1849010",
    "end": "1856020"
  },
  {
    "text": "quickly on the other the other dimension of sort of storage choices the official box still raid10 I don't get it there's sort of a",
    "start": "1856020",
    "end": "1865070"
  },
  {
    "text": "align in the box that asserts that MongoDB has been optimized for raid 10",
    "start": "1865070",
    "end": "1870620"
  },
  {
    "text": "on AWS HQ is got a great blog post I think this is the last six weeks or so",
    "start": "1870620",
    "end": "1876050"
  },
  {
    "text": "sort of with their performance numbers and observations we use LVM on top of MD",
    "start": "1876050",
    "end": "1881330"
  },
  {
    "text": "apt and using raid zero you know the the risk we run of course we lose a volume",
    "start": "1881330",
    "end": "1888080"
  },
  {
    "text": "and that instance size having our data triplicated out across the cluster and",
    "start": "1888080",
    "end": "1894050"
  },
  {
    "text": "then in snapshots is sufficient for me so that's my take on it use X FS or ext",
    "start": "1894050",
    "end": "1901100"
  },
  {
    "text": "F it makes snapshotting way way easier faster for this stuff and then you know",
    "start": "1901100",
    "end": "1907940"
  },
  {
    "text": "do the really low-hanging fruit of making sure you have the right mount options you know it's this is not like a",
    "start": "1907940",
    "end": "1913180"
  },
  {
    "text": "ninety percent drop off sort of in performance if you forget to do this it's not the end of the world but it is it is very little hanging fruit in there",
    "start": "1913180",
    "end": "1919670"
  },
  {
    "text": "is that sort of added added perf boost and so you might as well since you won't be taking advantage of these on on the",
    "start": "1919670",
    "end": "1925310"
  },
  {
    "text": "volume anyways so really quickly recommendations on selecting instance",
    "start": "1925310",
    "end": "1931820"
  },
  {
    "start": "1928000",
    "end": "1928000"
  },
  {
    "text": "lead with your working set requirements make sure you've got sufficient compute turn on EBS optimized as fists available",
    "start": "1931820",
    "end": "1938090"
  },
  {
    "text": "use provisioned by ops and then double check and make sure that you haven't",
    "start": "1938090",
    "end": "1943480"
  },
  {
    "text": "shrunk your run way down to like a week if you've got one and cost is important to you you know validated there so let's",
    "start": "1943480",
    "end": "1951800"
  },
  {
    "text": "talk deploying so this is really easy and so what I want to do actually is do",
    "start": "1951800",
    "end": "1958820"
  },
  {
    "text": "like the three four minute demo and just show you how easy it is to get set up with a a replica set in MongoDB so I'm",
    "start": "1958820",
    "end": "1970250"
  },
  {
    "text": "gonna do this all locally I've sort of pre-configured this for expediency",
    "start": "1970250",
    "end": "1975980"
  },
  {
    "text": "basically what I'm doing I'm going to spin up three D processes each one is going to get its own data directory",
    "start": "1975980",
    "end": "1981710"
  },
  {
    "text": "each one's gonna get it so important so they're running on the same machine of course but there's no until until there",
    "start": "1981710",
    "end": "1988700"
  },
  {
    "text": "configures replica set there's really no there is no of similarity or connection with them",
    "start": "1988700",
    "end": "1995440"
  },
  {
    "text": "I've got three of these config files just to speed it up here and each one's got its own port the path",
    "start": "1995440",
    "end": "2002279"
  },
  {
    "text": "and it's just a couple small optimizations when you're when you are running and development you don't need to pre provision massive amounts of",
    "start": "2002279",
    "end": "2008940"
  },
  {
    "text": "space so I have that turned on as well so d-file",
    "start": "2008940",
    "end": "2015229"
  },
  {
    "text": "instance one is up we can connect to it",
    "start": "2015229",
    "end": "2020869"
  },
  {
    "text": "there's nothing in here it's brand-new up it did they are all pre configured",
    "start": "2020869",
    "end": "2026369"
  },
  {
    "text": "for replica set so let me let me get to that in a second let's spin up the other two so now they're all up and running",
    "start": "2026369",
    "end": "2038570"
  },
  {
    "text": "and so the first thing is to move over to a replica set is to initiate it and",
    "start": "2040009",
    "end": "2050210"
  },
  {
    "text": "it knows it needs to be initiated and it starts doing that and you can see here in the background it's starting up it",
    "start": "2050210",
    "end": "2058980"
  },
  {
    "text": "goes through a start up comes up as a secondary and election occurs and now",
    "start": "2058980",
    "end": "2064049"
  },
  {
    "text": "it's the primary and if we look at it you can see our s status we've got one",
    "start": "2064049",
    "end": "2072240"
  },
  {
    "text": "member in our replica set it's the primary right fantastic now we've got these other nodes sitting in the",
    "start": "2072240",
    "end": "2079169"
  },
  {
    "text": "background they're not yet connected we haven't told yet about them before we do that let's just insert data got a",
    "start": "2079169",
    "end": "2088858"
  },
  {
    "text": "record inserted and we can DB foo find it and it's right there right really",
    "start": "2088859",
    "end": "2095460"
  },
  {
    "text": "really easy this this is why so many people love super fast to get",
    "start": "2095460",
    "end": "2101220"
  },
  {
    "text": "started so let's add those other nodes",
    "start": "2101220",
    "end": "2106910"
  },
  {
    "text": "RS add and you give it the host and the port",
    "start": "2107299",
    "end": "2114049"
  },
  {
    "text": "and so we've added the other ones you've probably seen the the scroll happening a little bit here in the background it's",
    "start": "2114400",
    "end": "2120859"
  },
  {
    "text": "adding and now if we do our RS status we can see that we've got all three nodes",
    "start": "2120859",
    "end": "2126710"
  },
  {
    "text": "in here the initial one is still the primary the other two are sort of into their startup sequence and and have",
    "start": "2126710",
    "end": "2133279"
  },
  {
    "text": "their initial sync happening if we wait a few seconds the second one's fully",
    "start": "2133279",
    "end": "2139519"
  },
  {
    "text": "initialized it's done it's sync it's now a secondary and all three are now",
    "start": "2139519",
    "end": "2144529"
  },
  {
    "text": "secondaries of course replicating foo",
    "start": "2144529",
    "end": "2149900"
  },
  {
    "text": "bar equals one is really really fast and so you know that normally in that you'd have a lot more data that would",
    "start": "2149900",
    "end": "2155989"
  },
  {
    "text": "replicate that would take a little time so that basically could give you a taste",
    "start": "2155989",
    "end": "2162739"
  },
  {
    "text": "of how easy it is to start scaling out and and how sort of approachable is this is our primary here let's close",
    "start": "2162739",
    "end": "2171979"
  },
  {
    "text": "it we're done if we query here because we're still connected to that wait so I",
    "start": "2171979",
    "end": "2177999"
  },
  {
    "text": "when I connect it on the command line there I connected to a specific instance normally in production you would give it",
    "start": "2177999",
    "end": "2183019"
  },
  {
    "text": "multiple nodes and so they can automatically failover I didn't do that so when I run another command it's can't",
    "start": "2183019",
    "end": "2190460"
  },
  {
    "text": "connect right the server's gone if I now connect to the second node we're the the",
    "start": "2190460",
    "end": "2196789"
  },
  {
    "text": "primary here you can see that it's failed over you can see the first ones",
    "start": "2196789",
    "end": "2202789"
  },
  {
    "text": "not helped not reachable and if we go back up here we can see that the data is",
    "start": "2202789",
    "end": "2208039"
  },
  {
    "text": "there and we can keep doing this right we can fail over again and we can go to",
    "start": "2208039",
    "end": "2213650"
  },
  {
    "text": "the third one now because there's no longer a sort of sufficient quorum to elect a primary if I go and look at the",
    "start": "2213650",
    "end": "2220819"
  },
  {
    "text": "status I've got two nodes down this third one is sitting as a secondary it's waiting for a majority to come back up so that that there can be a primary and",
    "start": "2220819",
    "end": "2227210"
  },
  {
    "text": "and this is primarily of course last thing you want is to have a cluster that splits in half and suddenly you're up",
    "start": "2227210",
    "end": "2232969"
  },
  {
    "text": "and running with with two primaries that's that's a bad thing we've as just",
    "start": "2232969",
    "end": "2240710"
  },
  {
    "text": "as aside one of the things we love that  we put really through its paces out of moments of sort of sheer",
    "start": "2240710",
    "end": "2246529"
  },
  {
    "text": "human error and stupid we have gotten multiple primaries up and running in our replica set and to",
    "start": "2246529",
    "end": "2252859"
  },
  {
    "text": "its credit when you try them to sort of reconcile a file those and undo your stupidity realizes that you've been",
    "start": "2252859",
    "end": "2258410"
  },
  {
    "text": "stupid and and helps you to fix the problem we've worked with a lot of other database systems that sort of had a I'm",
    "start": "2258410",
    "end": "2266150"
  },
  {
    "text": "taking my toys and going home mentality and one in particular that decided in recovery mode to delete 200 gigs of data",
    "start": "2266150",
    "end": "2272749"
  },
  {
    "text": "as a way to get back to a consistent state so big fans of making it",
    "start": "2272749",
    "end": "2279439"
  },
  {
    "text": "easy for us to be stupid so so that's",
    "start": "2279439",
    "end": "2285140"
  },
  {
    "text": "sort of the to kind of give you just a flavor of how easy it is to start",
    "start": "2285140",
    "end": "2290359"
  },
  {
    "text": "scaling out how approachable is as a system so of course you're not going",
    "start": "2290359",
    "end": "2297199"
  },
  {
    "start": "2295000",
    "end": "2295000"
  },
  {
    "text": "to approach your DevOps and and your deployment like this I hope in production when you start think about to",
    "start": "2297199",
    "end": "2304579"
  },
  {
    "text": "have ops you need to sort of go for the abilities reliability predictability repeatability audit ability and if you",
    "start": "2304579",
    "end": "2311569"
  },
  {
    "text": "do this of course you get easy replace ability and scalability and so really I think about is sort of always been a",
    "start": "2311569",
    "end": "2317900"
  },
  {
    "text": "philosophically Safa chol approach building my infrastructure so I can",
    "start": "2317900",
    "end": "2322969"
  },
  {
    "text": "treat it like an appliance if I have to SSH to a box it's sort of DevOps fail",
    "start": "2322969",
    "end": "2328059"
  },
  {
    "text": "and the nice thing about this is if you do this really really well all of those decisions that we sort of just went",
    "start": "2328059",
    "end": "2334400"
  },
  {
    "text": "through around planning become a lot less important because when you make the wrong decision you reconfigure you sort",
    "start": "2334400",
    "end": "2340759"
  },
  {
    "text": "of rerun your DevOps infrastructure and and you go on your way",
    "start": "2340759",
    "end": "2346359"
  },
  {
    "text": "and so there are a lot of tools that help you do this in the in the",
    "start": "2346359",
    "end": "2353209"
  },
  {
    "start": "2348000",
    "end": "2348000"
  },
  {
    "text": "marketplace for AWS marketplace there are a.m. eyes that are pre-configured with MongoDB best practices is",
    "start": "2353209",
    "end": "2360380"
  },
  {
    "text": "everything you need you go in you took the giant green button go on your way and you've got a MongoDB up instance up",
    "start": "2360380",
    "end": "2366259"
  },
  {
    "text": "and running if you are I mean if what I just sort of did in the demo appeals to you of actually spending time on the",
    "start": "2366259",
    "end": "2371929"
  },
  {
    "text": "command line and managing your infrastructure that way this is probably the best place to start because it is",
    "start": "2371929",
    "end": "2377299"
  },
  {
    "text": "definitely the component pieces and it's a little more of a go at yourself approach to stitching those together I think these",
    "start": "2377299",
    "end": "2384150"
  },
  {
    "text": "are really helpful as there's these are really fantastic for development and test it's not a DevOps solution they're",
    "start": "2384150",
    "end": "2390660"
  },
  {
    "text": "cloud formation templates for resource set up initial configuration we did this when we first moved to our to a replica",
    "start": "2390660",
    "end": "2397920"
  },
  {
    "text": "set about two years ago at the time and I am not actually sure if this has changed or not having cloud formation",
    "start": "2397920",
    "end": "2405779"
  },
  {
    "text": "and having the same things span regions was a little bit painful there and so ultimately we use chef today there are a",
    "start": "2405779",
    "end": "2411779"
  },
  {
    "text": "lot of other solutions out there that do similar things have slightly different philosophical approaches puppet ansible",
    "start": "2411779",
    "end": "2418019"
  },
  {
    "text": "saltstack there's more there's Amazon opsworks today we're pretty excited about this it does run chef solo",
    "start": "2418019",
    "end": "2424589"
  },
  {
    "text": "effectively and so for us we the the MongoDB cookbooks that we use which are the the maintainer on github is edia",
    "start": "2424589",
    "end": "2433499"
  },
  {
    "text": "light été light I believe those use some",
    "start": "2433499",
    "end": "2438809"
  },
  {
    "text": "of the search functionality that requires a chef server and so we can't use office works yet today but it's it's",
    "start": "2438809",
    "end": "2445109"
  },
  {
    "text": "definitely a appealing awkward thing that we're looking forward to moving to its some point hopefully in the future",
    "start": "2445109",
    "end": "2451099"
  },
  {
    "start": "2451000",
    "end": "2451000"
  },
  {
    "text": "so a few notes as you're deploying on security running a V PC you will have",
    "start": "2451099",
    "end": "2458219"
  },
  {
    "text": "complications of course from that if you're doing things cross region you've got to account for all of that if you have a lot of different sources that",
    "start": "2458219",
    "end": "2464789"
  },
  {
    "text": "need to ingress and your database you will have that complexity as well we've really architected such that",
    "start": "2464789",
    "end": "2470519"
  },
  {
    "text": "everything runs in our V PC our our ingress are basically the elastic load",
    "start": "2470519",
    "end": "2476640"
  },
  {
    "text": "balancers and our VPN server and so we sort of dodged both of these and it's",
    "start": "2476640",
    "end": "2482400"
  },
  {
    "text": "worked really really well for us  has a notion of key files this is",
    "start": "2482400",
    "end": "2489269"
  },
  {
    "text": "basically authentication between the nodes make sure you're using those and and increasingly with every release this",
    "start": "2489269",
    "end": "2496049"
  },
  {
    "text": "goes deeper and deeper has a fine-grained access control with roles and so if a lot of different users or",
    "start": "2496049",
    "end": "2503880"
  },
  {
    "text": "systems have access to your database definitely look at these and figure out what your your right sort of tackle",
    "start": "2503880",
    "end": "2509349"
  },
  {
    "text": "schema should be and if your advanced and we are we're not yet and to some",
    "start": "2509349",
    "end": "2516099"
  },
  {
    "text": "extent being the VPC helps us avoid this there's Kerberos support and there's",
    "start": "2516099",
    "end": "2521109"
  },
  {
    "text": "also SSL support for MongoDB for for inter-process MongoDB communications and",
    "start": "2521109",
    "end": "2527200"
  },
  {
    "text": "up to the app servers you have to either run MongoDB Enterprise for that or you have to do a custom build yourself this",
    "start": "2527200",
    "end": "2533470"
  },
  {
    "text": "is definitely something that we would love to see as part of the standard builds so okay",
    "start": "2533470",
    "end": "2541630"
  },
  {
    "text": "let's talk a little bit about maintaining it so we've picked picked our instances we've picked our topology",
    "start": "2541630",
    "end": "2547749"
  },
  {
    "text": "we've deployed it now it's time to run the sucker so when it comes to",
    "start": "2547749",
    "end": "2553839"
  },
  {
    "start": "2553000",
    "end": "2553000"
  },
  {
    "text": "monitoring MMS and for the folks that are running how many folks are",
    "start": "2553839",
    "end": "2560559"
  },
  {
    "text": "using MMS just out of curiosity most okay cool it's very good it's it's",
    "start": "2560559",
    "end": "2567400"
  },
  {
    "text": "holistic it's free monitoring there are a bunch of important metrics in here",
    "start": "2567400",
    "end": "2572769"
  },
  {
    "text": "that you want to keep your eye on if you're running a replica set you'll want to keep your eye on on your rep Oleg how",
    "start": "2572769",
    "end": "2578200"
  },
  {
    "text": "long your secondaries are taking to catch up especially if you're running and you know such that you are reading",
    "start": "2578200",
    "end": "2585069"
  },
  {
    "text": "off your secondaries it is an eventually consistent system and so if these are falling further and further behind you becoming less and less eventually",
    "start": "2585069",
    "end": "2591400"
  },
  {
    "text": "consistent page faults the time the time",
    "start": "2591400",
    "end": "2596829"
  },
  {
    "text": "you spent in RN locks these are sort of the top level metrics that we tend to look at there's a bunch of informative",
    "start": "2596829",
    "end": "2602229"
  },
  {
    "text": "ones as well the OP counters the number of connections queue lengths there's a",
    "start": "2602229",
    "end": "2609069"
  },
  {
    "text": "bunch of basic alerting so we alert on repla leg we learn on hosts down we have all that route into",
    "start": "2609069",
    "end": "2615069"
  },
  {
    "text": "our pager duty account so we can keep an eye on that and you can set for metric thresholds and then MMS integrates with",
    "start": "2615069",
    "end": "2624309"
  },
  {
    "text": "the slow query profiler as well so there's something you need to turn on you can set a threshold and basically",
    "start": "2624309",
    "end": "2630099"
  },
  {
    "text": "log every query that exceeds that threshold and duration it's 100 milliseconds by default and and then in",
    "start": "2630099",
    "end": "2636549"
  },
  {
    "text": "MMS you get a very nice GUI right there where you can see which queries are taking time",
    "start": "2636549",
    "end": "2641910"
  },
  {
    "text": "and then there's cloud watch tab watch you know it's kind of actually",
    "start": "2641910",
    "end": "2648330"
  },
  {
    "start": "2643000",
    "end": "2643000"
  },
  {
    "text": "discovering the last couple months that we've stumbled across and the more we play with it the more we like it and the",
    "start": "2648330",
    "end": "2654150"
  },
  {
    "text": "more time we spend with it very nice detailed level resource level or detailed resource level monitoring",
    "start": "2654150",
    "end": "2661640"
  },
  {
    "text": "especially we watch very closely around our volumes now knowing that we had miss provisioned them earlier so we look very",
    "start": "2661640",
    "end": "2666990"
  },
  {
    "text": "closely at the queue length we look very closely at our rewrite Layton sees you",
    "start": "2666990",
    "end": "2673440"
  },
  {
    "text": "get very versatile learning it all integrates with with Amazon simple notification service this is well we",
    "start": "2673440",
    "end": "2679860"
  },
  {
    "text": "have going to an email that comes into page or duty so we get alerts from both systems so that's the the core of how we",
    "start": "2679860",
    "end": "2688560"
  },
  {
    "text": "monitor there of course a lot of other third-party systems out there but those two really provide everything we need",
    "start": "2688560",
    "end": "2695400"
  },
  {
    "text": "for kind of understanding what's going on with the cluster let's talk a little bit about backups so there's a couple",
    "start": "2695400",
    "end": "2703140"
  },
  {
    "text": "different backup strategies the first thing you can do is you can run one of your secondaries on a delay and so when",
    "start": "2703140",
    "end": "2709200"
  },
  {
    "text": "data has changed you can delay it can be five minutes can be an hour and it won't get replicated it's great sort of as a",
    "start": "2709200",
    "end": "2717350"
  },
  {
    "text": "potential sort of fat-finger solution if you can realize in sort of close to real",
    "start": "2717350",
    "end": "2723510"
  },
  {
    "text": "time that you've made a mistake and if you can go to your secondary and extract that data it's it's a good solution I've",
    "start": "2723510",
    "end": "2729540"
  },
  {
    "text": "I've never 100% understood it as a backup solution for us you know a single",
    "start": "2729540",
    "end": "2736230"
  },
  {
    "text": "sort of change to a record it would be great but most of the operations that really sort of scare me from a backup",
    "start": "2736230",
    "end": "2741360"
  },
  {
    "text": "perspective are have side effects and have other collections that get written and basically having to untangle that",
    "start": "2741360",
    "end": "2749010"
  },
  {
    "text": "that diff and bring it back seems like more trouble than it would be worth for",
    "start": "2749010",
    "end": "2754260"
  },
  {
    "text": "Adelaide secondary there you can dump and restore it's a great tool we",
    "start": "2754260",
    "end": "2760560"
  },
  {
    "text": "actually use this a lot for moving subsets of our data into into staging and into development but for your large",
    "start": "2760560",
    "end": "2766800"
  },
  {
    "text": "collections if you're sharded and you've got multiple replicas sets to dump this becomes very impractical quickly",
    "start": "2766800",
    "end": "2774890"
  },
  {
    "text": "mongodb now provides a service it's managed it's secure its point in time I",
    "start": "2775150",
    "end": "2781490"
  },
  {
    "text": "don't know yet we haven't played with it I don't know how suitable it is it's a good question for the manga guys that are here for large deployments",
    "start": "2781490",
    "end": "2788890"
  },
  {
    "text": "it looks like it's expensive for us quite frankly and that's been that our biggest reason we've held off and as",
    "start": "2788890",
    "end": "2794720"
  },
  {
    "text": "instead we've relied on snapshots and I think this this may be you know one of",
    "start": "2794720",
    "end": "2801050"
  },
  {
    "text": "one of the best things about running MongoDB on Amazon Web Services as",
    "start": "2801050",
    "end": "2806810"
  },
  {
    "text": "opposed to doing it yourself is you get this amazing ability of snapshot it's fast it's easy it's scalable when you're",
    "start": "2806810",
    "end": "2813770"
  },
  {
    "text": "first setting it up you have to pay attention to consistency right and that usually especially true if you have a",
    "start": "2813770",
    "end": "2819830"
  },
  {
    "text": "raid configuration if you're starting out and so we have a tool we've open",
    "start": "2819830",
    "end": "2826790"
  },
  {
    "start": "2824000",
    "end": "2824000"
  },
  {
    "text": "sourced it it's called mongauli and Dolly the sheep and it does",
    "start": "2826790",
    "end": "2833869"
  },
  {
    "text": "automatic topology detection just snapshotting it does snapshot management assuming that you are using",
    "start": "2833869",
    "end": "2840380"
  },
  {
    "text": "EBS back MongoDB databases there are a few other assumptions it makes but they're really relatively lightweight",
    "start": "2840380",
    "end": "2846320"
  },
  {
    "text": "and they're primarily centered around limitations where you have to help but",
    "start": "2846320",
    "end": "2851990"
  },
  {
    "text": "find data so mapping specific mount points and volumes and is really the",
    "start": "2851990",
    "end": "2859850"
  },
  {
    "text": "main one you have to end up taking your volumes for it once you've got it all set up it's it's easy as sort of",
    "start": "2859850",
    "end": "2864920"
  },
  {
    "text": "mongauli backup we snapshot our databases hourly and we do it all with",
    "start": "2864920",
    "end": "2870260"
  },
  {
    "text": "this runs in jenkins and then every so often every other day every three days",
    "start": "2870260",
    "end": "2876290"
  },
  {
    "text": "we run the cleanup and so we keep about a week's worth of snapshots as a matter",
    "start": "2876290",
    "end": "2881720"
  },
  {
    "text": "of sort of routine and that's the github for it there would definitely love to",
    "start": "2881720",
    "end": "2888200"
  },
  {
    "text": "have your contributions would definitely love to have you check it out you know",
    "start": "2888200",
    "end": "2894140"
  },
  {
    "text": "the the it's we're running it in production the caveat of course is that this is a this is a critical piece of",
    "start": "2894140",
    "end": "2899540"
  },
  {
    "text": "infrastructure you should you should test it before you like download you",
    "start": "2899540",
    "end": "2905000"
  },
  {
    "text": "know get cloned deploy go to lunch don't do that like at least test it test your backups every so often and and",
    "start": "2905000",
    "end": "2912619"
  },
  {
    "text": "they're a bunch of cool tools the arch stuff that was announced yesterday in",
    "start": "2912619",
    "end": "2919069"
  },
  {
    "text": "the EBS snapshotting talk looks really interesting and so I think there's some good opportunities to maybe coalesce some of these tools as well some uh with",
    "start": "2919069",
    "end": "2928250"
  },
  {
    "start": "2927000",
    "end": "2927000"
  },
  {
    "text": "that just to sort of wrap up some of the conclusions MongoDB and Amazon Web",
    "start": "2928250",
    "end": "2933289"
  },
  {
    "text": "Services together really are sort of a happy Union we're very happy with the",
    "start": "2933289",
    "end": "2941089"
  },
  {
    "text": "combination as to one of the reasons is you really have great options for all deployment and workload sizes and and",
    "start": "2941089",
    "end": "2948859"
  },
  {
    "text": "for us at least as we've scaled out IO has has pretty much been our focal point and there are lots of good options there",
    "start": "2948859",
    "end": "2955789"
  },
  {
    "text": "and those those options are getting better and better by the day and then finally the other thing we've learned",
    "start": "2955789",
    "end": "2961010"
  },
  {
    "text": "from from doing it on the command line when we first were starting through the cob formation templates through where we",
    "start": "2961010",
    "end": "2966500"
  },
  {
    "text": "are today with with having chef manage at all for us is investing in that DevOps strategy taking time to learn it",
    "start": "2966500",
    "end": "2972410"
  },
  {
    "text": "to understand it to write your cookbooks to treat your infrastructure like an appliance that you don't connect",
    "start": "2972410",
    "end": "2978440"
  },
  {
    "text": "directly to it exposes the surface if you if you sort of swallow that pill and",
    "start": "2978440",
    "end": "2984200"
  },
  {
    "text": "make that investment it makes it near effortless and those dividends pay off for a very very long time so with that",
    "start": "2984200",
    "end": "2991910"
  },
  {
    "text": "thank you very much I'm supposed to remind you to to go and do the surveys and give feedback so please do that and",
    "start": "2991910",
    "end": "2999319"
  },
  {
    "text": "I think we've got about ten minutes or so if there are questions so yeah",
    "start": "2999319",
    "end": "3006568"
  }
]