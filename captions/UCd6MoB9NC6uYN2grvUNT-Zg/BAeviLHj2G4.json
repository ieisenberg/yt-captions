[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "hello everyone and welcome to this session about lighting your big data",
    "start": "3500",
    "end": "9300"
  },
  {
    "text": "fire with spark on AWS my name is Russell Nash I'm a Solutions Architect with AWS focusing on the data",
    "start": "9300",
    "end": "16529"
  },
  {
    "text": "space and I'm delighted to be spending the next little while with you talking about spark talking about why it's",
    "start": "16529",
    "end": "22890"
  },
  {
    "text": "become so popular so quickly what are the some of the use cases that our customers are using it for and also the",
    "start": "22890",
    "end": "29970"
  },
  {
    "text": "best the the best practices for using it on AWS so our agenda today is we're",
    "start": "29970",
    "end": "38129"
  },
  {
    "start": "36000",
    "end": "87000"
  },
  {
    "text": "going to take a little bit of a step back at first and we're going to look at the Hadoop family tree and we're going",
    "start": "38129",
    "end": "43260"
  },
  {
    "text": "to do that because that's gonna make it very obvious where spark fits into to",
    "start": "43260",
    "end": "48960"
  },
  {
    "text": "the big data landscape so that we can then we can see what it's actually",
    "start": "48960",
    "end": "54300"
  },
  {
    "text": "replacing and what kind of technologies is complementary with we're then going",
    "start": "54300",
    "end": "59609"
  },
  {
    "text": "to delve a little bit deeper into spark itself and look at all the goodies that",
    "start": "59609",
    "end": "64739"
  },
  {
    "text": "come with it and then we're going to look at Amazon EMR so EMR is managed",
    "start": "64739",
    "end": "71130"
  },
  {
    "text": "Hadoop service I'm gonna look at some of the characteristics of EMR and why that",
    "start": "71130",
    "end": "76140"
  },
  {
    "text": "works so nicely was spark and then we're gonna finally we're gonna look at marriage and we're gonna look at the",
    "start": "76140",
    "end": "81600"
  },
  {
    "text": "marriage of EMR with spark and talk about why that's a good thing so let's",
    "start": "81600",
    "end": "88020"
  },
  {
    "start": "87000",
    "end": "549000"
  },
  {
    "text": "kick off and look at the family tree so if we take a little bit of a step back there was a time when before we had",
    "start": "88020",
    "end": "93960"
  },
  {
    "text": "distributed computing became the norm we there was a time when we would try and",
    "start": "93960",
    "end": "99420"
  },
  {
    "text": "do the as much processes as we could on a single machine and we were really limited by the resources on that machine",
    "start": "99420",
    "end": "106259"
  },
  {
    "text": "so we would obviously have a certain amount of disk on that machine and a certain amount of computer memory that",
    "start": "106259",
    "end": "112350"
  },
  {
    "text": "we had to work with and so as that data set started to grow and some of the things that we wanted to do became a",
    "start": "112350",
    "end": "118259"
  },
  {
    "text": "little bit more complex we would look at making that machine larger and trying to",
    "start": "118259",
    "end": "123540"
  },
  {
    "text": "take advantage of you know bigger instance sizes more CPUs more discs etc",
    "start": "123540",
    "end": "129259"
  },
  {
    "text": "but obviously there's a limit to how far you can push that you can vertically scale and as Grace",
    "start": "129259",
    "end": "136280"
  },
  {
    "text": "Hopper said and she set this back in the 60s so she was well ahead of a time here she said that in Pioneer days they used",
    "start": "136280",
    "end": "144439"
  },
  {
    "text": "oxen for heavy pulling and when one ox couldn't budge a log they didn't try to",
    "start": "144439",
    "end": "149659"
  },
  {
    "text": "grow a bigger ox and she goes on to say after that that really we should be",
    "start": "149659",
    "end": "154700"
  },
  {
    "text": "looking at a network of computers rather than trying to do everything on a single",
    "start": "154700",
    "end": "160760"
  },
  {
    "text": "machine so as I said she was well ahead of a time because she'd said that over 50 years ago but basically we followed",
    "start": "160760",
    "end": "167540"
  },
  {
    "text": "her advice and if you look at what we then tried to do we then tried to grow beyond a single machine and we said okay",
    "start": "167540",
    "end": "175310"
  },
  {
    "text": "let's let's use a cluster of machines why don't we use the disk and the CPU in",
    "start": "175310",
    "end": "181099"
  },
  {
    "text": "the memory on a number of machines and then this gets us past the problem of scaling on one machine and obviously as",
    "start": "181099",
    "end": "186859"
  },
  {
    "text": "we need to grow we can add more machines to this cluster now the in theory that sounds fantastic but there's a couple of",
    "start": "186859",
    "end": "192769"
  },
  {
    "text": "issues a couple of problems that we really need to get around and one of them is that you need something to",
    "start": "192769",
    "end": "197840"
  },
  {
    "text": "really manage this whole process because if you're trying to utilize all the discs on those machines you need",
    "start": "197840",
    "end": "204199"
  },
  {
    "text": "something that's actually going to coordinate where does each piece of data live in this whole cluster and how does",
    "start": "204199",
    "end": "211340"
  },
  {
    "text": "it actually move between the different bits of compute that we've got on the different machines and so that was really why Hadoop was",
    "start": "211340",
    "end": "219650"
  },
  {
    "text": "born to try and solve this problem to really give us a framework to take the complexity out of this for us so that we",
    "start": "219650",
    "end": "227269"
  },
  {
    "text": "could actually just focus on writing applications rather than the minutiae of actually managing the data and the",
    "start": "227269",
    "end": "234260"
  },
  {
    "text": "compute and basically just just get get to it writing things so let's have a",
    "start": "234260",
    "end": "241129"
  },
  {
    "text": "little look at Hadoop from the ground up so so originally obviously at the bottom",
    "start": "241129",
    "end": "247549"
  },
  {
    "text": "of the stack you've got some infrastructure now that might be physical infrastructure if you're doing",
    "start": "247549",
    "end": "252650"
  },
  {
    "text": "this on premise or it might be virtual machines so bc2 if you're running this in in AWS and then on top of that within",
    "start": "252650",
    "end": "261289"
  },
  {
    "text": "layered HDFS so this was our data layer so HDFS is Hadoop's distributed",
    "start": "261289",
    "end": "267020"
  },
  {
    "text": "our system and this gave us the abstraction that we were looking for across the disk on the machines in our",
    "start": "267020",
    "end": "275000"
  },
  {
    "text": "in our cluster so that we're not having to write directly to which machine we can basically just pass the data to HDFS",
    "start": "275000",
    "end": "281060"
  },
  {
    "text": "and it will take care of distributing the data for us the process layer that",
    "start": "281060",
    "end": "286550"
  },
  {
    "text": "was originally designed for her too was called MapReduce and MapReduce was kind",
    "start": "286550",
    "end": "291590"
  },
  {
    "text": "of the first of its al-khattab to really do the magic of of allowing us to it to",
    "start": "291590",
    "end": "297530"
  },
  {
    "text": "write to this distributed framework now the framework itself that sits across",
    "start": "297530",
    "end": "302780"
  },
  {
    "text": "this whole thing and coordinates everything was called Hadoop named after",
    "start": "302780",
    "end": "308349"
  },
  {
    "text": "one of the original writers of Hadoop Doug cutting right named after his son's toy elephant and that became the new",
    "start": "308349",
    "end": "319060"
  },
  {
    "text": "distributed paradigm now as with anything that we introduced in the IT",
    "start": "319060",
    "end": "324380"
  },
  {
    "text": "industry that was fine for a certain amount of time and then people said hey",
    "start": "324380",
    "end": "329539"
  },
  {
    "text": "we really we want to not write to MapReduce directly because it's fairly low-level we're riding in in low-level",
    "start": "329539",
    "end": "336319"
  },
  {
    "text": "languages like Java which is fine if you're a Java developer but if I'm a a",
    "start": "336319",
    "end": "341380"
  },
  {
    "text": "slightly more business facing user I might want to use something that's a bit more business friendly like say",
    "start": "341380",
    "end": "347779"
  },
  {
    "text": "SQL for example so a couple of the first applications that were written the first",
    "start": "347779",
    "end": "353029"
  },
  {
    "text": "was hired which is a sequel like layer across Hadoop and MapReduce which allows",
    "start": "353029",
    "end": "358940"
  },
  {
    "text": "people to write in a sequel like language and that would be then be translated into MapReduce so that you",
    "start": "358940",
    "end": "365270"
  },
  {
    "text": "could get your result sets the other early one was was piqué and piqué provides you with a language that allows",
    "start": "365270",
    "end": "372529"
  },
  {
    "text": "you to transform data and so that was used on very extensively as Wallens and",
    "start": "372529",
    "end": "378529"
  },
  {
    "text": "still is to this day although the other day what the reason that it's called pica actually is because it eats",
    "start": "378529",
    "end": "384289"
  },
  {
    "text": "anything so you can throw pretty much anything at it and it will and it will deal with it so this was this was",
    "start": "384289",
    "end": "391460"
  },
  {
    "text": "obviously fine as as we went along again you know people people were using this quite happily and but wanted to push it",
    "start": "391460",
    "end": "397669"
  },
  {
    "text": "further so as Hadoop evolved when we got - a dupe - there was an extra layer that",
    "start": "397669",
    "end": "403900"
  },
  {
    "text": "was added in here to try and make things a little bit more flexible and that layer was called yarn so yarn stands for",
    "start": "403900",
    "end": "411850"
  },
  {
    "text": "yet another resource negotiator and it's basically a resource manager and what",
    "start": "411850",
    "end": "417850"
  },
  {
    "text": "that allowed us to do was to really break the tight association between",
    "start": "417850",
    "end": "422940"
  },
  {
    "text": "MapReduce and HDFS which really then allowed the the next kind of stage of",
    "start": "422940",
    "end": "428260"
  },
  {
    "text": "evolution for a lot of the Hadoop applications so what then happened was",
    "start": "428260",
    "end": "434290"
  },
  {
    "text": "that MapReduce was then joined by a number of other processing engines that",
    "start": "434290",
    "end": "440080"
  },
  {
    "text": "you could then use within this kind of framework so tears became very popular",
    "start": "440080",
    "end": "446410"
  },
  {
    "text": "so tears was quite quite close to MapReduce in terms of its API so the people could very easily migrate to",
    "start": "446410",
    "end": "452440"
  },
  {
    "text": "tears and offered much better performance than MapReduce and works very seamlessly with some of the",
    "start": "452440",
    "end": "457690"
  },
  {
    "text": "applications like hive and peak and then spark came along as well and spark was a",
    "start": "457690",
    "end": "463030"
  },
  {
    "text": "slightly more general-purpose engine and we'll look later on at some of the facets of it but you can see now this",
    "start": "463030",
    "end": "469270"
  },
  {
    "text": "this Hadoop paradigm really starting to grow in terms of the ecosystem and then",
    "start": "469270",
    "end": "475840"
  },
  {
    "text": "at the top we've got our applications there was an explosion of applications as well so just a couple of examples",
    "start": "475840",
    "end": "482169"
  },
  {
    "text": "that things like presto for example which were giving people very fast sequel access to the the data and HDFS",
    "start": "482169",
    "end": "491110"
  },
  {
    "text": "and also an s3 and then Apache HBase which gives them no sequel type of",
    "start": "491110",
    "end": "496479"
  },
  {
    "text": "functionality and then spark brought with it applications like spike sequel again a",
    "start": "496479",
    "end": "502479"
  },
  {
    "text": "sequel layer and then spark streaming as well which allowed you to deal with very high velocity data coming into the into",
    "start": "502479",
    "end": "509740"
  },
  {
    "text": "the cluster now one of the interesting things about this is that so you can see",
    "start": "509740",
    "end": "516849"
  },
  {
    "text": "now where spark kind of fits into the whole ecosystem and one of the interesting things to say here is that",
    "start": "516849",
    "end": "522610"
  },
  {
    "text": "although is showing spark running here on yarn within the Hadoop framework that's not the only way that you can run",
    "start": "522610",
    "end": "529089"
  },
  {
    "text": "spark so you can also run it standalone so you can actually just download it to your laptop and up and start using it or you can run it",
    "start": "529089",
    "end": "536320"
  },
  {
    "text": "on other resource managers such as me sauce as well so yeah that's not the only way but this tends to be the major",
    "start": "536320",
    "end": "543160"
  },
  {
    "text": "way that most people use farc because of the the Hadoop framework around it so",
    "start": "543160",
    "end": "549850"
  },
  {
    "start": "549000",
    "end": "600000"
  },
  {
    "text": "that's where spike fits in so let's have a little deeper look now into some of",
    "start": "549850",
    "end": "554889"
  },
  {
    "text": "the functionality that spark provides and the reasons why it's become as",
    "start": "554889",
    "end": "561040"
  },
  {
    "text": "popular as it has so anytime you look at any any kind of big data press Farkas is",
    "start": "561040",
    "end": "566199"
  },
  {
    "text": "always mentioned it's talked about a lot there's the spark conferences now a lot of organizations have bet their business",
    "start": "566199",
    "end": "572709"
  },
  {
    "text": "on spark almost every single big data vendor has announced support for spark so it's really starting to become the",
    "start": "572709",
    "end": "579250"
  },
  {
    "text": "the the technology around which the industry is really starting to to coalesce and when you boil it down",
    "start": "579250",
    "end": "586180"
  },
  {
    "text": "there's really two main reasons why spark is as popular as it is and essentially it's because it's fast and",
    "start": "586180",
    "end": "594790"
  },
  {
    "text": "rich which are two facets that are great for anything really but but fantastic",
    "start": "594790",
    "end": "600550"
  },
  {
    "start": "600000",
    "end": "710000"
  },
  {
    "text": "for spark users so if we look at why it's so fast there's a couple of reasons",
    "start": "600550",
    "end": "605829"
  },
  {
    "text": "for this so one is that it makes really good use of the memory within the",
    "start": "605829",
    "end": "611740"
  },
  {
    "text": "cluster now there's a bit of a misconception that it runs everything entirely in memory that's not quite true",
    "start": "611740",
    "end": "616959"
  },
  {
    "text": "but it just makes much more efficient use of the memory within your clusters so as opposed to say MapReduce which",
    "start": "616959",
    "end": "624279"
  },
  {
    "text": "tends to write to disk between between stages and between processes which is",
    "start": "624279",
    "end": "630310"
  },
  {
    "text": "obviously quite a slow process a spark doesn't do that likes to keep keep it's",
    "start": "630310",
    "end": "635380"
  },
  {
    "text": "it's data into memory as much as possible the other reason that it's so fast is because it's also there's a lot",
    "start": "635380",
    "end": "642490"
  },
  {
    "text": "of optimization in the processing that it does so it actually looks at the most",
    "start": "642490",
    "end": "648279"
  },
  {
    "text": "efficient way to run the job that you've given it and make sure that processors don't block each other if they don't",
    "start": "648279",
    "end": "654130"
  },
  {
    "text": "necessarily need to so um so two of the main reasons there while SPARC is is",
    "start": "654130",
    "end": "659410"
  },
  {
    "text": "much faster than the original MapReduce so the other reason that people like",
    "start": "659410",
    "end": "664959"
  },
  {
    "text": "SPARC and the reason that it continues its dominance is that it's extremely rich in the",
    "start": "664959",
    "end": "670120"
  },
  {
    "text": "functionality that you get out of the box so you've probably seen this type of",
    "start": "670120",
    "end": "675430"
  },
  {
    "text": "diagram before so whenever spikes talked about it's always kind of talked about in this in this type of way and so down",
    "start": "675430",
    "end": "681879"
  },
  {
    "text": "the bottom you've got some of the the core spark functionality the core engine",
    "start": "681879",
    "end": "687069"
  },
  {
    "text": "and then on top of that you've got a number of modules which are really designed to give you a lot of",
    "start": "687069",
    "end": "692620"
  },
  {
    "text": "out-of-the-box functionality that again just allow you to focus on the task at",
    "start": "692620",
    "end": "698259"
  },
  {
    "text": "hand rather than getting bogged down into the the details of kind of writing to some of the lower level languages so",
    "start": "698259",
    "end": "704439"
  },
  {
    "text": "let's step through each of these and talk about what they what they give you so if we look at spark core for starters",
    "start": "704439",
    "end": "711000"
  },
  {
    "start": "710000",
    "end": "801000"
  },
  {
    "text": "so essentially if you look at what the core of the engine does it does all of",
    "start": "711000",
    "end": "716980"
  },
  {
    "text": "the distributed processing for you so it will take care of talking to the data",
    "start": "716980",
    "end": "723339"
  },
  {
    "text": "and then abstracting that for use in whatever language you want to use and so if you're looking if you're looking just",
    "start": "723339",
    "end": "728829"
  },
  {
    "text": "at the spark core you can actually write in Python Java and Scala which are three",
    "start": "728829",
    "end": "735069"
  },
  {
    "text": "of the most popular languages for use in this space and that gives you an",
    "start": "735069",
    "end": "740379"
  },
  {
    "text": "enormous amount of flexibility and power to then write your applications the second part of what you get with spark",
    "start": "740379",
    "end": "747069"
  },
  {
    "text": "core is then obviously access to your distributed data sets now the great",
    "start": "747069",
    "end": "752500"
  },
  {
    "text": "thing here is that there's an enormous number of connectors that come with spark that allow you to access these",
    "start": "752500",
    "end": "758740"
  },
  {
    "text": "different data sets so if you look at some of the connectors that come either out of the box or via third parties",
    "start": "758740",
    "end": "765009"
  },
  {
    "text": "you've got connectors which are specific to particular data formats so you've got",
    "start": "765009",
    "end": "770769"
  },
  {
    "text": "connectors for JSON park' Avro and CSV for example but also other engines as",
    "start": "770769",
    "end": "778209"
  },
  {
    "text": "well such as HBase elasticsearch Postgres redshift hive and then",
    "start": "778209",
    "end": "784329"
  },
  {
    "text": "obviously other locations for data as well so obviously HDFS and also s3 there",
    "start": "784329",
    "end": "790420"
  },
  {
    "text": "as well so tremendously rich in terms of allowing you to pretty much get to any",
    "start": "790420",
    "end": "796300"
  },
  {
    "text": "type of data set and then be able to analyze it",
    "start": "796300",
    "end": "801310"
  },
  {
    "start": "801000",
    "end": "875000"
  },
  {
    "text": "now similar to what happened with the dupe and the rise of applications to try",
    "start": "801310",
    "end": "806769"
  },
  {
    "text": "and abstract people from some of the lower-level workings of some of the engines we can see that the the spark",
    "start": "806769",
    "end": "814839"
  },
  {
    "text": "model is very similar so one of the first cab off the rank there as has become very popular now is essentially a",
    "start": "814839",
    "end": "821589"
  },
  {
    "text": "sequel engine a sequel layer across spark and essentially what that gives",
    "start": "821589",
    "end": "827290"
  },
  {
    "text": "you is you're still talking to the same spark core engine you're still using the same connectors to reach your data but",
    "start": "827290",
    "end": "835300"
  },
  {
    "text": "instead of writing in low-level languages you can write in SQL an SQL",
    "start": "835300",
    "end": "841240"
  },
  {
    "text": "for those of you that were worried a few years ago that that sequel is on the way out that that people were talking about",
    "start": "841240",
    "end": "847660"
  },
  {
    "text": "its demise it is absolutely thriving and as I said pretty much any time there's a new a new",
    "start": "847660",
    "end": "854920"
  },
  {
    "text": "technology often the first user interface that they pop over it is some",
    "start": "854920",
    "end": "860319"
  },
  {
    "text": "kind of sequel interface because it is it is so well known and so so many people know how to drive it so spark",
    "start": "860319",
    "end": "867670"
  },
  {
    "text": "sequel is a fantastic way to get into spark without having to to learn a new language so the way that you can use",
    "start": "867670",
    "end": "875189"
  },
  {
    "start": "875000",
    "end": "979000"
  },
  {
    "text": "sequel with spark so you can use the command-line interface so you can just",
    "start": "875189",
    "end": "880800"
  },
  {
    "text": "log in via the command line and write your your raw SQL against the data and",
    "start": "880800",
    "end": "886059"
  },
  {
    "text": "that works very well or you can take a slightly higher level view so you can",
    "start": "886059",
    "end": "891339"
  },
  {
    "text": "step up and you can use a sequel editor so Zeppelin is a very popular one that's",
    "start": "891339",
    "end": "896529"
  },
  {
    "text": "often used with spark or you can actually through a JDBC driver you can",
    "start": "896529",
    "end": "901870"
  },
  {
    "text": "actually attach pretty much any bi tool that's out there and so that's an also",
    "start": "901870",
    "end": "906970"
  },
  {
    "text": "popular way as to again another layer abstraction where you're not even writing sequel you're actually just just",
    "start": "906970",
    "end": "912639"
  },
  {
    "text": "driving the the the user interface to the bi tool so the use cases that we see",
    "start": "912639",
    "end": "919389"
  },
  {
    "text": "here are we see a lot of exploratory type of work trying to understand you",
    "start": "919389",
    "end": "926050"
  },
  {
    "text": "know what what's contained in a data set people trying to kind of get their arms around what's in there running some type",
    "start": "926050",
    "end": "931600"
  },
  {
    "text": "of exploratory sequel but also we see it used for or ETL as well for extract transform and",
    "start": "931600",
    "end": "938230"
  },
  {
    "text": "load type purposes so it's very easy within here to actually set up a table",
    "start": "938230",
    "end": "945160"
  },
  {
    "text": "that points to a data set and then set up another table that points to let's say an empty an empty file system and",
    "start": "945160",
    "end": "952480"
  },
  {
    "text": "then in the process of copying the data between the tables you're actually doing the transformation step there so you may",
    "start": "952480",
    "end": "958840"
  },
  {
    "text": "be transforming between different formats so maybe from JSON to parque for example or you might actually be",
    "start": "958840",
    "end": "965110"
  },
  {
    "text": "transforming the data itself as it moves across into the new table so spikes",
    "start": "965110",
    "end": "971320"
  },
  {
    "text": "equal a fantastic way to do transformation as well as exploring you your dataset through sequel now one of",
    "start": "971320",
    "end": "980560"
  },
  {
    "start": "979000",
    "end": "1119000"
  },
  {
    "text": "the modules that gets a lot of attention with inspark and rightly so is spark streaming so",
    "start": "980560",
    "end": "987640"
  },
  {
    "text": "what we're finding with our customer base is that everybody is starting to look at high velocity data streaming",
    "start": "987640",
    "end": "995560"
  },
  {
    "text": "data whether that's coming from whether that's coming from a click stream so whether it's people clicking on your website or whether it's devices that are",
    "start": "995560",
    "end": "1004620"
  },
  {
    "text": "sending information to you very quickly you need to be able to not only capture",
    "start": "1004620",
    "end": "1011580"
  },
  {
    "text": "that information but also then obviously analyze it in flight as well so what",
    "start": "1011580",
    "end": "1017520"
  },
  {
    "text": "sparks streaming does for you is it helps you actually to to analyze that data as it comes in so what typically",
    "start": "1017520",
    "end": "1024240"
  },
  {
    "text": "happens is that you have some kind of capture mechanism at the front end so",
    "start": "1024240",
    "end": "1029939"
  },
  {
    "text": "those of you familiar with our ws might have seen Amazon Kinesis so this is a a",
    "start": "1029940",
    "end": "1037069"
  },
  {
    "text": "streaming engine that allows you to take in a large amount of messages through the front door",
    "start": "1037070",
    "end": "1043189"
  },
  {
    "text": "Apache Kafka is is another very popular open source very similar in in",
    "start": "1043190",
    "end": "1048449"
  },
  {
    "text": "architecture those two and what they do is they will capture the data for you and then make it available for you to",
    "start": "1048450",
    "end": "1054510"
  },
  {
    "text": "then consume with some kind of application and there's a number of ways that you can do that but one of the most",
    "start": "1054510",
    "end": "1060450"
  },
  {
    "text": "popular ways is to actually have a spark cluster with spark streaming running so",
    "start": "1060450",
    "end": "1067020"
  },
  {
    "text": "spark streaming will turn the stream into a number of micro",
    "start": "1067020",
    "end": "1072660"
  },
  {
    "text": "batches and what those micro batches do is they then allow you to to act on",
    "start": "1072660",
    "end": "1077880"
  },
  {
    "text": "those and so then later on in the pipeline you'll then use one of the",
    "start": "1077880",
    "end": "1083820"
  },
  {
    "text": "other spark engines whether it's spikes equal or or course bark to actually then",
    "start": "1083820",
    "end": "1089460"
  },
  {
    "text": "analyze those micro batches and then push out whatever results you're after so you can start to see that one of the",
    "start": "1089460",
    "end": "1096900"
  },
  {
    "text": "main attractions of spark is that it does everything in that one package so that you're not doing something",
    "start": "1096900",
    "end": "1103370"
  },
  {
    "text": "different after you've done the spike streaming you're staying within the spark engine and using some of the other",
    "start": "1103370",
    "end": "1108960"
  },
  {
    "text": "and facets of it to actually analyze the data so spark streaming extremely popular amongst a lot of our customers",
    "start": "1108960",
    "end": "1117650"
  },
  {
    "start": "1119000",
    "end": "1185000"
  },
  {
    "text": "so one of the other modules is actually spark R so R has become an extremely",
    "start": "1119360",
    "end": "1126570"
  },
  {
    "text": "popular language in recent times for more statistical type of analysis so gives you a richer set of functionality",
    "start": "1126570",
    "end": "1134429"
  },
  {
    "text": "than sequel does in the in the corner statistical space and one of the",
    "start": "1134429",
    "end": "1139700"
  },
  {
    "text": "limitations of R is that you are often limited by the amount of memory that you",
    "start": "1139700",
    "end": "1145170"
  },
  {
    "text": "have on a particular machine and so people want to run are on a much larger distributed data set and so spark can",
    "start": "1145170",
    "end": "1152730"
  },
  {
    "text": "actually help you do that and so again if you look at the you know that the the",
    "start": "1152730",
    "end": "1158190"
  },
  {
    "text": "core type of spark or talking to the",
    "start": "1158190",
    "end": "1163320"
  },
  {
    "text": "data sets down the bottom spark them a",
    "start": "1163320",
    "end": "1168690"
  },
  {
    "text": "delay to have R on the top that can then access that data so again just allowing users who are",
    "start": "1168690",
    "end": "1175710"
  },
  {
    "text": "familiar with spark familiar with our who love our can actually stay within the spark framework and and analyze",
    "start": "1175710",
    "end": "1182490"
  },
  {
    "text": "their data now another very popular",
    "start": "1182490",
    "end": "1189059"
  },
  {
    "start": "1185000",
    "end": "1497000"
  },
  {
    "text": "module within spark is spark ml now there's actually two machine learning",
    "start": "1189059",
    "end": "1194429"
  },
  {
    "text": "libraries in here there's one called spark ml another one called ml Lib and they're both involved in allowing you",
    "start": "1194429",
    "end": "1201600"
  },
  {
    "text": "to essentially create machine learning models with your data and then actually",
    "start": "1201600",
    "end": "1207270"
  },
  {
    "text": "then have you you run new data sets through your model and it for it to make",
    "start": "1207270",
    "end": "1212669"
  },
  {
    "text": "predictions for you so if we look at how that works so if you look at a typical machine learning pipeline inside spark",
    "start": "1212669",
    "end": "1221010"
  },
  {
    "text": "or outside the way you typically do it is you take your training data set so",
    "start": "1221010",
    "end": "1226650"
  },
  {
    "text": "let's say that you're looking to predict a particular particular variable so what you do first is you need to train the",
    "start": "1226650",
    "end": "1233039"
  },
  {
    "text": "model so that it actually knows what patterns lead to the particular outcome",
    "start": "1233039",
    "end": "1238200"
  },
  {
    "text": "that you're looking for so for example if you were training a model to look for fraudulent credit card transactions in",
    "start": "1238200",
    "end": "1244320"
  },
  {
    "text": "your training data set you would have a number of transactions with all the different variables so time place",
    "start": "1244320",
    "end": "1251280"
  },
  {
    "text": "merchant etc etcetera and then you'd",
    "start": "1251280",
    "end": "1256409"
  },
  {
    "text": "also have in that data set whether each of the transactions was actually fraudulent or not so essentially you're",
    "start": "1256409",
    "end": "1263370"
  },
  {
    "text": "giving the the model the answers which allows it to then work backwards from the answers and look at the actual",
    "start": "1263370",
    "end": "1269039"
  },
  {
    "text": "variables and say what are the patterns that we're seeing here that leads to a fraudulent outcome so that's what you",
    "start": "1269039",
    "end": "1276960"
  },
  {
    "text": "typically do and that typically involves quite a lot of data preparation because you have to plan as your data set you have to make sure that you've got the",
    "start": "1276960",
    "end": "1283049"
  },
  {
    "text": "right variables sitting in the data set that that are relevant for the model etc",
    "start": "1283049",
    "end": "1288539"
  },
  {
    "text": "so there's a bit of preparation to be done there and once you've got that you then typically split that training data",
    "start": "1288539",
    "end": "1293929"
  },
  {
    "text": "two ways so 70 or 80 percent of the data actually goes into training the model so",
    "start": "1293929",
    "end": "1302039"
  },
  {
    "text": "that's the data set that the model uses to train itself but then the 30 percent that you hold back you then use to",
    "start": "1302039",
    "end": "1308850"
  },
  {
    "text": "actually test the model so again because it's got the answers in it it means that",
    "start": "1308850",
    "end": "1314039"
  },
  {
    "text": "the model can then run that that new data set against the the new model that was created and then look at the answers",
    "start": "1314039",
    "end": "1321030"
  },
  {
    "text": "that it predicted versus the actual answers to see whether the model is actually accurate or not if it is if yet",
    "start": "1321030",
    "end": "1329370"
  },
  {
    "text": "if you're happy with that model you then create the model and then that model then sit there and allow you to push new",
    "start": "1329370",
    "end": "1337440"
  },
  {
    "text": "data into it so in our fraud transactions example you'd be putting new data through that obviously you",
    "start": "1337440",
    "end": "1343770"
  },
  {
    "text": "don't know if they're fraudulent or not and then the model will then make a prediction for you now you can do that",
    "start": "1343770",
    "end": "1350250"
  },
  {
    "text": "in batch so if you've got a whole a large file that you want to actually run",
    "start": "1350250",
    "end": "1355590"
  },
  {
    "text": "predictions across you can do that but what's becoming more common is that you actually want that predictive model to",
    "start": "1355590",
    "end": "1361470"
  },
  {
    "text": "run in near-real-time so you actually have near real-time data coming in so again in our transaction",
    "start": "1361470",
    "end": "1367320"
  },
  {
    "text": "type of example as someone actually ran their credit card through the machine",
    "start": "1367320",
    "end": "1372390"
  },
  {
    "text": "that would then send that off in near real time the model would then very quickly tell you whether it predicted",
    "start": "1372390",
    "end": "1378720"
  },
  {
    "text": "that that was a fortunate transaction or not so the prediction would pop out the",
    "start": "1378720",
    "end": "1384270"
  },
  {
    "text": "back and that's a fairly generic type of machine learning type of pipeline now",
    "start": "1384270",
    "end": "1389820"
  },
  {
    "text": "what I wanted to overlay on this for you was the different SPARC components that might be involved in this so typically",
    "start": "1389820",
    "end": "1395490"
  },
  {
    "text": "we see that this initial data preparation step is often served really",
    "start": "1395490",
    "end": "1401789"
  },
  {
    "text": "well by spikes equal so with spikes equal as I said before you can transform the data you can you can cleanse it you",
    "start": "1401789",
    "end": "1409169"
  },
  {
    "text": "can make sure that you've got the fields that you need and not the ones that you don't you can potentially split fields",
    "start": "1409169",
    "end": "1415350"
  },
  {
    "text": "up if you need to coalesce them together whatever makes sense for your for your model then the actual splitting of the",
    "start": "1415350",
    "end": "1423299"
  },
  {
    "text": "data the training the testing and then the creation of the model and also the",
    "start": "1423299",
    "end": "1429840"
  },
  {
    "text": "predictions themselves are going to be done by spark ml or ml lib and so those",
    "start": "1429840",
    "end": "1436380"
  },
  {
    "text": "libraries sit in there to help you to to do that and then if you're going to be",
    "start": "1436380",
    "end": "1441720"
  },
  {
    "text": "pushing near real-time data through then that's where spark streaming and can",
    "start": "1441720",
    "end": "1446909"
  },
  {
    "text": "help you do that and so I think this is a really great example of the one of the",
    "start": "1446909",
    "end": "1452970"
  },
  {
    "text": "attractions of spark is that each of these components you could do them in other technologies but the attraction of",
    "start": "1452970",
    "end": "1460350"
  },
  {
    "text": "spark is that you can do them all in one place and so one of the things that a lot of data scientists have an issue",
    "start": "1460350",
    "end": "1467070"
  },
  {
    "text": "with is that they a lot of their time preparing data or moving data between different systems",
    "start": "1467070",
    "end": "1472590"
  },
  {
    "text": "rather than actually doing what they really want to be doing which is actually training and testing their models and so with inspark this makes",
    "start": "1472590",
    "end": "1481830"
  },
  {
    "text": "their job much much easier so you can stay within the spark cluster you can use spikes equal to prep your data spark",
    "start": "1481830",
    "end": "1488070"
  },
  {
    "text": "ml to create your models and then spark streaming to push your new data through that model it's a great example of the",
    "start": "1488070",
    "end": "1494370"
  },
  {
    "text": "under para spark now one of the the lesser-known modules",
    "start": "1494370",
    "end": "1500280"
  },
  {
    "start": "1497000",
    "end": "1663000"
  },
  {
    "text": "of spark and probably because it's really still in its infancy infancy but",
    "start": "1500280",
    "end": "1505740"
  },
  {
    "text": "I wanted to mention here just for just for completeness is graph X now graph X is a graph processing engine that can",
    "start": "1505740",
    "end": "1514590"
  },
  {
    "text": "help you to analyze the relationships between entities so if you think about a",
    "start": "1514590",
    "end": "1520650"
  },
  {
    "text": "typical graph and now these entities could be people they could be places",
    "start": "1520650",
    "end": "1528210"
  },
  {
    "text": "they could be nodes on a network anything anything that's got relationships between them and can then",
    "start": "1528210",
    "end": "1534090"
  },
  {
    "text": "be modeled into a graph and graph databases and graph processing engines have become very popular recently to do",
    "start": "1534090",
    "end": "1540900"
  },
  {
    "text": "with a whole bunch of types of processing that are slightly more difficult to do with with relational",
    "start": "1540900",
    "end": "1546630"
  },
  {
    "text": "technologies now there's a little bit of a nuance in the in the in the graphing",
    "start": "1546630",
    "end": "1552030"
  },
  {
    "text": "world here which I think often gets glossed over and that is that very similar to the way that we look at",
    "start": "1552030",
    "end": "1558810"
  },
  {
    "text": "sequel databases there's also that kind of split in graph technologies as well",
    "start": "1558810",
    "end": "1564840"
  },
  {
    "text": "so if you think about the sequel world you've got your transaction databases so",
    "start": "1564840",
    "end": "1570690"
  },
  {
    "text": "your more sequels and your post quest engines etc and then you've got your databases that are more focused on",
    "start": "1570690",
    "end": "1576840"
  },
  {
    "text": "analytics so some of the MPP databases like Reggie for example and the same is",
    "start": "1576840",
    "end": "1582210"
  },
  {
    "text": "true in the graph world as well so some of the graph databases that you would have heard about things like neo4j and",
    "start": "1582210",
    "end": "1589020"
  },
  {
    "text": "an Orion DB they're typically more focused on transaction processing so you",
    "start": "1589020",
    "end": "1595530"
  },
  {
    "text": "know adding nodes to the to the graph doing lookups in the in the graphic cetera",
    "start": "1595530",
    "end": "1601080"
  },
  {
    "text": "then you have the graph processing engines and these typically work in more of a batch fashion where you're actually",
    "start": "1601080",
    "end": "1607620"
  },
  {
    "text": "running some analysis across the network so you know how many levels is this",
    "start": "1607620",
    "end": "1613530"
  },
  {
    "text": "particular entity separated from a different entity things like that things",
    "start": "1613530",
    "end": "1618750"
  },
  {
    "text": "where you have to to do a lot of a lot of scanning a lot of a lot of hops between the different nodes and so",
    "start": "1618750",
    "end": "1625710"
  },
  {
    "text": "graphics falls into that second category so it's a graph processing engine as opposed to a graph database like say",
    "start": "1625710",
    "end": "1632970"
  },
  {
    "text": "neo4j and so these two technologies typically would work together where you might use neo4j to actually be your kind",
    "start": "1632970",
    "end": "1639750"
  },
  {
    "text": "of online transaction graph database that you would use day-to-day to add nodes and update the the network and",
    "start": "1639750",
    "end": "1647400"
  },
  {
    "text": "then the graphics processing would then work offline to actually run those those",
    "start": "1647400",
    "end": "1653370"
  },
  {
    "text": "deep deep queries for you so as I said early days for graphics but I think you'll hear you'll hear a lot more from",
    "start": "1653370",
    "end": "1659370"
  },
  {
    "text": "that as it matures so that's the the",
    "start": "1659370",
    "end": "1666000"
  },
  {
    "start": "1663000",
    "end": "1773000"
  },
  {
    "text": "essence of SPARC and the different modules that come with spark so I wanted to give you a good overview of of the",
    "start": "1666000",
    "end": "1671310"
  },
  {
    "text": "different modules they're what I want to do now is just move on to Amazon EMR and",
    "start": "1671310",
    "end": "1676320"
  },
  {
    "text": "talk about that a little bit before we talk about why the two of them are so good together so if we go back to our",
    "start": "1676320",
    "end": "1682410"
  },
  {
    "text": "original graphic we were looking at at the Hadoop stack what EMR does for you",
    "start": "1682410",
    "end": "1689460"
  },
  {
    "text": "is instead of you installing the applications installing Hadoop having to",
    "start": "1689460",
    "end": "1695850"
  },
  {
    "text": "provision the underlying hardware and making sure that all of the nodes are talking to each other and that all the",
    "start": "1695850",
    "end": "1702840"
  },
  {
    "text": "all the ports are the right ports are open and everything's installed correctly and that's what EMR will do",
    "start": "1702840",
    "end": "1707910"
  },
  {
    "text": "for you so if you want to run a Hadoop cluster you would talk to the EMR console and you would say I want a",
    "start": "1707910",
    "end": "1714750"
  },
  {
    "text": "cluster of this particular size and I want these applications installed and then EMR will go away and do that for",
    "start": "1714750",
    "end": "1721290"
  },
  {
    "text": "you it will talk to easy to provision the instances it will install everything needs to be installed etc and you can",
    "start": "1721290",
    "end": "1730800"
  },
  {
    "text": "then obviously run all of the other applications that you need on top so that's basically what AMR does",
    "start": "1730800",
    "end": "1736950"
  },
  {
    "text": "and what I wanted to do now is just step through why that's a good thing so what are the some of the things that you get",
    "start": "1736950",
    "end": "1742350"
  },
  {
    "text": "with EMR that you don't get if you're running it yourself so the first is automation we'll look at what you get in",
    "start": "1742350",
    "end": "1750660"
  },
  {
    "text": "terms of automation the ability to D couple so the ability to decouple your processing layer from your disk layer",
    "start": "1750660",
    "end": "1757950"
  },
  {
    "text": "we'll look at elasticity integration with other Amazon engines the currency",
    "start": "1757950",
    "end": "1764970"
  },
  {
    "text": "of the open-source tools that sit on top of EMR and then obviously also being",
    "start": "1764970",
    "end": "1770130"
  },
  {
    "text": "able to run some of this at quite a low cost so let's look at automation so as I",
    "start": "1770130",
    "end": "1776580"
  },
  {
    "start": "1773000",
    "end": "1891000"
  },
  {
    "text": "said one of the the main reasons that customers like EMR is that a lot of the",
    "start": "1776580",
    "end": "1782370"
  },
  {
    "text": "work involved in setting up a Hadoop cluster is taken care of for them it's a",
    "start": "1782370",
    "end": "1787380"
  },
  {
    "text": "lot of the heavy lifting involved in creating the cluster is done for you so firstly obviously there's the",
    "start": "1787380",
    "end": "1792390"
  },
  {
    "text": "provisioning of the infrastructure itself so provisioning the ec2 instances EMI will do that the actual cluster",
    "start": "1792390",
    "end": "1799830"
  },
  {
    "text": "setup so making sure that everything is installed correctly on those on those",
    "start": "1799830",
    "end": "1804960"
  },
  {
    "text": "machines and making sure that each of the nodes is is talking to each other correctly the actual configuration of",
    "start": "1804960",
    "end": "1813090"
  },
  {
    "text": "the Hadoop cluster itself so there's a bunch of configuration options when you spin up Hadoop that you also have to",
    "start": "1813090",
    "end": "1819290"
  },
  {
    "text": "keep in mind so email will take care of that for you as well and then obviously there's the installing of all of the",
    "start": "1819290",
    "end": "1825720"
  },
  {
    "text": "applications so the the huge menagerie of different tools and animals and",
    "start": "1825720",
    "end": "1832310"
  },
  {
    "text": "zookeepers and you name it you can do that through through EMR as well it will",
    "start": "1832310",
    "end": "1837420"
  },
  {
    "text": "then install those tools for you it will also allow you to then submit jobs at",
    "start": "1837420",
    "end": "1843450"
  },
  {
    "text": "the time that you create the cluster so the use case we often see there with customers is that they have a very",
    "start": "1843450",
    "end": "1848490"
  },
  {
    "text": "specific task that they want done so they'll actually spin up a cluster and submit that job at the time that the",
    "start": "1848490",
    "end": "1855060"
  },
  {
    "text": "cluster comes up and then that cluster will run that job and then it will shut itself down very specific tasks but you",
    "start": "1855060",
    "end": "1862710"
  },
  {
    "text": "can obviously that can all be automated and then finally obviously monitoring",
    "start": "1862710",
    "end": "1867930"
  },
  {
    "text": "and failure handling so if one of the nodes falls over for any reason then EMI will monitor that and make sure that",
    "start": "1867930",
    "end": "1874380"
  },
  {
    "text": "that node comes back up so that automation component is really key to",
    "start": "1874380",
    "end": "1879570"
  },
  {
    "text": "making running a hood it cluster much easier than at can--but than it has been",
    "start": "1879570",
    "end": "1885120"
  },
  {
    "text": "typically and allows you to much more quickly get to the business of actually using the cluster itself so let's look a",
    "start": "1885120",
    "end": "1893640"
  },
  {
    "start": "1891000",
    "end": "2213000"
  },
  {
    "text": "look at decoupling so what do we mean by that so let's if we go back to our graphic one of the issues that a lot of",
    "start": "1893640",
    "end": "1900390"
  },
  {
    "text": "customers ran into when they were running a Hadoop cluster with HDFS as the as the sole place where they were",
    "start": "1900390",
    "end": "1907740"
  },
  {
    "text": "storing the data was that they then found that they had locked their compute",
    "start": "1907740",
    "end": "1913050"
  },
  {
    "text": "layer with their storage layer so because both of those layers were sitting on eventually on on the ammos",
    "start": "1913050",
    "end": "1920400"
  },
  {
    "text": "machines in the cluster if they needed to add more disk to the cluster they'd have to add more machines and those",
    "start": "1920400",
    "end": "1926640"
  },
  {
    "text": "machines would obviously come with extra CPU and extra memory which they had to pay for but didn't necessarily need so",
    "start": "1926640",
    "end": "1934020"
  },
  {
    "text": "what we wanted to do with the email was make sure that customers could use a different external storage layer that",
    "start": "1934020",
    "end": "1941880"
  },
  {
    "text": "was highly durable highly scalable low cost etc and use that with their Hadoop",
    "start": "1941880",
    "end": "1948330"
  },
  {
    "text": "clusters so EMR brings with it something called EMR FS which is EMRs file system",
    "start": "1948330",
    "end": "1955170"
  },
  {
    "text": "and this allows you to talk to s3 from your hadoop cluster and this then allows",
    "start": "1955170",
    "end": "1961620"
  },
  {
    "text": "you to successfully D couple your computer from your storage layer so you",
    "start": "1961620",
    "end": "1967530"
  },
  {
    "text": "can just load data into s3 as you need to so if you need more disk you just you just simply load more data in there's no",
    "start": "1967530",
    "end": "1973350"
  },
  {
    "text": "need to pre provision with s3 but you can then keep your cluster the same size",
    "start": "1973350",
    "end": "1979350"
  },
  {
    "text": "so you're not having to grow the computer on your cluster unnecessarily",
    "start": "1979350",
    "end": "1984470"
  },
  {
    "text": "now there's a couple of other really important benefits that come along with that and the main one is that now that",
    "start": "1984470",
    "end": "1990540"
  },
  {
    "text": "you've got your data and s3 and not locked into the cluster it means that you can actually start to spin up",
    "start": "1990540",
    "end": "1996740"
  },
  {
    "text": "separate clusters for different tasks so if you imagine you've got a situation where you've got your data and s3 you",
    "start": "1996740",
    "end": "2003990"
  },
  {
    "text": "might then have a persistent cluster that's running all the time that's allowing people to interactively query",
    "start": "2003990",
    "end": "2009990"
  },
  {
    "text": "the data in s3 either through spikes equal or presto or hi or any of the other interactive query engines but when",
    "start": "2009990",
    "end": "2018930"
  },
  {
    "text": "you want to run your batch jobs so maybe your ETL jobs your transformational jobs you can actually then spin up a",
    "start": "2018930",
    "end": "2025320"
  },
  {
    "text": "transient cluster so that doesn't have to run on the persistent cluster it doesn't have to interfere with the",
    "start": "2025320",
    "end": "2030600"
  },
  {
    "text": "interactive queries that are going on you simply spin up a cluster it's completely independent and then you run",
    "start": "2030600",
    "end": "2036570"
  },
  {
    "text": "that for as many hours as you need and then you shut it down and a lot of",
    "start": "2036570",
    "end": "2041880"
  },
  {
    "text": "customers follow this pattern and it's incredibly powerful because it allows you to really focus on making sure that",
    "start": "2041880",
    "end": "2048659"
  },
  {
    "text": "those clusters are dedicated for certain workloads and then you might have other workloads specific clusters and as we",
    "start": "2048660",
    "end": "2056010"
  },
  {
    "text": "say here that could be different sizes they could be different versions of some of the tools so if you've got a",
    "start": "2056010",
    "end": "2061560"
  },
  {
    "text": "dependency on an older version of the tool that's fine you spin up a specific cluster for that and so this gives you",
    "start": "2061560",
    "end": "2067830"
  },
  {
    "text": "incredible flexibility with the way that you start to use Hadoop and all that and all the applications as well now one of",
    "start": "2067830",
    "end": "2077129"
  },
  {
    "text": "the questions that then comes up is okay so if I'm using separate clusters then how do I ensure that tables I create for",
    "start": "2077130",
    "end": "2085320"
  },
  {
    "text": "example in my batch job are then available for people to to query with",
    "start": "2085320",
    "end": "2090750"
  },
  {
    "text": "the persistent cluster well this is where you use an external meta store so one of the things you can do when you",
    "start": "2090750",
    "end": "2096179"
  },
  {
    "text": "spin up a Hadoop clusters you can specify where the meta store is and so what what's common practice now is to",
    "start": "2096180",
    "end": "2102210"
  },
  {
    "text": "use an external meta store like a my sequel database and if you're running in amazon we'd recommend they put that on a",
    "start": "2102210",
    "end": "2108450"
  },
  {
    "text": "RDS so that it's it's managed by us and then you can make that highly available so that then as you spin up a cluster as",
    "start": "2108450",
    "end": "2115950"
  },
  {
    "text": "part of the configuration you simply say your first job is to then point to the meta store and make sure that you've got",
    "start": "2115950",
    "end": "2121980"
  },
  {
    "text": "access to all of the configuration all the tables that have been created and that's how you can share data between",
    "start": "2121980",
    "end": "2128520"
  },
  {
    "text": "different clusters and also different applications as well",
    "start": "2128520",
    "end": "2133690"
  },
  {
    "text": "now one of the other aspects to this then is is if you're bringing up different clusters for different",
    "start": "2133690",
    "end": "2139250"
  },
  {
    "text": "workloads you then got the ability to be much more focused with the instance types that you're using so as you know",
    "start": "2139250",
    "end": "2146359"
  },
  {
    "text": "with ec2 you've got a lot of different families of ec2 machines we've got general-purpose such as the m4s the m3s",
    "start": "2146359",
    "end": "2153490"
  },
  {
    "text": "compute C force and C 3 s and then heavier memory ones and then obviously instances with more storage and for",
    "start": "2153490",
    "end": "2161089"
  },
  {
    "text": "those of you who are watching the keynote this morning from re-invent you would have seen that there's a whole bunch of other additions to this that",
    "start": "2161089",
    "end": "2167990"
  },
  {
    "text": "are coming that are coming shortly to make the family even bigger but they're",
    "start": "2167990",
    "end": "2173059"
  },
  {
    "text": "nice thing about this from a big data perspective is that a lot of the types of workloads that we see mapped to these",
    "start": "2173059",
    "end": "2179660"
  },
  {
    "text": "quite well so batch process is obviously a pretty unfussy with what they what they use but machine learning is very",
    "start": "2179660",
    "end": "2185930"
  },
  {
    "text": "compute intensive so it loves machines with a lot of compute on them and then",
    "start": "2185930",
    "end": "2192529"
  },
  {
    "text": "you've got interactive analysis which is big on memory and then obviously if you do want to run a large HDFS cluster you",
    "start": "2192529",
    "end": "2198440"
  },
  {
    "text": "can do that with the bigger storage instances as well so again that",
    "start": "2198440",
    "end": "2204589"
  },
  {
    "text": "decoupling gives you that incredible flexibility with being able to have very",
    "start": "2204589",
    "end": "2210589"
  },
  {
    "text": "targeted types of clusters and then in terms of elasticity so one of the things",
    "start": "2210589",
    "end": "2217730"
  },
  {
    "start": "2213000",
    "end": "2289000"
  },
  {
    "text": "that is nice about using EMR is that you can really be very elastic with it as well so yes you can spin up the",
    "start": "2217730",
    "end": "2224420"
  },
  {
    "text": "classroom you can shut it down if you want to but some customers also want to keep it running but it is used at for",
    "start": "2224420",
    "end": "2232369"
  },
  {
    "text": "different purposes at different times so you can scale a cluster up if you need to so you can add add nodes to the",
    "start": "2232369",
    "end": "2238789"
  },
  {
    "text": "cluster you can scale it down again and a feature that we've recently added you can actually auto scale it as well so",
    "start": "2238789",
    "end": "2245180"
  },
  {
    "text": "similar to what you can do with ec2 so with an ec2 auto scaling group you can",
    "start": "2245180",
    "end": "2250339"
  },
  {
    "text": "say you know when the when a certain metric on the machine meets a certain threshold for a specified period of time",
    "start": "2250339",
    "end": "2257559"
  },
  {
    "text": "then you want an automatic action to happen you either want to scale that group up or scale it down and you can do",
    "start": "2257559",
    "end": "2264049"
  },
  {
    "text": "that with with EMR now as well so you can actually say you know when the",
    "start": "2264049",
    "end": "2269690"
  },
  {
    "text": "amount of memory on the machines at a certain level or the CPUs at a certain level for a period of time then either",
    "start": "2269690",
    "end": "2275030"
  },
  {
    "text": "add or subtract nodes from this cluster and again this makes it incredibly flexible and gives customers that kind",
    "start": "2275030",
    "end": "2281690"
  },
  {
    "text": "of very very flexible compute power that they haven't previously had with their",
    "start": "2281690",
    "end": "2287810"
  },
  {
    "text": "Hadoop clusters now in terms of integration obviously we've got close",
    "start": "2287810",
    "end": "2294560"
  },
  {
    "start": "2289000",
    "end": "2365000"
  },
  {
    "text": "integration with EMR and other AWS services so let's assume we have an EMI",
    "start": "2294560",
    "end": "2300230"
  },
  {
    "text": "cluster with SPARC running we've talked about AM RFS obviously you get very close to integration with s3 but you can",
    "start": "2300230",
    "end": "2307369"
  },
  {
    "text": "also connect up SPARC streaming to Amazon Kinesis very easily so that's how",
    "start": "2307369",
    "end": "2312530"
  },
  {
    "text": "you're able to capture your streaming data you can also connect into redshift so redshift is a petabyte scale MPP",
    "start": "2312530",
    "end": "2319730"
  },
  {
    "text": "sequel analytics database so you can use those two in conjunction as well you can then obviously wrap your cluster",
    "start": "2319730",
    "end": "2329210"
  },
  {
    "text": "with iam roles so that you can give the roles certain permissions that that the",
    "start": "2329210",
    "end": "2335660"
  },
  {
    "text": "cluster can then inherit but you then you don't that means you don't specifically need to give those permissions to users themselves you can",
    "start": "2335660",
    "end": "2342350"
  },
  {
    "text": "you can shield them behind the roles and also crucially you can also then start",
    "start": "2342350",
    "end": "2348800"
  },
  {
    "text": "to use Amazon kms so that's our key management service that will allow you to manage your encryption keys if you",
    "start": "2348800",
    "end": "2356450"
  },
  {
    "text": "want to encrypt the data at rest in s 3/4 EMR to very easily encrypt and",
    "start": "2356450",
    "end": "2362780"
  },
  {
    "text": "decrypt without term without too much drama now one of the really important",
    "start": "2362780",
    "end": "2370220"
  },
  {
    "start": "2365000",
    "end": "2441000"
  },
  {
    "text": "things for for a lot of customers is making sure that whatever technology",
    "start": "2370220",
    "end": "2376700"
  },
  {
    "text": "they use that's running their Hadoop cluster it allows them to use the most",
    "start": "2376700",
    "end": "2381950"
  },
  {
    "text": "current of the open source tools so there's a lot of development and a lot",
    "start": "2381950",
    "end": "2388520"
  },
  {
    "text": "of work going on in the open source community and so we want to make sure that we can get that into your hands as quickly as possible and so we've done a",
    "start": "2388520",
    "end": "2396500"
  },
  {
    "text": "lot of work with EMR over the last couple of years to make sure that we can do this and if you look at some of the dates here",
    "start": "2396500",
    "end": "2401550"
  },
  {
    "text": "specifically for spark you can see that the speed with which we can get those",
    "start": "2401550",
    "end": "2408780"
  },
  {
    "text": "changes and those new updates into your into your hands is really quite quick so if you look at spike 2.0 - which is the",
    "start": "2408780",
    "end": "2417030"
  },
  {
    "text": "latest version of spark that was released by the open-source community on November the 14th and we got that into",
    "start": "2417030",
    "end": "2424109"
  },
  {
    "text": "EMR fully supported with EMR within a week of that release so that really",
    "start": "2424109",
    "end": "2429780"
  },
  {
    "text": "isn't extremely important for a lot of customers who need to make sure that",
    "start": "2429780",
    "end": "2435540"
  },
  {
    "text": "they can they can use the latest versions of those tools as they come out",
    "start": "2435540",
    "end": "2442369"
  },
  {
    "text": "now all of these features are well and good but obviously cost is a factor and",
    "start": "2443540",
    "end": "2449640"
  },
  {
    "text": "you want to make sure that you can do this in a low-cost manner so there's a number of ways that you can keep your",
    "start": "2449640",
    "end": "2456170"
  },
  {
    "text": "added costs as low as possible with it with EMR one of them obviously we've",
    "start": "2456170",
    "end": "2461880"
  },
  {
    "text": "talked about which is to use transient clusters so you're not locked in to having a running cluster all the time",
    "start": "2461880",
    "end": "2467430"
  },
  {
    "text": "you can essentially spin it up and down as need be but also if you do have a",
    "start": "2467430",
    "end": "2473640"
  },
  {
    "text": "persistent cluster you can use reserved instances so of many of you would be familiar with reserved instances on ec2",
    "start": "2473640",
    "end": "2480720"
  },
  {
    "text": "so that's where you basically say look I know I'm going to use this instance for a year or three years and so I'm going",
    "start": "2480720",
    "end": "2487710"
  },
  {
    "text": "to lock that in and I'm actually then going to pay a much lower hourly rate for that reserved instance and so once",
    "start": "2487710",
    "end": "2494640"
  },
  {
    "text": "you get to a fairly steady state and you know exactly what you think you're going to be using you can then use reserved",
    "start": "2494640",
    "end": "2500040"
  },
  {
    "text": "instances to lower your cost and so a lot of customers use that with EMR as",
    "start": "2500040",
    "end": "2505109"
  },
  {
    "text": "well and that obviously then reduces the cost of the ec2 instances underneath now",
    "start": "2505109",
    "end": "2511080"
  },
  {
    "text": "the third way that you can save money with EMR is to use spot instances now",
    "start": "2511080",
    "end": "2517670"
  },
  {
    "text": "for those of you not familiar with spot instances the the spot market is a",
    "start": "2517670",
    "end": "2523290"
  },
  {
    "text": "market that we have created for any unused ec2 capacity we will make",
    "start": "2523290",
    "end": "2530670"
  },
  {
    "text": "available on the spot market now the the price of each of the spot instances",
    "start": "2530670",
    "end": "2536730"
  },
  {
    "text": "goes up and down depending on supply and demand for each different instance type but typically it is a fraction of the",
    "start": "2536730",
    "end": "2542880"
  },
  {
    "text": "cost of the on-demand price and so what this allows you to do is to then spin up",
    "start": "2542880",
    "end": "2548039"
  },
  {
    "text": "quite large clusters with spot instances and then get through your processing",
    "start": "2548039",
    "end": "2553230"
  },
  {
    "text": "much more quickly now the caveat there obviously is that if these spot price goes up and it goes above what your bid",
    "start": "2553230",
    "end": "2560940"
  },
  {
    "text": "price is then we may take that instance away from you so you need to use spot instances with workloads that can tolerate the loss of",
    "start": "2560940",
    "end": "2567569"
  },
  {
    "text": "a node so we wanted to make sure that EMI could do that so there's a particular type of node that you can use",
    "start": "2567569",
    "end": "2573000"
  },
  {
    "text": "within EMR that is perfect for use with the spot market so if we do take the instance away the cluster will keep",
    "start": "2573000",
    "end": "2580230"
  },
  {
    "text": "running you won't lose any data but whatever job that instance was doing Hadoop will just reassign that to a",
    "start": "2580230",
    "end": "2585869"
  },
  {
    "text": "different node within the within the cluster and so this was really important",
    "start": "2585869",
    "end": "2591270"
  },
  {
    "text": "to us to make sure that customers could use the spot market as much as possible and just to give you an example of some",
    "start": "2591270",
    "end": "2598289"
  },
  {
    "text": "of the savings there so if you look at a fairly standard machine and m32 extra large in the Sydney region the on-demand",
    "start": "2598289",
    "end": "2605400"
  },
  {
    "text": "price for that is 75 cents and the spot price it does vary but when I just",
    "start": "2605400",
    "end": "2611010"
  },
  {
    "text": "checked in its it hold it's hovering around 9 cents so that you can see that",
    "start": "2611010",
    "end": "2616020"
  },
  {
    "text": "you can have enormous savings there if you're able to use spot in your workload",
    "start": "2616020",
    "end": "2623660"
  },
  {
    "start": "2622000",
    "end": "2701000"
  },
  {
    "text": "so let's talk about let's talk about marriage let's talk about the marriage between spark and EMR so as you can see",
    "start": "2624380",
    "end": "2631309"
  },
  {
    "text": "spark gives you a lot of that really rich out-of-the-box functionality and what EMR brings to the table is the the",
    "start": "2631309",
    "end": "2640079"
  },
  {
    "text": "ability to control that cluster to be flexible in the way you spin it up and spin it down to make it as",
    "start": "2640079",
    "end": "2645690"
  },
  {
    "text": "cost-effective as possible and so these two together really work very well together so I wanted to just give you a",
    "start": "2645690",
    "end": "2652230"
  },
  {
    "text": "couple of examples of what our customers are doing so if you look at Yelp for",
    "start": "2652230",
    "end": "2657390"
  },
  {
    "text": "example so they're a recommendations site in the US they do a lot of machine",
    "start": "2657390",
    "end": "2665640"
  },
  {
    "text": "learning and ad targeting using sparkly Mr Hearst publishing",
    "start": "2665640",
    "end": "2671690"
  },
  {
    "text": "organization they published an enormous number of web properties around the world they use it for their web",
    "start": "2671690",
    "end": "2677000"
  },
  {
    "text": "analytics Washington Post again our targeting and recommendations you can",
    "start": "2677000",
    "end": "2682860"
  },
  {
    "text": "see organizations using it for security event streaming so CrowdStrike for example and bottom left I'm use it for",
    "start": "2682860",
    "end": "2689400"
  },
  {
    "text": "looking at security event anomalies revenue forecasting predictive marketing",
    "start": "2689400",
    "end": "2694860"
  },
  {
    "text": "you know that so there's a huge variance in the in the applications that the people use it for and just drilling into",
    "start": "2694860",
    "end": "2702810"
  },
  {
    "start": "2701000",
    "end": "2802000"
  },
  {
    "text": "the Washington Post example I wanted to show you this because it kind of talks to what we were talking about before",
    "start": "2702810",
    "end": "2708450"
  },
  {
    "text": "where a lot of customers use multiple different clusters so I Washington Post looking they look at the number of",
    "start": "2708450",
    "end": "2714690"
  },
  {
    "text": "clicks the number of videos that people look at any kind of interaction and they've got ml Lib they're running some",
    "start": "2714690",
    "end": "2720780"
  },
  {
    "text": "kind of machine learning algorithm that's segmenting their customer base which they then look to to map users to",
    "start": "2720780",
    "end": "2728130"
  },
  {
    "text": "specific ads but they also have a recommendation cluster as well that you can see down the bottom there as well",
    "start": "2728130",
    "end": "2734160"
  },
  {
    "text": "with they're recommending particular content to users similarly if you look at what gumgum do again you can see this",
    "start": "2734160",
    "end": "2742500"
  },
  {
    "text": "is a classic use case where they've got a 24 by 7 spark cluster up the top there",
    "start": "2742500",
    "end": "2748830"
  },
  {
    "text": "so that's a persistent cluster that's running all the time that's constantly ingesting ad impressions and clicks and",
    "start": "2748830",
    "end": "2755460"
  },
  {
    "text": "from that they typically do their their revenue forecasting and have an interactive dashboard and then they've",
    "start": "2755460",
    "end": "2761010"
  },
  {
    "text": "got a batch spark cluster underneath that which is looking at the click stream logs so that's you know not not",
    "start": "2761010",
    "end": "2769260"
  },
  {
    "text": "online and and on all the time but they use that in a very specific fashion they",
    "start": "2769260",
    "end": "2774600"
  },
  {
    "text": "then process a lot of that data that ends up in redshift and then down on the bottom there you can see they've got a whole bunch of other bunch of different",
    "start": "2774600",
    "end": "2782910"
  },
  {
    "text": "clusters that they run for for different workloads typically for you know for data exploration and testing except etc",
    "start": "2782910",
    "end": "2789510"
  },
  {
    "text": "so again just a great example of a customer who is using a number of",
    "start": "2789510",
    "end": "2794910"
  },
  {
    "text": "different very specific clusters some persistent some transient",
    "start": "2794910",
    "end": "2802490"
  },
  {
    "text": "so I wanted to leave you with that thought so that you can think about the",
    "start": "2803690",
    "end": "2808859"
  },
  {
    "text": "marriage of two fantastic technologies together and you can see how from what",
    "start": "2808859",
    "end": "2814500"
  },
  {
    "text": "we've talked about the all the goodies that you get from spark and then",
    "start": "2814500",
    "end": "2820079"
  },
  {
    "text": "obviously the the underlying infrastructure that you get with with EMR so I want to leave you with that I",
    "start": "2820079",
    "end": "2827640"
  },
  {
    "text": "also wanted to just point you to the the details page on the website which talks",
    "start": "2827640",
    "end": "2834059"
  },
  {
    "text": "specifically about EMR and sparks if you follow that link that'll give you more use cases and more details etc and I",
    "start": "2834059",
    "end": "2844260"
  },
  {
    "text": "also wanted to mention the AWS tech chat podcast which which I run with one of my",
    "start": "2844260",
    "end": "2852540"
  },
  {
    "text": "colleagues so this is a great way to get a roundup of all the the latest AWS",
    "start": "2852540",
    "end": "2859010"
  },
  {
    "text": "updates and news etc and we'll be having a couple of reinvents specific shows",
    "start": "2859010",
    "end": "2864599"
  },
  {
    "text": "coming up in December to help you wade through the enormous number of announcements that's happened this week",
    "start": "2864599",
    "end": "2870890"
  },
  {
    "text": "so you can you can download that from from iTunes or SoundCloud or if you follow that bitly link there that will",
    "start": "2870890",
    "end": "2877559"
  },
  {
    "text": "also take you to the landing page where you can where you can where you can sign",
    "start": "2877559",
    "end": "2883319"
  },
  {
    "text": "up as well so it's not it's not just big data content it's across all the AWS services although I don't tell my",
    "start": "2883319",
    "end": "2890400"
  },
  {
    "text": "co-host this but I try and sneak as much big data content in as I possibly can so",
    "start": "2890400",
    "end": "2896119"
  },
  {
    "text": "that was it from the from the from the presentation I wanted to give you if",
    "start": "2896119",
    "end": "2901319"
  },
  {
    "text": "there's any questions please send them through on the chat window more try and",
    "start": "2901319",
    "end": "2906809"
  },
  {
    "text": "get through them as as quick as we can",
    "start": "2906809",
    "end": "2911750"
  }
]