[
  {
    "start": "0",
    "end": "31000"
  },
  {
    "text": "so I'm Cecilia Deng and I'm a software engineer on the adus Lambda team um and",
    "start": "440",
    "end": "5720"
  },
  {
    "text": "I work on a component specifically responsible for processing streams as well as uh asynchronous invocations so",
    "start": "5720",
    "end": "13599"
  },
  {
    "text": "I'm here today to talk about realtime processing using ad SLA and I'll be joined by uh Andre Spritz and Marco Pi",
    "start": "13599",
    "end": "20800"
  },
  {
    "text": "Leoni from Thompson Reuters who will go over their architecture and how they use Adis Lambda and Kinesis streams to solve",
    "start": "20800",
    "end": "28119"
  },
  {
    "text": "their real-time processing problem so um my goal for today is to dive into",
    "start": "28119",
    "end": "36000"
  },
  {
    "start": "31000",
    "end": "31000"
  },
  {
    "text": "how going culous with Adis Lambda can help you um approach and solve a variety",
    "start": "36000",
    "end": "41640"
  },
  {
    "text": "of real-time processing uh needs and problems um and by the end of the talk I",
    "start": "41640",
    "end": "48039"
  },
  {
    "text": "hope you walk away with a better idea of the different options you have of generating real-time events to Lambda as",
    "start": "48039",
    "end": "55039"
  },
  {
    "text": "well as the differences on in how we process them um and because real time",
    "start": "55039",
    "end": "60120"
  },
  {
    "text": "really implies near real time and near real time really implies some delays um there's very few things in the world",
    "start": "60120",
    "end": "66200"
  },
  {
    "text": "that's instantaneous so understanding the sort of the differences and how we process these events will help you",
    "start": "66200",
    "end": "71400"
  },
  {
    "text": "better understand how your system is behaving um specifically I want to dive",
    "start": "71400",
    "end": "76479"
  },
  {
    "text": "into how Lambda pulls and processes streams as this can be one of the more involved processes um and also go into",
    "start": "76479",
    "end": "84799"
  },
  {
    "text": "uh what kind of behaviors that you might expect to see um when you use the stream Event Source and you know a little bit",
    "start": "84799",
    "end": "91360"
  },
  {
    "text": "of um uh best practices some metrics that you might see and want to alarm on stuff like that um to tie things up at",
    "start": "91360",
    "end": "98920"
  },
  {
    "text": "the end will uh and and illustrate what going a real time uh processing with",
    "start": "98920",
    "end": "105040"
  },
  {
    "text": "adus Lambda looks like in practice um we'll have Thompson neers coming up and talking about their",
    "start": "105040",
    "end": "112600"
  },
  {
    "text": "architecture so what are the different kinds of real-time event sources um we",
    "start": "112600",
    "end": "118079"
  },
  {
    "start": "113000",
    "end": "113000"
  },
  {
    "text": "generally talk about push event sources and pull event sources I like to think about it um in these three sort of broad",
    "start": "118079",
    "end": "125240"
  },
  {
    "text": "categories where push event sources can also be broken down into asynchronous invoke and uh synchronous invoke so push",
    "start": "125240",
    "end": "133720"
  },
  {
    "text": "event sources uh you can is really generating the event to notify Lambda",
    "start": "133720",
    "end": "139440"
  },
  {
    "text": "about something that just happened in real time and how this is done is using the asynchronous invocation API or the",
    "start": "139440",
    "end": "147239"
  },
  {
    "text": "synchronous invocation API and I'll dive into sort of um why I'm differentiating between these two in a little bit um or",
    "start": "147239",
    "end": "154800"
  },
  {
    "text": "you can use a the polar Event Source model um for streams and this is really more um you making this uh event uh as a",
    "start": "154800",
    "end": "164239"
  },
  {
    "text": "reaction to a real-time action available for consumption on a stream in real time",
    "start": "164239",
    "end": "170200"
  },
  {
    "text": "and then it's lambda's responsibility to then pull and process this record in in",
    "start": "170200",
    "end": "176840"
  },
  {
    "text": "uh response in real time you can um a lot of these event sources so for example we have Amazon S3 doing",
    "start": "176840",
    "end": "183400"
  },
  {
    "text": "asynchronous invocations um and the actions might be something like putting an item into your bucket um I don't know",
    "start": "183400",
    "end": "189920"
  },
  {
    "text": "if you guys were at the what sort of talks but we also um can do gets on uh",
    "start": "189920",
    "end": "195560"
  },
  {
    "text": "S3 events versus now so that can generate events um and then S3 will notify Lambda will notify us to do the",
    "start": "195560",
    "end": "202200"
  },
  {
    "text": "processing um you have Echo um for giving commands to an Alexa skill and",
    "start": "202200",
    "end": "207560"
  },
  {
    "text": "that will also notify Lambda to do some process ing um uh if you want to have",
    "start": "207560",
    "end": "213159"
  },
  {
    "text": "custom events you can also uh directly um your custom application can directly",
    "start": "213159",
    "end": "219959"
  },
  {
    "text": "respond and throw custom events uh and invoke Lambda directly using the asynchronous invocation API or the",
    "start": "219959",
    "end": "226480"
  },
  {
    "text": "synchronous one or you can throw it into a Kinesis stream and deliver these",
    "start": "226480",
    "end": "231519"
  },
  {
    "text": "events and make them available for Lambda consumption um that way as well",
    "start": "231519",
    "end": "236959"
  },
  {
    "text": "for streams uh the other option is Dynamo DB update streams so that's a feature you can turn on a Dy DB table",
    "start": "236959",
    "end": "243959"
  },
  {
    "text": "and they will deliver an events for example if you update an item onto your table make that event available in a",
    "start": "243959",
    "end": "250079"
  },
  {
    "text": "stream for consumption so",
    "start": "250079",
    "end": "255519"
  },
  {
    "text": "uh the difference I want to make between the sync and async is the idea of",
    "start": "255560",
    "end": "260680"
  },
  {
    "text": "pulling and then synchronising invoking is sort of more obvious with the streams but really asynchronously there is that",
    "start": "260680",
    "end": "266600"
  },
  {
    "text": "sort of idea behind it as well asynchronous generally means you make work available in a queue and then",
    "start": "266600",
    "end": "271880"
  },
  {
    "text": "you'll have to have uh some some work to be done to grab that from the queue and then also invoke it and let's kind of go",
    "start": "271880",
    "end": "278960"
  },
  {
    "text": "into more what um real-time push looks like who does realtime push this is any",
    "start": "278960",
    "end": "286600"
  },
  {
    "start": "283000",
    "end": "283000"
  },
  {
    "text": "integrator that uses Adis lambdas invoke API um so this can be S3 Amazon SNS um",
    "start": "286600",
    "end": "294080"
  },
  {
    "text": "Amazon Alexa who does the synchronous one and uh various iot um what it is is",
    "start": "294080",
    "end": "300600"
  },
  {
    "text": "event sources notifying Lambda for processing and because this is more of a",
    "start": "300600",
    "end": "305680"
  },
  {
    "text": "push notification uh model the the responsibility of owning this mapping is",
    "start": "305680",
    "end": "312400"
  },
  {
    "text": "really on the Event Source Lambda doesn't necessarily know who's going to notify it but S3 knows that when you put",
    "start": "312400",
    "end": "318160"
  },
  {
    "text": "a bucket um when you put an event or something into your bucket it they also have to notify Lambda that something",
    "start": "318160",
    "end": "324600"
  },
  {
    "text": "just happened um once Lambda receives the invoke they have to do the realtime",
    "start": "324600",
    "end": "329720"
  },
  {
    "text": "processing and that's when we get into the differences between um the synchronous and async so for async you",
    "start": "329720",
    "end": "338199"
  },
  {
    "text": "when you make the invoke Epi you get a 202 right away what happens in the back end is really you're telling us to uh",
    "start": "338199",
    "end": "345720"
  },
  {
    "text": "queue up that work and then you get that 202 and then in the back end amda Lambda will own the um processing of pulling",
    "start": "345720",
    "end": "353400"
  },
  {
    "text": "and then synchronously invoking making sure that function actually gets executed and processed um and compare",
    "start": "353400",
    "end": "360560"
  },
  {
    "text": "that to the synchronous invoke you don't really have that extra sort of work that needs to be done when you make that",
    "start": "360560",
    "end": "366360"
  },
  {
    "text": "invoke um right away you'll be telling Lambda start processing and it's going to wait for processing to be done before",
    "start": "366360",
    "end": "373000"
  },
  {
    "text": "you get a response back um and now to sort of dive into",
    "start": "373000",
    "end": "378960"
  },
  {
    "text": "real- time PLL and you might think it's like okay so we're pulling events andly invoking um how is this different than",
    "start": "378960",
    "end": "385880"
  },
  {
    "text": "like what what becomes more involved um and there's a couple factors that make Sy more involved first of all now you're",
    "start": "385880",
    "end": "391479"
  },
  {
    "text": "talking about pulling per Shard so you have various shards that you need to um all be grabbing data from uh you also",
    "start": "391479",
    "end": "398840"
  },
  {
    "text": "have the notion where you're not deleting data from when you process it so this is a stream so you have the",
    "start": "398840",
    "end": "404479"
  },
  {
    "text": "concept of checkpointing um and keeping track of where you are in the data uh where you are in the Stream and then you",
    "start": "404479",
    "end": "410400"
  },
  {
    "text": "also have to keep it in order in Kinesis there is the Paradigm where uh the promise that they have the records in",
    "start": "410400",
    "end": "417440"
  },
  {
    "text": "order so let's kind of go into that more detail so who does Real Time pull this",
    "start": "417440",
    "end": "423759"
  },
  {
    "start": "420000",
    "end": "420000"
  },
  {
    "text": "is Amazon Kinesis and Dynamo DB streams currently dyo DB um oh and what real",
    "start": "423759",
    "end": "430759"
  },
  {
    "text": "time pull is Lambda grabbing vents from a stream for processing so in this case the the mapping the configuration itself",
    "start": "430759",
    "end": "437919"
  },
  {
    "text": "is owned by Lambda and um when you want to configure a stream Event Source you call the Lambda API create Event Source",
    "start": "437919",
    "end": "445360"
  },
  {
    "text": "um and this tells Lambda that hey you need to start paying attention to the",
    "start": "445360",
    "end": "450720"
  },
  {
    "text": "stream and watching it for events so the real time triggered events is still owned by The Event Source uh Dynamo DB",
    "start": "450720",
    "end": "459199"
  },
  {
    "text": "is still responsible for creating this event whenever you update um your table",
    "start": "459199",
    "end": "464240"
  },
  {
    "text": "but then they make it available for real-time processing in a stream and similarly with Kinesis um if you have",
    "start": "464240",
    "end": "470120"
  },
  {
    "text": "your custom application and um you're creating a custom event you put it into",
    "start": "470120",
    "end": "475240"
  },
  {
    "text": "the stream and then that's the end of your responsibility Lambda then needs to grab it and then start P um batching up",
    "start": "475240",
    "end": "482960"
  },
  {
    "text": "these records and invoking it in the back end um what it looks like here is when",
    "start": "482960",
    "end": "490080"
  },
  {
    "text": "you uh as a customer when you put records into the stream or when Dynamo DB puts records in the Stream it's",
    "start": "490080",
    "end": "495360"
  },
  {
    "text": "available across different shards and then you have Lambda responsible for pulling it and synchronously invoking it",
    "start": "495360",
    "end": "502440"
  },
  {
    "text": "um and this is where I'll go into uh dive into more under the hood what happens in this section and some um",
    "start": "502440",
    "end": "510240"
  },
  {
    "text": "behaviors and observations to watch out for processing streams um I'll start off",
    "start": "510240",
    "end": "517320"
  },
  {
    "text": "by going into how the different configurations impact um processing and",
    "start": "517320",
    "end": "522560"
  },
  {
    "text": "throughput at the end so how you configure your kesis stream uh how you configure your Lambda when you create it",
    "start": "522560",
    "end": "529040"
  },
  {
    "text": "and then how you actually hook them up and using the create Event Source API um",
    "start": "529040",
    "end": "535519"
  },
  {
    "text": "and yeah and and how different configuration on each of these these components impacts the the",
    "start": "535519",
    "end": "541920"
  },
  {
    "text": "resulting processing so with Kinesis um it's made",
    "start": "541920",
    "end": "549320"
  },
  {
    "start": "545000",
    "end": "545000"
  },
  {
    "text": "up of shards and the thing to sort of uh really note here is Lambda is going to",
    "start": "549320",
    "end": "554880"
  },
  {
    "text": "be consuming from your stream so it's important to know what are the limitations on the Kinesis that will",
    "start": "554880",
    "end": "561240"
  },
  {
    "text": "impact Lambda consuming from your stream um this in includes each Shard",
    "start": "561240",
    "end": "567800"
  },
  {
    "text": "supporting reading up to 2 megabytes per second and also each chart supporting",
    "start": "567800",
    "end": "573079"
  },
  {
    "text": "five control plane reads per second so this will be across any consumers on the stream including the Lambda Event Source",
    "start": "573079",
    "end": "581800"
  },
  {
    "text": "it's also uh good to note for the kesa side that all data is stored replayable by default for 24 hours and this is a",
    "start": "581800",
    "end": "589360"
  },
  {
    "text": "soft limit um and uh which means that Lambda consumption can happen at various",
    "start": "589360",
    "end": "595959"
  },
  {
    "text": "different points um that's why we do checkpointing and um and you can also uh",
    "start": "595959",
    "end": "601880"
  },
  {
    "text": "be replayable um some sort of rules of thumbs when you're um creating your",
    "start": "601880",
    "end": "607800"
  },
  {
    "text": "stream and and putting records into your stream is to make sure that the partition key distributes work evenly",
    "start": "607800",
    "end": "613800"
  },
  {
    "text": "across your shards because that's how consumers are going to be able to parallelize processing across your",
    "start": "613800",
    "end": "619440"
  },
  {
    "text": "stream um and that's true for any consumers including Lambda and also A good rule of thumb is to pick a key with",
    "start": "619440",
    "end": "625680"
  },
  {
    "text": "more groups than shards um this will sort of uh help you guarantee that",
    "start": "625680",
    "end": "631440"
  },
  {
    "text": "you're make putting good work making them available across the",
    "start": "631440",
    "end": "636839"
  },
  {
    "start": "637000",
    "end": "637000"
  },
  {
    "text": "shards on the Lambda side um you have some configurations including memory so",
    "start": "637240",
    "end": "644040"
  },
  {
    "text": "memory not only configures the amount of ram that's available to your Lambda function but um it also",
    "start": "644040",
    "end": "651600"
  },
  {
    "text": "proportionally uh detailed how much CPU um disk space uh threads and other",
    "start": "651600",
    "end": "658320"
  },
  {
    "text": "resources a ailable to your Lambda execution so uh why is that important",
    "start": "658320",
    "end": "664040"
  },
  {
    "text": "well I mean besides the obious I guess um more memory if your Lambda function is CPU CP CPU bound generally means",
    "start": "664040",
    "end": "671639"
  },
  {
    "text": "faster execution and we'll go into uh why the function duration really impacts",
    "start": "671639",
    "end": "677880"
  },
  {
    "text": "stream processing in a little bit um more memory can also mean you can process larger size record batches or",
    "start": "677880",
    "end": "685320"
  },
  {
    "text": "you can process larger records period um if your records in your stream is",
    "start": "685320",
    "end": "690560"
  },
  {
    "text": "variable in size or you have variable traffic this is when this might um come into play you also have timeout so",
    "start": "690560",
    "end": "698560"
  },
  {
    "text": "increasing timeout allows for longer functions um and again this will the duration of your Lambda function will",
    "start": "698560",
    "end": "704600"
  },
  {
    "text": "impact processing um one thing to watch out for is if your timeout is too long than than",
    "start": "704600",
    "end": "711160"
  },
  {
    "text": "usual and uh your lamp it might take longer for you to recognize any errors that might happen um the permission",
    "start": "711160",
    "end": "717680"
  },
  {
    "text": "model of the Lambda function uh includes the execution role um and it's important",
    "start": "717680",
    "end": "723040"
  },
  {
    "text": "to note here that the execution role needs all the permissions in order to describe the stream uh and get records",
    "start": "723040",
    "end": "729240"
  },
  {
    "text": "from the stream um and this is just how we are able to grab records from",
    "start": "729240",
    "end": "734399"
  },
  {
    "text": "customer streams when you create the actual Event",
    "start": "734399",
    "end": "739800"
  },
  {
    "start": "737000",
    "end": "737000"
  },
  {
    "text": "Source between your Kinesis and Lambda function you have a couple configurations here including bat size",
    "start": "739800",
    "end": "746680"
  },
  {
    "text": "so um it's important to note here back size is not actually the number of",
    "start": "746680",
    "end": "753240"
  },
  {
    "text": "records that is going to be uh invoked in in the Lambda function it's not guaranteed to be that so um and and it's",
    "start": "753240",
    "end": "760240"
  },
  {
    "text": "also not equivalent to how many records we grab from the Kinesis stream when we process it um the batch size is really",
    "start": "760240",
    "end": "766720"
  },
  {
    "text": "the maximum number of records that we will invoke a Lambda function with and I",
    "start": "766720",
    "end": "772440"
  },
  {
    "text": "like to sort of this is you know a way to like to describe it is the effective batch size so like what we're actually",
    "start": "772440",
    "end": "778519"
  },
  {
    "text": "going to be throwing into your Lambda function is the minimum of Records available so that's how many records we",
    "start": "778519",
    "end": "784839"
  },
  {
    "text": "get in a get records call uh from your Kinesis stream um the maximum batch size",
    "start": "784839",
    "end": "790800"
  },
  {
    "text": "configuration that you configured um as well as any payload size limits in the synchronous invocation and today that is",
    "start": "790800",
    "end": "797360"
  },
  {
    "text": "6 megabytes so um batch size here when you increase it some things you might",
    "start": "797360",
    "end": "803160"
  },
  {
    "text": "expect to see is uh maybe L fewer Lambda invocations when you have larger batch",
    "start": "803160",
    "end": "808600"
  },
  {
    "text": "sizes but each Lambda evocation is able to process more data per function so that's really what um the batch size is",
    "start": "808600",
    "end": "815040"
  },
  {
    "text": "controlling there you also have starting position when you configure Event Source and um",
    "start": "815040",
    "end": "822560"
  },
  {
    "text": "the starting position tells us at what point in your stream are we going to start processing um the options right",
    "start": "822560",
    "end": "828959"
  },
  {
    "text": "now uh include trim Horizon which tells us to start processing from the very beginning of the stream uh and that's",
    "start": "828959",
    "end": "835839"
  },
  {
    "text": "the oldest record uh in the Stream and then latest so we'll start um processing from the most recent record in the",
    "start": "835839",
    "end": "842639"
  },
  {
    "text": "Stream um uh the newest one and it's always first and first out so we process on from there um and uh if you guys were",
    "start": "842639",
    "end": "850680"
  },
  {
    "text": "at the what's new announcement we also now have at timestamp as a starting",
    "start": "850680",
    "end": "856199"
  },
  {
    "text": "position and that really allows you to um uh be able to be particular about",
    "start": "856199",
    "end": "861519"
  },
  {
    "text": "where you want to process a new stream um you can enable and disable your stream pause processing your stream and",
    "start": "861519",
    "end": "868240"
  },
  {
    "text": "then um start up again at where you actually want to start processing again",
    "start": "868240",
    "end": "874079"
  },
  {
    "text": "and it also allows you to be able to replay data if you need and gives you a lot more control on uh where you want to",
    "start": "874079",
    "end": "880040"
  },
  {
    "text": "start processing your data when you configure Event Source you",
    "start": "880040",
    "end": "886320"
  },
  {
    "text": "can um configure multiple functions mapped to one stream or you can",
    "start": "886320",
    "end": "891800"
  },
  {
    "text": "configure multiple streams to one function so this is a many to many relationship and each mapping is really",
    "start": "891800",
    "end": "898480"
  },
  {
    "text": "a unique uh a unique key pair between the Kinesis AR and the Lambda AR and what this",
    "start": "898480",
    "end": "905360"
  },
  {
    "text": "really means is that each mapping each have their own unique Shard iterators so",
    "start": "905360",
    "end": "911480"
  },
  {
    "text": "this means that each mapping is reading from the stream all at can can they can all be at different parts in time so if",
    "start": "911480",
    "end": "918040"
  },
  {
    "text": "you have one Lambda function that um is processing slower or faster or doing other things in another Lambda function",
    "start": "918040",
    "end": "924320"
  },
  {
    "text": "they're not really impacting each other they all have their different checkpointing and iterators",
    "start": "924320",
    "end": "929720"
  },
  {
    "text": "um one thing to note though if you have too many consumers of your stream they might they're also competing for the",
    "start": "929720",
    "end": "936240"
  },
  {
    "text": "Kinesis read limits so that's when this comes into play um by configuring multiple um event sour uh Lambda",
    "start": "936240",
    "end": "942639"
  },
  {
    "text": "functions to the stream they all count as consumers so this is kind of an example",
    "start": "942639",
    "end": "950600"
  },
  {
    "start": "947000",
    "end": "947000"
  },
  {
    "text": "of what a record might look like when it reaches your Lambda function you can see here uh it's a list of Records that's",
    "start": "950600",
    "end": "958360"
  },
  {
    "text": "that square bracket is I I only have one in here though um and you get information like partition key uh vent",
    "start": "958360",
    "end": "966000"
  },
  {
    "text": "ID and the data your by 64 encoded",
    "start": "966000",
    "end": "971199"
  },
  {
    "text": "data under the hood What's um happening when Lambda is processing streams is we",
    "start": "971800",
    "end": "978199"
  },
  {
    "text": "pull so this is p pulling concurrently across the",
    "start": "978199",
    "end": "983519"
  },
  {
    "text": "shards um you would only process uh you would only have as many concurrent",
    "start": "983519",
    "end": "989120"
  },
  {
    "text": "Lambda function as the shards that you have in your stream and we'll go into more in that later um if we don't get",
    "start": "989120",
    "end": "995040"
  },
  {
    "text": "any records in A Shard we'll wait 250 SEC milliseconds before we grab the next",
    "start": "995040",
    "end": "1000360"
  },
  {
    "text": "batch so there's a general delay when we um don't see any records when we do see records then we start doing the batching",
    "start": "1000360",
    "end": "1008000"
  },
  {
    "text": "process and the invoking and then it's a matter of how long your Lambda function takes and that's when duration really",
    "start": "1008000",
    "end": "1013720"
  },
  {
    "text": "comes into play so when we grab records from Kinesis we try to grab as much as we can",
    "start": "1013720",
    "end": "1019720"
  },
  {
    "text": "and then we keep those that batch of Records in memory and then we do SUB batching to create the eventual payload",
    "start": "1019720",
    "end": "1026558"
  },
  {
    "text": "that will be invoked into Lambda so this is what I mean when you configure your bat size it doesn't mean how much we",
    "start": "1026559",
    "end": "1032760"
  },
  {
    "text": "grab from uh grab from Kinesis and it doesn't necessarily mean how much we're going to invoke your Lambda with",
    "start": "1032760",
    "end": "1039678"
  },
  {
    "text": "um when we get that uh eventual payload that we want to invoke Lambda with we're",
    "start": "1039679",
    "end": "1044760"
  },
  {
    "text": "going to invoke it synchronously so this is a request response type of invocation",
    "start": "1044760",
    "end": "1050720"
  },
  {
    "text": "and we do this because we need to honor kin's at least one semantics so this means we need to make sure that a batch",
    "start": "1050720",
    "end": "1058400"
  },
  {
    "text": "of Records Was processed or at least best try uh before we move on to the",
    "start": "1058400",
    "end": "1064440"
  },
  {
    "text": "next one and by best try I mean until those batch of records are expired so it",
    "start": "1064440",
    "end": "1069640"
  },
  {
    "text": "essentially um means that we we will try our best until it's no longer possible",
    "start": "1069640",
    "end": "1075400"
  },
  {
    "text": "uh to invoke that batch of Records before moving on um so yeah that that means that each",
    "start": "1075400",
    "end": "1081159"
  },
  {
    "text": "Shard blocks on in ordered synchronous invocation and this is why um uh your",
    "start": "1081159",
    "end": "1087360"
  },
  {
    "text": "Lambda function duration really um uh will impact your processing throughput",
    "start": "1087360",
    "end": "1093200"
  },
  {
    "text": "because really when one invocation is invoked we're waiting for that response before invoking the next",
    "start": "1093200",
    "end": "1099400"
  },
  {
    "text": "one here's sort of how um what it looks like Illustrated um per Shard lambda's",
    "start": "1099400",
    "end": "1106000"
  },
  {
    "text": "going to call get records I mean this is just how um uh processing streams Works uh we call",
    "start": "1106000",
    "end": "1111880"
  },
  {
    "text": "get records with uh the max limit currently that's 10K or 10 megabytes",
    "start": "1111880",
    "end": "1117480"
  },
  {
    "text": "from Kinesis and generally not what you get back especially if you have constant flow uh in order to honor the actual",
    "start": "1117480",
    "end": "1123520"
  },
  {
    "text": "read limit of Kinesis um and then if we don't get any record back we wait a little while and then we'll try to get",
    "start": "1123520",
    "end": "1129559"
  },
  {
    "text": "records again um if we do we keep those records in memory we do some sub",
    "start": "1129559",
    "end": "1135120"
  },
  {
    "text": "batching logic uh to get the effective batch size as I mentioned earlier um and then we do some formatting and then",
    "start": "1135120",
    "end": "1141640"
  },
  {
    "text": "invoke Lambda with the resulting payload um and then when we invoke the payload uh invoke Lambda it's going to",
    "start": "1141640",
    "end": "1148799"
  },
  {
    "text": "be with a synchronous invocation um the synchronous invocation",
    "start": "1148799",
    "end": "1156200"
  },
  {
    "start": "1153000",
    "end": "1153000"
  },
  {
    "text": "will block so this is uh blocking for ordered processing and again this will",
    "start": "1156200",
    "end": "1162240"
  },
  {
    "text": "is on each individual Shard which means there is no uh parallel or concurrent work happening on one shard",
    "start": "1162240",
    "end": "1169640"
  },
  {
    "text": "increasing the number of shards with even distributed work meaning you have uh even work across the shards allows",
    "start": "1169640",
    "end": "1176159"
  },
  {
    "text": "for increase concurrency um and uh increasing or changing the batch size if",
    "start": "1176159",
    "end": "1182559"
  },
  {
    "text": "it impacts duration of the Lambda function it might take longer to process more records and uh so it's important to",
    "start": "1182559",
    "end": "1188760"
  },
  {
    "text": "sort of know uh understand um how your Lambda function reacts to more records if it's uh a linear sort of relationship",
    "start": "1188760",
    "end": "1195760"
  },
  {
    "text": "it might not be um as impactful to your result resulting",
    "start": "1195760",
    "end": "1201039"
  },
  {
    "text": "processing now if your Lambda function fails um we currently retry until the",
    "start": "1201960",
    "end": "1208760"
  },
  {
    "text": "records are expired or the request successfully goes through and again this is in order to honor the at least once",
    "start": "1208760",
    "end": "1216640"
  },
  {
    "text": "semantics um what this means is if you have uh throttling or errors in your Lambda function it affects it",
    "start": "1216640",
    "end": "1222760"
  },
  {
    "text": "effectively holds up processing on that particular Shard we do retry with some exponential back off um so if there's",
    "start": "1222760",
    "end": "1229840"
  },
  {
    "text": "like some service disruption it shouldn't uh be that impactful but um throttles and errors are very important",
    "start": "1229840",
    "end": "1235520"
  },
  {
    "text": "to monitor when you set up these uh Kinesis event",
    "start": "1235520",
    "end": "1240320"
  },
  {
    "text": "sources so um some more little rules of THS I like to think about uh the maximal",
    "start": "1241200",
    "end": "1246880"
  },
  {
    "text": "theoretical throughput at least um as far as you know that you can from get from Kinesis is the number of shards",
    "start": "1246880",
    "end": "1253120"
  },
  {
    "text": "times the maximum read limit which is 2 megabytes per second uh the effective",
    "start": "1253120",
    "end": "1258280"
  },
  {
    "text": "theore etical throughput looks more like the number of shards times the effective batch size uh divided by how long your",
    "start": "1258280",
    "end": "1266159"
  },
  {
    "text": "function actually takes and also any Rees uh that might happen until it successfully goes through and um it's",
    "start": "1266159",
    "end": "1273240"
  },
  {
    "text": "it's kind of useful to sort of look at your put metrics on your Kinesis stream and compare them to your get metrics and",
    "start": "1273240",
    "end": "1278960"
  },
  {
    "text": "and maybe from some uh you know uh estimations um if the put rate is uh",
    "start": "1278960",
    "end": "1285480"
  },
  {
    "text": "consistently greater than ingestion throughput you probably are um going to",
    "start": "1285480",
    "end": "1290960"
  },
  {
    "text": "be falling behind in processing um in that case you might want to consider increasing the number",
    "start": "1290960",
    "end": "1296480"
  },
  {
    "text": "of shards so that you can increase parallel processing or optimizing your Lambda function um and decreasing the",
    "start": "1296480",
    "end": "1306000"
  },
  {
    "text": "duration so uh here are some metrics that are useful here um this is you",
    "start": "1306520",
    "end": "1311960"
  },
  {
    "start": "1307000",
    "end": "1307000"
  },
  {
    "text": "Kinesis provides get records um and and also put records so get records will",
    "start": "1311960",
    "end": "1318279"
  },
  {
    "text": "kind of tell you what the effective throughput is uh as far as Kinesis knows and the put records is your ingestion",
    "start": "1318279",
    "end": "1324640"
  },
  {
    "text": "rate so if your get records line is consistently higher than your put records line you're probably falling",
    "start": "1324640",
    "end": "1330240"
  },
  {
    "text": "behind in processing um another good metric to watch out for is iterator age milliseconds and this is uh account a",
    "start": "1330240",
    "end": "1338679"
  },
  {
    "text": "measurement of how old your last process record was if that number gets really high if it's close to 24 hours um it",
    "start": "1338679",
    "end": "1345480"
  },
  {
    "text": "means that the last process record was Pro was close to expiring and all subsequent records are probably also close to",
    "start": "1345480",
    "end": "1352960"
  },
  {
    "text": "expiring uh on the Lambda side we have metrics for invocation count duration uh",
    "start": "1353320",
    "end": "1359679"
  },
  {
    "text": "error count as well as throttle count so the error counts and throttle counts are important to monitor in this case um as",
    "start": "1359679",
    "end": "1365080"
  },
  {
    "text": "I've talked about it it uh greatly impacts processing um and we also have logs for all these metrics as well uh",
    "start": "1365080",
    "end": "1371799"
  },
  {
    "text": "including the amount of uh memory consumed as well as any custom logs that you have in your Lambda function",
    "start": "1371799",
    "end": "1379320"
  },
  {
    "text": "so some common observations um with all these configurations and and processing is uh the effective batch size so how",
    "start": "1379320",
    "end": "1386600"
  },
  {
    "text": "much how many records actually you see in a Lambda function execution might be less than what you've configured for",
    "start": "1386600",
    "end": "1392520"
  },
  {
    "text": "your uh conf batch size if you have low throughput in your stream um conversely",
    "start": "1392520",
    "end": "1398960"
  },
  {
    "text": "if you have high throughput in your stream your batch size will likely increase um when you have increased",
    "start": "1398960",
    "end": "1404480"
  },
  {
    "text": "Lambda duration uh it kind of holds up processing along the whole pipeline",
    "start": "1404480",
    "end": "1409880"
  },
  {
    "text": "which means uh there's decreased number of invocations as well as a decreased number of get records calls that's us",
    "start": "1409880",
    "end": "1416520"
  },
  {
    "text": "grabbing records from Kinesis so um and also if you have too many consumers on",
    "start": "1416520",
    "end": "1422559"
  },
  {
    "text": "your stream uh you might start to see uh the metric read provision throughput exceeded and that's um provided by",
    "start": "1422559",
    "end": "1429279"
  },
  {
    "text": "Kinesis um so that's another good one to alarm on um it basically tells you that",
    "start": "1429279",
    "end": "1435080"
  },
  {
    "text": "um you're starting to see some competition on your stream and the things that are trying to uh read from",
    "start": "1435080",
    "end": "1440320"
  },
  {
    "text": "it and process it so that's kind of a good overview of",
    "start": "1440320",
    "end": "1446240"
  },
  {
    "text": "uh of Lambda stream processing and now we're going to have Anders and Marco come",
    "start": "1446240",
    "end": "1452000"
  },
  {
    "text": "up uh good afternoon everyone so I'm Anders Marco Marco so we work for a",
    "start": "1457039",
    "end": "1463480"
  },
  {
    "text": "company called Thomson Reuters and for those who don't know Thomson Reuters we are active in a couple of different different Industries we're in the",
    "start": "1463480",
    "end": "1470000"
  },
  {
    "text": "financial industry we're in the legal industry we're in the tax and accounting industry and then the news industry you",
    "start": "1470000",
    "end": "1475840"
  },
  {
    "text": "might know the Rogers news agency we're going to talk about product Insight which is an internal tool that we",
    "start": "1475840",
    "end": "1481399"
  },
  {
    "text": "developed over last year um it's a usage see if I get this right it's a usage analytics solution that we",
    "start": "1481399",
    "end": "1488039"
  },
  {
    "text": "developed about a year ago we were tossed with a challenge we were to",
    "start": "1488039",
    "end": "1494360"
  },
  {
    "start": "1489000",
    "end": "1489000"
  },
  {
    "text": "identify and Define a solution for usage analytics tracking that enables our",
    "start": "1494360",
    "end": "1499559"
  },
  {
    "text": "internal product teams to uh take ownership of the usage data that we track um in addition to tracking and",
    "start": "1499559",
    "end": "1506480"
  },
  {
    "text": "visualizing the data we also had to uh connect it to other parts of th orer so",
    "start": "1506480",
    "end": "1511679"
  },
  {
    "text": "we could uh augment business reference data that we that we keep elsewhere um it had to be safe and secure um part of",
    "start": "1511679",
    "end": "1519919"
  },
  {
    "text": "Thomson orits is around news and usage of news fluctuates according to events breaking so we had to be able to um yeah",
    "start": "1519919",
    "end": "1528679"
  },
  {
    "text": "scale up as events were breaking scale down as they were subsidizing or going down and the other",
    "start": "1528679",
    "end": "1536000"
  },
  {
    "text": "part is we are a very very small team so a year ago we were four now we're five",
    "start": "1536000",
    "end": "1542159"
  },
  {
    "text": "six people so we we couldn't spend time on maintenance we just had to maintain themselves almost um and the last part",
    "start": "1542159",
    "end": "1550399"
  },
  {
    "text": "is they said you know launch in five months um next time what what how does",
    "start": "1550399",
    "end": "1557159"
  },
  {
    "start": "1556000",
    "end": "1556000"
  },
  {
    "text": "the solution look like okay so to reiterate the task was to create a multi-tenant platform for the",
    "start": "1557159",
    "end": "1563960"
  },
  {
    "text": "analysis of user user data inside on product and uh this this solution will",
    "start": "1563960",
    "end": "1570240"
  },
  {
    "text": "have realtime processing pipeline but also a full data export and it will span",
    "start": "1570240",
    "end": "1575600"
  },
  {
    "text": "client of hundreds of Rec per seconds and clients with millions of Recs per seconds we have to be more in compliance",
    "start": "1575600",
    "end": "1582919"
  },
  {
    "text": "by the current vendant and it will have to cost less than a current vendor will have to be easy to use by the end the",
    "start": "1582919",
    "end": "1589520"
  },
  {
    "text": "end user which are the product owners and the analyst and it will have to be delivering production in a few month and",
    "start": "1589520",
    "end": "1596440"
  },
  {
    "text": "everything will have to be implemented and managed by true developer and uh not working exclusively on this project so",
    "start": "1596440",
    "end": "1603840"
  },
  {
    "text": "given this premises uh we knew that we wanted to leverage amazu Service as much as possible so we started thinking about",
    "start": "1603840",
    "end": "1610679"
  },
  {
    "text": "it and says which is the best tool to manage a realtime data pipeline that",
    "start": "1610679",
    "end": "1615760"
  },
  {
    "text": "processing and uh with basically our answer was to use Kinesis stream what we get with Kinesis stream",
    "start": "1615760",
    "end": "1623039"
  },
  {
    "text": "is basically we can just throw data at it and what we get out of it it's it's a Ser serialized output and basically all",
    "start": "1623039",
    "end": "1630600"
  },
  {
    "text": "the events are uh strictly ordered inside the same shart but also we get uh",
    "start": "1630600",
    "end": "1636640"
  },
  {
    "text": "not the struct rate that allow us to power both parallel processing but also give you residence to failure so in the",
    "start": "1636640",
    "end": "1643520"
  },
  {
    "text": "way that we configure it we have up to seven days uh in case one consumer goes down then we can just fix the problem",
    "start": "1643520",
    "end": "1650120"
  },
  {
    "text": "and resume processing or just replay it all plus this uh K stream scale so if",
    "start": "1650120",
    "end": "1656039"
  },
  {
    "text": "you have if you need uh you had the need for more data you just add more sh so",
    "start": "1656039",
    "end": "1661360"
  },
  {
    "text": "the next step was to have a m data sets so the M data set is an immutable copy",
    "start": "1661360",
    "end": "1667840"
  },
  {
    "text": "of all the raow events ever ingested by your system and we decide to use um a",
    "start": "1667840",
    "end": "1673799"
  },
  {
    "text": "cross region replicated strip bucket a storage solution for it and uh the",
    "start": "1673799",
    "end": "1679360"
  },
  {
    "text": "importance of well having the master the master data set is of a capital importance even more for a very tiny",
    "start": "1679360",
    "end": "1685720"
  },
  {
    "text": "team like hours mostly because of replayability so let's imagine that we",
    "start": "1685720",
    "end": "1691320"
  },
  {
    "text": "just deploy something with a B that somehow in the later processing corrupts the data so what we can do is just uh",
    "start": "1691320",
    "end": "1697679"
  },
  {
    "text": "deploy a fix and just replay all the data that would solve it but also",
    "start": "1697679",
    "end": "1702760"
  },
  {
    "text": "something similar would also happen if the client send something wrong to us which like bad data or data in a wrong",
    "start": "1702760",
    "end": "1708360"
  },
  {
    "text": "format then at that point we can decide to just filter out the bad data or to sanitize the data in in a wrong format",
    "start": "1708360",
    "end": "1715480"
  },
  {
    "text": "and just reprocess it all plus uh so basically having a master what I'm",
    "start": "1715480",
    "end": "1721159"
  },
  {
    "text": "trying to say that having the master data set give you a resence across multiple type of of failure plus",
    "start": "1721159",
    "end": "1727159"
  },
  {
    "text": "furthermore if uh we want to introduce a new feature or a new transformation we",
    "start": "1727159",
    "end": "1732440"
  },
  {
    "text": "can apply to both the the recent data but also the historical ones since we have in the master data set",
    "start": "1732440",
    "end": "1738880"
  },
  {
    "text": "uh we decide to use the S3 as to solution because it's a very reliable one it also offer you the ability to pay",
    "start": "1738880",
    "end": "1744840"
  },
  {
    "text": "less for infrequent access but most importantly it because has S3 event notification and I'm going to describe",
    "start": "1744840",
    "end": "1750760"
  },
  {
    "text": "later why that's important for us however S3 deals with files not events so we need a solution that basically",
    "start": "1750760",
    "end": "1757159"
  },
  {
    "text": "just bdch those events into files and store them in a strip bucket and we are using kesis fireos for that which is a",
    "start": "1757159",
    "end": "1764159"
  },
  {
    "text": "very simple tool but also very useful that what it does is basically what that what I just described so take those",
    "start": "1764159",
    "end": "1770399"
  },
  {
    "text": "event watching file and ship it to S3 bucket it also offer you a compression",
    "start": "1770399",
    "end": "1776200"
  },
  {
    "text": "and encryption so the missing link here is how we take the data out of Kinesis strs and into Kinesis phos and we are",
    "start": "1776200",
    "end": "1782880"
  },
  {
    "text": "using a Landa for that we actually found in AWS lab's GitHub account an example",
    "start": "1782880",
    "end": "1788440"
  },
  {
    "text": "of it so we just tweak it and deploy it so let me spend the second year so what we have now it's a it's basically it's",
    "start": "1788440",
    "end": "1796000"
  },
  {
    "text": "it's a serverless com architecture that is able to collect user data and to",
    "start": "1796000",
    "end": "1801480"
  },
  {
    "text": "create an immutable data set it requires very small effort to set it up and after that point it's completely managed by",
    "start": "1801480",
    "end": "1808519"
  },
  {
    "text": "Amazon for us it the only thing that remains to do is to create a couple of",
    "start": "1808519",
    "end": "1814840"
  },
  {
    "text": "alert on cloudwatch mostly on Kinesis and Lambda to just to be notified something is wrong but for the rest of",
    "start": "1814840",
    "end": "1821559"
  },
  {
    "text": "it it's it is completely managed by Amazon and it is the core of our um",
    "start": "1821559",
    "end": "1828600"
  },
  {
    "text": "of analytic solution and since we are not managing it we can shift our attention to other parts of the system",
    "start": "1828600",
    "end": "1834720"
  },
  {
    "text": "trying to deliver new feature however we cannot let the uh user devices to send the data directly",
    "start": "1834720",
    "end": "1841440"
  },
  {
    "text": "to Kinesis stream because in our solution one of the requirement is that data has to be encrypted so what we did",
    "start": "1841440",
    "end": "1848919"
  },
  {
    "text": "is to add the fin layer on top of kesis streams which is basically composed of elb and engine ex color in on ec2",
    "start": "1848919",
    "end": "1858159"
  },
  {
    "text": "uh this is TCP LW balancing and what engx does after the TLs termination is",
    "start": "1858159",
    "end": "1863720"
  },
  {
    "text": "to add some metadata to the event to um well it also prision uh try to authorize",
    "start": "1863720",
    "end": "1871519"
  },
  {
    "text": "the request and then just encrypt encrypt the the data and ship it to Kinesis streams so the engine X are",
    "start": "1871519",
    "end": "1879480"
  },
  {
    "text": "deployed in an autoscaling uh group across to the to Ability Zone and we",
    "start": "1879480",
    "end": "1884880"
  },
  {
    "text": "actually working right now on uh replicating this Feld layer on a second region leveraging around 53 so we said",
    "start": "1884880",
    "end": "1892679"
  },
  {
    "text": "with Kinesis stream is able to have parallel processing so what we did also is to attach another realtime um",
    "start": "1892679",
    "end": "1899559"
  },
  {
    "text": "processing Pipeline and what happens here is that basically we extract the data from Kinesis streams we transform",
    "start": "1899559",
    "end": "1905480"
  },
  {
    "text": "it and integrate it with tonso services and we ship it to an Elis search cluster as a side note if we lose all the data",
    "start": "1905480",
    "end": "1912000"
  },
  {
    "text": "inside the search we basically can rebuild it because we have the master data set and the data inside",
    "start": "1912000",
    "end": "1918480"
  },
  {
    "text": "Stream So after the data is once the data is analysis search we surface it to the end user the product owner and the",
    "start": "1918480",
    "end": "1925120"
  },
  {
    "text": "analyst through a third party view Edition tool which is kibana by elastic",
    "start": "1925120",
    "end": "1930679"
  },
  {
    "text": "and um and at the same time we were working on this project our internal",
    "start": "1930679",
    "end": "1936799"
  },
  {
    "text": "group what our group was also working on a microsurface um platform an internal",
    "start": "1936799",
    "end": "1941919"
  },
  {
    "text": "one name Alpha we are using this platform for um deploying an um managing",
    "start": "1941919",
    "end": "1948679"
  },
  {
    "text": "the more than 100 keyan instances that we have it also manage the authentication and authorization layer",
    "start": "1948679",
    "end": "1954279"
  },
  {
    "text": "around it we also use this platform any other tool that we build around it like Administration console or PDF reporting",
    "start": "1954279",
    "end": "1962880"
  },
  {
    "text": "so last inarch and kibana uh covers most airp pass covering most of the use cases",
    "start": "1962880",
    "end": "1968399"
  },
  {
    "text": "we want to uh support however for a more advanced use we also wanted to offer an",
    "start": "1968399",
    "end": "1973600"
  },
  {
    "text": "alternative way to transform the data and let the user to export it so that could be integrated in something else",
    "start": "1973600",
    "end": "1978960"
  },
  {
    "text": "like elim producer shift or AWS AA or",
    "start": "1978960",
    "end": "1984120"
  },
  {
    "text": "anything really so as we said before an important feature of S3 was the S3 event",
    "start": "1984120",
    "end": "1991240"
  },
  {
    "text": "notification so once those are once you set it up basically what happens is that",
    "start": "1991240",
    "end": "1996799"
  },
  {
    "text": "you can have a Lambda function invoked every time a new file is an a bucket and can potentially take the cont of the",
    "start": "1996799",
    "end": "2003000"
  },
  {
    "text": "file it that's from in something else so in the way that we set it up every every",
    "start": "2003000",
    "end": "2008600"
  },
  {
    "text": "time there is a new file in the master data set as ined step a Lambda function",
    "start": "2008600",
    "end": "2014080"
  },
  {
    "text": "is invoked that take that file and create multiple file one for each tenant",
    "start": "2014080",
    "end": "2019760"
  },
  {
    "text": "so at that point we have the data set of each tenant completely separated by from",
    "start": "2019760",
    "end": "2026120"
  },
  {
    "text": "between each other and we can attach even more Lambda function to the specific uh 10 data set that allow us to",
    "start": "2026120",
    "end": "2034000"
  },
  {
    "text": "have a customized transformation in case we want to deliver uh the data to the",
    "start": "2034000",
    "end": "2041120"
  },
  {
    "text": "the specific tenant in a in a different format so this chain of S3 and Landa",
    "start": "2041120",
    "end": "2047320"
  },
  {
    "text": "offer you a flexible and fully managed processing pipeline plus the fact that",
    "start": "2047320",
    "end": "2052960"
  },
  {
    "text": "we are using S3 for both the intermediate the intermediate stage and the final output give you multiple",
    "start": "2052960",
    "end": "2059960"
  },
  {
    "text": "integration points so for example someone could just take the side that it wants to take the master data set as it",
    "start": "2059960",
    "end": "2066440"
  },
  {
    "text": "gets uh created and push it into an dup solution and it can just do that so",
    "start": "2066440",
    "end": "2071878"
  },
  {
    "text": "someone else could just say that is interesting having the raw events for one project or more than one and I would",
    "start": "2071879",
    "end": "2078919"
  },
  {
    "text": "just tap in what I call the intermediate storage so since we are also using uh we",
    "start": "2078919",
    "end": "2085398"
  },
  {
    "text": "are letting the user take the data directly from S3 we can use the IM user to control the access on it so just to",
    "start": "2085399",
    "end": "2093079"
  },
  {
    "text": "finish my part I just wanted to say that uh using S3 and Lambda in this function",
    "start": "2093079",
    "end": "2099680"
  },
  {
    "text": "plus all the other things that they announced this morning I think it's very powerful powerful and we want to",
    "start": "2099680",
    "end": "2105160"
  },
  {
    "text": "leverage way more of this in the future so where are we today um we",
    "start": "2105160",
    "end": "2114480"
  },
  {
    "start": "2111000",
    "end": "2111000"
  },
  {
    "text": "launched in at the end of February we launched two and a half months early um",
    "start": "2114480",
    "end": "2121119"
  },
  {
    "text": "which was you know 5 months is quick for Thomson roters and actually launching in 2 and a half months as opposed to 5",
    "start": "2121119",
    "end": "2127160"
  },
  {
    "text": "months was really really quick we have a huge adoption rate um",
    "start": "2127160",
    "end": "2134520"
  },
  {
    "text": "it's um the adoption rate is um you know we thought it'd be good but the adoption rate to cross thp Roes is a fact you",
    "start": "2134520",
    "end": "2141359"
  },
  {
    "text": "know what we thought um times a factor of five um every business unit within",
    "start": "2141359",
    "end": "2147160"
  },
  {
    "text": "Thompson roers is is um there's there are product teams in every business unit across THS Rus that is using it and or",
    "start": "2147160",
    "end": "2154520"
  },
  {
    "text": "testing it and ready to use it uh um what about the product itself we over since February we had a chance",
    "start": "2154520",
    "end": "2161119"
  },
  {
    "text": "of testing it in a couple of different ways um we're going to see how how many",
    "start": "2161119",
    "end": "2166319"
  },
  {
    "text": "events or how many requests we could test it for we targeted testing it for 5 billion requests in a month we're",
    "start": "2166319",
    "end": "2172400"
  },
  {
    "text": "actually able to push that limit up to 4,000 requests per second so equivalent",
    "start": "2172400",
    "end": "2177680"
  },
  {
    "text": "of 10 billion events per month and that's that's that's plenty for our",
    "start": "2177680",
    "end": "2183319"
  },
  {
    "text": "purposes um since March and this is the this is the good part here since March",
    "start": "2183319",
    "end": "2188920"
  },
  {
    "text": "we've had very very very little maintenance we've had no outages we've had no",
    "start": "2188920",
    "end": "2194319"
  },
  {
    "text": "downtown and we're using cloudwatch to monitor everything and this this part here about no outages and no maintenance",
    "start": "2194319",
    "end": "2200160"
  },
  {
    "text": "is extremely important for us because we are so tiny uh you know four five five people doing this",
    "start": "2200160",
    "end": "2208280"
  },
  {
    "text": "um what has really impressed our product managers is the latency it's the from",
    "start": "2208280",
    "end": "2214359"
  },
  {
    "text": "the from the capture of the single event to being visible in a in a Cubana chart",
    "start": "2214359",
    "end": "2220599"
  },
  {
    "text": "it's a few seconds um which is just superb from a usage tracking",
    "start": "2220599",
    "end": "2227079"
  },
  {
    "text": "setup um we've we've been able to test it with the Autos scaling feature so",
    "start": "2227079",
    "end": "2232440"
  },
  {
    "text": "Rogers news cover we cover news and there been several breaking news over the last year",
    "start": "2232440",
    "end": "2237960"
  },
  {
    "text": "or so um and for the US elections we saw a a more than doubling of events",
    "start": "2237960",
    "end": "2245319"
  },
  {
    "text": "captured in 24 hours compared to what we normally see on a day-to-day basis um",
    "start": "2245319",
    "end": "2252160"
  },
  {
    "text": "the UK EU referendum was almost more impressive because nobody expected the",
    "start": "2252160",
    "end": "2257680"
  },
  {
    "text": "British to vote what they in the way that they did so we had we literally",
    "start": "2257680",
    "end": "2262800"
  },
  {
    "text": "went to bed at night slept very soundly and next morning we woke up and we thought oh we we doubled in events over",
    "start": "2262800",
    "end": "2270200"
  },
  {
    "text": "night and luckily we're using AWS so we were all good and we can we could come in and just check the systems and make",
    "start": "2270200",
    "end": "2276839"
  },
  {
    "text": "sure that was okay uh so here you see you know the real",
    "start": "2276839",
    "end": "2282960"
  },
  {
    "start": "2280000",
    "end": "2280000"
  },
  {
    "text": "time um chart of that e referendum and",
    "start": "2282960",
    "end": "2288160"
  },
  {
    "text": "you could see literally uh track um asked to different districts and Britain",
    "start": "2288160",
    "end": "2294640"
  },
  {
    "text": "came in and suddenly people realized that this was not going in the way that people thought U so during that night",
    "start": "2294640",
    "end": "2301280"
  },
  {
    "text": "and so we see spikes there coming through so the right hand Spike there is the evening of the 23rd of June into the",
    "start": "2301280",
    "end": "2311560"
  },
  {
    "text": "24th and here you see the events that we captured uh during the US elections you can again you can see here we did we",
    "start": "2311839",
    "end": "2318680"
  },
  {
    "text": "realized that this would be a big big event so we were able to prepare a little bit more than the EU referendum",
    "start": "2318680",
    "end": "2324880"
  },
  {
    "text": "but again we tripled in size we're in 24 hours to what we normally could do and",
    "start": "2324880",
    "end": "2331280"
  },
  {
    "text": "then afterwards we just scal it down and and it's enormously beneficial for us because it's it's very very hard to",
    "start": "2331280",
    "end": "2336839"
  },
  {
    "text": "forecast news events because they they just break um so having having that Al",
    "start": "2336839",
    "end": "2342560"
  },
  {
    "text": "scaling having the architecture and the the uh tools that AWS uh provides is enormously beneficial",
    "start": "2342560",
    "end": "2350599"
  },
  {
    "text": "not only from a maintenance perspective but also from a cost and making this very very",
    "start": "2350599",
    "end": "2356520"
  },
  {
    "text": "efficient um and that's it from us thank you very much",
    "start": "2356520",
    "end": "2361880"
  },
  {
    "text": "um remember to complete your evaluations on behalf of Cecilia here um",
    "start": "2361880",
    "end": "2372680"
  }
]