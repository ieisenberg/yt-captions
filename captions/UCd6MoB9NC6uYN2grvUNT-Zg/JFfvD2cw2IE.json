[
  {
    "text": "hello hello hello hello thank you so much for joining us this this afternoon",
    "start": "60",
    "end": "5490"
  },
  {
    "text": "I know you have a choice of different talks to attend and you know different boot camps to train at and so really",
    "start": "5490",
    "end": "13349"
  },
  {
    "text": "glad that you decided to join us today here of course the session is about",
    "start": "13349",
    "end": "19140"
  },
  {
    "text": "Amazon Kinesis in case you were waiting to hear about IOT or last casters is not the right room just want to make sure",
    "start": "19140",
    "end": "25470"
  },
  {
    "text": "right this is a 400 level session so we're gonna move very quickly through a",
    "start": "25470",
    "end": "31410"
  },
  {
    "text": "lot of content so before I do the show of hands how many of you here use Amazon",
    "start": "31410",
    "end": "37920"
  },
  {
    "text": "Kinesis in some wave form that's healthy how many of you are just now getting",
    "start": "37920",
    "end": "45030"
  },
  {
    "text": "curious about streaming data because you've heard a lot about it on stage here and through all the customers and are curious so it's a 50/50 mix so I'm",
    "start": "45030",
    "end": "52860"
  },
  {
    "text": "assuming that some of you do know enough about streaming data such that because",
    "start": "52860",
    "end": "58230"
  },
  {
    "text": "it's a 400 level session we will make certain assumptions about how much you might already know so I'll do my very",
    "start": "58230",
    "end": "64350"
  },
  {
    "text": "best to give you kind of a broad overview while still going deep enough for those who have been kind of deep in",
    "start": "64350",
    "end": "70890"
  },
  {
    "text": "the trenches on Kinesis hopefully that will be a happy medium although it's never been known to work we will try",
    "start": "70890",
    "end": "77580"
  },
  {
    "text": "that out but really what you'll see is you hopefully go back with 10 things you know five things on ingestion with",
    "start": "77580",
    "end": "84659"
  },
  {
    "text": "Kinesis and five things on building applications with Kinesis client library which is a free open source library that",
    "start": "84659",
    "end": "91110"
  },
  {
    "text": "ships as part of part of the Canada Kinesis service to help you build stream processing applications perhaps more",
    "start": "91110",
    "end": "98460"
  },
  {
    "text": "even more excitingly I have with me on stage Nick parish nick is an ste at a draw one",
    "start": "98460",
    "end": "107549"
  },
  {
    "text": "of the leaders performance marketing at tech companies in the world and they've had some great experience in rolling out",
    "start": "107549",
    "end": "114470"
  },
  {
    "text": "Kinesis in production real time on on massive massive scale real time data",
    "start": "114470",
    "end": "119579"
  },
  {
    "text": "streams and things going to present of course a practitioners perspective but",
    "start": "119579",
    "end": "124799"
  },
  {
    "text": "what's also interesting is that they've built their a lot of their infrastructure the processing infrastructure that consumes from",
    "start": "124799",
    "end": "130530"
  },
  {
    "text": "Kinesis streams using storm and so this is important so that you get a good view for what it means to do build you're processing",
    "start": "130530",
    "end": "136950"
  },
  {
    "text": "applications using a framework that is not KCl a so so that's kind of the thought process over there so Amazon",
    "start": "136950",
    "end": "144450"
  },
  {
    "text": "Kinesis streams focus of this session focused on developers allows you to have fine-grained control over partitioning",
    "start": "144450",
    "end": "150900"
  },
  {
    "text": "scaling of data and then building your own custom stream processing applications using a framework of choice",
    "start": "150900",
    "end": "156800"
  },
  {
    "text": "Amazon Cleese's firehose is the stuff we announced today to easily load data",
    "start": "156800",
    "end": "162900"
  },
  {
    "text": "streams into destinations like s3 and redshift without writing any code and this is available broadly and Werner",
    "start": "162900",
    "end": "171060"
  },
  {
    "text": "announced today Amazon Kinesis analytics which is kind of the third member of the Kinesis platform which will enable you",
    "start": "171060",
    "end": "177600"
  },
  {
    "text": "to do sequel query directly on data streams that is not the focus of this session right so standard marketing",
    "start": "177600",
    "end": "189750"
  },
  {
    "text": "slide three things about Amazon Kinesis streams you create a stream you",
    "start": "189750",
    "end": "195150"
  },
  {
    "text": "provision it to the capacity you want you can change that capacity at any point of matching your data throughput",
    "start": "195150",
    "end": "200970"
  },
  {
    "text": "rate and volume so it's relatively easy to administer you then build your app processing application using your",
    "start": "200970",
    "end": "206760"
  },
  {
    "text": "framework of choice and overall it is a low cost platform you can exert a lot of control over the the workload itself and",
    "start": "206760",
    "end": "213930"
  },
  {
    "text": "and overall and therefore the cost that you incur so Kinesis streams started at",
    "start": "213930",
    "end": "220560"
  },
  {
    "text": "all two years ago in this very forum we first announced Amazon Kinesis streams can they give you kind of the high-level",
    "start": "220560",
    "end": "227070"
  },
  {
    "text": "gist of it all data you put into a provision entity called a stream gets",
    "start": "227070",
    "end": "233670"
  },
  {
    "text": "three we replicated across availability zones a replayable ordered stream of",
    "start": "233670",
    "end": "239340"
  },
  {
    "text": "events emerges from that stream the provision entity they've created once",
    "start": "239340",
    "end": "245130"
  },
  {
    "text": "you do that you can use something like the Kinesis client library to build your",
    "start": "245130",
    "end": "250230"
  },
  {
    "text": "custom applications deployed on your ec2 instances it's a lot of control a lot of",
    "start": "250230",
    "end": "255329"
  },
  {
    "text": "flexibility that you then trade off for the corresponding work that you do KCl",
    "start": "255329",
    "end": "260970"
  },
  {
    "text": "is not the only way you can build an application you will learn today how ad will have built build they're consuming applications",
    "start": "260970",
    "end": "267580"
  },
  {
    "text": "using storm there are several other connectors available including that for the broader Hadoop ecosystem of course",
    "start": "267580",
    "end": "274120"
  },
  {
    "text": "storm spark spark streaming and so on but the my focus will be to kind of talk",
    "start": "274120",
    "end": "280570"
  },
  {
    "text": "to you a little bit more about about the Kinesis client library so those are the five things that he gave to that okay so",
    "start": "280570",
    "end": "287640"
  },
  {
    "text": "so let's let's dig into details streaming data ingestion so how do you",
    "start": "287640",
    "end": "293950"
  },
  {
    "text": "put data so you first you provision an entity called a stream a stream is composed of units of scale called shards",
    "start": "293950",
    "end": "299770"
  },
  {
    "text": "a shard can do a megabyte per second ingress two megabytes per second egress there's a 1 is to 2 ratio on the data",
    "start": "299770",
    "end": "307330"
  },
  {
    "text": "capacity of any given shard how you put data is that your data producer does a",
    "start": "307330",
    "end": "312880"
  },
  {
    "text": "put API call there are a couple of versions of it there's a put record and I put records we'll talk about put",
    "start": "312880",
    "end": "318490"
  },
  {
    "text": "records and more detail next you put data each the size of a single record",
    "start": "318490",
    "end": "323919"
  },
  {
    "text": "can be no larger than one megabyte what we notice typically with streaming data is that the individual discrete record",
    "start": "323919",
    "end": "330070"
  },
  {
    "text": "tends to be much smaller kilobyte even sub kilobyte but you can easily have some flexibility to put up to a megabyte",
    "start": "330070",
    "end": "336729"
  },
  {
    "text": "size payload if you will into into your stream an essential component of the",
    "start": "336729",
    "end": "342190"
  },
  {
    "text": "put/call is a partition key you supply the partition key the partition key gets md5 hash and then that is distributed",
    "start": "342190",
    "end": "349630"
  },
  {
    "text": "across the corresponding shard into your stream we return success for each record put by way of a sequence number and as",
    "start": "349630",
    "end": "357520"
  },
  {
    "text": "of a month ago we are also fixing an approximate arrival timestamp on all the records",
    "start": "357520",
    "end": "362820"
  },
  {
    "text": "that are successfully put in the stream we're very we've chosen the name",
    "start": "362820",
    "end": "370930"
  },
  {
    "text": "approximate arrival timestamp very specifically because that is what it is",
    "start": "370930",
    "end": "376110"
  },
  {
    "text": "ok so so let's go let's go let's start digging deeper for those of you who are",
    "start": "376110",
    "end": "382030"
  },
  {
    "text": "getting started with kinases and for several of you who kind of made a lot of progress you fall in one of two camps",
    "start": "382030",
    "end": "387100"
  },
  {
    "text": "you'll fall on the camp that looks at Canisius and says ah I've got a great low-cost highly performant managed",
    "start": "387100",
    "end": "393340"
  },
  {
    "text": "buffer right and I would fully expect there about 65 to 70 percent",
    "start": "393340",
    "end": "398990"
  },
  {
    "text": "use cases not customers use cases will fall in that camp so what is what is",
    "start": "398990",
    "end": "404420"
  },
  {
    "text": "really that use case about it's about your data producers log servers smartphone application mobile",
    "start": "404420",
    "end": "411260"
  },
  {
    "text": "application sensors so on so forth are producing a rapid rate of data you want to make sure that you capture all the",
    "start": "411260",
    "end": "416930"
  },
  {
    "text": "data first and you create this kind of reliable buffer in there for those four",
    "start": "416930",
    "end": "422090"
  },
  {
    "text": "if you use cases of that kind then the partition key construct that we talked about that goes inside of each foot call",
    "start": "422090",
    "end": "428870"
  },
  {
    "text": "that distributes the data across the stream can be random if it's random then it's less likely that you will hit upon",
    "start": "428870",
    "end": "435440"
  },
  {
    "text": "any sort of a hot partition key and therefore a hot shot problem right in that situation your data producer as",
    "start": "435440",
    "end": "443030"
  },
  {
    "text": "long as you're able to generate a sufficiently high cardinality of partition keys to the shards in your",
    "start": "443030",
    "end": "449150"
  },
  {
    "text": "stream you will be more or less okay we'll talk more about what it means to",
    "start": "449150",
    "end": "456080"
  },
  {
    "text": "split a shard when you actually need to on the subsequent slide but the key thing is when you look at your use case",
    "start": "456080",
    "end": "461450"
  },
  {
    "text": "do you care about the partition key construct can you put any business logic around it or not a smaller segment of",
    "start": "461450",
    "end": "469040"
  },
  {
    "text": "use cases will belong in the world that says I want streaming Map Reduce and why is that every time you put data with a",
    "start": "469040",
    "end": "475190"
  },
  {
    "text": "given partition key it always gets routed into that specific shard right so",
    "start": "475190",
    "end": "481310"
  },
  {
    "text": "in effect all puts all records put on a partition key are getting naturally aggregated so it is in effect a mapper",
    "start": "481310",
    "end": "487910"
  },
  {
    "text": "function and this is interesting because on the consumer side then you will retrieve all data associate with the",
    "start": "487910",
    "end": "493640"
  },
  {
    "text": "partition key so for those of you who are pretty deep with the MapReduce model will will appreciate how this can",
    "start": "493640",
    "end": "499790"
  },
  {
    "text": "translate this translates a little bit into into that exact model in your case you will design the partition key",
    "start": "499790",
    "end": "505850"
  },
  {
    "text": "strategy to be something specific for you for instance you want to aggregate all data belonging to a given client",
    "start": "505850",
    "end": "513310"
  },
  {
    "text": "this might be their metering data you might want to aggregate all data belonging to being generated by a given",
    "start": "513310",
    "end": "521300"
  },
  {
    "text": "company because you are generating metered usage for them if you're in the",
    "start": "521300",
    "end": "526700"
  },
  {
    "text": "financial services space you want to track all the all the all transactions that have all trade transaction",
    "start": "526700",
    "end": "533240"
  },
  {
    "text": "happen for a given stock symbol so you want to put data associated with a given stock symbol with a given partition here",
    "start": "533240",
    "end": "540350"
  },
  {
    "text": "set of partition keys so you need to design a partition key strategy that then makes the most sense in that",
    "start": "540350",
    "end": "545600"
  },
  {
    "text": "specific use case okay so let's say now you've kind of figure that out great now",
    "start": "545600",
    "end": "553040"
  },
  {
    "text": "you go figure out how to put data in the vast majority of cases what we've learned is that the best way to get to",
    "start": "553040",
    "end": "559670"
  },
  {
    "text": "high frequency puts into your Kinesis stream is to use the put records API the",
    "start": "559670",
    "end": "565940"
  },
  {
    "text": "put records API can support up to 500 discrete records in a single call the",
    "start": "565940",
    "end": "571580"
  },
  {
    "text": "overall size of the API request itself can be up to five megabytes so you can",
    "start": "571580",
    "end": "577820"
  },
  {
    "text": "do the math as to how big an individual record can be such that you don't violate the overall API request size but",
    "start": "577820",
    "end": "584600"
  },
  {
    "text": "not violating obviously the individual record size constrained as well so you",
    "start": "584600",
    "end": "589850"
  },
  {
    "text": "can so the different records in that in that in that put trackers call can have different partition keys but it's",
    "start": "589850",
    "end": "595339"
  },
  {
    "text": "obviously being directed only to one single Kinesis train so just like before",
    "start": "595339",
    "end": "601730"
  },
  {
    "text": "a successful response will give you a sequence number and also the shard ID which basically tells you which shard",
    "start": "601730",
    "end": "607070"
  },
  {
    "text": "was that record routed into so that's easy the more interesting case is when",
    "start": "607070",
    "end": "612350"
  },
  {
    "text": "you get an unsuccessful response because this can this is not an atomic operation so out of 500 records that you put you",
    "start": "612350",
    "end": "619040"
  },
  {
    "text": "know maybe the 7th 418 and the 352nd failed so in those cases what you're",
    "start": "619040",
    "end": "625370"
  },
  {
    "text": "gonna get is you know is a response object that looks something like that this was the number of records that",
    "start": "625370",
    "end": "631459"
  },
  {
    "text": "failed and then forth for each record that failed you will get an error code an error message you know and what",
    "start": "631459",
    "end": "638029"
  },
  {
    "text": "potential at a record an error message so what does it look like it looks something like this",
    "start": "638029",
    "end": "643220"
  },
  {
    "text": "so in this is an example of a boot records response the first one succeeded it returned a sequence number two shot",
    "start": "643220",
    "end": "650089"
  },
  {
    "text": "ID the second one failed and the error code is a provision throughput exceeded exception and it tells you that the it",
    "start": "650089",
    "end": "657770"
  },
  {
    "text": "happened because the rate exceeded for the shard ID many zeros followed by one",
    "start": "657770",
    "end": "663020"
  },
  {
    "text": "in stream name called examples tree okay so now you're gonna get this response object back and you can detect",
    "start": "663020",
    "end": "669019"
  },
  {
    "text": "well which one's fit how many how many records failed and that put/call and what reason did they fail for so for",
    "start": "669019",
    "end": "675619"
  },
  {
    "text": "those of you who have any experience with provisioned systems will know that the provision throughput exceeded error",
    "start": "675619",
    "end": "680899"
  },
  {
    "text": "is kind of your nemesis we'll talk about how to deal with that a little bit in the Kinesis construct but the key thing",
    "start": "680899",
    "end": "686569"
  },
  {
    "text": "is that's the error code that we've experienced is the one you will watch out for the most okay so now that you",
    "start": "686569",
    "end": "693170"
  },
  {
    "text": "you your producer that has to examine if you do care about getting those failed records back in and you and you're not",
    "start": "693170",
    "end": "700279"
  },
  {
    "text": "tolerant for any data loss because of the producer not able to put data successfully into the Kinesis endpoint",
    "start": "700279",
    "end": "707389"
  },
  {
    "text": "you'll examine that object you'll check the failed count parameter and for each put records entry with a non null entry",
    "start": "707389",
    "end": "714290"
  },
  {
    "text": "for error code you'll have to package into a secondary follow-on put records",
    "start": "714290",
    "end": "720949"
  },
  {
    "text": "request cool okay that sounds like a big pain so sometimes it might be a great",
    "start": "720949",
    "end": "727309"
  },
  {
    "text": "idea to use this other thing a Kinesis producer library so this is also an open source library that we've",
    "start": "727309",
    "end": "732829"
  },
  {
    "text": "put out it's it's highly configurable so instead of writing the application using",
    "start": "732829",
    "end": "739850"
  },
  {
    "text": "put records for your producer which might be required in certain cases you",
    "start": "739850",
    "end": "745059"
  },
  {
    "text": "could also use this producer library build that into your data producer",
    "start": "745059",
    "end": "752149"
  },
  {
    "text": "application running across let's say your fleet of servers this is targeted for server-side environments not let's",
    "start": "752149",
    "end": "758959"
  },
  {
    "text": "say a mobile client that is putting data into your Kinesis stream so it's",
    "start": "758959",
    "end": "764089"
  },
  {
    "text": "definitely kind of for server-side environments and you can have the Kinesis producer library which can write",
    "start": "764089",
    "end": "770689"
  },
  {
    "text": "to one or more Kinesis streams and it has an automatic Cree configurable mechanism it collects the records that",
    "start": "770689",
    "end": "778069"
  },
  {
    "text": "is being generated by the producer it uses put records which means you know as we learned you can put multiple records",
    "start": "778069",
    "end": "783649"
  },
  {
    "text": "in in a single API across multiple shards in your stream it can track the",
    "start": "783649",
    "end": "788809"
  },
  {
    "text": "record age and enforces buffering times you know over here you have an example of of the",
    "start": "788809",
    "end": "797709"
  },
  {
    "text": "this is an instance of the Kinesis produced a configuration class you can then passes on into your Kinesis",
    "start": "797709",
    "end": "803839"
  },
  {
    "text": "producer constructor or you can know the configurations file from where you can set those set those properties the key",
    "start": "803839",
    "end": "812000"
  },
  {
    "text": "thing over here is that the Kinesis producer library plays really well with the Kinesis Kinesis client library we",
    "start": "812000",
    "end": "818930"
  },
  {
    "text": "shall talk about a little later more specifically the way where they play really well is let's say you are",
    "start": "818930",
    "end": "824589"
  },
  {
    "text": "aggregating data and to send a single larger record which is a mode that the",
    "start": "824589",
    "end": "830720"
  },
  {
    "text": "kpl also permits the Kinesis client library on the other hand can can unpack that and then expose to you the raw",
    "start": "830720",
    "end": "838040"
  },
  {
    "text": "record so then you can go about and do your processing so that's less there's one less you know important code",
    "start": "838040",
    "end": "844339"
  },
  {
    "text": "function that you need to write on the consumer side and perhaps most interestingly for many you can it also",
    "start": "844339",
    "end": "850279"
  },
  {
    "text": "have miss CloudWatch metrics so now you can keep track of how your producer is doing so so that that's that's very",
    "start": "850279",
    "end": "855620"
  },
  {
    "text": "useful something that we recommend and the github link is right there against that you can pull that down and",
    "start": "855620",
    "end": "861410"
  },
  {
    "text": "integrate the Kinesis producer library okay so now you're on your way to doing lots of great high-throughput puts",
    "start": "861410",
    "end": "867319"
  },
  {
    "text": "either using the put records API or the KPI this is probably the number one most",
    "start": "867319",
    "end": "874339"
  },
  {
    "text": "requested feature since we launched two years ago Kinesis streams launched with",
    "start": "874339",
    "end": "879860"
  },
  {
    "text": "a default 24-hour retention period and the which meant that great 24 hours a",
    "start": "879860",
    "end": "885380"
  },
  {
    "text": "day is safe but if you're consuming application for whatever reason does not is down and does not come out for a long",
    "start": "885380",
    "end": "892970"
  },
  {
    "text": "enough period of time such that is unable to catch up on that data it meant that the oldest 24-hour data would start",
    "start": "892970",
    "end": "900110"
  },
  {
    "text": "getting expired from the system as a customer you would see that as a data loss so now we have included a",
    "start": "900110",
    "end": "907160"
  },
  {
    "text": "configurable retention up to 7 days this comes with two new API is increase and",
    "start": "907160",
    "end": "913190"
  },
  {
    "text": "decrease stream retention period by default it's 24 hours we think it's going to be very useful for obvious",
    "start": "913190",
    "end": "919880"
  },
  {
    "text": "reasons you know the three-day weekend storm the the pager snafu and so on and",
    "start": "919880",
    "end": "926000"
  },
  {
    "text": "so forth that we all have to deal with but one of the most interesting modes of our that you can happily run with the D for",
    "start": "926000",
    "end": "933380"
  },
  {
    "text": "24 hours you step into the long weekend and you discover that on Saturday",
    "start": "933380",
    "end": "939350"
  },
  {
    "text": "evening something Bad's going on you can at that point use it as an always-on you",
    "start": "939350",
    "end": "944750"
  },
  {
    "text": "can use it as a response to an operational event so you know for those who lean manufacturing enough obviously",
    "start": "944750",
    "end": "950600"
  },
  {
    "text": "in in DevOps mode it's then and on cord you can use use it as an end on cord so",
    "start": "950600",
    "end": "956420"
  },
  {
    "text": "when the event is happening you can say hey increase my stream retention period right then which means all the data that",
    "start": "956420",
    "end": "962540"
  },
  {
    "text": "was already in the stream will now not age out at 24 hours but will age out only at whatever other increased",
    "start": "962540",
    "end": "969770"
  },
  {
    "text": "timeframe you had so so it can help kind of save save the situation if you will",
    "start": "969770",
    "end": "975350"
  },
  {
    "text": "and of course if your default mode is that you want to run the stream with you know three days or four is a retention",
    "start": "975350",
    "end": "981590"
  },
  {
    "text": "because you want to run back testing against data that was three days old in",
    "start": "981590",
    "end": "987230"
  },
  {
    "text": "this stream with your current data in tandem because you're cool like that you can also do that okay",
    "start": "987230",
    "end": "994510"
  },
  {
    "text": "the seed with topic number five kind of halfway through my portion of the of the",
    "start": "994510",
    "end": "1000430"
  },
  {
    "text": "talk so as I said earlier provision",
    "start": "1000430",
    "end": "1006460"
  },
  {
    "text": "throughput exceeded exception errors will be your nemesis this is true for most provision systems so you can always",
    "start": "1006460",
    "end": "1013000"
  },
  {
    "text": "run over provision thankfully Kinesis shards are relatively inexpensive and so we've noticed in practice that for many",
    "start": "1013000",
    "end": "1020350"
  },
  {
    "text": "in many cases costumes actually run a little over provision but that's not a",
    "start": "1020350",
    "end": "1026050"
  },
  {
    "text": "satisfactory answer right so two things number one you got to keep track of the",
    "start": "1026050",
    "end": "1032170"
  },
  {
    "text": "metrics while we will do the work needed to expose more metrics the ones that we should care the most about from an",
    "start": "1032170",
    "end": "1039040"
  },
  {
    "text": "ingestion standpoint are written down or on the slide in they're not going to read them but you can imagine the amount",
    "start": "1039040",
    "end": "1044650"
  },
  {
    "text": "of data that you're putting the incoming bytes record success rates and latencies are all in there while there is a split",
    "start": "1044650",
    "end": "1052360"
  },
  {
    "text": "shot in a merged shard API to help scale your stream up or down it can be",
    "start": "1052360",
    "end": "1058710"
  },
  {
    "text": "operationally complex to implement in practice and I think Nick will also touch upon that",
    "start": "1058710",
    "end": "1064240"
  },
  {
    "text": "and stop so to help you get started there is a scaling utility that will",
    "start": "1064240",
    "end": "1070120"
  },
  {
    "text": "that can monitor your stream and you can set it to say increase my short count by",
    "start": "1070120",
    "end": "1075130"
  },
  {
    "text": "20% or decrease my short con by 5% or increase my short count by some specific",
    "start": "1075130",
    "end": "1081940"
  },
  {
    "text": "absolute number 15 in response to in response to an event so it's a good way",
    "start": "1081940",
    "end": "1088179"
  },
  {
    "text": "to get started so that you don't have to necessarily work with with a very",
    "start": "1088179",
    "end": "1093970"
  },
  {
    "text": "lower-level split and merge short api's okay but but this is really but the important thing here is that you know",
    "start": "1093970",
    "end": "1100570"
  },
  {
    "text": "errors like these can happen and from an operational readiness perspective you need to know what tools you can use to",
    "start": "1100570",
    "end": "1108130"
  },
  {
    "text": "make sure that Kinesis can respond respond to that event with your control",
    "start": "1108130",
    "end": "1115710"
  },
  {
    "text": "okay now we're gonna switch directions a little bit we talked a lot about putting data so five things to help what we've",
    "start": "1115710",
    "end": "1123250"
  },
  {
    "text": "learned over the last two years that customers ask us about the most right",
    "start": "1123250",
    "end": "1128500"
  },
  {
    "text": "okay so now let's talk about the five things customers ask us about the most",
    "start": "1128500",
    "end": "1133630"
  },
  {
    "text": "on the Kinesis client library and for those of you who don't know what the KCl of the Keeney's client library is let's",
    "start": "1133630",
    "end": "1139960"
  },
  {
    "text": "do a really quick overview the client lab is free open source library for you",
    "start": "1139960",
    "end": "1145630"
  },
  {
    "text": "to build stream processing applications you have a tremendous variety of choices",
    "start": "1145630",
    "end": "1150700"
  },
  {
    "text": "on how to get started with stream processing applications we love all of them this is one of them that we've",
    "start": "1150700",
    "end": "1156790"
  },
  {
    "text": "built built for developers what does it do so think about building our",
    "start": "1156790",
    "end": "1163030"
  },
  {
    "text": "application with this library you run it on your ec2 instance what is gonna do is register us with the stream so it",
    "start": "1163030",
    "end": "1168850"
  },
  {
    "text": "connects to that Kinesis stream you wish to consume from it'll go ahead and enumerate the number of shots they have",
    "start": "1168850",
    "end": "1174070"
  },
  {
    "text": "a five-shot stream say okay I've counted five shards shot ID zero one through short ID zero five and then what is",
    "start": "1174070",
    "end": "1183550"
  },
  {
    "text": "gonna do is that it's going to keep build out a shard management table so for every single shot is gonna say okay",
    "start": "1183550",
    "end": "1189910"
  },
  {
    "text": "I'm what I'm gonna do next is spin up a worker on this instance where I'm going to spin up an entity called a record",
    "start": "1189910",
    "end": "1196390"
  },
  {
    "text": "processor and I'm gonna have a record processor bushard in the stream so it's gonna do this the record",
    "start": "1196390",
    "end": "1202480"
  },
  {
    "text": "processor shard and shard allocation it'll pull the rack it will pull the",
    "start": "1202480",
    "end": "1208000"
  },
  {
    "text": "data data records from the shards in the stream and then pushes it to the",
    "start": "1208000",
    "end": "1213100"
  },
  {
    "text": "corresponding record processor that is supposed to consume that data it will",
    "start": "1213100",
    "end": "1218170"
  },
  {
    "text": "help enable checkpoints so you can say hey after every thousand records that I see I want you to write a checkpoint so",
    "start": "1218170",
    "end": "1223870"
  },
  {
    "text": "for every record processor that is attached to every shard in your stream you get a checkpoint and then if your if",
    "start": "1223870",
    "end": "1230380"
  },
  {
    "text": "your stream scales so let's say you use the scaling utility you've added then shot ten more shots you stream the KCl",
    "start": "1230380",
    "end": "1236710"
  },
  {
    "text": "detects that because it's it constantly numerating the shards in your stream it's gonna say hey listen looks like I",
    "start": "1236710",
    "end": "1241780"
  },
  {
    "text": "see ten new shards I need to spin up ten new record processors across my set of",
    "start": "1241780",
    "end": "1246850"
  },
  {
    "text": "ec2 instances and we've balanced these associations as the stream scales up or",
    "start": "1246850",
    "end": "1252040"
  },
  {
    "text": "down let's look at a picture that says the same thread to get the concepts just one more time so there's the only higher-level slide here in this topic",
    "start": "1252040",
    "end": "1259840"
  },
  {
    "text": "you have an application the application is named you deploy it on an ec2",
    "start": "1259840",
    "end": "1267790"
  },
  {
    "text": "instance you can deploy it on many but poor ec2 instance there's a thing called a worker the worker runs record",
    "start": "1267790",
    "end": "1275650"
  },
  {
    "text": "processors record processors map to shards KCl takes care of all of that your business",
    "start": "1275650",
    "end": "1282760"
  },
  {
    "text": "logic sits in the record processor KCl does it's magic of enumerated shards and",
    "start": "1282760",
    "end": "1287860"
  },
  {
    "text": "and keeping track of who's doing what and then populates every record processor with the get records call",
    "start": "1287860",
    "end": "1293890"
  },
  {
    "text": "results that is pulled from the from the underlying shot and as I said all your",
    "start": "1293890",
    "end": "1299890"
  },
  {
    "text": "business logic lives inside of process records only if life were that easy",
    "start": "1299890",
    "end": "1305020"
  },
  {
    "text": "right so what's gonna happen is you're gonna spin up your your first application and you're gonna have it",
    "start": "1305020",
    "end": "1310720"
  },
  {
    "text": "start consuming data from you Kinesis stream and one of the first thing you'll say is ah I'm getting a bunch of empty",
    "start": "1310720",
    "end": "1316360"
  },
  {
    "text": "records and I know I better in the stream because I'm putting it and because Ari said I checked the metrics -",
    "start": "1316360",
    "end": "1321430"
  },
  {
    "text": "and data is indeed being put into my stream so so what gives so the consume the consumption model",
    "start": "1321430",
    "end": "1328330"
  },
  {
    "text": "with Kinesis of Kinesis applications is a pool way model so the developer here is expected",
    "start": "1328330",
    "end": "1334220"
  },
  {
    "text": "to call get records in a continuous loop with no back off so every call to get",
    "start": "1334220",
    "end": "1340820"
  },
  {
    "text": "records will return a shard iterator value so what is the shard iterator a",
    "start": "1340820",
    "end": "1346430"
  },
  {
    "text": "shard iterator is basically a cursor off the reader on the shard so it's a",
    "start": "1346430",
    "end": "1352670"
  },
  {
    "text": "position within the char to tell you which record am i reading right now inside of a shard and because there are",
    "start": "1352670",
    "end": "1358700"
  },
  {
    "text": "multiple shards and potentially multiple consuming applications each of them gets their own short iterator so the get",
    "start": "1358700",
    "end": "1365840"
  },
  {
    "text": "records operation is does not block so n returns immediately with either the relevant data records that is pulled out",
    "start": "1365840",
    "end": "1372800"
  },
  {
    "text": "or with an empty records element now an empty records element in turn can be",
    "start": "1372800",
    "end": "1377960"
  },
  {
    "text": "returned under a couple of conditions one there is legitimately no more data containing contained in the shard or the",
    "start": "1377960",
    "end": "1387530"
  },
  {
    "text": "more nuanced one is that there is no data near the part of the shard that is",
    "start": "1387530",
    "end": "1392600"
  },
  {
    "text": "pointed to right now by the shard iterator itself and this condition is subtle but it's it's kind of a necessary",
    "start": "1392600",
    "end": "1398870"
  },
  {
    "text": "design trade-off to avoid the condition of an unbounded seek time which",
    "start": "1398870",
    "end": "1404330"
  },
  {
    "text": "essentially will return will translate into very long latency right it's when",
    "start": "1404330",
    "end": "1409340"
  },
  {
    "text": "retrieving records so this so what's the net event the net of it is that the the",
    "start": "1409340",
    "end": "1414680"
  },
  {
    "text": "consuming application is to loop and call get records and handle the situation around empty records as a",
    "start": "1414680",
    "end": "1420950"
  },
  {
    "text": "matter of course right as as a part of the overall processing now in the production scenario the only time this",
    "start": "1420950",
    "end": "1427160"
  },
  {
    "text": "happens when the continuous is is exited is when the next shot iterator value",
    "start": "1427160",
    "end": "1432770"
  },
  {
    "text": "that you get is now right and when it is null it means that the current shard is",
    "start": "1432770",
    "end": "1437780"
  },
  {
    "text": "closed and the short can and the shard is closed typically after let's say you",
    "start": "1437780",
    "end": "1443630"
  },
  {
    "text": "split a shard into two child shards and the in the in the lifecycle of a char it",
    "start": "1443630",
    "end": "1449150"
  },
  {
    "text": "goes from you know open to being closed which means this short cannot be written to anymore all that can be read from",
    "start": "1449150",
    "end": "1455000"
  },
  {
    "text": "until that it until until this retention period expires",
    "start": "1455000",
    "end": "1461140"
  },
  {
    "text": "when they're in the next shot iterator is null it means that the current shot as I said is closed and that the short",
    "start": "1463160",
    "end": "1469320"
  },
  {
    "text": "iterator value otherwise would be pointing to something past the last record which which wouldn't make sense",
    "start": "1469320",
    "end": "1475830"
  },
  {
    "text": "now if the consuming application never calls the split shot or the merge charts",
    "start": "1475830",
    "end": "1480990"
  },
  {
    "text": "then the shard remains open and the calls of get records would never return our next shot iterator value that is not",
    "start": "1480990",
    "end": "1487440"
  },
  {
    "text": "so if you use the KCl the above consumption pattern is largely",
    "start": "1487440",
    "end": "1492930"
  },
  {
    "text": "abstracted for you which includes kind of the automatic handling of you know set of shorts denim dynamically change",
    "start": "1492930",
    "end": "1499320"
  },
  {
    "text": "and you know the important thing here is that as a developer you take care off of",
    "start": "1499320",
    "end": "1507570"
  },
  {
    "text": "the scenario when when there are empty empty records being being driven out of",
    "start": "1507570",
    "end": "1514260"
  },
  {
    "text": "the shard as part of a get get next records call make sure you're calling a continuous loop okay topic number two",
    "start": "1514260",
    "end": "1522030"
  },
  {
    "text": "some records are skipped right and in the key here is that the process records",
    "start": "1522030",
    "end": "1527550"
  },
  {
    "text": "business logic is where you'll have to handle this exception because the KCl is",
    "start": "1527550",
    "end": "1535740"
  },
  {
    "text": "abstracting a bunch of stuff for you it will tend to absorb a number of these situations and what you will need to do",
    "start": "1535740",
    "end": "1542910"
  },
  {
    "text": "is to kind of when this recurring failure and you don't want to get into the situation of having infinite retries",
    "start": "1542910",
    "end": "1549710"
  },
  {
    "text": "do not resend the batch of records back into your process records else what",
    "start": "1549710",
    "end": "1555510"
  },
  {
    "text": "you're going to do is purposefully skip those records so because the way it works is that process records for every",
    "start": "1555510",
    "end": "1561120"
  },
  {
    "text": "next batch of datas without restarting the record processor itself so the key",
    "start": "1561120",
    "end": "1566700"
  },
  {
    "text": "thing here is that in the process records logic just like we talked about dealing with dealing with empty records",
    "start": "1566700",
    "end": "1574710"
  },
  {
    "text": "that are returned you'll have to do a little bit of work to to handle this specific exception as well and to kind",
    "start": "1574710",
    "end": "1581700"
  },
  {
    "text": "of help shine some more light here we also have a ton of shard level metrics which are very frequently not used which",
    "start": "1581700",
    "end": "1589890"
  },
  {
    "text": "is kind of sad so there are a bunch of service level metrics that's great but when you use the Kinesis client",
    "start": "1589890",
    "end": "1596400"
  },
  {
    "text": "library we actually get a very deeper set of metrics on on a per shard level",
    "start": "1596400",
    "end": "1602250"
  },
  {
    "text": "which you can then use for a variety of reasons including saying am i running slow am i running behind",
    "start": "1602250",
    "end": "1607710"
  },
  {
    "text": "is there more data in the stream that I should be consuming is it the right time for me to be scaling the stream and a",
    "start": "1607710",
    "end": "1613920"
  },
  {
    "text": "lot of that great information is stuffed inside of these shard level metrics which are automatic are turned on by in",
    "start": "1613920",
    "end": "1621060"
  },
  {
    "text": "KCl by default and they're emitted into cloud watch so if you go into your cloud watch metrics console you will see on a",
    "start": "1621060",
    "end": "1627480"
  },
  {
    "text": "per shard basis for every KCl application all of these metrics okay",
    "start": "1627480",
    "end": "1634590"
  },
  {
    "text": "another topic very common and for those of you who are either already in production or or or will get started",
    "start": "1634590",
    "end": "1641280"
  },
  {
    "text": "with streaming data will go come across this this is a near real-time real-time system data scan you see being generated",
    "start": "1641280",
    "end": "1647220"
  },
  {
    "text": "being put and being consumed and what you don't want is for the system to slow",
    "start": "1647220",
    "end": "1652710"
  },
  {
    "text": "down but there might be scenarios when you go well the consumer is reading slower than expected so what's going on",
    "start": "1652710",
    "end": "1660000"
  },
  {
    "text": "are the most common reasons for the read throughput if you will be being slower",
    "start": "1660000",
    "end": "1667290"
  },
  {
    "text": "than expected is as follows Kinesis allows you to have multiple different",
    "start": "1667290",
    "end": "1672510"
  },
  {
    "text": "applications consuming from the strange stream and more specifically consuming data or reading it from the same shards",
    "start": "1672510",
    "end": "1679140"
  },
  {
    "text": "but it does have limits right each given shard has one megabyte per second in two megabytes a second out you can do a",
    "start": "1679140",
    "end": "1686040"
  },
  {
    "text": "thousand records per second ingress and it does five reads per second egress now",
    "start": "1686040",
    "end": "1692040"
  },
  {
    "text": "some of you are probably shocked wait that makes no sense now the good news is",
    "start": "1692040",
    "end": "1697530"
  },
  {
    "text": "that a single get records call can return up to ten megabytes of data so",
    "start": "1697530",
    "end": "1703110"
  },
  {
    "text": "you're getting a little bit of a bash functionality when you retrieve data from the shards in your stream the bad",
    "start": "1703110",
    "end": "1709710"
  },
  {
    "text": "news is that there's just five of those calls that you can make so if you start attaching multiple consumers by which I",
    "start": "1709710",
    "end": "1717330"
  },
  {
    "text": "mean independent applications you might come across situation where capacity is fine so you have all the data that's",
    "start": "1717330",
    "end": "1723480"
  },
  {
    "text": "saying you've enough read capacity in terms of the data being pulled out but you get throttled on the read TPS an",
    "start": "1723480",
    "end": "1730979"
  },
  {
    "text": "obvious thing is well we should go fix it yes but having said that having said",
    "start": "1730979",
    "end": "1736859"
  },
  {
    "text": "that you know if you get a provision throughput exceeded error the key thing is it can happen on ingress because",
    "start": "1736859",
    "end": "1743549"
  },
  {
    "text": "you're putting more data at a higher rate than the shard can take but can also happen on egress right however what",
    "start": "1743549",
    "end": "1754830"
  },
  {
    "text": "we noticed let's say that that if that is situation it is somewhat relatively it can't dress it by simply splitting",
    "start": "1754830",
    "end": "1761999"
  },
  {
    "text": "shots so if you have more more shards in your stream you are less likely to run up against that that specific problem",
    "start": "1761999",
    "end": "1768710"
  },
  {
    "text": "but let's say that that is not the issue what we notice most often when we go and",
    "start": "1768710",
    "end": "1773970"
  },
  {
    "text": "you know how debug code with customers is that the maximum number of get",
    "start": "1773970",
    "end": "1779159"
  },
  {
    "text": "records per call limit has been configured to a lower value right so as",
    "start": "1779159",
    "end": "1784440"
  },
  {
    "text": "I said earlier the the maximum get records we can retrieve 10,000 records in a single call right and and it's it's",
    "start": "1784440",
    "end": "1791309"
  },
  {
    "text": "actually very efficient call to go ahead and do that which is actually the system default and and we've noticed that",
    "start": "1791309",
    "end": "1799919"
  },
  {
    "text": "normally there is little to no impact over there when you when someone tries to configure it to a lower or a",
    "start": "1799919",
    "end": "1806279"
  },
  {
    "text": "different number you see more often than not this problem arise without necessary",
    "start": "1806279",
    "end": "1812009"
  },
  {
    "text": "having optimize the system so this is one of those cases where the system defaults might actually be a good idea",
    "start": "1812009",
    "end": "1818389"
  },
  {
    "text": "now the other obvious reason is that the logic inside of your process records is",
    "start": "1818389",
    "end": "1823830"
  },
  {
    "text": "just taking longer than expected and this happens a lot too which is my applications running slow I don't know",
    "start": "1823830",
    "end": "1829979"
  },
  {
    "text": "what is going on and and most often when",
    "start": "1829979",
    "end": "1836820"
  },
  {
    "text": "we have the conversation it is about no my application is perfect all right it is doing exactly what it's supposed",
    "start": "1836820",
    "end": "1841950"
  },
  {
    "text": "to do but still it's running slower and therefore the problem is on the Kinesis side because it may be unable to",
    "start": "1841950",
    "end": "1848159"
  },
  {
    "text": "retrieve records fast enough and typically what we've noticed is in case in that case when that does happen and",
    "start": "1848159",
    "end": "1853710"
  },
  {
    "text": "then we go how to figure out how to fix that but in most cases what we've seen is is some application",
    "start": "1853710",
    "end": "1862020"
  },
  {
    "text": "of writing process records has resulted in just you know a very CPU intensive",
    "start": "1862020",
    "end": "1867240"
  },
  {
    "text": "logic something that could have been written a little more efficiently or there is some sort of i/o blocking and",
    "start": "1867240",
    "end": "1872520"
  },
  {
    "text": "which case always start with making sure that you can run an empty record processor so record process is just",
    "start": "1872520",
    "end": "1878100"
  },
  {
    "text": "getting records and doing nothing else to see if your application is pulling data from the Kinesis stream fast enough",
    "start": "1878100",
    "end": "1884870"
  },
  {
    "text": "okay fourth topic and I move a little quicker if you remember there should be",
    "start": "1884870",
    "end": "1893040"
  },
  {
    "text": "one record processor pearl shard right that's that's how that's how the KCl allocation occurs but it can be the case",
    "start": "1893040",
    "end": "1900330"
  },
  {
    "text": "that sometimes multiple record processes are temporarily processing the same chart now the given worker running on an",
    "start": "1900330",
    "end": "1907140"
  },
  {
    "text": "ec2 instance loses network connectivity the KCl things at Oh that guy's out and need to go spin up another record",
    "start": "1907140",
    "end": "1913620"
  },
  {
    "text": "processor take its place to make sure that Candy's processing is happening so it spins up a new record processor but",
    "start": "1913620",
    "end": "1919860"
  },
  {
    "text": "the other one wasn't out for a long time it's out for me for a very brief moment in time and so it suddenly picks back up",
    "start": "1919860",
    "end": "1927030"
  },
  {
    "text": "and it also starts pulling data from the shard so now you have you know to record",
    "start": "1927030",
    "end": "1932250"
  },
  {
    "text": "processors both of whom are are consuming data from the same shard now",
    "start": "1932250",
    "end": "1938490"
  },
  {
    "text": "that is suppose the behavior of of KCl trying to spin up record processors move",
    "start": "1938490",
    "end": "1944280"
  },
  {
    "text": "the data using the checkpointing mechanism is something we want to do right we want to do that because because",
    "start": "1944280",
    "end": "1951030"
  },
  {
    "text": "that kind of logic helps keep things relatively simple and keeps things robust and going but in this specific",
    "start": "1951030",
    "end": "1958290"
  },
  {
    "text": "case when this does happen there are a couple of cases where grace will shut down as we perform and again from a",
    "start": "1958290",
    "end": "1964710"
  },
  {
    "text": "developer standpoint this is what you'll have to keep in mind right when whenever",
    "start": "1964710",
    "end": "1970500"
  },
  {
    "text": "the current call to a process records is completed you know in this case case we will invoke a method with the zombie",
    "start": "1970500",
    "end": "1977160"
  },
  {
    "text": "with a zombie reason shutdown reason and at that point you know the record",
    "start": "1977160",
    "end": "1982200"
  },
  {
    "text": "processors ability to gracefully shutdown exit and cleanup is actual is",
    "start": "1982200",
    "end": "1987720"
  },
  {
    "text": "part of coding best practices that has to be put in because very often we've seen unclean exits",
    "start": "1987720",
    "end": "1995860"
  },
  {
    "text": "meaning that there is some zombie record process continuing to read from a shard which was completely unexpected behavior",
    "start": "1995860",
    "end": "2002870"
  },
  {
    "text": "okay last topic duplicates is the streaming",
    "start": "2002870",
    "end": "2008760"
  },
  {
    "text": "error systems distributed lots of data coming in duplicates can happen duplicates can happen in two places one",
    "start": "2008760",
    "end": "2014400"
  },
  {
    "text": "is your producer rebooting data or because the internet happen input data maybe it was successful but the act got",
    "start": "2014400",
    "end": "2021030"
  },
  {
    "text": "lost in the way happens all the time tough problem to solve that's not what we were targeting right now what we're",
    "start": "2021030",
    "end": "2027090"
  },
  {
    "text": "gonna target instead is is when consumer retry so on the processing side stuff is",
    "start": "2027090",
    "end": "2033299"
  },
  {
    "text": "causing duplicates because this is something that that can happen with the case here and I want to make sure that",
    "start": "2033299",
    "end": "2039510"
  },
  {
    "text": "you guys know how best to deal with that with that situation so you know very",
    "start": "2039510",
    "end": "2044910"
  },
  {
    "text": "quickly there are three reasons why you you might see temporary rise in duplicates duplicate processing you know",
    "start": "2044910",
    "end": "2050970"
  },
  {
    "text": "you know a little bit now about the record processor shard modeling and how how the KCl operates if a worker",
    "start": "2050970",
    "end": "2056638"
  },
  {
    "text": "terminates unexpectedly the ec2 instance disappears you have new worker instances being added or removed which means KCl",
    "start": "2056639",
    "end": "2063240"
  },
  {
    "text": "is trying to move record processors in the shard allocations around or when shah's themselves are being merged or",
    "start": "2063240",
    "end": "2069000"
  },
  {
    "text": "split all these three conditions can lead to a temporary condition where retries are happening and therefore the",
    "start": "2069000",
    "end": "2076080"
  },
  {
    "text": "same data is being pulled and then reprocessed inside of the KCl and the record process within how do you deal",
    "start": "2076080",
    "end": "2088200"
  },
  {
    "text": "with it now while I would love to stand here and tell you about how exactly once the processing semantics are super",
    "start": "2088200",
    "end": "2093658"
  },
  {
    "text": "simple and they've all been solved for what we know it hasn't happened yet or",
    "start": "2093659",
    "end": "2098940"
  },
  {
    "text": "it has only happened with an enormous amount of difficulty pragmatically speaking what we've noticed is is as",
    "start": "2098940",
    "end": "2105900"
  },
  {
    "text": "follows make your final destination where that data is going to land to resilient arguably it's the most pragmatic",
    "start": "2105900",
    "end": "2111690"
  },
  {
    "text": "approach that we have noticed time and again has helped 85% of the cases your",
    "start": "2111690",
    "end": "2118260"
  },
  {
    "text": "mod is of course may very very you know very likely that you're using something like dynamo s3 or perhaps another",
    "start": "2118260",
    "end": "2124920"
  },
  {
    "text": "durable mechanism to get your final state of the data in one of the one of the mechanisms that",
    "start": "2124920",
    "end": "2132329"
  },
  {
    "text": "we've used to kind of mimic idempotent processing inside of the case here tends",
    "start": "2132329",
    "end": "2137430"
  },
  {
    "text": "to work pretty well and it goes as follows so you have a record processor you can tell the record processor always",
    "start": "2137430",
    "end": "2143579"
  },
  {
    "text": "process and you can set that in the config configuration properties always pull 5,000 records okay after you do",
    "start": "2143579",
    "end": "2150690"
  },
  {
    "text": "that you choose a file naming schema that might look something like what's on the board so you can say in this case",
    "start": "2150690",
    "end": "2156480"
  },
  {
    "text": "let's say we're putting data into s3 so you can say there's three prefix the",
    "start": "2156480",
    "end": "2162000"
  },
  {
    "text": "shard ID from which the process records is pulling data from and then the first sequence number within that batch of",
    "start": "2162000",
    "end": "2168660"
  },
  {
    "text": "records all of which is available in them okay so let's say that that you upload the file into s3 any checkpoint",
    "start": "2168660",
    "end": "2176039"
  },
  {
    "text": "because you have again the ability to checkpoint via KCl the last sequence number and let's say in this case",
    "start": "2176039",
    "end": "2182789"
  },
  {
    "text": "because you're pulling 5,000 records the last sequence number record last record",
    "start": "2182789",
    "end": "2188549"
  },
  {
    "text": "number will be end in 15,000 let's say it started first at 10,000 so let's say that something happens and you end up",
    "start": "2188549",
    "end": "2195119"
  },
  {
    "text": "reeling data Yury pulling data and you processing the same data twice but if it pulled from",
    "start": "2195119",
    "end": "2202349"
  },
  {
    "text": "from the same sequence number it's gonna end up rewriting based on that on that",
    "start": "2202349",
    "end": "2207410"
  },
  {
    "text": "specific schema so at the very minimum what you've done is you haven't created an overlapping file you ended up",
    "start": "2207410",
    "end": "2214140"
  },
  {
    "text": "rewriting the same file as part of your process records and this is an example of how you can I guess hack your way",
    "start": "2214140",
    "end": "2221460"
  },
  {
    "text": "into getting to an idempotent processing model and of course we can do much better ok",
    "start": "2221460",
    "end": "2227190"
  },
  {
    "text": "all that was super cool you can stand and read data from a variety of different mechanisms you know get api's",
    "start": "2227190",
    "end": "2233430"
  },
  {
    "text": "case here but the one that I'm most excited to kind of share with you is the",
    "start": "2233430",
    "end": "2238529"
  },
  {
    "text": "work that making the team at AdRoll have been doing with Kinesis and and storm",
    "start": "2238529",
    "end": "2245359"
  },
  {
    "text": "thanks",
    "start": "2245779",
    "end": "2248779"
  },
  {
    "text": "hey I'm Nick I'm a software engineer Admiral and as Eddie said for my portion",
    "start": "2253410",
    "end": "2258450"
  },
  {
    "text": "of the talk I'm gonna be kind of covering some of the work that we did to roll out Kinesis in our real-time data pipeline which is primarily driven by",
    "start": "2258450",
    "end": "2264210"
  },
  {
    "text": "storm so more specifically some of the things we're going to cover are what are",
    "start": "2264210",
    "end": "2270060"
  },
  {
    "text": "real-time data pipeline architecture looked like before we started using Kinesis and why we were motivated to",
    "start": "2270060",
    "end": "2275460"
  },
  {
    "text": "make the switch I'm gonna cover some things about how we set up our producer and consumer applications and some kind",
    "start": "2275460",
    "end": "2281430"
  },
  {
    "text": "of best practices in there and finally I wanted to talk about some of Kinesis is",
    "start": "2281430",
    "end": "2286920"
  },
  {
    "text": "service constraints and how they'll affect how you'll want to architect your real time data pipeline especially if",
    "start": "2286920",
    "end": "2292170"
  },
  {
    "text": "you're going to be having lots of applications concurrently consuming the same data from a single stream so quick",
    "start": "2292170",
    "end": "2299880"
  },
  {
    "text": "blurb about AdRoll more performance marketing platform and we enable brands to use their data to run intelligent ROI",
    "start": "2299880",
    "end": "2306090"
  },
  {
    "text": "positive marketing campaigns we're probably best known for doing retargeting I don't know if any of you",
    "start": "2306090",
    "end": "2311610"
  },
  {
    "text": "have ever had the experience where if you're shopping online for something like a refrigerator then suddenly after",
    "start": "2311610",
    "end": "2317010"
  },
  {
    "text": "viewing a refrigerator around on someone's webpage you're kind of chased around the internet for ads for a refrigerator that's that's us alright so",
    "start": "2317010",
    "end": "2326940"
  },
  {
    "text": "to do things like retargeting we leverage a number of real-time data sources so for one we're collecting",
    "start": "2326940",
    "end": "2332010"
  },
  {
    "text": "clickstream data from our advertisers websites via a JavaScript tag that we have our advertisers place on like the",
    "start": "2332010",
    "end": "2337140"
  },
  {
    "text": "footer of their website that kind of fords us data about what URLs the users that we've cookie to viewing we use this",
    "start": "2337140",
    "end": "2343980"
  },
  {
    "text": "data to build user profiles and inform our real-time bidding systems which are powered by machine learning which users",
    "start": "2343980",
    "end": "2349380"
  },
  {
    "text": "are most valuable to your brand's okay which is you should be showing ads to and also which ads you should be showing",
    "start": "2349380",
    "end": "2354660"
  },
  {
    "text": "to these users we're also constantly collecting data on the ads we're serving",
    "start": "2354660",
    "end": "2360180"
  },
  {
    "text": "so we want to know for each impression was it clicked did addressing high value user actually in your conversion and so on and also we constantly need to",
    "start": "2360180",
    "end": "2367800"
  },
  {
    "text": "monitor all of the real-time bidding auctions we participate in across a number of different exchanges we want to",
    "start": "2367800",
    "end": "2373200"
  },
  {
    "text": "know how often are we winning what's our average win price and that kind of thing so recency is very much",
    "start": "2373200",
    "end": "2379290"
  },
  {
    "text": "directly correlated with performance and most of these use cases and the sooner that we can act on our data the better",
    "start": "2379290",
    "end": "2384660"
  },
  {
    "text": "we perform the more opportunities would get to kind of drive revenue and show ads and who's kind of a",
    "start": "2384660",
    "end": "2392050"
  },
  {
    "text": "quick snapshot of kind of some of the scale of the events that we're dealing with",
    "start": "2392050",
    "end": "2397120"
  },
  {
    "text": "we served around 200 million impression events per day or something like that quarter of a terabyte of logs are involved there we process about 1.6",
    "start": "2397120",
    "end": "2404410"
  },
  {
    "text": "billion clip stream events from our advertisers websites or around 700 gigabytes of data per day and finally we",
    "start": "2404410",
    "end": "2410860"
  },
  {
    "text": "participate in about 60 billion real-time bidding auctions per day and that involves processing something like",
    "start": "2410860",
    "end": "2416170"
  },
  {
    "text": "80 terabytes of data per day and as of right now this is the this is the data",
    "start": "2416170",
    "end": "2422500"
  },
  {
    "text": "that were piping through Kinesis there's no practical reason why we couldn't also buy our RTB data through Kinesis and that's definitely something we'll probably be evaluating in the near",
    "start": "2422500",
    "end": "2428830"
  },
  {
    "text": "future but yeah this is the slice of the data that we're currently using Kinesis to process and so here's what our",
    "start": "2428830",
    "end": "2436900"
  },
  {
    "text": "architecture looks like before we move to Kinesis so we had a bunch of blog producing systems which are our real-time bidders and our ad servers",
    "start": "2436900",
    "end": "2442900"
  },
  {
    "text": "they constantly are generating logs and every 20 minutes they'll flush the data they've generated into a separate log",
    "start": "2442900",
    "end": "2449830"
  },
  {
    "text": "file per host and s3 then we use lambdas integration with s3 events to figure out",
    "start": "2449830",
    "end": "2455560"
  },
  {
    "text": "which file paths are for the newly generated log files before those file paths to a set of sqs queues which are",
    "start": "2455560",
    "end": "2461650"
  },
  {
    "text": "hooked up to our real-time data processing applications so those applications pull the sqs queues download the corresponding files that",
    "start": "2461650",
    "end": "2468790"
  },
  {
    "text": "are in the sqs messages process that data and then flush to our various end of pipeline data stores like s3 dynamodb",
    "start": "2468790",
    "end": "2474220"
  },
  {
    "text": "in our own HBase so because we're",
    "start": "2474220",
    "end": "2479530"
  },
  {
    "text": "flushing vlogs or we were flushing logs every 20 minutes on average for any given event for any given log event it",
    "start": "2479530",
    "end": "2485920"
  },
  {
    "text": "was 10 minutes before it was made available for processing in s3 and also because we're dumping so much data at",
    "start": "2485920",
    "end": "2491500"
  },
  {
    "text": "any given point in time it would usually be something like five minutes before any given event would be processed even after it's made available so end-to-end",
    "start": "2491500",
    "end": "2498040"
  },
  {
    "text": "our data processing latency in this old data pipeline architecture was something like 15 minutes and as I was kind of",
    "start": "2498040",
    "end": "2505270"
  },
  {
    "text": "alluding to before every second that we wait means kind of a missed opportunity and so we were very much motivated to",
    "start": "2505270",
    "end": "2512140"
  },
  {
    "text": "kind of reduce this this latency that we were experiencing and of course we could",
    "start": "2512140",
    "end": "2519200"
  },
  {
    "text": "logs more often to s3 so say we flushed our logs on a ten-minute interval instead of a 20-minute interval",
    "start": "2519200",
    "end": "2524589"
  },
  {
    "text": "theoretically that would reduce our end end data processing latency but that would also mean we were generating more",
    "start": "2524589",
    "end": "2529940"
  },
  {
    "text": "and more smaller and smaller log files as we kind of produce that interval and that kind of starts to kill our s3 performance and so we found flushing",
    "start": "2529940",
    "end": "2536660"
  },
  {
    "text": "logs every 20 minutes I'm kind of was a nice happy medium between the tension of maintaining s3 performance while also",
    "start": "2536660",
    "end": "2542930"
  },
  {
    "text": "kind of optimizing for latency but it also meant we're kind of bound to 15 minutes and to end processing timeline",
    "start": "2542930",
    "end": "2549920"
  },
  {
    "text": "and this is architecture so yeah we were",
    "start": "2549920",
    "end": "2555710"
  },
  {
    "text": "motivated to move this streaming service and kind of switch at the back end of our data pipeline and we came up with a list of requirements most of these are",
    "start": "2555710",
    "end": "2561859"
  },
  {
    "text": "pretty obvious we want it to be what we wanted to have low latency we wanted to be able to scale up to handle our data",
    "start": "2561859",
    "end": "2567410"
  },
  {
    "text": "load we would want our streaming service to be fault tolerant and probably",
    "start": "2567410",
    "end": "2574309"
  },
  {
    "text": "replicate our data to make our data more durable once we put the data in the streaming service highly available of course and finally we wanted the",
    "start": "2574309",
    "end": "2581000"
  },
  {
    "text": "streaming service to be globally deployables so that we could the motivation here would be we'd want to",
    "start": "2581000",
    "end": "2587809"
  },
  {
    "text": "host our streaming service near our log producing applications so we could flush logs from our log producers as",
    "start": "2587809",
    "end": "2593540"
  },
  {
    "text": "frequently and as reliably to our streaming service as possible because as as the less often we can flush logs kind",
    "start": "2593540",
    "end": "2601280"
  },
  {
    "text": "of the more data is constantly sitting on our log reducers and the more risk were kind of assuming if one of those",
    "start": "2601280",
    "end": "2607730"
  },
  {
    "text": "log producers were to die we probably lose that data so we want to get our data into our streaming service as quickly as possible so given all these",
    "start": "2607730",
    "end": "2616609"
  },
  {
    "text": "requirements we actually decided to try out Kafka first this was something like",
    "start": "2616609",
    "end": "2622730"
  },
  {
    "text": "two years ago I think it actually wasn't a part of that project so I'm probably not the best person to ask kind of for a big run down and why Kafka didn't work",
    "start": "2622730",
    "end": "2629000"
  },
  {
    "text": "out for us but kind of some of the themes were we had like lots of little issues with getting it set up and kind",
    "start": "2629000",
    "end": "2635059"
  },
  {
    "text": "of getting at Tunes weed issues with things like allocating the appropriate amount of disk space per topic setting",
    "start": "2635059",
    "end": "2640339"
  },
  {
    "text": "across datacenter mirroring and so on and in the end we kind of decided that it wasn't worth it to spell to spare all",
    "start": "2640339",
    "end": "2647510"
  },
  {
    "text": "the human resources that it would have required for us to maintainer kafka deployment so we kind of abandon that project so we",
    "start": "2647510",
    "end": "2656140"
  },
  {
    "text": "added the requirement that whatever streaming service which shows we need to have kind of a low ops overhead a low management overhead and naturally that",
    "start": "2656140",
    "end": "2663579"
  },
  {
    "text": "ended up leading us over to Kinesis so",
    "start": "2663579",
    "end": "2671410"
  },
  {
    "text": "again in our old data pipeline we're essentially just dumping all of our data into s3 to move to Kinesis all we really",
    "start": "2671410",
    "end": "2678640"
  },
  {
    "text": "had to do was dump our data in Kinesis now and once we started doing this we",
    "start": "2678640",
    "end": "2684459"
  },
  {
    "text": "pretty much immediately saw and to ends data pipeline processing latency dropped from 15 minutes to just three seconds",
    "start": "2684459",
    "end": "2691029"
  },
  {
    "text": "per vent obviously this is a huge win for us and I guess a quick note it",
    "start": "2691029",
    "end": "2696160"
  },
  {
    "text": "wasn't quite this simple to react detector data pipeline but I'll get more into that later so keep that in the back of your mind so here's a quick snapshot",
    "start": "2696160",
    "end": "2707079"
  },
  {
    "text": "of our Canisius usage so we have camisa streams hosted in five different regions five different AWS regions and across",
    "start": "2707079",
    "end": "2713950"
  },
  {
    "text": "all that we have 155 streams 264 shards and we're having about 2.5 billion log",
    "start": "2713950",
    "end": "2720219"
  },
  {
    "text": "events into 100 million Kinesis records per day and for all this this is",
    "start": "2720219",
    "end": "2727029"
  },
  {
    "text": "actually a screenshot from our billing page we only pay about three thousand dollars a month which is pretty awesome it's actually cheaper than the rent that",
    "start": "2727029",
    "end": "2733479"
  },
  {
    "text": "I pay in San Francisco sadly",
    "start": "2733479",
    "end": "2736859"
  },
  {
    "text": "so besides the recency improvements we actually scored a number of other wins when we moved to Kinesis so here's",
    "start": "2740229",
    "end": "2745779"
  },
  {
    "text": "actually a graph of dynamodb rights coming from one of our real-time data processing applications this is over",
    "start": "2745779",
    "end": "2752229"
  },
  {
    "text": "something like an eight-hour window and within this window of time we rolled out Kinesis hopefully it's pretty obvious",
    "start": "2752229",
    "end": "2757630"
  },
  {
    "text": "where the transition occurred so back when we were dealing with these kind of bursty batch workloads in our old data",
    "start": "2757630",
    "end": "2764169"
  },
  {
    "text": "pipeline we had to provision dynamodb for the peak of our load spikes and you",
    "start": "2764169",
    "end": "2770259"
  },
  {
    "text": "pay for provision throughput DynamoDB so we've kind of had a lot of this wasted throughput capacity and all kind of the",
    "start": "2770259",
    "end": "2776769"
  },
  {
    "text": "quiet parts of our load graph but once we move the Kinesis a really smooth at",
    "start": "2776769",
    "end": "2783309"
  },
  {
    "text": "our load and we were able to drop our DynamoDB provision throughput capacity on most of our tables something like in",
    "start": "2783309",
    "end": "2790539"
  },
  {
    "text": "we were able to cut it in something like 1/2 which saves us a ton of money I think we're one of if not the biggest",
    "start": "2790539",
    "end": "2796749"
  },
  {
    "text": "external consumer of DynamoDB and so they saved us a ton of money I think this saved us something like 25 times",
    "start": "2796749",
    "end": "2802959"
  },
  {
    "text": "the amount of money that we're actually spending on Kinesis for the same period of time so this was a huge cost saver on",
    "start": "2802959",
    "end": "2810489"
  },
  {
    "text": "top of all the recent statements which was awesome and also just smoothing out the load was great for system stability",
    "start": "2810489",
    "end": "2816519"
  },
  {
    "text": "in general I get paged for this system in the middle of the night much less since we rolled out Kinesis so",
    "start": "2816519",
    "end": "2821890"
  },
  {
    "text": "Kinesis was good for a good night's sleep as well all right so moving on to",
    "start": "2821890",
    "end": "2828249"
  },
  {
    "text": "some notes about our producer setup right now we have something like 300 server's constantly generating logs and",
    "start": "2828249",
    "end": "2834669"
  },
  {
    "text": "flushing that data to Kinesis at something like 700 records per second per host and most of these applications",
    "start": "2834669",
    "end": "2840969"
  },
  {
    "text": "most least Kinesis producer applications are written in Erlang and to interface with Kinesis there we wrote our own",
    "start": "2840969",
    "end": "2847269"
  },
  {
    "text": "in-house Kinesis client in Erlang which we've open sourced and all one of you that use Erlang can check it out on",
    "start": "2847269",
    "end": "2852969"
  },
  {
    "text": "github there so here's some kind of best practices we adhere to on our Kinesis",
    "start": "2852969",
    "end": "2861189"
  },
  {
    "text": "producers sorry the formatting on the slide kind of got it mangled there but one thing that we do is",
    "start": "2861189",
    "end": "2868820"
  },
  {
    "text": "we don't flush each of our log events in a separate Kinesis record as we're",
    "start": "2868820",
    "end": "2874010"
  },
  {
    "text": "generating blogs were kind of constantly concatenated to the them together and then we'll only flush are like little",
    "start": "2874010",
    "end": "2879530"
  },
  {
    "text": "micro batches of logs once that batch reaches a kind of critical mass of one megabyte which is",
    "start": "2879530",
    "end": "2884990"
  },
  {
    "text": "the maximum amount of data you can put in a single Kinesis record or if our data is relatively low volume for",
    "start": "2884990",
    "end": "2891020"
  },
  {
    "text": "whatever data we're trying to pass the Kinesis then we'll also flush on a one-second interval if it's been one",
    "start": "2891020",
    "end": "2896990"
  },
  {
    "text": "second since the last time we flushed our data doesn't just sit on our log producers and become stale so it turns",
    "start": "2896990",
    "end": "2902390"
  },
  {
    "text": "out this is a huge cost saver for us because part of Kinesis is billing is this notion of put payload unit where",
    "start": "2902390",
    "end": "2908960"
  },
  {
    "text": "you pay for every fully used or partially used 25 kilobyte chunk of data and each Kinesis record that you flush",
    "start": "2908960",
    "end": "2915619"
  },
  {
    "text": "to Kinesis our data tends to be something like one kilobyte in size so suppose we had 25 logs that we wanted to",
    "start": "2915619",
    "end": "2922430"
  },
  {
    "text": "push to Kinesis we pushed each of those logs in its own separate record we'd be paying for 25 put payload units but",
    "start": "2922430",
    "end": "2928640"
  },
  {
    "text": "because we're matching them together and flushing those all in a single record we only pay for one put payload unit and I think this saves us something like 2/3",
    "start": "2928640",
    "end": "2934849"
  },
  {
    "text": "of what we'd other always be paying on our Kinesis bill so instead of paying $9,000 you're paying $3,000 per month if",
    "start": "2934849",
    "end": "2943400"
  },
  {
    "text": "some of our data isn't making it into Kinesis often times we'll see kind of a direct revenue impact so we tend to be",
    "start": "2943400",
    "end": "2949040"
  },
  {
    "text": "pretty aggressive with our retry logic and some you know super common sense best practices there are you know exponentially back-off or successive",
    "start": "2949040",
    "end": "2955720"
  },
  {
    "text": "retries of putting data into Kinesis and also if you're flushing data on a consistent time interval and you're",
    "start": "2955720",
    "end": "2961520"
  },
  {
    "text": "running into issues with hots which ARDS consider introducing some jitter simple stuff also because we batch our data",
    "start": "2961520",
    "end": "2970900"
  },
  {
    "text": "together inside individual Kinesis records it'd be tough to do anything fancy with partition keys so we just",
    "start": "2970900",
    "end": "2976700"
  },
  {
    "text": "kind of use randomized partition keys and let our data fall where it falls I mean then we have the mandate that our downstream applications have to handle",
    "start": "2976700",
    "end": "2982760"
  },
  {
    "text": "kind of sorting and grouping of the data that they need to do to process whatever whatever data they're processing all",
    "start": "2982760",
    "end": "2991910"
  },
  {
    "text": "right so moving on to consumers we have something like 12 major applications using Kinesis right now we have a couple",
    "start": "2991910",
    "end": "2999349"
  },
  {
    "text": "airline applications but for the most we interface with Kinesis using apache storm and to interface with Kinesis",
    "start": "2999349",
    "end": "3006490"
  },
  {
    "text": "there we've used a slightly modified version of Amazon's camisa storms about",
    "start": "3006490",
    "end": "3012370"
  },
  {
    "text": "library which you can check out on github and I kind of wanted to get into",
    "start": "3012370",
    "end": "3018370"
  },
  {
    "text": "one specific optimization we made to some of the code and the vanilla storms about a library that Amazon wrote so",
    "start": "3018370",
    "end": "3025050"
  },
  {
    "text": "here's a snippet of code from Amazon's version of the library storms constantly",
    "start": "3025050",
    "end": "3031510"
  },
  {
    "text": "hungry for data and so it's constantly pulling this get next method whose only",
    "start": "3031510",
    "end": "3037210"
  },
  {
    "text": "job is just to grab data from kinases to kind of return it to the topology for processing and each time get next method",
    "start": "3037210",
    "end": "3043270"
  },
  {
    "text": "is called in the vanilla version of the library a request to grab data from",
    "start": "3043270",
    "end": "3048280"
  },
  {
    "text": "Kinesis gets kicked off and this is actually a synchronous request so once",
    "start": "3048280",
    "end": "3053470"
  },
  {
    "text": "this request gets kicked off your main application thread will just kind of sit there blocks waiting for the network round-trip and for whatever magic",
    "start": "3053470",
    "end": "3059020"
  },
  {
    "text": "happens behind the Kinesis endpoint to give you data waiting for that to complete before you know admitting the",
    "start": "3059020",
    "end": "3067180"
  },
  {
    "text": "data for the storm topology to process so we found this wasn't quite as performant as we would have liked it to",
    "start": "3067180",
    "end": "3073270"
  },
  {
    "text": "be so we ended up rewriting the code that pulls data from Kinesis something more like this where instead of grabbing",
    "start": "3073270",
    "end": "3081550"
  },
  {
    "text": "data in our main application thread in a back-end kind of a background thread we'll be kicking off will constantly",
    "start": "3081550",
    "end": "3089380"
  },
  {
    "text": "kick off request to grab data from Kinesis using the amazon Kinesis async client instead of the synchronous call",
    "start": "3089380",
    "end": "3096720"
  },
  {
    "text": "and then in our callback we buffer whatever data was returned by Kinesis and some sort of concurrent queue and",
    "start": "3096720",
    "end": "3104500"
  },
  {
    "text": "then when that request completes we just constantly kick up another one so in the background we're constantly pulling",
    "start": "3104500",
    "end": "3109900"
  },
  {
    "text": "Kinesis and constantly buffering data in memory in our concurrent queue all the while our main application thread is",
    "start": "3109900",
    "end": "3116170"
  },
  {
    "text": "still calling this 'get next method just like it was before but instead of making a request directly to Kinesis here all",
    "start": "3116170",
    "end": "3123010"
  },
  {
    "text": "we do is pop whatever data we have buffered on our concurrent queue and then we're not wasting cycles on network",
    "start": "3123010",
    "end": "3129340"
  },
  {
    "text": "round-trip and that kind of thing and I think from making this relatively simple change we saw something like a 10x data consumption",
    "start": "3129340",
    "end": "3135530"
  },
  {
    "text": "velocity speed up for our storm topologies so definitely something to consider if you're ever interfacing with",
    "start": "3135530",
    "end": "3141920"
  },
  {
    "text": "Kinesis using kind of the raw Java SDK use the asynchronous requests if you",
    "start": "3141920",
    "end": "3148340"
  },
  {
    "text": "need to access Kinesis performant we alright so for the last part of my talk",
    "start": "3148340",
    "end": "3156350"
  },
  {
    "text": "I kind of wanted to get into some of Kinesis service limitations and how they might affect how you want to architect",
    "start": "3156350",
    "end": "3161990"
  },
  {
    "text": "your data pipeline so as you mentioned before there's kind of this restriction",
    "start": "3161990",
    "end": "3167750"
  },
  {
    "text": "that when reading data from shards you can only do five transactions per second",
    "start": "3167750",
    "end": "3174370"
  },
  {
    "text": "so sorry this got cut off to what it",
    "start": "3174370",
    "end": "3179600"
  },
  {
    "text": "says there is it's tough to have a large number of applications per stream so suppose like AdRoll you have something",
    "start": "3179600",
    "end": "3185030"
  },
  {
    "text": "like 10 applications that wants to concurrently consume data from a single stream what you'd have to do is have",
    "start": "3185030",
    "end": "3192620"
  },
  {
    "text": "each of these applications do something like making coordinated 0.2 megabyte reads every two seconds and this gets",
    "start": "3192620",
    "end": "3200540"
  },
  {
    "text": "further complicated if you ever need to introduce transient applications like so you need to spin up a testing application and kind of plug it into a",
    "start": "3200540",
    "end": "3207260"
  },
  {
    "text": "stream or if you need to do some ad hoc processing on some section of the data that you have stored in your stream right now you need to have all of your",
    "start": "3207260",
    "end": "3213740"
  },
  {
    "text": "long-standing consumer applications adjust their polling intervals and this kind of becomes this big coordination mess and so it turns out life isn't",
    "start": "3213740",
    "end": "3224390"
  },
  {
    "text": "quite this simple but there are a number of solutions that you can do to kind of get around this if you have multiple applications that need to concurrently",
    "start": "3224390",
    "end": "3230600"
  },
  {
    "text": "consume data from a single stream so one solution is to have multiple layers of",
    "start": "3230600",
    "end": "3236330"
  },
  {
    "text": "Kinesis streams so here you'll have your",
    "start": "3236330",
    "end": "3241730"
  },
  {
    "text": "log reducers flush data into kind of an initial layer of camisa streams and then",
    "start": "3241730",
    "end": "3247550"
  },
  {
    "text": "you could have say a KCl application kind of in the middle of your initial layer Kinesis streams and your real-time data processing applications whose job",
    "start": "3247550",
    "end": "3255200"
  },
  {
    "text": "is just to route this data into a set of application specific Kinesis streams because each application gets its own",
    "start": "3255200",
    "end": "3260870"
  },
  {
    "text": "stream there's no contention for these five routes per second and there's kind of no scheduling nightmare",
    "start": "3260870",
    "end": "3266430"
  },
  {
    "text": "trying to pull they different kinases and also you could have some sort of kind of controller application spinning",
    "start": "3266430",
    "end": "3272369"
  },
  {
    "text": "up new streams and having your KC application kind of wrap the data to those new strings as well without",
    "start": "3272369",
    "end": "3279359"
  },
  {
    "text": "affecting your long-standing consuming applications one drawback here though is",
    "start": "3279359",
    "end": "3285539"
  },
  {
    "text": "this is potentially expensive depending on kind of your cost needs so there's",
    "start": "3285539",
    "end": "3290640"
  },
  {
    "text": "another alternative which would be to instead of flush data into a second layer for Ganesa streams you could have",
    "start": "3290640",
    "end": "3296640"
  },
  {
    "text": "just flush that data back into s3 you could do this using a kco application or",
    "start": "3296640",
    "end": "3302250"
  },
  {
    "text": "now firehoses the thing there's no real reason not to just use fire has to do this for you so s3 is a great data store",
    "start": "3302250",
    "end": "3311369"
  },
  {
    "text": "for having multiple applications concurrently consuming the same data but wait a second it seems like we kind of went through all this effort to react",
    "start": "3311369",
    "end": "3317730"
  },
  {
    "text": "attacked our data pipeline to not involve s3 so why would you want to do something like this well if you recall",
    "start": "3317730",
    "end": "3323549"
  },
  {
    "text": "in our old data pipeline we are flushing a separate file per log producing host",
    "start": "3323549",
    "end": "3329210"
  },
  {
    "text": "but one thing that this new pipeline architecture allows you to do with Kinesis is aggregate data across all of",
    "start": "3329210",
    "end": "3335789"
  },
  {
    "text": "your lock producing hosts in a single place in your KCl application or let firehose do it for you and flush all",
    "start": "3335789",
    "end": "3341190"
  },
  {
    "text": "that data in a single log file so remember we kind of had this problem where we didn't want to generate too",
    "start": "3341190",
    "end": "3346529"
  },
  {
    "text": "many small log files in s3 well suppose we have 20 lakh reducers in",
    "start": "3346529",
    "end": "3351539"
  },
  {
    "text": "our old data pipeline if you're flushing data every 20 minutes you're generating 20 files every 20 minutes but in this",
    "start": "3351539",
    "end": "3356670"
  },
  {
    "text": "new architecture we have the opportunity to flush data every one minute flush",
    "start": "3356670",
    "end": "3361859"
  },
  {
    "text": "data aggregated across all of our log producers in a single file that's three every one minute so after 20 minutes we still end up with 20 files but in this",
    "start": "3361859",
    "end": "3368579"
  },
  {
    "text": "new in this new way of flushing data we're making her data available for processing only 30 seconds after it's",
    "start": "3368579",
    "end": "3374640"
  },
  {
    "text": "generated as opposed to 10 minutes and depending on your applications SLA is I",
    "start": "3374640",
    "end": "3380339"
  },
  {
    "text": "mean kind of mix and match so if you have some applications that need to process data within 5 seconds of it",
    "start": "3380339",
    "end": "3385799"
  },
  {
    "text": "being generated then you can have something route that data to Kinesis stream then for other applications that",
    "start": "3385799",
    "end": "3392220"
  },
  {
    "text": "have might have more relaxed recency essays you could have those applications consuming from s3",
    "start": "3392220",
    "end": "3399950"
  },
  {
    "text": "so yeah it's kind of most of the points that I wanted to make so going back over some of the main points from my my",
    "start": "3400510",
    "end": "3407930"
  },
  {
    "text": "portion of the talk one consider hosting Kinesis near your data producer applications to facilitate offloading",
    "start": "3407930",
    "end": "3414410"
  },
  {
    "text": "logs kind of as quickly as possible so that your data isn't sitting on your lock reducer boxes where it's kind of in a",
    "start": "3414410",
    "end": "3419420"
  },
  {
    "text": "single point of failure scenario or if your log producer box falls over then you might lose data and once it's in",
    "start": "3419420",
    "end": "3425600"
  },
  {
    "text": "Kinesis it's through a replicated so it's probably gonna be much more durable and yeah it's better it's potentially",
    "start": "3425600",
    "end": "3433160"
  },
  {
    "text": "cheaper and it's probably gonna be more reliable to work with a smooth constant workload that streaming services afford",
    "start": "3433160",
    "end": "3439310"
  },
  {
    "text": "to you as opposed to kind of issues we had dealing with bursty batch workloads so if you're kind of like us and you're",
    "start": "3439310",
    "end": "3445970"
  },
  {
    "text": "coming from a backed workload for your real-time data processing I'm definitely something to consider on",
    "start": "3445970",
    "end": "3451700"
  },
  {
    "text": "this probably some gains to be had there I think about batching data inside",
    "start": "3451700",
    "end": "3456920"
  },
  {
    "text": "Kinesis records especially if you're like us and you have kind of small chunks of data that wouldn't fully utilize the Kinesis put payload unit of",
    "start": "3456920",
    "end": "3465950"
  },
  {
    "text": "25 kilobytes of each Kinesis but payload unit kind of costs try to use the",
    "start": "3465950",
    "end": "3471140"
  },
  {
    "text": "asynchronous Kinesis api if you're ever working with the raw Java SDK and then",
    "start": "3471140",
    "end": "3477290"
  },
  {
    "text": "finally think about putting something in the middle of Kinesis and your Kinesis",
    "start": "3477290",
    "end": "3482750"
  },
  {
    "text": "consumer applications if you think you're going to be having lots of applications that will need to concurrently consume the same data from the same stream back over to addy",
    "start": "3482750",
    "end": "3495369"
  },
  {
    "text": "thankfully that was that was incredible hopefully that was interesting to you guys too so we've got a minute and 50 seconds so",
    "start": "3500500",
    "end": "3508000"
  },
  {
    "text": "with the I think we can do seven questions and of course I will be available right after so in case you",
    "start": "3508000",
    "end": "3515020"
  },
  {
    "text": "don't like beer and partying you can talk to us",
    "start": "3515020",
    "end": "3519420"
  }
]