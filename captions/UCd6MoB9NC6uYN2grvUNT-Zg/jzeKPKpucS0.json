[
  {
    "start": "0",
    "end": "116000"
  },
  {
    "text": "hey everybody thanks for coming in on a Friday night last day of the show here to hear me talk a little bit about",
    "start": "500",
    "end": "6720"
  },
  {
    "text": "dynamodb my name is Ricola Han I'm a senior practice manager for AWS",
    "start": "6720",
    "end": "11849"
  },
  {
    "text": "professional services I focus on no SQL technology which means I do a lot of work with dynamo ISO is probably one of",
    "start": "11849",
    "end": "17820"
  },
  {
    "text": "the reasons why I'm here talking to you today so how does everybody's reinvent going good good conference been big exciting",
    "start": "17820",
    "end": "25350"
  },
  {
    "text": "ties for Dynamo I gotta say last year has been amazing watching their number of features that we've been able to release auto-scaling VPC endpoints",
    "start": "25350",
    "end": "32719"
  },
  {
    "text": "backup restore global tables and really amazing stuff we're going to talk today a little bit about actual just design",
    "start": "32719",
    "end": "39750"
  },
  {
    "text": "patterns so I'm gonna get into you know things like how do I build schema for scalable applications and whatnot and so",
    "start": "39750",
    "end": "46590"
  },
  {
    "text": "two what you can expect from this session is a little bit about when do I want to use no SQL and why I'm not gonna",
    "start": "46590",
    "end": "53340"
  },
  {
    "text": "spend too much time talking about the basics of dynamo this is generally speaking we've done a deep dive in the past where I started really low-level",
    "start": "53340",
    "end": "59190"
  },
  {
    "text": "and moved high-level but the feedback has been consistently people love the the deep you know content so this is",
    "start": "59190",
    "end": "64948"
  },
  {
    "text": "really more of a very advanced session as a 400 level session so we're not gonna spend a lot of time on an overview",
    "start": "64949",
    "end": "70799"
  },
  {
    "text": "of dynamo but we'll spend more time on on actual know SQL data modeling techniques design patterns that we use",
    "start": "70799",
    "end": "76909"
  },
  {
    "text": "to solve problems in some of our largest customer counts generally I do a lot of",
    "start": "76909",
    "end": "82380"
  },
  {
    "text": "work with the internal Amazon teams as well what we call the CDO the commercial digital and other development",
    "start": "82380",
    "end": "87720"
  },
  {
    "text": "organizations we have 12,000 micro services that run across Amazon and AWS",
    "start": "87720",
    "end": "94200"
  },
  {
    "text": "and I work with a lot of those teams when they deploy on dynamodb to build scalable solutions so we'll talk about",
    "start": "94200",
    "end": "100079"
  },
  {
    "text": "the design patterns that we use to actually do that and in the end of all this we're gonna roll it up into a real world example where we take all of the",
    "start": "100079",
    "end": "107100"
  },
  {
    "text": "patterns that I'm talking about the very low-level ones and the more advanced patterns and we're going to show you how",
    "start": "107100",
    "end": "112590"
  },
  {
    "text": "we put these things together into solutions that actually work for our customers and ourselves so the first",
    "start": "112590",
    "end": "117899"
  },
  {
    "start": "116000",
    "end": "238000"
  },
  {
    "text": "thing I like to talk to people about is what is the database to you and most of the time I get this answer right it's a",
    "start": "117899",
    "end": "123750"
  },
  {
    "text": "place to put stuff and that's actually not all that odd to hear because relational database technology's been",
    "start": "123750",
    "end": "129629"
  },
  {
    "text": "around for 30 years and we all kind of understand this stuff right it's not like and inner join is something we don't",
    "start": "129629",
    "end": "134820"
  },
  {
    "text": "know right however when you get into deploying new technologies it wasn't always that way with relational",
    "start": "134820",
    "end": "140010"
  },
  {
    "text": "technologies 30 years ago I used to interview developers I used to ask them what is an inner join and sometimes they",
    "start": "140010",
    "end": "145110"
  },
  {
    "text": "wouldn't give me the right answer right and so really what happens when new technologies roll out is is this thing",
    "start": "145110",
    "end": "150930"
  },
  {
    "text": "called technology trigger and in data processing that's about data pressure the ability of the system to process the",
    "start": "150930",
    "end": "155970"
  },
  {
    "text": "amount of data it's being asked to process in a reasonable amount of time or a reasonable cost and when one of those things it doesn't happen or is not",
    "start": "155970",
    "end": "163290"
  },
  {
    "text": "happening then that's what we'd call a technology trigger and we get some innovation in the market right so",
    "start": "163290",
    "end": "168300"
  },
  {
    "text": "generally what happens to new technologies comes some early adopters role this new technology out they have some success and the rest of the market",
    "start": "168300",
    "end": "174300"
  },
  {
    "text": "starts to look at says maybe that'll work for me so you go deploy the new technology they have miserable experiences and why do they have",
    "start": "174300",
    "end": "180510"
  },
  {
    "text": "miserable experiences because they don't bother to learn how to use the new tools and they continue to use the new tools the same way they use the old tools okay",
    "start": "180510",
    "end": "187770"
  },
  {
    "text": "and that's what's happening today I see a lot of teams out there deploying relational design patterns with no SQL",
    "start": "187770",
    "end": "193470"
  },
  {
    "text": "and it just doesn't work right I start having to manage joins at the application layer and pulling data off",
    "start": "193470",
    "end": "198540"
  },
  {
    "text": "across multiple tables and managing many-to-many mappings and things like this the relational databases have been",
    "start": "198540",
    "end": "204360"
  },
  {
    "text": "optimized for 30 years to do that they're gonna be much better at it than you will be in your application layer code so don't try and do that learn how",
    "start": "204360",
    "end": "210840"
  },
  {
    "text": "to use the new tools and will have a better experience and that's what we're gonna be talking about a little bit today but once those once those skills",
    "start": "210840",
    "end": "216720"
  },
  {
    "text": "kind of diffuse through the market then what we see is this this majority the early majority is coming in and and that",
    "start": "216720",
    "end": "223200"
  },
  {
    "text": "and the technology and the skill sets around it or commoditizing right and people have better experiences now",
    "start": "223200",
    "end": "228360"
  },
  {
    "text": "because we know how to use it that's that slope of enlightenment so if you want to avoid that big hey this is gonna work in the big crash right then let's",
    "start": "228360",
    "end": "235380"
  },
  {
    "text": "figure out how to use the new stuff and you'll have better experiences right off the bat alright so when do we use it SQL is about normalized relational data",
    "start": "235380",
    "end": "244050"
  },
  {
    "start": "238000",
    "end": "299000"
  },
  {
    "text": "structures it supports ad hoc queries which is really cool if I don't understand how the customers are actually going to be",
    "start": "244050",
    "end": "249690"
  },
  {
    "text": "using the data right so bi analytics OLAP style workloads work really well with SQL databases but they don't scale",
    "start": "249690",
    "end": "257160"
  },
  {
    "text": "all that well and we can build OLAP solutions on top of no SQL databases but if I want like real-time ad hoc queries",
    "start": "257160",
    "end": "263850"
  },
  {
    "text": "then your best bet is going to be an QL or relational store but if I'm running OLTP apps which is you know",
    "start": "263850",
    "end": "269820"
  },
  {
    "text": "mostly apps we build today transactional applications they always do things the same way when the buttons get clicked",
    "start": "269820",
    "end": "275220"
  },
  {
    "text": "the data has always read the same way it's always stored the same way I don't need to you know execute an ad-hoc query",
    "start": "275220",
    "end": "280710"
  },
  {
    "text": "to produce those views I can actually store the data the way the application uses it then I'm going to be able to make simpler queries that are gonna be",
    "start": "280710",
    "end": "286950"
  },
  {
    "text": "less expensive on the system and we'll explain how that actually works but so what we want is for no SQL as we look at",
    "start": "286950",
    "end": "294210"
  },
  {
    "text": "OLTP applications at scale right that's where we really the sweet spot for no SQL so Amazon DynamoDB we all know fully",
    "start": "294210",
    "end": "303120"
  },
  {
    "start": "299000",
    "end": "342000"
  },
  {
    "text": "managed no SQL database has always great features it's really the best the biggest advantage to is it's fully",
    "start": "303120",
    "end": "309270"
  },
  {
    "text": "managed right those of you that have run relational data stores in the past will",
    "start": "309270",
    "end": "314520"
  },
  {
    "text": "know that or non relational stores in the past and know that the scale and the management of all that infrastructure is",
    "start": "314520",
    "end": "320070"
  },
  {
    "text": "a lot of work involved in that and it's not really core to your business so if it's not something you want to actually have expertise in then maybe a managed",
    "start": "320070",
    "end": "327030"
  },
  {
    "text": "service is better off but really a bottom line is it's it's scales to any",
    "start": "327030",
    "end": "332850"
  },
  {
    "text": "workload that you need and it and it does it seamlessly and in an adaptive way so it's a much better solution than",
    "start": "332850",
    "end": "339780"
  },
  {
    "text": "trying to manage your own stores so we're not going to talk a lot about basics but we do need to understand a",
    "start": "339780",
    "end": "345090"
  },
  {
    "start": "342000",
    "end": "405000"
  },
  {
    "text": "fundamental construct so he's going to review real quick what a table structure looks like in dynamo DB tables have",
    "start": "345090",
    "end": "350640"
  },
  {
    "text": "items items have attributes items don't necessarily have to have all the same attributes but every item has to have at",
    "start": "350640",
    "end": "356039"
  },
  {
    "text": "least one attribute and that's the partition key the partition key uniquely identifies the item it actually uniquely",
    "start": "356039",
    "end": "361289"
  },
  {
    "text": "identifies what it despite what it describes a partition and partitions can store items if I provide an optional",
    "start": "361289",
    "end": "367020"
  },
  {
    "text": "sort key now that partition key uniquely identifies a a logical partition on the",
    "start": "367020",
    "end": "373080"
  },
  {
    "text": "storage node and they sort key attribute uniquely identifies the items within that logical partition all right I can",
    "start": "373080",
    "end": "378990"
  },
  {
    "text": "execute complex range queries against those sort key operators and really what I'm doing now is I'm modeling a",
    "start": "378990",
    "end": "385140"
  },
  {
    "text": "one-to-many relationship right on the table and so we're going to use this base construct and we're going to build on this to show you how to maintain",
    "start": "385140",
    "end": "391740"
  },
  {
    "text": "hierarchical relationships many to many relationships and and all kinds of",
    "start": "391740",
    "end": "397260"
  },
  {
    "text": "you know relational constructs that we need in our applications access patterns",
    "start": "397260",
    "end": "402920"
  },
  {
    "text": "so we get any data modeling it's really all about those aggregations right so",
    "start": "402920",
    "end": "408300"
  },
  {
    "text": "having that being able to maintain that one-to-many aggregation right on the parent table gives us a lot of flexibility but it doesn't matter what",
    "start": "408300",
    "end": "413760"
  },
  {
    "text": "type of application you are are writing it's it's all about looking at the data in context of some dimension right",
    "start": "413760",
    "end": "421020"
  },
  {
    "text": "aggregating the data items by order orders by customer parts by assembly you",
    "start": "421020",
    "end": "426630"
  },
  {
    "text": "know these and no it doesn't matter what type of application you're building you need to aggregate slice and dice that",
    "start": "426630",
    "end": "431880"
  },
  {
    "text": "data so we want to actually build data structures that allow us to do that in",
    "start": "431880",
    "end": "436980"
  },
  {
    "text": "the way that we've done that in the past is using this type of structure right there's normalized relational structure",
    "start": "436980",
    "end": "442050"
  },
  {
    "text": "this is a product catalog this is a pretty common type of example of many of",
    "start": "442050",
    "end": "447570"
  },
  {
    "text": "the common relationships that we'd see in a normalized structure one-to-one relationship between products and books one-to-many between albums and tracks",
    "start": "447570",
    "end": "454970"
  },
  {
    "text": "many-to-many between videos and actors and you can imagine the types of queries and the number of queries I would have",
    "start": "454970",
    "end": "460050"
  },
  {
    "text": "to execute in order to be able to pull back a list of all my products right at least three distinct queries joining",
    "start": "460050",
    "end": "465600"
  },
  {
    "text": "tables joining through mapping tables and and the CPU is burning itself up trying to produce this view right and",
    "start": "465600",
    "end": "471630"
  },
  {
    "text": "this is why relational databases don't scale when I push the data out to multiple tables I have to go reassemble it and that was great thirty years ago",
    "start": "471630",
    "end": "478410"
  },
  {
    "text": "when the most expensive resource in the data center was the disk and the least expensive resource was the CPU heck that",
    "start": "478410",
    "end": "483750"
  },
  {
    "text": "was a fixed asset that's fun whether I was using it or not I'd really rather save money by reducing the amount of",
    "start": "483750",
    "end": "489450"
  },
  {
    "text": "storage I have to deploy it was the hugest expense in the enterprise data center fast forward 30 40 years in the",
    "start": "489450",
    "end": "495210"
  },
  {
    "text": "reverse is true right storage is dirt cheap CPUs expensive I don't want to use technologies that are optimizing the least expensive resource",
    "start": "495210",
    "end": "502050"
  },
  {
    "text": "in the data center I want to optimize the most expensive resource in the data center right so maybe we don't want to",
    "start": "502050",
    "end": "507930"
  },
  {
    "text": "spend all of our CPU cycles assembling these views we want denormalize that data and we want to build you know",
    "start": "507930",
    "end": "513690"
  },
  {
    "text": "things that look more like this we have a hierarchical structure on the left that has been reduced to a collection of",
    "start": "513690",
    "end": "519780"
  },
  {
    "text": "elements or documents or items that I would put on a table and now when I want to produce my list of products I say",
    "start": "519780",
    "end": "526290"
  },
  {
    "text": "select star from products where X equals right where type equals look there's all my books heck I get all",
    "start": "526290",
    "end": "533160"
  },
  {
    "text": "my products it's one query select star from X there you go all my products that's a much simpler query you know",
    "start": "533160",
    "end": "539940"
  },
  {
    "text": "even relational databases execute select queries very very quickly right because they don't have to do anything they just go to the table they get the data so",
    "start": "539940",
    "end": "546570"
  },
  {
    "text": "this is what we're gonna try and do we're trying to build data structures allow us to create query patterns and or",
    "start": "546570",
    "end": "551790"
  },
  {
    "text": "more select star from than they are select star inner join right reduce the overhead on that CPU alright so we get",
    "start": "551790",
    "end": "559860"
  },
  {
    "text": "any design patterns of best practices around dynamo there's a couple things that we need to know about in the ecosystem because they're very important",
    "start": "559860",
    "end": "566580"
  },
  {
    "text": "constructs that we're gonna leverage for real-world solutions and these things are things like DynamoDB streams and AWS",
    "start": "566580",
    "end": "573210"
  },
  {
    "text": "lambda ok streams is the changelog for a dynamo DB table all right operations will go on to the stream so whenever I",
    "start": "573210",
    "end": "579990"
  },
  {
    "text": "do an insert update you know any type of execution or delete those operations",
    "start": "579990",
    "end": "585390"
  },
  {
    "text": "will show up on the stream and they can be hooked up to a lambda function so that every operation every write",
    "start": "585390",
    "end": "590460"
  },
  {
    "text": "operation that occurs will be processed by a lambda function so think of this as a stored procedure engine right for",
    "start": "590460",
    "end": "596340"
  },
  {
    "text": "DynamoDB and it's what's really neat about is it actually scales independently of the database head nodes",
    "start": "596340",
    "end": "601680"
  },
  {
    "text": "right dynamodb is a fully distributed system with no of itself but lambda is a execution infrastructure it's it's",
    "start": "601680",
    "end": "607530"
  },
  {
    "text": "completely off to the side what does that mean it means I cannot crush the processing space of my database server by writing bad code and injecting a bad",
    "start": "607530",
    "end": "615120"
  },
  {
    "text": "story procedure which I can easily do in any relational database right I mean that's a common problem is developers",
    "start": "615120",
    "end": "622230"
  },
  {
    "text": "push a bad stored procedure or a DBA pushes a bad story procedure and it'll just knock that database server sideways",
    "start": "622230",
    "end": "628500"
  },
  {
    "text": "right we cannot do that with with this particular car strap because they're totally disconnected so what do people",
    "start": "628500",
    "end": "633720"
  },
  {
    "start": "632000",
    "end": "727000"
  },
  {
    "text": "use it for many things primarily what we do is we use it for item level a table level metrics",
    "start": "633720",
    "end": "638970"
  },
  {
    "text": "all right so oftentimes what we want our summary aggregations write counts",
    "start": "638970",
    "end": "644220"
  },
  {
    "text": "averages complex computations write average sale price of all these items or",
    "start": "644220",
    "end": "649470"
  },
  {
    "text": "types of items on the table we don't support those types of ad-hoc queries with no SQL databases actually no no SQL",
    "start": "649470",
    "end": "655680"
  },
  {
    "text": "database does even if they tell you they do DB has aggregation framework but every one of those I'll tell you that works there is gonna",
    "start": "655680",
    "end": "661920"
  },
  {
    "text": "tell you it works great when you have a single replica set but when you start to shard you know eventually you gotta",
    "start": "661920",
    "end": "667380"
  },
  {
    "text": "bring all that data to one place to compute and it's gonna crash at scale right so in order to be able to to",
    "start": "667380",
    "end": "674730"
  },
  {
    "text": "maintain these running aggregations what we use is lambda and streams right as writes and updates occur on the table",
    "start": "674730",
    "end": "680130"
  },
  {
    "text": "lambda function then creates and maintains these running aggregations and pushes that data and that result set back into the table as metadata items",
    "start": "680130",
    "end": "687450"
  },
  {
    "text": "now in the application layer if I want to get my average sale price over the last 24 hours I select the metadata item",
    "start": "687450",
    "end": "692910"
  },
  {
    "text": "and that pre aggregated or pre computed result set is in there right so I basically calculated once I serve it up",
    "start": "692910",
    "end": "699030"
  },
  {
    "text": "many times again I've offloaded the CPU you know in immensely by doing this",
    "start": "699030",
    "end": "704400"
  },
  {
    "text": "right I just maintain these running aggregations in these item level and table level metrics as metadata it's a",
    "start": "704400",
    "end": "710100"
  },
  {
    "text": "very effective technique we also update third party systems cloudsearch roll data out to Kinesis firehose you can do",
    "start": "710100",
    "end": "716190"
  },
  {
    "text": "anything you want with lambda it's code in the sample on the last chart wasn't all that interesting it just wrote those",
    "start": "716190",
    "end": "721680"
  },
  {
    "text": "attributes out to the console but again you know it's it's code so you can pretty much do anything you want okay so",
    "start": "721680",
    "end": "728610"
  },
  {
    "start": "727000",
    "end": "775000"
  },
  {
    "text": "let's get into the meat of things we're gonna start kind of very high-level these are these are base patterns and",
    "start": "728610",
    "end": "734550"
  },
  {
    "text": "then we're gonna put all these things together into more complex designs as we go on and so some of you might be familiar with some of these constructs",
    "start": "734550",
    "end": "740370"
  },
  {
    "text": "from previous presentations or you know whatever online reading but I was going to start here again and build up to more",
    "start": "740370",
    "end": "746070"
  },
  {
    "text": "complex scenarios so the first thing we're talking about is write sharding write sharding is about distributing the load so in dynamodb you get a certain",
    "start": "746070",
    "end": "753690"
  },
  {
    "text": "amount of capacity per storage partition that's 1,000 WCU's or 3,000 RC use and",
    "start": "753690",
    "end": "759630"
  },
  {
    "text": "and if you exceed that on a per partition basis you're gonna get throttled so what happens in a scenario",
    "start": "759630",
    "end": "764850"
  },
  {
    "text": "like this where I have a voting scenario and maybe I have candy day aka B we're pulling in a hundred thousand votes a",
    "start": "764850",
    "end": "770220"
  },
  {
    "text": "second and I need to aggregate all of this information but I only have two candidates so how do I do that there's a",
    "start": "770220",
    "end": "776430"
  },
  {
    "start": "775000",
    "end": "803000"
  },
  {
    "text": "couple of ways we can go about it the first way would be look at queue based load leveling right we put a sqs",
    "start": "776430",
    "end": "781500"
  },
  {
    "text": "instance up in front of the table let the votes hit that queue bleed those queues out of your rate that the table",
    "start": "781500",
    "end": "787230"
  },
  {
    "text": "can take it now this is good this is great but it doesn't give me real-time aggregations so if I want like real-time",
    "start": "787230",
    "end": "792600"
  },
  {
    "text": "metrics dashboards who's winning type of stuff and I'm pulling a hundred thousand votes a second that's going to",
    "start": "792600",
    "end": "797810"
  },
  {
    "text": "be a while before I can bleed those back out at a thousand rights a second right so how do I fix that problem and the way",
    "start": "797810",
    "end": "804920"
  },
  {
    "start": "803000",
    "end": "882000"
  },
  {
    "text": "to do that is actually create multiple logical partitions to aggregate the votes across right so instead of Canada",
    "start": "804920",
    "end": "810680"
  },
  {
    "text": "a and B now I have 1080 a 1 through 9 or 0 through 10 whatever and what I do is I'm going to insert votes every time a",
    "start": "810680",
    "end": "817310"
  },
  {
    "text": "vote comes in I'm gonna randomly append a partition value between 0 and 10 to",
    "start": "817310",
    "end": "822860"
  },
  {
    "text": "that candidate ID so now instead of one candidate a bucket I have 10 and now in order to get that aggregation I can",
    "start": "822860",
    "end": "829160"
  },
  {
    "text": "scatter gather across all 10 buckets so I can run this as a concurrent parallel process and that's actually really good for dynamo dynamo scales linearly across",
    "start": "829160",
    "end": "836690"
  },
  {
    "text": "threads single thread performance in dynamo DB is not all that great because it's a REST API so every request I make",
    "start": "836690",
    "end": "842870"
  },
  {
    "text": "is forming a connection and tearing it down but when I run concurrently across multiple threads I can magnify the",
    "start": "842870",
    "end": "848270"
  },
  {
    "text": "throughput and it scales linearly you're not going to break it you can run a million threads against dynamo DB I think we have a distributed request",
    "start": "848270",
    "end": "854810"
  },
  {
    "text": "router we take 34.5 or 35 million requests per second somewhere around there on a daily basis so there's not",
    "start": "854810",
    "end": "862280"
  },
  {
    "text": "any number of requests you're going to run against our request for I was going to slow it down so this is what we call",
    "start": "862280",
    "end": "867320"
  },
  {
    "text": "a scatter gather pattern and what you do is you run that on a scheduled process that goes ahead and aggregates those",
    "start": "867320",
    "end": "872510"
  },
  {
    "text": "boats and pushes the counts up to again a metadata item you can run a lambda function on the back end that's pulling those votes off of the stream and",
    "start": "872510",
    "end": "878480"
  },
  {
    "text": "aggregating it and pushing it up to the metadata item as well a lot of different options for you here so that was nice",
    "start": "878480",
    "end": "884570"
  },
  {
    "text": "that's great and that's you know I'm spreading the data around randomly but I don't really know where the date is going when I'm spreading it in and I",
    "start": "884570",
    "end": "890030"
  },
  {
    "text": "have to read all the buckets and if maybe I want to break some election laws here and find out what my neighbor voted and I want to actually do it very",
    "start": "890030",
    "end": "896300"
  },
  {
    "text": "efficiently I want to know exactly where my neighbors vote is so instead of randomly spraying around well I'll do is",
    "start": "896300",
    "end": "902450"
  },
  {
    "text": "create my own hash range between 0 and 10 and all hashes social security number and then I'll write it into that bucket",
    "start": "902450",
    "end": "908360"
  },
  {
    "text": "that way on the read side if I know whose vote I'm trying to go get and I know it's Social Security number I know",
    "start": "908360",
    "end": "913640"
  },
  {
    "text": "exactly what bucket to go read because if he did vote it'll be there right so you can choose which way when when I",
    "start": "913640",
    "end": "920210"
  },
  {
    "text": "when I write on the right pattern if I always need to get all the data all the time and just do a random spray but you",
    "start": "920210",
    "end": "925730"
  },
  {
    "text": "know if I need to selectively get those results but I also need to be able to scale the ingestion and maybe I don't use a random algorithm I",
    "start": "925730",
    "end": "931919"
  },
  {
    "text": "use a hash algorithm on a known attribute and then I can go get that data what I need to all right so a very",
    "start": "931919",
    "end": "936989"
  },
  {
    "text": "good way to be able scale your rights by adding logical partitions and either selectively or or just gather all that",
    "start": "936989",
    "end": "943679"
  },
  {
    "text": "data at once however you need it now how many partitions do I need it's just pretty straightforward math right I got",
    "start": "943679",
    "end": "949739"
  },
  {
    "start": "944000",
    "end": "985000"
  },
  {
    "text": "a number of items that I'm expecting to see in a logical partition the average item size take the RC use divided by the",
    "start": "949739",
    "end": "957720"
  },
  {
    "text": "RC use and the request multiplied by rest request per second divided by the partition max okay so if I have a",
    "start": "957720",
    "end": "965429"
  },
  {
    "text": "hundred thousand votes here coming in per logical partition I'm reading all those votes ten times a second and my",
    "start": "965429",
    "end": "971039"
  },
  {
    "text": "average item size is 200 kilobytes then I'm gonna need 17 physical partitions to be able to handle that read load so I",
    "start": "971039",
    "end": "977970"
  },
  {
    "text": "would tell the customer use 20 as your in factor so to speak when you spread those votes around that way you can have",
    "start": "977970",
    "end": "983579"
  },
  {
    "text": "a little bit of headroom right on there on the right side again just math a little bit of tweak here is that rights",
    "start": "983579",
    "end": "990269"
  },
  {
    "start": "985000",
    "end": "1021000"
  },
  {
    "text": "occur on the per item basis and wcu is the minimum cost of a right so if your item size is lower than 1k it's going to",
    "start": "990269",
    "end": "997379"
  },
  {
    "text": "cost you one K so your average item size or 1k whichever is greater right is the",
    "start": "997379",
    "end": "1002569"
  },
  {
    "text": "cost of the right so in this particular case if I have a hundred thousand votes at 200 bytes I actually need a hundred",
    "start": "1002569",
    "end": "1007699"
  },
  {
    "text": "partitions because I only get a hundred a thousand rights per partition and it's gonna cost me a hundred thousand WCS",
    "start": "1007699",
    "end": "1012919"
  },
  {
    "text": "right so looking at the rights looking at the reads looking at the insertion rates these are the ways that you're",
    "start": "1012919",
    "end": "1020239"
  },
  {
    "text": "gonna be able to figure out how many partitions you need but and you again increase the throughput through concurrency add logical partitions to be",
    "start": "1020239",
    "end": "1027769"
  },
  {
    "text": "able to ingest more data to be able to serve up more data and use this when the right or the read workload is not",
    "start": "1027769",
    "end": "1033339"
  },
  {
    "text": "horizontally scalable all right time based workflows this is a scenario we're",
    "start": "1033339",
    "end": "1039980"
  },
  {
    "start": "1035000",
    "end": "1070000"
  },
  {
    "text": "talking about it's really about processing the entire table so a lot of times I get into situations where customers a table scanning because they",
    "start": "1039980",
    "end": "1045829"
  },
  {
    "text": "need to get all the items on the table that meet a certain state right and since you know we have this nice partition and sort key but that only",
    "start": "1045829",
    "end": "1052129"
  },
  {
    "text": "give me the items within a given partition that made the state so how do I get all the items across all the partitions on the table and sometimes",
    "start": "1052129",
    "end": "1058610"
  },
  {
    "text": "creating a GSI on those keys if there low dimensionality could actually be a problem right so if you know in the",
    "start": "1058610",
    "end": "1065710"
  },
  {
    "text": "lower level conversations Dyna Motors thing called hotkeys we want to avoid so how do we do that we just talked about",
    "start": "1065710",
    "end": "1071200"
  },
  {
    "start": "1070000",
    "end": "1214000"
  },
  {
    "text": "right shorting on the table right well I'm gonna talk about right sharding on the GSI so in this particular example we've got active tickets so tickets are",
    "start": "1071200",
    "end": "1079090"
  },
  {
    "text": "coming in on a trouble ticketing system I've got people on the backend processing these tickets and I have a process that I need to be able to you",
    "start": "1079090",
    "end": "1085690"
  },
  {
    "text": "know process tickets that haven't been acted on a certain amount of time right they're still pending and that have expired and so the way I'll do this is I",
    "start": "1085690",
    "end": "1092080"
  },
  {
    "text": "on every time I insert a ticket on the table I'm going to create this you know attribute called GSI key it's going to have this random value between 0 and in",
    "start": "1092080",
    "end": "1098620"
  },
  {
    "text": "its extended attribute on the item on the table it's just partitioned on the event ID all right so I just got all",
    "start": "1098620",
    "end": "1103690"
  },
  {
    "text": "these events coming in so then what I'm going to do is I create a GSI and I'm going to use the GSI key is the",
    "start": "1103690",
    "end": "1109000"
  },
  {
    "text": "partition key and I'm going to use the timestamp as there as the sort key so what have I done I've now spread all the",
    "start": "1109000",
    "end": "1115540"
  },
  {
    "text": "items on the table across n partitions on the GSI in essence now I've been able to scatter gather the GSI with a",
    "start": "1115540",
    "end": "1121840"
  },
  {
    "text": "selective sort key condition and get all the expired tickets on the table very efficiently and very effectively off of",
    "start": "1121840",
    "end": "1127420"
  },
  {
    "text": "my GSI and I don't have to run a table scan so now I can run a schedule lambda process that runs every 15 minutes or so",
    "start": "1127420",
    "end": "1133420"
  },
  {
    "text": "in scans the entire table space on the GSI and finds all of the expired tickets",
    "start": "1133420",
    "end": "1138460"
  },
  {
    "text": "I could have one expired ticket on the table the table could be a terabyte and all I'm gonna pay is the cost of doing n",
    "start": "1138460",
    "end": "1144400"
  },
  {
    "text": "reads on the GSI right I have 100 partitions on the GSI that's 100 WCU's or $100 to use all right that's that's a",
    "start": "1144400",
    "end": "1151510"
  },
  {
    "text": "lot cheaper than scanning the entire table and paying it terabytes worth of RCU cost right so it's a very good",
    "start": "1151510",
    "end": "1156970"
  },
  {
    "text": "pattern for being able to selectively read items off the entire the entire table space when you need to is to use",
    "start": "1156970",
    "end": "1163030"
  },
  {
    "text": "these right sharted GSI is very common pattern that we use across the our customer base subsequently if we have",
    "start": "1163030",
    "end": "1171730"
  },
  {
    "text": "tickets expire you can use TTL details a really cool solution set an attribute on the items I don't that those that",
    "start": "1171730",
    "end": "1177610"
  },
  {
    "text": "attribute has a timestamp and once that timestamp expires there's a sweeper that runs in the background it's going to",
    "start": "1177610",
    "end": "1183220"
  },
  {
    "text": "pull all those items off the table so you can then pick those items up off the stream because those are TTL deletes",
    "start": "1183220",
    "end": "1188590"
  },
  {
    "text": "they'll show up as system delete so you'll be able to differentiate the TTL delete from an application layer delete",
    "start": "1188590",
    "end": "1194020"
  },
  {
    "text": "and you can process them accordingly put them on an archive table send them out to s3 just let them fall on the floor or whatever you want",
    "start": "1194020",
    "end": "1199360"
  },
  {
    "text": "so again TTL great solution for kind of cleaning off your table right charted",
    "start": "1199360",
    "end": "1204640"
  },
  {
    "text": "GSIS excellent solution for being able to efficiently scan the entire table space these are very important design",
    "start": "1204640",
    "end": "1210730"
  },
  {
    "text": "patterns we'll talk about as we roll all this stuff up next one we're going to look at is product catalog I saw this",
    "start": "1210730",
    "end": "1217690"
  },
  {
    "start": "1214000",
    "end": "1241000"
  },
  {
    "text": "one on Black Friday Cyber Monday I was actually up in the war room in Seattle was a pretty cool experience watching",
    "start": "1217690",
    "end": "1224080"
  },
  {
    "text": "the infrastructure scale dynamically the way DynamoDB does it's an amazing platform when you really look at how it",
    "start": "1224080",
    "end": "1230260"
  },
  {
    "text": "operates there's no database in the world that works this way even our competitors can't offer the kind of",
    "start": "1230260",
    "end": "1235690"
  },
  {
    "text": "scalability and elasticity options that dynamo has it's really wonderful to see",
    "start": "1235690",
    "end": "1241230"
  },
  {
    "start": "1241000",
    "end": "1260000"
  },
  {
    "text": "but you know oftentimes when we get into these types of thundering herd's",
    "start": "1241230",
    "end": "1246370"
  },
  {
    "text": "scenarios right everyone wants this so Black Friday Cyber Monday it's all day the front page deals what are the hot",
    "start": "1246370",
    "end": "1253150"
  },
  {
    "text": "active deals right we want to want to go get this item we want to get that item and there's a subset of them and you got",
    "start": "1253150",
    "end": "1258460"
  },
  {
    "text": "everybody coming and hitting a single partition space yeah this uneven or unbalanced request distribution across",
    "start": "1258460",
    "end": "1265210"
  },
  {
    "start": "1260000",
    "end": "1343000"
  },
  {
    "text": "the key space so what we want to do now is put out a cache up in front of the table and Dax is a wonderful solution",
    "start": "1265210",
    "end": "1271059"
  },
  {
    "text": "was introduced earlier this year it's a fully integrated front-end cache for dynamo dB I operates in write-through",
    "start": "1271059",
    "end": "1276940"
  },
  {
    "text": "mode it's fully distributed so it scales to very very large size I offer sub one",
    "start": "1276940",
    "end": "1282760"
  },
  {
    "text": "millisecond latency and you don't have to pay for the reads all right you pay for the Dax instance that deploys on your V PC use a custom",
    "start": "1282760",
    "end": "1289090"
  },
  {
    "text": "driver for this that will connect straight to that Dax instance and anything that's read and written through",
    "start": "1289090",
    "end": "1294460"
  },
  {
    "text": "the Dax incidents will be or read or written through the Dax instance would just be retrieved from the table and",
    "start": "1294460",
    "end": "1300760"
  },
  {
    "text": "updated in cash right you don't have to worry about populating the cash it's totally transparent so it's a",
    "start": "1300760",
    "end": "1305800"
  },
  {
    "text": "really neat solution I've lots of customers now using this it gives us that sub one millisecond latency that so",
    "start": "1305800",
    "end": "1312160"
  },
  {
    "text": "many customers need especially ad ad serving and whatnot the latency is",
    "start": "1312160",
    "end": "1317260"
  },
  {
    "text": "incredibly important right I mean if I one millisecond can be the difference between making the sale and not oftentimes a web service he's going to",
    "start": "1317260",
    "end": "1324850"
  },
  {
    "text": "broker that ad out to many providers so the guy that can bring it back fast is so we used to have customers doing",
    "start": "1324850",
    "end": "1330200"
  },
  {
    "text": "parallel reads right or you know so that they could make sure they have the lowest latency read off the table and",
    "start": "1330200",
    "end": "1336799"
  },
  {
    "text": "now they've been able to cut their read cost in half because they're just reading once from Dax and they're getting the sub one millisecond that they need all right when you put the",
    "start": "1336799",
    "end": "1345889"
  },
  {
    "text": "action for the table much different request distribution we're starting to see now all of this data the reads are",
    "start": "1345889",
    "end": "1351919"
  },
  {
    "text": "hitting the Dax instance the pressure on the table is alleviated we're not going to see that in balanced query load so",
    "start": "1351919",
    "end": "1357669"
  },
  {
    "text": "very good solution for that type of Thundering Herd so now we're getting a targeting queries this we're getting",
    "start": "1357669",
    "end": "1363110"
  },
  {
    "text": "composite keys and this is the crux of relational modeling and no SQL okay when",
    "start": "1363110",
    "end": "1369019"
  },
  {
    "text": "you start to look at it's one thing to say one to many relationship parent-child that's easy to maintain but how do I maintain something where I have",
    "start": "1369019",
    "end": "1375320"
  },
  {
    "text": "a many-to-many or I have a parent-child child relationship and these types of multi-tiered hierarchies and we get into",
    "start": "1375320",
    "end": "1382130"
  },
  {
    "text": "this we'll start using composite keys and so a simple scenario for composite key would be let's say I have a gaming",
    "start": "1382130",
    "end": "1387950"
  },
  {
    "start": "1383000",
    "end": "1531000"
  },
  {
    "text": "scenario a bunch of users and we have game sessions with status and I'm interested in sorting the users game",
    "start": "1387950",
    "end": "1394880"
  },
  {
    "text": "sessions by date and filtering on their status all right so DynamoDB supports two types of query conditions we have a",
    "start": "1394880",
    "end": "1401600"
  },
  {
    "text": "sort key condition and we have a query we have a filter condition the sort key condition is very selective it's applied",
    "start": "1401600",
    "end": "1408019"
  },
  {
    "text": "to the sort key and it restricts the number of items that the system will read it's a basically defines what your",
    "start": "1408019",
    "end": "1414409"
  },
  {
    "text": "read cost is the filter condition is not as I guess you'd say selective on the",
    "start": "1414409",
    "end": "1419510"
  },
  {
    "text": "read write it's applied after the read so you're going to read everything that matches the store key condition and then you're going to filter out the items",
    "start": "1419510",
    "end": "1424909"
  },
  {
    "text": "that don't match the filter condition so in this particular example I'm saying give me everything sorted by date that's",
    "start": "1424909",
    "end": "1430070"
  },
  {
    "text": "great my store key is a date stamp so items are always sorted by date and I want everything that's pending so I'm",
    "start": "1430070",
    "end": "1437090"
  },
  {
    "text": "going to read these three items I'm going to filter out the in progress item because it doesn't meet my filter condition I guess and back to in this",
    "start": "1437090",
    "end": "1442850"
  },
  {
    "text": "particular case not a problem these three items total one less than one RCU so the filter query is just as effective",
    "start": "1442850",
    "end": "1448610"
  },
  {
    "text": "as a sort key it's going to cost me one RCU to read three is going to cost in one RCU to read two so having a more",
    "start": "1448610",
    "end": "1454700"
  },
  {
    "text": "selective sort key doesn't really help me here but in many cases it can be a huge day right I might have to read thousands of",
    "start": "1454700",
    "end": "1461480"
  },
  {
    "text": "records out of the partition to only get a few so if that's the case then what I want to do is I want to create what we call composite keys and I create in this",
    "start": "1461480",
    "end": "1468409"
  },
  {
    "text": "example I create a status date I would concatenate those two values together and create a generated attribute and now",
    "start": "1468409",
    "end": "1474200"
  },
  {
    "text": "what this looks like on the table is it creates a nested hierarchy right I have",
    "start": "1474200",
    "end": "1479450"
  },
  {
    "text": "now game sessions that are sorted by state and date and I can use range",
    "start": "1479450",
    "end": "1486140"
  },
  {
    "text": "queries on the state and the date so in this figure example I say it begins with pending but maybe I only want the",
    "start": "1486140",
    "end": "1492140"
  },
  {
    "text": "pending in the last 24 hours I can say it's a greater than pending underbar",
    "start": "1492140",
    "end": "1497240"
  },
  {
    "text": "date 1 and set that date to 21 24 hours ago that would give me everything that's",
    "start": "1497240",
    "end": "1502399"
  },
  {
    "text": "pending that's greater than 24 hours or that's within the last 24 hours right so very neat ways now that I can start to",
    "start": "1502399",
    "end": "1509330"
  },
  {
    "text": "play with these sort key conditions to give me highly selective result sets right without having to use filter",
    "start": "1509330",
    "end": "1515179"
  },
  {
    "text": "conditions so again it all depends on when you use one versus the other when am i reading then how much does it",
    "start": "1515179",
    "end": "1520820"
  },
  {
    "text": "cost and what's the complexity of the query structure the key structure I'm trying to build those trade-offs here",
    "start": "1520820",
    "end": "1526580"
  },
  {
    "text": "but you can always come up with the sort key that's going to give you a nice selective result set another way to be",
    "start": "1526580",
    "end": "1532640"
  },
  {
    "start": "1531000",
    "end": "1573000"
  },
  {
    "text": "able to deal with getting highly selective sets is use sparse GS eyes GSI isn't dynamodb are always what we call",
    "start": "1532640",
    "end": "1539630"
  },
  {
    "text": "sparse items that do not have those attributes just won't show up ok so let's say I want to find all the particular items on",
    "start": "1539630",
    "end": "1546169"
  },
  {
    "text": "a table and they're all the particular people that have an award right and only the users that have an award will have",
    "start": "1546169",
    "end": "1552200"
  },
  {
    "text": "the award attribute then I can create a GSI on the award attribute and now when I scan that GSI I'm going to be reading",
    "start": "1552200",
    "end": "1558289"
  },
  {
    "text": "only the the users that actually have awards so neat thing about this like I can provision a lot less are wcu to the",
    "start": "1558289",
    "end": "1565309"
  },
  {
    "text": "GSI because it's not taking all the data is taking a fraction of the data right so I don't need nearly as much capacity",
    "start": "1565309",
    "end": "1570710"
  },
  {
    "text": "on the GSI as I need on the table all right neat ways to play with concatenate",
    "start": "1570710",
    "end": "1576220"
  },
  {
    "text": "composite keys by concatenated attributes and we'll get into some pretty complex examples of this coming",
    "start": "1576220",
    "end": "1581690"
  },
  {
    "text": "up pretty quick here and again we're rolling all this stuff up into a real-world use case so a vertical",
    "start": "1581690",
    "end": "1588140"
  },
  {
    "start": "1586000",
    "end": "1627000"
  },
  {
    "text": "partitioning this is another one where it's a pretty you know I got a lot of customers to talk about you know the limitations",
    "start": "1588140",
    "end": "1595330"
  },
  {
    "text": "being the item size right we need to we need you know we use MongoDB and they support sixteen megabyte documents and I",
    "start": "1595330",
    "end": "1601960"
  },
  {
    "text": "spent two years at MongoDB and the first thing I would tell the customer is why are you using sixteen megabyte documents and there's good use cases for it don't",
    "start": "1601960",
    "end": "1608230"
  },
  {
    "text": "get me wrong but the reality is most people start using large documents they don't understand the cost of a large document right every time they update an",
    "start": "1608230",
    "end": "1614650"
  },
  {
    "text": "attribute within that document it costs me the size of the document can I have to read the whole thing to get that integer right so sometimes maybe it's",
    "start": "1614650",
    "end": "1621790"
  },
  {
    "text": "not such a good idea to use large documents maybe it's a better idea to use segmented documents right and we'll",
    "start": "1621790",
    "end": "1627910"
  },
  {
    "start": "1627000",
    "end": "1649000"
  },
  {
    "text": "talk about why that you know a use case where that's very you know that emphasizes that and we'll talk about",
    "start": "1627910",
    "end": "1633070"
  },
  {
    "text": "workflow management so let's say we have a reports table coming in you know report processors have pending and",
    "start": "1633070",
    "end": "1638650"
  },
  {
    "text": "process report items and I need to give an app I'm building an app that needs to give them a view of this so I might look",
    "start": "1638650",
    "end": "1644020"
  },
  {
    "text": "at this and say my first cut at this is no problem I'll create a pending view right on the table and I'll create a GSI for my process view and when I look like",
    "start": "1644020",
    "end": "1650260"
  },
  {
    "start": "1649000",
    "end": "1670000"
  },
  {
    "text": "this where I partition on the owner and I sort on the you know on a composite",
    "start": "1650260",
    "end": "1656350"
  },
  {
    "text": "key of pending slash you know time stamp right and so let's say those items are",
    "start": "1656350",
    "end": "1661900"
  },
  {
    "text": "actually pretty large you know we've got some report bodies here that contain annotations and history and all kinds of information in there and so these",
    "start": "1661900",
    "end": "1667300"
  },
  {
    "text": "reports average 256 kilobytes each so what does that really look like to select somebody's you know pending queue",
    "start": "1667300",
    "end": "1675100"
  },
  {
    "start": "1670000",
    "end": "1687000"
  },
  {
    "text": "and let's say we're looking at 50 items per query that's a lot of RCU right in",
    "start": "1675100",
    "end": "1680890"
  },
  {
    "text": "this particular case is 1600 hours to use to read that pending queue and that's that's going to be really",
    "start": "1680890",
    "end": "1685960"
  },
  {
    "text": "expensive so maybe we need to do something different so a better approach here might be to say okay I'll create a reports table and the reports table is",
    "start": "1685960",
    "end": "1692380"
  },
  {
    "start": "1687000",
    "end": "1713000"
  },
  {
    "text": "going to be a hash only table it's going to be hashed on the report ID and I need to create a GSI that's going to be a",
    "start": "1692380",
    "end": "1697930"
  },
  {
    "text": "pending and I'll create a process GSI as well and those are going to be basically",
    "start": "1697930",
    "end": "1703170"
  },
  {
    "text": "set up for users to be able to read only",
    "start": "1703170",
    "end": "1708280"
  },
  {
    "text": "the summary data right they're not getting the bodies they're only getting the summary data from their from their",
    "start": "1708280",
    "end": "1713860"
  },
  {
    "start": "1713000",
    "end": "1747000"
  },
  {
    "text": "view so now what we have is a GSI that gives us our pending and processed views and we can do it's very very efficient",
    "start": "1713860",
    "end": "1720610"
  },
  {
    "text": "select off of those GS is it only reads the small data and when the user wants that you see a report they click the report",
    "start": "1720610",
    "end": "1726129"
  },
  {
    "text": "they're gonna go pull the report back up off the reports table so that's it it's actually a very good workflow for you",
    "start": "1726129",
    "end": "1731529"
  },
  {
    "text": "know a web application right why are you having like an MVC model where okay guy you know it executes some operation that",
    "start": "1731529",
    "end": "1737679"
  },
  {
    "text": "updates the model and changes the view and we expect to see that type of round-trip operation right so but I want",
    "start": "1737679",
    "end": "1742779"
  },
  {
    "text": "to see a summary view and then I want to see a detailed view right that's a pretty pretty good workflow all right so distribute those one",
    "start": "1742779",
    "end": "1750129"
  },
  {
    "start": "1747000",
    "end": "1759000"
  },
  {
    "text": "toomanyitems I guess you'd say we carve off the big data don't select it until you need it and then it reduces the the",
    "start": "1750129",
    "end": "1758109"
  },
  {
    "text": "overhead on the table all right so let's get into some more advanced concepts",
    "start": "1758109",
    "end": "1763149"
  },
  {
    "start": "1759000",
    "end": "1770000"
  },
  {
    "text": "here we start combining some of these things the first thing we're going to talk about is transactional workflows these are very important obviously in",
    "start": "1763149",
    "end": "1769749"
  },
  {
    "text": "relational databases but they it's funny what's funny is that the most transactional requirements and what I found in OLTP apps don't really come",
    "start": "1769749",
    "end": "1776409"
  },
  {
    "start": "1770000",
    "end": "1818000"
  },
  {
    "text": "from the nature of the workflow itself it comes from the nature of the database right when we have relational databases",
    "start": "1776409",
    "end": "1781899"
  },
  {
    "text": "we distribute the data that builds our application layer items objects we distribute this data across",
    "start": "1781899",
    "end": "1787749"
  },
  {
    "text": "multiple tables and then when I write that or save that object back to the database what happens I have to break",
    "start": "1787749",
    "end": "1793210"
  },
  {
    "text": "that object up and save it in multiple places now all of a sudden I need a transaction because I need that data to",
    "start": "1793210",
    "end": "1798519"
  },
  {
    "text": "commit all at once or not at all because I don't want someone else to build that application layer object and get half the new data and half the old data right",
    "start": "1798519",
    "end": "1805299"
  },
  {
    "text": "well if I be normal eyes the data then that kind of eliminates a lot of the use",
    "start": "1805299",
    "end": "1810340"
  },
  {
    "text": "cases for for acid so it's not nearly as common as you might think that you",
    "start": "1810340",
    "end": "1815409"
  },
  {
    "text": "actually need it once you get into a denormalize datastore but there are times you do write so let's talk about",
    "start": "1815409",
    "end": "1820840"
  },
  {
    "start": "1818000",
    "end": "1910000"
  },
  {
    "text": "that versioning auditing you know when when when I have a committed version of an object so this might be emulating an",
    "start": "1820840",
    "end": "1827019"
  },
  {
    "text": "acid transaction on a single item and we've got here is a situation where I have an item ID defines the partition",
    "start": "1827019",
    "end": "1832450"
  },
  {
    "text": "and then I have versioned instances of that item and I have one special version",
    "start": "1832450",
    "end": "1838419"
  },
  {
    "text": "which is the v-0 item right v-0 is the current version so when I save that out",
    "start": "1838419",
    "end": "1843460"
  },
  {
    "text": "in the very first time I'm going to create this partition for item ID 1 I'm actually going to write two items in that partition v-0 and v1 and on the v-0",
    "start": "1843460",
    "end": "1850869"
  },
  {
    "text": "item I'm going to set the current version attribute to one ok so now anyone who reads the item and says I",
    "start": "1850869",
    "end": "1856239"
  },
  {
    "text": "want the current version says begins with zero they get the current version and they can actually see which version that is alright even though they're always",
    "start": "1856239",
    "end": "1862690"
  },
  {
    "text": "getting V zero someone else comes along they want to create a new version in this case we go ahead and create v3 you",
    "start": "1862690",
    "end": "1868570"
  },
  {
    "text": "use a conditional insert right conditionally insert the v3 item if it",
    "start": "1868570",
    "end": "1874450"
  },
  {
    "text": "fails and somebody else is working on this thing already right and you can go back and you can check the v-0 item",
    "start": "1874450",
    "end": "1879820"
  },
  {
    "text": "until the version bumps up to v3 and you know that it's committed now I'll try and create v4 right you can execute",
    "start": "1879820",
    "end": "1885580"
  },
  {
    "text": "multiple operations on the v3 item and when you're done editing that item you overwrite or clobber the v-0 item with",
    "start": "1885580",
    "end": "1891970"
  },
  {
    "text": "the new v3 item and you update the v3 version and you do that with a conditional updates and as long as the",
    "start": "1891970",
    "end": "1898270"
  },
  {
    "text": "convert current version is - that way you know nobody came along behind you and changed things under the covers right so this is how you maintain these",
    "start": "1898270",
    "end": "1904510"
  },
  {
    "text": "types of transactional workflows in no SQL database and this works very well on a single item but how do we lock",
    "start": "1904510",
    "end": "1911380"
  },
  {
    "start": "1910000",
    "end": "1960000"
  },
  {
    "text": "multiple items or a whole partition of items and so or create item sets that we",
    "start": "1911380",
    "end": "1917350"
  },
  {
    "text": "can now version so let's this scenario we'll talk about warehousing and let's have a picker running around the warehouse and I have an Assemblies table",
    "start": "1917350",
    "end": "1924700"
  },
  {
    "text": "with a select star from assemblies where ID equals one he gets a list of products he goes i picks those products no",
    "start": "1924700",
    "end": "1930550"
  },
  {
    "text": "problem let's say this 724 operation it never shuts down we're familiar with that amazon fulfillment centers they",
    "start": "1930550",
    "end": "1936190"
  },
  {
    "text": "operate that way right item tables they cannot go offline to be updated so the",
    "start": "1936190",
    "end": "1941560"
  },
  {
    "text": "purchasing department comes along and they're going to go and find a new source and they're updating the items for that assembly and they start",
    "start": "1941560",
    "end": "1947860"
  },
  {
    "text": "inserting items on the table meanwhile the Pickers come along all of a sudden you sees three four five six items you",
    "start": "1947860",
    "end": "1953620"
  },
  {
    "text": "doesn't know what to go you goes get some all right that's not right that's not good we need to know which ones are valid versions right so how do we do",
    "start": "1953620",
    "end": "1960310"
  },
  {
    "start": "1960000",
    "end": "2092000"
  },
  {
    "text": "that we do it with metadata we start to insert and decorate these partitions with metadata items that describe things",
    "start": "1960310",
    "end": "1965770"
  },
  {
    "text": "like lock state and you know what are the valid version sets in this particular example we've got an item",
    "start": "1965770",
    "end": "1971170"
  },
  {
    "text": "here that has a item list attribute and each one of those items each one of those arrays of items is a valid version",
    "start": "1971170",
    "end": "1979570"
  },
  {
    "text": "set when the reader comes along he selects this if he doesn't see all the items in the partition for a particular version said he knows which ones he has",
    "start": "1979570",
    "end": "1985870"
  },
  {
    "text": "a complete set of he can choose whether to use that completed version or valid version set",
    "start": "1985870",
    "end": "1991580"
  },
  {
    "text": "read again you can determine whether or not the partition is locked by looking at the lockstate attribute right is it",
    "start": "1991580",
    "end": "1997700"
  },
  {
    "text": "zero nobody's working on this partition is it one know somebody is I can lock that partition with conditional update",
    "start": "1997700",
    "end": "2003250"
  },
  {
    "text": "on that metadata item now what's important about this and you're gonna see a lot of transaction libraries for",
    "start": "2003250",
    "end": "2008560"
  },
  {
    "text": "DynamoDB right people have tried to implement these types of solutions they don't work and the reason why is",
    "start": "2008560",
    "end": "2014560"
  },
  {
    "text": "DynamoDB is a fully distributed request router there's no guarantee that order of requests are gonna you know when that",
    "start": "2014560",
    "end": "2020410"
  },
  {
    "text": "are issued at a time across to clients right so it's like the two generals problem how do I guarantee want you know",
    "start": "2020410",
    "end": "2026350"
  },
  {
    "text": "delivery in order in sequence and once only across the distributed system it's not possible right so what will happen",
    "start": "2026350",
    "end": "2033820"
  },
  {
    "text": "is you'll have a somebody come along and try and establish a lock somebody else comes along and reads the lock the guy",
    "start": "2033820",
    "end": "2039400"
  },
  {
    "text": "that tried to establish the lockout a p99 late and see the guy that tried to read the lock got a p1 latency he gets",
    "start": "2039400",
    "end": "2044410"
  },
  {
    "text": "he gets the read says it's clean right he goes over tries to read the item now he gets a p99 latency and the guy that",
    "start": "2044410",
    "end": "2050649"
  },
  {
    "text": "got the lock well he comes through and he gets like v p1 latency updates so I just read a dirty item but I don't know",
    "start": "2050650",
    "end": "2056139"
  },
  {
    "text": "that right the only way to know that the item is dirty is to if the lock state comes down in the query because",
    "start": "2056140",
    "end": "2061720"
  },
  {
    "text": "nothing's gonna change those items while I'm reading them all right so if I do a query on the partition and I say get",
    "start": "2061720",
    "end": "2067060"
  },
  {
    "text": "everything where ID equals 1 and the metadata that describes the lock comes down in that query then the state of that lock is consistent with the data",
    "start": "2067060",
    "end": "2073929"
  },
  {
    "text": "that I'm looking at and that's the only way to know so you have to maintain this lock metadata in the partitions that I'm",
    "start": "2073930",
    "end": "2079000"
  },
  {
    "text": "querying or it won't work or on the items that I'm walking or it won't work all right so stay away from those",
    "start": "2079000",
    "end": "2084730"
  },
  {
    "text": "transaction libraries are just going to cause you problems I end up having to undo a lot of that when you know a lot",
    "start": "2084730",
    "end": "2089770"
  },
  {
    "text": "of the teams that I work with and I'm using that stuff all right geo hashing is another technique and I'm",
    "start": "2089770",
    "end": "2096190"
  },
  {
    "start": "2092000",
    "end": "2157000"
  },
  {
    "text": "not going to really get into doing this because we have a nice client-side library that does it for you but just so",
    "start": "2096190",
    "end": "2102910"
  },
  {
    "text": "you understand what's going on in the covers it's pretty straightforward process to geotag and what you're really doing is you just take some map area",
    "start": "2102910",
    "end": "2108730"
  },
  {
    "text": "draw quadrants you draw quadrants within the quadrants and quadrants within those quadrants until you get down to some level of glory annual area that you find",
    "start": "2108730",
    "end": "2115030"
  },
  {
    "text": "to be interesting for your application and along the way you're just tagging and creating a hash of all of these",
    "start": "2115030",
    "end": "2120910"
  },
  {
    "text": "quadrant IDs and a big long string and you're using that as a sort key on your now I can issue range queries against",
    "start": "2120910",
    "end": "2126220"
  },
  {
    "text": "that sort key to find out who's in my quadrant who's in quadrants that are adjacent to my quadrant and whatnot",
    "start": "2126220",
    "end": "2132190"
  },
  {
    "text": "again we have a full geo hash library client-side geo hashing library it does this for you but just so you know under",
    "start": "2132190",
    "end": "2137290"
  },
  {
    "text": "the covers a lot of people don't understand that though this is this is really what geo indexing is right so",
    "start": "2137290",
    "end": "2142510"
  },
  {
    "text": "when people talk about I got a geo index and do these guys support geo indexing and well all they're really doing is geo",
    "start": "2142510",
    "end": "2147910"
  },
  {
    "text": "hashing under the covers so this will probably end up at some point becoming part of the DynamoDB API but it's not",
    "start": "2147910",
    "end": "2154210"
  },
  {
    "text": "yet so I'd use the client-side library okay composite key modeling getting into",
    "start": "2154210",
    "end": "2161950"
  },
  {
    "start": "2157000",
    "end": "2166000"
  },
  {
    "text": "multi-tiered hierarchies right we talked about this two-tiered hierarchy before let's get into something that's a little bit more you know relational and so in",
    "start": "2161950",
    "end": "2169570"
  },
  {
    "text": "this particular example we've got an application that pulls up office locations around the country in various cities right and so the scheme of the",
    "start": "2169570",
    "end": "2175869"
  },
  {
    "text": "relational schema that describes this particular tables on the Left you know the pretty straightforward hierarchy",
    "start": "2175869",
    "end": "2181000"
  },
  {
    "text": "country state City office again I want to get all the offices and giving city it's pretty complex query I got to",
    "start": "2181000",
    "end": "2187240"
  },
  {
    "text": "execute on the right hand side we're using a composites or key approach where I basically have the country as the hash",
    "start": "2187240",
    "end": "2193960"
  },
  {
    "text": "or the partition key and I have the city hash or the states at hash city hash",
    "start": "2193960",
    "end": "2200020"
  },
  {
    "text": "office ID as the Tsar key right so now I can actually get all the cities and all the offices given state by you know",
    "start": "2200020",
    "end": "2206710"
  },
  {
    "text": "select from country where location starts with state right all the city starts with state hash city right",
    "start": "2206710",
    "end": "2213550"
  },
  {
    "text": "specific office state hash city sash offers that's hash office ID again I've",
    "start": "2213550",
    "end": "2219460"
  },
  {
    "text": "taken a very complex query inner joins through a multi-tiered hierarchy and I flattened it and turned it into a",
    "start": "2219460",
    "end": "2226630"
  },
  {
    "text": "composite key structure on the table that gives me the ability to execute simple queries and get the same result",
    "start": "2226630",
    "end": "2232119"
  },
  {
    "text": "sets all right the other thing you can do is take those hierarchies and you can put them into a table as JSON attributes",
    "start": "2232119",
    "end": "2238630"
  },
  {
    "start": "2233000",
    "end": "2279000"
  },
  {
    "text": "this is going back to our product catalog example where you know I've got a couple items that describe books",
    "start": "2238630",
    "end": "2243670"
  },
  {
    "text": "albums and movies and I've taken the hierarchy of tables in that multi-tier structure and I've collapsed them into a",
    "start": "2243670",
    "end": "2248800"
  },
  {
    "text": "JSON object that sits on or attribute that sits on each one of these items now at the top level of the item are the",
    "start": "2248800",
    "end": "2255310"
  },
  {
    "text": "attributes I want to index right so you know dynamodb cannot support does not support",
    "start": "2255310",
    "end": "2263110"
  },
  {
    "text": "indexing JSON attributes but the way it is indexed em is to project them when you write the attribute ID them into the",
    "start": "2263110",
    "end": "2268760"
  },
  {
    "text": "table right there's no reason why I can't take the attribute up and out of the JSON and make a first-class attribute on the item once I do that",
    "start": "2268760",
    "end": "2274850"
  },
  {
    "text": "then sure I can I can put the GSI on that so that's a great way to do that okay this is starting to roll it all up",
    "start": "2274850",
    "end": "2282560"
  },
  {
    "start": "2279000",
    "end": "2320000"
  },
  {
    "text": "and this is one of the most useful design patterns that we run across is",
    "start": "2282560",
    "end": "2288050"
  },
  {
    "text": "the adjacency list pattern so if you talk to graph guys they're all tell you everything's a graph problem and everything is a graph problem because",
    "start": "2288050",
    "end": "2294320"
  },
  {
    "text": "graphs are all about relationships and since almost olt v8p apps are about aggregations those are relationships so",
    "start": "2294320",
    "end": "2299870"
  },
  {
    "text": "you can look at everything as a graph right so a graphs are really simply adjacency lists in the aces lists are",
    "start": "2299870",
    "end": "2305120"
  },
  {
    "text": "awesome for no SQL databases and this the way we actually build those things you define partitions on the table",
    "start": "2305120",
    "end": "2310280"
  },
  {
    "text": "basically is partitioned on a node ID and then we start adding edges items into those partitions right and these",
    "start": "2310280",
    "end": "2316370"
  },
  {
    "text": "are the edges that define our nodes now the first edge that we're gonna add to every node is that is the edge that",
    "start": "2316370",
    "end": "2321770"
  },
  {
    "start": "2320000",
    "end": "2351000"
  },
  {
    "text": "describes itself right so you can see we have a node ID of one in the top item",
    "start": "2321770",
    "end": "2327530"
  },
  {
    "text": "that is actually a person node and it has an edge that targets itself and it has some data that describes the edge",
    "start": "2327530",
    "end": "2333470"
  },
  {
    "text": "right so every edge that we throw on this table is going to have this attribute called data and if you haven't",
    "start": "2333470",
    "end": "2338960"
  },
  {
    "text": "noticed something yet you see that how the data is not always the same in one case is it's a name in one case it's a",
    "start": "2338960",
    "end": "2344330"
  },
  {
    "text": "it's a date you know there's it's a city name right so you know what's up with that that attribute is overloaded all right so",
    "start": "2344330",
    "end": "2351020"
  },
  {
    "start": "2351000",
    "end": "2558000"
  },
  {
    "text": "what we're gonna do now is we're gonna take that thing and we're going to create overloaded GS eyes and I'm going",
    "start": "2351020",
    "end": "2356960"
  },
  {
    "text": "to actually index the data attribute and what I've done now is I've created an overloaded GSI that allows me to query",
    "start": "2356960",
    "end": "2363320"
  },
  {
    "text": "that all those partitions the 0 to n partitions and if I want to get everybody that was born in a given right",
    "start": "2363320",
    "end": "2370990"
  },
  {
    "text": "date I can query with a date string on that if I want to get somebody who was has a given name or starts with a given",
    "start": "2370990",
    "end": "2378890"
  },
  {
    "text": "name I can query with a proper name right and it was born in Finland it was born",
    "start": "2378890",
    "end": "2385070"
  },
  {
    "text": "on this date right so very interesting queries I can start to execute and what done now as I've taken one attribute",
    "start": "2385070",
    "end": "2390680"
  },
  {
    "text": "data and I've indexed three different types so what this really means is that",
    "start": "2390680",
    "end": "2397230"
  },
  {
    "text": "it's not about five TS is per table it's not really a limit right it's about how",
    "start": "2397230",
    "end": "2402810"
  },
  {
    "text": "many item types am i throwing on the table and I can index five attributes per item type because I can always call",
    "start": "2402810",
    "end": "2408480"
  },
  {
    "text": "the attributes the same name in which case they'll show up on the same index and they might be different types might be a date in one case it might be a",
    "start": "2408480",
    "end": "2413640"
  },
  {
    "text": "string in the other heck it might be might be a hexadecimal conversion of a number right I can use that I do that",
    "start": "2413640",
    "end": "2419550"
  },
  {
    "text": "sometimes when I want to do I am running out indexes I'll just convert the number into a 4x8 hexadecimal string it's what",
    "start": "2419550",
    "end": "2424980"
  },
  {
    "text": "that string sortable and I can actually execute a range query against a hexadecimal string and return a numeric",
    "start": "2424980",
    "end": "2430410"
  },
  {
    "text": "results set if I need to so there's a variety of ways that you can index a number of attributes I have applications",
    "start": "2430410",
    "end": "2436350"
  },
  {
    "text": "where the next 4050 attributes just because that's the number of items that we have on the table and we can start to",
    "start": "2436350",
    "end": "2441570"
  },
  {
    "text": "index those attributes if you almost think about it's like I can index how many indexes am I gonna put on a table in a relational database you're not",
    "start": "2441570",
    "end": "2447480"
  },
  {
    "text": "gonna put five indexes on a table all right so every item type is really a table schema so think about it that way and",
    "start": "2447480",
    "end": "2453930"
  },
  {
    "text": "that's how we can deal with overloading now that's fantastic but the best part about this is real graph queries are not",
    "start": "2453930",
    "end": "2459570"
  },
  {
    "text": "simple adjacency lists right real graph queries are talking about things like they were in any States sub tree",
    "start": "2459570",
    "end": "2465540"
  },
  {
    "text": "aggregations right friends of my friends right you know breadth first search is no",
    "start": "2465540",
    "end": "2470850"
  },
  {
    "text": "drinking's how do we do all this stuff well we use EMR okay and we run this EMR",
    "start": "2470850",
    "end": "2476040"
  },
  {
    "text": "job as a scheduled process and it'll grind through the table on a regular basis and project all those graph query",
    "start": "2476040",
    "end": "2481260"
  },
  {
    "text": "results back onto the edges so when I query the table and I look for hey let me go get everybody who's my friends and",
    "start": "2481260",
    "end": "2487230"
  },
  {
    "text": "all those edges come back I say well I want to see all the friends of my friends that data is written to each one of the edges that constitute my friends",
    "start": "2487230",
    "end": "2494280"
  },
  {
    "text": "all right so now of a sudden I have a graph database that scales to any size and delivers sub one millisecond latency",
    "start": "2494280",
    "end": "2500700"
  },
  {
    "text": "with Dax in front of it I have a reference implementation that my team did it has a million nodes and ten million edges we didn't put Dax in front",
    "start": "2500700",
    "end": "2507360"
  },
  {
    "text": "of it but we're delivering sub or low single-digit millisecond latency on any query right so you know most",
    "start": "2507360",
    "end": "2515220"
  },
  {
    "text": "applications don't need real-time result sets from graph query right like I go to LinkedIn if I add you",
    "start": "2515220",
    "end": "2521860"
  },
  {
    "text": "as a contact it's not like instantly every single contact of mine is going to know that you're my contact right at the end of the day they'll run a job and",
    "start": "2521860",
    "end": "2528160"
  },
  {
    "text": "they'll grind through all the data and they'll project all this data back into the database as metadata right this is",
    "start": "2528160",
    "end": "2534340"
  },
  {
    "text": "the way most graph applications actually work because craft databases don't scale and I know we have Neptune Neptune is",
    "start": "2534340",
    "end": "2540400"
  },
  {
    "text": "fantastic and I think it's really going to be you know a game changer for graph",
    "start": "2540400",
    "end": "2545410"
  },
  {
    "text": "but Neptune won't be able to scale to this level I could put a billion nodes on this table 10 billion edges it won't",
    "start": "2545410",
    "end": "2551800"
  },
  {
    "text": "matter because it doesn't matter how big the table gets the latency is consistent at any size right so rolling it all up",
    "start": "2551800",
    "end": "2560260"
  },
  {
    "start": "2558000",
    "end": "2630000"
  },
  {
    "text": "into a real-world use case we'll talk about the audible eBook sync service this is a internal team here at Amazon",
    "start": "2560260",
    "end": "2567460"
  },
  {
    "text": "if you're familiar with and was on Kindle right you get ebooks in Kindle there's audiobook content that's",
    "start": "2567460",
    "end": "2573100"
  },
  {
    "text": "associated with those ebooks and the ebooks are audible audible ebooks sync service is responsible for understanding",
    "start": "2573100",
    "end": "2578170"
  },
  {
    "text": "what content you have access to and what content belongs to these particular",
    "start": "2578170",
    "end": "2584460"
  },
  {
    "text": "audio files and where you are when you're playing the book write an actual I'm playing this on my device I stop I",
    "start": "2584460",
    "end": "2591580"
  },
  {
    "text": "come back I want to resume right so that's what this service does it's really it's just a straightforward mapping table that they put in the",
    "start": "2591580",
    "end": "2597700"
  },
  {
    "text": "middle of a bunch of relational tables they have audio products they have ebook products and they map these audio",
    "start": "2597700",
    "end": "2603340"
  },
  {
    "text": "products and the sink info to these various ebook products right pretty",
    "start": "2603340",
    "end": "2608860"
  },
  {
    "text": "straightforward now the team was having a trouble trying to understand how are they going to implement this in no SQL right they had several ideas they kind",
    "start": "2608860",
    "end": "2616210"
  },
  {
    "text": "of went through the implementations and they said you know this is just the latency is too high the overhead operational overheads too high so they",
    "start": "2616210",
    "end": "2622780"
  },
  {
    "text": "chose to come and engage my team it's a blackbelt team you know again I said we worked with a lot of the internal",
    "start": "2622780",
    "end": "2628000"
  },
  {
    "text": "services and so we came up with a nice design for them and you know bottom line is have a very large number of access",
    "start": "2628000",
    "end": "2634120"
  },
  {
    "start": "2630000",
    "end": "2649000"
  },
  {
    "text": "patterns so why do I use this this that's why I use this example right most use cases for small micro services they",
    "start": "2634120",
    "end": "2639940"
  },
  {
    "text": "don't have a lot of access patterns but these guys did so they needed to pull off all their tables all these mapping infos they have various consumers of all",
    "start": "2639940",
    "end": "2646420"
  },
  {
    "text": "this information so 20 different access patterns and so what we ended up doing was building a really nice",
    "start": "2646420",
    "end": "2651460"
  },
  {
    "start": "2649000",
    "end": "2723000"
  },
  {
    "text": "table schema here it's a single table it's an adjacency list implementation we have nodes that are defined as the a",
    "start": "2651460",
    "end": "2657970"
  },
  {
    "text": "book a CRS which are the audiobook audio content records ebook ACRs which is a",
    "start": "2657970",
    "end": "2663130"
  },
  {
    "text": "book audible content records and then they're the mappings between those two so on the audible content record on the",
    "start": "2663130",
    "end": "2670120"
  },
  {
    "text": "on the a book ACR record we actually maintain the mapping to the e-book so you see that edge there it's a second",
    "start": "2670120",
    "end": "2675370"
  },
  {
    "text": "one down and then we also the one to many mapping between the a book a CRS and the tracks right that's also",
    "start": "2675370",
    "end": "2681280"
  },
  {
    "text": "maintained in the e-book a CR partition so I want to get all the tracks for an a book a CR I can say select where a book",
    "start": "2681280",
    "end": "2687280"
  },
  {
    "text": "a CR I'd equals x and starts with a book a CR ID I'll get all the tracks for that a book a CR right so on so forth when I",
    "start": "2687280",
    "end": "2694510"
  },
  {
    "text": "get all the mappings I want to get and now they wanted version control on the a book a CR record right so we implemented",
    "start": "2694510",
    "end": "2700300"
  },
  {
    "text": "version control you can see that v-0 hash a book a CR that's the actual metadata describes the a book a CR in",
    "start": "2700300",
    "end": "2706540"
  },
  {
    "text": "its version so every time they add a new version they're gonna clobber v-0 they're gonna create a new version item",
    "start": "2706540",
    "end": "2712180"
  },
  {
    "text": "right so the only edge that we actually need on the e-book a CR note is the e-book ACR edge the one that describes",
    "start": "2712180",
    "end": "2717880"
  },
  {
    "text": "itself right because the mappings to it are mapped on the a book ACR and then we can create a couple indexes to pull back",
    "start": "2717880",
    "end": "2724540"
  },
  {
    "text": "all of the relationships here right like I want a cue I my query by a scene I want query by SKU I want a query for all",
    "start": "2724540",
    "end": "2730510"
  },
  {
    "text": "of my everything's associated with this eBook ACR right this is what's done on the GSIS right so now what I have is I have a",
    "start": "2730510",
    "end": "2736870"
  },
  {
    "text": "single table I have three GS eyes and I know this is an eye chart you're not gonna be able go through it all but what",
    "start": "2736870",
    "end": "2742300"
  },
  {
    "start": "2737000",
    "end": "2831000"
  },
  {
    "text": "we're really doing here is I'm creating a map of all of their access patterns to which index with which sort key",
    "start": "2742300",
    "end": "2748240"
  },
  {
    "text": "conditions in which filter conditions to produce the results that they were looking for right now we went through",
    "start": "2748240",
    "end": "2753430"
  },
  {
    "text": "this exercise with this team because they have a legacy API they need to support so if you look at these access",
    "start": "2753430",
    "end": "2758890"
  },
  {
    "text": "patterns a lot of these things seem to be relational right give me all the mappings for this particular thing why so that I can go get all those things",
    "start": "2758890",
    "end": "2765010"
  },
  {
    "text": "right so the actual workflow against this table is not the most efficient but",
    "start": "2765010",
    "end": "2770530"
  },
  {
    "text": "we gave them them on we've given them a really nice efficient table that they're gonna be able to change for the next generation API is gonna be a lot simpler",
    "start": "2770530",
    "end": "2776560"
  },
  {
    "text": "customers are not going to have to come in and get the mappings then get the items they're just going give them a query condition they're gonna get those items but the table",
    "start": "2776560",
    "end": "2782980"
  },
  {
    "text": "supports their legacy API too so they don't have to do anything to support both so that's what's really nice about",
    "start": "2782980",
    "end": "2788350"
  },
  {
    "text": "this we wrapped all this stuff up together and that is a really solid real-world example of taking all of",
    "start": "2788350",
    "end": "2793600"
  },
  {
    "text": "those basic design patterns that I talked about and rolling it up into something that is actually usable and",
    "start": "2793600",
    "end": "2798940"
  },
  {
    "text": "applicable to your real-world problems I know we went through a whole bunch of content in a very short period of time this was the result of a lot of feedback",
    "start": "2798940",
    "end": "2806350"
  },
  {
    "text": "that we've gotten over the over the last couple sessions at reinvent you guys wanted the technical stuff so I gave it to you",
    "start": "2806350",
    "end": "2812110"
  },
  {
    "text": "so hopefully you liked it the contents gonna be available up online the charts will be available up online and again if",
    "start": "2812110",
    "end": "2818140"
  },
  {
    "text": "you're running any projects you can always engage the the specialist sa team or my team I just going through AWS",
    "start": "2818140",
    "end": "2825100"
  },
  {
    "text": "support or your account team and we're happy to be there for you so thanks a lot and have a good",
    "start": "2825100",
    "end": "2832320"
  },
  {
    "start": "2831000",
    "end": "2962000"
  },
  {
    "text": "so we've got a few minutes any questions",
    "start": "2836290",
    "end": "2840400"
  },
  {
    "text": "go through all this stuff we are actually doing a refresh on the docks that will be up on the public site",
    "start": "2842859",
    "end": "2848780"
  },
  {
    "text": "sometime in the first half of next year it's gonna happen a lot of this content probably the best place to get a hold of",
    "start": "2848780",
    "end": "2854000"
  },
  {
    "text": "this content is the slide deck in the presentation here from reinvent that I",
    "start": "2854000",
    "end": "2859430"
  },
  {
    "text": "had time for a blog that unfortunately just yeah I'm sorry",
    "start": "2859430",
    "end": "2870280"
  },
  {
    "text": "encryption well there's a client-side encryption library for DynamoDB we've",
    "start": "2870609",
    "end": "2876890"
  },
  {
    "text": "had obviously it's a very high priority request we've had from lots of people as you can see dynamo is moving in the",
    "start": "2876890",
    "end": "2883430"
  },
  {
    "text": "speed of light right now as far as features go that's all I can really tell you I I don't have a timeline on any of that stuff I can certainly tell you that",
    "start": "2883430",
    "end": "2890180"
  },
  {
    "text": "that as far as a product feature request is not unique no that's it all right go",
    "start": "2890180",
    "end": "2901030"
  },
  {
    "text": "true you know I mean relational database with the JSON attribute that's cool thing",
    "start": "2902470",
    "end": "2909049"
  },
  {
    "text": "there's no doubt about it you can store a hierarchy inside of a row I mean but it's still a relational database right",
    "start": "2909049",
    "end": "2914420"
  },
  {
    "text": "and you're still gonna be in so now you can use relational databases in a denormalized manner and that helps you",
    "start": "2914420",
    "end": "2920299"
  },
  {
    "text": "do that and when you do that you offload the CPU and you achieve a lot of the same benefits so you can cause like a",
    "start": "2920299",
    "end": "2926180"
  },
  {
    "text": "Postgres oracle ms SQL you can make all these databases scale much better by using these denormalized access patterns",
    "start": "2926180",
    "end": "2932390"
  },
  {
    "text": "but they're not going to scale to the level of a new SQL database all no SQL databases are partition tolerant whereas",
    "start": "2932390",
    "end": "2937819"
  },
  {
    "text": "relational databases are not so you have you know layer technologies that people made available PG router things like",
    "start": "2937819",
    "end": "2944480"
  },
  {
    "text": "this to be able to help with partitioning of those databases but fundamentally they're not designed to operate in a partition tolerant mode so",
    "start": "2944480",
    "end": "2950539"
  },
  {
    "text": "they're just not going to scale as well but you can make them scale better by applying denormalized design patterns and using things like JSON attributes",
    "start": "2950539",
    "end": "2957140"
  },
  {
    "text": "and whatnot all right all right thanks everybody",
    "start": "2957140",
    "end": "2963849"
  }
]