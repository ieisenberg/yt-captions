[
  {
    "text": "hi everyone welcome to the session and thanks for coming my name is Maddy I'm a",
    "start": "960",
    "end": "6089"
  },
  {
    "text": "software engineer at signal effects and today I'm going to cover how we scale and operate the last exhibition signal",
    "start": "6089",
    "end": "12840"
  },
  {
    "text": "effects what you should expect from the session is four main topics one we'll",
    "start": "12840",
    "end": "18720"
  },
  {
    "text": "talk about how he use elastic stretch at signal effects - we're going to talk",
    "start": "18720",
    "end": "23760"
  },
  {
    "text": "about the important metrics and important alerts that you need to worry about three we're going to talk about",
    "start": "23760",
    "end": "29750"
  },
  {
    "text": "elastic search capacity planning and the last topic is about how we do zero",
    "start": "29750",
    "end": "35190"
  },
  {
    "text": "downtime regarding of elastic search I'll cover a little bit of the elastic search basics if if some of the audience",
    "start": "35190",
    "end": "41670"
  },
  {
    "text": "is not familiar with elastic search so elastic search should signal effects",
    "start": "41670",
    "end": "47870"
  },
  {
    "text": "first I want to clarify that this is not the Amazon elastic search service this",
    "start": "47870",
    "end": "54420"
  },
  {
    "text": "is basically a signal effects deploying elastic search on top of ec2",
    "start": "54420",
    "end": "60680"
  },
  {
    "text": "signal effects is a cloud monitoring solution and as part of ingesting a ton",
    "start": "60680",
    "end": "67590"
  },
  {
    "text": "of metrics into the system we have to make them available for search and that's what elastic search comes into",
    "start": "67590",
    "end": "73770"
  },
  {
    "text": "play we use it for three main use cases one is being able to run ad-hoc queries against the data we should be able to",
    "start": "73770",
    "end": "80549"
  },
  {
    "text": "find a metric by the metric name by properties on the metrics etc the second",
    "start": "80549",
    "end": "86100"
  },
  {
    "text": "use cases we use it for those suggestions so people can type in as they type things are being suggested and",
    "start": "86100",
    "end": "91470"
  },
  {
    "text": "that's being driven by elastic search in the backend and our third use case is obviously the full-text search you type",
    "start": "91470",
    "end": "98790"
  },
  {
    "text": "for example here the word watch and you get things that include the word watch and the results",
    "start": "98790",
    "end": "105829"
  },
  {
    "text": "the clusters that we have in productions you actually have four of them",
    "start": "105829",
    "end": "111360"
  },
  {
    "text": "I'm just going to focus on the biggest one because that's the most interesting it's a 50 for data nodes three master",
    "start": "111360",
    "end": "117420"
  },
  {
    "text": "nodes six client nodes they're all deployed across the availability zones we have over 1.3 I think my last week it",
    "start": "117420",
    "end": "125759"
  },
  {
    "text": "was 1.5 billion unique documents in the system we have 10 plus terabyte of data",
    "start": "125759",
    "end": "131849"
  },
  {
    "text": "this is only the primary if you include the replicas that jumps to 30 terabyte we have about 270 shots in the system",
    "start": "131849",
    "end": "138590"
  },
  {
    "text": "this includes primaries and replicas and we do sustained 75 queries per second",
    "start": "138590",
    "end": "144560"
  },
  {
    "text": "this is queries coming from our services the cluster itself because it has so many number of shards handles about",
    "start": "144560",
    "end": "150830"
  },
  {
    "text": "3,000 queries per second and usually sustained load is a thousand",
    "start": "150830",
    "end": "156800"
  },
  {
    "text": "indexing operation per second and we do spiked to three thousand sometimes five or six thousands indexing operation per",
    "start": "156800",
    "end": "163580"
  },
  {
    "text": "second the way we deploy this on AWS one we use docker it makes everything easier",
    "start": "163580",
    "end": "171170"
  },
  {
    "text": "from a kind of development development and production standpoint so what you do in development is very similar to what",
    "start": "171170",
    "end": "176600"
  },
  {
    "text": "you do in end production we've been using docker since they want it it made things a lot easier we use a mix of",
    "start": "176600",
    "end": "182480"
  },
  {
    "text": "elasticsearch versions - 0.33 and 175 we've been trying to",
    "start": "182480",
    "end": "188120"
  },
  {
    "text": "migrate everything - to the - point X release it's a lot much better we",
    "start": "188120",
    "end": "193550"
  },
  {
    "text": "orchestrate all these target containers using a open source framework called maestro ng it was written by one of our",
    "start": "193550",
    "end": "201530"
  },
  {
    "text": "engineers our biggest cluster in terms of the machine types we use for the data",
    "start": "201530",
    "end": "207860"
  },
  {
    "text": "nodes we use I to the 2x large nodes these come with 61 gigs of memory we do",
    "start": "207860",
    "end": "213890"
  },
  {
    "text": "use only 16 gigs and the past we've been running with 31 gigs of memory and you",
    "start": "213890",
    "end": "219080"
  },
  {
    "text": "know this one we reduced to 16 gigs they were performance improvement and most of these performance improvements were due",
    "start": "219080",
    "end": "225200"
  },
  {
    "text": "to the smaller garbage collections that happen when es needs to allocate more",
    "start": "225200",
    "end": "231740"
  },
  {
    "text": "memory for the master nodes these are stateless knows that they don't have to be that powerful we use an 3 dot large",
    "start": "231740",
    "end": "238340"
  },
  {
    "text": "we allocate 2 gigabytes to the JVM heap and the client nodes a little bit",
    "start": "238340",
    "end": "244240"
  },
  {
    "text": "beefier these are also stateless machines and we use em 3 dot X large with 10 gigs of heap one thing that's",
    "start": "244240",
    "end": "251180"
  },
  {
    "text": "that's that we we found out by experience is that it's it turned out to be a very good decision to split these",
    "start": "251180",
    "end": "257780"
  },
  {
    "text": "into different tiers by default when you install elasticsearch all these different responsibilities end up being",
    "start": "257780",
    "end": "263750"
  },
  {
    "text": "on the same nodes and that's fine for development staging but once when you once you go to production it becomes a",
    "start": "263750",
    "end": "269930"
  },
  {
    "text": "completely different story for example you definitely don't want your master notes to be on your data notes because",
    "start": "269930",
    "end": "275120"
  },
  {
    "text": "if you're if you have JVM thrashing and the system is is is basically trying to",
    "start": "275120",
    "end": "281870"
  },
  {
    "text": "get CPU it's not going to be able to get CPU and then anything that's related to",
    "start": "281870",
    "end": "286990"
  },
  {
    "text": "heartbeat of the master is not going to make it in time and you can have all sorts of issues when that happens so",
    "start": "286990",
    "end": "292730"
  },
  {
    "text": "definitely split the master nodes into dedicated machines usually you should put them on either three or five",
    "start": "292730",
    "end": "299060"
  },
  {
    "text": "machines and configure the cluster to only elect the master if there is a",
    "start": "299060",
    "end": "304460"
  },
  {
    "text": "quorum which is very very important setting and then the client nodes are also as important because they act as a",
    "start": "304460",
    "end": "311480"
  },
  {
    "text": "fuse for the system you don't want to lose any data nodes because Dana knows hold the data and if you lose one and",
    "start": "311480",
    "end": "317390"
  },
  {
    "text": "you have to restart it it's gonna have to sync up the data again which is a very intensive from an i/o perspective",
    "start": "317390",
    "end": "323060"
  },
  {
    "text": "or CPU perspective operation so you want to have client nodes isolate the system",
    "start": "323060",
    "end": "329840"
  },
  {
    "text": "as much as you can so if something goes bad is gonna go bad on the client nodes and the client nodes will crash for",
    "start": "329840",
    "end": "334970"
  },
  {
    "text": "example and since they're stateless you just restart them and it's fine another thing nice about the client nodes is",
    "start": "334970",
    "end": "341060"
  },
  {
    "text": "that they're aware of the racket distribution of the data nodes so if you",
    "start": "341060",
    "end": "346550"
  },
  {
    "text": "talk to a client node in a z1 it's gonna talk to data nodes in the same easy and",
    "start": "346550",
    "end": "353150"
  },
  {
    "text": "obviously if those are not available it's it's gonna go to other nodes in other disease but the preference is to",
    "start": "353150",
    "end": "359630"
  },
  {
    "text": "the same AZ which makes latency better it also avoid costly cross easy traffic",
    "start": "359630",
    "end": "366250"
  },
  {
    "text": "we deployed this to cross three availability zones we used basically yes",
    "start": "366250",
    "end": "371300"
  },
  {
    "text": "right awareness to do this so you have one one primary and two replicas one in HSE so you can you can completely lose",
    "start": "371300",
    "end": "378340"
  },
  {
    "text": "an AZ and still have access to your data my cap restore are very important you",
    "start": "378340",
    "end": "386419"
  },
  {
    "text": "definitely want to have this if something that happens you should be able to restore your data we use the AWS",
    "start": "386419",
    "end": "391910"
  },
  {
    "text": "cloud plugin it's it's very easy to use actually you just need to set up to set",
    "start": "391910",
    "end": "398330"
  },
  {
    "text": "it up we backup to s3 we do incremental backups it's supported",
    "start": "398330",
    "end": "403430"
  },
  {
    "text": "by default by the plug-in we use a nest an inversion s3 bucket the reason we",
    "start": "403430",
    "end": "409340"
  },
  {
    "text": "went for inversion is because of a bug in the AWS cloud plug-in that when you",
    "start": "409340",
    "end": "415789"
  },
  {
    "text": "have a lot of versions the plug-in actually tries to fetch all those versions and would usually timeout as",
    "start": "415789",
    "end": "421520"
  },
  {
    "text": "you have more and more incremental backups so we turned off versioning we would love to turn it back on because if",
    "start": "421520",
    "end": "427490"
  },
  {
    "text": "you have versioning enable on your s3 bucket one of the features of s3 is you can actually do cross region replication",
    "start": "427490",
    "end": "433699"
  },
  {
    "text": "so you can have a copy of your a few backup in and a completely different reason for disaster recovery purposes we",
    "start": "433699",
    "end": "440180"
  },
  {
    "text": "do use also a dedicated DPC s3 endpoint this allows us to channel the backup",
    "start": "440180",
    "end": "446090"
  },
  {
    "text": "traffic into a single endpoint instead of putting it on the same VPC with all",
    "start": "446090",
    "end": "451490"
  },
  {
    "text": "the other services usually the backups are pretty heavy and they do consume bandwidth we also use instance profiles for",
    "start": "451490",
    "end": "458770"
  },
  {
    "text": "authentication to s3 so you don't have to store any username or password this",
    "start": "458770",
    "end": "464120"
  },
  {
    "text": "works really nicely and for the frequency of running the backups obviously this depends on your case but",
    "start": "464120",
    "end": "471110"
  },
  {
    "text": "in ours we do we have a cron job that does a snapshot every hour and then we",
    "start": "471110",
    "end": "477080"
  },
  {
    "text": "rotate the backups on a weekly basis ok so let's let's jump to how we monitor",
    "start": "477080",
    "end": "484009"
  },
  {
    "text": "and alert on elasticsearch so this is one of the dashboards that I usually you",
    "start": "484009",
    "end": "489289"
  },
  {
    "text": "would like to look at and it has what I think are the key metrics that we usually care about an hour for our use",
    "start": "489289",
    "end": "496729"
  },
  {
    "text": "case CPU load was one of the most problematic areas we were always having",
    "start": "496729",
    "end": "501979"
  },
  {
    "text": "CPU spikes and queries that can that can spike resources on the system so CPU was one",
    "start": "501979",
    "end": "508550"
  },
  {
    "text": "of our top things to watch in the system and obviously it could be i/o if you were like very indexing heavy it could",
    "start": "508550",
    "end": "514849"
  },
  {
    "text": "be anything else but for us it was CPU so we do watch CPU on the data nodes that's that's on the on the left side",
    "start": "514849",
    "end": "521870"
  },
  {
    "text": "and then we do watch it on on the client nodes it's usually a lot much smaller",
    "start": "521870",
    "end": "527390"
  },
  {
    "text": "there are some spikes due to the queries and you see that so spikes correlate with the spikes on the day or not the master nodes are usually not very spiky",
    "start": "527390",
    "end": "534630"
  },
  {
    "text": "I think the spikes there are actually and related to elasticsearch and then",
    "start": "534630",
    "end": "539730"
  },
  {
    "text": "what we watch next is the queries coming into the system so we do monitor the",
    "start": "539730",
    "end": "545910"
  },
  {
    "text": "queries on both sides one is on the client side which is the services that actually access the elasticsearch and",
    "start": "545910",
    "end": "551040"
  },
  {
    "text": "the other side is on elasticsearch itself the good thing about monitoring",
    "start": "551040",
    "end": "556440"
  },
  {
    "text": "this on the client side is that you were able to determine by service which service is actually sending query to",
    "start": "556440",
    "end": "562380"
  },
  {
    "text": "your elasticsearch cluster which is useful if you have a spike in traffic and you need to figure out where those",
    "start": "562380",
    "end": "567690"
  },
  {
    "text": "queries are coming from and then we obviously watch the queries on the elasticsearch side and we expect as you",
    "start": "567690",
    "end": "574350"
  },
  {
    "text": "can see here it's it's about 3,000 or 3700 queries per second across the whole",
    "start": "574350",
    "end": "581040"
  },
  {
    "text": "cluster and we also watch the indexing requests we do about 3,000 in this case and then",
    "start": "581040",
    "end": "590960"
  },
  {
    "text": "after that what we wanna watch is the merging that's going on behind the",
    "start": "590960",
    "end": "596610"
  },
  {
    "text": "scenes as you know elasticsearch uses Lucene underneath and busine keeps its",
    "start": "596610",
    "end": "602010"
  },
  {
    "text": "data and changed so when you update or delete a document you are actually not updating it in place you're changing a",
    "start": "602010",
    "end": "608310"
  },
  {
    "text": "lucien segment you're actually adding more data to a Lusine segment you're never changing a Lusine segment and at",
    "start": "608310",
    "end": "614760"
  },
  {
    "text": "some point you can have so many Lucene segments that your search performance is going to get impacted so what Lucene",
    "start": "614760",
    "end": "620280"
  },
  {
    "text": "does behind the scenes is merge these Lucene segments into a signal into a single segment and that merge operation",
    "start": "620280",
    "end": "627240"
  },
  {
    "text": "is very IO and CPU intensive so we usually keep an eye on it and if we see a CPU in spike if you see if you see a",
    "start": "627240",
    "end": "633930"
  },
  {
    "text": "spike in CPU we're usually either able to correlate it to a spike in queries to a spike in indexing or to spike and an",
    "start": "633930",
    "end": "641339"
  },
  {
    "text": "inversion going on in the system so this is kind of the happy face of the",
    "start": "641339",
    "end": "646500"
  },
  {
    "text": "equation we usually look at this dashboard pretty much every day just to make sure things are looking sane but",
    "start": "646500",
    "end": "652500"
  },
  {
    "text": "it's very important that you have a detection system that allows you to figure out when things are not going as",
    "start": "652500",
    "end": "659040"
  },
  {
    "text": "usual and for this what we usually focus on is high CPU users and obviously low",
    "start": "659040",
    "end": "665670"
  },
  {
    "text": "disk space you definitely don't wanna disk space definitely donor disk space don't do that",
    "start": "665670",
    "end": "672020"
  },
  {
    "text": "sustained high heat usage sometimes the queries will well for example if you",
    "start": "672020",
    "end": "677610"
  },
  {
    "text": "have an aggregation and that aggregation has high cardinality field you're going to consume a ton of memory and when that",
    "start": "677610",
    "end": "683100"
  },
  {
    "text": "happens you're gonna see your heap stay I'd say 80% for a very long period of",
    "start": "683100",
    "end": "689130"
  },
  {
    "text": "time and that's a bad time because if you get another query like that your system is gonna die with an with an out",
    "start": "689130",
    "end": "695340"
  },
  {
    "text": "of memory exception and that's not good so you want to alert and and and have someone take a look at that when it",
    "start": "695340",
    "end": "700410"
  },
  {
    "text": "happens master notes availability is very important because since we set up the cluster with a quorum number of",
    "start": "700410",
    "end": "707040"
  },
  {
    "text": "eligible masters if we lose one master note we still okay but we if you lose another one we're in trouble so usually",
    "start": "707040",
    "end": "713640"
  },
  {
    "text": "if we lose one master node we wake up somebody to to start to start another another instance the cluster state green",
    "start": "713640",
    "end": "719940"
  },
  {
    "text": "yellow red very clear what's going on yellow is kinda in a mid state it it could be completely fine what we like to",
    "start": "719940",
    "end": "726870"
  },
  {
    "text": "do in this case is instead of watching directly the yellow state we watch our SL A's so in terms of queries we would",
    "start": "726870",
    "end": "734220"
  },
  {
    "text": "like our queries to stay within 100 millisecond so the cluster could be yellow but as long as we're still within",
    "start": "734220",
    "end": "740160"
  },
  {
    "text": "100 millisecond of grey latency we're completely okay we don't we don't have to wake up somebody if it's like 2:00",
    "start": "740160",
    "end": "745740"
  },
  {
    "text": "a.m. to fix the problem and bring up another node but we will deal with it in the morning if the SLA goes down while",
    "start": "745740",
    "end": "752190"
  },
  {
    "text": "losing machines then we would like somebody to to take a look at it and this gets a little bit more complicated",
    "start": "752190",
    "end": "758550"
  },
  {
    "text": "with in a sine charge because you lose a machine and then the machine may come back and elasticsearch is gonna reassign",
    "start": "758550",
    "end": "764430"
  },
  {
    "text": "the shards that usually what's what happens but if it does not what's gonna end up happening it you can have an",
    "start": "764430",
    "end": "769830"
  },
  {
    "text": "assigned shards that will stick around for a long time and we do have an alert and we know like based on how much time",
    "start": "769830",
    "end": "777300"
  },
  {
    "text": "it takes us to recover a node it takes us about an hour to recover a node so if",
    "start": "777300",
    "end": "782310"
  },
  {
    "text": "we see in an assign chart for more than an hour and a half we know that we have we have to add capacity basically and",
    "start": "782310",
    "end": "788430"
  },
  {
    "text": "that's when we wake up somebody the last one that we think is very important is the thread pool rejections so the way",
    "start": "788430",
    "end": "794820"
  },
  {
    "text": "elasticsearch deals with load it basically for things like search it has",
    "start": "794820",
    "end": "799890"
  },
  {
    "text": "a thread pool which has a fixed number of threads these are the threads that will do the search and then in front of that thread pool it has a queue and like",
    "start": "799890",
    "end": "806820"
  },
  {
    "text": "you has a fixed size and at some point as you're piling up more search requests against the cluster if the system is not",
    "start": "806820",
    "end": "813240"
  },
  {
    "text": "able to deal with those requests fast enough those requests will end up being",
    "start": "813240",
    "end": "818279"
  },
  {
    "text": "in the queue and once the queue reaches a certain size which is the limit of the queue elastics which is basically gonna",
    "start": "818279",
    "end": "825000"
  },
  {
    "text": "start protecting itself and saying I cannot I cannot run this request anymore and when you see that it's it's usually",
    "start": "825000",
    "end": "830940"
  },
  {
    "text": "an indication that you're either running out of capacity like your system is growing and you're running a lot more",
    "start": "830940",
    "end": "836370"
  },
  {
    "text": "requests and the system just cannot handle it that's that's number one number two is you may have a bad service",
    "start": "836370",
    "end": "842940"
  },
  {
    "text": "like somebody write a little wrote a loop and that will be just hitting your cluster from like 50 or 60 threads and",
    "start": "842940",
    "end": "848010"
  },
  {
    "text": "it's basically taking causing elasticsearch to do to do those projections so you want to be able to",
    "start": "848010",
    "end": "853230"
  },
  {
    "text": "identify and and react to that pretty quickly one thing that's also we found extremely",
    "start": "853230",
    "end": "860130"
  },
  {
    "text": "important is to be able to test your detectors and alerts so when you set up an alert usually what you will do is you",
    "start": "860130",
    "end": "866459"
  },
  {
    "text": "will use tribal knowledge like we know my our queries per second doesn't",
    "start": "866459",
    "end": "871620"
  },
  {
    "text": "usually go behind beyond 10,000 per second so I'm gonna sit on alert based on that but most of the time what's",
    "start": "871620",
    "end": "877920"
  },
  {
    "text": "gonna happen is you will get an alert at 2 a.m. and you figure oh like 10,000 that that's probably okay like maybe",
    "start": "877920",
    "end": "884250"
  },
  {
    "text": "it's 15,000 and you kind of keep tuning the system over and over again and if",
    "start": "884250",
    "end": "889380"
  },
  {
    "text": "you have an ops team the ops team will get upset about this so a good thing to have is to be able to simulate based on",
    "start": "889380",
    "end": "897690"
  },
  {
    "text": "historical data if an if an alert will actually fire or not so that you know if if your alert is good enough",
    "start": "897690",
    "end": "905390"
  },
  {
    "text": "okay let's talk about capacity planning this is one of those areas that a lot of people say is black magic",
    "start": "905390",
    "end": "911990"
  },
  {
    "text": "it's actually a pretty a pretty hard thing to figure out so we'll try to kind of entangle it a little bit so there are",
    "start": "911990",
    "end": "920490"
  },
  {
    "text": "two things to think about from a capacity perspective for elasticsearch and they're obviously indexing and queries so indexing is is",
    "start": "920490",
    "end": "929760"
  },
  {
    "text": "mostly CPU and i/o intensive because you're getting document you need to do talk if you're using like crazy and",
    "start": "929760",
    "end": "936660"
  },
  {
    "text": "grams and stuff like that then that's gonna be expensive and then eventually need to put things on this that causes",
    "start": "936660",
    "end": "942450"
  },
  {
    "text": "IO load on the system and while this is happening you could have murders that happen in",
    "start": "942450",
    "end": "949590"
  },
  {
    "text": "the background the problem with murders is that you don't actually control them we don't control when they happen this",
    "start": "949590",
    "end": "956220"
  },
  {
    "text": "used to be really bad in the previous versions of elasticsearch one point X basically there were a lot of knobs that",
    "start": "956220",
    "end": "963300"
  },
  {
    "text": "you can tune to control this but it was manual and in two point X this completely changed so leucine has this",
    "start": "963300",
    "end": "970290"
  },
  {
    "text": "concept of adaptive merging which basically tried to spread the merging",
    "start": "970290",
    "end": "975960"
  },
  {
    "text": "load over a longer period of time by throttling the i/o and CPU consumption",
    "start": "975960",
    "end": "981240"
  },
  {
    "text": "and that makes those Murshid merges a lot more smoother from a resource perspective which is which is pretty",
    "start": "981240",
    "end": "986850"
  },
  {
    "text": "nice so if you have an option don't don't use anything less than 2 point X",
    "start": "986850",
    "end": "992300"
  },
  {
    "text": "queries is the other side of the beast and this is actually depends on on your",
    "start": "992300",
    "end": "998550"
  },
  {
    "text": "use case so some new skills are very query intensive and there's not much indexing going on other use cases are",
    "start": "998550",
    "end": "1004130"
  },
  {
    "text": "very indexing heavy and less number of queries our use case is both we get we",
    "start": "1004130",
    "end": "1009140"
  },
  {
    "text": "get both sides we have steady indexing load and we have steady query load which is a little bit problematic so for",
    "start": "1009140",
    "end": "1015230"
  },
  {
    "text": "queries what we have seen in our case is mostly CPU load and memory alone and usually the CPU load is coming either",
    "start": "1015230",
    "end": "1022130"
  },
  {
    "text": "from expensive queries that have to load us to load a lot of stuff into the heap and",
    "start": "1022130",
    "end": "1028420"
  },
  {
    "text": "also the garbage collection pressure on the system this is a JVM system so garbage collection is going to consume",
    "start": "1028420",
    "end": "1034730"
  },
  {
    "text": "CPU and you want to keep that below 5% of your total CPU consumption so it's",
    "start": "1034730",
    "end": "1039800"
  },
  {
    "text": "another thing to keep in mind so the next question that comes into place is",
    "start": "1039800",
    "end": "1046270"
  },
  {
    "text": "elasticsearch is elastic so how can you deal with capacity problems",
    "start": "1046270",
    "end": "1053290"
  },
  {
    "text": "since you have an elastic system you can easily add nodes and scale up the",
    "start": "1053290",
    "end": "1058370"
  },
  {
    "text": "cluster that way that's very easily set but in practice there are couple issues",
    "start": "1058370",
    "end": "1064130"
  },
  {
    "text": "so just to explain in elastic Serge the data is partitioned and each one of",
    "start": "1064130",
    "end": "1070200"
  },
  {
    "text": "these partition is basically called shard and elastic search terms and you have a number of primary shots so you",
    "start": "1070200",
    "end": "1077190"
  },
  {
    "text": "can say I want my data to be split into two primary shots and then you can create as many replicas as you want so",
    "start": "1077190",
    "end": "1083940"
  },
  {
    "text": "on your left we have a system with two nodes two primary shots and",
    "start": "1083940",
    "end": "1089630"
  },
  {
    "text": "one replicas a basic so you have two shots that are replicas so in this case you have two shots running on each node",
    "start": "1089630",
    "end": "1096330"
  },
  {
    "text": "so say your system is not is now running close to 70% CPU and you would like to",
    "start": "1096330",
    "end": "1101970"
  },
  {
    "text": "keep that under 50% to allow for some growth so what you do is very easily you",
    "start": "1101970",
    "end": "1107190"
  },
  {
    "text": "spin up two extra two extra nodes elasticsearch is gonna automatically move some of the shards across the nodes",
    "start": "1107190",
    "end": "1113759"
  },
  {
    "text": "and now you cannot doubled your capacity immediately and you will see technically your CPU load go down from 70% to",
    "start": "1113759",
    "end": "1119789"
  },
  {
    "text": "something close to 30 or 35% which is pretty nice but say that after a couple months",
    "start": "1119789",
    "end": "1125460"
  },
  {
    "text": "you're back to the same problem it's running at 70% so what do you do now so there couple of",
    "start": "1125460",
    "end": "1132419"
  },
  {
    "text": "options on how to deal with this one of them is to add more replicas if you had",
    "start": "1132419",
    "end": "1138330"
  },
  {
    "text": "another replica set you can add more machines and move those replicas sets to those machines however this is only",
    "start": "1138330",
    "end": "1144450"
  },
  {
    "text": "gonna help with the query load if your problem is not query load if it's",
    "start": "1144450",
    "end": "1150240"
  },
  {
    "text": "indexing then you pretty much stuck because all these shorts would have to do exactly the same amount of work for",
    "start": "1150240",
    "end": "1155759"
  },
  {
    "text": "indexing so if you're indexing is the guy who is consuming 50 or 60% of your CPU you can hit the wall at some point",
    "start": "1155759",
    "end": "1162000"
  },
  {
    "text": "and you cannot index fast enough so this kind of brings the question as the the",
    "start": "1162000",
    "end": "1167309"
  },
  {
    "text": "question that I get product here is how do I pick up a right number of primary shot in this case I pick two but why",
    "start": "1167309",
    "end": "1173100"
  },
  {
    "text": "didn't I pick six why did I pick a thousand like what how should I pick this number one thing that I probably",
    "start": "1173100",
    "end": "1179460"
  },
  {
    "text": "didn't mention is that once you pick this number you cannot change it anymore you can just increase it and expect",
    "start": "1179460",
    "end": "1185519"
  },
  {
    "text": "things to work and it doesn't you cannot do that so that's a decision that you have to make up front there it really",
    "start": "1185519",
    "end": "1192779"
  },
  {
    "text": "depends on the use case I'll talk about how we dealt with this but this is one of the recommendation that that you will",
    "start": "1192779",
    "end": "1198480"
  },
  {
    "text": "find people talking about so you create an index with one you simulate what you expect your",
    "start": "1198480",
    "end": "1205620"
  },
  {
    "text": "indexing load to be and you observe what's the impact on CPU and i/o and you",
    "start": "1205620",
    "end": "1211950"
  },
  {
    "text": "you find where it breaks what breaks depend on you maybe you wanted to stay within 50% for you that's that's",
    "start": "1211950",
    "end": "1218220"
  },
  {
    "text": "basically where it breaks because you don't want to consume more than 50% to over all locate for future growth and then you do the same thing with grades",
    "start": "1218220",
    "end": "1224610"
  },
  {
    "text": "and why this is hard is sometimes when you start up your company or when you build your elastic storage cluster you",
    "start": "1224610",
    "end": "1230940"
  },
  {
    "text": "actually still don't know how the indexing is gonna look like you don't know how the queries are gonna look like and even if you guess you may end up",
    "start": "1230940",
    "end": "1237240"
  },
  {
    "text": "with it completely wrong gasps and this is what basically pushed us to come",
    "start": "1237240",
    "end": "1243720"
  },
  {
    "text": "with this mechanism of recharging elastic search basically allowing us to",
    "start": "1243720",
    "end": "1249720"
  },
  {
    "text": "change the number of primary shards without having any downtime and that's what we're gonna talk about and",
    "start": "1249720",
    "end": "1256140"
  },
  {
    "text": "obviously something I forgot to mention here this consumption this one is is one of the easiest to figure out if you can",
    "start": "1256140",
    "end": "1263240"
  },
  {
    "text": "populate your index with documents that you think are representative of your data set you just need to extrapolate",
    "start": "1263240",
    "end": "1268770"
  },
  {
    "text": "your average document size and figure out how much how many documents you can",
    "start": "1268770",
    "end": "1274080"
  },
  {
    "text": "store basically storage has been something that's that's actually easy to to do capacity plan for but when you",
    "start": "1274080",
    "end": "1280409"
  },
  {
    "text": "combine all these things storage plus CPU plus IO you also have to pick an",
    "start": "1280409",
    "end": "1286200"
  },
  {
    "text": "instance type you have to pick how much memory again allocated the J game it gets it gets a little bit complicated",
    "start": "1286200",
    "end": "1292620"
  },
  {
    "text": "but everything is measurable and and that's why you want to try these things and adjust things as you build up your",
    "start": "1292620",
    "end": "1299100"
  },
  {
    "text": "system and for single effects what we ended up doing when we started this we made some back of the envelope",
    "start": "1299100",
    "end": "1304260"
  },
  {
    "text": "calculation we figured out that we will be okay for six shots and three months later we had 18 nodes with three with",
    "start": "1304260",
    "end": "1311610"
  },
  {
    "text": "with six shots and 18 replicas and sorry twelve replicas and we were at capacity we couldn't add more capacity so we were",
    "start": "1311610",
    "end": "1318750"
  },
  {
    "text": "basically stuck and the cluster was running hot all the time and this is where the next section comes into play",
    "start": "1318750",
    "end": "1324870"
  },
  {
    "text": "is how do you actually do this this reshoring without taking the system down so",
    "start": "1324870",
    "end": "1330889"
  },
  {
    "text": "restarting is usually useful for the example that I mentioned being able to change your number primary shots from",
    "start": "1330889",
    "end": "1337400"
  },
  {
    "text": "say six shots to twelve or eighteen or whatever it's also important if you want",
    "start": "1337400",
    "end": "1342890"
  },
  {
    "text": "to change your mapping file so say you had a field and you didn't enable dog",
    "start": "1342890",
    "end": "1349670"
  },
  {
    "text": "values on that field and I was a mistake and you want to fix it today you cannot fix it I mean okay you can technically",
    "start": "1349670",
    "end": "1355760"
  },
  {
    "text": "fix it by creating a new field with a different name and kind of doing all that migration magic",
    "start": "1355760",
    "end": "1361150"
  },
  {
    "text": "backward compatibility plus minus one compatibility etc but if you have a large number of these",
    "start": "1361150",
    "end": "1367760"
  },
  {
    "text": "changes that you need to make over time for example at some point when we were running 1.4 we were not using dog values at all",
    "start": "1367760",
    "end": "1374420"
  },
  {
    "text": "and we wanted to switch the dog values because device bring huge improvement in terms of CPU and memory consumption so",
    "start": "1374420",
    "end": "1381380"
  },
  {
    "text": "we had to do even even though we didn't have to do a rashard like this the number of shots was the same but we had",
    "start": "1381380",
    "end": "1387620"
  },
  {
    "text": "to do the mapping without any downtime a couple of solution to this problem if you're indexing is is read-only you in",
    "start": "1387620",
    "end": "1394310"
  },
  {
    "text": "paradise that's that's awesome it's like that's the most beautiful thing it's immutable does not change you can just",
    "start": "1394310",
    "end": "1401780"
  },
  {
    "text": "use aliases pawn the alias to the current index copy everything to a new index nothing is changing so no race",
    "start": "1401780",
    "end": "1407870"
  },
  {
    "text": "condition I think it's awesome if it's not like us then you're you're in",
    "start": "1407870",
    "end": "1413780"
  },
  {
    "text": "trouble and so how do you deal with that so Before we jump into that I'd like to",
    "start": "1413780",
    "end": "1419240"
  },
  {
    "text": "go through our metadata storage architecture I think it's important to this discussion so that we have at a",
    "start": "1419240",
    "end": "1426530"
  },
  {
    "text": "very high level these are the main components we have services on the system these services have a library that we call the meta base client",
    "start": "1426530",
    "end": "1433790"
  },
  {
    "text": "there is Kafka that allows us to queue things we have a component called meta base this is basically the pipe the data",
    "start": "1433790",
    "end": "1440510"
  },
  {
    "text": "pipeline and we have Cassandra as our main source of truth and elasticsearch as our search index so we'll follow a",
    "start": "1440510",
    "end": "1449150"
  },
  {
    "text": "right to the system so number one is we in queue what we call a write command",
    "start": "1449150",
    "end": "1454460"
  },
  {
    "text": "that gets in queue on the right topic let's get it gets pulled by one of the meta base instances then we basically",
    "start": "1454460",
    "end": "1460700"
  },
  {
    "text": "write it into Cassandra and then what we do is we in queue an indexing command on",
    "start": "1460700",
    "end": "1466130"
  },
  {
    "text": "the index topic the indexing command is pretty much the ID of the document that we just wrote to",
    "start": "1466130",
    "end": "1472580"
  },
  {
    "text": "Cassandra and that's pretty much it and then one of the meta base instances will will pull from the indexing topic right",
    "start": "1472580",
    "end": "1479000"
  },
  {
    "text": "now it's it's the same tier but eventually we'll split those into a Cassandra right tier in and an es",
    "start": "1479000",
    "end": "1484760"
  },
  {
    "text": "indexing tier and then what we do is we read the document from Cassandra and then we put it into a elasticsearch when",
    "start": "1484760",
    "end": "1493010"
  },
  {
    "text": "we implemented this elasticsearch did not have support for incremental updates so we had to pull the data from",
    "start": "1493010",
    "end": "1498500"
  },
  {
    "text": "Cassandra again our system allows incremental writes so the service when",
    "start": "1498500",
    "end": "1504080"
  },
  {
    "text": "an accused or right it includes an incremental right that's why we have to also fetch the document from Cassandra",
    "start": "1504080",
    "end": "1509299"
  },
  {
    "text": "has actually turned to be a pretty nice thing because since Cassandra is the source of truth we can actually reorder",
    "start": "1509299",
    "end": "1515360"
  },
  {
    "text": "things on the indexing topic because the source of truth is not all sauce the data basically and then once we read it",
    "start": "1515360",
    "end": "1521750"
  },
  {
    "text": "from Cassandra we index it into elasticsearch so there couple things about this architecture",
    "start": "1521750",
    "end": "1526990"
  },
  {
    "text": "one is the queuing aspect and this is awesome because it allows you to completely decouple Cassandra from",
    "start": "1526990",
    "end": "1533899"
  },
  {
    "text": "elasticsearch and what you can do as well you can put throttling controls on",
    "start": "1533899",
    "end": "1539179"
  },
  {
    "text": "your indexing so if you're ok with indexing being behind by say 10 second",
    "start": "1539179",
    "end": "1544519"
  },
  {
    "text": "or 15 seconds then you can tune down your indexing load to avoid loading up",
    "start": "1544519",
    "end": "1552590"
  },
  {
    "text": "the system basically and having cut cut in the middle allows you to do this pretty easily so",
    "start": "1552590",
    "end": "1557950"
  },
  {
    "text": "how does this returning process work so we'll go through the phases of this of",
    "start": "1557950",
    "end": "1564470"
  },
  {
    "text": "this operation for the prerequisites this is what we required to do this we require all",
    "start": "1564470",
    "end": "1572019"
  },
  {
    "text": "system services that are querying elasticsearch to query against an alias",
    "start": "1572019",
    "end": "1577580"
  },
  {
    "text": "they never know what the index name and they shouldn't care so in this case we have an 8 an alias called my index that",
    "start": "1577580",
    "end": "1583639"
  },
  {
    "text": "alias is pointing to the actual index name and you notice the underscore v1",
    "start": "1583639",
    "end": "1588769"
  },
  {
    "text": "which is the version of the index from the client perspective this does not matter prerequisite number two is we",
    "start": "1588769",
    "end": "1595610"
  },
  {
    "text": "maintain an indexing state the indexing state is used by the meta based",
    "start": "1595610",
    "end": "1600830"
  },
  {
    "text": "component that's responsible for indexing the data into we only have one component that can write to elastic",
    "start": "1600830",
    "end": "1606800"
  },
  {
    "text": "search and the indexing state contains three bit three bits of information one what is the current index name and",
    "start": "1606800",
    "end": "1613280"
  },
  {
    "text": "notice this is the index name not the alias two what is the generation number of the index the generation number is",
    "start": "1613280",
    "end": "1620660"
  },
  {
    "text": "something that we add to every document that we write to elasticsearch and this generation number is what allows us to",
    "start": "1620660",
    "end": "1627200"
  },
  {
    "text": "do this and handle concurrency at the same time and that's what I'm gonna cover in the next slides",
    "start": "1627200",
    "end": "1633250"
  },
  {
    "text": "to start the migration you obviously have to create the new index say you had",
    "start": "1633250",
    "end": "1638270"
  },
  {
    "text": "six shards in the past now you create the new index with 18 shards you change your mappings you did all the good stuff",
    "start": "1638270",
    "end": "1643790"
  },
  {
    "text": "that you wanted to do you're ready to go the first step you do is you bump up the",
    "start": "1643790",
    "end": "1649040"
  },
  {
    "text": "generation number when you bump up the generation number what you're basically doing is any incoming indexed operation",
    "start": "1649040",
    "end": "1655610"
  },
  {
    "text": "after the generation number has been changed it's gonna have generation number 43 what practically that means is",
    "start": "1655610",
    "end": "1663200"
  },
  {
    "text": "that anything that's less or equal than 42 is a fixed set it's sorry it's a",
    "start": "1663200",
    "end": "1669950"
  },
  {
    "text": "bounded set it's not gonna increase it's only gonna decrease so if you add a new document it does not get impacted if you",
    "start": "1669950",
    "end": "1677150"
  },
  {
    "text": "update an existing document that documents a had version 5 now that version is gonna get bumped to version",
    "start": "1677150",
    "end": "1683120"
  },
  {
    "text": "43 and while this is happening what you can do is you can do a scroll migration",
    "start": "1683120",
    "end": "1690110"
  },
  {
    "text": "of documents of generation less or equal than 42 and since that set is is bounded",
    "start": "1690110",
    "end": "1698030"
  },
  {
    "text": "then you're not racing with anybody and then while this is happening the system is changing which is the difficulty in",
    "start": "1698030",
    "end": "1704300"
  },
  {
    "text": "how and how you deal with this so you get new stuff that gets added you get new stuff that gets updated and at the",
    "start": "1704300",
    "end": "1711980"
  },
  {
    "text": "end of the what we call the bulk indexing phase what you end up with is two different indices which is expected",
    "start": "1711980",
    "end": "1717650"
  },
  {
    "text": "because things are changing while you are migrating so now what you need to do is somehow reconcile those indices and",
    "start": "1717650",
    "end": "1726170"
  },
  {
    "text": "because you have this generation number that makes the reconciliation very easy",
    "start": "1726170",
    "end": "1731210"
  },
  {
    "text": "so what you do in this case you bump up the generation one more time from 43 to",
    "start": "1731210",
    "end": "1736360"
  },
  {
    "text": "44 and then what you have to do to reconcile assist and I'm sorry one one",
    "start": "1736360",
    "end": "1742279"
  },
  {
    "text": "very important thing when you do this you also said that last bit that I didn't talk about before which is called",
    "start": "1742279",
    "end": "1748249"
  },
  {
    "text": "extra that's an extra index and what it means to the indexer is that instead of",
    "start": "1748249",
    "end": "1753320"
  },
  {
    "text": "writing to a single index now go ahead and write the two indices at the same",
    "start": "1753320",
    "end": "1758419"
  },
  {
    "text": "time and why this is nice is now because anything that comes after this point we",
    "start": "1758419",
    "end": "1763669"
  },
  {
    "text": "know is gonna be in both indices so from now on there is no race condition going",
    "start": "1763669",
    "end": "1769309"
  },
  {
    "text": "on in the system everything goes to both illnesses so what the problem becomes is",
    "start": "1769309",
    "end": "1774350"
  },
  {
    "text": "that anything is that change from the moment we started the but the bulk indexing to the moment we",
    "start": "1774350",
    "end": "1782470"
  },
  {
    "text": "changed to enable double publishing or double writing that's what we have to reconcile and that's exactly generation",
    "start": "1782470",
    "end": "1789830"
  },
  {
    "text": "43 in which we have to migrate so what will migrating generation 43",
    "start": "1789830",
    "end": "1795440"
  },
  {
    "text": "obviously stuff will change so if anything gets updated we'll have it in both so it's gonna get",
    "start": "1795440",
    "end": "1801399"
  },
  {
    "text": "automatically updated if it's not updated and that's the goal of the",
    "start": "1801399",
    "end": "1807590"
  },
  {
    "text": "indexing is find all documents at revision 43 and reindex those documents",
    "start": "1807590",
    "end": "1814039"
  },
  {
    "text": "so the reindex saying there's an interesting race condition because when you read the document and then index it",
    "start": "1814039",
    "end": "1820039"
  },
  {
    "text": "again it may actually have changed and you don't want to overwrite the previous change in our case it's easy to deal",
    "start": "1820039",
    "end": "1825470"
  },
  {
    "text": "with because cassandra is our source of truth and the way we do our writes into the system",
    "start": "1825470",
    "end": "1831320"
  },
  {
    "text": "the writes basically gets realized to a single object so we're writing to a single object those rights will happen",
    "start": "1831320",
    "end": "1837169"
  },
  {
    "text": "one after the other so to reindex we just rewrite those documents we just rewrite a dummy field we update a",
    "start": "1837169",
    "end": "1842480"
  },
  {
    "text": "timestamp and like that causes those documents to be indexed if you don't have that kind of capability you can",
    "start": "1842480",
    "end": "1848509"
  },
  {
    "text": "also use elasticsearch optimistic concurrency control so you fetch the document it has a version and you try to",
    "start": "1848509",
    "end": "1853940"
  },
  {
    "text": "update it given that version number if it fails you try again basically actually you don't even need to try because if it fails it means it has been",
    "start": "1853940",
    "end": "1860059"
  },
  {
    "text": "we indexed and since you're double writing you should be fine so we continue doing this find all those",
    "start": "1860059",
    "end": "1866869"
  },
  {
    "text": "things and at some point we reach what we call a perfect sync between the indices so now the two witnesses are the",
    "start": "1866869",
    "end": "1873400"
  },
  {
    "text": "exact same copy and we're still reading from the earliest and the LS is still pointing",
    "start": "1873400",
    "end": "1880049"
  },
  {
    "text": "to to the current to the current index so now what you can do is start some a",
    "start": "1880049",
    "end": "1886570"
  },
  {
    "text": "be testing on the new index a couple ways to do this what we do in our case is is usually",
    "start": "1886570",
    "end": "1893100"
  },
  {
    "text": "we have a separate customer and our production environment that's basically us and we can only",
    "start": "1893100",
    "end": "1900690"
  },
  {
    "text": "instruct that customer to access the new index and that's a quick way for us to verify if this index is good or not",
    "start": "1900690",
    "end": "1907169"
  },
  {
    "text": "we've actually never seen that it end up being bad so we never had to rollback but it's a good it's a good kind of",
    "start": "1907169",
    "end": "1913390"
  },
  {
    "text": "confidence test that you do another thing that that we also do is we tee off a portion of our query traffic to the",
    "start": "1913390",
    "end": "1920409"
  },
  {
    "text": "new index and the reason we do this is that we would like to warm up the caches so when we switch the indices we don't",
    "start": "1920409",
    "end": "1926740"
  },
  {
    "text": "have that cold restart moment where everything becomes slow till the caches are all filled up so we send portion of",
    "start": "1926740",
    "end": "1933700"
  },
  {
    "text": "the traffic we don't care about the results is just to warm up the caches and at some point what you have to do is",
    "start": "1933700",
    "end": "1939789"
  },
  {
    "text": "you just need to switch the alias from the current index to the new index and this is this kind of the cool moment",
    "start": "1939789",
    "end": "1945850"
  },
  {
    "text": "when you say I'm gonna do it and everybody is kind of holding their stuff and trying to make sure nothing is gonna",
    "start": "1945850",
    "end": "1953020"
  },
  {
    "text": "is gonna burn you know usually nothing it's more stressful but nothing nothing usually burns and even if something goes",
    "start": "1953020",
    "end": "1959440"
  },
  {
    "text": "bad you can always switch the index you can have the inconvenience of impacting some of these queries but you're not",
    "start": "1959440",
    "end": "1964570"
  },
  {
    "text": "losing any data which is which is extremely important you still double writing so once you you switch and you",
    "start": "1964570",
    "end": "1971110"
  },
  {
    "text": "think the index is good another step that we do in this case is we bump up the index generation again one more time",
    "start": "1971110",
    "end": "1978279"
  },
  {
    "text": "and I'll explain why we do that we change the current index so now we stop",
    "start": "1978279",
    "end": "1983289"
  },
  {
    "text": "double writing we only write to the target index and the reason we bump up the generation number is it's a back-up",
    "start": "1983289",
    "end": "1991000"
  },
  {
    "text": "plan so if something goes bad and we figure it out three days later instead of having to Reem I great again",
    "start": "1991000",
    "end": "1997659"
  },
  {
    "text": "and when we do that people are having trouble because of the issue we discovered what we have to Reem I great",
    "start": "1997659",
    "end": "2002760"
  },
  {
    "text": "is stuff that comes after version 45 we know that anything 44 or lower is",
    "start": "2002760",
    "end": "2008750"
  },
  {
    "text": "already in the old index which makes which makes this simple we honestly never had to do this and I hope we'll",
    "start": "2008750",
    "end": "2014750"
  },
  {
    "text": "never get to do it but it's it's there just in case we need it so this is all interesting",
    "start": "2014750",
    "end": "2020890"
  },
  {
    "text": "how do we deal with failures what's what's what's involved in this process",
    "start": "2020890",
    "end": "2026800"
  },
  {
    "text": "so when you're running this at a large",
    "start": "2026800",
    "end": "2032750"
  },
  {
    "text": "scale things will likely fail so we'll see nodes go down for example",
    "start": "2032750",
    "end": "2038140"
  },
  {
    "text": "and things stalling you will see timeouts networking issues and when",
    "start": "2038140",
    "end": "2045080"
  },
  {
    "text": "you're migrating a large number of documents in our case we're talking about billions of documents if something",
    "start": "2045080",
    "end": "2050720"
  },
  {
    "text": "that happens after you migrate and 95 percent of your documents and that",
    "start": "2050720",
    "end": "2056450"
  },
  {
    "text": "already took four or five days you don't restart from scratch so to deal with",
    "start": "2056450",
    "end": "2061760"
  },
  {
    "text": "that what we do in our system is we partition our documents and by",
    "start": "2061760",
    "end": "2066830"
  },
  {
    "text": "partitioning I means we take the idea of the document and we hatch that into a",
    "start": "2066830",
    "end": "2071919"
  },
  {
    "text": "64k bucket and into 64 size bucket so each document is is is hashed into a",
    "start": "2071919",
    "end": "2077780"
  },
  {
    "text": "value between 0 and 64 K minus 1 and we",
    "start": "2077780",
    "end": "2082850"
  },
  {
    "text": "use that hash to group documents into buckets and instead of migrating everything we just migrate by ranges of",
    "start": "2082850",
    "end": "2089419"
  },
  {
    "text": "buckets and really keep those bucket sizes to about a million so we migrate",
    "start": "2089419",
    "end": "2094909"
  },
  {
    "text": "by chunks of million documents and this turned out to be extremely useful because if something goes bad worst",
    "start": "2094910",
    "end": "2101330"
  },
  {
    "text": "cases we have to ream I grate a million documents which is much better than ream a grating billion documents and also",
    "start": "2101330",
    "end": "2107420"
  },
  {
    "text": "what this does is it allows us to use elastic search most more efficiently",
    "start": "2107420",
    "end": "2113210"
  },
  {
    "text": "when we do the migration we have to create a scroll over the data and when",
    "start": "2113210",
    "end": "2118550"
  },
  {
    "text": "you do a scroll over the data what you're actually doing behind the scenes you're doing a consistent",
    "start": "2118550",
    "end": "2123970"
  },
  {
    "text": "scanning of the Lucene segments so what Lucene has to do it has to keep track of",
    "start": "2123970",
    "end": "2129860"
  },
  {
    "text": "all the leucine although the scene segments that you had when you started this scroll so that you get a consistent",
    "start": "2129860",
    "end": "2136760"
  },
  {
    "text": "scroll so when you have these mergers happening behind the scene your segments will not will not be deleted because",
    "start": "2136760",
    "end": "2144079"
  },
  {
    "text": "you're still holding to that file descriptor and the scroll that started at index zero didn't get to that segment",
    "start": "2144079",
    "end": "2150799"
  },
  {
    "text": "yet that segment has has document number one billion so what ends up happening you're gonna you're gonna consume",
    "start": "2150799",
    "end": "2157549"
  },
  {
    "text": "storage in our case this was mostly fine in most of the cases but in some cases",
    "start": "2157549",
    "end": "2162859"
  },
  {
    "text": "this this may be a problem if if you're doing a lot of merges for example if you're deleting all the time and you want to see your data go away this can",
    "start": "2162859",
    "end": "2169970"
  },
  {
    "text": "be a problem the other thing that I didn't mention is deletions deletion is is usually an",
    "start": "2169970",
    "end": "2176599"
  },
  {
    "text": "interesting problem in data systems the way we we deal with that is is just",
    "start": "2176599",
    "end": "2182750"
  },
  {
    "text": "replace the deletion with an update it's not it's not a deletion so anytime we delete the doc and we actually do this",
    "start": "2182750",
    "end": "2188509"
  },
  {
    "text": "only during the migration usual time we just delete stuff we don't care but when",
    "start": "2188509",
    "end": "2194150"
  },
  {
    "text": "we do migration what we have to do is instead of deleting the document we add what we call a deletion marker and the",
    "start": "2194150",
    "end": "2202009"
  },
  {
    "text": "deletion marker is replaces the deletion was an update and then once the migration is done we have",
    "start": "2202009",
    "end": "2209269"
  },
  {
    "text": "to go and clean up those documents that have the deletion marker and obviously on the query side we have to deal with those deletion document delete deletion",
    "start": "2209269",
    "end": "2216170"
  },
  {
    "text": "markers otherwise we're going to show documents that have been deleted so all our queries query clients will actually",
    "start": "2216170",
    "end": "2222440"
  },
  {
    "text": "automatically filter out these documents performance is another important aspect",
    "start": "2222440",
    "end": "2229089"
  },
  {
    "text": "for the migration this is a very heavy operation like there's tens of terabytes",
    "start": "2229089",
    "end": "2234740"
  },
  {
    "text": "of data that you're actually moving when you're doing this so the first thing we do is migrating by",
    "start": "2234740",
    "end": "2243799"
  },
  {
    "text": "partition ranges already said this sorry the second thing we do is because we don't overload the current",
    "start": "2243799",
    "end": "2250910"
  },
  {
    "text": "cluster at some point you get you're gonna do you're gonna write the two indices so",
    "start": "2250910",
    "end": "2256609"
  },
  {
    "text": "you're doubling your indexing load so what we do is we add temporary nodes and",
    "start": "2256609",
    "end": "2262359"
  },
  {
    "text": "when we create the new index we can instruct elasticsearch to place that new",
    "start": "2262359",
    "end": "2268039"
  },
  {
    "text": "index on only those nodes and that way we can control the load",
    "start": "2268039",
    "end": "2273270"
  },
  {
    "text": "the migration puts on on the target on the target cluster this may not work in",
    "start": "2273270",
    "end": "2278940"
  },
  {
    "text": "some cases if you have a large cluster you cannot just add more machines it does cost money",
    "start": "2278940",
    "end": "2284790"
  },
  {
    "text": "if the migration has to take a week to run that's that's not something you need to take into account for for costing",
    "start": "2284790",
    "end": "2291510"
  },
  {
    "text": "purposes so they're a couple of times we actually run this on the same cluster and usually",
    "start": "2291510",
    "end": "2298440"
  },
  {
    "text": "when you go up when we run it on on the same cluster a little bit more careful about how we do this so one important",
    "start": "2298440",
    "end": "2304500"
  },
  {
    "text": "thing that we do on the target index is to completely disable the refreshes if you're not querying in index there is no",
    "start": "2304500",
    "end": "2309720"
  },
  {
    "text": "reason to refresh at every second it's just a waste of CPU and i/o we should just completely disable it and when you",
    "start": "2309720",
    "end": "2316590"
  },
  {
    "text": "disable that this speed improvement is huge like you're gonna see you're indexing go from something like couple",
    "start": "2316590",
    "end": "2322950"
  },
  {
    "text": "thousand to 20,000 or 30,000 indexing operations per second so it's totally worth it it makes the first phase of the",
    "start": "2322950",
    "end": "2329369"
  },
  {
    "text": "migration extremely fast we also start with no replicas this is the trade off it depends how",
    "start": "2329369",
    "end": "2336990"
  },
  {
    "text": "much risk you want to take versus how much you want this operation to take if",
    "start": "2336990",
    "end": "2342030"
  },
  {
    "text": "you add replicas in our case we do quorum indexing so we require a quorum",
    "start": "2342030",
    "end": "2347760"
  },
  {
    "text": "to respond to synchronously write the data so when we when we have a replication it actually slows down the",
    "start": "2347760",
    "end": "2354180"
  },
  {
    "text": "indexing significantly so we just take the risk and say we don't expect anything to go down we just gonna do it",
    "start": "2354180",
    "end": "2361680"
  },
  {
    "text": "without replicas and then at some point we'll have to add replicas obviously before we switch the production traffic",
    "start": "2361680",
    "end": "2367530"
  },
  {
    "text": "to go to those machines this worked for us eight times out of nine we lost a node",
    "start": "2367530",
    "end": "2375869"
  },
  {
    "text": "one time and we basically had to restart from scratch but it was only one time",
    "start": "2375869",
    "end": "2381530"
  },
  {
    "text": "another thing that we noticed while doing the migration is that when you're doing scroll operation",
    "start": "2381530",
    "end": "2388670"
  },
  {
    "text": "elasticsearch by default returns the document in short order so it's let's",
    "start": "2388670",
    "end": "2394770"
  },
  {
    "text": "say you have shot 0 1 2 and you you want to fetch all documents much matching some filter there's no ranking going on",
    "start": "2394770",
    "end": "2401580"
  },
  {
    "text": "so elasticsearch the most efficient way to give you documents is to fetch them first from shards you and then from",
    "start": "2401580",
    "end": "2407880"
  },
  {
    "text": "short one and from Chotu etc there's no sorting nor ranking is very cheap and the problem with this approach is when",
    "start": "2407880",
    "end": "2415739"
  },
  {
    "text": "you're fetching those those documents and you have to reinvest them again what",
    "start": "2415739",
    "end": "2421140"
  },
  {
    "text": "what's gonna happen is you're gonna end up exactly on the same shot you kind of fetching from one chart and indexing",
    "start": "2421140",
    "end": "2426390"
  },
  {
    "text": "into another shot and it's exactly the same shot so when this was happening when I look at my dashboard",
    "start": "2426390",
    "end": "2432829"
  },
  {
    "text": "what I see is I see three hosts spiking and then that spike is gonna continue",
    "start": "2432829",
    "end": "2438119"
  },
  {
    "text": "and then it goes down and then a different three hosts go up and the reason for the number three is because",
    "start": "2438119",
    "end": "2443910"
  },
  {
    "text": "we have one primary and two replicas and they all have the same load so this",
    "start": "2443910",
    "end": "2448979"
  },
  {
    "text": "means that we are not utilizing the cluster completely because we're just focusing on three hosts of the time so",
    "start": "2448979",
    "end": "2454890"
  },
  {
    "text": "there's a very simple solution to this problem they just you randomize the results of what we do have a field on",
    "start": "2454890",
    "end": "2461369"
  },
  {
    "text": "our documents that allows us to it's it's it's an updated on",
    "start": "2461369",
    "end": "2467660"
  },
  {
    "text": "timestamp basically and we just sort the documents by that timestamp and this was",
    "start": "2467660",
    "end": "2474569"
  },
  {
    "text": "it I'll take any questions and you can sign up for a free trial for signal",
    "start": "2474569",
    "end": "2480329"
  },
  {
    "text": "effects and please check out our booth thank you",
    "start": "2480329",
    "end": "2485599"
  }
]