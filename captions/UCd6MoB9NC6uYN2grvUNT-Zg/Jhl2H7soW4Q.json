[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "thank you all for attending my name is suhas kulkarni i'm a vp of engineering and also lead",
    "start": "3840",
    "end": "10000"
  },
  {
    "text": "architect at gree international i'm responsible for",
    "start": "10000",
    "end": "15280"
  },
  {
    "text": "looking after the company technology strategy and future proofing our tax tech stack to",
    "start": "15280",
    "end": "21600"
  },
  {
    "text": "meet the demands of our growing business for the next four to five years",
    "start": "21600",
    "end": "26720"
  },
  {
    "text": "and with me co-presenting is kandarp shah who led the team in building the data",
    "start": "26720",
    "end": "33440"
  },
  {
    "text": "platform for gree and we have some really good information for all of you you can take",
    "start": "33440",
    "end": "41440"
  },
  {
    "text": "it back and solve your business problems and and the talk outline uh today is",
    "start": "41440",
    "end": "48399"
  },
  {
    "start": "45000",
    "end": "135000"
  },
  {
    "text": "mobile game analytics as a use case uh how we built our platform to address",
    "start": "48399",
    "end": "53760"
  },
  {
    "text": "this use case and some of the decisions we took uh mistakes we made and and also",
    "start": "53760",
    "end": "60879"
  },
  {
    "text": "challenges we faced along the way uh kind of give you a good uh feel for",
    "start": "60879",
    "end": "66000"
  },
  {
    "text": "what it meant for us to build the platform and and then we'll go deeper into you know",
    "start": "66000",
    "end": "72320"
  },
  {
    "text": "the new analytics architecture leveraging aws tech something that",
    "start": "72320",
    "end": "78720"
  },
  {
    "text": "helped us uh solve our business problems and at the end",
    "start": "78720",
    "end": "83920"
  },
  {
    "text": "you know the lessons we learned uh it's the best practice i would call something that you can take back uh",
    "start": "83920",
    "end": "91360"
  },
  {
    "text": "but before that quick paul how many of you are from gaming uh space",
    "start": "91360",
    "end": "97840"
  },
  {
    "text": "wow great how about the console and okay",
    "start": "97840",
    "end": "105280"
  },
  {
    "text": "free to play mobile gaming awesome how about b2c",
    "start": "105280",
    "end": "111840"
  },
  {
    "text": "space consumer facing business and b2b okay this looks like good",
    "start": "111840",
    "end": "119360"
  },
  {
    "text": "distribution so i think uh today's discussion will definitely uh you know kind of applies to all of",
    "start": "119360",
    "end": "126320"
  },
  {
    "text": "you and especially mobile gaming where we are going to discuss more deeply and",
    "start": "126320",
    "end": "136080"
  },
  {
    "start": "135000",
    "end": "187000"
  },
  {
    "text": "so before that who we are we are a global gaming company with our with 1800 employees",
    "start": "136160",
    "end": "143920"
  },
  {
    "text": "worldwide we have our studio in tokyo san francisco and vancouver and our games are played",
    "start": "143920",
    "end": "152560"
  },
  {
    "text": "in six different continents and 13 of the games are produced in north america and knights and dragons",
    "start": "152560",
    "end": "160879"
  },
  {
    "text": "for example was the top 10 grossing in 41 countries",
    "start": "160879",
    "end": "166000"
  },
  {
    "text": "and also crime city which is in the rpg genre was stopped and grossing in 140",
    "start": "166000",
    "end": "171680"
  },
  {
    "text": "countries we focus on three different genres casino rpg and strategy",
    "start": "171680",
    "end": "179040"
  },
  {
    "text": "there will be more to be added to the portfolio so that's who we are um",
    "start": "179040",
    "end": "185920"
  },
  {
    "text": "i will jump into uh success factors in gaming uh you know what it means to launch",
    "start": "185920",
    "end": "192640"
  },
  {
    "start": "187000",
    "end": "321000"
  },
  {
    "text": "successful games uh some of the key ingredients are you have to have great uh you know uh storyline great",
    "start": "192640",
    "end": "200239"
  },
  {
    "text": "gameplay and and then you have to have great mechanics and and with that you also need good content",
    "start": "200239",
    "end": "206879"
  },
  {
    "text": "uh so we are all about entertaining uh you know users uh so to to bring all that you have to have",
    "start": "206879",
    "end": "214640"
  },
  {
    "text": "great content gameplay and mechanics so with that combination of that you you can effectively engage your audience",
    "start": "214640",
    "end": "222480"
  },
  {
    "text": "and that will mean your retention of those users will be pretty you know",
    "start": "222480",
    "end": "228560"
  },
  {
    "text": "very good and there's a strong correlation between retention and your monetization so along the way",
    "start": "228560",
    "end": "235439"
  },
  {
    "text": "you hope to generate more of those micro transactions within the game that will you know",
    "start": "235439",
    "end": "242799"
  },
  {
    "text": "improve your game performance so so the key thing is you have to continuously keep adding these",
    "start": "244319",
    "end": "249599"
  },
  {
    "text": "the content you should never be in a situation where players run out of content and that's",
    "start": "249599",
    "end": "255519"
  },
  {
    "text": "the key thing so with all that you would you know game producers they launched the game it goes live",
    "start": "255519",
    "end": "262320"
  },
  {
    "text": "there's a marketing span behind it it does fine for the first few months and you know the games",
    "start": "262320",
    "end": "268960"
  },
  {
    "text": "metrics are no longer you know you know they're not the same as at the time of launch uh why is that and",
    "start": "268960",
    "end": "275199"
  },
  {
    "text": "uh there are a lot of different things that come into play so you know it's a great game but i know",
    "start": "275199",
    "end": "282320"
  },
  {
    "text": "it was working fine but white is not working also similar scenario is when a gaming",
    "start": "282320",
    "end": "288000"
  },
  {
    "text": "company acquires another one and it which worked well after the acquisition you know for some reason it",
    "start": "288000",
    "end": "293600"
  },
  {
    "text": "does not work again you go back to those questions why why not uh why why kpis are not looking that",
    "start": "293600",
    "end": "300639"
  },
  {
    "text": "good and why is my game not successful anymore",
    "start": "300639",
    "end": "305759"
  },
  {
    "text": "so as a game producer you'd you'd have to ask these questions you know oops sorry",
    "start": "305759",
    "end": "313039"
  },
  {
    "start": "321000",
    "end": "370000"
  },
  {
    "text": "okay why is game not performing well why players are no longer spending and",
    "start": "321680",
    "end": "328560"
  },
  {
    "text": "why is the retention metric uh is is so poor and how can you change that around um",
    "start": "328560",
    "end": "334639"
  },
  {
    "text": "you know how do you improve your arc dow average revenue per dau and how how can i increase the",
    "start": "334639",
    "end": "341360"
  },
  {
    "text": "percentage spenders within your game uh again that that that's again uh depends is dependent on your user",
    "start": "341360",
    "end": "348800"
  },
  {
    "text": "retention uh so to answer all of that you have you need game analytics and insights that's",
    "start": "348800",
    "end": "355600"
  },
  {
    "text": "really a one of the basis although there are success factors for gaming uh but in addition to that you have to",
    "start": "355600",
    "end": "362639"
  },
  {
    "text": "have game analytics and insight as a complement to to for the long-term success of your games",
    "start": "362639",
    "end": "369520"
  },
  {
    "text": "so we set out to build analytics platform in-house and",
    "start": "369520",
    "end": "375759"
  },
  {
    "start": "370000",
    "end": "600000"
  },
  {
    "text": "at gri we are we have a very data-driven culture all of our game designers producers",
    "start": "375759",
    "end": "383360"
  },
  {
    "text": "are completely are data driven for making some of the very important business",
    "start": "383360",
    "end": "388880"
  },
  {
    "text": "decisions to optimize the game design also how effectively are we spending our",
    "start": "388880",
    "end": "395440"
  },
  {
    "text": "marketing uh money and and it's all about you know if you look about think about",
    "start": "395440",
    "end": "401199"
  },
  {
    "text": "uh secret of gaming no longer a secret but uh it's all about",
    "start": "401199",
    "end": "406880"
  },
  {
    "text": "acquiring high quality users and then retaining them for a longer period of time and",
    "start": "406880",
    "end": "411919"
  },
  {
    "text": "then monetize those users and at the same time you know entertain them uh that's that's really what it it takes",
    "start": "411919",
    "end": "418639"
  },
  {
    "text": "to produce uh great games and again if you look at uh",
    "start": "418639",
    "end": "425440"
  },
  {
    "text": "to optimize game you know game design to optimize your marketing spend you have to collect",
    "start": "425440",
    "end": "431120"
  },
  {
    "text": "all that key information and on the client side you know we we",
    "start": "431120",
    "end": "436319"
  },
  {
    "text": "capture a lot of game events things like player login uh tutorial completion uh your battle",
    "start": "436319",
    "end": "442400"
  },
  {
    "text": "fight that's going on within your game and uh if it's a casino or slots game",
    "start": "442400",
    "end": "447759"
  },
  {
    "text": "what all the ui all the spin events and things like that so a lot of events are captured",
    "start": "447759",
    "end": "455039"
  },
  {
    "text": "also from a player experience point of view you have to know is the game crashing what's your percentage crash",
    "start": "455039",
    "end": "461120"
  },
  {
    "text": "you know what's the stability of your game so that's on the performance of the game and with respect to player experience",
    "start": "461120",
    "end": "468000"
  },
  {
    "text": "and things like how many users are playing uh during the day how many sessions and",
    "start": "468000",
    "end": "473039"
  },
  {
    "text": "things like that so all of that information is captured from from your clients mobile clients and then on the",
    "start": "473039",
    "end": "481360"
  },
  {
    "text": "user acquisition side it's all about knowing building a campaign performance where",
    "start": "481360",
    "end": "488560"
  },
  {
    "text": "you have to know the users that you acquired uh these are the you know the money you",
    "start": "488560",
    "end": "494400"
  },
  {
    "text": "spend to acquire users how are they doing within the game what's their player behavior are they spending more than",
    "start": "494400",
    "end": "500400"
  },
  {
    "text": "uh what we paid for so it's all about ltv lifetime value of the game uh versus the amount of money you spend",
    "start": "500400",
    "end": "507120"
  },
  {
    "text": "to acquire that user so that's where campaign performance is super important",
    "start": "507120",
    "end": "512880"
  },
  {
    "text": "for optimizing your marketing spend and also we collect ad clicks downloads and a ton of other",
    "start": "512880",
    "end": "519760"
  },
  {
    "text": "information impressions from each of the ad network we rely on to acquire these users",
    "start": "519760",
    "end": "526880"
  },
  {
    "text": "so again you want to spend on you know ad network that that is",
    "start": "526880",
    "end": "532160"
  },
  {
    "text": "bringing high quality users quality of users doesn't have to be in terms of revenue but",
    "start": "532160",
    "end": "538320"
  },
  {
    "text": "it also means how active are the users within your game so quality can be different from for",
    "start": "538320",
    "end": "543839"
  },
  {
    "text": "different people within the organization so uh attribution you also want to credit though you know",
    "start": "543839",
    "end": "551360"
  },
  {
    "text": "the ad networks that actually brought those users there should not be any duplication uh between ad networks claiming that you",
    "start": "551360",
    "end": "557760"
  },
  {
    "text": "know these users came from me versus you know the other ad network on the engagement",
    "start": "557760",
    "end": "563200"
  },
  {
    "text": "side it's all about player targeting how can you convert non-spenders into spenders you know how",
    "start": "563200",
    "end": "569200"
  },
  {
    "text": "can you recognize the inactive players what kind of action can you take",
    "start": "569200",
    "end": "575200"
  },
  {
    "text": "again that's that's all uh you know you need a segmentation tool for example uh to",
    "start": "575200",
    "end": "581360"
  },
  {
    "text": "identify and target those key users and uh we also collect uh you know",
    "start": "581360",
    "end": "586640"
  },
  {
    "text": "events like iap in-app purchases soft currency balance hard currency balance and",
    "start": "586640",
    "end": "592480"
  },
  {
    "text": "and so on and so forth so as you can see a lot of information is captured",
    "start": "592480",
    "end": "600000"
  },
  {
    "text": "from a data collection if i were to summarize so you have events coming from your mobile devices",
    "start": "600000",
    "end": "607040"
  },
  {
    "text": "android ios and game servers there are there's a distinction we",
    "start": "607040",
    "end": "614000"
  },
  {
    "text": "collect high value events on the game server side low value",
    "start": "614000",
    "end": "619519"
  },
  {
    "text": "high volume kind of events originate from the from the mobile devices from the client",
    "start": "619519",
    "end": "624800"
  },
  {
    "text": "side so there's a reason for that you know all the iap",
    "start": "624800",
    "end": "630240"
  },
  {
    "text": "you know your purchase events uh player login events these are these drive key performance indicators",
    "start": "630240",
    "end": "636720"
  },
  {
    "text": "and based on which you make your business you know decisions uh so that those have to be",
    "start": "636720",
    "end": "642320"
  },
  {
    "text": "accurate and reliable um there's a good chance you may lose events on the client side and although",
    "start": "642320",
    "end": "649200"
  },
  {
    "text": "we have built sdks to ensure you know offline events get ultimately",
    "start": "649200",
    "end": "654560"
  },
  {
    "text": "eventually you know sent over to our backend systems and ad networks that's that's the other",
    "start": "654560",
    "end": "659920"
  },
  {
    "text": "area where we collect a lot of you know information so you can see there's a lot of things a lot of",
    "start": "659920",
    "end": "665760"
  },
  {
    "text": "information gathering happening within the game or in the game ecosystem and",
    "start": "665760",
    "end": "670959"
  },
  {
    "text": "quickly looking at the data size and growth uh each event that comes to the analytics",
    "start": "670959",
    "end": "677360"
  },
  {
    "text": "platform is about one kilobyte and we collect about 500 million events per day",
    "start": "677360",
    "end": "683360"
  },
  {
    "text": "it comes to around 500 gig on certain uh weekends certain events live ops",
    "start": "683360",
    "end": "689760"
  },
  {
    "text": "events uh this can actually spike up so this could go up to one terabyte for it for",
    "start": "689760",
    "end": "695360"
  },
  {
    "text": "that given day although it is not a big data scale by any means but",
    "start": "695360",
    "end": "700399"
  },
  {
    "text": "over the life time of the game uh we you are really working with the petabyte scale of data",
    "start": "700399",
    "end": "706640"
  },
  {
    "text": "uh in your system and again if you multiply that with number of game titles you are producing",
    "start": "706640",
    "end": "712240"
  },
  {
    "text": "its order of magnitude size and we it's a json format",
    "start": "712240",
    "end": "720399"
  },
  {
    "start": "720000",
    "end": "810000"
  },
  {
    "text": "looking at the database schema how we organize how we store data every game that we produce",
    "start": "721040",
    "end": "727519"
  },
  {
    "text": "has its own database schema and that's where you know your game",
    "start": "727519",
    "end": "734959"
  },
  {
    "text": "designers and uh and and the bi analysts or the data scientists",
    "start": "734959",
    "end": "740560"
  },
  {
    "text": "scientists within the organization decide you know what kind of events you want to capture you know whether it is about player",
    "start": "740560",
    "end": "747360"
  },
  {
    "text": "behavior or whether it's about the game performance about how can we optimize game design and things like that so",
    "start": "747360",
    "end": "753279"
  },
  {
    "text": "uh based on that every game event within a game is a table for us within",
    "start": "753279",
    "end": "759279"
  },
  {
    "text": "and and then highly denormalized structure uh very schema aware",
    "start": "759279",
    "end": "765040"
  },
  {
    "text": "so there are about 40 to 50 uh tables or events that are generated for each of the game and roughly if you",
    "start": "765040",
    "end": "772959"
  },
  {
    "text": "have about 20 different titles you know it comes down to 1000 games 1000 uh tables within your data",
    "start": "772959",
    "end": "778959"
  },
  {
    "text": "warehouse so as we add more game titles that's only going to increase so that's the",
    "start": "778959",
    "end": "785680"
  },
  {
    "text": "kind of a database schema uh so as you can see you know we are collecting information",
    "start": "785680",
    "end": "792320"
  },
  {
    "text": "you know as as number of games uh are produced or when when your games",
    "start": "792320",
    "end": "798160"
  },
  {
    "text": "become big hit we need a system that can collect all of this information",
    "start": "798160",
    "end": "803360"
  },
  {
    "text": "process it and store it in your backend database so that's really the primary requirement for us and so",
    "start": "803360",
    "end": "811839"
  },
  {
    "start": "810000",
    "end": "859000"
  },
  {
    "text": "we need a data collection and streaming to database and that's the capability we need",
    "start": "811839",
    "end": "816880"
  },
  {
    "text": "it also should be you know there should be zero data loss or zero data corruption these two thing",
    "start": "816880",
    "end": "823279"
  },
  {
    "text": "you know requirements are super important because you want your users to trust your data",
    "start": "823279",
    "end": "828320"
  },
  {
    "text": "if there is a data loss your metrics are skewed and there's no way you can trust that data or",
    "start": "828320",
    "end": "834240"
  },
  {
    "text": "or make a you know your business decisions so uh it's credibility is everything in",
    "start": "834240",
    "end": "839920"
  },
  {
    "text": "whenever it comes to your data platforms and that's the number one",
    "start": "839920",
    "end": "845120"
  },
  {
    "text": "goal for for our uh in a platform and of course guaranteed data delivery",
    "start": "845120",
    "end": "850399"
  },
  {
    "text": "is is is also important data delay is fine as long as you know it makes it through uh",
    "start": "850399",
    "end": "856320"
  },
  {
    "text": "so that's those are the key requirements uh uh i want to talk about uh you know",
    "start": "856320",
    "end": "863440"
  },
  {
    "text": "briefly about some of the architectures or platforms that we built initially",
    "start": "863440",
    "end": "868639"
  },
  {
    "text": "and then then look at the new one later on so initially we started off with our",
    "start": "868639",
    "end": "875279"
  },
  {
    "start": "873000",
    "end": "937000"
  },
  {
    "text": "generation one analytics platform usually if you look at any gaming company if you if",
    "start": "875279",
    "end": "880639"
  },
  {
    "text": "if you produce the first game it's uh it's likely that analytics is a is is a",
    "start": "880639",
    "end": "887199"
  },
  {
    "text": "is an afterthought and so we we were also in the same boat we had one engineer working on um you",
    "start": "887199",
    "end": "894720"
  },
  {
    "text": "know built a lamp stack and it's a it was a sharded uh mysql",
    "start": "894720",
    "end": "900079"
  },
  {
    "text": "database and and all the game logic and as well as analytics logging and",
    "start": "900079",
    "end": "905680"
  },
  {
    "text": "loading of data was happening uh in in the same logic so although we decoupled those two",
    "start": "905680",
    "end": "912000"
  },
  {
    "text": "processes but uh what we quickly found out was as",
    "start": "912000",
    "end": "917040"
  },
  {
    "text": "our game start became popular more events started coming in we were just unable to keep up with the",
    "start": "917040",
    "end": "924160"
  },
  {
    "text": "incoming data data rate so it was not scalable so we quickly realized that within few weeks",
    "start": "924160",
    "end": "930959"
  },
  {
    "text": "of uh you know putting this into production uh so we decided to go next generation",
    "start": "930959",
    "end": "938000"
  },
  {
    "start": "937000",
    "end": "1066000"
  },
  {
    "text": "we relied on flume and also had an mpp data data warehouse at",
    "start": "938000",
    "end": "944079"
  },
  {
    "text": "the time it was the right call but during those times plume was a project",
    "start": "944079",
    "end": "950079"
  },
  {
    "text": "in infancy it was very immature and we also did not have deep understanding",
    "start": "950079",
    "end": "955519"
  },
  {
    "text": "of flume and with limited engineering resources",
    "start": "955519",
    "end": "961040"
  },
  {
    "text": "we we thought we knew everything about flume there were three problems we ran into on the flume",
    "start": "961040",
    "end": "967600"
  },
  {
    "text": "side one was we enabled this feature of flume called reliable delivery mechanism",
    "start": "967600",
    "end": "975519"
  },
  {
    "text": "which guarantees you know data delivery to your data warehouse so what ended up happening was due to",
    "start": "975519",
    "end": "981839"
  },
  {
    "text": "network latency and also the timeout uh in a tuning of that it's it's based",
    "start": "981839",
    "end": "987600"
  },
  {
    "text": "on ac system where agents would look for the acknowledgement that data",
    "start": "987600",
    "end": "993759"
  },
  {
    "text": "has been delivered if it does not see it depending on your network latency",
    "start": "993759",
    "end": "999440"
  },
  {
    "text": "it will try to resend that to the collectors and so what that meant was you know we were",
    "start": "999440",
    "end": "1005680"
  },
  {
    "text": "duplicating data in our data warehouse so we were constantly in the business of",
    "start": "1005680",
    "end": "1010720"
  },
  {
    "text": "deduplicating uh information data in our data warehouse and so that was one and we also did not have good",
    "start": "1010720",
    "end": "1018880"
  },
  {
    "text": "monitoring process control and alerts set up thinking back you know i wish we had",
    "start": "1018880",
    "end": "1024880"
  },
  {
    "text": "those but uh some of these agents would go down without our knowledge and the disc would fill up affecting our",
    "start": "1024880",
    "end": "1031438"
  },
  {
    "text": "overall game performance as well so uh that's the second one and the plugins that we",
    "start": "1031439",
    "end": "1037280"
  },
  {
    "text": "developed on the collector side to load data it was uh not optimized enough that",
    "start": "1037280",
    "end": "1044798"
  },
  {
    "text": "again it was not keeping up with the inflow of data so the overall etl speed was slow so uh",
    "start": "1044799",
    "end": "1052799"
  },
  {
    "text": "so again um just because we did not have deep understanding of flume",
    "start": "1052799",
    "end": "1058080"
  },
  {
    "text": "and not did not have enough resources to invest into maintaining the system uh you know",
    "start": "1058080",
    "end": "1064320"
  },
  {
    "text": "we decided we need a system that we can manage ourselves something that is in our under our control so what we then did",
    "start": "1064320",
    "end": "1072160"
  },
  {
    "start": "1066000",
    "end": "1171000"
  },
  {
    "text": "was built a relay engine which replaced the flume uh very simple in its design and it was",
    "start": "1072160",
    "end": "1079280"
  },
  {
    "text": "scalable too uh all it is is you have these senders which would collect all of your analytics",
    "start": "1079280",
    "end": "1084880"
  },
  {
    "text": "logs send it to you know uh listeners and you can scale listeners",
    "start": "1084880",
    "end": "1090320"
  },
  {
    "text": "based on a number of you know events and these listeners would log them again locally and then another",
    "start": "1090320",
    "end": "1097360"
  },
  {
    "text": "process would pick up those log files and send it to the next stage which is replicator again replicator would",
    "start": "1097360",
    "end": "1104240"
  },
  {
    "text": "replicate your log files based on these things and number of consumers you had",
    "start": "1104240",
    "end": "1109280"
  },
  {
    "text": "and and then you had copiers which would collect that and load them into database and and we also had a consumer that",
    "start": "1109280",
    "end": "1117120"
  },
  {
    "text": "would go to s3 so all the raw logs would go there there were times when you know due to",
    "start": "1117120",
    "end": "1123520"
  },
  {
    "text": "data issues we had to constantly go back to raw logs reprocess them load them again",
    "start": "1123520",
    "end": "1129120"
  },
  {
    "text": "and there are some other use cases that came out was to bi team decided hey we need our",
    "start": "1129120",
    "end": "1136160"
  },
  {
    "text": "own dedicated cluster so which meant we had to have one more uh data warehouse in a sink",
    "start": "1136160",
    "end": "1142559"
  },
  {
    "text": "and and that was one then there was another use case where we need to monitor certain kpis real",
    "start": "1142559",
    "end": "1148400"
  },
  {
    "text": "time and so we had like four or five consumers and the replicator was doing mindless i o a",
    "start": "1148400",
    "end": "1155679"
  },
  {
    "text": "lot of replication and there is no compression we didn't have enough time to really invest into this tech and",
    "start": "1155679",
    "end": "1162640"
  },
  {
    "text": "although it was scaling well but maintenance wise it was uh you know quite high",
    "start": "1162640",
    "end": "1168240"
  },
  {
    "text": "so we abandoned that and so challenges we faced uh hard to",
    "start": "1168240",
    "end": "1174799"
  },
  {
    "start": "1171000",
    "end": "1213000"
  },
  {
    "text": "maintain and scale uh spike in live ops events uh would slow down some of the uh high",
    "start": "1174799",
    "end": "1182400"
  },
  {
    "text": "value events that we were processing and it's it's adding a new sync was difficult",
    "start": "1182400",
    "end": "1188640"
  },
  {
    "text": "and anytime we do writes to data warehouse it affected the query performance for bi users",
    "start": "1188640",
    "end": "1195200"
  },
  {
    "text": "so while you're loading data someone is querying and if you think about if someone is running a long running transaction or",
    "start": "1195200",
    "end": "1202080"
  },
  {
    "text": "query it will bring down all of your processing so again there is a data latency introduced in in",
    "start": "1202080",
    "end": "1208799"
  },
  {
    "text": "your system and so those are some of the challenges we we faced",
    "start": "1208799",
    "end": "1213840"
  },
  {
    "start": "1213000",
    "end": "1330000"
  },
  {
    "text": "so again going back to original requirements and going through three different",
    "start": "1213840",
    "end": "1219120"
  },
  {
    "text": "architectures of you know different platforms you know we added few more requirements",
    "start": "1219120",
    "end": "1224400"
  },
  {
    "text": "based on what we learned we need uh you know near real-time data",
    "start": "1224400",
    "end": "1229840"
  },
  {
    "text": "latency and ability to do real-time ad hoc analysis and ease of adding consumers and the",
    "start": "1229840",
    "end": "1237520"
  },
  {
    "text": "last one is very important which is managed service we did not want to manage these",
    "start": "1237520",
    "end": "1244000"
  },
  {
    "text": "platforms ourselves and we are in the business of making games",
    "start": "1244000",
    "end": "1249200"
  },
  {
    "text": "uh and the time it takes or the resources you have to invest in in",
    "start": "1249200",
    "end": "1254480"
  },
  {
    "text": "making your analytics platform highly resilient and reliable",
    "start": "1254480",
    "end": "1259600"
  },
  {
    "text": "uh it takes a lot of energy out of our business so uh that was uh you know a clear goal for",
    "start": "1259600",
    "end": "1266720"
  },
  {
    "text": "us you know something that we you know at the time we saw aws came up with the",
    "start": "1266720",
    "end": "1271760"
  },
  {
    "text": "great you know technology components we can directly put into our our stack and then make it",
    "start": "1271760",
    "end": "1278640"
  },
  {
    "text": "highly resilient so uh so then we decided to",
    "start": "1278640",
    "end": "1283919"
  },
  {
    "text": "take on you know i mean adopt all of the aws stack and make it you know completely managed",
    "start": "1283919",
    "end": "1290559"
  },
  {
    "text": "service uh so that was something that that i wish we had done in the past but but in the end you know",
    "start": "1290559",
    "end": "1297440"
  },
  {
    "text": "we knew we were on the right track so uh with that i would like to introduce uh kandar shah who led the team in",
    "start": "1297440",
    "end": "1304559"
  },
  {
    "text": "adopting this technology and making it production ready and he actually did the team entire team did that in in",
    "start": "1304559",
    "end": "1311280"
  },
  {
    "text": "record time you know migrating to redshift as well as you know",
    "start": "1311280",
    "end": "1316320"
  },
  {
    "text": "adopting kinesis so",
    "start": "1316320",
    "end": "1319840"
  },
  {
    "text": "thanks to us so uh so when we started starting out",
    "start": "1322000",
    "end": "1328000"
  },
  {
    "text": "with the new analytics architecture we set up like pretty clear requirement that what what we want to achieve from",
    "start": "1328000",
    "end": "1334080"
  },
  {
    "start": "1330000",
    "end": "1339000"
  },
  {
    "text": "this architecture so keeping those requirements in mind uh we wanted to like start building out",
    "start": "1334080",
    "end": "1340320"
  },
  {
    "start": "1339000",
    "end": "1345000"
  },
  {
    "text": "simple so uh we started with ingesting a data",
    "start": "1340320",
    "end": "1345440"
  },
  {
    "start": "1345000",
    "end": "1384000"
  },
  {
    "text": "storing that data and then visualize it analyzing that data",
    "start": "1345440",
    "end": "1350559"
  },
  {
    "text": "so now for ingestion like uh you can either use like some of the open source technologies like kafka flume or scribe",
    "start": "1350559",
    "end": "1357280"
  },
  {
    "text": "or you can create your own uh rest server but in that case like you have to manage",
    "start": "1357280",
    "end": "1362640"
  },
  {
    "text": "your own cluster and then deal with some of the scalability issues so we decided to go with the kinesis for",
    "start": "1362640",
    "end": "1369520"
  },
  {
    "text": "injection of data for storing uh we are using amazon s3 for permanent archival of the storage",
    "start": "1369520",
    "end": "1375919"
  },
  {
    "text": "and uh we are using redshift so uh we can like run any error analysis",
    "start": "1375919",
    "end": "1381039"
  },
  {
    "text": "on top of our data so with that said uh i'm going to like",
    "start": "1381039",
    "end": "1387520"
  },
  {
    "start": "1384000",
    "end": "1399000"
  },
  {
    "text": "walk through each uh components of the this simple store data pipeline uh we have like producer sender consumer",
    "start": "1387520",
    "end": "1395039"
  },
  {
    "text": "and the loader uh and if you look at the producer uh we have like all of all of the game",
    "start": "1395039",
    "end": "1401760"
  },
  {
    "start": "1399000",
    "end": "1433000"
  },
  {
    "text": "services developed using either php or python and uh whenever these game services get called",
    "start": "1401760",
    "end": "1407919"
  },
  {
    "text": "uh we generate few metrics uh uh it can be like player login event or it can be",
    "start": "1407919",
    "end": "1413039"
  },
  {
    "text": "a level up event uh apart from that we also generate data on the client side and those data get forwarded",
    "start": "1413039",
    "end": "1420960"
  },
  {
    "text": "uh to analytic servers using sdk in terms of the data uh we decided to",
    "start": "1420960",
    "end": "1428799"
  },
  {
    "text": "use like schema over format uh from the beginning so uh we use a json format",
    "start": "1428799",
    "end": "1434640"
  },
  {
    "start": "1433000",
    "end": "1502000"
  },
  {
    "text": "and that helped us like uh running using different tools for processing our",
    "start": "1434640",
    "end": "1439760"
  },
  {
    "text": "data some of the so each event like",
    "start": "1439760",
    "end": "1444960"
  },
  {
    "text": "apart from each each record basically has like even specific data and",
    "start": "1444960",
    "end": "1450480"
  },
  {
    "text": "in addition to that it also has like some uh metadata that we use for each event uh it has like game name",
    "start": "1450480",
    "end": "1458320"
  },
  {
    "text": "uh what is the event called uh then it has some unique id and uh uh time when that event occurred",
    "start": "1458320",
    "end": "1466320"
  },
  {
    "text": "so uh so we use those metadata for like two far to to like",
    "start": "1466320",
    "end": "1473360"
  },
  {
    "text": "uh process that event further into the pipeline and uh this is another example of the",
    "start": "1473360",
    "end": "1479520"
  },
  {
    "text": "event we have uh it's a battle fight event uh and all of those events goes to a local file",
    "start": "1479520",
    "end": "1487279"
  },
  {
    "text": "into the same server and that analytics files get rotated every 30 seconds",
    "start": "1487279",
    "end": "1493039"
  },
  {
    "text": "so we decouple the process of sending data from producing data and",
    "start": "1493039",
    "end": "1499360"
  },
  {
    "text": "that helped us a lot along the way so uh kinesis sender so",
    "start": "1499360",
    "end": "1507360"
  },
  {
    "start": "1502000",
    "end": "1531000"
  },
  {
    "text": "what sender does like it's uh it's a sitting on each web server and each web server is running this process",
    "start": "1507360",
    "end": "1514000"
  },
  {
    "text": "in a background and uh it performs several tasks it has uh it connects to kinesis and uh",
    "start": "1514000",
    "end": "1522000"
  },
  {
    "text": "get the list of this list of the shots we have uh then it's constantly listening to the",
    "start": "1522000",
    "end": "1527440"
  },
  {
    "text": "analytics files that we are creating on the local machine uh those files uh are given to a",
    "start": "1527440",
    "end": "1533840"
  },
  {
    "start": "1531000",
    "end": "1583000"
  },
  {
    "text": "specific worker within the sender uh so let's say for example this particular worker is given like two analytics files to process with is",
    "start": "1533840",
    "end": "1541440"
  },
  {
    "text": "going to read a file line by line and going to create a buffer of those data so kinesis has a put record",
    "start": "1541440",
    "end": "1549039"
  },
  {
    "text": "limit of 50 kilobytes so we keep building the buffer until we reach that limit",
    "start": "1549039",
    "end": "1554559"
  },
  {
    "text": "and that 50 kb is like compressed uh compressed data limit so we are",
    "start": "1554559",
    "end": "1559760"
  },
  {
    "text": "compressing data as well so once we reach that limit uh we compress that data and then uh send that",
    "start": "1559760",
    "end": "1567440"
  },
  {
    "text": "data to kinesis using put record call so this is how this is simplified version of like what sender looks like",
    "start": "1567440",
    "end": "1574880"
  },
  {
    "text": "and uh this data goes to like each shard uh in a round robin fashion uh for this",
    "start": "1574880",
    "end": "1581279"
  },
  {
    "text": "particular stream so some of the design choices we uh made",
    "start": "1581279",
    "end": "1586400"
  },
  {
    "start": "1583000",
    "end": "1689000"
  },
  {
    "text": "while building this sender uh should we use like single stream or we should use like a game",
    "start": "1586400",
    "end": "1591600"
  },
  {
    "text": "stream per game uh managing a single stream would be like much easier than managing",
    "start": "1591600",
    "end": "1597279"
  },
  {
    "text": "a stream per game we had like 20 plus games and if we have to manage 20 plus game it might have like taken much more resources",
    "start": "1597279",
    "end": "1604159"
  },
  {
    "text": "so we choose to go out with a single stream and uh processing from a single stream itself was like",
    "start": "1604159",
    "end": "1609760"
  },
  {
    "text": "fast enough in our case so so that worked out very well for",
    "start": "1609760",
    "end": "1614880"
  },
  {
    "text": "uh should i batch event or single should i just send even as i'm generating so in our cases in our",
    "start": "1614880",
    "end": "1621039"
  },
  {
    "text": "case like uh our record size is like hardly one kilobyte so uh we decided to",
    "start": "1621039",
    "end": "1628559"
  },
  {
    "text": "batch those events so we can like assign more data in the same in a single call",
    "start": "1628559",
    "end": "1634640"
  },
  {
    "text": "we are also compressing the data so uh that allowed us to like send four to five times data compared to",
    "start": "1634640",
    "end": "1641679"
  },
  {
    "text": "like if we haven't compressed that data and uh there are cases when our",
    "start": "1641679",
    "end": "1647200"
  },
  {
    "text": "event size is more than 50 50 kilobyte so in that case like if we compress data we can still use the",
    "start": "1647200",
    "end": "1653600"
  },
  {
    "text": "same pipeline to send those kind of event like one of the example is like crash log event which is usually like between 24 kilobyte to 200",
    "start": "1653600",
    "end": "1661360"
  },
  {
    "text": "kilobyte so compression allowed us to like send the same event through the kinesis pipeline partition key versus",
    "start": "1661360",
    "end": "1669200"
  },
  {
    "text": "explicit has key hkey so we are using explicit hash key",
    "start": "1669200",
    "end": "1674640"
  },
  {
    "text": "so we can like distribute load evenly across each chart uh if you have like uh",
    "start": "1674640",
    "end": "1679760"
  },
  {
    "text": "if you are running an application exclusively for doing aggregations a partition key would",
    "start": "1679760",
    "end": "1687120"
  },
  {
    "text": "be a better option so as we are storing all of our",
    "start": "1687120",
    "end": "1694080"
  },
  {
    "start": "1689000",
    "end": "1717000"
  },
  {
    "text": "analytics data into our local files uh it is very important that we get the deployment",
    "start": "1694080",
    "end": "1699520"
  },
  {
    "text": "process right otherwise like if the server comes up and there is no sender running on that particular machine",
    "start": "1699520",
    "end": "1704960"
  },
  {
    "text": "then you'll start filling up that disk and at the same time like if the server is going away",
    "start": "1704960",
    "end": "1710399"
  },
  {
    "text": "and there is some data still left on the server you will be you will be losing those",
    "start": "1710399",
    "end": "1716320"
  },
  {
    "text": "data so uh so we use ami for each of our game",
    "start": "1716320",
    "end": "1722399"
  },
  {
    "start": "1717000",
    "end": "1731000"
  },
  {
    "text": "uh game group so each game has like its own name i defined uh and we constantly update this ami",
    "start": "1722399",
    "end": "1728640"
  },
  {
    "text": "amazon machine image and uh each each uh group is also like auto skills so",
    "start": "1728640",
    "end": "1735440"
  },
  {
    "start": "1731000",
    "end": "1742000"
  },
  {
    "text": "whenever like instance comes up we we create instance using ami",
    "start": "1735440",
    "end": "1740720"
  },
  {
    "text": "and so this ami has a sender already built in whenever",
    "start": "1740720",
    "end": "1748080"
  },
  {
    "start": "1742000",
    "end": "1885000"
  },
  {
    "text": "like whenever we need to update a sender we deploy out to s3 object and from s3 like it goes to ami",
    "start": "1748080",
    "end": "1756640"
  },
  {
    "text": "and so let's say in this case like when we are adding a new instance in that particular",
    "start": "1756640",
    "end": "1762000"
  },
  {
    "text": "group we put that particular server into a wait state so we can like do some we can perform some user actions",
    "start": "1762000",
    "end": "1769360"
  },
  {
    "text": "as ami can be out of date and there might be some deployments that you did after",
    "start": "1769360",
    "end": "1774480"
  },
  {
    "text": "that uh after building that ami so we want to make sure that we update all the application",
    "start": "1774480",
    "end": "1780240"
  },
  {
    "text": "uh before uh we can put that into a production so so we do a puppet run uh before we",
    "start": "1780240",
    "end": "1786799"
  },
  {
    "text": "put that uh server into production and that pattern uh take care of like updating",
    "start": "1786799",
    "end": "1792720"
  },
  {
    "text": "any uh softwares that needs to be updated and once after the after that finishes",
    "start": "1792720",
    "end": "1800320"
  },
  {
    "text": "we we do several checks and that checks if that if it passes that checks uh we",
    "start": "1800320",
    "end": "1807200"
  },
  {
    "text": "put that server into proceed state and from that it goes to like in service so now you can see like",
    "start": "1807200",
    "end": "1812640"
  },
  {
    "text": "we have two servers running into this particular group and it's healthy and receiving traffic and also",
    "start": "1812640",
    "end": "1818960"
  },
  {
    "text": "sending data to the next further into the pipeline",
    "start": "1818960",
    "end": "1823840"
  },
  {
    "text": "so uh when the server is going away uh we we use the same hooks uh",
    "start": "1824080",
    "end": "1830799"
  },
  {
    "text": "another hooks that that aws provides so we put it in a terminating wait state",
    "start": "1830799",
    "end": "1836000"
  },
  {
    "text": "and then uh we do some checks uh and make sure that we send all the",
    "start": "1836000",
    "end": "1841039"
  },
  {
    "text": "analytics files that we have to further into the pipeline and if any locks that we need to back up uh we",
    "start": "1841039",
    "end": "1848240"
  },
  {
    "text": "do those as well at that time and once we are done with that uh that we put it in a proceed state",
    "start": "1848240",
    "end": "1854399"
  },
  {
    "text": "so from there it goes to uh it terminated and then it uh we back to",
    "start": "1854399",
    "end": "1859679"
  },
  {
    "text": "our one one server auto scaling group so this is our uh deployment process look like uh as we",
    "start": "1859679",
    "end": "1867519"
  },
  {
    "text": "are also using ebs for our data volume uh we could also like look for ebs instance that is that has some data",
    "start": "1867519",
    "end": "1875120"
  },
  {
    "text": "uh that not being not yet send and we can send we can attach that dbs volume and then",
    "start": "1875120",
    "end": "1880880"
  },
  {
    "text": "send that data to further into the pipeline so consumer so uh this is a simple store",
    "start": "1880880",
    "end": "1889919"
  },
  {
    "start": "1885000",
    "end": "1894000"
  },
  {
    "text": "consumer which take care of like storing data to s3",
    "start": "1889919",
    "end": "1894880"
  },
  {
    "start": "1894000",
    "end": "2116000"
  },
  {
    "text": "so this is our amazon kinesis stream uh so we help this consumer using kcl",
    "start": "1895919",
    "end": "1901760"
  },
  {
    "text": "library and uh kcl library uh take care of like communicating with the shots and making",
    "start": "1901760",
    "end": "1907519"
  },
  {
    "text": "sure you have like enough workers processing each shots uh it also does checkpoint checkpointing",
    "start": "1907519",
    "end": "1914000"
  },
  {
    "text": "for you so it makes development really fast so we are using kcl for consumers",
    "start": "1914000",
    "end": "1920480"
  },
  {
    "text": "so in this case like we are running to consumer and consumer is auto skilled as well so if your",
    "start": "1920480",
    "end": "1928000"
  },
  {
    "text": "consumer is doing some cpu heavy processing and it's lagging behind uh it's going to",
    "start": "1928000",
    "end": "1934240"
  },
  {
    "text": "add more consumers as well and uh instead of running like two workers on",
    "start": "1934240",
    "end": "1939919"
  },
  {
    "text": "the same box uh that will like it will be running like one",
    "start": "1939919",
    "end": "1945279"
  },
  {
    "text": "one worker part per machine so from there uh so kcl also provides you record",
    "start": "1945279",
    "end": "1952399"
  },
  {
    "text": "interface which uh emits record data and so you",
    "start": "1952399",
    "end": "1957519"
  },
  {
    "text": "can so we take that that record as and also we are compressed compressing data when we send data to",
    "start": "1957519",
    "end": "1964080"
  },
  {
    "text": "kinesis uh we decompress that data and uh after we do a decompression we do",
    "start": "1964080",
    "end": "1969840"
  },
  {
    "text": "some uh check for duplicates uh the reason we are doing a duplicate check here is",
    "start": "1969840",
    "end": "1974880"
  },
  {
    "text": "uh because when you send data to kinesis uh you it might be possible that you are",
    "start": "1974880",
    "end": "1980080"
  },
  {
    "text": "receiving a http 500 response type and if you receive http 500 uh you don't know whether data",
    "start": "1980080",
    "end": "1987840"
  },
  {
    "text": "already made it to the kinesis or not so in that in that case you have to redraw that and",
    "start": "1987840",
    "end": "1993760"
  },
  {
    "text": "if you do a retry and uh your first call already made it to kinesis then you will be like duplicating that",
    "start": "1993760",
    "end": "2000320"
  },
  {
    "text": "data into kinesis so this is something that we didn't find out like when we started building up",
    "start": "2000320",
    "end": "2006480"
  },
  {
    "text": "uh this consumer application but after like after uh after like realizing that we are getting",
    "start": "2006480",
    "end": "2011760"
  },
  {
    "text": "duplicates uh we we find out that it's because of the http 500",
    "start": "2011760",
    "end": "2016960"
  },
  {
    "text": "that we are retrying for put record calls so once we do this duplicate check",
    "start": "2016960",
    "end": "2024559"
  },
  {
    "text": "uh we divide that data into a target table and as we are sending all the events to",
    "start": "2024559",
    "end": "2030159"
  },
  {
    "text": "a single stream we need to make sure that we are buffering into the right uh",
    "start": "2030159",
    "end": "2036000"
  },
  {
    "text": "we are buffering right uh then after we divide it to target table uh we do some simple validation for that",
    "start": "2036000",
    "end": "2043519"
  },
  {
    "text": "particular data and this validation happens based on like schema and redshift",
    "start": "2043519",
    "end": "2050480"
  },
  {
    "text": "then we do transformation uh for that particular data and uh we do a transformation to like",
    "start": "2051919",
    "end": "2059118"
  },
  {
    "text": "dsp delimited subreddit format uh and just to make sure that we are",
    "start": "2059119",
    "end": "2064158"
  },
  {
    "text": "preparing that data for a load into redshift so that dsv formatted data",
    "start": "2064159",
    "end": "2070720"
  },
  {
    "text": "we buffer it again we check for a certain size or if the timeout kicks in uh we compress",
    "start": "2070720",
    "end": "2076800"
  },
  {
    "text": "that data and then uh that data goes to s3 so at the same time we also send data to",
    "start": "2076800",
    "end": "2084878"
  },
  {
    "text": "like rds instance which is just some kind of metadata information for s3 object that we are creating and uh",
    "start": "2084879",
    "end": "2092480"
  },
  {
    "text": "that helps us basically in uh in loading process for redshift and uh",
    "start": "2092480",
    "end": "2099359"
  },
  {
    "text": "we we store like file name uh what is the given what is the name of the game and uh",
    "start": "2099359",
    "end": "2105359"
  },
  {
    "text": "table name then uh how many records within that particular file and like what is the file size so we",
    "start": "2105359",
    "end": "2112000"
  },
  {
    "text": "store those kind of data and that's going to help us into loading process",
    "start": "2112000",
    "end": "2117599"
  },
  {
    "start": "2116000",
    "end": "2154000"
  },
  {
    "text": "so now uh loading data into redshift",
    "start": "2117599",
    "end": "2122800"
  },
  {
    "text": "so uh so if we use that metadata information to load data uh whenever we need to load",
    "start": "2122800",
    "end": "2129359"
  },
  {
    "text": "data into redshift we query this table uh and then we get list of events that",
    "start": "2129359",
    "end": "2134640"
  },
  {
    "text": "we need to load uh once we get that list of events we get like what are the files that i need",
    "start": "2134640",
    "end": "2140320"
  },
  {
    "text": "to load for this particular event and uh once once we get that uh we start",
    "start": "2140320",
    "end": "2145680"
  },
  {
    "text": "a transaction uh we create a manifest file uh which is just a list of files that",
    "start": "2145680",
    "end": "2151680"
  },
  {
    "text": "that we want to load from s3 uh after that we execute a copy command on",
    "start": "2151680",
    "end": "2157920"
  },
  {
    "text": "a red shift using that manifest file and uh redshift gets data from s3",
    "start": "2157920",
    "end": "2164480"
  },
  {
    "text": "because we are storing data into s3 so so it gets data it's gets it up from s3 and then we",
    "start": "2164480",
    "end": "2171440"
  },
  {
    "text": "update status of uh that particular copy statement uh back to the process uh we",
    "start": "2171440",
    "end": "2179599"
  },
  {
    "text": "and after that once that is done uh we do another copy command in the same transaction and we do like",
    "start": "2179599",
    "end": "2186560"
  },
  {
    "text": "batch of those copy commands into same transaction uh the reason we are doing it this way because uh as",
    "start": "2186560",
    "end": "2193040"
  },
  {
    "text": "suhash mentioned before that we are we have like thousand plus tables uh in our data warehouse and if we had",
    "start": "2193040",
    "end": "2200079"
  },
  {
    "text": "to do it single copy commit for thousand plus tables it was taking a long time so after talking with like aws",
    "start": "2200079",
    "end": "2208160"
  },
  {
    "text": "support team uh we find out that uh if we instead of doing a single",
    "start": "2208160",
    "end": "2213440"
  },
  {
    "text": "copy commit if you do a batch copy commit uh it would help us a lot and uh after",
    "start": "2213440",
    "end": "2219440"
  },
  {
    "text": "switching it to batch copy uh we actually got 15 50 gain",
    "start": "2219440",
    "end": "2224560"
  },
  {
    "text": "in our loading time so before it used to take like eight hours and now we bring it",
    "start": "2224560",
    "end": "2230560"
  },
  {
    "text": "down to like four hours and so after we commit that transaction",
    "start": "2230560",
    "end": "2236240"
  },
  {
    "text": "uh we update status back to a metadata db so next time when we run loading process we",
    "start": "2236240",
    "end": "2242480"
  },
  {
    "text": "know like what files need to be loaded so so this is like ore everything",
    "start": "2242480",
    "end": "2250640"
  },
  {
    "start": "2246000",
    "end": "2254000"
  },
  {
    "text": "together for a simple store architecture that we have so game server sending data kinases",
    "start": "2250640",
    "end": "2258560"
  },
  {
    "text": "uh consumer receiving data from kinesis and then uh consumer uploading that",
    "start": "2258560",
    "end": "2265040"
  },
  {
    "text": "delimited separated format into s3 and from s3",
    "start": "2265040",
    "end": "2270560"
  },
  {
    "text": "data goes to redshift apart from the dsv we also store another format json format",
    "start": "2270560",
    "end": "2278560"
  },
  {
    "text": "that goes to s3 as well and we we store s3 format we store json format",
    "start": "2278560",
    "end": "2284960"
  },
  {
    "text": "specifically so we can run emr job on top of that data and uh",
    "start": "2284960",
    "end": "2291040"
  },
  {
    "text": "there are a few use cases that we run emr jobs on json json data on s3 uh we calculate",
    "start": "2291040",
    "end": "2298480"
  },
  {
    "text": "rational length for each player each game play and we do it using emr jobs",
    "start": "2298480",
    "end": "2303920"
  },
  {
    "text": "on top of json data so if you know about redshift you might",
    "start": "2303920",
    "end": "2309520"
  },
  {
    "text": "wonder like why why we need to use like two separate format like dsv and json uh we could have just as redshift",
    "start": "2309520",
    "end": "2316720"
  },
  {
    "text": "supports a json format forwarding data you could have just use that format for",
    "start": "2316720",
    "end": "2321920"
  },
  {
    "text": "loading data into redshift uh so we we choose to uh use dsp",
    "start": "2321920",
    "end": "2326960"
  },
  {
    "text": "for two reasons like we can minimize the network i o uh and uh that way like uh uh it's",
    "start": "2326960",
    "end": "2334400"
  },
  {
    "text": "basically do it can load data faster and uh for some reason if uh as our",
    "start": "2334400",
    "end": "2341280"
  },
  {
    "text": "client are not schema aware they might send some extra column which is not yet in a",
    "start": "2341280",
    "end": "2347680"
  },
  {
    "text": "red shift so in that case uh if we use json format for that particular data it will",
    "start": "2347680",
    "end": "2353599"
  },
  {
    "text": "just silently ignored and uh so we will be missing that column but if we",
    "start": "2353599",
    "end": "2359040"
  },
  {
    "text": "use dsp format we know that it's going to be rejected by redshift and",
    "start": "2359040",
    "end": "2364720"
  },
  {
    "text": "we can rerun the process after fixing the schema into red shift so that's the reason we are using like two different format for uh",
    "start": "2364720",
    "end": "2371440"
  },
  {
    "text": "for loading process so so there was a simple store uh",
    "start": "2371440",
    "end": "2377920"
  },
  {
    "start": "2373000",
    "end": "2387000"
  },
  {
    "text": "consumer apart from simple store consumer uh we also have like another consumer for",
    "start": "2377920",
    "end": "2383440"
  },
  {
    "text": "uh calculating real-time aggregates and uh so this consumer getting data from",
    "start": "2383440",
    "end": "2389839"
  },
  {
    "text": "kinesis uh it it gets the config file uh which basically says that what are the",
    "start": "2389839",
    "end": "2396320"
  },
  {
    "text": "metrics i want to keep track of and it does and it has also like time interval in that config file",
    "start": "2396320",
    "end": "2402240"
  },
  {
    "text": "and based on that it keep calculating data from in memory for that uh time interval and",
    "start": "2402240",
    "end": "2408880"
  },
  {
    "text": "at the end of the time interval that data goes to dynamodb table and we have stats dashboard that's",
    "start": "2408880",
    "end": "2416880"
  },
  {
    "text": "using those uh data for so any business user can like see",
    "start": "2416880",
    "end": "2423359"
  },
  {
    "text": "in real time like what's happening uh within the game so so that was the real-time aggregate",
    "start": "2423359",
    "end": "2430240"
  },
  {
    "start": "2426000",
    "end": "2456000"
  },
  {
    "text": "so we we also have another consumer for for a spark cluster that we are running",
    "start": "2430240",
    "end": "2436319"
  },
  {
    "text": "now spark spark is a uh really good at processing large amount of data and uh it can run any map reduce kind of",
    "start": "2436319",
    "end": "2443760"
  },
  {
    "text": "jobs uh similar to hadoop uh or faster than hadoop if the data is in memory you can like",
    "start": "2443760",
    "end": "2449760"
  },
  {
    "text": "run jobs like 100 times faster than hadoop so so we are using spark",
    "start": "2449760",
    "end": "2456240"
  },
  {
    "start": "2456000",
    "end": "2530000"
  },
  {
    "text": "so we have this spark cluster uh which is running kinesis streaming library and this library take care of in terms",
    "start": "2458560",
    "end": "2466079"
  },
  {
    "text": "of running kcl and then uh it gets data from kinesis uh and you can specify like batch",
    "start": "2466079",
    "end": "2472960"
  },
  {
    "text": "interval in which you want to load data into spark so uh we are using five-minute",
    "start": "2472960",
    "end": "2478720"
  },
  {
    "text": "batch interval so uh it keep that library keep getting data from kinesis and then",
    "start": "2478720",
    "end": "2485440"
  },
  {
    "text": "at the end of the batch interval like it loads data into spark a specific data set which is called rdd",
    "start": "2485440",
    "end": "2491760"
  },
  {
    "text": "and from rdd we convert that data convert that rdd into schema rdd uh the reason we are doing",
    "start": "2491760",
    "end": "2498720"
  },
  {
    "text": "schema already because our end goal for spark right now is to uh to be able to like query sql uh",
    "start": "2498720",
    "end": "2506800"
  },
  {
    "text": "to be able to like use sql sql queries so if we do a schema rdd uh it's a schema",
    "start": "2506800",
    "end": "2512800"
  },
  {
    "text": "over format and you can run any uh sql or high quality on top of that data",
    "start": "2512800",
    "end": "2518160"
  },
  {
    "text": "so so we use parsequal and spark also provide like jdbc server that",
    "start": "2518160",
    "end": "2523839"
  },
  {
    "text": "you can use to to get interface for running any sql",
    "start": "2523839",
    "end": "2529040"
  },
  {
    "text": "statement so this is our final architecture look",
    "start": "2529040",
    "end": "2534720"
  },
  {
    "start": "2530000",
    "end": "2567000"
  },
  {
    "text": "like with all the consumers so we have like consumer running for a",
    "start": "2534720",
    "end": "2540000"
  },
  {
    "text": "spa spark cluster and jdbc server for sql interface",
    "start": "2540000",
    "end": "2545760"
  },
  {
    "text": "then there is a consumer for real-time aggregates and there is a consumer for uploading data",
    "start": "2545760",
    "end": "2550960"
  },
  {
    "text": "to s3 so so this is how it looks like with three consumers running right now and",
    "start": "2550960",
    "end": "2558160"
  },
  {
    "text": "so some of the lessons uh we learned during uh this process is uh",
    "start": "2561119",
    "end": "2567599"
  },
  {
    "start": "2567000",
    "end": "2642000"
  },
  {
    "text": "for a sender site like it's always good to decouple your uh data generation from sending so in",
    "start": "2567599",
    "end": "2573920"
  },
  {
    "text": "case like uh it allows you to like optimize your uh your data send uh data send so in our",
    "start": "2573920",
    "end": "2581359"
  },
  {
    "text": "case like we are batching compressing data and also like uh for some reason like third party service",
    "start": "2581359",
    "end": "2588800"
  },
  {
    "text": "that you are using is not is down or it's having more latency you can",
    "start": "2588800",
    "end": "2594319"
  },
  {
    "text": "you can back it up and then without affecting your real uh web service performance uh",
    "start": "2594319",
    "end": "2601920"
  },
  {
    "text": "do a batch and compression uh whenever whenever whenever you can that will allow you to like send more data within",
    "start": "2601920",
    "end": "2608000"
  },
  {
    "text": "a single call uh would record http 500 can result in",
    "start": "2608000",
    "end": "2613599"
  },
  {
    "text": "duplicate that's something that we learned after building a consumer uh i would",
    "start": "2613599",
    "end": "2620000"
  },
  {
    "text": "also recommend like monitor provision throughput exceeded exception uh that can",
    "start": "2620000",
    "end": "2625680"
  },
  {
    "text": "help you uh that can help you like notifying about uh additional stream capacity required",
    "start": "2625680",
    "end": "2631680"
  },
  {
    "text": "for your for your stream so if you like start getting these exceptions like a lot frequently that means like your",
    "start": "2631680",
    "end": "2638079"
  },
  {
    "text": "sender is uh not able to like send all the data it's generating",
    "start": "2638079",
    "end": "2643920"
  },
  {
    "start": "2642000",
    "end": "2654000"
  },
  {
    "text": "so the lessons we learned for consumer side use kcl whenever possible it makes uh",
    "start": "2644240",
    "end": "2650079"
  },
  {
    "text": "communicating with the shots and resharding very easy",
    "start": "2650079",
    "end": "2655680"
  },
  {
    "start": "2654000",
    "end": "2665000"
  },
  {
    "text": "auto scale your consumer group so and monitor the load constantly on that one so you can",
    "start": "2656400",
    "end": "2662319"
  },
  {
    "text": "like add more stream capacity as needed and in overall like provision enough",
    "start": "2662319",
    "end": "2669599"
  },
  {
    "text": "shards ahead of time so it can take any pics in your data",
    "start": "2669599",
    "end": "2676240"
  },
  {
    "start": "2674000",
    "end": "2693000"
  },
  {
    "text": "shut down gracefully uh so any producer sender application any sender or consumer application",
    "start": "2676240",
    "end": "2683920"
  },
  {
    "text": "should handle like shut down gracefully uh and uh just to end and by committing any checkpoint that",
    "start": "2683920",
    "end": "2689760"
  },
  {
    "text": "that it needs to so that way you are not duplicating data",
    "start": "2689760",
    "end": "2694880"
  },
  {
    "start": "2693000",
    "end": "2705000"
  },
  {
    "text": "and follow aws best practices for any error retries and exponential back off",
    "start": "2695680",
    "end": "2702318"
  },
  {
    "text": "so some of the takeovers from the talk uh guinness is like makes data available",
    "start": "2702800",
    "end": "2708240"
  },
  {
    "text": "for processing within a few seconds so uh you can build real-time application uh using it",
    "start": "2708240",
    "end": "2714720"
  },
  {
    "start": "2713000",
    "end": "2733000"
  },
  {
    "text": "uh it has robust api uh it provides like various library kcl and connector libraries there is",
    "start": "2714720",
    "end": "2720560"
  },
  {
    "text": "also like resharing libraries that you can use now uh and it's very important to like uh test your",
    "start": "2720560",
    "end": "2726640"
  },
  {
    "text": "application logic by restarting your stream so you know how it behaves like when you reshad your",
    "start": "2726640",
    "end": "2732480"
  },
  {
    "text": "stream uh in overall like aws is like mandate",
    "start": "2732480",
    "end": "2737680"
  },
  {
    "text": "services uh it's a very scalable uh it's cost effective and uh you can get up and running using",
    "start": "2737680",
    "end": "2745119"
  },
  {
    "text": "guinness uh any aws services like very quickly so uh in our case like",
    "start": "2745119",
    "end": "2751520"
  },
  {
    "text": "we built this data pipeline like in just four months uh like starting from scratch like understanding like what are the",
    "start": "2751520",
    "end": "2758160"
  },
  {
    "text": "what it provides and then building prototype or testing it and then making it production ready",
    "start": "2758160",
    "end": "2763359"
  },
  {
    "text": "uh it only took like four four months with like three engineers and like ops uh support",
    "start": "2763359",
    "end": "2771760"
  },
  {
    "text": "cool uh thanks everyone that was it",
    "start": "2771760",
    "end": "2780720"
  }
]