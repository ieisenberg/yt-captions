[
  {
    "text": "thank you so my name is Steven moon so I'm a specialist Solutions Architect with our data and analytics team one of",
    "start": "30",
    "end": "8280"
  },
  {
    "text": "the things that is a former kind of DBA database architect engineer a lot of",
    "start": "8280",
    "end": "13340"
  },
  {
    "text": "DBAs roles are turning into more of data architecture and engineering in the context of building data pipelines",
    "start": "13340",
    "end": "19170"
  },
  {
    "text": "especially indicate cloud you're taking away a lot of the administrative things so what I'm going to talk about today is",
    "start": "19170",
    "end": "25080"
  },
  {
    "text": "a little bit about data ops a little bit about some the data supply chain",
    "start": "25080",
    "end": "30449"
  },
  {
    "text": "pipeline which is something I talked about last year got a lot of good feedback so I really wanted to kind of",
    "start": "30449",
    "end": "36840"
  },
  {
    "text": "refresh it for this year based on some of the customer feedback I've gotten and then I'm going to put it in the context",
    "start": "36840",
    "end": "43379"
  },
  {
    "text": "of AWS Lake formation which if you heard my colleague Chandra's speech right before mine or talk right before mine he",
    "start": "43379",
    "end": "49500"
  },
  {
    "text": "did a really good job of an in-depth so I'm not going to go in depth in lake formation I'm gonna try to tie it all together in kind of a harmonious way so",
    "start": "49500",
    "end": "56520"
  },
  {
    "text": "that you kind of understand how those fit together and so what I hope you get of this that you're able to use some of",
    "start": "56520",
    "end": "61739"
  },
  {
    "text": "these principles and approaches and things to build your own data pipelines and data lakes so data ops just quickly",
    "start": "61739",
    "end": "69060"
  },
  {
    "text": "data ops is really this is which is the application of DevOps processes to data",
    "start": "69060",
    "end": "76200"
  },
  {
    "text": "one of the fundamental differences though is that data Ops is really an operational function it's not an IT",
    "start": "76200",
    "end": "81840"
  },
  {
    "text": "function and so one of the things that's come out of that is data ops engineering which is a lot of what your DBA roles",
    "start": "81840",
    "end": "87710"
  },
  {
    "text": "data architect roles and things like that are starting to turn into it's kind of a natural transition for this comes",
    "start": "87710",
    "end": "94380"
  },
  {
    "text": "from the are the relational world or the no sequel world and things like that is to be able to build out those pipelines",
    "start": "94380",
    "end": "99990"
  },
  {
    "text": "not a data scientist per se but I'm enabling the pipeline for the data science analytics reporting community so",
    "start": "99990",
    "end": "107130"
  },
  {
    "text": "a couple things from the data ops manifesto I think are really important when you're designing this has taken to account is continually satisfy your",
    "start": "107130",
    "end": "114090"
  },
  {
    "text": "customers so at AWS we have leadership principles which very succinctly tie",
    "start": "114090",
    "end": "120360"
  },
  {
    "text": "into a lot of the data ops principles that you're gonna see here so customer obsession for example is one of our",
    "start": "120360",
    "end": "126149"
  },
  {
    "text": "leadership principles that is also one of the data ops principles embrace change right your data pipeline is going",
    "start": "126149",
    "end": "133050"
  },
  {
    "text": "to be dynamic it's never gonna be static you're always going to getting new sources of data in",
    "start": "133050",
    "end": "138090"
  },
  {
    "text": "which you're trying to deliver results out of those new sources of data not to stagnate into one particular way of",
    "start": "138090",
    "end": "143280"
  },
  {
    "text": "doing things reflect write learn and be curious Azhar one of our leadership",
    "start": "143280",
    "end": "149370"
  },
  {
    "text": "principles and as you're building this pipelines take back to reflect on some of the things that you're doing during",
    "start": "149370",
    "end": "155190"
  },
  {
    "text": "the build process that you can change or make better that's really the power of the cloud is the agility to be able to",
    "start": "155190",
    "end": "160500"
  },
  {
    "text": "do that few other ones are going to be disposable environments I'm going to talk quite a bit about this in some",
    "start": "160500",
    "end": "166860"
  },
  {
    "text": "upcoming slides it's really unique as you're decoupling storage from compute and so I don't necessarily have to keep",
    "start": "166860",
    "end": "172950"
  },
  {
    "text": "datasets in their current configuration or a stanchion I don't have to keep compute in its instantiation or",
    "start": "172950",
    "end": "179280"
  },
  {
    "text": "configuration I can use it when I need it and spin it down so I'm not paying for data stored all over the place I'm",
    "start": "179280",
    "end": "184620"
  },
  {
    "text": "not paying for compute and processing that I don't need simplicity right from",
    "start": "184620",
    "end": "190500"
  },
  {
    "text": "a security perspective complexity is the enemy of security and so one of the things you'll notice and this is",
    "start": "190500",
    "end": "196080"
  },
  {
    "text": "virtually all of its herbalists right it's very straightforward it's you know I'm trying to weed out a lot of the",
    "start": "196080",
    "end": "201210"
  },
  {
    "text": "complexity so I'm trying to simplify the architecture and finally analytics is manufacturing I talked about this is an",
    "start": "201210",
    "end": "207300"
  },
  {
    "text": "operational thing so when you talk to think about a supply chain pipeline and how that works from shipping to warehouses - you know delivery Mart's",
    "start": "207300",
    "end": "215250"
  },
  {
    "text": "and all that think about it in that context as you're actually manufacturing or producing data sets to be consumed by",
    "start": "215250",
    "end": "221640"
  },
  {
    "text": "the end-user whoever that Yuen's or may be so data supply chain is you know securely democratizing data to deliver",
    "start": "221640",
    "end": "228360"
  },
  {
    "text": "it communities of interest when they need it and how they need it very straightforward right it's not an IT operational function from an operating",
    "start": "228360",
    "end": "236220"
  },
  {
    "text": "model perspective you know you're kind of your current state is going to be your a lot of folks are in diversified",
    "start": "236220",
    "end": "242280"
  },
  {
    "text": "where they've got data scattered all over the place you may have a logistics you know environments that have",
    "start": "242280",
    "end": "247680"
  },
  {
    "text": "different systems of record you might have sales or you know different systems that are you know kind of out there in",
    "start": "247680",
    "end": "253080"
  },
  {
    "text": "the ether and really what you're trying to do is coordinate those you're not trying to unify the systems you're trying to coordinate the data for a",
    "start": "253080",
    "end": "259310"
  },
  {
    "text": "360-degree view there's actually a project that just recently won an award called vet 360 is something I",
    "start": "259310",
    "end": "266509"
  },
  {
    "text": "worked on a few years back that's actually doing that for veterans where they're looking at you know not only",
    "start": "266509",
    "end": "272509"
  },
  {
    "text": "medical but also the business aspect of the VA and how they can better deliver",
    "start": "272509",
    "end": "277580"
  },
  {
    "text": "quality services to the veterans and they do that by looking at the veteran in 360 degrees of all the things that",
    "start": "277580",
    "end": "283850"
  },
  {
    "text": "can pose you know a veteran in the VA so it's important to think about in that perspective is you're not unifying",
    "start": "283850",
    "end": "289190"
  },
  {
    "text": "systems of record you're coordinating data one of the outcomes of that may be you release API so that other systems",
    "start": "289190",
    "end": "296539"
  },
  {
    "text": "can sync back up with your data so that you can like sync addresses for example or you can sync other data elements via",
    "start": "296539",
    "end": "303500"
  },
  {
    "text": "API to back to the systems of record when you create new master data management and some of the other things",
    "start": "303500",
    "end": "308979"
  },
  {
    "text": "so some of the design principles that I talked about a minimized disruption to",
    "start": "308979",
    "end": "314630"
  },
  {
    "text": "data producers and how they deliver their data one of the things I've learned working in the DoD quite a bit",
    "start": "314630",
    "end": "320870"
  },
  {
    "text": "is that if you go to a data producer that you don't own or have influence over you tell them to change their system to give you your data that's a",
    "start": "320870",
    "end": "327860"
  },
  {
    "text": "lot of times a non-starter for a lot of different reasons so the best thing that you can do is trying to get that data is to minimize",
    "start": "327860",
    "end": "334250"
  },
  {
    "text": "the disruption to that data owner so that they don't have to change their system or the way that they do business to get you your data or the data that",
    "start": "334250",
    "end": "340760"
  },
  {
    "text": "you need configuration the 80/20 rule focus on 80% of the use cases that can",
    "start": "340760",
    "end": "347060"
  },
  {
    "text": "be satisfied with configurable components it's kind of a hard and fast rule that if you have to do anything above you know if your configuration is",
    "start": "347060",
    "end": "354620"
  },
  {
    "text": "such that you're starting to write code and things like that you might be a custom develop solution and it's not the",
    "start": "354620",
    "end": "361340"
  },
  {
    "text": "wrong answer it's just something to think about when you're procuring tools in buying software as part of your data",
    "start": "361340",
    "end": "367909"
  },
  {
    "text": "pipeline right tool for the right job I see people all the time buying tools and",
    "start": "367909",
    "end": "375530"
  },
  {
    "text": "then backing their processes into tools really your process that drive the tooling not the other way around right",
    "start": "375530",
    "end": "382460"
  },
  {
    "text": "there's lots of good tools out there do a lot of different things but really you want to focus on the process and the organization not the tool and so I'll",
    "start": "382460",
    "end": "389780"
  },
  {
    "text": "talk about in a little while some of the stuff that we have different tools that are the right tool for the right job",
    "start": "389780",
    "end": "395979"
  },
  {
    "text": "driving on that a little bit conscious decoupling so the right tool today may not be the",
    "start": "395979",
    "end": "401120"
  },
  {
    "text": "right tool tomorrow and so that goes into you know you may wake up in the morning and have some new requirement",
    "start": "401120",
    "end": "406970"
  },
  {
    "text": "that you didn't know you were gonna have and the tools that you have may or may not meet that and so by decoupling like",
    "start": "406970",
    "end": "413900"
  },
  {
    "text": "compute and storage for example you can change the tooling that you're processing data with whether it could be",
    "start": "413900",
    "end": "419360"
  },
  {
    "text": "a graph or it could be Hadoop it could be something else in order to meet that business need and so you really want to",
    "start": "419360",
    "end": "425510"
  },
  {
    "text": "make sure that your your data your processing and then pieces of the pipeline or decoupled and it's gonna see",
    "start": "425510",
    "end": "431660"
  },
  {
    "text": "that in a minute so that you're able to agile II move in and out of different tool sets without affecting the",
    "start": "431660",
    "end": "437720"
  },
  {
    "text": "underlying data sets and then data residency this is a big one to is you should be able to access data where it",
    "start": "437720",
    "end": "444020"
  },
  {
    "text": "lives regardless of where they live and so you don't want to move data around all over the place one it can get expensive and two it can really screw",
    "start": "444020",
    "end": "450290"
  },
  {
    "text": "your results right if you have it's there's a if you remember in grade school when you tell one person a secret",
    "start": "450290",
    "end": "456590"
  },
  {
    "text": "and it would go around the room to 30 different people it would be different by the time it got to that thirtieth person passing data around works the",
    "start": "456590",
    "end": "463040"
  },
  {
    "text": "same way and so you know if you ask a commander or a leader you know if they ask you a question they want one answer",
    "start": "463040",
    "end": "469430"
  },
  {
    "text": "they don't want to go to three different systems and get three different answers and so you want your you want people accessing the data where it lives that",
    "start": "469430",
    "end": "476120"
  },
  {
    "text": "you control the data that they access you control the format of it the structure of it and what they see so",
    "start": "476120",
    "end": "482260"
  },
  {
    "text": "conceptually it looks like this right I've got data producers that are shipping data I'm ingesting it you know",
    "start": "482260",
    "end": "487940"
  },
  {
    "text": "if you think about a in in login logistics world you receive it you inspect it you might stage it you might",
    "start": "487940",
    "end": "494419"
  },
  {
    "text": "archive it right but one of the things are starting to do is you're starting to catalog and profile for data lineage right because they do lineage is super",
    "start": "494419",
    "end": "500780"
  },
  {
    "text": "important as you go through the pipeline to be able to trace back the source and all the transformations that happen along the way",
    "start": "500780",
    "end": "507130"
  },
  {
    "text": "processing data is 80 percent any any data science project any analytics",
    "start": "507130",
    "end": "512300"
  },
  {
    "text": "project the processing is nothing to do with the data science it's 80% of the work that you're going to be doing that's going to be cleansing the data",
    "start": "512300",
    "end": "519070"
  },
  {
    "text": "aggregating it associating and merging the data that's going to be you know",
    "start": "519070",
    "end": "524180"
  },
  {
    "text": "kind of traditional data warehousing type stuff ETL type stuff enriching the data is",
    "start": "524180",
    "end": "529370"
  },
  {
    "text": "going to be more of applying statistics and some data science techniques to the data",
    "start": "529370",
    "end": "534710"
  },
  {
    "text": "to assimilate it transform it do some data engineering but also doing some",
    "start": "534710",
    "end": "539840"
  },
  {
    "text": "synthesis and I spend a little bit of time in a minute talking about data synthesis because I think that's important too the end result is you're",
    "start": "539840",
    "end": "546110"
  },
  {
    "text": "securely democratizing the data cataloging the data profiling the data so that end users can discover and",
    "start": "546110",
    "end": "553130"
  },
  {
    "text": "explore the data they're able to search it in the catalog they're able to search it in the lake they're also able to analyze and report",
    "start": "553130",
    "end": "558950"
  },
  {
    "text": "on it at different layers and so you've really got two different types of users out there you've kind of got your business intelligence users who are going to do",
    "start": "558950",
    "end": "564860"
  },
  {
    "text": "reporting some may be more sophisticated and sequel you know there's different levels there and then you've got your",
    "start": "564860",
    "end": "570080"
  },
  {
    "text": "data science users right I'm probably a novice data slash user I'm you know but you may all gone and building deep blur",
    "start": "570080",
    "end": "575600"
  },
  {
    "text": "you know neural nets deep learning models and things like that and so you're able to democratize that data and",
    "start": "575600",
    "end": "581600"
  },
  {
    "text": "produce that data in a way that each one of those is able to utilize the what you're really delivering though is",
    "start": "581600",
    "end": "587120"
  },
  {
    "text": "information products right to information consumers write summaries dashboards insights machine learning",
    "start": "587120",
    "end": "593870"
  },
  {
    "text": "models or products that you're developing for your applications right that's something that you're you're delivering out of that and so really",
    "start": "593870",
    "end": "600380"
  },
  {
    "text": "your data is being analyzed reported on to deliver products to you know to leaders quad charts is a big one that",
    "start": "600380",
    "end": "606620"
  },
  {
    "text": "I've you know had to build over the years ingest there's no single tool for ingesting data I'm gonna talk a lot",
    "start": "606620",
    "end": "612230"
  },
  {
    "text": "about ingesting data so I won't spend too much time on this really what you want to do is focus on cultivating your",
    "start": "612230",
    "end": "617270"
  },
  {
    "text": "organizational competencies and the process for engaging with data suppliers don't worry about the tooling we can get",
    "start": "617270",
    "end": "624560"
  },
  {
    "text": "the data in there's lots of ways to do that focus on understanding how to do it building and by doing that you're going",
    "start": "624560",
    "end": "630650"
  },
  {
    "text": "to be building Tiger teams that understand organizational domains of the data suppliers that's much more",
    "start": "630650",
    "end": "635720"
  },
  {
    "text": "important than understanding the technology because you really need to have people to understand the organizational domains of the data so",
    "start": "635720",
    "end": "641120"
  },
  {
    "text": "logistics or finance or accounting or sales because that's going to be the",
    "start": "641120",
    "end": "646220"
  },
  {
    "text": "powerful thing that's going to pull all that data together and you're developing templates for mo use in AI CDs to govern",
    "start": "646220",
    "end": "652490"
  },
  {
    "text": "their relationships these are not 30 page documents these are things to say that you know we're going to give you this data at this time it's going to be",
    "start": "652490",
    "end": "659390"
  },
  {
    "text": "this data here the here are the columns here the attributes about those the metadata about the data and then if",
    "start": "659390",
    "end": "666120"
  },
  {
    "text": "we change something here's the process we're going to tell you what to change something so you're not putting the onus on the data supplier other than just to",
    "start": "666120",
    "end": "673170"
  },
  {
    "text": "have a communication mechanism for them to go back and forth to do that and what you're going to come out with really is",
    "start": "673170",
    "end": "678420"
  },
  {
    "text": "a small set of common patterns that you can use to standardize and automate the ingestion process that can scale to",
    "start": "678420",
    "end": "685220"
  },
  {
    "text": "thousands of data suppliers right that's really what you're looking for in practice you're probably going to see",
    "start": "685220",
    "end": "690780"
  },
  {
    "text": "five seven eight different common patterns that you're using out there to ingest data it could be the same type of",
    "start": "690780",
    "end": "695940"
  },
  {
    "text": "data but you might have to ingest it in different ways processing extract and load that's",
    "start": "695940",
    "end": "701670"
  },
  {
    "text": "cleansing your data entities and attributes remain distinct so if you think about if I had a system like ten different systems of record in HR I",
    "start": "701670",
    "end": "708330"
  },
  {
    "text": "might exist in ten different places right so I want to see that data and it's somewhat its raw format where I'm",
    "start": "708330",
    "end": "714870"
  },
  {
    "text": "not integrating that data entity resolution is where you start that process where aggregating the data or",
    "start": "714870",
    "end": "721140"
  },
  {
    "text": "I'm instantiating the instance as a single instance I exist one time but I'm",
    "start": "721140",
    "end": "726990"
  },
  {
    "text": "going to maintain my attributes across all those different systems because that may be valuable to some analysts down downstream they want to see that you",
    "start": "726990",
    "end": "733800"
  },
  {
    "text": "know something about me exists in different states in different places one",
    "start": "733800",
    "end": "739320"
  },
  {
    "text": "of the things that you is dispara tidies across system right if I exist in ten systems I might have the spirit IDs",
    "start": "739320",
    "end": "744450"
  },
  {
    "text": "across ten systems that becomes an attribute linked to generally a natural or synthetic UID or good so that you can",
    "start": "744450",
    "end": "752370"
  },
  {
    "text": "identify it within your data Lake within your data warehouse in your data Lake but I can also trace it back to where it",
    "start": "752370",
    "end": "758460"
  },
  {
    "text": "came from so that goes back to the data lineage associating that is really defining the relationships amongst the",
    "start": "758460",
    "end": "765870"
  },
  {
    "text": "different entities so that's where you're going to apply business relationship rules about the data and so that's where a lot of the 80% comes in",
    "start": "765870",
    "end": "772380"
  },
  {
    "text": "that kind of entity resolution place and it's really important when I talk about lake formation to think about this and a",
    "start": "772380",
    "end": "777570"
  },
  {
    "text": "little while with some of the stuff that you're going to hear about entity resolution in that eventually get to kind of a master data management where",
    "start": "777570",
    "end": "783120"
  },
  {
    "text": "you're integrating instances of attributes and entities into a single structure right that's the type of stuff",
    "start": "783120",
    "end": "789660"
  },
  {
    "text": "that now you can not only feed that into your data leg but you can also create an API layer to allow other systems to",
    "start": "789660",
    "end": "795510"
  },
  {
    "text": "synchronize themselves back with the master data so if addresses is a good example I was used because the",
    "start": "795510",
    "end": "803100"
  },
  {
    "text": "project that worked on the VA was actually they had veterans had addresses all over the place and different systems and they were different right so how do",
    "start": "803100",
    "end": "809070"
  },
  {
    "text": "i sync those up well if I try to create an API layer in between all those it would be you know it's an interior",
    "start": "809070",
    "end": "814590"
  },
  {
    "text": "problem with a number of connections that would be very hard to manage so the mass we mastered the addresses in in one",
    "start": "814590",
    "end": "821880"
  },
  {
    "text": "place and then created an API or a set of API is really so that applications could come get that data as they needed",
    "start": "821880",
    "end": "828030"
  },
  {
    "text": "it we didn't force it on and we just allowed them to come get it so again you're not interrupting anything that the end user is doing so that's the",
    "start": "828030",
    "end": "834270"
  },
  {
    "text": "merging of it enriching data is really where you get into some of the neat stuff like assimilating data which is",
    "start": "834270",
    "end": "840240"
  },
  {
    "text": "you know organizing stuff to be consumed by and I've got a good example of this here in a minute consumed by communities of interest so",
    "start": "840240",
    "end": "846390"
  },
  {
    "text": "it's going to be structured as facts think about like a star schema you have a facts things about transactions or",
    "start": "846390",
    "end": "851760"
  },
  {
    "text": "whatever it is graphs if you're familiar with you know",
    "start": "851760",
    "end": "856920"
  },
  {
    "text": "RDF soar semantic graphs looking for how to determine depth and breadth of",
    "start": "856920",
    "end": "861960"
  },
  {
    "text": "relationships among things entities time series is a big one we actually had a",
    "start": "861960",
    "end": "867120"
  },
  {
    "text": "new time series database that was just released back in reinvent this year and or matrices right you might take that",
    "start": "867120",
    "end": "874410"
  },
  {
    "text": "table the facts and then turn it into it integers so that you're able to build",
    "start": "874410",
    "end": "879420"
  },
  {
    "text": "machine learning models against that and really it's all driven by questions generated from the communities of interest and so you may not be going out",
    "start": "879420",
    "end": "886740"
  },
  {
    "text": "and like grabbing a bunch of data all in one shot really what you should be doing is going to the end users what are the",
    "start": "886740",
    "end": "892530"
  },
  {
    "text": "questions you're asking where does that data live and start pulling it forward and so if you're a Christian Chris DM",
    "start": "892530",
    "end": "899400"
  },
  {
    "text": "it's kind of the project scope of that transforming the data you know standardizing the data is a good example",
    "start": "899400",
    "end": "904470"
  },
  {
    "text": "of transforming everything needs to be in centimeters that's a standardization Engineering has to do more of you know",
    "start": "904470",
    "end": "910170"
  },
  {
    "text": "normalizing data so if I've got ranges of data that can vary greatly between",
    "start": "910170",
    "end": "915330"
  },
  {
    "text": "attributes I might want to normalize that down so I can understand scope of the data or how the you know how it",
    "start": "915330",
    "end": "922380"
  },
  {
    "text": "changes in one data you know one data element affect changes in another is an interpolating data an extrapolated",
    "start": "922380",
    "end": "928360"
  },
  {
    "text": "interpolating adding data within a data set extrapolating is getting additional",
    "start": "928360",
    "end": "933730"
  },
  {
    "text": "data points based on the data that's already there synthesis is really interesting to me and it's something",
    "start": "933730",
    "end": "938769"
  },
  {
    "text": "that I've been working on a lot is really synthesizing data one of the",
    "start": "938769",
    "end": "943839"
  },
  {
    "text": "reasons that I'm starting to see data synthesis is that it's a force multiplier and so if you want to be able",
    "start": "943839",
    "end": "949360"
  },
  {
    "text": "to community source problems out to let's say FFRDCs or you arcs like",
    "start": "949360",
    "end": "955600"
  },
  {
    "text": "research universities and things like that but you can't give them data let's say you're a DoD entity and you can't actually give them the data right how do",
    "start": "955600",
    "end": "960970"
  },
  {
    "text": "you do that to be able to community source the data so that you can multiply your force to build machine learning",
    "start": "960970",
    "end": "966820"
  },
  {
    "text": "models so that you know you can use those models in your mission and so you do that by synthesizing data so obvious",
    "start": "966820",
    "end": "972640"
  },
  {
    "text": "skating data is one part of that but it's not the only part and the reason is",
    "start": "972640",
    "end": "977709"
  },
  {
    "text": "there's actually some pretty strong examples where a few skated data has",
    "start": "977709",
    "end": "983560"
  },
  {
    "text": "been sent out and people were able to run like attribution attacks against it based on other data publicly available",
    "start": "983560",
    "end": "988750"
  },
  {
    "text": "data so obvious gating is the first part the second is anonymization of the data",
    "start": "988750",
    "end": "993760"
  },
  {
    "text": "which is applying privacy models to the data that can excuse the statistical results so you can't actually run some",
    "start": "993760",
    "end": "999730"
  },
  {
    "text": "of those attributions attack there's a whole set of different privacy models that vary between you know you know 30%",
    "start": "999730",
    "end": "1006899"
  },
  {
    "text": "skew or 5% or whatever it is depending on the sensitivity of your data and how much time you want to spend on redoing",
    "start": "1006899",
    "end": "1014010"
  },
  {
    "text": "those models once you get them back in but data synthesis something that's pretty interesting that you can do that",
    "start": "1014010",
    "end": "1019260"
  },
  {
    "text": "you know part of what you'd feed into your data lake which will kind of see in a minute cataloging is is super important because",
    "start": "1019260",
    "end": "1025918"
  },
  {
    "text": "you want your users to be able to go search the data without having to go actually search the data like what do I",
    "start": "1025919",
    "end": "1031438"
  },
  {
    "text": "have right so you're gonna have glossary scon cept concept descriptions data models it's hugely important and they",
    "start": "1031439",
    "end": "1037260"
  },
  {
    "text": "don't think of data models in the relational term think about them in conceptual logical terms like how do",
    "start": "1037260",
    "end": "1042329"
  },
  {
    "text": "entities relate to each other they could relate to each other and be represented as a graph they can be represented as a third degree you know",
    "start": "1042329",
    "end": "1049200"
  },
  {
    "text": "boyce codd like normalized model really what the model is to show how everything's related and in the attributes of those things are their",
    "start": "1049200",
    "end": "1055850"
  },
  {
    "text": "classifications of data and then summary statistics is a big one because I actually use these a lot when",
    "start": "1055850",
    "end": "1062160"
  },
  {
    "text": "I'm doing exercises for customers to size their workloads so if I'm sizing a database workload I'm looking at you",
    "start": "1062160",
    "end": "1068730"
  },
  {
    "text": "know throughput I ops you know CPU ization memory but really I'm looking at average and skew and I'm looking at",
    "start": "1068730",
    "end": "1074850"
  },
  {
    "text": "quartiles to be able to size that because what that helps me do is cost optimize it and so if somebody can go in",
    "start": "1074850",
    "end": "1080490"
  },
  {
    "text": "a catalogue and look at summary statistics for data sets they can get a good idea of the type of data set that's",
    "start": "1080490",
    "end": "1085950"
  },
  {
    "text": "in there and where it may be valuable so circles I'm concerned I talked about this a little bit last year you know it",
    "start": "1085950",
    "end": "1091830"
  },
  {
    "text": "really has to do with how you ingest data so some systems you control right you can go and do you know do things on",
    "start": "1091830",
    "end": "1097650"
  },
  {
    "text": "that system to order to get that data some systems you influence where you don't own the system but you have",
    "start": "1097650",
    "end": "1103590"
  },
  {
    "text": "influence over the owners of that system so you've got a little more flexibility and how you may get that data most",
    "start": "1103590",
    "end": "1108750"
  },
  {
    "text": "things fall into interests right I don't own the system I have very little influence over the system but I need the",
    "start": "1108750",
    "end": "1115080"
  },
  {
    "text": "data so how do you get that data right the thing is why is that important",
    "start": "1115080",
    "end": "1121800"
  },
  {
    "text": "because it determines how you're going to get that data in your system you may have a data base out there that you might have control over that you can",
    "start": "1121800",
    "end": "1128190"
  },
  {
    "text": "hook an ETL product into that's great that works great that same database could exist somewhere else where you",
    "start": "1128190",
    "end": "1134430"
  },
  {
    "text": "have an interest in it but there's no way they're gonna let you hook an ETL tool into it right you just you don't",
    "start": "1134430",
    "end": "1139440"
  },
  {
    "text": "own it they don't trust or whatever for whatever reason they're not gonna let you do that and so that really determines how you're going to ingest",
    "start": "1139440",
    "end": "1145050"
  },
  {
    "text": "that data so logically you're you know you're essentially building a data port right a place for all the data to come",
    "start": "1145050",
    "end": "1151110"
  },
  {
    "text": "into you're building a data warehouse you know data Lake does not replace a",
    "start": "1151110",
    "end": "1157350"
  },
  {
    "text": "data warehouse a data warehouse can feed as one of the feeders of a data lega data warehouses are still important",
    "start": "1157350",
    "end": "1162960"
  },
  {
    "text": "right you think about holding all of your data in a structured way that's relational and you can go you know query",
    "start": "1162960",
    "end": "1168330"
  },
  {
    "text": "it and do things against it you know from a searching perspective and just housing your data it's still an",
    "start": "1168330",
    "end": "1173490"
  },
  {
    "text": "important part of it it's also gonna have some your reference in your master data and the data Mart's think of that",
    "start": "1173490",
    "end": "1178650"
  },
  {
    "text": "data Mart's conceptually right a data Mart is not maybe it can physically be instantiated in a lot of different ways",
    "start": "1178650",
    "end": "1184050"
  },
  {
    "text": "all of its feeding your catalog and then all of its feeding your lake so one of the things you'll notice is nobody's actually accessing your data warehouse",
    "start": "1184050",
    "end": "1190530"
  },
  {
    "text": "here everybody's actually accessing it through the lake that's how you're gonna deliver things like reporting as a",
    "start": "1190530",
    "end": "1196440"
  },
  {
    "text": "service analytics is a service in machine learning is a service where you might set up an environment for users to",
    "start": "1196440",
    "end": "1201960"
  },
  {
    "text": "come in use your tools to do those things whatever it is that they made to do they made you do reporting okay",
    "start": "1201960",
    "end": "1208740"
  },
  {
    "text": "here's here's the tool set to do it here's the endpoint to do it they may need to do you know run some type of",
    "start": "1208740",
    "end": "1215790"
  },
  {
    "text": "machine learning against it here's how they do that the other thing that you want to be able to set up is bring your own tools so you",
    "start": "1215790",
    "end": "1221640"
  },
  {
    "text": "might have a whole user base that uses tableau I mean that's pretty common out there you might have MicroStrategy I see",
    "start": "1221640",
    "end": "1228210"
  },
  {
    "text": "those two probably the most common right and so you want users to be able to use their own tooling but access your data",
    "start": "1228210",
    "end": "1233910"
  },
  {
    "text": "where it where it lives not where they live and that's what the data Lake is doing so in AWS terms they might have",
    "start": "1233910",
    "end": "1239460"
  },
  {
    "text": "their own V PC with their own tooling and you give them permission to access your lake with their tooling assuming",
    "start": "1239460",
    "end": "1245850"
  },
  {
    "text": "they have the right security context and all that around it and then the output is kind of the same thing you're outputting information so just a quick",
    "start": "1245850",
    "end": "1252630"
  },
  {
    "text": "recap on some of the portfolio's that we're going to talk about so lake formation is a new service it's not it",
    "start": "1252630",
    "end": "1259140"
  },
  {
    "text": "is conglomeration of other services so s3 glue and some other things in our",
    "start": "1259140",
    "end": "1264630"
  },
  {
    "text": "analytics portfolio you can see I mean you know redshift is sequel based analytics EMR as our Hadoop managed",
    "start": "1264630",
    "end": "1272090"
  },
  {
    "text": "system service Athena for interactive queries Kinesis is going to be your",
    "start": "1272090",
    "end": "1278550"
  },
  {
    "text": "streaming there's a couple different solutions there and then kind of in the database world some of this is a lot gonna be more for transactional stuff so",
    "start": "1278550",
    "end": "1285210"
  },
  {
    "text": "kind of would focus on is Aurora Postgres actually I use quite a bit for some of the stuff I'm going to talk about Neptune is pretty important too",
    "start": "1285210",
    "end": "1292680"
  },
  {
    "text": "but all of its kind of the formational layer that's the the bottom layers I mean lake formation which includes Glu",
    "start": "1292680",
    "end": "1298140"
  },
  {
    "text": "s3 and a few other things so just keep in mind this is not all-encompassing",
    "start": "1298140",
    "end": "1303360"
  },
  {
    "text": "there's other stuff to it but this is kind of the major components of some of the things I'm going to talk about so",
    "start": "1303360",
    "end": "1308580"
  },
  {
    "text": "you probably saw this in the last you know data Lake it's this I think this is a great definition that one of my colleagues came up with is very simple",
    "start": "1308580",
    "end": "1314220"
  },
  {
    "text": "right it's just the central repository for structured and unstructured data at scale that's it that's that's what it",
    "start": "1314220",
    "end": "1319920"
  },
  {
    "text": "does so the stuff that we're gonna talk about is what built around it you know why do you do data legs right you want to be able to store any type of",
    "start": "1319920",
    "end": "1326640"
  },
  {
    "text": "data structured semi-structured unstructured data in a single place that people can access it it scales out to",
    "start": "1326640",
    "end": "1332190"
  },
  {
    "text": "exabytes we have customers today that are petabyte and exabytes scale may not be in a single s3 bucket but that isn't",
    "start": "1332190",
    "end": "1339030"
  },
  {
    "text": "that their data Lake is composed of multiple buckets and that's completely fine there's nothing wrong with that diverse set of analytics remember right",
    "start": "1339030",
    "end": "1346290"
  },
  {
    "text": "tool for the right job decoupling I don't really care what a customer comes",
    "start": "1346290",
    "end": "1352080"
  },
  {
    "text": "to my you know to do their analytics with all I care about is they have the right security credentials and their",
    "start": "1352080",
    "end": "1357990"
  },
  {
    "text": "tool can access the lake right most of the tools out there can access s3 today I mean if you look at like tableaus or",
    "start": "1357990",
    "end": "1363690"
  },
  {
    "text": "micro strata of those things they can access s3 they can access either earth or via EMR or they can access through",
    "start": "1363690",
    "end": "1369510"
  },
  {
    "text": "redshift so all of that a lot of the the third party ISPs out there able to do that today like I said I can work on the",
    "start": "1369510",
    "end": "1376350"
  },
  {
    "text": "data without any movement so as I talked about before access it where it lives not where your users live and then low",
    "start": "1376350",
    "end": "1382830"
  },
  {
    "text": "cost for storage and analytics you know frugality right what I'm trying to do for customers is build the lowest",
    "start": "1382830",
    "end": "1389060"
  },
  {
    "text": "highest value lowest cost you know solution cost optimized solution for",
    "start": "1389060",
    "end": "1396240"
  },
  {
    "text": "their data supply chain pipeline right and so most of that costs are a lot of that you know in that you can wring out",
    "start": "1396240",
    "end": "1402390"
  },
  {
    "text": "in the data layer by storing your data in the low in in s3 so a little bit",
    "start": "1402390",
    "end": "1408840"
  },
  {
    "text": "about the physical architecture it's kind of an example so let's say I have a data center somewhere right and I have a WSI here and I've got databases I don't",
    "start": "1408840",
    "end": "1416640"
  },
  {
    "text": "care they could be Oracle they could be sequel server or Postgres or whatever they are they could be anything if I own",
    "start": "1416640",
    "end": "1422250"
  },
  {
    "text": "that system I might use database migration service to load the data and then do all of the CDC into that into an",
    "start": "1422250",
    "end": "1427860"
  },
  {
    "text": "operational data store that's in Amazon and Aurora right so an operational data store is just going to capture the data",
    "start": "1427860",
    "end": "1433200"
  },
  {
    "text": "as it is in it into a central location right it's not doing anything today it's not you know you might do some very very",
    "start": "1433200",
    "end": "1440220"
  },
  {
    "text": "light cleaning in there but generally it's just a place the stage and ingest all of your data to get it into AWS",
    "start": "1440220",
    "end": "1446700"
  },
  {
    "text": "right if the neat thing about eight it you know database migration service it's a CDC tool that has a you could",
    "start": "1446700",
    "end": "1451770"
  },
  {
    "text": "functions on most of your our DBMS is out there I might have data from",
    "start": "1451770",
    "end": "1456780"
  },
  {
    "text": "databases that I can't hook into a2 database migration service for whatever reason and so I might be",
    "start": "1456780",
    "end": "1463020"
  },
  {
    "text": "spitting out CSV files or I might be spitting out Park a file I don't really care I might be collecting something",
    "start": "1463020",
    "end": "1468390"
  },
  {
    "text": "else in a different way that I need to get into AWS I might want to get that in real time though so I might use Canisius",
    "start": "1468390",
    "end": "1474000"
  },
  {
    "text": "to stream that into a bucket right may not be my data Lake it may just be a staging bucket because I might want to do some inspecting on that before it in",
    "start": "1474000",
    "end": "1480420"
  },
  {
    "text": "my lake I might want to you know look at it to make sure you know put apply something to it to make sure that it's",
    "start": "1480420",
    "end": "1485970"
  },
  {
    "text": "you know in the right format or this or that and so this is I don't own the you know Kinesis I can do that without using",
    "start": "1485970",
    "end": "1492030"
  },
  {
    "text": "database migration service and I'm allowing like I'm not putting the onus on my customer or my data supplier to",
    "start": "1492030",
    "end": "1498450"
  },
  {
    "text": "really do anything I'm not own logs right I might have I'm having spitting logs out to something in some security",
    "start": "1498450",
    "end": "1504660"
  },
  {
    "text": "context I might have a you know and you know I need to get it into AWS what is",
    "start": "1504660",
    "end": "1509760"
  },
  {
    "text": "new service is actually pretty cool called data sync and it'll actually sync up data that's sitting in your data",
    "start": "1509760",
    "end": "1515280"
  },
  {
    "text": "center like an NFS share into AWS and one of those syncs is going to be an s3 I've actually played her out this some",
    "start": "1515280",
    "end": "1521520"
  },
  {
    "text": "and it's a pretty neat way to do this and what it does is it optimizes the API calls to s3 on your behalf she actually",
    "start": "1521520",
    "end": "1528570"
  },
  {
    "text": "set it up in the console such your target up and then you can just drop change records or whatever it is in there and it will sync it into your",
    "start": "1528570",
    "end": "1534600"
  },
  {
    "text": "bucket just so what I'm showing is a lot of different options to get data into AWS these aren't the only options these",
    "start": "1534600",
    "end": "1540540"
  },
  {
    "text": "are just three options based on three use cases modal loads your data warehouse right you might have",
    "start": "1540540",
    "end": "1546000"
  },
  {
    "text": "structured data that you're letting in data warehouse you might have data coming in in CSV files or pipe delimited files whatever it is and you might want",
    "start": "1546000",
    "end": "1552360"
  },
  {
    "text": "to load it with DMS and glue so glue is our ETL product that also does data cataloging so you get some combination",
    "start": "1552360",
    "end": "1557760"
  },
  {
    "text": "of those two to load your data warehouse again you might have a warehouse in Aurora that's separate from your operational data store you could go",
    "start": "1557760",
    "end": "1563880"
  },
  {
    "text": "straight into your warehouse if you didn't want to store that operational data right in that operational data can",
    "start": "1563880",
    "end": "1569250"
  },
  {
    "text": "be ephemeral so just this is just an example there's a lot of different variations you could do here so your",
    "start": "1569250",
    "end": "1575160"
  },
  {
    "text": "data Lake is going to sit out here and you might load all of your raw data that you're getting in let's just say your",
    "start": "1575160",
    "end": "1580260"
  },
  {
    "text": "your community of interest is I want the raw data I don't want you to do anything to it I just want it as it comes in its",
    "start": "1580260",
    "end": "1585930"
  },
  {
    "text": "original file load it in the lake I might want to extract data sets and",
    "start": "1585930",
    "end": "1590970"
  },
  {
    "text": "build Mart's from my warehouse in different configurations based on the access patterns and I might also put that in my",
    "start": "1590970",
    "end": "1597719"
  },
  {
    "text": "data Lake I'm gonna talk about in detail on that in a minute about what that may look like then I can access that with",
    "start": "1597719",
    "end": "1602729"
  },
  {
    "text": "really anything Amazon Athena redshift EMR quick site all these things sage",
    "start": "1602729",
    "end": "1608070"
  },
  {
    "text": "maker or other tools right I could access all of this in a single location so the power of that is let's say I've",
    "start": "1608070",
    "end": "1614729"
  },
  {
    "text": "got a database and I've got data set a that's sitting out there I've just created this data set and I put it in a",
    "start": "1614729",
    "end": "1620489"
  },
  {
    "text": "data Lake I might instantiate that data set as a table of facts that I want to do some kind of sequel based analytics",
    "start": "1620489",
    "end": "1626519"
  },
  {
    "text": "against I just like sequel a lot and I'm like I'm a sequel guy can't grew up in that world so on I can hit that with",
    "start": "1626519",
    "end": "1632999"
  },
  {
    "text": "Amazon redshift right I can spin up another Amazon redshift cluster if I have another set of communities of",
    "start": "1632999",
    "end": "1639329"
  },
  {
    "text": "interest that want to do it on the same table of facts I might want to represent a as a graph because I want to look at",
    "start": "1639329",
    "end": "1645509"
  },
  {
    "text": "relationships between things that I can't do in a fact table because it's very hard to do recursiveness and sequel",
    "start": "1645509",
    "end": "1651389"
  },
  {
    "text": "they when you get to a certain layer of recursive miss and sequel it becomes really really inefficient that's what graphs are really good at is",
    "start": "1651389",
    "end": "1656789"
  },
  {
    "text": "traversing relationships and depth and breadth so I might represent that same thing in the lake as a graph I might",
    "start": "1656789",
    "end": "1663179"
  },
  {
    "text": "load it in the Neptune I might use Amazon EMR with spark graphics to do that same set of data just manifest and",
    "start": "1663179",
    "end": "1670829"
  },
  {
    "text": "instantiated in a different way I might load that into the matrix into a matrix I might turn anything into integers so",
    "start": "1670829",
    "end": "1678179"
  },
  {
    "text": "that I can hit it with Amazon EMR with MX with you know some machine learning algorithm or something I might deploy",
    "start": "1678179",
    "end": "1684809"
  },
  {
    "text": "Apache MX net on that so what I'm doing is I'm able to instantiate that set of data in different ways depending on the",
    "start": "1684809",
    "end": "1691229"
  },
  {
    "text": "community of interest and how they want to actually use the data so the warehouse is going to be you know a",
    "start": "1691229",
    "end": "1696450"
  },
  {
    "text": "permanent data store for structured data right I'm not that's where all my data lives and then I'm differentiating piece",
    "start": "1696450",
    "end": "1702749"
  },
  {
    "text": "structured and unstructured data on purpose so for permanent data it's going to the permanent data store for",
    "start": "1702749",
    "end": "1708450"
  },
  {
    "text": "structured data no direct access so I don't allow my end users to access my",
    "start": "1708450",
    "end": "1713849"
  },
  {
    "text": "data warehouse and amazon.com and you go to our storefront we don't allow you to access our distribution center right and",
    "start": "1713849",
    "end": "1719669"
  },
  {
    "text": "so actually is a real there's a warehouse in the sense that nobody's actually it's just the holding for everything",
    "start": "1719669",
    "end": "1725230"
  },
  {
    "text": "all of the data that we're pulling in the data lake is an ephemeral dynamic data storage for structured data and so",
    "start": "1725230",
    "end": "1732310"
  },
  {
    "text": "as that data set gets updated in my warehouse I can continually rebuild those data sets in different in different Stan she",
    "start": "1732310",
    "end": "1739300"
  },
  {
    "text": "ations right I can rebuild fact tables and graphs I might I might rebuild that fact table in a different in a different",
    "start": "1739300",
    "end": "1745480"
  },
  {
    "text": "shape based on some use case data sets are purpose-built for on the use case so",
    "start": "1745480",
    "end": "1750970"
  },
  {
    "text": "the right tool right like I said I might have communities of interest that want to see Network you know or want to do",
    "start": "1750970",
    "end": "1756910"
  },
  {
    "text": "network analysis against the graph analysis against that I might have others that want to do some other thing so I can instantiate that data in a lot",
    "start": "1756910",
    "end": "1763240"
  },
  {
    "text": "of different ways many of your own ratio of tools to data so I can use different",
    "start": "1763240",
    "end": "1768550"
  },
  {
    "text": "tools as many really as tools as I want to on the same set of data on the same and Stan she ation of that data and you",
    "start": "1768550",
    "end": "1775000"
  },
  {
    "text": "only pay for data processing as it's needed right in a lot of ways you're only paying for storage that's needed if you're if you're pulling all this out of",
    "start": "1775000",
    "end": "1781120"
  },
  {
    "text": "the warehouse and then destroying it as you're done with it and so when I'm done I can spin down all",
    "start": "1781120",
    "end": "1786790"
  },
  {
    "text": "of that stuff you know I might leave a red shift cluster up because I'm doing some kind of daily reporting or I you",
    "start": "1786790",
    "end": "1792820"
  },
  {
    "text": "know Neptuna I've loaded that in because I want to do something with that so I can leave all that up so lake formation",
    "start": "1792820",
    "end": "1798790"
  },
  {
    "text": "is let me kind of so the challenges you know is really around maintaining catalog configuring and managing access",
    "start": "1798790",
    "end": "1804220"
  },
  {
    "text": "control audit logging and a lot of this can take a while to build so that's why we came up with lake formation lake",
    "start": "1804220",
    "end": "1810100"
  },
  {
    "text": "formation is a conglomeration of different services that we have built together to deliver some capabilities",
    "start": "1810100",
    "end": "1816940"
  },
  {
    "text": "that aren't it's it's an economies of scale thing or kind of needs a scope thing I should say where they're able to",
    "start": "1816940",
    "end": "1823030"
  },
  {
    "text": "deliver services individually you couldn't build out and so really what it",
    "start": "1823030",
    "end": "1828100"
  },
  {
    "text": "you know tips typical steps of building a data Lake you get step storage the things I talked about in the pipeline",
    "start": "1828100",
    "end": "1833620"
  },
  {
    "text": "you're going to do cleansing your data you're going to configure your security policies and things like that which is really important I mean you're gonna",
    "start": "1833620",
    "end": "1840250"
  },
  {
    "text": "make it available right so how does it work right so the first thing I want to",
    "start": "1840250",
    "end": "1845740"
  },
  {
    "text": "ingest and register my data so if you go back to like your operational data store right you might be staging all of your",
    "start": "1845740",
    "end": "1851260"
  },
  {
    "text": "data from all your different sources in ODS and then you actually ingest that",
    "start": "1851260",
    "end": "1856540"
  },
  {
    "text": "data into your data Lake we'll talk about that in a minute security control you set up your permissions on the data in the glue",
    "start": "1856540",
    "end": "1862870"
  },
  {
    "text": "catalog you collaborate user data by allowing people to search your catalog and you can monitor and audit it because",
    "start": "1862870",
    "end": "1869410"
  },
  {
    "text": "we're logging everything that you do so a couple of some of the key concepts I'm going to touch on are blueprints and",
    "start": "1869410",
    "end": "1875710"
  },
  {
    "text": "importers these are templates for actually building your ETL and your schemas and your partition management we're actually the enhanced data catalog",
    "start": "1875710",
    "end": "1883360"
  },
  {
    "text": "so the glue catalog is being updated quite a bit to leverage some of the things I'll talk about you know enable",
    "start": "1883360",
    "end": "1888820"
  },
  {
    "text": "you to record more data tag objects at the database table and column level",
    "start": "1888820",
    "end": "1895140"
  },
  {
    "text": "eventually it will get to the row level which I think is really interesting where you know you'll be able to query",
    "start": "1895140",
    "end": "1900580"
  },
  {
    "text": "based on contents of ranges within you know within attributes and rows which is",
    "start": "1900580",
    "end": "1905590"
  },
  {
    "text": "kind of interesting in Ville transformations is probably one of the most important ones because as I talked about that 80% of the data pipeline is",
    "start": "1905590",
    "end": "1912820"
  },
  {
    "text": "in processing and so what ml transformations is taking some of our own internal algorithms and applying",
    "start": "1912820",
    "end": "1918820"
  },
  {
    "text": "that to your that we use internally applying that to your data set so you heard trying to talk about deduplication",
    "start": "1918820",
    "end": "1924970"
  },
  {
    "text": "and fuzzy logic and so we're starting to do is take a lot of that work out of the",
    "start": "1924970",
    "end": "1930460"
  },
  {
    "text": "manual process and actually applying machine learning against the data to be able to do a lot of that processing for",
    "start": "1930460",
    "end": "1936100"
  },
  {
    "text": "you record D dupe is the first one I'll talk a little bit about that too and then the security piece is really",
    "start": "1936100",
    "end": "1941230"
  },
  {
    "text": "important too because now you're able to secure your data in one place and your users in one place instead of through a",
    "start": "1941230",
    "end": "1947830"
  },
  {
    "text": "whole bunch of tools so which register hs3 forms the storage later for your lake",
    "start": "1947830",
    "end": "1953770"
  },
  {
    "text": "formation I mean everybody's pretty familiar with s3 or mostly familiar with s3 it's an object store store anything I",
    "start": "1953770",
    "end": "1960610"
  },
  {
    "text": "mean literally can store anything you register your buckets with they contain your data so you can use existing",
    "start": "1960610",
    "end": "1967180"
  },
  {
    "text": "buckets where you can ask Lake formation to create buckets for you and then your data is stored in your account you have",
    "start": "1967180",
    "end": "1973810"
  },
  {
    "text": "access to it we don't own it it's no lock-in so think about an object store there's I we don't really care how you",
    "start": "1973810",
    "end": "1980260"
  },
  {
    "text": "store your data as far as the format we wanted you to optimize it obviously but that's completely up to the customer so",
    "start": "1980260",
    "end": "1985780"
  },
  {
    "text": "there's no lock-in you can take your data in and out anytime you want to so easily load your data into the lake so",
    "start": "1985780",
    "end": "1992350"
  },
  {
    "text": "what we're developing is blueprints to be able to automate some of that process so some of the blueprints we have today",
    "start": "1992350",
    "end": "1998639"
  },
  {
    "text": "referral rora for RDS we've got some for Kinesis and cloud trail and what it is allows you to load your data in one shot",
    "start": "1998639",
    "end": "2005340"
  },
  {
    "text": "and then incrementally load data as the change as changes in the data occur the",
    "start": "2005340",
    "end": "2012000"
  },
  {
    "text": "blueprints allow you to do that you know point to the source tell us the location",
    "start": "2012000",
    "end": "2018990"
  },
  {
    "text": "to load your data lake and then specify how often you want to load it what the blueprint does is it discovers the",
    "start": "2018990",
    "end": "2024629"
  },
  {
    "text": "source tables and schemas are familiar with glue it actually creates the source table in Glu as an object meta object",
    "start": "2024629",
    "end": "2031190"
  },
  {
    "text": "automatically convert to the target data format glue can do a lot of conversions",
    "start": "2031190",
    "end": "2036840"
  },
  {
    "text": "down streams you can actually create like hard pay Park Hae files and things like that so depending on your use case you can use glue to create different",
    "start": "2036840",
    "end": "2043769"
  },
  {
    "text": "file formats automatically partition the data and keep that track of what you've",
    "start": "2043769",
    "end": "2049589"
  },
  {
    "text": "processed and you can customize all that as well so that's kind of the interesting thing about it too so more and more blueprints",
    "start": "2049589",
    "end": "2055950"
  },
  {
    "text": "will become available as time goes on so what does that look like how is that",
    "start": "2055950",
    "end": "2061050"
  },
  {
    "text": "kind of work you know the blueprints are built on a workflow built on the catalog which is your glue catalog it's going to",
    "start": "2061050",
    "end": "2067050"
  },
  {
    "text": "be connections databases tables that are going to be instantiated with glue jobs",
    "start": "2067050",
    "end": "2073648"
  },
  {
    "text": "and then the crawlers that are actually going to look on your data in the lake and then start building those things so",
    "start": "2073649",
    "end": "2080310"
  },
  {
    "text": "the enhanced data catalog you know enable users to record more metadata about their data so at the",
    "start": "2080310",
    "end": "2085770"
  },
  {
    "text": "table and column level so that it's searchable so if you're familiar with data catalogs you want you know that's",
    "start": "2085770",
    "end": "2091138"
  },
  {
    "text": "kind of a data stewards role within all of this so you've got people that are defining what is the data right well",
    "start": "2091139",
    "end": "2096599"
  },
  {
    "text": "that's a business and the data stewardship role search and collaborate",
    "start": "2096599",
    "end": "2101760"
  },
  {
    "text": "across multiple users so your your you",
    "start": "2101760",
    "end": "2106829"
  },
  {
    "text": "know data owners data stewards can add properties to tables and columns you can",
    "start": "2106829",
    "end": "2112140"
  },
  {
    "text": "also add sensitivity levels which is kind of interesting in a as part of the column definitions part of the column",
    "start": "2112140",
    "end": "2118319"
  },
  {
    "text": "properties and so if you have columns that may be sensitive within a table so like social security numbers for example",
    "start": "2118319",
    "end": "2124109"
  },
  {
    "text": "it's kind of an easy one right that's you know personally identifiable information you might have health records or something like that you could",
    "start": "2124109",
    "end": "2130109"
  },
  {
    "text": "also have your own you know sensitive levels for depending on your line of business or organization in what you're",
    "start": "2130109",
    "end": "2136319"
  },
  {
    "text": "doing and add those as part of the catalog so and that's going to float back into the security model right so at",
    "start": "2136319",
    "end": "2142740"
  },
  {
    "text": "the column level I may only allow users on the you know on a row when we do allow road based filtering in there to",
    "start": "2142740",
    "end": "2149460"
  },
  {
    "text": "only query data within a table at this sensitivity level well that's not there yet but that is on the roadmap to be",
    "start": "2149460",
    "end": "2156029"
  },
  {
    "text": "able to have row based query properties and attributes so MFA informations I",
    "start": "2156029",
    "end": "2161309"
  },
  {
    "text": "kind of touched on it's a it's applying algorithms machine learning models to",
    "start": "2161309",
    "end": "2167309"
  },
  {
    "text": "the data to be able to do a lot of the processing for you so the biggest one is deduplication is so it's applying fuzzy",
    "start": "2167309",
    "end": "2174150"
  },
  {
    "text": "logic to your data sets to be able to de doop a lot of that stuff so when you talk about entity resolution that's what",
    "start": "2174150",
    "end": "2180000"
  },
  {
    "text": "that is right it's saying I've got all these I exist in three different places it may be Stephen with a pH it might be",
    "start": "2180000",
    "end": "2187049"
  },
  {
    "text": "Steven with a V it might be Stephen with an A it might be you know Esteban I don't know any of those things right and",
    "start": "2187049",
    "end": "2193500"
  },
  {
    "text": "this is going to go through and look at attributes about me within those different data sets and make a",
    "start": "2193500",
    "end": "2199049"
  },
  {
    "text": "determination based on some logic I'm a model that's been developed as to you",
    "start": "2199049",
    "end": "2204180"
  },
  {
    "text": "know these you know five instances of me are actually the same person that's a",
    "start": "2204180",
    "end": "2209430"
  },
  {
    "text": "huge benefit from the manual process of doing that there's some other tools out there and stuff to do that today but",
    "start": "2209430",
    "end": "2215579"
  },
  {
    "text": "being able to do that as part of your flow and part of your pipeline and actually not having to manage it as you",
    "start": "2215579",
    "end": "2221519"
  },
  {
    "text": "know manage servers and things like that is is hugely beneficial to customers because it takes a lot of the work that's a manual process today so just",
    "start": "2221519",
    "end": "2228960"
  },
  {
    "text": "another comment on that I'm pretty excited about the the email transformations because that's been the long pole for a lot of customers in",
    "start": "2228960",
    "end": "2235829"
  },
  {
    "text": "implementing good data processing then the enhanced governments layer is really",
    "start": "2235829",
    "end": "2242190"
  },
  {
    "text": "important so there's actually a separate security model for the data in lake",
    "start": "2242190",
    "end": "2247319"
  },
  {
    "text": "formation that's separate from I am and so you assign I am principals will talk",
    "start": "2247319",
    "end": "2252750"
  },
  {
    "text": "about to users and roles to data lake formation but you're going to manage",
    "start": "2252750",
    "end": "2258590"
  },
  {
    "text": "the access to that within lake formation itself and so you can control data access with grant and control revoke",
    "start": "2258590",
    "end": "2265700"
  },
  {
    "text": "permissions you can specify permissions on tables and columns rather than on",
    "start": "2265700",
    "end": "2271250"
  },
  {
    "text": "buckets so I talked about that at the call at table level calm levels need to be row level you can easily view granted",
    "start": "2271250",
    "end": "2278300"
  },
  {
    "text": "policies to users and all of its auditable so everything we do is audited dry it with in AWS the API is",
    "start": "2278300",
    "end": "2283970"
  },
  {
    "text": "everything's very easy to audit so when your security guy or I a person comes you can here is everything so you can",
    "start": "2283970",
    "end": "2291530"
  },
  {
    "text": "also search and view permissions granted to users and you can revoke policies so the interesting thing is let's say you",
    "start": "2291530",
    "end": "2296780"
  },
  {
    "text": "have a user that's in lake formation and they've used EMR and I'm going to talk about how that works in a minute let's",
    "start": "2296780",
    "end": "2302540"
  },
  {
    "text": "say they use the company I don't have to go to all those tools to revoke permissions I just go here and revoke",
    "start": "2302540",
    "end": "2307760"
  },
  {
    "text": "permissions and then they can't access it anymore I'm gonna explain why in a minute that's a hugely different from",
    "start": "2307760",
    "end": "2313190"
  },
  {
    "text": "having to go to all these different systems to revoke permissions to be able to securely you know if somebody leaves",
    "start": "2313190",
    "end": "2319250"
  },
  {
    "text": "a company you have a security incident like that you know auditing monitoring real-time again we're logging everything",
    "start": "2319250",
    "end": "2325360"
  },
  {
    "text": "you know you can publish things to cloud watch if you're in a cloud watch it's you know it's where we're piping all of",
    "start": "2325360",
    "end": "2331130"
  },
  {
    "text": "our logs to you can set search you can also create a seam system if you want to stream that out of cloud watch it stays like Splunk or something like that we've",
    "start": "2331130",
    "end": "2338120"
  },
  {
    "text": "had a lot of customers are using Splunk for security incident in event management and so that all can be funneled into there as well so it's",
    "start": "2338120",
    "end": "2344060"
  },
  {
    "text": "really neat because you do a lot of interesting correlations once you get all that in one place so you secure once",
    "start": "2344060",
    "end": "2350120"
  },
  {
    "text": "access multiple ways so I actually set up my user access and Lake formation I don't set it up in the tools themselves",
    "start": "2350120",
    "end": "2355880"
  },
  {
    "text": "so as you know I have my data catalog my access control is applied against that data catalog the user tries to access",
    "start": "2355880",
    "end": "2363380"
  },
  {
    "text": "via one of the services so right now it's integrated tightly integrated with AWS services you know other vendors and",
    "start": "2363380",
    "end": "2370220"
  },
  {
    "text": "things I'm sure over time will start integrating their own products with that security model so services send",
    "start": "2370220",
    "end": "2375950"
  },
  {
    "text": "credentials to Lake formation based on you know on behalf of the user and then lake formation returns temporary",
    "start": "2375950",
    "end": "2382400"
  },
  {
    "text": "conditionals allowing them access and based on the control in the lake based off of the catalog that's there and so",
    "start": "2382400",
    "end": "2388520"
  },
  {
    "text": "again if you have a user leave or you have a security incident you could automate it probably to a certain extent",
    "start": "2388520",
    "end": "2393950"
  },
  {
    "text": "where if you had an incident you could take users they didn't have any access to the data even though they're still",
    "start": "2393950",
    "end": "2399350"
  },
  {
    "text": "sitting in the tooling or they still have all these tools that they're able to access so again I said grant tables",
    "start": "2399350",
    "end": "2405260"
  },
  {
    "text": "you know table permissions calling level permissions user to could be a super user user one could be you know have a",
    "start": "2405260",
    "end": "2411500"
  },
  {
    "text": "specific need and so you may only want to give them access to specific columns",
    "start": "2411500",
    "end": "2416510"
  },
  {
    "text": "like well we're going to expand this into row based at some point in the future where you know you might only",
    "start": "2416510",
    "end": "2422960"
  },
  {
    "text": "want to let a user see these rows based on this range of data and that's pretty interesting too because now you can",
    "start": "2422960",
    "end": "2429020"
  },
  {
    "text": "apply like security labels and things like that to it on a row base within the table itself so again on the security",
    "start": "2429020",
    "end": "2435740"
  },
  {
    "text": "workflow just you know if I queer you know say I run a query request access",
    "start": "2435740",
    "end": "2440870"
  },
  {
    "text": "for that query and say you know table or set of tables and late formation short-term security credentials get sent",
    "start": "2440870",
    "end": "2447620"
  },
  {
    "text": "back to the service the service requests the objects comprising that query that",
    "start": "2447620",
    "end": "2454520"
  },
  {
    "text": "table or tables based on the security credentials and then the object returns whatever it is back to the service",
    "start": "2454520",
    "end": "2461630"
  },
  {
    "text": "itself so principles can be I am users so if you're maybe with our I am product",
    "start": "2461630",
    "end": "2467270"
  },
  {
    "text": "or I am service I am roles also interesting is active directive fed actory directory Active Directory",
    "start": "2467270",
    "end": "2473420"
  },
  {
    "text": "Federation can also be principals so just a just a quick example of how to do",
    "start": "2473420",
    "end": "2479390"
  },
  {
    "text": "you know import data so when you go in the you know go in there you define your blueprint type import source",
    "start": "2479390",
    "end": "2487400"
  },
  {
    "text": "you know source path connect lieu connection so this is just kind of like this is the table I'm going to import",
    "start": "2487400",
    "end": "2492830"
  },
  {
    "text": "here's how the table if you haven't used glue it's you know it looks like a table right when you and there it's it's",
    "start": "2492830",
    "end": "2498610"
  },
  {
    "text": "logical right you have some underlying data structure that composes that table and so that's what that would look like",
    "start": "2498610",
    "end": "2503840"
  },
  {
    "text": "when you pull that in with bloom grant permissions to it and so you're granting",
    "start": "2503840",
    "end": "2509540"
  },
  {
    "text": "permissions to the principles you know different access to the tables just as an example that shows you know creating",
    "start": "2509540",
    "end": "2515120"
  },
  {
    "text": "tables or alter dropping but you can also grant access permissions to the tables and then run the query and athena",
    "start": "2515120",
    "end": "2520760"
  },
  {
    "text": "so i can be one user that has access to run this and i'm gonna see stuff right I might have another user that comes in",
    "start": "2520760",
    "end": "2528470"
  },
  {
    "text": "because they they still have access to Athena but they don't have access to the data so they can't see anything and so",
    "start": "2528470",
    "end": "2534530"
  },
  {
    "text": "that kind of talks about the security model where I'm securing the data with users I'm not securing the services",
    "start": "2534530",
    "end": "2539930"
  },
  {
    "text": "themselves and that's really important so a couple things about the pricing so there's no additional charges for late",
    "start": "2539930",
    "end": "2546109"
  },
  {
    "text": "formation you actually you just pay for the underlying service so you pay for s3 you pay for glue you pay for those",
    "start": "2546109",
    "end": "2551599"
  },
  {
    "text": "things but it's not there's no additional charge for late formation a couple of FAQ so it you know q2 19 so",
    "start": "2551599",
    "end": "2559960"
  },
  {
    "text": "it's when we're planning on GA data lineage is on the roadmap so I bring",
    "start": "2559960",
    "end": "2565490"
  },
  {
    "text": "that up because that's one that I actually am pretty stoked about as well because data lineage being able to trace",
    "start": "2565490",
    "end": "2570980"
  },
  {
    "text": "data back to the source is really important and so and we'll glue extend existing certifications yes is a short",
    "start": "2570980",
    "end": "2579200"
  },
  {
    "text": "answer of the question because late formation in and of itself is not a new service it's built off of services that already are FedRAMP approved or DoD srg",
    "start": "2579200",
    "end": "2586369"
  },
  {
    "text": "impact level approved so with that I appreciate everybody coming I will be in",
    "start": "2586369",
    "end": "2591950"
  },
  {
    "text": "the back outside if anybody has any questions thank you [Applause]",
    "start": "2591950",
    "end": "2600860"
  }
]