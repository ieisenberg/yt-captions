[
  {
    "text": "hi everyone and welcome to this short",
    "start": "2720",
    "end": "5120"
  },
  {
    "text": "snackable video about how to monitor the",
    "start": "5120",
    "end": "7120"
  },
  {
    "text": "health and performance of your Amazon",
    "start": "7120",
    "end": "8639"
  },
  {
    "text": "Elasticash serverless cache My name is",
    "start": "8639",
    "end": "10960"
  },
  {
    "text": "Damon Lyle I'm a principal solutions",
    "start": "10960",
    "end": "12880"
  },
  {
    "text": "architect here at AWS and I hope you'll",
    "start": "12880",
    "end": "15280"
  },
  {
    "text": "find this session interesting and can",
    "start": "15280",
    "end": "16960"
  },
  {
    "text": "immediately put it to use So let's dive",
    "start": "16960",
    "end": "19000"
  },
  {
    "text": "in Elasticash serverless is a big shift",
    "start": "19000",
    "end": "22160"
  },
  {
    "text": "from the traditional cluster management",
    "start": "22160",
    "end": "24080"
  },
  {
    "text": "approach There are no nodes to size no",
    "start": "24080",
    "end": "26960"
  },
  {
    "text": "shards to rebalance and it automatically",
    "start": "26960",
    "end": "29439"
  },
  {
    "text": "scales based on your application's needs",
    "start": "29439",
    "end": "31840"
  },
  {
    "text": "But just because it's serverless doesn't",
    "start": "31840",
    "end": "33600"
  },
  {
    "text": "mean you shouldn't know what's going on",
    "start": "33600",
    "end": "35040"
  },
  {
    "text": "under the hood Monitoring your cache is",
    "start": "35040",
    "end": "37440"
  },
  {
    "text": "still essential especially if you care",
    "start": "37440",
    "end": "39520"
  },
  {
    "text": "about application performance user",
    "start": "39520",
    "end": "41440"
  },
  {
    "text": "experience and cost efficiency And the",
    "start": "41440",
    "end": "43840"
  },
  {
    "text": "good news is AWS gives you the metrics",
    "start": "43840",
    "end": "46239"
  },
  {
    "text": "you need to stay ahead of issues",
    "start": "46239",
    "end": "47840"
  },
  {
    "text": "understand usage patterns and optimize",
    "start": "47840",
    "end": "49760"
  },
  {
    "text": "your design Now a quick note on",
    "start": "49760",
    "end": "51760"
  },
  {
    "text": "terminology before we dive in If you're",
    "start": "51760",
    "end": "54079"
  },
  {
    "text": "using Elasticash serverless today you're",
    "start": "54079",
    "end": "56480"
  },
  {
    "text": "almost certainly using Valkyrie Valky is",
    "start": "56480",
    "end": "59280"
  },
  {
    "text": "a drop-in replacement for Reddus OSS and",
    "start": "59280",
    "end": "61920"
  },
  {
    "text": "is backed by the Linux Foundation",
    "start": "61920",
    "end": "63520"
  },
  {
    "text": "ensuring it will remain open source",
    "start": "63520",
    "end": "65320"
  },
  {
    "text": "forever So I may use the terms",
    "start": "65320",
    "end": "67600"
  },
  {
    "text": "Elasticash and Valky interchangeably in",
    "start": "67600",
    "end": "70880"
  },
  {
    "text": "this video for simplicity's sake but in",
    "start": "70880",
    "end": "73280"
  },
  {
    "text": "reality Elasticash is a fully managed",
    "start": "73280",
    "end": "76080"
  },
  {
    "text": "service provided by Amazon Web Services",
    "start": "76080",
    "end": "79280"
  },
  {
    "text": "while Valky is one of several engines",
    "start": "79280",
    "end": "81280"
  },
  {
    "text": "that Elasticash supports",
    "start": "81280",
    "end": "83920"
  },
  {
    "text": "All right with that said the first",
    "start": "83920",
    "end": "85680"
  },
  {
    "text": "metric we'll discuss focuses on memory",
    "start": "85680",
    "end": "88360"
  },
  {
    "text": "capacity This metric is called bytes",
    "start": "88360",
    "end": "90720"
  },
  {
    "text": "used for cache and it's exactly what you",
    "start": "90720",
    "end": "92479"
  },
  {
    "text": "would think It is how much memory your",
    "start": "92479",
    "end": "94560"
  },
  {
    "text": "cache is using but not just for your",
    "start": "94560",
    "end": "96479"
  },
  {
    "text": "data It also includes Valky's metadata",
    "start": "96479",
    "end": "98960"
  },
  {
    "text": "buffers and so on Essentially everything",
    "start": "98960",
    "end": "101920"
  },
  {
    "text": "that Valky needs to operate not just the",
    "start": "101920",
    "end": "104079"
  },
  {
    "text": "data set itself So you can think of this",
    "start": "104079",
    "end": "106479"
  },
  {
    "text": "really as bytes used for Valkyrie In",
    "start": "106479",
    "end": "109439"
  },
  {
    "text": "this screenshot from Cloudatch you can",
    "start": "109439",
    "end": "111360"
  },
  {
    "text": "see that I ran a workload that",
    "start": "111360",
    "end": "112960"
  },
  {
    "text": "consistently added new keys into the",
    "start": "112960",
    "end": "114799"
  },
  {
    "text": "cache This cache is set to have a",
    "start": "114799",
    "end": "117119"
  },
  {
    "text": "maximum memory capacity of 1 GB So you",
    "start": "117119",
    "end": "119920"
  },
  {
    "text": "can see as it grew it never exceeded",
    "start": "119920",
    "end": "121920"
  },
  {
    "text": "that value Elasticash serverless will",
    "start": "121920",
    "end": "124560"
  },
  {
    "text": "automatically scale the cluster up to",
    "start": "124560",
    "end": "126240"
  },
  {
    "text": "the maximum memory limit you specify",
    "start": "126240",
    "end": "128720"
  },
  {
    "text": "You'll want to monitor this metric to",
    "start": "128720",
    "end": "130319"
  },
  {
    "text": "understand growth trends If you're",
    "start": "130319",
    "end": "132400"
  },
  {
    "text": "consistently close to your cap it might",
    "start": "132400",
    "end": "134239"
  },
  {
    "text": "be time to rethink what data you're",
    "start": "134239",
    "end": "135680"
  },
  {
    "text": "storing or raise that limit And if",
    "start": "135680",
    "end": "137680"
  },
  {
    "text": "you're constantly at the maximum you've",
    "start": "137680",
    "end": "139200"
  },
  {
    "text": "configured you're going to see evictions",
    "start": "139200",
    "end": "140879"
  },
  {
    "text": "when you attempt to write more data",
    "start": "140879",
    "end": "142720"
  },
  {
    "text": "Which leads us to the next topic",
    "start": "142720",
    "end": "144760"
  },
  {
    "text": "evictions As you add keys to Elasticash",
    "start": "144760",
    "end": "147520"
  },
  {
    "text": "the memory footprint will grow You have",
    "start": "147520",
    "end": "149440"
  },
  {
    "text": "the option to set a maximum memory limit",
    "start": "149440",
    "end": "151520"
  },
  {
    "text": "in serverless up to a maximum of 5 TB",
    "start": "151520",
    "end": "155040"
  },
  {
    "text": "and the cache will automatically scale",
    "start": "155040",
    "end": "156800"
  },
  {
    "text": "up to your defined limit if it needs to",
    "start": "156800",
    "end": "159200"
  },
  {
    "text": "But once it hits the limit that you",
    "start": "159200",
    "end": "160959"
  },
  {
    "text": "define Elasticash needs to make room And",
    "start": "160959",
    "end": "163920"
  },
  {
    "text": "that's where evictions come into play",
    "start": "163920",
    "end": "166160"
  },
  {
    "text": "You can see in this screenshot that we",
    "start": "166160",
    "end": "167840"
  },
  {
    "text": "included the evictions metric with the",
    "start": "167840",
    "end": "169599"
  },
  {
    "text": "memory metric This shows that as we",
    "start": "169599",
    "end": "172080"
  },
  {
    "text": "continued to write new data into the",
    "start": "172080",
    "end": "173920"
  },
  {
    "text": "cache the memory did not grow but",
    "start": "173920",
    "end": "176160"
  },
  {
    "text": "Elasticash did have to evict keys so it",
    "start": "176160",
    "end": "178879"
  },
  {
    "text": "could make room for the new data being",
    "start": "178879",
    "end": "180319"
  },
  {
    "text": "written Elasticash serverless uses a",
    "start": "180319",
    "end": "182879"
  },
  {
    "text": "fixed eviction policy called volatile",
    "start": "182879",
    "end": "185159"
  },
  {
    "text": "LRU or least recently used which is",
    "start": "185159",
    "end": "188560"
  },
  {
    "text": "important to remember That means when",
    "start": "188560",
    "end": "190400"
  },
  {
    "text": "memory is full it will start evicting",
    "start": "190400",
    "end": "192080"
  },
  {
    "text": "the least recently used keys but only if",
    "start": "192080",
    "end": "194480"
  },
  {
    "text": "they have a time to live or TTL set on",
    "start": "194480",
    "end": "198159"
  },
  {
    "text": "them So keys without a TTL assigned will",
    "start": "198159",
    "end": "201440"
  },
  {
    "text": "not be evicted which may lead to",
    "start": "201440",
    "end": "203200"
  },
  {
    "text": "unnecessary memory usage But what if",
    "start": "203200",
    "end": "205920"
  },
  {
    "text": "memory fills up with non-expiring keys",
    "start": "205920",
    "end": "208319"
  },
  {
    "text": "which means they don't have time to live",
    "start": "208319",
    "end": "210120"
  },
  {
    "text": "values you'll likely run into out of",
    "start": "210120",
    "end": "212799"
  },
  {
    "text": "memory errors which would then cause",
    "start": "212799",
    "end": "214640"
  },
  {
    "text": "write commands to fail So what should",
    "start": "214640",
    "end": "216959"
  },
  {
    "text": "you watch for if evictions are low and",
    "start": "216959",
    "end": "219200"
  },
  {
    "text": "occasional that's fine But sustained or",
    "start": "219200",
    "end": "221599"
  },
  {
    "text": "sudden spikes especially during peak app",
    "start": "221599",
    "end": "223920"
  },
  {
    "text": "traffic are worth investigating Look at",
    "start": "223920",
    "end": "226319"
  },
  {
    "text": "how much memory you're using how often",
    "start": "226319",
    "end": "228159"
  },
  {
    "text": "you're writing and whether your keys are",
    "start": "228159",
    "end": "230000"
  },
  {
    "text": "configured with appropriate time to live",
    "start": "230000",
    "end": "231920"
  },
  {
    "text": "values Evictions are your early warning",
    "start": "231920",
    "end": "234560"
  },
  {
    "text": "system that your working set is getting",
    "start": "234560",
    "end": "236319"
  },
  {
    "text": "too big for your memory ceiling Let's",
    "start": "236319",
    "end": "238480"
  },
  {
    "text": "move on to see what's actually causing",
    "start": "238480",
    "end": "240000"
  },
  {
    "text": "that churn For that we look at get type",
    "start": "240000",
    "end": "242400"
  },
  {
    "text": "commands and settype commands These",
    "start": "242400",
    "end": "244799"
  },
  {
    "text": "metrics show you the number of read and",
    "start": "244799",
    "end": "246879"
  },
  {
    "text": "write operations your cache is handling",
    "start": "246879",
    "end": "249360"
  },
  {
    "text": "Things like get het and lrange for reads",
    "start": "249360",
    "end": "252720"
  },
  {
    "text": "and set h set and s add for writes If",
    "start": "252720",
    "end": "256400"
  },
  {
    "text": "you see high read volume you'll want to",
    "start": "256400",
    "end": "258160"
  },
  {
    "text": "check that your cache hit rate is",
    "start": "258160",
    "end": "259519"
  },
  {
    "text": "staying healthy which we'll look at on",
    "start": "259519",
    "end": "261040"
  },
  {
    "text": "the next slide But if you see a large",
    "start": "261040",
    "end": "263199"
  },
  {
    "text": "drop in get type commands where it would",
    "start": "263199",
    "end": "264960"
  },
  {
    "text": "normally be high that might be a traffic",
    "start": "264960",
    "end": "267199"
  },
  {
    "text": "routing issue or maybe a service is down",
    "start": "267199",
    "end": "269360"
  },
  {
    "text": "that is running your application With",
    "start": "269360",
    "end": "271440"
  },
  {
    "text": "settype commands if you're seeing high",
    "start": "271440",
    "end": "273440"
  },
  {
    "text": "write traffic especially sustained",
    "start": "273440",
    "end": "275280"
  },
  {
    "text": "writes with no time to live values",
    "start": "275280",
    "end": "277280"
  },
  {
    "text": "associated with them that could lead to",
    "start": "277280",
    "end": "279199"
  },
  {
    "text": "memory pressure and eventually trigger",
    "start": "279199",
    "end": "281080"
  },
  {
    "text": "evictions Keep in mind you might have a",
    "start": "281080",
    "end": "283280"
  },
  {
    "text": "batch job that runs on a regular basis",
    "start": "283280",
    "end": "285759"
  },
  {
    "text": "So this number may not always be",
    "start": "285759",
    "end": "287880"
  },
  {
    "text": "consistent You need to understand your",
    "start": "287880",
    "end": "289919"
  },
  {
    "text": "workload first to know whether this is",
    "start": "289919",
    "end": "292000"
  },
  {
    "text": "abnormal behavior or not Watch for",
    "start": "292000",
    "end": "294560"
  },
  {
    "text": "changes here over time These two metrics",
    "start": "294560",
    "end": "297040"
  },
  {
    "text": "also give you a good feel to know if",
    "start": "297040",
    "end": "298560"
  },
  {
    "text": "your workload is either read heavy or",
    "start": "298560",
    "end": "300800"
  },
  {
    "text": "write heavy and tell a kind of story of",
    "start": "300800",
    "end": "303040"
  },
  {
    "text": "what your app is doing with the cache",
    "start": "303040",
    "end": "305199"
  },
  {
    "text": "Next up are those reads and writes",
    "start": "305199",
    "end": "307199"
  },
  {
    "text": "actually effective and if so how",
    "start": "307199",
    "end": "309360"
  },
  {
    "text": "effective are they if you need to check",
    "start": "309360",
    "end": "311520"
  },
  {
    "text": "at a glance to see how effective your",
    "start": "311520",
    "end": "313120"
  },
  {
    "text": "cache is you can look at the cache hit",
    "start": "313120",
    "end": "315199"
  },
  {
    "text": "rate metric Here you can see that the",
    "start": "315199",
    "end": "317280"
  },
  {
    "text": "cache hit rate starts at zero and",
    "start": "317280",
    "end": "319360"
  },
  {
    "text": "steadily climbs until it reaches one",
    "start": "319360",
    "end": "321120"
  },
  {
    "text": "which is a 100% cache hit rate For most",
    "start": "321120",
    "end": "323919"
  },
  {
    "text": "workloads you want that hit rate to be",
    "start": "323919",
    "end": "325520"
  },
  {
    "text": "above 0.9 or above 90% if you can",
    "start": "325520",
    "end": "328880"
  },
  {
    "text": "Anything lower and you might be writing",
    "start": "328880",
    "end": "330800"
  },
  {
    "text": "data that never gets reused or evicting",
    "start": "330800",
    "end": "333120"
  },
  {
    "text": "too early We calculate hit rate with",
    "start": "333120",
    "end": "335680"
  },
  {
    "text": "this formula Cash hit rate equals cash",
    "start": "335680",
    "end": "338560"
  },
  {
    "text": "hits divided by the sum of cash hits",
    "start": "338560",
    "end": "340639"
  },
  {
    "text": "plus cash misses These two metrics cash",
    "start": "340639",
    "end": "344080"
  },
  {
    "text": "hits and cash misses tell you how",
    "start": "344080",
    "end": "346400"
  },
  {
    "text": "effective your cash really is A hit",
    "start": "346400",
    "end": "348960"
  },
  {
    "text": "means Elasticash had the data in memory",
    "start": "348960",
    "end": "351759"
  },
  {
    "text": "and is exactly what we want A miss means",
    "start": "351759",
    "end": "354880"
  },
  {
    "text": "it wasn't in memory If you get a cache",
    "start": "354880",
    "end": "356800"
  },
  {
    "text": "miss that isn't necessarily unexpected",
    "start": "356800",
    "end": "359039"
  },
  {
    "text": "for a caching use case That's because",
    "start": "359039",
    "end": "361039"
  },
  {
    "text": "your application may go to another",
    "start": "361039",
    "end": "362639"
  },
  {
    "text": "source to fetch the data and then",
    "start": "362639",
    "end": "364479"
  },
  {
    "text": "refreshes the cache with new results",
    "start": "364479",
    "end": "366560"
  },
  {
    "text": "Though this process will certainly be",
    "start": "366560",
    "end": "368000"
  },
  {
    "text": "slower than if it had been in cache in",
    "start": "368000",
    "end": "369759"
  },
  {
    "text": "the first place But if your use case is",
    "start": "369759",
    "end": "372000"
  },
  {
    "text": "something other than caching query",
    "start": "372000",
    "end": "373680"
  },
  {
    "text": "results maybe it's a session store a",
    "start": "373680",
    "end": "375919"
  },
  {
    "text": "geospatial index a leaderboard a",
    "start": "375919",
    "end": "378160"
  },
  {
    "text": "streaming use case or something else",
    "start": "378160",
    "end": "380080"
  },
  {
    "text": "this is something to watch out for",
    "start": "380080",
    "end": "381840"
  },
  {
    "text": "Remember when we talked about evictions",
    "start": "381840",
    "end": "383280"
  },
  {
    "text": "a few minutes ago consider this If your",
    "start": "383280",
    "end": "385840"
  },
  {
    "text": "cash miss rate is high and your eviction",
    "start": "385840",
    "end": "388000"
  },
  {
    "text": "rate is also high that might mean your",
    "start": "388000",
    "end": "390160"
  },
  {
    "text": "cash is churning too fast Also if your",
    "start": "390160",
    "end": "392720"
  },
  {
    "text": "miss rate is high but evictions are low",
    "start": "392720",
    "end": "394880"
  },
  {
    "text": "you might not be caching enough data to",
    "start": "394880",
    "end": "396560"
  },
  {
    "text": "begin with As with many things in life",
    "start": "396560",
    "end": "398560"
  },
  {
    "text": "and especially in monitoring systems one",
    "start": "398560",
    "end": "401360"
  },
  {
    "text": "data point should not be considered in a",
    "start": "401360",
    "end": "403240"
  },
  {
    "text": "vacuum Take a more holistic approach and",
    "start": "403240",
    "end": "405919"
  },
  {
    "text": "combine metrics to get a better picture",
    "start": "405919",
    "end": "407680"
  },
  {
    "text": "of the health of your cache Now let's",
    "start": "407680",
    "end": "409680"
  },
  {
    "text": "talk about latency which is a hot topic",
    "start": "409680",
    "end": "412240"
  },
  {
    "text": "when discussing elasticashe because it's",
    "start": "412240",
    "end": "414319"
  },
  {
    "text": "incredibly fast where even a millisecond",
    "start": "414319",
    "end": "417199"
  },
  {
    "text": "of network latency can have a dramatic",
    "start": "417199",
    "end": "419400"
  },
  {
    "text": "impact In this example we have our",
    "start": "419400",
    "end": "421840"
  },
  {
    "text": "applications running on Amazon EC2 with",
    "start": "421840",
    "end": "424720"
  },
  {
    "text": "a Valky client First these clients make",
    "start": "424720",
    "end": "427520"
  },
  {
    "text": "a request Now the first latency related",
    "start": "427520",
    "end": "430000"
  },
  {
    "text": "issue we face is the network hop from",
    "start": "430000",
    "end": "432479"
  },
  {
    "text": "the application to the service This is",
    "start": "432479",
    "end": "434720"
  },
  {
    "text": "usually a few hundred microsconds one",
    "start": "434720",
    "end": "436479"
  },
  {
    "text": "way Next the request arrives at the",
    "start": "436479",
    "end": "438800"
  },
  {
    "text": "elasticasheach node responsible for the",
    "start": "438800",
    "end": "440560"
  },
  {
    "text": "request The request goes through the",
    "start": "440560",
    "end": "442479"
  },
  {
    "text": "network stack and kernel Then it gets",
    "start": "442479",
    "end": "444560"
  },
  {
    "text": "put into client input buffers by a",
    "start": "444560",
    "end": "446400"
  },
  {
    "text": "multi-threaded IO model At this point we",
    "start": "446400",
    "end": "448880"
  },
  {
    "text": "start timers for two metrics The first",
    "start": "448880",
    "end": "451039"
  },
  {
    "text": "is called successful read request",
    "start": "451039",
    "end": "452720"
  },
  {
    "text": "latency and the second is successful",
    "start": "452720",
    "end": "455280"
  },
  {
    "text": "write request latency Both metrics",
    "start": "455280",
    "end": "458080"
  },
  {
    "text": "measure the total time spent within the",
    "start": "458080",
    "end": "459599"
  },
  {
    "text": "elasticasheach engine itself not",
    "start": "459599",
    "end": "461599"
  },
  {
    "text": "including operating system overhead or",
    "start": "461599",
    "end": "463599"
  },
  {
    "text": "network latency We'll come back to this",
    "start": "463599",
    "end": "465680"
  },
  {
    "text": "in a moment The next step is when the",
    "start": "465680",
    "end": "467759"
  },
  {
    "text": "command actually starts to execute",
    "start": "467759",
    "end": "469360"
  },
  {
    "text": "within the engine As an example a simple",
    "start": "469360",
    "end": "471840"
  },
  {
    "text": "operation like a get or a set will",
    "start": "471840",
    "end": "474479"
  },
  {
    "text": "usually take just a few micros",
    "start": "474479",
    "end": "476080"
  },
  {
    "text": "secondsonds to complete Once it's done",
    "start": "476080",
    "end": "478240"
  },
  {
    "text": "processing it gets sent to the client",
    "start": "478240",
    "end": "480000"
  },
  {
    "text": "output buffers through the same",
    "start": "480000",
    "end": "481520"
  },
  {
    "text": "multi-threaded IO model that we used for",
    "start": "481520",
    "end": "483919"
  },
  {
    "text": "the client input buffers Once the",
    "start": "483919",
    "end": "486240"
  },
  {
    "text": "response leaves the buffer and gets sent",
    "start": "486240",
    "end": "488080"
  },
  {
    "text": "to the kernel for outbound delivery",
    "start": "488080",
    "end": "490479"
  },
  {
    "text": "that's when we stop the timer for both",
    "start": "490479",
    "end": "492160"
  },
  {
    "text": "request latency metrics After that the",
    "start": "492160",
    "end": "495039"
  },
  {
    "text": "response then has to go through the",
    "start": "495039",
    "end": "496479"
  },
  {
    "text": "kernel and network stack again and",
    "start": "496479",
    "end": "498879"
  },
  {
    "text": "importantly make another network hop",
    "start": "498879",
    "end": "501120"
  },
  {
    "text": "back to the client So you have two",
    "start": "501120",
    "end": "503440"
  },
  {
    "text": "metrics you can monitor in Cloudatch for",
    "start": "503440",
    "end": "505360"
  },
  {
    "text": "serverless latency which are successful",
    "start": "505360",
    "end": "507680"
  },
  {
    "text": "read request latency and successful",
    "start": "507680",
    "end": "510319"
  },
  {
    "text": "write request latency A healthy system",
    "start": "510319",
    "end": "513120"
  },
  {
    "text": "with a normal load will usually be",
    "start": "513120",
    "end": "515360"
  },
  {
    "text": "around 1 to 2 milliseconds Now to be",
    "start": "515360",
    "end": "517839"
  },
  {
    "text": "clear only your application sees the",
    "start": "517839",
    "end": "520000"
  },
  {
    "text": "roundtrip latency The service of course",
    "start": "520000",
    "end": "522560"
  },
  {
    "text": "doesn't have visibility into what the",
    "start": "522560",
    "end": "524880"
  },
  {
    "text": "total roundtrip latency is since it",
    "start": "524880",
    "end": "527200"
  },
  {
    "text": "doesn't know how long the request took",
    "start": "527200",
    "end": "528720"
  },
  {
    "text": "to get to it or how long it took the",
    "start": "528720",
    "end": "530640"
  },
  {
    "text": "response to arrive at the client So if",
    "start": "530640",
    "end": "532800"
  },
  {
    "text": "cloudatch is showing 1 to 2 milliseconds",
    "start": "532800",
    "end": "535200"
  },
  {
    "text": "but your application is seeing 10",
    "start": "535200",
    "end": "536959"
  },
  {
    "text": "milliseconds for a round trip it means",
    "start": "536959",
    "end": "539040"
  },
  {
    "text": "that the 8 millisecond difference is",
    "start": "539040",
    "end": "540720"
  },
  {
    "text": "coming from something else All right",
    "start": "540720",
    "end": "543519"
  },
  {
    "text": "since we've talked about server and",
    "start": "543519",
    "end": "545040"
  },
  {
    "text": "client latency let's talk now about",
    "start": "545040",
    "end": "547040"
  },
  {
    "text": "client connections themselves We have",
    "start": "547040",
    "end": "549120"
  },
  {
    "text": "two metrics to look at current",
    "start": "549120",
    "end": "551040"
  },
  {
    "text": "connections and new connections Current",
    "start": "551040",
    "end": "553519"
  },
  {
    "text": "connections is pretty straightforward It",
    "start": "553519",
    "end": "555519"
  },
  {
    "text": "shows you how many active connections",
    "start": "555519",
    "end": "557040"
  },
  {
    "text": "your cluster currently has open",
    "start": "557040",
    "end": "559360"
  },
  {
    "text": "Elasticash is built to handle a very",
    "start": "559360",
    "end": "561519"
  },
  {
    "text": "large number of concurrent connections",
    "start": "561519",
    "end": "563279"
  },
  {
    "text": "up to 65,000 per serverless cache That's",
    "start": "563279",
    "end": "566000"
  },
  {
    "text": "a lot but it's still a ceiling Ideally",
    "start": "566000",
    "end": "568399"
  },
  {
    "text": "your current connections metric stays",
    "start": "568399",
    "end": "569920"
  },
  {
    "text": "fairly consistent below the 65,000",
    "start": "569920",
    "end": "572480"
  },
  {
    "text": "connection limit Taking a look at our",
    "start": "572480",
    "end": "574640"
  },
  {
    "text": "Cloudatch screenshot here we see that",
    "start": "574640",
    "end": "576640"
  },
  {
    "text": "there are consistently just a handful of",
    "start": "576640",
    "end": "578640"
  },
  {
    "text": "current connections It's a good sign",
    "start": "578640",
    "end": "580720"
  },
  {
    "text": "that the number of current connections",
    "start": "580720",
    "end": "582080"
  },
  {
    "text": "whatever that number is is fairly",
    "start": "582080",
    "end": "584000"
  },
  {
    "text": "constant It means we always have a few",
    "start": "584000",
    "end": "585920"
  },
  {
    "text": "clients connected which we would expect",
    "start": "585920",
    "end": "588160"
  },
  {
    "text": "This should roughly match the total",
    "start": "588160",
    "end": "589519"
  },
  {
    "text": "number of applications you have",
    "start": "589519",
    "end": "590800"
  },
  {
    "text": "accessing the cache or the number of",
    "start": "590800",
    "end": "592880"
  },
  {
    "text": "connections per application Conversely",
    "start": "592880",
    "end": "595519"
  },
  {
    "text": "if this metric suddenly drops especially",
    "start": "595519",
    "end": "598000"
  },
  {
    "text": "during normal usage hours it might",
    "start": "598000",
    "end": "600000"
  },
  {
    "text": "indicate network issues timeouts or",
    "start": "600000",
    "end": "602560"
  },
  {
    "text": "authentication failures It might even",
    "start": "602560",
    "end": "604959"
  },
  {
    "text": "mean users aren't able to access their",
    "start": "604959",
    "end": "606720"
  },
  {
    "text": "data So this is a good metric to alarm",
    "start": "606720",
    "end": "608800"
  },
  {
    "text": "on as well This metric doesn't tell the",
    "start": "608800",
    "end": "611360"
  },
  {
    "text": "whole story though It only captures the",
    "start": "611360",
    "end": "613360"
  },
  {
    "text": "number of connections at the time the",
    "start": "613360",
    "end": "614640"
  },
  {
    "text": "metric was captured But what if there",
    "start": "614640",
    "end": "616560"
  },
  {
    "text": "was an error in the application maybe it",
    "start": "616560",
    "end": "618959"
  },
  {
    "text": "continuously opens a connection",
    "start": "618959",
    "end": "620800"
  },
  {
    "text": "immediately closes it for some reason",
    "start": "620800",
    "end": "622560"
  },
  {
    "text": "over and over In that case we won't see",
    "start": "622560",
    "end": "624959"
  },
  {
    "text": "a big change in the current connections",
    "start": "624959",
    "end": "626399"
  },
  {
    "text": "metric That's because this is a kind of",
    "start": "626399",
    "end": "628560"
  },
  {
    "text": "point in time snapshot of that metric So",
    "start": "628560",
    "end": "631279"
  },
  {
    "text": "whenever the metric was recorded that's",
    "start": "631279",
    "end": "633200"
  },
  {
    "text": "how many existing connections were in",
    "start": "633200",
    "end": "634720"
  },
  {
    "text": "place But luckily for us Elasticash has",
    "start": "634720",
    "end": "637680"
  },
  {
    "text": "another metric called new connections",
    "start": "637680",
    "end": "639839"
  },
  {
    "text": "which actually counts the number of",
    "start": "639839",
    "end": "641360"
  },
  {
    "text": "connections that have occurred over a",
    "start": "641360",
    "end": "642959"
  },
  {
    "text": "span of time Now in this new diagram we",
    "start": "642959",
    "end": "645600"
  },
  {
    "text": "blended in new connections with current",
    "start": "645600",
    "end": "647519"
  },
  {
    "text": "connections and we see a much different",
    "start": "647519",
    "end": "649760"
  },
  {
    "text": "picture We're seeing almost 2,500 new",
    "start": "649760",
    "end": "652640"
  },
  {
    "text": "connections every minute It's so high in",
    "start": "652640",
    "end": "655120"
  },
  {
    "text": "fact that we can just barely see the",
    "start": "655120",
    "end": "656880"
  },
  {
    "text": "current connections line near the bottom",
    "start": "656880",
    "end": "659399"
  },
  {
    "text": "2500 new connections per minute is equal",
    "start": "659399",
    "end": "662240"
  },
  {
    "text": "to about 40 new connections every second",
    "start": "662240",
    "end": "664640"
  },
  {
    "text": "That's not good for your application but",
    "start": "664640",
    "end": "666720"
  },
  {
    "text": "even more importantly that's not good",
    "start": "666720",
    "end": "668320"
  },
  {
    "text": "for your customers If you see sudden",
    "start": "668320",
    "end": "670320"
  },
  {
    "text": "spikes in this metric it might mean a",
    "start": "670320",
    "end": "672480"
  },
  {
    "text": "misbehaving app that isn't reusing",
    "start": "672480",
    "end": "674480"
  },
  {
    "text": "connections properly It might also mean",
    "start": "674480",
    "end": "676800"
  },
  {
    "text": "that there was a large disconnect for",
    "start": "676800",
    "end": "678640"
  },
  {
    "text": "some reason and a large number of",
    "start": "678640",
    "end": "680240"
  },
  {
    "text": "clients are attempting to reconnect A",
    "start": "680240",
    "end": "682720"
  },
  {
    "text": "best practice in this situation is to",
    "start": "682720",
    "end": "684800"
  },
  {
    "text": "modify your client's connection logic to",
    "start": "684800",
    "end": "687200"
  },
  {
    "text": "implement an exponential backoff",
    "start": "687200",
    "end": "689040"
  },
  {
    "text": "approach This way when it times out",
    "start": "689040",
    "end": "690959"
  },
  {
    "text": "trying to connect it will pause longer",
    "start": "690959",
    "end": "692880"
  },
  {
    "text": "between each retry Next up is Elasticash",
    "start": "692880",
    "end": "696079"
  },
  {
    "text": "processing units or",
    "start": "696079",
    "end": "698279"
  },
  {
    "text": "ECPUs ECPUs are a serverless specific",
    "start": "698279",
    "end": "701519"
  },
  {
    "text": "metric that replaces traditional CPU",
    "start": "701519",
    "end": "703680"
  },
  {
    "text": "usage They represent a blend of command",
    "start": "703680",
    "end": "706240"
  },
  {
    "text": "execution and network transfer load This",
    "start": "706240",
    "end": "709040"
  },
  {
    "text": "is the metric customers will want to",
    "start": "709040",
    "end": "710959"
  },
  {
    "text": "track when considering their budget as",
    "start": "710959",
    "end": "713279"
  },
  {
    "text": "it is directly tied to the cost of",
    "start": "713279",
    "end": "714959"
  },
  {
    "text": "serverless Our documentation and pricing",
    "start": "714959",
    "end": "717839"
  },
  {
    "text": "page have several examples of what",
    "start": "717839",
    "end": "719839"
  },
  {
    "text": "constitutes an",
    "start": "719839",
    "end": "721079"
  },
  {
    "text": "eCPU But in short reads and writes",
    "start": "721079",
    "end": "724320"
  },
  {
    "text": "require one eCPU for each kilob of data",
    "start": "724320",
    "end": "727160"
  },
  {
    "text": "transferred For example a get command",
    "start": "727160",
    "end": "730000"
  },
  {
    "text": "that transfers 3.2 kilob of data will",
    "start": "730000",
    "end": "732880"
  },
  {
    "text": "consume 3.2",
    "start": "732880",
    "end": "735079"
  },
  {
    "text": "ecpus Commands that require additional",
    "start": "735079",
    "end": "737839"
  },
  {
    "text": "vCPU time or transfer more than 1 kilob",
    "start": "737839",
    "end": "740720"
  },
  {
    "text": "of data will consume proportionally more",
    "start": "740720",
    "end": "742959"
  },
  {
    "text": "eCPUs In the screenshot here we can see",
    "start": "742959",
    "end": "745680"
  },
  {
    "text": "that depending on the minute we're",
    "start": "745680",
    "end": "747360"
  },
  {
    "text": "getting anywhere from 4,000 eCPUs to up",
    "start": "747360",
    "end": "750560"
  },
  {
    "text": "to 215,000 eCPUs Again this metric",
    "start": "750560",
    "end": "754639"
  },
  {
    "text": "reflects both processing time and data",
    "start": "754639",
    "end": "756920"
  },
  {
    "text": "transferred Higher eCPU usage means your",
    "start": "756920",
    "end": "759920"
  },
  {
    "text": "cache is working harder and keep in mind",
    "start": "759920",
    "end": "762240"
  },
  {
    "text": "that isn't a bad thing You want your",
    "start": "762240",
    "end": "764480"
  },
  {
    "text": "cache active so that it relieves",
    "start": "764480",
    "end": "766480"
  },
  {
    "text": "pressure from slower or overworked",
    "start": "766480",
    "end": "768399"
  },
  {
    "text": "backend systems All right During this",
    "start": "768399",
    "end": "771200"
  },
  {
    "text": "video we've covered several metrics that",
    "start": "771200",
    "end": "773040"
  },
  {
    "text": "give you visibility into the health",
    "start": "773040",
    "end": "775040"
  },
  {
    "text": "performance and capacity of your",
    "start": "775040",
    "end": "776959"
  },
  {
    "text": "Elasticash serverless cache From traffic",
    "start": "776959",
    "end": "779760"
  },
  {
    "text": "and latency to memory and processing the",
    "start": "779760",
    "end": "782800"
  },
  {
    "text": "system does the scaling and management",
    "start": "782800",
    "end": "784720"
  },
  {
    "text": "for you but your insight into these",
    "start": "784720",
    "end": "786800"
  },
  {
    "text": "metrics helps you make smart proactive",
    "start": "786800",
    "end": "789160"
  },
  {
    "text": "decisions I'd encourage you to visit our",
    "start": "789160",
    "end": "791360"
  },
  {
    "text": "documentation where you can see even",
    "start": "791360",
    "end": "792959"
  },
  {
    "text": "more metrics to monitor and why they're",
    "start": "792959",
    "end": "794920"
  },
  {
    "text": "important Thanks again for joining me I",
    "start": "794920",
    "end": "797279"
  },
  {
    "text": "hope this gave you a solid foundation to",
    "start": "797279",
    "end": "798800"
  },
  {
    "text": "build from So keep monitoring keep",
    "start": "798800",
    "end": "801279"
  },
  {
    "text": "optimizing and we'll see you next time",
    "start": "801279",
    "end": "805720"
  }
]