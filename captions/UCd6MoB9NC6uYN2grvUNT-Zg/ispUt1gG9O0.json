[
  {
    "text": "In this video, you'll see how to extend \nyour data warehouse to your data lake.",
    "start": "80",
    "end": "3695"
  },
  {
    "text": "Using a modern data architecture,",
    "start": "4160",
    "end": "6000"
  },
  {
    "text": "you can move data from Amazon Redshift into an\nAmazon Simple Storage Service (Amazon S3) data lake,",
    "start": "6000",
    "end": "11770"
  },
  {
    "text": "where it can be used to build visualizations with Amazon QuickSight and\ncreate machine learning (ML) models with Amazon SageMaker.",
    "start": "11770",
    "end": "18913"
  },
  {
    "text": "To get started, let’s navigate to Amazon Redshift.",
    "start": "20064",
    "end": "22703"
  },
  {
    "text": "For our purposes, we already \nhave a two-node Redshift cluster.",
    "start": "25133",
    "end": "28224"
  },
  {
    "text": "Let's open the Redshift query \neditor to review our infrastructure.",
    "start": "28864",
    "end": "31904"
  },
  {
    "text": "We’ll begin with this node.",
    "start": "33724",
    "end": "35050"
  },
  {
    "text": "Nyctaxi is the name of our database.",
    "start": "35451",
    "end": "37697"
  },
  {
    "text": "Within nyctaxi, we see a spectrum schema and taxi schema.",
    "start": "37873",
    "end": "41546"
  },
  {
    "text": "Next, let’s go to AWS Glue,",
    "start": "43840",
    "end": "45916"
  },
  {
    "text": "where we have a job in place to load data from\nthe S3 bucket into the Redshift database.",
    "start": "45916",
    "end": "50185"
  },
  {
    "text": "The job reads SQL files in the S3 bucket, \nloads them into the Redshift data warehouse,",
    "start": "55543",
    "end": "60263"
  },
  {
    "text": "and then transmits them \nback to S3 as Parquet files.",
    "start": "60263",
    "end": "63205"
  },
  {
    "text": "Let's look at the Glue job in AWS \nStep Functions Workflow Studio.",
    "start": "64364",
    "end": "67977"
  },
  {
    "text": "In the first node is the ReadFilterJob that reads and loads data\nfrom the S3 bucket into the nyctaxi taxi schema as SQL files.",
    "start": "74294",
    "end": "81667"
  },
  {
    "text": "Let's look at the script.",
    "start": "82534",
    "end": "83511"
  },
  {
    "text": "This code creates a temporary table called NYC green taxi\nand the data is then loaded into the taxi schema.",
    "start": "86552",
    "end": "92485"
  },
  {
    "text": "The ReportJob node transforms the material \nin the nyctaxi database into Parquet format",
    "start": "94612",
    "end": "99812"
  },
  {
    "text": "and unloads the files back into the S3 bucket.",
    "start": "99812",
    "end": "102144"
  },
  {
    "text": "Here you can see the code for taking \nthe data from the NYC green taxi table",
    "start": "104733",
    "end": "108493"
  },
  {
    "text": "and unloading it into the \nS3 bucket as a Parquet file.",
    "start": "108493",
    "end": "111184"
  },
  {
    "text": "Let's apply and exit the workflow.",
    "start": "113280",
    "end": "115040"
  },
  {
    "text": "Now we can return to our \nstate machine and execute it.",
    "start": "117902",
    "end": "120382"
  },
  {
    "text": "Let’s return to the AWS Glue console to \nconfirm the completion of both job runs.",
    "start": "126000",
    "end": "130224"
  },
  {
    "text": "Here you can see that both jobs succeeded.",
    "start": "134095",
    "end": "136175"
  },
  {
    "text": "Now we'll go to the Redshift query editor \nto run a query on the table we just created.",
    "start": "137282",
    "end": "141314"
  },
  {
    "text": "Here we can see the table data.",
    "start": "143625",
    "end": "145065"
  },
  {
    "text": "Now let's access the S3 bucket for our data lake \nand confirm the Parquet files loaded properly.",
    "start": "147280",
    "end": "152079"
  },
  {
    "text": "As you can see, the nyctaxi schema \ndata has been saved in Parquet format.",
    "start": "157143",
    "end": "161063"
  },
  {
    "text": "Now that we have moved our \ndata into the data lake,",
    "start": "162054",
    "end": "164294"
  },
  {
    "text": "we can use visualizations and \nML models to interpret the data.",
    "start": "164294",
    "end": "167456"
  },
  {
    "text": "Before we do that, let's review the infrastructure \nwe have set up to perform these analytics.",
    "start": "167734",
    "end": "171855"
  },
  {
    "text": "In the AWS Glue Data Catalog, we have created a database\nto store copies of the data we'll use for analytics.",
    "start": "173334",
    "end": "179094"
  },
  {
    "text": "We've also built and run a crawler to mine the Parquet files\nstored in the data lake and populate a table in the Data Catalog.",
    "start": "181006",
    "end": "186898"
  },
  {
    "text": "Let's look at the table.",
    "start": "188353",
    "end": "189368"
  },
  {
    "text": "Let's briefly review the \ndataset using Amazon Athena.",
    "start": "198033",
    "end": "200844"
  },
  {
    "text": "The table has records including the date,",
    "start": "208134",
    "end": "209974"
  },
  {
    "text": "distance, and fare earned \nfrom a vendor's taxi trips.",
    "start": "209974",
    "end": "212518"
  },
  {
    "text": "Now that we have established the data lake, we can use Amazon QuickSight\nbusiness intelligence tools to interpret and analyze the data.",
    "start": "214640",
    "end": "221166"
  },
  {
    "text": "Let's ingest the data from \nour Redshift cluster database.",
    "start": "224013",
    "end": "226696"
  },
  {
    "text": "Since we want to perform visualizations \nwith data from the NYC green taxi table,",
    "start": "240013",
    "end": "244185"
  },
  {
    "text": "we'll select the taxi schema table.",
    "start": "244185",
    "end": "245927"
  },
  {
    "text": "At this point, we can preview the data.",
    "start": "248624",
    "end": "250303"
  },
  {
    "text": "On the left, is a complete list of fields, and \nat the bottom of the screen is our dataset.",
    "start": "252593",
    "end": "256575"
  },
  {
    "text": "We can adjust data types, if necessary.",
    "start": "258805",
    "end": "260725"
  },
  {
    "text": "Since we don't need to make adjustments, \nwe can save and visualize the dataset.",
    "start": "261824",
    "end": "265344"
  },
  {
    "text": "For our first chart, let's graph \na count of trips over time.",
    "start": "267685",
    "end": "270725"
  },
  {
    "text": "To do this, we’ll drag the \ndata we need to the graph.",
    "start": "271974",
    "end": "274534"
  },
  {
    "text": "The visualization now shows a count \nof trips over time by trip type.",
    "start": "278616",
    "end": "282128"
  },
  {
    "text": "Let's add another graph to show \nthe history of trip distances.",
    "start": "284565",
    "end": "287548"
  },
  {
    "text": "For this, we'll need to add a calculated field.",
    "start": "289755",
    "end": "291995"
  },
  {
    "text": "We'll name the field and \nsupply the formula to compute.",
    "start": "294325",
    "end": "296805"
  },
  {
    "text": "Let's save the field.",
    "start": "302175",
    "end": "303215"
  },
  {
    "text": "We'll use a clustered bar combo \nchart for this visualization.",
    "start": "305184",
    "end": "308115"
  },
  {
    "text": "Let's apply the field we \njust created to the x-axis.",
    "start": "310475",
    "end": "313239"
  },
  {
    "text": "Let's add the \"total amount\" and \"pickup \ndate time\" fields to complete the graph.",
    "start": "316607",
    "end": "320379"
  },
  {
    "text": "Here we can see the final visualization.",
    "start": "327595",
    "end": "329515"
  },
  {
    "text": "We can also build ML models from \nRedshift data warehouse material.",
    "start": "331765",
    "end": "335045"
  },
  {
    "text": "Let's go to our second Redshift cluster node to review\nthe infrastructure we have set up for machine learning.",
    "start": "335695",
    "end": "340335"
  },
  {
    "text": "This information schema involves sales data.",
    "start": "343213",
    "end": "345533"
  },
  {
    "text": "Here are all the datasets included in the schema.",
    "start": "347907",
    "end": "350147"
  },
  {
    "text": "We also have an external schema called spectrum.ml \nwithin an external table called sales_raw.",
    "start": "351347",
    "end": "356227"
  },
  {
    "text": "We have taken the raw sales data and added some custom filters\nto create a new table in the public schema called sales_clean.",
    "start": "358056",
    "end": "363896"
  },
  {
    "text": "Let’s run this query.",
    "start": "365104",
    "end": "366169"
  },
  {
    "text": "We'll use this table for building \nML models using Amazon SageMaker.",
    "start": "369473",
    "end": "372992"
  },
  {
    "text": "For demonstration purposes, our Jupyter \nnotebook instance is already running.",
    "start": "375416",
    "end": "379056"
  },
  {
    "text": "The first step installs these packages.",
    "start": "381333",
    "end": "383488"
  },
  {
    "text": "The next step imports AWS Data Wrangler, which \nis used to connect to the Redshift cluster.",
    "start": "386394",
    "end": "391367"
  },
  {
    "text": "We already have our user connected.",
    "start": "394596",
    "end": "396116"
  },
  {
    "text": "Next, we’ll aggregate the data \nin the \"sales clean\" table",
    "start": "399832",
    "end": "402632"
  },
  {
    "text": "and then split it into a training \ndataset and a testing dataset.",
    "start": "402632",
    "end": "405686"
  },
  {
    "text": "Let's also run a time-series forecasting model so we can\nvisualize seasonality and any other noise apparent in the data.",
    "start": "408555",
    "end": "414395"
  },
  {
    "text": "With this code, we'll create a time series forecast model.",
    "start": "418043",
    "end": "420986"
  },
  {
    "text": "To do this, we’ll implement a version of a standard method called\nthe Auto Regressive Integrated Moving Average (ARIMA).",
    "start": "421470",
    "end": "427336"
  },
  {
    "text": "Here we can see the model that's been created.",
    "start": "429587",
    "end": "431507"
  },
  {
    "text": "We'll also run a command that plots \nforecasted versus actual sales.",
    "start": "434335",
    "end": "438000"
  },
  {
    "text": "Now, we're going to run this command that contains a random product code\nto demonstrate pulling data from Redshift to use for machine learning.",
    "start": "441494",
    "end": "447666"
  },
  {
    "text": "We’ll forecast this data for the next \n30 days and visualize the result.",
    "start": "451793",
    "end": "455633"
  },
  {
    "text": "The blue line represents actual values, \nand the yellow line shows the forecast.",
    "start": "464237",
    "end": "468208"
  },
  {
    "text": "We were able to use the same Redshift cluster for visualizing data\nas we did to build machine learning models.",
    "start": "469037",
    "end": "474156"
  },
  {
    "text": "You've just seen how you can extend \nyour data warehouse to your data lake.",
    "start": "475316",
    "end": "478449"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "479597",
    "end": "482719"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "482936",
    "end": "484947"
  }
]