[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "afternoon everyone my name is Graham McAllister I'm a senior principal engineer on the RTS team I spent a lot",
    "start": "0",
    "end": "6930"
  },
  {
    "text": "of time on the Postgres engines and specifically Aurora Postgres and that's what we're gonna be covering today many",
    "start": "6930",
    "end": "13290"
  },
  {
    "text": "of you have probably seen a lot of stuff about aurora over the past couple of years both on the aurora my sequel side and when we announced or a Postgres last",
    "start": "13290",
    "end": "19920"
  },
  {
    "text": "year here at reinvent what i'm going to do today is dive a little deeper than what we've done before i'm gonna try to",
    "start": "19920",
    "end": "25439"
  },
  {
    "text": "get behind the numbers that we've you know you might have seen in sort of the marketing numbers of hey we're you know up to three times faster I really think",
    "start": "25439",
    "end": "32250"
  },
  {
    "text": "it's important to understand why you get benefits for being on Aurora and that's what we're going to talk about today so",
    "start": "32250",
    "end": "38250"
  },
  {
    "start": "38000",
    "end": "132000"
  },
  {
    "text": "but first I really want to talk about sort of the fact that we have two engines in RDS that are both Postgres",
    "start": "38250",
    "end": "44250"
  },
  {
    "text": "and how they relate to each other so from a client perspective you shouldn't",
    "start": "44250",
    "end": "49800"
  },
  {
    "text": "care we have already s Postgres and it uses EBS for storage we have a roar of",
    "start": "49800",
    "end": "55079"
  },
  {
    "text": "Postgres and it uses Aurora storage so that's the main difference but from a client perspective from a connectivity",
    "start": "55079",
    "end": "60239"
  },
  {
    "text": "perspective it's exactly the same thing you can use whatever application on either one of them to go into more",
    "start": "60239",
    "end": "66450"
  },
  {
    "text": "detail you know they both run Postgres nine six they wrote the same extensions we have backup recovery we have",
    "start": "66450",
    "end": "72360"
  },
  {
    "text": "point-in-time recovery we have the you know high availability and durability for both of them now Aurora has higher",
    "start": "72360",
    "end": "79020"
  },
  {
    "text": "durability but they both are both durable solutions they're secure we have VPC we have security groups we have all",
    "start": "79020",
    "end": "85590"
  },
  {
    "text": "the good stuff SSL encryption at rest we support read replicas on both of them we",
    "start": "85590",
    "end": "91049"
  },
  {
    "text": "support cross region snapshots and like all RDS things we have scaled compute and online scale storage now Aurora has",
    "start": "91049",
    "end": "98310"
  },
  {
    "text": "some extra stuff there as well the two things today that we're missing to make these the same is we don't have cross",
    "start": "98310",
    "end": "105600"
  },
  {
    "text": "region replication and we don't have outbound logical replication sort of the Postgres standard logical replication",
    "start": "105600",
    "end": "111329"
  },
  {
    "text": "support yet these are items we're working on I don't have a date for you but we've heard loud and clear from the",
    "start": "111329",
    "end": "117750"
  },
  {
    "text": "customers that is very critical for us to add to make these the same so so",
    "start": "117750",
    "end": "123000"
  },
  {
    "text": "really it's about choice because we want these engines from the general perspective to be equal as far as their",
    "start": "123000",
    "end": "129810"
  },
  {
    "text": "capabilities at a base level so let's go into the base archetype sure warrants or walk through that",
    "start": "129810",
    "end": "135390"
  },
  {
    "start": "132000",
    "end": "330000"
  },
  {
    "text": "before we get into too much death so you know one of the things about Aurora",
    "start": "135390",
    "end": "140880"
  },
  {
    "text": "that's different is that the storage is across three availability zones so that's the purple big box that I have",
    "start": "140880",
    "end": "146520"
  },
  {
    "text": "there and then if you the lighter sort of purple blue boxes those represent sort of Aurora storage nodes right so we",
    "start": "146520",
    "end": "153330"
  },
  {
    "text": "have many of them each in a in different a ZZZ and so when you actually store",
    "start": "153330",
    "end": "158340"
  },
  {
    "text": "data you actually end up with six copies and we have a basic chunk size of 10-gig and so what I'm representing here is",
    "start": "158340",
    "end": "164940"
  },
  {
    "text": "that the different colors show the different copies of your data spread out across the three availability zones so",
    "start": "164940",
    "end": "171060"
  },
  {
    "text": "six copies three availability zones when you do a right you're not writing a block you're writing a log record so",
    "start": "171060",
    "end": "178170"
  },
  {
    "text": "this is fundamentally different and we'll cover more of that later in the talk but essentially what happens is we go",
    "start": "178170",
    "end": "183630"
  },
  {
    "text": "and we update six places with that log record we only need four for quorum and we'll cover that in more detail later as",
    "start": "183630",
    "end": "189510"
  },
  {
    "text": "well but this means it's a highly durable system right when we do a read",
    "start": "189510",
    "end": "196290"
  },
  {
    "text": "we can read from any of those six copies well three copies are full blocks but",
    "start": "196290",
    "end": "203070"
  },
  {
    "text": "usually we read from the local one because that's going to be the fastest path but not always because you can be having a problem with the machine is so",
    "start": "203070",
    "end": "209010"
  },
  {
    "text": "it can read from any of them now because we only require for six quorum you can",
    "start": "209010",
    "end": "214799"
  },
  {
    "text": "actually have times where some of the blocks are not up-to-date because they haven't finished writing so we can",
    "start": "214799",
    "end": "220950"
  },
  {
    "text": "actually do repair at the you know at the individual sort of Postgres block level between these copies right so this",
    "start": "220950",
    "end": "228900"
  },
  {
    "text": "is a parallel system that's all happening behind the scenes and even if",
    "start": "228900",
    "end": "234360"
  },
  {
    "text": "you lose a whole storage node it doesn't matter because you have six copies all we do is spin up another copy on another",
    "start": "234360",
    "end": "240570"
  },
  {
    "text": "storage node and make a copy right of that 10 gig segment so you can see that even if we had a number of problems",
    "start": "240570",
    "end": "247049"
  },
  {
    "text": "we're gonna repair this in parallel right so there's no single point of sort of fixing that would happen in a more",
    "start": "247049",
    "end": "253370"
  },
  {
    "text": "classic system one of the other things that you can do is you can attach read-only nodes or replicas right and",
    "start": "253370",
    "end": "261479"
  },
  {
    "text": "you can have multiples of them and they attach the same storage so it doesn't matter whether you have",
    "start": "261479",
    "end": "267419"
  },
  {
    "text": "or five read-only nodes you still end up with the exact same storage configuration and we just we handle the",
    "start": "267419",
    "end": "273840"
  },
  {
    "text": "hotspots and moving stuff around to make sure you have enough IO to meet whatever requirements you have if you have for",
    "start": "273840",
    "end": "280680"
  },
  {
    "text": "example a whole easy go down for network or just a bunch of hosts what we're gonna do is we're gonna fail you over",
    "start": "280680",
    "end": "287069"
  },
  {
    "text": "and that happens automatically it takes about 30 seconds including the you know",
    "start": "287069",
    "end": "292620"
  },
  {
    "text": "propagation maybe a little longer depending on the DNS and from then you're writing now on this new node so",
    "start": "292620",
    "end": "299400"
  },
  {
    "text": "that's pretty seamless and now you're using all your failover nodes you can use those for reads but you can also",
    "start": "299400",
    "end": "305669"
  },
  {
    "text": "have multiple places you can failover to so that increases your availability this storage is up to 64 terabytes so we just",
    "start": "305669",
    "end": "312930"
  },
  {
    "text": "announced that our RDS Postgres now supports 16 still larger for Aurora and the nice thing is that's automatically",
    "start": "312930",
    "end": "319439"
  },
  {
    "text": "scalable you no longer have to say I need two terabytes I need three terabytes you just put data in your database it just grows and that's it so",
    "start": "319439",
    "end": "326849"
  },
  {
    "text": "there's no no problem with with growing your space one of the other big changes",
    "start": "326849",
    "end": "332520"
  },
  {
    "start": "330000",
    "end": "466000"
  },
  {
    "text": "is that you know we're log base so let's dive into that so on the left hand side",
    "start": "332520",
    "end": "339210"
  },
  {
    "text": "I have sort of a picture of how classic Postgres works so if you imagine you have a whole bunch of work that you're",
    "start": "339210",
    "end": "344969"
  },
  {
    "text": "just ready to commit and that's what I'm showing with those sort of orange boxes when you say commit basically all those",
    "start": "344969",
    "end": "352139"
  },
  {
    "text": "all that work moves in on many sessions into the log buffer this is you know post rest as a great group commit works",
    "start": "352139",
    "end": "358620"
  },
  {
    "text": "quite well but the interesting problem is now that log buffer has to flush down to disk and while that's happening any",
    "start": "358620",
    "end": "364649"
  },
  {
    "text": "new work that's queued up that wants to commit can't because they can't get into the log buffer and so you have to wait",
    "start": "364649",
    "end": "370919"
  },
  {
    "text": "for that log to sink before you can actually do the new work and so the",
    "start": "370919",
    "end": "375990"
  },
  {
    "text": "challenge is the more work that you're trying to group commit the bigger the log buffer the longer the log buffer takes and this causes a you know a sort",
    "start": "375990",
    "end": "383520"
  },
  {
    "text": "of a motion that is causes jitter in your system right on Postgres it's quite",
    "start": "383520",
    "end": "388589"
  },
  {
    "text": "different when work comes in it doesn't matter you know I'm showing sort of ABS or ordered transactions coming in it",
    "start": "388589",
    "end": "395580"
  },
  {
    "text": "doesn't matter when they come in they're gonna just propagate down to the storage so there is no single queue they",
    "start": "395580",
    "end": "400980"
  },
  {
    "text": "just go as soon as they're ready so as soon as you say commit they're submitted but how do we know that they're done",
    "start": "400980",
    "end": "406860"
  },
  {
    "text": "well we have a durability tracking system so we keep track of every transaction and we know what's been done",
    "start": "406860",
    "end": "413730"
  },
  {
    "text": "and it actually is down to the single block change right so in this case what we can see is that a has been",
    "start": "413730",
    "end": "419880"
  },
  {
    "text": "acknowledged by you know to be by two C's only got one out of four so nothing's going to be returned to the",
    "start": "419880",
    "end": "426390"
  },
  {
    "text": "customer yet once a gets to four then we'll say that's durable in the commitment will return right now you'll",
    "start": "426390",
    "end": "433500"
  },
  {
    "text": "notice that C is also at four but we didn't say it's done yet and that's because it's a relational database we",
    "start": "433500",
    "end": "439320"
  },
  {
    "text": "like strict ordering right you know we don't want out of order on our transaction so we have to wait for B to",
    "start": "439320",
    "end": "445050"
  },
  {
    "text": "happen right so once B gets to five or four like in this case five then we can acknowledge all of them and you'll see",
    "start": "445050",
    "end": "451140"
  },
  {
    "text": "that E is at five but again not acknowledged because DS not there so this way there is no single point of",
    "start": "451140",
    "end": "456810"
  },
  {
    "text": "contention it's all at the end where we gather where we are and this means that",
    "start": "456810",
    "end": "462090"
  },
  {
    "text": "we can do much more concurrency than you can on standard Postgres the other",
    "start": "462090",
    "end": "467520"
  },
  {
    "start": "466000",
    "end": "600000"
  },
  {
    "text": "change is that we don't do checkpoints so let's walk through what that looks like in Postgres you have a block in",
    "start": "467520",
    "end": "472800"
  },
  {
    "text": "memory you do sum up you do an update you get some changes if it's the first time you've touched that block since the",
    "start": "472800",
    "end": "479730"
  },
  {
    "text": "checkpoint you eject a full page right or an fbw into the wall stream right and this can be quite large now any",
    "start": "479730",
    "end": "488820"
  },
  {
    "text": "subsequent ones don't actually cause you know before a next checkpoint but you can see that we used up a lot of space",
    "start": "488820",
    "end": "493920"
  },
  {
    "text": "in the wall so we checkpoint that block out that has to you know get rid of the data file and then we have to archive it",
    "start": "493920",
    "end": "500040"
  },
  {
    "text": "so that's pretty expensive right and the reason for this is when we're writing in",
    "start": "500040",
    "end": "505740"
  },
  {
    "text": "Postgres from writing 8k right but as you can see really from a linux perspective it's writing 4k and so if in",
    "start": "505740",
    "end": "513330"
  },
  {
    "text": "the middle of that write your server goes down guess what you got half of a checkpoint block written that's very bad",
    "start": "513330",
    "end": "519479"
  },
  {
    "text": "if you if you let this go you wouldn't even know you just have corruption right",
    "start": "519479",
    "end": "525030"
  },
  {
    "text": "so of course Postgres has the mechanism called full page rights to protect this so what happens is that if you do crash",
    "start": "525030",
    "end": "531930"
  },
  {
    "text": "during a checkpoint when you come back up it's going use the full blocks in the log to repair the data file and make it all good and",
    "start": "531930",
    "end": "538780"
  },
  {
    "text": "this is great because this thing runs all the time and it's you know always acting so you know it's very well tested",
    "start": "538780",
    "end": "545230"
  },
  {
    "text": "but it's a bit expensive so let's walk through how we do this differently in in",
    "start": "545230",
    "end": "551650"
  },
  {
    "text": "Aurora so again on the Left I have Postgres and we'll just walk through that same example the checkpoint the",
    "start": "551650",
    "end": "558430"
  },
  {
    "text": "archive now remember we've got a also we want it to be durable because we want to back it up so we have to wait for that",
    "start": "558430",
    "end": "563710"
  },
  {
    "text": "wall file to get archived then we can write it to a three so we do that for RDS in Aurora we get that same block in",
    "start": "563710",
    "end": "570640"
  },
  {
    "text": "memory we do the update get those changes and we send those vectors down to or storage and for more storage we're",
    "start": "570640",
    "end": "579550"
  },
  {
    "text": "continuously backing up those changes to s3 and that's it so you can kind of see",
    "start": "579550",
    "end": "584680"
  },
  {
    "text": "that there's a dramatic difference in the amount of writing we're doing with Aurora than we are with classic posters",
    "start": "584680",
    "end": "591750"
  },
  {
    "text": "that's an extra one and yeah and so no checkpoints means no full-page writes and that's gonna be really important",
    "start": "591750",
    "end": "597160"
  },
  {
    "text": "when I show you some other information so we're talking about full page rights",
    "start": "597160",
    "end": "603280"
  },
  {
    "start": "600000",
    "end": "650000"
  },
  {
    "text": "and the first time you touch a block after a checkpoint well if you have a very small database you probably touch",
    "start": "603280",
    "end": "608710"
  },
  {
    "text": "the same block over and over a lot but if your database gets larger and larger and if you have any kind of random work",
    "start": "608710",
    "end": "614320"
  },
  {
    "text": "that's going on it's going to touch a lot of blocks and that's what I'm going to show here so I have a illustration of",
    "start": "614320",
    "end": "620110"
  },
  {
    "text": "a b-tree and let's say we do an insert into 124 right so that's gonna go down and touch a bunch of blocks and modify",
    "start": "620110",
    "end": "626740"
  },
  {
    "text": "probably the leaf node on that block right and then if we do another random insert we're gonna touch a bunch of",
    "start": "626740",
    "end": "631870"
  },
  {
    "text": "other blocks and then a third one right so in the end here every time we touched",
    "start": "631870",
    "end": "638740"
  },
  {
    "text": "a block basically we did a full page right on that one index right so this sounds a little expensive right three",
    "start": "638740",
    "end": "645010"
  },
  {
    "text": "full pages yeah you know maybe but let's look at a you know a little more serious case so to do that I built up a test",
    "start": "645010",
    "end": "652390"
  },
  {
    "text": "where I built this nine column table and it's got some random values in it it's",
    "start": "652390",
    "end": "657730"
  },
  {
    "text": "got some you know sort of sequence based ones that are right-leaning some boolean x' a timestamp that's gonna",
    "start": "657730",
    "end": "663220"
  },
  {
    "text": "be right leaning and then I indexed every column now this isn't normal right you normally don't have an index",
    "start": "663220",
    "end": "668930"
  },
  {
    "text": "on every column but a lot of people have very broad tables that have hundreds of columns right so they end up with lots",
    "start": "668930",
    "end": "676700"
  },
  {
    "text": "of indexes in some cases way more than nine right and so if you could imagine now you're gonna update this index you",
    "start": "676700",
    "end": "683570"
  },
  {
    "text": "might cause as many as 10 full-page writes on every time you insert or update this table and to show you what",
    "start": "683570",
    "end": "689690"
  },
  {
    "text": "that looks like here's a benchmark we're running with that doing inserts vertical",
    "start": "689690",
    "end": "695329"
  },
  {
    "start": "690000",
    "end": "751000"
  },
  {
    "text": "axis is inserts per second so higher is better and then we just ran this over 300 minutes so the red is stock RDS",
    "start": "695329",
    "end": "701870"
  },
  {
    "text": "Postgres no changes to parameters and what you see is you get almost 25,000",
    "start": "701870",
    "end": "707029"
  },
  {
    "text": "writes per set or inserts per second but you don't get that for very long because the working set gets larger and larger",
    "start": "707029",
    "end": "712700"
  },
  {
    "text": "and you start touching more pages and guess what full-page writes happen you start going slower so how do you how do",
    "start": "712700",
    "end": "720200"
  },
  {
    "text": "you make this better you increase the time for the check points do that by increasing the wall-sized and so that's",
    "start": "720200",
    "end": "725660"
  },
  {
    "text": "what I've shown in the green and you can clearly see that this helps you know we get a good like you know 20 minute run",
    "start": "725660",
    "end": "732110"
  },
  {
    "text": "before it starts slowing down and it's still better than the red but it's not great the top line is Aurora in blue and",
    "start": "732110",
    "end": "740060"
  },
  {
    "text": "what you can see there is it's consistent because it doesn't matter what size your database is because we're not ejecting any full pages",
    "start": "740060",
    "end": "746589"
  },
  {
    "text": "Arkan our performance remains consistent over time so that's inserts what about",
    "start": "746589",
    "end": "752660"
  },
  {
    "start": "751000",
    "end": "796000"
  },
  {
    "text": "updates so I take the same table do an update to one of the random values into one of the timestamps and what we see is",
    "start": "752660",
    "end": "760040"
  },
  {
    "text": "a very similar result so in this case",
    "start": "760040",
    "end": "765190"
  },
  {
    "text": "I'm doing two updates per transaction vertical axis again larger is better the",
    "start": "765190",
    "end": "772339"
  },
  {
    "text": "red you know almost 4,000 TPS not bad when we increase the wall we get up to",
    "start": "772339",
    "end": "778579"
  },
  {
    "text": "almost 5,000 but we can see again with Aurora we get closer to we get around 17,000 so more than four times better",
    "start": "778579",
    "end": "785990"
  },
  {
    "text": "than our base setup right and this is just straight out of the box you know if there do any tuning and so that shows",
    "start": "785990",
    "end": "791839"
  },
  {
    "text": "again the cost of doing the full page writes and the check points one of the",
    "start": "791839",
    "end": "796910"
  },
  {
    "start": "796000",
    "end": "889000"
  },
  {
    "text": "other key things for Postgres that really is challenging is recovery time when you're doing a lot of",
    "start": "796910",
    "end": "803060"
  },
  {
    "text": "Rights so what I've shown here on the vertical axis is the recovery time in seconds so obviously you would like that",
    "start": "803060",
    "end": "809450"
  },
  {
    "text": "to be smaller so your boss yells at you less on the vertical access is being on",
    "start": "809450",
    "end": "814940"
  },
  {
    "text": "the bottom access is is basically the benchmark result number so higher is better your boss wants more work done",
    "start": "814940",
    "end": "821690"
  },
  {
    "text": "out of your database right so when we started this what we can see is we started the benchmark with a low number",
    "start": "821690",
    "end": "827510"
  },
  {
    "text": "you know a smaller number of users we're generating about three gig between checkpoints of wall that has to be",
    "start": "827510",
    "end": "833779"
  },
  {
    "text": "applied so it takes about 19 seconds we get about you know not quite 20000 rights per second so it's not bad but as",
    "start": "833779",
    "end": "841130"
  },
  {
    "text": "we move up as we keep increasing the number of workers right we get more transactions per second now more like in",
    "start": "841130",
    "end": "846649"
  },
  {
    "text": "the thirty thousand range but recovery is 50 seconds still not bad but then you look at this one where it's a hundred",
    "start": "846649",
    "end": "852529"
  },
  {
    "text": "and twenty three seconds when we when we're pushing forty thousand TPS still not bad but it's not a very good",
    "start": "852529",
    "end": "859790"
  },
  {
    "text": "trade-off so with Aurora because we don't have the check point we don't have any recovery we do recovery continuously",
    "start": "859790",
    "end": "867250"
  },
  {
    "text": "we get a number that's quite a bit different so we get a hundred and thirty four thousand and we have a recovery",
    "start": "867250",
    "end": "873980"
  },
  {
    "text": "time of about three seconds so this is part of what we're trying to do with Aurora is try to not have to have you",
    "start": "873980",
    "end": "879860"
  },
  {
    "text": "make trade-offs you know would you like you know good availability or would you like good performance I'd like both so",
    "start": "879860",
    "end": "886370"
  },
  {
    "text": "that's what we're doing so the other thing that we do differently is quorum",
    "start": "886370",
    "end": "892010"
  },
  {
    "start": "889000",
    "end": "946000"
  },
  {
    "text": "as I said so we're a where we need four out of six of our rights to succeed to to knowledge a commit and I'm gonna walk",
    "start": "892010",
    "end": "898579"
  },
  {
    "text": "you through why we did that so we looked at six way replication and when I say",
    "start": "898579",
    "end": "904040"
  },
  {
    "text": "six way I mean three locations two copies in each because if you imagine EVs it's got two copies behind it so",
    "start": "904040",
    "end": "909380"
  },
  {
    "text": "when we first add multi Z we actually looked at this and to illustrate how this works you can imagine the grey",
    "start": "909380",
    "end": "915199"
  },
  {
    "text": "lines being your local rights and then the red ones being remote and then you know you're gonna get your local right",
    "start": "915199",
    "end": "920329"
  },
  {
    "text": "back really quickly and then you're gonna start the rights on the remote ones and then they're gonna come back",
    "start": "920329",
    "end": "925370"
  },
  {
    "text": "and then you know you have to wait for all these pieces to happen right so in the end you finish but you notice",
    "start": "925370",
    "end": "931399"
  },
  {
    "text": "there's quite a gap between the first one in the last one and that's because you know locality how",
    "start": "931399",
    "end": "936560"
  },
  {
    "text": "far apart the data centers are you know how busy the individual servers are so this makes for a slower you know",
    "start": "936560",
    "end": "943970"
  },
  {
    "text": "performance system so to show you this we ran a high concurrency synchronous",
    "start": "943970",
    "end": "950089"
  },
  {
    "start": "946000",
    "end": "1006000"
  },
  {
    "text": "benchmark with either two copies that's the blue are four copies in two locations or three nodes six copies in",
    "start": "950089",
    "end": "957560"
  },
  {
    "text": "gray latency is on the vertical axis you'd like that to be smaller and then on the bottom this is the percentiles of",
    "start": "957560",
    "end": "964040"
  },
  {
    "text": "the of the run so the 50th percentile basically the average we see is not really that different you know these",
    "start": "964040",
    "end": "970339"
  },
  {
    "text": "extra copies don't seem bad it's like six versus seven most people would say this is acceptable but the thing to look",
    "start": "970339",
    "end": "976880"
  },
  {
    "text": "at is the four nines on the far side so this is one in 10,000 right is now taking almost four times as",
    "start": "976880",
    "end": "985339"
  },
  {
    "text": "long to complete so if you can imagine your application you know and you're",
    "start": "985339",
    "end": "991220"
  },
  {
    "text": "trying to keep the jitter down for your customer it's gonna be like well most people are happy but the odd customer is",
    "start": "991220",
    "end": "997190"
  },
  {
    "text": "going to be very unhappy with the performance right so we like the durability of the three node six coffee",
    "start": "997190",
    "end": "1002740"
  },
  {
    "text": "but we didn't like the price you have to pay so again choices right so what we do different with Aurora so in Aurora we",
    "start": "1002740",
    "end": "1010330"
  },
  {
    "text": "basically send out six writes simultaneously it's not even you know three and three it's it's six",
    "start": "1010330",
    "end": "1015610"
  },
  {
    "text": "simultaneously those are shown in the red Network and then they start coming back right and as soon as we have four",
    "start": "1015610",
    "end": "1021940"
  },
  {
    "text": "of them we're done we can acknowledge the commit right so if location two is a little slower that's fine today it might",
    "start": "1021940",
    "end": "1028600"
  },
  {
    "text": "be location two tomorrow might be location three it's a little slower it doesn't matter so this results in a much",
    "start": "1028600",
    "end": "1035110"
  },
  {
    "text": "lower jitter factor than you have on traditional synchronous replication systems so we can get we can actually",
    "start": "1035110",
    "end": "1042339"
  },
  {
    "text": "get performance that's better than no copies like just a single AZ set up with",
    "start": "1042339",
    "end": "1048280"
  },
  {
    "text": "with Aurora and that's what I'm going to show you here so this is sis bench",
    "start": "1048280",
    "end": "1053460"
  },
  {
    "start": "1050000",
    "end": "1105000"
  },
  {
    "text": "typically a lot of my sequel folks use it we use it because it's a little heavy writing on them PG bench for some of our",
    "start": "1053460",
    "end": "1060820"
  },
  {
    "text": "tests and what I'm showing is vertical axis is response time so in this case lower is better",
    "start": "1060820",
    "end": "1066570"
  },
  {
    "text": "Postgres this was just ec2 with like 45,000 provisioned I ops",
    "start": "1066570",
    "end": "1071970"
  },
  {
    "text": "single easy no backups just a base insulation and then the yellow line is",
    "start": "1071970",
    "end": "1077080"
  },
  {
    "text": "Amazon Aurora again 6 copies 3a Z's right continuous backups and what you",
    "start": "1077080",
    "end": "1083230"
  },
  {
    "text": "see here is can you see the checkpoint intervals right it's pretty clear how much effect checkpointing has on the",
    "start": "1083230",
    "end": "1090250"
  },
  {
    "text": "latency and jitter for your system right and what you see with Aurora is because we don't have that we are a much more",
    "start": "1090250",
    "end": "1097419"
  },
  {
    "text": "consistent right there's a much smoother system you from an application perspective your customers are gonna be",
    "start": "1097419",
    "end": "1103480"
  },
  {
    "text": "a lot happier let's move on and talk a little bit about replicas and clones like I should be talking like Star Wars",
    "start": "1103480",
    "end": "1110200"
  },
  {
    "start": "1105000",
    "end": "1158000"
  },
  {
    "text": "when I say this one so standard Postgres we support replicas and RDS and they",
    "start": "1110200",
    "end": "1117280"
  },
  {
    "text": "work quite well so you know you have a head node easy to read right node you",
    "start": "1117280",
    "end": "1122559"
  },
  {
    "text": "have EBS storage when you ask for a replica we take a snapshot we restore that snapshot to another set of EBS",
    "start": "1122559",
    "end": "1128260"
  },
  {
    "text": "volumes and then we attach a new head node and then you play catch-up right so because that all takes a little bit of",
    "start": "1128260",
    "end": "1133659"
  },
  {
    "text": "time to happen so once that's done you're all caught up any new updates",
    "start": "1133659",
    "end": "1139390"
  },
  {
    "text": "coming through are gonna flow through right but essentially when you do an update you're gonna write to EBS and get",
    "start": "1139390",
    "end": "1144520"
  },
  {
    "text": "that returned right and you're gonna send the asynchronous replication request a cause if that's in memory",
    "start": "1144520",
    "end": "1150250"
  },
  {
    "text": "that's great if it's not it's got to be read for me bs and then it's got to be updated and written TBS so that's that",
    "start": "1150250",
    "end": "1156159"
  },
  {
    "text": "takes a little bit of extra work right on Aurora it's it's quite a bit different what we do there is because we",
    "start": "1156159",
    "end": "1162760"
  },
  {
    "start": "1158000",
    "end": "1245000"
  },
  {
    "text": "have shared storage the writes are quite different so we have the read-only node",
    "start": "1162760",
    "end": "1168460"
  },
  {
    "text": "you can attach those it doesn't take very long because we don't have to copy the data you don't end up with extra",
    "start": "1168460",
    "end": "1173559"
  },
  {
    "text": "copies of the data you have to pay for so when we do that same update essentially we do that same writing but",
    "start": "1173559",
    "end": "1179200"
  },
  {
    "text": "now we're doing it so more storage but notice that now the read-only node can actually see that data so we still have",
    "start": "1179200",
    "end": "1186340"
  },
  {
    "text": "an asynchronous replication process to go between these two but it's for doing",
    "start": "1186340",
    "end": "1191440"
  },
  {
    "text": "basically updates in memory only it basically needs to change if you have the block in memory and I'll show you",
    "start": "1191440",
    "end": "1197260"
  },
  {
    "text": "how that works with something like with PG bench",
    "start": "1197260",
    "end": "1201630"
  },
  {
    "text": "so PG bench by default on the readwrite node has four tables and it's going to",
    "start": "1203010",
    "end": "1208870"
  },
  {
    "text": "touch all of them when it does it's write workload on the read-only node it only reads the accounts table so if",
    "start": "1208870",
    "end": "1215680"
  },
  {
    "text": "you're running this benchmark essentially when you do that asynchronous replication and apply you",
    "start": "1215680",
    "end": "1220870"
  },
  {
    "text": "have to have all those tables loaded in they all have to be touched right on Aurora Post grass again we're",
    "start": "1220870",
    "end": "1227290"
  },
  {
    "text": "only reading the accounts table so we only have to replicate and apply the changes for the accounts table so",
    "start": "1227290",
    "end": "1234690"
  },
  {
    "text": "anything to history or anything else doesn't even have to be replayed so this requires less CPU and it also means less",
    "start": "1234690",
    "end": "1241900"
  },
  {
    "text": "jitter in your in your apply process so to show this kind of in graphic detail I",
    "start": "1241900",
    "end": "1247690"
  },
  {
    "start": "1245000",
    "end": "1357000"
  },
  {
    "text": "ran PG bench readwrite 8,000 TPS so that's about you know 30,000 writes per",
    "start": "1247690",
    "end": "1254320"
  },
  {
    "text": "second on the primary and then 200,000 read requests per second on the replicas",
    "start": "1254320",
    "end": "1260200"
  },
  {
    "text": "so the orange line hopefully you can see that is the writes per second we were",
    "start": "1260200",
    "end": "1265240"
  },
  {
    "text": "doing on the replica the Green Line is the replication delay and that's in",
    "start": "1265240",
    "end": "1270490"
  },
  {
    "text": "seconds for some reason originally RDS which I guess it's my fault because I started a thing so we did seconds for",
    "start": "1270490",
    "end": "1277570"
  },
  {
    "text": "the replication delay instead of milliseconds still something we need to fix and the blue line is the reads per",
    "start": "1277570",
    "end": "1283450"
  },
  {
    "text": "second so notice we're not doing any reads to start with we're doing a lot of writes I'm running this benchmark and",
    "start": "1283450",
    "end": "1289270"
  },
  {
    "text": "then ten minutes in I run an update on the PG bench history table right so I",
    "start": "1289270",
    "end": "1294580"
  },
  {
    "text": "updated every row in PG the PG bench history right so this is a massive backfill no one's ever done a massive",
    "start": "1294580",
    "end": "1301300"
  },
  {
    "text": "backfill on their production system have they built an index well you know in the middle of the day now never happens so",
    "start": "1301300",
    "end": "1308740"
  },
  {
    "text": "what you can see is that as soon as we do that the replication delay starts climbing and we're losing 30 seconds for",
    "start": "1308740",
    "end": "1315340"
  },
  {
    "text": "every 60 seconds of elapsed time so this is not good right and you'll notice that",
    "start": "1315340",
    "end": "1321370"
  },
  {
    "text": "like the amount of writing is going down because we're we're actually struggling to apply as much change because we've",
    "start": "1321370",
    "end": "1326590"
  },
  {
    "text": "got all this extra work that has to be done and then you'll notice after about another 10 minutes we start doing a lot of reads well why is that because to",
    "start": "1326590",
    "end": "1333310"
  },
  {
    "text": "start with the PG bench history the first stuff that we were up dating was in memory but most the PG bench history had fallen out of memory",
    "start": "1333310",
    "end": "1339460"
  },
  {
    "text": "because it wasn't in use now has to be brought back in that even caused us to even slow down further on the",
    "start": "1339460",
    "end": "1344740"
  },
  {
    "text": "replication right so we're falling further and further behind so this is problematic and this is one of the things that customers disliked about",
    "start": "1344740",
    "end": "1350800"
  },
  {
    "text": "sort of replicas in general is they're a little fragile you can you can manage them but you have to be on top of it",
    "start": "1350800",
    "end": "1356880"
  },
  {
    "text": "this is the same graph I messed up and the axes are on the wrong side sorry",
    "start": "1356880",
    "end": "1362560"
  },
  {
    "start": "1357000",
    "end": "1404000"
  },
  {
    "text": "about that the other thing to note is this is in milliseconds so when you see that big chart it's not seconds so same",
    "start": "1362560",
    "end": "1370990"
  },
  {
    "text": "thing now we're doing the back fill but it's kind of hard to see where I did the back fill isn't it because there's not",
    "start": "1370990",
    "end": "1376990"
  },
  {
    "text": "any real change what we see is that the replication is between about 5 to 15 milliseconds and that's what we",
    "start": "1376990",
    "end": "1384580"
  },
  {
    "text": "typically see on an Aurora replica so you can do large operations and this is because that backfilled the PG bench",
    "start": "1384580",
    "end": "1390550"
  },
  {
    "text": "history we didn't have to do anything because most of PG bench history was not in the read only note right so very easy",
    "start": "1390550",
    "end": "1397870"
  },
  {
    "text": "no problems this makes replication much smoother on Aurora the other feature",
    "start": "1397870",
    "end": "1405610"
  },
  {
    "start": "1404000",
    "end": "1501000"
  },
  {
    "text": "that we just launched last week which we haven't even really announced is called fast clones and this is a feature is",
    "start": "1405610",
    "end": "1411640"
  },
  {
    "text": "just super cool we've had an owner my sequel for a little bit I think it's just a great use case you have your",
    "start": "1411640",
    "end": "1418210"
  },
  {
    "text": "rewrite you have your application you have your primary storage there in the orange and I'm showing sort of blocks in",
    "start": "1418210",
    "end": "1424210"
  },
  {
    "text": "blue there so you can have your normal read-only you know attached to your storage but what's really cool is let's",
    "start": "1424210",
    "end": "1430450"
  },
  {
    "text": "say your developer says you know I'd really like a copy of the database well the traditional way would be you know take a snapshot restore that snapshot",
    "start": "1430450",
    "end": "1436690"
  },
  {
    "text": "give them a copy of your database and you have to pay for all that storage with fast clones essentially you fire up",
    "start": "1436690",
    "end": "1442240"
  },
  {
    "text": "a read write note and we create a new clone storage and basically that clone storage is pointing at the primary",
    "start": "1442240",
    "end": "1448480"
  },
  {
    "text": "storage so we don't copy all the data and until the developer for example does an update there's no storage that you're",
    "start": "1448480",
    "end": "1456040"
  },
  {
    "text": "paying for so that when you do that first update now you've got to start paying for a little bit of storage that's been updated this is so this is",
    "start": "1456040",
    "end": "1462910"
  },
  {
    "text": "you know if you've got a 60 terabyte database you can now give your all your developers their own copy of the database because you",
    "start": "1462910",
    "end": "1469660"
  },
  {
    "text": "not actually paying for 60 terabytes for each of them the other cool thing is you know some people have sort of regulatory",
    "start": "1469660",
    "end": "1475030"
  },
  {
    "text": "reporting requirements with it like Oh every night you know midnight we want to run all these reports now you can cut a",
    "start": "1475030",
    "end": "1480580"
  },
  {
    "text": "fast clone at midnight and run your reports and you don't have to worry about vacuuming or anything else you",
    "start": "1480580",
    "end": "1486130"
  },
  {
    "text": "know timing out this is also you know really useful because you can go build",
    "start": "1486130",
    "end": "1491260"
  },
  {
    "text": "your own indexes on it or you don't make any changes that you need to do while you're doing reporting so very powerful",
    "start": "1491260",
    "end": "1496510"
  },
  {
    "text": "feature a basic feature now in our storage engine so let's talk a little",
    "start": "1496510",
    "end": "1502120"
  },
  {
    "start": "1501000",
    "end": "1642000"
  },
  {
    "text": "bit about caching caching is a bit different on on Aurora because guess",
    "start": "1502120",
    "end": "1507610"
  },
  {
    "text": "what we don't have a file system and Postgres uses a file system so here I'm",
    "start": "1507610",
    "end": "1512650"
  },
  {
    "text": "showing the amount of memory in our 416 in excel its 488 gig of ram it's a big",
    "start": "1512650",
    "end": "1517810"
  },
  {
    "text": "box right so with Postgres you got to have space for the Postgres processes",
    "start": "1517810",
    "end": "1523180"
  },
  {
    "text": "and os mostly we think about that being about a quarter of the ram maybe a little less depending on your your",
    "start": "1523180",
    "end": "1528580"
  },
  {
    "text": "application for RDS we configure shared buffers at one quarter of the system memory and the rest is Linux page cache",
    "start": "1528580",
    "end": "1535690"
  },
  {
    "text": "so in this case about half so that's pretty reasonable when you do a select in Postgres essentially you're asking",
    "start": "1535690",
    "end": "1542380"
  },
  {
    "text": "hey is that block in shared buffers if not go get it right and that's gonna go",
    "start": "1542380",
    "end": "1548320"
  },
  {
    "text": "to the Linux page cache and say is it in the page cache no well go get it from EBS and return it right so that's a lot",
    "start": "1548320",
    "end": "1553750"
  },
  {
    "text": "of work and hopefully you don't have to do that too much but if you've got a large working set you may the other",
    "start": "1553750",
    "end": "1560290"
  },
  {
    "text": "thing to note is that because you have two caches you end up with a lot of duplication where a lot of the shared",
    "start": "1560290",
    "end": "1565690"
  },
  {
    "text": "buffers are in the page cache as well so even though you should have 3/4 of the",
    "start": "1565690",
    "end": "1571000"
  },
  {
    "text": "memory for caching you end up with more like half an Aurora this is quite",
    "start": "1571000",
    "end": "1576970"
  },
  {
    "text": "different because we have no file system we still have the Postgres processes the OS processes that hasn't changed",
    "start": "1576970",
    "end": "1583420"
  },
  {
    "text": "we set the shared buffers to 3/4 and so essentially all the rest of the box is",
    "start": "1583420",
    "end": "1589330"
  },
  {
    "text": "for caching so when you do a select we check to see if it's in shared buffers just like Postgres but then if it's not",
    "start": "1589330",
    "end": "1594970"
  },
  {
    "text": "we go to our storage and we get it and we return it so this is more efficient",
    "start": "1594970",
    "end": "1600070"
  },
  {
    "text": "one of the nice things about Postgres was how's it using the Linux page cache when",
    "start": "1600070",
    "end": "1605169"
  },
  {
    "text": "the Postgres processes would die you lose the shared buffers but you have your living space cache right",
    "start": "1605169",
    "end": "1610389"
  },
  {
    "text": "so when the database restarts you don't have to reload all the stuff and you tend to have pretty good performance well we don't have a living space cash",
    "start": "1610389",
    "end": "1617440"
  },
  {
    "text": "so we had to do something about this so we did a feature called survivable cache and that essentially goes and does it",
    "start": "1617440",
    "end": "1624730"
  },
  {
    "text": "the proper invalidations on the cache leaves the right shared buffers there and you can go about and and run your",
    "start": "1624730",
    "end": "1632320"
  },
  {
    "text": "system so it behaves exactly like Postgres except for now it's a much larger cache so as always I'd like to",
    "start": "1632320",
    "end": "1639340"
  },
  {
    "text": "examine what does this look like in real life so running around a PG bench",
    "start": "1639340",
    "end": "1644379"
  },
  {
    "start": "1642000",
    "end": "1786000"
  },
  {
    "text": "read-only workload and this is quite a large scale it's like 22,000 so that's about 350 gig right so this is this",
    "start": "1644379",
    "end": "1650830"
  },
  {
    "text": "quite large on our 416 that has 488 so vertical access transactions per second",
    "start": "1650830",
    "end": "1657840"
  },
  {
    "text": "so what we can see with Aurora we're using 75% of the box for the shared buffers and we get you know six hundred",
    "start": "1657840",
    "end": "1665789"
  },
  {
    "text": "eighty plus thousand almost seven hundred thousand you know basically reads per second that's pretty",
    "start": "1665789",
    "end": "1671769"
  },
  {
    "text": "impressive when you think about it right like that's a lot of that's a lot of reason for a second so I was like okay",
    "start": "1671769",
    "end": "1677379"
  },
  {
    "text": "now I'm gonna run this on RDS Postgres and we'll see what we get well we only got a little more than four hundred and",
    "start": "1677379",
    "end": "1682899"
  },
  {
    "text": "seventeen thousand so this was a little like hmm what's going on here we're like one point six times difference well we",
    "start": "1682899",
    "end": "1690639"
  },
  {
    "text": "were doing 18,000 retie ops as it turns out I was like oh that's probably why and the reason is because of that double",
    "start": "1690639",
    "end": "1697960"
  },
  {
    "text": "buffering right we're really only got about 250 gig of usable page cache and",
    "start": "1697960",
    "end": "1704320"
  },
  {
    "text": "so we didn't fit in okay that's easy to solve right let's make the shared",
    "start": "1704320",
    "end": "1710230"
  },
  {
    "text": "buffers smaller let's make it 10% that way you know more like about 70 or 80% of",
    "start": "1710230",
    "end": "1715960"
  },
  {
    "text": "the box is available to the Linux page cache and this is where the surprise came in we got slower that's not a",
    "start": "1715960",
    "end": "1724899"
  },
  {
    "text": "response that you know you normally think you're like wait but now I don't have any reads so I should be faster",
    "start": "1724899",
    "end": "1730149"
  },
  {
    "text": "well I should be but I'm not because I have heavy double buffering right because now most of the blocks have to",
    "start": "1730149",
    "end": "1736539"
  },
  {
    "text": "be read from the page cache which means every time I got to go to a block I got to do those two hops so I burned a lot more CPU and I",
    "start": "1736539",
    "end": "1743049"
  },
  {
    "text": "spent a lot more time and that slows you down and so this is actually I mean it's a little contrived but not much right I",
    "start": "1743049",
    "end": "1749860"
  },
  {
    "text": "mean many people have very large working sets and this is the kind of you know thing you're gonna see now just to show",
    "start": "1749860",
    "end": "1756040"
  },
  {
    "text": "you that there's nothing odd going on here I configured RDS Postgres with 75% of the cache and it gives us basically",
    "start": "1756040",
    "end": "1761920"
  },
  {
    "text": "the same number within a you know small rounding error almost you know within 1% right but of course the thing is once",
    "start": "1761920",
    "end": "1768790"
  },
  {
    "text": "you do that with RDS Postgres you don't have any survivable cache anymore because you're not you know emilynics",
    "start": "1768790",
    "end": "1773860"
  },
  {
    "text": "page cache so this again is one of those choice things we've made it so you don't have to choose between the best",
    "start": "1773860",
    "end": "1780190"
  },
  {
    "text": "performance by having no double buffering or survivable cache you get both so vacuuming I'm sure nobody in",
    "start": "1780190",
    "end": "1789070"
  },
  {
    "start": "1786000",
    "end": "1863000"
  },
  {
    "text": "here who's used Postgres as a problem with vacuum right it just works perfectly never had never had any issues",
    "start": "1789070",
    "end": "1795100"
  },
  {
    "text": "I'm sure okay so we just skip this let's flip through it okay so I'm gonna quickly walk through why vacuuming",
    "start": "1795100",
    "end": "1801340"
  },
  {
    "text": "matters here I have three blocks bunch of tuples or rows in them right if I go",
    "start": "1801340",
    "end": "1806590"
  },
  {
    "text": "update those tuples I get like a you know I end up marking the old one is dead entering the new one right so this",
    "start": "1806590",
    "end": "1812770"
  },
  {
    "text": "is pretty common so if I do that and then I do some more inserts and then I run vacuum guess what well if no one's",
    "start": "1812770",
    "end": "1819070"
  },
  {
    "text": "using those we get to clean them up and then when we go and let's say update some more tuples we're gonna reuse that",
    "start": "1819070",
    "end": "1824620"
  },
  {
    "text": "space right so this is why from a space management perspective it's important right if we don't run vacuum and we do",
    "start": "1824620",
    "end": "1832660"
  },
  {
    "text": "the exact same thing guess what now we're into a new block right because we couldn't reuse the space and so what",
    "start": "1832660",
    "end": "1838600"
  },
  {
    "text": "that turns into is you end up with more blocks more cache misses right you end",
    "start": "1838600",
    "end": "1845590"
  },
  {
    "text": "up with non hot updates for those of you that know how the the hot optimization works it requires you to be updating in",
    "start": "1845590",
    "end": "1851770"
  },
  {
    "text": "the same block so if you're moving blocks around and that of course on Postgres requires more full-page writes",
    "start": "1851770",
    "end": "1857290"
  },
  {
    "text": "so this is really bad on Postgres but it's still still a concern on aurora so",
    "start": "1857290",
    "end": "1862919"
  },
  {
    "text": "this is what happens if you run a long-running benchmark TPS on the vertical axis you'd like it to be",
    "start": "1862919",
    "end": "1868320"
  },
  {
    "text": "basically staying the same the red line is kind of the goal and what we see over time is if you're not",
    "start": "1868320",
    "end": "1874299"
  },
  {
    "text": "running vacuum you end up with the black line is your average where over time it just basically gets slower and slower because you have more blocks you have",
    "start": "1874299",
    "end": "1881470"
  },
  {
    "text": "more cache misses you know it's just kind of bad right and of course there's the other small reason why you need a",
    "start": "1881470",
    "end": "1886929"
  },
  {
    "text": "vacuum is that there's a transaction ID limit right and if you don't vacuum you're under gonna end up in single user",
    "start": "1886929",
    "end": "1893200"
  },
  {
    "text": "mode vacuuming and your boss in your office quite irate so and you know it's",
    "start": "1893200",
    "end": "1899230"
  },
  {
    "text": "surprising because Postgres can actually write it quite a rate that the you know that the transaction wrap around is a",
    "start": "1899230",
    "end": "1905110"
  },
  {
    "text": "real thing for any high right customer so we wanted to make vacuum better so",
    "start": "1905110",
    "end": "1911950"
  },
  {
    "text": "one of the first things we did was we did an intelligent vacuum prefetch so in Postgres one of the nice things they did",
    "start": "1911950",
    "end": "1918100"
  },
  {
    "text": "in nine six it's really fundamental is that they added a freeze map or a frozen map so you now know where the blocks you",
    "start": "1918100",
    "end": "1924519"
  },
  {
    "text": "are that were the blocks that you need to freeze are right as opposed to scanning the entire table on a vacuum",
    "start": "1924519",
    "end": "1930340"
  },
  {
    "text": "you can actually now just figure out and go get the right ones so this is illustrating that concept where the",
    "start": "1930340",
    "end": "1936130"
  },
  {
    "text": "Green are things that have been frozen and the one that's sort of the peaches color are non frozen right so you'd",
    "start": "1936130",
    "end": "1941830"
  },
  {
    "text": "expect basically Postgres to go through and read just the ones it needs and vacuum them right well because Postgres relies on the file",
    "start": "1941830",
    "end": "1949840"
  },
  {
    "text": "system and it's read ahead it has a nice optimization that says if that block was within 32 blocks of the",
    "start": "1949840",
    "end": "1956230"
  },
  {
    "text": "last block well let's just read them all so in actuality here's what Postgres",
    "start": "1956230",
    "end": "1961360"
  },
  {
    "text": "will do in this situation right now that's not bad because the file system",
    "start": "1961360",
    "end": "1967330"
  },
  {
    "text": "will be doing read ahead and so it will be trying to help you write but there's lots of other stuff going on on your system you're doing a lot more eye ops",
    "start": "1967330",
    "end": "1973720"
  },
  {
    "text": "one of the things that most people don't get is that the vacuum delay that you can set when you're doing i/o actually",
    "start": "1973720",
    "end": "1980230"
  },
  {
    "text": "kicks in on each one of these and so a lot of times vacuum is not set",
    "start": "1980230",
    "end": "1985240"
  },
  {
    "text": "aggressively enough because of this thing so we're like well we don't have a file system we don't have read ahead what are we gonna do well we did a",
    "start": "1985240",
    "end": "1991960"
  },
  {
    "text": "prefetch so we go figure out from the map one of the things that we need to go read then we go collect those addresses",
    "start": "1991960",
    "end": "1998320"
  },
  {
    "text": "and we do one batch i/o up to 255 requests at once",
    "start": "1998320",
    "end": "2004020"
  },
  {
    "text": "so this makes a substantial difference on how fast we can vacuum and I'll show you that in a minute the other thing",
    "start": "2004020",
    "end": "2010800"
  },
  {
    "start": "2009000",
    "end": "2041000"
  },
  {
    "text": "that's very interesting is playing with the concept of vacuuming and memory being much more aggressive about your",
    "start": "2010800",
    "end": "2016320"
  },
  {
    "text": "vacuuming and I'll walk you through why that's why that can be a good thing so if you have particularly work loads that",
    "start": "2016320",
    "end": "2023429"
  },
  {
    "text": "are suited for this ones where you're just depending on to the heat pages like basically insert only or where you have",
    "start": "2023429",
    "end": "2029760"
  },
  {
    "text": "mostly right-leaning indexes this can be a very good technique of course you",
    "start": "2029760",
    "end": "2035370"
  },
  {
    "text": "can't have long-running transactions because obviously if you do that they're gonna block back in so we're back to our",
    "start": "2035370",
    "end": "2042510"
  },
  {
    "start": "2041000",
    "end": "2098000"
  },
  {
    "text": "standard kind of example here and I'm showing post grass just to kind of give you the again the differences so with",
    "start": "2042510",
    "end": "2048450"
  },
  {
    "text": "the Block in memory we're doing a bunch of inserts for example so we fill that block up and we check one it right but",
    "start": "2048450",
    "end": "2055950"
  },
  {
    "text": "it's not frozen right so at some point later I mean we have to archive it too we have",
    "start": "2055950",
    "end": "2061408"
  },
  {
    "text": "to come back and load that right so we basically go and vacuum it up",
    "start": "2061409",
    "end": "2067250"
  },
  {
    "text": "bring it into memory freeze it well guess what that's the first time we've touched it since a check point get a",
    "start": "2067250",
    "end": "2072330"
  },
  {
    "text": "full page right lucky winner so again in Aurora we don't have to do this but in post Chris you do and then of course you",
    "start": "2072330",
    "end": "2078300"
  },
  {
    "text": "have to recheck point that block out and do the wall archive of it right so this is quite expensive so this is why I'm",
    "start": "2078300",
    "end": "2083460"
  },
  {
    "text": "Postgres you have to be very careful about how you tune your vacuuming because otherwise you can cause a lot more effort you know a lot more work to",
    "start": "2083460",
    "end": "2090060"
  },
  {
    "text": "be happening because if you freeze something and then someone comes and touches that block again like two seconds later you're like oh that was",
    "start": "2090060",
    "end": "2095429"
  },
  {
    "text": "kind of wasted I did a whole bunch of work for nothing if you vacuum in memory",
    "start": "2095429",
    "end": "2100580"
  },
  {
    "start": "2098000",
    "end": "2234000"
  },
  {
    "text": "it's so much better because you have your block of memory you're doing all those inserts and then you vacuum right",
    "start": "2100580",
    "end": "2106859"
  },
  {
    "text": "and then you checkpoint and you archive it and you're done right there's no read back from disk there's no full extra",
    "start": "2106859",
    "end": "2113550"
  },
  {
    "text": "full page in the in the log right but the problem is you need to do this before checkpoints that's really hard to",
    "start": "2113550",
    "end": "2120480"
  },
  {
    "text": "do unless you of course you don't have checkpoints which Aurora doesn't write",
    "start": "2120480",
    "end": "2126150"
  },
  {
    "text": "so that's one of the things that we can do that makes a difference so you can set this on a per table basis and that's",
    "start": "2126150",
    "end": "2132930"
  },
  {
    "text": "what actually you know really cool with Postgres is that this is very tunable for your application you can say the one table I know I can vacuum in memory",
    "start": "2132930",
    "end": "2139710"
  },
  {
    "text": "so you can set the parameters to be aggressive right now if you're in Postgres you'd have to increase the",
    "start": "2139710",
    "end": "2145200"
  },
  {
    "text": "checkpoint timeout to try to make it so that you have more time to do it of course we don't have to do that in fact",
    "start": "2145200",
    "end": "2150299"
  },
  {
    "text": "you don't even get to set that into work because it doesn't do anything the next thing is I wanted to show what this",
    "start": "2150299",
    "end": "2156720"
  },
  {
    "text": "looks like so I ran manual vacuums just to make it easy on myself so if you get",
    "start": "2156720",
    "end": "2162780"
  },
  {
    "text": "a vacuum in memory before a checkpoint in Postgres it's about 72 seconds to vacuum this table I was looking at now",
    "start": "2162780",
    "end": "2169140"
  },
  {
    "text": "of course in Aurora it doesn't matter because there's no checkpoint so it's equivalent packing in Mary after the",
    "start": "2169140",
    "end": "2175319"
  },
  {
    "text": "checkpoint in Postgres is 152 seconds so you can see it take twice as much time because it had to eject all those full",
    "start": "2175319",
    "end": "2180780"
  },
  {
    "text": "pages and do a bunch of stuff right so we still in memory but it has to go through that extra work now again not",
    "start": "2180780",
    "end": "2186480"
  },
  {
    "text": "applicable to Aurora because we don't have a checkpoint vacuuming not in memory so this means it's on disk it's",
    "start": "2186480",
    "end": "2192809"
  },
  {
    "text": "got a load in on Postgres that took 402 seconds so you can see if you could do vacuuming in memory it's still a still a",
    "start": "2192809",
    "end": "2200190"
  },
  {
    "text": "good idea in Postgres but you'll notice the second number right 163 seconds that's what it that's",
    "start": "2200190",
    "end": "2205440"
  },
  {
    "text": "what it is for vacuuming on Aurora so you can see that that prefetch really does make quite a difference in our",
    "start": "2205440",
    "end": "2212040"
  },
  {
    "text": "ability to vacuum right and because it's a manual vacuum there's no delays really on that this is just about raw speed of",
    "start": "2212040",
    "end": "2218520"
  },
  {
    "text": "how fast you can fetch the blocks in so what we've done is we've set the tunable zhannar Aurora so that vacuum is more",
    "start": "2218520",
    "end": "2224309"
  },
  {
    "text": "aggressive because basically it's less expensive so you can be much more aggressive with your vacuum which means",
    "start": "2224309",
    "end": "2230400"
  },
  {
    "text": "you having a lot less problems keeping up let's talk a little bit about monitoring and performance so how many",
    "start": "2230400",
    "end": "2239460"
  },
  {
    "start": "2234000",
    "end": "2358000"
  },
  {
    "text": "people here know what our hands monitoring offering is whew okay good",
    "start": "2239460",
    "end": "2244890"
  },
  {
    "text": "not as many as I'd hope so this is this is our base layer and hence monitoring you get granular OS statistics and a",
    "start": "2244890",
    "end": "2252059"
  },
  {
    "text": "process list and you can do it at a one-second granularity if you want so what I'm showing here is the process",
    "start": "2252059",
    "end": "2257190"
  },
  {
    "text": "list and I bring this up because we added a new process right that you would never have seen before and this is the",
    "start": "2257190",
    "end": "2262710"
  },
  {
    "text": "Aurora storage daemon so this is the piece that talks to Aurora storage so you'll see that consuming CPU as it's",
    "start": "2262710",
    "end": "2268349"
  },
  {
    "text": "sending down writes and getting reads the other major change is performance insights this is one of the most exciting things",
    "start": "2268349",
    "end": "2274950"
  },
  {
    "text": "that you know we've worked on for a while outside of core database because you know the customers have been telling",
    "start": "2274950",
    "end": "2280440"
  },
  {
    "text": "us this is all great you make the thing more scalable but I still need help tuning it right I still need to make my",
    "start": "2280440",
    "end": "2286140"
  },
  {
    "text": "DBAs job easier so we worked on performance insights and this is a weight based model so we basically",
    "start": "2286140",
    "end": "2292770"
  },
  {
    "text": "sample once a second and we look at what the processes are doing and in this case like green is CPU so if you're on CPU",
    "start": "2292770",
    "end": "2299760"
  },
  {
    "text": "it'll be that and if you're waiting for something it'll be one of the other colors so we can see from this graph we",
    "start": "2299760",
    "end": "2304830"
  },
  {
    "text": "have this spike right and that black line is the maximum number of CPUs so we're actually like basically burning",
    "start": "2304830",
    "end": "2311040"
  },
  {
    "text": "all of our CPUs right and we'd be like what happened right and this is the TPCC workload that I was playing with we're",
    "start": "2311040",
    "end": "2319230"
  },
  {
    "text": "TPCC like workload I was playing with so we can we can see that there's a problem we can zoom in right so we can zoom into",
    "start": "2319230",
    "end": "2325920"
  },
  {
    "text": "that time spot and see oh that's a select count star query right kind of an evil query so but we're like hmm well",
    "start": "2325920",
    "end": "2334500"
  },
  {
    "text": "what was going on with that query before and what we can see is we can zoom back out and then we can just select the one query and you can clearly see that",
    "start": "2334500",
    "end": "2341160"
  },
  {
    "text": "something changed right the number of the number of executions of that query dramatically changed at",
    "start": "2341160",
    "end": "2347160"
  },
  {
    "text": "this one period right so this allows you to diagnose you know problems much quicker and so this is out now with in",
    "start": "2347160",
    "end": "2354240"
  },
  {
    "text": "preview with Aurora Postgres so you can see that problem and of course you know",
    "start": "2354240",
    "end": "2360180"
  },
  {
    "start": "2358000",
    "end": "2476000"
  },
  {
    "text": "to wrap it up doesn't do you much good if you can't run it so how do you migrate into Aurora Postgres well",
    "start": "2360180",
    "end": "2367890"
  },
  {
    "text": "there's three sort of main ways today we're working on another one that we should have out here in a bit but I can",
    "start": "2367890",
    "end": "2373740"
  },
  {
    "text": "talk to you offline about that the first is data migration services which I'll cover PG dump is you restore pretty much",
    "start": "2373740",
    "end": "2380130"
  },
  {
    "text": "everyone's familiar with that I'm not going to talk too much about that and then snapshot import so DMS is a logical",
    "start": "2380130",
    "end": "2385980"
  },
  {
    "text": "replication engine essentially allows you to do ongoing replication or you know one-time moves those are all the",
    "start": "2385980",
    "end": "2393000"
  },
  {
    "text": "engines that we support over on the left hand side so Postgres Oracle sequel server Maria my sequel including Aurora",
    "start": "2393000",
    "end": "2399450"
  },
  {
    "text": "my sequel so whether it's an on-premise instance ec2 or RDS you can basically",
    "start": "2399450",
    "end": "2404610"
  },
  {
    "text": "hook a DMS server configure it to say I want to go from you know this database to my",
    "start": "2404610",
    "end": "2412380"
  },
  {
    "text": "newer or Postgres and then what its gonna do you have to load your tables",
    "start": "2412380",
    "end": "2417600"
  },
  {
    "text": "and schema because it doesn't do that yet and then it's gonna do a full select consistent select to do the initial load",
    "start": "2417600",
    "end": "2423600"
  },
  {
    "text": "and while it's doing that it's keeping all the change data regardless of the engine and then it's gonna replay that",
    "start": "2423600",
    "end": "2429390"
  },
  {
    "text": "and catch up so at that point you basically the two databases are the same and you can failover and be done so this",
    "start": "2429390",
    "end": "2436890"
  },
  {
    "text": "works well if you're coming from an older version of Postgres or if you're coming from something that's not",
    "start": "2436890",
    "end": "2441960"
  },
  {
    "text": "Postgres if you're an RDS today one of the ways you can come in I mean you can use those other two methods is snapshot",
    "start": "2441960",
    "end": "2448590"
  },
  {
    "text": "import so if your application you have RDS Postgres shut your application down basically you take a snapshot and then",
    "start": "2448590",
    "end": "2455790"
  },
  {
    "text": "you import that snapshot and that's gonna get physically converted into an Aurora Postgres database and then you",
    "start": "2455790",
    "end": "2461400"
  },
  {
    "text": "just start your application on because it doesn't need any changes because it's still just Postgres now like I said we're doing some improvements to this",
    "start": "2461400",
    "end": "2466860"
  },
  {
    "text": "and we should have some additional functionality in this space shortly as well and I think yeah and with that I'm",
    "start": "2466860",
    "end": "2477060"
  },
  {
    "start": "2476000",
    "end": "2506000"
  },
  {
    "text": "finished thank you very much and I'll take questions there are there are mics",
    "start": "2477060",
    "end": "2486720"
  },
  {
    "text": "up front if you want it or you can just just yell it out - that's fine I can't it's hard to see anyone have",
    "start": "2486720",
    "end": "2494280"
  },
  {
    "text": "questions I really can't see oh there we go there you go yeah yeah so the",
    "start": "2494280",
    "end": "2506640"
  },
  {
    "start": "2506000",
    "end": "2539000"
  },
  {
    "text": "question was today we announced a multi writer or multi master for Aurora do we support that not today",
    "start": "2506640",
    "end": "2513360"
  },
  {
    "text": "so that the preview right now is for Aurora my sequel to start with it's definitely something we'll be working on",
    "start": "2513360",
    "end": "2519150"
  },
  {
    "text": "but we don't have a timeline yet on it other questions yes yes",
    "start": "2519150",
    "end": "2531110"
  },
  {
    "text": "right so the question is how do we deal",
    "start": "2535350",
    "end": "2540940"
  },
  {
    "start": "2539000",
    "end": "2648000"
  },
  {
    "text": "with how do we deal with regions that only have to a ZZZ Aurora is only in",
    "start": "2540940",
    "end": "2547330"
  },
  {
    "text": "regions that have three availability zones not all the availability zones are",
    "start": "2547330",
    "end": "2552670"
  },
  {
    "text": "available for general customers so there are some regions where the third one is a small thing that is just a four Aurora",
    "start": "2552670",
    "end": "2560170"
  },
  {
    "text": "Aurora like services so whenever you see a roar you know you're in 3a Z's and six",
    "start": "2560170",
    "end": "2566320"
  },
  {
    "text": "copies hi can you please give us some",
    "start": "2566320",
    "end": "2571480"
  },
  {
    "text": "details about how seven less Aurora is going to work sure the question is serverless Aurora so again that's one",
    "start": "2571480",
    "end": "2579310"
  },
  {
    "text": "that we've started preview on in my sequel and you know at some point we'll will have four four Postgres as well the",
    "start": "2579310",
    "end": "2585850"
  },
  {
    "text": "the basic model is that it's a you're going through a proxy and so what we",
    "start": "2585850",
    "end": "2592900"
  },
  {
    "text": "could do is we can actually hold the connections and move them around so when",
    "start": "2592900",
    "end": "2598060"
  },
  {
    "text": "you have a server you know we can scale it up and down and you don't lose your connections right so that's the sort of",
    "start": "2598060",
    "end": "2604180"
  },
  {
    "text": "the basic piece and then because we know what you're doing with the thing if you stop using it we can actually you know",
    "start": "2604180",
    "end": "2610750"
  },
  {
    "text": "have it sleep and then wake back up again when a new query comes in so by having that sort of intermediary a piece",
    "start": "2610750",
    "end": "2617620"
  },
  {
    "text": "that's that's how that one works yes",
    "start": "2617620",
    "end": "2622560"
  },
  {
    "text": "right right sure yeah yeah yeah so yeah",
    "start": "2628610",
    "end": "2649200"
  },
  {
    "start": "2648000",
    "end": "2688000"
  },
  {
    "text": "so the question is how do you convert something like a sequel server instance so we have a tool called SCT schema",
    "start": "2649200",
    "end": "2654330"
  },
  {
    "text": "conversion tool so it'll help you do analysis and the conversion now it may not be able to convert everything but",
    "start": "2654330",
    "end": "2659640"
  },
  {
    "text": "one of the good things about the tools it'll tell you what things you might have to rewrite right that just don't natively pour it over I mean Postgres is",
    "start": "2659640",
    "end": "2666150"
  },
  {
    "text": "a quite quite a good engine in its richness so people find it so much easier to report it than some of the other engines that we support and then",
    "start": "2666150",
    "end": "2673140"
  },
  {
    "text": "you can use DMS to move your data and key pop right and so then you know you basically do the move and you keep up",
    "start": "2673140",
    "end": "2679230"
  },
  {
    "text": "and then yeah so we have many customers that move from Oracle or sequel server in so the question is is Arora used",
    "start": "2679230",
    "end": "2689880"
  },
  {
    "start": "2688000",
    "end": "2732000"
  },
  {
    "text": "strictly for OLTP or data warehousing I mean we believe obviously invest of reads sort of you know models and that's",
    "start": "2689880",
    "end": "2696150"
  },
  {
    "text": "why we have things like redshift and Athena and spectrum but of course you know lots of folks have lots of",
    "start": "2696150",
    "end": "2702270"
  },
  {
    "text": "different use cases and data warehousing to one person means you know running some small reports for someone for other",
    "start": "2702270",
    "end": "2708000"
  },
  {
    "text": "people it means turning through terabytes of data right so I think it depends to some degree but we definitely have a lot of light to medium data",
    "start": "2708000",
    "end": "2714930"
  },
  {
    "text": "warehousing and OLAP kind of stuff happening on arora today and I mean you know Postgres is it's adding more",
    "start": "2714930",
    "end": "2720720"
  },
  {
    "text": "parallel features is making it you know a more comprehensive database for doing data warehousing yes is there an easy",
    "start": "2720720",
    "end": "2732660"
  },
  {
    "start": "2732000",
    "end": "2781000"
  },
  {
    "text": "way to compare costs yes I don't know if our cost comparison calculator has",
    "start": "2732660",
    "end": "2739130"
  },
  {
    "text": "something in there at the moment but I mean the way to think about it is that the the per server costs are slightly",
    "start": "2739130",
    "end": "2746760"
  },
  {
    "text": "higher but you only pay for the storage you use as opposed to what you have to allocate then we have i/o charges so you",
    "start": "2746760",
    "end": "2754350"
  },
  {
    "text": "can kind of look at how many iOS you're doing today and kind of get a rough idea of what you you know what you'd be",
    "start": "2754350",
    "end": "2759750"
  },
  {
    "text": "you know going for an Aurora that's I think I'll check and see if the cost calculator handles that yes yeah so I",
    "start": "2759750",
    "end": "2782310"
  },
  {
    "start": "2781000",
    "end": "2838000"
  },
  {
    "text": "mean the question is really about like sort of doing testing of you know kind of trying it in the mean like oh I don't",
    "start": "2782310",
    "end": "2787740"
  },
  {
    "text": "you know let me go back so today as I said we don't support outbound replication for for Aurora Postgres we",
    "start": "2787740",
    "end": "2794700"
  },
  {
    "text": "will when we have that you could basically you can have DMS running sort of one way and then you know then you",
    "start": "2794700",
    "end": "2800850"
  },
  {
    "text": "can go back the other way it's a little complex but it can be done you know so",
    "start": "2800850",
    "end": "2806820"
  },
  {
    "text": "it just it's not there's no easy switch or anything it is a little bit of work to do questions in it when is a fast",
    "start": "2806820",
    "end": "2818970"
  },
  {
    "text": "clone available it's available now we just because of the marketing stuff going on we we didn't actually do an",
    "start": "2818970",
    "end": "2824370"
  },
  {
    "text": "announcement last week in a probably in announced but it's in the documentation today it works so yep so how does fast",
    "start": "2824370",
    "end": "2840150"
  },
  {
    "start": "2838000",
    "end": "2908000"
  },
  {
    "text": "clone handle foreign keys I mean you could imagine that essentially it's the exact like they're separate databases",
    "start": "2840150",
    "end": "2846000"
  },
  {
    "text": "it's at the block level so if you change seven blocks there's seven locks difference so everything you change you",
    "start": "2846000",
    "end": "2852480"
  },
  {
    "text": "change right there it's not a logical database level at all it's all at the physical block level so it's all at the",
    "start": "2852480",
    "end": "2859650"
  },
  {
    "text": "storage so it works perfectly there's no you know there's no difference it's like you made another copy of the database",
    "start": "2859650",
    "end": "2865380"
  },
  {
    "text": "you're just not paying for it no no once",
    "start": "2865380",
    "end": "2871380"
  },
  {
    "text": "you take the clone you're basically on a separate timeline at that point so no changes from the primary now flow down just you know just whatever you do there",
    "start": "2871380",
    "end": "2880920"
  },
  {
    "text": "yep",
    "start": "2880920",
    "end": "2883460"
  },
  {
    "text": "are you talking from like a the head node yeah so the question is really",
    "start": "2907550",
    "end": "2913380"
  },
  {
    "start": "2908000",
    "end": "2963000"
  },
  {
    "text": "about like sort of what's the what's the availability model on the on the non storage or storage pieces yeah so on",
    "start": "2913380",
    "end": "2920640"
  },
  {
    "text": "those you can think of them as being ephemeral right there is nothing persisted on those machines so they're",
    "start": "2920640",
    "end": "2927060"
  },
  {
    "text": "essentially disposable and that's why you have you can have multiply cui can either just replace it and it'll connect",
    "start": "2927060",
    "end": "2932640"
  },
  {
    "text": "back to the storage it'll be running or if you have a read-only copy will failover to it right so there's nothing",
    "start": "2932640",
    "end": "2938070"
  },
  {
    "text": "that we care about on the OS and that's actually very much the same as sort of what we do with RDS today in the multi",
    "start": "2938070",
    "end": "2945660"
  },
  {
    "text": "azy just that we use a different kind of storage for doing that right",
    "start": "2945660",
    "end": "2950690"
  },
  {
    "start": "2963000",
    "end": "2996000"
  },
  {
    "text": "yeah so we don't have a physical the question is we can use something like PG backrest we don't allow physical import into RDS",
    "start": "2963069",
    "end": "2972259"
  },
  {
    "text": "today for Postgres so you basically have to come in through a logical mechanism so something like DMS or dump and load",
    "start": "2972259",
    "end": "2978499"
  },
  {
    "text": "right it's definitely something we've had a lot of customers ask us about and you know trying to try to figure out but",
    "start": "2978499",
    "end": "2983779"
  },
  {
    "text": "that's because of our security model and some of the other things it's a little complicated in the back yeah what are",
    "start": "2983779",
    "end": "2997489"
  },
  {
    "start": "2996000",
    "end": "3056000"
  },
  {
    "text": "kind of the got you one of the gotchas from going from Oracle to Postgres I think the biggest one is just the difference on the MVCC so vacuum because",
    "start": "2997489",
    "end": "3005380"
  },
  {
    "text": "it's sort of a debt based system is a little different like whenever I ran Oracle like if you've got the Oracle thing up and running and you're running",
    "start": "3005380",
    "end": "3011229"
  },
  {
    "text": "for two hours you're like yeah it works you can basically be like that's it I know it's gonna work with Postgres you",
    "start": "3011229",
    "end": "3016449"
  },
  {
    "text": "have to go and look and say am i keeping up with like vacuuming and these other things because if you're not you might",
    "start": "3016449",
    "end": "3022209"
  },
  {
    "text": "look like you're working and then 14 days later you run out of transaction IDs right so that's kind of the slight",
    "start": "3022209",
    "end": "3027309"
  },
  {
    "text": "difference you know that's that's probably one of the biggest ones that people just don't kind of get right off",
    "start": "3027309",
    "end": "3032589"
  },
  {
    "text": "the bat that they have to do a little more long haul and you know to understand the change yes",
    "start": "3032589",
    "end": "3040798"
  },
  {
    "start": "3056000",
    "end": "3113000"
  },
  {
    "text": "so the question is really about ec2 terminations and stuff yeah so I mean well because we manage all the instances",
    "start": "3056200",
    "end": "3063100"
  },
  {
    "text": "right so we get the terminate like if you know an instance is going to be replaced we know that ahead of time and we'll do",
    "start": "3063100",
    "end": "3068230"
  },
  {
    "text": "that like during a maintenance window is the typical thing yeah so I mean the",
    "start": "3068230",
    "end": "3075790"
  },
  {
    "text": "failover times you're as low as like 10 seconds but you still DNS one of the nice things with Aurora and with the",
    "start": "3075790",
    "end": "3082300"
  },
  {
    "text": "basically Postgres in general is the JDBC driver now supports multiple endpoints so you can actually give it",
    "start": "3082300",
    "end": "3088000"
  },
  {
    "text": "you know let's say you have like three or four instances in a cluster you can actually give the JDBC driver all of",
    "start": "3088000",
    "end": "3094000"
  },
  {
    "text": "those and it will try them and so you don't actually have to wait for DNS so you can actually get slightly faster failover from that",
    "start": "3094000",
    "end": "3101760"
  },
  {
    "text": "yeah the question is what would be the kind of minimum version for Postgres that you'd like to be at before you move",
    "start": "3112630",
    "end": "3118630"
  },
  {
    "start": "3113000",
    "end": "3170000"
  },
  {
    "text": "into Aurora I mean I don't I mean obviously nine would be would be a good",
    "start": "3118630",
    "end": "3124210"
  },
  {
    "text": "starting point but I mean you know we we've always supported nine three and above so I you know kind of I think",
    "start": "3124210",
    "end": "3129550"
  },
  {
    "text": "that's the current supported list right and you know nine four obviously is the one that starts supporting logical",
    "start": "3129550",
    "end": "3135400"
  },
  {
    "text": "replication so if you're using DMS that come in you need to be at that at least but yeah there is depending on how",
    "start": "3135400",
    "end": "3140860"
  },
  {
    "text": "you're coming in you know how to customer say do I have to go to nine six before I can use DMS to come in I'm like no you can go nine four and you can",
    "start": "3140860",
    "end": "3146770"
  },
  {
    "text": "basically do your major version upgrade as part of you know moving in so yeah",
    "start": "3146770",
    "end": "3152820"
  },
  {
    "text": "right yeah I mean so I think we only",
    "start": "3163620",
    "end": "3172840"
  },
  {
    "start": "3170000",
    "end": "3242000"
  },
  {
    "text": "take in a couple away just like checkpoint interval obviously he's gone",
    "start": "3172840",
    "end": "3178000"
  },
  {
    "text": "which isn't vacuum I mean it's associated and then there's the there's the miss or the dirty miss cost or the",
    "start": "3178000",
    "end": "3184660"
  },
  {
    "text": "dirty page cost one because there is no sort of such thing because we don't checkpoint again right so we've just",
    "start": "3184660",
    "end": "3190210"
  },
  {
    "text": "removed the ability to modify those because they weren't doing anything and we didn't want to have people be frustrated by like they're like I change",
    "start": "3190210",
    "end": "3196420"
  },
  {
    "text": "this number and it doesn't seem to have an effect it's like yes so everything we've left still behaves the same right",
    "start": "3196420",
    "end": "3203380"
  },
  {
    "text": "we set the defaults for RDS I refer our or Postgres more aggressive than RDS",
    "start": "3203380",
    "end": "3210580"
  },
  {
    "text": "because again we've made back in Cheaper so we feel like we can set it more aggressive without impacting the",
    "start": "3210580",
    "end": "3215830"
  },
  {
    "text": "customer right but that's really about it all the other things sort of apply you know like you know how many tuples",
    "start": "3215830",
    "end": "3221290"
  },
  {
    "text": "you want you know what percentage of the table you want to have changed before you start vacuuming is really",
    "start": "3221290",
    "end": "3226300"
  },
  {
    "text": "application specific so you know all those kind of ones we've left because we don't you know you know your application",
    "start": "3226300",
    "end": "3232330"
  },
  {
    "text": "hopefully anymore",
    "start": "3232330",
    "end": "3237660"
  },
  {
    "start": "3242000",
    "end": "3290000"
  },
  {
    "text": "when are we going to support copying s-3 into aurora directly soon it is it is",
    "start": "3242260",
    "end": "3250840"
  },
  {
    "text": "something that's on our roadmap so it is something that we want to do I don't have a specific date at the moment for",
    "start": "3250840",
    "end": "3257260"
  },
  {
    "text": "it but yeah it is it is something we want to do and we have a lot of customers who want both in and out for",
    "start": "3257260",
    "end": "3262300"
  },
  {
    "text": "s3 obviously a very useful feature that we have in the Aurora my sequel in general what you'd see is any sort of",
    "start": "3262300",
    "end": "3268480"
  },
  {
    "text": "the major kind of features that we have an Aurora my sequel whether it's like lamb and integration those kind of things are all things that we want to",
    "start": "3268480",
    "end": "3273610"
  },
  {
    "text": "bring to Aurora Postgres you know we obviously want the family to be very consistent so customers aren't confused",
    "start": "3273610",
    "end": "3278920"
  },
  {
    "text": "by the differences so back there the",
    "start": "3278920",
    "end": "3290710"
  },
  {
    "start": "3290000",
    "end": "3355000"
  },
  {
    "text": "question is about dynamo features in sort of Aurora yeah I mean I think at",
    "start": "3290710",
    "end": "3297100"
  },
  {
    "text": "some point I mean Dax I don't know I don't think that necessarily makes a lot of sense because with the RORO one of the things",
    "start": "3297100",
    "end": "3303100"
  },
  {
    "text": "is we're trying to get away from actually have to have caching because you can just have more readers right and",
    "start": "3303100",
    "end": "3308530"
  },
  {
    "text": "because you're not paying for more storage it's actually possibly just as cost-effective to run just you know",
    "start": "3308530",
    "end": "3314710"
  },
  {
    "text": "Aurora on the sort of CDC output I mean today you can do that with DMS or you",
    "start": "3314710",
    "end": "3320770"
  },
  {
    "text": "can just grab the logical replication slot on RDS Postgres and that's obviously what we want to have for Aurora Postgres we haven't talked about",
    "start": "3320770",
    "end": "3326860"
  },
  {
    "text": "you know directly just having like you know a flag that says please you know send my logical update stream to Kinesis",
    "start": "3326860",
    "end": "3333010"
  },
  {
    "text": "but I mean it's a reasonable reasonable thing for us to consider definitely put it on the list yes",
    "start": "3333010",
    "end": "3340350"
  },
  {
    "text": "sorry you're gonna have to speak up I think white area right so the question",
    "start": "3345570",
    "end": "3355840"
  },
  {
    "start": "3355000",
    "end": "3390000"
  },
  {
    "text": "is if you hit like transaction wraparound is there anything different in Aurora then there is an RDS opposed to us nope not at the moment it's",
    "start": "3355840",
    "end": "3362859"
  },
  {
    "text": "definitely an area that we want to you know do work in and you know work with the community on I mean obviously it says you know it's still an issue but",
    "start": "3362859",
    "end": "3369280"
  },
  {
    "text": "yeah there's another question yeah",
    "start": "3369280",
    "end": "3373440"
  },
  {
    "text": "so I think the question is around you know are we gonna follow the Postgres standards around their language and stuff and yeah I mean we plan to be",
    "start": "3389480",
    "end": "3395780"
  },
  {
    "start": "3390000",
    "end": "3457000"
  },
  {
    "text": "Postgres compatible right I mean this is our customers don't want another database right they like Postgres they",
    "start": "3395780",
    "end": "3401690"
  },
  {
    "text": "like capabilities well we've tried to do is take some of the things that we thought we could improve based on making it cloud native but you know we're gonna",
    "start": "3401690",
    "end": "3408950"
  },
  {
    "text": "we're working on Postgres 10 for aurora and 4 RDS obviously you know we plan to",
    "start": "3408950",
    "end": "3414890"
  },
  {
    "text": "support 11 when it comes out you know in 12 and 13 so on right we might add extra things but you know",
    "start": "3414890",
    "end": "3422000"
  },
  {
    "text": "that's sort of how Postgres works it has extensions it has extra stuff but the core is the core and for that you know",
    "start": "3422000",
    "end": "3427520"
  },
  {
    "text": "we want to remain 100% compatible so you know kind of the promise is you'll always get at least all the Postgres",
    "start": "3427520",
    "end": "3433820"
  },
  {
    "text": "stuff maybe a few more things but at least all the Postgres stuff yes will",
    "start": "3433820",
    "end": "3447710"
  },
  {
    "text": "you talk about that offline yeah I'll talk about yes in the back behind so the",
    "start": "3447710",
    "end": "3457880"
  },
  {
    "start": "3457000",
    "end": "3481000"
  },
  {
    "text": "multi master like I said is is we're just improve you now with my sequel and it's something that we want to add for",
    "start": "3457880",
    "end": "3463130"
  },
  {
    "text": "Postgres but we don't have a firm timeline on that at the moment obviously",
    "start": "3463130",
    "end": "3468319"
  },
  {
    "text": "it's an engine it's an engine level feature so it's going to require quite a bit of difference it's not a storage level feature so over yeah so the",
    "start": "3468319",
    "end": "3481910"
  },
  {
    "start": "3481000",
    "end": "3517000"
  },
  {
    "text": "question is is the query optimizer the same yes it's exactly the same now there are obviously there's not any",
    "start": "3481910",
    "end": "3489260"
  },
  {
    "text": "differences on the optimizer there obviously is some differences on you know what it takes to write and read so",
    "start": "3489260",
    "end": "3494270"
  },
  {
    "text": "you know you can have some behavior change but if I think everyone that I've talked to so far their applications work",
    "start": "3494270",
    "end": "3500630"
  },
  {
    "text": "exactly the same you know from a query client perspective on both the engines so yeah",
    "start": "3500630",
    "end": "3508690"
  },
  {
    "start": "3517000",
    "end": "3577000"
  },
  {
    "text": "the question is are we contributing changes back to the core we are contributing changes back",
    "start": "3517089",
    "end": "3522339"
  },
  {
    "text": "we haven't contributed any large sort of features yet there are a number we're working on that we will be doing that",
    "start": "3522339",
    "end": "3528789"
  },
  {
    "text": "with but we've contributed you know a number of we found a number of bugs interesting some of them that we found",
    "start": "3528789",
    "end": "3534969"
  },
  {
    "text": "have been around for a while and for some reason because of the nature of how much we can write and some of the things",
    "start": "3534969",
    "end": "3540849"
  },
  {
    "text": "we found some some ones that hadn't been noticed before so that was good the customers were nice to point those",
    "start": "3540849",
    "end": "3546849"
  },
  {
    "text": "out to us and we you know we submitted patches and work with the community on getting those into the mainline so yeah",
    "start": "3546849",
    "end": "3553299"
  },
  {
    "text": "I mean we you know we we plan to we want a healthy Postgres community we believe",
    "start": "3553299",
    "end": "3558339"
  },
  {
    "text": "it's the the right thing for our customers that's what our customers want so we're gonna continue working with",
    "start": "3558339",
    "end": "3563440"
  },
  {
    "text": "them you know to make that happen you see I think we have any other questions",
    "start": "3563440",
    "end": "3572469"
  },
  {
    "text": "well thank you very much thanks for coming have a good rest your conference [Applause]",
    "start": "3572469",
    "end": "3579580"
  }
]