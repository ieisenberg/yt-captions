[
  {
    "text": "- Hello. I'm Aparna\nRamani, VP of AI, Data,",
    "start": "0",
    "end": "4290"
  },
  {
    "text": "and Developer Infrastructure\nEngineering at Meta,",
    "start": "4290",
    "end": "7380"
  },
  {
    "text": "and Pytorch Foundation Board member.",
    "start": "7380",
    "end": "10170"
  },
  {
    "text": "It is my pleasure to talk about",
    "start": "10170",
    "end": "11820"
  },
  {
    "text": "Meta's AI relationship with AWS.",
    "start": "11820",
    "end": "14820"
  },
  {
    "text": "Our collaboration has\nbeen expanding since 2018,",
    "start": "14820",
    "end": "17970"
  },
  {
    "text": "when Meta AI researchers started using AWS",
    "start": "17970",
    "end": "20670"
  },
  {
    "text": "for state-of-the-art AI research.",
    "start": "20670",
    "end": "23040"
  },
  {
    "text": "PyTorch is seeing great adoption",
    "start": "23040",
    "end": "24660"
  },
  {
    "text": "among large enterprises and startups,",
    "start": "24660",
    "end": "27570"
  },
  {
    "text": "and is a leading machine\nlearning framework today.",
    "start": "27570",
    "end": "30450"
  },
  {
    "text": "For years now, Meta's Pytorch engineers",
    "start": "30450",
    "end": "32790"
  },
  {
    "text": "have been collaborating with AWS",
    "start": "32790",
    "end": "34650"
  },
  {
    "text": "on key Pytorch projects such as",
    "start": "34650",
    "end": "36720"
  },
  {
    "text": "co-leading and maintaining TorchServe,",
    "start": "36720",
    "end": "39240"
  },
  {
    "text": "and making open source\ncontributions to TorchElastic.",
    "start": "39240",
    "end": "41970"
  },
  {
    "text": "More recently, we've been working together",
    "start": "41970",
    "end": "44220"
  },
  {
    "text": "on Pytorch enhancements for AWS",
    "start": "44220",
    "end": "46260"
  },
  {
    "text": "purpose-built ML chips,\nInferentia and Trainium.",
    "start": "46260",
    "end": "50460"
  },
  {
    "text": "We are excited to see AWS launch",
    "start": "50460",
    "end": "52890"
  },
  {
    "text": "Tranium based EC2 instances.",
    "start": "52890",
    "end": "55410"
  },
  {
    "text": "Our engineers saw near linear scaling",
    "start": "55410",
    "end": "58080"
  },
  {
    "text": "across the cranium cluster\nfor large language models.",
    "start": "58080",
    "end": "60990"
  },
  {
    "text": "Meta has also collaborated\nextensively with AWS",
    "start": "60990",
    "end": "64080"
  },
  {
    "text": "to provide native Pytorch support",
    "start": "64080",
    "end": "65700"
  },
  {
    "text": "for these new trainium powered instances.",
    "start": "65700",
    "end": "68451"
  },
  {
    "text": "AWS contributed a new accelerated backend",
    "start": "68451",
    "end": "70710"
  },
  {
    "text": "to torch distributed,",
    "start": "70710",
    "end": "72150"
  },
  {
    "text": "that makes it really easy\nto migrate your models",
    "start": "72150",
    "end": "74190"
  },
  {
    "text": "to trainium instances.",
    "start": "74190",
    "end": "75420"
  },
  {
    "text": "This also enables developers",
    "start": "75420",
    "end": "76890"
  },
  {
    "text": "to seamlessly integrate Pytorch\nwith their applications,",
    "start": "76890",
    "end": "79920"
  },
  {
    "text": "and leverage the speed",
    "start": "79920",
    "end": "81210"
  },
  {
    "text": "of distributor training\nlibraries and models.",
    "start": "81210",
    "end": "83280"
  },
  {
    "text": "We look forward to\ncontinuing our collaboration",
    "start": "83280",
    "end": "85380"
  },
  {
    "text": "through the Pytorch Foundation and beyond.",
    "start": "85380",
    "end": "87950"
  },
  {
    "text": "(upbeat outro music)",
    "start": "87950",
    "end": "91033"
  }
]