[
  {
    "text": "If you look at what's happening, \nreally we are seeing",
    "start": "0",
    "end": "2085"
  },
  {
    "text": "the confluence of three trends coming together.",
    "start": "2085",
    "end": "5470"
  },
  {
    "text": "First is the availability of large amounts of \nunlabeled data",
    "start": "5471",
    "end": "10835"
  },
  {
    "text": "from the Internet for unsupervised training.",
    "start": "10835",
    "end": "14580"
  },
  {
    "text": "The second important trend, is the evolution \nof model architectures towards Transformers.",
    "start": "14580",
    "end": "20279"
  },
  {
    "text": "The third part is the emergence of \ndomain specialization in hardware.",
    "start": "20280",
    "end": "26040"
  },
  {
    "text": "I recently sat down with \nSudipta Sengupta and Dan Roth,",
    "start": "40980",
    "end": "44520"
  },
  {
    "text": "both of whom are Distinguished Scientists at AWS.",
    "start": "44520",
    "end": "48420"
  },
  {
    "text": "I wanted to speak with them about architectures \nand models and learn more about the role they",
    "start": "48420",
    "end": "53519"
  },
  {
    "text": "play in the seemingly sudden \npopularity of Generative AI.",
    "start": "53520",
    "end": "58116"
  },
  {
    "text": "Sudpita and Dan have a long history and \ndeep knowledge of large language models",
    "start": "58116",
    "end": "63300"
  },
  {
    "text": "and the need for custom hardware to support \nthe growth and development in this space.",
    "start": "63300",
    "end": "68880"
  },
  {
    "text": "I was curious how they see traditional \narchitectures evolving and what they",
    "start": "68880",
    "end": "74399"
  },
  {
    "text": "think might be empowering this growth.",
    "start": "74400",
    "end": "77280"
  },
  {
    "text": "Dan, Sudipta, thank you for taking time \nto meet with me today and talk about this",
    "start": "77280",
    "end": "82260"
  },
  {
    "text": "magical area of generative AI. You both \nare distinguished scientists at Amazon.",
    "start": "82260",
    "end": "88800"
  },
  {
    "text": "How did you get into this role? \nBecause it's a quite unique role.",
    "start": "88800",
    "end": "92820"
  },
  {
    "text": "All my career has been in academia, \nso I've been for about 20 years.",
    "start": "92820",
    "end": "98400"
  },
  {
    "text": "I was a professor at University of Illinois \nin Urbana Champagne, then the last five,",
    "start": "98400",
    "end": "103500"
  },
  {
    "text": "six years at University of Pennsylvania \ndoing work in a wide range of topics in AI,",
    "start": "103500",
    "end": "111300"
  },
  {
    "text": "machine learning, reasoning, \nand natural language processing.",
    "start": "111300",
    "end": "115320"
  },
  {
    "text": "Sudipta-",
    "start": "115320",
    "end": "117480"
  },
  {
    "text": "I was before this at Microsoft Research, \nand before that at Bell Labs. And one of the",
    "start": "117480",
    "end": "123000"
  },
  {
    "text": "best things I liked in my previous research career \nwas not just doing the research, but getting it",
    "start": "123000",
    "end": "131760"
  },
  {
    "text": "into products, kind of understanding the end \nto end pipeline from conception to production",
    "start": "131760",
    "end": "138120"
  },
  {
    "text": "and meeting customer needs. So when I joined \nAmazon and AWS, I kind of doubled down on that.",
    "start": "138120",
    "end": "145260"
  },
  {
    "text": "So if you look at your space and this \nwhole generative AI seems to have just",
    "start": "145260",
    "end": "151019"
  },
  {
    "text": "come around the corner out of nowhere, \nbut I don't think that's the case, is it?",
    "start": "151020",
    "end": "156833"
  },
  {
    "text": "I mean, you've been working \non this for quite a while already.",
    "start": "156833",
    "end": "160319"
  },
  {
    "text": "It's a process that, in fact, \nhas been going for 30-40 years.",
    "start": "160320",
    "end": "163740"
  },
  {
    "text": "In fact, if you look at the progress in machine \nlearning, and maybe even more significantly in",
    "start": "163740",
    "end": "169980"
  },
  {
    "text": "the context of natural language processing and \nrepresentation of natural languages, say in the",
    "start": "169980",
    "end": "174360"
  },
  {
    "text": "last ten years, and more rapidly, let's say in \nthe last five years since Transformers came up.",
    "start": "174360",
    "end": "181440"
  },
  {
    "text": "But a lot of the building blocks \nactually were there ten years ago.",
    "start": "181440",
    "end": "185880"
  },
  {
    "text": "And some of the key ideas actually earlier,",
    "start": "185880",
    "end": "188820"
  },
  {
    "text": "only that we didn't have the \narchitecture to support this one.",
    "start": "188820",
    "end": "191687"
  },
  {
    "text": "Really we are seeing the confluence of \nthree trends coming together.",
    "start": "191687",
    "end": "195720"
  },
  {
    "text": "First is the availability of \nlarge amounts of unlabeled",
    "start": "195720",
    "end": "201120"
  },
  {
    "text": "data from the Internet for unsupervised training.",
    "start": "201120",
    "end": "205680"
  },
  {
    "text": "The models get a lot of their basic \ncapabilities from this unsupervised training.",
    "start": "205680",
    "end": "212040"
  },
  {
    "text": "Examples are like basic grammar, language \nunderstanding, and knowledge about facts.",
    "start": "212040",
    "end": "218219"
  },
  {
    "text": "The second important trend is the evolution \nof model architectures towards Transformers,",
    "start": "218220",
    "end": "223740"
  },
  {
    "text": "where they can take input context into",
    "start": "223740",
    "end": "227040"
  },
  {
    "text": "account and dynamically attend \nto different parts of the input.",
    "start": "227040",
    "end": "232019"
  },
  {
    "text": "The third part is the emergence of \ndomain specialization in hardware,",
    "start": "232020",
    "end": "237300"
  },
  {
    "text": "where you can exploit the computation structure \nof deep learning to keep riding on Moore's Law.",
    "start": "237300",
    "end": "245460"
  },
  {
    "text": "Transformer-based neural network architectures \nhave led to the rise of large-scale models",
    "start": "245460",
    "end": "251706"
  },
  {
    "text": "that contain hundreds of billions of parameters.",
    "start": "251706",
    "end": "255396"
  },
  {
    "text": "A parameter is a configuration variable internal \nto the model",
    "start": "255397",
    "end": "259595"
  },
  {
    "text": "whose value can be adjusted to optimize \nthe model's performance",
    "start": "259595",
    "end": "263464"
  },
  {
    "text": "For example, in a neural network, the connections \nbetween neurons",
    "start": "263464",
    "end": "268264"
  },
  {
    "text": "are defined by parameters such as weights and biases,",
    "start": "268265",
    "end": "271551"
  },
  {
    "text": "which can be tuned during training in order to \nimprove the network's accuracy.",
    "start": "271551",
    "end": "276869"
  },
  {
    "text": "During training, the parameters are updated \niteratively in order to minimize the error",
    "start": "276869",
    "end": "283494"
  },
  {
    "text": "between the network's predictions \nand the actual values,",
    "start": "283494",
    "end": "286972"
  },
  {
    "text": "and to improve the model's performance \non the given task.",
    "start": "286972",
    "end": "290926"
  },
  {
    "text": "To give you a sense of scale, model \nsizes have increased from 100s of millions",
    "start": "290926",
    "end": "296123"
  },
  {
    "text": "to 100s of billions in just a few years.",
    "start": "296123",
    "end": "300356"
  },
  {
    "text": "I wanted to understand what that \nreally means, and where we are headed next.",
    "start": "300356",
    "end": "305580"
  },
  {
    "text": "Parameters is just one part of the story. It's not \njust about number of parameters, but also training",
    "start": "305580",
    "end": "313319"
  },
  {
    "text": "data and volume and the training methodology.\nSo you can think about increasing parameters",
    "start": "313320",
    "end": "320280"
  },
  {
    "text": "as kind of increasing the representational \ncapacity of the model to learn from the data.",
    "start": "320280",
    "end": "326280"
  },
  {
    "text": "So as this learning capacity increases, you \nneed to satisfy it with diverse high quality",
    "start": "326280",
    "end": "333780"
  },
  {
    "text": "and large volume of data. In fact, in the \ncommunity today, there is understanding of",
    "start": "333780",
    "end": "341040"
  },
  {
    "text": "empirical scaling laws that predict the optimal \ncombinations of model size and data volume to",
    "start": "341040",
    "end": "349320"
  },
  {
    "text": "maximize accuracy for a given compute budget.\nWe have these models that are based on billions",
    "start": "349320",
    "end": "356580"
  },
  {
    "text": "of parameters. Yeah, and the corpus \nis the complete data on the internet",
    "start": "356580",
    "end": "362340"
  },
  {
    "text": "and customers can fine tune this by adding \njust a few hundred examples of what it is.",
    "start": "362340",
    "end": "368280"
  },
  {
    "text": "How is that possible? That \nit's only a few hundred that",
    "start": "368280",
    "end": "371760"
  },
  {
    "text": "are needed to actually create a new task model?",
    "start": "371760",
    "end": "375180"
  },
  {
    "text": "If all you care about is one task, if \nyou want to do text classification or",
    "start": "375180",
    "end": "379440"
  },
  {
    "text": "sentiment analysis and you don't care about \nanything else, it's still better perhaps to",
    "start": "379440",
    "end": "385140"
  },
  {
    "text": "just stay with the old machine learning \nwith strong models but annotate data.",
    "start": "385140",
    "end": "390000"
  },
  {
    "text": "The model is going to be \nsmall, no latency, less cost.",
    "start": "390000",
    "end": "395940"
  },
  {
    "text": "But AWS has a lot of models like this that \nsolve specific problems very very well.",
    "start": "395940",
    "end": "402480"
  },
  {
    "text": "Now, if you want models that you can actually \nvery easily move from one task to another that",
    "start": "402480",
    "end": "409800"
  },
  {
    "text": "are capable of performing multiple tasks, then \nthese abilities of foundational model come in.",
    "start": "409800",
    "end": "415319"
  },
  {
    "text": "Because these models kind of \nknow language in a sense, right?",
    "start": "415320",
    "end": "419400"
  },
  {
    "text": "So they know how to generate sentences,",
    "start": "419400",
    "end": "421500"
  },
  {
    "text": "they have an understanding of what \ncomes next in a given sentence.",
    "start": "421500",
    "end": "425520"
  },
  {
    "text": "And now if you want to specialize it to text",
    "start": "425520",
    "end": "428940"
  },
  {
    "text": "classification or to sentiment analysis \nor to question answering or summarization,",
    "start": "428940",
    "end": "433860"
  },
  {
    "text": "you need to give it supervised data, \nannotated data and fine tune on this.",
    "start": "433860",
    "end": "439860"
  },
  {
    "text": "And basically it kind of massages \nthe space of the function that we",
    "start": "439860",
    "end": "444599"
  },
  {
    "text": "are using for prediction in the right way and \nhundreds of examples are often sufficient.",
    "start": "444600",
    "end": "448800"
  },
  {
    "text": "So the fine tuning is basically supervised.",
    "start": "448800",
    "end": "452460"
  },
  {
    "text": "Supervised, yeah.",
    "start": "452460",
    "end": "453240"
  },
  {
    "text": "So you combine supervised and \nunsupervised learning in the same bucket.",
    "start": "453240",
    "end": "458280"
  },
  {
    "text": "Basically, again, this is very well aligned with \nour understanding in the cognitive sciences of",
    "start": "458280",
    "end": "464520"
  },
  {
    "text": "early childhood development that the kids, \nbabies, toddlers learn really well just",
    "start": "464520",
    "end": "470280"
  },
  {
    "text": "by observation who is speaking, pointing \ncorrelating with spoken speech and so on.",
    "start": "470280",
    "end": "476639"
  },
  {
    "text": "So a lot of this unsupervised learning \nis going on and kind of quote unquote,",
    "start": "476640",
    "end": "482700"
  },
  {
    "text": "free unlabeled data that's available \nin vast amounts on the Internet.",
    "start": "482700",
    "end": "488580"
  },
  {
    "text": "One component that I want \nto add that really led to",
    "start": "488580",
    "end": "491400"
  },
  {
    "text": "this breakthrough is the issue of representation.",
    "start": "491400",
    "end": "494340"
  },
  {
    "text": "So if you think about how to represent words,",
    "start": "494340",
    "end": "498060"
  },
  {
    "text": "it used to be in old machine learning \nthat words for us were discrete objects.",
    "start": "498060",
    "end": "502380"
  },
  {
    "text": "So you open a dictionary, you see \nwords and they are listed this way.",
    "start": "502380",
    "end": "505200"
  },
  {
    "text": "So there is a “table” and \nthere's a “desk” somewhere there,",
    "start": "505200",
    "end": "508020"
  },
  {
    "text": "and there are completely different things.",
    "start": "508020",
    "end": "509639"
  },
  {
    "text": "So what happened about ten years ago is that we \nmoved completely to continuous representation of",
    "start": "509640",
    "end": "516300"
  },
  {
    "text": "words, where the idea is that we represent words \nas vectors, dense vectors, where similar words,",
    "start": "516300",
    "end": "522180"
  },
  {
    "text": "semantically, are represented very \nclose to each other in this space.",
    "start": "522180",
    "end": "526920"
  },
  {
    "text": "So now table and desk are next to each other. \nSo that's the first step that allows us to",
    "start": "526920",
    "end": "532079"
  },
  {
    "text": "actually move to more semantic representation \nof words and then sentences and larger units.",
    "start": "532080",
    "end": "538620"
  },
  {
    "text": "So that's kind of the key breakthrough.",
    "start": "538620",
    "end": "540420"
  },
  {
    "text": "And the next step was to \nrepresent things contextually.",
    "start": "540420",
    "end": "543600"
  },
  {
    "text": "So the word “table” that we sit next to now,",
    "start": "543600",
    "end": "546899"
  },
  {
    "text": "versus the word table that we are using to \nstore data in, are now going to be different",
    "start": "546900",
    "end": "552480"
  },
  {
    "text": "elements in this vector space because they \ncome, they appear in different contexts.",
    "start": "552480",
    "end": "556860"
  },
  {
    "text": "A big reason for the rise of foundation \nmodels is the transformer architecture.",
    "start": "556860",
    "end": "561928"
  },
  {
    "text": "Transformers pre-process text inputs as embeddings,",
    "start": "561929",
    "end": "566321"
  },
  {
    "text": "which are mathematical representations of a word.",
    "start": "566321",
    "end": "569553"
  },
  {
    "text": "When encoded in vector space, words \nthat are closer together",
    "start": "569554",
    "end": "574537"
  },
  {
    "text": "are expected to be closer in meaning.",
    "start": "574537",
    "end": "577253"
  },
  {
    "text": "These embeddings are processed through \nan encoder component",
    "start": "577253",
    "end": "580826"
  },
  {
    "text": "that captures contextual information \nfrom an input sequence.",
    "start": "580826",
    "end": "584660"
  },
  {
    "text": "A decoder component can apply this\nencoded context",
    "start": "584660",
    "end": "589048"
  },
  {
    "text": " and emit output text.",
    "start": "589049",
    "end": "592307"
  },
  {
    "text": "Compared to its predecessors, like recurrent neural nets,",
    "start": "592307",
    "end": "596046"
  },
  {
    "text": "transformers are more parallelizable",
    "start": "596046",
    "end": "598238"
  },
  {
    "text": "because they do not process words \nsequentially one at a time;",
    "start": "598238",
    "end": "601496"
  },
  {
    "text": "but instead, process the entire input all \nat once during the learning cycle.",
    "start": "601497",
    "end": "606540"
  },
  {
    "text": "Now that we have this, you can encode these \nkind of things in this neural architecture,",
    "start": "606540",
    "end": "611279"
  },
  {
    "text": "very dense neural architecture, \nmultilayer neural architecture.",
    "start": "611280",
    "end": "614760"
  },
  {
    "text": "And now you can start representing larger object \nand you can represent semantics of bigger objects.",
    "start": "614760",
    "end": "622380"
  },
  {
    "text": "How is it that the Transformer architecture",
    "start": "622380",
    "end": "627258"
  },
  {
    "text": "allows you to do unsupervised training?",
    "start": "627258",
    "end": "630001"
  },
  {
    "text": "Why is that? Why do you no longer \nneed to label the data?",
    "start": "630001",
    "end": "634019"
  },
  {
    "text": "So really, when you learn representation \nof word, what we do is self-training.",
    "start": "634020",
    "end": "640140"
  },
  {
    "text": "I think a better terminology \nfor it is self-training.",
    "start": "640140",
    "end": "642840"
  },
  {
    "text": "The idea is that you take a sentence that is \ncorrect, that you read it of the newspaper,",
    "start": "642840",
    "end": "647160"
  },
  {
    "text": "you drop a word and you try to predict the world \ngiven the context, either the two sided context",
    "start": "647160",
    "end": "653759"
  },
  {
    "text": "or the left sided context.\nAnd so essentially you",
    "start": "653760",
    "end": "657120"
  },
  {
    "text": "do supervised learning, right?\nBecause you're trying to predict",
    "start": "657120",
    "end": "659640"
  },
  {
    "text": "a world and you know the truth.\nSo you can verify whether your",
    "start": "659640",
    "end": "662880"
  },
  {
    "text": "predictor model actually does it well or not.\nBut you don't need to annotate data for this.",
    "start": "662880",
    "end": "668340"
  },
  {
    "text": "And this is the basic, very \nsimple objective function.",
    "start": "668340",
    "end": "672360"
  },
  {
    "text": "Drop a word, try to predict it.\nThat drives almost all the learning",
    "start": "672360",
    "end": "677100"
  },
  {
    "text": "that we are doing today.\nAnd it gives us the ability",
    "start": "677100",
    "end": "679800"
  },
  {
    "text": "to learn good representations of words.",
    "start": "679800",
    "end": "683204"
  },
  {
    "text": "If I look at not only at the past five\nyears with these larger models,",
    "start": "683204",
    "end": "687448"
  },
  {
    "text": "but if I look at \nsort of the evolution of machine learning in the",
    "start": "687448",
    "end": "690720"
  },
  {
    "text": "past 10-15 years, it seems to have been \nsort of this lockstep where new software",
    "start": "690720",
    "end": "696899"
  },
  {
    "text": "arises, new hardware is being built, \nnew software comes, new hardware.",
    "start": "696900",
    "end": "701160"
  },
  {
    "text": "And a sort of acceleration \nhappened of the application of it.",
    "start": "701160",
    "end": "706440"
  },
  {
    "text": "So most of this was done on \nGPUs and the evolution of GPUs,",
    "start": "706440",
    "end": "710820"
  },
  {
    "text": "but they're extremely power hungry beasts.\nWhy are GPUs the best way of training this?",
    "start": "710820",
    "end": "719100"
  },
  {
    "text": "And why are we moving to custom \nsilicon? Because of the power?",
    "start": "719100",
    "end": "724199"
  },
  {
    "text": "Yes so one of the things that is fundamental \nin computing is if you can specialize the",
    "start": "724200",
    "end": "730380"
  },
  {
    "text": "computation, you can make the silicon optimized \nfor that specific computation structure instead",
    "start": "730380",
    "end": "737520"
  },
  {
    "text": "of being very generic like CPUs are.\nSo what is interesting about deep",
    "start": "737520",
    "end": "743400"
  },
  {
    "text": "learning is that it's essentially \nlow precision linear algebra, right?",
    "start": "743400",
    "end": "748080"
  },
  {
    "text": "So if I can do this linear algebra really \nwell, then I can have a very power efficient,",
    "start": "748080",
    "end": "756300"
  },
  {
    "text": "cost efficient, high performance \nprocessor for deep learning.",
    "start": "756300",
    "end": "760380"
  },
  {
    "text": "Is the architecture of, let's say,",
    "start": "760380",
    "end": "763620"
  },
  {
    "text": "the Trainium radically different \nfrom, let's say, general purpose GPU?",
    "start": "763620",
    "end": "767460"
  },
  {
    "text": "So really, it is really \noptimized for deep learning.",
    "start": "767460",
    "end": "771780"
  },
  {
    "text": "So the systolic array for matrix \nmultiplication, you have like a",
    "start": "771780",
    "end": "778380"
  },
  {
    "text": "small number of large systolic arrays and \nthe memory hierarchy is optimized for deep",
    "start": "778380",
    "end": "783600"
  },
  {
    "text": "learning workload patterns versus something \nlike a GPU, which has to kind of cater to a",
    "start": "783600",
    "end": "790800"
  },
  {
    "text": "broader set of markets like high performance \ncomputing, HPC graphics and deep learning.",
    "start": "790800",
    "end": "796740"
  },
  {
    "text": "So the more you can specialize and scope down \nthe domain, the more you can optimize in silicon.",
    "start": "796740",
    "end": "804480"
  },
  {
    "text": "And that's the opportunity that we \nare seeing currently in deep learning.",
    "start": "804480",
    "end": "808860"
  },
  {
    "text": "Okay, if I think about sort of the hype that is \nall there in the past days or the past weeks,",
    "start": "808860",
    "end": "817079"
  },
  {
    "text": "it looks like this is the end all of machine \nlearning and this little magic happens.",
    "start": "817080",
    "end": "826200"
  },
  {
    "text": "But there must be limitations to this.\nThere are things that they can do well",
    "start": "826200",
    "end": "829500"
  },
  {
    "text": "and things that it probably cannot do \nwell at all. Do you have a sense of that?",
    "start": "829500",
    "end": "833760"
  },
  {
    "text": "We have to understand that language \nmodels cannot do everything right.",
    "start": "833760",
    "end": "837360"
  },
  {
    "text": "So aggregation is a key thing that they cannot do.",
    "start": "837360",
    "end": "839579"
  },
  {
    "text": "Various logical operations is \nsomething that they cannot do well.",
    "start": "839580",
    "end": "843300"
  },
  {
    "text": "Arithmetic is a key thing \nor mathematical reasoning.",
    "start": "843300",
    "end": "847200"
  },
  {
    "text": "So what language models can \ndo today, if trained properly,",
    "start": "847200",
    "end": "851400"
  },
  {
    "text": "is to generate some mathematical expressions \nwell, but they cannot do the math.",
    "start": "851400",
    "end": "856020"
  },
  {
    "text": "So you have to figure out mechanism to \nenrich these with calculators, if you want.",
    "start": "856020",
    "end": "859860"
  },
  {
    "text": "Spatial reasoning, this is \nsomething that requires grounding.",
    "start": "859860",
    "end": "864600"
  },
  {
    "text": "So if I tell you go straight \nand then turn left and then",
    "start": "864600",
    "end": "867540"
  },
  {
    "text": "turn left and then turn left, where are you now?",
    "start": "867540",
    "end": "869706"
  },
  {
    "text": "It's something that three year old’s will know,",
    "start": "869706",
    "end": "873779"
  },
  {
    "text": "but language models will not \nbecause they are not grounded.",
    "start": "873780",
    "end": "877200"
  },
  {
    "text": "And there are various kinds of \nreasoning, common sense reasoning.",
    "start": "877200",
    "end": "880800"
  },
  {
    "text": "I talked about temporal reasoning a little bit.",
    "start": "880800",
    "end": "883019"
  },
  {
    "text": "These models don't have a notion of \ntime unless it's written somewhere.",
    "start": "883020",
    "end": "886920"
  },
  {
    "text": "Can we expect that these problems \nwill be solved over time or...?",
    "start": "886920",
    "end": "891626"
  },
  {
    "text": "I think they will be solved.",
    "start": "891626",
    "end": "893126"
  },
  {
    "text": "Some of these challenges are also opportunities.",
    "start": "893126",
    "end": "895860"
  },
  {
    "text": "So when a language model does not know how \nto do something, it can figure out that it",
    "start": "895860",
    "end": "901800"
  },
  {
    "text": "needs to call an external agent, as Dan said, \nhe gave the example of calculators, right?",
    "start": "901800",
    "end": "906240"
  },
  {
    "text": "So if I can't do the math, I can \ngenerate an expression which the",
    "start": "906240",
    "end": "911160"
  },
  {
    "text": "calculator will execute correctly, right?\nSo I think we are going to see opportunities",
    "start": "911160",
    "end": "916764"
  },
  {
    "text": "for language models to call external \nagents or APIs to do what they don't",
    "start": "916764",
    "end": "924540"
  },
  {
    "text": "know how to do and just call them with the right \narguments and synthesize the results back into",
    "start": "924540",
    "end": "930779"
  },
  {
    "text": "the conversation or their output.\nThat's a huge opportunity.",
    "start": "930780",
    "end": "934796"
  },
  {
    "text": "Well, thank you very much, guys. I really \nenjoyed this. You really educated me on the",
    "start": "934796",
    "end": "942060"
  },
  {
    "text": "real truth behind large language models \nand generative AI. Thank you very much.",
    "start": "942060",
    "end": "946740"
  },
  {
    "text": "Thank you.",
    "start": "946740",
    "end": "948899"
  },
  {
    "text": "Thanks to Sudipta and Dan for taking us on this",
    "start": "948900",
    "end": "951600"
  },
  {
    "text": "journey to understand where \nthe future might be headed.",
    "start": "951600",
    "end": "954000"
  },
  {
    "text": "And to begin to shed light on the \nscale of these vastly complex models.",
    "start": "954000",
    "end": "959760"
  },
  {
    "text": "And, the energy needed from custom hardware to \nreach new efficiencies in performance and cost.",
    "start": "959760",
    "end": "967020"
  },
  {
    "text": "The industry is rapidly working to better \nunderstand where these technologies are headed.",
    "start": "967020",
    "end": "972240"
  },
  {
    "text": "I’m looking forward to learning more from \nengineers and developers about the impact",
    "start": "972240",
    "end": "977040"
  },
  {
    "text": "Generative AI will have on everything from \ncoding to testing to building infrastructure.",
    "start": "977040",
    "end": "982800"
  }
]