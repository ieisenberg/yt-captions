[
  {
    "start": "0",
    "end": "163000"
  },
  {
    "text": "my name is EV Dan weeks and I both work on the Netflix Big Data platform team",
    "start": "120",
    "end": "5879"
  },
  {
    "text": "and we are very happy to be here today to talk about how we run Presto and Spark on the Netflix Big Data",
    "start": "5879",
    "end": "13879"
  },
  {
    "text": "platform so in this session we're going to talk about our big data scale um the",
    "start": "13879",
    "end": "19160"
  },
  {
    "text": "architecture that we use to achieve the scale that we need for Presto and Spark we're going to cover the expanding use",
    "start": "19160",
    "end": "25519"
  },
  {
    "text": "cases that we have the performance that we see the contributions that we make to achieve some of the performance gain",
    "start": "25519",
    "end": "31720"
  },
  {
    "text": "that we get how we Deploy on EMR and also how we integrate with the Netflix",
    "start": "31720",
    "end": "38920"
  },
  {
    "text": "infrastructure so at NX uh product decision is largely data informed and data driven and",
    "start": "39079",
    "end": "46120"
  },
  {
    "text": "um we use data to find the most optimal solution for any product idea that we",
    "start": "46120",
    "end": "51520"
  },
  {
    "text": "have and at Netflix The Big Data platform is leverage across the whole company for doing analytics and ETL",
    "start": "51520",
    "end": "57480"
  },
  {
    "text": "needs and I'm going to go through three examples that we use for the big uh that we use um the Big Data platform",
    "start": "57480",
    "end": "65320"
  },
  {
    "text": "for so this is a screenshot of our annite application it is an AB test analytics application that we use um our",
    "start": "65320",
    "end": "72640"
  },
  {
    "text": "product manager would use this application to interactively look at how the test Cas is performing at Netflix we",
    "start": "72640",
    "end": "79240"
  },
  {
    "text": "could uh at any given point in time we could be running hundreds of test cases at the same time so in this screenshot",
    "start": "79240",
    "end": "85960"
  },
  {
    "text": "you see that they would look at retention and streaming hours across different test cells across different regions and different",
    "start": "85960",
    "end": "92159"
  },
  {
    "text": "devices the data that power this application is generated by ETL processes running on the big data",
    "start": "92159",
    "end": "99280"
  },
  {
    "text": "platform we really care about user streaming experience so we also publish a monthly dashboard that shows the",
    "start": "99280",
    "end": "105960"
  },
  {
    "text": "average user streaming speed across different isps across different countries uh so in case you're wondering",
    "start": "105960",
    "end": "113000"
  },
  {
    "text": "why your streaming doesn't work so well you can you know take a look at this dashboard",
    "start": "113000",
    "end": "118079"
  },
  {
    "text": "also so we also use uh the Big Data platform to train data model for our recommendation engine and also uh uh",
    "start": "118079",
    "end": "125560"
  },
  {
    "text": "help build the search index for our service so these are just three examples that I picked and there are many more",
    "start": "125560",
    "end": "132160"
  },
  {
    "text": "use case that we use the big data platform for in Netflix so onto our scale um currently",
    "start": "132160",
    "end": "139319"
  },
  {
    "text": "we are one of the top three biggest service running at Netflix on AWS the",
    "start": "139319",
    "end": "144680"
  },
  {
    "text": "biggest is atlas which is our Telemetry system um the second biggest is Cassandra they run over 200 data Rings",
    "start": "144680",
    "end": "151800"
  },
  {
    "text": "across all the regions uh that we support and uh that we run on and Cassandra is our online data",
    "start": "151800",
    "end": "159159"
  },
  {
    "text": "store and we are the third and the scale that we need comes",
    "start": "159159",
    "end": "165360"
  },
  {
    "start": "163000",
    "end": "163000"
  },
  {
    "text": "from our business we currently have over 65 million members in over 50 countries supporting over a thousand devices",
    "start": "165360",
    "end": "172400"
  },
  {
    "text": "serving 10 billion hours per quarter accounting for over 37% of internet traffic during peak hours",
    "start": "172400",
    "end": "180519"
  },
  {
    "start": "180000",
    "end": "180000"
  },
  {
    "text": "with all that business needs to translate into this big data scale that we have we currently have over 25 pyte",
    "start": "180519",
    "end": "187159"
  },
  {
    "text": "of data on S3 and S3 is our data warehouse storage layer and on a daily",
    "start": "187159",
    "end": "193120"
  },
  {
    "text": "basis we run ETL and analytics jobs that read Around 10% of this data um process",
    "start": "193120",
    "end": "199519"
  },
  {
    "text": "them and in terms of data size we sped back out or generate back out uh 10% of",
    "start": "199519",
    "end": "205239"
  },
  {
    "text": "the data that we read and put it back onto S3 so that gives you an idea of how active or the data Final heart looks",
    "start": "205239",
    "end": "212159"
  },
  {
    "text": "like in our data warehouse and on a daily basis we get 550 billion events",
    "start": "212159",
    "end": "217480"
  },
  {
    "text": "from our service and we have uh around 350 active users at Netflix using our",
    "start": "217480",
    "end": "224400"
  },
  {
    "text": "big data platform in the past four weeks so we need to continue to scale",
    "start": "224400",
    "end": "229799"
  },
  {
    "text": "our platform because we want to be in 200 countries by the end of next year so let's move on to talk about the",
    "start": "229799",
    "end": "236799"
  },
  {
    "text": "architecture that we built to achieve the scale that we need so first let's take a look at where the",
    "start": "236799",
    "end": "242680"
  },
  {
    "start": "241000",
    "end": "241000"
  },
  {
    "text": "data comes from um there are two main data pipeline the first one is the event pipeline an example of an event would be",
    "start": "242680",
    "end": "250319"
  },
  {
    "text": "like you click play on a device or you search for a movie on our website 550",
    "start": "250319",
    "end": "256880"
  },
  {
    "text": "billions events go into the Kafka data Pipeline and then go into urser urser is a distributed system that we built that",
    "start": "256880",
    "end": "263960"
  },
  {
    "text": "actually um merge each Kafka Topic at the end of the hour and each Kafka topic",
    "start": "263960",
    "end": "270840"
  },
  {
    "text": "would present as a hive table in our data warehouse and we also for certain Kafka topic or Hive table we would pick",
    "start": "270840",
    "end": "276919"
  },
  {
    "text": "out key value pair to promote them into top column in Hi and then Ursel is going",
    "start": "276919",
    "end": "282240"
  },
  {
    "text": "to write the data onto estri and write the metadata onto hi one point to note",
    "start": "282240",
    "end": "287320"
  },
  {
    "text": "is we are in the in the middle of transitioning to Kafka um because it has",
    "start": "287320",
    "end": "292759"
  },
  {
    "text": "at least one semantics it's more reliable and durable and also that's largely where the industry is going for",
    "start": "292759",
    "end": "299240"
  },
  {
    "text": "data pipeline in the open source World um for the dimension pipeline Dimension",
    "start": "299240",
    "end": "304400"
  },
  {
    "text": "data is something like user subscription data we have an eges pipeline that we build that we do daily extraction from",
    "start": "304400",
    "end": "311919"
  },
  {
    "text": "our uh online data store Cassandra into our data warehouse for the uh Dimension",
    "start": "311919",
    "end": "319720"
  },
  {
    "text": "data and then if you imagine yourself looking to the right side of the S3 bucket um that's the layer um",
    "start": "319720",
    "end": "326880"
  },
  {
    "text": "architecture diagram of our big data platform on the lowest layers of storage um we use S3 as our data warehouse",
    "start": "326880",
    "end": "333160"
  },
  {
    "text": "storage um our data warehouse is largely in park file format on top we provide a",
    "start": "333160",
    "end": "338880"
  },
  {
    "text": "number of big data processing tools for our users to use and I'm going to go over in more details in the next slide",
    "start": "338880",
    "end": "345919"
  },
  {
    "text": "um there are two key services on top that we built one is Genie Genie is a metad um uh is a Federated execution",
    "start": "345919",
    "end": "353240"
  },
  {
    "text": "engine that expose an API for the users to specify a specific Big Data command",
    "start": "353240",
    "end": "360080"
  },
  {
    "text": "or SLA and priority so that Genie is going to issue can issue the job in um",
    "start": "360080",
    "end": "366160"
  },
  {
    "text": "one of the Clusters uh in the compute layer and Genie itself is actually open",
    "start": "366160",
    "end": "371720"
  },
  {
    "text": "source on Netflix OSS um and we are also aware of a couple of companies um that",
    "start": "371720",
    "end": "377080"
  },
  {
    "text": "are using Genie one advantage of Genie is also that you could actually swap out cluster in the back without impacting",
    "start": "377080",
    "end": "383599"
  },
  {
    "text": "the user and that's why we could do R black pushes of our cluster when we need to do cluster upgrade",
    "start": "383599",
    "end": "389919"
  },
  {
    "text": "metat is a Federated metadata service that proxy metadata request back to the original data sources that we have like",
    "start": "389919",
    "end": "396680"
  },
  {
    "text": "Hive Terra dat Rush shift and RDS um and it is it provides an integrated um API",
    "start": "396680",
    "end": "404240"
  },
  {
    "text": "layer for all the tools to access metadata and it also provide an opportunity for us to add additional",
    "start": "404240",
    "end": "409919"
  },
  {
    "text": "metadata context like data category TTL retention data ownership that is",
    "start": "409919",
    "end": "415800"
  },
  {
    "text": "Meaningful for us to track in the Big Data platform team and we will open source medicad in the",
    "start": "415800",
    "end": "422160"
  },
  {
    "text": "near future on Netflix OSS as well on the tools layer we provide a a whole set",
    "start": "422160",
    "end": "427919"
  },
  {
    "text": "of tools for our users to use so that they could be more self- sered more productive and and the data platform",
    "start": "427919",
    "end": "433599"
  },
  {
    "text": "team can scale a lot better I'm not going to go over all of them and I just want to point out that both lipstick and",
    "start": "433599",
    "end": "439639"
  },
  {
    "text": "inviso are actually open source on Netflix OSS as well so if you're interested you could take a",
    "start": "439639",
    "end": "445479"
  },
  {
    "text": "look then on top of the uh tools layer we buil an integrated API layer so that",
    "start": "445479",
    "end": "451160"
  },
  {
    "text": "that that layer is actually a python library that allow integrated access to all the tools and services we use that",
    "start": "451160",
    "end": "457879"
  },
  {
    "text": "API uh from our external scheduler to schedule jobs in our system and we also",
    "start": "457879",
    "end": "463080"
  },
  {
    "text": "have a One-Stop shop Big Data portal built on top of that API that allow our users to do interactive uh querying or",
    "start": "463080",
    "end": "471039"
  },
  {
    "text": "or or access to our tools and service so zooming into the compute",
    "start": "471039",
    "end": "477919"
  },
  {
    "start": "476000",
    "end": "476000"
  },
  {
    "text": "layer we provide five different big data processing engines for our users to use",
    "start": "477919",
    "end": "483479"
  },
  {
    "text": "or tools for our users to use Hive is for analytics pick is for ETL Presto is",
    "start": "483479",
    "end": "489520"
  },
  {
    "text": "for interactive data exploration plus more which I'm going to talk about later",
    "start": "489520",
    "end": "494720"
  },
  {
    "text": "Druid is the back end for our AB test analytics application if you may remember a few slides ago I showed aite",
    "start": "494720",
    "end": "501800"
  },
  {
    "text": "that's uh back by Druid it's surf as a inmemory cache so that users can do interactive slice and dice we also",
    "start": "501800",
    "end": "508680"
  },
  {
    "text": "included spar in our platform this year um it's going to expand the use case of our platform by allowing users to more",
    "start": "508680",
    "end": "516000"
  },
  {
    "text": "efficiently do interactive ml algorithm real time analytics and",
    "start": "516000",
    "end": "521680"
  },
  {
    "text": "more so in this session I'm going to focus on talking about Presto and Dan is going to focus on talking about",
    "start": "521680",
    "end": "530320"
  },
  {
    "text": "spark but before we get on to presto I just want to point out the key underpinning of our big data",
    "start": "530320",
    "end": "536480"
  },
  {
    "text": "architecture which is we choose S3 as our data warehouse house storage",
    "start": "536480",
    "end": "542079"
  },
  {
    "start": "542000",
    "end": "542000"
  },
  {
    "text": "layer Amazon S3 is where we put our data and it's the single source of Truth and",
    "start": "542200",
    "end": "548240"
  },
  {
    "text": "it's St and the reason why is definitely it would be likely more durable and reliable than your hdfs cluster it allow",
    "start": "548240",
    "end": "555720"
  },
  {
    "text": "us to separate storage from compute basically it allow us to run different clusters on the same set of data on S3",
    "start": "555720",
    "end": "563600"
  },
  {
    "text": "and also it allow us to easily do rep back pushes for example when we need to upgrade the soft on a cluster we could",
    "start": "563600",
    "end": "570880"
  },
  {
    "text": "spin up a new cluster except start routing new jobs to it from Genie um",
    "start": "570880",
    "end": "576440"
  },
  {
    "text": "wait for the old cluster to finish its job and then deprecate the old",
    "start": "576440",
    "end": "581760"
  },
  {
    "text": "cluster and it's a common belief and questions about um the performance of S3",
    "start": "581760",
    "end": "587120"
  },
  {
    "start": "583000",
    "end": "583000"
  },
  {
    "text": "if you re use S3 as a storage layer it's going to be slow and we just want to clarify a little bit before you ask that",
    "start": "587120",
    "end": "594519"
  },
  {
    "text": "actually as3 it has a much bigger Fleet than your cluster so it could sustain a",
    "start": "594519",
    "end": "599560"
  },
  {
    "text": "lot bigger throughput than you could um you could offload some of the work onto S3 in terms of a purely uh Benchmark in",
    "start": "599560",
    "end": "607640"
  },
  {
    "text": "a purely benchmarking environment we do see that um S3 H reuse S3 there is some",
    "start": "607640",
    "end": "613680"
  },
  {
    "text": "re performance impact uh because it's definitely going to have more latencies but um in our use cases um we use we our",
    "start": "613680",
    "end": "622440"
  },
  {
    "text": "jobs are very CPU bang and it's very large complex job that has multi-stages and the impact is only going to be at",
    "start": "622440",
    "end": "629440"
  },
  {
    "text": "the read stage at the beginning and the right stage at the end so it's very insignificant the benefits definitely",
    "start": "629440",
    "end": "635240"
  },
  {
    "text": "outweighs the cost same goes for right and actually sometime we see that writing to S3 is a little faster because",
    "start": "635240",
    "end": "641399"
  },
  {
    "text": "S3 is eventually consistent so hopefully that helps explain that and now we can",
    "start": "641399",
    "end": "647040"
  },
  {
    "text": "talk about presto presto is an open source",
    "start": "647040",
    "end": "653360"
  },
  {
    "text": "distribute a SQL engine Open Source by Facebook on GitHub it is an interactive",
    "start": "653360",
    "end": "658760"
  },
  {
    "text": "analytic engine um it is very different than other traditional big data processing engine in a sense that data",
    "start": "658760",
    "end": "665800"
  },
  {
    "text": "streams through the Presto workers never touch dis and all stain memory and hence",
    "start": "665800",
    "end": "670920"
  },
  {
    "text": "it can achieve the interactiveness that it has and it also scale up to petabyte scale in our use case as well as",
    "start": "670920",
    "end": "678079"
  },
  {
    "text": "Facebook use case before right before they open source it so why do we love Presto it just",
    "start": "678079",
    "end": "685519"
  },
  {
    "start": "682000",
    "end": "682000"
  },
  {
    "text": "simply fit into our architecture very well it's integrated with Hive metastore",
    "start": "685519",
    "end": "690639"
  },
  {
    "text": "and it also integrated with Amazon S3 um when we first started using Presto we",
    "start": "690639",
    "end": "695760"
  },
  {
    "text": "spent about a month to collaborate with a Facebook engineer to work on the S3 file system in Presto and after a month",
    "start": "695760",
    "end": "702959"
  },
  {
    "text": "we have an initial version of the S3 file system working so that's pretty easy for",
    "start": "702959",
    "end": "708959"
  },
  {
    "text": "us um and it works at ped by scale it is an NCC quote syntax so your",
    "start": "708959",
    "end": "715800"
  },
  {
    "text": "analysts or Engineers or data scientists they usually know about NC SQL and it's",
    "start": "715800",
    "end": "722480"
  },
  {
    "text": "open source so we could enhance it for the for our own use case it's written in Java and it's really fast so how fast so",
    "start": "722480",
    "end": "732079"
  },
  {
    "text": "we bench mark Presto and hi again um Presto is both when we do the Benchmark",
    "start": "732079",
    "end": "737839"
  },
  {
    "text": "we're using Presto 114 1.0 running on met",
    "start": "737839",
    "end": "743480"
  },
  {
    "text": "reduced and um the key takeaway of this um chart is",
    "start": "743480",
    "end": "749800"
  },
  {
    "text": "that Presto finish in seconds High finish in minutes on the first two",
    "start": "749800",
    "end": "754959"
  },
  {
    "text": "queries you can see that um if you are running query that more complex you will likely achieve more Savings in Presto",
    "start": "754959",
    "end": "762639"
  },
  {
    "text": "because it could avoid a lot of uh it could avoid more dis guio because if",
    "start": "762639",
    "end": "767920"
  },
  {
    "text": "it's complex queries in Hive you are likely to have more stages and have more dis guio um the last query show that Neo",
    "start": "767920",
    "end": "774440"
  },
  {
    "text": "in a hze deck scenario press toades run really fast um and it's actually super",
    "start": "774440",
    "end": "779519"
  },
  {
    "text": "fast for big data analytic queries and I'm going to cover why that's so fast and we did some contributions to make",
    "start": "779519",
    "end": "785639"
  },
  {
    "text": "that happen so at Netflix we run around uh",
    "start": "785639",
    "end": "791079"
  },
  {
    "text": "3500 Presto queres a day out of which 60% finish within 5 seconds and over 90%",
    "start": "791079",
    "end": "797279"
  },
  {
    "text": "finish within a minute and that's extremely fast in terms of querying big data and that's why our users love it so",
    "start": "797279",
    "end": "804639"
  },
  {
    "text": "much it tremendously improved their productivity",
    "start": "804639",
    "end": "809480"
  },
  {
    "start": "810000",
    "end": "810000"
  },
  {
    "text": "so when we first started using Presto is for interactive um data exploration and experimentation but because Presto runs",
    "start": "811199",
    "end": "818519"
  },
  {
    "text": "so well um we start to expand uh using Presto in more use cases um there are",
    "start": "818519",
    "end": "825240"
  },
  {
    "text": "users who use it for data validation if they suspect their ETL job um has",
    "start": "825240",
    "end": "830720"
  },
  {
    "text": "probably some problem in the data they would use uh press tool to do data validation to make sure the data that",
    "start": "830720",
    "end": "836480"
  },
  {
    "text": "they generate from the ETL process is within their r of expectation we also use Presto as a",
    "start": "836480",
    "end": "843600"
  },
  {
    "text": "backend for that AB test analytic application ignite that I show a few slides ago I mentioned that we use Dru",
    "start": "843600",
    "end": "850759"
  },
  {
    "text": "for interactive uh uh slice and dice but there's one thing that Drew we cannot do is the distinct con query so in that",
    "start": "850759",
    "end": "858079"
  },
  {
    "text": "case the uh ignite application actually point to a uh Presto cluster and to do",
    "start": "858079",
    "end": "864680"
  },
  {
    "text": "the uh interactive uh and do the distinct con queries for the interactiveness of Presto so that the",
    "start": "864680",
    "end": "870399"
  },
  {
    "text": "application can still uh have the interactiveness that we need for the product manager to",
    "start": "870399",
    "end": "877639"
  },
  {
    "text": "use and in terms of reporting we use Tableau and micro strategy going through",
    "start": "878240",
    "end": "883639"
  },
  {
    "text": "Hive server to Hive um for our big data reporting needs but uh we definitely",
    "start": "883639",
    "end": "890519"
  },
  {
    "text": "want to move to presto because it's so much faster and uh we recently implemented the web connector interface",
    "start": "890519",
    "end": "897040"
  },
  {
    "text": "in press uh in in too to connect to presto um we're testing that out but we",
    "start": "897040",
    "end": "902320"
  },
  {
    "text": "are also aware that teradata is going to come up with an Enterprise obbc and jdbc driver for Presto and we're excited",
    "start": "902320",
    "end": "909240"
  },
  {
    "text": "about it and we'll certainly test it out when it comes",
    "start": "909240",
    "end": "914240"
  },
  {
    "text": "out and because Presto runs so well our users keep pushing us to actually make Presto available for ETL but Presto is",
    "start": "914600",
    "end": "922040"
  },
  {
    "text": "not designed for ETL as I mentioned earlier all data that go through Presto never touch dis and it stays in memory",
    "start": "922040",
    "end": "928759"
  },
  {
    "text": "so so in other words if you have if you run a query on a large set of data you could fail the query or even push over",
    "start": "928759",
    "end": "936319"
  },
  {
    "text": "the cluster it's possible so it's not really designed for ETL but what we're exploring with our users is maybe there",
    "start": "936319",
    "end": "943399"
  },
  {
    "text": "is an happy medium where they could run schedule job on Presto potentially on a",
    "start": "943399",
    "end": "949040"
  },
  {
    "text": "dedicated cluster that they could actually sacrifice some",
    "start": "949040",
    "end": "954240"
  },
  {
    "text": "reliability for the gain of having the query run um fast most of the time if",
    "start": "954240",
    "end": "960759"
  },
  {
    "text": "they if they could you know live with having it fail occasionally so we'll see how that goes and see how that use case",
    "start": "960759",
    "end": "969600"
  },
  {
    "text": "developed so on to talking about our deployment um we're currently running version 114 um with some patches all of",
    "start": "970000",
    "end": "977319"
  },
  {
    "start": "972000",
    "end": "972000"
  },
  {
    "text": "these patches are public so you can run what we run except for one patch and I'll talk about why that is not public",
    "start": "977319",
    "end": "983600"
  },
  {
    "text": "yet um we deploy Presto on top of Amazon EMR uh using a booster action we",
    "start": "983600",
    "end": "989880"
  },
  {
    "text": "actually run two separate sets of cluster for Presto and for her worklow and it's because Presto need a different",
    "start": "989880",
    "end": "995800"
  },
  {
    "text": "kind of instance Presto needs uh a lot of memory versus the Hado cluster needs a lot of dis for temporary storage um",
    "start": "995800",
    "end": "1004040"
  },
  {
    "text": "for the Presto cluster we're not using any of the Hado service we're not running on",
    "start": "1004040",
    "end": "1009079"
  },
  {
    "text": "hdfs um and we are using EMR primarily for like resizing the cluster and note",
    "start": "1009079",
    "end": "1016959"
  },
  {
    "text": "replacement we currently have two production cluster running one is for ad hoc querying that I mentioned um one is",
    "start": "1017240",
    "end": "1023720"
  },
  {
    "start": "1018000",
    "end": "1018000"
  },
  {
    "text": "also for the application cluster which is the backend cluster for ignite the ab test analytic application the reason why",
    "start": "1023720",
    "end": "1030400"
  },
  {
    "text": "we need two production cluster is because uh of resource isolation impresso they manage resource by",
    "start": "1030400",
    "end": "1036918"
  },
  {
    "text": "limiting the number of queries that users can run and they also can specify",
    "start": "1036919",
    "end": "1042720"
  },
  {
    "text": "a limit on the memory that a query can have but it doesn't cover all the operators yet meaning that a single",
    "start": "1042720",
    "end": "1049440"
  },
  {
    "text": "query can still take over the cluster and potentially push it over and in that case um what we did is to separate the",
    "start": "1049440",
    "end": "1057080"
  },
  {
    "text": "two cluster so that the interactive ignite application can have a dedicated cluster for running the queries that it",
    "start": "1057080",
    "end": "1064440"
  },
  {
    "text": "needs one thing um also interesting is that um for the ignite application",
    "start": "1064440",
    "end": "1069520"
  },
  {
    "text": "cluster we're using Dynamic sizing by querying the Netflix Spa API F SP",
    "start": "1069520",
    "end": "1075919"
  },
  {
    "text": "Netflix Spa is the continuous delivery platform that we use and it provides an",
    "start": "1075919",
    "end": "1081039"
  },
  {
    "text": "API that allow us to see what are the unused but reserved instances that we",
    "start": "1081039",
    "end": "1086559"
  },
  {
    "text": "have across all the accounts all the region all the different zones and all",
    "start": "1086559",
    "end": "1092159"
  },
  {
    "text": "the instance types so basically we can find out how many instances that we could borrow and it works really well",
    "start": "1092159",
    "end": "1098400"
  },
  {
    "text": "for this application cluster because when it's the busiest during the business hours when a product manager is",
    "start": "1098400",
    "end": "1103840"
  },
  {
    "text": "working actually it's the trough time of outside traffic so actually we could Leverage The unus used capacity um for",
    "start": "1103840",
    "end": "1111120"
  },
  {
    "text": "our application cluster so this chart shows that over the course of 4 days how our how we",
    "start": "1111120",
    "end": "1117640"
  },
  {
    "start": "1114000",
    "end": "1114000"
  },
  {
    "text": "actually size up and down our application clusters and you see that we're leveraging all three um XL and 4L",
    "start": "1117640",
    "end": "1125520"
  },
  {
    "text": "um it's just that it just happened that over those four days we can't find any available a32",
    "start": "1125520",
    "end": "1133159"
  },
  {
    "text": "Excel so how we integrate it with the Netflix infrastructure so um this diagram here is showing that there is a",
    "start": "1133159",
    "end": "1139600"
  },
  {
    "start": "1134000",
    "end": "1134000"
  },
  {
    "text": "presto cluster with one coordinator and many Presto workers each of these notes is running the Presto uh uh aesto",
    "start": "1139600",
    "end": "1147360"
  },
  {
    "text": "process and there is also a CYO process that we inject into the cluster via our",
    "start": "1147360",
    "end": "1152960"
  },
  {
    "text": "boostrap action basically the Sao process on the coordinator is sending the query information and the user",
    "start": "1152960",
    "end": "1159799"
  },
  {
    "text": "information down to Kafka our data Pipeline and then um we can do data",
    "start": "1159799",
    "end": "1165000"
  },
  {
    "text": "lineage analysis Downstream and all of the nodes on the Presto",
    "start": "1165000",
    "end": "1171600"
  },
  {
    "text": "clusters also send metrics that we care about and also send uh jmx metrics that's provided by Presto down to Atlas",
    "start": "1171600",
    "end": "1178760"
  },
  {
    "text": "which is our Telemetry system from the coordinator we send uh metrics like user uh queries complete uh failed from the",
    "start": "1178760",
    "end": "1186880"
  },
  {
    "text": "workers um process weend uh statistics uh metrics like S3 read errors or number",
    "start": "1186880",
    "end": "1193720"
  },
  {
    "text": "of concurrent S3 connection at this point in time",
    "start": "1193720",
    "end": "1198840"
  },
  {
    "text": "so then I'm going to spend the rest of the time that I have on the stage to talk about the contributions that we",
    "start": "1199480",
    "end": "1204960"
  },
  {
    "text": "make and the performance gain that we achieve so in the last year and a half",
    "start": "1204960",
    "end": "1210760"
  },
  {
    "start": "1208000",
    "end": "1208000"
  },
  {
    "text": "that we use Presto we make quite a few contributions back um we worked on the",
    "start": "1210760",
    "end": "1215799"
  },
  {
    "text": "S3 file system we optimized a couple of queries type that our users use we added",
    "start": "1215799",
    "end": "1221440"
  },
  {
    "text": "various functions for complex type like map array and structs to close the gap",
    "start": "1221440",
    "end": "1226720"
  },
  {
    "text": "between what's available in presto and what's available in Hive this year we",
    "start": "1226720",
    "end": "1231880"
  },
  {
    "text": "spent a lot of time to optimize pet file format um with",
    "start": "1231880",
    "end": "1237760"
  },
  {
    "text": "Presto everything you see here is already um available out there the only",
    "start": "1237760",
    "end": "1242840"
  },
  {
    "text": "thing that's not available is vectorized read and I'll explain why um everything else is available uh the one the two",
    "start": "1242840",
    "end": "1250240"
  },
  {
    "text": "items that are in asteris are the one that is available as patches not contributed back yet I mean not",
    "start": "1250240",
    "end": "1256520"
  },
  {
    "text": "committed yet so I'm going to spend the time to talk about predicate push down and vectorized",
    "start": "1256520",
    "end": "1263080"
  },
  {
    "text": "Street and before I do that let's take a look at paret far format and what it is",
    "start": "1263080",
    "end": "1268120"
  },
  {
    "text": "p is a columnal file format it is supported across the Big Data engines",
    "start": "1268120",
    "end": "1273400"
  },
  {
    "text": "that we use and because it's so neutrally supported across these engines",
    "start": "1273400",
    "end": "1278760"
  },
  {
    "text": "the performance gain that you are going to see that we have in Presto can arguably be implemented across these",
    "start": "1278760",
    "end": "1284240"
  },
  {
    "text": "engines as well so that's the advantage that we have picking a neutral file format it works well on est3 and most of",
    "start": "1284240",
    "end": "1292080"
  },
  {
    "text": "our data in our data warehouse is in paret file format and um as far as we know we might be the biggest Data",
    "start": "1292080",
    "end": "1298159"
  },
  {
    "text": "Warehouse on par F format and um if you find another one let me know",
    "start": "1298159",
    "end": "1305559"
  },
  {
    "start": "1305000",
    "end": "1305000"
  },
  {
    "text": "um so basically let's take a look at how parket file is uh organized so um Park",
    "start": "1305559",
    "end": "1312400"
  },
  {
    "text": "file organized rows in rows group each row groups represent a number of rows and they're organized in column chunks",
    "start": "1312400",
    "end": "1318679"
  },
  {
    "text": "each column chunk has multiple data pages and if the cardinality of the value in that column chunk is below a",
    "start": "1318679",
    "end": "1325039"
  },
  {
    "text": "certain threshold it would have a dictionary page that contain all the unique values in that column",
    "start": "1325039",
    "end": "1330520"
  },
  {
    "text": "chunk then in the footer it will have the statistics and metadata for all the row groups and all the column chunks in",
    "start": "1330520",
    "end": "1336880"
  },
  {
    "text": "each row group when we talk about vectorized read we're talking about how we optimize the read on the column chunk",
    "start": "1336880",
    "end": "1343440"
  },
  {
    "text": "when we're talking about predicate push down we're talking about how we leverage the statistics in the column CH metadata",
    "start": "1343440",
    "end": "1349960"
  },
  {
    "text": "and the dictionary page to achieve data pruning for the performance gain we",
    "start": "1349960",
    "end": "1355279"
  },
  {
    "start": "1355000",
    "end": "1355000"
  },
  {
    "text": "get so for vectorized read we basically Implement a new paret API to read column",
    "start": "1355279",
    "end": "1360720"
  },
  {
    "text": "chunk by badges instead of row by row we implement the similar change in Presto to integrate with the new API um because",
    "start": "1360720",
    "end": "1368919"
  },
  {
    "text": "Presto is a vectorized execution engine um we actually in in a read bound query",
    "start": "1368919",
    "end": "1376400"
  },
  {
    "text": "or in a readon query in Presto when we B Ben Market we see two x performance game for primitive data type which is really",
    "start": "1376400",
    "end": "1382520"
  },
  {
    "text": "good and there's a lot of interest in the community to do that spark is thinking about using it because um they",
    "start": "1382520",
    "end": "1389240"
  },
  {
    "text": "are also thinking about to make it a vectorized um engine Hive is already a",
    "start": "1389240",
    "end": "1394559"
  },
  {
    "text": "vectorized engine so there is some activities in the community to actually uh implement this in Hive for drill we",
    "start": "1394559",
    "end": "1401760"
  },
  {
    "text": "actually um collaborate with the drill team to design the paret um Voris API so",
    "start": "1401760",
    "end": "1408720"
  },
  {
    "text": "the reason why this patch in Presto is not available yet is because we're waiting for the pet um jira to be",
    "start": "1408720",
    "end": "1414039"
  },
  {
    "text": "committed once it's committed we'll make it available and we're talking about probably weeks not",
    "start": "1414039",
    "end": "1420480"
  },
  {
    "text": "months for predicate push down um we're really excited about the performance game we see and it's best illustrate by",
    "start": "1420480",
    "end": "1427840"
  },
  {
    "start": "1421000",
    "end": "1421000"
  },
  {
    "text": "an example so let's say the predicate column is AB test ID and you're looking",
    "start": "1427840",
    "end": "1433200"
  },
  {
    "text": "for value 10 so we first look at the stat",
    "start": "1433200",
    "end": "1438440"
  },
  {
    "text": "statistics on the predicate column chunk for the value that you're looking for if",
    "start": "1438440",
    "end": "1443799"
  },
  {
    "text": "the value that you're looking for fall out of the column chunk statistics we can completely read that road group in",
    "start": "1443799",
    "end": "1450000"
  },
  {
    "text": "paret but let's say the value that you're looking up for actually fall into the statistics of the column chunk stats",
    "start": "1450000",
    "end": "1457880"
  },
  {
    "text": "what it need to do is actually go to the rad group and read the dictionary page and see if the value exists if it",
    "start": "1457880",
    "end": "1463159"
  },
  {
    "text": "doesn't it can skip if it doesn't if it if it does exist it will end up reading all the data pages in that road",
    "start": "1463159",
    "end": "1471240"
  },
  {
    "text": "group so if you step back to think about it it is the best uh case if you could",
    "start": "1471240",
    "end": "1477760"
  },
  {
    "text": "actually cluster or sort or order the data by the predicate column um for",
    "start": "1477760",
    "end": "1483600"
  },
  {
    "text": "example if your predicate column is very evenly distributed across the whole file",
    "start": "1483600",
    "end": "1488640"
  },
  {
    "text": "the statistics range for each column chunk will be really wide you will end up falling into the range when we do",
    "start": "1488640",
    "end": "1495320"
  },
  {
    "text": "predicate push down most of the time and then which is not good because then you end up needing to read the row group and",
    "start": "1495320",
    "end": "1502399"
  },
  {
    "text": "if the predicate column value actually appears have many times have a lot of repetitions again that's not good if",
    "start": "1502399",
    "end": "1509399"
  },
  {
    "text": "it's evenly distributed because then you will find it in multiple dictionary and end up reading a lot more data Pages",
    "start": "1509399",
    "end": "1515960"
  },
  {
    "text": "than you need so basically it worked best if you cluster the data by the predicate column that you are choosing",
    "start": "1515960",
    "end": "1522600"
  },
  {
    "text": "on most of the time much like how we plan a hive petition but in some cases",
    "start": "1522600",
    "end": "1527880"
  },
  {
    "text": "hive ition doesn't work well because the predicate column that you are most likely select on has a lot of distinct",
    "start": "1527880",
    "end": "1534880"
  },
  {
    "text": "value the cardinality is very high um then it doesn't make sense to create",
    "start": "1534880",
    "end": "1539960"
  },
  {
    "text": "millions of Hive petitions and overload them at a store so we really excited about the result I'm going to share that",
    "start": "1539960",
    "end": "1546520"
  },
  {
    "text": "next but what I want to point out is um we are likely and thinking about implementing this predicate push down",
    "start": "1546520",
    "end": "1553000"
  },
  {
    "text": "logic in spark hi and pck as well um simply because it works so well",
    "start": "1553000",
    "end": "1558880"
  },
  {
    "text": "so let's look at a real use case and the performance we get so we do analytics on Atlas data Atlas collect over billion",
    "start": "1558880",
    "end": "1565440"
  },
  {
    "start": "1559000",
    "end": "1559000"
  },
  {
    "text": "and time of Time series data um from our service most common analytics use case",
    "start": "1565440",
    "end": "1572120"
  },
  {
    "text": "we have on alysis they want a particular metric for a particular application for a particular time period like this",
    "start": "1572120",
    "end": "1578960"
  },
  {
    "text": "example and we pick a day to look at how many distinct metrics we have and that",
    "start": "1578960",
    "end": "1584399"
  },
  {
    "text": "day that we picked we saw 1.7 million so it's very high card ity is high selectivity so what we did is we",
    "start": "1584399",
    "end": "1591320"
  },
  {
    "text": "restation the data so that it's clustered by the app name and the metrix so we see 10x performance gain on the",
    "start": "1591320",
    "end": "1598000"
  },
  {
    "text": "query running on the Presto cluster which is very very significant we also",
    "start": "1598000",
    "end": "1603720"
  },
  {
    "text": "observed that we have uh 170x Improvement on CPU usage and also",
    "start": "1603720",
    "end": "1610240"
  },
  {
    "text": "all the io metrics so basically what is saying the cluster is actually much more free for doing other",
    "start": "1610240",
    "end": "1616520"
  },
  {
    "text": "workload so like I said we are likely to implement it in other engine um but this",
    "start": "1616520",
    "end": "1622240"
  },
  {
    "text": "patch is already um committed in Presto and so you could use it as",
    "start": "1622240",
    "end": "1627960"
  },
  {
    "text": "well so that's all I have for Presto I just want to point out that we're going to have two tack blocks talking",
    "start": "1627960",
    "end": "1634200"
  },
  {
    "text": "specifically how we use parket file format and also how we do the performance Improvement of presto on on",
    "start": "1634200",
    "end": "1641399"
  },
  {
    "text": "spark file format so stay tuned for that and with that I'm going to have Dan come up to talk about Spark",
    "start": "1641399",
    "end": "1650120"
  },
  {
    "text": "[Applause]",
    "start": "1651340",
    "end": "1657080"
  },
  {
    "text": "thank you um so this next section we're going to be talking a little bit about the work that we've done to integrate",
    "start": "1657080",
    "end": "1662760"
  },
  {
    "text": "spark into our platform and I have to point out that spark is the newest addition to the platform so it's in a",
    "start": "1662760",
    "end": "1668640"
  },
  {
    "text": "very different state of integration than Presto was so Presto were're spending a lot of time working on optimization and",
    "start": "1668640",
    "end": "1675720"
  },
  {
    "text": "building out new use cases and functionality whereas we're trying to figure out how best to integrate it with the platform and make it play well with",
    "start": "1675720",
    "end": "1682120"
  },
  {
    "text": "the other tools that we have so for those of you who are not in the Big Data space spark is you know a",
    "start": "1682120",
    "end": "1689320"
  },
  {
    "text": "general engine for doing distributed computation and you know we have a lot of engines that do this kind of work and",
    "start": "1689320",
    "end": "1695760"
  },
  {
    "text": "Spark is a little bit different in the sense that it's not a domain specific tool so Hive Pig are very domain",
    "start": "1695760",
    "end": "1703240"
  },
  {
    "text": "specific and that you're going to be doing analytics with them you're going to be doing uh some sort of data transformation or ETL with those spark",
    "start": "1703240",
    "end": "1709960"
  },
  {
    "text": "is built on top of a set of core libraries for executing any sort of data",
    "start": "1709960",
    "end": "1715000"
  },
  {
    "text": "transformation and then on top of that you build domain specific tools but the nice thing about this is that they can",
    "start": "1715000",
    "end": "1720480"
  },
  {
    "text": "all play together very well so why do we want to use spark I",
    "start": "1720480",
    "end": "1727159"
  },
  {
    "start": "1725000",
    "end": "1725000"
  },
  {
    "text": "mean we've just described a very mature platform that we've been using to do big data processing for years and we have",
    "start": "1727159",
    "end": "1734279"
  },
  {
    "text": "Hive for ETL or I mean we have Hive for analytics we've got Pig for ETL so what",
    "start": "1734279",
    "end": "1740279"
  },
  {
    "text": "does this bring to the platform that wasn't there originally and what we're actually seeing right now is a growing",
    "start": "1740279",
    "end": "1745640"
  },
  {
    "text": "number of cases where people need either iterative use of data or they're going to be doing some sort of machine",
    "start": "1745640",
    "end": "1750760"
  },
  {
    "text": "learning and they need to scale this out most of this kind of work right now is being done on single instances or",
    "start": "1750760",
    "end": "1756640"
  },
  {
    "text": "somehow sharding across other clusters to achieve the same kind of result so spark provides this and is one of their",
    "start": "1756640",
    "end": "1763159"
  },
  {
    "text": "you know the core features of that particular platform the other thing that we see is an increase in programmatic use cases",
    "start": "1763159",
    "end": "1769880"
  },
  {
    "text": "and what I mean by programmatic use cases is cases where we're actually either adding more logic to the actual",
    "start": "1769880",
    "end": "1775799"
  },
  {
    "text": "job or we're doing something in addition to what would normally be done via you",
    "start": "1775799",
    "end": "1781440"
  },
  {
    "text": "know Hive or Pig that is domain specific and we've got ways that we kind of shoehorn these into those tools with pig",
    "start": "1781440",
    "end": "1788039"
  },
  {
    "text": "you know you can use load Funk store Funk to do extra functionality but it's not really idiomatic of pig and it",
    "start": "1788039",
    "end": "1793360"
  },
  {
    "text": "doesn't make a whole lot of sense when you're looking at something and you've got these strange conventions to do this extra work work so one good example of",
    "start": "1793360",
    "end": "1799799"
  },
  {
    "text": "that is what we call continuous processing and what we do with continuous processing is we actually try",
    "start": "1799799",
    "end": "1804880"
  },
  {
    "text": "and decouple what data you need to process from the actual execution so",
    "start": "1804880",
    "end": "1810039"
  },
  {
    "text": "rather than saying oh I need to process the last8 hours of this table because that's what the new data is and having",
    "start": "1810039",
    "end": "1816720"
  },
  {
    "text": "to explicitly call those out we use an external tool to keep track of that and you have just kind of a high Watermark",
    "start": "1816720",
    "end": "1822519"
  },
  {
    "text": "where you can go and you can say get me all the data that I haven't processed yet and then at the end of these jobs",
    "start": "1822519",
    "end": "1827640"
  },
  {
    "text": "you might say well I want to do some sort of analytic across them to make sure that the data actually conforms to",
    "start": "1827640",
    "end": "1833799"
  },
  {
    "text": "what I expect it to be doing this in spark makes a lot of sense because it's very programmatic you can build these",
    "start": "1833799",
    "end": "1839039"
  },
  {
    "text": "things in with your job as well where with the other tools it's a little bit more difficult to",
    "start": "1839039",
    "end": "1846120"
  },
  {
    "text": "do so we're not the only team at Netflix that's actually using spark uh we have",
    "start": "1846440",
    "end": "1852240"
  },
  {
    "text": "an image that is provided by another team uh using the same Source repository that we use so we collaborate on the",
    "start": "1852240",
    "end": "1858039"
  },
  {
    "text": "actual um uh building out of features and things like that but they have a a separate omy that they can spin up for",
    "start": "1858039",
    "end": "1863600"
  },
  {
    "text": "specific use cases and this includes the entire Berkeley data analytics stack and",
    "start": "1863600",
    "end": "1869000"
  },
  {
    "text": "the important thing about this is It's really more for targeted use cases if you're going to be doing streaming or you have a machine learning job where",
    "start": "1869000",
    "end": "1875559"
  },
  {
    "text": "you have certain slas you need to have a dedicated cluster for that and that's what they use this for however in our",
    "start": "1875559",
    "end": "1882080"
  },
  {
    "text": "case we're actually deploying a little bit differently and the reason that we do that is because we're more focused on",
    "start": "1882080",
    "end": "1888279"
  },
  {
    "text": "the batch side of things doing large scale processing where you're not as concerned about the SLA but you want the",
    "start": "1888279",
    "end": "1894200"
  },
  {
    "text": "scale and our Focus there is also on resource utilization making sure that we're actually getting good throughput",
    "start": "1894200",
    "end": "1900120"
  },
  {
    "text": "and we're harnessing all the power that we uh all the power of the cluster all the time so this means that we can",
    "start": "1900120",
    "end": "1906000"
  },
  {
    "text": "actually do some interesting things by running it on top of yarn one is that we actually provide spark as a service so",
    "start": "1906000",
    "end": "1912240"
  },
  {
    "text": "instead of having a standalone cluster we actually provide spark as something that is somewhat decoupled from the",
    "start": "1912240",
    "end": "1918799"
  },
  {
    "text": "actual execution uh we run this on top of yarn in EMR so EMR is just going to be",
    "start": "1918799",
    "end": "1925679"
  },
  {
    "text": "providing the Hadoop cluster with yarn running on top of it and we're providing the actual application and again the focus is more",
    "start": "1925679",
    "end": "1932360"
  },
  {
    "text": "batch analytics but we do supp support the full stack capability so you can do interactive we'll go into that a little",
    "start": "1932360",
    "end": "1938639"
  },
  {
    "text": "bit as well as any of the other tools that stand on top of the the spark core",
    "start": "1938639",
    "end": "1944000"
  },
  {
    "start": "1944000",
    "end": "1944000"
  },
  {
    "text": "libraries so one thing that we can do as far as providing spark as a service is",
    "start": "1944000",
    "end": "1949240"
  },
  {
    "text": "version support and this is kind of an interesting concept and makes something that is typically very hard very easy",
    "start": "1949240",
    "end": "1955880"
  },
  {
    "text": "and that is doing upgrades or supporting multiple versions of the same product on the same cluster at the same time so if",
    "start": "1955880",
    "end": "1962399"
  },
  {
    "text": "you think about something like Hive where you're tied with a specific version of the metastore there's a lot of work that goes into making sure that",
    "start": "1962399",
    "end": "1969799"
  },
  {
    "text": "when you do an upgrade everything upgrades seamlessly together and everybody's running at the same version",
    "start": "1969799",
    "end": "1975399"
  },
  {
    "text": "so with spark the way that we do this was we actually send the application with the you know",
    "start": "1975399",
    "end": "1981519"
  },
  {
    "text": "the execution to the cluster and this allows us to separate those two concepts so what how this actually works is we",
    "start": "1981519",
    "end": "1986880"
  },
  {
    "text": "have wrappers around the spark shells you can provide an argument for which version we can do custom versions um and",
    "start": "1986880",
    "end": "1993320"
  },
  {
    "text": "what it does is it talks to Genie the script will talk to Genie it'll get the information from Genie about where the",
    "start": "1993320",
    "end": "1999600"
  },
  {
    "text": "application is in S3 so we just you know tar up a build of of spark put it in an",
    "start": "1999600",
    "end": "2004840"
  },
  {
    "text": "S3 location and then it also uh gen uh contains the configuration",
    "start": "2004840",
    "end": "2009880"
  },
  {
    "text": "information for all the Clusters So based on what other parameters you set you can identify which cluster you want",
    "start": "2009880",
    "end": "2015320"
  },
  {
    "text": "to run it on it'll grab the configuration files take these things put them together create a custom execution environment for you and ship",
    "start": "2015320",
    "end": "2022559"
  },
  {
    "text": "all of this to the cluster for execution and this is very effective because we can basically have any number of",
    "start": "2022559",
    "end": "2028360"
  },
  {
    "text": "versions of spark running at the same time on the same cluster makes upgrading pretty much seamless and this is",
    "start": "2028360",
    "end": "2034639"
  },
  {
    "text": "particularly important for spark because spark is the most active Apache project right now and so they're iterating very",
    "start": "2034639",
    "end": "2040960"
  },
  {
    "text": "very quickly and we can't have all of our users have to go recheck their jobs every single time that we need to do a",
    "start": "2040960",
    "end": "2047480"
  },
  {
    "text": "small patch or a push they can stay on the version that's working for them and then we can we can actually switch",
    "start": "2047480",
    "end": "2052878"
  },
  {
    "text": "versions and tests to make sure that they can actually upgrade seamlessly and then only handle the cases where it doesn't work",
    "start": "2052879",
    "end": "2060200"
  },
  {
    "text": "um so the next topic is actually about multi-tenancy um this is very important",
    "start": "2060200",
    "end": "2065358"
  },
  {
    "text": "in order to get the the scale and execute uh using the the entire cluster",
    "start": "2065359",
    "end": "2071358"
  },
  {
    "text": "so just a little bit ago we we talked a about separating compute from storage",
    "start": "2071359",
    "end": "2078398"
  },
  {
    "text": "and you might be saying well it doesn't make sense at this point why don't you just run a separate spark cluster you",
    "start": "2078399",
    "end": "2083520"
  },
  {
    "text": "can do that at this you know it's what we do with Presto um and the reason is if you look at this chart right here",
    "start": "2083520",
    "end": "2089398"
  },
  {
    "text": "this is actually one of our big clusters and all of the map reduce jobs that are running on it and I've highlighted the spark jobs and as you can see spark is",
    "start": "2089399",
    "end": "2096158"
  },
  {
    "text": "not dominating the cluster by any you know people are still using for exploratory use cases and just feeling",
    "start": "2096159",
    "end": "2102240"
  },
  {
    "text": "out the capabilities and figuring out how they're going to build their their workflows with spark the important thing here is that",
    "start": "2102240",
    "end": "2108520"
  },
  {
    "text": "there's a trade-off between resource utilization and performance if we stood up a separate spark cluster it would",
    "start": "2108520",
    "end": "2113839"
  },
  {
    "text": "either have to be completely oversized in order to get the performance they want or it's going to be so small to be",
    "start": "2113839",
    "end": "2119720"
  },
  {
    "text": "resource efficient and they're not going to get any performance out of it so we really wanted to actually coexist with",
    "start": "2119720",
    "end": "2125480"
  },
  {
    "text": "the other applications that are running on top of yarn and in order for this to work one of the most important features",
    "start": "2125480",
    "end": "2131560"
  },
  {
    "start": "2131000",
    "end": "2131000"
  },
  {
    "text": "is dynamic allocation so this is something that was introduced into spark in 1.2 and it's been iterated on many",
    "start": "2131560",
    "end": "2139000"
  },
  {
    "text": "times I believe uh 1.6 has probably the most robust implementation that'll be",
    "start": "2139000",
    "end": "2145280"
  },
  {
    "text": "out uh sometime here in the future uh but the the point is the dynamic allocation allows us to harness the",
    "start": "2145280",
    "end": "2151960"
  },
  {
    "text": "entire scale of the cluster optimize for resource utilization and the last point is most important still Prov provide",
    "start": "2151960",
    "end": "2158359"
  },
  {
    "text": "interactive performance which is one of the nice features about spark is that you've got a cluster waiting to do",
    "start": "2158359",
    "end": "2164119"
  },
  {
    "text": "computation you're not going to be submitting jobs and to understand this you kind of need to understand the",
    "start": "2164119",
    "end": "2169359"
  },
  {
    "text": "execution model of map reduce and Spark so if you look at the bottom the",
    "start": "2169359",
    "end": "2175160"
  },
  {
    "start": "2170000",
    "end": "2170000"
  },
  {
    "text": "traditional execution model is that a task and a container have a life cycle that are bound together so one task will",
    "start": "2175160",
    "end": "2181640"
  },
  {
    "text": "exists with one container when the work is done those get torn down and somebody else will be able to reuse those",
    "start": "2181640",
    "end": "2186920"
  },
  {
    "text": "resources spark on yarn Works a little bit differently in that you have persistence",
    "start": "2186920",
    "end": "2192760"
  },
  {
    "text": "so the the containers are going to be allocated and they're going to stay there through the entire life cycle of the job and then tasks are kind of",
    "start": "2192760",
    "end": "2199400"
  },
  {
    "text": "streamed through as the executors process them what dynamic allocation does is it looks at how many tasks are",
    "start": "2199400",
    "end": "2207280"
  },
  {
    "text": "pending for any sort of transform that's uh occurring at that time and then we'll",
    "start": "2207280",
    "end": "2212720"
  },
  {
    "text": "scale up and scale Down based on how many resources it thinks it needs so the way that we actually configure this is",
    "start": "2212720",
    "end": "2218720"
  },
  {
    "text": "we configure just three executors when you launch a spark job so whether you're doing an interactive shell or you're",
    "start": "2218720",
    "end": "2224000"
  },
  {
    "text": "doing uh you know a submit and forget typ type of execution you get three initially and what that allows you to do",
    "start": "2224000",
    "end": "2230520"
  },
  {
    "text": "is any sort of quick operation like a take 10 to just look at what your data looks like figure out what you're going",
    "start": "2230520",
    "end": "2235560"
  },
  {
    "text": "to join on uh find the right columns they'll execute very quickly because there are some containers just waiting to process those tasks whereas when you",
    "start": "2235560",
    "end": "2243119"
  },
  {
    "text": "do processing for let's say an entire month it's going to scale up based on how many tasks are generated from you",
    "start": "2243119",
    "end": "2249440"
  },
  {
    "text": "know the split calculations and everything else so it's not really important to",
    "start": "2249440",
    "end": "2255960"
  },
  {
    "text": "know what the The Legend is on this but it it's a graph of one particular execution using Dynamic allocation and",
    "start": "2255960",
    "end": "2262599"
  },
  {
    "text": "what you're looking at is that spike is really what spark is considering it needs in order to um process the",
    "start": "2262599",
    "end": "2268839"
  },
  {
    "text": "resources as fast as possible and the the lower lines are more about what was actually allocated and deallocated",
    "start": "2268839",
    "end": "2274960"
  },
  {
    "text": "during the execution and the important thing here is that these these are not perfect uh in a sense that you're not",
    "start": "2274960",
    "end": "2281280"
  },
  {
    "text": "getting exactly what you want exactly when you want but overall the execution is very good and it means that we can",
    "start": "2281280",
    "end": "2287400"
  },
  {
    "text": "share resources with other map reduce applications and other things on the cluster",
    "start": "2287400",
    "end": "2293440"
  },
  {
    "text": "effectively so one other thing about spark is it allows you to cach data so if you're",
    "start": "2294359",
    "end": "2300880"
  },
  {
    "text": "doing iterative processing interactive processing and you've got a data set that data set you can fit into memory",
    "start": "2300880",
    "end": "2306160"
  },
  {
    "text": "you just load it into memory and doesn't have to go back to dis every time it wants to do some processing well this works really well except Dynamic",
    "start": "2306160",
    "end": "2312440"
  },
  {
    "text": "allocation originally was designed to remove executors from the cluster containers from the cluster when it",
    "start": "2312440",
    "end": "2319079"
  },
  {
    "text": "realizes there isn't any work to do and this was kind of in contradiction to the the caching idea because what happens is",
    "start": "2319079",
    "end": "2325079"
  },
  {
    "text": "you load a bunch of data into memory and then it says okay I'm done processing all of this and it takes them all away",
    "start": "2325079",
    "end": "2330359"
  },
  {
    "text": "so this was one of the the first issues that we ran into with cach data and the",
    "start": "2330359",
    "end": "2335400"
  },
  {
    "start": "2334000",
    "end": "2334000"
  },
  {
    "text": "solution to this was you know adding a different timeout for containers that actually uh cash data so we set this to",
    "start": "2335400",
    "end": "2343240"
  },
  {
    "text": "15 minutes which means that people who are doing interactive work have enough time to work with it but if they stop",
    "start": "2343240",
    "end": "2348800"
  },
  {
    "text": "doing it it's going to release all those resources back to the pool so other jobs and other people can take advantage of",
    "start": "2348800",
    "end": "2355760"
  },
  {
    "text": "them preemption so preemption is a concept that's been in Hadoop for a long",
    "start": "2355760",
    "end": "2362000"
  },
  {
    "start": "2356000",
    "end": "2356000"
  },
  {
    "text": "time now but running spark on top of yarn shows you that there are some kind of differences between the philosophy of",
    "start": "2362000",
    "end": "2369160"
  },
  {
    "text": "the platform spark had no concept of what preemption was originally and so that meant that in a dynamic environment",
    "start": "2369160",
    "end": "2375000"
  },
  {
    "text": "like ours where you have different jobs with different weights a new job comes in and it starts taking resources away",
    "start": "2375000",
    "end": "2380800"
  },
  {
    "text": "from a spark job and what happens is spark considers that failures because all it's seeing is executors that are",
    "start": "2380800",
    "end": "2387400"
  },
  {
    "text": "being killed now this is a problem because if it happens enough to the right tasks then your spark job fails so",
    "start": "2387400",
    "end": "2394520"
  },
  {
    "text": "this started happening and we realized you know this is a particular problem because it needs to understand not just",
    "start": "2394520",
    "end": "2400119"
  },
  {
    "text": "how spark works but how yarn works as well so this was taken care of and the solution was to make sure that they they",
    "start": "2400119",
    "end": "2405760"
  },
  {
    "text": "understand the preempted tasks can be retried and shouldn't be retried this next section I'm going to",
    "start": "2405760",
    "end": "2411960"
  },
  {
    "text": "be talking about some of the the work that uh or issues that we've run into while reading processing and writing",
    "start": "2411960",
    "end": "2417800"
  },
  {
    "text": "data and some of this actually applies more to AWS some of it is core spark and",
    "start": "2417800",
    "end": "2423640"
  },
  {
    "text": "some of it is the way that Hadoop Works in general",
    "start": "2423640",
    "end": "2428760"
  },
  {
    "start": "2428000",
    "end": "2428000"
  },
  {
    "text": "so the first is actually listing in S3 S3 definitely has additional latency in",
    "start": "2428760",
    "end": "2435200"
  },
  {
    "text": "comparison to hdfs and one of the major parts of any job of course is the setup where you go and you calculate splits",
    "start": "2435200",
    "end": "2440560"
  },
  {
    "text": "and you do list operations now we have a lot of metadata",
    "start": "2440560",
    "end": "2445720"
  },
  {
    "text": "and our metadata is so large that clients can't necessarily always hold it",
    "start": "2445720",
    "end": "2451000"
  },
  {
    "text": "all in memory uh we run into problems with projecting uh or pruning partitions and with this many one of the issues you",
    "start": "2451000",
    "end": "2458079"
  },
  {
    "text": "run into as well is how long it takes to get through all of those in order to",
    "start": "2458079",
    "end": "2463160"
  },
  {
    "text": "list and when this becomes a problem then the clients will take hours before they can actually submit a job to the",
    "start": "2463160",
    "end": "2469280"
  },
  {
    "text": "cluster and that's time that we don't really want to waste on the on the client side we want most of the computation be done to be done on the",
    "start": "2469280",
    "end": "2476520"
  },
  {
    "start": "2476000",
    "end": "2476000"
  },
  {
    "text": "cluster so in Hadoop there's actually this concept of just parallelizing this uh and because spark actually reuses",
    "start": "2476520",
    "end": "2483040"
  },
  {
    "text": "many of the same Hadoop classes you'd think you'd be able to set these properties and get this parallelism but you can't and that's partially due",
    "start": "2483040",
    "end": "2490200"
  },
  {
    "start": "2490000",
    "end": "2490000"
  },
  {
    "text": "to the way that spark actually handles partitions it actually keeps each one of them as a a wrapped Hadoop rdd and then",
    "start": "2490200",
    "end": "2498119"
  },
  {
    "text": "they go through them in sequence so if you set this property to do parallelized listing all it's going to do is it's",
    "start": "2498119",
    "end": "2504440"
  },
  {
    "text": "actually going to list each Hadoop rdd or within each Hadoop rdd but there's only one partition anyway so you don't",
    "start": "2504440",
    "end": "2509760"
  },
  {
    "text": "get any performance benefit so there are a couple Jos that address this um and there really two",
    "start": "2509760",
    "end": "2517920"
  },
  {
    "text": "issues associated with uh using listing with S3 the first one as I mentioned and",
    "start": "2517920",
    "end": "2524280"
  },
  {
    "text": "the second one is that Hadoop does this second check to look for locality and since we're using S3 as our data",
    "start": "2524280",
    "end": "2529880"
  },
  {
    "text": "warehouse we already know that it's not going to be data local um you're not going to be able to get your compute",
    "start": "2529880",
    "end": "2535359"
  },
  {
    "text": "resources to where the data is so why do this check it adds an extra call to S3",
    "start": "2535359",
    "end": "2540760"
  },
  {
    "text": "for every single file that has to be listed so the solution was actually to change this a little bit",
    "start": "2540760",
    "end": "2547319"
  },
  {
    "text": "so the change in includes um how spark actually handles the the set of",
    "start": "2547319",
    "end": "2553880"
  },
  {
    "text": "partitions and to list them in parallel and secondly to use the Amazon S3 client directly to get the information about",
    "start": "2553880",
    "end": "2561200"
  },
  {
    "text": "the files so that it doesn't do the second check like Hadoop does and as you can see the performance",
    "start": "2561200",
    "end": "2568800"
  },
  {
    "start": "2566000",
    "end": "2566000"
  },
  {
    "text": "Improvement is very significant we're actually not talking about a whole lot of partitions here I mean 720 is pretty",
    "start": "2568800",
    "end": "2574240"
  },
  {
    "text": "small we regularly have jobs that are doing 10,000 partition itions even 100,000 partitions and as you can see",
    "start": "2574240",
    "end": "2580440"
  },
  {
    "text": "from the times associated with this many of those would never be able to even kick off so this is a particularly",
    "start": "2580440",
    "end": "2587319"
  },
  {
    "text": "important Improvement and uh is is great for integration with S3 I believe the",
    "start": "2587319",
    "end": "2592400"
  },
  {
    "text": "the jurus for this are still outstanding but they're being worked on and reviewed right now the next is actually writing data to",
    "start": "2592400",
    "end": "2599839"
  },
  {
    "start": "2597000",
    "end": "2597000"
  },
  {
    "text": "S3 when Hadoop was built it was really tightly integrated with the hdfs file",
    "start": "2599839",
    "end": "2605040"
  },
  {
    "text": "system it makes a lot of assumptions about about how that file system behaves in order to do its commit logic",
    "start": "2605040",
    "end": "2611040"
  },
  {
    "text": "specifically Atomic renames so that's how it kind of does its you know um",
    "start": "2611040",
    "end": "2616200"
  },
  {
    "text": "choosing which task actually was successful it moves it into the final location using an atomic rename and as",
    "start": "2616200",
    "end": "2622400"
  },
  {
    "text": "you may know S3 doesn't really support this a rename operation or a move operation is really just a copy and delete and that's bad for a number of",
    "start": "2622400",
    "end": "2629559"
  },
  {
    "text": "different reasons one because of the extra time that it takes there's eventual consistency associated with it",
    "start": "2629559",
    "end": "2635359"
  },
  {
    "text": "and lastly if you're using version buckets like we are you get a second copy of all your data in S3 which you",
    "start": "2635359",
    "end": "2641599"
  },
  {
    "text": "probably don't want to pay for so the solution to this is actually to change the behavior of how output",
    "start": "2641599",
    "end": "2648359"
  },
  {
    "start": "2645000",
    "end": "2645000"
  },
  {
    "text": "commits are done and what we do is we actually change the logic so that it",
    "start": "2648359",
    "end": "2653920"
  },
  {
    "text": "writes out to local dis first for the temporary storage and then when it actually does the commit operation we do",
    "start": "2653920",
    "end": "2659920"
  },
  {
    "text": "an upload to S3 and this can actually be pretty efficient because you can use multi-art upload for large files and",
    "start": "2659920",
    "end": "2666040"
  },
  {
    "text": "other sorts of optimization um and of course this avoids all of the",
    "start": "2666040",
    "end": "2671240"
  },
  {
    "text": "problems that we just discussed before you know there's still issues with eventual consistency but they're different kind of issues you don't have",
    "start": "2671240",
    "end": "2677119"
  },
  {
    "text": "to worry about you know um the the copy not being available on the the following",
    "start": "2677119",
    "end": "2683119"
  },
  {
    "text": "list operation and the last thing of course is we add the additional functional functionality to write it to",
    "start": "2683119",
    "end": "2688839"
  },
  {
    "text": "a new S3 prefix so equivalently an hdfs path each time we do this using the hive",
    "start": "2688839",
    "end": "2695440"
  },
  {
    "text": "metastore so that we can just you know postfix some sort of identifier to the end of the the U partition location to",
    "start": "2695440",
    "end": "2703359"
  },
  {
    "text": "prevent any sort of update inconsistency that can happen on",
    "start": "2703359",
    "end": "2709040"
  },
  {
    "text": "S3 now I mentioned a lot of issues that we ran into that's that's actually just a subset of the things that we've been",
    "start": "2709760",
    "end": "2716040"
  },
  {
    "text": "working on to integrate this into the platform we've made a number of contributions to spark and a lot of",
    "start": "2716040",
    "end": "2721839"
  },
  {
    "text": "those other issues uh you know it's a combination of work that we've done work that the community's done as I said it's",
    "start": "2721839",
    "end": "2727040"
  },
  {
    "text": "very active community and so there's a lot of work going into making it work in the cloud as well as um you know in the",
    "start": "2727040",
    "end": "2733880"
  },
  {
    "text": "data center so what are the next steps for Netflix as I mentioned this is very",
    "start": "2733880",
    "end": "2739280"
  },
  {
    "start": "2735000",
    "end": "2735000"
  },
  {
    "text": "early on in the integration with our platform we still need to do more work as far as metrics are concerned you know",
    "start": "2739280",
    "end": "2744960"
  },
  {
    "text": "with Hadoop it's been around for a long time we collect a lot of metrics we understand how jobs are performing and",
    "start": "2744960",
    "end": "2750559"
  },
  {
    "text": "we can't really do the same thing right now with spark and we want to have that kind of insight into how these jobs are are running uh the next is data lineage",
    "start": "2750559",
    "end": "2758200"
  },
  {
    "text": "we have a lot of data a lot of tables and with as many people that are using the system it it's hard to know that if",
    "start": "2758200",
    "end": "2763880"
  },
  {
    "text": "you're going to make a change to a data source who's going to be impacted or if you're trying to find who's producing",
    "start": "2763880",
    "end": "2769160"
  },
  {
    "text": "data where they are and who's actually you know um making that data available so data lineage is something that we",
    "start": "2769160",
    "end": "2775119"
  },
  {
    "text": "built into all the other tools in our stack and we can answer these questions but right now with spark it's a little bit of a black box and we're working on",
    "start": "2775119",
    "end": "2781359"
  },
  {
    "text": "that as well and lastly As Eva had mentioned before the par a integration know we're a very big paret user so",
    "start": "2781359",
    "end": "2788880"
  },
  {
    "text": "we're interested in the uh the performance improvements that she had discussed we want to add them you know",
    "start": "2788880",
    "end": "2794040"
  },
  {
    "text": "to spark and to all the other engines as well so there are some key takeaways",
    "start": "2794040",
    "end": "2800680"
  },
  {
    "text": "from this talk and the first of course is decoupling our compute from our storage and this is why we use S3 for",
    "start": "2800680",
    "end": "2807440"
  },
  {
    "text": "our data warehousing there are a lot of other advantages to using S3 including you know the durability the versioning",
    "start": "2807440",
    "end": "2814880"
  },
  {
    "text": "but separating the compute from storage actually makes a lot of things that are hard when you Sil that you know in a",
    "start": "2814880",
    "end": "2820960"
  },
  {
    "text": "typical data center where you Silo your data with your actual compute very easy to do in the",
    "start": "2820960",
    "end": "2827240"
  },
  {
    "text": "cloud the next is that we use EMR a little bit differently than most people do um we use it not just as a provider",
    "start": "2827240",
    "end": "2835200"
  },
  {
    "text": "of map ruce but as a cluster manager so with Presto we're not actually using the Hadoop distribution at all and we just",
    "start": "2835200",
    "end": "2841720"
  },
  {
    "text": "take advantage of the fact that they're monitoring the cluster removing any sort of uh problematic node",
    "start": "2841720",
    "end": "2847800"
  },
  {
    "text": "and then uh the you know intelligent grow shrink capability that they've",
    "start": "2847800",
    "end": "2853400"
  },
  {
    "text": "recently put into to the cluster management uh spark we run a little bit differently we actually use it as a",
    "start": "2853400",
    "end": "2859559"
  },
  {
    "text": "co-tenant on on yarn and so in this case we're actually using the full resources of at least the Hadoop distribution that",
    "start": "2859559",
    "end": "2866520"
  },
  {
    "text": "they provide and lastly you know we're very committed to doing open- Source projects",
    "start": "2866520",
    "end": "2872440"
  },
  {
    "text": "to contributing our work back to the community because everybody can take",
    "start": "2872440",
    "end": "2877760"
  },
  {
    "text": "advantage of these resources we and we take advantage of the contributions that other companies are making and if you're",
    "start": "2877760",
    "end": "2885599"
  },
  {
    "text": "interested you can always take a look at either the patches that we've provided or you can take a look at you know the",
    "start": "2885599",
    "end": "2891319"
  },
  {
    "text": "the um the jurus that I that I listed earlier so what's",
    "start": "2891319",
    "end": "2897520"
  },
  {
    "text": "next you know we mentioned that we do some graceful shrink or some uh um",
    "start": "2897520",
    "end": "2904280"
  },
  {
    "text": "resizing of clusters based on utilization other things we actually want to expand these use cases we want",
    "start": "2904280",
    "end": "2909400"
  },
  {
    "text": "to do this more with our our larger Hadoop clusters we want to integrate it with",
    "start": "2909400",
    "end": "2915200"
  },
  {
    "text": "other tools in our ecosystem like Atlas and Spiner so that we both know what the",
    "start": "2915200",
    "end": "2921760"
  },
  {
    "text": "load is on the cluster what's available and do more Dynamic reshaping of the",
    "start": "2921760",
    "end": "2926800"
  },
  {
    "text": "Clusters to take advantage of the resources that we do",
    "start": "2926800",
    "end": "2931558"
  },
  {
    "text": "have and the last thing is we want to expand use cases figure out how to",
    "start": "2933200",
    "end": "2939079"
  },
  {
    "text": "harness these tools better to solve problems for people who are using the platform integrate",
    "start": "2939079",
    "end": "2945599"
  },
  {
    "text": "spark and explore spark as a an option for",
    "start": "2945599",
    "end": "2952319"
  },
  {
    "text": "ETL so with that thank you very much and we can take",
    "start": "2953960",
    "end": "2959520"
  },
  {
    "text": "a few questions",
    "start": "2959520",
    "end": "2962880"
  }
]