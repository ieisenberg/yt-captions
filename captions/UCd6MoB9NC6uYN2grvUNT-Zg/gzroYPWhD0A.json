[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "okay yesterday I actually gave a demo where I was very happy to see one",
    "start": "30",
    "end": "7529"
  },
  {
    "text": "thousand and eight cores all say hello world so today is is more of just a talk",
    "start": "7529",
    "end": "14280"
  },
  {
    "text": "and an explanation of basically using h AWS for HPC okay let's see if I can get",
    "start": "14280",
    "end": "23310"
  },
  {
    "text": "this right okay so we like to say it's about science not about servers our",
    "start": "23310",
    "end": "31489"
  },
  {
    "text": "ultimate goal is to basically allow researchers to extend into the cloud without having to have much IT",
    "start": "31489",
    "end": "38309"
  },
  {
    "text": "experience oops that's not what I wanted to do oh well I'm a little bit ahead of",
    "start": "38309",
    "end": "44399"
  },
  {
    "start": "39000",
    "end": "39000"
  },
  {
    "text": "myself so the we're not there yet but we're",
    "start": "44399",
    "end": "50039"
  },
  {
    "text": "getting there particularly the partners I encourage you to go look at ronan out",
    "start": "50039",
    "end": "55050"
  },
  {
    "text": "in the exhibition hall and the Intel booth they're one of our partners who is",
    "start": "55050",
    "end": "60449"
  },
  {
    "text": "doing an excellent job of trying to make it more transparent for researchers to get on to the cloud and handle some of",
    "start": "60449",
    "end": "66630"
  },
  {
    "text": "the scary things like budgets some of",
    "start": "66630",
    "end": "71700"
  },
  {
    "text": "the great features that we have in HPC at AWS are the ability to experiment",
    "start": "71700",
    "end": "78840"
  },
  {
    "text": "without fear it's almost a company motto to fail fast you know the objective is",
    "start": "78840",
    "end": "84479"
  },
  {
    "text": "not to spend a year doing some research and setting up your servers and spending an enormous amount of time and",
    "start": "84479",
    "end": "90060"
  },
  {
    "text": "simulation only to discover that you made a fundamental error and insight at the beginning it's nice to know that",
    "start": "90060",
    "end": "96840"
  },
  {
    "text": "quickly I've done that before and my background I spent 25 years as an",
    "start": "96840",
    "end": "102540"
  },
  {
    "text": "aerospace professor and I've had my own embarrassments where I spent a year doing research only to figure out that",
    "start": "102540",
    "end": "107850"
  },
  {
    "text": "one of my initial assumptions was incorrect some of the things that we",
    "start": "107850",
    "end": "113310"
  },
  {
    "text": "allow are the ability to start and stop instances which is critical for cost in",
    "start": "113310",
    "end": "119909"
  },
  {
    "text": "fact that's probably the way people save money the most is to be able to when you're not working to stop an instance",
    "start": "119909",
    "end": "126180"
  },
  {
    "text": "or in your language maybe a node would be the right term to stop it and",
    "start": "126180",
    "end": "132520"
  },
  {
    "text": "you're only charged for we use so when you stop it you're not paying for it we also have spot pricing spot pricing is",
    "start": "132520",
    "end": "138610"
  },
  {
    "text": "really good for researchers because we're not we're not banks where we have to deal with transactions immediate",
    "start": "138610",
    "end": "145120"
  },
  {
    "text": "transactions we're not Netflix where you don't want to have customers have to wait and for their streaming will",
    "start": "145120",
    "end": "151990"
  },
  {
    "text": "research typically you know we can wait a little while if the resources aren't available we're used to queues notice my",
    "start": "151990",
    "end": "159340"
  },
  {
    "text": "shirt says notice my shirt says there's no queue in the cloud but with spot you",
    "start": "159340",
    "end": "167080"
  },
  {
    "text": "can actually accept acusa what is spot spot is our excess capacity it means",
    "start": "167080",
    "end": "172090"
  },
  {
    "text": "that if there is excess capacity we can we charge you between 90 to 70 percent",
    "start": "172090",
    "end": "177730"
  },
  {
    "text": "less than on-demand prices so it's a really good way to save money and",
    "start": "177730",
    "end": "183400"
  },
  {
    "text": "researchers tend to use that by using",
    "start": "183400",
    "end": "188890"
  },
  {
    "text": "cloud you don't have to worry about the infrastructure that updates the you always get what's best so I'm later on",
    "start": "188890",
    "end": "195880"
  },
  {
    "text": "I'm gonna show benchmarks against the archer which is the UK cray for it",
    "start": "195880",
    "end": "202690"
  },
  {
    "text": "supplied for research the problem with that is that it's now getting kind of old it uses Ivy Bridge technology it's",
    "start": "202690",
    "end": "209050"
  },
  {
    "text": "about five years old and you know using skylake technology where we're twice as",
    "start": "209050",
    "end": "216370"
  },
  {
    "text": "fast so you get the benefit of continual updates you know you can pretty much",
    "start": "216370",
    "end": "221489"
  },
  {
    "text": "guess that if something is new on the horizon AWS is gonna be one of the first",
    "start": "221489",
    "end": "226630"
  },
  {
    "text": "to have it so by being part of AWS you can get the best the quickest network",
    "start": "226630",
    "end": "232690"
  },
  {
    "text": "storage all the services are continually updating you have to worry about updating the operating system some of",
    "start": "232690",
    "end": "241959"
  },
  {
    "text": "the ways of people use AWS is to run many jobs in parallel tlg aerospace as",
    "start": "241959",
    "end": "247720"
  },
  {
    "text": "an example will run instead of one job in serial and take a week to do a",
    "start": "247720",
    "end": "254440"
  },
  {
    "text": "customer's workload they run them all in parallel and get them done overnight that allows them to do their engineer",
    "start": "254440",
    "end": "260410"
  },
  {
    "text": "work in their analysis over the week doesn't cost any more to run them in parallel as it does serial because you",
    "start": "260410",
    "end": "265960"
  },
  {
    "text": "pay for what you use the if you run on demand",
    "start": "265960",
    "end": "271390"
  },
  {
    "text": "you have no queue typically in 95% of the cases if you run spot you're also",
    "start": "271390",
    "end": "276430"
  },
  {
    "text": "going to get immediate immediate satisfaction you'll you'll run right out",
    "start": "276430",
    "end": "282550"
  },
  {
    "text": "of the box you can you don't have to worry about pre provisioning there's no",
    "start": "282550",
    "end": "287800"
  },
  {
    "text": "ordering of parts and saying I need a cluster of so many cores waiting six months for it to get to get delivered",
    "start": "287800",
    "end": "294400"
  },
  {
    "text": "installed we can set up a cluster this afternoon and run I did I launched a",
    "start": "294400",
    "end": "301150"
  },
  {
    "text": "true HPC cluster with 1008 Lake cores did that yesterday the entire",
    "start": "301150",
    "end": "308590"
  },
  {
    "text": "demo was a half an hour the cost advantages really is that you're only",
    "start": "308590",
    "end": "315400"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "paying for what you use and you don't have to over-provision guessing what the workloads going to be each cluster can",
    "start": "315400",
    "end": "322810"
  },
  {
    "text": "be actually launched per job if you want to in fact some people actually do that",
    "start": "322810",
    "end": "327820"
  },
  {
    "text": "for particularly for highly coupled schemes throughout a couple of codes that take hours or even days to run with",
    "start": "327820",
    "end": "338020"
  },
  {
    "text": "AWS and the pages you go pricing and the scaling the auto scaling that's really",
    "start": "338020",
    "end": "343930"
  },
  {
    "text": "where you save the money is not paying for what you're not using a lot of people look at what what would it cost",
    "start": "343930",
    "end": "350890"
  },
  {
    "text": "to run my own Prem facility on AWS if you do that without looking at all the",
    "start": "350890",
    "end": "357640"
  },
  {
    "text": "benefits of cloud you're actually going to pay more on AWS the benefit is the",
    "start": "357640",
    "end": "362980"
  },
  {
    "text": "scaling the ability to shrink and expand and only pay for what you use and I mean",
    "start": "362980",
    "end": "369070"
  },
  {
    "text": "how many of you are in situations where your cluster is perfectly sized it's a hundred percent utilized no queue no",
    "start": "369070",
    "end": "376240"
  },
  {
    "text": "that doesn't exist I've never seen that I got started in AWS and 2014 when I was",
    "start": "376240",
    "end": "383890"
  },
  {
    "text": "in a consulting company with two people because we had access to AWS because we were using AWS we were able to bid on",
    "start": "383890",
    "end": "391510"
  },
  {
    "text": "jobs that NASA and Boeing bit on we didn't have any infrastructure",
    "start": "391510",
    "end": "396560"
  },
  {
    "text": "all we did is we did we're able to bid on the job using AWS and there's passed",
    "start": "396560",
    "end": "401690"
  },
  {
    "text": "the cost on to to the and actually was cheaper than NASA or Boeing could do",
    "start": "401690",
    "end": "407060"
  },
  {
    "text": "because well NASA's government so different cost structure but we could actually bid on large jobs and do it",
    "start": "407060",
    "end": "414980"
  },
  {
    "text": "cheaper because we weren't we weren't having to support infrastructure a lot",
    "start": "414980",
    "end": "420470"
  },
  {
    "text": "of popular workloads genomics modeling and simulation I've come from a background of computation of fluid",
    "start": "420470",
    "end": "426500"
  },
  {
    "text": "dynamics and what I'm going to show later or some CFD simulations or snot",
    "start": "426500",
    "end": "432020"
  },
  {
    "text": "simulations but results timing results Montecarlo embarrassingly parallel kind of schemes are are very easy to run in",
    "start": "432020",
    "end": "440900"
  },
  {
    "text": "AWS I had a colleague who ran a a financial simulation for a proof of",
    "start": "440900",
    "end": "448970"
  },
  {
    "text": "concept for a bank in London and ran 1.3 million cores and that was in our excess",
    "start": "448970",
    "end": "455570"
  },
  {
    "text": "capacity so the amount of resources is sort of astronomical some of the",
    "start": "455570",
    "end": "463580"
  },
  {
    "start": "462000",
    "end": "462000"
  },
  {
    "text": "important enablers is we have a variety of choices for node types for example we",
    "start": "463580",
    "end": "470360"
  },
  {
    "text": "can you can choose how much memory you want per core our class of compute would be 4 gigabytes per core but we also have",
    "start": "470360",
    "end": "478340"
  },
  {
    "text": "high memory instances if you need if you have applications that require more",
    "start": "478340",
    "end": "483350"
  },
  {
    "text": "memory at 16 gigabits per core we have some some some nodes some instances that",
    "start": "483350",
    "end": "493220"
  },
  {
    "text": "go up to 12 and a half terabytes per core so there's a lot of options we have",
    "start": "493220",
    "end": "499460"
  },
  {
    "text": "GPUs we have for those that are really adventurous FPGAs those are field",
    "start": "499460",
    "end": "507110"
  },
  {
    "text": "programmable gate arrays I've seen some very interesting simulations while they look very expensive when you look at the",
    "start": "507110",
    "end": "513080"
  },
  {
    "text": "price because you can tailor it for your problem they end up being cheaper than using a classic skylake or whatever",
    "start": "513080",
    "end": "520159"
  },
  {
    "text": "intelligent we have choices between Intel AMD arm type processors so",
    "start": "520159",
    "end": "528019"
  },
  {
    "text": "depending on your workload and what what you want how efficient you want it you",
    "start": "528019",
    "end": "533569"
  },
  {
    "text": "have you have lots of choices we have some networking capabilities so you can",
    "start": "533569",
    "end": "541129"
  },
  {
    "text": "actually tailor your your workload depending on or you can tailor your your",
    "start": "541129",
    "end": "547519"
  },
  {
    "text": "cluster or your compute to have the right kind of network performance you",
    "start": "547519",
    "end": "553519"
  },
  {
    "text": "want and I'll talk a little bit about that later storage we have many many different storage options our new fsx",
    "start": "553519",
    "end": "560209"
  },
  {
    "text": "for lustre oh darn it I'm a little bit too heavy-handed here and there appears to be no way to go back ok a little bit",
    "start": "560209",
    "end": "569649"
  },
  {
    "text": "fsx for lustre is a really is a new",
    "start": "569649",
    "end": "575810"
  },
  {
    "text": "lustre type file system that shared very high performance and is backed up on our",
    "start": "575810",
    "end": "580970"
  },
  {
    "text": "s3 service so it's really slick because it scales dynamically and then it's",
    "start": "580970",
    "end": "586610"
  },
  {
    "text": "backed up on s3 we have lost the",
    "start": "586610",
    "end": "592399"
  },
  {
    "text": "previous slide I think that's pretty much what I wanted to cover on the previous slide but we'll get to it again later the model to look at on the Left",
    "start": "592399",
    "end": "599899"
  },
  {
    "text": "we have a classic on-prem facility where you're trying to pack all your workloads into a given size you have so many cores",
    "start": "599899",
    "end": "607370"
  },
  {
    "text": "you have so much time and everyone's gotta wait in a queue whereas because of",
    "start": "607370",
    "end": "612769"
  },
  {
    "text": "our resources I like to say we're to the user we're virtually infinite well we're not",
    "start": "612769",
    "end": "619040"
  },
  {
    "text": "I'll I'll give some stories about where we are not quite infinite in a little while but the the reality is that for",
    "start": "619040",
    "end": "627439"
  },
  {
    "text": "most users you can burst in and use as much resources as you want at any time",
    "start": "627439",
    "end": "634069"
  },
  {
    "text": "and you can run your job when you want to run it so which architecture do you",
    "start": "634069",
    "end": "639589"
  },
  {
    "text": "do you choose one size does not fit all but we give you the ability to design a",
    "start": "639589",
    "end": "645709"
  },
  {
    "text": "computer if you're an IT person a lot of IT people are scared of the cloud cuz they're afraid they're going to lose",
    "start": "645709",
    "end": "651199"
  },
  {
    "text": "their job no actually it's going to increase the the need for IT resources because",
    "start": "651199",
    "end": "657980"
  },
  {
    "text": "instead of your job being getting up at 2:00 in the morning to find out why your server crashed and look at your script",
    "start": "657980",
    "end": "664100"
  },
  {
    "text": "and try to debug your script that is actually what AWS does you don't have to",
    "start": "664100",
    "end": "669890"
  },
  {
    "text": "do that heavy lifting your job will be to sit down with your researchers on a daily basis and say what's the best",
    "start": "669890",
    "end": "675500"
  },
  {
    "text": "architecture I get to design a computer continually and optimize its it's a lot",
    "start": "675500",
    "end": "682310"
  },
  {
    "text": "I think it'll be a lot more fun the type of architecture is going to depend on",
    "start": "682310",
    "end": "688310"
  },
  {
    "text": "the experience of the user the desired deployment method characteristics of the",
    "start": "688310",
    "end": "695210"
  },
  {
    "text": "application my background being computation of fluids I'm very interested in seeing how different algorithms will tie into different",
    "start": "695210",
    "end": "702020"
  },
  {
    "text": "architectures and I would like to eventually see an optimum way to balance",
    "start": "702020",
    "end": "709520"
  },
  {
    "text": "the architecture versus the the application we have a new product this",
    "start": "709520",
    "end": "716780"
  },
  {
    "text": "is not the only way to launch a cluster but we have AWS parallel cluster which",
    "start": "716780",
    "end": "722060"
  },
  {
    "text": "makes it much easier the demo I ran yesterday I showed the script that we used and we're able to launch a thousand",
    "start": "722060",
    "end": "729950"
  },
  {
    "text": "eight skylake cores what's really kind of neat about it is that it dynamically allocates the compute nodes as needed so",
    "start": "729950",
    "end": "737360"
  },
  {
    "text": "until you submit your job you don't have compute nodes which means you're not paying for them you submit your job you",
    "start": "737360",
    "end": "743120"
  },
  {
    "text": "say you need to you know thousand two thousand four thousand whatever it starts launching those compute nodes and",
    "start": "743120",
    "end": "748370"
  },
  {
    "text": "then runs your job and when it's done those compute notes go away there are",
    "start": "748370",
    "end": "755450"
  },
  {
    "text": "other products that launch clusters I mentioned Ronin earlier they now incorporate the ability to launch",
    "start": "755450",
    "end": "761600"
  },
  {
    "text": "clusters within Ronin the ideal would I think for researchers would be just a",
    "start": "761600",
    "end": "767270"
  },
  {
    "text": "click click click webpage we're not there yet but we have partners that are",
    "start": "767270",
    "end": "772550"
  },
  {
    "text": "working on that there's many different ways to launch a cluster parallel",
    "start": "772550",
    "end": "778190"
  },
  {
    "text": "clusters is our choice is built on AWS CloudFormation the thing that's really",
    "start": "778190",
    "end": "783260"
  },
  {
    "text": "neat about this to me is that with a 30-page configuration file I can change how many nodes I want",
    "start": "783260",
    "end": "790910"
  },
  {
    "text": "I can change the type of node I can change the member the storage choice",
    "start": "790910",
    "end": "795949"
  },
  {
    "text": "from let's say a shared file system on using NFS on the head node to a shared",
    "start": "795949",
    "end": "802579"
  },
  {
    "text": "file system using lustre I have so many choices that I can do interactively I can change my network speed I can do all",
    "start": "802579",
    "end": "809299"
  },
  {
    "text": "this with I said interactively I'm sorry and in the config file just one line of code and I can change my architecture",
    "start": "809299",
    "end": "817160"
  },
  {
    "text": "significantly and it's very easy to do when I can launch it in five minutes later check it out",
    "start": "817160",
    "end": "822859"
  },
  {
    "text": "doesn't not what I wanted kill it watch another one in terms of computation what",
    "start": "822859",
    "end": "833509"
  },
  {
    "text": "I'm talking about today is mostly cluster HPC or tightly coupled schemes this is where a particular code requires",
    "start": "833509",
    "end": "841009"
  },
  {
    "text": "communication with all the processors that are involved that's as opposed to",
    "start": "841009",
    "end": "846109"
  },
  {
    "text": "Grid HPC or loosely coupled embarrassingly parallel codes like Monte Carlo schemes where each thread is",
    "start": "846109",
    "end": "852559"
  },
  {
    "text": "totally independent of the other there is no need for communication we can also",
    "start": "852559",
    "end": "858259"
  },
  {
    "text": "run glid grids of clusters so for example I mentioned tlg aerospace that that used to run a single simulation and",
    "start": "858259",
    "end": "865369"
  },
  {
    "text": "it took about a week to get through their workload instead they launched multiple clusters and they can actually",
    "start": "865369",
    "end": "870409"
  },
  {
    "text": "run them simultaneously get the job done overnight and instead of waiting a week that basically overnight they have all",
    "start": "870409",
    "end": "876679"
  },
  {
    "text": "their results Greek computing examples",
    "start": "876679",
    "end": "881859"
  },
  {
    "start": "879000",
    "end": "879000"
  },
  {
    "text": "Large Hadron Collider in CERN uses us to do some data analysis that plot and the",
    "start": "881859",
    "end": "889579"
  },
  {
    "text": "lower-left are in the mid left sorry mid right I'm backwards here mid",
    "start": "889579",
    "end": "894709"
  },
  {
    "text": "right shows their workload you can actually see a weekend when everyone was off but that's their workload over time",
    "start": "894709",
    "end": "901699"
  },
  {
    "text": "if you were gonna buy a facility for that that's a little bit hard what do you choose if you pick the plateau let",
    "start": "901699",
    "end": "909619"
  },
  {
    "text": "me see if I can do this if you click the pick the plateau here to size your machine well you're going to have a huge queue later",
    "start": "909619",
    "end": "915530"
  },
  {
    "text": "when the workload comes in if you pick something up here to cover the maximum you're going to have a lot of excess",
    "start": "915530",
    "end": "921920"
  },
  {
    "text": "capacity that's wasted and a lot of cost in that so how do you choose that well",
    "start": "921920",
    "end": "928780"
  },
  {
    "text": "this lower plot shows us the white line here is the total CPU used with AWS by",
    "start": "928780",
    "end": "938810"
  },
  {
    "text": "using auto scaling follows exactly the workload you're only paying for what's",
    "start": "938810",
    "end": "944360"
  },
  {
    "text": "being used so it's a very very cost effective means for a project like the",
    "start": "944360",
    "end": "952370"
  },
  {
    "text": "Large Hadron Collider Clemson University used 1.1 million V CPUs",
    "start": "952370",
    "end": "960050"
  },
  {
    "text": "let me quickly define a V CPU because this always confuses researchers if you",
    "start": "960050",
    "end": "965180"
  },
  {
    "text": "look at your Intel chips or Intel selections everything is listed by V",
    "start": "965180",
    "end": "972650"
  },
  {
    "text": "CPUs in a G's take something I should",
    "start": "972650",
    "end": "978740"
  },
  {
    "text": "put this down in between the V CPUs our",
    "start": "978740",
    "end": "985540"
  },
  {
    "text": "threads in Telus has so such high parallelism within each CPU that they",
    "start": "985540",
    "end": "991070"
  },
  {
    "text": "allow two threads to run through each CPU when you're doing high performance computing typically you're using the",
    "start": "991070",
    "end": "997250"
  },
  {
    "text": "math coprocessor quite a bit and so you're using the same registers the same parts of the chip so it doesn't really",
    "start": "997250",
    "end": "1004270"
  },
  {
    "text": "help to hyper thread so we usually turn off hyper threading automatic now I mean",
    "start": "1004270",
    "end": "1010990"
  },
  {
    "text": "I'm a little bit in trouble maybe it's telling me I need to hurry anyways",
    "start": "1010990",
    "end": "1017830"
  },
  {
    "text": "1.3 million V CPUs for natural language tightly coupled schemes this is the",
    "start": "1017830",
    "end": "1024130"
  },
  {
    "text": "cluster computing the line up the middle there that's kind of faint that's ideal",
    "start": "1024130",
    "end": "1030579"
  },
  {
    "text": "scaling that's if everything was perfectly parallel well you have a lot of things within the code you have",
    "start": "1030579",
    "end": "1036040"
  },
  {
    "text": "network latency you have you have memory",
    "start": "1036040",
    "end": "1041050"
  },
  {
    "text": "latency actually most codes are memory to memory latency not",
    "start": "1041050",
    "end": "1047640"
  },
  {
    "text": "not network latency network latency is always blamed though this is for Worf",
    "start": "1047640",
    "end": "1054180"
  },
  {
    "text": "which is the weather code and that blue that darker blue line shows that the",
    "start": "1054180",
    "end": "1060090"
  },
  {
    "text": "scaling relative to the ideal performance computational fluids this is",
    "start": "1060090",
    "end": "1066480"
  },
  {
    "text": "an sis similar plot ok the computational",
    "start": "1066480",
    "end": "1076530"
  },
  {
    "text": "fluid dynamics is actually code that scales extremely well because it is mostly memory bound oh there we go thank",
    "start": "1076530",
    "end": "1082320"
  },
  {
    "start": "1081000",
    "end": "1081000"
  },
  {
    "text": "you it is mostly memory bound the - green line there by the way again is ideal scaling we get a fair amount of speed up",
    "start": "1082320",
    "end": "1089040"
  },
  {
    "text": "you'll notice this is the number of cores we're down to 20 16 cores 2016",
    "start": "1089040",
    "end": "1094590"
  },
  {
    "text": "cores in this case and it's still scaling quite well ok this is the kind",
    "start": "1094590",
    "end": "1100350"
  },
  {
    "text": "of plot you would see if you're running on a Cray or Fujitsu or something the turning point where you start to get",
    "start": "1100350",
    "end": "1106650"
  },
  {
    "text": "an excess is actually a little bit or",
    "start": "1106650",
    "end": "1112680"
  },
  {
    "text": "that's where you're going to see differences in different architectures and I will talk a little about the network if I have time because that's",
    "start": "1112680",
    "end": "1119160"
  },
  {
    "text": "always a question people have we've had a lot of innovations lately I would have to say if I were to simplify or give a",
    "start": "1119160",
    "end": "1127980"
  },
  {
    "text": "simple description of how HPC evolved on AWS I would almost have to say that researchers found AWS first and then AWS",
    "start": "1127980",
    "end": "1135660"
  },
  {
    "text": "is now starting to look at research as being a very important part of our",
    "start": "1135660",
    "end": "1141120"
  },
  {
    "text": "product line and so in the last year we've launched as I mentioned parallel",
    "start": "1141120",
    "end": "1146640"
  },
  {
    "text": "clusters our cluster management fully managed HPC cluster tool we also have",
    "start": "1146640",
    "end": "1153570"
  },
  {
    "text": "our high-performance shared file system fsx for lustre which I mentioned we have high clock speed compute instances e1d",
    "start": "1153570",
    "end": "1161100"
  },
  {
    "text": "which is sustained for gigahertz we have our network instance high bandwidth and",
    "start": "1161100",
    "end": "1168840"
  },
  {
    "text": "since c5n which has a hundred gigabit per second networking capability and",
    "start": "1168840",
    "end": "1174060"
  },
  {
    "text": "this is per node ok our network is very different than most it's not a backbone it's not a",
    "start": "1174060",
    "end": "1179969"
  },
  {
    "text": "but you get well if InfiniBand said it's a point-to-point we have our high-performance interconnect EFA",
    "start": "1179969",
    "end": "1186509"
  },
  {
    "text": "elastic fabric adapter which has halved our latency we have again apparel",
    "start": "1186509",
    "end": "1193739"
  },
  {
    "text": "cluster we also have a multi node parallel job support for AWS batch so people doing genomics can actually do a",
    "start": "1193739",
    "end": "1200309"
  },
  {
    "text": "scheduler schedule the process where you'd have different compute nodes doing",
    "start": "1200309",
    "end": "1205829"
  },
  {
    "text": "different tasks in a schedule basically supporting supporting containers okay",
    "start": "1205829",
    "end": "1217289"
  },
  {
    "text": "here's an example of what EFA does this is a CFD code meta comp CFD plus plus on",
    "start": "1217289",
    "end": "1225329"
  },
  {
    "text": "a 24 million course this was actually done for fun I don't think that Klingon",
    "start": "1225329",
    "end": "1230639"
  },
  {
    "text": "battle cruiser will ever fly in the atmosphere but it is kind of a cool little little plot there the again the",
    "start": "1230639",
    "end": "1237719"
  },
  {
    "text": "red line is ideal scaling the purple line is pre EFA a pre elastic fabric",
    "start": "1237719",
    "end": "1242969"
  },
  {
    "text": "adapter and this is where you get to about 500 cores and your network gets saturated due to inner processor",
    "start": "1242969",
    "end": "1249539"
  },
  {
    "text": "communication with the FA we continue up well past a thousand cores without that",
    "start": "1249539",
    "end": "1255599"
  },
  {
    "text": "network saturation so this is a case where the network actually is the limiting factor and we've we're doing",
    "start": "1255599",
    "end": "1262229"
  },
  {
    "text": "quite well I will say that we're running lots of benchmarks now against praise and similar machines and actually doing",
    "start": "1262229",
    "end": "1268139"
  },
  {
    "text": "quite well competing very favorably with some of the supercomputers so let's look",
    "start": "1268139",
    "end": "1275549"
  },
  {
    "text": "at some do some scaling benchmarks so we're an archer which is as I mentioned",
    "start": "1275549",
    "end": "1280769"
  },
  {
    "text": "a little bit of a dated machine so we expect to do better and AWS we use the C",
    "start": "1280769",
    "end": "1288209"
  },
  {
    "text": "one D which is our 4 gigahertz 24 processors per instance and then we ran",
    "start": "1288209",
    "end": "1294059"
  },
  {
    "text": "a c5n which is our our 100 gigabit per second this turns out to not use the FA",
    "start": "1294059",
    "end": "1301139"
  },
  {
    "text": "I have this in here but I found out from my colleague that they did not use the FA for this so this is actually not our",
    "start": "1301139",
    "end": "1306569"
  },
  {
    "text": "fast late our low latency but it is our high and network bandwidth we used open foam and open",
    "start": "1306569",
    "end": "1313380"
  },
  {
    "text": "foam if you're familiar is actually a suite of CFD tools we use the pimple",
    "start": "1313380",
    "end": "1318510"
  },
  {
    "text": "foam which is this the most common for aerodynamic calculations when you do",
    "start": "1318510",
    "end": "1325170"
  },
  {
    "text": "calculations you need to play around with a process in the calculation it's not just a simple turn the switch and go",
    "start": "1325170",
    "end": "1332790"
  },
  {
    "text": "because each architecture is going to have a little bit better performance with different method methodologies",
    "start": "1332790",
    "end": "1337830"
  },
  {
    "text": "within the code this is all kind of you know if you know CFD it might mean",
    "start": "1337830",
    "end": "1344400"
  },
  {
    "text": "something when you run you have to consider a lot of different things when",
    "start": "1344400",
    "end": "1350730"
  },
  {
    "text": "we deal with customers to do benchmarks we say well give us your big code it's",
    "start": "1350730",
    "end": "1355740"
  },
  {
    "text": "very common that people give it a little small test problem and they say how does that scale know that that really isn't",
    "start": "1355740",
    "end": "1361350"
  },
  {
    "text": "you know you're not gonna run your test problem give us your big regular application because give us your biggest application",
    "start": "1361350",
    "end": "1367830"
  },
  {
    "text": "we'll test that because that's what you want to be able to do and and so benchmarking should be on the large",
    "start": "1367830",
    "end": "1375330"
  },
  {
    "text": "examples you need to normally the optimization for tightly coupled schemes",
    "start": "1375330",
    "end": "1380910"
  },
  {
    "text": "are tightly coupled algorithms like you haven't CFD for example is how many cells compute cells you need per core",
    "start": "1380910",
    "end": "1387480"
  },
  {
    "text": "and so you're gonna play some games and optimizing that sometimes you're better",
    "start": "1387480",
    "end": "1393179"
  },
  {
    "text": "off actually shutting off some processors on a node because you get cache conflicts and the cache speed",
    "start": "1393179",
    "end": "1400980"
  },
  {
    "text": "actually slows things down so optimizing you know if you have a 36 core a core",
    "start": "1400980",
    "end": "1408000"
  },
  {
    "text": "instance sometimes it's better to run 32 cores and let 4 sit idle because you're actually going to get a faster solution",
    "start": "1408000",
    "end": "1413840"
  },
  {
    "text": "lots of games to play an optimization networking we have a placement group",
    "start": "1413840",
    "end": "1420270"
  },
  {
    "text": "because one of the one of the people are one of the things people are concerned about is the distance between nodes if",
    "start": "1420270",
    "end": "1426990"
  },
  {
    "text": "you have a node that's on the other side of the the data center that's not going to be helped because the latency between",
    "start": "1426990",
    "end": "1432240"
  },
  {
    "text": "that's going to be large so we specify placements groups to put all the the cores or they all the nodes and I'm in a",
    "start": "1432240",
    "end": "1438179"
  },
  {
    "text": "certain near region checking MPI libraries these are all sort of optimized optimization",
    "start": "1438179",
    "end": "1445110"
  },
  {
    "text": "tools quickly if you've used lambdas law Amdahl basically specifies what the",
    "start": "1445110",
    "end": "1451380"
  },
  {
    "text": "scaling is based on the number of cores the I or the ideal is of course that you",
    "start": "1451380",
    "end": "1456990"
  },
  {
    "text": "double the number of cores you double the performance you quadruple quadruple and so on those are the diagonal lines",
    "start": "1456990",
    "end": "1462960"
  },
  {
    "text": "that I showed in those earlier plots however gustafson said well that's not",
    "start": "1462960",
    "end": "1470910"
  },
  {
    "text": "how we run typical codes we don't take a code do we and or a workload and then",
    "start": "1470910",
    "end": "1477300"
  },
  {
    "text": "double the number of cores typically what happens is when we take a workload and we double the number of cores we",
    "start": "1477300",
    "end": "1483480"
  },
  {
    "text": "also double the size of the problem because we're going to increase our workload as we get more and more compute",
    "start": "1483480",
    "end": "1488760"
  },
  {
    "text": "and are the size of our problems so Gustafson scalings a little bit better a little bit more realistic follows the",
    "start": "1488760",
    "end": "1495930"
  },
  {
    "text": "actual workloads that we have so what I'm going to show is kind of Gustav sins follows Gustav sins law and not am dolls",
    "start": "1495930",
    "end": "1503280"
  },
  {
    "text": "so these are results from a fairly",
    "start": "1503280",
    "end": "1508470"
  },
  {
    "start": "1504000",
    "end": "1504000"
  },
  {
    "text": "significant customer and I can't declare I can't tell you who the customers yet",
    "start": "1508470",
    "end": "1514380"
  },
  {
    "text": "we're really close and it's an exciting one I'll tell you that on the Left",
    "start": "1514380",
    "end": "1519920"
  },
  {
    "text": "vertical axis is the seconds per time step which means that down is better",
    "start": "1519920",
    "end": "1525690"
  },
  {
    "text": "that's less time okay to compute you know it's always nice to have up be good",
    "start": "1525690",
    "end": "1532440"
  },
  {
    "text": "but in this case down is good on the lower axis of cells procore so as we go",
    "start": "1532440",
    "end": "1538200"
  },
  {
    "text": "to the left running more and more cores to the right as fewer and fewer these",
    "start": "1538200",
    "end": "1545160"
  },
  {
    "text": "are two simulations we ran a fine and a medium mesh the fine mesh is 280 million",
    "start": "1545160",
    "end": "1554190"
  },
  {
    "text": "cells if you do CFD you know that's pretty pretty fine I'm that's that's",
    "start": "1554190",
    "end": "1559590"
  },
  {
    "text": "pretty high a quarter of a billion cells in our CFD mesh the course was a hundred",
    "start": "1559590",
    "end": "1565320"
  },
  {
    "text": "and forty million we actually ran 400 million cells I don't have that on the",
    "start": "1565320",
    "end": "1571410"
  },
  {
    "text": "because we did a slightly different architecture to be comparing apples to oranges in that case and so I left those",
    "start": "1571410",
    "end": "1577800"
  },
  {
    "text": "off but 200 280 million cell calculations and open foam on the left",
    "start": "1577800",
    "end": "1584610"
  },
  {
    "text": "down here we have our 2000 core simulations and one of the things you",
    "start": "1584610",
    "end": "1592320"
  },
  {
    "text": "can see is Archer which is blue is the hot pretty much the highest on all of these",
    "start": "1592320",
    "end": "1597780"
  },
  {
    "text": "in other words Archer is slower of course we expected that because it's an older machine but we see the Z 1d is you",
    "start": "1597780",
    "end": "1606390"
  },
  {
    "text": "know twice as over twice as fast out to a roughly a thousand cores and still",
    "start": "1606390",
    "end": "1611970"
  },
  {
    "text": "competes favorably about 30 to 40 percent fast or even up to 2,000 cores than Archer one of the things that this",
    "start": "1611970",
    "end": "1619800"
  },
  {
    "text": "tells you is that the network is really not the limiting factor on open foam in",
    "start": "1619800",
    "end": "1625260"
  },
  {
    "text": "fact open from the way it's written just a lot of file writes and the file rights are the limiting act factor but we have",
    "start": "1625260",
    "end": "1631020"
  },
  {
    "text": "with our FS x4 luster and the fact you can have local disks really helps with",
    "start": "1631020",
    "end": "1637200"
  },
  {
    "text": "an open foam latency it's really a",
    "start": "1637200",
    "end": "1642720"
  },
  {
    "start": "1640000",
    "end": "1640000"
  },
  {
    "text": "measure of latency and everyone blames the the the network interconnect and as",
    "start": "1642720",
    "end": "1648510"
  },
  {
    "text": "I try to explain it's not it's actually rarely the latency via or sorry the",
    "start": "1648510",
    "end": "1656820"
  },
  {
    "text": "internet in the network interconnect it's usually the memory that's the most",
    "start": "1656820",
    "end": "1663690"
  },
  {
    "text": "common cause of latency but you also nonparallel code that's actually the worst a lot of researchers will come in",
    "start": "1663690",
    "end": "1669510"
  },
  {
    "text": "and say you know I just ran on a 16 processor to run any faster do you have any parallel code no ok well um you know",
    "start": "1669510",
    "end": "1676170"
  },
  {
    "text": "it's you have to have a good parallel code there are codes that do not really",
    "start": "1676170",
    "end": "1683310"
  },
  {
    "text": "run as well on AWS we found that with the codes at do quantum mechanics like",
    "start": "1683310",
    "end": "1688470"
  },
  {
    "text": "quantum espresso do not scale terrifically well beyond about 4 to 6",
    "start": "1688470",
    "end": "1695250"
  },
  {
    "text": "nodes you're going to start losing performance but CFD runs extremely well whether codes run extremely well",
    "start": "1695250",
    "end": "1703270"
  },
  {
    "text": "so a lot of things will scale well did you know 2000 4000 cores and so on okay",
    "start": "1703270",
    "end": "1711550"
  },
  {
    "text": "I want to acknowledge dr. Neil Ashton of Oxford University actually did the open",
    "start": "1711550",
    "end": "1716560"
  },
  {
    "text": "phone calculations for us with his access to Archer Stephen Sachs at AWS",
    "start": "1716560",
    "end": "1721750"
  },
  {
    "text": "did the 400 million cell cases you know which I did not show because it'd be a little bit of apples to oranges",
    "start": "1721750",
    "end": "1729280"
  },
  {
    "start": "1729000",
    "end": "1729000"
  },
  {
    "text": "I do want to close out one thing I did mention that we like to think of",
    "start": "1729280",
    "end": "1734950"
  },
  {
    "text": "ourselves an infinite resource we actually became so popular there's e1 DS and our c5 ends actually in our p3 GPU",
    "start": "1734950",
    "end": "1743500"
  },
  {
    "text": "nodes as soon as they hit the the market they got saturated we of course are",
    "start": "1743500",
    "end": "1750540"
  },
  {
    "text": "fulfilling those needs rapidly and so now you can know it's not not as big an",
    "start": "1750540",
    "end": "1756730"
  },
  {
    "text": "issue I do want to mention that because I did feel like I was a little bit of a liar earlier when I said oh you can just",
    "start": "1756730",
    "end": "1764020"
  },
  {
    "text": "go on to c5 ends and grab as many as you want and then people started getting hosed but we we've corrected that issue",
    "start": "1764020",
    "end": "1771670"
  },
  {
    "text": "we just weren't expecting that much interest in them and so we you know",
    "start": "1771670",
    "end": "1777640"
  },
  {
    "text": "we're expanding rapidly the interest has really taken off this researchers",
    "start": "1777640",
    "end": "1783940"
  },
  {
    "text": "handbook gives you some basic guidelines and how to how to get started on AWS",
    "start": "1783940",
    "end": "1789370"
  },
  {
    "text": "from a researchers perspective I will say that although it's two years old at AWS that means it's rather dated doesn't",
    "start": "1789370",
    "end": "1796330"
  },
  {
    "text": "have things like parallel cluster in it it doesn't have FS x4 luster doesn't have our c5 n z1 D so it's a little bit",
    "start": "1796330",
    "end": "1804520"
  },
  {
    "text": "behind but it's a good you can download it to get an idea and get started and on the next slide I'll have a little bigger",
    "start": "1804520",
    "end": "1812200"
  },
  {
    "text": "URL so if you want to grab that so you can download that for free ok so I've left 16 seconds for questions",
    "start": "1812200",
    "end": "1821280"
  },
  {
    "text": "that was on purpose actually I'd like to thank you for for your time",
    "start": "1821280",
    "end": "1828220"
  },
  {
    "text": "and and I'll be around if you have any questions I can give you my business card I'm based in London",
    "start": "1828220",
    "end": "1834169"
  },
  {
    "text": "yeah but you know ignore the time difference and I'll get back to you if",
    "start": "1834169",
    "end": "1839600"
  },
  {
    "text": "you have questions okay thank you",
    "start": "1839600",
    "end": "1843309"
  }
]