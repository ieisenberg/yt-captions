[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "we'll go ahead and get started well some people are still filing in this is the",
    "start": "4500",
    "end": "9840"
  },
  {
    "text": "deep dive on amazon kinesis we did swap the sessions today DynamoDB will follow",
    "start": "9840",
    "end": "17130"
  },
  {
    "text": "directly after this one apologies for that change and my name is ian myers i'm",
    "start": "17130",
    "end": "22949"
  },
  {
    "text": "a solution architect working with customers around europe on big data",
    "start": "22949",
    "end": "28619"
  },
  {
    "text": "platforms based in the UK and very pleased to be able to join you guys today so we're going to talk about",
    "start": "28619",
    "end": "36660"
  },
  {
    "start": "35000",
    "end": "104000"
  },
  {
    "text": "conesus which is Amazon's big data streaming platform designed to",
    "start": "36660",
    "end": "43379"
  },
  {
    "text": "facilitate doing real-time processing of data at any scale the fundamental",
    "start": "43379",
    "end": "50700"
  },
  {
    "text": "premise of Kinesis is that you need to decrease the amount of time between when",
    "start": "50700",
    "end": "57360"
  },
  {
    "text": "an event happens and when you can analyze it and so we're able to use",
    "start": "57360",
    "end": "64410"
  },
  {
    "text": "Kinesis as a way of moving data around we can use it as a publish-subscribe",
    "start": "64410",
    "end": "71600"
  },
  {
    "text": "messaging mechanism and we can use it as an application eventing framework it's",
    "start": "71600",
    "end": "78240"
  },
  {
    "text": "designed to be very low-cost very easy to use and extremely elastic to work",
    "start": "78240",
    "end": "84750"
  },
  {
    "text": "with your data volumes whether they're very very small or in the terabytes per hour it's also designed to give you very",
    "start": "84750",
    "end": "92340"
  },
  {
    "text": "very simple and widespread access to clients to process the data or to push",
    "start": "92340",
    "end": "100560"
  },
  {
    "text": "data into Kinesis directly the architecture of Kinesis is as you see on",
    "start": "100560",
    "end": "108600"
  },
  {
    "start": "104000",
    "end": "208000"
  },
  {
    "text": "the left as many data sources and as many different types of data as you",
    "start": "108600",
    "end": "114030"
  },
  {
    "text": "might want to push into your infrastructure these data sources can be",
    "start": "114030",
    "end": "120329"
  },
  {
    "text": "absolutely any type that you require conesus is a binary based system doesn't",
    "start": "120329",
    "end": "127740"
  },
  {
    "text": "have any notion of what the data is that sits on the stream and so you can really use the format that makes most sense for",
    "start": "127740",
    "end": "134190"
  },
  {
    "text": "your application we see customers use JSON data xml protocol buffers thrift whatever you",
    "start": "134190",
    "end": "141150"
  },
  {
    "text": "may find works best for your application and that data is pushed into a Kinesis",
    "start": "141150",
    "end": "147210"
  },
  {
    "text": "stream which is a regional resource that is extremely highly durable the",
    "start": "147210",
    "end": "154860"
  },
  {
    "text": "implementation of that stream is through a set of what we call shards and assured",
    "start": "154860",
    "end": "160920"
  },
  {
    "text": "is the unit of capacity that allows you to grow and shrink this stream elastically and then on the back end we",
    "start": "160920",
    "end": "167640"
  },
  {
    "text": "have our applications which are consuming this data from the stream and unlike a queuing mechanism conesus it's",
    "start": "167640",
    "end": "175590"
  },
  {
    "text": "designed to support as many applications as you require each application can consume the same stream can get the same",
    "start": "175590",
    "end": "184110"
  },
  {
    "text": "amount of data but will not be at exactly the same point in the stream at any one time they can go back to the",
    "start": "184110",
    "end": "191430"
  },
  {
    "text": "beginning of the stream and read data from the beginning they can do whatever they want independently against the very",
    "start": "191430",
    "end": "197670"
  },
  {
    "text": "same data set and the stream is offering",
    "start": "197670",
    "end": "203280"
  },
  {
    "text": "a 24-hour retention period of your data at any scale so we would use Kinesis to",
    "start": "203280",
    "end": "211170"
  },
  {
    "text": "build for example a metering architecture where we push in data files",
    "start": "211170",
    "end": "217500"
  },
  {
    "text": "or data streams that include metering data for example that would go into",
    "start": "217500",
    "end": "224459"
  },
  {
    "text": "Kinesis and be available for 24 hours for processing by a set of Kinesis enabled applications that might do",
    "start": "224459",
    "end": "231060"
  },
  {
    "text": "things like auditing our metering records or be doing incremental bill",
    "start": "231060",
    "end": "240510"
  },
  {
    "text": "calculations or providing an API to read that data one of those applications",
    "start": "240510",
    "end": "246510"
  },
  {
    "text": "would be storing our stream in whatever format makes the most sense for you into",
    "start": "246510",
    "end": "252330"
  },
  {
    "text": "a platform like s3 so that we can access that data with a variety of other tools",
    "start": "252330",
    "end": "257489"
  },
  {
    "text": "including tools like elastic MapReduce or Amazon redshift and we'll talk about",
    "start": "257489",
    "end": "263070"
  },
  {
    "text": "some of the ways that we make that very very simple to manage so the core",
    "start": "263070",
    "end": "268289"
  },
  {
    "start": "267000",
    "end": "521000"
  },
  {
    "text": "components of Kinesis that you should consider first is the stream it's a named resource that you can use",
    "start": "268289",
    "end": "276180"
  },
  {
    "text": "to create a security policy around a type of data some customers will have",
    "start": "276180",
    "end": "282569"
  },
  {
    "text": "one stream for their entire business other customers may create a separate",
    "start": "282569",
    "end": "288120"
  },
  {
    "text": "stream for every discrete data type that they want to move around in their business and either is completely fine",
    "start": "288120",
    "end": "296150"
  },
  {
    "text": "you can decide which of those strategies or a mix of those most correctly",
    "start": "296150",
    "end": "302669"
  },
  {
    "text": "represents your application data and then these shards are the real workhorse",
    "start": "302669",
    "end": "308909"
  },
  {
    "text": "that do the work for us and assured is always present there's at least one",
    "start": "308909",
    "end": "314039"
  },
  {
    "text": "shard in stream and you can add and remove shards from your stream to give",
    "start": "314039",
    "end": "319050"
  },
  {
    "text": "yourself more or less capacity as required each shard will run at one mega",
    "start": "319050",
    "end": "324479"
  },
  {
    "text": "second or 1,000 rights per second and then you can read the data back out of",
    "start": "324479",
    "end": "331110"
  },
  {
    "text": "that shard at twice that speed two thousand reads a second or two Meg and you can add shards to give yourself",
    "start": "331110",
    "end": "338699"
  },
  {
    "text": "whatever throughput you require and we have customers running at many terabytes per hour by adding lots of shards to",
    "start": "338699",
    "end": "345870"
  },
  {
    "text": "their stream the shards store that data in a highly durable fashion across",
    "start": "345870",
    "end": "352349"
  },
  {
    "text": "multiple availability zones in a region and so a put into your stream is as",
    "start": "352349",
    "end": "360060"
  },
  {
    "text": "durable as putting it into for example DynamoDB and this can enable some really",
    "start": "360060",
    "end": "365699"
  },
  {
    "text": "powerful new application use cases for example where previously you've managed",
    "start": "365699",
    "end": "372240"
  },
  {
    "text": "files on your application instances and needed to worry about disk space and so",
    "start": "372240",
    "end": "379560"
  },
  {
    "text": "on we can now stream through those files in real time on the applications and",
    "start": "379560",
    "end": "385080"
  },
  {
    "text": "move every log line into Kinesis that single event is now multi AZ durable we",
    "start": "385080",
    "end": "392490"
  },
  {
    "text": "can throw the file away because we can reconstitute it on the back end when you",
    "start": "392490",
    "end": "399419"
  },
  {
    "text": "write data into Kinesis you supply what's called a partition key partition",
    "start": "399419",
    "end": "404459"
  },
  {
    "text": "key is defined by you and it allows us to root your data to a",
    "start": "404459",
    "end": "409949"
  },
  {
    "text": "short conesus offers you a very important characteristic of a stream",
    "start": "409949",
    "end": "415979"
  },
  {
    "text": "that not many other platforms can offer and that is perfectly ordered delivery",
    "start": "415979",
    "end": "421469"
  },
  {
    "text": "of data to your consumers in the order that they were put in by the producers",
    "start": "421469",
    "end": "427349"
  },
  {
    "text": "on the basis of the partition key so what would a partition keep be well it's",
    "start": "427349",
    "end": "433169"
  },
  {
    "text": "usually very obvious from your application what kind of partition key you would use it might be the session on",
    "start": "433169",
    "end": "439649"
  },
  {
    "text": "a web application or it might be the user ID and the contract that you get",
    "start": "439649",
    "end": "446519"
  },
  {
    "text": "from using Kinesis is that you will be able to consume the data in exactly the order that you put it on at any scale",
    "start": "446519",
    "end": "454259"
  },
  {
    "text": "across multiple availability zones and that gives us some really powerful primitives for building things like",
    "start": "454259",
    "end": "459829"
  },
  {
    "text": "behavioral analytics where we want to be able to track what happened over a sequence of time you can have as many",
    "start": "459829",
    "end": "467610"
  },
  {
    "text": "partition keys as you want and they generally follow the same best practices as you would find for a redshift",
    "start": "467610",
    "end": "473519"
  },
  {
    "text": "distribution key or a hash key and DynamoDB we want a large number of",
    "start": "473519",
    "end": "478589"
  },
  {
    "text": "discrete values and we want them to be uniformly distributed over the range of all possible input values and then",
    "start": "478589",
    "end": "487860"
  },
  {
    "text": "lastly there's a concept of a sequence number when you push data into Kinesis",
    "start": "487860",
    "end": "494490"
  },
  {
    "text": "it will give you back a sequence number that allows you to uniquely identify that record on the stream and allows you",
    "start": "494490",
    "end": "503610"
  },
  {
    "text": "to agree the effective conesus identity for a record between the producer and a",
    "start": "503610",
    "end": "510059"
  },
  {
    "text": "consumer the sequence number is guaranteed to be unique and again can be",
    "start": "510059",
    "end": "516479"
  },
  {
    "text": "used at any scale for things like deduplication which we're going to talk about so let's talk about getting data",
    "start": "516479",
    "end": "523258"
  },
  {
    "text": "into Kinesis so you have the ability to push data from a variety of different",
    "start": "523259",
    "end": "530370"
  },
  {
    "text": "sources and we call these producers producers use a put or a put records",
    "start": "530370",
    "end": "537449"
  },
  {
    "text": "which is a batch interface to push day into a stream The Shard is automatically",
    "start": "537449",
    "end": "543059"
  },
  {
    "text": "selected and we give you back a sequence number for that right the partition key",
    "start": "543059",
    "end": "548970"
  },
  {
    "text": "is analyzed in the call and used to route the request to the correct partition key if you don't care about",
    "start": "548970",
    "end": "556529"
  },
  {
    "text": "the order that the data comes back off of Kinesis then you can just use a big random number easy enough so when we're",
    "start": "556529",
    "end": "567389"
  },
  {
    "text": "pushing this data in we can have as many producers as we require and we can",
    "start": "567389",
    "end": "572429"
  },
  {
    "text": "support whatever ingest rate is required by adding shards so it is entirely possible to build a mobile application",
    "start": "572429",
    "end": "579779"
  },
  {
    "text": "that uses the AWS sdk to push data directly off the mobile into Kinesis and",
    "start": "579779",
    "end": "586499"
  },
  {
    "text": "we're very happy to take millions of producers there's no limit on how many producers we can have pushing data in",
    "start": "586499",
    "end": "594919"
  },
  {
    "text": "apologies for that",
    "start": "595909",
    "end": "599119"
  },
  {
    "text": "and the other thing I want to highlight is the fact that an individual event can now be up to one megabyte in size up",
    "start": "601740",
    "end": "610260"
  },
  {
    "text": "until about two weeks ago they were 50k but it's one of the things we really heard from customers that they needed a",
    "start": "610260",
    "end": "616530"
  },
  {
    "text": "much larger event we're going to talk about how to use that additional capacity a little bit later we also",
    "start": "616530",
    "end": "624990"
  },
  {
    "text": "added something new which is the Kinesis producer library the producer library is",
    "start": "624990",
    "end": "630390"
  },
  {
    "text": "a brand new module that gives you a way of asynchronously producing data for",
    "start": "630390",
    "end": "637350"
  },
  {
    "text": "Kinesis so that you can run at virtually any scale within your application and",
    "start": "637350",
    "end": "642600"
  },
  {
    "text": "the right latency to Kinesis will not affect your application performance it's",
    "start": "642600",
    "end": "648660"
  },
  {
    "text": "designed to run as an async demon that sits alongside your producer code base",
    "start": "648660",
    "end": "654410"
  },
  {
    "text": "and certainly is intended for the application server sort of producer and",
    "start": "654410",
    "end": "660080"
  },
  {
    "text": "one of the features that the kpl offers you is a feature called aggregation",
    "start": "660080",
    "end": "666710"
  },
  {
    "start": "665000",
    "end": "722000"
  },
  {
    "text": "aggregation allows us to take better advantage of this one megabyte payload size by automatically concatenated",
    "start": "666710",
    "end": "674790"
  },
  {
    "text": "together messages onto a stream and we're going to see how this can be used",
    "start": "674790",
    "end": "681090"
  },
  {
    "text": "to get a massive cost savings the data itself is protocol buffers when you use",
    "start": "681090",
    "end": "688490"
  },
  {
    "text": "Kinesis producer library aggregation and we effectively buffer all of that data",
    "start": "688490",
    "end": "693960"
  },
  {
    "text": "in memory and then concatenate it on to a much more compact stream so we can",
    "start": "693960",
    "end": "699060"
  },
  {
    "text": "take better use of the one megabyte events the other thing that the kpl does",
    "start": "699060",
    "end": "704850"
  },
  {
    "text": "for you is it allows you to write to multiple streams concurrently so you can",
    "start": "704850",
    "end": "710910"
  },
  {
    "text": "have a single event being fanned out and you don't actually need to worry about individual streams you simply indicate",
    "start": "710910",
    "end": "717630"
  },
  {
    "text": "the stream name when you do a right and the kpl sorts out all rooting for you",
    "start": "717630",
    "end": "722690"
  },
  {
    "start": "722000",
    "end": "770000"
  },
  {
    "text": "other ways people get data in is what you would absolutely expect Apache flume",
    "start": "722690",
    "end": "728760"
  },
  {
    "text": "for working with log files you can create a Kinesis channel and the data will be automatically pushed to kenise",
    "start": "728760",
    "end": "735230"
  },
  {
    "text": "is you can use fluent d for the same use case and then of course the most",
    "start": "735230",
    "end": "742810"
  },
  {
    "text": "commonly used logging frameworks of log4j Apache Commons logging and log for",
    "start": "742810",
    "end": "748670"
  },
  {
    "text": "net all have a Pender's that are available this is a really nice way of taking an existing application that's",
    "start": "748670",
    "end": "755270"
  },
  {
    "text": "generating log events that you need to analyze and simply being able to move away from files and move to streams the",
    "start": "755270",
    "end": "763700"
  },
  {
    "text": "log management framework itself is doing all of the hard work of pushing the data onto Kinesis and then we can consume",
    "start": "763700",
    "end": "771380"
  },
  {
    "start": "770000",
    "end": "854000"
  },
  {
    "text": "that and reconstitute the files at the stage of getting the data back out at",
    "start": "771380",
    "end": "778430"
  },
  {
    "text": "the core of consumer applications we have the Kinesis client library again",
    "start": "778430",
    "end": "783950"
  },
  {
    "text": "like the kpl it's a library that you will include in your code and has a",
    "start": "783950",
    "end": "789020"
  },
  {
    "text": "variety of different features for different customers requirements the first version we did was in Java and",
    "start": "789020",
    "end": "795910"
  },
  {
    "text": "then we quickly added support for no Jas for Ruby and then we did the multilang",
    "start": "795910",
    "end": "804250"
  },
  {
    "text": "KCl and the idea the multi-language environment is that we can never keep up",
    "start": "804250",
    "end": "809900"
  },
  {
    "text": "with all the possible languages that our customers want to use so instead let's make working with Kinesis streamed data",
    "start": "809900",
    "end": "817720"
  },
  {
    "text": "extremely pluggable so you can run the multi-language demon and it has specific",
    "start": "817720",
    "end": "823940"
  },
  {
    "text": "bindings and examples for Python but you can actually plug in anything you want",
    "start": "823940",
    "end": "829540"
  },
  {
    "text": "you could use bash you could use shell commands of any type to process your",
    "start": "829540",
    "end": "836060"
  },
  {
    "text": "data you could use cobalt if you want and the KCl gives us some really",
    "start": "836060",
    "end": "842720"
  },
  {
    "text": "important capabilities to consume the data in that it does automatic state",
    "start": "842720",
    "end": "849320"
  },
  {
    "text": "management and tracking of where our application has gotten to in the stream",
    "start": "849320",
    "end": "855910"
  },
  {
    "text": "what you'll get is a very simplified model for working with streaming data in",
    "start": "855910",
    "end": "862040"
  },
  {
    "text": "that you build your application in such a way that you're completely abstracted",
    "start": "862040",
    "end": "868730"
  },
  {
    "text": "from thing the distributed nature of Kinesis instead with the KCl a worker thread is",
    "start": "868730",
    "end": "877429"
  },
  {
    "text": "created for each of the shards that you're consuming data from and that thread conforms to an interface that",
    "start": "877429",
    "end": "884629"
  },
  {
    "text": "gets pushed a block of data every time there's something to consume from the",
    "start": "884629",
    "end": "890480"
  },
  {
    "text": "stream so when we build applications with kcl we're actually having all the",
    "start": "890480",
    "end": "896990"
  },
  {
    "text": "data push to us and all we have to do is implement the logic to process that data",
    "start": "896990",
    "end": "902449"
  },
  {
    "text": "and then say yep I'm happy with that checkpoint and the application will move",
    "start": "902449",
    "end": "908179"
  },
  {
    "text": "forward it also takes all the responsibility for dealing with the elastic nature of Kinesis by adding and",
    "start": "908179",
    "end": "915470"
  },
  {
    "text": "removing these worker threads every time the topology of our stream changes every",
    "start": "915470",
    "end": "920749"
  },
  {
    "text": "time we add new shards or remove shards the KCl will take responsibility for",
    "start": "920749",
    "end": "926449"
  },
  {
    "text": "making sure that an application executor is working against that data and then",
    "start": "926449",
    "end": "932689"
  },
  {
    "text": "there's a simple semantics checkpoint allows your application to say I'm happy with where I've gotten to now let me get",
    "start": "932689",
    "end": "940730"
  },
  {
    "text": "some more data if you don't check point you'll be given the same block of data back again and so you can very easily",
    "start": "940730",
    "end": "946160"
  },
  {
    "text": "retry operations that might have failed the other thing the KCl does for us is it babysits our application it will",
    "start": "946160",
    "end": "953540"
  },
  {
    "text": "restart workers if they fail for some reason and so when we build applications",
    "start": "953540",
    "end": "958850"
  },
  {
    "text": "with kcl we really get this very very simple way of interacting with a huge",
    "start": "958850",
    "end": "964519"
  },
  {
    "text": "distributed system and where we don't have to solve any distributed system problems ourselves so we built a lot of",
    "start": "964519",
    "end": "972170"
  },
  {
    "start": "969000",
    "end": "1039000"
  },
  {
    "text": "nice things with that KCl that we can share with you like conesus connectors",
    "start": "972170",
    "end": "977860"
  },
  {
    "text": "these are components which are given to you on github open source code and for",
    "start": "977860",
    "end": "985220"
  },
  {
    "text": "instance you have an automatic archiver to s3 this will just store your stream",
    "start": "985220",
    "end": "991040"
  },
  {
    "text": "for posterity in s3 and it's extremely good pattern to store your data in its",
    "start": "991040",
    "end": "998149"
  },
  {
    "text": "most raw event source format that you can",
    "start": "998149",
    "end": "1003449"
  },
  {
    "text": "you can then use the redshift connector to subsequently load data directly into",
    "start": "1003680",
    "end": "1009330"
  },
  {
    "text": "redshift it's a simple configuration where you describe the format of your",
    "start": "1009330",
    "end": "1015750"
  },
  {
    "text": "data whether it's JSON data or CSV and then the connector will just automatically load that data after it's",
    "start": "1015750",
    "end": "1021750"
  },
  {
    "text": "been made to persist into s3 there's also a batch appender for DynamoDB and",
    "start": "1021750",
    "end": "1027319"
  },
  {
    "text": "an automatic indexing component for elastic search and these can all run at",
    "start": "1027320",
    "end": "1034530"
  },
  {
    "text": "exactly the same time on a single stream you just have to spin them up and run them and you can extend them to build",
    "start": "1034530",
    "end": "1041670"
  },
  {
    "start": "1039000",
    "end": "1098000"
  },
  {
    "text": "complex transformation pipelines the core of the connectors application is at",
    "start": "1041670",
    "end": "1049320"
  },
  {
    "text": "first the transformer a transformer allows you to go from the format of the",
    "start": "1049320",
    "end": "1055380"
  },
  {
    "text": "data that sits on the stream to some other format that you want to omit you",
    "start": "1055380",
    "end": "1060900"
  },
  {
    "text": "then have filtration capability so you can say I'm only interested in events of a certain type or you can do some sort",
    "start": "1060900",
    "end": "1068790"
  },
  {
    "text": "of cleansing that says actually we think these aren't records that we want to keep you then have a buffer the buffer",
    "start": "1068790",
    "end": "1075030"
  },
  {
    "text": "could be memory-based it could be disk-based you could buffer into a database and then lastly you have an",
    "start": "1075030",
    "end": "1081450"
  },
  {
    "text": "emitter and an emitter will write the data to s3 it will subsequently load",
    "start": "1081450",
    "end": "1086730"
  },
  {
    "text": "into redshift and there are a lot of interesting implementations of emitter zout in the wild for talking to elastic",
    "start": "1086730",
    "end": "1094800"
  },
  {
    "text": "hash or MongoDB you name it we can also consume data with storm we supply you",
    "start": "1094800",
    "end": "1103800"
  },
  {
    "start": "1098000",
    "end": "1112000"
  },
  {
    "text": "with a spout for Kinesis so you can push data into your storm topologies from",
    "start": "1103800",
    "end": "1110250"
  },
  {
    "text": "Kinesis and of course apache spark",
    "start": "1110250",
    "end": "1116450"
  },
  {
    "start": "1112000",
    "end": "1170000"
  },
  {
    "text": "there's native support within spark streaming for Kinesis as a data source you create a d stream dynamic stream for",
    "start": "1116450",
    "end": "1126210"
  },
  {
    "text": "each shard and then the spark receiver component will automatically commit",
    "start": "1126210",
    "end": "1131310"
  },
  {
    "text": "records that have been persisted into an RDD and processed by your application and this is a great",
    "start": "1131310",
    "end": "1138419"
  },
  {
    "text": "opportunity for me to shamelessly plug the fact that we just launched support for spark natively within elastic",
    "start": "1138419",
    "end": "1145349"
  },
  {
    "text": "mapreduce so it's never been easier to run spark on AWS you also get some",
    "start": "1145349",
    "end": "1153179"
  },
  {
    "text": "really important features of elastic mapreduce that you weren't able to leverage very easily before for example",
    "start": "1153179",
    "end": "1159119"
  },
  {
    "text": "the EMR file system which is an overlay on top of HDFS that gives you massively",
    "start": "1159119",
    "end": "1164729"
  },
  {
    "text": "improved performance for your spark jobs when they're working with s3 based data and of course we have AWS lambda which",
    "start": "1164729",
    "end": "1175499"
  },
  {
    "start": "1170000",
    "end": "1298000"
  },
  {
    "text": "is able to act as a receiver of data from Kinesis it's one of the native",
    "start": "1175499",
    "end": "1181889"
  },
  {
    "text": "event sources that conesus supports we have a stateless JavaScript or java",
    "start": "1181889",
    "end": "1188340"
  },
  {
    "text": "based piece of logic that you compile with all of its dependencies and then",
    "start": "1188340",
    "end": "1194909"
  },
  {
    "text": "deploy with an AWS lambda you create the Kinesis stream is the event source for",
    "start": "1194909",
    "end": "1201149"
  },
  {
    "text": "that function and then you just allow us to host that logic scale it up and down",
    "start": "1201149",
    "end": "1208289"
  },
  {
    "text": "elastically and only pay for the actual indications that you make so you're able",
    "start": "1208289",
    "end": "1213929"
  },
  {
    "text": "to have a completely managed stream infrastructure with Kinesis and a",
    "start": "1213929",
    "end": "1219389"
  },
  {
    "text": "completely managed and serverless compute infrastructure to receive that data the AWS sdk is built in to lambda",
    "start": "1219389",
    "end": "1226950"
  },
  {
    "text": "all you have to do is provide the logic how much RAM it requires and how long",
    "start": "1226950",
    "end": "1232200"
  },
  {
    "text": "you want to allow an individual function to run and then we automatically use a",
    "start": "1232200",
    "end": "1238559"
  },
  {
    "text": "KCl and embedded KCl within lambda to push data out of the shard and into your",
    "start": "1238559",
    "end": "1243960"
  },
  {
    "text": "lambda function there's also some really nice community libraries for writing",
    "start": "1243960",
    "end": "1250429"
  },
  {
    "text": "lambda functions in Python and then go and we do give you access to the",
    "start": "1250429",
    "end": "1256979"
  },
  {
    "text": "underlying file system that sits within the lambda container so that you can do interesting buffering and so on and of",
    "start": "1256979",
    "end": "1264419"
  },
  {
    "text": "course lambda functions can call each other so you can start to build very powerful event-driven architecture",
    "start": "1264419",
    "end": "1270020"
  },
  {
    "text": "with Kinesis as the event handling mechanism and lambda as the processor",
    "start": "1270020",
    "end": "1275760"
  },
  {
    "text": "for that data lambda functions might be creating new Kinesis streams and so your",
    "start": "1275760",
    "end": "1281580"
  },
  {
    "text": "entire application can be built with Kinesis for streaming lambda for compute",
    "start": "1281580",
    "end": "1288900"
  },
  {
    "text": "and probably dynamodb for long-term storage and that would be a completely",
    "start": "1288900",
    "end": "1294060"
  },
  {
    "text": "serverless architecture which is extremely powerful so why would you use",
    "start": "1294060",
    "end": "1299490"
  },
  {
    "text": "Kinesis versus doing this yourself well let's keep in mind that durability of",
    "start": "1299490",
    "end": "1306230"
  },
  {
    "text": "these events is of paramount importance these are what happened on your",
    "start": "1306230",
    "end": "1312480"
  },
  {
    "text": "application and so they represent the most important data source that you can use to understand your application",
    "start": "1312480",
    "end": "1319460"
  },
  {
    "text": "conesus writes every single event synchronously to multiple availability zones and you don't have to do any ops",
    "start": "1319460",
    "end": "1327630"
  },
  {
    "text": "you just provision the stream and get on with the business logic if we can trust",
    "start": "1327630",
    "end": "1335100"
  },
  {
    "text": "that with building this yourself you're going to have to consider whether the application is staging that event data",
    "start": "1335100",
    "end": "1342540"
  },
  {
    "text": "only in memory how often it sinks to disk you're going to have to configure replication yourself and ultimately your",
    "start": "1342540",
    "end": "1350070"
  },
  {
    "text": "recovery point is whatever the replication lag is between two separate machines which is a very different",
    "start": "1350070",
    "end": "1357030"
  },
  {
    "text": "prospect for durability than the out-of-the-box to delivery to multiple",
    "start": "1357030",
    "end": "1362220"
  },
  {
    "text": "az's with no operations also from a performance perspective when we launched",
    "start": "1362220",
    "end": "1369030"
  },
  {
    "text": "Kinesis it had what we call TP 99 or 99.9 percent of events TP 99 latency of",
    "start": "1369030",
    "end": "1377910"
  },
  {
    "text": "between three to four seconds we've now shrunk that down to below one second and",
    "start": "1377910",
    "end": "1384510"
  },
  {
    "text": "TP 50 is around 50 milliseconds so you're able to build systems that have",
    "start": "1384510",
    "end": "1389940"
  },
  {
    "text": "low latency requirements off the back of a infrastructure that will scale to virtually any size if you do this",
    "start": "1389940",
    "end": "1399240"
  },
  {
    "text": "yourself things like the scale and the latency is going to be based upon CPU it's going to",
    "start": "1399240",
    "end": "1406350"
  },
  {
    "text": "be based upon storage and you also have",
    "start": "1406350",
    "end": "1411570"
  },
  {
    "text": "to consider what happens if your cluster is interrupted for some reason whereas",
    "start": "1411570",
    "end": "1417450"
  },
  {
    "text": "because Kinesis is a regional service we don't need to worry about the interruption and in fact we can lose a",
    "start": "1417450",
    "end": "1423030"
  },
  {
    "text": "Z's within Kinesis and you'll never know that it happened from an availability",
    "start": "1423030",
    "end": "1429150"
  },
  {
    "text": "perspective you also have to consider how do you build a topology that will",
    "start": "1429150",
    "end": "1435960"
  },
  {
    "text": "give us the availability and durability characteristics that you require for this essential data many systems that",
    "start": "1435960",
    "end": "1443760"
  },
  {
    "text": "Moran multiple brokers actually have a database underneath them that requires",
    "start": "1443760",
    "end": "1449880"
  },
  {
    "text": "quorum in order to be up so if you build that on multiple availability zones if",
    "start": "1449880",
    "end": "1455100"
  },
  {
    "text": "they can't talk to each other for a short period of time you may not be able to write to this infrastructure and now",
    "start": "1455100",
    "end": "1461310"
  },
  {
    "text": "we've got millions of producers who are unable to push their data end just really really not what we want your",
    "start": "1461310",
    "end": "1469650"
  },
  {
    "text": "availability then becomes either only as good as a single availability zone or as",
    "start": "1469650",
    "end": "1475260"
  },
  {
    "text": "good as the interconnect between them whereas with Kinesis because we're always implementing three separate",
    "start": "1475260",
    "end": "1481320"
  },
  {
    "text": "storage facilities we can survive an AZ lost without any interruption of service",
    "start": "1481320",
    "end": "1487910"
  },
  {
    "text": "also from an ops perspective you have to consider the fact that with Kinesis you",
    "start": "1487910",
    "end": "1493380"
  },
  {
    "text": "provision the stream and if you want to add and remove capacity it's a single request whereas when you roll this",
    "start": "1493380",
    "end": "1499980"
  },
  {
    "text": "yourself you have to build the instance install the software and configure it you have to think about managing disk",
    "start": "1499980",
    "end": "1506340"
  },
  {
    "text": "space and compaction you have to manage the replication yourself and it's a very",
    "start": "1506340",
    "end": "1511350"
  },
  {
    "text": "big job the other thing that we tend to",
    "start": "1511350",
    "end": "1516600"
  },
  {
    "text": "see is that when you build these yourself if you want to scale up and down beyond the capacity that you",
    "start": "1516600",
    "end": "1522720"
  },
  {
    "text": "planned upfront you often have to migrate to a totally new infrastructure that increases for example the partition",
    "start": "1522720",
    "end": "1529080"
  },
  {
    "text": "count and not going to be very onerous on your application it can mean new application",
    "start": "1529080",
    "end": "1534150"
  },
  {
    "text": "installations completely and we can really deal with any type of elastic",
    "start": "1534150",
    "end": "1541070"
  },
  {
    "text": "response of our applications with Kinesis scaling up and down dynamically",
    "start": "1541070",
    "end": "1547190"
  },
  {
    "text": "based upon what you're pushing in we can maintain a fleet that's very very small",
    "start": "1547190",
    "end": "1552840"
  },
  {
    "text": "during quiet times and then scale up in the day and keep in mind when we use the KCl to process that data because we're",
    "start": "1552840",
    "end": "1561660"
  },
  {
    "text": "automatically getting a thread provisioned for each shard our",
    "start": "1561660",
    "end": "1566850"
  },
  {
    "text": "application fleet on the back end is going to scale up and down accordingly",
    "start": "1566850",
    "end": "1572690"
  },
  {
    "text": "when you build this yourself you do have to do quite a lot of planning up front and you have to choose a partition count",
    "start": "1572690",
    "end": "1580800"
  },
  {
    "text": "that's going to allow you to meet your long term input rates which can be",
    "start": "1580800",
    "end": "1588420"
  },
  {
    "text": "complex and result in you over-provisioning with Kinesis when we",
    "start": "1588420",
    "end": "1594330"
  },
  {
    "start": "1591000",
    "end": "1639000"
  },
  {
    "text": "want to scale we can either do that manually or we can run an elastic",
    "start": "1594330",
    "end": "1599790"
  },
  {
    "text": "Beanstalk based auto scaling demon which is cited at the bottom there and the way",
    "start": "1599790",
    "end": "1606390"
  },
  {
    "text": "that Kinesis works is that you split and merge shards together and a shard is",
    "start": "1606390",
    "end": "1612150"
  },
  {
    "text": "always available until it hits the 24-hour retention period so in this case",
    "start": "1612150",
    "end": "1618420"
  },
  {
    "text": "if we took shards 1 and 2 and we wanted to create additional capacity we would",
    "start": "1618420",
    "end": "1624120"
  },
  {
    "text": "split shard to in half and that would give us double the capacity or a thousand writes per second on each and",
    "start": "1624120",
    "end": "1630150"
  },
  {
    "text": "then when we want to go back to two shards we would merge shards 2 and 3 back together and that's something that",
    "start": "1630150",
    "end": "1637380"
  },
  {
    "text": "can be done automatically the last point about Kinesis that makes a big",
    "start": "1637380",
    "end": "1642810"
  },
  {
    "start": "1639000",
    "end": "1700000"
  },
  {
    "text": "difference is the cost kinesis it's designed to be extremely cost-effective",
    "start": "1642810",
    "end": "1648500"
  },
  {
    "text": "so that you can focus on the consumer applications that you're writing and not",
    "start": "1648500",
    "end": "1653700"
  },
  {
    "text": "on the infrastructure used to move data around you can start as small as a single shard which gives you a thousand",
    "start": "1653700",
    "end": "1660180"
  },
  {
    "text": "TPS and then add and remove and shards cost you one and a half cents per hour",
    "start": "1660180",
    "end": "1666870"
  },
  {
    "text": "if we compare that to something that you're going to have to do yourself you're going to build ec2 instances you",
    "start": "1666870",
    "end": "1672130"
  },
  {
    "text": "need to size those they probably need quite a lot of RAM we're going to run a multi AZ configuration so we can't just",
    "start": "1672130",
    "end": "1678190"
  },
  {
    "text": "run one broker we need multiple brokers we're going to use auto scaling and then",
    "start": "1678190",
    "end": "1683260"
  },
  {
    "text": "you also need to invest the time to instrument where your consumer applications are in the stream using",
    "start": "1683260",
    "end": "1688990"
  },
  {
    "text": "maybe something like cloud watch in order to understand how your application",
    "start": "1688990",
    "end": "1694570"
  },
  {
    "text": "is performing and we get all of that for free with Kinesis so it's just work you don't need to do also really pleased",
    "start": "1694570",
    "end": "1702360"
  },
  {
    "start": "1700000",
    "end": "1782000"
  },
  {
    "text": "along with the one megabyte size increase we also did a price cup we drop",
    "start": "1702360",
    "end": "1711730"
  },
  {
    "text": "the price on the second of june so this is fairly new news and specifically we restructured the pricing model to be",
    "start": "1711730",
    "end": "1719110"
  },
  {
    "text": "able to support the kpl better it used to be that we charge you two point eight",
    "start": "1719110",
    "end": "1724150"
  },
  {
    "text": "cents for every million records you pushed on to Kinesis but now when you're",
    "start": "1724150",
    "end": "1729520"
  },
  {
    "text": "running the kpl with the aggregation feature we're going to make much better use of that one meg of data and so we've",
    "start": "1729520",
    "end": "1737500"
  },
  {
    "text": "changed the pricing models now be what we call payload units or 25k blocks of",
    "start": "1737500",
    "end": "1743170"
  },
  {
    "text": "data and we drop the price in half to one point four cents for every million",
    "start": "1743170",
    "end": "1750120"
  },
  {
    "text": "payload units payload units are admittedly a little bit difficult to imagine so I've given you an example of",
    "start": "1750120",
    "end": "1757260"
  },
  {
    "text": "50,000 TPS running at 50,000 TPS for an entire month the old price would have",
    "start": "1757260",
    "end": "1763750"
  },
  {
    "text": "been six hundred and seventy-eight dollars and now with kpl based",
    "start": "1763750",
    "end": "1769210"
  },
  {
    "text": "aggregation on a 512 byte event it will cost us only three hundred dollars so",
    "start": "1769210",
    "end": "1775660"
  },
  {
    "text": "about a fifty percent price reduction for an infrastructure that will scale up and down as needed now there are a few",
    "start": "1775660",
    "end": "1785410"
  },
  {
    "start": "1782000",
    "end": "1825000"
  },
  {
    "text": "application best practices that I wanted to talk about keeping in mind the mantra of dr. Vogel's or CTO which is",
    "start": "1785410",
    "end": "1793810"
  },
  {
    "text": "everything fails all the time when you see services like DynamoDB s",
    "start": "1793810",
    "end": "1799510"
  },
  {
    "text": "three kinesis we have built them with this mantra in mind when you operate a",
    "start": "1799510",
    "end": "1805240"
  },
  {
    "text": "system at the scale of something like Kinesis there are machines failing all the time the storage infrastructure is",
    "start": "1805240",
    "end": "1812620"
  },
  {
    "text": "failing all the time and we have to be prepared to operate in that sort of environment so how can we help you build",
    "start": "1812620",
    "end": "1819970"
  },
  {
    "text": "systems on Kinesis that will allow you to work with this constraint that",
    "start": "1819970",
    "end": "1826030"
  },
  {
    "start": "1825000",
    "end": "1965000"
  },
  {
    "text": "everything fails all the time and the first one is that we do really need to tolerate the failure of components",
    "start": "1826030",
    "end": "1832630"
  },
  {
    "text": "within our consumer applications this could be because somebody puts on a bad event it's not formatted properly and",
    "start": "1832630",
    "end": "1840630"
  },
  {
    "text": "your logic just wasn't set up to deal with that and so it crashes there should be no human task involved in restarting",
    "start": "1840630",
    "end": "1847960"
  },
  {
    "text": "that process but it also might be that we add and remove threads just because",
    "start": "1847960",
    "end": "1854980"
  },
  {
    "text": "we're scaling up and down and so when we",
    "start": "1854980",
    "end": "1860110"
  },
  {
    "text": "use auto scaling for our consumer applications or we use lambda we're going to have a very dynamic environment",
    "start": "1860110",
    "end": "1866830"
  },
  {
    "text": "where we need to be able to tolerate individual failures so one of the most",
    "start": "1866830",
    "end": "1875620"
  },
  {
    "text": "important mantras which I'm going to decompose a little bit later is the fact that we really don't recommend that you",
    "start": "1875620",
    "end": "1880960"
  },
  {
    "text": "store data locally your workers need to be seen as transient stateless assets",
    "start": "1880960",
    "end": "1888330"
  },
  {
    "text": "that perform a function and can be restarted at any time also absolutely",
    "start": "1888330",
    "end": "1896860"
  },
  {
    "text": "recommend that you take advantage of platforms like lambda or elastic Beanstalk to host your consumer",
    "start": "1896860",
    "end": "1903790"
  },
  {
    "text": "applications why elastic Beanstalk will it gives us multi availability zones for",
    "start": "1903790",
    "end": "1908800"
  },
  {
    "text": "free it allows us to scale up and down very easily and it allows us to manage",
    "start": "1908800",
    "end": "1913990"
  },
  {
    "text": "application versions with virtually no effort we simply upload a new version of",
    "start": "1913990",
    "end": "1919360"
  },
  {
    "text": "our consumer logic we make it live and Beanstalk handles rolling out that code",
    "start": "1919360",
    "end": "1924820"
  },
  {
    "text": "base on to our consumer fleet in kc l is",
    "start": "1924820",
    "end": "1930040"
  },
  {
    "text": "very nicely able to deal with that sort auto by allowing a worker to be killed",
    "start": "1930040",
    "end": "1936230"
  },
  {
    "text": "and then restarted on the old code on the new code base that you've just installed and lastly we have to keep in",
    "start": "1936230",
    "end": "1946009"
  },
  {
    "text": "mind that the best sort of infrastructure is the one that you don't have to manage at all so taking",
    "start": "1946009",
    "end": "1951230"
  },
  {
    "text": "advantage of lambda means there are no servers for you to ever have to worry about restarting or scaling up and down",
    "start": "1951230",
    "end": "1957740"
  },
  {
    "text": "we simply run the required number of lambda containers to meet your event",
    "start": "1957740",
    "end": "1963440"
  },
  {
    "text": "volume so on the point of application stayed a very common pattern that we see",
    "start": "1963440",
    "end": "1969909"
  },
  {
    "start": "1965000",
    "end": "2015000"
  },
  {
    "text": "customers struggling with is that they will bind a consumer to a fixed number",
    "start": "1969909",
    "end": "1976340"
  },
  {
    "text": "of partitions and they will use that fixed mapping to do things like",
    "start": "1976340",
    "end": "1982519"
  },
  {
    "text": "maintaining a database on a note and then they'll create a read API that's",
    "start": "1982519",
    "end": "1988129"
  },
  {
    "text": "able to go and require that application by looking at the local storage of those",
    "start": "1988129",
    "end": "1993679"
  },
  {
    "text": "systems and and federating the request and unfortunately everything fails all",
    "start": "1993679",
    "end": "1998960"
  },
  {
    "text": "the time so we get into a situation where one of these has fallen over and we have to consider how we recover that",
    "start": "1998960",
    "end": "2006340"
  },
  {
    "text": "event data in order for our consumers to carry on in order to deal with",
    "start": "2006340",
    "end": "2013590"
  },
  {
    "text": "accommodating that read load so what we do is we replace that instance using",
    "start": "2013590",
    "end": "2019330"
  },
  {
    "text": "something like auto scaling and then in order to rebuild that application state we have to read from the beginning of",
    "start": "2019330",
    "end": "2025269"
  },
  {
    "text": "the stream for all time on those partitions that we were responsible for and this model is extremely difficult to",
    "start": "2025269",
    "end": "2034450"
  },
  {
    "text": "work with and actually results in quite a long startup time and means that you have to write all of the orchestration",
    "start": "2034450",
    "end": "2041769"
  },
  {
    "text": "to make sure that this recovery works well instead we need to externalize the",
    "start": "2041769",
    "end": "2051339"
  },
  {
    "start": "2048000",
    "end": "2067000"
  },
  {
    "text": "state for our consumer applications so that they can be completely transient so",
    "start": "2051339",
    "end": "2058270"
  },
  {
    "text": "for example if we can use tool like dynamo DB which is also regional highly",
    "start": "2058270",
    "end": "2064540"
  },
  {
    "text": "available highly durable service then we can blow away our consumers at",
    "start": "2064540",
    "end": "2070450"
  },
  {
    "start": "2067000",
    "end": "2079000"
  },
  {
    "text": "any time and our database is still viable there's no need to ever have to",
    "start": "2070450",
    "end": "2075490"
  },
  {
    "text": "replay a stream to recover that database and then lastly of course just to",
    "start": "2075490",
    "end": "2082060"
  },
  {
    "start": "2079000",
    "end": "2101000"
  },
  {
    "text": "reiterate the point if we use lambda there's nothing to fail at all we",
    "start": "2082060",
    "end": "2087580"
  },
  {
    "text": "guarantee that a lambda function will be invoked for a payload of data on Kinesis where that is may be anywhere within the",
    "start": "2087580",
    "end": "2095950"
  },
  {
    "text": "fleet of machines that makes up lambda the other pattern that's extremely",
    "start": "2095950",
    "end": "2103000"
  },
  {
    "start": "2101000",
    "end": "2134000"
  },
  {
    "text": "important to consider is the need for item potency an item potency is the",
    "start": "2103000",
    "end": "2108700"
  },
  {
    "text": "property of a system that allows us to reprocess the same input events and end up with the same end application state",
    "start": "2108700",
    "end": "2115870"
  },
  {
    "text": "and can also be summed up as saying that we get exactly once processing semantics",
    "start": "2115870",
    "end": "2124590"
  },
  {
    "text": "it doesn't matter if an event gets repeated we say yeah we've seen that so",
    "start": "2124590",
    "end": "2131050"
  },
  {
    "text": "what does this actually mean in practice and how does it relate to Kinesis well",
    "start": "2131050",
    "end": "2136390"
  },
  {
    "start": "2134000",
    "end": "2202000"
  },
  {
    "text": "because of the fact that Kinesis is a distributed system running across multiple availability zones it is",
    "start": "2136390",
    "end": "2144100"
  },
  {
    "text": "possible that in some certain conditions the client that you're using to push",
    "start": "2144100",
    "end": "2152020"
  },
  {
    "text": "data into Kinesis may retry an event for instance they correctly right to one",
    "start": "2152020",
    "end": "2159550"
  },
  {
    "text": "storage node they get any acknowledgement from another storage node but then they get a failure and the",
    "start": "2159550",
    "end": "2166300"
  },
  {
    "text": "client interprets that as a redirect and will retry that event and this is just",
    "start": "2166300",
    "end": "2175360"
  },
  {
    "text": "something that's built into our SDK we handle a lot of retry events automatically so that you don't have to",
    "start": "2175360",
    "end": "2181480"
  },
  {
    "text": "worry about them and Kinesis producers are no different and in this case what",
    "start": "2181480",
    "end": "2188500"
  },
  {
    "text": "we could end up with is two of the exact same event on Kinesis but with different",
    "start": "2188500",
    "end": "2194050"
  },
  {
    "text": "sequence numbers and that's difficult for customers to deal with and it's",
    "start": "2194050",
    "end": "2199780"
  },
  {
    "text": "something that absolutely must consider and we've heard that feedback loud and clear so we're",
    "start": "2199780",
    "end": "2205390"
  },
  {
    "start": "2202000",
    "end": "2207000"
  },
  {
    "text": "going to do something about it so in the coming months we're going to build something called producer managed item",
    "start": "2205390",
    "end": "2212140"
  },
  {
    "start": "2207000",
    "end": "2308000"
  },
  {
    "text": "potency the way this will work is that kinesis will maintain a sized window in",
    "start": "2212140",
    "end": "2220590"
  },
  {
    "text": "DynamoDB with which it will do automatic item potency checking if you so choose",
    "start": "2220590",
    "end": "2227470"
  },
  {
    "text": "to turn on the feature then if we do a",
    "start": "2227470",
    "end": "2233860"
  },
  {
    "text": "right from the producer and that record is made durable but for some reason we",
    "start": "2233860",
    "end": "2240970"
  },
  {
    "text": "were to end up sending back an error code to the client we now know the",
    "start": "2240970",
    "end": "2246700"
  },
  {
    "text": "request ID from your data that makes that unique the way that you interact",
    "start": "2246700",
    "end": "2254590"
  },
  {
    "text": "with this is you tell us you give us a class within the producer library that",
    "start": "2254590",
    "end": "2261130"
  },
  {
    "text": "tells us how to find the ID in your message that's all you have to do we",
    "start": "2261130",
    "end": "2266710"
  },
  {
    "text": "will do this automatically and then when the retry happens which absolutely needs",
    "start": "2266710",
    "end": "2272440"
  },
  {
    "text": "to continue happening we get this condition check failed exception from",
    "start": "2272440",
    "end": "2277660"
  },
  {
    "text": "DynamoDB we know that your record has now been made durable but we saw that",
    "start": "2277660",
    "end": "2285490"
  },
  {
    "text": "there is an error before and so we won't restore that message but we will acknowledge it with an okay which will",
    "start": "2285490",
    "end": "2294460"
  },
  {
    "text": "allow for much much more simple to manage item potently processed",
    "start": "2294460",
    "end": "2300400"
  },
  {
    "text": "applications where we're doing a lot of the hard work to de doop on the front end so to wrap up conesus really offers",
    "start": "2300400",
    "end": "2313600"
  },
  {
    "start": "2308000",
    "end": "2381000"
  },
  {
    "text": "us a system that is much easier to manage than having to build it yourself",
    "start": "2313600",
    "end": "2319170"
  },
  {
    "text": "we get extremely high performance up two terabytes per hour with low latency on",
    "start": "2319170",
    "end": "2325150"
  },
  {
    "text": "the order of 50 milliseconds to about 200 and crucially the durability of that",
    "start": "2325150",
    "end": "2333370"
  },
  {
    "text": "data is always multi AZ this data is your crown",
    "start": "2333370",
    "end": "2338559"
  },
  {
    "text": "jewels and so you need to protect it we get extremely elastic performance at a",
    "start": "2338559",
    "end": "2345339"
  },
  {
    "text": "very low cost so that you can focus on hosting the logic in the most",
    "start": "2345339",
    "end": "2350740"
  },
  {
    "text": "appropriate place for you and you can focus on building that logic rather than",
    "start": "2350740",
    "end": "2356019"
  },
  {
    "text": "paying for managing big hard and complex infrastructure and we can take advantage",
    "start": "2356019",
    "end": "2363279"
  },
  {
    "text": "of a really nice ecosystem of tools that allow us to do things like making the",
    "start": "2363279",
    "end": "2368769"
  },
  {
    "text": "stream 11 9s durable on s3 we can process it with spark and we can build",
    "start": "2368769",
    "end": "2375519"
  },
  {
    "text": "applications with the kpl and lambda so with that I'll say thanks for your",
    "start": "2375519",
    "end": "2382749"
  },
  {
    "start": "2381000",
    "end": "2399000"
  },
  {
    "text": "attention today enjoy building applications with Kinesis and lambda and we'll be around to answer any questions",
    "start": "2382749",
    "end": "2389740"
  },
  {
    "text": "that anybody has after the talk",
    "start": "2389740",
    "end": "2394440"
  }
]