[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "good afternoon and welcome I am tan Chima software development manager for",
    "start": "2000",
    "end": "8639"
  },
  {
    "text": "Amazon Cloud Drive Services team and with me is Ashish Mishra he's our senior",
    "start": "8639",
    "end": "14639"
  },
  {
    "text": "developer and uh today we will share with you how Amazon Cloud Drive Services",
    "start": "14639",
    "end": "19760"
  },
  {
    "text": "is built on top of AWS and how we are able to scale our",
    "start": "19760",
    "end": "25119"
  },
  {
    "text": "Serv services to millions of users hundreds of billions service requests",
    "start": "25119",
    "end": "30720"
  },
  {
    "text": "and terabytes of uploads and downloads at a very high frequencies and low",
    "start": "30720",
    "end": "38280"
  },
  {
    "text": "latencies to give you a quick overview of the topics we will be covering today we will give you an overview of Amazon",
    "start": "38280",
    "end": "46719"
  },
  {
    "start": "40000",
    "end": "147000"
  },
  {
    "text": "Cloud Drive we will share with you some of the key use cases and U some of the",
    "start": "46719",
    "end": "52199"
  },
  {
    "text": "challenges around enabling those use cases on the services side more",
    "start": "52199",
    "end": "57480"
  },
  {
    "text": "specifically and then um we share with you what were some of the key AWS building blocks which we leverage to",
    "start": "57480",
    "end": "65000"
  },
  {
    "text": "solve some of our big challenges and um and how we integrated",
    "start": "65000",
    "end": "71240"
  },
  {
    "text": "different AWS services to build our Amazon Cloud Drive Services",
    "start": "71240",
    "end": "77920"
  },
  {
    "text": "architecture and then Ashish will get deep into some of the key components like how did we build a very scalable",
    "start": "77920",
    "end": "85320"
  },
  {
    "text": "and robust asynchronous pipeline which uh enables our content and met metadata",
    "start": "85320",
    "end": "92119"
  },
  {
    "text": "inje and also we leverage different AWS data stores so how do we manage",
    "start": "92119",
    "end": "98560"
  },
  {
    "text": "consistency the data consistency across different AWS uh data Stores um and then",
    "start": "98560",
    "end": "105640"
  },
  {
    "text": "finally we will uh share with you uh Amazon Cloud Drive API this is a new uh",
    "start": "105640",
    "end": "112719"
  },
  {
    "text": "offering we just announced uh this week this enables all the developers to build cool applications on top of our services",
    "start": "112719",
    "end": "121840"
  },
  {
    "text": "so before we get started I just want to kind of get a sense um show off hands how many people here actually use Amazon",
    "start": "122520",
    "end": "129679"
  },
  {
    "text": "Cloud Drive few of few good and uh Amazon",
    "start": "129679",
    "end": "137160"
  },
  {
    "text": "Prime photos all right all all of you so I'll so let me give you a quick overview for",
    "start": "137160",
    "end": "145080"
  },
  {
    "text": "um all the other audience here so Amazon Cloud Drive is a place where you can",
    "start": "145080",
    "end": "151680"
  },
  {
    "start": "147000",
    "end": "273000"
  },
  {
    "text": "store any kind of content photos videos documents any kind of files and you can",
    "start": "151680",
    "end": "158120"
  },
  {
    "text": "access this content from anywhere on the web or you could use one of our",
    "start": "158120",
    "end": "163640"
  },
  {
    "text": "application we support different platforms we have applications for iOS",
    "start": "163640",
    "end": "168680"
  },
  {
    "text": "Android PCS fire OS if you have Kindles or Fire TV and then we are also uh",
    "start": "168680",
    "end": "176000"
  },
  {
    "text": "continuing to work to support other platforms uh the Mac application will be available in uh in",
    "start": "176000",
    "end": "183159"
  },
  {
    "text": "near future and uh if you want to going to learn more about uh Amazon Cloud",
    "start": "183159",
    "end": "188680"
  },
  {
    "text": "Drive uh you can go to amazon.com cloud drive let me quickly",
    "start": "188680",
    "end": "194000"
  },
  {
    "text": "show you what that looks like on the web so if you go to",
    "start": "194000",
    "end": "201840"
  },
  {
    "text": "amazon.com cloud drive it takes you to this page where you can uh uh",
    "start": "201840",
    "end": "208239"
  },
  {
    "text": "see let me bring the this here",
    "start": "208239",
    "end": "212400"
  },
  {
    "text": "quickly so so this is the page where you can go and you can uh see uh the journal",
    "start": "218080",
    "end": "224360"
  },
  {
    "text": "information about Amazon Cloud Drive one thing we recently announced was if you're a Amazon Prime member you get",
    "start": "224360",
    "end": "231599"
  },
  {
    "text": "unlimited photos and that's something you see right on the top and if you go down here uh you could find more",
    "start": "231599",
    "end": "238599"
  },
  {
    "text": "information including what are the different kind of plants we offer and um",
    "start": "238599",
    "end": "244760"
  },
  {
    "text": "further down you can find uh the application you might be interested in um like I said we have uh fire OS",
    "start": "244760",
    "end": "252599"
  },
  {
    "text": "Android and iOS uh and then also the PC application you",
    "start": "252599",
    "end": "259320"
  },
  {
    "text": "can download here and um so let me share with you what were",
    "start": "259320",
    "end": "268280"
  },
  {
    "text": "some of the key key use cases for Amazon Cloud Drive one of",
    "start": "268280",
    "end": "275560"
  },
  {
    "start": "273000",
    "end": "350000"
  },
  {
    "text": "the big thing we our services needed to enable was uh be able to scale and store",
    "start": "275560",
    "end": "283520"
  },
  {
    "text": "any kind of content so the content comes in a in a large variety and it comes in",
    "start": "283520",
    "end": "290080"
  },
  {
    "text": "a very high volume and obviously it comes in a in a very high velocity and",
    "start": "290080",
    "end": "296199"
  },
  {
    "text": "so the challenge here is how do you store this content in in such a way that it's durable and",
    "start": "296199",
    "end": "303880"
  },
  {
    "text": "it's available you know as quickly as you upload it and then we are able to",
    "start": "303880",
    "end": "308919"
  },
  {
    "text": "scale to millions of customers at at the velocity and volume at which",
    "start": "308919",
    "end": "314000"
  },
  {
    "text": "comes the the second one is related to the content when you get lot of variety",
    "start": "314000",
    "end": "320479"
  },
  {
    "text": "of content you get variety of Mera as well and the metadata with more variety",
    "start": "320479",
    "end": "325600"
  },
  {
    "text": "of content the metadata keeps growing and the challenge is what do you use used to store this metadata which",
    "start": "325600",
    "end": "331960"
  },
  {
    "text": "continues to grow and changes and and you should be able to provide different",
    "start": "331960",
    "end": "338000"
  },
  {
    "text": "ways to get to this metadata so that your clients and application can build some Innovative uh organization and",
    "start": "338000",
    "end": "345759"
  },
  {
    "text": "navigation features for your end customers another big challenge we had",
    "start": "345759",
    "end": "353479"
  },
  {
    "start": "350000",
    "end": "400000"
  },
  {
    "text": "was uh with the with the high volume velocity and variety of content we get",
    "start": "353479",
    "end": "359360"
  },
  {
    "text": "uh the content gets uploaded by say one device maybe you use your camera you use",
    "start": "359360",
    "end": "366240"
  },
  {
    "text": "your PC you upload this content but then you want to view this content on your phone and your tablets and so we wanted",
    "start": "366240",
    "end": "373400"
  },
  {
    "text": "to make sure that we give a really good experience on multiple devices and for",
    "start": "373400",
    "end": "378479"
  },
  {
    "text": "this we wanted to uh create different formats or previews or thumbnails",
    "start": "378479",
    "end": "383759"
  },
  {
    "text": "depending upon you know if it's a videos or document or photos and be able to do",
    "start": "383759",
    "end": "389720"
  },
  {
    "text": "that at a again at a very high pace and and scale that to terabytes of uploads",
    "start": "389720",
    "end": "396120"
  },
  {
    "text": "which happens you know at a very rapid Pace now the content doesn't just get",
    "start": "396120",
    "end": "403280"
  },
  {
    "text": "stored on Cloud Drive it is also shared by uh the customers with their friends",
    "start": "403280",
    "end": "410319"
  },
  {
    "text": "and families they want to experience that content with their friends and families and so the scaling problem just",
    "start": "410319",
    "end": "415720"
  },
  {
    "text": "kind of increases so so so you can imagine like if you have to kind of you",
    "start": "415720",
    "end": "421520"
  },
  {
    "text": "know build a system where you were supporting for say say you know uh",
    "start": "421520",
    "end": "428160"
  },
  {
    "text": "millions of maybe downloads but now with the more friends and families which are",
    "start": "428160",
    "end": "433280"
  },
  {
    "text": "downloading maybe the same content you have to scale even further right and so the download needed to be optimized",
    "start": "433280",
    "end": "440680"
  },
  {
    "text": "further and then you might have experience in running some of the large scale Services the the key challenge",
    "start": "440680",
    "end": "448039"
  },
  {
    "text": "here is how do you monitor these services proactively in near real time and be able to detect some unusual",
    "start": "448039",
    "end": "455120"
  },
  {
    "text": "parents and then be proactive in taking care of maybe some of the the things",
    "start": "455120",
    "end": "461639"
  },
  {
    "text": "which may degrade your services performance right so these were some of the big challenges there were others but",
    "start": "461639",
    "end": "469080"
  },
  {
    "text": "these were the by far the biggest challenges we faced so let's look into",
    "start": "469080",
    "end": "474560"
  },
  {
    "text": "how we leveraged AWS to solve these so",
    "start": "474560",
    "end": "479840"
  },
  {
    "start": "475000",
    "end": "495000"
  },
  {
    "text": "the first one like I said was we wanted to we wanted to have a very robust",
    "start": "479840",
    "end": "485759"
  },
  {
    "text": "content repository where we can store variety of content with very high",
    "start": "485759",
    "end": "491520"
  },
  {
    "text": "velocity and and be able to scale and this is where we use S3 so S3 just works",
    "start": "491520",
    "end": "498599"
  },
  {
    "start": "495000",
    "end": "560000"
  },
  {
    "text": "out of the box there's nothing more we do and and it provides a very highly",
    "start": "498599",
    "end": "504039"
  },
  {
    "text": "available robust and scalable content repository uh we ALS Al leverage Cloud",
    "start": "504039",
    "end": "511000"
  },
  {
    "text": "front um to solve our sharing scenario where we wanted to scale even further",
    "start": "511000",
    "end": "517518"
  },
  {
    "text": "for download paths for the metadata storage uh there",
    "start": "517519",
    "end": "525040"
  },
  {
    "text": "is some known metadata which we understood very well but then there was metad data which continues to grow as we",
    "start": "525040",
    "end": "531080"
  },
  {
    "text": "get variety of content so we needed a again a very highly scalable very fast",
    "start": "531080",
    "end": "538040"
  },
  {
    "text": "reads and writes a metadata store which provided more like a Nole document",
    "start": "538040",
    "end": "545519"
  },
  {
    "text": "database functionality but at the same time it was fast for key value kind of",
    "start": "545519",
    "end": "551279"
  },
  {
    "text": "lookups and and and optimize to do some of the known queries parts and this is",
    "start": "551279",
    "end": "557920"
  },
  {
    "text": "where we leverage Dynamo DB and um for more flexible indexing we",
    "start": "557920",
    "end": "566120"
  },
  {
    "text": "leverage Cloud search and and Cloud search provides us flexibility in sense",
    "start": "566120",
    "end": "572279"
  },
  {
    "text": "where new metadata becomes more known and now we want to Cann of provide query",
    "start": "572279",
    "end": "578120"
  },
  {
    "text": "patterns to our clients then you just go to Cloud search and enable the indexing on top and and you're able to kind of",
    "start": "578120",
    "end": "584320"
  },
  {
    "text": "provide the needed uh filtering sorting grouping all kind of advanced uh query",
    "start": "584320",
    "end": "591680"
  },
  {
    "text": "capabilities which are needed for a Content inje uh when the",
    "start": "591680",
    "end": "598680"
  },
  {
    "text": "content gets uploaded uh like I mentioned earlier we create different previews of videos for this uh we use uh",
    "start": "598680",
    "end": "607040"
  },
  {
    "text": "the AWS elastic transcoding services to generate different formats so we can um",
    "start": "607040",
    "end": "613040"
  },
  {
    "text": "support different devices and uh for the metadata inje um when we write the metad",
    "start": "613040",
    "end": "620600"
  },
  {
    "text": "data to Dynamo DB we also push the metadata to kesa stream and from there",
    "start": "620600",
    "end": "627920"
  },
  {
    "text": "it it gets uh picked up and pushed into different messaging queue uh where we",
    "start": "627920",
    "end": "633200"
  },
  {
    "text": "leverage sqs and and different messaging cues are optimized depending upon what we're",
    "start": "633200",
    "end": "639639"
  },
  {
    "text": "trying to do like there is a messaging queue which is optimized just for for um",
    "start": "639639",
    "end": "644800"
  },
  {
    "text": "uh Cloud search uh like you need to be smart how you use cloud",
    "start": "644800",
    "end": "650120"
  },
  {
    "text": "search the the amount of metadata you want to kind of push to Cloud search uh",
    "start": "650120",
    "end": "655440"
  },
  {
    "text": "you you have to kind of batch it appropriately and then you also have to kind of tweak the frequency at which you",
    "start": "655440",
    "end": "661360"
  },
  {
    "text": "push to Cloud search to get the best out of cloud search so there is a messaging queue for that uh there's another",
    "start": "661360",
    "end": "668600"
  },
  {
    "text": "messaging queue which we use to back up our metadata into S3 and then there is",
    "start": "668600",
    "end": "674519"
  },
  {
    "text": "another one which we use for notifications uh through which different clients and different Services integrate",
    "start": "674519",
    "end": "681120"
  },
  {
    "text": "uh to our services uh for analytics and monitoring",
    "start": "681120",
    "end": "688040"
  },
  {
    "text": "um like we get you know billions of service calls and obviously the logs",
    "start": "688040",
    "end": "693120"
  },
  {
    "text": "just grow and this is where we leverage EMR heavily we process those and then we",
    "start": "693120",
    "end": "698360"
  },
  {
    "text": "push that data into red shift which is used heavily for our operational",
    "start": "698360",
    "end": "703399"
  },
  {
    "text": "analytics and helps us to get a better sense of our services and uh for U monitoring all the",
    "start": "703399",
    "end": "711560"
  },
  {
    "text": "AWS Services we use cloud watch heavily and for deploying and provisioning and",
    "start": "711560",
    "end": "718959"
  },
  {
    "text": "and managing our ec2 instances we use cloud formation very heavily so let me give you a sense how",
    "start": "718959",
    "end": "726480"
  },
  {
    "text": "all this comes together so as you can see here uh S3 is",
    "start": "726480",
    "end": "733680"
  },
  {
    "text": "our content repository the metad DAT is stored in Dynamo and we use elastic cache to Cache some of the more frequent",
    "start": "733680",
    "end": "742880"
  },
  {
    "text": "uh query results which we know won't change a lot and which are needed for um",
    "start": "742880",
    "end": "748560"
  },
  {
    "text": "optimizing different parts and then the asynchronous pipeline is made of kesis",
    "start": "748560",
    "end": "754720"
  },
  {
    "text": "uh when the data gets written to Dynamo it gets pushed into that stream and then from there on we have different",
    "start": "754720",
    "end": "760160"
  },
  {
    "text": "messaging queue for elastic search um backing up into S3 and also uh for doing",
    "start": "760160",
    "end": "768720"
  },
  {
    "text": "our notification for uh for letting other services integrate with us already",
    "start": "768720",
    "end": "774839"
  },
  {
    "text": "talked about Cloud search it provides uh flexible indexing fulltech search and",
    "start": "774839",
    "end": "781079"
  },
  {
    "text": "all kind of advanced queries where you want to kind of group and sort and and you know provide multiple filtering",
    "start": "781079",
    "end": "788440"
  },
  {
    "text": "capabilities and then EMR and red shift enable our red shift and for our",
    "start": "788440",
    "end": "795199"
  },
  {
    "text": "management we heavily use cloud watch and cloud formation so at this point",
    "start": "795199",
    "end": "801440"
  },
  {
    "text": "I'll uh uh let asish come over and uh talk more and get deep into our",
    "start": "801440",
    "end": "808120"
  },
  {
    "text": "asynchronous pipeline line and how we achieve the data consistency thank",
    "start": "808120",
    "end": "814800"
  },
  {
    "text": "[Applause] you hi everyone uh my name is Ashish",
    "start": "816020",
    "end": "822399"
  },
  {
    "text": "Mishra I'm an engineer on the Cloud Drive team and as I drill into the",
    "start": "822399",
    "end": "827839"
  },
  {
    "text": "architecture a bit hopefully some of the design patterns will be familiar to you and uh applicable to your own",
    "start": "827839",
    "end": "834920"
  },
  {
    "text": "services so what problem were we trying to solve um Cloud Drive launched about",
    "start": "834920",
    "end": "840199"
  },
  {
    "text": "three years ago on a largely homegrown set of processing pipelines and",
    "start": "840199",
    "end": "845759"
  },
  {
    "text": "databases um over time we have scaled out to where we now have tens of pedabytes of content as thean said we're",
    "start": "845759",
    "end": "853120"
  },
  {
    "text": "getting hundreds of millions of service calls per day and um our backends have",
    "start": "853120",
    "end": "860600"
  },
  {
    "text": "not scaled to the level that we want without adding a lot of operational overhead we found we were spending about",
    "start": "860600",
    "end": "866959"
  },
  {
    "text": "80% of our time on monitoring ing replacing hosts checking for performance",
    "start": "866959",
    "end": "872680"
  },
  {
    "text": "at a pretty low level so uh one of our goals was to be able to scale out to the level that we",
    "start": "872680",
    "end": "879120"
  },
  {
    "text": "expect now this growth in traffic pattern is not smooth or predictable it's very",
    "start": "879120",
    "end": "884680"
  },
  {
    "text": "bursty and we expect over the next year as we launch more features more integration with Amazon backends that",
    "start": "884680",
    "end": "892240"
  },
  {
    "text": "this traffic growth will continue the other aspect of it was our",
    "start": "892240",
    "end": "897680"
  },
  {
    "text": "functional requirements keep keep growing we are launching the public API we expect to have a whole new set of",
    "start": "897680",
    "end": "904639"
  },
  {
    "text": "requirements from our previous use cases and we wanted to be able to expand fluidly as time goes by to add more",
    "start": "904639",
    "end": "913880"
  },
  {
    "text": "requirements so um a little bit about what we were trying to achieve here one",
    "start": "913880",
    "end": "919440"
  },
  {
    "start": "916000",
    "end": "1073000"
  },
  {
    "text": "of our core goals is durability and this is something we could not compromise on",
    "start": "919440",
    "end": "924480"
  },
  {
    "text": "the uh customer impact would be uh somebody uploads their baby photos to Cloud Drive deletes them from their",
    "start": "924480",
    "end": "931160"
  },
  {
    "text": "local device um that's something where we need",
    "start": "931160",
    "end": "936279"
  },
  {
    "text": "guarantees as strong as S3 provides um the next one was we need to",
    "start": "936279",
    "end": "942079"
  },
  {
    "text": "be able to scale out and this is a challenge with many of the traditional database systems um scaleout requires",
    "start": "942079",
    "end": "949800"
  },
  {
    "text": "either downtime or a huge performance lag for periods of hours that wasn't",
    "start": "949800",
    "end": "955880"
  },
  {
    "text": "acceptable to our customer base we want uh core operations to have a",
    "start": "955880",
    "end": "962800"
  },
  {
    "text": "very high availability and low latency and very consistent performance um this is not something",
    "start": "962800",
    "end": "970480"
  },
  {
    "text": "that extends to queries so queries might be show me all my files that were uploaded in 2013 show me all the files",
    "start": "970480",
    "end": "978600"
  },
  {
    "text": "in this folder that are ordered by name or show me all the my uploads which have",
    "start": "978600",
    "end": "985160"
  },
  {
    "text": "uh you know a certain string in them like reinvent here we can accept eventual consistency",
    "start": "985160",
    "end": "991440"
  },
  {
    "text": "we don't need strong R your rights consistency um but we do want to have",
    "start": "991440",
    "end": "997040"
  },
  {
    "text": "near realtime updates of queries so we cannot take you know patch processing which takes an hour to",
    "start": "997040",
    "end": "1004519"
  },
  {
    "text": "update um the other aspect of queries is our requirements are all over the board",
    "start": "1004519",
    "end": "1010560"
  },
  {
    "text": "we have requirements for strong uh filtering like very precise filtering um",
    "start": "1010560",
    "end": "1016440"
  },
  {
    "text": "we have requirements for particular ordering some of our queries populate ux like in",
    "start": "1016440",
    "end": "1022240"
  },
  {
    "text": "the website uh tan just showed you some of our queries are for backend business",
    "start": "1022240",
    "end": "1027839"
  },
  {
    "text": "metrics for analytics monitoring and there we can have uh kind",
    "start": "1027839",
    "end": "1032959"
  },
  {
    "text": "of higher latency but the queries are much more heavyweight um and finally we have some",
    "start": "1032959",
    "end": "1041120"
  },
  {
    "text": "custom requirements as we scale out the third party API we're going to have developers come in with query",
    "start": "1041120",
    "end": "1047880"
  },
  {
    "text": "requirements and aggregations that are really specific to their",
    "start": "1047880",
    "end": "1053039"
  },
  {
    "text": "application so uh Shiva had a great visual in his previous presentation um",
    "start": "1053039",
    "end": "1058559"
  },
  {
    "text": "with a relational database as this mutant Swiss army knife and we know that",
    "start": "1058559",
    "end": "1063840"
  },
  {
    "text": "model doesn't work we have uh specific backends that are optimized for particular use cases that they handled",
    "start": "1063840",
    "end": "1071559"
  },
  {
    "text": "best so um when dealing with these back ends we adopted kind of a hybrid Lambda",
    "start": "1071559",
    "end": "1078360"
  },
  {
    "start": "1073000",
    "end": "1208000"
  },
  {
    "text": "architecture and I'll explain why I say hybrid in a few minutes but the main concept is you have",
    "start": "1078360",
    "end": "1085159"
  },
  {
    "text": "a central replication stream of updates and all your backends are reading out of",
    "start": "1085159",
    "end": "1090400"
  },
  {
    "text": "the same stream so um what what back ends am I describing well key components of the",
    "start": "1090400",
    "end": "1098240"
  },
  {
    "text": "architecture we have a highly resilient durable content store content here is",
    "start": "1098240",
    "end": "1104240"
  },
  {
    "text": "the actual payload of the files it might be the uh the btes of your PDF and so",
    "start": "1104240",
    "end": "1110559"
  },
  {
    "text": "forth we have a primary metadata store and this is the authoritative source of information about uploaded metadata",
    "start": "1110559",
    "end": "1118280"
  },
  {
    "text": "metadata would be extracted exf information from your uh photographs it",
    "start": "1118280",
    "end": "1123480"
  },
  {
    "text": "might be like OCR text from your PDFs the size of the metadata is uh you know",
    "start": "1123480",
    "end": "1130320"
  },
  {
    "text": "maybe an order of magnitude to three orders of magnitude smaller than content we have uh query planes which",
    "start": "1130320",
    "end": "1138320"
  },
  {
    "text": "are secondary stores and this gets into what I said before where different",
    "start": "1138320",
    "end": "1143679"
  },
  {
    "text": "backends are optimized for different use cases and handle them really well tying them together we have a",
    "start": "1143679",
    "end": "1150720"
  },
  {
    "text": "central replication stream which populates these uh query back ends and finally we have uh replication",
    "start": "1150720",
    "end": "1158360"
  },
  {
    "text": "workflows which uh transform the data as needed so you may do filtering you may",
    "start": "1158360",
    "end": "1164039"
  },
  {
    "text": "do adaptation to a different schema for your particular backend you're writing to",
    "start": "1164039",
    "end": "1170080"
  },
  {
    "text": "so if we look at uh these building blocks they these roles are pretty well filled by platforms that are provided by",
    "start": "1170080",
    "end": "1177960"
  },
  {
    "text": "AWS for Content store you have S3 it's uh extremely resilient to 99 or more um",
    "start": "1177960",
    "end": "1187080"
  },
  {
    "text": "for the primary metadata plane you have Dynamo DB and this gives you extremely predictable performance and it's",
    "start": "1187080",
    "end": "1194200"
  },
  {
    "text": "extremely fast for individual reads and writes you have a limited degree of query ability with secondary indexes",
    "start": "1194200",
    "end": "1201559"
  },
  {
    "text": "local and Global for richer uh query functionality",
    "start": "1201559",
    "end": "1206799"
  },
  {
    "text": "um you have multiple choices one of them is cloud search and this gives you full",
    "start": "1206799",
    "end": "1212960"
  },
  {
    "text": "text search it's going to give you indexing and to some degree it's going to give you like sorting and pagination",
    "start": "1212960",
    "end": "1219080"
  },
  {
    "text": "ability for more heavyweight aggregation you have Amazon red shift this gives you",
    "start": "1219080",
    "end": "1225159"
  },
  {
    "text": "essentially data warehouse uh query capabilities you could do something like well what is the 90th",
    "start": "1225159",
    "end": "1232080"
  },
  {
    "text": "percentile of customer corpuses across all of the millions of files um all the",
    "start": "1232080",
    "end": "1237280"
  },
  {
    "text": "millions of customers and billions of files in our database for the replication stream",
    "start": "1237280",
    "end": "1243440"
  },
  {
    "text": "sorry um we have Kinesis this is um a durable resilient stream it's going to",
    "start": "1243440",
    "end": "1250280"
  },
  {
    "text": "give you uh the ability via sharding to scale out horizontally to an essentially unlimited",
    "start": "1250280",
    "end": "1257000"
  },
  {
    "text": "degree and finally for our replication workflows here you have a couple of",
    "start": "1257000",
    "end": "1262480"
  },
  {
    "text": "options um for our architecture our use case has very simple Transformations",
    "start": "1262480",
    "end": "1269320"
  },
  {
    "text": "they're relatively self-contained so sqs is a good lightweight model for this",
    "start": "1269320",
    "end": "1274559"
  },
  {
    "text": "where each sqs message corresponds to the state of your workflow if you're doing more complex",
    "start": "1274559",
    "end": "1281200"
  },
  {
    "text": "mapper reducer multi-step uh workflows perhaps you you want to leverage some of the uh more uh heavyweight op Ops given",
    "start": "1281200",
    "end": "1289200"
  },
  {
    "text": "by AWS like swf um how does this all tie",
    "start": "1289200",
    "end": "1296679"
  },
  {
    "text": "together so there's a lot of information on this uh diagram I'm going to go roughly counterclockwise as I describ",
    "start": "1296679",
    "end": "1304360"
  },
  {
    "text": "like a day in the life of uh an uploaded node um users call apis in Cloud Drive",
    "start": "1304360",
    "end": "1311120"
  },
  {
    "text": "service to perform their Crow operations um for All rights um we publish a",
    "start": "1311120",
    "end": "1317520"
  },
  {
    "text": "notification message to Kinesis and we subsequently write to the Dynamo DB",
    "start": "1317520",
    "end": "1323480"
  },
  {
    "text": "table and this ordering is important um if the Kinesis publish fails then the",
    "start": "1323480",
    "end": "1329600"
  },
  {
    "text": "entire transaction is rejected however if the Kinesis uh right fails we may",
    "start": "1329600",
    "end": "1335200"
  },
  {
    "text": "still get throttled or have like loss of availability of Dynamo DB and we'll come",
    "start": "1335200",
    "end": "1340520"
  },
  {
    "text": "back and validate our data against Animo DB later in the pipeline um the uh contents of the Rec",
    "start": "1340520",
    "end": "1348679"
  },
  {
    "text": "could are the primary keyy which is the hash key and range key in Dynamo DB and a version number which I'll describe in",
    "start": "1348679",
    "end": "1355279"
  },
  {
    "text": "some more detail in a little while reading off of the Kinesis stream",
    "start": "1355279",
    "end": "1360640"
  },
  {
    "text": "you have multiple consumers um you could have special case uh consumers that do",
    "start": "1360640",
    "end": "1366039"
  },
  {
    "text": "you know split off by customers and have per customer processing you can have uh",
    "start": "1366039",
    "end": "1371320"
  },
  {
    "text": "notifications to an external system like uh SNS you can also have kind of an",
    "start": "1371320",
    "end": "1377279"
  },
  {
    "text": "incremental backup that reads out chunks of updates and saves them essentially as a right ahead log and um to populate the",
    "start": "1377279",
    "end": "1385559"
  },
  {
    "text": "backends we have uh list we have replication resilient workflows that use",
    "start": "1385559",
    "end": "1390880"
  },
  {
    "text": "sqs to model the state of the workflow what sqs buys us it it gives us um kind",
    "start": "1390880",
    "end": "1398279"
  },
  {
    "text": "of back off and retri semantics you could have a worker fail the message stays in sqs you can have uh the desire",
    "start": "1398279",
    "end": "1404960"
  },
  {
    "text": "to back off and this is uh implemented by extending the VIS ibility timeout in",
    "start": "1404960",
    "end": "1411080"
  },
  {
    "text": "sqs um one uh consumer of these workflows would be writing to Cloud",
    "start": "1411080",
    "end": "1416480"
  },
  {
    "text": "search and we can read out batches of messages from sqs because it's more efficient to publish to Cloud search in",
    "start": "1416480",
    "end": "1423039"
  },
  {
    "text": "that mode um the workers as I said come back and validate the information against",
    "start": "1423039",
    "end": "1429080"
  },
  {
    "text": "Dynamo DB so this is where we slightly deviate from a pure Lambda architecture",
    "start": "1429080",
    "end": "1434640"
  },
  {
    "text": "in that the stream is not the authoritative Source Dynamo DB our primary metadata table is the",
    "start": "1434640",
    "end": "1440520"
  },
  {
    "text": "authoritative Source um if you're building this out uh in the next year or the next few months",
    "start": "1440520",
    "end": "1446960"
  },
  {
    "text": "you have another option which is to use Dynamo DB update streams um in place of",
    "start": "1446960",
    "end": "1452440"
  },
  {
    "text": "the Kinesis stream yes um this is purely the metadata",
    "start": "1452440",
    "end": "1459320"
  },
  {
    "text": "pipeline I'll describe how content gets to S3 uh shortly um finally in the query path we",
    "start": "1459320",
    "end": "1467679"
  },
  {
    "text": "have a we we have logic to merge the query results from cloud search or whatever",
    "start": "1467679",
    "end": "1473520"
  },
  {
    "text": "our query backend is along with a small set of recent updates from Dynamo DB and",
    "start": "1473520",
    "end": "1479640"
  },
  {
    "text": "the reason for doing this is you have a certain degree of lag in your pipeline you also may have a certain degree of",
    "start": "1479640",
    "end": "1485880"
  },
  {
    "text": "lag between uh publishing results to your backend and having them available in the",
    "start": "1485880",
    "end": "1491559"
  },
  {
    "text": "index and so to uh to override this lag based on your use case you can get a",
    "start": "1491559",
    "end": "1497039"
  },
  {
    "text": "small set of unfill results from Dynamo DB just for a small uh window of time merge them in memory",
    "start": "1497039",
    "end": "1504480"
  },
  {
    "text": "with the uh larger set of query results which you've gotten from your batch",
    "start": "1504480",
    "end": "1510639"
  },
  {
    "text": "processing a little bit about our metadata model this is extremely simple and this is by Design um we uh the core",
    "start": "1512600",
    "end": "1521480"
  },
  {
    "start": "1513000",
    "end": "1648000"
  },
  {
    "text": "notion of an atom in Cloud Drive is a node which could represent a file a single file or it could represent a",
    "start": "1521480",
    "end": "1527960"
  },
  {
    "text": "collection or it could represent other units of metadata which we call",
    "start": "1527960",
    "end": "1533360"
  },
  {
    "text": "assets um the node is identified by a customer and a node ID um within the",
    "start": "1533360",
    "end": "1540080"
  },
  {
    "text": "Dynamo DB table we have a single GSI which is on the last modified date the",
    "start": "1540080",
    "end": "1546640"
  },
  {
    "text": "last modified date is uh the epoch milliseconds and we cannot use this",
    "start": "1546640",
    "end": "1552440"
  },
  {
    "text": "really as a strictly increasing counter because of clock skew and other uh uh",
    "start": "1552440",
    "end": "1557520"
  },
  {
    "text": "issues with using the time stamp but we can use it as an approximation to get say within a certain amount of window um",
    "start": "1557520",
    "end": "1564880"
  },
  {
    "text": "say 5 minutes we can guarantee that our clock CU was not varying by more than 20",
    "start": "1564880",
    "end": "1570559"
  },
  {
    "text": "seconds um then you have the additional attributes which correspond to uh the",
    "start": "1570559",
    "end": "1576559"
  },
  {
    "text": "obvious metadata you expect to see for uploads to Cloud Drive um you have the",
    "start": "1576559",
    "end": "1581960"
  },
  {
    "text": "kind uh the type of the node you have a list of parents parents are essentially",
    "start": "1581960",
    "end": "1588080"
  },
  {
    "text": "up upward pointers to the collections containing that node and the reason for keeping upward rather than downward",
    "start": "1588080",
    "end": "1594720"
  },
  {
    "text": "pointers is we can bound the number of upward pointers for a node in in the sense that",
    "start": "1594720",
    "end": "1601880"
  },
  {
    "text": "um from a customer uh perspective you would not have like 100,000 tags on a",
    "start": "1601880",
    "end": "1607360"
  },
  {
    "text": "single photograph um the set of downward pointers cannot be bounded you could have arbitrarily large",
    "start": "1607360",
    "end": "1614559"
  },
  {
    "text": "collections so instead of uh keeping downward pointers we save save the output pointers that way we can",
    "start": "1614559",
    "end": "1620039"
  },
  {
    "text": "guarantee that the record is bounded in size and we rely on our index backends",
    "start": "1620039",
    "end": "1625600"
  },
  {
    "text": "to maintain the reverse relationship you have name uh status and",
    "start": "1625600",
    "end": "1631399"
  },
  {
    "text": "you have additional content metadata which may be like the md5 or the Sha 256",
    "start": "1631399",
    "end": "1636799"
  },
  {
    "text": "of the uploaded content it's going to be perhaps for photos it's going to be like image ex metadata for documents it might",
    "start": "1636799",
    "end": "1644279"
  },
  {
    "text": "be other extracted information so uh the most common concern um is",
    "start": "1644279",
    "end": "1653679"
  },
  {
    "start": "1648000",
    "end": "1786000"
  },
  {
    "text": "consistency um one of two uh reasons that people generally push back on",
    "start": "1653679",
    "end": "1659320"
  },
  {
    "text": "having multiple backends one is query lag which I addressed before we can handle that by merging recent updates",
    "start": "1659320",
    "end": "1667000"
  },
  {
    "text": "within a fixed bounded window from the authoritative store the other concern people have is consistency how do you",
    "start": "1667000",
    "end": "1673640"
  },
  {
    "text": "know that your separate backends have consistent data and isn't the the store",
    "start": "1673640",
    "end": "1679360"
  },
  {
    "text": "going to diverge over time because of code bugs data corruption Etc so our uh guarantee around this is",
    "start": "1679360",
    "end": "1686600"
  },
  {
    "text": "number one roow level Atomic operations which Dynamo DB gives us and uh most of",
    "start": "1686600",
    "end": "1692519"
  },
  {
    "text": "the backends we care about or like would leverage heavily for example redis MySQL",
    "start": "1692519",
    "end": "1698159"
  },
  {
    "text": "Cloud search um red shift all have roow level atomicity of your",
    "start": "1698159",
    "end": "1704519"
  },
  {
    "text": "updates um and this is good enough for our use case for in Cloud Drive where you do not require typically uh",
    "start": "1704519",
    "end": "1711640"
  },
  {
    "text": "consistency between multiple file operations because files are handled independently this also lets us scale",
    "start": "1711640",
    "end": "1717919"
  },
  {
    "text": "out by parallelizing operations across different files now you do have some requirements of consistency between a",
    "start": "1717919",
    "end": "1724240"
  },
  {
    "text": "file and its immediate parent but beyond that um we don't need multi row",
    "start": "1724240",
    "end": "1729640"
  },
  {
    "text": "transactions at a really high level within a row you have a per uh per",
    "start": "1729640",
    "end": "1736039"
  },
  {
    "text": "row revision number and the guarantee here is that this increases monotonically as the row gets updated",
    "start": "1736039",
    "end": "1743600"
  },
  {
    "text": "and we enforce this in Dynamo DB via conditional puts um we can also enforce this in our",
    "start": "1743600",
    "end": "1750000"
  },
  {
    "text": "other backends I'll discuss this shortly um deletes uh are not tracked as",
    "start": "1750000",
    "end": "1755880"
  },
  {
    "text": "deleting the row entirely but they're simply a status update and they also involve incrementing the version number",
    "start": "1755880",
    "end": "1762640"
  },
  {
    "text": "on the row so they are essentially metadata updates even though the ux built off of these is going to be",
    "start": "1762640",
    "end": "1768760"
  },
  {
    "text": "different obviously at every layer of our architecture this revision number",
    "start": "1768760",
    "end": "1774799"
  },
  {
    "text": "determines precedence if you have um two versions of the same row you can always",
    "start": "1774799",
    "end": "1779919"
  },
  {
    "text": "know which is the authoritative one by looking at the higher revision",
    "start": "1779919",
    "end": "1785559"
  },
  {
    "text": "number so this protects us against uh most of the edge cases that we're",
    "start": "1785559",
    "end": "1791000"
  },
  {
    "text": "concerned about for consistency so the three that um uh could arise as a result",
    "start": "1791000",
    "end": "1797679"
  },
  {
    "text": "of your distributed pipeline number one out of order updates where your primary data store",
    "start": "1797679",
    "end": "1803960"
  },
  {
    "text": "gets you know perhaps you recycle a file and then restore it um and with within",
    "start": "1803960",
    "end": "1810279"
  },
  {
    "text": "your distributed pipeline those good get get uh merged and applied out of order um this is we have a canonical sense of",
    "start": "1810279",
    "end": "1817600"
  },
  {
    "text": "the most recent update which is going to be the version number of the row um missed updates um these are",
    "start": "1817600",
    "end": "1825840"
  },
  {
    "text": "handled by all of our replication to backends goes into resilient workflows",
    "start": "1825840",
    "end": "1832480"
  },
  {
    "text": "um in the case of uh Cloud search as I showed you it was sqs any individual",
    "start": "1832480",
    "end": "1838000"
  },
  {
    "text": "worker could die or die multiple times the message will stay in sqs and only be",
    "start": "1838000",
    "end": "1843880"
  },
  {
    "text": "uh removed once the uh the update has been successfully written to Cloud search um similarly every backend has a",
    "start": "1843880",
    "end": "1852480"
  },
  {
    "text": "resilient workflow that uh could fail arbitrarily number of times um",
    "start": "1852480",
    "end": "1858880"
  },
  {
    "text": "there we can add exponential back off but eventually the update will be propagated and finally you have spurious",
    "start": "1858880",
    "end": "1866440"
  },
  {
    "text": "updates um where the concern is that uh a change may not have been accepted by",
    "start": "1866440",
    "end": "1872559"
  },
  {
    "text": "your primary store for whatever reason but it still goes through your pipeline and ends up in one of the secondary",
    "start": "1872559",
    "end": "1879120"
  },
  {
    "text": "backends and the uh resolution to this is uh we always come back to our primary",
    "start": "1879120",
    "end": "1884600"
  },
  {
    "text": "store and uh once again we can use the revision number number to know uh",
    "start": "1884600",
    "end": "1889720"
  },
  {
    "text": "whether uh what's in the primary store should supersede what's in the secondary backend we we have the merge logic to",
    "start": "1889720",
    "end": "1895960"
  },
  {
    "text": "combined results here um a little bit more about some of",
    "start": "1895960",
    "end": "1903799"
  },
  {
    "start": "1901000",
    "end": "2049000"
  },
  {
    "text": "the back ends we've used um I mentioned Cloud search before uh this is fully",
    "start": "1903799",
    "end": "1909080"
  },
  {
    "text": "hosted it scales out with minimal manual intervention needed on our part um we",
    "start": "1909080",
    "end": "1915320"
  },
  {
    "text": "are able to scale uh out across multiple domains because we can effectively Shard",
    "start": "1915320",
    "end": "1921240"
  },
  {
    "text": "by customer ID and assume that the the query traffic against Cloud search is",
    "start": "1921240",
    "end": "1926360"
  },
  {
    "text": "going to be within the scope of a single customer within this uh within the scope",
    "start": "1926360",
    "end": "1931480"
  },
  {
    "text": "it gives us uh full text search um in addition it gives us you know uh the",
    "start": "1931480",
    "end": "1936840"
  },
  {
    "text": "ability to compose indexes arbitrarily to have uh nested Boolean queries which uh we",
    "start": "1936840",
    "end": "1944960"
  },
  {
    "text": "could not get directly from our Dynamo DB table um Amazon red shift is great at handling",
    "start": "1944960",
    "end": "1952279"
  },
  {
    "text": "larger level aggregations across the scope of our entire uh Corpus which",
    "start": "1952279",
    "end": "1957399"
  },
  {
    "text": "could be like tens of billions of rows so U we use it for business metrics for",
    "start": "1957399",
    "end": "1962880"
  },
  {
    "text": "engineering decisions around scaling um within red shift we have uh two levels",
    "start": "1962880",
    "end": "1969000"
  },
  {
    "text": "of uh tables one is databases for the service logs which is essentially flux",
    "start": "1969000",
    "end": "1974320"
  },
  {
    "text": "information um so an example would be well what volume of uploads did we see",
    "start": "1974320",
    "end": "1980480"
  },
  {
    "text": "from this particular device in the last two weeks the other one is going to be a",
    "start": "1980480",
    "end": "1986519"
  },
  {
    "text": "current state of our metadata Corpus and um this would handle uh queries like",
    "start": "1986519",
    "end": "1993000"
  },
  {
    "text": "well how much storage are we using in S3 for current version of PDFs how much uh",
    "start": "1993000",
    "end": "2000120"
  },
  {
    "text": "traffic or how uh how much files uh are were uploaded by a given application",
    "start": "2000120",
    "end": "2005880"
  },
  {
    "text": "ID um one key for uh updates to Red shift because the primary key is Not",
    "start": "2005880",
    "end": "2012760"
  },
  {
    "text": "Guaranteed unique um when we update the metad DAT store we use a staging table",
    "start": "2012760",
    "end": "2018159"
  },
  {
    "text": "to perform upserts and as before we use the revision number to uh guarantee that",
    "start": "2018159",
    "end": "2025200"
  },
  {
    "text": "the staging table will override the target table if and only if the version in the staging table is",
    "start": "2025200",
    "end": "2031760"
  },
  {
    "text": "higher so this uh allows us at any given point of time to have a near realtime",
    "start": "2031760",
    "end": "2036960"
  },
  {
    "text": "stream of up um to go to our red shift metadata table and the only constraint here is uh how",
    "start": "2036960",
    "end": "2044279"
  },
  {
    "text": "large we want the batches to be for efficiency additional query layers um a",
    "start": "2044279",
    "end": "2052800"
  },
  {
    "start": "2049000",
    "end": "2175000"
  },
  {
    "text": "bunch of our uh Services have specific requirements around um caching of data",
    "start": "2052800",
    "end": "2059040"
  },
  {
    "text": "in memory and an example is our photo service which has particular media specific uh queries that it wants to run",
    "start": "2059040",
    "end": "2067118"
  },
  {
    "text": "and uh this is handled by having listeners the same level as the ones populating red shift and Cloud search",
    "start": "2067119",
    "end": "2074960"
  },
  {
    "text": "also can populate these uh inmemory app caches if you uh if you can handle or if",
    "start": "2074960",
    "end": "2081280"
  },
  {
    "text": "you can accept an extra Network roundtrip elastic cache is a great uh solution for inmemory caches in our case",
    "start": "2081280",
    "end": "2088480"
  },
  {
    "text": "the size of the caches happens to be fairly small so we actually keep them on the application service",
    "start": "2088480",
    "end": "2095679"
  },
  {
    "text": "itself um you have addition Dynamo DB tables and this is uh very similar to",
    "start": "2095679",
    "end": "2101480"
  },
  {
    "text": "having a GSI the difference being in this uh architecture you're no longer bounded by the limit of 5",
    "start": "2101480",
    "end": "2108560"
  },
  {
    "text": "gsis um the reason you might want to use Dynamo DB is uh specific of our use",
    "start": "2108560",
    "end": "2114960"
  },
  {
    "text": "cases that have very strong requirements around uh latency and an example would",
    "start": "2114960",
    "end": "2120240"
  },
  {
    "text": "be population of a ux from a particular uh query in this case Dynamo DB meets",
    "start": "2120240",
    "end": "2126880"
  },
  {
    "text": "this need well because uh the performance of uh queries is extremely",
    "start": "2126880",
    "end": "2131920"
  },
  {
    "text": "predictable it's linear and the cost is also extremely predictable it's more expensive of course and it is not uh",
    "start": "2131920",
    "end": "2139400"
  },
  {
    "text": "flexible but we can handle that by adding additional Dynamo DB tables um finally we have additional",
    "start": "2139400",
    "end": "2147240"
  },
  {
    "text": "query layers all of which follow the same design pattern um and as we kind of",
    "start": "2147240",
    "end": "2152720"
  },
  {
    "text": "build out our rest API as I mentioned we expect to add more backends and we have",
    "start": "2152720",
    "end": "2158440"
  },
  {
    "text": "the flexibility to add these and scale them out without affecting say our uh ux",
    "start": "2158440",
    "end": "2164760"
  },
  {
    "text": "that is listening to uh Dynamo DB or which or any crud operations which are accessing the primary metadata",
    "start": "2164760",
    "end": "2173599"
  },
  {
    "start": "2175000",
    "end": "2235000"
  },
  {
    "text": "table uh little bit about our content download and upload path really um as",
    "start": "2175200",
    "end": "2180920"
  },
  {
    "text": "mentioned we have S3 as the primary backend and um to handle oneof stream",
    "start": "2180920",
    "end": "2187200"
  },
  {
    "text": "processing think a common requirement would be like cutting thumbnails for a particular size and for this kind of",
    "start": "2187200",
    "end": "2194359"
  },
  {
    "text": "stream processing we have a proxy layer that sits in front of the S3 and uh",
    "start": "2194359",
    "end": "2200520"
  },
  {
    "text": "streams all uploads and downloads through it um this proxy layer is",
    "start": "2200520",
    "end": "2205760"
  },
  {
    "text": "essentially a stateless client to the Cloud Drive service that I showed in the previous diagram it's not inherently",
    "start": "2205760",
    "end": "2213400"
  },
  {
    "text": "different from any other third party client it doesn't have to keep its own uh metadata store or uh any kind of uh",
    "start": "2213400",
    "end": "2221920"
  },
  {
    "text": "longlived workflows it simply writes those back to the metadata as an update and it's subsequently handled for",
    "start": "2221920",
    "end": "2228800"
  },
  {
    "text": "indexing and so forth in Cloud Drive",
    "start": "2228800",
    "end": "2233079"
  },
  {
    "start": "2235000",
    "end": "2346000"
  },
  {
    "text": "service what did we learn going through this exercise um we had huge benefits",
    "start": "2235400",
    "end": "2243440"
  },
  {
    "text": "from offloading our operational workload to these AWS platforms um we going from",
    "start": "2243440",
    "end": "2252079"
  },
  {
    "text": "kind of a hosted database to Dynamo DB has cut down our uh overhead for the the",
    "start": "2252079",
    "end": "2258720"
  },
  {
    "text": "core metadata store by about 90% um there is a flip side of course",
    "start": "2258720",
    "end": "2265040"
  },
  {
    "text": "which is as you add more moving pieces this is not a a function of moving to a",
    "start": "2265040",
    "end": "2270839"
  },
  {
    "text": "particular architecture but as your requirements grow if you add more pieces there is overhead to uh to support and",
    "start": "2270839",
    "end": "2278920"
  },
  {
    "text": "uh keep track of outages in these different pieces so um you do need to add in",
    "start": "2278920",
    "end": "2285800"
  },
  {
    "text": "integration testing at every uh step in the pipeline and in most cases for",
    "start": "2285800",
    "end": "2291240"
  },
  {
    "text": "multiple steps for example um one level of integration testing would be crud",
    "start": "2291240",
    "end": "2296359"
  },
  {
    "text": "operations being written into Dynamo DB another one would be uh Downstream",
    "start": "2296359",
    "end": "2301480"
  },
  {
    "text": "workers picking up those updates from the pipeline and uh of course those workers",
    "start": "2301480",
    "end": "2306880"
  },
  {
    "text": "writing updates to whatever back end that they choose um avoid cascading failures in",
    "start": "2306880",
    "end": "2313920"
  },
  {
    "text": "your dependencies as you add more dependencies you don't want kind of an outage uh kind of a temporary outage in",
    "start": "2313920",
    "end": "2320200"
  },
  {
    "text": "one to Break functionality all the way through your system and the mitigation",
    "start": "2320200",
    "end": "2325640"
  },
  {
    "text": "around this um really is to have fan out to your dependencies at the top layer",
    "start": "2325640",
    "end": "2331800"
  },
  {
    "text": "don't create long chains of dependencies um as long as you have uh",
    "start": "2331800",
    "end": "2337599"
  },
  {
    "text": "uh fan out at the top layer a failure in any one component is really only going to impact the particular use case that's",
    "start": "2337599",
    "end": "2344480"
  },
  {
    "text": "uh reading out of that component this also gives us decoupling from say uh you know red shift with our",
    "start": "2344480",
    "end": "2351599"
  },
  {
    "start": "2346000",
    "end": "2420000"
  },
  {
    "text": "primary website finally data consistency um a",
    "start": "2351599",
    "end": "2358119"
  },
  {
    "text": "big win for us was being able to have granularity of our guarantees around",
    "start": "2358119",
    "end": "2363480"
  },
  {
    "text": "consistency what I mean by that is we did not need uh to build out kind of",
    "start": "2363480",
    "end": "2369040"
  },
  {
    "text": "transactions that span like hundreds or thousands of rows um the benefit to that",
    "start": "2369040",
    "end": "2374280"
  },
  {
    "text": "is uh number one performance of course but also scaleout we can really efficiently paralyze operations across",
    "start": "2374280",
    "end": "2382280"
  },
  {
    "text": "rows um even within a row using optimistic uh version checks we don't",
    "start": "2382280",
    "end": "2388440"
  },
  {
    "text": "need to have uh forced locks or uh mutexes across our distributed",
    "start": "2388440",
    "end": "2395440"
  },
  {
    "text": "system so um if you can break down your data model such that your consistency",
    "start": "2395440",
    "end": "2400880"
  },
  {
    "text": "guarantees are self-contained then you can build out sort of an kind of incremental sequential uh system for",
    "start": "2400880",
    "end": "2408760"
  },
  {
    "text": "revisions similar to how we did without having to couple uh these these revision",
    "start": "2408760",
    "end": "2414680"
  },
  {
    "text": "sequences across like a whole bunch of different",
    "start": "2414680",
    "end": "2419000"
  },
  {
    "text": "rows um that's about it for uh my discussion of the architecture um one of",
    "start": "2419839",
    "end": "2425800"
  },
  {
    "start": "2420000",
    "end": "2494000"
  },
  {
    "text": "the big use cases that we were trying to support has been building out a rest API",
    "start": "2425800",
    "end": "2431520"
  },
  {
    "text": "for thirdparty developers to use and they can run queries they can make service calls against these backends",
    "start": "2431520",
    "end": "2438040"
  },
  {
    "text": "that I described here and uh a little bit more about the rest API I'll uh turn it over to the",
    "start": "2438040",
    "end": "2444079"
  },
  {
    "text": "Lo",
    "start": "2444079",
    "end": "2447079"
  },
  {
    "text": "thank so as I mentioned uh earlier uh this week we announced uh Amazon Cloud",
    "start": "2450040",
    "end": "2457040"
  },
  {
    "text": "Drive API this is a rest based API which you all can leverage and uh uh build new",
    "start": "2457040",
    "end": "2464319"
  },
  {
    "text": "apps on uh on top of Cloud Drive services or you could integrate this API",
    "start": "2464319",
    "end": "2470000"
  },
  {
    "text": "with your existing apps and uh uh serve millions of Amazon customers uh here's a",
    "start": "2470000",
    "end": "2476440"
  },
  {
    "text": "link uh a little bit long where you can find more experience and I think one",
    "start": "2476440",
    "end": "2481480"
  },
  {
    "text": "paper was passed to you guys where you can find more information about this we also have a booth uh where you can come",
    "start": "2481480",
    "end": "2489359"
  },
  {
    "text": "and visit us if you want going to learn more about this API so just a few call for action for",
    "start": "2489359",
    "end": "2497800"
  },
  {
    "start": "2494000",
    "end": "2528000"
  },
  {
    "text": "you guys if you haven't already tried Amazon Cloud Drive please do so uh if",
    "start": "2497800",
    "end": "2502880"
  },
  {
    "text": "you're a Amazon Prime member try Amazon Prime photos uh gives you unlimited photo",
    "start": "2502880",
    "end": "2509800"
  },
  {
    "text": "storage and if you're a developer build some cool apps on top of our of our services",
    "start": "2509800",
    "end": "2518680"
  },
  {
    "text": "and uh please uh do give us your feedback thank you again for uh attending this session",
    "start": "2519480",
    "end": "2525720"
  },
  {
    "text": "[Applause]",
    "start": "2525720",
    "end": "2530790"
  }
]