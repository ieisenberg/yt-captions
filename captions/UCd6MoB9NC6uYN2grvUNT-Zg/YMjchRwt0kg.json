[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "great hello everyone so I'm going to get started my name is Brian O'Connor I'm the technical director of the analysis",
    "start": "2210",
    "end": "9059"
  },
  {
    "text": "core at the UCSC genomics Institute and today I'm going to tell you about our large-scale cloud-based analysis of",
    "start": "9059",
    "end": "15570"
  },
  {
    "text": "cancer genomes in particular I'm going to tell you about the lessons that we learn from the Pequod's project so just",
    "start": "15570",
    "end": "22590"
  },
  {
    "start": "21000",
    "end": "21000"
  },
  {
    "text": "as an overview for my slides I'm really kind of dividing things into three parts",
    "start": "22590",
    "end": "27660"
  },
  {
    "text": "with peak cog being the past and sort of lessons learned on how to do large-scale",
    "start": "27660",
    "end": "33079"
  },
  {
    "text": "genomics analysis on the cloud and I'm looking towards the present in terms of translating what we've done into tools",
    "start": "33079",
    "end": "39480"
  },
  {
    "text": "that other people can use and finally I'm talking about the future through the",
    "start": "39480",
    "end": "44910"
  },
  {
    "text": "Global Alliance and establishing standards by which we can have other people use similar infrastructure and",
    "start": "44910",
    "end": "50850"
  },
  {
    "text": "how that be compatible with each other through the ga4gh so just as a background",
    "start": "50850",
    "end": "57649"
  },
  {
    "start": "55000",
    "end": "55000"
  },
  {
    "text": "the PIA cog project is a large international collaboration so it stands for pan-cancer analysis of whole genomes",
    "start": "57649",
    "end": "64948"
  },
  {
    "text": "it was organized by the ICGC and it comprises of approximately 5,800",
    "start": "64949",
    "end": "73979"
  },
  {
    "text": "whole human genomes that correspond to about 2,800 cancer donors 1,300 of which",
    "start": "73979",
    "end": "80250"
  },
  {
    "text": "had RNA seek data now the goal of the technical working group was to consistently analyze this data set it",
    "start": "80250",
    "end": "87780"
  },
  {
    "text": "came from a variety of projects that use different workflows so we want to enable scientists to do awesome analysis on",
    "start": "87780",
    "end": "94530"
  },
  {
    "text": "this by having a consistent alignment and variant calling somatic variant calling pipeline applied to this data",
    "start": "94530",
    "end": "100320"
  },
  {
    "text": "set the challenges for this project were you know it's scoped in its geographic",
    "start": "100320",
    "end": "105540"
  },
  {
    "text": "distribution this was not a funded effort this is actually a collaboration of volunteers all around the world about",
    "start": "105540",
    "end": "112619"
  },
  {
    "text": "700 scientists were involved in this project and that meant we had eight different sites to store the data",
    "start": "112619",
    "end": "118049"
  },
  {
    "text": "through a system called gino's which you can think of it as a multi server SFTP server we peaked at about nine hundred",
    "start": "118049",
    "end": "124770"
  },
  {
    "text": "terabytes with this project and the biggest challenge for my team and I back at oh I CR was building the",
    "start": "124770",
    "end": "131489"
  },
  {
    "text": "infrastructure that could on the consistent analysis pipelines across 14",
    "start": "131489",
    "end": "137750"
  },
  {
    "text": "cloud an HPC environment simultaneously three of those were commercial Amazon",
    "start": "137750",
    "end": "142880"
  },
  {
    "text": "being our go-to cloud environment on that side seven OpenStack environments and four HPC environments so at the peak",
    "start": "142880",
    "end": "150350"
  },
  {
    "text": "of the project we were managing about six hundred and thirty VMs across these environments that correspond to about",
    "start": "150350",
    "end": "156200"
  },
  {
    "text": "15,000 cores so it was a very challenging infrastructure project for us in terms of the pipeline's themselves",
    "start": "156200",
    "end": "163730"
  },
  {
    "start": "158000",
    "end": "158000"
  },
  {
    "text": "we were trying to establish core analysis pipelines that included BWA mem",
    "start": "163730",
    "end": "169190"
  },
  {
    "text": "alignment to a standardized genome and then best practice pipelines that were created with our team and the team at",
    "start": "169190",
    "end": "176989"
  },
  {
    "text": "Sanger dkf said amble and the broad for calling somatic variants these variant",
    "start": "176989",
    "end": "182690"
  },
  {
    "text": "calls would then be filtered and merged and consensus called and that consensus",
    "start": "182690",
    "end": "187790"
  },
  {
    "text": "calling process was validated through a 63 donor validation data set and further",
    "start": "187790",
    "end": "194510"
  },
  {
    "text": "analysis was then done downstream and the filtered merged in consensus called variants that process was really in full",
    "start": "194510",
    "end": "202430"
  },
  {
    "text": "swing by summer time and now we're in the state of actually working on",
    "start": "202430",
    "end": "207470"
  },
  {
    "text": "publications from this project so it's a very exciting period of time for the P cog project from the technical",
    "start": "207470",
    "end": "214070"
  },
  {
    "text": "infrastructure side the challenges of spreading our analysis across so many different environments",
    "start": "214070",
    "end": "219310"
  },
  {
    "text": "we learned several key lessons that I think are interesting and informative of our future work the first one was a",
    "start": "219310",
    "end": "226400"
  },
  {
    "text": "lesson about cloud policies and how those can really shape the capabilities of what we can do in clouds the second",
    "start": "226400",
    "end": "232880"
  },
  {
    "text": "was about how to make tools portable when we have so many environments to work in the third was about how to make",
    "start": "232880",
    "end": "238489"
  },
  {
    "text": "the execution platform distributed and fault tolerant and finally we learned a",
    "start": "238489",
    "end": "244040"
  },
  {
    "text": "surprising lesson about commercial cloud costs so on the first side going into this",
    "start": "244040",
    "end": "250940"
  },
  {
    "start": "248000",
    "end": "248000"
  },
  {
    "text": "project TCGA data we could not place on the commercial clouds ICGC data we could",
    "start": "250940",
    "end": "256430"
  },
  {
    "text": "only transiently process but things started to change in march of 2015 the",
    "start": "256430",
    "end": "262010"
  },
  {
    "text": "nih updated their dbgap policy that allowed us to you use commercial clouds",
    "start": "262010",
    "end": "267440"
  },
  {
    "text": "for processing data in May of 2015 I CG sadako updated the ICGC cloud policy and",
    "start": "267440",
    "end": "274730"
  },
  {
    "text": "that allowed us to not only compute in those environments but also redistribute",
    "start": "274730",
    "end": "279790"
  },
  {
    "text": "icgc data which comprise about half this data set through commercial cloud environments it enabled really cool",
    "start": "279790",
    "end": "286610"
  },
  {
    "text": "partnerships here with Amazon the public data sets program allowing us to redistribute peacott data through",
    "start": "286610",
    "end": "293060"
  },
  {
    "text": "through Amazon s3 and that enabled our partners with seven bridges and DNAnexus that provided compute for this platform",
    "start": "293060",
    "end": "300410"
  },
  {
    "text": "to access that data within that cloud environment so that was a small thing in",
    "start": "300410",
    "end": "305450"
  },
  {
    "start": "304000",
    "end": "304000"
  },
  {
    "text": "terms of a policy change but it really shifted the way that we did our compute on the project so originally we would",
    "start": "305450",
    "end": "312530"
  },
  {
    "text": "look at a standardized sort of single centralized metadata index figure out",
    "start": "312530",
    "end": "319190"
  },
  {
    "text": "what needed to be run across our fleet of clouds in queue work in those clouds and then if we're working in Amazon",
    "start": "319190",
    "end": "325910"
  },
  {
    "text": "Cloud spin up spot instances that needed to pull the data from these external gino's repositories these external",
    "start": "325910",
    "end": "332900"
  },
  {
    "text": "storage repositories running around the globe and that was not ideal because that pipe was limited so we could only",
    "start": "332900",
    "end": "338810"
  },
  {
    "text": "pull in so much data to process in Amazon when these policy changes were",
    "start": "338810",
    "end": "344420"
  },
  {
    "start": "343000",
    "end": "343000"
  },
  {
    "text": "put into effect we suddenly could actually host ICGC data within the amazon environment we can load it we",
    "start": "344420",
    "end": "351230"
  },
  {
    "text": "could load it into s3 and that allowed us to pre cache data into that",
    "start": "351230",
    "end": "356840"
  },
  {
    "text": "environment and then do much larger scale compute on that data because of the scalability of s3 and that also",
    "start": "356840",
    "end": "362960"
  },
  {
    "text": "allowed us to share within the cloud environment with DNAnexus and seven bridges so a great shift for us and that",
    "start": "362960",
    "end": "370250"
  },
  {
    "start": "368000",
    "end": "368000"
  },
  {
    "text": "project was really something that that changed the way that we worked something else that had a really amazing impact on",
    "start": "370250",
    "end": "376700"
  },
  {
    "text": "the peak hog project was really seeing docker released as a 1.0",
    "start": "376700",
    "end": "383080"
  },
  {
    "text": "about halfway through our core analysis on this project docker released its 1.0",
    "start": "383080",
    "end": "389450"
  },
  {
    "text": "and we felt comfortable using it in this project as a key component what we ended up doing is moving to a",
    "start": "389450",
    "end": "395810"
  },
  {
    "text": "model where we moved away from a model of building worker nodes on-demand using",
    "start": "395810",
    "end": "401310"
  },
  {
    "text": "ansible playbooks to actually set up a worker node from scratch install all the tools on it install the workflow that",
    "start": "401310",
    "end": "407940"
  },
  {
    "text": "was a very time-consuming process that was very error-prone we moved instead to making portable docker images that",
    "start": "407940",
    "end": "415350"
  },
  {
    "text": "contained everything in the kitchen sink so we included workflow definitions config files some of the reference files",
    "start": "415350",
    "end": "421290"
  },
  {
    "text": "all the tools that we needed and we covered the core pipelines this was bwa",
    "start": "421290",
    "end": "426510"
  },
  {
    "text": "mm alignment and the three somatic variant calling pipelines from Brody cave said amble and Sanger and so this",
    "start": "426510",
    "end": "433950"
  },
  {
    "text": "was really transformative for us because suddenly we could move around these lightweight docker containers these",
    "start": "433950",
    "end": "440400"
  },
  {
    "text": "docker images had everything that we needed in them it really cut down our provisioning process I really improved",
    "start": "440400",
    "end": "446670"
  },
  {
    "text": "the reliability that provisioning but also it allowed us to start exploring other environments for compute including",
    "start": "446670",
    "end": "452490"
  },
  {
    "text": "handing these to HPC environments that were participants in P COG and having those docker containers run there as",
    "start": "452490",
    "end": "457980"
  },
  {
    "text": "well so the third lesson that we learned is really you know building and running",
    "start": "457980",
    "end": "464630"
  },
  {
    "start": "459000",
    "end": "459000"
  },
  {
    "text": "infrastructure on the cloud is different you don't want to treat it the same way that you've worked previously so going",
    "start": "464630",
    "end": "470640"
  },
  {
    "text": "into this a lot of the academic folks and myself had more experience with HPC so our first sort of architecture for",
    "start": "470640",
    "end": "477270"
  },
  {
    "text": "running analysis within P Cod was really setting up something that looked like a traditional HPC cluster we run send sub",
    "start": "477270",
    "end": "484590"
  },
  {
    "text": "grid engine we had cluster running we created these little mini clusters across the clouds and scheduled out work",
    "start": "484590",
    "end": "491010"
  },
  {
    "text": "to them as you sort of typically do what we found though is in environments like",
    "start": "491010",
    "end": "496650"
  },
  {
    "text": "Amazon where we're interacting with a spot marketplace and we'd lose nodes or open stack environments that were",
    "start": "496650",
    "end": "502080"
  },
  {
    "text": "created for peak hog and actually new environments so they're still having sort of teething issues these",
    "start": "502080",
    "end": "508470"
  },
  {
    "text": "environments were not amenable to running sge and having sge lose nodes so",
    "start": "508470",
    "end": "514349"
  },
  {
    "text": "instead we kind of shifted halfway through to an architecture where one VM processed one donor ran one workflow and",
    "start": "514350",
    "end": "522000"
  },
  {
    "text": "at least that was a self-contained infrastructure but we weren't particularly happy with it because we didn't really want to work in that way",
    "start": "522000",
    "end": "529500"
  },
  {
    "text": "we want to work in a way that was much more nimble and able to deal with failure on its own rather than having us",
    "start": "529500",
    "end": "535410"
  },
  {
    "text": "reschedule work that failed so we ultimately in conjunction with moving to docker based tools and workflows we",
    "start": "535410",
    "end": "543839"
  },
  {
    "text": "built our architecture what we called architecture 3 and the hallmark of that was each of the cloud environments we",
    "start": "543839",
    "end": "549930"
  },
  {
    "text": "worked in including Amazon had a queue and we would in queue the work that needed to be done in that environment",
    "start": "549930",
    "end": "555480"
  },
  {
    "text": "and that queue is read by provisioner that would spin up VMs in Amazon it was",
    "start": "555480",
    "end": "561839"
  },
  {
    "text": "spot instances and those spot instances would have a worker daemon on them that would DQ work and run the particular",
    "start": "561839",
    "end": "569100"
  },
  {
    "text": "docker based workflow for that particular sample what that gave us is a",
    "start": "569100",
    "end": "574110"
  },
  {
    "text": "system that if workers disappeared it automatically knew how to rien khao work so that we could be very robust to",
    "start": "574110",
    "end": "581040"
  },
  {
    "text": "failure whatever that failure was all right so the fourth lesson that we",
    "start": "581040",
    "end": "586380"
  },
  {
    "start": "583000",
    "end": "583000"
  },
  {
    "text": "took away from peacock that actually turned out to be something that surprised most of the technical working",
    "start": "586380",
    "end": "591449"
  },
  {
    "text": "group was really doing a sort of look at our expenses looking at the Institute's",
    "start": "591449",
    "end": "598529"
  },
  {
    "text": "that built out OpenStack environments specifically for this project and look at the cost of running on Amazon so we",
    "start": "598529",
    "end": "605639"
  },
  {
    "text": "used Amazon quite heavily in this project especially early on a bootstrapper way into running we could",
    "start": "605639",
    "end": "611250"
  },
  {
    "text": "get running on Amazon much more quickly than any of the custom-built OpenStack environments so we had a lot of",
    "start": "611250",
    "end": "616980"
  },
  {
    "text": "information about the true cost of running this analysis on Amazon using the spot marketplace and we had a pretty",
    "start": "616980",
    "end": "623610"
  },
  {
    "text": "good idea of what the costs were associated with building out over the course of a year and a half or so to two",
    "start": "623610",
    "end": "629579"
  },
  {
    "text": "and a half years was the the entire lifespan of the project we had a pretty good idea of the cost of building out",
    "start": "629579",
    "end": "635310"
  },
  {
    "text": "our academic OpenStack cloud environments so there's an interesting piece in nature",
    "start": "635310",
    "end": "641250"
  },
  {
    "text": "and 2015 data analysis create a cloud Commons encourage you to take a look at",
    "start": "641250",
    "end": "646260"
  },
  {
    "text": "it because this figure is obviously too small here but the take-home point is when we did the math we looked at",
    "start": "646260",
    "end": "652019"
  },
  {
    "text": "building out infrastructure staffing it up keeping it supported and compared that to running the same pipeline the",
    "start": "652019",
    "end": "657930"
  },
  {
    "text": "Sanger pipeline on Amazon we actually had an order of magnitude difference in the price Amazon significantly cheaper",
    "start": "657930",
    "end": "664720"
  },
  {
    "text": "for us to do compute on so that was surprising at the end of the day when we looked at the actual cost of us running",
    "start": "664720",
    "end": "671530"
  },
  {
    "text": "this analysis on Amazon it boiled down to only about sixty two dollars per donor and that may seem expensive to",
    "start": "671530",
    "end": "678700"
  },
  {
    "text": "some but put in the context of the three to four thousand dollars required to actually sequence these samples it's a",
    "start": "678700",
    "end": "685390"
  },
  {
    "text": "drop in the bucket so we're actually very pleased with the cost savings and the scalability the ability to to spike",
    "start": "685390",
    "end": "691900"
  },
  {
    "text": "our usage in Amazon that was what that was afforded by this cloud all right so",
    "start": "691900",
    "end": "698650"
  },
  {
    "start": "697000",
    "end": "697000"
  },
  {
    "text": "what about the the legacy of the peacock we learned a lot of great lessons a lot of scientists got to use cloud",
    "start": "698650",
    "end": "704320"
  },
  {
    "text": "environments for the very first time we saw that it was a viable thing for analyzing cancer genomics data in the in",
    "start": "704320",
    "end": "710980"
  },
  {
    "text": "the public clouds we're in the phase right now where publications are coming soon so this is",
    "start": "710980",
    "end": "716380"
  },
  {
    "text": "very exciting phase for the project in terms of a legacy for the project beyond just the publications",
    "start": "716380",
    "end": "722310"
  },
  {
    "text": "the Amazon AWS public data sets program has allowed us to redistribute the ICGC",
    "start": "722310",
    "end": "729580"
  },
  {
    "text": "peacock data through s/3 s/3 on the Amazon Cloud environment and I think",
    "start": "729580",
    "end": "735580"
  },
  {
    "text": "that's really transformative that and the NCI cloud pilots kind of give us a glimpse of the future of what it's like",
    "start": "735580",
    "end": "741040"
  },
  {
    "text": "what it's going to be like to work with large-scale data sets large-scale cancer data sets on cloud environments so this",
    "start": "741040",
    "end": "747880"
  },
  {
    "text": "is great for a researcher because I can come in I can identify the data and get a token that allows me to access it from",
    "start": "747880",
    "end": "754750"
  },
  {
    "text": "the DCC portal DCC icgc org and then i can go on to the cloud environment i",
    "start": "754750",
    "end": "760210"
  },
  {
    "text": "don't have to worry about downloading the data from EGA or SRA or some other repository which many of you know might",
    "start": "760210",
    "end": "767110"
  },
  {
    "text": "take weeks or months to do and I don't have to worry about the storage cost of storing the bama I can just get to my",
    "start": "767110",
    "end": "772720"
  },
  {
    "text": "actual compute on it so I think those are very powerful sort of legacy items frumpy cog in terms of what are we doing",
    "start": "772720",
    "end": "780790"
  },
  {
    "text": "right now between my group it at Santa Cruz between oh I see are between a lot",
    "start": "780790",
    "end": "785890"
  },
  {
    "text": "of other people that have contributed to the technical working group on P COG the focus for us is to get our workflows",
    "start": "785890",
    "end": "792220"
  },
  {
    "text": "that we used in P cog shared obviously we want those to be part of our publications but on top of that we want",
    "start": "792220",
    "end": "799250"
  },
  {
    "text": "to take the tools and infrastructure that we build for Pete Cobb and make that something that other people can use",
    "start": "799250",
    "end": "805370"
  },
  {
    "text": "as well that other large scale projects or even smaller scale projects can take advantage of so I think there's three",
    "start": "805370",
    "end": "811250"
  },
  {
    "text": "key tools from peak hog or inspired by the work of peak cog but I think are",
    "start": "811250",
    "end": "816590"
  },
  {
    "text": "generally useful and and things that many people will find useful and those are redwood which is",
    "start": "816590",
    "end": "824000"
  },
  {
    "text": "our file storage system on top of s3 that's docstore it's our platform for",
    "start": "824000",
    "end": "829160"
  },
  {
    "text": "sharing docker based tools and workflows and toil which is coming out of the",
    "start": "829160",
    "end": "834350"
  },
  {
    "text": "Santa Cruz group this is our workflow execution platform that goes beyond the",
    "start": "834350",
    "end": "839510"
  },
  {
    "text": "sort of compute that peak OGG dead so what I'm personally working on right now",
    "start": "839510",
    "end": "844670"
  },
  {
    "start": "842000",
    "end": "842000"
  },
  {
    "text": "at at Santa Cruz is taking all of these components that we've built over inspired by Pequod and linking them",
    "start": "844670",
    "end": "852350"
  },
  {
    "text": "together into a larger ecosystem what we're doing here is building a system that by stringing these tools",
    "start": "852350",
    "end": "858200"
  },
  {
    "text": "together we can automate the process of data coming into the Institute and so it starts on the left-hand side where at",
    "start": "858200",
    "end": "864980"
  },
  {
    "text": "the bottom people are submitting data to us these are our collaborators they're submitting data to us through an upload",
    "start": "864980",
    "end": "870770"
  },
  {
    "text": "bian that writes it into the data storage service aka redwood from that we've established a metadata standard",
    "start": "870770",
    "end": "877580"
  },
  {
    "text": "and index that metadata standard to build a donor oriented document that makes it very clear what analysis has",
    "start": "877580",
    "end": "884180"
  },
  {
    "text": "and has not been done on that particular donor and from that we have an Action Service currently we're using Luigi for",
    "start": "884180",
    "end": "890570"
  },
  {
    "text": "this that queries our elasticsearch metadata index figures out what work needs to be done and then n queues that",
    "start": "890570",
    "end": "897050"
  },
  {
    "text": "work in our queue which calls toil to actually do the workflow execution so",
    "start": "897050",
    "end": "902330"
  },
  {
    "text": "the system altogether is able to process data and hand it back to our collaborators hand it back to other",
    "start": "902330",
    "end": "907610"
  },
  {
    "text": "scientists at the Institute and in terms of the actual tools and workflows we're",
    "start": "907610",
    "end": "913340"
  },
  {
    "text": "placing those on docstore to get the greatest sort of benefit from other people using those those tools as well",
    "start": "913340",
    "end": "919810"
  },
  {
    "text": "so just to dive into the details of the actual projects that I'm talking about",
    "start": "919810",
    "end": "925580"
  },
  {
    "start": "920000",
    "end": "920000"
  },
  {
    "text": "here the Redwood system is a scalable storage system built on top that's three",
    "start": "925580",
    "end": "930740"
  },
  {
    "text": "it is based on the ICGC storage system that powers the peacock data-sharing",
    "start": "930740",
    "end": "936110"
  },
  {
    "text": "through the Amazon public data sets program in this client it's quite",
    "start": "936110",
    "end": "941810"
  },
  {
    "text": "different from the way that we worked in peacock originally where we actually had these smtp like servers that would",
    "start": "941810",
    "end": "948050"
  },
  {
    "text": "actually handle the entire transaction and entire transfer of data to a worker node in this system the client running",
    "start": "948050",
    "end": "954800"
  },
  {
    "text": "on an ec2 host will simply use a token to request a particular",
    "start": "954800",
    "end": "960370"
  },
  {
    "text": "blob of data essentially and that storage service that then authenticates",
    "start": "960370",
    "end": "966380"
  },
  {
    "text": "that request will hand back a signed URL and so that the outcome of that is whether it's the client is uploading or",
    "start": "966380",
    "end": "972590"
  },
  {
    "text": "downloading data is the actual large data transfers coming from s3 and what does that mean that means it's a much",
    "start": "972590",
    "end": "979040"
  },
  {
    "text": "more scalable system than the approach that peak hog took that actually bottlenecked in many ways on the storage",
    "start": "979040",
    "end": "985570"
  },
  {
    "text": "transfer since we're doing our heavy lifting coming directly from s3 it means",
    "start": "985570",
    "end": "990590"
  },
  {
    "text": "I can throw hundreds of simultaneous clients on hundreds of nodes hitting this and not seeing a pre appreciable",
    "start": "990590",
    "end": "996620"
  },
  {
    "text": "decrease in the transfer speed and this is an example of that as I'm adding more",
    "start": "996620",
    "end": "1001920"
  },
  {
    "text": "client nodes but do simultaneous downloads I'm not really seeing a degradation of course this is why we",
    "start": "1001920",
    "end": "1008320"
  },
  {
    "text": "love s3 we love the fact that it can handle so much data in terms of workflow",
    "start": "1008320",
    "end": "1014200"
  },
  {
    "text": "sharing we really loved the approach that we took with with docker we really",
    "start": "1014200",
    "end": "1020320"
  },
  {
    "text": "thought that helped the project to help to make our tools and workflows much more mobile but one of the issues that",
    "start": "1020320",
    "end": "1026620"
  },
  {
    "text": "we ran into is we didn't actually have a standardized way of saying this docker image has this tool in it it takes these",
    "start": "1026620",
    "end": "1034120"
  },
  {
    "text": "parameters and this is how you run it that was something that if you go on you know docker hub or kway teow right now",
    "start": "1034120",
    "end": "1040510"
  },
  {
    "text": "you basically read a readme and you figure out how to run it we needed some sort of standardized approach where we",
    "start": "1040510",
    "end": "1046720"
  },
  {
    "text": "could have a standard that would describe how to run the scientific tool and so for the docstore project which is",
    "start": "1046720",
    "end": "1053350"
  },
  {
    "text": "formalizing our sharing of docker based tools and workflows we embraced the cwl",
    "start": "1053350",
    "end": "1058480"
  },
  {
    "text": "and Whittle projects so this is common workflow language workflow definition language",
    "start": "1058480",
    "end": "1064270"
  },
  {
    "text": "respectively cwl is its own open source project Whittle is coming out of the road and what these",
    "start": "1064270",
    "end": "1071490"
  },
  {
    "text": "descriptors allow us to do is have a very clear human and machine readable",
    "start": "1071490",
    "end": "1077529"
  },
  {
    "text": "format that tells us what the inputs are what the outputs are going to be how to construct the command so we've",
    "start": "1077529",
    "end": "1083860"
  },
  {
    "text": "essentially through docstore standardized the way that we describe tools and workflows in docker images now",
    "start": "1083860",
    "end": "1091149"
  },
  {
    "text": "as an academic project we didn't want to reinvent the wheel I'm sure almost everyone in this room actively uses",
    "start": "1091149",
    "end": "1098140"
  },
  {
    "text": "github and bitbucket for storing source and maybe storing a docker file I'm sure",
    "start": "1098140",
    "end": "1103210"
  },
  {
    "text": "most people in this room use kwai tai o or docker hub we're actually building the docker images so that's exactly what",
    "start": "1103210",
    "end": "1109779"
  },
  {
    "text": "we want to do on docstore we want to leverage those with Docs or acting as a registry that brings that information",
    "start": "1109779",
    "end": "1115870"
  },
  {
    "text": "between Quay and github together with that descriptor that standardizes the",
    "start": "1115870",
    "end": "1121029"
  },
  {
    "text": "way that we can describe how to run that tool or that workflow and so what we're doing with github is we store the docker",
    "start": "1121029",
    "end": "1127210"
  },
  {
    "text": "file we store a doc store cwl or a doc store twiddle and that docker file is",
    "start": "1127210",
    "end": "1133990"
  },
  {
    "text": "then built by Kwai dot IO which builds the docker image and hosts it and doc",
    "start": "1133990",
    "end": "1139120"
  },
  {
    "text": "store is really bringing those those two things together in order to have a common view of the content of github the",
    "start": "1139120",
    "end": "1146620"
  },
  {
    "text": "content of Kwai dot IO and that descriptor itself so where we out with",
    "start": "1146620",
    "end": "1151659"
  },
  {
    "start": "1149000",
    "end": "1149000"
  },
  {
    "text": "doc store I'm really really happy to say that doc stores actually reached its 1.0 release so this is a production system",
    "start": "1151659",
    "end": "1158529"
  },
  {
    "text": "multiple groups are using it now to exchange docker based tools and workflows it supports a ga4gh standard",
    "start": "1158529",
    "end": "1165399"
  },
  {
    "text": "that'll tell you a little bit more about at the end of the talk and it also supports the latest cwl cwl itself is a",
    "start": "1165399",
    "end": "1172929"
  },
  {
    "text": "project just released 1.0 we support two entity types we support docker based",
    "start": "1172929",
    "end": "1178720"
  },
  {
    "text": "tools and we also support cwo and Whittle native workflows as well which are modeled as documents and right now",
    "start": "1178720",
    "end": "1186730"
  },
  {
    "text": "the Peacock group is really working hard to make sure all of our content including the core pipelines are being",
    "start": "1186730",
    "end": "1192460"
  },
  {
    "text": "ported and available in docstore we have all of them except abroad at the moment",
    "start": "1192460",
    "end": "1197509"
  },
  {
    "text": "so in terms of what the docstore looks like you can you can look at it today you can use it today it's in production",
    "start": "1197509",
    "end": "1203389"
  },
  {
    "text": "docstore org the site has pretty much all the functionality that you need",
    "start": "1203389",
    "end": "1208559"
  },
  {
    "text": "through the site itself including search and tool management if I dive into a",
    "start": "1208559",
    "end": "1213720"
  },
  {
    "text": "particular page you can see that it describes information about the",
    "start": "1213720",
    "end": "1218909"
  },
  {
    "text": "container if the font was a little bit bigger you could probably see that it's giving you information about who made",
    "start": "1218909",
    "end": "1223950"
  },
  {
    "text": "this container when was it made links back to github links back to Kwai dot IO",
    "start": "1223950",
    "end": "1230039"
  },
  {
    "text": "and description discussion sharing links but",
    "start": "1230039",
    "end": "1235080"
  },
  {
    "text": "something that's the most important thing here one of the outcomes of standardizing on cwl are wittle",
    "start": "1235080",
    "end": "1240179"
  },
  {
    "text": "standardizing the way that we describe how to run the tool is it means that every tool or workflow that you find on",
    "start": "1240179",
    "end": "1245730"
  },
  {
    "text": "docstore has a common methodology of executing it so you always construct the",
    "start": "1245730",
    "end": "1251009"
  },
  {
    "text": "command the same way this makes it so much easier to find tools and execute them in a consistent way to facilitate",
    "start": "1251009",
    "end": "1257340"
  },
  {
    "start": "1257000",
    "end": "1257000"
  },
  {
    "text": "that we have a very lightweight command-line tool if you find something that you like on Docs or you want to run",
    "start": "1257340",
    "end": "1263009"
  },
  {
    "text": "it on your machine that supports docker this command-line tool will help you to provision your files from standard urls",
    "start": "1263009",
    "end": "1269850"
  },
  {
    "text": "pull the docker container execute the CW l or Whittle wrapper to essentially",
    "start": "1269850",
    "end": "1275009"
  },
  {
    "text": "create a docker run and then provision the files back out so this is a simple mechanism for you to use something today",
    "start": "1275009",
    "end": "1281129"
  },
  {
    "text": "that you find a doc store what we're trying to work towards in the future is we don't want to be the only execution",
    "start": "1281129",
    "end": "1286710"
  },
  {
    "text": "platform out there we actually want seven bridges and Kure bursts and other commercial platforms as well as open",
    "start": "1286710",
    "end": "1293580"
  },
  {
    "text": "source platforms like galaxy and consonants to be able to pull and use tools and workflows from docstore as",
    "start": "1293580",
    "end": "1299580"
  },
  {
    "text": "well so we hope that in the long run our goal is to have docstore tools rentable",
    "start": "1299580",
    "end": "1304649"
  },
  {
    "text": "in a wide variety of platforms that allow you to scale up so as a scientist",
    "start": "1304649",
    "end": "1309659"
  },
  {
    "text": "another feature that I think is really important about docstore something that came out of the Pequod's project is very",
    "start": "1309659",
    "end": "1315389"
  },
  {
    "text": "very clear versioning being extremely important if I have a large cohort of data that I've run in a previous version",
    "start": "1315389",
    "end": "1320820"
  },
  {
    "text": "of the pipeline I may have new data that I want to bring in and use with that same version the pipeline so docstore",
    "start": "1320820",
    "end": "1327389"
  },
  {
    "text": "provides a very clear link between the github codebase the docker image hosted",
    "start": "1327389",
    "end": "1332730"
  },
  {
    "text": "on quay or docker hub and the descriptor that describes how to run that particular version of the tool and this",
    "start": "1332730",
    "end": "1338940"
  },
  {
    "text": "enables you to find older tools older versions of tools and recreate or supplement existing datasets",
    "start": "1338940",
    "end": "1346580"
  },
  {
    "text": "it also has again bringing together the content of github and and cuellar docker",
    "start": "1346580",
    "end": "1352710"
  },
  {
    "text": "hub it's bringing together a single place that you can understand what the docker file contains how that particular",
    "start": "1352710",
    "end": "1358590"
  },
  {
    "text": "tool was built and also the descriptor itself like how the tool is described in",
    "start": "1358590",
    "end": "1364530"
  },
  {
    "text": "whittler cwl what its inputs or outputs are and finally as a developer if you",
    "start": "1364530",
    "end": "1371400"
  },
  {
    "text": "follow our SOP if you use github or bitbucket to use quad io which provides",
    "start": "1371400",
    "end": "1377070"
  },
  {
    "text": "us a nice api we can automate the registration of content from your Kwai",
    "start": "1377070",
    "end": "1382650"
  },
  {
    "text": "IO and github account through the site so if you followed our development SOP which is a very lightweight SOP you can",
    "start": "1382650",
    "end": "1389400"
  },
  {
    "text": "basically gain the advantage of going into docstore seeing that your content that was built on clay and clicking a",
    "start": "1389400",
    "end": "1395520"
  },
  {
    "text": "button to publish it all right so what's coming soon to doc store we have several new features",
    "start": "1395520",
    "end": "1401880"
  },
  {
    "start": "1397000",
    "end": "1397000"
  },
  {
    "text": "coming up I think one of the most important is integrating the concept of public test datasets so if you find a",
    "start": "1401880",
    "end": "1408660"
  },
  {
    "text": "tool on Doc's tour I want to be able to run it with known good data see examples of what the inputs and outputs should be",
    "start": "1408660",
    "end": "1414870"
  },
  {
    "text": "the other thing is we're actively working with commercial and open source providers to enable execution of tools",
    "start": "1414870",
    "end": "1422070"
  },
  {
    "text": "and workflows from doc store but as an ongoing process that we're actively working on and we're also looking at",
    "start": "1422070",
    "end": "1427530"
  },
  {
    "text": "things like signing docker containers so we have a chain of trust and cross-site indexing so if we have many copies of",
    "start": "1427530",
    "end": "1433800"
  },
  {
    "text": "doc store running around the world we want to be able to cross-site index them and show their content across the",
    "start": "1433800",
    "end": "1440280"
  },
  {
    "text": "network of DOC store instances so hopefully that gives you an idea of some",
    "start": "1440280",
    "end": "1445500"
  },
  {
    "text": "of the of the storage system that are kind of coming out of the P COG project the docstore org which is coming out of",
    "start": "1445500",
    "end": "1452580"
  },
  {
    "text": "our sort of standardized way of packaging tools from the Pequod's project I want to talk a little bit",
    "start": "1452580",
    "end": "1458340"
  },
  {
    "text": "about how to do more efficient compute on u.s. than what we did with peak hog",
    "start": "1458340",
    "end": "1464080"
  },
  {
    "text": "so peak hog was remarkable and being able to run in so many different environments but we really didn't",
    "start": "1464080",
    "end": "1470800"
  },
  {
    "text": "explore too great detail the way of making those individual workflows more",
    "start": "1470800",
    "end": "1475930"
  },
  {
    "text": "efficient and toil here is a response to making individual workflows run more",
    "start": "1475930",
    "end": "1481180"
  },
  {
    "text": "efficiently at a lower cost in peak cog we ran everything in the kitchen sink was put into a docker image one VM one",
    "start": "1481180",
    "end": "1489160"
  },
  {
    "text": "docker image running one donor was processed on that image we want to do something more efficient where we can",
    "start": "1489160",
    "end": "1494860"
  },
  {
    "text": "actually scale out individual jobs from a workflow and grow and shrink the cluster on-demand in order to increase",
    "start": "1494860",
    "end": "1501460"
  },
  {
    "text": "our our turnaround time for samples so the toil system is really built with",
    "start": "1501460",
    "end": "1506590"
  },
  {
    "text": "that it's built with large-scale efficient work on Amazon as the key sort of driving feature a recent recompute",
    "start": "1506590",
    "end": "1514330"
  },
  {
    "text": "sauce managing a 30k core cluster and processing about 20,000 RNA seek samples",
    "start": "1514330",
    "end": "1521800"
  },
  {
    "text": "so we're showing that we can actually do this sort of fine-grained job granularity fine-grained execution",
    "start": "1521800",
    "end": "1528150"
  },
  {
    "text": "efficiently robustly and at scale so to give you some more details about what",
    "start": "1528150",
    "end": "1533770"
  },
  {
    "text": "makes toil unique and special some of the more advanced features unlike peak",
    "start": "1533770",
    "end": "1539050"
  },
  {
    "text": "cog which had very static workflows toil workflows can actually be dynamic the workflow can change during execution",
    "start": "1539050",
    "end": "1545590"
  },
  {
    "text": "this is something that not many workflow execution or workflow definition languages can support another thing that",
    "start": "1545590",
    "end": "1552850"
  },
  {
    "text": "I think really makes it stand out is its ability to execute services for a given workflow",
    "start": "1552850",
    "end": "1560980"
  },
  {
    "text": "and the common example that we use quite a bit at Santa Cruz is executing and creating a spark cluster",
    "start": "1560980",
    "end": "1568780"
  },
  {
    "text": "for the execution of workflows that contain atom steps so we're using atom here to speed up our band manipulations",
    "start": "1568780",
    "end": "1575590"
  },
  {
    "text": "and some of our pipelines and it's taking advantage of the fact that toil knows how to spin up a spark cluster in",
    "start": "1575590",
    "end": "1582430"
  },
  {
    "text": "order to satisfy the needs of those jobs from a user perspective a lot of people",
    "start": "1582430",
    "end": "1588220"
  },
  {
    "text": "like toil because it's just pure Python very very simple API",
    "start": "1588220",
    "end": "1593280"
  },
  {
    "text": "you have all the functionality the Python programming language you can see here we're just defining a very simple",
    "start": "1593280",
    "end": "1599409"
  },
  {
    "text": "task we're assigning the word that we want to echo basically",
    "start": "1599409",
    "end": "1605679"
  },
  {
    "text": "and we're telling it how much memory how many cores are required so we're giving",
    "start": "1605679",
    "end": "1610779"
  },
  {
    "text": "those consumable resource requirements to our job and then telling toil to execute this miniature one-step workflow",
    "start": "1610779",
    "end": "1618309"
  },
  {
    "text": "so it's a very simple syntax but very powerful because of its underlying support of Python in addition we're",
    "start": "1618309",
    "end": "1625570"
  },
  {
    "text": "supporting various environments in production we're using mezzos workers on",
    "start": "1625570",
    "end": "1630580"
  },
  {
    "text": "AWS that are provisioned in an auto scaling capability with with toil but other",
    "start": "1630580",
    "end": "1637779"
  },
  {
    "text": "batch processing systems have been submitted by the community including sge LSF & slurm",
    "start": "1637779",
    "end": "1645899"
  },
  {
    "text": "so finally I think that the key underlying most important sort of pieces of toil",
    "start": "1645899",
    "end": "1652149"
  },
  {
    "text": "are its scalability and robustness so one of the ways that we've made it's scalable is the job store this is the",
    "start": "1652149",
    "end": "1659730"
  },
  {
    "text": "service that tracks the jobs that are completed and also intermediate files",
    "start": "1659730",
    "end": "1664870"
  },
  {
    "text": "that are completed this is designed to handle many concurrent workers on top of",
    "start": "1664870",
    "end": "1670899"
  },
  {
    "text": "that we're using mezzos as our actual workers and we've shown in testing that we can scale out through the auto",
    "start": "1670899",
    "end": "1677710"
  },
  {
    "text": "scaling code of toil we can scale out to about 50 K nodes on Amazon ec2 so that's",
    "start": "1677710",
    "end": "1684279"
  },
  {
    "text": "quite I think it gives us quite a bit of headroom for scaling out our computes and the workers themselves have been",
    "start": "1684279",
    "end": "1690100"
  },
  {
    "text": "written in such a way to really reduce round trips to the central master they're capable of doing their own",
    "start": "1690100",
    "end": "1695830"
  },
  {
    "text": "scheduling on a node they're essentially able to batch up scheduling to reduce",
    "start": "1695830",
    "end": "1701139"
  },
  {
    "text": "that that interaction with a centralized master in terms of robustness I think",
    "start": "1701139",
    "end": "1706269"
  },
  {
    "start": "1703000",
    "end": "1703000"
  },
  {
    "text": "what really convinced me that toil ISM is a cool project is seeing the ability to take a toil cluster absolutely",
    "start": "1706269",
    "end": "1713139"
  },
  {
    "text": "destroy it and then bring it back online and have a pick up where it left off so individual jobs are check pointed upon",
    "start": "1713139",
    "end": "1719200"
  },
  {
    "text": "completion this is fundamentally different than what we did in peak hog if a given VM running a given docker",
    "start": "1719200",
    "end": "1725519"
  },
  {
    "text": "based workflow for a given donor fail peacock we had to start from ground zero in this system if particular steps fail",
    "start": "1725519",
    "end": "1733900"
  },
  {
    "text": "because of a VM going offline because of spot instance prices for example the",
    "start": "1733900",
    "end": "1739420"
  },
  {
    "text": "system is smart enough to checkpoint the previous jobs and resume where it left off so that's very very powerful in an",
    "start": "1739420",
    "end": "1746500"
  },
  {
    "text": "environment where notes may come and go the other thing that I think is interesting here is the toil job store",
    "start": "1746500",
    "end": "1753130"
  },
  {
    "text": "is designed to work in a variety of context we primarily use it in Amazon with s3 and simple DV backing it but for",
    "start": "1753130",
    "end": "1760810"
  },
  {
    "text": "development purposes you can use a shared file system or local file system and it also supports emerging support",
    "start": "1760810",
    "end": "1767650"
  },
  {
    "text": "for Azure and Google Cloud hmm ok so in terms of understanding does",
    "start": "1767650",
    "end": "1773290"
  },
  {
    "text": "the system and it sounds great on paper but does it actually deliver when we use it for large-scale computes something on",
    "start": "1773290",
    "end": "1779320"
  },
  {
    "text": "the order of peak hog or maybe even larger and that's exactly what the group did so about three months ago we did a",
    "start": "1779320",
    "end": "1785590"
  },
  {
    "text": "twenty thousand RNA seek sample recompute on Amazon ec2 and this",
    "start": "1785590",
    "end": "1791470"
  },
  {
    "text": "pipeline was very straightforward it's just using Callisto star and RS m in parallel steps here and the data set",
    "start": "1791470",
    "end": "1799510"
  },
  {
    "text": "that we're using is the TCGA data set along with GTECH target and P not and so",
    "start": "1799510",
    "end": "1805300"
  },
  {
    "text": "I think this slide really in a nutshell underscores the scalability and robustness and why toil is the step",
    "start": "1805300",
    "end": "1811540"
  },
  {
    "text": "forward for us beyond what we did in P COG so on the the graph on the left hand side what you're looking at is that cyan",
    "start": "1811540",
    "end": "1817690"
  },
  {
    "text": "color that is just running the RNA seek workflow as we did with peak hog single",
    "start": "1817690",
    "end": "1823630"
  },
  {
    "text": "VM running a pipeline for a particular sample the dark blue line is showing the",
    "start": "1823630",
    "end": "1829930"
  },
  {
    "text": "cost of running in toil mode where there's an auto scaling cluster",
    "start": "1829930",
    "end": "1835180"
  },
  {
    "text": "behind-the-scenes nodes are scaling up nodes are scaling down and what we're finding is we're going from about $8 a",
    "start": "1835180",
    "end": "1840910"
  },
  {
    "text": "sample down to about 385 or so per sample all right so that's a huge drop",
    "start": "1840910",
    "end": "1847570"
  },
  {
    "text": "by just taking advantage of the ebb and flow of computational needs for a cluster at a particular moment in time",
    "start": "1847570",
    "end": "1853830"
  },
  {
    "text": "another step down is actually then using spot instances and taking advantage of",
    "start": "1853830",
    "end": "1859000"
  },
  {
    "text": "that robustness to failure response does instances go away they can be repositioned as the price comes back",
    "start": "1859000",
    "end": "1865880"
  },
  {
    "text": "down and with that we're able to drop the pipeline that originally ran for about eight dollars a sample down to a",
    "start": "1865880",
    "end": "1871700"
  },
  {
    "text": "dollar thirty so really just doing some very simple things not rewriting the pipeline not fundamentally changing the",
    "start": "1871700",
    "end": "1877909"
  },
  {
    "text": "infrastructure that we're using we're able to really drop the price down on the right-hand side what I'm showing is",
    "start": "1877909",
    "end": "1883730"
  },
  {
    "text": "the cores used over time and the two red circles indicate where the spot market",
    "start": "1883730",
    "end": "1890120"
  },
  {
    "text": "place spiked and we lost all of our worker nodes so what's happening here is the system is recovering from that it's",
    "start": "1890120",
    "end": "1897049"
  },
  {
    "text": "launching worker nodes again and it's picking up where it left off ultimately to hit about 32,000 cores managed by",
    "start": "1897049",
    "end": "1903289"
  },
  {
    "text": "toil what I think is really cool is in P cog it took us you know over a year to",
    "start": "1903289",
    "end": "1909110"
  },
  {
    "text": "do the analysis and this effort it was about four days to do the 20,000 because",
    "start": "1909110",
    "end": "1914630"
  },
  {
    "text": "we could really spike our our usage in Amazon and really take advantage of the scalability of infrastructure like s3",
    "start": "1914630",
    "end": "1922899"
  },
  {
    "start": "1922000",
    "end": "1922000"
  },
  {
    "text": "alright so I gave you an idea of what peak hog showed the community I think it",
    "start": "1922899",
    "end": "1929059"
  },
  {
    "text": "showed a lot of cancer researchers that we could use the power of commercial clouds for large-scale scientific work I",
    "start": "1929059",
    "end": "1936200"
  },
  {
    "text": "think that was a fundamental thing that came out of the pea cog project our current work on Redwood docstore and",
    "start": "1936200",
    "end": "1942950"
  },
  {
    "text": "toil really formalized the lessons we've learned and we've tried to translate",
    "start": "1942950",
    "end": "1948370"
  },
  {
    "text": "what we've learned from pea cog into things that other people can use that the community can pick up and use right",
    "start": "1948370",
    "end": "1954529"
  },
  {
    "text": "now our future work is to extend that even farther by focusing on how can we",
    "start": "1954529",
    "end": "1959929"
  },
  {
    "text": "establish standards for redwood docstore and toil so other other groups other",
    "start": "1959929",
    "end": "1966409"
  },
  {
    "text": "efforts can implement similar systems but provide common api's so I can use",
    "start": "1966409",
    "end": "1972289"
  },
  {
    "text": "toil and I can use other systems in a common way so that's our current focus for future work here and this is all",
    "start": "1972289",
    "end": "1978919"
  },
  {
    "start": "1977000",
    "end": "1977000"
  },
  {
    "text": "really being focused on by the ga4gh this is the Global Alliance for genomics",
    "start": "1978919",
    "end": "1984710"
  },
  {
    "text": "and health this is where our API standards effort is taking place so I'm the co-leader of the containers and",
    "start": "1984710",
    "end": "1991250"
  },
  {
    "text": "workflows task team and our priority there is to establish a that abstract",
    "start": "1991250",
    "end": "1997760"
  },
  {
    "text": "and make generic the facilities of docstore and toil and these other pieces",
    "start": "1997760",
    "end": "2003549"
  },
  {
    "text": "of infrastructure so the first one we've worked on is the tool registry API and",
    "start": "2003549",
    "end": "2008860"
  },
  {
    "text": "this is just a very generic way of describing this docker image is available here's its descriptor here's",
    "start": "2008860",
    "end": "2016570"
  },
  {
    "text": "its source and quai dot IO this API we hope that multiple tools like docstore",
    "start": "2016570",
    "end": "2023500"
  },
  {
    "text": "will support it and that way we can essentially have a federated network of ga4gh tool registry supporting systems",
    "start": "2023500",
    "end": "2031330"
  },
  {
    "text": "so we can share tools and workflows in a standardized way so docstore currently",
    "start": "2031330",
    "end": "2036370"
  },
  {
    "text": "supports it and we're working with bioshock and others for those repositories of docker based tools to",
    "start": "2036370",
    "end": "2043200"
  },
  {
    "text": "essentially expose their tools to the world using this API so another API that we're working on",
    "start": "2043200",
    "end": "2049510"
  },
  {
    "text": "through the ga4gh is the workflow and task execution api's and these api's are",
    "start": "2049510",
    "end": "2056020"
  },
  {
    "text": "essentially the same api under the hood but what they're allowing us to do is to take a wittle or cwl workflow or a",
    "start": "2056020",
    "end": "2062260"
  },
  {
    "text": "docker based tool and actually make a request to a system that supports the",
    "start": "2062260",
    "end": "2067300"
  },
  {
    "text": "API to then run that tool with the parameters we've provided as a result we",
    "start": "2067300",
    "end": "2073060"
  },
  {
    "text": "get back a status URL and then can retrieve file outputs locations as well",
    "start": "2073060",
    "end": "2078550"
  },
  {
    "text": "as standard error standard out of the status and and that sort of thing and so this is an area of active development",
    "start": "2078550",
    "end": "2084250"
  },
  {
    "text": "right now to establish this API standard we're hoping to have the first version of this API schema released in early",
    "start": "2084250",
    "end": "2091480"
  },
  {
    "text": "2017 as far as the tool registry API that's already released as a 1.0 schema",
    "start": "2091480",
    "end": "2098500"
  },
  {
    "text": "release as of last month so where are we headed I would love to",
    "start": "2098500",
    "end": "2104710"
  },
  {
    "start": "2100000",
    "end": "2100000"
  },
  {
    "text": "have had this infrastructure these standards docstore toil redwood I would",
    "start": "2104710",
    "end": "2110080"
  },
  {
    "text": "love to have had those two years ago when we worked on peak hog and we really scaled out our infrastructure and built",
    "start": "2110080",
    "end": "2116710"
  },
  {
    "text": "out so many different tools in the future though I think establishing standards on top of things like docstore",
    "start": "2116710",
    "end": "2124030"
  },
  {
    "text": "toil the storage systems establishing ga4gh GH",
    "start": "2124030",
    "end": "2129760"
  },
  {
    "text": "on top of these systems I think is really important because it will enable future systems future large-scale",
    "start": "2129760",
    "end": "2135970"
  },
  {
    "text": "distributed cancer analysis projects like icgc med to access a variety of",
    "start": "2135970",
    "end": "2142200"
  },
  {
    "text": "tool registry workflow registry sites like doc store or bioshock it would",
    "start": "2142200",
    "end": "2147940"
  },
  {
    "text": "allow us to schedule out workflows in a standardized way to say toil or",
    "start": "2147940",
    "end": "2153070"
  },
  {
    "text": "consonants firecloud or seven bridges and those individual steps can then run in a variety of cloud environments using",
    "start": "2153070",
    "end": "2159730"
  },
  {
    "text": "the task execution API driven by things such as cost and also location of data",
    "start": "2159730",
    "end": "2165930"
  },
  {
    "text": "so we're hoping that the ga4gh standards enable this sort of view of the world",
    "start": "2165930",
    "end": "2171490"
  },
  {
    "text": "going forward in the future and we're working closely with a lot of these organizations this is not necessarily",
    "start": "2171490",
    "end": "2177160"
  },
  {
    "text": "commitment from these organizations but more of a vision of how this might work in the future",
    "start": "2177160",
    "end": "2182430"
  },
  {
    "text": "so with that I just want to thank a ton of people with the ga4gh in particular",
    "start": "2182430",
    "end": "2188440"
  },
  {
    "text": "Lincoln Josh Gaddy Peter and Yann for leading the technical working group of P cog Vincent for the storage",
    "start": "2188440",
    "end": "2195460"
  },
  {
    "text": "infrastructure Denis for docstore Kyle for the tasks API Peter for the workflow",
    "start": "2195460",
    "end": "2200890"
  },
  {
    "text": "API Jeff for the co-leader of the J for gh task team and Hannes and Frank and",
    "start": "2200890",
    "end": "2210040"
  },
  {
    "text": "the toil team for toil infrastructure I do want to mention that these projects",
    "start": "2210040",
    "end": "2215740"
  },
  {
    "start": "2213000",
    "end": "2213000"
  },
  {
    "text": "are all open source we welcome your contributions we welcome your use of these projects we want people to use",
    "start": "2215740",
    "end": "2221619"
  },
  {
    "text": "these we want people to benefit from the infrastructure that we've built for peak hog and infrastructure inspired by what",
    "start": "2221619",
    "end": "2227830"
  },
  {
    "text": "we did in peak hog so for more information about redwood check out the ICGC DCC get repository Doc's stories",
    "start": "2227830",
    "end": "2236980"
  },
  {
    "text": "and production right now you can use it find oliver documentation on docstore org and toil is something that you can",
    "start": "2236980",
    "end": "2243760"
  },
  {
    "text": "spin up today write workflows locally on your laptop and transition those to AWS following the guides on or read the docs",
    "start": "2243760",
    "end": "2251790"
  },
  {
    "text": "at this point I'm going to hand it over to Angel Pizarro to get the perspective",
    "start": "2251790",
    "end": "2258190"
  },
  {
    "text": "from AWS",
    "start": "2258190",
    "end": "2261089"
  },
  {
    "text": "years years of work really fantastic results I love seeing",
    "start": "2269210",
    "end": "2275640"
  },
  {
    "text": "the progression of good projects like that develop into something that the whole community can use so so kudos to to all",
    "start": "2275640",
    "end": "2283320"
  },
  {
    "text": "of the international scientists doing doing that work my name is angel Pizarro I work with the research and technical",
    "start": "2283320",
    "end": "2289080"
  },
  {
    "text": "computing team focusing on genomics and life sciences here at AWS work with folks across the globe on these types of",
    "start": "2289080",
    "end": "2296460"
  },
  {
    "text": "problems and given the backdrop that that you just",
    "start": "2296460",
    "end": "2301500"
  },
  {
    "text": "saw I want to give why AWS cares about these problems what our perspective is and sort of directions of of why we do",
    "start": "2301500",
    "end": "2310530"
  },
  {
    "text": "what we do right so enabling science I think nothing is is more important to",
    "start": "2310530",
    "end": "2317220"
  },
  {
    "start": "2312000",
    "end": "2312000"
  },
  {
    "text": "AWS than changing the world and science is one of those areas that we want to change for the better and for the",
    "start": "2317220",
    "end": "2323970"
  },
  {
    "text": "previous talk I hope you could recognize some of these some of these features that",
    "start": "2323970",
    "end": "2329869"
  },
  {
    "text": "deploying to AWS enabled specifically the scaling of compute resources only",
    "start": "2329869",
    "end": "2335550"
  },
  {
    "text": "when they were needed to when they're actually needed drastically reducing the time to a",
    "start": "2335550",
    "end": "2341250"
  },
  {
    "text": "result because in science really any single result is it is not as important",
    "start": "2341250",
    "end": "2349200"
  },
  {
    "text": "as what comes after it and and what you do with those results and how you integrate those results with other data",
    "start": "2349200",
    "end": "2355820"
  },
  {
    "text": "of course cost was greatly reduced and it cannot emphasize enough what a",
    "start": "2355820",
    "end": "2364170"
  },
  {
    "text": "game-changer is to be able to securely share data in place for some of these",
    "start": "2364170",
    "end": "2369240"
  },
  {
    "text": "large cohorts the download and compute model worked up to a certain point in the life sciences",
    "start": "2369240",
    "end": "2376470"
  },
  {
    "text": "it doesn't work anymore when your datasets are literally petabytes of data",
    "start": "2376470",
    "end": "2381600"
  },
  {
    "text": "so being able to securely share data in place bring the compute to the data is a",
    "start": "2381600",
    "end": "2386970"
  },
  {
    "text": "huge change or science and one for the better and also having a global community being able access that data in",
    "start": "2386970",
    "end": "2394690"
  },
  {
    "text": "the same way should not be overlooked as well so this is this is we have an open",
    "start": "2394690",
    "end": "2401230"
  },
  {
    "start": "2398000",
    "end": "2398000"
  },
  {
    "text": "data program and this is how we see the world we really think that data is",
    "start": "2401230",
    "end": "2406869"
  },
  {
    "text": "critical infrastructure for a lot of different types of sciences and businesses and software development and",
    "start": "2406869",
    "end": "2413230"
  },
  {
    "text": "ISPs again if your if your project depends on a petabyte of data to",
    "start": "2413230",
    "end": "2419740"
  },
  {
    "text": "actually work then data is a critical infrastructure",
    "start": "2419740",
    "end": "2425020"
  },
  {
    "text": "piece of what you're trying to do for instance the exact database proved",
    "start": "2425020",
    "end": "2432810"
  },
  {
    "text": "they had a very interesting paper about being able to look at cancer data and",
    "start": "2432810",
    "end": "2438640"
  },
  {
    "text": "only sequence the tumor as opposed to a tumor and normal tissue so that you can",
    "start": "2438640",
    "end": "2443800"
  },
  {
    "text": "see the differences with this large backing database from a population of people you can actually reconstruct what",
    "start": "2443800",
    "end": "2451330"
  },
  {
    "text": "would be different if you had sequenced the normal tissue and and that matters because you've essentially cut the cost",
    "start": "2451330",
    "end": "2458260"
  },
  {
    "text": "of sequencing a cancer genome in half and that's really really going to matter as we start getting genome sequencing",
    "start": "2458260",
    "end": "2465180"
  },
  {
    "text": "integrated within the clinic so again we believe open data is critical infrastructure and",
    "start": "2465180",
    "end": "2471780"
  },
  {
    "text": "to date the public data sets program I'll use a mouse is really here where",
    "start": "2471780",
    "end": "2480670"
  },
  {
    "text": "we're creating a place to put data at rest and making available in place the",
    "start": "2480670",
    "end": "2486609"
  },
  {
    "text": "work that Brian just showed is in this area where you have data catalogues you have basic API s to access that data and",
    "start": "2486609",
    "end": "2494109"
  },
  {
    "text": "the tools and visualizations to make that available we have a little bit of complex api's currently under",
    "start": "2494109",
    "end": "2501430"
  },
  {
    "text": "development for working with the data and so on and so forth so where are we with with bioinformatics",
    "start": "2501430",
    "end": "2508470"
  },
  {
    "text": "of course we have data generation taken care of pretty well we can we can we can produce a lot of hell of a lot data with",
    "start": "2508470",
    "end": "2515080"
  },
  {
    "text": "the current instrumentation that's in place whether that's selects the base sequence thing or the newer nanopore",
    "start": "2515080",
    "end": "2520690"
  },
  {
    "text": "sequences that are coming out and then as we showed with the IC see data set",
    "start": "2520690",
    "end": "2528070"
  },
  {
    "text": "and the peacock data set you can make that those types of data is available at rest we have groups like the Global",
    "start": "2528070",
    "end": "2534490"
  },
  {
    "text": "Alliance really making basic api's that enable folks to talk in the same",
    "start": "2534490",
    "end": "2540070"
  },
  {
    "text": "language and access the data in standard ways and then you have data visualizations like CC bio portal and",
    "start": "2540070",
    "end": "2547390"
  },
  {
    "text": "data distribution frameworks like the national repositories like NIH and and the EBI over in Europe",
    "start": "2547390",
    "end": "2555930"
  },
  {
    "text": "visualization we do have some some pretty good tools you know the genome browser from UCSC is one of the",
    "start": "2555990",
    "end": "2562540"
  },
  {
    "text": "granddaddy tools it's been out for 12 years if not more and it's still highly",
    "start": "2562540",
    "end": "2567940"
  },
  {
    "text": "developed highly used by the community the rest of this though is really work that's under development predictive",
    "start": "2567940",
    "end": "2573760"
  },
  {
    "text": "modeling we're not there yet for genomics and life sciences I'll be frank complex api's there's there's a few of",
    "start": "2573760",
    "end": "2580810"
  },
  {
    "text": "them but there's not really a good sense of what what is capable and doable",
    "start": "2580810",
    "end": "2586200"
  },
  {
    "text": "consumer applications you're starting to get different",
    "start": "2586200",
    "end": "2592770"
  },
  {
    "text": "different applications of genomics in the consumer space but how useful that is is still an open",
    "start": "2592770",
    "end": "2600040"
  },
  {
    "text": "question data driven journals like can we can we create public policy based off of",
    "start": "2600040",
    "end": "2606040"
  },
  {
    "text": "genomic data that's coming in for instance if you're doing ubiquitous sequencing of sewers across the city can",
    "start": "2606040",
    "end": "2612040"
  },
  {
    "text": "you spot food deserts by sequencing the poo maybe I think that would be pretty",
    "start": "2612040",
    "end": "2617560"
  },
  {
    "text": "cool free idea so you know Amazon s3 I'll refer to the",
    "start": "2617560",
    "end": "2627190"
  },
  {
    "start": "2622000",
    "end": "2622000"
  },
  {
    "text": "previous talk LFS 303 about creating data Lake architectures for science",
    "start": "2627190",
    "end": "2633850"
  },
  {
    "text": "space but we see s3 is a critical component because you can start storing",
    "start": "2633850",
    "end": "2639010"
  },
  {
    "text": "not only genomic data but data that's coming out of your electronic medical record data that's coming out of your",
    "start": "2639010",
    "end": "2644920"
  },
  {
    "text": "family history from our smart connected devices from more regulated workloads",
    "start": "2644920",
    "end": "2650470"
  },
  {
    "text": "can you do passive monitoring of people integrate that all within a common data",
    "start": "2650470",
    "end": "2656440"
  },
  {
    "text": "platform and then start bringing in to fools from either machine learning or",
    "start": "2656440",
    "end": "2662710"
  },
  {
    "text": "classical statistics to develop new models of ongoing care and providing",
    "start": "2662710",
    "end": "2668770"
  },
  {
    "text": "care and obviously visualization is a good place if you did miss the last",
    "start": "2668770",
    "end": "2674619"
  },
  {
    "text": "session 303 life science 303 it's actually going to be repeated tomorrow at 3:30 so look for it in your in your",
    "start": "2674619",
    "end": "2681670"
  },
  {
    "text": "in your schedule as I mentioned the public data sets program does have the",
    "start": "2681670",
    "end": "2687430"
  },
  {
    "start": "2684000",
    "end": "2684000"
  },
  {
    "text": "Cancer Genome Atlas and the ICGC cancer data sets are two separate petabyte",
    "start": "2687430",
    "end": "2693070"
  },
  {
    "text": "scale data sets we're working with the National Cancer Institute to make more data available we also have some",
    "start": "2693070",
    "end": "2698829"
  },
  {
    "text": "reference public data so if you're an algorithm developer and you don't TCGA and icgc are both controlled axes right",
    "start": "2698829",
    "end": "2705310"
  },
  {
    "text": "they are derived from humans they are consented for research but obviously we",
    "start": "2705310",
    "end": "2710589"
  },
  {
    "text": "don't want folks to do bad things with that data so there's there's a minimal blocker to make sure that you're not",
    "start": "2710589",
    "end": "2716770"
  },
  {
    "text": "you're not a nefarious person but we do have reference data sets that are out there in a public domain",
    "start": "2716770",
    "end": "2722410"
  },
  {
    "text": "including the 1,000 genomes project which is has actually 2,500 genomes so it's kind of misnomer at genome in a",
    "start": "2722410",
    "end": "2730030"
  },
  {
    "text": "bottle as well as a human micro microbiome project and and",
    "start": "2730030",
    "end": "2735160"
  },
  {
    "text": "also getting into the crop sciences as part of the public datasets program we",
    "start": "2735160",
    "end": "2741400"
  },
  {
    "text": "also have Earth Sciences and and other data types for internet and social sciences like tax forms so if that's",
    "start": "2741400",
    "end": "2749470"
  },
  {
    "text": "your thing but let's",
    "start": "2749470",
    "end": "2754650"
  },
  {
    "text": "that's the picture with data that's picture with open data as platform we saw that providing that data on on",
    "start": "2754650",
    "end": "2762099"
  },
  {
    "text": "scalable systems like AWS does lead to different architectures than your",
    "start": "2762099",
    "end": "2768010"
  },
  {
    "text": "traditional HPC cluster that has a shared file system a scheduler a fixed",
    "start": "2768010",
    "end": "2773680"
  },
  {
    "text": "size resource that you have to contend with other users we saw over the years",
    "start": "2773680",
    "end": "2781500"
  },
  {
    "text": "refinement of the computational model to discrete applications and workflows that",
    "start": "2781500",
    "end": "2787569"
  },
  {
    "text": "distribute processes on a per job basis so so what else is out there what is the",
    "start": "2787569",
    "end": "2792880"
  },
  {
    "text": "future for bioinformatic and life sciences we're and I'd like to discuss a",
    "start": "2792880",
    "end": "2798290"
  },
  {
    "text": "little bit today about serverless science again I'll refer to a different session earlier today oh that goes into",
    "start": "2798290",
    "end": "2804110"
  },
  {
    "text": "much more depth about using lambda 4 for control systems that's the life sciences",
    "start": "2804110",
    "end": "2809510"
  },
  {
    "text": "301 you can catch it on youtube but today we're going to talk about a",
    "start": "2809510",
    "end": "2815420"
  },
  {
    "text": "few other ones so this is a little bit slightly out of",
    "start": "2815420",
    "end": "2821090"
  },
  {
    "start": "2818000",
    "end": "2818000"
  },
  {
    "text": "context I don't have enough time to really delve into the details of AWS lambda which is our service event-driven",
    "start": "2821090",
    "end": "2827960"
  },
  {
    "text": "compute service but the gist is you write a piece of functionality a very",
    "start": "2827960",
    "end": "2835430"
  },
  {
    "text": "small piece of functionality and don't have to manage the servers to run that piece of functionality it's a micro",
    "start": "2835430",
    "end": "2842090"
  },
  {
    "text": "service and AWS itself will handle the continuous scaling of that resource for",
    "start": "2842090",
    "end": "2848210"
  },
  {
    "text": "you and bill you at sub-second metering the",
    "start": "2848210",
    "end": "2853750"
  },
  {
    "text": "key scenarios for AWS lamda are data processing where you've got stateless",
    "start": "2853750",
    "end": "2859880"
  },
  {
    "start": "2855000",
    "end": "2855000"
  },
  {
    "text": "processing of discrete data as it's coming in you have back-end development systems so being able to execute",
    "start": "2859880",
    "end": "2866900"
  },
  {
    "text": "server-side back-end processes across the platform and you also have control",
    "start": "2866900",
    "end": "2872090"
  },
  {
    "text": "systems that react to data as the system change right so what does that look like for bioinformatics",
    "start": "2872090",
    "end": "2879070"
  },
  {
    "start": "2878000",
    "end": "2878000"
  },
  {
    "text": "we have this on github our AWS labs site we do have a cloud formation template",
    "start": "2879070",
    "end": "2886610"
  },
  {
    "text": "and a walkthrough that shows you how to do image thumbnail processing based off",
    "start": "2886610",
    "end": "2894050"
  },
  {
    "text": "of data that goes into s3 so you we have confirmation templates that you send in an image that image will trigger a SNS",
    "start": "2894050",
    "end": "2902960"
  },
  {
    "text": "event and we have lambda functions listening for that SNS event that will create a thumbnail and put that",
    "start": "2902960",
    "end": "2908030"
  },
  {
    "text": "thumbnail back in s3 and I said well that's actually pretty close to how genomics processes work with with Oxford",
    "start": "2908030",
    "end": "2915050"
  },
  {
    "text": "and nano right Oxford nano being a small thumb drive sequencer you get data as",
    "start": "2915050",
    "end": "2921470"
  },
  {
    "text": "it's coming off of the instrument in real time and you have to call the DNA bases that you saw from event data so",
    "start": "2921470",
    "end": "2927980"
  },
  {
    "text": "electrical signals there's an open source caller out of Jared symptoms lab",
    "start": "2927980",
    "end": "2933230"
  },
  {
    "text": "called nano call and so I took the cloud formation script I forked a repository I",
    "start": "2933230",
    "end": "2938720"
  },
  {
    "text": "the hardest thing about this was actually statically compiling Nano calls so I could wrap it up in a zip and send",
    "start": "2938720",
    "end": "2944720"
  },
  {
    "text": "it off to lambda but it actually works and it took a few minutes of work and now you have a resilient scalable system",
    "start": "2944720",
    "end": "2951560"
  },
  {
    "text": "for dealing with real time genomic data again if you have an air filter that's",
    "start": "2951560",
    "end": "2957230"
  },
  {
    "text": "looking for specific pathogens that might be brought into an environment you",
    "start": "2957230",
    "end": "2963050"
  },
  {
    "text": "actually have a pretty simple scalable system to address those needs another case study so we're looking at",
    "start": "2963050",
    "end": "2970310"
  },
  {
    "text": "back in data processing station X's gene pool platform this is a case study that we recently released a few months ago I",
    "start": "2970310",
    "end": "2978109"
  },
  {
    "text": "believe that they're at the converged health booth so if you want to talk to station X please feel to reach out to",
    "start": "2978109",
    "end": "2983720"
  },
  {
    "text": "them but they essentially created a they wrapped a our function",
    "start": "2983720",
    "end": "2991220"
  },
  {
    "text": "and statically compiled are within lambda so this is a rather large program but they were able to fit that within a",
    "start": "2991220",
    "end": "2998319"
  },
  {
    "text": "lambda function and their U and use their control plane which is pH high",
    "start": "2998319",
    "end": "3003880"
  },
  {
    "text": "compliant segment out the data that's not pH high send it in through to lambda",
    "start": "3003880",
    "end": "3009310"
  },
  {
    "text": "and then get the results back and now they don't have to run a fleet of dedicated servers which which there is a",
    "start": "3009310",
    "end": "3015040"
  },
  {
    "text": "much larger cost and much less performant than having it scale on an as-needed basis so this is a pretty",
    "start": "3015040",
    "end": "3022869"
  },
  {
    "text": "interesting one you can read up more about it but it and it shows you hybrid",
    "start": "3022869",
    "end": "3028150"
  },
  {
    "text": "architectures of still having ec2 environment as your main data platform with a service",
    "start": "3028150",
    "end": "3035950"
  },
  {
    "text": "environment that is handling data at scale so pretty interesting and if you want to go full control system all",
    "start": "3035950",
    "end": "3043089"
  },
  {
    "text": "lambda group 0 out of Australia just released a",
    "start": "3043089",
    "end": "3049270"
  },
  {
    "text": "complete service data processing for crisper cast 9 searches right and",
    "start": "3049270",
    "end": "3056040"
  },
  {
    "text": "what's great about this is is a that there is literally no servers in this it's completely a service environment",
    "start": "3056040",
    "end": "3062740"
  },
  {
    "text": "and they did some really interesting things because they're doing whole genome scans and obviously lambda is a",
    "start": "3062740",
    "end": "3069349"
  },
  {
    "text": "resource-constrained algorithm so you have to segment the search database into",
    "start": "3069349",
    "end": "3074420"
  },
  {
    "text": "lots of different lambda functions based off of the parameters that you get in so the way",
    "start": "3074420",
    "end": "3080210"
  },
  {
    "text": "they did it is they took a candidate CRISPR site they did an immediate search for the template scan and gave it a",
    "start": "3080210",
    "end": "3087680"
  },
  {
    "text": "score and then return that result to the user and in the back end they said okay I am going to create a lambda function",
    "start": "3087680",
    "end": "3095570"
  },
  {
    "text": "to create SNS topics to do the full genome scan so we get more resilient",
    "start": "3095570",
    "end": "3101540"
  },
  {
    "text": "information on that CRISPR search that would give you off target effects or other",
    "start": "3101540",
    "end": "3107349"
  },
  {
    "text": "other likely sites that might be better right and then all of those results go",
    "start": "3107349",
    "end": "3113030"
  },
  {
    "text": "back in - back into dynamodb and we have a polling process within this static s3",
    "start": "3113030",
    "end": "3118310"
  },
  {
    "text": "site that will update your result as these functions finish so very very cool",
    "start": "3118310",
    "end": "3124099"
  },
  {
    "text": "architecture I think we're going to see a lot of different architectures using lambda within bioinformatics and a",
    "start": "3124099",
    "end": "3132849"
  },
  {
    "text": "pretty pretty cool innovative work so that's all I have for you today I want",
    "start": "3132849",
    "end": "3138050"
  },
  {
    "text": "to thank Brian I want to thank the entire research international community for making that resource available if",
    "start": "3138050",
    "end": "3143359"
  },
  {
    "text": "you guys have any questions for us if you want to dive deeper in any of these subjects please save your questions",
    "start": "3143359",
    "end": "3149150"
  },
  {
    "text": "we'll be in the back of the room and have a good reinvent thank you",
    "start": "3149150",
    "end": "3154930"
  },
  {
    "text": "[Applause]",
    "start": "3154930",
    "end": "3161160"
  }
]