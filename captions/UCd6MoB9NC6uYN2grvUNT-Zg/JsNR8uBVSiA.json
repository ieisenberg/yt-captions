[
  {
    "start": "0",
    "end": "29000"
  },
  {
    "text": "hello everyone welcome and thank you for attending this session today",
    "start": "6589",
    "end": "11759"
  },
  {
    "text": "you cannot hear us oh cool my name is project Adam Lee I'm the",
    "start": "11759",
    "end": "17220"
  },
  {
    "text": "principal product manager for AWS glue and with me I have our pencha he is the",
    "start": "17220",
    "end": "23369"
  },
  {
    "text": "engineering manager for data at Robin Hood so we have a packed agenda today we",
    "start": "23369",
    "end": "31769"
  },
  {
    "start": "29000",
    "end": "46000"
  },
  {
    "text": "will be talking about building data leaks I will be going over how glue plays a part in building data leaks on",
    "start": "31769",
    "end": "39149"
  },
  {
    "text": "AWS and then arpan is going to talk about how Robin Hood built a data Lake on AWS but before we do that let's do a",
    "start": "39149",
    "end": "48809"
  },
  {
    "start": "46000",
    "end": "99000"
  },
  {
    "text": "quick refresher on AWS Glu AWS glue is a service data catalog and ETL service it",
    "start": "48809",
    "end": "56899"
  },
  {
    "text": "provides a data catalog which is a central metadata repository that is hive meta store compatible the data catalog",
    "start": "56899",
    "end": "64978"
  },
  {
    "text": "helps you automatically discover the metadata associated with your data and then add that as table definitions to a",
    "start": "64979",
    "end": "72510"
  },
  {
    "text": "central metadata repository we also have ETL jobs in Glu which are essentially",
    "start": "72510",
    "end": "80460"
  },
  {
    "text": "jobs that run on a server less spark environment AWS glue helps you",
    "start": "80460",
    "end": "86430"
  },
  {
    "text": "automatically generate scripts either in Python or Scala and these scripts are entirely customizable you can also bring",
    "start": "86430",
    "end": "93270"
  },
  {
    "text": "in your own code and run it on the server les environment that glue provides these are some of our customers",
    "start": "93270",
    "end": "102390"
  },
  {
    "start": "99000",
    "end": "128000"
  },
  {
    "text": "we have customers who are migrating their data of warehouses to Amazon",
    "start": "102390",
    "end": "107790"
  },
  {
    "text": "redshift using AWS glue we also have customers who are building data leaks on",
    "start": "107790",
    "end": "113630"
  },
  {
    "text": "AWS using glue and then we have customers who are using the ETL jobs in",
    "start": "113630",
    "end": "119189"
  },
  {
    "text": "glue to bring data from various different sources together normalize it pivot it and make it available for their",
    "start": "119189",
    "end": "125880"
  },
  {
    "text": "data science projects this is what you're saying as you can see from these",
    "start": "125880",
    "end": "133230"
  },
  {
    "start": "128000",
    "end": "149000"
  },
  {
    "text": "Co glue has been increasingly used in data Lake architectures and customers are",
    "start": "133230",
    "end": "139410"
  },
  {
    "text": "finding value when they use AWS glue especially around reducing the",
    "start": "139410",
    "end": "145140"
  },
  {
    "text": "operational overhead that they have and we've been busy over the last year",
    "start": "145140",
    "end": "151980"
  },
  {
    "start": "149000",
    "end": "184000"
  },
  {
    "text": "we've added some notable features like the metadata access policies to the AWS",
    "start": "151980",
    "end": "158280"
  },
  {
    "text": "Glu data catalog integration with Amazon Sage Maker a connector to DynamoDB to",
    "start": "158280",
    "end": "165180"
  },
  {
    "text": "crawl the data in dynamodb as well as to use DynamoDB as a source for your ETL",
    "start": "165180",
    "end": "171150"
  },
  {
    "text": "jobs and enter an encryption we've also made the service available in more",
    "start": "171150",
    "end": "177090"
  },
  {
    "text": "regions AWS glue is now available in 12 regions worldwide so now let's take a",
    "start": "177090",
    "end": "186600"
  },
  {
    "start": "184000",
    "end": "219000"
  },
  {
    "text": "look at how to build a data Lake on AWS so Dana Lake is a central repository",
    "start": "186600",
    "end": "194220"
  },
  {
    "text": "that allows you to store semi structured unstructured as well as structured data",
    "start": "194220",
    "end": "200130"
  },
  {
    "text": "are coming from relational sources at massive scale not only does the data",
    "start": "200130",
    "end": "205440"
  },
  {
    "text": "Lake allow you to store this data but it also supports a variety of analytics",
    "start": "205440",
    "end": "210990"
  },
  {
    "text": "tools that help you derive insights from this data and data leaks provide this at",
    "start": "210990",
    "end": "216630"
  },
  {
    "text": "low cost so quick with a quick show of hands how many of you here are building",
    "start": "216630",
    "end": "223170"
  },
  {
    "start": "219000",
    "end": "273000"
  },
  {
    "text": "a data Lake or have an operational data Lake already most of you so this will be",
    "start": "223170",
    "end": "229950"
  },
  {
    "text": "very familiar these are some of the typical steps that our customers take when they are building a data leak they",
    "start": "229950",
    "end": "237000"
  },
  {
    "text": "have to set up their storage a storage that can support a variety of data formats and massive scale they have to",
    "start": "237000",
    "end": "245580"
  },
  {
    "text": "move data from their data sources into this storage layer they then have to clean transform and catalog this data so",
    "start": "245580",
    "end": "253290"
  },
  {
    "text": "it's discoverable and make sure that this data is accessed in a way that's",
    "start": "253290",
    "end": "258600"
  },
  {
    "text": "compliant with their security and compliance policies and finally with the",
    "start": "258600",
    "end": "263850"
  },
  {
    "text": "data leak you want to make sure that the data is widely available to all the users in your organization across a variety of use cases so let's",
    "start": "263850",
    "end": "272639"
  },
  {
    "text": "step through these steps s3 forms a",
    "start": "272639",
    "end": "278460"
  },
  {
    "start": "273000",
    "end": "310000"
  },
  {
    "text": "great storage layer for your data leaks s3 is an object store built to store and",
    "start": "278460",
    "end": "284729"
  },
  {
    "text": "retrieve any amounts of data from anywhere it is designed for eleven ninth",
    "start": "284729",
    "end": "290729"
  },
  {
    "text": "of durability and the security and has the most comprehensive security and",
    "start": "290729",
    "end": "295949"
  },
  {
    "text": "compliance capabilities s3 also supports a wide variety of tools to analyze data",
    "start": "295949",
    "end": "302520"
  },
  {
    "text": "address so that you don't have to move data or duplicate data the second step",
    "start": "302520",
    "end": "311279"
  },
  {
    "start": "310000",
    "end": "403000"
  },
  {
    "text": "is moving data to your s3 data like storage AWS provides the most amount of",
    "start": "311279",
    "end": "321779"
  },
  {
    "text": "ways in which you can move data to the data like storage in s3 you can move",
    "start": "321779",
    "end": "329039"
  },
  {
    "text": "data from your on-premise sources using services like AWS Direct Connect AWS",
    "start": "329039",
    "end": "335250"
  },
  {
    "text": "snowball AWS zero-based Migration Service and AWS Storage Gateway",
    "start": "335250",
    "end": "341240"
  },
  {
    "text": "you can also ingest and load data from streaming sources using a SS Kinesis",
    "start": "341240",
    "end": "348050"
  },
  {
    "text": "firehose Kinesis data streams and Kinesis data of videos video streams and",
    "start": "348050",
    "end": "355639"
  },
  {
    "text": "you can load IRT data using the AWS IOT core but moving data and storing data is",
    "start": "355639",
    "end": "367500"
  },
  {
    "text": "not enough Gartner defines start data as the information assets that organize",
    "start": "367500",
    "end": "374839"
  },
  {
    "text": "organizations collect process and store during regular business activities but",
    "start": "374839",
    "end": "381870"
  },
  {
    "text": "generally fail to use for other purposes like analytics with the amount of data",
    "start": "381870",
    "end": "387719"
  },
  {
    "text": "that organizations are storing today and with the increase of machine generated",
    "start": "387719",
    "end": "392789"
  },
  {
    "text": "data log data the amount of dark data that is not discoverable and available",
    "start": "392789",
    "end": "398399"
  },
  {
    "text": "for analytics is growing exponential so let's take a look at how aw-ooh helps",
    "start": "398399",
    "end": "406450"
  },
  {
    "text": "you make your data more discoverable as you can see here Glu can help you take",
    "start": "406450",
    "end": "414190"
  },
  {
    "text": "the raw data that you have in Amazon s3 process it through various different",
    "start": "414190",
    "end": "419980"
  },
  {
    "text": "stages of curation through the staging phase where you are probably creating",
    "start": "419980",
    "end": "425410"
  },
  {
    "text": "some intermediate results to the processed and curated phase where that data is now available partitioned in a",
    "start": "425410",
    "end": "433900"
  },
  {
    "text": "format that is optimized for querying and analytics in your s3 bucket Glu also",
    "start": "433900",
    "end": "441460"
  },
  {
    "text": "provides crawlers and we'll take a deeper look at what crawlers are to crawl and automatically catalog this",
    "start": "441460",
    "end": "448090"
  },
  {
    "text": "data in a central Glu data catalog and the glue data catalog provides you a",
    "start": "448090",
    "end": "454840"
  },
  {
    "text": "view into your data leak it makes your data are more discoverable through the search that is available in the AWS Glu",
    "start": "454840",
    "end": "462100"
  },
  {
    "text": "data catalog and you can enrich the metadata that is discovered by crawlers",
    "start": "462100",
    "end": "467230"
  },
  {
    "text": "and add your own custom metadata in the blue Reiter catalog so let's take a look",
    "start": "467230",
    "end": "475150"
  },
  {
    "start": "473000",
    "end": "529000"
  },
  {
    "text": "at crawlers crawlers essentially scan the data that you give us access to",
    "start": "475150",
    "end": "481350"
  },
  {
    "text": "extract the metadata associated with that data which includes the schema for your table definite tables that are",
    "start": "481350",
    "end": "487840"
  },
  {
    "text": "defined on top of your data they also extract certain data statistics and",
    "start": "487840",
    "end": "494610"
  },
  {
    "text": "discover high style partitions for data that's stored in Amazon s3 crawlers can",
    "start": "494610",
    "end": "501490"
  },
  {
    "text": "be run on a schedule so they can keep track of any schema changes as well as pick up new partitions as they arrive",
    "start": "501490",
    "end": "508120"
  },
  {
    "text": "and they run on the server les environment that Glu provides you only",
    "start": "508120",
    "end": "513580"
  },
  {
    "text": "pay for the time that the crawler actually runs and crawlers come in",
    "start": "513580",
    "end": "518950"
  },
  {
    "text": "built-in with a set of classifiers that help them identify a wide variety of formats but you can also write your own",
    "start": "518950",
    "end": "525340"
  },
  {
    "text": "using grok expressions this is an",
    "start": "525340",
    "end": "530680"
  },
  {
    "start": "529000",
    "end": "576000"
  },
  {
    "text": "example of a table that's been added by crawlers this table points to a data set that",
    "start": "530680",
    "end": "537190"
  },
  {
    "text": "contains Amazon reviews and as you can see crawlers have identified the format",
    "start": "537190",
    "end": "543190"
  },
  {
    "text": "and added that as a classification they have also added the location of the data and they've added certain hive meta",
    "start": "543190",
    "end": "552070"
  },
  {
    "text": "store compatible properties as well as certain data statistics as table properties crawlers have also detected",
    "start": "552070",
    "end": "559270"
  },
  {
    "text": "the partitioning key and added the register the available partitions in the blue data catalog and you can add your",
    "start": "559270",
    "end": "566290"
  },
  {
    "text": "own custom properties like the one that I've added there and further enrich the metadata and make this data more",
    "start": "566290",
    "end": "573100"
  },
  {
    "text": "discoverable moving on to the ETL jobs",
    "start": "573100",
    "end": "579970"
  },
  {
    "start": "576000",
    "end": "617000"
  },
  {
    "text": "are available in AWS glue we've built a library on top of SPARC that provides",
    "start": "579970",
    "end": "586480"
  },
  {
    "text": "certain transformations that customers use to clean reduce and transform their",
    "start": "586480",
    "end": "591730"
  },
  {
    "text": "data before using something like spark sequel to run sequel base ETL on that",
    "start": "591730",
    "end": "599710"
  },
  {
    "text": "data the library has dynamic frames which are similar to data frames but more",
    "start": "599710",
    "end": "606370"
  },
  {
    "text": "optimized for ETL operations and you can always run spark without using the glue",
    "start": "606370",
    "end": "614050"
  },
  {
    "text": "ETL libraries so in the libraries we",
    "start": "614050",
    "end": "620440"
  },
  {
    "text": "also have functions that allow you to convert a data frame into a dynamic frame or a dynamic frame back to a data",
    "start": "620440",
    "end": "628090"
  },
  {
    "text": "frame so you can mix and match and use the transformations that make most sense for your use case the glue ETL library",
    "start": "628090",
    "end": "636250"
  },
  {
    "text": "also provides the connector to the catalog so you can read a table directly into a dynamic frame and start",
    "start": "636250",
    "end": "643710"
  },
  {
    "text": "transforming that data and if all of this is not enough for a use case you",
    "start": "643710",
    "end": "649540"
  },
  {
    "text": "can always write custom transformations and import those as custom libraries in the glue ETL job the glue ETL jobs can",
    "start": "649540",
    "end": "660220"
  },
  {
    "text": "keep track of the data that it has already processed with job bookmarks so",
    "start": "660220",
    "end": "666790"
  },
  {
    "text": "if you have the job bookmark enabled on a source in your ETL script the job can keep track of what",
    "start": "666790",
    "end": "673180"
  },
  {
    "text": "data it has already processed and only process the deltas or the incremental data you can enable the bookmark at",
    "start": "673180",
    "end": "680500"
  },
  {
    "text": "which point glue will only process the deltas you can disable the bookmark at which point you will process the entire",
    "start": "680500",
    "end": "686800"
  },
  {
    "text": "data set or you can temporarily pause the bookmark which is often useful if",
    "start": "686800",
    "end": "692020"
  },
  {
    "text": "you're doing some code modifications and you want to just test it without advancing the bookmark you can also",
    "start": "692020",
    "end": "699580"
  },
  {
    "text": "reuse a double use glue ETL jobs by parameterizing the code that is part of",
    "start": "699580",
    "end": "707200"
  },
  {
    "text": "your ETL jobs the code is stored in your s3 buckets you can use the same script",
    "start": "707200",
    "end": "712779"
  },
  {
    "text": "across multiple AWS glue jobs and you can also trigger the glue job the same",
    "start": "712779",
    "end": "718750"
  },
  {
    "text": "glue job across multiple triggers and across multiple conditions so this is",
    "start": "718750",
    "end": "727630"
  },
  {
    "start": "726000",
    "end": "798000"
  },
  {
    "text": "how you would use triggers and glue triggers in glue are entities that help",
    "start": "727630",
    "end": "733120"
  },
  {
    "text": "you stitch jobs together so jobs do the work and triggers help you stitch",
    "start": "733120",
    "end": "739390"
  },
  {
    "text": "different jobs together to build complex workflows we have three types of",
    "start": "739390",
    "end": "745420"
  },
  {
    "text": "triggers in glue we have a schedule based trigger where you can trigger a job based on the time of the day we have",
    "start": "745420",
    "end": "753850"
  },
  {
    "text": "job event based triggers which can watch for certain events from one or more jobs",
    "start": "753850",
    "end": "759730"
  },
  {
    "text": "and then trigger downstream jobs and we also have on-demand triggers which can",
    "start": "759730",
    "end": "766690"
  },
  {
    "text": "be triggered through AWS lambda on notifications such such as the s3 put",
    "start": "766690",
    "end": "772990"
  },
  {
    "text": "notifications the triggers help you mix and match conditions using the controls",
    "start": "772990",
    "end": "779110"
  },
  {
    "text": "that we have in glue so you can trigger when all of the conditions are met or",
    "start": "779110",
    "end": "784839"
  },
  {
    "text": "any of the conditions are met and you can also configure your jobs to be retried and have a job timeout to guard",
    "start": "784839",
    "end": "793150"
  },
  {
    "text": "against any runaway jobs and all the job metrics which are",
    "start": "793150",
    "end": "802570"
  },
  {
    "start": "798000",
    "end": "845000"
  },
  {
    "text": "sparked metrics are pushed to Amazon Cloud Watch as well as available through",
    "start": "802570",
    "end": "808570"
  },
  {
    "text": "the AWS blue console these metrics allow you to debug out of memory exceptions",
    "start": "808570",
    "end": "814260"
  },
  {
    "text": "they also allow you to identify data SKUs and help you plan the dpu capacity",
    "start": "814260",
    "end": "820570"
  },
  {
    "text": "required for your jobs better you can set up alarms for certain job conditions",
    "start": "820570",
    "end": "825810"
  },
  {
    "text": "in cloud watch and then take subsequent action based on these metrics so now we",
    "start": "825810",
    "end": "835510"
  },
  {
    "text": "come to the fourth step which is securing your data once the data is",
    "start": "835510",
    "end": "841090"
  },
  {
    "text": "prepped and discoverable in your data leak so by default all s3 objects are",
    "start": "841090",
    "end": "849370"
  },
  {
    "text": "private you can grant access to the data in s3 using a combination of resource",
    "start": "849370",
    "end": "854950"
  },
  {
    "text": "based policies such as the s3 bucket policies and i.m policies the IAM user",
    "start": "854950",
    "end": "861070"
  },
  {
    "text": "policies you can also use AWS kms to encrypt data both in transit as well as",
    "start": "861070",
    "end": "868860"
  },
  {
    "text": "encrypted at rest and you can use object tagging and enable tag based policies to",
    "start": "868860",
    "end": "877240"
  },
  {
    "text": "get access to the data that you have stored in s3 but securing data is only part of the story you should also be",
    "start": "877240",
    "end": "885670"
  },
  {
    "start": "884000",
    "end": "922000"
  },
  {
    "text": "securing the metadata that points to this data and AWS glue allows you to add",
    "start": "885670",
    "end": "893500"
  },
  {
    "text": "I am base user policies for scoping down access to this metadata or you can also",
    "start": "893500",
    "end": "901540"
  },
  {
    "text": "use the resource base policies that are managed by AWS glue which are similar to the s3 bucket policies to scope down the",
    "start": "901540",
    "end": "909460"
  },
  {
    "text": "access to this metadata you have one policy per catalog and you can also",
    "start": "909460",
    "end": "916240"
  },
  {
    "text": "enable cross account access using resource based policies in AWS glue",
    "start": "916240",
    "end": "922470"
  },
  {
    "start": "922000",
    "end": "947000"
  },
  {
    "text": "these are the resources that you can define permissions on you can define",
    "start": "922950",
    "end": "928510"
  },
  {
    "text": "permissions at the catalog levels data base or table level you can also define permissions to access the connections",
    "start": "928510",
    "end": "936160"
  },
  {
    "text": "that you have defined the JDBC connections and glue as well as the functions and here are some examples of",
    "start": "936160",
    "end": "941200"
  },
  {
    "text": "what a resource Arn would look like for these resources so the final step is",
    "start": "941200",
    "end": "950680"
  },
  {
    "text": "making sure that your data is easily accessible to a wide range of applications in your organization as",
    "start": "950680",
    "end": "957100"
  },
  {
    "text": "well as to different teams and users so with the glue data catalog and the",
    "start": "957100",
    "end": "962650"
  },
  {
    "start": "960000",
    "end": "1008000"
  },
  {
    "text": "integration that it has with different services your data is easily accessible",
    "start": "962650",
    "end": "969810"
  },
  {
    "text": "through multiple different services glue data catalog integrates with Amazon Athena out of box you can also you",
    "start": "969810",
    "end": "977620"
  },
  {
    "text": "choose to use the glue data catalog as a meta store with Amazon redshift spectrum",
    "start": "977620",
    "end": "983680"
  },
  {
    "text": "and you can use the glue data catalog as",
    "start": "983680",
    "end": "988870"
  },
  {
    "text": "an as a replacement for your external hive meta store with Amazon EMR with the",
    "start": "988870",
    "end": "996100"
  },
  {
    "text": "cross account access you can make the same view into your data available",
    "start": "996100",
    "end": "1001140"
  },
  {
    "text": "across multiple different AWS accounts and multiple different AWS users so",
    "start": "1001140",
    "end": "1009060"
  },
  {
    "start": "1008000",
    "end": "1057000"
  },
  {
    "text": "let's put it all together you have your data coming from various different sources and AWS provides a wide range of",
    "start": "1009060",
    "end": "1017280"
  },
  {
    "text": "services to move this data in its raw form into Amazon s3 you can then use",
    "start": "1017280",
    "end": "1023100"
  },
  {
    "text": "glue the ETL jobs in glue to process this data and make it available for",
    "start": "1023100",
    "end": "1029329"
  },
  {
    "text": "analytics and you can catalog this data at various different stages using the",
    "start": "1029329",
    "end": "1035430"
  },
  {
    "text": "glue crawlers into a central metadata repository provided by the AWS glue data",
    "start": "1035430",
    "end": "1042150"
  },
  {
    "text": "catalog and that data then becomes readily available through a variety of",
    "start": "1042150",
    "end": "1048449"
  },
  {
    "text": "analytic services for your four-armed for your analytics needs so with that",
    "start": "1048449",
    "end": "1056490"
  },
  {
    "text": "I'm gonna hand it to arpan to talk about how Robin Hood build their data leak",
    "start": "1056490",
    "end": "1061920"
  },
  {
    "text": "thank you projector [Applause]",
    "start": "1061920",
    "end": "1070669"
  },
  {
    "text": "so for those who don't know Robin Hood is a commission-free platform that",
    "start": "1070669",
    "end": "1075899"
  },
  {
    "text": "allows users to trade crypto currencies stocks ETFs and options very easily our",
    "start": "1075899",
    "end": "1084539"
  },
  {
    "text": "mission is to democratize America's financial system and making data-driven decisions is a big part of that today I",
    "start": "1084539",
    "end": "1091409"
  },
  {
    "text": "want to talk about why Robin Hood needed a data Lake how we thought about it how",
    "start": "1091409",
    "end": "1097710"
  },
  {
    "text": "we ended up building it our architecture that we used and lessons and ideas that",
    "start": "1097710",
    "end": "1102960"
  },
  {
    "text": "you can take with you hopefully and build your own data Lakes on top of AWS",
    "start": "1102960",
    "end": "1109460"
  },
  {
    "start": "1109000",
    "end": "1116000"
  },
  {
    "text": "so to start I want to talk a little bit about the world and what it looked like before Robin it had a data link this",
    "start": "1109460",
    "end": "1117419"
  },
  {
    "start": "1116000",
    "end": "1174000"
  },
  {
    "text": "might be a problem that many of you guys are familiar with data at Robin Hood lived in silos that been very easily",
    "start": "1117419",
    "end": "1123480"
  },
  {
    "text": "talked to each other many people call these data ponds we had transactional",
    "start": "1123480",
    "end": "1128489"
  },
  {
    "text": "data in Amazon RDS we had analytics data in Amazon redshift we had files in Astri",
    "start": "1128489",
    "end": "1134840"
  },
  {
    "text": "streaming data in Kafka documents and elasticsearch and a bunch of third party",
    "start": "1134840",
    "end": "1140309"
  },
  {
    "text": "data sources as well did a scientists product managers anyone who really",
    "start": "1140309",
    "end": "1146309"
  },
  {
    "text": "needed answers from this data needed to learn the dialects for each of these different sources independently and try",
    "start": "1146309",
    "end": "1154769"
  },
  {
    "text": "to figure out what these sources were talking about through these dialects",
    "start": "1154769",
    "end": "1160129"
  },
  {
    "text": "even after someone had actually figured out these dialects if they really needed to join across these data sets they'd",
    "start": "1160129",
    "end": "1165960"
  },
  {
    "text": "have to put all that data in a central place and then go ahead and make those transformations or computations that",
    "start": "1165960",
    "end": "1171570"
  },
  {
    "text": "they needed to get their insights another problem we were facing was we",
    "start": "1171570",
    "end": "1177720"
  },
  {
    "start": "1174000",
    "end": "1232000"
  },
  {
    "text": "really needed to scale our compute and storage independently for example we had",
    "start": "1177720",
    "end": "1184679"
  },
  {
    "text": "all our analytics data in redshift our data started growing really rapidly and very soon we were receiving terabytes of",
    "start": "1184679",
    "end": "1191730"
  },
  {
    "text": "data every single day meanwhile the use cases for this data weren't scaling at a",
    "start": "1191730",
    "end": "1197399"
  },
  {
    "text": "similar and as a result we had the need to constantly upscale our redshift instances as our data was scaling not as",
    "start": "1197399",
    "end": "1204430"
  },
  {
    "text": "the use of that data was actually scaling this problem was not just limited to redshift we were seeing this",
    "start": "1204430",
    "end": "1210760"
  },
  {
    "text": "across all the other places that we had data as well on RDS instances in our elastic search where ever we really",
    "start": "1210760",
    "end": "1216550"
  },
  {
    "text": "needed to start querying our data the rate of data growth a traveler was much faster than our use and as a result we",
    "start": "1216550",
    "end": "1221800"
  },
  {
    "text": "were seeing ourselves scale this compute as well this was becoming really really expensive and we really needed to have",
    "start": "1221800",
    "end": "1228310"
  },
  {
    "text": "both those levers available to us finally it became harder and harder to",
    "start": "1228310",
    "end": "1236590"
  },
  {
    "text": "scale the number of users and govern their usage in in a coherent manner as",
    "start": "1236590",
    "end": "1242320"
  },
  {
    "text": "people were accessing different data stores and accessing elasticsearch on one day or redshift on another we had to",
    "start": "1242320",
    "end": "1249550"
  },
  {
    "text": "maintain separate access patterns ACLs or permission groups for each of these the overhead of managing all of these",
    "start": "1249550",
    "end": "1255190"
  },
  {
    "text": "and keeping them in think was getting harder and harder and in a small company like Robin Hood this was a substantial",
    "start": "1255190",
    "end": "1261460"
  },
  {
    "text": "overhead as we were adding more employees and more users on our all our data in addition guaranteeing data",
    "start": "1261460",
    "end": "1267850"
  },
  {
    "text": "quality was becoming more and more difficult since we didn't have a central place where schema evolution was being tracked since we didn't have a central",
    "start": "1267850",
    "end": "1275560"
  },
  {
    "text": "repository of all the different transformations that had occurred on the data it was very hard for someone to know what the data lifecycle had been",
    "start": "1275560",
    "end": "1282640"
  },
  {
    "text": "when they were actually using it so as a result of this close to a year ago we",
    "start": "1282640",
    "end": "1289150"
  },
  {
    "start": "1285000",
    "end": "1306000"
  },
  {
    "text": "started thinking about embarking on a journey to build our data Lake as Robin Hood is extremely lean as a company we",
    "start": "1289150",
    "end": "1296080"
  },
  {
    "text": "wanted to make sure that we were doing this very methodically in a manner that would allow us to build out a solution",
    "start": "1296080",
    "end": "1301330"
  },
  {
    "text": "that would benefit the customers within the company at the same time do this very quickly so this is the architecture",
    "start": "1301330",
    "end": "1307930"
  },
  {
    "start": "1306000",
    "end": "1475000"
  },
  {
    "text": "that we have at Robin Hood we have six different components and I can walk you",
    "start": "1307930",
    "end": "1313750"
  },
  {
    "text": "through each of each and every one of these first we have the ingestion layer we have variety of different sources as",
    "start": "1313750",
    "end": "1320320"
  },
  {
    "text": "I mentioned earlier that bring data into our system third party sources",
    "start": "1320320",
    "end": "1325570"
  },
  {
    "text": "transactional sources analytics data we use a variety of batch and streaming",
    "start": "1325570",
    "end": "1331100"
  },
  {
    "text": "close to get this data into our storage layer which happens to be a three we use",
    "start": "1331100",
    "end": "1336289"
  },
  {
    "text": "s3 for a lot of the reasons that project I mentioned earlier you know it offers us high durability it's HDFS compatible",
    "start": "1336289",
    "end": "1343160"
  },
  {
    "text": "it offers bucket versioning a lot of really nice features that make it very great for having a variety of data that",
    "start": "1343160",
    "end": "1351049"
  },
  {
    "text": "is extremely different in various forms documents can live in the same place as your structured data in addition to that",
    "start": "1351049",
    "end": "1358130"
  },
  {
    "text": "we decided to keep some of our data in Amazon redshift for the more analytics heavy reporting heavy workloads then we",
    "start": "1358130",
    "end": "1366320"
  },
  {
    "text": "have the processing layer we explored a bunch of different data computing frameworks and we settled down on spark",
    "start": "1366320",
    "end": "1372620"
  },
  {
    "text": "as one that is extremely easy for us to maintain its it supports both Python and",
    "start": "1372620",
    "end": "1378409"
  },
  {
    "text": "Scala and it has a bunch of different dialects for both streaming processing as well as processing of batch compute",
    "start": "1378409",
    "end": "1384020"
  },
  {
    "text": "workflows we ended up leveraging Apache airflow as our scheduler to manage these",
    "start": "1384020",
    "end": "1389630"
  },
  {
    "text": "different different jobs and leverage both glue as well as EMR to manage our",
    "start": "1389630",
    "end": "1395299"
  },
  {
    "text": "spark workflows since glue integrates very seamlessly into attina for a querying layer we leverage both",
    "start": "1395299",
    "end": "1401840"
  },
  {
    "text": "the redshift interface that we have as well as Athena and since you know we have the ability to use redshift",
    "start": "1401840",
    "end": "1407960"
  },
  {
    "text": "spectrum and other tools we can actually maintain this duality of having hot data in redshift while we have other data in",
    "start": "1407960",
    "end": "1413960"
  },
  {
    "text": "s3 that we can then leverage as needed it's not just enough however to have",
    "start": "1413960",
    "end": "1419179"
  },
  {
    "text": "your data and have it accessible you really need to make sure that the data quality is very high and for this we",
    "start": "1419179",
    "end": "1424250"
  },
  {
    "text": "built a custom validation layer and this is something that has proven to be very valuable because every time a new data",
    "start": "1424250",
    "end": "1430789"
  },
  {
    "text": "set gets operated on or we have a processing step that takes place we immediately validate to make sure",
    "start": "1430789",
    "end": "1436220"
  },
  {
    "text": "that the data quality has not degraded so this is data set level custom checks that we apply based on what the data",
    "start": "1436220",
    "end": "1442850"
  },
  {
    "text": "should look like what the qualities of this data are and what we expect to be",
    "start": "1442850",
    "end": "1448010"
  },
  {
    "text": "things that don't change finally as I said earlier we have a variety of users",
    "start": "1448010",
    "end": "1453260"
  },
  {
    "text": "who want insights from this data we want to make data to be self-sufficient and people who need answers to be able to",
    "start": "1453260",
    "end": "1459409"
  },
  {
    "text": "get them themselves and for that we have Jupiter notebooks that data scientists and people who are very proficient",
    "start": "1459409",
    "end": "1464840"
  },
  {
    "text": "and code can leverage and looker witches are bi layer that others you know in the company who are analysts or product",
    "start": "1464840",
    "end": "1472340"
  },
  {
    "text": "managers can leverage to get their own data let's dive a little bit deeper into",
    "start": "1472340",
    "end": "1477950"
  },
  {
    "start": "1475000",
    "end": "1530000"
  },
  {
    "text": "the processing layer in the processing layer we took a largely hands-off",
    "start": "1477950",
    "end": "1483350"
  },
  {
    "text": "approach where we don't manage much of the infrastructure ourselves we leverage Apache airflow to control the ETL jobs",
    "start": "1483350",
    "end": "1489890"
  },
  {
    "text": "and scheduling we leverage the glue crawlers to start figuring out what our unstructured data looks like and store",
    "start": "1489890",
    "end": "1495980"
  },
  {
    "text": "this metadata into the glue catalog where it is then versioned and allows us to know what the data changes have been",
    "start": "1495980",
    "end": "1502279"
  },
  {
    "text": "what are the new partitions that have been added and have those all in sync in one place finally we have serverless",
    "start": "1502279",
    "end": "1509120"
  },
  {
    "text": "spark by AWS glue and EMR for a really large customized workflows that both",
    "start": "1509120",
    "end": "1514940"
  },
  {
    "text": "plug in very seamlessly into this data catalog so that as we add more downstream processing steps it's very",
    "start": "1514940",
    "end": "1520730"
  },
  {
    "text": "easy for us to actually leverage that same tool and the same library and the same framework which is spark to get all",
    "start": "1520730",
    "end": "1527059"
  },
  {
    "text": "of our work done what's even better is that plugs directly into the querying",
    "start": "1527059",
    "end": "1532190"
  },
  {
    "text": "layer the AWS glue catalog as I said earlier manages the partitions for you",
    "start": "1532190",
    "end": "1537559"
  },
  {
    "text": "and updates them as needed so that when you're using redshift or redshift spectrum or presto it's very easy for",
    "start": "1537559",
    "end": "1545059"
  },
  {
    "text": "you to know what all of this data looks like and talk to it in the same sequel dialect we decided upon presto because",
    "start": "1545059",
    "end": "1552620"
  },
  {
    "text": "of its ability to join across multiple data sources and it's very efficient computation in memory for the storage",
    "start": "1552620",
    "end": "1558890"
  },
  {
    "text": "layer since presto can operate on top of s3 as well as can plug into redshift",
    "start": "1558890",
    "end": "1564190"
  },
  {
    "text": "this makes it very easy for us to run queries across multiple data sets finally as I said earlier we have hot",
    "start": "1564190",
    "end": "1571909"
  },
  {
    "text": "data in redshift and we have much cooler data that we occasionally need to access ad hoc in attina and used by a presto on",
    "start": "1571909",
    "end": "1579760"
  },
  {
    "text": "presto to manage costs we made sure to use columnar columnar data formats like",
    "start": "1579760",
    "end": "1585620"
  },
  {
    "text": "parquet as well as computing such as snappy compression to make sure that fewer files need to be scanned more data",
    "start": "1585620",
    "end": "1592250"
  },
  {
    "text": "can be loaded into memory and the queries that we are running are extremely fast redshift is already very optimized for queries",
    "start": "1592250",
    "end": "1598820"
  },
  {
    "text": "but as I said earlier the costs killed as you have to scale both compute and storage simultaneously we only keep the",
    "start": "1598820",
    "end": "1605659"
  },
  {
    "text": "more commonly used patterns or reporting workloads and redshift and rely on Athena for everything else so I want to",
    "start": "1605659",
    "end": "1615590"
  },
  {
    "text": "stop here for a second and talk a little bit about how I think you know a lot of",
    "start": "1615590",
    "end": "1620659"
  },
  {
    "text": "companies and a lot of users can actually leverage glue in similar ways and can offer a lot of value for people",
    "start": "1620659",
    "end": "1626090"
  },
  {
    "text": "who want to build data leaks with a lean team or get started with working on data leaks take a step back the key element",
    "start": "1626090",
    "end": "1634100"
  },
  {
    "start": "1631000",
    "end": "1716000"
  },
  {
    "text": "of data leaks that is very powerful is that it enables you to both keep track of raw and unstructured data as well as",
    "start": "1634100",
    "end": "1640549"
  },
  {
    "text": "structured data in the past in more traditional use cases where you just had",
    "start": "1640549",
    "end": "1645590"
  },
  {
    "text": "a data warehousing approach the schema of the data was very closely coupled with the actual usage of the data so you",
    "start": "1645590",
    "end": "1652340"
  },
  {
    "text": "have to upfront think about how all the different data patterns would be and then choose data warehousing solution",
    "start": "1652340",
    "end": "1658399"
  },
  {
    "text": "that would then suffice for all of these different data needs with the data leak you can actually keep this raw data in",
    "start": "1658399",
    "end": "1665029"
  },
  {
    "text": "the original format that it appears in in your industry and then as individual use cases appear you can then process",
    "start": "1665029",
    "end": "1671299"
  },
  {
    "text": "those for the actual use case that you care about this allows you to make sure that in the future if a new use case",
    "start": "1671299",
    "end": "1677299"
  },
  {
    "text": "emerges you still have there our data and so you can process this as you need to for that individual use case down",
    "start": "1677299",
    "end": "1682580"
  },
  {
    "text": "down the road glue actually makes this very very easy for you it seamlessly plugs into a variety of data formats it",
    "start": "1682580",
    "end": "1689240"
  },
  {
    "text": "plugs into Park a JSON CSV Project I mentioned drop patterns as well so no",
    "start": "1689240",
    "end": "1695149"
  },
  {
    "text": "matter what your data looks like glue can apply the schema that it actually",
    "start": "1695149",
    "end": "1700279"
  },
  {
    "text": "holds after the fact in addition you have the catalog that plugs in to your EMR or your spark or any other",
    "start": "1700279",
    "end": "1707110"
  },
  {
    "text": "distributed data computing library you care about and then you can use those to process those individual workloads as",
    "start": "1707110",
    "end": "1713029"
  },
  {
    "text": "needed down the road in addition going",
    "start": "1713029",
    "end": "1718399"
  },
  {
    "start": "1716000",
    "end": "1778000"
  },
  {
    "text": "server list allows you to focus on building the entire data Lake at Robinhood over the last six or seven",
    "start": "1718399",
    "end": "1724009"
  },
  {
    "text": "months was built by just three engineers we now have over 15 data sets that shipped in the last few months and over",
    "start": "1724009",
    "end": "1731090"
  },
  {
    "text": "a petabyte of data that's in in compressed format in the beginning",
    "start": "1731090",
    "end": "1736309"
  },
  {
    "text": "when you're starting out building data licks it can seem a little intimidating to take all of this data that you have",
    "start": "1736309",
    "end": "1741350"
  },
  {
    "text": "put into one place and then provide so many use cases on top of that glue actually minimizes the operational",
    "start": "1741350",
    "end": "1747590"
  },
  {
    "text": "overhead for you you don't need to tune clusters you don't need DevOps resources off the bat your engineers can focus on learning",
    "start": "1747590",
    "end": "1754340"
  },
  {
    "text": "spark or whatever framework that you care about and finally in the beginning when you're building data lakes it's",
    "start": "1754340",
    "end": "1760519"
  },
  {
    "text": "really normal to see spiky data patterns you add a new data set the amount of data processing you do suddenly spikes",
    "start": "1760519",
    "end": "1766999"
  },
  {
    "text": "to xs-3x as routinely you don't have to keep tuning your clusters or tuning your workloads every single time you have a",
    "start": "1766999",
    "end": "1772549"
  },
  {
    "text": "service environment you throw it at the problem and it just works in addition as",
    "start": "1772549",
    "end": "1780470"
  },
  {
    "start": "1778000",
    "end": "1841000"
  },
  {
    "text": "I said earlier and this might be true for many people here you might have a multi data destination data solution in",
    "start": "1780470",
    "end": "1786830"
  },
  {
    "text": "mind you might want to use redshift for certain kinds of data loads you might want to use pressed or Athena for other",
    "start": "1786830",
    "end": "1792590"
  },
  {
    "text": "data loads you might amuse EMR it's very important to have something that can actually plug in very seamlessly across",
    "start": "1792590",
    "end": "1798590"
  },
  {
    "text": "the board Amazon redshift for us handles small mutable data and reporting workloads and ad-hoc models or extremely",
    "start": "1798590",
    "end": "1805730"
  },
  {
    "text": "large data set queries that need to run happen on patina goo actually provides",
    "start": "1805730",
    "end": "1811970"
  },
  {
    "text": "libraries out of the box that plug into each and every one of these and because the data catalog also integrates with",
    "start": "1811970",
    "end": "1817909"
  },
  {
    "text": "all of this your schema transformations that are happening as data grows is also",
    "start": "1817909",
    "end": "1823070"
  },
  {
    "text": "integrated very seamlessly into all of these different use cases in addition since it plugs very seamlessly into",
    "start": "1823070",
    "end": "1828649"
  },
  {
    "text": "attina as I said you are out of the box have a querying layer as soon as you start building the data like so that",
    "start": "1828649",
    "end": "1834139"
  },
  {
    "text": "your users actually see value on this as soon as you start building it as opposed to months down the road when you hope to",
    "start": "1834139",
    "end": "1839539"
  },
  {
    "text": "provide value so if you do decide to use",
    "start": "1839539",
    "end": "1844879"
  },
  {
    "text": "it up this clue for your data leaks there are some key learnings that we felt are really valuable and hopefully",
    "start": "1844879",
    "end": "1850999"
  },
  {
    "text": "they can help you as you're building your own data leaks firstly it's very important to set up testing version",
    "start": "1850999",
    "end": "1856700"
  },
  {
    "start": "1853000",
    "end": "1936000"
  },
  {
    "text": "control and a dev environment very early since you're in a service environment a lot of these things don't come out of",
    "start": "1856700",
    "end": "1862159"
  },
  {
    "text": "the box for example versioning doesn't come out of the box at AWS glue square so what Robin Hood has done is we have",
    "start": "1862159",
    "end": "1868729"
  },
  {
    "text": "Jenkin builds that occur as soon as every commit lands into a centralized repository that keeps all our glue",
    "start": "1868729",
    "end": "1874789"
  },
  {
    "text": "scripts because we have version releases these all end up as a separate path in",
    "start": "1874789",
    "end": "1880759"
  },
  {
    "text": "s3 and when we need to roll back or we catch a bug it's very easy for us to do",
    "start": "1880759",
    "end": "1885799"
  },
  {
    "text": "this by just pointing the job to a different path where our version script lives in addition glue provides the",
    "start": "1885799",
    "end": "1893989"
  },
  {
    "text": "development endpoint and this is really valuable as engineers are starting to work on datasets you really want to have",
    "start": "1893989",
    "end": "1900559"
  },
  {
    "text": "the ability to play with the data to get a feel for what it is as you understand and try to build your workflow on top of",
    "start": "1900559",
    "end": "1905960"
  },
  {
    "text": "this this is something that's been really valuable at Robin Hood and I would highly recommend setting this up for your developers so that they",
    "start": "1905960",
    "end": "1911749"
  },
  {
    "text": "actually understand and can play with the data before they send it send off their script into a service Lloyd in addition glue allows you to package",
    "start": "1911749",
    "end": "1919429"
  },
  {
    "text": "shared library so if you see common code patterns emerging it's very advisable to kind of bundle",
    "start": "1919429",
    "end": "1926089"
  },
  {
    "text": "these into a shared library that you submit with every each and every one of your glue scripts so this glue scripts",
    "start": "1926089",
    "end": "1931609"
  },
  {
    "text": "are not repeating code and they're reusing code as much as they can in",
    "start": "1931609",
    "end": "1938179"
  },
  {
    "start": "1936000",
    "end": "2008000"
  },
  {
    "text": "addition I would recommend not being afraid to compliment aw with Amazon EMR",
    "start": "1938179",
    "end": "1943789"
  },
  {
    "text": "as needed it doubles glue while it abstracts so many things away for you from you it does limit the amount of",
    "start": "1943789",
    "end": "1950749"
  },
  {
    "text": "configuration and configurability that you have for example it obviously does not allow vertical scaling just as yet",
    "start": "1950749",
    "end": "1956719"
  },
  {
    "text": "and there are workloads for instance that Robinhood when we had to work on a",
    "start": "1956719",
    "end": "1962269"
  },
  {
    "text": "large amount of our analytics data which was in many many terabytes you might want to tune the CPU and memory balance",
    "start": "1962269",
    "end": "1968629"
  },
  {
    "text": "very and have fine-grained control over that and for use cases such as that EMR can give you a lot more configurability",
    "start": "1968629",
    "end": "1974440"
  },
  {
    "text": "in addition extensive executor logs that are sometimes necessary to debug when",
    "start": "1974440",
    "end": "1979820"
  },
  {
    "text": "issues arise are not easily visible in AWS glue and you might wanna use the",
    "start": "1979820",
    "end": "1984889"
  },
  {
    "text": "Ammar because it gives you a lot more understanding of each and every step that takes place in your SPARC job and",
    "start": "1984889",
    "end": "1991099"
  },
  {
    "text": "if you have custom configurations of the patchy SPARC or you have a team that actually can support it and you really",
    "start": "1991099",
    "end": "1996889"
  },
  {
    "text": "want to have your own version of everything that's running you Martin can give that for you so highly",
    "start": "1996889",
    "end": "2002210"
  },
  {
    "text": "recommend using that especially if you need additional spark functionality things like spark UI and other things",
    "start": "2002210",
    "end": "2009010"
  },
  {
    "start": "2008000",
    "end": "2072000"
  },
  {
    "text": "it's also very important to monitor the jobs and data that you have very carefully since glue makes it so easy",
    "start": "2009010",
    "end": "2015200"
  },
  {
    "text": "for you to get started it's very it's very simple to forget that you still need to understand what's going on underneath the hood for example you",
    "start": "2015200",
    "end": "2022850"
  },
  {
    "text": "really need to know what the crawler is doing you need to know what a dpu is and these things all tie in very closely",
    "start": "2022850",
    "end": "2029960"
  },
  {
    "text": "with the metrics that Glu exports that you really want to keep an eye on for example if you don't really keep an eye",
    "start": "2029960",
    "end": "2035450"
  },
  {
    "text": "on it's very it's very easy to overuse the crawler and start seeing much higher cost than you need if the schema that you know does not",
    "start": "2035450",
    "end": "2041840"
  },
  {
    "text": "change you can just add partitions directly and not wait for the crawler to detect these because you already know",
    "start": "2041840",
    "end": "2046910"
  },
  {
    "text": "what the data is going to look like in addition as you start monitoring your SLA and you start seeing that your data",
    "start": "2046910",
    "end": "2052909"
  },
  {
    "text": "grows you might seeing you might start seeing your jobs take longer and you need to adjust the dpu allocation that",
    "start": "2052910",
    "end": "2058399"
  },
  {
    "text": "you have for your jobs on the fly so it's very important even though this is",
    "start": "2058400",
    "end": "2063500"
  },
  {
    "text": "service to kind of really have your eye on all of these metrics so that you're kind of Kooning this and cake making sure that downstream customers aren't",
    "start": "2063500",
    "end": "2069560"
  },
  {
    "text": "impacted as your data grows at Robinhood we use a very powerful scheduler to",
    "start": "2069560",
    "end": "2076370"
  },
  {
    "start": "2072000",
    "end": "2161000"
  },
  {
    "text": "manage our AWS Glu jobs we leverage Apache our flow to manage complicated DAGs of different AWS glue jobs this is",
    "start": "2076370",
    "end": "2083149"
  },
  {
    "text": "very powerful because we don't just use glue to take a certain data set and make it available we often have multiple data",
    "start": "2083150",
    "end": "2089810"
  },
  {
    "text": "sets that kind of build on top of each other and managing these dependencies making sure that they kind of all plug",
    "start": "2089810",
    "end": "2095149"
  },
  {
    "text": "in together is very important we were already using air flow to manage other computation jobs and tasks that we had",
    "start": "2095150",
    "end": "2102740"
  },
  {
    "text": "so using a scheduler that works for you for your company that plugs in already into the existing development",
    "start": "2102740",
    "end": "2108140"
  },
  {
    "text": "environment that you have is really really important your scheduler should ideally you know alert to your point",
    "start": "2108140",
    "end": "2113990"
  },
  {
    "text": "failures enforce s delays have retry logic one particular feature that is really",
    "start": "2113990",
    "end": "2119810"
  },
  {
    "text": "powerful for us in air flow that I that I definitely recommend is the ability to set concurrency limits on your AWS",
    "start": "2119810",
    "end": "2127220"
  },
  {
    "text": "account you might have a certain number of DP use that you hope to be using at any point in time airflow allows you to set separate",
    "start": "2127220",
    "end": "2133650"
  },
  {
    "text": "queues for the different data jobs you need and so you can have a high data or high dpu job a low DP u q-- medium DP u",
    "start": "2133650",
    "end": "2142079"
  },
  {
    "text": "queue and then set concurrency limits on all of these so that your lower DP u jobs don't overwhelm the higher ones or",
    "start": "2142079",
    "end": "2148559"
  },
  {
    "text": "vice versa and you have a limitation on how many of each to run at any given point in time that way you you're",
    "start": "2148559",
    "end": "2154230"
  },
  {
    "text": "ensured that you know your jobs will not wait for DP use or you will not exhaust your resources when you don't expect them to so finally to draw it all",
    "start": "2154230",
    "end": "2163230"
  },
  {
    "start": "2161000",
    "end": "2235000"
  },
  {
    "text": "together I want to talk about how using AWS and these service technologies Robin",
    "start": "2163230",
    "end": "2171539"
  },
  {
    "text": "was actually able to achieve a data Lake in very short amount of time we have",
    "start": "2171539",
    "end": "2176849"
  },
  {
    "text": "solved all the issues that I pointed out earlier we no longer have data silos scaling our users and scaling governance",
    "start": "2176849",
    "end": "2182460"
  },
  {
    "text": "is a lot easier and we have been able to separate our compute and storage so that costs are a lot more manageable in addition to that we have been able to",
    "start": "2182460",
    "end": "2188760"
  },
  {
    "text": "build a lot of new applications on top of our data that weren't possible before because of how United our data is now",
    "start": "2188760",
    "end": "2194490"
  },
  {
    "text": "people don't need to have different dialects that they understand they can write sequel across the board and build on top of",
    "start": "2194490",
    "end": "2201569"
  },
  {
    "text": "things with the unified scream frameworks such as spark if you're more interest if you're interested in this and want to learn more about this we",
    "start": "2201569",
    "end": "2207720"
  },
  {
    "text": "have another talk tomorrow at startup central and that dives deeper into specific details on how we actually",
    "start": "2207720",
    "end": "2213599"
  },
  {
    "text": "ended up building each and every one of these components and if you're even more interested definitely come talk to me",
    "start": "2213599",
    "end": "2219390"
  },
  {
    "text": "afterwards since we're hiring in the data team at Robin Hood with that you know I'd like to thank you guys and",
    "start": "2219390",
    "end": "2225480"
  },
  {
    "text": "project and me we'll be around to answer any questions that you guys have thank you [Applause]",
    "start": "2225480",
    "end": "2237120"
  }
]