[
  {
    "start": "0",
    "end": "102000"
  },
  {
    "text": "no I like I know everyone it's",
    "start": "770",
    "end": "7319"
  },
  {
    "text": "privileged to be with it's my privilege to be with all of you and a warm welcome to day two of reinvent 2018 over seven",
    "start": "7319",
    "end": "17820"
  },
  {
    "text": "years ago when we started building Aurora we had a simple mission we wanted",
    "start": "17820",
    "end": "27570"
  },
  {
    "text": "any person anywhere in the world to be",
    "start": "27570",
    "end": "32668"
  },
  {
    "text": "able to run and access databases and all they would need is business application",
    "start": "32669",
    "end": "41059"
  },
  {
    "text": "they would not need to worry about provisioning need highly skilled",
    "start": "41059",
    "end": "46350"
  },
  {
    "text": "operators managing their databases make trade-offs between performance",
    "start": "46350",
    "end": "52579"
  },
  {
    "text": "availability durability and cost and by doing so we believe we can transform",
    "start": "52579",
    "end": "61699"
  },
  {
    "text": "everyone in the world to reimagine the databases in the cloud hi my name is",
    "start": "61699",
    "end": "68640"
  },
  {
    "text": "Camille Gupta I am a senior engineering manager at AWS today I'm going to show",
    "start": "68640",
    "end": "75840"
  },
  {
    "text": "you not only the new innovations we did in Aurora but also how we did it and",
    "start": "75840",
    "end": "82580"
  },
  {
    "text": "we're going to talk about how we are adding new capabilities like multi-master parallel query serverless",
    "start": "82580",
    "end": "92600"
  },
  {
    "text": "into our Aurora offering let's dive deep",
    "start": "92600",
    "end": "97619"
  },
  {
    "text": "into Aurora my sequel our vision for",
    "start": "97619",
    "end": "105899"
  },
  {
    "start": "102000",
    "end": "185000"
  },
  {
    "text": "Amazon relational database services is to give you choices and recommendations",
    "start": "105899",
    "end": "112880"
  },
  {
    "text": "so that you can make the best decision for your applications on the one hand we",
    "start": "112880",
    "end": "122520"
  },
  {
    "text": "offer open source solutions for our customers who needs who likes the",
    "start": "122520",
    "end": "129390"
  },
  {
    "text": "simplicity and the cost-effectiveness of them but the challenge is that they lack",
    "start": "129390",
    "end": "137160"
  },
  {
    "text": "enterprise-grade performance and reliability that our customer needs for mission-critical",
    "start": "137160",
    "end": "144209"
  },
  {
    "text": "applications on the other hand we also",
    "start": "144209",
    "end": "149400"
  },
  {
    "text": "offer old-guard commercial-grade database engines for our customers who need enterprise",
    "start": "149400",
    "end": "156180"
  },
  {
    "text": "enterprise-grade performance and reliability even though they are",
    "start": "156180",
    "end": "162900"
  },
  {
    "text": "expensive with lock ins and punitive licensing terms one of the early",
    "start": "162900",
    "end": "169800"
  },
  {
    "text": "feedback we received from you guys is to build something that combines the best",
    "start": "169800",
    "end": "176250"
  },
  {
    "text": "of both worlds and we have created an Aurora for you",
    "start": "176250",
    "end": "182599"
  },
  {
    "text": "with aurora you no longer have to make the trade-offs you get the commercial",
    "start": "185200",
    "end": "191560"
  },
  {
    "text": "grade performance availability durability at the cost and simplicity of",
    "start": "191560",
    "end": "199050"
  },
  {
    "text": "open source solutions and it's delivered",
    "start": "199050",
    "end": "204220"
  },
  {
    "text": "to you as a managed service here are",
    "start": "204220",
    "end": "209680"
  },
  {
    "start": "208000",
    "end": "229000"
  },
  {
    "text": "some of our customers who have been using Aurora Nasdaq Hulu",
    "start": "209680",
    "end": "215320"
  },
  {
    "text": "Airbnb Zynga ancestry some big names as you can see Aurora continues to be the",
    "start": "215320",
    "end": "223959"
  },
  {
    "text": "fastest growing service in the AWS history with that intro I'm going to",
    "start": "223959",
    "end": "232720"
  },
  {
    "text": "first talk about performance and then we will deep dive into availability",
    "start": "232720",
    "end": "237850"
  },
  {
    "text": "durability and manageability you know",
    "start": "237850",
    "end": "243040"
  },
  {
    "start": "241000",
    "end": "280000"
  },
  {
    "text": "when databases first came out they looked like this monolithic stack in a",
    "start": "243040",
    "end": "249280"
  },
  {
    "text": "single box and with local storage we",
    "start": "249280",
    "end": "254620"
  },
  {
    "text": "were trading availability and durability to get better performance over time we",
    "start": "254620",
    "end": "263350"
  },
  {
    "text": "decoupled compute from storage and this allowed us to be able to scale customize",
    "start": "263350",
    "end": "270280"
  },
  {
    "text": "and manage each of those layers separately but the monolithic stack",
    "start": "270280",
    "end": "276850"
  },
  {
    "text": "still remained the same and then we added more such boxes it's the same",
    "start": "276850",
    "end": "285790"
  },
  {
    "start": "280000",
    "end": "321000"
  },
  {
    "text": "sequel stack everywhere nothing changed moreover now we need heavyweight",
    "start": "285790",
    "end": "293700"
  },
  {
    "text": "distributed consensus protocols like Paxos our raft for data application but",
    "start": "293700",
    "end": "303250"
  },
  {
    "text": "they perform poorly for databases because of multiple phases and multiple rounds and different synchronized",
    "start": "303250",
    "end": "310510"
  },
  {
    "text": "synchronization points involved and they add jitter and latencies to your",
    "start": "310510",
    "end": "317460"
  },
  {
    "text": "quarries with Aurora we made two key",
    "start": "317460",
    "end": "324310"
  },
  {
    "text": "contributions first we push down log",
    "start": "324310",
    "end": "329710"
  },
  {
    "text": "applicator down to the storage and that allowed us to construct pages from the",
    "start": "329710",
    "end": "337720"
  },
  {
    "text": "logs themselves this is really cool",
    "start": "337720",
    "end": "344610"
  },
  {
    "text": "because we don't have to write full pages anymore",
    "start": "344610",
    "end": "349620"
  },
  {
    "text": "unlike traditional database engines where they have to write both logs and pages Aurora only the right logs and",
    "start": "350040",
    "end": "360990"
  },
  {
    "text": "this means that we have significantly less IO and you have to do much less",
    "start": "360990",
    "end": "368680"
  },
  {
    "text": "work on the engine we don't have any checkpointing anymore you don't have to",
    "start": "368680",
    "end": "376780"
  },
  {
    "text": "worry about cache evictions or background flushing anymore second",
    "start": "376780",
    "end": "384250"
  },
  {
    "text": "instead of using heavyweight consensus protocols we use four out of six quorum",
    "start": "384250",
    "end": "389800"
  },
  {
    "text": "with local tracking and the reason we can avoid these heavyweight consensus is",
    "start": "389800",
    "end": "396840"
  },
  {
    "text": "because we can exploit the monotonically increasing lock sequence number from the",
    "start": "396840",
    "end": "403720"
  },
  {
    "text": "database engine to order the writes and so when storage not receive the rights",
    "start": "403720",
    "end": "409930"
  },
  {
    "text": "they just accept the rights there is no voting involved and back and forth to whether to accept it or not to accept it",
    "start": "409930",
    "end": "418560"
  },
  {
    "text": "and we'll see both of these things in bit more detail later but what you get",
    "start": "418950",
    "end": "425710"
  },
  {
    "text": "out of these you get significantly better write performance you get read",
    "start": "425710",
    "end": "434680"
  },
  {
    "text": "scale out because the replicas are sharing the storage with the master you",
    "start": "434680",
    "end": "442600"
  },
  {
    "text": "get a Z plus one failure tolerance so Aurora stores",
    "start": "442600",
    "end": "448310"
  },
  {
    "text": "six copies two copies per easy even when",
    "start": "448310",
    "end": "454580"
  },
  {
    "text": "you're managing in your large fleet even your data centers the nodes constantly go up and now that happens but we argue",
    "start": "454580",
    "end": "466130"
  },
  {
    "text": "that while that is happening and entire easy goes down on top Aurora can handle",
    "start": "466130",
    "end": "472610"
  },
  {
    "text": "it no problem you get instant data base redo recovery",
    "start": "472610",
    "end": "479350"
  },
  {
    "text": "because we don't have to do anything all we have to do is find out a point and do",
    "start": "479350",
    "end": "486830"
  },
  {
    "text": "some math before the time of the crash",
    "start": "486830",
    "end": "492159"
  },
  {
    "text": "let's look into these each of these things so again no more trade-offs",
    "start": "495040",
    "end": "501500"
  },
  {
    "text": "between performance availability and durability let's see how the first",
    "start": "501500",
    "end": "509120"
  },
  {
    "start": "507000",
    "end": "613000"
  },
  {
    "text": "optimization the la log applicator works in action here we have four transactions",
    "start": "509120",
    "end": "516140"
  },
  {
    "text": "with master and replicas with storage at the bottom six ways replicated let's say",
    "start": "516140",
    "end": "523909"
  },
  {
    "text": "transaction t1 started and as you can see all the six storage nodes and",
    "start": "523910",
    "end": "530060"
  },
  {
    "text": "replicas received the change and so if master and replicas have to read that",
    "start": "530060",
    "end": "536900"
  },
  {
    "text": "page again they will get the latest version of the page",
    "start": "536900",
    "end": "541569"
  },
  {
    "text": "now let's say t2 t3 and t4 all of them started",
    "start": "547840",
    "end": "552880"
  },
  {
    "text": "notice that the changes in purple are coalesced but the changes in blue and",
    "start": "552880",
    "end": "559700"
  },
  {
    "text": "green are kept on the side and this is because the replica clock is still at purple and when master and replica reads",
    "start": "559700",
    "end": "574180"
  },
  {
    "text": "they will get the correct version of the page for master as you can see the",
    "start": "574180",
    "end": "580640"
  },
  {
    "text": "storage node applies the blue and the green changes on demand on the fly and",
    "start": "580640",
    "end": "590150"
  },
  {
    "text": "eventually the replica clock will go to green and we will coalesce and garbage",
    "start": "590150",
    "end": "596990"
  },
  {
    "text": "collect that Delta log records so hopefully you can see how we can",
    "start": "596990",
    "end": "603320"
  },
  {
    "text": "construct pages from the logs themselves now why does this matter to you right",
    "start": "603320",
    "end": "610880"
  },
  {
    "text": "let's take a look here we learn a suspension workload to compare the i/o",
    "start": "610880",
    "end": "617570"
  },
  {
    "start": "613000",
    "end": "700000"
  },
  {
    "text": "profile for stop my sequel running on EBS with Aurora on the left with my",
    "start": "617570",
    "end": "630350"
  },
  {
    "text": "sequel running on EBS you can see it has to replicate all kinds of data and as a",
    "start": "630350",
    "end": "638240"
  },
  {
    "text": "result it has to do 7.7 X more i/o than",
    "start": "638240",
    "end": "644000"
  },
  {
    "text": "Aurora and results in 35 times less",
    "start": "644000",
    "end": "652250"
  },
  {
    "text": "transactions",
    "start": "652250",
    "end": "655030"
  },
  {
    "text": "the other thing to note here is that step one three and four are synchronous",
    "start": "660870",
    "end": "668540"
  },
  {
    "text": "and so what that means is it adds jitter",
    "start": "670010",
    "end": "675620"
  },
  {
    "text": "to your application and we will see that in a second why that matters and the",
    "start": "675620",
    "end": "681810"
  },
  {
    "text": "contrast Aurora uses four out of six quorum so it's much more resilient to",
    "start": "681810",
    "end": "687900"
  },
  {
    "text": "jail latencies so even if one of the",
    "start": "687900",
    "end": "692910"
  },
  {
    "text": "storage note is taking time that's totally okay because we just need four acknowledgments again here we ran the",
    "start": "692910",
    "end": "703860"
  },
  {
    "text": "system and read and write workload on both my sequel and Aurora and as you can",
    "start": "703860",
    "end": "711150"
  },
  {
    "text": "see Aurora did ten times better on the right workload and 2.5 times better on",
    "start": "711150",
    "end": "719010"
  },
  {
    "text": "the lead workload here is another",
    "start": "719010",
    "end": "725820"
  },
  {
    "start": "724000",
    "end": "740000"
  },
  {
    "text": "example with bulk loading and indexing",
    "start": "725820",
    "end": "733340"
  },
  {
    "text": "again Aurora is 2.5 times faster than my sequel let's take a look at the read",
    "start": "734240",
    "end": "743490"
  },
  {
    "start": "740000",
    "end": "827000"
  },
  {
    "text": "replicas for your OLTP reads and OLAP queries on the Left we have my sequel",
    "start": "743490",
    "end": "751890"
  },
  {
    "text": "bin log wrap location typically used in the my sequel community and on the right",
    "start": "751890",
    "end": "758370"
  },
  {
    "text": "we have Aurora physical replication",
    "start": "758370",
    "end": "762500"
  },
  {
    "text": "unlike my sequel where it has to transmit the full rows and statements",
    "start": "764120",
    "end": "770760"
  },
  {
    "text": "for the Bill Naga replication Aurora only transfers the log records which are just the Delta changes",
    "start": "770760",
    "end": "778550"
  },
  {
    "text": "the other thing to note is that the bin log has to apply all the rights on each",
    "start": "782800",
    "end": "789920"
  },
  {
    "text": "of the slaves with Aurora there is no right I owe on the replicas in fact for",
    "start": "789920",
    "end": "801320"
  },
  {
    "text": "the pages that are not in the buffer pool of replicas there is no redial",
    "start": "801320",
    "end": "808180"
  },
  {
    "text": "involved for application purposes so to summarize there is no right io",
    "start": "808600",
    "end": "813790"
  },
  {
    "text": "there is no right redial for application and there is no extra storage involved",
    "start": "813790",
    "end": "821080"
  },
  {
    "start": "827000",
    "end": "894000"
  },
  {
    "text": "here we are comparing the Aurora logical replica lag with the role of physical",
    "start": "827830",
    "end": "834320"
  },
  {
    "text": "replica lag so the experiment that we",
    "start": "834320",
    "end": "841220"
  },
  {
    "text": "ran here is in the same instance it's the same Hardware same software just",
    "start": "841220",
    "end": "848750"
  },
  {
    "text": "switching from build our application to another application so it's Apple trebles as you can see on the left the",
    "start": "848750",
    "end": "857960"
  },
  {
    "text": "graph isn't seconds within first ten minutes of heavy load the lag went up to",
    "start": "857960",
    "end": "864620"
  },
  {
    "text": "five minutes with Aurora on the right",
    "start": "864620",
    "end": "870310"
  },
  {
    "text": "consistently stayed under 20 milliseconds for hours and hours",
    "start": "871750",
    "end": "878110"
  },
  {
    "text": "yes sir logical is the bin lock replication and physical is the log",
    "start": "881240",
    "end": "887700"
  },
  {
    "text": "record based application I'll take the questions then let's chuckle let's take",
    "start": "887700",
    "end": "897060"
  },
  {
    "start": "894000",
    "end": "1116000"
  },
  {
    "text": "a look at the second optimization again",
    "start": "897060",
    "end": "902400"
  },
  {
    "text": "the same setup with for transactions storage at the bottom six ways",
    "start": "902400",
    "end": "908400"
  },
  {
    "text": "replicated we also have a durability tracker on the right and set of",
    "start": "908400",
    "end": "916410"
  },
  {
    "text": "weighting transactions have not been acknowledged back to the client yet",
    "start": "916410",
    "end": "922070"
  },
  {
    "text": "let's say we did started all of those transactions and at some point the state",
    "start": "926890",
    "end": "932320"
  },
  {
    "text": "looks like this and as you can see the orange changes from transaction t1",
    "start": "932320",
    "end": "938610"
  },
  {
    "text": "already achieved four out of six quorum so we can mark that committed and",
    "start": "938610",
    "end": "944560"
  },
  {
    "text": "acknowledge it back to the client let's",
    "start": "944560",
    "end": "951250"
  },
  {
    "text": "say at some later point state may look like this as you can see the changes in",
    "start": "951250",
    "end": "961120"
  },
  {
    "text": "blue corresponding to transaction 3 even though they achieved the right quorum we",
    "start": "961120",
    "end": "966130"
  },
  {
    "text": "have not acknowledged it yet to understand this better let me step back",
    "start": "966130",
    "end": "972250"
  },
  {
    "text": "a bit let's see how flushing works in traditional databases in traditional",
    "start": "972250",
    "end": "979990"
  },
  {
    "text": "databases you accumulate the set of log records put it into a log buffer and",
    "start": "979990",
    "end": "985860"
  },
  {
    "text": "group commit you get the second change of second set of changes and you commit",
    "start": "985860",
    "end": "992200"
  },
  {
    "text": "again so it's all sequential and as soon as it's flushed you acknowledge back to",
    "start": "992200",
    "end": "999820"
  },
  {
    "text": "the client with Aurora instead when we receive the",
    "start": "999820",
    "end": "1005520"
  },
  {
    "text": "right we immediately issued to the storage node and so it may happen the",
    "start": "1005520",
    "end": "1013980"
  },
  {
    "text": "right issued later may get flushed first it's going to different storage nodes",
    "start": "1013980",
    "end": "1019340"
  },
  {
    "text": "but instead we check here on the return path that we make sure that we only",
    "start": "1019340",
    "end": "1026189"
  },
  {
    "text": "acknowledge in the order it's important otherwise we will have holes in the",
    "start": "1026190",
    "end": "1032280"
  },
  {
    "text": "right headlock and that will not work and the reason it matters to you is",
    "start": "1032280",
    "end": "1039839"
  },
  {
    "text": "because the flush operation is a heavy operation you don't want the sequential writes there it adds latency and jitter",
    "start": "1039839",
    "end": "1048230"
  },
  {
    "text": "and we'll see that in a second",
    "start": "1048230",
    "end": "1052130"
  },
  {
    "text": "now let's say at some other state the",
    "start": "1053850",
    "end": "1064290"
  },
  {
    "text": "state may look like this and as you can see all the changes achieved the quorum",
    "start": "1064290",
    "end": "1070110"
  },
  {
    "text": "four out of six right core and so we can acknowledge all of them and finally",
    "start": "1070110",
    "end": "1080030"
  },
  {
    "text": "everybody in the storage nor gets all the changes so there is no consensus",
    "start": "1081290",
    "end": "1088280"
  },
  {
    "text": "there's no waiting anywhere other than just doing a four out of six quorum and",
    "start": "1088280",
    "end": "1095340"
  },
  {
    "text": "the reason is because we exploited the sequencing from the head node from the",
    "start": "1095340",
    "end": "1100440"
  },
  {
    "text": "database node I didn't talk about reads",
    "start": "1100440",
    "end": "1105750"
  },
  {
    "text": "here but if you're interested please refer to our or a stigmata paper we have a paper on that or you can catch me off",
    "start": "1105750",
    "end": "1111990"
  },
  {
    "text": "after the call after the talk here",
    "start": "1111990",
    "end": "1119100"
  },
  {
    "start": "1116000",
    "end": "1219000"
  },
  {
    "text": "we ran suspension gained and recorded",
    "start": "1119100",
    "end": "1125760"
  },
  {
    "text": "the latencies as you can see Aurora",
    "start": "1125760",
    "end": "1131580"
  },
  {
    "text": "known not only has the lower latencies but also is much steadier more precisely",
    "start": "1131580",
    "end": "1140840"
  },
  {
    "text": "based upon the standard deviation of the two datasets are arise more than two",
    "start": "1140840",
    "end": "1146610"
  },
  {
    "text": "hundred times consistent and has 25 times lower latencies this is despite the fact that",
    "start": "1146610",
    "end": "1154770"
  },
  {
    "text": "in this example we are pushing forty five times more work than Mexico so you",
    "start": "1154770",
    "end": "1163500"
  },
  {
    "text": "may be wondering what is this big spikes that is happening well this is my sequel",
    "start": "1163500",
    "end": "1169470"
  },
  {
    "text": "checkpointing and when my sequel has to checkpoint",
    "start": "1169470",
    "end": "1175970"
  },
  {
    "text": "it has to do a lot of background writes which interferes with the foreground",
    "start": "1175970",
    "end": "1181170"
  },
  {
    "text": "transactions and so to summarize we get significantly",
    "start": "1181170",
    "end": "1189330"
  },
  {
    "text": "better latencies and less jitter because of three reasons one we don't have",
    "start": "1189330",
    "end": "1195929"
  },
  {
    "text": "checkpoints because we construct the pages from the logs second we can do out",
    "start": "1195929",
    "end": "1203370"
  },
  {
    "text": "of order flush third we do not have any",
    "start": "1203370",
    "end": "1208710"
  },
  {
    "text": "heavyweight consensus and we use four out of six quorum so even if one or two",
    "start": "1208710",
    "end": "1214230"
  },
  {
    "text": "storage note is taking time that's okay",
    "start": "1214230",
    "end": "1217730"
  },
  {
    "start": "1219000",
    "end": "1290000"
  },
  {
    "text": "what else we did here are some software improvements example that we did in last",
    "start": "1220820",
    "end": "1226830"
  },
  {
    "text": "few years on the left is the my sequel thread model you get a connection you",
    "start": "1226830",
    "end": "1233220"
  },
  {
    "text": "open a thread clearly that doesn't scale with connections instead with Aurora we",
    "start": "1233220",
    "end": "1239789"
  },
  {
    "text": "added a thread pool with event based eople and latch free task queue allows",
    "start": "1239789",
    "end": "1246720"
  },
  {
    "text": "us to scale much better with high number of connections you know when you're",
    "start": "1246720",
    "end": "1257399"
  },
  {
    "text": "pushing so much load in the system if you have a global lock if they will all",
    "start": "1257399",
    "end": "1264450"
  },
  {
    "text": "content and all the other efforts that you do did with respect to throughput will go in win with my sequel any",
    "start": "1264450",
    "end": "1275399"
  },
  {
    "text": "changes any updates will lock the whole lock table instead with Aurora we allow",
    "start": "1275399",
    "end": "1281669"
  },
  {
    "text": "concurrent access to the individual lock genes so there is no contention",
    "start": "1281669",
    "end": "1289700"
  },
  {
    "start": "1290000",
    "end": "1317000"
  },
  {
    "text": "not only software improved but also hardware also improved and as you can",
    "start": "1291250",
    "end": "1296890"
  },
  {
    "text": "see year-over-year adora is getting better and better all I can do",
    "start": "1296890",
    "end": "1303720"
  },
  {
    "text": "200,000 lights per second and whopping 700,000 reads per second and it's",
    "start": "1303720",
    "end": "1313810"
  },
  {
    "text": "getting better each day well some of our",
    "start": "1313810",
    "end": "1320710"
  },
  {
    "start": "1317000",
    "end": "1343000"
  },
  {
    "text": "some of my customer comes to me and asked what about parameters performance",
    "start": "1320710",
    "end": "1325720"
  },
  {
    "text": "parameters well Aurora already pre Tunes or Auto Tunes",
    "start": "1325720",
    "end": "1330850"
  },
  {
    "text": "different parameters for you on different hardware configuration unless",
    "start": "1330850",
    "end": "1336160"
  },
  {
    "text": "you're doing something peculiar you should get the best performance out of the box",
    "start": "1336160",
    "end": "1342870"
  },
  {
    "start": "1343000",
    "end": "1413000"
  },
  {
    "text": "now there are few performance parameters like in ODB log file size in ODB flash a",
    "start": "1345270",
    "end": "1352390"
  },
  {
    "text": "transaction commit sync bin log that allows us in my sequel to get better",
    "start": "1352390",
    "end": "1357670"
  },
  {
    "text": "performance but that usually comes with a trade-off here is one example with in",
    "start": "1357670",
    "end": "1368050"
  },
  {
    "text": "ODB log file size where you can get better performance but you also increase",
    "start": "1368050",
    "end": "1374530"
  },
  {
    "text": "your recovery time the reason is that if",
    "start": "1374530",
    "end": "1380860"
  },
  {
    "text": "you increase the log file size you effectively delaying your checkpoint and",
    "start": "1380860",
    "end": "1387000"
  },
  {
    "text": "so if you delay your checkpoint the more redo logs you will accumulate and when",
    "start": "1387000",
    "end": "1393220"
  },
  {
    "text": "my sequel has to recover from the crash it will have to apply all of those log",
    "start": "1393220",
    "end": "1398560"
  },
  {
    "text": "records on a single card and that takes",
    "start": "1398560",
    "end": "1404650"
  },
  {
    "text": "time so again no more traders well so much",
    "start": "1404650",
    "end": "1415430"
  },
  {
    "start": "1413000",
    "end": "1520000"
  },
  {
    "text": "about single master let's talk about multi master before we jump in let's",
    "start": "1415430",
    "end": "1423260"
  },
  {
    "text": "take a quick look at the space when we",
    "start": "1423260",
    "end": "1429650"
  },
  {
    "text": "first started we had one node sequel but",
    "start": "1429650",
    "end": "1436340"
  },
  {
    "text": "it was very hard to scale overtime we",
    "start": "1436340",
    "end": "1442610"
  },
  {
    "text": "did manual sharding but it was hard to manage you can get hot partitions if",
    "start": "1442610",
    "end": "1448820"
  },
  {
    "text": "you're if you have to run DDL across these shards it's it's very painful not",
    "start": "1448820",
    "end": "1455720"
  },
  {
    "text": "to mention if you have to do cross short transactions clearly not there with no",
    "start": "1455720",
    "end": "1464780"
  },
  {
    "text": "sequel we achieve the scalability and it's offered as a service but they it",
    "start": "1464780",
    "end": "1472130"
  },
  {
    "text": "fundamentally lacks the support for transactions and so the customers that I",
    "start": "1472130",
    "end": "1484580"
  },
  {
    "text": "talk to you they love the transaction model they love to be able to model",
    "start": "1484580",
    "end": "1490700"
  },
  {
    "text": "about it to be able to reason about it it's very hard to write apps with eventual consistent systems with the",
    "start": "1490700",
    "end": "1498890"
  },
  {
    "text": "Aurora single master we addressed most of them but they were few gaps and with",
    "start": "1498890",
    "end": "1505160"
  },
  {
    "text": "Aurora multi master we are addressing most of those gaps by adding right",
    "start": "1505160",
    "end": "1510290"
  },
  {
    "text": "scalability and database right availability for our customers",
    "start": "1510290",
    "end": "1519520"
  },
  {
    "start": "1520000",
    "end": "1564000"
  },
  {
    "text": "let's take a look at some of the existing multi-master solutions first we",
    "start": "1520940",
    "end": "1527029"
  },
  {
    "text": "have a shared disk model with gaseous fuse together the challenge here is that",
    "start": "1527029",
    "end": "1537729"
  },
  {
    "text": "they use pessimistic locking the other big challenge is that they require high",
    "start": "1537729",
    "end": "1544759"
  },
  {
    "text": "cache coherence traffic and so you",
    "start": "1544759",
    "end": "1549769"
  },
  {
    "text": "either need expensive interconnect between the nodes typically put together in a small data center in the room or",
    "start": "1549769",
    "end": "1557179"
  },
  {
    "text": "you suffer from hot bloc's ping-ponging across the nodes then there are systems",
    "start": "1557179",
    "end": "1568519"
  },
  {
    "start": "1564000",
    "end": "1630000"
  },
  {
    "text": "which use readwrite technique basically the idea here is that if you want to",
    "start": "1568519",
    "end": "1575389"
  },
  {
    "text": "modify anything you first read something and then modify based on that value and",
    "start": "1575389",
    "end": "1580869"
  },
  {
    "text": "at the time of the commit you check if anybody anyone else has modified",
    "start": "1580869",
    "end": "1588019"
  },
  {
    "text": "anything that you read if someone did then you about your transaction if din't",
    "start": "1588019",
    "end": "1595549"
  },
  {
    "text": "you come into a transaction and if you can do that for all the transactions in",
    "start": "1595549",
    "end": "1601399"
  },
  {
    "text": "a particular order you will always come up with the same outcome and so what",
    "start": "1601399",
    "end": "1608149"
  },
  {
    "text": "these systems do they replicate the transaction independently run them in",
    "start": "1608149",
    "end": "1614119"
  },
  {
    "text": "the same particular order and you get your distributed databases but again the",
    "start": "1614119",
    "end": "1621649"
  },
  {
    "text": "challenge is that coming up with this particular order for all the nodes becomes a bottom line and finally then",
    "start": "1621649",
    "end": "1632330"
  },
  {
    "start": "1630000",
    "end": "1681000"
  },
  {
    "text": "their systems no sequel like systems which where there is range partitioned",
    "start": "1632330",
    "end": "1639220"
  },
  {
    "text": "they typically use paxos within a partition and then to pc across the partitions",
    "start": "1639429",
    "end": "1646509"
  },
  {
    "text": "the challenge here is that if you have a skew in your access pattern which quite",
    "start": "1649760",
    "end": "1655620"
  },
  {
    "text": "typical then you will end up with hot partitions for example let's say you are",
    "start": "1655620",
    "end": "1661230"
  },
  {
    "text": "you partitioned by date time for your queries you will end up writing to the",
    "start": "1661230",
    "end": "1667620"
  },
  {
    "text": "law and the end of the partition also",
    "start": "1667620",
    "end": "1673890"
  },
  {
    "text": "they happen to use heavyweight consensus protocol for commits with Aurora",
    "start": "1673890",
    "end": "1683220"
  },
  {
    "start": "1681000",
    "end": "2088000"
  },
  {
    "text": "there is no pessimistic locking there is no global ordering there is no global",
    "start": "1683220",
    "end": "1688680"
  },
  {
    "text": "commit coordination instead Aurora multi-master is based upon three three",
    "start": "1688680",
    "end": "1695820"
  },
  {
    "text": "techniques number one optimistic",
    "start": "1695820",
    "end": "1702120"
  },
  {
    "text": "conflict resolution to understand this better let's say we have transaction t1",
    "start": "1702120",
    "end": "1707880"
  },
  {
    "text": "and t2 if they write you two different tables there is no conflict and they can",
    "start": "1707880",
    "end": "1715860"
  },
  {
    "text": "independently write but if transaction t1 and t2 modify the",
    "start": "1715860",
    "end": "1722310"
  },
  {
    "text": "same page beat you in this diagram one of them will achieve the quorum the",
    "start": "1722310",
    "end": "1727500"
  },
  {
    "text": "other one will not so one of them will win the other one will retry",
    "start": "1727500",
    "end": "1735020"
  },
  {
    "text": "one thing I forgot to mention is that with single master we used monotonic",
    "start": "1741930",
    "end": "1749059"
  },
  {
    "text": "monotonically increasing lock sequence number from the head node with multi master instead of having a single",
    "start": "1749059",
    "end": "1755190"
  },
  {
    "text": "monotonic monotonically increasing lock sequence number we have partitioned monotonic Allisyn sequencing from the",
    "start": "1755190",
    "end": "1762000"
  },
  {
    "text": "head node and we use that to order the rights second we have a decoupled system",
    "start": "1762000",
    "end": "1774230"
  },
  {
    "text": "with logging layer pushed down to the storage there is no coupling between the",
    "start": "1774230",
    "end": "1781020"
  },
  {
    "text": "transaction layer and the physical layer the logging layer and so that we can manage the physical conflicts in the",
    "start": "1781020",
    "end": "1787350"
  },
  {
    "text": "storage and the logical conflicts between the in the database layer and by",
    "start": "1787350",
    "end": "1793440"
  },
  {
    "text": "data bit by logical conflicts I mean that transaction conflicts and we will see in in a bit",
    "start": "1793440",
    "end": "1799170"
  },
  {
    "text": "with a better example what that means I also note that the the storage volume is",
    "start": "1799170",
    "end": "1807570"
  },
  {
    "text": "divided into set of partitions and they don't there's no coupling involved they",
    "start": "1807570",
    "end": "1813240"
  },
  {
    "text": "don't talk to each other and there is no coupling between the head node itself",
    "start": "1813240",
    "end": "1821480"
  },
  {
    "text": "finally we use microservices architecture to run services in the",
    "start": "1822740",
    "end": "1829650"
  },
  {
    "text": "cluster so that if any one of them is",
    "start": "1829650",
    "end": "1836160"
  },
  {
    "text": "temporarily down it doesn't impact your whole cluster these are minimal independent and resilient services",
    "start": "1836160",
    "end": "1845150"
  },
  {
    "text": "in short Aurora only coordinates when it",
    "start": "1847700",
    "end": "1852720"
  },
  {
    "text": "has to coordinate let's see that with examples let's say we have client C 1",
    "start": "1852720",
    "end": "1862320"
  },
  {
    "text": "and C 2 talking to blue master and orange master respectively let's",
    "start": "1862320",
    "end": "1870090"
  },
  {
    "text": "consider a simple case where they are writing to different tables they both",
    "start": "1870090",
    "end": "1877440"
  },
  {
    "text": "begin the transaction they update",
    "start": "1877440",
    "end": "1882659"
  },
  {
    "text": "different tables and they both commit",
    "start": "1882659",
    "end": "1888950"
  },
  {
    "text": "no synchronization",
    "start": "1891520",
    "end": "1894840"
  },
  {
    "text": "now the same setup but more interesting case let's say they write to the same",
    "start": "1899320",
    "end": "1905530"
  },
  {
    "text": "row of the same table again they both",
    "start": "1905530",
    "end": "1911290"
  },
  {
    "text": "start transaction b t1 which is the blue transaction o t1 which is an orange transaction and they write to the same",
    "start": "1911290",
    "end": "1921540"
  },
  {
    "text": "row of the same table in this case the",
    "start": "1921540",
    "end": "1927100"
  },
  {
    "text": "blue master get the forwards so it will acknowledge back to the client and the",
    "start": "1927100",
    "end": "1934680"
  },
  {
    "text": "orange master will ask the client to retry now just because there is no",
    "start": "1934680",
    "end": "1948700"
  },
  {
    "text": "physical conflict it doesn't mean that that there is no transaction conflict",
    "start": "1948700",
    "end": "1954780"
  },
  {
    "text": "let's see that with an example again the same set up and they're both",
    "start": "1954780",
    "end": "1962050"
  },
  {
    "text": "trying to write to the same entry of the same table they both started at",
    "start": "1962050",
    "end": "1970240"
  },
  {
    "text": "transactions and let's say c1 updates",
    "start": "1970240",
    "end": "1977410"
  },
  {
    "text": "the table 1 with Row 1 and page 1 and at",
    "start": "1977410",
    "end": "1985150"
  },
  {
    "text": "some point let's say this change will replicate to the orange master because there is a earth synchronous replication",
    "start": "1985150",
    "end": "1991060"
  },
  {
    "text": "going on again there is no synchronous coordination involved it's a synchronous",
    "start": "1991060",
    "end": "1996700"
  },
  {
    "text": "replication that happens over time so let's say in this example c2 is just",
    "start": "1996700",
    "end": "2002130"
  },
  {
    "text": "waiting and you know it really should update eventually",
    "start": "2002130",
    "end": "2007250"
  },
  {
    "text": "now note that if C to have to issue the update at this point it will act on top",
    "start": "2009929",
    "end": "2016510"
  },
  {
    "text": "of the latest version of the page and so from storage perspective this is okay",
    "start": "2016510",
    "end": "2021760"
  },
  {
    "text": "the storage if you want to write your next version of the page sure there is no conflict because you're reading the",
    "start": "2021760",
    "end": "2032230"
  },
  {
    "text": "latest version of the page however we detect that in the database itself and",
    "start": "2032230",
    "end": "2040620"
  },
  {
    "text": "the way we detect is is we use MVCC and",
    "start": "2040620",
    "end": "2045940"
  },
  {
    "text": "so the idea is you look at the transaction identifier in the row who",
    "start": "2045940",
    "end": "2052118"
  },
  {
    "text": "modified it and you see that okay this transaction this row seems to be",
    "start": "2052119",
    "end": "2058148"
  },
  {
    "text": "modified with transaction ID let's say 10 but this doesn't seem to me committed yet and once you find it out okay",
    "start": "2058149",
    "end": "2065169"
  },
  {
    "text": "let me roll back it's not allowed",
    "start": "2065169",
    "end": "2069810"
  },
  {
    "text": "and eventually Sivan commits so as you can see that there is no global lock",
    "start": "2075129",
    "end": "2082128"
  },
  {
    "text": "manager here here we ran again the",
    "start": "2082129",
    "end": "2090648"
  },
  {
    "text": "suspense workload and you can see we first started with one note and added",
    "start": "2090649",
    "end": "2099859"
  },
  {
    "text": "another note at five minute mark and",
    "start": "2099859",
    "end": "2105789"
  },
  {
    "text": "another two nodes at ten minute mark at",
    "start": "2105789",
    "end": "2113180"
  },
  {
    "text": "fifteen we simulated by crashing one of the nodes and you can see it came back",
    "start": "2113180",
    "end": "2122239"
  },
  {
    "text": "up at sixteen and life's move on this is",
    "start": "2122239",
    "end": "2132349"
  },
  {
    "text": "really cool okay this is about right side but what",
    "start": "2132349",
    "end": "2139729"
  },
  {
    "start": "2135000",
    "end": "2224000"
  },
  {
    "text": "about reads how do we do reads in multi-master more precisely how do we",
    "start": "2139729",
    "end": "2148099"
  },
  {
    "text": "achieve linearize ability to illustrate the problem first let me go through an",
    "start": "2148099",
    "end": "2153140"
  },
  {
    "text": "example let's say John and Bob are very good friends and one day John decided to",
    "start": "2153140",
    "end": "2159529"
  },
  {
    "text": "propose to Sara and so he updated his",
    "start": "2159529",
    "end": "2164630"
  },
  {
    "text": "status and sent a post",
    "start": "2164630",
    "end": "2170528"
  },
  {
    "text": "if John simply do a local read we'll see",
    "start": "2173020",
    "end": "2178030"
  },
  {
    "text": "okay sorry Bob if simply do a local read we'll see okay",
    "start": "2178030",
    "end": "2184210"
  },
  {
    "text": "John tried it looks like it didn't work out however if he would do global reach he",
    "start": "2184210",
    "end": "2195820"
  },
  {
    "text": "will find out that Sarah accepted so he can pick up the phone and call John to",
    "start": "2195820",
    "end": "2202510"
  },
  {
    "text": "come naturally Jim so the basic idea is",
    "start": "2202510",
    "end": "2207520"
  },
  {
    "text": "you get to read your own local changes",
    "start": "2207520",
    "end": "2212530"
  },
  {
    "text": "but if you want to read the global changes everything that happened in the whole cluster at that time you need",
    "start": "2212530",
    "end": "2218680"
  },
  {
    "text": "global reads and that is linearize ability let's see how we do it in this",
    "start": "2218680",
    "end": "2232210"
  },
  {
    "start": "2224000",
    "end": "2344000"
  },
  {
    "text": "example we have three nodes node N one and two and three client issues a request to n1 and one in turn sends a",
    "start": "2232210",
    "end": "2242350"
  },
  {
    "text": "hello request to n2 and n3 and when n2",
    "start": "2242350",
    "end": "2247720"
  },
  {
    "text": "and n3 received this request they respond with a timestamp at which they saw this hello request let's call the",
    "start": "2247720",
    "end": "2256869"
  },
  {
    "text": "time T 2 and T 3",
    "start": "2256869",
    "end": "2260069"
  },
  {
    "text": "and one then waits to get the changes from n2 and n3 till time T 2 and T 3",
    "start": "2263240",
    "end": "2271369"
  },
  {
    "text": "respectively and once n1 gets all the changes it",
    "start": "2271369",
    "end": "2278420"
  },
  {
    "text": "performs the read to the storage and",
    "start": "2278420",
    "end": "2283570"
  },
  {
    "text": "return the results back to the point so you can see it adds Layton sees only if",
    "start": "2283570",
    "end": "2295490"
  },
  {
    "text": "you're doing the global reads and it's configurable per session now there are other solutions out there who does this",
    "start": "2295490",
    "end": "2303589"
  },
  {
    "text": "on the right path but the challenge is that with the right path you don't have an option if you're doing it on the",
    "start": "2303589",
    "end": "2309589"
  },
  {
    "text": "right path you have to always do it because you fundamentally have to introduce a weight to make sure the time",
    "start": "2309589",
    "end": "2316550"
  },
  {
    "text": "has advanced so buddy so that if once I returned acknowledgement anybody else will start the transaction will always",
    "start": "2316550",
    "end": "2323869"
  },
  {
    "text": "see the time afterwards you have to wait to that that point in your right path",
    "start": "2323869",
    "end": "2329240"
  },
  {
    "text": "and you to do that for every transaction instead if you do it in the read path",
    "start": "2329240",
    "end": "2334430"
  },
  {
    "text": "you can choose you may want to pay the penalty you may not want to so in sum in",
    "start": "2334430",
    "end": "2345500"
  },
  {
    "start": "2344000",
    "end": "2376000"
  },
  {
    "text": "summary we achieve linear scaling with optimistic conflict resolution",
    "start": "2345500",
    "end": "2351040"
  },
  {
    "text": "continuous availability with microservices architecture enterprise great durability with six",
    "start": "2351040",
    "end": "2358550"
  },
  {
    "text": "copies two copies per easy we also do backups on top and we support indexing",
    "start": "2358550",
    "end": "2366130"
  },
  {
    "text": "constraints triggers procedures functions for your relational arrays",
    "start": "2366130",
    "end": "2374260"
  },
  {
    "start": "2376000",
    "end": "2566000"
  },
  {
    "text": "okay so much about LTP let's talk about OLAP queries first is bad scans and so",
    "start": "2377420",
    "end": "2388319"
  },
  {
    "text": "the idea here is whenever you get a so this is for in-memory workloads and so",
    "start": "2388319",
    "end": "2394349"
  },
  {
    "text": "let's imagine you have a big machine with the huge buffer pool and if you are",
    "start": "2394349",
    "end": "2400500"
  },
  {
    "text": "issuing a big query that has to scan a lot of rows what happens is when my",
    "start": "2400500",
    "end": "2406289"
  },
  {
    "text": "sequel receives requests it asked the underlying engine which is the inner DB one but you fetch the tupple one by one",
    "start": "2406289",
    "end": "2413549"
  },
  {
    "text": "and every time it has to traverse the whole tree take the latches along the way and get read the chapel and process",
    "start": "2413549",
    "end": "2421680"
  },
  {
    "text": "that topple and that is very inefficient what we do instead we get in batches so",
    "start": "2421680",
    "end": "2430140"
  },
  {
    "text": "we request to give a batch and we tune that dynamically based upon the instance",
    "start": "2430140",
    "end": "2436920"
  },
  {
    "text": "size and everything for you on top we",
    "start": "2436920",
    "end": "2441990"
  },
  {
    "text": "also add just-in-time optimization to make it even better",
    "start": "2441990",
    "end": "2447410"
  },
  {
    "text": "second is hash joints I mean the idea is very simple you take the smaller side",
    "start": "2449450",
    "end": "2455809"
  },
  {
    "text": "build a hash table and then you scan the outer side and probe into the hash table",
    "start": "2455809",
    "end": "2463068"
  },
  {
    "text": "but of course in practice it's much more involved you have to deal with SKUs you",
    "start": "2463309",
    "end": "2468539"
  },
  {
    "text": "have to deal with duplicates net nutshell you have to minimize the number of passes you are making over your data",
    "start": "2468539",
    "end": "2474869"
  },
  {
    "text": "set not to mention you have to build an optimizer to decide when to use hash",
    "start": "2474869",
    "end": "2481440"
  },
  {
    "text": "joint when not to this optimization is",
    "start": "2481440",
    "end": "2487140"
  },
  {
    "text": "relevant for EQ join and works for both in memory and out of cache workloads",
    "start": "2487140",
    "end": "2495920"
  },
  {
    "text": "a KP in shark a synchronous key prefetch",
    "start": "2498040",
    "end": "2503140"
  },
  {
    "text": "is for non equations or for some eq",
    "start": "2503140",
    "end": "2508900"
  },
  {
    "text": "joints as well if for example you have smaller router and you have a high cardinality index on the inner that may",
    "start": "2508900",
    "end": "2516790"
  },
  {
    "text": "be me it then it may make sense but the",
    "start": "2516790",
    "end": "2522370"
  },
  {
    "text": "idea is here is when you scan the outer",
    "start": "2522370",
    "end": "2529260"
  },
  {
    "text": "instead of looking up for each row in ER and load the page from the storage",
    "start": "2529260",
    "end": "2536160"
  },
  {
    "text": "synchronously you prefetch you basically look ahead the set of the rows and ask",
    "start": "2536160",
    "end": "2544470"
  },
  {
    "text": "the storage to fetch those pages into the buffer pool and so when that other",
    "start": "2544470",
    "end": "2549700"
  },
  {
    "text": "thread comes and tries to look up it will always find the pages in memory",
    "start": "2549700",
    "end": "2555660"
  },
  {
    "text": "this optimization only works for out of cache workloads because that's what it",
    "start": "2555660",
    "end": "2560710"
  },
  {
    "text": "does it brings the pages in advance in the buffer pool so as a result we saw on",
    "start": "2560710",
    "end": "2569970"
  },
  {
    "start": "2566000",
    "end": "2582000"
  },
  {
    "text": "average on half of the queries more than 2x speed up with the peak speed up of",
    "start": "2569970",
    "end": "2575200"
  },
  {
    "text": "18x this is a TPC etched like workload",
    "start": "2575200",
    "end": "2580050"
  },
  {
    "start": "2582000",
    "end": "2773000"
  },
  {
    "text": "the other big thing we did in this area is peril quarry we g-eight that thing",
    "start": "2582480",
    "end": "2589720"
  },
  {
    "text": "last few months ago and so the idea here is to push the processing down to the",
    "start": "2589720",
    "end": "2597190"
  },
  {
    "text": "storage and this is very similar to what we did for logging so we pushed down the log applicator and now we are pushing",
    "start": "2597190",
    "end": "2603370"
  },
  {
    "text": "down the query processing layer the thousands of storage nodes",
    "start": "2603370",
    "end": "2608070"
  },
  {
    "text": "and moving the computation down closer to the data means you will get much less",
    "start": "2610840",
    "end": "2617800"
  },
  {
    "text": "data back from the storage node and also it will reduce the buffer pool pollution",
    "start": "2617800",
    "end": "2623950"
  },
  {
    "text": "and we will see that in a second why that matters but before that let me talk",
    "start": "2623950",
    "end": "2631990"
  },
  {
    "text": "briefly about how we did it on the left",
    "start": "2631990",
    "end": "2638680"
  },
  {
    "text": "with gray arrow the request comes in we augment the request with a list of pages",
    "start": "2638680",
    "end": "2645280"
  },
  {
    "text": "and the page Alison or the version at which it is on and we send it down to",
    "start": "2645280",
    "end": "2650920"
  },
  {
    "text": "the storage all the storage node involve",
    "start": "2650920",
    "end": "2656050"
  },
  {
    "text": "in this query returned to streams clean stream and dirty stream clean stream is",
    "start": "2656050",
    "end": "2664690"
  },
  {
    "text": "a set of records that has not been modified since the query started the",
    "start": "2664690",
    "end": "2671710"
  },
  {
    "text": "dirty stream is a set of records that have been modified since the query started so for the clean stream you can",
    "start": "2671710",
    "end": "2681610"
  },
  {
    "text": "push that directly to the aggregator right because there are multiple storage nodes here all of them have to combine the results right before you can send it",
    "start": "2681610",
    "end": "2687730"
  },
  {
    "text": "back but for dirty stream we first have to apply the undo and then the sequel",
    "start": "2687730",
    "end": "2693520"
  },
  {
    "text": "function on top and then we feed it back to the aggregator you combine all these",
    "start": "2693520",
    "end": "2699880"
  },
  {
    "text": "streams from different storage nodes to get the final result now this is one of",
    "start": "2699880",
    "end": "2707350"
  },
  {
    "text": "the problems there are several other problems like how do you get the list of pages in a way that doesn't affect your",
    "start": "2707350",
    "end": "2714370"
  },
  {
    "text": "OLTP performance how do you do flow control between the head node and storage nodes how do you",
    "start": "2714370",
    "end": "2720760"
  },
  {
    "text": "run these queries in a secure container on the storage remember storage is a multi-tenant storage how do you deal",
    "start": "2720760",
    "end": "2730150"
  },
  {
    "text": "with failures in the storage node so that if one storage in one storage node goes down you queries are not impacted",
    "start": "2730150",
    "end": "2738300"
  },
  {
    "text": "so if you're interested in knowing learning more about this we have a chalk talk on Thursday or you can catch me",
    "start": "2738300",
    "end": "2743920"
  },
  {
    "text": "after now this is an active area of work today",
    "start": "2743920",
    "end": "2751119"
  },
  {
    "text": "we push down predicates projections",
    "start": "2751119",
    "end": "2756780"
  },
  {
    "text": "sequel functions joints for joints we use semi joint reduction basically",
    "start": "2756780",
    "end": "2763710"
  },
  {
    "text": "effectively the idea is to grim build a bloom filter on the outer and send it",
    "start": "2763710",
    "end": "2769270"
  },
  {
    "text": "down to the storage and so storage don't can filter it out and you can see that",
    "start": "2769270",
    "end": "2774720"
  },
  {
    "start": "2773000",
    "end": "2799000"
  },
  {
    "text": "before that so we ran the again the TP CH like work load and this is a result",
    "start": "2774720",
    "end": "2780670"
  },
  {
    "text": "by the way just for PQ alone and so we saw 120 X x speed up peak speed up and",
    "start": "2780670",
    "end": "2789119"
  },
  {
    "text": "eight out of twenty two queries saw an order of magnitude better performance",
    "start": "2789119",
    "end": "2795540"
  },
  {
    "start": "2799000",
    "end": "2902000"
  },
  {
    "text": "so because I was mentioning about the filtering so on the left you can see I",
    "start": "2802460",
    "end": "2807690"
  },
  {
    "text": "ran a PPC H like query six which is a effectively a filter on the line item",
    "start": "2807690",
    "end": "2814349"
  },
  {
    "text": "table and you can see if you don't use",
    "start": "2814349",
    "end": "2819750"
  },
  {
    "text": "parallel query you will have to get 956",
    "start": "2819750",
    "end": "2825150"
  },
  {
    "text": "GB of data back from the storage node to head node and to process it in the database engine but if you use parallel",
    "start": "2825150",
    "end": "2832349"
  },
  {
    "text": "query you can filter down that to 4gb and that's the only the amount of data",
    "start": "2832349",
    "end": "2837539"
  },
  {
    "text": "you actually get back so that's 240 X reduction in the amount of data that is",
    "start": "2837539",
    "end": "2843000"
  },
  {
    "text": "being sent back from the storage node to head node and this is meaningful for you",
    "start": "2843000",
    "end": "2848579"
  },
  {
    "text": "guys because this means it doesn't affect your GP performance and on the",
    "start": "2848579",
    "end": "2855270"
  },
  {
    "text": "right you can see I use the data set 150 GB just to be easy on myself because the",
    "start": "2855270",
    "end": "2862020"
  },
  {
    "text": "buffer pool size is right around there the actual impact may be slightly more or less depending upon your workload but",
    "start": "2862020",
    "end": "2868710"
  },
  {
    "text": "the idea that I'm trying to reflect here is that as you're pushing your OLAP",
    "start": "2868710",
    "end": "2874529"
  },
  {
    "text": "queries on top of OLTP queries you can",
    "start": "2874529",
    "end": "2879539"
  },
  {
    "text": "see that it will create a buffer pool pollution once you issue the OLAP queries and we'll bring it bring in the",
    "start": "2879539",
    "end": "2887010"
  },
  {
    "text": "pages that are not hard set for LGB and which will impact the oil performance ok",
    "start": "2887010",
    "end": "2898920"
  },
  {
    "text": "let's talk about availability and durability",
    "start": "2898920",
    "end": "2903230"
  },
  {
    "text": "so first why do we need it why do we need a Z plus one failure",
    "start": "2904990",
    "end": "2912250"
  },
  {
    "text": "tolerance as I said before when you have",
    "start": "2912250",
    "end": "2918430"
  },
  {
    "text": "a large fleet it's natural to have nodes go coming up and go up and down but that",
    "start": "2918430",
    "end": "2925300"
  },
  {
    "text": "doesn't mean that easy cannot go down when these nodes are down it can happen",
    "start": "2925300",
    "end": "2930460"
  },
  {
    "text": "together so how does Aurora handle it we",
    "start": "2930460",
    "end": "2939970"
  },
  {
    "text": "keep six copies two copies per easy so even if let's say an easy go down goes",
    "start": "2939970",
    "end": "2945520"
  },
  {
    "text": "down you will lose two copies and additional node node goes down you will lose a third copy but since you write to",
    "start": "2945520",
    "end": "2953230"
  },
  {
    "text": "four places at the first place you still have one copy left somewhere because",
    "start": "2953230",
    "end": "2958390"
  },
  {
    "text": "when we did four out of six right column",
    "start": "2958390",
    "end": "2962308"
  },
  {
    "text": "you can also see if we would have done two out of three quorum that will not work let's say we did two out of three",
    "start": "2964710",
    "end": "2971890"
  },
  {
    "text": "quorum one copy in each ac and let's say an easy goes down plus an extra node",
    "start": "2971890",
    "end": "2978010"
  },
  {
    "text": "goes down so you lost two copies but you did two out of three quorum so maybe the",
    "start": "2978010",
    "end": "2983619"
  },
  {
    "text": "two copies are on right on these small two nodes and you have fundamentally",
    "start": "2983619",
    "end": "2989079"
  },
  {
    "text": "lost that on top we do continuous backup to s3",
    "start": "2989079",
    "end": "2999610"
  },
  {
    "start": "2993000",
    "end": "3062000"
  },
  {
    "text": "just briefly how it works is again storage volume is divided into partitions each partition has six",
    "start": "3000360",
    "end": "3007330"
  },
  {
    "text": "segments each segment is 10 GB or in",
    "start": "3007330",
    "end": "3013690"
  },
  {
    "text": "fact the partition is 10 GB and there are six segments in that and so what we",
    "start": "3013690",
    "end": "3019480"
  },
  {
    "text": "do is we take the snapshot of these segments and any log records that have",
    "start": "3019480",
    "end": "3026080"
  },
  {
    "text": "been written at as of this time and we stream that to s3 at the time of the",
    "start": "3026080",
    "end": "3035290"
  },
  {
    "text": "restore we first restored this snapshot of each segment and then apply the Delta",
    "start": "3035290",
    "end": "3042010"
  },
  {
    "text": "log records after that to get the latest version of the page all the pages and",
    "start": "3042010",
    "end": "3051720"
  },
  {
    "text": "since this is done in storage it doesn't impact your database performance it's",
    "start": "3051720",
    "end": "3058150"
  },
  {
    "text": "all happening in the background on different notes now there are times",
    "start": "3058150",
    "end": "3064600"
  },
  {
    "start": "3062000",
    "end": "3139000"
  },
  {
    "text": "where we accidentally deleted a round table or forgot to put a ver class in a",
    "start": "3064600",
    "end": "3072040"
  },
  {
    "text": "delete statement so we have a database",
    "start": "3072040",
    "end": "3077530"
  },
  {
    "text": "backtrack now you may ask why not just restore it point in time restore well",
    "start": "3077530",
    "end": "3083350"
  },
  {
    "text": "this is much faster operation you're talking about couple of minutes versus couple of hours and so the way it works",
    "start": "3083350",
    "end": "3093850"
  },
  {
    "text": "is you in this example you can see we went to t2 and we realize oops we made a",
    "start": "3093850",
    "end": "3101350"
  },
  {
    "text": "mistake so we went back to t1 so we mark that window invisible and then let's say",
    "start": "3101350",
    "end": "3107560"
  },
  {
    "text": "we went till t4 in purple and realize oh dammit we made another mistake so we",
    "start": "3107560",
    "end": "3113500"
  },
  {
    "text": "rolled back to t3 and so we marked that for band as well invisible now one thing",
    "start": "3113500",
    "end": "3119200"
  },
  {
    "text": "to note here is that these ranges or invisible markers are not destructive it's not like once you mark it it's you",
    "start": "3119200",
    "end": "3125920"
  },
  {
    "text": "lost it forever it says likes like a sly so you can yeah exactly you can go back and forth so you can say okay oh damn it",
    "start": "3125920",
    "end": "3133480"
  },
  {
    "text": "I went too far back so you can come back that's fine we talked about this so we",
    "start": "3133480",
    "end": "3143020"
  },
  {
    "start": "3139000",
    "end": "3169000"
  },
  {
    "text": "get instant crash you do recovery basically there is no check pointing and Aurora right with traditional databases",
    "start": "3143020",
    "end": "3149799"
  },
  {
    "text": "you have to replay all the logs since the last checkpoint and apply that in a single thread but with Aurora",
    "start": "3149799",
    "end": "3157390"
  },
  {
    "text": "there is no checkpoint so I mean it's all happening in the storage you just find out the point at which you accept",
    "start": "3157390",
    "end": "3163839"
  },
  {
    "text": "it the last right before the crash",
    "start": "3163839",
    "end": "3167250"
  },
  {
    "start": "3169000",
    "end": "3270000"
  },
  {
    "text": "you can have up to 15 lead replicas you can define the failover order on them",
    "start": "3172700",
    "end": "3180140"
  },
  {
    "text": "maybe you wanted certain only certain replicas in the same is you want to keep your master in the same is you can do",
    "start": "3180140",
    "end": "3186240"
  },
  {
    "text": "that by defining the order or maybe you want to keep your replicas or sorry the master of the same instance type so you",
    "start": "3186240",
    "end": "3193740"
  },
  {
    "text": "can define the order and that will be useful so when when the master node goes master node goes down we will choose",
    "start": "3193740",
    "end": "3200400"
  },
  {
    "text": "based upon the failover order which node to pick up or which instance to pick up as a candidate for promotion now",
    "start": "3200400",
    "end": "3210900"
  },
  {
    "text": "replicas share the storage with the master so anytime master goes down and replica takes over even though there is",
    "start": "3210900",
    "end": "3217980"
  },
  {
    "text": "a a synchronous replication going on you still don't lose the data the reason is it's the same storage so it may have",
    "start": "3217980",
    "end": "3227310"
  },
  {
    "text": "only thing is it may happen that the replicas may be little bit behind in memory so the pages and cache may be a",
    "start": "3227310",
    "end": "3234870"
  },
  {
    "text": "little bit behind right because the applique didn't know before the crash some of the things let's say 10 millisecond ago what happened in the on",
    "start": "3234870",
    "end": "3241110"
  },
  {
    "text": "the master and so what we do when we promote replicas we figure it out what",
    "start": "3241110",
    "end": "3247770"
  },
  {
    "text": "changed between between the time master crashed and the time I was on before",
    "start": "3247770",
    "end": "3254400"
  },
  {
    "text": "before the promotion and he find out the set of changes that happened during that",
    "start": "3254400",
    "end": "3260970"
  },
  {
    "text": "time and you just invalidate those pages and reread them from the storage back again the latest version",
    "start": "3260970",
    "end": "3268520"
  },
  {
    "start": "3270000",
    "end": "3314000"
  },
  {
    "text": "let's see how availability the continuous availability works with multi master with an example let's say there",
    "start": "3271799",
    "end": "3277990"
  },
  {
    "text": "is an app with three three nodes in the system and let's say master one went",
    "start": "3277990",
    "end": "3284170"
  },
  {
    "text": "down notice that the other nodes will",
    "start": "3284170",
    "end": "3289720"
  },
  {
    "text": "keep on operating as before and you can distribute your connections to the other",
    "start": "3289720",
    "end": "3297069"
  },
  {
    "text": "nodes and independently while that is happening the nor that crashed comes",
    "start": "3297069",
    "end": "3303250"
  },
  {
    "text": "back up it happening independently and once it's back you can add the",
    "start": "3303250",
    "end": "3309039"
  },
  {
    "text": "connections back global replication now",
    "start": "3309039",
    "end": "3316780"
  },
  {
    "start": "3314000",
    "end": "3369000"
  },
  {
    "text": "the two reasons where why our customers want global replication one is the dr",
    "start": "3316780",
    "end": "3323640"
  },
  {
    "text": "what happens if a whole region goes down and the other big reason is to be able",
    "start": "3323640",
    "end": "3329619"
  },
  {
    "text": "to provide lower latency leads in globally so a good example would be like",
    "start": "3329619",
    "end": "3336309"
  },
  {
    "text": "let's say I'm building a book a travel booking website then the searches that",
    "start": "3336309",
    "end": "3342700"
  },
  {
    "text": "is happening people are doing all the time in different countries and regions it will be served locally and so that",
    "start": "3342700",
    "end": "3350650"
  },
  {
    "text": "will be fast but the actual booking may happen in a single region because only very limited",
    "start": "3350650",
    "end": "3357039"
  },
  {
    "text": "searches results in the booking you can pay the cost at that time that's fine just one example may be useful let's",
    "start": "3357039",
    "end": "3367599"
  },
  {
    "text": "talk about manageability so this is again than feature we announced a few",
    "start": "3367599",
    "end": "3372609"
  },
  {
    "text": "months ago it's already in GA on five six",
    "start": "3372609",
    "end": "3378539"
  },
  {
    "text": "what this is it allows you to have a single dashboard to monitor and",
    "start": "3380829",
    "end": "3385839"
  },
  {
    "text": "root-cause your load issues and",
    "start": "3385839",
    "end": "3391349"
  },
  {
    "text": "essentially you can group by sequel hosts users and weights over time or",
    "start": "3391349",
    "end": "3400479"
  },
  {
    "text": "over any of the metrics I said so you can go sequel over weights sequel over users or users over hosts you can do",
    "start": "3400479",
    "end": "3407650"
  },
  {
    "text": "anything just check out the dashboard if you know it's better to just play with",
    "start": "3407650",
    "end": "3413589"
  },
  {
    "text": "it now the problem could be you have a",
    "start": "3413589",
    "end": "3420549"
  },
  {
    "text": "high CPU you you have a you know high i/o latency or you have a some LOC",
    "start": "3420549",
    "end": "3425619"
  },
  {
    "text": "weights going on but once you look at",
    "start": "3425619",
    "end": "3431049"
  },
  {
    "text": "this once you identify the problem you can take an action on them you may want",
    "start": "3431049",
    "end": "3436839"
  },
  {
    "text": "to tune your query or maybe add more resources depending upon the situation",
    "start": "3436839",
    "end": "3443249"
  },
  {
    "text": "again this is done in a way that doesn't affect the database performance we do",
    "start": "3443849",
    "end": "3450819"
  },
  {
    "text": "not just simply use performance schema because that has impact on the performance we did several optimizations",
    "start": "3450819",
    "end": "3458380"
  },
  {
    "text": "so that you guys don't see an impact to enable the performance insights we",
    "start": "3458380",
    "end": "3466539"
  },
  {
    "start": "3465000",
    "end": "3498000"
  },
  {
    "text": "simplified the management for you you",
    "start": "3466539",
    "end": "3472390"
  },
  {
    "text": "don't have to pre provision the storage it will automatically grow you don't",
    "start": "3472390",
    "end": "3478150"
  },
  {
    "text": "have to configure the number of replicas you can just define the minimum and maximum and based upon the load we will",
    "start": "3478150",
    "end": "3484390"
  },
  {
    "text": "grow it or shrink it in fact you can",
    "start": "3484390",
    "end": "3490150"
  },
  {
    "text": "define custom end points one four year old teepee reads and one for your OLAP queries",
    "start": "3490150",
    "end": "3496739"
  },
  {
    "start": "3498000",
    "end": "3542000"
  },
  {
    "text": "but serverless we can scale the instance up and down and so this is really useful",
    "start": "3498940",
    "end": "3508539"
  },
  {
    "text": "if you have let's say a dev test workload or you know some sporadic workload unpredictable workload you can",
    "start": "3508539",
    "end": "3516280"
  },
  {
    "text": "save a lot of cost even with the provisioned Aurora",
    "start": "3516280",
    "end": "3522760"
  },
  {
    "text": "which is a normal Aurora we announce stop and start feature so you can stop",
    "start": "3522760",
    "end": "3528280"
  },
  {
    "text": "an instance if you're not using it and will be stopped for you know seven days before it will automatically resume or",
    "start": "3528280",
    "end": "3534579"
  },
  {
    "text": "you can resume in between whatever you like and let's see how that works to",
    "start": "3534579",
    "end": "3547329"
  },
  {
    "start": "3542000",
    "end": "3600000"
  },
  {
    "text": "understand that to understand server less we need to first understand the three layers the database proxy layer",
    "start": "3547329",
    "end": "3553990"
  },
  {
    "text": "where your application is connecting to and notice that this is a multi-tenant",
    "start": "3553990",
    "end": "3560039"
  },
  {
    "text": "distributed proxy layer and so any one of the machine goes down it doesn't",
    "start": "3560039",
    "end": "3565089"
  },
  {
    "text": "affect your whole application it's maybe there's few connections going through that proxy second we have a warm pool of",
    "start": "3565089",
    "end": "3573640"
  },
  {
    "text": "Aurora instances of different sizes all ready to go so that we can scale you up",
    "start": "3573640",
    "end": "3579130"
  },
  {
    "text": "and down quickly and third on my left is",
    "start": "3579130",
    "end": "3584410"
  },
  {
    "text": "the monitoring service which is basically continuously monitoring your database and seeing what's the activity",
    "start": "3584410",
    "end": "3589750"
  },
  {
    "text": "going on and if it see there's a spike it will automatically go and if you'll",
    "start": "3589750",
    "end": "3594970"
  },
  {
    "text": "see it's ideal will automatically shrink it down now the way you may be you may",
    "start": "3594970",
    "end": "3600069"
  },
  {
    "text": "be asking how do you persist these connections how do you make sure that there is no impact to the application",
    "start": "3600069",
    "end": "3605289"
  },
  {
    "text": "and so the way that works is you first attach the new instance as a replica to",
    "start": "3605289",
    "end": "3611650"
  },
  {
    "text": "the old instance and then you signal the original writer to find a point where",
    "start": "3611650",
    "end": "3619720"
  },
  {
    "text": "there's no active transaction I'm sorry loosely speaking here but find a safe point basically and once you find out",
    "start": "3619720",
    "end": "3626440"
  },
  {
    "text": "that point since then you respond to all the proxy",
    "start": "3626440",
    "end": "3631800"
  },
  {
    "text": "with the message saying that please redirect the traffic to the new instance",
    "start": "3631800",
    "end": "3638400"
  },
  {
    "text": "and loop back anything that proxy is sending to you back to the proxy all the",
    "start": "3638400",
    "end": "3644460"
  },
  {
    "text": "incoming my sequel traffic back back to the proxy and once my sequel get this sorry once the proxy gets this message",
    "start": "3644460",
    "end": "3650580"
  },
  {
    "text": "the control message saying that okay you are asking me to redirect that's okay I got it you're scaling it will redirect",
    "start": "3650580",
    "end": "3657420"
  },
  {
    "text": "it will take all the my sequel incoming traffic from the node from the database known and redirected back to the new",
    "start": "3657420",
    "end": "3663510"
  },
  {
    "text": "node and once it learns that it will reader of course the new incoming traffic directly to the new node and",
    "start": "3663510",
    "end": "3669900"
  },
  {
    "text": "close the connection with the old node and so you can see you didn't lose your",
    "start": "3669900",
    "end": "3675240"
  },
  {
    "text": "connections the connection persist while we are doing all this mumbo-jumbo",
    "start": "3675240",
    "end": "3680900"
  },
  {
    "text": "here is the simulation we're on so you can see it for yourself",
    "start": "3682910",
    "end": "3687980"
  },
  {
    "text": "finally we are introduced a web service data API for your lambda applications on",
    "start": "3700200",
    "end": "3705609"
  },
  {
    "text": "top of service and this what this is is basically you can send us an HTTP",
    "start": "3705609",
    "end": "3712150"
  },
  {
    "text": "request and you know not no longer have to worry about connection pooling we",
    "start": "3712150",
    "end": "3717220"
  },
  {
    "text": "will take care for you here are some",
    "start": "3717220",
    "end": "3723789"
  },
  {
    "text": "related talks if you are interested in learning more about parallel query serverless and multi-master with that said thank",
    "start": "3723789",
    "end": "3735670"
  },
  {
    "text": "you everyone and have a good rest of the Congress [Applause]",
    "start": "3735670",
    "end": "3744949"
  }
]