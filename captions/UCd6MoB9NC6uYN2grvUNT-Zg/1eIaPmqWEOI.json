[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "good morning folks welcome to the",
    "start": "60",
    "end": "5250"
  },
  {
    "text": "getting started with Amazon redshift session I know it's a little early I hope you had some coffee and I'm not",
    "start": "5250",
    "end": "11400"
  },
  {
    "text": "gonna do is off on me big enogh before I get started so my name is Pavan",
    "start": "11400",
    "end": "18180"
  },
  {
    "text": "petha kuchi I am a principal product manager with Amazon redshift I've been",
    "start": "18180",
    "end": "23220"
  },
  {
    "text": "with AWS for about five years spent quite a bit of time on our dears a",
    "start": "23220",
    "end": "29130"
  },
  {
    "text": "little bit of time on ElastiCache so all within our database services portfolio",
    "start": "29130",
    "end": "35570"
  },
  {
    "start": "35000",
    "end": "53000"
  },
  {
    "text": "so today is an introductory session I'm going to briefly introduce what Amazon",
    "start": "35630",
    "end": "41730"
  },
  {
    "text": "redshift is we're going to talk a little bit about the benefits of the service how some of the customers are using it",
    "start": "41730",
    "end": "49280"
  },
  {
    "text": "and then a few tips to get started so",
    "start": "49280",
    "end": "55110"
  },
  {
    "start": "53000",
    "end": "157000"
  },
  {
    "text": "here is a brief overview of the AWS big data portfolio so we have services like",
    "start": "55110",
    "end": "61620"
  },
  {
    "text": "Direct Connect AWS import/export and snowball that enables you to move large",
    "start": "61620",
    "end": "66630"
  },
  {
    "text": "amounts of data and you can purchase that data across a variety of AWS",
    "start": "66630",
    "end": "72450"
  },
  {
    "text": "services from s3 to Amazon DynamoDB and you can run analytics on this data using",
    "start": "72450",
    "end": "81619"
  },
  {
    "text": "Amazon EMR Amazon redshift and a few other services we have within the big",
    "start": "81619",
    "end": "87119"
  },
  {
    "text": "data portfolio so Amazon redshift which is the topic of the day so it's a fast",
    "start": "87119",
    "end": "95000"
  },
  {
    "text": "simple and cost-effective petabyte scale data warehouses the",
    "start": "95000",
    "end": "100320"
  },
  {
    "text": "service Amazon EMR is easy to use analytics platform built around the",
    "start": "100320",
    "end": "106350"
  },
  {
    "text": "powerful Hadoop ecosystem and Amazon",
    "start": "106350",
    "end": "111479"
  },
  {
    "text": "machine learning is a service that has been internally used within amazon.com",
    "start": "111479",
    "end": "116850"
  },
  {
    "text": "for serving use cases such as a recommendation engine and then has been",
    "start": "116850",
    "end": "122340"
  },
  {
    "text": "externalized a year or so ago Amazon quick site is a",
    "start": "122340",
    "end": "127890"
  },
  {
    "text": "cloud powered business intelligence service that enables end-use to build visualizations create",
    "start": "127890",
    "end": "133700"
  },
  {
    "text": "storyboards and share their insights with different other users within the",
    "start": "133700",
    "end": "139099"
  },
  {
    "text": "organization so you have all the tools",
    "start": "139099",
    "end": "144349"
  },
  {
    "text": "to collect Big Data to process it to analyzer to share it with across your",
    "start": "144349",
    "end": "151790"
  },
  {
    "text": "organization using a few clicks on AWS",
    "start": "151790",
    "end": "156909"
  },
  {
    "text": "so what is Amazon redshift so like I described earlier Amazon redshift is",
    "start": "156909",
    "end": "162769"
  },
  {
    "start": "157000",
    "end": "248000"
  },
  {
    "text": "aimed at making data warehousing faster or simpler and cost-effective so to",
    "start": "162769",
    "end": "170480"
  },
  {
    "text": "begin with redshift is a relational data warehouse so we have Postgres based",
    "start": "170480",
    "end": "176510"
  },
  {
    "text": "front-end so which gives you the full blown sequel compatibility and we have",
    "start": "176510",
    "end": "183079"
  },
  {
    "text": "MPP or massively parallel processing back-end that enables you to scale from",
    "start": "183079",
    "end": "189290"
  },
  {
    "text": "anywhere between a few gigs to hundreds of terabytes to even multiple petabytes",
    "start": "189290",
    "end": "194620"
  },
  {
    "text": "it's significantly faster than row based stores and I'll talk a little bit about",
    "start": "194620",
    "end": "201470"
  },
  {
    "text": "why that is the case it's also significantly faster an order of magnitude faster than sequel on Hadoop",
    "start": "201470",
    "end": "209980"
  },
  {
    "text": "it's an order of magnitude cheaper than other massively parallel processing",
    "start": "209980",
    "end": "216049"
  },
  {
    "text": "database engines out there in the market so it's a managed service and we'll talk",
    "start": "216049",
    "end": "221269"
  },
  {
    "text": "a little bit about what that means at a high level all the database operations that you care about backups restoration",
    "start": "221269",
    "end": "228799"
  },
  {
    "text": "patching each a disaster recovery all of these things are made extremely simple",
    "start": "228799",
    "end": "236419"
  },
  {
    "text": "with the servers it starts at less than thousand dollars per terabyte per year and you can get",
    "start": "236419",
    "end": "243590"
  },
  {
    "text": "started with under with 25 cents per hour so traditionally data warehousing",
    "start": "243590",
    "end": "251209"
  },
  {
    "start": "248000",
    "end": "404000"
  },
  {
    "text": "vendors have focused on the Left leftmost part of the slide which is the enterprise segment and deals involve",
    "start": "251209",
    "end": "259220"
  },
  {
    "text": "multiple negotiations multi-year deployments multi-million dollars",
    "start": "259220",
    "end": "266020"
  },
  {
    "text": "so this the enterprise customer segment certainly has been adopting redshift",
    "start": "266780",
    "end": "273080"
  },
  {
    "text": "pretty widely and for some of the reasons that we described earlier it's significantly more cost effective than",
    "start": "273080",
    "end": "279590"
  },
  {
    "text": "legacy player platforms it's extremely easy to use and it enables higher DBA",
    "start": "279590",
    "end": "285860"
  },
  {
    "text": "productivity that said there are a lot of small companies and medium-sized",
    "start": "285860",
    "end": "292340"
  },
  {
    "text": "companies that in today's world generate a lot of data so you think about the social mobile and IOT companies that",
    "start": "292340",
    "end": "300530"
  },
  {
    "text": "also are generating a lot and lot of data and the option to analyze this data",
    "start": "300530",
    "end": "307610"
  },
  {
    "text": "has historically been around you know either they have to make do with a row based tool that can't scale and can't",
    "start": "307610",
    "end": "313729"
  },
  {
    "text": "perform as much as they want or use huddle based platforms which are also a",
    "start": "313729",
    "end": "321229"
  },
  {
    "text": "bit slower and harder to use especially for end users so redshift makes that",
    "start": "321229",
    "end": "330650"
  },
  {
    "text": "value proposition around performance significantly better it also given the",
    "start": "330650",
    "end": "336950"
  },
  {
    "text": "sequence compatibility and the post restaurant and it means end users don't have to learn programming scripting",
    "start": "336950",
    "end": "342320"
  },
  {
    "text": "figuring out how to move data across different data silos and again given the",
    "start": "342320",
    "end": "349910"
  },
  {
    "text": "compatibility aspects it works easily with any of the BI tools that you used",
    "start": "349910",
    "end": "355250"
  },
  {
    "text": "today it also works well with Hadoop and will talk a little bit about that and some of the other services such as",
    "start": "355250",
    "end": "361610"
  },
  {
    "text": "machine learning and real-time streaming of data so in addition to this",
    "start": "361610",
    "end": "367669"
  },
  {
    "text": "particular segment we are also seeing significant adoption of red shift red",
    "start": "367669",
    "end": "373070"
  },
  {
    "text": "shift within the SAS community and one of the important reasons there is SAS",
    "start": "373070",
    "end": "379940"
  },
  {
    "text": "companies not only want to provide applications to their customers they also want to provide intelligence from",
    "start": "379940",
    "end": "386690"
  },
  {
    "text": "the data that they're gathering to end-users as well and given the model of pay-as-you-go",
    "start": "386690",
    "end": "393580"
  },
  {
    "text": "and grow as you need that mortal alliance quite well with how their",
    "start": "393580",
    "end": "398919"
  },
  {
    "text": "business works and how a Tobias works as well and of course that is part of the appeal as well so this is a quick",
    "start": "398919",
    "end": "406870"
  },
  {
    "start": "404000",
    "end": "455000"
  },
  {
    "text": "snapshot of the Forrester wave which was released late last year and as you can",
    "start": "406870",
    "end": "412659"
  },
  {
    "text": "see here I don't know how to use a pointer but so Brett Schiff has been",
    "start": "412659",
    "end": "419879"
  },
  {
    "text": "noted as one of the leaders within the enterprise data warehouse Forrester wave and redshift is just about three years",
    "start": "419879",
    "end": "429190"
  },
  {
    "text": "old right now and given its placement alongside companies that have had data",
    "start": "429190",
    "end": "436539"
  },
  {
    "text": "warehousing technologies for decades so I think we've come a long way within a",
    "start": "436539",
    "end": "442360"
  },
  {
    "text": "short three years time span and primarily because of the customer momentum because of the feature set and",
    "start": "442360",
    "end": "450520"
  },
  {
    "text": "the fast paced feature delivery as well a quick snapshot of some of the",
    "start": "450520",
    "end": "457599"
  },
  {
    "text": "customers that have been using Amazon redshift and we'll talk a little bit",
    "start": "457599",
    "end": "463330"
  },
  {
    "text": "about Nasdaq shortly so let's talk a",
    "start": "463330",
    "end": "468400"
  },
  {
    "start": "467000",
    "end": "577000"
  },
  {
    "text": "little bit about Amazon redshift architecture so how does the MPP system work so redshift has a leader node which",
    "start": "468400",
    "end": "477520"
  },
  {
    "text": "acts as a simple sequel endpoint so when you're executing queries through a bi",
    "start": "477520",
    "end": "482830"
  },
  {
    "text": "tool or a sequel client the leader node is the one that accepts your queries it",
    "start": "482830",
    "end": "489310"
  },
  {
    "text": "stores the metadata associated with your data model the tables views and objects",
    "start": "489310",
    "end": "495190"
  },
  {
    "text": "that you create so that metadata is part of the leader node it optimizes the",
    "start": "495190",
    "end": "500289"
  },
  {
    "text": "query plan and it generates a byte code after parse in the query and pushes that",
    "start": "500289",
    "end": "507310"
  },
  {
    "text": "code into the compute node so that each compute node can execute the queries and parallel so the computer themselves they",
    "start": "507310",
    "end": "515890"
  },
  {
    "text": "are they have the local column they store the data locally in the columnar",
    "start": "515890",
    "end": "522279"
  },
  {
    "text": "format and all the operations that you care about including query execution data loading",
    "start": "522279",
    "end": "530920"
  },
  {
    "text": "backup saturation resizes all of these operations happen pattern across the",
    "start": "530920",
    "end": "536800"
  },
  {
    "text": "compute nodes which means as you add more nodes your performance of these operations scales linearly surprising as",
    "start": "536800",
    "end": "545680"
  },
  {
    "text": "we described earlier starts at 25 cents per hour you can start with 160 gigs and",
    "start": "545680",
    "end": "551080"
  },
  {
    "text": "grow up to 2 petabytes of compressed data so we have two different platforms an SSD based platform and an HDD based",
    "start": "551080",
    "end": "559360"
  },
  {
    "text": "platform and so the SSD based platform is five times more expensive per",
    "start": "559360",
    "end": "565660"
  },
  {
    "text": "gigabyte but it also performs 10 to 15 times better than the HDD platform so",
    "start": "565660",
    "end": "571960"
  },
  {
    "text": "depending on your price to performance requirements you can pick one or the other and so their customers a that use",
    "start": "571960",
    "end": "579610"
  },
  {
    "start": "577000",
    "end": "707000"
  },
  {
    "text": "both as well the lifecycle the older data into HDD based restores they keep",
    "start": "579610",
    "end": "585730"
  },
  {
    "text": "the hard data in the SSD based clusters for faster performance so we talked a",
    "start": "585730",
    "end": "595030"
  },
  {
    "text": "little bit about how Amazon redshift performs significantly better than some",
    "start": "595030",
    "end": "601030"
  },
  {
    "text": "of the other alternatives and so why is that the case so columnar stores as some of you may be",
    "start": "601030",
    "end": "606100"
  },
  {
    "text": "familiar like redshift the consume dramatically less i/o compared to those",
    "start": "606100",
    "end": "611260"
  },
  {
    "text": "stores and the reason for that is imagine a table with hundred columns and if you're querying five columns out of",
    "start": "611260",
    "end": "618010"
  },
  {
    "text": "that which is 5% of the data when you're using a roast or it gets all the data",
    "start": "618010",
    "end": "624910"
  },
  {
    "text": "and discards 95% of it that you don't want but it still performs at i/o",
    "start": "624910",
    "end": "630630"
  },
  {
    "text": "columnist roles such as redshift only gets accesses the columns that you're",
    "start": "630630",
    "end": "636430"
  },
  {
    "text": "specifying in a query columnar format also lends to high level of data",
    "start": "636430",
    "end": "642370"
  },
  {
    "text": "compression so imagine having column like general where you just have two",
    "start": "642370",
    "end": "648130"
  },
  {
    "text": "values and so that compresses significantly when compression not only",
    "start": "648130",
    "end": "654339"
  },
  {
    "text": "lends itself to savings of space it also lends for better performance because redshift",
    "start": "654339",
    "end": "660710"
  },
  {
    "text": "performs queries uncompressed data and the more comprehensive data the less I owe that has read it redshift also uses",
    "start": "660710",
    "end": "669440"
  },
  {
    "text": "what we call a zone maps and we will talk about zone maps and how sort casework at a high level zone map store",
    "start": "669440",
    "end": "676810"
  },
  {
    "text": "can be thought of as indexes that are stored in memory so that when we execute",
    "start": "676810",
    "end": "681920"
  },
  {
    "text": "a query only the bare minimum amount of balaam",
    "start": "681920",
    "end": "687230"
  },
  {
    "text": "minimum number of blocks are fetched from the data storage with directed a",
    "start": "687230",
    "end": "694430"
  },
  {
    "text": "storage and large data block sizes so we use one megabyte block sizes within redshift so all of these lend themselves",
    "start": "694430",
    "end": "701390"
  },
  {
    "text": "for higher query throughput and performance we talked a bit about how",
    "start": "701390",
    "end": "710230"
  },
  {
    "text": "all of these operations are managed in the compute node level hence penalized and scale linearly so",
    "start": "710230",
    "end": "720710"
  },
  {
    "start": "720000",
    "end": "765000"
  },
  {
    "text": "that also gets you better performance and from a hardware perspective we work",
    "start": "720710",
    "end": "726980"
  },
  {
    "text": "with ec2 we work with an infrastructure team to ensure that the platforms that we support are optimized for high i/o",
    "start": "726980",
    "end": "735590"
  },
  {
    "text": "intensive workloads so we are able to do 4 gigabytes of scans per second per node",
    "start": "735590",
    "end": "742040"
  },
  {
    "text": "within redshift and this number is only gonna go up as we add more as we refresh",
    "start": "742040",
    "end": "747890"
  },
  {
    "text": "the hardware with enhanced networking we are able to process over million packets for nor per second and we also have a",
    "start": "747890",
    "end": "757520"
  },
  {
    "text": "regular cadence of order past improvements for which you don't have to do anything they get past and improve",
    "start": "757520",
    "end": "763250"
  },
  {
    "text": "the performance of the system so a couple of examples recent examples about",
    "start": "763250",
    "end": "768350"
  },
  {
    "start": "765000",
    "end": "851000"
  },
  {
    "text": "the performance improvements that we have done so the the second generation HDD instance type it provides twice as",
    "start": "768350",
    "end": "775970"
  },
  {
    "text": "much memory twice as much of compute capacity 1.5 times this throughput as the older generation HDD based instance",
    "start": "775970",
    "end": "784040"
  },
  {
    "text": "type and it's priced at the exact same price so you're not paying anything extra for all of the additional computational",
    "start": "784040",
    "end": "792860"
  },
  {
    "text": "resources you are getting out of this new generation instance type it provides performant improvement of over 50% on",
    "start": "792860",
    "end": "800540"
  },
  {
    "text": "average in addition we have released",
    "start": "800540",
    "end": "805660"
  },
  {
    "text": "they've improved the i/o and commit performance and ruled out a few",
    "start": "805660",
    "end": "810920"
  },
  {
    "text": "improvements associated with that earlier this year and what this does is it reduces amount of time that is needed",
    "start": "810920",
    "end": "817550"
  },
  {
    "text": "to commit data which means it improves the write throughput and indirectly it",
    "start": "817550",
    "end": "823250"
  },
  {
    "text": "improves the read throughput as well and so this demonstrate this is the actual",
    "start": "823250",
    "end": "830750"
  },
  {
    "text": "customer workload that demonstrated a 35 percent improvement in TPC das that also that demonstrated similar improvement as",
    "start": "830750",
    "end": "837350"
  },
  {
    "text": "well so at a high level the idea is within the last few months the performance of the system improved by",
    "start": "837350",
    "end": "843290"
  },
  {
    "text": "about 2x you know without customers having to do anything to realize the",
    "start": "843290",
    "end": "849200"
  },
  {
    "text": "benefits redshift as we described",
    "start": "849200",
    "end": "854390"
  },
  {
    "start": "851000",
    "end": "890000"
  },
  {
    "text": "discussed is priced at starts at thousand dollars per terabyte per year so that is for a three-year reservation",
    "start": "854390",
    "end": "862000"
  },
  {
    "text": "and the HDD platform so here is a range of prices as you look at dc1 as you look",
    "start": "862000",
    "end": "870140"
  },
  {
    "text": "at the different pricing models pricing is pretty straightforward it's just the number of nodes that you are using times",
    "start": "870140",
    "end": "876020"
  },
  {
    "text": "the price per hour for that particular node there are no extra charges for the leader node the",
    "start": "876020",
    "end": "882529"
  },
  {
    "text": "charges here are for compressed gigabyte or terabyte so we described discussed",
    "start": "882529",
    "end": "893540"
  },
  {
    "start": "890000",
    "end": "1026000"
  },
  {
    "text": "about redshift being a managed service so what does a managed service mean so",
    "start": "893540",
    "end": "899329"
  },
  {
    "text": "at a high level provisioning is extremely straightforward so with a few clicks you can get started with the",
    "start": "899329",
    "end": "905600"
  },
  {
    "text": "cluster that ranges from a few gigs to few terabytes to multiple petabytes you",
    "start": "905600",
    "end": "913070"
  },
  {
    "text": "can backups associated with these as you load data it gets continuously backed up",
    "start": "913070",
    "end": "920720"
  },
  {
    "text": "and incrementally backed up into s3 their multiple copies of data",
    "start": "920720",
    "end": "926330"
  },
  {
    "text": "within the cluster so as you write data into a particular node it's not synchronously replicated into a",
    "start": "926330",
    "end": "932720"
  },
  {
    "text": "different node within the cluster which means if there is a node failure we are able to replace that node automatically",
    "start": "932720",
    "end": "940550"
  },
  {
    "text": "and hydrated behind-the-scenes data is also incremental backed up into s3 so",
    "start": "940550",
    "end": "947810"
  },
  {
    "text": "what that what that means is if you have at Alibi size cluster and if you only",
    "start": "947810",
    "end": "952970"
  },
  {
    "text": "update at 10 gigs since the last backup only that 10 gigs gets backed up into a",
    "start": "952970",
    "end": "958640"
  },
  {
    "text": "string so we develop a lineage as you backup additional data and you can also",
    "start": "958640",
    "end": "965840"
  },
  {
    "text": "keep backups and sync across regions so for disaster recovery so you can say backup my backups in u.s. East region",
    "start": "965840",
    "end": "973730"
  },
  {
    "text": "needs to be synced up with backups in the u.s. west region and it gets done automatically behind the scenes and one",
    "start": "973730",
    "end": "981530"
  },
  {
    "text": "of the interesting features that is pretty understated is what we call a",
    "start": "981530",
    "end": "986810"
  },
  {
    "text": "streaming restore so irrespective of the size of the database or the backup so",
    "start": "986810",
    "end": "993110"
  },
  {
    "text": "whether you have a terabyte or tender bytes restoring from that backup takes just a few minutes so what we do is we",
    "start": "993110",
    "end": "999620"
  },
  {
    "text": "provision a cluster we load the metadata and open up access for the cluster to",
    "start": "999620",
    "end": "1004690"
  },
  {
    "text": "you while lazily loading the backup from the from s3 into local storage so you",
    "start": "1004690",
    "end": "1011260"
  },
  {
    "text": "can perform reads you can perform writes while the block stream into the cluster",
    "start": "1011260",
    "end": "1016480"
  },
  {
    "text": "and of course the performance is going to be a bit degraded until the nodes are completely hydrated but what this sort",
    "start": "1016480",
    "end": "1023890"
  },
  {
    "text": "of what streaming restore enables is dealing with failures in a significantly",
    "start": "1023890",
    "end": "1031750"
  },
  {
    "text": "faster manner so when you have local failures within a cluster so whether",
    "start": "1031750",
    "end": "1037750"
  },
  {
    "text": "multiple discs failed fails within a node or a node fails within a cluster or",
    "start": "1037750",
    "end": "1044010"
  },
  {
    "text": "we automatically monitor that and provision a replacement and given that",
    "start": "1044010",
    "end": "1051640"
  },
  {
    "text": "we have data mirrored in other nodes within the cluster we hydrate the newly prevision node with",
    "start": "1051640",
    "end": "1058720"
  },
  {
    "text": "data from the mirror so all of these is taken care of automatically if the",
    "start": "1058720",
    "end": "1065950"
  },
  {
    "text": "failure is local to particular availability zone failures that are at",
    "start": "1065950",
    "end": "1071200"
  },
  {
    "text": "an availability zone level are extremely rare but they do happen and when that",
    "start": "1071200",
    "end": "1076270"
  },
  {
    "text": "happens given you have your latest back of stored in s3 you can restore from it",
    "start": "1076270",
    "end": "1084580"
  },
  {
    "text": "very quickly with the streaming restore functionality and even if regional level",
    "start": "1084580",
    "end": "1092620"
  },
  {
    "text": "disaster happens which again is even more rare you can keep back given that you can",
    "start": "1092620",
    "end": "1099610"
  },
  {
    "text": "keep backups across regions in sync you can restore your cluster very quickly in a different region for business",
    "start": "1099610",
    "end": "1106510"
  },
  {
    "text": "continuity purposes security is",
    "start": "1106510",
    "end": "1112120"
  },
  {
    "start": "1109000",
    "end": "1247000"
  },
  {
    "text": "something that is built-in across every component of redshift you can load data",
    "start": "1112120",
    "end": "1118900"
  },
  {
    "text": "in an encrypted manner from s3 you can use client-side encryption you can still use service ID encryption to encrypt",
    "start": "1118900",
    "end": "1125680"
  },
  {
    "text": "data with an s3 and load encrypted data from s3 into redshift and once the data",
    "start": "1125680",
    "end": "1131050"
  },
  {
    "text": "comes into redshift you can encrypt it at rest all the blocks on the various",
    "start": "1131050",
    "end": "1137500"
  },
  {
    "text": "different disks get their own encryption keys and all the block level encryption",
    "start": "1137500",
    "end": "1143260"
  },
  {
    "text": "keys are encrypted with the cluster level key which is in turn encrypted with the master key so we use envelope",
    "start": "1143260",
    "end": "1149610"
  },
  {
    "text": "hierarchical and all of based encryption while connecting to the database you can",
    "start": "1149610",
    "end": "1156130"
  },
  {
    "text": "use SSL so encryption in transit you can",
    "start": "1156130",
    "end": "1161530"
  },
  {
    "text": "use EC DHT ciphers for perfect forward security and from a network networking",
    "start": "1161530",
    "end": "1168970"
  },
  {
    "text": "perspective Amazon BBC provides you with network isolation and we use a slightly",
    "start": "1168970",
    "end": "1176920"
  },
  {
    "text": "different model for BBC so when you create a redshift cluster and deployed within your own V PC the leader node",
    "start": "1176920",
    "end": "1185830"
  },
  {
    "text": "becomes part of your V PC the compute no themselves are part of an Amazon",
    "start": "1185830",
    "end": "1191730"
  },
  {
    "text": "internal vbc which nobody else has access to including yourself so you can",
    "start": "1191730",
    "end": "1197040"
  },
  {
    "text": "access the data within the compute nodes through the leader node but you don't have direct access to the database files",
    "start": "1197040",
    "end": "1204170"
  },
  {
    "text": "for security purposes you can order",
    "start": "1204170",
    "end": "1210800"
  },
  {
    "text": "logins you can order it user activity so all of this can be dumped into an s3",
    "start": "1210800",
    "end": "1217050"
  },
  {
    "text": "bucket that you own and you can analyze it at a later point we have a double scroll trail integration which enables",
    "start": "1217050",
    "end": "1223290"
  },
  {
    "text": "you to you to track all the API calls that have been made on your particular",
    "start": "1223290",
    "end": "1228360"
  },
  {
    "text": "cluster for compliance purposes so the multiple compliance initiatives that",
    "start": "1228360",
    "end": "1233940"
  },
  {
    "text": "we've been through sock one two three the PCI DSS FedRAMP which is the federal",
    "start": "1233940",
    "end": "1239850"
  },
  {
    "text": "government compliance standard and HIPAA /ba which is for healthcare so unlike",
    "start": "1239850",
    "end": "1249780"
  },
  {
    "text": "on-premises software we innovate pretty quickly so typically you pay premium for",
    "start": "1249780",
    "end": "1256710"
  },
  {
    "text": "getting maintenance updates for on promises software you also get patches",
    "start": "1256710",
    "end": "1263700"
  },
  {
    "text": "or you know new feature enhancements probably once every six months or a year so with redshift we we have a continuous",
    "start": "1263700",
    "end": "1273090"
  },
  {
    "text": "deployment model so we release improvements and features almost a two-week cadence and all these",
    "start": "1273090",
    "end": "1280910"
  },
  {
    "text": "improvements are directly patched into your cluster without you having to worrying worried about it so they have",
    "start": "1280910",
    "end": "1289920"
  },
  {
    "start": "1288000",
    "end": "1378000"
  },
  {
    "text": "various data science related functionality available to you so",
    "start": "1289920",
    "end": "1295170"
  },
  {
    "text": "approximate functions as an example so it enables you to get not necessarily",
    "start": "1295170",
    "end": "1305550"
  },
  {
    "text": "precise but closely accurate data within 2% error range for queries including",
    "start": "1305550",
    "end": "1311960"
  },
  {
    "text": "select you know count distinct and such and so you get results very fast but",
    "start": "1311960",
    "end": "1318990"
  },
  {
    "text": "they're within the 2% error range and there are lots of use cases where statistically significant data is good enough and",
    "start": "1318990",
    "end": "1325769"
  },
  {
    "text": "getting it faster is important so we have user-defined functions which enable",
    "start": "1325769",
    "end": "1332009"
  },
  {
    "text": "you to write your own Python functions that can be embedded in sequel you can also use thousands of functions that are",
    "start": "1332009",
    "end": "1338340"
  },
  {
    "text": "available to through native Python libraries as well so the machine",
    "start": "1338340",
    "end": "1344639"
  },
  {
    "text": "learning service we talked a little bit about it and you can also use ourselves in conjunction with redshift pretty",
    "start": "1344639",
    "end": "1351360"
  },
  {
    "text": "easily and we work with these vendors pretty closely for example it's as if",
    "start": "1351360",
    "end": "1356759"
  },
  {
    "text": "you're writing a query on sass instead of just fetching the raw data and processing it on sighs what's as does is",
    "start": "1356759",
    "end": "1363960"
  },
  {
    "text": "it Maps functions from sass into redshift and processes the queries natively on redshift so that you can",
    "start": "1363960",
    "end": "1370230"
  },
  {
    "text": "take advantage of the MPP functionality and then the results are returned to you through sass and this all transparent we",
    "start": "1370230",
    "end": "1380639"
  },
  {
    "start": "1378000",
    "end": "1416000"
  },
  {
    "text": "have a fairly large and brewing ecosystem of partners we respect the investments that you made in your data",
    "start": "1380639",
    "end": "1387450"
  },
  {
    "text": "integration side in your business intelligence side so partners including",
    "start": "1387450",
    "end": "1394159"
  },
  {
    "text": "informatica tableau MicroStrategy so we work with all of these partners very",
    "start": "1394159",
    "end": "1400019"
  },
  {
    "text": "closely that ensure that our solutions mutually are certified and you have",
    "start": "1400019",
    "end": "1405929"
  },
  {
    "text": "support ability we have also have a range of system integrators that can help you with migration and",
    "start": "1405929",
    "end": "1414169"
  },
  {
    "text": "implementation so it it appears in",
    "start": "1414169",
    "end": "1419580"
  },
  {
    "start": "1416000",
    "end": "1446000"
  },
  {
    "text": "general we believe in the philosophy of service-oriented architectures so we don't sell monolithic solutions that you",
    "start": "1419580",
    "end": "1428759"
  },
  {
    "text": "may or may not use you can pick and choose between multiple services and",
    "start": "1428759",
    "end": "1434460"
  },
  {
    "text": "a lot of them work with Richard for various use cases so let's talk a little",
    "start": "1434460",
    "end": "1440880"
  },
  {
    "text": "bit about some of the use cases and how customers are leveraging redshift",
    "start": "1440880",
    "end": "1447230"
  },
  {
    "start": "1446000",
    "end": "1527000"
  },
  {
    "text": "so NTT DoCoMo so they are Japan's largest mobile carrier they have 68",
    "start": "1447230",
    "end": "1454530"
  },
  {
    "text": "million customers the generate tens of terabytes of data per day across the",
    "start": "1454530",
    "end": "1460290"
  },
  {
    "text": "mobile network and over all the data set sizes to petabytes compressed around six",
    "start": "1460290",
    "end": "1466980"
  },
  {
    "text": "petabytes uncompressed and they use this data for a variety of data science",
    "start": "1466980",
    "end": "1474780"
  },
  {
    "text": "applications across the marketing operations functions and they were using",
    "start": "1474780",
    "end": "1481320"
  },
  {
    "text": "greenplum on-premises for this platform and they encountered scaling challenges",
    "start": "1481320",
    "end": "1488250"
  },
  {
    "text": "given the amount of data that they were bringing in on a daily basis and also performance issues and they wanted a",
    "start": "1488250",
    "end": "1495570"
  },
  {
    "text": "platform that has that meets their security bar and give them it's the telecom information and it's very",
    "start": "1495570",
    "end": "1503880"
  },
  {
    "text": "sensitive they wanted a very secure platform and they also needed a hybrid environment because this particular data",
    "start": "1503880",
    "end": "1511200"
  },
  {
    "text": "is generated on-premises and they were not going to move there on promises transactional systems into the cloud yet",
    "start": "1511200",
    "end": "1518460"
  },
  {
    "text": "so they wanted a analytics environment that can interoperate with the",
    "start": "1518460",
    "end": "1523650"
  },
  {
    "text": "conferences data center so they migrated",
    "start": "1523650",
    "end": "1530220"
  },
  {
    "text": "to redshift about a year ago and so this is a high-level view of their architecture their data is generated on",
    "start": "1530220",
    "end": "1537150"
  },
  {
    "text": "promises they have a direct connect line with one of the data centers of AWS the",
    "start": "1537150",
    "end": "1543960"
  },
  {
    "text": "process this data they move that data into s3 on a regular basis and then load",
    "start": "1543960",
    "end": "1549210"
  },
  {
    "text": "that internet shift it's a very large cluster 125 node ds2 8 excels $4000 ecp",
    "start": "1549210",
    "end": "1558570"
  },
  {
    "text": "use and 30 terabytes of RAM is just huge given the amount of data that they are analyzing so what the founders of the my",
    "start": "1558570",
    "end": "1569410"
  },
  {
    "text": "a lot of their analytics queries improved by over 10x they are able to",
    "start": "1569410",
    "end": "1576190"
  },
  {
    "text": "roll out new business intelligence applications very quickly and they have",
    "start": "1576190",
    "end": "1581830"
  },
  {
    "text": "significantly less operational overhead another example is Nasdaq and as you're",
    "start": "1581830",
    "end": "1589540"
  },
  {
    "start": "1586000",
    "end": "1670000"
  },
  {
    "text": "pretty familiar they run exchanges across a lot of different countries in the world and",
    "start": "1589540",
    "end": "1596710"
  },
  {
    "text": "their use cases around analyzing orders ask bids and trade executions of market",
    "start": "1596710",
    "end": "1603700"
  },
  {
    "text": "data across seven exchanges they generate about seven billion rows of",
    "start": "1603700",
    "end": "1609429"
  },
  {
    "text": "data ID and the use case is largely around identifying analyzing client",
    "start": "1609429",
    "end": "1616690"
  },
  {
    "text": "activity fraud detection surveillance billing etc and they were using the",
    "start": "1616690",
    "end": "1622179"
  },
  {
    "text": "sequel server instance on premises for this use case and it was very expensive",
    "start": "1622179",
    "end": "1628870"
  },
  {
    "text": "for them they were spending over a million dollars a year and the problem that they had was around capacity",
    "start": "1628870",
    "end": "1634780"
  },
  {
    "text": "similar to NTT DoCoMo they were generating a lot of data a day and so they had to go polish the data sitting",
    "start": "1634780",
    "end": "1640750"
  },
  {
    "text": "on their data warehousing environment before they move new data in so the the",
    "start": "1640750",
    "end": "1646600"
  },
  {
    "text": "data set that they could store in that environment was just about a year and so",
    "start": "1646600",
    "end": "1652570"
  },
  {
    "text": "their requirement was a solution that provides some lower total cost of",
    "start": "1652570",
    "end": "1657640"
  },
  {
    "text": "ownership and similar to NTT DoCoMo you can imagine the financial data there is",
    "start": "1657640",
    "end": "1663910"
  },
  {
    "text": "stringent security requirements regulatory needs as well so this is",
    "start": "1663910",
    "end": "1671860"
  },
  {
    "text": "their architecture it's a little bit similar to the previous example in that it's also a hybrid environment so they",
    "start": "1671860",
    "end": "1678340"
  },
  {
    "text": "have their transaction systems running on premises and then they load that data",
    "start": "1678340",
    "end": "1685020"
  },
  {
    "text": "periodically into s3 and then copy it into redshift and they have over 300",
    "start": "1685020",
    "end": "1694690"
  },
  {
    "text": "terabytes of compressed data or about a petabyte of uncompressed and two point",
    "start": "1694690",
    "end": "1699880"
  },
  {
    "text": "seven trillion rows all of that involves derivations what nasstrac does is somewhat you know LD",
    "start": "1699880",
    "end": "1706410"
  },
  {
    "text": "like in that the load the data the processors the derive columns based on",
    "start": "1706410",
    "end": "1713780"
  },
  {
    "text": "different computational requirements and then create new tables based off it so",
    "start": "1713780",
    "end": "1720540"
  },
  {
    "text": "all of that they do within redshift and they have lot of tables with over a",
    "start": "1720540",
    "end": "1727680"
  },
  {
    "text": "billion rows and actually a tables with hundred billion rules so overall the migration took about",
    "start": "1727680",
    "end": "1733020"
  },
  {
    "text": "seven man months so that kind of gives you a rough ballpark around you know the",
    "start": "1733020",
    "end": "1738890"
  },
  {
    "text": "effort involved in order to migrate from a different database engine and the",
    "start": "1738890",
    "end": "1745440"
  },
  {
    "text": "they're able to get this at a fraction of the cost of their legacy environment they have a lot more storage they're",
    "start": "1745440",
    "end": "1752730"
  },
  {
    "text": "able to grow more and similar to the",
    "start": "1752730",
    "end": "1758280"
  },
  {
    "text": "previous example they got better performance for a variety of queries that they have migrated so I'm gonna",
    "start": "1758280",
    "end": "1768120"
  },
  {
    "text": "talk a little bit about how to get started a few tips and tricks so in general the provisioning experience on",
    "start": "1768120",
    "end": "1775290"
  },
  {
    "text": "redshift is very straightforward all you have to do is go to the management console you create you specify a cluster",
    "start": "1775290",
    "end": "1782340"
  },
  {
    "start": "1777000",
    "end": "1820000"
  },
  {
    "text": "identifier and you know the master user the password associated with it so this",
    "start": "1782340",
    "end": "1790290"
  },
  {
    "text": "will be your sis admin like user you specify the type of nodes you want to",
    "start": "1790290",
    "end": "1797790"
  },
  {
    "text": "run you specify the number of nodes you want to run in your faster and so that could either be dependent on the amount",
    "start": "1797790",
    "end": "1804360"
  },
  {
    "text": "of data that you're bringing in or it could be dependent on the performance requirements and over time as you start",
    "start": "1804360",
    "end": "1811020"
  },
  {
    "text": "using redshift you might realize you need fewer nodes or you might need more nodes and so as you realize the need for",
    "start": "1811020",
    "end": "1823280"
  },
  {
    "start": "1820000",
    "end": "1840000"
  },
  {
    "text": "reducing the size of your cluster or increasing the size of your cluster could be dependent on the growth of data",
    "start": "1823280",
    "end": "1828330"
  },
  {
    "text": "it could be dependent on your performance requirements it's very straightforward to do a resize operation",
    "start": "1828330",
    "end": "1834360"
  },
  {
    "text": "on redshift so you can go from pen notes already known so whatever is your target size",
    "start": "1834360",
    "end": "1840299"
  },
  {
    "start": "1840000",
    "end": "1883000"
  },
  {
    "text": "requirements and so what happens with the research process is your cluster",
    "start": "1840299",
    "end": "1845549"
  },
  {
    "text": "remains online for Reed Reed taxes as during the research process we provision",
    "start": "1845549",
    "end": "1851370"
  },
  {
    "text": "a new cluster with the target configuration the copy data from your source nodes to your target nodes and",
    "start": "1851370",
    "end": "1858990"
  },
  {
    "text": "all of that happens in parallel and of course there's redistribution involved because you know you've distributed data",
    "start": "1858990",
    "end": "1864510"
  },
  {
    "text": "in a certain way and you're adding more node so this redistribution involved but all of that happens while you are able",
    "start": "1864510",
    "end": "1872309"
  },
  {
    "text": "to access your cluster for reporting or reads and you're only charged for the",
    "start": "1872309",
    "end": "1879030"
  },
  {
    "text": "source cluster until the research operation is complete so data modeling",
    "start": "1879030",
    "end": "1885539"
  },
  {
    "start": "1883000",
    "end": "2005000"
  },
  {
    "text": "so let's talk a little bit about some of the things that are different for",
    "start": "1885539",
    "end": "1890970"
  },
  {
    "text": "redshift as you think about using it from a different platform so salt keys",
    "start": "1890970",
    "end": "1896429"
  },
  {
    "text": "and distribution keys are pretty important components of redshift let's",
    "start": "1896429",
    "end": "1902039"
  },
  {
    "text": "understand a little bit about it so let's take a query let's say you have a log a time series log data and you're",
    "start": "1902039",
    "end": "1912150"
  },
  {
    "text": "trying to execute a query select round start from these logs where data equals a particular date so if you have an",
    "start": "1912150",
    "end": "1919770"
  },
  {
    "text": "unsorted table and in this particular example we have four blocks and each block like we discussed earlier is a",
    "start": "1919770",
    "end": "1926190"
  },
  {
    "text": "megabyte and so each column is broken down into multiple megabyte blocks so we",
    "start": "1926190",
    "end": "1932039"
  },
  {
    "text": "are here taking a particular column and now if this for what happens this each",
    "start": "1932039",
    "end": "1937470"
  },
  {
    "text": "blocks min and Max values gets told as zone maps in memory so you have force",
    "start": "1937470",
    "end": "1943799"
  },
  {
    "text": "June 28 June 8 30 F 12 22 and 25 all right so there's a min/max values for",
    "start": "1943799",
    "end": "1949650"
  },
  {
    "text": "each of the blocks as far as the dates are concerned and now you're trying to search for a particular date and so what",
    "start": "1949650",
    "end": "1956400"
  },
  {
    "text": "happens is you're scanning multiple blocks here because the data is unsorted",
    "start": "1956400",
    "end": "1962990"
  },
  {
    "text": "you have overlap across these blocks so 9th June 2013",
    "start": "1962990",
    "end": "1968850"
  },
  {
    "text": "is part of the first block the second block and fourth block so you're almost",
    "start": "1968850",
    "end": "1974130"
  },
  {
    "text": "doing a full scan there if the data is sorted and as you can see here there's",
    "start": "1974130",
    "end": "1981930"
  },
  {
    "text": "no or lab between the date ranges across these blocks so you're gonna just query",
    "start": "1981930",
    "end": "1988950"
  },
  {
    "text": "for one one single block and of course this is just an example as you scale it to millions of blocks it makes a",
    "start": "1988950",
    "end": "1995250"
  },
  {
    "text": "difference so it's an order of n versus an order one complexity between unsorted",
    "start": "1995250",
    "end": "2001940"
  },
  {
    "text": "and sorted columns and so we have three different types of sort keys that are",
    "start": "2001940",
    "end": "2009290"
  },
  {
    "text": "available with redshift single column which is the example that we have seen earlier and compound we just means you",
    "start": "2009290",
    "end": "2016130"
  },
  {
    "text": "have multiple columns and interleaved I'll talk about that so single column is",
    "start": "2016130",
    "end": "2021530"
  },
  {
    "text": "pretty straightforward so you have an example of a date which is what column",
    "start": "2021530",
    "end": "2028910"
  },
  {
    "text": "is sorted by and rest of the regions rest of the columns associated with this",
    "start": "2028910",
    "end": "2035330"
  },
  {
    "text": "table and now let's take the example of a compound salt and this is similar to",
    "start": "2035330",
    "end": "2042290"
  },
  {
    "text": "you know compound index in Ruhe based environments so the data is sorted by",
    "start": "2042290",
    "end": "2048260"
  },
  {
    "text": "the first column and then within the first column it's sort of in a second and within the second where third right",
    "start": "2048260",
    "end": "2053360"
  },
  {
    "text": "and what happens is if you're trying to query for the first column you get an",
    "start": "2053360",
    "end": "2059899"
  },
  {
    "text": "order of one complexity if you're trying to do second or third it's you know probably order of n complexity so it",
    "start": "2059900",
    "end": "2068810"
  },
  {
    "text": "helps if you're in compound sort model it helps if your queries have both the",
    "start": "2068810",
    "end": "2073820"
  },
  {
    "text": "first or the second or first second and third while you do the queries the third",
    "start": "2073820",
    "end": "2081409"
  },
  {
    "text": "example is interleaved sorting and so we use a space-filling curve based",
    "start": "2081410",
    "end": "2089240"
  },
  {
    "text": "algorithm to sort this data in an optimal way such that you get same",
    "start": "2089240",
    "end": "2094940"
  },
  {
    "text": "amount of selectivity for each of those columns what that means is irrespective",
    "start": "2094940",
    "end": "2100340"
  },
  {
    "text": "of whether you're querying by the second or third column you get similar amount of performance and so this is",
    "start": "2100340",
    "end": "2107450"
  },
  {
    "text": "because we store it internally within a format that lends to this type of",
    "start": "2107450",
    "end": "2112910"
  },
  {
    "text": "performance but this has a trade-off in that in the previous example if you're",
    "start": "2112910",
    "end": "2118010"
  },
  {
    "text": "querying by the first column you get an order of one complexity here if you're",
    "start": "2118010",
    "end": "2123500"
  },
  {
    "text": "searching for any of these columns you get order of n power 1 by 3 complexity",
    "start": "2123500",
    "end": "2132230"
  },
  {
    "text": "of three columns so you're doing a cube root of n so if you have a million blocks it means you're gonna scan very",
    "start": "2132230",
    "end": "2139280"
  },
  {
    "text": "few blocks in this particular model that said you're not still getting an order one complexity so there is a bit of a",
    "start": "2139280",
    "end": "2145700"
  },
  {
    "text": "trade-off in using compound versus interleaved sort keys and of course the single versus compound there isn't a",
    "start": "2145700",
    "end": "2152869"
  },
  {
    "text": "trade-off it depends on your use case and the patterns but at a high level it's important to understand how single",
    "start": "2152869",
    "end": "2160089"
  },
  {
    "text": "compound and relieve sort keys work so that you can use it for as part of your",
    "start": "2160089",
    "end": "2167270"
  },
  {
    "text": "data modeling exercise the next part so sort keys is about how the columns are",
    "start": "2167270",
    "end": "2175190"
  },
  {
    "start": "2169000",
    "end": "2353000"
  },
  {
    "text": "sorted and you can think of them at a high level they're very different from indexes but you can think of them",
    "start": "2175190",
    "end": "2180710"
  },
  {
    "text": "notionally as indexes you're coming from row based environments the distribution",
    "start": "2180710",
    "end": "2186770"
  },
  {
    "text": "Keys is how the data gets distributed across the various compute nodes and so",
    "start": "2186770",
    "end": "2195020"
  },
  {
    "text": "we have three types of distribution available even based distribution key",
    "start": "2195020",
    "end": "2200450"
  },
  {
    "text": "based and all and so let's look at how the how each of these work so let's this",
    "start": "2200450",
    "end": "2206869"
  },
  {
    "text": "is a simple table ID general name and if you're doing an even distribution it's a",
    "start": "2206869",
    "end": "2213020"
  },
  {
    "text": "round-robin distribution so all we do is you know the first row gets attributed",
    "start": "2213020",
    "end": "2219349"
  },
  {
    "text": "to the first node second row the second node and you know goes so on the second",
    "start": "2219349",
    "end": "2226910"
  },
  {
    "text": "one is a key based distribution in the key base distribution you know uses a hash function so the know",
    "start": "2226910",
    "end": "2235850"
  },
  {
    "text": "are allocated to various nodes based on what the hash value is and so that was",
    "start": "2235850",
    "end": "2245660"
  },
  {
    "text": "one example and you know the gender particular so if you are distributing the gender based on a key then it just",
    "start": "2245660",
    "end": "2252740"
  },
  {
    "text": "goes to to a nonce right because of the hashing so the reason why key bales",
    "start": "2252740",
    "end": "2258500"
  },
  {
    "text": "distribution is important is if you are trying to given the MPP environment all",
    "start": "2258500",
    "end": "2264920"
  },
  {
    "text": "your records within a table gets distributed across the various nodes so if you have a giant fact table that",
    "start": "2264920",
    "end": "2270980"
  },
  {
    "text": "you're joining with the giant distribution table it's ideal that both",
    "start": "2270980",
    "end": "2276290"
  },
  {
    "text": "of them are based on key based distribution and you're using the same key so that when you are joining these",
    "start": "2276290",
    "end": "2283520"
  },
  {
    "text": "two tables the joins are co-located so",
    "start": "2283520",
    "end": "2288980"
  },
  {
    "text": "given the hash function for the same key results in the same node if you have two different tables that you are joining by",
    "start": "2288980",
    "end": "2294320"
  },
  {
    "text": "and using a key based distribution the joins are co-located otherwise what",
    "start": "2294320",
    "end": "2299450"
  },
  {
    "text": "happens is we'll have to do a broadcast across the various compute nodes straight you're joining data that is",
    "start": "2299450",
    "end": "2304970"
  },
  {
    "text": "sitting in one table in this node with data sitting in another table in a",
    "start": "2304970",
    "end": "2310760"
  },
  {
    "text": "different node so they need to talk to each other so depending on the volume of data that",
    "start": "2310760",
    "end": "2316730"
  },
  {
    "text": "is involved in your query and the amount of broadcast that is required you could have some performance degradation",
    "start": "2316730",
    "end": "2322940"
  },
  {
    "text": "associated with that so it's important to understand the distribution concept",
    "start": "2322940",
    "end": "2328940"
  },
  {
    "text": "and use it appropriately for you know choose between a key based distribution and even distribution as you see fit and",
    "start": "2328940",
    "end": "2338800"
  },
  {
    "text": "so some thumb rules around the third one is all right so all is just all the data",
    "start": "2338800",
    "end": "2345470"
  },
  {
    "text": "within the table is present on every node so it's just replicated in mass",
    "start": "2345470",
    "end": "2351560"
  },
  {
    "text": "across each of the compute nodes so here are a few thumb rules of thinking about",
    "start": "2351560",
    "end": "2356830"
  },
  {
    "start": "2353000",
    "end": "2441000"
  },
  {
    "text": "distribution so if you have tables that require no joints you can go do an even",
    "start": "2356830",
    "end": "2364820"
  },
  {
    "text": "distribution so given there's no giant implication you're not going to have a broadcast for",
    "start": "2364820",
    "end": "2371309"
  },
  {
    "text": "smaller dimension tables less than thousand rows I think it's reasonable to think about even distribution or even",
    "start": "2371309",
    "end": "2377849"
  },
  {
    "text": "all works and the reason for that is there isn't much of an overhead when",
    "start": "2377849",
    "end": "2383160"
  },
  {
    "text": "you're broadcasting a few thousand rows across the compute nodes so each compute node is connected with each other with a",
    "start": "2383160",
    "end": "2389400"
  },
  {
    "text": "10 gig network so if you're broadcasting a few nodes a few rows it doesn't really",
    "start": "2389400",
    "end": "2394769"
  },
  {
    "text": "matter the key base distribution is something that you'll want to consider for large fact tables large dimension",
    "start": "2394769",
    "end": "2402390"
  },
  {
    "text": "tables large fact tables and dimension tables that are joined using the same keys use that as the distribution style",
    "start": "2402390",
    "end": "2410400"
  },
  {
    "text": "and for medium dimension tables thousand",
    "start": "2410400",
    "end": "2415740"
  },
  {
    "text": "rows to you know multiple million rows you can use an all distribution usually",
    "start": "2415740",
    "end": "2421500"
  },
  {
    "text": "a few million rows the storage associated with it is not too much right it's a few gigs probably and having it",
    "start": "2421500",
    "end": "2430410"
  },
  {
    "text": "across all the nodes means if you're doing a join you're not going to have a broadcast all the data associated with",
    "start": "2430410",
    "end": "2436099"
  },
  {
    "text": "your dimension tables are present in all the nodes so those are some of the",
    "start": "2436099",
    "end": "2444559"
  },
  {
    "start": "2441000",
    "end": "2473000"
  },
  {
    "text": "things that I would keep in mind from a data modelling perspective as you start",
    "start": "2444559",
    "end": "2449849"
  },
  {
    "text": "using redshift and let's talk a little bit about loading data so there are",
    "start": "2449849",
    "end": "2455490"
  },
  {
    "text": "multiple options to load data a lot of that involve s3 so if you have flat",
    "start": "2455490",
    "end": "2461849"
  },
  {
    "text": "files you can load it into s3 using the multi-part upload and from s3 to",
    "start": "2461849",
    "end": "2468180"
  },
  {
    "text": "redshift there's a simple copy command that you can execute to load the data",
    "start": "2468180",
    "end": "2473720"
  },
  {
    "start": "2473000",
    "end": "2495000"
  },
  {
    "text": "you can also use the variety of data integration tools that we discussed earlier after doing EPL if your data is coming",
    "start": "2473720",
    "end": "2481200"
  },
  {
    "text": "from transactional sources that needs aggregation on roll-ups that sort of stuff you can use ETL tools",
    "start": "2481200",
    "end": "2487890"
  },
  {
    "text": "you know a bunch of them and they in turn integrate with redshift using s3",
    "start": "2487890",
    "end": "2493579"
  },
  {
    "text": "in-between for streaming data Amazon",
    "start": "2493579",
    "end": "2499470"
  },
  {
    "start": "2495000",
    "end": "2552000"
  },
  {
    "text": "Kinesis is an option if you're not familiar with Kinesis so it's a service",
    "start": "2499470",
    "end": "2504750"
  },
  {
    "text": "that enables you to stream hundreds of terabytes per hour across hundreds of",
    "start": "2504750",
    "end": "2510299"
  },
  {
    "text": "thousands of data producers you can process that using can you collect it",
    "start": "2510299",
    "end": "2515549"
  },
  {
    "text": "and process it using Kinesis there's a new service called Kinesis firehose that enables you to just you know define the",
    "start": "2515549",
    "end": "2524630"
  },
  {
    "text": "throughput that is needed for your streaming data and then a destination",
    "start": "2524630",
    "end": "2530039"
  },
  {
    "text": "whether it says 3 or redshift and as data comes in kala gets collected gets",
    "start": "2530039",
    "end": "2535109"
  },
  {
    "text": "loaded into redshift automatically and their customers who load data every five minutes granularity for streaming data",
    "start": "2535109",
    "end": "2542880"
  },
  {
    "text": "it's it's not real time but it's sort of near real-time depending upon your use case so Kinesis is another way of",
    "start": "2542880",
    "end": "2550049"
  },
  {
    "text": "loading data into redshift for streaming data and so here is a high-level tip to",
    "start": "2550049",
    "end": "2555690"
  },
  {
    "start": "2552000",
    "end": "2661000"
  },
  {
    "text": "keep in mind as you load data from s3 into red shift we recommend using multiple input files",
    "start": "2555690",
    "end": "2562259"
  },
  {
    "text": "depending upon how many slices you have within a node and how many nodes here",
    "start": "2562259",
    "end": "2569460"
  },
  {
    "text": "within a cluster and slices a compute unit so you can think of it closer to a",
    "start": "2569460",
    "end": "2575220"
  },
  {
    "text": "core so each slice within redshift can parallely perform operations and we talked about parallelization earlier and",
    "start": "2575220",
    "end": "2582029"
  },
  {
    "text": "it's not necessarily at a node level it's actually even more granular you have multiple slices within each node",
    "start": "2582029",
    "end": "2587549"
  },
  {
    "text": "that can go and execute a job in parallel and so that is important",
    "start": "2587549",
    "end": "2592589"
  },
  {
    "text": "because as you if you're loading a single file from s3 into redshift what",
    "start": "2592589",
    "end": "2600660"
  },
  {
    "text": "happens is only one slice is doing all the work so you if you have a you know",
    "start": "2600660",
    "end": "2605880"
  },
  {
    "text": "8xl cluster or you have 16 slices on it and only one slice is going to do all the work which means it's going to be",
    "start": "2605880",
    "end": "2611369"
  },
  {
    "text": "relatively slow so you can typically get around 100 megabytes per second within a",
    "start": "2611369",
    "end": "2618359"
  },
  {
    "text": "particular node so the bandwidth for the load that is available to you using a single input",
    "start": "2618359",
    "end": "2623410"
  },
  {
    "text": "file means you're getting a fraction of it so if you have multiple files in this",
    "start": "2623410",
    "end": "2629920"
  },
  {
    "text": "particular case given there are 16 slices in this node you'll want to have multiple six you know sixteen thirty-two",
    "start": "2629920",
    "end": "2637120"
  },
  {
    "text": "multiples of 16 as a number of files if you have thousands of files it doesn't really matter because you already have a",
    "start": "2637120",
    "end": "2642760"
  },
  {
    "text": "multiple in there and so then what happens is each of these 16 slices goes",
    "start": "2642760",
    "end": "2648640"
  },
  {
    "text": "and starts picking up a file in parallel and starts loading into redshift and so there you get the full bandwidth",
    "start": "2648640",
    "end": "2655030"
  },
  {
    "text": "available to you by each node to load the data and so we talked about",
    "start": "2655030",
    "end": "2663840"
  },
  {
    "start": "2661000",
    "end": "2675000"
  },
  {
    "text": "provisioning we talked about loading data we talked about some of the tips",
    "start": "2663840",
    "end": "2671140"
  },
  {
    "text": "and tricks for data modeling let's understand a little bit about how to query data it's very straightforward so",
    "start": "2671140",
    "end": "2676900"
  },
  {
    "start": "2675000",
    "end": "2708000"
  },
  {
    "text": "we have JDBC ODBC drivers given the post for our syntax and compatibility you can",
    "start": "2676900",
    "end": "2683170"
  },
  {
    "text": "use standard drivers to connect to redshift you can either use standard JDBC ODBC poster drivers or redshift",
    "start": "2683170",
    "end": "2691510"
  },
  {
    "text": "supplied custom drivers which are more performance optimized and if you're",
    "start": "2691510",
    "end": "2697810"
  },
  {
    "text": "using you know the BI tools here all of them talk to that shift using our custom",
    "start": "2697810",
    "end": "2705160"
  },
  {
    "text": "drivers that gives you a better performance you can also use you know",
    "start": "2705160",
    "end": "2713620"
  },
  {
    "start": "2708000",
    "end": "2765000"
  },
  {
    "text": "sequel client directly to talk to redshift again standard drivers or",
    "start": "2713620",
    "end": "2718660"
  },
  {
    "text": "custom drivers there's also SAR bi servers that you can have redshift to",
    "start": "2718660",
    "end": "2726190"
  },
  {
    "text": "talk to and this is typically the case where you have lots of different users trying to access the reporting platform",
    "start": "2726190",
    "end": "2732940"
  },
  {
    "text": "and you'll typically have a server to serve these reports and different BI",
    "start": "2732940",
    "end": "2741070"
  },
  {
    "text": "tools have different functions and features available for you so Pablo has",
    "start": "2741070",
    "end": "2747930"
  },
  {
    "text": "what what is called as Tablo extracts that load the data periodically from the cluster so that if you have thousands of",
    "start": "2747930",
    "end": "2753690"
  },
  {
    "text": "users accessing the reporting platform you go through an extract and store",
    "start": "2753690",
    "end": "2759060"
  },
  {
    "text": "directly accessing the data warehousing cluster MicroStrategy has similar concepts as well and we have integration",
    "start": "2759060",
    "end": "2771140"
  },
  {
    "start": "2765000",
    "end": "2801000"
  },
  {
    "text": "in the console directly so that you can see how your queries are performing you",
    "start": "2771140",
    "end": "2776970"
  },
  {
    "text": "can look at slow performing queries you can look at when a particular query is being performed what is the resource",
    "start": "2776970",
    "end": "2783420"
  },
  {
    "text": "utilization life was you know CPU usage spiking and is that correlated with a",
    "start": "2783420",
    "end": "2788550"
  },
  {
    "text": "particular query running is the network throughput spiking because of particular",
    "start": "2788550",
    "end": "2794610"
  },
  {
    "text": "queries so you can do those sorts of analysis directly using the management console we also have of course you know",
    "start": "2794610",
    "end": "2804570"
  },
  {
    "start": "2801000",
    "end": "2876000"
  },
  {
    "text": "given its sequel database engine with the cost-based optimizer you can look at",
    "start": "2804570",
    "end": "2811109"
  },
  {
    "text": "the execution plan you can look at the query plans as well so query plan is",
    "start": "2811109",
    "end": "2817050"
  },
  {
    "text": "generated you know before the execution happens and you have a way to look at the execution plan as well the execution",
    "start": "2817050",
    "end": "2823140"
  },
  {
    "text": "plan is gonna be a little different then you're probably used to in the non MPP world so in the execution plan you can",
    "start": "2823140",
    "end": "2830100"
  },
  {
    "text": "look at the average time it took for a particular step to execute across the",
    "start": "2830100",
    "end": "2836609"
  },
  {
    "text": "different compute nodes so if you see that one particular node is having an",
    "start": "2836609",
    "end": "2842550"
  },
  {
    "text": "outsized impact on execution or it took a longer time on average to run a particular step it's possible that you",
    "start": "2842550",
    "end": "2849150"
  },
  {
    "text": "might have a skew and distribution right so we talked a bit about even distribution key-based distribution etc so if you are distributing if your data",
    "start": "2849150",
    "end": "2856980"
  },
  {
    "text": "is skewed and you have lot of it stored on one node versus the other it's possible that the query execution on",
    "start": "2856980",
    "end": "2863940"
  },
  {
    "text": "that particular node could take a longer time so the execution plan can help you identify those sorts of issues and",
    "start": "2863940",
    "end": "2872070"
  },
  {
    "text": "figure out options to remedy that so that's largely it we have a few",
    "start": "2872070",
    "end": "2879480"
  },
  {
    "start": "2876000",
    "end": "2899000"
  },
  {
    "text": "resources we have a few related sessions here that might be of interest we have a quick side session this afternoon an",
    "start": "2879480",
    "end": "2886800"
  },
  {
    "text": "Amazon machine learning session and the database migration service session so if you are planning to migrate from a",
    "start": "2886800",
    "end": "2893250"
  },
  {
    "text": "different database engine internet shift what going to the database migration session that's it folks any questions",
    "start": "2893250",
    "end": "2902400"
  },
  {
    "start": "2899000",
    "end": "3070000"
  },
  {
    "text": "and happy to take",
    "start": "2902400",
    "end": "2905390"
  },
  {
    "text": "yeah that's right",
    "start": "2919109",
    "end": "2926140"
  },
  {
    "text": "mmm-hmm right so the reason for that is",
    "start": "2930580",
    "end": "2936070"
  },
  {
    "text": "we don't want to provide direct access into the database files so you can load",
    "start": "2936070",
    "end": "2941530"
  },
  {
    "text": "the data you know from s3 or whichever source and it goes directly into the",
    "start": "2941530",
    "end": "2946900"
  },
  {
    "text": "compute nodes and it happens through the internal VPC the reason why those three",
    "start": "2946900",
    "end": "2952600"
  },
  {
    "text": "pieces are separate is because for security reasons we don't want to provide direct access to the underlying",
    "start": "2952600",
    "end": "2957700"
  },
  {
    "text": "database files so you don't directly have access which means nobody else ever has access to come and directly talk to",
    "start": "2957700",
    "end": "2965020"
  },
  {
    "text": "the compute nodes so that is the reason why the V pieces are separate you have",
    "start": "2965020",
    "end": "2970720"
  },
  {
    "text": "all the access to the beta through the leader node right if you execute a query if you're loading data all of that",
    "start": "2970720",
    "end": "2977530"
  },
  {
    "text": "happens seamlessly you don't have to do anything additional it's just a security measure to keep the data secure you have",
    "start": "2977530",
    "end": "2990270"
  },
  {
    "text": "hello is there any benefit to using redshift for for data set the order like",
    "start": "2992160",
    "end": "2999640"
  },
  {
    "text": "50 to 100 gigabytes of data is that 50 200 gigabytes yes yes so we have so our",
    "start": "2999640",
    "end": "3007200"
  },
  {
    "text": "smallest cluster is a single node dc1 large and that starts at 160 gigs of",
    "start": "3007200",
    "end": "3014150"
  },
  {
    "text": "compressed data and a lot of customers use a single node cluster that said I",
    "start": "3014150",
    "end": "3020460"
  },
  {
    "text": "would recommend using at least two nodes which would take you to you know 300 plus gigs the reason for that is single",
    "start": "3020460",
    "end": "3027720"
  },
  {
    "text": "node clusters have durability risk so if the cluster you know for whatever reason if the hardware fails you'll have to",
    "start": "3027720",
    "end": "3034200"
  },
  {
    "text": "restore from your backup and which might be somewhat stale right because we do incremental continuous backups but they",
    "start": "3034200",
    "end": "3041340"
  },
  {
    "text": "happen at a periodicity if you're having at least two nodes then it what it means is the you know the other node acts as a",
    "start": "3041340",
    "end": "3048000"
  },
  {
    "text": "mirror to your first node so you can tolerate at least one node failure and you know get go on with your work so it's",
    "start": "3048000",
    "end": "3055980"
  },
  {
    "text": "certainly reasonable with lots of customers using it at you know hundreds of gigabytes range",
    "start": "3055980",
    "end": "3062900"
  },
  {
    "text": "yeah sure does red shift support Jason",
    "start": "3068310",
    "end": "3075130"
  },
  {
    "start": "3070000",
    "end": "3265000"
  },
  {
    "text": "be support ward Jason Jason B I'm sorry",
    "start": "3075130",
    "end": "3081390"
  },
  {
    "text": "sorry I didn't is it a data format that you're asking yeah it's yeah we do okay we do",
    "start": "3081390",
    "end": "3087460"
  },
  {
    "text": "supported but not necessarily in the way that Posterous does so you can load JSON",
    "start": "3087460",
    "end": "3095020"
  },
  {
    "text": "data into red shift through the copy command and you need to have a JSON",
    "start": "3095020",
    "end": "3100510"
  },
  {
    "text": "hearts file which Maps your elements right to the underlying data model so if",
    "start": "3100510",
    "end": "3105850"
  },
  {
    "text": "you have some elements that don't have a mapping to a data model then they get skipped and so so that's the aspect",
    "start": "3105850",
    "end": "3114130"
  },
  {
    "text": "which might be a little different we also have native JSON functions so if you load the data without flattening it",
    "start": "3114130",
    "end": "3120340"
  },
  {
    "text": "through the path file you can load that directly into redshift and there are functions that you can execute to query",
    "start": "3120340",
    "end": "3127150"
  },
  {
    "text": "the jason elements but you're gonna have a little bit of a performance impact because you're parsing data on the fly",
    "start": "3127150",
    "end": "3133990"
  },
  {
    "text": "to execute the query is versus you know having it in a flattened model support",
    "start": "3133990",
    "end": "3144310"
  },
  {
    "text": "what s3 yeah so we don't support nested documents right now but we do have plans",
    "start": "3144310",
    "end": "3151240"
  },
  {
    "text": "to do that in the future hello the question I have is you Dean",
    "start": "3151240",
    "end": "3156940"
  },
  {
    "text": "mentioned data pipeline yes is that not the recommended way to load no it is a",
    "start": "3156940",
    "end": "3162460"
  },
  {
    "text": "recommended way to load as well it's one of the other options the reason why I'm I was focusing a little bit more on the",
    "start": "3162460",
    "end": "3167950"
  },
  {
    "text": "part in front is lots of customers have questions around hey like would my data integration solution work with redshift",
    "start": "3167950",
    "end": "3175030"
  },
  {
    "text": "but data pipeline is certainly an option to orchestrate jobs and move data across",
    "start": "3175030",
    "end": "3181330"
  },
  {
    "text": "aw services and from on promises environments as well okay thank you",
    "start": "3181330",
    "end": "3189300"
  },
  {
    "text": "any other questions",
    "start": "3191880",
    "end": "3195869"
  },
  {
    "text": "so you had shown on your slide how you can view the performance do you have anything for troubleshooting like",
    "start": "3205470",
    "end": "3211680"
  },
  {
    "text": "profile in the database where you can capture the traffic the query the parameters being passed in yeah so we",
    "start": "3211680",
    "end": "3217920"
  },
  {
    "text": "have a set of system tables so some of the data in the system tables is",
    "start": "3217920",
    "end": "3223349"
  },
  {
    "text": "reflected in the console and we are in the process of adding more functionality to the console but the system tables",
    "start": "3223349",
    "end": "3229740"
  },
  {
    "text": "have a lot more information around telemetry and some of the aspects that you talked about so we have a good",
    "start": "3229740",
    "end": "3236099"
  },
  {
    "text": "github repository with a set of scripts that acts as a system tables for various use cases and presented they should have",
    "start": "3236099",
    "end": "3243500"
  },
  {
    "text": "added it in here but I'll add it before posting it to the slides here any other",
    "start": "3243500",
    "end": "3254579"
  },
  {
    "text": "questions okay thanks guys",
    "start": "3254579",
    "end": "3262819"
  }
]