[
  {
    "start": "0",
    "end": "266000"
  },
  {
    "text": "all right everyone we'll probably get started as the last few people trickling so just want to let everybody",
    "start": "1079",
    "end": "7640"
  },
  {
    "text": "know my name is Andrew Braham I manage the cloud network engineering team at Netflix just to give you a bit of",
    "start": "7640",
    "end": "12719"
  },
  {
    "text": "background on my team we're primary responsible for managing all Network components within the abws environment",
    "start": "12719",
    "end": "19240"
  },
  {
    "text": "it's like elbs n gateways Direct Connect we're also responsible for our DNS",
    "start": "19240",
    "end": "25760"
  },
  {
    "text": "infrastructure troubleshooting any sort of instance connectivity and we also build tools and services to help improve",
    "start": "25760",
    "end": "31599"
  },
  {
    "text": "the visibility within the network co-presenting with me today is Lori feroli thank you Andrew I'm very excited",
    "start": "31599",
    "end": "39840"
  },
  {
    "text": "to be here today telling you about our adventure moving from ec2 classic into",
    "start": "39840",
    "end": "44879"
  },
  {
    "text": "VPC I'm the senior program manager at Netflix who was responsible for the migration",
    "start": "44879",
    "end": "50719"
  },
  {
    "text": "program for those out there that don't know anything about Netflix Netflix is a",
    "start": "50719",
    "end": "56359"
  },
  {
    "text": "predominant internet Television Network that has over 80 million members right now and they live in more than 190",
    "start": "56359",
    "end": "64158"
  },
  {
    "text": "countries our members watch over 125 million hours of television shows",
    "start": "64159",
    "end": "71320"
  },
  {
    "text": "documentaries Originals every single day for you might not know that we",
    "start": "71320",
    "end": "78439"
  },
  {
    "text": "didn't start in the cloud we actually started in our own data center where things were going pretty well we were",
    "start": "78439",
    "end": "84560"
  },
  {
    "text": "happy with the services that we gave our customers until 2008 a database outage",
    "start": "84560",
    "end": "91159"
  },
  {
    "text": "meant that we couldn't ship DVDs for our to our customers for three entire days",
    "start": "91159",
    "end": "96720"
  },
  {
    "text": "probably seemed like a long time when you were waiting for a DVD to show up in your",
    "start": "96720",
    "end": "103439"
  },
  {
    "text": "mailbox we selected Amazon web services to go into uh to be our cloud",
    "start": "103680",
    "end": "109840"
  },
  {
    "text": "provider to give us flexibility reliability and expandability and a presence in the",
    "start": "109840",
    "end": "119000"
  },
  {
    "text": "cloud one of the other things that we did is we decided not to forklift our",
    "start": "119000",
    "end": "124439"
  },
  {
    "text": "applications from the our data center but actually to move it with thought",
    "start": "124439",
    "end": "130759"
  },
  {
    "text": "into the cloud we moved away from a monolithic application model to a",
    "start": "130759",
    "end": "137319"
  },
  {
    "text": "distributed and microservice model that allowed us to develop a very resilient",
    "start": "137319",
    "end": "143959"
  },
  {
    "text": "environment that had reduced dependencies between Services it took us",
    "start": "143959",
    "end": "149280"
  },
  {
    "text": "a while to get there but in 2010 we were able to launch out of Amazon's Us East",
    "start": "149280",
    "end": "156519"
  },
  {
    "text": "environment we also optimized for ec2 classic all of the networking was set up",
    "start": "156519",
    "end": "163480"
  },
  {
    "text": "that way all of the areas that we needed to ensure that we had performance and",
    "start": "163480",
    "end": "169519"
  },
  {
    "text": "reliability 2011 we went into our second region into EU West we had",
    "start": "169519",
    "end": "177080"
  },
  {
    "text": "about 20 million or 30 million members at that point in time and we opened our",
    "start": "177080",
    "end": "184159"
  },
  {
    "text": "services to Latin America by 2012 we continue developing our",
    "start": "184159",
    "end": "190640"
  },
  {
    "text": "environments our tools our Telemetry our monitoring and our services optimized",
    "start": "190640",
    "end": "196920"
  },
  {
    "text": "for multi- region deployment as well as reliability and",
    "start": "196920",
    "end": "202159"
  },
  {
    "text": "ec2 uh features 2013 we open our third",
    "start": "202159",
    "end": "208080"
  },
  {
    "text": "active region on Us West by",
    "start": "208080",
    "end": "214560"
  },
  {
    "text": "2014 we've pretty much moved from our data center into the",
    "start": "214560",
    "end": "221920"
  },
  {
    "text": "cloud what we've done essentially is built a mountain of ec2 granite that we",
    "start": "221920",
    "end": "229720"
  },
  {
    "text": "now need to move into",
    "start": "229720",
    "end": "234680"
  },
  {
    "text": "vbc this system was actually moving working very well for our customers they",
    "start": "237079",
    "end": "243239"
  },
  {
    "text": "were delighted we were delighted resiliency was up we had a lot of good",
    "start": "243239",
    "end": "250879"
  },
  {
    "text": "performance and so today we're going to talk about why we made that decision to move",
    "start": "250879",
    "end": "258479"
  },
  {
    "text": "what that Granite Mountain was made out of how we managed to move it in the time frame that we had and what we",
    "start": "258479",
    "end": "266479"
  },
  {
    "start": "266000",
    "end": "266000"
  },
  {
    "text": "learned VPC provided a lot of advantages that we were looking for one of the key",
    "start": "266479",
    "end": "273520"
  },
  {
    "text": "areas we wanted to explore was the area of configurability being able to form",
    "start": "273520",
    "end": "280360"
  },
  {
    "text": "our networks the way that we wanted them to be formed and Andrew is going to talk about that in detail another area that",
    "start": "280360",
    "end": "287479"
  },
  {
    "text": "we were looking for was the increased performance reduced latency as well as",
    "start": "287479",
    "end": "294160"
  },
  {
    "text": "better throughput and of course for our Diagnostics we were using looking for",
    "start": "294160",
    "end": "300520"
  },
  {
    "text": "the flow logs that would give us in insight into the traffic that was moving",
    "start": "300520",
    "end": "305800"
  },
  {
    "text": "before our instances and last but not least we were looking for those new instance types and data uh regions that",
    "start": "305800",
    "end": "313840"
  },
  {
    "text": "were going to be opened to vbc only Netflix is",
    "start": "313840",
    "end": "320360"
  },
  {
    "text": "big one of the things that we needed to do is we needed to look at the entire",
    "start": "320360",
    "end": "325440"
  },
  {
    "text": "environment but there were special pieces that were going to make the move",
    "start": "325440",
    "end": "330479"
  },
  {
    "text": "easier some of them were our Central tools tools like our delivery system",
    "start": "330479",
    "end": "337880"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "Spiner Spiner not only needed to move into VPC without experience any outages",
    "start": "337880",
    "end": "345199"
  },
  {
    "text": "it also needed to provide a way for our service teams to move easily and quickly",
    "start": "345199",
    "end": "351520"
  },
  {
    "text": "and developed a oneclick migration button that button allowed teams to move",
    "start": "351520",
    "end": "358400"
  },
  {
    "text": "their Pipelines and their asgs seamlessly net or uh Spiner actually was",
    "start": "358400",
    "end": "367120"
  },
  {
    "text": "the one that did the the heavy lifting of with duplicating the pipelines and",
    "start": "367120",
    "end": "373280"
  },
  {
    "text": "asgs creating security groups and making sure they were aligned as well as attaching Classic Link and not but Le",
    "start": "373280",
    "end": "381639"
  },
  {
    "text": "not but the least uh to have the um name",
    "start": "381639",
    "end": "387280"
  },
  {
    "text": "conflicts resolved we also have a large realtime Telemetry",
    "start": "387280",
    "end": "395000"
  },
  {
    "text": "environment that reports over three billion metrics every single",
    "start": "395000",
    "end": "401919"
  },
  {
    "text": "minute this system is called Atlas which has been open source and it's our window",
    "start": "401919",
    "end": "407479"
  },
  {
    "text": "into the health of our environment without having the the tool",
    "start": "407479",
    "end": "413840"
  },
  {
    "text": "to see the visibility we didn't would not have known whether we were doing well in our migration whether our",
    "start": "413840",
    "end": "421560"
  },
  {
    "text": "services was running flawlessly or if we were impacting our",
    "start": "421560",
    "end": "427000"
  },
  {
    "text": "customers We additionally took information that was available in Atlas and some of our other uh tools to build",
    "start": "427000",
    "end": "434759"
  },
  {
    "text": "dashboards that told us exactly who was in VPC and who was in classic to be able",
    "start": "434759",
    "end": "441240"
  },
  {
    "text": "to Prior prioritize what needed to be moved late",
    "start": "441240",
    "end": "446680"
  },
  {
    "text": "um what needed to be moved we also had hundreds of databases terab um pedabytes",
    "start": "446680",
    "end": "454280"
  },
  {
    "text": "of data that needed to move without any down time our databased engineering team",
    "start": "454280",
    "end": "462080"
  },
  {
    "text": "spent some time developing a process that allowed things to move",
    "start": "462080",
    "end": "467720"
  },
  {
    "text": "flawlessly we had thousands of services that were being developed by dozens of",
    "start": "467720",
    "end": "472800"
  },
  {
    "text": "teams who all moved independently and finally we had over a",
    "start": "472800",
    "end": "479879"
  },
  {
    "text": "100,000 instances that we moved within our time",
    "start": "479879",
    "end": "484960"
  },
  {
    "text": "frame Netflix moves fast and one of the things that we",
    "start": "487120",
    "end": "493240"
  },
  {
    "text": "couldn't do was slow down how fast Netflix",
    "start": "493240",
    "end": "498800"
  },
  {
    "text": "moves we needed to ensure that our engineering teams experienced a seamless",
    "start": "498800",
    "end": "505240"
  },
  {
    "start": "499000",
    "end": "499000"
  },
  {
    "text": "experience moving their applications we wanted make sure that",
    "start": "505240",
    "end": "510319"
  },
  {
    "text": "the velocity of innovation wasn't impacted so what we did was built a",
    "start": "510319",
    "end": "515719"
  },
  {
    "text": "migration team made up of network engineering database engineering security and operational folks who learn",
    "start": "515719",
    "end": "524360"
  },
  {
    "text": "the system very well could make changes and G give guidance to our teams that",
    "start": "524360",
    "end": "533040"
  },
  {
    "text": "made sure that as teams were moving the velocity of their Innovation was not",
    "start": "533040",
    "end": "538279"
  },
  {
    "text": "interrupted by their process of moving and finally unlike our um our move from",
    "start": "538279",
    "end": "546320"
  },
  {
    "text": "the data center into the cloud we had only taken advantage of things opportun",
    "start": "546320",
    "end": "554880"
  },
  {
    "text": "opportunist I can't say that word um only taking special things into",
    "start": "554880",
    "end": "561000"
  },
  {
    "text": "consideration we were not going to change the velocity of our move for",
    "start": "561000",
    "end": "567240"
  },
  {
    "text": "changes that would be been beneficial to our",
    "start": "567240",
    "end": "572240"
  },
  {
    "start": "572000",
    "end": "572000"
  },
  {
    "text": "environment we also broke down our services into very simple classifications the criteria was size",
    "start": "572320",
    "end": "579880"
  },
  {
    "text": "and criticality simply if an application was over 200 instances it was considered",
    "start": "579880",
    "end": "587800"
  },
  {
    "text": "large and if a system would stop something from stream A customer from",
    "start": "587800",
    "end": "594240"
  },
  {
    "text": "streaming it would be considered critical",
    "start": "594240",
    "end": "599320"
  },
  {
    "start": "599000",
    "end": "599000"
  },
  {
    "text": "we also knew that our holiday season was the the a special season for our",
    "start": "599480",
    "end": "606000"
  },
  {
    "text": "customers that's when they gathered in in their hometowns they met their",
    "start": "606000",
    "end": "611640"
  },
  {
    "text": "friends they had brand new devices that they wanted to plug in and see stream so",
    "start": "611640",
    "end": "617839"
  },
  {
    "text": "we set our timeline between January and the end of September to ensure that we",
    "start": "617839",
    "end": "623959"
  },
  {
    "text": "were completed with our migration before the holiday season we we had made many changes to",
    "start": "623959",
    "end": "631320"
  },
  {
    "text": "our environment and our Tooling in 2015 and we were ready to open the gates for",
    "start": "631320",
    "end": "637839"
  },
  {
    "text": "those Migra the systems to migrate but we had made lots of assumptions that we",
    "start": "637839",
    "end": "642880"
  },
  {
    "text": "didn't know were valid or invalid so we opened the gate for our small non-critical applications and during",
    "start": "642880",
    "end": "649680"
  },
  {
    "text": "that first few weeks we hardened our infrastructure we uh tested our",
    "start": "649680",
    "end": "655200"
  },
  {
    "text": "assumptions and made sure that our tooling was working as we expected we also had applications and",
    "start": "655200",
    "end": "663079"
  },
  {
    "text": "services that did not um move quickly so",
    "start": "663079",
    "end": "668600"
  },
  {
    "text": "we opened our gate for those systems that were going to take the entire three quarters to",
    "start": "668600",
    "end": "674399"
  },
  {
    "text": "move the next area that we opened the gate for was non-critical but large",
    "start": "674399",
    "end": "679880"
  },
  {
    "text": "systems those were the over 200 instances where we needed to develop a",
    "start": "679880",
    "end": "685399"
  },
  {
    "text": "process to move our reservations from ec2 classic into",
    "start": "685399",
    "end": "691720"
  },
  {
    "text": "VPC that ensured that we had enough capacity to move the application without",
    "start": "691720",
    "end": "697920"
  },
  {
    "text": "issue and then finally our critical apps those are the ones that if we moved",
    "start": "697920",
    "end": "703920"
  },
  {
    "text": "poorly would impact our users we managed those tightly and well",
    "start": "703920",
    "end": "711440"
  },
  {
    "text": "and then finally starting in October we had our cleanup activities that I'm not",
    "start": "711440",
    "end": "716920"
  },
  {
    "text": "going to talk about so now I want to hand this off to",
    "start": "716920",
    "end": "725360"
  },
  {
    "text": "Andrew all right thanks Lori as Laura mentions we built a",
    "start": "725360",
    "end": "730560"
  },
  {
    "text": "mountain with an ec2 classic now in order for Netflix to take advantage of all the benefits from VPC we needed to",
    "start": "730560",
    "end": "738040"
  },
  {
    "text": "think about and understand what are some of the behavioral differences that are within VPC now we built quite a few",
    "start": "738040",
    "end": "744000"
  },
  {
    "start": "740000",
    "end": "740000"
  },
  {
    "text": "tools and services in VPC or sorry in classic and based on those behaviors we",
    "start": "744000",
    "end": "749639"
  },
  {
    "text": "had certain things in mind so we need to understand what are some of the new features we can take advantage of what are some of the changes we needed to",
    "start": "749639",
    "end": "755519"
  },
  {
    "text": "account for once we understood what those changes were we leveraged our",
    "start": "755519",
    "end": "761440"
  },
  {
    "text": "partnership with AWS now we have a great partnership and in knowing that we explained to them how we wanted to move",
    "start": "761440",
    "end": "767480"
  },
  {
    "text": "what are some of the things we were thinking about bouncing ideas back and forth off each other and knowing what they were going to deliver I'm going to",
    "start": "767480",
    "end": "773959"
  },
  {
    "text": "kind of walk you through the Journey in terms of 2016 once we understood what our ideal St was going to be that was effectively",
    "start": "773959",
    "end": "780760"
  },
  {
    "text": "going to be our our Northstar our Guiding Light anytime we make critical decisions we always wanted to ensure",
    "start": "780760",
    "end": "785839"
  },
  {
    "text": "that we're actually moving towards that final state with that being the case we also",
    "start": "785839",
    "end": "791000"
  },
  {
    "text": "wanted to figure out what's going to be our overall migration plan now how many of you guys have heard",
    "start": "791000",
    "end": "796360"
  },
  {
    "text": "of uh chaos engineering show a hands all right so we've actually gotten pretty",
    "start": "796360",
    "end": "801600"
  },
  {
    "text": "good at this in terms of failing over our service between regions and we do this quite often so one of the models we",
    "start": "801600",
    "end": "806800"
  },
  {
    "text": "thought about in terms of migrating Services was actually kind of doing the big bang we thought about all right let's build up the entire environment",
    "start": "806800",
    "end": "812480"
  },
  {
    "text": "within VPC and simply fail that over out of classic into VPC one of the issues that's came out of",
    "start": "812480",
    "end": "819320"
  },
  {
    "text": "that is that it was going to take a huge amount of coordination with all the engineering teams to get them all aligned so that wasn't really going to",
    "start": "819320",
    "end": "825560"
  },
  {
    "text": "be the ideal uh method the other option we thought about was really kind of mimicking classic to",
    "start": "825560",
    "end": "832040"
  },
  {
    "text": "kind of minimize the overall changes as we move from classic over into VPC and what I mean by that is effectively if",
    "start": "832040",
    "end": "837720"
  },
  {
    "text": "you think about it public submits within VPC are really similar just to the",
    "start": "837720",
    "end": "843079"
  },
  {
    "text": "general Network within classic deploy instances they have the ability to get public addresses and they also have",
    "start": "843079",
    "end": "848360"
  },
  {
    "text": "private addresses but in going that route we'd probably lose a lot of advantages of actually taking or putting",
    "start": "848360",
    "end": "854399"
  },
  {
    "text": "systems into the private subnets so ultimately we decided to go with the uh the New Frontier we",
    "start": "854399",
    "end": "860759"
  },
  {
    "text": "effectively planned on moving services slowly into VPC leveraging Classic Link which we'll",
    "start": "860759",
    "end": "868199"
  },
  {
    "start": "868000",
    "end": "868000"
  },
  {
    "text": "talk about it to a bit now as far as our technical challenges the way we wanted to group these was really around trying to minimize the",
    "start": "868199",
    "end": "874880"
  },
  {
    "text": "impact to the rest of the engineering teams at Netflix we wanted to group it in a manner where most of the work was",
    "start": "874880",
    "end": "880560"
  },
  {
    "text": "done by as fewest teams as possible and of course it just so happens that most of the work they needed to be done R",
    "start": "880560",
    "end": "886000"
  },
  {
    "text": "evolved around Network routing how does assistants communicate to one another as they move over to VPC also ensuring they",
    "start": "886000",
    "end": "893000"
  },
  {
    "text": "can still communicate across accounts and also to the third party Partners the other ma major area of",
    "start": "893000",
    "end": "899320"
  },
  {
    "text": "concern revolved around DNS there were some distinct uh differences between the way DNS behaved inside of ec2 classic",
    "start": "899320",
    "end": "906600"
  },
  {
    "text": "versus VPC I'll get more into that a bit later lastly security groups were the",
    "start": "906600",
    "end": "912360"
  },
  {
    "text": "things we really need to think about now it's very related to how traffic flows across the environments and the last",
    "start": "912360",
    "end": "917680"
  },
  {
    "text": "thing we wanted to do is actually compromise our security we don't want to have to open up you know all the security groups just to make the",
    "start": "917680",
    "end": "923920"
  },
  {
    "text": "migration easy we're always thinking of how do we overall just improve the environment and increase mentally",
    "start": "923920",
    "end": "929480"
  },
  {
    "text": "improved security prior to actually moving things we thought about early on what are",
    "start": "929480",
    "end": "936560"
  },
  {
    "start": "931000",
    "end": "931000"
  },
  {
    "text": "account strategy was going to look like over time a majority of the Netflix Services the streaming services are",
    "start": "936560",
    "end": "942920"
  },
  {
    "text": "actually within a few key accounts those have grown organically over time which has caused you know a",
    "start": "942920",
    "end": "950000"
  },
  {
    "text": "bit of confusion in terms of what is the primary purpose of it different uh deployment models different security",
    "start": "950000",
    "end": "956079"
  },
  {
    "text": "groups and also kind of uh creates a very complex I guess data flow model when you have to map out what are all",
    "start": "956079",
    "end": "961480"
  },
  {
    "text": "the dependencies between those environments so some of the things we",
    "start": "961480",
    "end": "967959"
  },
  {
    "text": "thought about were really around security compartmentalization if there was a compromise if there was a breach",
    "start": "967959",
    "end": "973519"
  },
  {
    "text": "could we somehow take advantage of accounts to either slow that down or contain that",
    "start": "973519",
    "end": "978720"
  },
  {
    "text": "breach the other thing we thought about was really the administrative domain we want to ensure our Engineers have the",
    "start": "978720",
    "end": "984839"
  },
  {
    "text": "ability to deploy manage their systems as needed and give them the necessary",
    "start": "984839",
    "end": "990279"
  },
  {
    "text": "permissions to do everything they need but at the same time we also want to sure that we didn't give them too many permissions for example we didn't need",
    "start": "990279",
    "end": "997000"
  },
  {
    "text": "to have our finance team to have full access to their classic environment Andor",
    "start": "997000",
    "end": "1002079"
  },
  {
    "text": "VPC same as any other uh you know internal",
    "start": "1002079",
    "end": "1007279"
  },
  {
    "text": "team the other thing we thought about at scale is really rate limits now often",
    "start": "1007279",
    "end": "1012360"
  },
  {
    "text": "times we hear Hey the cloud it can infinitely scale but at our scale we often run into uh kind of those upper",
    "start": "1012360",
    "end": "1019680"
  },
  {
    "text": "limits and with AWS some of these limits are really bound around accounts and so",
    "start": "1019680",
    "end": "1025400"
  },
  {
    "text": "if we're able to decompose our accounts into smaller subsets we'd reduce our likelihood of actually hitting these",
    "start": "1025400",
    "end": "1030918"
  },
  {
    "text": "upper bounds things like you know API calls know we do thousands of calls a second",
    "start": "1030919",
    "end": "1036959"
  },
  {
    "text": "sometimes we do hit those upper limits lastly we also thought about capacity from a couple of Dimensions so",
    "start": "1036959",
    "end": "1043918"
  },
  {
    "text": "the first one with is really around IP addresses IP addresses managing those",
    "start": "1043919",
    "end": "1049120"
  },
  {
    "text": "there's something new within VPC and that's something we needed to consider especially as we're growing it also as",
    "start": "1049120",
    "end": "1055039"
  },
  {
    "text": "far as all the other service components that actually take up addresses the other thing we thought about was really around reserved",
    "start": "1055039",
    "end": "1061160"
  },
  {
    "text": "instances so we have multiple accounts and we actually share reserved inst instances between accounts so one of the",
    "start": "1061160",
    "end": "1068600"
  },
  {
    "text": "things to keep in mind if you guys are sitting on brand new accounts the actual azs themselves",
    "start": "1068600",
    "end": "1074480"
  },
  {
    "text": "aren't necessarily on line so if you have an account a a a b and c a b and c aren't necessarily the same for your",
    "start": "1074480",
    "end": "1081080"
  },
  {
    "text": "second account so you want to make sure those are aligned before you deploy any sort of service that way if you want to move reservations in between you can",
    "start": "1081080",
    "end": "1087679"
  },
  {
    "text": "easily do that now as far as account classification we wanted to make this simple and easy for all of our",
    "start": "1087679",
    "end": "1093360"
  },
  {
    "start": "1089000",
    "end": "1089000"
  },
  {
    "text": "engineering teams to uh to remember if you have too many rules in place it just makes things confusing people ask a",
    "start": "1093360",
    "end": "1099440"
  },
  {
    "text": "bunch of questions they not quite sure where to put things and they just end up putting it wherever so we look at this",
    "start": "1099440",
    "end": "1105159"
  },
  {
    "text": "really around business purposes what are the services who who are they serving what are they needed for to really help",
    "start": "1105159",
    "end": "1111600"
  },
  {
    "text": "simplify what those categories look like secondly we also thought about the",
    "start": "1111600",
    "end": "1118400"
  },
  {
    "text": "operational model now we have centralized teams that build a lot of tools to cover most of the internal",
    "start": "1118400",
    "end": "1123760"
  },
  {
    "text": "engineering teams but it's not full coverage we don't have 100% adoption which is",
    "start": "1123760",
    "end": "1128919"
  },
  {
    "text": "okay however at scale you want to keep things as simple as possible so if you have a similar operational model within",
    "start": "1128919",
    "end": "1135039"
  },
  {
    "text": "an account it makes it very easy for auditing purposes and also to ensure that everybody's at the same either",
    "start": "1135039",
    "end": "1140120"
  },
  {
    "text": "patch level uh security level configuration level whatever it may be but having consistency across the",
    "start": "1140120",
    "end": "1146000"
  },
  {
    "text": "accounts definitely makes things a lot easier and the last thing we thought about was really around user access who",
    "start": "1146000",
    "end": "1152080"
  },
  {
    "text": "are the users that are going to be accessing these services and these accounts are they maybe customers that",
    "start": "1152080",
    "end": "1157120"
  },
  {
    "text": "are you know on the internet coming into the service are they internal teams that need to access some of these services or",
    "start": "1157120",
    "end": "1162559"
  },
  {
    "text": "maybe some of your third- party Partners so those are the things we thought about as we divided up our accounts",
    "start": "1162559",
    "end": "1169120"
  },
  {
    "text": "now from a networking perspective we look at traffic in two buckets really one is really around Regional routing",
    "start": "1169120",
    "end": "1175440"
  },
  {
    "text": "and the other is around Global routing now as I mentioned Classic Link",
    "start": "1175440",
    "end": "1181159"
  },
  {
    "text": "was one of the key components that helped us Su successfully migrate into uh",
    "start": "1181159",
    "end": "1186559"
  },
  {
    "text": "VPC So Classic Link really allows classic instances to communicate",
    "start": "1186559",
    "end": "1192080"
  },
  {
    "text": "to VPC instances within the same account within the same account within the same region",
    "start": "1192080",
    "end": "1198880"
  },
  {
    "start": "1198000",
    "end": "1198000"
  },
  {
    "text": "so the first thing we need to consider was really around IP address allocation as I mentioned before this is something new we didn't have to think about within",
    "start": "1198880",
    "end": "1205400"
  },
  {
    "text": "classic Amazon pretty much took care of that for us in classic didn't really need to think about it so the first item",
    "start": "1205400",
    "end": "1212200"
  },
  {
    "text": "we had to contend with was the actual address space itself",
    "start": "1212200",
    "end": "1217240"
  },
  {
    "text": "108 so I'm sure many of you with data centers Corporate Offices even some of your partners probably use the 10 space",
    "start": "1217240",
    "end": "1224320"
  },
  {
    "text": "as well so we had to figure out how do we overcome this",
    "start": "1224320",
    "end": "1229519"
  },
  {
    "text": "the second thing we thought about was really around uh globally unique IP addresses we have a global foot",
    "start": "1229559",
    "end": "1236240"
  },
  {
    "text": "footprint we're in four regions multiple accounts multiple",
    "start": "1236240",
    "end": "1241919"
  },
  {
    "text": "vpcs with that being the case we want to ensure that any sort of communication we didn't have to contend with overlapping",
    "start": "1241919",
    "end": "1248120"
  },
  {
    "text": "IP overlapping IP space the last thing we needed to think",
    "start": "1248120",
    "end": "1253520"
  },
  {
    "text": "about was what's going to be the proper size for us really thinking about scaling what the future growth is going to be you want to properly size your",
    "start": "1253520",
    "end": "1262200"
  },
  {
    "text": "vpcs so how did we address this so obviously there's a couple of other rfc's that have space available the",
    "start": "1263960",
    "end": "1270159"
  },
  {
    "text": "192168 16 it's about 65,000 addresses we already have over 100,000 instances up",
    "start": "1270159",
    "end": "1276720"
  },
  {
    "text": "and running so if we think about that space in terms of how we fail over from region to region even thinking about",
    "start": "1276720",
    "end": "1282600"
  },
  {
    "text": "additional growth that still wasn't going to be large enough for us so we also looked at the 1726 sl12 Network as well that's about a",
    "start": "1282600",
    "end": "1290720"
  },
  {
    "text": "little over a million usable addresses when we looked at that and we looked at how we wanted to decompose our",
    "start": "1290720",
    "end": "1296799"
  },
  {
    "text": "accounts that still wasn't large enough if we actually moved into that we probably wouldn't be able to grow um",
    "start": "1296799",
    "end": "1303559"
  },
  {
    "text": "over a year so RFC 6598 the address is not routable on the",
    "start": "1303559",
    "end": "1310039"
  },
  {
    "text": "internet provides just a bit over 4 million usable IP spaces or IP",
    "start": "1310039",
    "end": "1316240"
  },
  {
    "text": "addresses and effectively what allow to grow for the next 3 to four years now one thing to keep in mind",
    "start": "1316240",
    "end": "1323799"
  },
  {
    "text": "about this we knew this going into uh the migration that this particular space",
    "start": "1323799",
    "end": "1329440"
  },
  {
    "text": "actually was not resolvable in ec2 DNS but I'll get to a bit that later",
    "start": "1329440",
    "end": "1334679"
  },
  {
    "text": "so as far as uh IP address reservations are concerned again this is something new",
    "start": "1334679",
    "end": "1340400"
  },
  {
    "start": "1335000",
    "end": "1335000"
  },
  {
    "text": "that our engineering teams needed to take up on so internally we built a couple of tools the first one was Cloud",
    "start": "1340400",
    "end": "1345720"
  },
  {
    "text": "IP Cloud IP effectively allows engineering teams to reserve public addresses uh themselves it's like a",
    "start": "1345720",
    "end": "1352360"
  },
  {
    "text": "self-service tool if you need you know public addresses public blocks this is a tool you could use and you wouldn't have",
    "start": "1352360",
    "end": "1358520"
  },
  {
    "text": "to worry about any sort of overlap or contention between different teams the second thing was eni Auto",
    "start": "1358520",
    "end": "1365120"
  },
  {
    "text": "attach eni is Were A New Concept within VPC which you can effectively move around now for us the combination of",
    "start": "1365120",
    "end": "1371960"
  },
  {
    "text": "this and eni auto attach we could effectively tag these to our application clusters so that way when there's any",
    "start": "1371960",
    "end": "1378240"
  },
  {
    "text": "sort autoscaling events any sort of chaos event actually takes out uh particular instances the associated IPS",
    "start": "1378240",
    "end": "1384760"
  },
  {
    "text": "still remain with that cluster type now the VPC subit layout is",
    "start": "1384760",
    "end": "1391919"
  },
  {
    "start": "1388000",
    "end": "1388000"
  },
  {
    "text": "something we thought long and hard about now as I mentioned before size was definitely something of",
    "start": "1391919",
    "end": "1397120"
  },
  {
    "text": "concern and so we have three categories so is external subnets this is effectively for services that are that",
    "start": "1397120",
    "end": "1404440"
  },
  {
    "text": "are have public facing Services we have internal subnets those don't have services that need to",
    "start": "1404440",
    "end": "1411440"
  },
  {
    "text": "access to the internet directly and the last one is really partner submits partner submits you can",
    "start": "1411440",
    "end": "1417799"
  },
  {
    "text": "they're somewhat analogous to uh VPC in points to whereby it allows other",
    "start": "1417799",
    "end": "1422880"
  },
  {
    "text": "services internally to talk to some of our third party",
    "start": "1422880",
    "end": "1427158"
  },
  {
    "text": "Partners now as I mentioned planning out the size of the network is something you need to uh be very thoughtful about so",
    "start": "1428080",
    "end": "1435360"
  },
  {
    "text": "for us we knew know about how many instances we were G to have what that growth would look like but we also had",
    "start": "1435360",
    "end": "1441640"
  },
  {
    "text": "to account for all the other service components within VPC elbs take up IP addresses n gateways take up IP",
    "start": "1441640",
    "end": "1448799"
  },
  {
    "text": "addresses even launching a Lambda functions takes up addresses we actually ran into an issue where Lambda sucked up",
    "start": "1448799",
    "end": "1454799"
  },
  {
    "text": "about the 7,500 addresses so you need to keep that in",
    "start": "1454799",
    "end": "1460080"
  },
  {
    "text": "mind so the largest s that we had available to us was a SL",
    "start": "1460080",
    "end": "1465240"
  },
  {
    "text": "16 and as I mentioned we're always thinking about how do we improve our overall security So within classic",
    "start": "1465520",
    "end": "1471399"
  },
  {
    "text": "everything pretty much had a public IP address but all those Services actually didn't have internet facing services and",
    "start": "1471399",
    "end": "1477440"
  },
  {
    "text": "didn't need it so since we had the opportunity to actually move these into private subnets we wanted to move a",
    "start": "1477440",
    "end": "1482520"
  },
  {
    "text": "majority of our services into these internal submits and as far as the Netflix model",
    "start": "1482520",
    "end": "1488120"
  },
  {
    "text": "is concerned in terms of how we actually leverage uh VPC and azs we use what is known as a 3az model so we use 3 azs for",
    "start": "1488120",
    "end": "1496799"
  },
  {
    "text": "every account in every region so that way if there is a any sort of a failure we can still run out of that",
    "start": "1496799",
    "end": "1502440"
  },
  {
    "text": "region and not have to worry about capacity so again with that being the case we need to make the internal Subs",
    "start": "1502440",
    "end": "1508799"
  },
  {
    "text": "as large as possible so you have to cut these up on the bit boundary so you end up with",
    "start": "1508799",
    "end": "1514320"
  },
  {
    "text": "418s the 418s are allocated to the internal subnets so now we're going to carve up",
    "start": "1514320",
    "end": "1520039"
  },
  {
    "text": "the uh the last 318 so now we have the SL 20s we",
    "start": "1520039",
    "end": "1527080"
  },
  {
    "text": "allocated those to our external internal subnets then were CED up the last uh 20",
    "start": "1527080",
    "end": "1532799"
  },
  {
    "text": "to 22s and allocated those for our partner subnets so we were pretty efficient at",
    "start": "1532799",
    "end": "1538320"
  },
  {
    "text": "actually maximizing the the use of the IP space especially with our 3az model",
    "start": "1538320",
    "end": "1544200"
  },
  {
    "text": "now for some of you guys that might think about using a 2 a model you can probably actually use uh the IP space more",
    "start": "1544200",
    "end": "1550600"
  },
  {
    "text": "efficiently now at the end of the day this is our 3az model with the three different subnet types in each a",
    "start": "1550600",
    "end": "1559679"
  },
  {
    "text": "now as I mentioned one of the things we did want to do is actually bring a lot of these Services into the internal",
    "start": "1559679",
    "end": "1565039"
  },
  {
    "text": "subnet which means if it actually needs to go out to the internet it still needs to Traverse the N Gateway now in one of",
    "start": "1565039",
    "end": "1573039"
  },
  {
    "text": "our regions we probably push an average probably about you know",
    "start": "1573039",
    "end": "1578200"
  },
  {
    "text": "50 gigabytes going outbounds it's quite large so with natat",
    "start": "1578200",
    "end": "1583799"
  },
  {
    "text": "gateways and understanding of what that size were we need to figure out how do we actually scale out Nat gateways",
    "start": "1583799",
    "end": "1590039"
  },
  {
    "text": "so there are a couple of things we thought about so the first one was really around sharding the",
    "start": "1590039",
    "end": "1596720"
  },
  {
    "start": "1591000",
    "end": "1591000"
  },
  {
    "text": "subnets so in our typical model with just the single or with a single suet per a z you have a default gateway",
    "start": "1596720",
    "end": "1604279"
  },
  {
    "text": "points to the N Gateway that's great but what if we actually Shard that that",
    "start": "1604279",
    "end": "1609880"
  },
  {
    "text": "route make a high side of the internet low side internet those are both pointed",
    "start": "1609880",
    "end": "1614919"
  },
  {
    "text": "to two different n gateways you can split that even further you can actually build some automation around this",
    "start": "1614919",
    "end": "1620679"
  },
  {
    "text": "monitoring that gateways monitor the performance as the capacity starts to increase you can actually launch",
    "start": "1620679",
    "end": "1625960"
  },
  {
    "text": "additional that gateways and further cover of those subnets actually on the Fly we've actually tested this in moving",
    "start": "1625960",
    "end": "1631840"
  },
  {
    "text": "traffic between that gateways think we've seen maybe what one or two packet drops so it's actually pretty effective",
    "start": "1631840",
    "end": "1638360"
  },
  {
    "text": "but the one thing we didn't like about this is that you can potentially create hot spots so for example you have you know one of your service clusters may be",
    "start": "1638360",
    "end": "1644600"
  },
  {
    "text": "talking to one of your third- party Partners in that particular case that third party partner is probably within a",
    "start": "1644600",
    "end": "1650240"
  },
  {
    "text": "particular IP range which would make make that traffic actually go through that one n Gateway and so you would get",
    "start": "1650240",
    "end": "1655799"
  },
  {
    "text": "in a Hotpot on that net Gateway so we looked at this a little bit differently as well so rather than sharting the",
    "start": "1655799",
    "end": "1660840"
  },
  {
    "text": "actual sub or the actual Network route we thought about further spoting up",
    "start": "1660840",
    "end": "1666039"
  },
  {
    "text": "those subnets so you have two subnets you can even break it up to the four",
    "start": "1666039",
    "end": "1671240"
  },
  {
    "text": "subnets so each subnet now has its own route table and its own uh dedicated net",
    "start": "1671240",
    "end": "1677799"
  },
  {
    "text": "Gateway and again you can even keep going further if",
    "start": "1677799",
    "end": "1684600"
  },
  {
    "text": "necessary now I'm going to shift gears here and talk about uh Classic Link so just to provide you guys a bit of",
    "start": "1684679",
    "end": "1690000"
  },
  {
    "start": "1685000",
    "end": "1685000"
  },
  {
    "text": "context we have the green account on the left has a classic deployment and also V VPC deployment then we have the blue",
    "start": "1690000",
    "end": "1697519"
  },
  {
    "text": "account same thing classic and VPC and this is all within a region now as I",
    "start": "1697519",
    "end": "1704960"
  },
  {
    "text": "mentioned when you think about classic and how your classic instances actually communicate to one",
    "start": "1704960",
    "end": "1710679"
  },
  {
    "text": "another all that traffic is actually private so for example if you have services that are in the green account",
    "start": "1710679",
    "end": "1717799"
  },
  {
    "text": "they want to talk to the blue account that actually works as expected",
    "start": "1717799",
    "end": "1723080"
  },
  {
    "text": "we need make a public DNS call to the other account you actually get back a private address so you can think about",
    "start": "1723080",
    "end": "1729360"
  },
  {
    "text": "it as you know one big private writing domain so everything within a particular region is all actually a part of that",
    "start": "1729360",
    "end": "1734760"
  },
  {
    "text": "108 Network now with that being the case we need to turn on Classic Link in the",
    "start": "1734760",
    "end": "1741600"
  },
  {
    "text": "way those systems actually communicated we need to ensure that still had that same communication",
    "start": "1741600",
    "end": "1746919"
  },
  {
    "text": "flow so if we migrated the system our expectation was that you could actually still talk to it now when I Circle back",
    "start": "1746919",
    "end": "1753559"
  },
  {
    "text": "to the issue I mentioned earlier with that RFC 6598 as far as the 1064 those",
    "start": "1753559",
    "end": "1759200"
  },
  {
    "text": "addresses were not resolvable by ec2 DNS and so we knew",
    "start": "1759200",
    "end": "1765640"
  },
  {
    "text": "this what ended up happening is you actually got back to public address and so when that system wanted",
    "start": "1765640",
    "end": "1771399"
  },
  {
    "text": "to communicate to A system that migrated into VPC that traffic would actually go outside",
    "start": "1771399",
    "end": "1777279"
  },
  {
    "text": "externally and if it hit that Security Group wouldn't be allowed in because now it's external traffic so how do we address",
    "start": "1777279",
    "end": "1784760"
  },
  {
    "start": "1784000",
    "end": "1784000"
  },
  {
    "text": "this so most of our service uses Eureka which is open source it's our service",
    "start": "1784760",
    "end": "1790240"
  },
  {
    "text": "Discovery and in knowing this Eureka actually has the ability to capture uh four different attributes you can get",
    "start": "1790240",
    "end": "1797039"
  },
  {
    "text": "the public host name the public address private address private hos name all these are in there",
    "start": "1797039",
    "end": "1804200"
  },
  {
    "text": "so what we ended up doing to kind of buy some time was actually modify the",
    "start": "1804200",
    "end": "1809320"
  },
  {
    "text": "behavior as systems were deployed into VPC for the systems that were deployed into VPC rather than registering with",
    "start": "1809320",
    "end": "1815840"
  },
  {
    "text": "their ec2 public host name we'd have them register with their private address so that way when a service inside of",
    "start": "1815840",
    "end": "1822320"
  },
  {
    "text": "classic attempted to talk to the service that migrated into VPC they actually get back the private address directly",
    "start": "1822320",
    "end": "1829039"
  },
  {
    "text": "therefore we eliminated the issue in the manner in which uh the ec2 public host names were",
    "start": "1829039",
    "end": "1834679"
  },
  {
    "text": "resolving now while we knew this was the case as I",
    "start": "1834679",
    "end": "1840720"
  },
  {
    "text": "mentioned at the same time Amazon was also working on a couple other features to actually address this because not everything within our environment",
    "start": "1840720",
    "end": "1846880"
  },
  {
    "text": "actually use this the Discover uh our Discovery service and so in order to fix this they",
    "start": "1846880",
    "end": "1853720"
  },
  {
    "text": "worked in a feature called DNS over Classic Link and what this did was actually IC the behavior and how DNS",
    "start": "1853720",
    "end": "1859919"
  },
  {
    "text": "worked when it communicated across accounts within classic so when you call the ec2 public host name for a service",
    "start": "1859919",
    "end": "1865600"
  },
  {
    "text": "that's in VPC now you actually get back the private IP address which forces the",
    "start": "1865600",
    "end": "1871200"
  },
  {
    "text": "transport to go over that internal connection so now we had to consider how",
    "start": "1871200",
    "end": "1878559"
  },
  {
    "start": "1877000",
    "end": "1877000"
  },
  {
    "text": "do we allow system to communicate to other accounts as they migrate because you get this crazy",
    "start": "1878559",
    "end": "1884679"
  },
  {
    "text": "Matrix so ideally we should be able to communicate to this in the same manner as it was within classic but again we",
    "start": "1884679",
    "end": "1891240"
  },
  {
    "text": "ran into a similar issue so based on the timing know we knew we wanted to move and we had to start early in January",
    "start": "1891240",
    "end": "1898600"
  },
  {
    "text": "based on our timelines we went to sure that we're actually done before we got to the holiday seasons so with this being the case you",
    "start": "1898600",
    "end": "1906440"
  },
  {
    "text": "know we knew Amazon was also building additional services to account for these particular use cases so there are two",
    "start": "1906440",
    "end": "1913799"
  },
  {
    "text": "things they they worked on to help people migrate from classic into",
    "start": "1913799",
    "end": "1920200"
  },
  {
    "text": "VPC so the first one was Classic Link over peering so you needed to peer the vpcs",
    "start": "1920200",
    "end": "1926720"
  },
  {
    "text": "from the two accounts and this would actually extend the class link functionality into that",
    "start": "1926720",
    "end": "1932559"
  },
  {
    "text": "second account for that VPC and again the second piece was really around DNS so the DNS overing",
    "start": "1932559",
    "end": "1939720"
  },
  {
    "text": "feature allowed communication between these two accounts two environments to",
    "start": "1939720",
    "end": "1945200"
  },
  {
    "text": "resolve back to the private address now when you stack all these features",
    "start": "1945200",
    "end": "1952000"
  },
  {
    "text": "together you have Classic Link in one account your second account third account fourth",
    "start": "1952000",
    "end": "1957960"
  },
  {
    "text": "account what we ended up doing is actually creating a fully peered mesh of",
    "start": "1957960",
    "end": "1963000"
  },
  {
    "text": "vpcs on the back ends and once this is done you can affec",
    "start": "1963000",
    "end": "1968080"
  },
  {
    "text": "everything thing as a single routing domain so no matter the order of operations in which you move your",
    "start": "1968080",
    "end": "1973600"
  },
  {
    "text": "systems you would still have that communication flow between those services",
    "start": "1973600",
    "end": "1979840"
  },
  {
    "start": "1980000",
    "end": "1980000"
  },
  {
    "text": "now once we address the transport layer we need to figure out how do we",
    "start": "1980320",
    "end": "1985919"
  },
  {
    "text": "manage Classic Link at scale now now we have thousands of uh services",
    "start": "1985919",
    "end": "1994159"
  },
  {
    "text": "within classic and also within VPC and so with that being the case",
    "start": "1994159",
    "end": "1999600"
  },
  {
    "text": "there's thousands of uh autoskill events that happen on a daily basis and we",
    "start": "1999600",
    "end": "2004679"
  },
  {
    "text": "needed to ensure that classing was enabled for all of these at all times so three particular scenarios we",
    "start": "2004679",
    "end": "2010720"
  },
  {
    "text": "had to address so the first one was around addressing new Services we deployed into",
    "start": "2010720",
    "end": "2017799"
  },
  {
    "text": "VPC sorry classic once those Services were deployed into classic within new",
    "start": "2018159",
    "end": "2023679"
  },
  {
    "text": "configuration we knew classing was already going to be uh automatically enabled because that feature was actually added to the pipelines so when",
    "start": "2023679",
    "end": "2029840"
  },
  {
    "text": "they use Spiner to deploy those classing was already on the Second Use case we need to",
    "start": "2029840",
    "end": "2035320"
  },
  {
    "text": "address was real and instances that were running in some cases people would deploy stuff",
    "start": "2035320",
    "end": "2040360"
  },
  {
    "text": "and they might not push code for a quarter no which is perfectly fine but we need to account for ensuring that",
    "start": "2040360",
    "end": "2046039"
  },
  {
    "text": "those Services had classically enabled so we built a service internally that would actually sweep the entire fleet",
    "start": "2046039",
    "end": "2052839"
  },
  {
    "text": "and enabled classically on all those instances so that addressed those particular instances but now we also had",
    "start": "2052839",
    "end": "2058398"
  },
  {
    "text": "to account for Autos scaling events so in those cases if an Autos event actually happened it'd launch without",
    "start": "2058399",
    "end": "2065240"
  },
  {
    "text": "classy link which was going to be a problem so we had to ensure that wasn't going to happen so what we ended up doing there is we actually went through",
    "start": "2065240",
    "end": "2072079"
  },
  {
    "text": "swapped out all of the launch configs placed new launch configs in place that",
    "start": "2072079",
    "end": "2077679"
  },
  {
    "text": "actually when an Autos scale event occurred it actually launched new instances with Classic Link",
    "start": "2077679",
    "end": "2082919"
  },
  {
    "text": "enabled so that's how we accounted for that now as laori",
    "start": "2082919",
    "end": "2089079"
  },
  {
    "text": "mentioned we had to categorize our services this is really around the order of operations to ensure that we wouldn't",
    "start": "2089079",
    "end": "2095800"
  },
  {
    "text": "run into any sort of problems as far as communication flow because we knew those features from Amazon were",
    "start": "2095800",
    "end": "2101839"
  },
  {
    "text": "coming and so what we ended up doing is actually mapping our dependencies so we",
    "start": "2101839",
    "end": "2107720"
  },
  {
    "start": "2102000",
    "end": "2102000"
  },
  {
    "text": "basically We R the flow logs we also R some services off the actual instances",
    "start": "2107720",
    "end": "2113200"
  },
  {
    "text": "to determine what are all those dependencies what services talk to one another we also enhanced that with some",
    "start": "2113200",
    "end": "2119359"
  },
  {
    "text": "metadata to figure out how do Services communicate to one another what services are being leveraged so for example your",
    "start": "2119359",
    "end": "2125680"
  },
  {
    "text": "service talk to say um sqs S3 things of",
    "start": "2125680",
    "end": "2130960"
  },
  {
    "text": "that nature we wanted to know what that actually looked like and did that sort of analysis and so early on during our",
    "start": "2130960",
    "end": "2136400"
  },
  {
    "text": "migration we only wanted the migrate services that only had internal communication within a single",
    "start": "2136400",
    "end": "2142680"
  },
  {
    "text": "account for the more complex services for services that had cross account communication we held off until those",
    "start": "2142680",
    "end": "2149280"
  },
  {
    "text": "new features were available from uh from AWS that way that would minimize the",
    "start": "2149280",
    "end": "2154520"
  },
  {
    "text": "disruption in terms of how the applications were able to communicate to one another other now as far as Global routing is",
    "start": "2154520",
    "end": "2162560"
  },
  {
    "text": "concerns with VPC it has the advantage of you know connecting on the back with",
    "start": "2162560",
    "end": "2167720"
  },
  {
    "text": "either VPN or direct connect so in our case we have to use Direct",
    "start": "2167720",
    "end": "2173960"
  },
  {
    "start": "2173000",
    "end": "2173000"
  },
  {
    "text": "Connect we use direct conect to provide High redundency between our",
    "start": "2174119",
    "end": "2180280"
  },
  {
    "text": "Pops in addition with our Global backbone to provide full communication",
    "start": "2180280",
    "end": "2187119"
  },
  {
    "start": "2182000",
    "end": "2182000"
  },
  {
    "text": "pathways between all of our regions this is also one of the reasons",
    "start": "2187119",
    "end": "2192400"
  },
  {
    "text": "why we wanted to have globally unique IP addresses to ensure that there wasn't any sort of",
    "start": "2192400",
    "end": "2197760"
  },
  {
    "text": "conflict now once this was in place one of the things you know people should also think",
    "start": "2198280",
    "end": "2204560"
  },
  {
    "text": "about is when you have multiple accounts and you're trying to enforce separation between those accounts you should also",
    "start": "2204560",
    "end": "2209839"
  },
  {
    "text": "think about you know how do you enforce that separation at the network layer so if you have these test accounts and they should only talk to one another even",
    "start": "2209839",
    "end": "2215760"
  },
  {
    "text": "though they're globally dispersed you can actually enforce that on your backbone so one thing to keep in",
    "start": "2215760",
    "end": "2222000"
  },
  {
    "text": "mind now even with the the backbone in place we still had some other issues we",
    "start": "2222000",
    "end": "2227160"
  },
  {
    "start": "2223000",
    "end": "2223000"
  },
  {
    "text": "had to to account for so we have Labs that are on premise that you know test",
    "start": "2227160",
    "end": "2233960"
  },
  {
    "text": "like Xboxes PlayStations TVs things of that nature that actually need to talk to services in the",
    "start": "2233960",
    "end": "2240560"
  },
  {
    "text": "cloud so there's a problem since we actually use the 10 space we thought",
    "start": "2240560",
    "end": "2245760"
  },
  {
    "text": "about actually reing the offices that's going to be a huge undertaking there are",
    "start": "2245760",
    "end": "2251240"
  },
  {
    "text": "already a ton of tools and services around the labs and those",
    "start": "2251240",
    "end": "2256400"
  },
  {
    "text": "functionality so we looked at other options in terms of Trying to minimize the disruption to other teams so rather than having our lab engineering team",
    "start": "2256400",
    "end": "2262640"
  },
  {
    "text": "modify all their tools we wanted to see if we could somehow take care of this at the network layer we're already doing a",
    "start": "2262640",
    "end": "2267880"
  },
  {
    "text": "bulk of the work so just some incremental work for us and so we looked",
    "start": "2267880",
    "end": "2273079"
  },
  {
    "text": "at this obviously the expectation was for the labs to be able to communicate to those services once they moved into",
    "start": "2273079",
    "end": "2280599"
  },
  {
    "text": "VPC however once you turn on Classic Link effectively the lab space that rat",
    "start": "2280599",
    "end": "2289440"
  },
  {
    "text": "was hijacked all that traffic would now flow into uh to Classic because of Classic Link you inherently get that 10/",
    "start": "2289440",
    "end": "2296119"
  },
  {
    "text": "State Network that just points the local so there a couple things we had to do to address this traffic",
    "start": "2296119",
    "end": "2303520"
  },
  {
    "text": "flow so how do we do this so we looked at a couple of things so one we needed",
    "start": "2303520",
    "end": "2310480"
  },
  {
    "text": "to natat the traffic going into to lab space so the way we actually ipd our",
    "start": "2310480",
    "end": "2317640"
  },
  {
    "text": "offices our lab space um we kind of accounted for you know a",
    "start": "2317640",
    "end": "2324040"
  },
  {
    "text": "variety of different scenarios and with that being the case we're actually able just to Simply swap out the first octet",
    "start": "2324040",
    "end": "2330520"
  },
  {
    "text": "with a different octet so that's how we actually added that particular traffic so that helps the or actually that",
    "start": "2330520",
    "end": "2338119"
  },
  {
    "text": "addresses the communication between the VPC with classically enabled going to our Labs now the second piece we needed",
    "start": "2338119",
    "end": "2344160"
  },
  {
    "text": "to address was really around DNS so when Services communicated to the labs they use the local resolvers in those labs",
    "start": "2344160",
    "end": "2351319"
  },
  {
    "text": "those lab resolvers would actually return with the 10 address which again is going to be a problem so we built",
    "start": "2351319",
    "end": "2357920"
  },
  {
    "text": "custom DNS servers in VPC all those instances leverage those custom DNS servers and when they",
    "start": "2357920",
    "end": "2363880"
  },
  {
    "text": "communicated with the labs those custom DNS servers would intercept the response",
    "start": "2363880",
    "end": "2369400"
  },
  {
    "text": "and also modify that octed so now we have the DNS response modified and also the OCTA modified at the network layer",
    "start": "2369400",
    "end": "2377200"
  },
  {
    "text": "and effectively communication would flow between all",
    "start": "2377200",
    "end": "2381480"
  },
  {
    "start": "2382000",
    "end": "2382000"
  },
  {
    "text": "these so if you combine you know Classic Link",
    "start": "2382359",
    "end": "2387680"
  },
  {
    "text": "pering the Ning we did on the back end effectively our Global infrastructure can now communicate without any sort of",
    "start": "2387680",
    "end": "2394359"
  },
  {
    "text": "Ip conflicts so what did we learn from this there",
    "start": "2394359",
    "end": "2401480"
  },
  {
    "start": "2401000",
    "end": "2401000"
  },
  {
    "text": "cute things to think about so one as far as the IP address scheme is really around making sure that's really uh",
    "start": "2401480",
    "end": "2410200"
  },
  {
    "text": "globally unique just to avoid all these conflicts having to reip things is not an easy undertaking it's quite a",
    "start": "2410200",
    "end": "2418079"
  },
  {
    "text": "pain the other thing you want to think about is really around traffic patterns understanding traffic patterns really",
    "start": "2418079",
    "end": "2423920"
  },
  {
    "text": "helps to figure out what is the right order of operations in terms of high how you actually migrate your systems from",
    "start": "2423920",
    "end": "2429319"
  },
  {
    "text": "classic into VPC this was definitely key for us especially knowing that there were new features that were going to",
    "start": "2429319",
    "end": "2435160"
  },
  {
    "text": "address some of our pain points one of the other things to really",
    "start": "2435160",
    "end": "2441480"
  },
  {
    "text": "consider is partner engagement we work with a lot of third-party Partners so",
    "start": "2441480",
    "end": "2446520"
  },
  {
    "text": "you want to engage with these folks very early on especially around like you know Whit listing particular services in some",
    "start": "2446520",
    "end": "2453440"
  },
  {
    "text": "cases we had some folks that took you know few weeks to actually update their white lists so you want to give them",
    "start": "2453440",
    "end": "2459680"
  },
  {
    "text": "enough heads up as possible the last thing I thought about was reducing our technical debt we",
    "start": "2459680",
    "end": "2468079"
  },
  {
    "text": "definitely wanted to leverage the opportunity as we moved over from classic into VPC to reduce as much",
    "start": "2468079",
    "end": "2473440"
  },
  {
    "text": "technical debt as possible but at the same time we don't want to have the technical debt actually paralyze us and",
    "start": "2473440",
    "end": "2479000"
  },
  {
    "text": "not move so it's okay to bring some of that stuff over but just think about you know what are the things you would bring",
    "start": "2479000",
    "end": "2484119"
  },
  {
    "text": "over and how you'd actually address it at a later date now want to Circle back with all these",
    "start": "2484119",
    "end": "2489760"
  },
  {
    "start": "2487000",
    "end": "2487000"
  },
  {
    "text": "features and things that were added on so we have Classic Link we have Classic Link over DNS we have DNS over peering",
    "start": "2489760",
    "end": "2496880"
  },
  {
    "text": "we have ec2 Dena support for non RFC 1918 space we have our Classic Link service we had all these things we put",
    "start": "2496880",
    "end": "2503960"
  },
  {
    "text": "together now we put them all together just so our engineering teams can actually hit that easy button one",
    "start": "2503960",
    "end": "2511480"
  },
  {
    "text": "click of the button the servers migrates from classic into VPC all the heavy lifting was done in the back ends",
    "start": "2511480",
    "end": "2519240"
  },
  {
    "text": "so in February that's where we're at and as of today we migrated over 99% of over a 100,000 instances by doing",
    "start": "2519680",
    "end": "2528280"
  },
  {
    "text": "all this work we now have a new Mountain in",
    "start": "2528280",
    "end": "2535240"
  },
  {
    "text": "VPC",
    "start": "2535640",
    "end": "2538640"
  },
  {
    "text": "thanks so I believe there's a eval to complete and then we're going to allocate at least 15 minutes for",
    "start": "2544640",
    "end": "2550640"
  },
  {
    "text": "questions but here are some other related uh talks from Netflix",
    "start": "2550640",
    "end": "2557079"
  }
]