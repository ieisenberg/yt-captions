[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "is Luke Anderson and I'm from the business development team at Amazon Web Services today I'm going to be talking",
    "start": "30",
    "end": "6870"
  },
  {
    "text": "about building your data Lakes that cost less and deliver results faster what",
    "start": "6870",
    "end": "14969"
  },
  {
    "text": "we're going to cover today is do a quick explanation of how we define data Lakes today at AWS a dive into how to reduce",
    "start": "14969",
    "end": "22320"
  },
  {
    "text": "costs of building a data Lake and then how to increase your performance and then ultimately how to make sure that",
    "start": "22320",
    "end": "28470"
  },
  {
    "text": "we're planning and evolving for the future when we have finished this",
    "start": "28470",
    "end": "34620"
  },
  {
    "start": "32000",
    "end": "32000"
  },
  {
    "text": "webinar hopefully some of the things you've seen will spark ideas of how to build systems that will better enable",
    "start": "34620",
    "end": "40230"
  },
  {
    "text": "your business as there is an increasingly a recognition that businesses need to be more data driven",
    "start": "40230",
    "end": "46050"
  },
  {
    "text": "to enable automation and to enhance the decision making of the business now is a",
    "start": "46050",
    "end": "51149"
  },
  {
    "text": "good time to rethink how to go about that more and more we see are our",
    "start": "51149",
    "end": "56640"
  },
  {
    "text": "customers moving from old legacy approaches of buying inexpensive data infrastructure which often takes months",
    "start": "56640",
    "end": "63390"
  },
  {
    "text": "or years to start getting results from to start thinking to be more agile and nimble to be truly business focused we",
    "start": "63390",
    "end": "71280"
  },
  {
    "text": "believe there are three ways they're beginning to think differently the first is to start the projects with the",
    "start": "71280",
    "end": "77250"
  },
  {
    "text": "specific business outcomes in mind start backwards from the insights and actions",
    "start": "77250",
    "end": "82350"
  },
  {
    "text": "you want and in work backwards to a streamlined design number two is",
    "start": "82350",
    "end": "87540"
  },
  {
    "text": "experimentation start with a lean design use just enough data to test your ideas",
    "start": "87540",
    "end": "93119"
  },
  {
    "text": "use just enough services to test those ideas the design of the system is to scale up capacity as and when you need",
    "start": "93119",
    "end": "100229"
  },
  {
    "text": "it so if you hear some a great result you scale with that one up and the other ones they didn't work out can just be",
    "start": "100229",
    "end": "106079"
  },
  {
    "text": "turned off think win quick fail cheap and finally speed with agility and",
    "start": "106079",
    "end": "112439"
  },
  {
    "text": "timeliness our customers are changing their markets they're redefining or service levels and customer experience",
    "start": "112439",
    "end": "118560"
  },
  {
    "text": "means in those markets much of this is that they move quickly when an",
    "start": "118560",
    "end": "123780"
  },
  {
    "text": "opportunity presents itself and the business wants to move an opportunity they think in terms of weeks to design",
    "start": "123780",
    "end": "129450"
  },
  {
    "text": "and minutes to deploy this gives a material advantage over businesses that will wait six",
    "start": "129450",
    "end": "135810"
  },
  {
    "text": "months to get approvals to buy an appliance or more storage we've had many",
    "start": "135810",
    "end": "141000"
  },
  {
    "text": "customers here succeeding in Southeast Asia especially in Singapore where we have some great customers like the red",
    "start": "141000",
    "end": "146430"
  },
  {
    "text": "Mart and grab and a number of others that are fundamentally changing aspects of our daily lives as consumers so",
    "start": "146430",
    "end": "155489"
  },
  {
    "start": "154000",
    "end": "154000"
  },
  {
    "text": "traditionally if you're doing an on-premise or traditional analytics you very much likely have an architecture",
    "start": "155489",
    "end": "160560"
  },
  {
    "text": "that looks like this we have a lot of duplication and sprawl where you may",
    "start": "160560",
    "end": "165750"
  },
  {
    "text": "have your structured data and databases data Mart's and ultimately maybe rolling up to a data warehouse of some sort and",
    "start": "165750",
    "end": "172230"
  },
  {
    "text": "you also probably store all business data in traditional storage arrays and for the more advanced teams focused on",
    "start": "172230",
    "end": "178739"
  },
  {
    "text": "analytics and machine learning you probably have some Hadoop clusters no sequel databases to perform more",
    "start": "178739",
    "end": "184500"
  },
  {
    "text": "advanced analytics if we start to think about how to reduce cost and increase",
    "start": "184500",
    "end": "190260"
  },
  {
    "text": "performance this is the lowest hanging fruit because one of the biggest costs reduction is to eliminate the multiple",
    "start": "190260",
    "end": "196530"
  },
  {
    "text": "copies of data and if you have an environment like this it is quite likely you have a large amount of duplicate",
    "start": "196530",
    "end": "202889"
  },
  {
    "text": "data another way to improve performance is to stop moving data around between arrays and different applications and",
    "start": "202889",
    "end": "209849"
  },
  {
    "text": "focus and work from a central set of data that is going to give you the biggest performance improvement the way",
    "start": "209849",
    "end": "219810"
  },
  {
    "start": "217000",
    "end": "217000"
  },
  {
    "text": "we define data lakes here at AWS is that it's an architecture and not a solution any sir it's an architect's that",
    "start": "219810",
    "end": "226979"
  },
  {
    "text": "provides virtually limitless centralized storage capabilities and where you can then process the data where it lives",
    "start": "226979",
    "end": "233569"
  },
  {
    "text": "some of the key attributes we're going to use to define this are one we",
    "start": "233569",
    "end": "238949"
  },
  {
    "text": "decoupled storage and compute which is both to optimize cost and optimize performance number two we're going to",
    "start": "238949",
    "end": "246449"
  },
  {
    "text": "focus on getting the data and rapidly and transform it as quickly as possible into a format that you can actually use",
    "start": "246449",
    "end": "252750"
  },
  {
    "text": "to deliver business results number three how to secure multi-tenancy because",
    "start": "252750",
    "end": "258359"
  },
  {
    "text": "ultimately you're going to have a lot of people wanting to use this to data and platform and it's got to keep your seaso",
    "start": "258359",
    "end": "263880"
  },
  {
    "text": "and other security folks happy number four query in place because that's going to",
    "start": "263880",
    "end": "269430"
  },
  {
    "text": "be the most cost-effective and performant way to do it and finally five schema on read so you can rapidly",
    "start": "269430",
    "end": "275130"
  },
  {
    "text": "iterate AWS big data analytic services",
    "start": "275130",
    "end": "281640"
  },
  {
    "start": "277000",
    "end": "277000"
  },
  {
    "text": "enable our customers and yourselves to easily run any analytics workload whether it be batch ad-hoc real-time IOT",
    "start": "281640",
    "end": "289020"
  },
  {
    "text": "and predictive analytics and at any scale whether it's gigabytes 2 terabytes",
    "start": "289020",
    "end": "294150"
  },
  {
    "text": "or petabytes and even to excersice in a secure fashion at the lowest possible cost AWS provides a highly scaleable",
    "start": "294150",
    "end": "302100"
  },
  {
    "text": "available secure and cost-effective data store that lets you store data in its native format and easily extract value",
    "start": "302100",
    "end": "309420"
  },
  {
    "text": "from your data this is particularly true now that many customers see much of their new data created directly in the",
    "start": "309420",
    "end": "316170"
  },
  {
    "text": "cloud with Amazon s3 being the home to the vast majority of it with much more",
    "start": "316170",
    "end": "321540"
  },
  {
    "text": "operating experience and scale and a broader set of analytic services available than anywhere else s3 and our",
    "start": "321540",
    "end": "327540"
  },
  {
    "text": "portfolio of big data and analytics services is to clean up one choice for you to build your data Lake and analytic",
    "start": "327540",
    "end": "333300"
  },
  {
    "text": "solutions with by turning this into an architecture diagram using some common AWS services you can now see what the",
    "start": "333300",
    "end": "340530"
  },
  {
    "text": "outcome is on the screen at the moment so starting at the bottom you've got to get the data integrated with the",
    "start": "340530",
    "end": "346800"
  },
  {
    "text": "platform which means moving data that you can onto the platform where it lives but you've got to be got to know your",
    "start": "346800",
    "end": "353520"
  },
  {
    "text": "pretty collecting it from a multiple different data sources from third-party partners and even from on-premise",
    "start": "353520",
    "end": "359640"
  },
  {
    "text": "equipment so as you can see at the bottom there are five services listed here and this is by no means exhaustive",
    "start": "359640",
    "end": "365640"
  },
  {
    "text": "it's just an initial view the database migration service which you see at the bottom left or DMS it's not going to be",
    "start": "365640",
    "end": "373200"
  },
  {
    "text": "covered today but as with all of our services detailed documentation is",
    "start": "373200",
    "end": "378270"
  },
  {
    "text": "available on our website the next to the snowball of snowmobile are our data ingestion and migration",
    "start": "378270",
    "end": "384570"
  },
  {
    "text": "appliances snow will being the on-premise appliance and snowmobile is",
    "start": "384570",
    "end": "389910"
  },
  {
    "text": "physically a truck that we drive to your data center both of which is where you",
    "start": "389910",
    "end": "395430"
  },
  {
    "text": "plug it into your sauce on-premise data stores upload your storage we",
    "start": "395430",
    "end": "401370"
  },
  {
    "text": "either and move that to our regions and input the data the final two Kinesis",
    "start": "401370",
    "end": "407580"
  },
  {
    "text": "data firehose and data streams will be covered later in this presentation then",
    "start": "407580",
    "end": "413310"
  },
  {
    "text": "as we move up through there into the data Lake foundational layer which is where our storage storage object storage",
    "start": "413310",
    "end": "420510"
  },
  {
    "text": "solutions of s3 and Glacia live and then also at the data Lake layer we have the",
    "start": "420510",
    "end": "426000"
  },
  {
    "text": "AWS group which will help you transform the data to get it useful as quickly as possible",
    "start": "426000",
    "end": "431210"
  },
  {
    "text": "then comes the analytics tools on top of that you can pick and choose from a wide array of analytics tools both native AWS",
    "start": "431210",
    "end": "438270"
  },
  {
    "text": "services to do things like take-two warehousing big data processing and interactive query but there's also a",
    "start": "438270",
    "end": "445260"
  },
  {
    "text": "whole ecosystem of third-party tools and developers that both your data scientists and yourself can use the",
    "start": "445260",
    "end": "452940"
  },
  {
    "text": "kidneys to use the tool that you're most comfortable with because ultimately you'll be on the hook to deliver the insights and answers that",
    "start": "452940",
    "end": "458430"
  },
  {
    "text": "drive the business outcomes so why do we",
    "start": "458430",
    "end": "464820"
  },
  {
    "start": "461000",
    "end": "461000"
  },
  {
    "text": "choose s3 and by extension the Glacia platforms for the data Lake the first is",
    "start": "464820",
    "end": "471120"
  },
  {
    "text": "the first reason is unmatched durability availability and scalability because if",
    "start": "471120",
    "end": "476700"
  },
  {
    "text": "this is going to be the core part of your business it has to be rock solid and reliable you've got to be able to",
    "start": "476700",
    "end": "483570"
  },
  {
    "text": "count on that data as being permanent and solid and available for when you need it",
    "start": "483570",
    "end": "489349"
  },
  {
    "text": "the second one the security compliance and audit capabilities are important",
    "start": "489349",
    "end": "494729"
  },
  {
    "text": "you're going to have business critical information and maybe even personal data in there and even in certain industries",
    "start": "494729",
    "end": "501419"
  },
  {
    "text": "you're going to have to want to share this data with other organizations so you have to make sure you've got very",
    "start": "501419",
    "end": "507479"
  },
  {
    "text": "granular airtight control of who can access what and when to make sure your compliance teams and teams are happy the",
    "start": "507479",
    "end": "516870"
  },
  {
    "text": "third one object level control at any scale to provide that very granular level of how",
    "start": "516870",
    "end": "522029"
  },
  {
    "text": "you control not just access to your data but what happens to your data I use",
    "start": "522029",
    "end": "527310"
  },
  {
    "text": "things like lifecycle in fourth one is you want to get quick business insights into your data which I'll talk of in",
    "start": "527310",
    "end": "533550"
  },
  {
    "text": "about later the methods the mechanisms to integrate the data bring it in and create hybrid",
    "start": "533550",
    "end": "538790"
  },
  {
    "text": "architectures and then finally it's all about partners with a broad ecosystem we have twice as many partner integrations",
    "start": "538790",
    "end": "545480"
  },
  {
    "text": "as other leading providers out there so",
    "start": "545480",
    "end": "550520"
  },
  {
    "text": "now let's talk about once you start this journey you can build a cost optimize data Lake and how can you reduce your",
    "start": "550520",
    "end": "556910"
  },
  {
    "text": "data lake costs just pause there for a",
    "start": "556910",
    "end": "561920"
  },
  {
    "start": "559000",
    "end": "559000"
  },
  {
    "text": "second to make sure the webinars catching up with everybody so we're all in sync so the first thing you want to",
    "start": "561920",
    "end": "576020"
  },
  {
    "text": "think about is data tearing because one size doesn't fit all when it comes to data first you're going to have data",
    "start": "576020",
    "end": "583100"
  },
  {
    "text": "that is very hot needed to be accessed frequently and where you need the highest levels of performance these",
    "start": "583100",
    "end": "589220"
  },
  {
    "text": "characteristics and requirements will drive up your cost base primarily due to the highly optimized storage and compute",
    "start": "589220",
    "end": "595310"
  },
  {
    "text": "platforms required so not all your data will need to be stored here once that",
    "start": "595310",
    "end": "600380"
  },
  {
    "text": "data in the hottest area is ages and cools it can be stored on lower cost architecture that with the right",
    "start": "600380",
    "end": "606620"
  },
  {
    "text": "metadata and tagging can enable things like automated life cycle for example you'd want to keep your hottest data and",
    "start": "606620",
    "end": "613220"
  },
  {
    "text": "EMR and Hadoop and in high performing processing services like Amazon redshift then once it cools apply life cycling to",
    "start": "613220",
    "end": "621380"
  },
  {
    "text": "lower cost tears like s3 or s3 and frequent access and then keep more",
    "start": "621380",
    "end": "626480"
  },
  {
    "text": "historical data set stored in Amazon Glacia so this is the foundational area for controlling costs the second area is",
    "start": "626480",
    "end": "640940"
  },
  {
    "start": "634000",
    "end": "634000"
  },
  {
    "text": "processing page four in place as much as possible to further enable cost reductions essentially avoid having to",
    "start": "640940",
    "end": "648110"
  },
  {
    "text": "move data around will save both from monetary cost as well as time costs to do so this will enable multiple tools to",
    "start": "648110",
    "end": "655850"
  },
  {
    "text": "share the same data sets which is the biggest cost producer here rather than moving to and fro and having multiple",
    "start": "655850",
    "end": "661640"
  },
  {
    "text": "copies of data exists in each tool so all our new common analytic tools like",
    "start": "661640",
    "end": "667370"
  },
  {
    "text": "Amazon Athena red spectrum and Amazon EMR and AWS glue can all process data in place on s3 now",
    "start": "667370",
    "end": "675060"
  },
  {
    "text": "we're going to start covering each one of these during the presentation if your",
    "start": "675060",
    "end": "682440"
  },
  {
    "start": "680000",
    "end": "680000"
  },
  {
    "text": "workload requires highly distributed processing framework such as a dupe and spark Amazon EMR might be the tool of choice",
    "start": "682440",
    "end": "689210"
  },
  {
    "text": "with EMR you can decouple compute and storage which means you can one shutdown",
    "start": "689210",
    "end": "694830"
  },
  {
    "text": "here mark rosters with no data loss right-size EMR clusters independently of",
    "start": "694830",
    "end": "700170"
  },
  {
    "text": "storage and three have multiple Amazon EMR clusters running concurrently it is",
    "start": "700170",
    "end": "707220"
  },
  {
    "text": "ideal to compress your data set using standard compression algorithms such as gzip LZ / and snapping each of these",
    "start": "707220",
    "end": "714480"
  },
  {
    "text": "have their pros and cons with respect to spit ability compression ratio and compression speed for example gzip is",
    "start": "714480",
    "end": "721980"
  },
  {
    "text": "not suitable has high compression ratio and medium speed compared to snappy which is also not specific has low",
    "start": "721980",
    "end": "728100"
  },
  {
    "text": "compression ratio but is very fast also to achieve faster performance columnar",
    "start": "728100",
    "end": "733620"
  },
  {
    "text": "formats and aggregating small files to larger objects will result in the best performance you will notice that",
    "start": "733620",
    "end": "740370"
  },
  {
    "text": "irrespective of which AWS query and tool you choose aggregating files into larger",
    "start": "740370",
    "end": "746160"
  },
  {
    "text": "objects is integral to reducing costs few benefits of having in larger files",
    "start": "746160",
    "end": "752250"
  },
  {
    "text": "are faster listening fewer s3 requests and less metadata to manage there are",
    "start": "752250",
    "end": "757530"
  },
  {
    "text": "also a number of other considerations such as using support instances where you can save up to 80% of your compute",
    "start": "757530",
    "end": "764280"
  },
  {
    "text": "cost for running the clusters and then finally use elastic scaling so you can scale up and scale down to meet demand",
    "start": "764280",
    "end": "771030"
  },
  {
    "text": "this would enable you to spin up multi clusters segments your analytics workloads and fundamentally run them",
    "start": "771030",
    "end": "777930"
  },
  {
    "text": "from one data store on s3 next up is",
    "start": "777930",
    "end": "785520"
  },
  {
    "start": "783000",
    "end": "783000"
  },
  {
    "text": "Amazon redshift spectrum which is a tool that allows you to run Amazon redshift SQL queries against vast amounts of data",
    "start": "785520",
    "end": "793110"
  },
  {
    "text": "in your Amazon s3 data lake without having to go through tedious and time-consuming ETL process",
    "start": "793110",
    "end": "799760"
  },
  {
    "text": "Amazon redshift spectrum apply sophisticated query optimizer Asian and scales processing across thousands of rows to",
    "start": "799760",
    "end": "806579"
  },
  {
    "text": "deliver fast performance some of the recommended practices that can help you optimize your concurrent workload",
    "start": "806579",
    "end": "811920"
  },
  {
    "text": "performance using redshift which are the first one is data warehousing with hundreds of terabytes of data the",
    "start": "811920",
    "end": "818879"
  },
  {
    "text": "restore process can take a long time and results in the data latency issues with Amazon redshift spectrum you can move",
    "start": "818879",
    "end": "825420"
  },
  {
    "text": "the largest tables to Amazon s3 and each redshift cluster needs to keep only a",
    "start": "825420",
    "end": "831149"
  },
  {
    "text": "small amount of that data and local discs because of that reduction data volume it'd be much faster to spin up or",
    "start": "831149",
    "end": "838170"
  },
  {
    "text": "restore multiple read-only Amazon redshift clusters to handle these seasonal spiky query work or clothes",
    "start": "838170",
    "end": "845180"
  },
  {
    "text": "similar to EMR Amazon redshift spectrum reads only from columns of a file that",
    "start": "845180",
    "end": "850800"
  },
  {
    "text": "are needed for the query so Amazon redshift spectrum charges you by the amount of dates that is scanned from the",
    "start": "850800",
    "end": "856829"
  },
  {
    "text": "s3 per query an example of a column format is parquet that stores data in",
    "start": "856829",
    "end": "862920"
  },
  {
    "text": "column a format so Amazon redshift spectrum can eliminate the unneeded columns during the scan and the third",
    "start": "862920",
    "end": "870930"
  },
  {
    "text": "ways there are certain SQL operations that can be pushed down to Amazon redshift spectrum layer and you want to",
    "start": "870930",
    "end": "877230"
  },
  {
    "text": "take advantage of these wherever possible such as the group by clause certain string functions common",
    "start": "877230",
    "end": "882449"
  },
  {
    "text": "aggregate functions such as the count average some min etc moving on to Amazon",
    "start": "882449",
    "end": "891480"
  },
  {
    "start": "888000",
    "end": "888000"
  },
  {
    "text": "Athena Amazon Athena is our interactive query service that makes it easy to",
    "start": "891480",
    "end": "896970"
  },
  {
    "text": "analyze the data stored in Amazon s3 using standard sequel Athena is service",
    "start": "896970",
    "end": "902819"
  },
  {
    "text": "so there's no infrastructure to manage and you only pay for the queries that you run simply point your data in Amazon",
    "start": "902819",
    "end": "910290"
  },
  {
    "text": "s3 define your schema and then start querying using standard sequel syntax",
    "start": "910290",
    "end": "916279"
  },
  {
    "text": "compressing your data can also speed up your queries significantly as long as",
    "start": "916279",
    "end": "921420"
  },
  {
    "text": "the files over an optimal size all the files are spital now splittable files",
    "start": "921420",
    "end": "927029"
  },
  {
    "text": "allows amazon's execute Athena's execution engine to spit the reading of a file by multiple readers to increase",
    "start": "927029",
    "end": "934559"
  },
  {
    "text": "parallelism if you have a single unsuitable file then only a single reader is able to",
    "start": "934559",
    "end": "940170"
  },
  {
    "text": "read the file while all other readers sit idle for Athena we recommend using either Apache park'",
    "start": "940170",
    "end": "947190"
  },
  {
    "text": "or Apache o RC which can press dated by default and Ospital when there are not",
    "start": "947190",
    "end": "952980"
  },
  {
    "text": "an option then you can also try other third party such as be zipped or gzip with an optimal file size queries tend",
    "start": "952980",
    "end": "960720"
  },
  {
    "text": "to run more efficiently when reading data can be paralyzed and blocks of data can be read sequentially ensuring that",
    "start": "960720",
    "end": "967440"
  },
  {
    "text": "your file formats are splittable helps with this ability to run them in parallel regardless of how large your",
    "start": "967440",
    "end": "973050"
  },
  {
    "text": "files may be however if your files are too small generally less than 128",
    "start": "973050",
    "end": "978870"
  },
  {
    "text": "megabytes then the execution engine may be spending additional time with the",
    "start": "978870",
    "end": "984450"
  },
  {
    "text": "overhead of opening s3 files visiting directories or getting the object metadata on the other hand if your file",
    "start": "984450",
    "end": "993060"
  },
  {
    "text": "is not splittable and the files are too large then query processing waits until a single reader has completed reading",
    "start": "993060",
    "end": "1000230"
  },
  {
    "text": "the entire file that can reduce parallelism and also performance so we",
    "start": "1000230",
    "end": "1006890"
  },
  {
    "text": "recommend a larger block size if you have tables of many columns to ensure that each column block remains at a size",
    "start": "1006890",
    "end": "1012620"
  },
  {
    "text": "that allows for efficient sequential i/o",
    "start": "1012620",
    "end": "1017080"
  },
  {
    "text": "so we've taught for the last few slides we talked a lot about using the right data format like parque loro IC or some",
    "start": "1018760",
    "end": "1026270"
  },
  {
    "text": "sort of other compression columnar compression format but if you got a streaming type workload were you doing a",
    "start": "1026270",
    "end": "1032240"
  },
  {
    "text": "lot of mock diving like security analytics it may not be feasible to use",
    "start": "1032240",
    "end": "1037880"
  },
  {
    "text": "these formats so at Amazon we focus on putting more intelligence into the storage layer so if you can't use any of",
    "start": "1037880",
    "end": "1045260"
  },
  {
    "text": "the format's we've previously discussed there are other ways you can do this we",
    "start": "1045260",
    "end": "1051200"
  },
  {
    "text": "shall discuss a little bit later some of",
    "start": "1051200",
    "end": "1057050"
  },
  {
    "text": "the tools that we just discussed today retrieve the entire object from s3 or Glacia even if you only need to pull a",
    "start": "1057050",
    "end": "1064310"
  },
  {
    "text": "small piece of data out traditionally you always restore a block of and then the analytics tool scans all",
    "start": "1064310",
    "end": "1071390"
  },
  {
    "text": "through that data and then he only chooses what parts that data they need to run any further queries or or",
    "start": "1071390",
    "end": "1077450"
  },
  {
    "text": "generate some insights and it's just the way traditional storage packs on platforms works when they'll coupled",
    "start": "1077450",
    "end": "1083480"
  },
  {
    "text": "with analytics tools so as I mentioned",
    "start": "1083480",
    "end": "1089419"
  },
  {
    "text": "there are at Amazon we've been investigating how to do this how do we move more intelligence down into the",
    "start": "1089419",
    "end": "1095299"
  },
  {
    "text": "storage layer so what I want to talk about now is Amazon s3 select and Glacia",
    "start": "1095299",
    "end": "1101600"
  },
  {
    "text": "select which you may have seen released or mentioned at reinvent last year and g8 earlier this year",
    "start": "1101600",
    "end": "1107889"
  },
  {
    "text": "now with select both s3 and glacier select it allows you to perform sequel",
    "start": "1107889",
    "end": "1113600"
  },
  {
    "text": "expressions to select a subset of data from an object which is a much more efficient way than retrieving the whole",
    "start": "1113600",
    "end": "1120260"
  },
  {
    "text": "object and having the analytics tool run that scanning and filtering so what was",
    "start": "1120260",
    "end": "1128480"
  },
  {
    "text": "the motivation around this so we just discussed Amazon redshift spectrum and",
    "start": "1128480",
    "end": "1134750"
  },
  {
    "text": "how you could query data directly from s3 but if you look at a live customer",
    "start": "1134750",
    "end": "1140120"
  },
  {
    "text": "example of using Amazon register spectrum they ran 50,000 queries which",
    "start": "1140120",
    "end": "1146570"
  },
  {
    "text": "fetched six petabytes of data from s3 to run those queries but when we looked at",
    "start": "1146570",
    "end": "1152149"
  },
  {
    "text": "the actual amount of data used by redshift it was six hundred and fifty terabytes so roughly ten percent of the",
    "start": "1152149",
    "end": "1158510"
  },
  {
    "text": "data was needed so think about a model where you could intelligently push down",
    "start": "1158510",
    "end": "1163580"
  },
  {
    "text": "to the storage layer the ability to do that data scanning there and only the return the results that redshift",
    "start": "1163580",
    "end": "1169909"
  },
  {
    "text": "spectrum needed number one that's going to greatly reduce redshift spectrum scan",
    "start": "1169909",
    "end": "1175250"
  },
  {
    "text": "cost but number two is going to greatly increase performance so this is one of",
    "start": "1175250",
    "end": "1180470"
  },
  {
    "text": "the main drivers around implementing and launching s3 and Glacia select so what",
    "start": "1180470",
    "end": "1189980"
  },
  {
    "text": "is it so essentially allows you to select a filter set of data from within an object",
    "start": "1189980",
    "end": "1196070"
  },
  {
    "text": "using a standard sequel statement so it's the first content aware API within the Amazon S",
    "start": "1196070",
    "end": "1201690"
  },
  {
    "text": "family unlike unlike Athena and spectrum it operates within the s3 system where",
    "start": "1201690",
    "end": "1208080"
  },
  {
    "text": "the s3 system is doing the status scanning and data filtering work it operates on a per object basis not",
    "start": "1208080",
    "end": "1215519"
  },
  {
    "text": "across a group of objects so it's not a replacement for Amazon Athena or register spectrum or even EMR and it's",
    "start": "1215519",
    "end": "1223019"
  },
  {
    "text": "important to note it doesn't change some of the other characteristics of how s3 works it's just an addition enhancement",
    "start": "1223019",
    "end": "1228929"
  },
  {
    "text": "to the existing API is SDK and CLI so",
    "start": "1228929",
    "end": "1234269"
  },
  {
    "text": "who's going to use this I mean obviously it's a great plugin for Amazon redshift spectrum Athena and EMR and other custom",
    "start": "1234269",
    "end": "1241320"
  },
  {
    "text": "query engines where it can optimize and reduce to cost and improve their performance but ultimately if you're a",
    "start": "1241320",
    "end": "1247620"
  },
  {
    "text": "data scientist you can go and use this directly either through the SDK or CLI and do things like diving into logs or",
    "start": "1247620",
    "end": "1254580"
  },
  {
    "text": "peering logs without having to spin up a separate service to do it the way it",
    "start": "1254580",
    "end": "1266669"
  },
  {
    "text": "works is you specify your input which are the files that you want to do and perform the select on and today week",
    "start": "1266669",
    "end": "1273330"
  },
  {
    "text": "apparently support CSV TSV JSON and gzip and then issues sequel statements",
    "start": "1273330",
    "end": "1279269"
  },
  {
    "text": "consisting of select from and we're against those objects and then you get the output of the data that's within",
    "start": "1279269",
    "end": "1284789"
  },
  {
    "text": "that object that matches all of those conditions in the sequel statement you issued one feature of s3 select and",
    "start": "1284789",
    "end": "1292529"
  },
  {
    "text": "vaisya select is you can change the input and output format so you could start with an input format of CSV and",
    "start": "1292529",
    "end": "1298980"
  },
  {
    "text": "have the output format in JSON so think",
    "start": "1298980",
    "end": "1305159"
  },
  {
    "start": "1301000",
    "end": "1301000"
  },
  {
    "text": "about if you want to get an object into a pattern match today what you have to do is essentially get the object yet",
    "start": "1305159",
    "end": "1310980"
  },
  {
    "text": "write some sort of function which could run on lambda and you then have something external to that to then",
    "start": "1310980",
    "end": "1316620"
  },
  {
    "text": "perform the filtering and running more a select statements and getting the data you want from it all with s3 what you",
    "start": "1316620",
    "end": "1323639"
  },
  {
    "text": "essentially do is select the object give your statement and s3 is going to return the result without needing any external",
    "start": "1323639",
    "end": "1329460"
  },
  {
    "text": "filtering or external computation to do so",
    "start": "1329460",
    "end": "1334128"
  },
  {
    "start": "1333000",
    "end": "1333000"
  },
  {
    "text": "you can also apply this with AWS lamda which is our service compute platform",
    "start": "1335940",
    "end": "1341049"
  },
  {
    "text": "and start to do things like lamda triggers to go and do the s3 select operations against the objects in s3 and",
    "start": "1341049",
    "end": "1348280"
  },
  {
    "text": "return the output so now you can start really using this to integrate with some",
    "start": "1348280",
    "end": "1353950"
  },
  {
    "text": "of your existing applications today so",
    "start": "1353950",
    "end": "1361870"
  },
  {
    "start": "1360000",
    "end": "1360000"
  },
  {
    "text": "what you should see loading up once your screens shortly weather might just be a bit of a lag you'll start to see",
    "start": "1361870",
    "end": "1368980"
  },
  {
    "text": "something on to the screen which is an example of something one of our solution architects wrote that essentially",
    "start": "1368980",
    "end": "1374890"
  },
  {
    "text": "enables you to do service MapReduce if you look on the left this is code where",
    "start": "1374890",
    "end": "1380169"
  },
  {
    "text": "you could use our lambda platform to do service MapReduce and the actual data scanning as well but then on the right",
    "start": "1380169",
    "end": "1387640"
  },
  {
    "text": "by plugging into s3 select we essentially push that scan and data selection work down to s3 so you can see",
    "start": "1387640",
    "end": "1395380"
  },
  {
    "text": "the difference between the before and after s3 select made this capability it moved",
    "start": "1395380",
    "end": "1400510"
  },
  {
    "text": "it to me 2x faster at one fifth of the cost and now with all live presentations",
    "start": "1400510",
    "end": "1410380"
  },
  {
    "text": "I am going to try a quick demonstration by showing a video I have sort of",
    "start": "1410380",
    "end": "1416620"
  },
  {
    "text": "cheated on this one so apologies he's a recorded demos hopefully you can see the screen that's just popped up again this",
    "start": "1416620",
    "end": "1423460"
  },
  {
    "text": "is this is an example that's been written by one of our solution architects just to make sure I've got the right one here so essentially it's",
    "start": "1423460",
    "end": "1430720"
  },
  {
    "text": "doing a query against a CSV file which contains airport codes now this first",
    "start": "1430720",
    "end": "1437140"
  },
  {
    "text": "example that you're going to see is it's just a general listing with using",
    "start": "1437140",
    "end": "1444340"
  },
  {
    "text": "standard s3 and then s3 select so this was a live recorded demo and the results",
    "start": "1444340",
    "end": "1453880"
  },
  {
    "text": "I'm coming through which is always there we go so as you can see on this one",
    "start": "1453880",
    "end": "1460210"
  },
  {
    "text": "without s3 select it was 1.89 versus",
    "start": "1460210",
    "end": "1465250"
  },
  {
    "text": "with selector 0.32 which shows an 83% improvement but let's get a",
    "start": "1465250",
    "end": "1471159"
  },
  {
    "text": "little bit more granular in there using the exactly the same data set but now",
    "start": "1471159",
    "end": "1476320"
  },
  {
    "text": "we're being a little bit more specific we're searching for the LA s Airport code which you hope you can see in my",
    "start": "1476320",
    "end": "1482980"
  },
  {
    "text": "screen here so we're looking for la s Airport code in both and here the",
    "start": "1482980",
    "end": "1493090"
  },
  {
    "text": "difference is with s without history select it was 2.0 five seconds versus",
    "start": "1493090",
    "end": "1498940"
  },
  {
    "text": "zero point two seven with s three select which shows a 77 percent improvement now",
    "start": "1498940",
    "end": "1505480"
  },
  {
    "text": "this is a very basic example but it gives you the idea of how using s3",
    "start": "1505480",
    "end": "1511029"
  },
  {
    "text": "select directly through command-line interfaces you can start achieving some significant benefits so as I mentioned",
    "start": "1511029",
    "end": "1525039"
  },
  {
    "start": "1522000",
    "end": "1522000"
  },
  {
    "text": "earlier the s3 select has a presto connector so it works with your existing hive meta stores which is typically what",
    "start": "1525039",
    "end": "1531820"
  },
  {
    "text": "is used with EMR Hadoop and they can automatically convert the predicates into s3 select request and then pushes",
    "start": "1531820",
    "end": "1538539"
  },
  {
    "text": "it down so another example here once the",
    "start": "1538539",
    "end": "1544779"
  },
  {
    "start": "1540000",
    "end": "1540000"
  },
  {
    "text": "screen loads up to give it more time so this example of a presto connector",
    "start": "1544779",
    "end": "1550149"
  },
  {
    "text": "before and after scenario of running a specific query on the Left versus the s3",
    "start": "1550149",
    "end": "1555490"
  },
  {
    "text": "selects presto connector on the right now what is showing here is the the",
    "start": "1555490",
    "end": "1560679"
  },
  {
    "text": "before and after it was a 5x faster at 140th of the CPU so it's a big",
    "start": "1560679",
    "end": "1565899"
  },
  {
    "text": "game-changer in terms of accelerating your your big data requests and performance and also reducing your",
    "start": "1565899",
    "end": "1571720"
  },
  {
    "text": "overall processing requests generally from the number of queries and also the amount of computation required to run",
    "start": "1571720",
    "end": "1578919"
  },
  {
    "text": "those queries just pause there for a",
    "start": "1578919",
    "end": "1584230"
  },
  {
    "text": "second just to make sure this screen finishes loading up",
    "start": "1584230",
    "end": "1588929"
  },
  {
    "start": "1596000",
    "end": "1596000"
  },
  {
    "text": "so moving on to Glacia select now the latest selectors even more interesting",
    "start": "1602820",
    "end": "1607870"
  },
  {
    "text": "because glacier as I mentioned back in that tearing of your day to reduce costs is where the colder tiers of your data",
    "start": "1607870",
    "end": "1615309"
  },
  {
    "text": "is stored out all startup much lower cost now the need has soap so where we also",
    "start": "1615309",
    "end": "1623590"
  },
  {
    "text": "induced introduced s3 select we also introduced Glacia select where you can",
    "start": "1623590",
    "end": "1629620"
  },
  {
    "text": "use the same sequel query to do sequel statements against the data that's archived in glacier where glacier will",
    "start": "1629620",
    "end": "1636309"
  },
  {
    "text": "filter that data restore the output that matches those statements into an s3 location and notify you that the data is",
    "start": "1636309",
    "end": "1643419"
  },
  {
    "text": "ready to use so all the richness that we've just discussed about s3 select can",
    "start": "1643419",
    "end": "1649240"
  },
  {
    "text": "now apply to your archive data in cold archive as well so one to the next I",
    "start": "1649240",
    "end": "1660580"
  },
  {
    "start": "1657000",
    "end": "1657000"
  },
  {
    "text": "have how it works this is just a process workflow of just showing what I just described a minute ago we're using the",
    "start": "1660580",
    "end": "1668020"
  },
  {
    "text": "glacier - select command-line interface you basically select what part of the data you want from the glacier vault",
    "start": "1668020",
    "end": "1675720"
  },
  {
    "text": "today see what will then go and hydrate that data perform the query make the",
    "start": "1675720",
    "end": "1682120"
  },
  {
    "text": "data available an s3 location and then notify you using Amazon SNS that that",
    "start": "1682120",
    "end": "1688059"
  },
  {
    "text": "data is now ready to be used so without having to spin up any computation or",
    "start": "1688059",
    "end": "1693220"
  },
  {
    "text": "other analytic services or do any of the hydration of the day to yourself Amazon glare is a select will actually do this",
    "start": "1693220",
    "end": "1699340"
  },
  {
    "text": "for you",
    "start": "1699340",
    "end": "1701730"
  },
  {
    "text": "so that's uh I'm on the common ways you can reduce costs I'm going to switch",
    "start": "1707140",
    "end": "1712700"
  },
  {
    "text": "gears now and talk about how you can deliver results faster but ultimate if you're going to turn this into date business if you're going to turn this",
    "start": "1712700",
    "end": "1718280"
  },
  {
    "text": "data into business value the quick you can turn it into those outcomes the bigger the competitive odds that you're",
    "start": "1718280",
    "end": "1724070"
  },
  {
    "text": "going to have so let's put this all",
    "start": "1724070",
    "end": "1731600"
  },
  {
    "start": "1727000",
    "end": "1727000"
  },
  {
    "text": "together if you're looking to optimize performance when your data link there are three things you should look at the",
    "start": "1731600",
    "end": "1737990"
  },
  {
    "text": "first it is recommended that you aggregates small files into creating",
    "start": "1737990",
    "end": "1743000"
  },
  {
    "text": "large objects so that your requests costs go down you get faster listings",
    "start": "1743000",
    "end": "1748160"
  },
  {
    "text": "and you have less metadata to manage your goal should be to aggregate files to be more than a hundred and twenty",
    "start": "1748160",
    "end": "1754160"
  },
  {
    "text": "eight megabytes for best results and you can do so by using the Amazon EMR s3 disk CP group by function or if you have",
    "start": "1754160",
    "end": "1762380"
  },
  {
    "text": "a streaming or IOT type workload Kinesis firehose comes in very handy to be able to consolidate small files and putting",
    "start": "1762380",
    "end": "1769640"
  },
  {
    "text": "the files on s3 number two we just discussed s3 and also Glacia select",
    "start": "1769640",
    "end": "1777200"
  },
  {
    "text": "which is a capability within our object storage framework that enables big data",
    "start": "1777200",
    "end": "1782240"
  },
  {
    "text": "applications to run up to four hundred percent faster depending on the type of query and also much cheaper by",
    "start": "1782240",
    "end": "1788900"
  },
  {
    "text": "retrieving only amount of data that is required for a particular query instead of retrieving the whole object so",
    "start": "1788900",
    "end": "1795650"
  },
  {
    "text": "reduces the costs obviously from reducing the number of queries and the time as well because you're only getting",
    "start": "1795650",
    "end": "1800960"
  },
  {
    "text": "the data back that you need and third another way to achieve faster query",
    "start": "1800960",
    "end": "1806450"
  },
  {
    "text": "results is by formatting the data columnar format such as Parque and oh I",
    "start": "1806450",
    "end": "1811820"
  },
  {
    "text": "see are used to reduce costs thereby reducing cost and performance there is",
    "start": "1811820",
    "end": "1818180"
  },
  {
    "text": "another feature within earmark or DM RFS consistent view and if you enable this",
    "start": "1818180",
    "end": "1823820"
  },
  {
    "text": "feature you can be confident that all your files will be processed as intended when you run a change series of",
    "start": "1823820",
    "end": "1829250"
  },
  {
    "text": "MapReduce jobs the MRFs consistent view creates and uses metadata in an Amazon",
    "start": "1829250",
    "end": "1835100"
  },
  {
    "text": "DynamoDB table to maintain a consistent view of your three objects this table track certain",
    "start": "1835100",
    "end": "1840750"
  },
  {
    "text": "operations but does not hold any of your data the information in the table is used to confirm that the results",
    "start": "1840750",
    "end": "1847440"
  },
  {
    "text": "returned from an s3 list operation are as expected thereby allowing EMR FS to",
    "start": "1847440",
    "end": "1852900"
  },
  {
    "text": "check list consistency and read after write consistency just pause there just",
    "start": "1852900",
    "end": "1859890"
  },
  {
    "text": "make sure the keeping up-to-date in sync so Amazon Kinesis so Kinesis is",
    "start": "1859890",
    "end": "1878370"
  },
  {
    "start": "1874000",
    "end": "1874000"
  },
  {
    "text": "essentially a streaming tool which is focused on real time streaming workloads and it's all about collecting processing",
    "start": "1878370",
    "end": "1885630"
  },
  {
    "text": "and analyzing data streams in real time now with the addition of Kinesis video",
    "start": "1885630",
    "end": "1890820"
  },
  {
    "text": "streams on the left you can now easily process and analyze video streams and",
    "start": "1890820",
    "end": "1895950"
  },
  {
    "text": "data streams on AWS Kinesis video streams lets you use video audio and other time encoded data to build",
    "start": "1895950",
    "end": "1902910"
  },
  {
    "text": "applications at power robot smart cities manufacturing automation security",
    "start": "1902910",
    "end": "1908730"
  },
  {
    "text": "monitoring and machine learning Kinesis streams which are now called Kinesis",
    "start": "1908730",
    "end": "1914160"
  },
  {
    "text": "data streams enables you to build custom real-time applications using popular",
    "start": "1914160",
    "end": "1919230"
  },
  {
    "text": "streaming processing frameworks of your choice and then moving further to the right Kinesis firehose which we are now",
    "start": "1919230",
    "end": "1926070"
  },
  {
    "text": "calling Kinesis data firehose makes it easy to collect and load data into streams and",
    "start": "1926070",
    "end": "1932870"
  },
  {
    "text": "Kinesis analytics which we're now calling Kinesis data analytics makes it easy to process and analyze these data",
    "start": "1932990",
    "end": "1939450"
  },
  {
    "text": "streams in real-time with sequel another",
    "start": "1939450",
    "end": "1950820"
  },
  {
    "text": "key element to focus on when you think about delivering results quicker is",
    "start": "1950820",
    "end": "1955950"
  },
  {
    "text": "around how you can reduce the time it takes to prepare your data before we can actually run analytics against the data",
    "start": "1955950",
    "end": "1961950"
  },
  {
    "text": "a recent study by CrowdFlower who surveyed about 80 data scientists",
    "start": "1961950",
    "end": "1968490"
  },
  {
    "text": "about their jobs they found that data scientists spend approximately 60% of their time on",
    "start": "1968490",
    "end": "1974960"
  },
  {
    "text": "cleaning and organizing data collecting datasets comes in a second at 19 percent",
    "start": "1974960",
    "end": "1980690"
  },
  {
    "text": "of the time meaning data scientists spend around 80% of their time on preparing and managing data for analysis",
    "start": "1980690",
    "end": "1988330"
  },
  {
    "text": "they also found that data preparation was the least enjoyable parts of their work and I don't blame them through us",
    "start": "1988330",
    "end": "1994400"
  },
  {
    "text": "so the key question we have to ask ourselves is if we can make data preparation easier can we minimize the",
    "start": "1994400",
    "end": "2001960"
  },
  {
    "text": "time that people are collecting data sets and cleaning and organizing data I mean ideally freeing up 80% of their of",
    "start": "2001960",
    "end": "2010180"
  },
  {
    "text": "their existing workload so this is where",
    "start": "2010180",
    "end": "2020830"
  },
  {
    "start": "2018000",
    "end": "2018000"
  },
  {
    "text": "AWS glue comes in a jovial ooh is a fully managed extract transform and load",
    "start": "2020830",
    "end": "2026740"
  },
  {
    "text": "or ETL service that makes it easy for customers to prepare and load their data for analytics you can create and run and",
    "start": "2026740",
    "end": "2034540"
  },
  {
    "text": "eat your job with a few clicks in the AWS management console where you simply point AWS glue to your data stored on",
    "start": "2034540",
    "end": "2042580"
  },
  {
    "text": "AWS an AWS glue discovers your data and stores the associated metadata in the",
    "start": "2042580",
    "end": "2048610"
  },
  {
    "text": "AWS glue data catalog once this has been catalogues your data is immediately",
    "start": "2048610",
    "end": "2055030"
  },
  {
    "text": "searchable query writable and available for ETL a direct glue generates the code",
    "start": "2055030",
    "end": "2062440"
  },
  {
    "text": "executes your data transformations and data loading processes glue generates",
    "start": "2062440",
    "end": "2067870"
  },
  {
    "text": "Python code that is customizable reusable and portable so once your ETL",
    "start": "2067870",
    "end": "2073360"
  },
  {
    "text": "job is ready you can schedule it to run on AWS glues fully managed scale-out Apache spark environment one thing to",
    "start": "2073360",
    "end": "2081700"
  },
  {
    "text": "note here is AWS glue is completely service so there's no infrastructure to buy set up or management it's",
    "start": "2081700",
    "end": "2088720"
  },
  {
    "text": "automatically provisions the environment needed to complete the job at hand and newest customers pay for only the",
    "start": "2088720",
    "end": "2095350"
  },
  {
    "text": "compute resources consumed while running that ETL job and then with AWS group the",
    "start": "2095350",
    "end": "2101590"
  },
  {
    "text": "data can be made available for analytics in minutes depending on the size and good job you're running so some of the",
    "start": "2101590",
    "end": "2108670"
  },
  {
    "text": "key characteristics of aw-oooo the first is is easy aw-oooo automates",
    "start": "2108670",
    "end": "2114220"
  },
  {
    "text": "much of the effort in building maintaining and running ETL jobs AWS",
    "start": "2114220",
    "end": "2119380"
  },
  {
    "text": "glue cools your data sources identifies data formats and suggests schemas and",
    "start": "2119380",
    "end": "2124480"
  },
  {
    "text": "transformations glue also automatic generates the code to execute your data",
    "start": "2124480",
    "end": "2129970"
  },
  {
    "text": "transformations and loading processes the second a do is glue is integrated",
    "start": "2129970",
    "end": "2136600"
  },
  {
    "text": "across a wide range of AWS services such as Amazon Aurora or Amazon RDS Amazon",
    "start": "2136600",
    "end": "2145870"
  },
  {
    "text": "Group provides out-of-the-box integration with also Amazon Athena EMR regice spectrum and any Apache hive meta",
    "start": "2145870",
    "end": "2152920"
  },
  {
    "text": "store compatible application so pretty much many of the services we've discussed earlier in this",
    "start": "2152920",
    "end": "2158470"
  },
  {
    "text": "presentation the third is sort of repetition but it's service again there's no infrastructure",
    "start": "2158470",
    "end": "2164710"
  },
  {
    "text": "to provision or manage and then and then finally its developer friendly so AWS",
    "start": "2164710",
    "end": "2170860"
  },
  {
    "text": "pollute generates ETL code as we said that is customizable and reusable and portable but using familiar technology",
    "start": "2170860",
    "end": "2177490"
  },
  {
    "text": "Python and SPARC you can also import custom readers writers and transformations into your glue ETL code",
    "start": "2177490",
    "end": "2185340"
  },
  {
    "text": "since the code blue generates is based on open frameworks there is no",
    "start": "2185340",
    "end": "2191500"
  },
  {
    "text": "lock-in and you can use it anywhere now",
    "start": "2191500",
    "end": "2201120"
  },
  {
    "start": "2197000",
    "end": "2197000"
  },
  {
    "text": "as we start getting deeper into it and as we start observing more of our customer behaviors and how our customers",
    "start": "2201120",
    "end": "2207970"
  },
  {
    "text": "are using data it has been observed that you know people songs look into machine",
    "start": "2207970",
    "end": "2214240"
  },
  {
    "text": "learning a little bit more detail and what we have seen is it can be a very time-consuming process to build train",
    "start": "2214240",
    "end": "2220720"
  },
  {
    "text": "and deploy models and not only that it takes a lot of specialized expertise to",
    "start": "2220720",
    "end": "2226510"
  },
  {
    "text": "be able to do this this is where we've introduced Amazon sage maker that is a fully managed service that enables",
    "start": "2226510",
    "end": "2233020"
  },
  {
    "text": "developers and data scientists to quickly and easily build train and",
    "start": "2233020",
    "end": "2238090"
  },
  {
    "text": "deploy machining more machine learning models at any scale sage maker removes all of the barriers",
    "start": "2238090",
    "end": "2244940"
  },
  {
    "text": "that typically slow down developers who want to use machine learning techniques and capabilities so one of the key",
    "start": "2244940",
    "end": "2253100"
  },
  {
    "text": "things that we're doing with sage maker is to remove that complexity which is holding you back from using machine",
    "start": "2253100",
    "end": "2259610"
  },
  {
    "text": "learning techniques and we've identified there are four main features or areas to",
    "start": "2259610",
    "end": "2264920"
  },
  {
    "text": "focus on with sage maker and the first is the end to end machine learning",
    "start": "2264920",
    "end": "2269960"
  },
  {
    "text": "platform so amazon ironman offers a familiar integrated development of Frant",
    "start": "2269960",
    "end": "2274970"
  },
  {
    "text": "environment that you can start processing your training data set and developing your algorithms immediately",
    "start": "2274970",
    "end": "2281320"
  },
  {
    "text": "the second one is 0 set up amazon ironman provides hosted jupiter notebooks that require no setup so you",
    "start": "2281320",
    "end": "2288620"
  },
  {
    "text": "can begin processing and update your training data sets and developing your algorithms immediately the third one is",
    "start": "2288620",
    "end": "2296380"
  },
  {
    "text": "flexible model training so with native support for bringing your own algorithms",
    "start": "2296380",
    "end": "2301490"
  },
  {
    "text": "and framework models into ironman provides a lot of flexibility so at the",
    "start": "2301490",
    "end": "2306740"
  },
  {
    "text": "moment i'm and provides native apache MX net and tensorflow support and offers a",
    "start": "2306740",
    "end": "2312110"
  },
  {
    "text": "range of built-in high performance machine learning algorithms in addition to supporting popular open source out of",
    "start": "2312110",
    "end": "2318380"
  },
  {
    "text": "the rooms and then finally it's paid by the second with amazon ironman you pay",
    "start": "2318380",
    "end": "2325160"
  },
  {
    "text": "only for what you use authoring training and hosting is billed by the second with",
    "start": "2325160",
    "end": "2331280"
  },
  {
    "text": "no minimum fees and no upfront commitments so those are just some of",
    "start": "2331280",
    "end": "2342740"
  },
  {
    "text": "the common ways to improve performance of your data lake ultimately we're looking at optimizing the end-to-end",
    "start": "2342740",
    "end": "2349010"
  },
  {
    "text": "workflow how to build your data pipelines and get your data ready to",
    "start": "2349010",
    "end": "2354320"
  },
  {
    "text": "process in the quickest possible way but ultimately this is always going to be a journey so another key factor in",
    "start": "2354320",
    "end": "2361400"
  },
  {
    "text": "building the data lake in terms of reducing costs and improving performance and agility is to plan for the future",
    "start": "2361400",
    "end": "2369400"
  },
  {
    "text": "this is we're building a data Lake on a platform like Amazon s3 gives you a big advantage because you can evolve it by",
    "start": "2371569",
    "end": "2378739"
  },
  {
    "text": "utilizing a whole host of tools out there some of which we've just discussed these tools allow you to experiment with",
    "start": "2378739",
    "end": "2385880"
  },
  {
    "text": "the underlying datasets collected and increased agility in your analytics process to ultimately deliver insights",
    "start": "2385880",
    "end": "2392930"
  },
  {
    "text": "and answers in a far more cost-effective and performant way and this as we said",
    "start": "2392930",
    "end": "2403640"
  },
  {
    "text": "they last the last few minutes has been spent very much at a high level of how",
    "start": "2403640",
    "end": "2409369"
  },
  {
    "text": "to build your data project in AWS to reduce costs and to deliver results faster there is a wide range of training",
    "start": "2409369",
    "end": "2417739"
  },
  {
    "text": "online training available on our WWE AWS training website where you can start",
    "start": "2417739",
    "end": "2423739"
  },
  {
    "text": "looking at big data get more details on Amazon redshift spectrum Amazon s3 or",
    "start": "2423739",
    "end": "2430219"
  },
  {
    "text": "even Amazon s3 select s3 select with the relevant certifications as well and",
    "start": "2430219",
    "end": "2438430"
  },
  {
    "text": "finally I just wanted to say thank you for joining today I do hope this has",
    "start": "2438430",
    "end": "2443959"
  },
  {
    "text": "been helpful in letting you know a few ways that you can get better results more quickly and for less cost",
    "start": "2443959",
    "end": "2451719"
  }
]