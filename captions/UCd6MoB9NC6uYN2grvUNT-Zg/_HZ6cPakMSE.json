[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "hello everyone and welcome to this",
    "start": "89",
    "end": "2580"
  },
  {
    "text": "session where we're going to talk about",
    "start": "2580",
    "end": "3720"
  },
  {
    "text": "querying your data Lake",
    "start": "3720",
    "end": "5819"
  },
  {
    "text": "my name is Russell Nash and I'm a",
    "start": "5819",
    "end": "7140"
  },
  {
    "text": "Solutions Architect with AWS and in this",
    "start": "7140",
    "end": "9840"
  },
  {
    "text": "session we're going to talk about using",
    "start": "9840",
    "end": "11969"
  },
  {
    "text": "s3 as your data Lake and some of the",
    "start": "11969",
    "end": "14820"
  },
  {
    "text": "different tools and techniques you can",
    "start": "14820",
    "end": "16350"
  },
  {
    "text": "use to query that data so let's talk",
    "start": "16350",
    "end": "19320"
  },
  {
    "text": "first about using s3 as your storage",
    "start": "19320",
    "end": "22500"
  },
  {
    "text": "layer so s3 has some really key features",
    "start": "22500",
    "end": "25859"
  },
  {
    "start": "23000",
    "end": "84000"
  },
  {
    "text": "that make it perfect for use in a day",
    "start": "25859",
    "end": "27960"
  },
  {
    "text": "Lake first of all it's an object store",
    "start": "27960",
    "end": "30480"
  },
  {
    "text": "and this means that you can load up",
    "start": "30480",
    "end": "31890"
  },
  {
    "text": "pretty much any type of object that you",
    "start": "31890",
    "end": "34140"
  },
  {
    "text": "like into s3 secondly it's very low cost",
    "start": "34140",
    "end": "38190"
  },
  {
    "text": "and so this is important as your data",
    "start": "38190",
    "end": "39750"
  },
  {
    "text": "volumes grow you don't want the cost of",
    "start": "39750",
    "end": "42239"
  },
  {
    "text": "your storage layer to be blowing out",
    "start": "42239",
    "end": "43760"
  },
  {
    "text": "it's highly scalable so you don't need",
    "start": "43760",
    "end": "46559"
  },
  {
    "text": "to pre provision space within s3 you",
    "start": "46559",
    "end": "49170"
  },
  {
    "text": "simply load up data as you need it and",
    "start": "49170",
    "end": "51920"
  },
  {
    "text": "importantly we offer you 11 lines of",
    "start": "51920",
    "end": "54870"
  },
  {
    "text": "durability the data that goes into s3",
    "start": "54870",
    "end": "57149"
  },
  {
    "text": "and we're able to do that because every",
    "start": "57149",
    "end": "60030"
  },
  {
    "text": "object that you give us we actually copy",
    "start": "60030",
    "end": "62670"
  },
  {
    "text": "multiple times to multiple locations to",
    "start": "62670",
    "end": "65909"
  },
  {
    "text": "ensure that we can give you 11 lines of",
    "start": "65909",
    "end": "68520"
  },
  {
    "text": "durability so when you load up data into",
    "start": "68520",
    "end": "70380"
  },
  {
    "text": "s3 you don't then don't also need to",
    "start": "70380",
    "end": "72720"
  },
  {
    "text": "back it up it's it's you've got that",
    "start": "72720",
    "end": "74610"
  },
  {
    "text": "that that durability taken care of for",
    "start": "74610",
    "end": "77040"
  },
  {
    "text": "you",
    "start": "77040",
    "end": "77630"
  },
  {
    "text": "so let's look now at some of the",
    "start": "77630",
    "end": "79590"
  },
  {
    "text": "framework that we use to discuss modern",
    "start": "79590",
    "end": "82590"
  },
  {
    "text": "data architectures so if you look at",
    "start": "82590",
    "end": "85290"
  },
  {
    "start": "84000",
    "end": "418000"
  },
  {
    "text": "this framework you can see several",
    "start": "85290",
    "end": "87240"
  },
  {
    "text": "different boxes here and let me talk you",
    "start": "87240",
    "end": "88829"
  },
  {
    "text": "through them quickly so on the far left",
    "start": "88829",
    "end": "91200"
  },
  {
    "text": "we have the different sources that we're",
    "start": "91200",
    "end": "93960"
  },
  {
    "text": "going to load into our data Lake we're",
    "start": "93960",
    "end": "97079"
  },
  {
    "text": "then going to look at how you ingest",
    "start": "97079",
    "end": "98670"
  },
  {
    "text": "data into the lake depending on the",
    "start": "98670",
    "end": "100770"
  },
  {
    "text": "sources that you have then in that top",
    "start": "100770",
    "end": "103740"
  },
  {
    "text": "box we're going to look at the different",
    "start": "103740",
    "end": "105720"
  },
  {
    "text": "options for batch processing and how you",
    "start": "105720",
    "end": "108270"
  },
  {
    "text": "do that the box below that you can see",
    "start": "108270",
    "end": "110850"
  },
  {
    "text": "is the speed layer or the real-time",
    "start": "110850",
    "end": "113520"
  },
  {
    "text": "layer and then on the right we'll be",
    "start": "113520",
    "end": "115770"
  },
  {
    "text": "moving into the serving layer and the",
    "start": "115770",
    "end": "118140"
  },
  {
    "text": "serving layer is where we then make the",
    "start": "118140",
    "end": "120210"
  },
  {
    "text": "data available to the interested parties",
    "start": "120210",
    "end": "123270"
  },
  {
    "text": "on the far right hand side there so",
    "start": "123270",
    "end": "124649"
  },
  {
    "text": "you'll see your data analysts your data",
    "start": "124649",
    "end": "127110"
  },
  {
    "text": "scientists and some downstream",
    "start": "127110",
    "end": "129239"
  },
  {
    "text": "applications that want to",
    "start": "129239",
    "end": "131300"
  },
  {
    "text": "is the data into your lake and it's the",
    "start": "131300",
    "end": "134060"
  },
  {
    "text": "serving layer where will we be spending",
    "start": "134060",
    "end": "136040"
  },
  {
    "text": "most of our time in this session so let",
    "start": "136040",
    "end": "139160"
  },
  {
    "text": "me talk you through each of these boxes",
    "start": "139160",
    "end": "140600"
  },
  {
    "text": "in turn so in terms of the sources that",
    "start": "140600",
    "end": "143150"
  },
  {
    "text": "you have most customers have a variety",
    "start": "143150",
    "end": "145490"
  },
  {
    "text": "of different sources and I've just put a",
    "start": "145490",
    "end": "146990"
  },
  {
    "text": "couple onto this slide for you so you",
    "start": "146990",
    "end": "149180"
  },
  {
    "text": "can see you've got highly structured",
    "start": "149180",
    "end": "150740"
  },
  {
    "text": "data that might be living in databases",
    "start": "150740",
    "end": "152870"
  },
  {
    "text": "for example there might be flat files in",
    "start": "152870",
    "end": "155510"
  },
  {
    "text": "your organization you may have log files",
    "start": "155510",
    "end": "158120"
  },
  {
    "text": "for example that come from from hardware",
    "start": "158120",
    "end": "161150"
  },
  {
    "text": "from servers you might have device data",
    "start": "161150",
    "end": "163690"
  },
  {
    "text": "mobile data location data or clickstream",
    "start": "163690",
    "end": "166760"
  },
  {
    "text": "data for example so just an example of",
    "start": "166760",
    "end": "168860"
  },
  {
    "text": "some of the different types of data that",
    "start": "168860",
    "end": "171230"
  },
  {
    "text": "you could have in your organization that",
    "start": "171230",
    "end": "173120"
  },
  {
    "text": "you want to get into your day lake so",
    "start": "173120",
    "end": "176750"
  },
  {
    "text": "let's have a look now at some of the",
    "start": "176750",
    "end": "177920"
  },
  {
    "text": "ingestion options for some of those",
    "start": "177920",
    "end": "180410"
  },
  {
    "text": "different sources so what you use for",
    "start": "180410",
    "end": "183530"
  },
  {
    "text": "ingest will obviously depend on the",
    "start": "183530",
    "end": "185570"
  },
  {
    "text": "source of data that you have and so some",
    "start": "185570",
    "end": "187640"
  },
  {
    "text": "of the examples I have here are for",
    "start": "187640",
    "end": "189290"
  },
  {
    "text": "example ETL so that stands for extract",
    "start": "189290",
    "end": "192370"
  },
  {
    "text": "transform and load and this is a number",
    "start": "192370",
    "end": "196430"
  },
  {
    "text": "of different tools out there on the",
    "start": "196430",
    "end": "197989"
  },
  {
    "text": "market we have a number of fantastic",
    "start": "197989",
    "end": "199280"
  },
  {
    "text": "partners in this space and that can help",
    "start": "199280",
    "end": "201650"
  },
  {
    "text": "you to get data out of highly structured",
    "start": "201650",
    "end": "204130"
  },
  {
    "text": "databases for example and into your data",
    "start": "204130",
    "end": "207860"
  },
  {
    "text": "Lake you can see DMS there as well so",
    "start": "207860",
    "end": "210650"
  },
  {
    "text": "that's our database migration service",
    "start": "210650",
    "end": "212450"
  },
  {
    "text": "and that's our own ETL tool that you can",
    "start": "212450",
    "end": "214970"
  },
  {
    "text": "use for accessing certain sources of",
    "start": "214970",
    "end": "217220"
  },
  {
    "text": "databases again to get data into your",
    "start": "217220",
    "end": "219290"
  },
  {
    "text": "lake there's also a number of open",
    "start": "219290",
    "end": "221630"
  },
  {
    "text": "source tools that you that you can use",
    "start": "221630",
    "end": "223459"
  },
  {
    "text": "so we have things like fluent D log4j",
    "start": "223459",
    "end": "225920"
  },
  {
    "text": "and heaps of others that you can use to",
    "start": "225920",
    "end": "228290"
  },
  {
    "text": "get data into s3 if you happen to have",
    "start": "228290",
    "end": "231860"
  },
  {
    "text": "an enormous amount of data on premise",
    "start": "231860",
    "end": "233750"
  },
  {
    "text": "that is too much to push Daniel Internet",
    "start": "233750",
    "end": "236690"
  },
  {
    "text": "pipe then you can use something like AWS",
    "start": "236690",
    "end": "238940"
  },
  {
    "text": "snowball for example so this is a",
    "start": "238940",
    "end": "240890"
  },
  {
    "text": "physical device that we will ship to you",
    "start": "240890",
    "end": "243200"
  },
  {
    "text": "you can load your data onto it ship it",
    "start": "243200",
    "end": "245780"
  },
  {
    "text": "back to us and then we will load that",
    "start": "245780",
    "end": "247250"
  },
  {
    "text": "data up into s3 for you and that last",
    "start": "247250",
    "end": "250459"
  },
  {
    "text": "one down the bottom you see there is",
    "start": "250459",
    "end": "251810"
  },
  {
    "text": "Kinesis and this is the service that we",
    "start": "251810",
    "end": "253940"
  },
  {
    "text": "use to deal with very fast-moving data",
    "start": "253940",
    "end": "256190"
  },
  {
    "text": "streaming data if you will so let's move",
    "start": "256190",
    "end": "259579"
  },
  {
    "text": "on now to the batch layer and the",
    "start": "259580",
    "end": "262340"
  },
  {
    "text": "Batchelor you can see there typically",
    "start": "262340",
    "end": "264020"
  },
  {
    "text": "consists",
    "start": "264020",
    "end": "264800"
  },
  {
    "text": "of moving data into s3 and then",
    "start": "264800",
    "end": "267949"
  },
  {
    "text": "transforming that data with some kind of",
    "start": "267949",
    "end": "270169"
  },
  {
    "text": "processing engine",
    "start": "270169",
    "end": "271250"
  },
  {
    "text": "now most people like to use Hadoop in in",
    "start": "271250",
    "end": "274520"
  },
  {
    "text": "this particular place in the",
    "start": "274520",
    "end": "277009"
  },
  {
    "text": "architecture and so we'll talk about",
    "start": "277009",
    "end": "278810"
  },
  {
    "text": "Hadoop and we'll talk about EMR as well",
    "start": "278810",
    "end": "281210"
  },
  {
    "text": "as an option a little bit later on so",
    "start": "281210",
    "end": "283960"
  },
  {
    "text": "whatever tool you use for transformation",
    "start": "283960",
    "end": "286460"
  },
  {
    "text": "that data is then picked up from s3",
    "start": "286460",
    "end": "288440"
  },
  {
    "text": "transformed and then put back into s3",
    "start": "288440",
    "end": "291250"
  },
  {
    "text": "ready for the serving layer but what",
    "start": "291250",
    "end": "294530"
  },
  {
    "text": "about streaming data how do we deal with",
    "start": "294530",
    "end": "296240"
  },
  {
    "text": "that so if you look at the the box below",
    "start": "296240",
    "end": "298789"
  },
  {
    "text": "the batch layer you'll see the speed",
    "start": "298789",
    "end": "300379"
  },
  {
    "text": "layer there and in here data is",
    "start": "300379",
    "end": "302870"
  },
  {
    "text": "typically captured with Kinesis and it",
    "start": "302870",
    "end": "306740"
  },
  {
    "text": "is then you have the option to process",
    "start": "306740",
    "end": "308900"
  },
  {
    "text": "it however you like so you can see a",
    "start": "308900",
    "end": "310789"
  },
  {
    "text": "couple of options there you can either",
    "start": "310789",
    "end": "312590"
  },
  {
    "text": "create an application so that",
    "start": "312590",
    "end": "314719"
  },
  {
    "text": "application will run on ec2 machines now",
    "start": "314719",
    "end": "317960"
  },
  {
    "text": "and that gives you tremendous",
    "start": "317960",
    "end": "318819"
  },
  {
    "text": "flexibility with how you deal with the",
    "start": "318819",
    "end": "321259"
  },
  {
    "text": "data that's coming off the Kinesis",
    "start": "321259",
    "end": "323120"
  },
  {
    "text": "stream now a lot of customers are",
    "start": "323120",
    "end": "325370"
  },
  {
    "text": "obviously moving to service",
    "start": "325370",
    "end": "326569"
  },
  {
    "text": "architectures and so you'll find that in",
    "start": "326569",
    "end": "328669"
  },
  {
    "text": "fact instead of using an application on",
    "start": "328669",
    "end": "330830"
  },
  {
    "text": "ec2 they prefer to use lambda for",
    "start": "330830",
    "end": "333169"
  },
  {
    "text": "example and lambda is a service where",
    "start": "333169",
    "end": "335090"
  },
  {
    "text": "you simply give us your code and we will",
    "start": "335090",
    "end": "337610"
  },
  {
    "text": "run that code for you there's no service",
    "start": "337610",
    "end": "339500"
  },
  {
    "text": "to manage there so you can run that code",
    "start": "339500",
    "end": "341960"
  },
  {
    "text": "in response to an event that happens for",
    "start": "341960",
    "end": "343880"
  },
  {
    "text": "example or you can actually point it to",
    "start": "343880",
    "end": "346069"
  },
  {
    "text": "a Kinesis stream and the lambda function",
    "start": "346069",
    "end": "348259"
  },
  {
    "text": "will pull the stream waiting for records",
    "start": "348259",
    "end": "350210"
  },
  {
    "text": "and then it will it will kick off your",
    "start": "350210",
    "end": "351770"
  },
  {
    "text": "code once that record is available or",
    "start": "351770",
    "end": "355370"
  },
  {
    "text": "you may want to use open source tools as",
    "start": "355370",
    "end": "357199"
  },
  {
    "text": "well so there's tools like apache spark",
    "start": "357199",
    "end": "359210"
  },
  {
    "text": "and flink for example also very popular",
    "start": "359210",
    "end": "361909"
  },
  {
    "text": "as stream processing engines taking data",
    "start": "361909",
    "end": "365240"
  },
  {
    "text": "off Kinesis",
    "start": "365240",
    "end": "366849"
  },
  {
    "text": "now the outputs for this real-time data",
    "start": "366849",
    "end": "370430"
  },
  {
    "text": "is obviously going to be things like",
    "start": "370430",
    "end": "372289"
  },
  {
    "text": "real-time dashboards so you may want to",
    "start": "372289",
    "end": "373909"
  },
  {
    "text": "load things into elasticsearch for",
    "start": "373909",
    "end": "375889"
  },
  {
    "text": "example but also you might want to do",
    "start": "375889",
    "end": "377960"
  },
  {
    "text": "predictive analytics and you want to do",
    "start": "377960",
    "end": "379460"
  },
  {
    "text": "this very very quickly which is why you",
    "start": "379460",
    "end": "381500"
  },
  {
    "text": "want to point this at the near real-time",
    "start": "381500",
    "end": "382729"
  },
  {
    "text": "stream for our purposes though we want",
    "start": "382729",
    "end": "385669"
  },
  {
    "text": "to take the the real-time data and we",
    "start": "385669",
    "end": "387680"
  },
  {
    "text": "want to move it into s3 because we want",
    "start": "387680",
    "end": "389719"
  },
  {
    "text": "to marry that speed layer with the batch",
    "start": "389719",
    "end": "392419"
  },
  {
    "text": "layer so that we can then that make that",
    "start": "392419",
    "end": "394339"
  },
  {
    "text": "available to the serving layer so let's",
    "start": "394339",
    "end": "396830"
  },
  {
    "text": "move on to the serving layer so",
    "start": "396830",
    "end": "398540"
  },
  {
    "text": "there's lots and lots of different",
    "start": "398540",
    "end": "399950"
  },
  {
    "text": "options that you have for querying data",
    "start": "399950",
    "end": "401990"
  },
  {
    "text": "in the serving layer but I want to cut",
    "start": "401990",
    "end": "404210"
  },
  {
    "text": "touch on three main ones today with you",
    "start": "404210",
    "end": "406970"
  },
  {
    "text": "in this session so we're going to have a",
    "start": "406970",
    "end": "408620"
  },
  {
    "text": "look at Amazon EMR we're going to look",
    "start": "408620",
    "end": "411050"
  },
  {
    "text": "at Amazon Athena and then we're going to",
    "start": "411050",
    "end": "412970"
  },
  {
    "text": "touch on Amazon redshift as well so",
    "start": "412970",
    "end": "415910"
  },
  {
    "text": "let's look first now at Amazon Amr so",
    "start": "415910",
    "end": "419420"
  },
  {
    "start": "418000",
    "end": "461000"
  },
  {
    "text": "EMR is our managed Hadoop service and so",
    "start": "419420",
    "end": "422690"
  },
  {
    "text": "this means that we actually manage the",
    "start": "422690",
    "end": "424640"
  },
  {
    "text": "Hadoop stack for you it is highly",
    "start": "424640",
    "end": "427910"
  },
  {
    "text": "optimized with s3 and we'll talk about",
    "start": "427910",
    "end": "429860"
  },
  {
    "text": "how that works and why it's important a",
    "start": "429860",
    "end": "431510"
  },
  {
    "text": "little bit later on and it's also got",
    "start": "431510",
    "end": "433880"
  },
  {
    "text": "very strong open source support as well",
    "start": "433880",
    "end": "436100"
  },
  {
    "text": "and this is really important to you",
    "start": "436100",
    "end": "438260"
  },
  {
    "text": "because if you're using open source",
    "start": "438260",
    "end": "440150"
  },
  {
    "text": "tools you want to be using the most",
    "start": "440150",
    "end": "442190"
  },
  {
    "text": "up-to-date versions of those tools so",
    "start": "442190",
    "end": "444110"
  },
  {
    "text": "when a new version of spark comes out",
    "start": "444110",
    "end": "445850"
  },
  {
    "text": "for example we work very hard to get",
    "start": "445850",
    "end": "447980"
  },
  {
    "text": "that into EMR as soon as we possibly can",
    "start": "447980",
    "end": "450410"
  },
  {
    "text": "to make sure that you're using the",
    "start": "450410",
    "end": "452000"
  },
  {
    "text": "versions of spark that you want to use",
    "start": "452000",
    "end": "453950"
  },
  {
    "text": "so EMR very very strong with open source",
    "start": "453950",
    "end": "457100"
  },
  {
    "text": "support so let me show you this in a",
    "start": "457100",
    "end": "459380"
  },
  {
    "text": "little bit more detail so here we have a",
    "start": "459380",
    "end": "462800"
  },
  {
    "text": "typical Hadoop stack and you can see",
    "start": "462800",
    "end": "465470"
  },
  {
    "text": "down the bottom there starting from the",
    "start": "465470",
    "end": "466970"
  },
  {
    "text": "bottom up we have the infrastructure",
    "start": "466970",
    "end": "468890"
  },
  {
    "text": "layer so if this was on Amazon Cloud for",
    "start": "468890",
    "end": "472610"
  },
  {
    "text": "example these machines would be easy to",
    "start": "472610",
    "end": "474620"
  },
  {
    "text": "machines you then have a data layer so",
    "start": "474620",
    "end": "477500"
  },
  {
    "text": "typically that would be HDFS and then",
    "start": "477500",
    "end": "480590"
  },
  {
    "text": "you have the process layer so this might",
    "start": "480590",
    "end": "482240"
  },
  {
    "text": "be MapReduce it might be tears it might",
    "start": "482240",
    "end": "484520"
  },
  {
    "text": "be running spark or potentially a",
    "start": "484520",
    "end": "485900"
  },
  {
    "text": "combination of all two the two or three",
    "start": "485900",
    "end": "487780"
  },
  {
    "text": "then the framework is Hadoop and then of",
    "start": "487780",
    "end": "490520"
  },
  {
    "text": "course on top of that you've got the",
    "start": "490520",
    "end": "492200"
  },
  {
    "text": "applications that you want to run so",
    "start": "492200",
    "end": "494630"
  },
  {
    "text": "typically most of our customers will use",
    "start": "494630",
    "end": "496670"
  },
  {
    "text": "those applications at the top layer",
    "start": "496670",
    "end": "498920"
  },
  {
    "text": "because they're much easier to use than",
    "start": "498920",
    "end": "500900"
  },
  {
    "text": "writing down at the process layer for",
    "start": "500900",
    "end": "502850"
  },
  {
    "text": "example and those applications as many",
    "start": "502850",
    "end": "506330"
  },
  {
    "text": "of you would know there's a very rich",
    "start": "506330",
    "end": "507770"
  },
  {
    "text": "ecosystem of applications out there",
    "start": "507770",
    "end": "509600"
  },
  {
    "text": "whether it gives you sequel like layers",
    "start": "509600",
    "end": "511970"
  },
  {
    "text": "on top of Hadoop or no sequel layers or",
    "start": "511970",
    "end": "515780"
  },
  {
    "text": "machine learning etc or transformational",
    "start": "515780",
    "end": "518870"
  },
  {
    "text": "layers those applications very very rich",
    "start": "518870",
    "end": "521539"
  },
  {
    "text": "supporting lots and lots of different",
    "start": "521540",
    "end": "523130"
  },
  {
    "text": "workloads so where does EMR fit into",
    "start": "523130",
    "end": "526850"
  },
  {
    "text": "this picture if you look on the",
    "start": "526850",
    "end": "528620"
  },
  {
    "text": "left-hand side there you can see that's",
    "start": "528620",
    "end": "529970"
  },
  {
    "text": "where really EMR sits",
    "start": "529970",
    "end": "531810"
  },
  {
    "text": "what an mr does is it manages this stack",
    "start": "531810",
    "end": "534570"
  },
  {
    "text": "for you",
    "start": "534570",
    "end": "535350"
  },
  {
    "text": "so this Hadoop stack you could spin it",
    "start": "535350",
    "end": "537660"
  },
  {
    "text": "up yourself so you could spin up ec2",
    "start": "537660",
    "end": "539430"
  },
  {
    "text": "machines you could install Hadoop you",
    "start": "539430",
    "end": "541950"
  },
  {
    "text": "could configure it",
    "start": "541950",
    "end": "542970"
  },
  {
    "text": "you could then install the applications",
    "start": "542970",
    "end": "544680"
  },
  {
    "text": "make sure they we're all working",
    "start": "544680",
    "end": "545820"
  },
  {
    "text": "together but there's a lot of heavy",
    "start": "545820",
    "end": "547620"
  },
  {
    "text": "lifting involved in doing that so what",
    "start": "547620",
    "end": "550140"
  },
  {
    "text": "iam R does is it manages the stack for",
    "start": "550140",
    "end": "552630"
  },
  {
    "text": "you so you talk to the EMR console or",
    "start": "552630",
    "end": "555630"
  },
  {
    "text": "through the API or the CLI and you tell",
    "start": "555630",
    "end": "558150"
  },
  {
    "text": "a llamar that you want a cluster of a",
    "start": "558150",
    "end": "559800"
  },
  {
    "text": "certain size certain applications you",
    "start": "559800",
    "end": "562080"
  },
  {
    "text": "want installed and you let EMR do the",
    "start": "562080",
    "end": "564480"
  },
  {
    "text": "heavy lifting of bringing up the cluster",
    "start": "564480",
    "end": "566520"
  },
  {
    "text": "managing the cluster and then you simply",
    "start": "566520",
    "end": "569580"
  },
  {
    "text": "get to use the applications so the end",
    "start": "569580",
    "end": "572490"
  },
  {
    "text": "the end result is you're still using the",
    "start": "572490",
    "end": "574110"
  },
  {
    "text": "the open source applications as you",
    "start": "574110",
    "end": "576150"
  },
  {
    "text": "would normally but you've spent a lot",
    "start": "576150",
    "end": "578100"
  },
  {
    "text": "less time and energy actually getting",
    "start": "578100",
    "end": "580200"
  },
  {
    "text": "the cluster up to that point",
    "start": "580200",
    "end": "581370"
  },
  {
    "text": "EMR also brings another really strong",
    "start": "581370",
    "end": "584160"
  },
  {
    "text": "benefit with it as well and that is that",
    "start": "584160",
    "end": "588060"
  },
  {
    "text": "it allows you to use s3 as a data source",
    "start": "588060",
    "end": "591200"
  },
  {
    "text": "now what we've done there you can see on",
    "start": "591200",
    "end": "593580"
  },
  {
    "text": "this slide we have something called EMR",
    "start": "593580",
    "end": "595740"
  },
  {
    "text": "FS so emails file system and this allows",
    "start": "595740",
    "end": "598980"
  },
  {
    "text": "EMR to work with s3 in a highly",
    "start": "598980",
    "end": "601860"
  },
  {
    "text": "optimized fashion and what this then",
    "start": "601860",
    "end": "604470"
  },
  {
    "text": "does is opens up all those applications",
    "start": "604470",
    "end": "606930"
  },
  {
    "text": "to then start using s3 and so you can",
    "start": "606930",
    "end": "610170"
  },
  {
    "text": "imagine in our data Lake scenario where",
    "start": "610170",
    "end": "611790"
  },
  {
    "text": "our data is in s3 that's extremely",
    "start": "611790",
    "end": "613800"
  },
  {
    "text": "important so let's look a little bit",
    "start": "613800",
    "end": "616980"
  },
  {
    "text": "more closely at how some of these",
    "start": "616980",
    "end": "618480"
  },
  {
    "text": "applications actually query data from s3",
    "start": "618480",
    "end": "622040"
  },
  {
    "start": "622000",
    "end": "835000"
  },
  {
    "text": "now hive was the first sequel I player",
    "start": "622040",
    "end": "625920"
  },
  {
    "text": "on Hadoop so we're going to start with",
    "start": "625920",
    "end": "627390"
  },
  {
    "text": "that and let's say you have data on s3",
    "start": "627390",
    "end": "630960"
  },
  {
    "text": "and you also have users who want to",
    "start": "630960",
    "end": "633660"
  },
  {
    "text": "access that data so you spin up a",
    "start": "633660",
    "end": "636180"
  },
  {
    "text": "cluster with hive on it and the first",
    "start": "636180",
    "end": "638400"
  },
  {
    "text": "thing that needs to happen is you need",
    "start": "638400",
    "end": "640260"
  },
  {
    "text": "to create an object that points to the",
    "start": "640260",
    "end": "642870"
  },
  {
    "text": "data so that you can query it so you run",
    "start": "642870",
    "end": "645870"
  },
  {
    "text": "a create table statement as you can see",
    "start": "645870",
    "end": "647640"
  },
  {
    "text": "here and what that does is it creates a",
    "start": "647640",
    "end": "650040"
  },
  {
    "text": "table in the catalog and this points to",
    "start": "650040",
    "end": "654060"
  },
  {
    "text": "the data on s3 now at this point no data",
    "start": "654060",
    "end": "657930"
  },
  {
    "text": "is actually moving we're simply creating",
    "start": "657930",
    "end": "660360"
  },
  {
    "text": "an object that is then pointing to the",
    "start": "660360",
    "end": "662640"
  },
  {
    "text": "data but what we now have",
    "start": "662640",
    "end": "664910"
  },
  {
    "text": "is an object that looks like a sequel",
    "start": "664910",
    "end": "666830"
  },
  {
    "text": "table that allows us to use SQL to query",
    "start": "666830",
    "end": "670580"
  },
  {
    "text": "the data that's sitting in s3",
    "start": "670580",
    "end": "672110"
  },
  {
    "text": "so now you can see the users can run SQL",
    "start": "672110",
    "end": "676010"
  },
  {
    "text": "statements and hive will convert and",
    "start": "676010",
    "end": "680120"
  },
  {
    "text": "extract the data from s3 before",
    "start": "680120",
    "end": "682460"
  },
  {
    "text": "returning the results to the user so a",
    "start": "682460",
    "end": "685460"
  },
  {
    "text": "very powerful way of letting people",
    "start": "685460",
    "end": "687380"
  },
  {
    "text": "familiar with SQL get access to data on",
    "start": "687380",
    "end": "689600"
  },
  {
    "text": "s3 now what happened over time was that",
    "start": "689600",
    "end": "692990"
  },
  {
    "text": "hive was found to be a little bit slower",
    "start": "692990",
    "end": "696170"
  },
  {
    "text": "than than it then people wanted it to be",
    "start": "696170",
    "end": "698390"
  },
  {
    "text": "it wasn't quite as interactive and so",
    "start": "698390",
    "end": "700700"
  },
  {
    "text": "other tools sprang up that were still",
    "start": "700700",
    "end": "703640"
  },
  {
    "text": "gave people sequel access but in a much",
    "start": "703640",
    "end": "706010"
  },
  {
    "text": "faster fashion a much more interactive",
    "start": "706010",
    "end": "707650"
  },
  {
    "text": "type of speed than hive was was doing",
    "start": "707650",
    "end": "710990"
  },
  {
    "text": "however it made sense that these tools",
    "start": "710990",
    "end": "714140"
  },
  {
    "text": "use the same catalog that hive was using",
    "start": "714140",
    "end": "717460"
  },
  {
    "text": "and so you can see here we've got spike",
    "start": "717460",
    "end": "720110"
  },
  {
    "text": "sequel and presto and Pig for example",
    "start": "720110",
    "end": "722510"
  },
  {
    "text": "and the idea is that once you create the",
    "start": "722510",
    "end": "724760"
  },
  {
    "text": "table in say hi or presto you can then",
    "start": "724760",
    "end": "728030"
  },
  {
    "text": "access it from multiple engines and",
    "start": "728030",
    "end": "730700"
  },
  {
    "text": "that's really important that we share",
    "start": "730700",
    "end": "732170"
  },
  {
    "text": "that catalog a different amongst the",
    "start": "732170",
    "end": "734090"
  },
  {
    "text": "different engines that we have okay so",
    "start": "734090",
    "end": "736130"
  },
  {
    "text": "let's move on now to Amazon Athena now",
    "start": "736130",
    "end": "740120"
  },
  {
    "text": "you've heard me talk a little bit about",
    "start": "740120",
    "end": "741200"
  },
  {
    "text": "presto and presto was a tool that was",
    "start": "741200",
    "end": "744440"
  },
  {
    "text": "designed really to also give sequel",
    "start": "744440",
    "end": "746720"
  },
  {
    "text": "access to s3 and HDFS but to be much",
    "start": "746720",
    "end": "749960"
  },
  {
    "text": "much faster than hive had been to really",
    "start": "749960",
    "end": "752510"
  },
  {
    "text": "give people an interactive type of",
    "start": "752510",
    "end": "754220"
  },
  {
    "text": "experience so presto is popular because",
    "start": "754220",
    "end": "756980"
  },
  {
    "text": "it's fast and it's also easy to use and",
    "start": "756980",
    "end": "759260"
  },
  {
    "text": "what we found was a lot of our customers",
    "start": "759260",
    "end": "761600"
  },
  {
    "text": "were spinning up EMR and Hadoop clusters",
    "start": "761600",
    "end": "764900"
  },
  {
    "text": "in order to use presto to access the s3",
    "start": "764900",
    "end": "767690"
  },
  {
    "text": "data now whenever we see customers doing",
    "start": "767690",
    "end": "770870"
  },
  {
    "text": "a lot of heavy lifting we like to try",
    "start": "770870",
    "end": "773810"
  },
  {
    "text": "and ease their burden by introducing a",
    "start": "773810",
    "end": "775580"
  },
  {
    "text": "managed service so this is what we did",
    "start": "775580",
    "end": "777770"
  },
  {
    "text": "in this case so we took presto and we",
    "start": "777770",
    "end": "780890"
  },
  {
    "text": "turn that into a managed service so what",
    "start": "780890",
    "end": "782840"
  },
  {
    "text": "Athena allows you to do is to query data",
    "start": "782840",
    "end": "785390"
  },
  {
    "text": "on s3 users presto underneath for covers",
    "start": "785390",
    "end": "788810"
  },
  {
    "text": "but it's a completely serverless",
    "start": "788810",
    "end": "790940"
  },
  {
    "text": "type of service so you're not having to",
    "start": "790940",
    "end": "793580"
  },
  {
    "text": "spin up anything you're not having to",
    "start": "793580",
    "end": "795230"
  },
  {
    "text": "manage your cluster and obvious",
    "start": "795230",
    "end": "796910"
  },
  {
    "text": "you're not patching any clusters so the",
    "start": "796910",
    "end": "799370"
  },
  {
    "text": "way this works in Athena is similar to",
    "start": "799370",
    "end": "801260"
  },
  {
    "text": "the way that works in hive so first you",
    "start": "801260",
    "end": "803090"
  },
  {
    "text": "create the table as you would in hive",
    "start": "803090",
    "end": "804650"
  },
  {
    "text": "and this sets up the object in the",
    "start": "804650",
    "end": "806690"
  },
  {
    "text": "catalog that points to the data so users",
    "start": "806690",
    "end": "810260"
  },
  {
    "text": "can then query that table through SQL",
    "start": "810260",
    "end": "813760"
  },
  {
    "text": "now what happens when the query finishes",
    "start": "813760",
    "end": "817660"
  },
  {
    "text": "well the Athena cluster doesn't persist",
    "start": "817660",
    "end": "821480"
  },
  {
    "text": "that the catalog does and this is",
    "start": "821480",
    "end": "824630"
  },
  {
    "text": "important so the server is no longer in",
    "start": "824630",
    "end": "826790"
  },
  {
    "text": "the environment but the catalog where",
    "start": "826790",
    "end": "828650"
  },
  {
    "text": "you've created all of those tables still",
    "start": "828650",
    "end": "830780"
  },
  {
    "text": "persists in your environment so next",
    "start": "830780",
    "end": "832430"
  },
  {
    "text": "time you go to run a query",
    "start": "832430",
    "end": "833570"
  },
  {
    "text": "those tables are going to be there now",
    "start": "833570",
    "end": "836690"
  },
  {
    "start": "835000",
    "end": "995000"
  },
  {
    "text": "let's move on to redshift so rather than",
    "start": "836690",
    "end": "839900"
  },
  {
    "text": "being based on Hadoop redshift is",
    "start": "839900",
    "end": "842090"
  },
  {
    "text": "actually a fully fledged database it's",
    "start": "842090",
    "end": "844550"
  },
  {
    "text": "an MPP database and MPP stands for",
    "start": "844550",
    "end": "847370"
  },
  {
    "text": "massively parallel processing and this",
    "start": "847370",
    "end": "849710"
  },
  {
    "text": "is a breed of database databases that",
    "start": "849710",
    "end": "852160"
  },
  {
    "text": "they scale horizontally and that allows",
    "start": "852160",
    "end": "855050"
  },
  {
    "text": "you to load up very large amounts of",
    "start": "855050",
    "end": "856730"
  },
  {
    "text": "data they're optimized for analytics and",
    "start": "856730",
    "end": "858890"
  },
  {
    "text": "this allows you to move from very small",
    "start": "858890",
    "end": "861200"
  },
  {
    "text": "clusters from gigabytes all the way",
    "start": "861200",
    "end": "863420"
  },
  {
    "text": "through to exabytes if you need to it's",
    "start": "863420",
    "end": "866420"
  },
  {
    "text": "also fully relational and this means",
    "start": "866420",
    "end": "868370"
  },
  {
    "text": "that you create tables you create",
    "start": "868370",
    "end": "870440"
  },
  {
    "text": "relationships between those tables and",
    "start": "870440",
    "end": "872330"
  },
  {
    "text": "you can move your data model into",
    "start": "872330",
    "end": "873950"
  },
  {
    "text": "redshift and get started very easily so",
    "start": "873950",
    "end": "877250"
  },
  {
    "text": "let's look at how you get data in and",
    "start": "877250",
    "end": "878870"
  },
  {
    "text": "run queries within redshift so let's say",
    "start": "878870",
    "end": "882170"
  },
  {
    "text": "we have some some user's data in a file",
    "start": "882170",
    "end": "884930"
  },
  {
    "text": "in s3 and normally with redshift what",
    "start": "884930",
    "end": "887870"
  },
  {
    "text": "we'll do is we'll create the table that",
    "start": "887870",
    "end": "891140"
  },
  {
    "text": "you can see there the create table",
    "start": "891140",
    "end": "892430"
  },
  {
    "text": "statement and then we will load the data",
    "start": "892430",
    "end": "894170"
  },
  {
    "text": "room from s3 using the copy command and",
    "start": "894170",
    "end": "897340"
  },
  {
    "text": "this will actually then load that data",
    "start": "897340",
    "end": "900260"
  },
  {
    "text": "into rigid so the table can now be",
    "start": "900260",
    "end": "903830"
  },
  {
    "text": "queried and the data will come from the",
    "start": "903830",
    "end": "906260"
  },
  {
    "text": "local table in rigid so we no longer",
    "start": "906260",
    "end": "908960"
  },
  {
    "text": "need the s3 data because we have a full",
    "start": "908960",
    "end": "911780"
  },
  {
    "text": "copy of that within redshift but what",
    "start": "911780",
    "end": "915440"
  },
  {
    "text": "happens if we want to use redshift a",
    "start": "915440",
    "end": "917930"
  },
  {
    "text": "query data that sits on s3 but we don't",
    "start": "917930",
    "end": "920930"
  },
  {
    "text": "want to have to load it into us into",
    "start": "920930",
    "end": "922550"
  },
  {
    "text": "redshift first well this is where",
    "start": "922550",
    "end": "924650"
  },
  {
    "text": "redshift spectrum comes in so spectrum",
    "start": "924650",
    "end": "927440"
  },
  {
    "text": "is a new feature we've added to red",
    "start": "927440",
    "end": "929300"
  },
  {
    "text": "shift to a laid",
    "start": "929300",
    "end": "930740"
  },
  {
    "text": "we external data that sits in s3 so",
    "start": "930740",
    "end": "934760"
  },
  {
    "text": "let's say we have some orders data that",
    "start": "934760",
    "end": "937430"
  },
  {
    "text": "sits on s3 and let's say this is a very",
    "start": "937430",
    "end": "939410"
  },
  {
    "text": "large file because we're getting lots",
    "start": "939410",
    "end": "940880"
  },
  {
    "text": "and lots of orders so we want to be at a",
    "start": "940880",
    "end": "943010"
  },
  {
    "text": "query that but we don't really",
    "start": "943010",
    "end": "944180"
  },
  {
    "text": "necessarily want to load that into",
    "start": "944180",
    "end": "945950"
  },
  {
    "text": "Richard so what we do is we first create",
    "start": "945950",
    "end": "949490"
  },
  {
    "text": "an external table that points to s3 and",
    "start": "949490",
    "end": "952880"
  },
  {
    "text": "then we'll be able to query that data",
    "start": "952880",
    "end": "955339"
  },
  {
    "text": "without loading it the real power of",
    "start": "955339",
    "end": "959149"
  },
  {
    "text": "this though is when we query from both",
    "start": "959149",
    "end": "961490"
  },
  {
    "text": "local and s3 tables and join them",
    "start": "961490",
    "end": "964220"
  },
  {
    "text": "together so here you can see that we're",
    "start": "964220",
    "end": "967580"
  },
  {
    "text": "joining our local user data that's",
    "start": "967580",
    "end": "970760"
  },
  {
    "text": "sitting in redshift with our big orders",
    "start": "970760",
    "end": "973760"
  },
  {
    "text": "file that's sitting in s3 so we get the",
    "start": "973760",
    "end": "975709"
  },
  {
    "text": "best of both worlds so redshift spectrum",
    "start": "975709",
    "end": "978170"
  },
  {
    "text": "is enormous ly helpful when you've got",
    "start": "978170",
    "end": "981020"
  },
  {
    "text": "large data sets in in your data Lake in",
    "start": "981020",
    "end": "983420"
  },
  {
    "text": "s3 but you also have data within",
    "start": "983420",
    "end": "985580"
  },
  {
    "text": "redshift itself and you want to join",
    "start": "985580",
    "end": "987500"
  },
  {
    "text": "across the two so we've talked about a",
    "start": "987500",
    "end": "990560"
  },
  {
    "text": "couple of different options let now",
    "start": "990560",
    "end": "992690"
  },
  {
    "text": "let's compare the options that we've",
    "start": "992690",
    "end": "994730"
  },
  {
    "text": "looked at so here I want to look at four",
    "start": "994730",
    "end": "996829"
  },
  {
    "start": "995000",
    "end": "1166000"
  },
  {
    "text": "different attributes across the",
    "start": "996829",
    "end": "998480"
  },
  {
    "text": "different tools so I've grouped the open",
    "start": "998480",
    "end": "1000490"
  },
  {
    "text": "source engines together since they run",
    "start": "1000490",
    "end": "1003250"
  },
  {
    "text": "on Hadoop so that sparks equal hive and",
    "start": "1003250",
    "end": "1005680"
  },
  {
    "text": "presto and I've put a box around",
    "start": "1005680",
    "end": "1007810"
  },
  {
    "text": "redshift and spectrum because you can't",
    "start": "1007810",
    "end": "1009970"
  },
  {
    "text": "run spectrum stand alone you need to",
    "start": "1009970",
    "end": "1012220"
  },
  {
    "text": "have that redshift cluster running as",
    "start": "1012220",
    "end": "1014529"
  },
  {
    "text": "well so the first thing I wanted to look",
    "start": "1014529",
    "end": "1017020"
  },
  {
    "text": "at was this notion of schema so a schema",
    "start": "1017020",
    "end": "1020470"
  },
  {
    "text": "is when we're actually doing the create",
    "start": "1020470",
    "end": "1022149"
  },
  {
    "text": "table statement and we're creating that",
    "start": "1022149",
    "end": "1023980"
  },
  {
    "text": "association between the columns the data",
    "start": "1023980",
    "end": "1026678"
  },
  {
    "text": "types and the data itself and so with",
    "start": "1026679",
    "end": "1030550"
  },
  {
    "text": "the the first few options there we have",
    "start": "1030550",
    "end": "1033188"
  },
  {
    "text": "a notion of schema on breed and what",
    "start": "1033189",
    "end": "1035800"
  },
  {
    "text": "that means is that at the time that we",
    "start": "1035800",
    "end": "1037510"
  },
  {
    "text": "load the data into s3 we don't",
    "start": "1037510",
    "end": "1039880"
  },
  {
    "text": "necessarily have to have that schemer",
    "start": "1039880",
    "end": "1041438"
  },
  {
    "text": "created it's not until we go to read the",
    "start": "1041439",
    "end": "1044199"
  },
  {
    "text": "data that we actually need to do that",
    "start": "1044199",
    "end": "1046030"
  },
  {
    "text": "create table statement now this differs",
    "start": "1046030",
    "end": "1048700"
  },
  {
    "text": "slightly from redshift we were loading",
    "start": "1048700",
    "end": "1050500"
  },
  {
    "text": "data into the redshift local disk where",
    "start": "1050500",
    "end": "1053260"
  },
  {
    "text": "that's called a schema on right and that",
    "start": "1053260",
    "end": "1055570"
  },
  {
    "text": "means we have to create the table first",
    "start": "1055570",
    "end": "1057429"
  },
  {
    "text": "before we load the data arena now",
    "start": "1057429",
    "end": "1061120"
  },
  {
    "text": "looking at the cost model so the cost",
    "start": "1061120",
    "end": "1062710"
  },
  {
    "text": "models differ across the different",
    "start": "1062710",
    "end": "1064230"
  },
  {
    "text": "so so if you look at the ones that run",
    "start": "1064230",
    "end": "1066870"
  },
  {
    "text": "on top of Hadoop you're going to pay for",
    "start": "1066870",
    "end": "1068789"
  },
  {
    "text": "those by the hour because you're running",
    "start": "1068789",
    "end": "1070559"
  },
  {
    "text": "on a cluster and therefore you're going",
    "start": "1070559",
    "end": "1072179"
  },
  {
    "text": "to be paying for that hourly",
    "start": "1072179",
    "end": "1073610"
  },
  {
    "text": "Athena you pay for per query as you do",
    "start": "1073610",
    "end": "1077399"
  },
  {
    "text": "also with redshift spectrum redshift",
    "start": "1077399",
    "end": "1080519"
  },
  {
    "text": "itself you also pay for by the hour",
    "start": "1080519",
    "end": "1082679"
  },
  {
    "text": "because it is actually a cluster in",
    "start": "1082679",
    "end": "1084899"
  },
  {
    "text": "terms of the data store that they",
    "start": "1084899",
    "end": "1086399"
  },
  {
    "text": "support the ones that run on Hadoop",
    "start": "1086399",
    "end": "1089220"
  },
  {
    "text": "support both s3 and HDFS and then athena",
    "start": "1089220",
    "end": "1093389"
  },
  {
    "text": "is s3 only as is redshift spectrum and",
    "start": "1093389",
    "end": "1096210"
  },
  {
    "text": "then redshift itself obviously reads off",
    "start": "1096210",
    "end": "1098309"
  },
  {
    "text": "its own local data storm and then in",
    "start": "1098309",
    "end": "1100950"
  },
  {
    "text": "terms of performance typically sparks",
    "start": "1100950",
    "end": "1103409"
  },
  {
    "text": "equal and hive the slowest out of this",
    "start": "1103409",
    "end": "1106080"
  },
  {
    "text": "group because they're more",
    "start": "1106080",
    "end": "1107070"
  },
  {
    "text": "general-purpose engines presto is much",
    "start": "1107070",
    "end": "1109559"
  },
  {
    "text": "quicker for sequel queries because it is",
    "start": "1109559",
    "end": "1111299"
  },
  {
    "text": "a dedicated engine that's really been",
    "start": "1111299",
    "end": "1113279"
  },
  {
    "text": "designed specifically for that and",
    "start": "1113279",
    "end": "1115049"
  },
  {
    "text": "Athena has a similar footprint to presto",
    "start": "1115049",
    "end": "1117630"
  },
  {
    "text": "because they run off the same technology",
    "start": "1117630",
    "end": "1119929"
  },
  {
    "text": "typically redshift is the quickest",
    "start": "1119929",
    "end": "1122519"
  },
  {
    "text": "because it is a dedicated database so",
    "start": "1122519",
    "end": "1126870"
  },
  {
    "text": "given those aspects let's now look at",
    "start": "1126870",
    "end": "1128490"
  },
  {
    "text": "some of the use cases of that that they",
    "start": "1128490",
    "end": "1130139"
  },
  {
    "text": "would support so if you look at hive and",
    "start": "1130139",
    "end": "1132600"
  },
  {
    "text": "spark sequel typically they're used for",
    "start": "1132600",
    "end": "1134850"
  },
  {
    "text": "transformational types of queries more",
    "start": "1134850",
    "end": "1137250"
  },
  {
    "text": "batch type workloads",
    "start": "1137250",
    "end": "1139080"
  },
  {
    "text": "whereas presto is designed for",
    "start": "1139080",
    "end": "1141299"
  },
  {
    "text": "interactive sequel queries across s3 and",
    "start": "1141299",
    "end": "1143639"
  },
  {
    "text": "HDFS and Athena is similar but in a",
    "start": "1143639",
    "end": "1146730"
  },
  {
    "text": "server less fashion and talks just to s3",
    "start": "1146730",
    "end": "1149279"
  },
  {
    "text": "and then redshift is really your fully",
    "start": "1149279",
    "end": "1152370"
  },
  {
    "text": "featured sequel database so hopefully",
    "start": "1152370",
    "end": "1155610"
  },
  {
    "text": "that gives you a bit of a feel for when",
    "start": "1155610",
    "end": "1157409"
  },
  {
    "text": "you might use different engines now I",
    "start": "1157409",
    "end": "1159809"
  },
  {
    "text": "wanted to finish off our session with a",
    "start": "1159809",
    "end": "1161279"
  },
  {
    "text": "little look at how you tune these",
    "start": "1161279",
    "end": "1163559"
  },
  {
    "text": "engines that are looking at the data on",
    "start": "1163559",
    "end": "1165299"
  },
  {
    "text": "s3 so these tuning tips are for the",
    "start": "1165299",
    "end": "1169409"
  },
  {
    "text": "engines our query data and s3 the",
    "start": "1169409",
    "end": "1171510"
  },
  {
    "text": "redshift local tables have separate",
    "start": "1171510",
    "end": "1173789"
  },
  {
    "text": "tuning techniques to this this is really",
    "start": "1173789",
    "end": "1175830"
  },
  {
    "text": "about data and s3 and when you think",
    "start": "1175830",
    "end": "1177600"
  },
  {
    "text": "about it's actually more about how you",
    "start": "1177600",
    "end": "1179429"
  },
  {
    "text": "actually control the data on s3 as it is",
    "start": "1179429",
    "end": "1182490"
  },
  {
    "text": "about the engines themselves so let's",
    "start": "1182490",
    "end": "1185549"
  },
  {
    "text": "look first at data formats so there are",
    "start": "1185549",
    "end": "1189389"
  },
  {
    "text": "a number of different data formats that",
    "start": "1189389",
    "end": "1190559"
  },
  {
    "text": "you can choose from and they loosely",
    "start": "1190559",
    "end": "1193110"
  },
  {
    "text": "fall into two main groups so there's the",
    "start": "1193110",
    "end": "1195210"
  },
  {
    "text": "row based formats and some of these are",
    "start": "1195210",
    "end": "1197370"
  },
  {
    "text": "very familiar",
    "start": "1197370",
    "end": "1198040"
  },
  {
    "text": "such as CSV and JSON and some of the",
    "start": "1198040",
    "end": "1200650"
  },
  {
    "text": "newer robust formats like Avro for",
    "start": "1200650",
    "end": "1202570"
  },
  {
    "text": "example but then there are also columnar",
    "start": "1202570",
    "end": "1205180"
  },
  {
    "text": "formats like Park a and oh I see now you",
    "start": "1205180",
    "end": "1208870"
  },
  {
    "text": "might be asking why do we need columnar",
    "start": "1208870",
    "end": "1210910"
  },
  {
    "text": "formats as well well it turns out that a",
    "start": "1210910",
    "end": "1213460"
  },
  {
    "text": "lot of the workloads that we're looking",
    "start": "1213460",
    "end": "1215260"
  },
  {
    "text": "at in the data Lake are about analyzing",
    "start": "1215260",
    "end": "1217840"
  },
  {
    "text": "data and we found that with with the",
    "start": "1217840",
    "end": "1220630"
  },
  {
    "text": "analysis type of workloads the row based",
    "start": "1220630",
    "end": "1223090"
  },
  {
    "text": "formats were just too slow and that's",
    "start": "1223090",
    "end": "1224770"
  },
  {
    "text": "why the columnar format sprung up",
    "start": "1224770",
    "end": "1226420"
  },
  {
    "text": "because there are a lot lot faster so",
    "start": "1226420",
    "end": "1228820"
  },
  {
    "text": "let's look at why columnar formats are",
    "start": "1228820",
    "end": "1231610"
  },
  {
    "text": "typically faster so if you look at a",
    "start": "1231610",
    "end": "1233290"
  },
  {
    "text": "simple table we've got three columns",
    "start": "1233290",
    "end": "1234820"
  },
  {
    "text": "here in a number of rows and you can see",
    "start": "1234820",
    "end": "1237490"
  },
  {
    "text": "how these row based formats will lay",
    "start": "1237490",
    "end": "1240010"
  },
  {
    "text": "this table out on the disk typically the",
    "start": "1240010",
    "end": "1242680"
  },
  {
    "text": "way your brain imagines that it will so",
    "start": "1242680",
    "end": "1244450"
  },
  {
    "text": "it writes out a row there's a marker",
    "start": "1244450",
    "end": "1246070"
  },
  {
    "text": "writes had another row another marker",
    "start": "1246070",
    "end": "1248050"
  },
  {
    "text": "etc now that's fine but when we run an",
    "start": "1248050",
    "end": "1250960"
  },
  {
    "text": "analytical query we might say give me",
    "start": "1250960",
    "end": "1253840"
  },
  {
    "text": "the average age by state for example I",
    "start": "1253840",
    "end": "1256650"
  },
  {
    "text": "don't need the ID column for that query",
    "start": "1256650",
    "end": "1259390"
  },
  {
    "text": "but I still have to read it in order to",
    "start": "1259390",
    "end": "1262210"
  },
  {
    "text": "get to the data that I'm after so",
    "start": "1262210",
    "end": "1264130"
  },
  {
    "text": "there's a lot of redundant reading going",
    "start": "1264130",
    "end": "1265630"
  },
  {
    "text": "on there and you can imagine that as",
    "start": "1265630",
    "end": "1267400"
  },
  {
    "text": "your table gets wider as we add more",
    "start": "1267400",
    "end": "1269260"
  },
  {
    "text": "columns then this is going to get slower",
    "start": "1269260",
    "end": "1271810"
  },
  {
    "text": "and slower so how does a columnar format",
    "start": "1271810",
    "end": "1274420"
  },
  {
    "text": "differ to a row based format so what you",
    "start": "1274420",
    "end": "1278050"
  },
  {
    "text": "can see here down the bottom is the way",
    "start": "1278050",
    "end": "1279580"
  },
  {
    "text": "that a columnar format will actually",
    "start": "1279580",
    "end": "1281980"
  },
  {
    "text": "write that data to the disk so it writes",
    "start": "1281980",
    "end": "1284680"
  },
  {
    "text": "out a whole column a marker column",
    "start": "1284680",
    "end": "1287170"
  },
  {
    "text": "marker so now when I run my query and I",
    "start": "1287170",
    "end": "1291490"
  },
  {
    "text": "say give me average age by state I can",
    "start": "1291490",
    "end": "1294070"
  },
  {
    "text": "skip completely the ID column and again",
    "start": "1294070",
    "end": "1297550"
  },
  {
    "text": "you can imagine it with a wider table",
    "start": "1297550",
    "end": "1299080"
  },
  {
    "text": "this effect will be even more pronounced",
    "start": "1299080",
    "end": "1301330"
  },
  {
    "text": "so this is why the columnar formats are",
    "start": "1301330",
    "end": "1304360"
  },
  {
    "text": "much much faster for analytical types of",
    "start": "1304360",
    "end": "1306730"
  },
  {
    "text": "queries so let's move on now to",
    "start": "1306730",
    "end": "1310090"
  },
  {
    "text": "compression so there are many different",
    "start": "1310090",
    "end": "1312460"
  },
  {
    "text": "types of compression I've just shown a",
    "start": "1312460",
    "end": "1313720"
  },
  {
    "text": "couple of them here gzip bzip2 ah but",
    "start": "1313720",
    "end": "1318730"
  },
  {
    "text": "what are the different facets we need to",
    "start": "1318730",
    "end": "1320560"
  },
  {
    "text": "consider when we're trying to choose a",
    "start": "1320560",
    "end": "1322960"
  },
  {
    "text": "particular format so the first two of",
    "start": "1322960",
    "end": "1326320"
  },
  {
    "text": "these size and speed we we kind of used",
    "start": "1326320",
    "end": "1329800"
  },
  {
    "text": "to that trade-off and so",
    "start": "1329800",
    "end": "1331750"
  },
  {
    "text": "typically any compression algorithm that",
    "start": "1331750",
    "end": "1334450"
  },
  {
    "text": "can compress the data to a very small",
    "start": "1334450",
    "end": "1336910"
  },
  {
    "text": "size tends to be a little bit slower",
    "start": "1336910",
    "end": "1338740"
  },
  {
    "text": "than one that doesn't get you quite the",
    "start": "1338740",
    "end": "1341230"
  },
  {
    "text": "level of groups the same level of",
    "start": "1341230",
    "end": "1342520"
  },
  {
    "text": "compression most customers I talk to in",
    "start": "1342520",
    "end": "1345850"
  },
  {
    "text": "this space are more concerned about",
    "start": "1345850",
    "end": "1347470"
  },
  {
    "text": "speed than they are about size because",
    "start": "1347470",
    "end": "1349990"
  },
  {
    "text": "they want to make sure that their users",
    "start": "1349990",
    "end": "1351520"
  },
  {
    "text": "are getting the queries back as quickly",
    "start": "1351520",
    "end": "1353380"
  },
  {
    "text": "as possible in the data Lake area",
    "start": "1353380",
    "end": "1356380"
  },
  {
    "text": "there's a couple of other facets we need",
    "start": "1356380",
    "end": "1358360"
  },
  {
    "text": "to consider so one of them is is the",
    "start": "1358360",
    "end": "1360760"
  },
  {
    "text": "format's splittable and what we mean by",
    "start": "1360760",
    "end": "1363430"
  },
  {
    "text": "this is that can a process reach into a",
    "start": "1363430",
    "end": "1366400"
  },
  {
    "text": "file and actually pull out certain",
    "start": "1366400",
    "end": "1368950"
  },
  {
    "text": "chunks of it because you can imagine in",
    "start": "1368950",
    "end": "1371050"
  },
  {
    "text": "a parallel context you want those",
    "start": "1371050",
    "end": "1373240"
  },
  {
    "text": "processes to reach in and pull out",
    "start": "1373240",
    "end": "1374800"
  },
  {
    "text": "chunks of the file and send it to",
    "start": "1374800",
    "end": "1376450"
  },
  {
    "text": "different processors which is obviously",
    "start": "1376450",
    "end": "1379630"
  },
  {
    "text": "a lot more efficient however if the",
    "start": "1379630",
    "end": "1381580"
  },
  {
    "text": "format's not splittable then we're not",
    "start": "1381580",
    "end": "1384010"
  },
  {
    "text": "able to do that that whole file is going",
    "start": "1384010",
    "end": "1385960"
  },
  {
    "text": "to have to go to one processing engine",
    "start": "1385960",
    "end": "1387640"
  },
  {
    "text": "and the fourth option there is the data",
    "start": "1387640",
    "end": "1390610"
  },
  {
    "text": "format as well so certain compression",
    "start": "1390610",
    "end": "1393520"
  },
  {
    "text": "algorithms work better with certain data",
    "start": "1393520",
    "end": "1395380"
  },
  {
    "text": "formats and so for example if you choose",
    "start": "1395380",
    "end": "1397690"
  },
  {
    "text": "park' that works particular particularly",
    "start": "1397690",
    "end": "1400270"
  },
  {
    "text": "well with snappy so you might may find",
    "start": "1400270",
    "end": "1402340"
  },
  {
    "text": "that your choice choice of data format",
    "start": "1402340",
    "end": "1404050"
  },
  {
    "text": "actually informs your choice of",
    "start": "1404050",
    "end": "1406570"
  },
  {
    "text": "compression over in them as well so",
    "start": "1406570",
    "end": "1408190"
  },
  {
    "text": "let's look now at petitioning so",
    "start": "1408190",
    "end": "1412150"
  },
  {
    "start": "1411000",
    "end": "1549000"
  },
  {
    "text": "petitioning is a really important tuning",
    "start": "1412150",
    "end": "1415120"
  },
  {
    "text": "technique that is going to save you a",
    "start": "1415120",
    "end": "1417070"
  },
  {
    "text": "lot of time when you're scanning data in",
    "start": "1417070",
    "end": "1419020"
  },
  {
    "text": "your data lake so if you look at what we",
    "start": "1419020",
    "end": "1421090"
  },
  {
    "text": "have here let's say we have some Deo and",
    "start": "1421090",
    "end": "1422980"
  },
  {
    "text": "s3 and we have we have a file per year",
    "start": "1422980",
    "end": "1427290"
  },
  {
    "text": "but we don't have any petitioning in our",
    "start": "1427290",
    "end": "1429640"
  },
  {
    "text": "create table statement so what's going",
    "start": "1429640",
    "end": "1432130"
  },
  {
    "text": "to happen now when we run our query even",
    "start": "1432130",
    "end": "1433930"
  },
  {
    "text": "though our query has a where clause the",
    "start": "1433930",
    "end": "1436390"
  },
  {
    "text": "system has no way of knowing how to skip",
    "start": "1436390",
    "end": "1439870"
  },
  {
    "text": "the files on the disk to get to the data",
    "start": "1439870",
    "end": "1441790"
  },
  {
    "text": "that we're after it has to read",
    "start": "1441790",
    "end": "1443200"
  },
  {
    "text": "everything which you can imagine is very",
    "start": "1443200",
    "end": "1444700"
  },
  {
    "text": "inefficient so how do we avoid reading",
    "start": "1444700",
    "end": "1448360"
  },
  {
    "text": "every file what we do is we include",
    "start": "1448360",
    "end": "1450850"
  },
  {
    "text": "petitioning information in our create",
    "start": "1450850",
    "end": "1453760"
  },
  {
    "text": "table statement you can see the clause",
    "start": "1453760",
    "end": "1455560"
  },
  {
    "text": "there now we can skip the files that",
    "start": "1455560",
    "end": "1458830"
  },
  {
    "text": "don't contain the data that we need",
    "start": "1458830",
    "end": "1460990"
  },
  {
    "text": "we're just going to read the files that",
    "start": "1460990",
    "end": "1462340"
  },
  {
    "text": "we need and this is extremely important",
    "start": "1462340",
    "end": "1464110"
  },
  {
    "text": "for",
    "start": "1464110",
    "end": "1464470"
  },
  {
    "text": "for months as your data volumes grow now",
    "start": "1464470",
    "end": "1468460"
  },
  {
    "text": "before I go I just wanted to give you a",
    "start": "1468460",
    "end": "1469900"
  },
  {
    "text": "quick word on our high memory instances",
    "start": "1469900",
    "end": "1472929"
  },
  {
    "text": "so lots of the tools that we've",
    "start": "1472929",
    "end": "1474789"
  },
  {
    "text": "discussed they love memory so spark",
    "start": "1474789",
    "end": "1477130"
  },
  {
    "text": "presto etc love lots of memory we have",
    "start": "1477130",
    "end": "1479650"
  },
  {
    "text": "the x1 instance which contains Intel",
    "start": "1479650",
    "end": "1482260"
  },
  {
    "text": "Xeon Haswell processors up to 2",
    "start": "1482260",
    "end": "1484900"
  },
  {
    "text": "terabytes of RAM per instance and up to",
    "start": "1484900",
    "end": "1487150"
  },
  {
    "text": "128 V CPUs we're also working on newer",
    "start": "1487150",
    "end": "1491559"
  },
  {
    "text": "instances as well that have got even",
    "start": "1491559",
    "end": "1493150"
  },
  {
    "text": "more memory than this so fabulous if you",
    "start": "1493150",
    "end": "1495789"
  },
  {
    "text": "need a lot of memory now talking of",
    "start": "1495789",
    "end": "1498429"
  },
  {
    "text": "Intel we talked a lot at Amazon about",
    "start": "1498429",
    "end": "1501600"
  },
  {
    "text": "innovation but there's also lots",
    "start": "1501600",
    "end": "1503470"
  },
  {
    "text": "happening at the chip level too and",
    "start": "1503470",
    "end": "1505299"
  },
  {
    "text": "Intel have a lot of features built into",
    "start": "1505299",
    "end": "1507370"
  },
  {
    "text": "their chips which you can take advantage",
    "start": "1507370",
    "end": "1509049"
  },
  {
    "text": "of in the AWS cloud I'll put a couple of",
    "start": "1509049",
    "end": "1512080"
  },
  {
    "text": "examples here on the slide but check out",
    "start": "1512080",
    "end": "1513730"
  },
  {
    "text": "the Intel page on the AWS website for",
    "start": "1513730",
    "end": "1516640"
  },
  {
    "text": "more details now you can also continue",
    "start": "1516640",
    "end": "1519039"
  },
  {
    "text": "your learning path on AWS with online",
    "start": "1519039",
    "end": "1522220"
  },
  {
    "text": "labs instructor-led classes and",
    "start": "1522220",
    "end": "1525250"
  },
  {
    "text": "certification as well so thank you very",
    "start": "1525250",
    "end": "1527950"
  },
  {
    "text": "much for attending",
    "start": "1527950",
    "end": "1528850"
  },
  {
    "text": "AWS innovate we really appreciate your",
    "start": "1528850",
    "end": "1531190"
  },
  {
    "text": "time for this session please do fill out",
    "start": "1531190",
    "end": "1533169"
  },
  {
    "text": "the survey so that we can continue to",
    "start": "1533169",
    "end": "1535120"
  },
  {
    "text": "improve and I hope you enjoy the rest of",
    "start": "1535120",
    "end": "1537460"
  },
  {
    "text": "the sessions thank you very much",
    "start": "1537460",
    "end": "1540899"
  },
  {
    "text": "you",
    "start": "1547299",
    "end": "1549360"
  }
]