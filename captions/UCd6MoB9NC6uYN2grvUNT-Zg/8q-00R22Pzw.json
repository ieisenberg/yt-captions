[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": "hi and welcome to the accelerated data links webinar my name is Sharon and I'll",
    "start": "9090",
    "end": "14410"
  },
  {
    "text": "be helping host today's webinar we'll be running an hour-long session which will include 15 minutes of Q&A at the very",
    "start": "14410",
    "end": "20949"
  },
  {
    "text": "end of the session for joined today by poll Maci who is our big data specialist Solutions Architect at AWS link Pete hi",
    "start": "20949",
    "end": "29169"
  },
  {
    "text": "Delano will be our presenter for today take it away home thanks Sharon well",
    "start": "29169",
    "end": "37000"
  },
  {
    "text": "good morning everyone yeah as Sharon said my name is Paul may see I'm a solution architect at AWS",
    "start": "37000",
    "end": "43950"
  },
  {
    "text": "specializing in big data and analytics and so thank you for taking the time to",
    "start": "43950",
    "end": "49630"
  },
  {
    "text": "join me today as I present the accelerated like webinar ah just a",
    "start": "49630",
    "end": "55390"
  },
  {
    "text": "little housekeeping before we get started if you have any questions during the presentation please type them into",
    "start": "55390",
    "end": "62530"
  },
  {
    "text": "the question box in your GoToWebinar control panel and I'll answer as many as",
    "start": "62530",
    "end": "67750"
  },
  {
    "text": "I can at the end of the webinar and finally the slides will be made",
    "start": "67750",
    "end": "73030"
  },
  {
    "text": "available at the end of the after the webinar today will be a deep dive into a",
    "start": "73030",
    "end": "81490"
  },
  {
    "text": "data Lake solution however to begin with I'd like to take a few minutes to cover for what challenges organizations face",
    "start": "81490",
    "end": "88810"
  },
  {
    "text": "to really leverage and gain insights from their data then I'd like to talk",
    "start": "88810",
    "end": "94869"
  },
  {
    "text": "about how the accelerated dad-like solution can provide a way forward to address these challenges and then we'll",
    "start": "94869",
    "end": "103030"
  },
  {
    "text": "dive right in and I'll go through the architecture how we onboard data into the environment I'll walk you through a",
    "start": "103030",
    "end": "109299"
  },
  {
    "text": "live demonstration of the solution I'll do a brief wrap-up and then I'll take",
    "start": "109299",
    "end": "114969"
  },
  {
    "text": "some questions so let's get started one",
    "start": "114969",
    "end": "120279"
  },
  {
    "start": "120000",
    "end": "217000"
  },
  {
    "text": "of the biggest challenges we see organizations face with our data silos",
    "start": "120279",
    "end": "125829"
  },
  {
    "text": "where data is spread across multiple systems multiple environments in spreadsheets SharePoint lists or even",
    "start": "125829",
    "end": "133689"
  },
  {
    "text": "Word documents on someone's laptop and trying to bring all that data together can be a near-impossible task for many",
    "start": "133689",
    "end": "140379"
  },
  {
    "text": "organizations however they know that if they could bring all of this data",
    "start": "140379",
    "end": "145510"
  },
  {
    "text": "together it would be able to produce insights and benefits for the organization that they could have never",
    "start": "145510",
    "end": "150939"
  },
  {
    "text": "got before from their current environment the second challenge we see",
    "start": "150939",
    "end": "156760"
  },
  {
    "text": "is with security where data is stored in a range of locations where some of those",
    "start": "156760",
    "end": "162129"
  },
  {
    "text": "areas have no security controls in place the data has not been classified and",
    "start": "162129",
    "end": "167530"
  },
  {
    "text": "worst of all you know the data could be joined and some information inferred from it then there's a potential for a",
    "start": "167530",
    "end": "174639"
  },
  {
    "text": "data breach we hear from organizations",
    "start": "174639",
    "end": "179950"
  },
  {
    "text": "that they have poor or no data governance when it comes to understanding their data they they want",
    "start": "179950",
    "end": "185680"
  },
  {
    "text": "to know where the data came from what's in it how and when it was created had business operational and technical terms and want",
    "start": "185680",
    "end": "194290"
  },
  {
    "text": "to be able to search for any of this information and finally we hear about scalability where the the volume of data",
    "start": "194290",
    "end": "201519"
  },
  {
    "text": "being collected and stored and then queried is accelerating at a rate that",
    "start": "201519",
    "end": "206590"
  },
  {
    "text": "the servers and systems are struggling to keep up so to address those challenges I came up with a solution",
    "start": "206590",
    "end": "213459"
  },
  {
    "text": "known as the accelerated data Lake so",
    "start": "213459",
    "end": "219639"
  },
  {
    "start": "217000",
    "end": "328000"
  },
  {
    "text": "firstly the solution is centralized and scalable using Amazon s3 as the data",
    "start": "219639",
    "end": "224979"
  },
  {
    "text": "store see you will at last be able to bring in and join data from your data warehouses spreadsheets api's or even",
    "start": "224979",
    "end": "232629"
  },
  {
    "text": "things like SMTP servers the moment the data arrives in the data Lake it is",
    "start": "232629",
    "end": "239590"
  },
  {
    "text": "secured using rules that can include data classification and use cases after",
    "start": "239590",
    "end": "246120"
  },
  {
    "text": "being secured metadata is then applied this initially can include things like",
    "start": "246120",
    "end": "252310"
  },
  {
    "text": "data lineage time stamps checksums data owners application owners together with",
    "start": "252310",
    "end": "257709"
  },
  {
    "text": "business and technical terms and all of these can be easily customized users are",
    "start": "257709",
    "end": "266800"
  },
  {
    "text": "able to use the sequel skills they already have to query the data and it can be accessed",
    "start": "266800",
    "end": "271810"
  },
  {
    "text": "by many of the modern bi visualization tools the onboard thing of processing of",
    "start": "271810",
    "end": "280210"
  },
  {
    "text": "data files is a repeatable pattern that can be easily extended and customized",
    "start": "280210",
    "end": "285300"
  },
  {
    "text": "and finally the solution can become your organisation's analytical data science",
    "start": "285300",
    "end": "291850"
  },
  {
    "text": "and machine learning foundation where services such as Athena our interactive sequel query service EMR our big data",
    "start": "291850",
    "end": "299320"
  },
  {
    "text": "service or sage maker our machine learning service all have the ability to directly access this data the",
    "start": "299320",
    "end": "306220"
  },
  {
    "text": "accelerated unalike solution future priests organization so that as you explore additional AWS web services or",
    "start": "306220",
    "end": "314889"
  },
  {
    "text": "as new services become available they will also be able to access the same data foundation but you know best of all",
    "start": "314889",
    "end": "323260"
  },
  {
    "text": "the accelerated data Lake is available to everyone today it's on github we've",
    "start": "323260",
    "end": "329860"
  },
  {
    "start": "328000",
    "end": "404000"
  },
  {
    "text": "open sourced it and it's at AWS samples accelerated data Lake it includes the",
    "start": "329860",
    "end": "338650"
  },
  {
    "text": "data Lake pipeline which is a set of CloudFormation scripts instructions on",
    "start": "338650",
    "end": "343990"
  },
  {
    "text": "how to deploy the solution and data configuration security and metadata",
    "start": "343990",
    "end": "349600"
  },
  {
    "text": "templates to get you up and going fast you can deploy the solution yourself or",
    "start": "349600",
    "end": "356620"
  },
  {
    "text": "you can engage our professional services team or one of our many AWS partners so",
    "start": "356620",
    "end": "363850"
  },
  {
    "text": "let's take a look at what success looks like with using the accelerated data Lake solution firstly we recommend",
    "start": "363850",
    "end": "373419"
  },
  {
    "text": "starting small choosing only a few data sets that can produce some insights that your organization can immediately gain",
    "start": "373419",
    "end": "380530"
  },
  {
    "text": "benefits from you can then leverage the accelerated data Lake solution and the",
    "start": "380530",
    "end": "385780"
  },
  {
    "text": "security and metadata templates included to establish a repeatable workflow he",
    "start": "385780",
    "end": "391870"
  },
  {
    "text": "didn't produce the insights for your organization improve and iterate by adding further datasets and refining",
    "start": "391870",
    "end": "398080"
  },
  {
    "text": "your security and metadata and repeat",
    "start": "398080",
    "end": "402569"
  },
  {
    "start": "404000",
    "end": "648000"
  },
  {
    "text": "let's now take a look at the architecture of the accelerated data Lake solution beginning with the",
    "start": "404449",
    "end": "411060"
  },
  {
    "text": "high-level data flow the solution is broken up into four",
    "start": "411060",
    "end": "417990"
  },
  {
    "text": "stages the first stage is the initiation stage where a data file is processed",
    "start": "417990",
    "end": "425219"
  },
  {
    "text": "either at a particular time or the moment it arrives in s3 using AWS lambda",
    "start": "425219",
    "end": "431699"
  },
  {
    "text": "our service service then there is our processing stage which is the repeatable",
    "start": "431699",
    "end": "438150"
  },
  {
    "text": "process where the onboarding of data into the data lake actually occurs we",
    "start": "438150",
    "end": "444000"
  },
  {
    "text": "then have the cataloging stage where the s3 objects metadata is stored in dynamo",
    "start": "444000",
    "end": "449279"
  },
  {
    "text": "dB and indexed with elastic search using elastic search allows us to perform ad",
    "start": "449279",
    "end": "456509"
  },
  {
    "text": "hoc query of the metadata using API calls or the metadata can actually be",
    "start": "456509",
    "end": "462120"
  },
  {
    "text": "visualized using Cabana dashboards which are also part of the elastic search service and finally there is the data",
    "start": "462120",
    "end": "470550"
  },
  {
    "text": "Lake story stage where all of the processing for the data file is when it's all completed the file is moved",
    "start": "470550",
    "end": "477659"
  },
  {
    "text": "from the staging location for when it arrives in s3 and move to a roo location",
    "start": "477659",
    "end": "482969"
  },
  {
    "text": "making it available to the organization as I mentioned a few slides back the",
    "start": "482969",
    "end": "489779"
  },
  {
    "text": "daleks solution is able to be accessed by most of AWS s services enabling",
    "start": "489779",
    "end": "495479"
  },
  {
    "text": "analytics and insights let's now dive a little deeper into the processing",
    "start": "495479",
    "end": "502469"
  },
  {
    "text": "pipeline stage so using the example of a",
    "start": "502469",
    "end": "507719"
  },
  {
    "text": "data file immediately being processed once it arrives in Amazon s3 in a",
    "start": "507719",
    "end": "513899"
  },
  {
    "text": "staging location a lambda function is triggered from the file being saved in",
    "start": "513899",
    "end": "519300"
  },
  {
    "text": "s3 then the lambda function which contains a payload of information both",
    "start": "519300",
    "end": "525510"
  },
  {
    "text": "about the data file I'll and the event that happened itself starts an AWS step",
    "start": "525510",
    "end": "531779"
  },
  {
    "text": "function a step function lets you design and run service workflow",
    "start": "531779",
    "end": "537830"
  },
  {
    "text": "in this case the step function performs a series of steps each of which are",
    "start": "537830",
    "end": "543140"
  },
  {
    "text": "separate lambda functions the first lambda function finds the matching file",
    "start": "543140",
    "end": "550130"
  },
  {
    "text": "specification for the data file in s3 a file specification is like a like a list",
    "start": "550130",
    "end": "557930"
  },
  {
    "text": "of properties or settings for each data file type it is stored in the DynamoDB",
    "start": "557930",
    "end": "563839"
  },
  {
    "text": "table and it includes it includes each data files s3 is source and destination",
    "start": "563839",
    "end": "570760"
  },
  {
    "text": "location its metadata and security settings so there's one file specification for each data file type so",
    "start": "570760",
    "end": "579230"
  },
  {
    "text": "using the file specification information that we get from the DynamoDB table validation is performed on the data file",
    "start": "579230",
    "end": "587980"
  },
  {
    "text": "s3 object tags are added to the s3 data",
    "start": "587980",
    "end": "593000"
  },
  {
    "text": "file the s3 object tags are key value pairs and and we use them for security",
    "start": "593000",
    "end": "598010"
  },
  {
    "text": "and we create values like classification in is sensitive or PII is true and they",
    "start": "598010",
    "end": "604700"
  },
  {
    "text": "are used to control access to s3 objects in conjunction with using Amazon",
    "start": "604700",
    "end": "610850"
  },
  {
    "text": "identity access management metadata is",
    "start": "610850",
    "end": "616040"
  },
  {
    "text": "then add into the s3 object using metadata those metadata tags then the",
    "start": "616040",
    "end": "622250"
  },
  {
    "text": "objects metadata and security values are stored in and catalogued in dynamodb and",
    "start": "622250",
    "end": "628730"
  },
  {
    "text": "elastic search and then finally the s3 object is moved from the staging or",
    "start": "628730",
    "end": "635270"
  },
  {
    "text": "where it was landed to a row location and at that point the data can be",
    "start": "635270",
    "end": "641750"
  },
  {
    "text": "accessed by the organization based on the security that was applied so let's",
    "start": "641750",
    "end": "650000"
  },
  {
    "start": "648000",
    "end": "721000"
  },
  {
    "text": "take a look at how we on border new data file type into the data like onboarding",
    "start": "650000",
    "end": "658400"
  },
  {
    "text": "of data is made up of two steps first you create one of those new file",
    "start": "658400",
    "end": "665260"
  },
  {
    "text": "specification entries in a dynamo DB table which i've called data sources",
    "start": "665260",
    "end": "671240"
  },
  {
    "text": "and remember this file specification contains the file settings of the new data type s3 object tags and metadata",
    "start": "671240",
    "end": "679509"
  },
  {
    "text": "the second thing we have to do is actually create a folder structure in s3",
    "start": "679509",
    "end": "684949"
  },
  {
    "text": "for where the new data files will be stored in both the staging and raw bucket locations this is an important",
    "start": "684949",
    "end": "692480"
  },
  {
    "text": "step as it not only keeps your data organized but makes clearing of the data using services like Amazon Athene are",
    "start": "692480",
    "end": "700040"
  },
  {
    "text": "much more optimized and efficient so let's now look at each component of a",
    "start": "700040",
    "end": "706069"
  },
  {
    "text": "single file specification that is stored inside dynamodb and then what we'll look",
    "start": "706069",
    "end": "712519"
  },
  {
    "text": "at afterwards is how we can structure our s3 buckets and folders for optimized storage and query first off we have file",
    "start": "712519",
    "end": "724699"
  },
  {
    "start": "721000",
    "end": "846000"
  },
  {
    "text": "settings now hopefully you can all see this but as you can see because we're",
    "start": "724699",
    "end": "730490"
  },
  {
    "text": "using DynamoDB it's a no sequel database the information is stored in JSON and",
    "start": "730490",
    "end": "737179"
  },
  {
    "text": "can be easily customized to begin with as part of the file settings we need to",
    "start": "737179",
    "end": "742730"
  },
  {
    "text": "create a unique name for the file type in this case it's sample c MX",
    "start": "742730",
    "end": "748600"
  },
  {
    "text": "we also then include a file name pattern using a regular expression and this is",
    "start": "748600",
    "end": "755629"
  },
  {
    "text": "used to match the data file to its correct file specification within dynamo and then we add these staging andrew",
    "start": "755629",
    "end": "765319"
  },
  {
    "text": "locations for the new data type and that's all we really need to do to begin with next to the object tags those",
    "start": "765319",
    "end": "772910"
  },
  {
    "text": "security tags I mentioned earlier which are used for applying security to the s3",
    "start": "772910",
    "end": "778189"
  },
  {
    "text": "object as you can see we've included some examples like data classification data owner however you can create up",
    "start": "778189",
    "end": "785329"
  },
  {
    "text": "create up to ten of your own key value pairs to use as s3 object tags then",
    "start": "785329",
    "end": "793309"
  },
  {
    "text": "within the file specification we have two types of metadata the first is simple metadata which could be a data",
    "start": "793309",
    "end": "799939"
  },
  {
    "text": "owner timestamp md5 checksum or or a dish of the data and these values are",
    "start": "799939",
    "end": "806490"
  },
  {
    "text": "attached to the s3 object on the data file within the staging location and",
    "start": "806490",
    "end": "812570"
  },
  {
    "text": "then we have extended metadata which is catalogued to a separate dynamodb in",
    "start": "812570",
    "end": "819090"
  },
  {
    "text": "table as well as in elasticsearch and as you can see it's quite comprehensive these extended metadata values can",
    "start": "819090",
    "end": "826650"
  },
  {
    "text": "include business operational and technical terms field names data types",
    "start": "826650",
    "end": "831900"
  },
  {
    "text": "and Heald aliases and again you can customize all these values let's now",
    "start": "831900",
    "end": "838620"
  },
  {
    "text": "look at how we can structure our s3 buckets and folders for optimized storage and query by default when the",
    "start": "838620",
    "end": "848730"
  },
  {
    "start": "846000",
    "end": "1082000"
  },
  {
    "text": "accelerator done Lake solution is deployed it creates a group of s3 buckets and their State engine roar",
    "start": "848730",
    "end": "855350"
  },
  {
    "text": "curated gold validation data discovery and logging and yes all of these names",
    "start": "855350",
    "end": "862830"
  },
  {
    "text": "can be customized each of these s3 buckets they represent different states",
    "start": "862830",
    "end": "869430"
  },
  {
    "text": "or aggregations and views of the data that begins its life from in most cases",
    "start": "869430",
    "end": "874700"
  },
  {
    "text": "disparate systems and sources so let's take a look at folder structures within",
    "start": "874700",
    "end": "880320"
  },
  {
    "text": "the s3 data Lake within the staging bucket we find that storing data in",
    "start": "880320",
    "end": "886230"
  },
  {
    "text": "folders that represent a hierarchy of the source system or schema and then by",
    "start": "886230",
    "end": "892110"
  },
  {
    "text": "table or file type have been really effective ways of organizing the data as",
    "start": "892110",
    "end": "897600"
  },
  {
    "text": "it arrives for the raw bucket which usually stores untransformed data on the",
    "start": "897600",
    "end": "904950"
  },
  {
    "text": "staging bucket but actually has the security and metadata applied to the objects storing the data in a data",
    "start": "904950",
    "end": "911820"
  },
  {
    "text": "domain or like a topic rather than a system or source really brings to life the purpose of a data Lake which is you",
    "start": "911820",
    "end": "918840"
  },
  {
    "text": "know to bring all of this data together regardless of source in an organized fashion and make it making it really",
    "start": "918840",
    "end": "924600"
  },
  {
    "text": "easily consumable so if you look at the example here I've grouped Wireless related data into a",
    "start": "924600",
    "end": "932130"
  },
  {
    "text": "wireless folder so see a mixed data maybe you can see here another folder",
    "start": "932130",
    "end": "938339"
  },
  {
    "text": "might be provided by a network team on a daily basis while the API raw data file",
    "start": "938339",
    "end": "944309"
  },
  {
    "text": "might be an hourly file coming in from an external API call but as you can see they're rolled up underneath a single",
    "start": "944309",
    "end": "950759"
  },
  {
    "text": "location called Wireless now we're doing so there is another benefit to storing",
    "start": "950759",
    "end": "957720"
  },
  {
    "text": "data by data domain by using Amazon Athena or Amazon a.m. our big data",
    "start": "957720",
    "end": "963990"
  },
  {
    "text": "service you can now query the data in s3 like a database so where the wireless",
    "start": "963990",
    "end": "970379"
  },
  {
    "text": "folder is like a schema the semak's and API rule folders are like tables and the",
    "start": "970379",
    "end": "978059"
  },
  {
    "text": "folders below them are like data partitions so you can potentially do queries like this using sequel the",
    "start": "978059",
    "end": "986790"
  },
  {
    "text": "number of users using c mix can be select the count of the user by these from the cmx folder where the years 2019",
    "start": "986790",
    "end": "996480"
  },
  {
    "text": "or we can actually join folders by doing queries like this where we're saying",
    "start": "996480",
    "end": "1001879"
  },
  {
    "text": "let's find who the common users are between c MX data and an api feed for",
    "start": "1001879",
    "end": "1007970"
  },
  {
    "text": "the same year and all this can be done without a database and it's scale now",
    "start": "1007970",
    "end": "1016160"
  },
  {
    "text": "on a sidenote before I go on across to the demonstration I've spoken a lot",
    "start": "1016160",
    "end": "1022819"
  },
  {
    "text": "about s3 object tags and metadata and so I just like to quickly clarify how they",
    "start": "1022819",
    "end": "1029298"
  },
  {
    "text": "are applied to an s3 object so this recently I was in New Zealand so I've",
    "start": "1029299",
    "end": "1034548"
  },
  {
    "text": "got this photo here that was a pretty good photo and you can see when you take",
    "start": "1034549",
    "end": "1039678"
  },
  {
    "text": "a photo with a camera it actually creates exif data or metadata about the",
    "start": "1039679",
    "end": "1047178"
  },
  {
    "text": "image and you can see that the data isn't actually attached to the image but",
    "start": "1047179",
    "end": "1052490"
  },
  {
    "text": "is is effectively attached to the file this is conceptually the same when we're",
    "start": "1052490",
    "end": "1059029"
  },
  {
    "text": "working with s3 object tags and and metadata they aren't added to the data",
    "start": "1059029",
    "end": "1065210"
  },
  {
    "text": "but they live next to the data so the s3 object whether it's a CSV file",
    "start": "1065210",
    "end": "1071809"
  },
  {
    "text": "a bro or party or whatever the format is that remains unchanged alright let's get",
    "start": "1071809",
    "end": "1078919"
  },
  {
    "text": "on to the demo so the demo today I'm",
    "start": "1078919",
    "end": "1084139"
  },
  {
    "start": "1082000",
    "end": "1131000"
  },
  {
    "text": "going to show you is showing you real data they're being processed live using",
    "start": "1084139",
    "end": "1089210"
  },
  {
    "text": "the accelerated data Lake solution the dump data is being generated from an AWS",
    "start": "1089210",
    "end": "1096049"
  },
  {
    "text": "IOT simulator that produces the equal data now the IOT simulator is available",
    "start": "1096049",
    "end": "1103249"
  },
  {
    "text": "for anyone to deploy themselves and I've put the link up there and you'll get this as part of the pack and so you can",
    "start": "1103249",
    "end": "1109580"
  },
  {
    "text": "give that a try if you like the simulator lets you create your own devices and integrate them with our AWS",
    "start": "1109580",
    "end": "1115549"
  },
  {
    "text": "IOT services if you would like to know more about our IOT services let me know",
    "start": "1115549",
    "end": "1122090"
  },
  {
    "text": "after the session and I'll connect you with Craig Lawton who is our specialist solution architect focusing on IOT so in",
    "start": "1122090",
    "end": "1132860"
  },
  {
    "start": "1131000",
    "end": "1568000"
  },
  {
    "text": "the demonstration I will be using the AWS console you know the web console and I will be showing you the s3 data like",
    "start": "1132860",
    "end": "1141470"
  },
  {
    "text": "structure how the s3 objects are actually moved having security and",
    "start": "1141470",
    "end": "1146659"
  },
  {
    "text": "metadata applied I'm going to show you the step function that is processing each of the files through the pipeline",
    "start": "1146659",
    "end": "1154059"
  },
  {
    "text": "we'll take a look at the dynamo DB table and it's called data sources like I",
    "start": "1154059",
    "end": "1159980"
  },
  {
    "text": "mentioned before that lets you onboard your data in literally minutes not hours or days and you'll see why in just a",
    "start": "1159980",
    "end": "1167059"
  },
  {
    "text": "little while and then I'm going to show you the combiner dashboard that is displaying live metrics of the data",
    "start": "1167059",
    "end": "1173139"
  },
  {
    "text": "arriving in the data lay so let's jump across to the AWS console",
    "start": "1173139",
    "end": "1182470"
  },
  {
    "text": "just jumping across okay so here you can",
    "start": "1196150",
    "end": "1204670"
  },
  {
    "text": "see I have the IOT device simulator webpage open now this is not the webpage",
    "start": "1204670",
    "end": "1212110"
  },
  {
    "text": "for where you go and get the code for the simulator this is actually the webpage of the simulator running and you",
    "start": "1212110",
    "end": "1218800"
  },
  {
    "text": "can see that I have had in the past 99 vehicles in this particular case running",
    "start": "1218800",
    "end": "1223930"
  },
  {
    "text": "and if I go across to automotive here on the left hand side what you'll see is",
    "start": "1223930",
    "end": "1234850"
  },
  {
    "text": "I've still got some vehicles running and you'll see over time I've had lots of",
    "start": "1234850",
    "end": "1240460"
  },
  {
    "text": "different VIN numbers of the equals running over the last month or so now",
    "start": "1240460",
    "end": "1246550"
  },
  {
    "text": "that data is being streamed into s3 and I'll do is take you across to the s3",
    "start": "1246550",
    "end": "1253830"
  },
  {
    "text": "page from the AWS console now as you can",
    "start": "1253830",
    "end": "1258970"
  },
  {
    "text": "see the solution has pre-built all of the buckets that I spoke about earlier",
    "start": "1258970",
    "end": "1266470"
  },
  {
    "text": "where we have staging we have raw we have curated we have gold we have",
    "start": "1266470",
    "end": "1273190"
  },
  {
    "text": "validation we have data discovery and again as I mentioned these are all customizable now the duck is arriving",
    "start": "1273190",
    "end": "1281080"
  },
  {
    "text": "from the IOT service into staging the trouble is it's happening so quickly",
    "start": "1281080",
    "end": "1286780"
  },
  {
    "text": "that if we try and look at that data it's already gone so this data here is",
    "start": "1286780",
    "end": "1292690"
  },
  {
    "text": "actually from February and you can see that the way that I've organized this structure is I'm clicking through so you",
    "start": "1292690",
    "end": "1298570"
  },
  {
    "text": "can see I'm in the staging folder a bucket and as I go through you can see",
    "start": "1298570",
    "end": "1303640"
  },
  {
    "text": "the partitioning and organization of the data that you can see I've got no March",
    "start": "1303640",
    "end": "1308830"
  },
  {
    "text": "data coming through and that's because it's being immediately processed so I'm gonna go back out to the top level of s3",
    "start": "1308830",
    "end": "1315850"
  },
  {
    "text": "and I'm going to go to the raw bucket",
    "start": "1315850",
    "end": "1321300"
  },
  {
    "text": "now as you can see I've actually got a different folder structure than what I",
    "start": "1321300",
    "end": "1326620"
  },
  {
    "text": "presented earlier but you can see it's grouped by ADA so if I go into vehicle which is my",
    "start": "1326620",
    "end": "1333130"
  },
  {
    "text": "domain I can see there's other other sets of data that support that domain so",
    "start": "1333130",
    "end": "1338980"
  },
  {
    "text": "I could do queries that actually relate to whether on vehicles and traffic all at once in this particular case I'm",
    "start": "1338980",
    "end": "1345970"
  },
  {
    "text": "gonna dive further into the IOT data so when I go into IOT data you'll see that",
    "start": "1345970",
    "end": "1351430"
  },
  {
    "text": "as I click on each folder just like a normal file system I'm moving through effectively data partitions now they are",
    "start": "1351430",
    "end": "1360310"
  },
  {
    "text": "by date but they can be whatever whatever structure the data needs to be",
    "start": "1360310",
    "end": "1365860"
  },
  {
    "text": "to support the initial organization of the data now you can see here I've got",
    "start": "1365860",
    "end": "1372280"
  },
  {
    "text": "the twelfth the third and the actual simulator is a little bit slow because it's is producing data today but it's a",
    "start": "1372280",
    "end": "1378910"
  },
  {
    "text": "it's just a little bit out of time but it is running now these are the files",
    "start": "1378910",
    "end": "1385330"
  },
  {
    "text": "coming through from the simulator and what I'm gonna do is click on one of these files and I'm going to show you",
    "start": "1385330",
    "end": "1391480"
  },
  {
    "text": "the properties of the file now what would do that I've got a range of",
    "start": "1391480",
    "end": "1397510"
  },
  {
    "text": "different settings here and one of the settings that we spoke about earlier worth tags and this is where we manage",
    "start": "1397510",
    "end": "1404710"
  },
  {
    "text": "how we can provide access to the s3 objects now I'm gonna click on this little link here you'll see that",
    "start": "1404710",
    "end": "1411540"
  },
  {
    "text": "automatically as this process is running it's creating four sets of tags and you",
    "start": "1411540",
    "end": "1417340"
  },
  {
    "text": "can see here I've got one that says used cases analytics classification is private and I know here the application",
    "start": "1417340",
    "end": "1423160"
  },
  {
    "text": "owner is now I can create a rule that says hey you know what as long as that person has the access to a use case",
    "start": "1423160",
    "end": "1431110"
  },
  {
    "text": "that's analytics and the classification is private they can access any of this data and that's done through I am using",
    "start": "1431110",
    "end": "1439030"
  },
  {
    "text": "the words that are inside against a s3 object so it's pretty powerful you're",
    "start": "1439030",
    "end": "1444580"
  },
  {
    "text": "able to create context and apply data classification and and user access all",
    "start": "1444580",
    "end": "1451000"
  },
  {
    "text": "in one area so that's for the object tags let's take a look at the metadata",
    "start": "1451000",
    "end": "1456430"
  },
  {
    "text": "that's applied over here you can see there's quite a lot of metadata you can",
    "start": "1456430",
    "end": "1462640"
  },
  {
    "text": "see the we have who's the data owner we've got the analytics team what is the quality",
    "start": "1462640",
    "end": "1468879"
  },
  {
    "text": "of the data what's the description of the data okay on the bottom here where",
    "start": "1468879",
    "end": "1474789"
  },
  {
    "text": "did it start where did it end up going what's the file size there's the",
    "start": "1474789",
    "end": "1480909"
  },
  {
    "text": "checksum so I know that if anything changes within this file and in this spot this data has been catalogued",
    "start": "1480909",
    "end": "1487090"
  },
  {
    "text": "inside the data catalog in both DynamoDB and elasticsearch I know that this checksum shouldn't",
    "start": "1487090",
    "end": "1493360"
  },
  {
    "text": "calculate to be the same so you can see we can actually put a lot of different",
    "start": "1493360",
    "end": "1499929"
  },
  {
    "text": "fields in here and you can add as many or as little as you want and they can grow as your organization matures with",
    "start": "1499929",
    "end": "1507190"
  },
  {
    "text": "them metadata requirements so let's take",
    "start": "1507190",
    "end": "1514509"
  },
  {
    "text": "a look at the step function that is actually processing all of these files a",
    "start": "1514509",
    "end": "1521249"
  },
  {
    "text": "step function again provides a flow that allows us to use a range of different",
    "start": "1521249",
    "end": "1528369"
  },
  {
    "text": "services with an AWS to go through either a series or parallel process in",
    "start": "1528369",
    "end": "1535090"
  },
  {
    "text": "this particular case we have one called the file process so now I'm going to click on that and you'll see as I go in",
    "start": "1535090",
    "end": "1542289"
  },
  {
    "text": "there I love seeing this is what I love you know succeed succeed succeed and you",
    "start": "1542289",
    "end": "1547480"
  },
  {
    "text": "can see the time here is actually right because the the data Lake solution is tagging that is running correctly in the",
    "start": "1547480",
    "end": "1555269"
  },
  {
    "text": "local time let's go in and take a look at the actual step function and what",
    "start": "1555269",
    "end": "1560679"
  },
  {
    "text": "actually occurred in there now as I scroll down I'm just going to expand on",
    "start": "1560679",
    "end": "1567129"
  },
  {
    "text": "this screen yet and you can see this step function as I mentioned go through a range of steps and some of these",
    "start": "1567129",
    "end": "1574389"
  },
  {
    "start": "1568000",
    "end": "1608000"
  },
  {
    "text": "weren't in the previous slide only for simplification but you can see here we're finding a matching specification",
    "start": "1574389",
    "end": "1582659"
  },
  {
    "text": "we're attaching metadata we're attaching security tags we're sending the tags and",
    "start": "1582659",
    "end": "1589149"
  },
  {
    "text": "meta data to dinamo we're cataloging the tags and meta data in elasticsearch and",
    "start": "1589149",
    "end": "1594490"
  },
  {
    "text": "then we're moving the file from staging to final or it could be from",
    "start": "1594490",
    "end": "1599620"
  },
  {
    "text": "source to destination you can make any of these names up as you wish but our standard our standard solution has these",
    "start": "1599620",
    "end": "1607480"
  },
  {
    "text": "names and what you can see is that the path is is very very simple and you can",
    "start": "1607480",
    "end": "1614650"
  },
  {
    "start": "1608000",
    "end": "1676000"
  },
  {
    "text": "add new features in as you need to without having to recompile the whole",
    "start": "1614650",
    "end": "1619900"
  },
  {
    "text": "solution and what I mean by that is you could create a new Lander function that might need some some additional",
    "start": "1619900",
    "end": "1625630"
  },
  {
    "text": "validation or you want to split the data at a particular point well you could write a lambda function which is written",
    "start": "1625630",
    "end": "1632559"
  },
  {
    "text": "in languages like Python or JavaScript or Java or PHP and you can write it so",
    "start": "1632559",
    "end": "1639460"
  },
  {
    "text": "that it will split that data out and a particular step here without having to recompile the entire stack here because",
    "start": "1639460",
    "end": "1646059"
  },
  {
    "text": "they're all individual lambda functions and you can see here that if there's any",
    "start": "1646059",
    "end": "1652480"
  },
  {
    "text": "problems at any stage we add the error to a queue and we actually send off a",
    "start": "1652480",
    "end": "1658240"
  },
  {
    "text": "notification to a particular whether",
    "start": "1658240",
    "end": "1663730"
  },
  {
    "text": "it's it's team or those to an email or it could go to a mobile we have this",
    "start": "1663730",
    "end": "1669309"
  },
  {
    "text": "ability to really choose the direction of the workflow very easily so that's",
    "start": "1669309",
    "end": "1677260"
  },
  {
    "start": "1676000",
    "end": "2038000"
  },
  {
    "text": "the step function let's take a look at the Dynamo table that looks after the",
    "start": "1677260",
    "end": "1682809"
  },
  {
    "text": "file specification now that was where we had those four blocks of settings for",
    "start": "1682809",
    "end": "1691120"
  },
  {
    "text": "the each data type that you bring into the environment so let's go in there you",
    "start": "1691120",
    "end": "1697840"
  },
  {
    "text": "can see here I'm in dynamo dB oh no sequel database and we have a table called data sources now the data sources",
    "start": "1697840",
    "end": "1706510"
  },
  {
    "text": "table has a unique key which is the file type so when I click on that just keep",
    "start": "1706510",
    "end": "1712030"
  },
  {
    "text": "that in mind when I click on that I'm going to go across to items and look at",
    "start": "1712030",
    "end": "1717520"
  },
  {
    "text": "the items within the actual table and you'll see here the file type I've",
    "start": "1717520",
    "end": "1724390"
  },
  {
    "text": "created one for connected car I've got one for cm X I've got a another one here",
    "start": "1724390",
    "end": "1729820"
  },
  {
    "text": "but going to the connected car while and just take a look at what's going on inside this settings file you know it is",
    "start": "1729820",
    "end": "1736610"
  },
  {
    "text": "quite a lot of data so I might break this up into these stages but the first thing is we need to give each of the",
    "start": "1736610",
    "end": "1743509"
  },
  {
    "text": "entries of each data type a unique name in this particular case we've got connected car you may have SAP HANA CRM",
    "start": "1743509",
    "end": "1751990"
  },
  {
    "text": "data you may have anatase ER schema you may have something else the great thing",
    "start": "1751990",
    "end": "1758750"
  },
  {
    "text": "is you can make these whatever you need to need to be for your organisation and you can see there's no coding here all",
    "start": "1758750",
    "end": "1765350"
  },
  {
    "text": "we're doing is bringing words to onboard data let's take a look at these file",
    "start": "1765350",
    "end": "1770570"
  },
  {
    "text": "settings which is the main part of how we configure the movement and",
    "start": "1770570",
    "end": "1777009"
  },
  {
    "text": "consumption of the data from staging to raw what you can see here is that we",
    "start": "1777009",
    "end": "1783649"
  },
  {
    "text": "have the file name panel which is the regular expression that the file name",
    "start": "1783649",
    "end": "1789080"
  },
  {
    "text": "needs to meet to actually be processed we have a staging folder and the bucket",
    "start": "1789080",
    "end": "1798289"
  },
  {
    "text": "so those combined actually define where the source of the data is coming from",
    "start": "1798289",
    "end": "1804289"
  },
  {
    "text": "and then we actually have the destination which is the wrong bucket and again you can make these really go",
    "start": "1804289",
    "end": "1811190"
  },
  {
    "text": "anywhere we can change the names of these to suit your organization but for the solution we have used staging and",
    "start": "1811190",
    "end": "1818389"
  },
  {
    "text": "raw to get you going we can even choose whether to calculate the md5 checksum",
    "start": "1818389",
    "end": "1824539"
  },
  {
    "text": "for each file as it's being processed if the file pressings processing",
    "start": "1824539",
    "end": "1831139"
  },
  {
    "text": "succeeds we were able to send a notification to a team saying yes it succeeded or you could send that off to",
    "start": "1831139",
    "end": "1836570"
  },
  {
    "text": "another task if it fails we could send that off to to another note send that",
    "start": "1836570",
    "end": "1842960"
  },
  {
    "text": "off to another notification endpoint and something else can happen in reaction to a failed processing step so let's take a",
    "start": "1842960",
    "end": "1853700"
  },
  {
    "text": "look now at the metadata I actually reduce the security tags and you can see",
    "start": "1853700",
    "end": "1859759"
  },
  {
    "text": "here I've got security tags that say application owner is i ott classification is private well you know",
    "start": "1859759",
    "end": "1866480"
  },
  {
    "text": "what I actually might need to change this and if you've got a data governance team or a chief data office they might",
    "start": "1866480",
    "end": "1872629"
  },
  {
    "text": "have authorization to do this and they could go in say well you know what from now on this data is now going to be",
    "start": "1872629",
    "end": "1879369"
  },
  {
    "text": "sensitive and you know what I've just changed it and so by simply making small",
    "start": "1879369",
    "end": "1887119"
  },
  {
    "text": "changes to the settings in the DynamoDB entry that now will change the files as",
    "start": "1887119",
    "end": "1895609"
  },
  {
    "text": "they are coming in through the pipeline now this can trigger a retrospective update across all of the previous files",
    "start": "1895609",
    "end": "1902600"
  },
  {
    "text": "as well so the power of this is amazing you don't have to code you can change",
    "start": "1902600",
    "end": "1908929"
  },
  {
    "text": "the words using normal typing in an environment that has a GUI and that will",
    "start": "1908929",
    "end": "1914239"
  },
  {
    "text": "propagate out through the risk of the environment let's take a look at",
    "start": "1914239",
    "end": "1919249"
  },
  {
    "text": "metadata as you can see we have got these fields as a starting point I may",
    "start": "1919249",
    "end": "1928129"
  },
  {
    "text": "decide you know what this isn't just a vehicle it's now going to be trucks we",
    "start": "1928129",
    "end": "1933230"
  },
  {
    "text": "know that it's spelled correctly the UV tracks and that now will also change the",
    "start": "1933230",
    "end": "1940879"
  },
  {
    "text": "rest of the environment and we know this is now platinum because we know the IOT",
    "start": "1940879",
    "end": "1946070"
  },
  {
    "text": "service is running really well and I think that will do the minute we make those changes that's being",
    "start": "1946070",
    "end": "1952369"
  },
  {
    "text": "propagated through the environment and finally we have extended metadata now",
    "start": "1952369",
    "end": "1957799"
  },
  {
    "text": "this can be anything that you want I've included things like what is the format",
    "start": "1957799",
    "end": "1962840"
  },
  {
    "text": "of the data within the actual data set I've got still the data quality I can",
    "start": "1962840",
    "end": "1969769"
  },
  {
    "text": "include data elements where each of the fields I can include an alternative name",
    "start": "1969769",
    "end": "1974989"
  },
  {
    "text": "a description and a timestamp now some people might go you know what we can use",
    "start": "1974989",
    "end": "1982009"
  },
  {
    "text": "Calibra oh we can use informatica or we can do all of these things with these services by other providers and the one",
    "start": "1982009",
    "end": "1990259"
  },
  {
    "text": "thing we do find with going down the dynamodb an elastic search approach is",
    "start": "1990259",
    "end": "1996590"
  },
  {
    "text": "that a lot of organized don't know what they want to include as metadata before they make a large",
    "start": "1996590",
    "end": "2002720"
  },
  {
    "text": "commitment to buying a software application which is going to help them with their metadata so we find that it",
    "start": "2002720",
    "end": "2009409"
  },
  {
    "text": "will have customers come in and actually start using this environment and slowly maturing their metadata a year later we",
    "start": "2009409",
    "end": "2017510"
  },
  {
    "text": "see them looking at it going you know what we're ready to have this conversation with those other vendors and actually look at bringing that into",
    "start": "2017510",
    "end": "2023090"
  },
  {
    "text": "a different solution but what a great way to start that you can go through customize update and mature all of your",
    "start": "2023090",
    "end": "2031190"
  },
  {
    "text": "security and data governance with just a change into a few words inside a table",
    "start": "2031190",
    "end": "2039669"
  },
  {
    "start": "2038000",
    "end": "2107000"
  },
  {
    "text": "finally I'm going to show you the data coming through into Cabana so Cabana is part of the elastic elastic",
    "start": "2039940",
    "end": "2047990"
  },
  {
    "text": "search stuper elastic search stack where we have elastic search as the indexing",
    "start": "2047990",
    "end": "2054260"
  },
  {
    "text": "engine cabana is the visualization engine so you can create dashboards and do interactive querying on the data and",
    "start": "2054260",
    "end": "2061099"
  },
  {
    "text": "this is live so what's happening is as data is coming in through the simulator",
    "start": "2061099",
    "end": "2066919"
  },
  {
    "text": "I'm able to create effectively a like a sock for data integrity I've got the",
    "start": "2066919",
    "end": "2073970"
  },
  {
    "text": "files coming in I'm looking at the files coming in just generally as counts I'm",
    "start": "2073970",
    "end": "2079368"
  },
  {
    "text": "looking at them over time in will X and in blocks and then I'm looking at again",
    "start": "2079369",
    "end": "2086839"
  },
  {
    "text": "another view broken up again by agri aggregate files and this can be",
    "start": "2086839",
    "end": "2092480"
  },
  {
    "text": "customized so that you can have a heat map you can have geospatial data you can include a whole range of values to",
    "start": "2092480",
    "end": "2099890"
  },
  {
    "text": "really augmented data and that can be displayed in inside Cabana so there you",
    "start": "2099890",
    "end": "2108560"
  },
  {
    "start": "2107000",
    "end": "2217000"
  },
  {
    "text": "have it were able to go through store data in s3 by configuring a dynamo",
    "start": "2108560",
    "end": "2116560"
  },
  {
    "text": "dynamo entry inside a table the step function continually runs through the",
    "start": "2116560",
    "end": "2123410"
  },
  {
    "text": "same pipeline of work so when you onboard data the only thing you need to do is really add a new entry inside",
    "start": "2123410",
    "end": "2129650"
  },
  {
    "text": "dynamo and some folders and you can begin visualizing your data and querying your metadata",
    "start": "2129650",
    "end": "2135870"
  },
  {
    "text": "within cabana and elasticsearch so let's jump back to the presentation and we'll",
    "start": "2135870",
    "end": "2143820"
  },
  {
    "text": "close off why is easier said than done",
    "start": "2143820",
    "end": "2154100"
  },
  {
    "text": "okay made it okay so to wrap up the",
    "start": "2167450",
    "end": "2174330"
  },
  {
    "text": "accelerated data solution really gives you the ability to centralize and scale",
    "start": "2174330",
    "end": "2179970"
  },
  {
    "text": "your data you can have security from day zero so the moment it lands in the",
    "start": "2179970",
    "end": "2185400"
  },
  {
    "text": "environment you know that only the users or services who should access that should be able to access that data can",
    "start": "2185400",
    "end": "2191280"
  },
  {
    "text": "you know where the data came from it's you can have data lineage data owners",
    "start": "2191280",
    "end": "2196410"
  },
  {
    "text": "application owners the minute the data arrives you can query it using languages and tools that are common today the",
    "start": "2196410",
    "end": "2204540"
  },
  {
    "text": "solution is repeatable and extensible by using that pipeline and just onboarding more data sets and it becomes your",
    "start": "2204540",
    "end": "2213510"
  },
  {
    "text": "analytical and data science foundation it's really going to enable your data",
    "start": "2213510",
    "end": "2220730"
  },
  {
    "start": "2217000",
    "end": "2299000"
  },
  {
    "text": "support your data security and data governance grow and scale in harmony with your organization and you can grant",
    "start": "2220730",
    "end": "2228210"
  },
  {
    "text": "the environment access to your analytics machine learning and AI ecosystem within",
    "start": "2228210",
    "end": "2233280"
  },
  {
    "text": "AWS I've included a few links in the",
    "start": "2233280",
    "end": "2239970"
  },
  {
    "text": "deck which hopefully you get a chance to have a look at I've included security because I believe security is one of the",
    "start": "2239970",
    "end": "2246900"
  },
  {
    "text": "most important things to talk about with anything that we do in cloud today I gain included the link to the github",
    "start": "2246900",
    "end": "2253860"
  },
  {
    "text": "repo for the accelerated data Lake I've got a couple of blogs out the first one",
    "start": "2253860",
    "end": "2259740"
  },
  {
    "text": "is about how we take those disparate systems and siloed data and how we start looking",
    "start": "2259740",
    "end": "2265319"
  },
  {
    "text": "at that data in a different way in looking them as data domains and then we",
    "start": "2265319",
    "end": "2273989"
  },
  {
    "text": "have the second one where we're securing our data using those security object",
    "start": "2273989",
    "end": "2279539"
  },
  {
    "text": "tags on f3 and also using metadata tags I did say recently there was a another",
    "start": "2279539",
    "end": "2287190"
  },
  {
    "text": "blog put out by would calm which is a bit like daily deals and they build a very similar kind of service data like",
    "start": "2287190",
    "end": "2294209"
  },
  {
    "text": "on AWS so I recommend you take a look so",
    "start": "2294209",
    "end": "2299849"
  },
  {
    "start": "2299000",
    "end": "2737000"
  },
  {
    "text": "that's it I might call it a wrap and take some time for some questions okay",
    "start": "2299849",
    "end": "2313529"
  },
  {
    "text": "so the first question I've got is can data be validated and enriched in the",
    "start": "2313529",
    "end": "2319859"
  },
  {
    "text": "data pipeline so yes the the way we go about",
    "start": "2319859",
    "end": "2325130"
  },
  {
    "text": "validating data in the pipeline is is part of the actual accelerated data Lake",
    "start": "2325130",
    "end": "2330569"
  },
  {
    "text": "solution what happens is as one of the steps inside the step function there is",
    "start": "2330569",
    "end": "2337199"
  },
  {
    "text": "a validation step in mind build I don't have it in there but when you go to the",
    "start": "2337199",
    "end": "2343259"
  },
  {
    "text": "github repo you will be able to see there is a validation stage and that happens at the very beginning because we",
    "start": "2343259",
    "end": "2349859"
  },
  {
    "text": "want to be able to say well okay I'm going to go into dynamo I'm going to find the matching file specification and",
    "start": "2349859",
    "end": "2356219"
  },
  {
    "text": "then I go and find out what I need to do to validate the data as part of the solution we use a framework or now I",
    "start": "2356219",
    "end": "2367890"
  },
  {
    "text": "remember off the top of my head is JSON schema and sees the skimmer and their",
    "start": "2367890",
    "end": "2373410"
  },
  {
    "text": "Python libraries that are available and we use them as our validation method so",
    "start": "2373410",
    "end": "2380940"
  },
  {
    "text": "you can with those particular frameworks or libraries you're able to say what",
    "start": "2380940",
    "end": "2387029"
  },
  {
    "text": "order a set of headings should be inside a particular data file you're able to",
    "start": "2387029",
    "end": "2393089"
  },
  {
    "text": "say as well as that what contents need to be in each of the field",
    "start": "2393089",
    "end": "2399240"
  },
  {
    "text": "so you can say does it need to be numeric it needs to have you know a B or C in a particular field and so on and",
    "start": "2399240",
    "end": "2408030"
  },
  {
    "text": "that exists for CSV and for JSON data so",
    "start": "2408030",
    "end": "2414480"
  },
  {
    "text": "I've got another question can web services be invoked to validate and enrich the data sure can and there's no",
    "start": "2414480",
    "end": "2422369"
  },
  {
    "text": "reason why you can't trigger another task off to go to another service to",
    "start": "2422369",
    "end": "2428880"
  },
  {
    "text": "actually do a particular pre-processing of data and actually augmenting of data",
    "start": "2428880",
    "end": "2435140"
  },
  {
    "text": "generally though we find that it's better to land the raw data into the raw",
    "start": "2435140",
    "end": "2442350"
  },
  {
    "text": "location unchanged so that you do have this source of truth and then trigger a",
    "start": "2442350",
    "end": "2448800"
  },
  {
    "text": "second lambda function to actually go and do the or be further augmentation of",
    "start": "2448800",
    "end": "2455340"
  },
  {
    "text": "the data and reaching of the data can a",
    "start": "2455340",
    "end": "2463440"
  },
  {
    "text": "scenic weari those metadata tags I would",
    "start": "2463440",
    "end": "2469560"
  },
  {
    "text": "say yes we could do it I think that it would be it's a we have a couple of ways",
    "start": "2469560",
    "end": "2478109"
  },
  {
    "text": "of doing it it's probably better you'd end up duplicating it because Athena really does rely on having a data",
    "start": "2478109",
    "end": "2485970"
  },
  {
    "text": "catalog or using we use AWS glue to",
    "start": "2485970",
    "end": "2490980"
  },
  {
    "text": "actually create a catalog that lets you connect to DynamoDB there's a method",
    "start": "2490980",
    "end": "2496619"
  },
  {
    "text": "there but that doesn't get you to Athena so I'm just so I'm just thinking this out as we go I would think that the only",
    "start": "2496619",
    "end": "2502740"
  },
  {
    "text": "way you could really do that with Athena at this stage would be to duplicate that data into a tree in a file and it's not",
    "start": "2502740",
    "end": "2510510"
  },
  {
    "text": "a lot of data so that would be fairly fairly straightforward to do okay I have",
    "start": "2510510",
    "end": "2518460"
  },
  {
    "text": "another question can you point me to where I can find out how to connect my organization's datasets to s3 okay now",
    "start": "2518460",
    "end": "2530860"
  },
  {
    "text": "we can't point you to how you can connect your organization's data sets to s3 because there's probably lots of",
    "start": "2530860",
    "end": "2536920"
  },
  {
    "text": "different permutations but we do have a range of services to help connect data that is on-premise to be sent to s3 we",
    "start": "2536920",
    "end": "2547270"
  },
  {
    "text": "have that we like I suppose we can start with a file gateway we can do a snowball",
    "start": "2547270",
    "end": "2556360"
  },
  {
    "text": "I'm not going to go through all of these definitions right now there's a range of different things I know hopefully you can google these or go onto our site we",
    "start": "2556360",
    "end": "2565120"
  },
  {
    "text": "can use data sync there's a new service called data sync if you have done other",
    "start": "2565120",
    "end": "2571540"
  },
  {
    "text": "bases on premise we also have our data migration service that not only moves",
    "start": "2571540",
    "end": "2577690"
  },
  {
    "text": "tables it you know it does have a schema conversion tool as well to help you map",
    "start": "2577690",
    "end": "2583420"
  },
  {
    "text": "out your scheme is on-premise to go to our databases or even to s3 in in AWS is",
    "start": "2583420",
    "end": "2593770"
  },
  {
    "text": "part is pipeline part of data Lake could",
    "start": "2593770",
    "end": "2599710"
  },
  {
    "text": "someone who write that just give me a little bit more maybe on there are we",
    "start": "2599710",
    "end": "2606340"
  },
  {
    "text": "talking about a is this is the step function pipeline part of the data like",
    "start": "2606340",
    "end": "2613330"
  },
  {
    "text": "maybe does anyone want to just extend that a little bit because yeah okay okay",
    "start": "2613330",
    "end": "2627010"
  },
  {
    "text": "hey yeah so the the data lake itself if you think of how okay so I'm going to",
    "start": "2627010",
    "end": "2632380"
  },
  {
    "text": "answer it like this the data Lake in s3 is really the data foundation the step",
    "start": "2632380",
    "end": "2639730"
  },
  {
    "text": "function pipe is is really the process or the pipeline for moving data from the",
    "start": "2639730",
    "end": "2647500"
  },
  {
    "text": "various different buckets or even other services there's into the data Lake so it supports the data Lake but it's not",
    "start": "2647500",
    "end": "2654300"
  },
  {
    "text": "really part of the data like that does that the data like really is just the s3 storage layer in this particular case of",
    "start": "2654300",
    "end": "2663130"
  },
  {
    "text": "course data lakes can be also databases and they can be a range",
    "start": "2663130",
    "end": "2669490"
  },
  {
    "text": "of other services within AWS but for this particular solution the the base layer is the s3 bucket of the s3 buckets",
    "start": "2669490",
    "end": "2679620"
  },
  {
    "text": "okay does anyone have some more questions okay how do you metadata",
    "start": "2680190",
    "end": "2695830"
  },
  {
    "text": "settings make the data more secure okay so metadata metadata on its own",
    "start": "2695830",
    "end": "2704220"
  },
  {
    "text": "doesn't make it more secure all that's doing is providing you information about",
    "start": "2704220",
    "end": "2711130"
  },
  {
    "text": "the about the actual data what we use to",
    "start": "2711130",
    "end": "2716710"
  },
  {
    "text": "make things more secure are the object tags and the way the object tags work I",
    "start": "2716710",
    "end": "2723610"
  },
  {
    "text": "actually have a slide if I could just flick through I'll see if I can find it for you I'll bring a slide up which actually might be able to answer that if I've got",
    "start": "2723610",
    "end": "2730300"
  },
  {
    "text": "it here give me just a moment because it's now this is in one of my blogs as",
    "start": "2730300",
    "end": "2739000"
  },
  {
    "start": "2737000",
    "end": "3119000"
  },
  {
    "text": "well and I'm just gonna go on why I'm just going to show you this current slide here so here we go the object tags",
    "start": "2739000",
    "end": "2750840"
  },
  {
    "text": "combined with storing the data files in a particular data domain structure allow",
    "start": "2750840",
    "end": "2759820"
  },
  {
    "text": "you to create a combination that when usually though I am allows you only",
    "start": "2759820",
    "end": "2765640"
  },
  {
    "text": "particular people or users access to sets of data oops sorry but that is why I like now",
    "start": "2765640",
    "end": "2773080"
  },
  {
    "text": "what you can see here we look at that Wireless scenario that we looked at before we have cmx data and you can see",
    "start": "2773080",
    "end": "2779050"
  },
  {
    "text": "that I've actually got it down as a sensitive classification and I've got",
    "start": "2779050",
    "end": "2784840"
  },
  {
    "text": "the nick year one here which is public for whatever reason now what I'm able to",
    "start": "2784840",
    "end": "2790720"
  },
  {
    "text": "do is create those s3 tags to represent",
    "start": "2790720",
    "end": "2795840"
  },
  {
    "text": "each of the data classic Asian types within a particular data",
    "start": "2795840",
    "end": "2803380"
  },
  {
    "text": "layer like as you can see with those folders and then what happens is is I",
    "start": "2803380",
    "end": "2808660"
  },
  {
    "text": "can use those words within identity access management now the way that would",
    "start": "2808660",
    "end": "2814870"
  },
  {
    "text": "work is let's say I create a policy a set of rules that say well okay if I",
    "start": "2814870",
    "end": "2820930"
  },
  {
    "text": "want someone to be able to access CMX data I create a policy within identity",
    "start": "2820930",
    "end": "2826930"
  },
  {
    "text": "access management called wireless sensitive now within that I'm able to use the object tags as a set of rules",
    "start": "2826930",
    "end": "2835720"
  },
  {
    "text": "for deciding whether some someone or a service can access the data within a",
    "start": "2835720",
    "end": "2843820"
  },
  {
    "text": "particular s3 location and the way I would do that is here I'm able to create a set of words in the actual policy that",
    "start": "2843820",
    "end": "2851500"
  },
  {
    "text": "is the classification equal sensitive and the domain equals Wireless so that",
    "start": "2851500",
    "end": "2858220"
  },
  {
    "text": "naturally when attached to a user a group or a role and then potentially",
    "start": "2858220",
    "end": "2864600"
  },
  {
    "text": "integrated with a Active Directory will automatically decide whether based on",
    "start": "2864600",
    "end": "2871570"
  },
  {
    "text": "your access you can get access to CMX just using those words so you can imagine if you have classification and",
    "start": "2871570",
    "end": "2878920"
  },
  {
    "text": "sensitive classification is sensitive and the domain is Wireless and you needed to have both of those and your s3",
    "start": "2878920",
    "end": "2886090"
  },
  {
    "text": "tag for a CMX folder only has one of those or you're not going to get access",
    "start": "2886090",
    "end": "2891610"
  },
  {
    "text": "to that data you need to have both of those terms it can be an end or or cenario but we could probably deep dive",
    "start": "2891610",
    "end": "2898960"
  },
  {
    "text": "into that separately but I would definitely take a look at the the second blog that I've I've put up which covers",
    "start": "2898960",
    "end": "2906790"
  },
  {
    "text": "how we do security using the data domains the policies and the s3 tags",
    "start": "2906790",
    "end": "2914400"
  },
  {
    "text": "okay any more questions I'm not scared",
    "start": "2916380",
    "end": "2922360"
  },
  {
    "text": "I'll try and answer anything nothing personal that I can never get for those things",
    "start": "2922360",
    "end": "2929340"
  },
  {
    "text": "one question that all I get asked a lot is what is the difference between lake",
    "start": "2930849",
    "end": "2937929"
  },
  {
    "text": "formation the new service from AWS that was announced that reinvent 2018 and",
    "start": "2937929",
    "end": "2946209"
  },
  {
    "text": "this solution the it's a little bit tricky they're not identical lake",
    "start": "2946209",
    "end": "2952749"
  },
  {
    "text": "formation is a higher order service which does build out identity access",
    "start": "2952749",
    "end": "2958119"
  },
  {
    "text": "management policies it does build out s3 bucket structures and it does have an",
    "start": "2958119",
    "end": "2965979"
  },
  {
    "text": "awesome feature of being able to do machine learning to work out data",
    "start": "2965979",
    "end": "2971259"
  },
  {
    "text": "commonality across multiple data sets which is just just fantastic the thing",
    "start": "2971259",
    "end": "2978549"
  },
  {
    "text": "that separates that solution from what the accelerator data Lake is is that the",
    "start": "2978549",
    "end": "2985689"
  },
  {
    "text": "accelerator data Lake provides you with the templates and the QuickStart to actually really start joining data today",
    "start": "2985689",
    "end": "2993819"
  },
  {
    "text": "if you have some sample data sets that you want to put into s3 you've seen now",
    "start": "2993819",
    "end": "2999519"
  },
  {
    "text": "that you've only got to put some couple of folders inside s3 and copy and copy",
    "start": "2999519",
    "end": "3005099"
  },
  {
    "text": "some of those dynamo entries that i've got you and just do copy and paste and you create new ones and just put onboard",
    "start": "3005099",
    "end": "3012299"
  },
  {
    "text": "that data that's where the difference is you're able to use the templates and the",
    "start": "3012299",
    "end": "3017729"
  },
  {
    "text": "and the customization of the CloudFormation script to really custom build a solution that's really fast to",
    "start": "3017729",
    "end": "3024689"
  },
  {
    "text": "deploy and really quick to get insights it's really about that customization and",
    "start": "3024689",
    "end": "3029729"
  },
  {
    "text": "speeds of delivery okay so I mean",
    "start": "3029729",
    "end": "3037709"
  },
  {
    "text": "another question which is how do you allow search against elastic search with",
    "start": "3037709",
    "end": "3043739"
  },
  {
    "text": "security in mind okay so in that regard",
    "start": "3043739",
    "end": "3049549"
  },
  {
    "text": "there's a range of different ways of doing that obviously one of the most",
    "start": "3049549",
    "end": "3055199"
  },
  {
    "text": "common ones we recommend is using Amazon kognito and you're able to create a set of view",
    "start": "3055199",
    "end": "3062309"
  },
  {
    "text": "user pools and your own - those pools are granted access to particular services so there's one",
    "start": "3062309",
    "end": "3068580"
  },
  {
    "text": "particular method so definitely take a look online at Amazon Cognito because",
    "start": "3068580",
    "end": "3074430"
  },
  {
    "text": "that is one method and there's a couple of our blogs about how we go about securing access to two Cabana dashboards",
    "start": "3074430",
    "end": "3082620"
  },
  {
    "text": "and elasticsearch using that method the other thing that you can do is when",
    "start": "3082620",
    "end": "3089070"
  },
  {
    "text": "you're in elastic search you are able to create policies that are going to control quite granularly who can access",
    "start": "3089070",
    "end": "3096450"
  },
  {
    "text": "the elastic search environment and that can be down to an IP address or an account or even a user so lots of",
    "start": "3096450",
    "end": "3103920"
  },
  {
    "text": "different ways of doing it definitely go into the elastic search documents and",
    "start": "3103920",
    "end": "3109230"
  },
  {
    "text": "you can read about how we have different approaches to to enforcing security with",
    "start": "3109230",
    "end": "3114390"
  },
  {
    "text": "elastic search do I have an example I am",
    "start": "3114390",
    "end": "3120840"
  },
  {
    "start": "3119000",
    "end": "3159000"
  },
  {
    "text": "policy for this page see the blog post okay give me a second what okay here we",
    "start": "3120840",
    "end": "3128460"
  },
  {
    "text": "go I got shot who would have thought I'd have one handy and what I'll do I'll try and I've got to put something down here",
    "start": "3128460",
    "end": "3135060"
  },
  {
    "text": "let me see if I can look over there if you've just gone all I'm gonna try and zoom in here and show you this guy so",
    "start": "3135060",
    "end": "3143480"
  },
  {
    "text": "here is an example I'm gonna try and you know I'm doing I'm trying to go up on",
    "start": "3143480",
    "end": "3148620"
  },
  {
    "text": "the board and talk but you can't see me um let me just point with the mouse [Music]",
    "start": "3148620",
    "end": "3155600"
  },
  {
    "text": "sorry it's just a little bit tricky trying to zoom in and do all this let me",
    "start": "3155600",
    "end": "3160950"
  },
  {
    "start": "3159000",
    "end": "3589000"
  },
  {
    "text": "just jump back here now I've destroyed this yeah hang on a second sorry everyone I'm just trying to click",
    "start": "3160950",
    "end": "3168030"
  },
  {
    "text": "multiple screens to get to the slide okay yeah actually I did have to say let",
    "start": "3168030",
    "end": "3177810"
  },
  {
    "text": "me what am I do is if you want to you could email me that and what I'll do is",
    "start": "3177810",
    "end": "3183630"
  },
  {
    "text": "I'll send you out that information as part of this actually that's what I'll do I'll add how we do an I am policy and",
    "start": "3183630",
    "end": "3190350"
  },
  {
    "text": "all those extra parts to the slide deck so you've got that extra information",
    "start": "3190350",
    "end": "3196880"
  },
  {
    "text": "this is a good one I got one more question here which is would redshift fit into this process as in from s3",
    "start": "3197589",
    "end": "3204849"
  },
  {
    "text": "stating to redshift I see this is",
    "start": "3204849",
    "end": "3209980"
  },
  {
    "text": "there's two parts to this when we have women moving from staging I really love",
    "start": "3209980",
    "end": "3216819"
  },
  {
    "text": "seeing that we keep the raw data in a raw state inside s3 and there's a couple",
    "start": "3216819",
    "end": "3223030"
  },
  {
    "text": "of reasons for it the first thing is and I'm not saying you don't go to Ritchie so I'm not saying that but I definitely",
    "start": "3223030",
    "end": "3230319"
  },
  {
    "text": "believe that if you save the data in raw in the raw bucket or even when you start",
    "start": "3230319",
    "end": "3235329"
  },
  {
    "text": "optimizing the data by compressing in the indexing a whole range of other things what that allows you to do is",
    "start": "3235329",
    "end": "3242280"
  },
  {
    "text": "provide a real granular level of data for the art of the possible that someone",
    "start": "3242280",
    "end": "3249130"
  },
  {
    "text": "could come in there and and really try and join data like you've never joined",
    "start": "3249130",
    "end": "3254980"
  },
  {
    "text": "before I think that's a really important thing so it's agnostic to the service so if you did want to use sage maker or you",
    "start": "3254980",
    "end": "3261190"
  },
  {
    "text": "wanted to use some of our you know our machine learning or AI services you",
    "start": "3261190",
    "end": "3266319"
  },
  {
    "text": "could do it but then there is a time and we do actually have code for this as",
    "start": "3266319",
    "end": "3272440"
  },
  {
    "text": "well I need to put me some github is an automated process of when the data arrives in the raw location it triggers",
    "start": "3272440",
    "end": "3280779"
  },
  {
    "text": "a secondary lambda function and that lambda function then moves the data from",
    "start": "3280779",
    "end": "3287109"
  },
  {
    "text": "the raw location directly into redshift into a particular table and you won't",
    "start": "3287109",
    "end": "3293319"
  },
  {
    "text": "believe it the code that is written to do that is almost identical to the data",
    "start": "3293319",
    "end": "3298329"
  },
  {
    "text": "like pipeline because the lambda functions are really light touch and they're very granular in what they do",
    "start": "3298329",
    "end": "3304299"
  },
  {
    "text": "but they're they're also quite agnostic so the process of stepping through and",
    "start": "3304299",
    "end": "3309339"
  },
  {
    "text": "moving data from raw to rip from the raw bar code or any bucket to register is go",
    "start": "3309339",
    "end": "3315190"
  },
  {
    "text": "and get the configuration from dynamo for which redshift for which data file",
    "start": "3315190",
    "end": "3320369"
  },
  {
    "text": "don't get the redshift command which will be copied from s3 to redshift and",
    "start": "3320369",
    "end": "3326799"
  },
  {
    "text": "then we've got some parameters for copying from rich from s3 to redshift which includes what",
    "start": "3326799",
    "end": "3333100"
  },
  {
    "text": "is the source format what are the what is the delimiter what are the fields and so on so yeah we definitely see that",
    "start": "3333100",
    "end": "3340420"
  },
  {
    "text": "redshift is an endpoint but also as data is getting larger and larger and larger",
    "start": "3340420",
    "end": "3346780"
  },
  {
    "text": "we're seeing people storing data in redshift that's only potentially 60 days",
    "start": "3346780",
    "end": "3352870"
  },
  {
    "text": "old where maybe three months old and they keep the rest of the data stealing",
    "start": "3352870",
    "end": "3357940"
  },
  {
    "text": "s3 and the reason being is that you're able to bring up the older data the warm",
    "start": "3357940",
    "end": "3363400"
  },
  {
    "text": "data or even the cold data up from s3 using redshift spectrum and you're only",
    "start": "3363400",
    "end": "3370090"
  },
  {
    "text": "paying for the data you query see even though rich of spectrum spins up a set",
    "start": "3370090",
    "end": "3376300"
  },
  {
    "text": "of compute to actually do the movement of data from on from the s3 location up",
    "start": "3376300",
    "end": "3383200"
  },
  {
    "text": "to up to redshift you're only paying for the compute of the querying of the data",
    "start": "3383200",
    "end": "3389110"
  },
  {
    "text": "coming up through the redshift only for that data that needs to come up to me chief to support the query I hope that",
    "start": "3389110",
    "end": "3395080"
  },
  {
    "text": "answers your question okay this is a",
    "start": "3395080",
    "end": "3402070"
  },
  {
    "text": "good one can we use elastic search for doing text searching across all data",
    "start": "3402070",
    "end": "3407080"
  },
  {
    "text": "files like office documents in the data lake or is it just used to aggregate events or reporting for Cabana okay with",
    "start": "3407080",
    "end": "3417430"
  },
  {
    "text": "elastic search and how we index information in it as part of the data",
    "start": "3417430",
    "end": "3424030"
  },
  {
    "text": "Lake because we can put information around the file counts the number of",
    "start": "3424030",
    "end": "3432760"
  },
  {
    "text": "rows all of the file aliases the business operational technical terms we",
    "start": "3432760",
    "end": "3439720"
  },
  {
    "text": "can do searches like this we can go show me where all of the customer data is",
    "start": "3439720",
    "end": "3445300"
  },
  {
    "text": "where we have an ID as a field and it",
    "start": "3445300",
    "end": "3450430"
  },
  {
    "text": "doesn't have PII and what elasticsearch can do it can bring back you can bring",
    "start": "3450430",
    "end": "3456370"
  },
  {
    "text": "back all of the files or the dot or the data file types that match that and then",
    "start": "3456370",
    "end": "3462460"
  },
  {
    "text": "you can drill into that and go okay biking bring back all of the different file types I should be able to do a second",
    "start": "3462460",
    "end": "3468130"
  },
  {
    "text": "query and actually look at what real extended metadata is in there about the",
    "start": "3468130",
    "end": "3473500"
  },
  {
    "text": "file about the file types and where we see people use that a lot is like in a subscription or a self-service model",
    "start": "3473500",
    "end": "3481680"
  },
  {
    "text": "everyone should be able to see the extended metadata and they should be able to see what type of data is",
    "start": "3481680",
    "end": "3488620"
  },
  {
    "text": "available within the organization and then they should be able to go well okay I can see who the data owner is and I",
    "start": "3488620",
    "end": "3495790"
  },
  {
    "text": "can see the security policy applied so what I should be able to do using the",
    "start": "3495790",
    "end": "3501340"
  },
  {
    "text": "API is the istick Search API and together with potentially some one or",
    "start": "3501340",
    "end": "3507220"
  },
  {
    "text": "two other services with AWS which I practiced think about you could send off a request to subscribe to a particular",
    "start": "3507220",
    "end": "3514750"
  },
  {
    "text": "s3 location where the data is that you want to query actually live so that's",
    "start": "3514750",
    "end": "3520120"
  },
  {
    "text": "how we do that if you want to do text searches across all of the data files",
    "start": "3520120",
    "end": "3525900"
  },
  {
    "text": "inside the data like that is a tricky one because if you think of the data",
    "start": "3525900",
    "end": "3532930"
  },
  {
    "text": "like how I explained it earlier about that we're storing that up in domains and then folders and then data",
    "start": "3532930",
    "end": "3540130"
  },
  {
    "text": "partitions we don't really have that wild card search across the whole of the",
    "start": "3540130",
    "end": "3545800"
  },
  {
    "text": "data set so any last questions before we",
    "start": "3545800",
    "end": "3552570"
  },
  {
    "text": "close for today well thank you everyone",
    "start": "3552570",
    "end": "3560500"
  },
  {
    "text": "again for taking the time today to to hear about how we do the accelerated",
    "start": "3560500",
    "end": "3566230"
  },
  {
    "text": "data Lake and thank you for all your questions I really hope it's been valuable and please reach out to me to",
    "start": "3566230",
    "end": "3575440"
  },
  {
    "text": "ask any further questions if you have anything that you want to go deeper in into alright have a great rest of the",
    "start": "3575440",
    "end": "3582220"
  },
  {
    "text": "day bye bye",
    "start": "3582220",
    "end": "3585540"
  }
]