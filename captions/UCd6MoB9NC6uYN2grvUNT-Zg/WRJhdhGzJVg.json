[
  {
    "start": "0",
    "end": "114000"
  },
  {
    "text": "good morning and welcome today we will focus on how do you build consumer",
    "start": "0",
    "end": "7350"
  },
  {
    "text": "focused web services especially consumer focused content based web services for",
    "start": "7350",
    "end": "15420"
  },
  {
    "text": "millions of customers and how do you build a robust content store make it",
    "start": "15420",
    "end": "20460"
  },
  {
    "text": "where you can store tens of billions of files files which have variety of",
    "start": "20460",
    "end": "27119"
  },
  {
    "text": "content type and how do you store the metadata for this variety of content in",
    "start": "27119",
    "end": "33690"
  },
  {
    "text": "a flexible way such that you can find and organize the content for your",
    "start": "33690",
    "end": "39510"
  },
  {
    "text": "customers easily and as your content",
    "start": "39510",
    "end": "45500"
  },
  {
    "text": "continues to double at a rapid pace how do you still provide a consistent customer experience will share with you",
    "start": "45500",
    "end": "54600"
  },
  {
    "text": "how we achieve this today I am collagen Chima software developer manager for",
    "start": "54600",
    "end": "60300"
  },
  {
    "text": "Amazon Cloud Drive services team and with me here today is one of our key",
    "start": "60300",
    "end": "65970"
  },
  {
    "text": "senior developer Kevin Christian today we will share with you how we build and",
    "start": "65970",
    "end": "71010"
  },
  {
    "text": "scaled Amazon Cloud Drive services to serve millions of consumers leveraging",
    "start": "71010",
    "end": "77520"
  },
  {
    "text": "AWS technologies and how we build a robust content store leveraging s3 and",
    "start": "77520",
    "end": "83450"
  },
  {
    "text": "provided unlimited storage to our customers the goal of this session is to",
    "start": "83450",
    "end": "90420"
  },
  {
    "text": "provide some key learnings from our experience and provide you with some",
    "start": "90420",
    "end": "96360"
  },
  {
    "text": "effective design patterns and best practices that you can take and apply if",
    "start": "96360",
    "end": "102090"
  },
  {
    "text": "you're building highly scalable content based web services on top of s3",
    "start": "102090",
    "end": "109610"
  },
  {
    "start": "114000",
    "end": "114000"
  },
  {
    "text": "i'll start by providing an overview of what is Amazon Cloud Drive I'll share",
    "start": "114180",
    "end": "119620"
  },
  {
    "text": "with you some of the key challenges we had and some of the design goals we had and then I'll walk you through the",
    "start": "119620",
    "end": "126960"
  },
  {
    "text": "design architecture which helped us achieve the scaling performance and the",
    "start": "126960",
    "end": "132940"
  },
  {
    "text": "fault tolerance goals we had and then Kevin will come over and he will get",
    "start": "132940",
    "end": "138010"
  },
  {
    "text": "deeper into the content store and share with you how we leveraged s3 where it",
    "start": "138010",
    "end": "144010"
  },
  {
    "text": "worked out of the box and where we face some challenges and we have to do something more custom so what is Amazon",
    "start": "144010",
    "end": "154600"
  },
  {
    "start": "152000",
    "end": "152000"
  },
  {
    "text": "Cloud Drive amazon cloud drive is a consumer cloud storage service where",
    "start": "154600",
    "end": "160209"
  },
  {
    "text": "customers can store any type of content their photos videos document they can",
    "start": "160209",
    "end": "166270"
  },
  {
    "text": "back up their devices or any other content they may have now if you mainly",
    "start": "166270",
    "end": "172180"
  },
  {
    "text": "want to secure your memories and just want to back up your photos then there is unlimited photos subscription plan",
    "start": "172180",
    "end": "180180"
  },
  {
    "text": "which comes with additional 5 GB where you can store any other type of content which is not photos however if you",
    "start": "180180",
    "end": "188230"
  },
  {
    "text": "really want to take more device backups and really want to push more files videos and documents then there is",
    "start": "188230",
    "end": "195040"
  },
  {
    "text": "unlimited everything subscription plan which is probably the right plant for you now you can get started for free for",
    "start": "195040",
    "end": "201850"
  },
  {
    "text": "three months and try them out if you are a amazon prime subscriber you get the",
    "start": "201850",
    "end": "208720"
  },
  {
    "text": "unlimited photos for free it's a it's a great plan and if you want to learn more about these you can go to amazon.com /",
    "start": "208720",
    "end": "215890"
  },
  {
    "text": "cloud drive how do you use Amazon Cloud",
    "start": "215890",
    "end": "222100"
  },
  {
    "text": "Drive we have a web-based app which you can access from all modern browsers on",
    "start": "222100",
    "end": "228370"
  },
  {
    "text": "any device or you can download one of the mobile app on iOS Android or iOS and",
    "start": "228370",
    "end": "236280"
  },
  {
    "text": "it provides great photo auto uploading capabilities and it also provides some",
    "start": "236280",
    "end": "242709"
  },
  {
    "text": "great organization and sharing capabilities as well but if you really want to get a manager",
    "start": "242709",
    "end": "248349"
  },
  {
    "text": "files and push them from your Mac and PCs we have file uploader and downloader apps which you can download on Mac or PC",
    "start": "248349",
    "end": "256840"
  },
  {
    "text": "and again you can go to the website and and see what we have out there so what's",
    "start": "256840",
    "end": "265479"
  },
  {
    "start": "264000",
    "end": "264000"
  },
  {
    "text": "in here for developers and partners well we have restful api s and android and",
    "start": "265479",
    "end": "271870"
  },
  {
    "text": "iOS SDKs these are the same api's and SDKs which we leverage to build our apps",
    "start": "271870",
    "end": "278530"
  },
  {
    "text": "which I just mentioned and if you have some great cool innovative ideas you can",
    "start": "278530",
    "end": "284800"
  },
  {
    "text": "build apps reach millions of customers rapidly and Amazon Cloud Drive is is is",
    "start": "284800",
    "end": "293530"
  },
  {
    "text": "a subscription-based service so there is no free tier so if if your apps help us",
    "start": "293530",
    "end": "300460"
  },
  {
    "text": "reach more customers and design for subscriptions then there are some great",
    "start": "300460",
    "end": "305530"
  },
  {
    "text": "revenue sharing opportunities waiting for you again if you are interested you can go to the URL at the bottom of the",
    "start": "305530",
    "end": "312160"
  },
  {
    "text": "slide slowly we are gaining good",
    "start": "312160",
    "end": "317800"
  },
  {
    "text": "momentum and there are some great third party partners and developers building",
    "start": "317800",
    "end": "323530"
  },
  {
    "text": "some really cool apps there are files which allows you to manage your photos",
    "start": "323530",
    "end": "330010"
  },
  {
    "text": "in a cool interactive way and there are some great file management and syncing",
    "start": "330010",
    "end": "335200"
  },
  {
    "text": "file syncing apps as well as and again we are relatively new in the space so if",
    "start": "335200",
    "end": "342370"
  },
  {
    "text": "you have some cool innovative ideas you can build some apps help us reach more",
    "start": "342370",
    "end": "348460"
  },
  {
    "text": "customers and and there are some great revenue sharing opportunities here for you so now that you have a good sense of",
    "start": "348460",
    "end": "358240"
  },
  {
    "start": "356000",
    "end": "356000"
  },
  {
    "text": "what Amazon Cloud Drive is here are some of the key challenges we faced and if",
    "start": "358240",
    "end": "364660"
  },
  {
    "text": "you are building a highly scalable content based web service you might be facing some similar challenges maybe not",
    "start": "364660",
    "end": "370840"
  },
  {
    "text": "at the same degree level but generally they are pretty sane and really what this comes down to is how",
    "start": "370840",
    "end": "377289"
  },
  {
    "text": "you handle huge volume variety and velocity of content which continues to",
    "start": "377289",
    "end": "382839"
  },
  {
    "text": "grow at a rapid pace and how do you store the related metadata in such a",
    "start": "382839",
    "end": "388330"
  },
  {
    "text": "ways so that if you get a new content type you don't have to change your schemas but still you are able to do",
    "start": "388330",
    "end": "394719"
  },
  {
    "text": "indexing such that you can find organize the content in a meaningful way for your",
    "start": "394719",
    "end": "400960"
  },
  {
    "text": "customers and in our case we get hundreds of millions of service requests",
    "start": "400960",
    "end": "407889"
  },
  {
    "text": "on a daily basis so how do you log these different log all the service requests",
    "start": "407889",
    "end": "414219"
  },
  {
    "text": "which are coming away and how do you build analytics and some monitoring so you can proactively manage your services",
    "start": "414219",
    "end": "422499"
  },
  {
    "text": "and in above all it's not about how you effectively do this but how do you",
    "start": "422499",
    "end": "428349"
  },
  {
    "text": "efficiently do it so you gain the performance and your cost is lower here",
    "start": "428349",
    "end": "436509"
  },
  {
    "start": "434000",
    "end": "434000"
  },
  {
    "text": "are some of the key design goals we had and if you look at this list it probably looks very familiar if you are again",
    "start": "436509",
    "end": "442360"
  },
  {
    "text": "building highly scalable web apps or services probably you have the same list and in our case it came down to really a",
    "start": "442360",
    "end": "449769"
  },
  {
    "text": "few things one was how do you provide consistently great customer experience",
    "start": "449769",
    "end": "455649"
  },
  {
    "text": "as you continue to grow your volume and and you use a base as more and more",
    "start": "455649",
    "end": "461499"
  },
  {
    "text": "users are coming consecutively and hitting your services and in our case",
    "start": "461499",
    "end": "469569"
  },
  {
    "text": "there is also and maybe this is the same case you guys are also dealing with we",
    "start": "469569",
    "end": "475330"
  },
  {
    "text": "have customers who are using different types of mobile devices they are uploading photos from the phones they",
    "start": "475330",
    "end": "481240"
  },
  {
    "text": "want to experience that from tablets and mac and pc so how do you provide near real-time experience to your customers",
    "start": "481240",
    "end": "488110"
  },
  {
    "text": "when they're doing this and again how do you ensure there is no loss of data and",
    "start": "488110",
    "end": "493889"
  },
  {
    "text": "again how do you keep the cost low so",
    "start": "493889",
    "end": "498959"
  },
  {
    "start": "498000",
    "end": "498000"
  },
  {
    "text": "let me share with you how we achieve this and this is a very high level overview architecture design for Amazon",
    "start": "498959",
    "end": "507339"
  },
  {
    "text": "Cloud Drive the key components in here or r 1 is our content store which is built",
    "start": "507339",
    "end": "514310"
  },
  {
    "text": "on top of s three as three provided as the durability the reliability and the",
    "start": "514310",
    "end": "519530"
  },
  {
    "text": "performance we needed and all the related metadata is stored into into a",
    "start": "519530",
    "end": "525500"
  },
  {
    "text": "mirror datastore which is built on top of dynamo DB and dynamo dB provided as",
    "start": "525500",
    "end": "530810"
  },
  {
    "text": "the fast reads and writes for the metadata it also provided the flexible metadata store we were looking for now",
    "start": "530810",
    "end": "538670"
  },
  {
    "text": "for advanced coding capabilities we have a separate indexing engine as well let",
    "start": "538670",
    "end": "545300"
  },
  {
    "text": "me share with you what happens on upload and a download such that you get a good sense how all this comes together so",
    "start": "545300",
    "end": "553520"
  },
  {
    "text": "when a when a customer uses one of our apps and pushes a file through the app",
    "start": "553520",
    "end": "561650"
  },
  {
    "text": "will hit our Amazon Cloud Drive service and the service at that point we'll put",
    "start": "561650",
    "end": "567080"
  },
  {
    "text": "the metadata into the metadata store and put the content in a pending state at",
    "start": "567080",
    "end": "572480"
  },
  {
    "text": "the same time the content will start uploading into s3 and once the content",
    "start": "572480",
    "end": "577910"
  },
  {
    "text": "is fully uploaded we then mark that content available in a marinated store",
    "start": "577910",
    "end": "583280"
  },
  {
    "text": "at the same time when it's available we push the metadata into the Kinesis",
    "start": "583280",
    "end": "590900"
  },
  {
    "text": "stream from there it's picked up by several SQS workers there is SQS worker",
    "start": "590900",
    "end": "598000"
  },
  {
    "text": "which pushes the metadata into our indexing engine there is another one",
    "start": "598000",
    "end": "603440"
  },
  {
    "text": "which pushes and publishes into an alerting engine which is built on top of redshift and then we also send some",
    "start": "603440",
    "end": "610700"
  },
  {
    "text": "downstream notifications to to the services which are integrating with us",
    "start": "610700",
    "end": "616930"
  },
  {
    "text": "like the Kindle services now when the",
    "start": "616930",
    "end": "622040"
  },
  {
    "text": "content is available at the same time we also start some content processing especially for videos and for documents",
    "start": "622040",
    "end": "629330"
  },
  {
    "text": "in case of videos we leverage Amazon Elastic transcoding services to cut some",
    "start": "629330",
    "end": "634910"
  },
  {
    "text": "previews and thumbnails and in case of documents we create some PDF files such",
    "start": "634910",
    "end": "640040"
  },
  {
    "text": "that we can provide preview functionality on any device special on mobile and web to our customers now",
    "start": "640040",
    "end": "648050"
  },
  {
    "text": "if you notice synchronously we just write the content to s3 and the metadata",
    "start": "648050",
    "end": "654769"
  },
  {
    "text": "into dynamo and this helps us to keep low latency xand and we leverage the",
    "start": "654769",
    "end": "661100"
  },
  {
    "text": "scaling and the durability of both s3 and dynamo to for the scalar customers",
    "start": "661100",
    "end": "667610"
  },
  {
    "text": "and everything else is done asynchronously like the populating the indexing engine or processing the",
    "start": "667610",
    "end": "674180"
  },
  {
    "text": "content as well so this is a key design pattern which we leverage to scale and",
    "start": "674180",
    "end": "679310"
  },
  {
    "text": "at the same time process the metadata and the content and provide the near real-time experience which our customers",
    "start": "679310",
    "end": "686269"
  },
  {
    "text": "are looking for this loosely coupled architecture also helps to build more",
    "start": "686269",
    "end": "691339"
  },
  {
    "text": "fault tolerance like if there is a shoe with our indexing engine it doesn't really hinder the upload of file here",
    "start": "691339",
    "end": "699970"
  },
  {
    "text": "now when a customer requests to download a file say in this case let's take a",
    "start": "699970",
    "end": "705949"
  },
  {
    "text": "case of maybe they want to download a photo from their mobile device in that",
    "start": "705949",
    "end": "711139"
  },
  {
    "text": "case the app will hit our service we will go get the meta data from dynamo get the content from s3 if if this image",
    "start": "711139",
    "end": "719959"
  },
  {
    "text": "is really a huge image then we cut the preview of the image into a probe into a",
    "start": "719959",
    "end": "726230"
  },
  {
    "text": "proper device friendly format and we also cash that preview of that image so",
    "start": "726230",
    "end": "734589"
  },
  {
    "text": "smartly cashing on the download path helps us to scale on all the download",
    "start": "734589",
    "end": "740720"
  },
  {
    "text": "requests so overall we rely heavily on",
    "start": "740720",
    "end": "746120"
  },
  {
    "text": "dynamo and s3 for scaling performance and durability and then we do all the",
    "start": "746120",
    "end": "752600"
  },
  {
    "text": "other processing asynchronously which helps us to achieve the goals I mentioned earlier at this point I would",
    "start": "752600",
    "end": "758449"
  },
  {
    "text": "like Kevin to come over and share with you how we leverage s3 what were some of",
    "start": "758449",
    "end": "763760"
  },
  {
    "text": "the challenges we faced and where we have to do something more custom thank you",
    "start": "763760",
    "end": "770500"
  },
  {
    "text": "me back there we go thanks star Lochan",
    "start": "771749",
    "end": "777600"
  },
  {
    "text": "so how do we use s3 in in cloud drive as tarlochan has mentioned it is the",
    "start": "780959",
    "end": "787529"
  },
  {
    "start": "781000",
    "end": "781000"
  },
  {
    "text": "storage it is the persistence engine for our customer content and we have a lot",
    "start": "787529",
    "end": "792699"
  },
  {
    "text": "of customer content but we don't do a whole lot that's very special as far as",
    "start": "792699",
    "end": "797949"
  },
  {
    "text": "how we use s3 to store that data we also generate quite a bit of derived content",
    "start": "797949",
    "end": "804550"
  },
  {
    "text": "so for example when you upload a video it might not be in a format that's playable on all of the mobile devices",
    "start": "804550",
    "end": "811720"
  },
  {
    "text": "that you want to view it on so we will transcode that video when you upload a document similarly if it's a word",
    "start": "811720",
    "end": "818889"
  },
  {
    "text": "document and you want to view it on your phone will transcode it or translate it",
    "start": "818889",
    "end": "824170"
  },
  {
    "text": "into a PDF format so that you can at least view it on all kinds of mobile devices so we have a lot of derived",
    "start": "824170",
    "end": "830170"
  },
  {
    "text": "content most of that goes into s three as well and I'll give you more details for both of those categories of how we",
    "start": "830170",
    "end": "837250"
  },
  {
    "text": "do that we generate a lot of log files because we have a lot of servers running",
    "start": "837250",
    "end": "842500"
  },
  {
    "text": "and I'll talk about one particular path that we use that involves s3 for",
    "start": "842500",
    "end": "847870"
  },
  {
    "text": "processing log files and getting useful data out of them we also use s3 for some",
    "start": "847870",
    "end": "853779"
  },
  {
    "text": "areas that might not immediately come to mind but we found it's a very quick win",
    "start": "853779",
    "end": "859149"
  },
  {
    "text": "for us such as for coordinating changes to dynamic configuration across our",
    "start": "859149",
    "end": "864490"
  },
  {
    "text": "fleet I'll talk about that we back up our metadata is equally important to us",
    "start": "864490",
    "end": "870100"
  },
  {
    "text": "as our as our content if we were to lose the metadata we wouldn't know where to find the content so we're pretty",
    "start": "870100",
    "end": "876339"
  },
  {
    "text": "religious about backing up our DynamoDB tables in case we ever needed to do a",
    "start": "876339",
    "end": "882610"
  },
  {
    "text": "disaster recovery operation but we also find that that's useful when we need to",
    "start": "882610",
    "end": "887949"
  },
  {
    "text": "do things that would involve a table scan on our production table when we",
    "start": "887949",
    "end": "893889"
  },
  {
    "text": "have the dynamo backups in s3 we can operate on those instead and figure out",
    "start": "893889",
    "end": "899319"
  },
  {
    "text": "for example for a backfill operation what files we need to operate on without affecting the the running",
    "start": "899319",
    "end": "906190"
  },
  {
    "text": "production system and all of this s3 interaction that we do we do using the",
    "start": "906190",
    "end": "912459"
  },
  {
    "text": "regular amazon AWS sdk our services are primarily written in Java so there's no",
    "start": "912459",
    "end": "919510"
  },
  {
    "text": "back doors that as an Amazon team we have into s3 that you all don't have we use the standard consumer s3 service",
    "start": "919510",
    "end": "927490"
  },
  {
    "text": "through the standard SDK with one exception that I'll talk about so",
    "start": "927490",
    "end": "935020"
  },
  {
    "start": "934000",
    "end": "934000"
  },
  {
    "text": "customer content we use stant the standard s3 storage options no reduced",
    "start": "935020",
    "end": "940930"
  },
  {
    "text": "redundancy storage we don't use glacier all of our customer content when you",
    "start": "940930",
    "end": "946600"
  },
  {
    "text": "upload a file it goes into a single bucket per region which means that we",
    "start": "946600",
    "end": "951910"
  },
  {
    "text": "have a lot of objects in a single bucket and that's okay because s3 has essentially no limit on the number of",
    "start": "951910",
    "end": "959170"
  },
  {
    "text": "objects that you can store in an s3 bucket where it does have limitations is",
    "start": "959170",
    "end": "964420"
  },
  {
    "text": "on operations that involve similar customer similar key prefixes so in",
    "start": "964420",
    "end": "970870"
  },
  {
    "text": "order to support listing objects out of a bucket by key prefix and some of the",
    "start": "970870",
    "end": "976120"
  },
  {
    "text": "other operations that s3 has it has to partition keys by key prefix and if you",
    "start": "976120",
    "end": "983649"
  },
  {
    "text": "structure your keys in such a way that a particular prefix is being hit harder",
    "start": "983649",
    "end": "989560"
  },
  {
    "text": "than the others you can run into performance problems so in order to avoid that what we do is generate random",
    "start": "989560",
    "end": "995980"
  },
  {
    "text": "key so there should be a completely even spread of operations across all of the",
    "start": "995980",
    "end": "1001290"
  },
  {
    "text": "prefixes of our billions of objects in our content bucket we store that",
    "start": "1001290",
    "end": "1006480"
  },
  {
    "text": "randomly generated key and DynamoDB and so we have to go to DynamoDB anytime we",
    "start": "1006480",
    "end": "1011490"
  },
  {
    "text": "want to download content in order to find out where it is in our content bucket that means that the list",
    "start": "1011490",
    "end": "1017490"
  },
  {
    "text": "operations essentially are not much use for us but that's a price we're willing to pay we also use the s3 server-side",
    "start": "1017490",
    "end": "1026400"
  },
  {
    "text": "encryption option for all of our content so all the files that you upload the",
    "start": "1026400",
    "end": "1032400"
  },
  {
    "text": "cloud drive or our encrypted at rest and they are also encrypted on the move because we use SSL",
    "start": "1032400",
    "end": "1038449"
  },
  {
    "text": "or TLS between all of our various services we generate a lot of log files",
    "start": "1038449",
    "end": "1047659"
  },
  {
    "start": "1044000",
    "end": "1044000"
  },
  {
    "text": "as I mentioned at the moment we've got over 800 servers running across our",
    "start": "1047659",
    "end": "1052730"
  },
  {
    "text": "three regions will go well over a thousand at our peak time of year",
    "start": "1052730",
    "end": "1057760"
  },
  {
    "text": "interestingly our peak time of year is different from the rest of Amazon for the rest of the company it tends to be",
    "start": "1057760",
    "end": "1063650"
  },
  {
    "text": "the Christmas buying season for us it's the Christmas gift opening season so",
    "start": "1063650",
    "end": "1068840"
  },
  {
    "text": "it's Christmas morning when people up when people unwrap their new kindles and",
    "start": "1068840",
    "end": "1074090"
  },
  {
    "text": "their fire TVs and things like that and hook them up to their prime accounts and start to import images from Facebook and",
    "start": "1074090",
    "end": "1080510"
  },
  {
    "text": "whatnot that is where we see our annual peak of service load so at the moment",
    "start": "1080510",
    "end": "1088700"
  },
  {
    "text": "I'm not talking about peak but at the moment we're generating over 200 gigabytes of log files an hour and we",
    "start": "1088700",
    "end": "1095179"
  },
  {
    "text": "deliver those to an internal Amazon service a very simple web service called timber and timber archives those for us",
    "start": "1095179",
    "end": "1101990"
  },
  {
    "text": "it encrypts them first they archives them in s3 and keeps track of and gives us various ways of locating those files",
    "start": "1101990",
    "end": "1108799"
  },
  {
    "text": "and accessing them so I'm going to talk about what we do with those log well",
    "start": "1108799",
    "end": "1114169"
  },
  {
    "text": "first I want to talk about types of log files so that you'll understand what i",
    "start": "1114169",
    "end": "1120200"
  },
  {
    "start": "1117000",
    "end": "1117000"
  },
  {
    "text": "mean when i say service logs so we have normal application logs just like i'm sure your applications do the",
    "start": "1120200",
    "end": "1125600"
  },
  {
    "text": "application writes out a message it gets time-stamped severity tagged and that's",
    "start": "1125600",
    "end": "1131330"
  },
  {
    "text": "very useful for debugging individual service failures it's not very useful",
    "start": "1131330",
    "end": "1137659"
  },
  {
    "text": "for understanding the overall performance of our system for that we rely very heavily on service logs and",
    "start": "1137659",
    "end": "1144370"
  },
  {
    "text": "service logs follow a standard format across all of Amazon it's one record per",
    "start": "1144370",
    "end": "1149809"
  },
  {
    "text": "service invocation and we write a lot of timing information and counters in there and that's where all of our metrics come",
    "start": "1149809",
    "end": "1156260"
  },
  {
    "text": "from service logs feed into our service dashboards so we can watch the real time health of our system and then we archive",
    "start": "1156260",
    "end": "1162830"
  },
  {
    "text": "them off in timber along with our other log files we also save some walk some why logs those quickly become so big as to",
    "start": "1162830",
    "end": "1169970"
  },
  {
    "text": "be of limited use we don't save off any any web access logs for example other",
    "start": "1169970",
    "end": "1177770"
  },
  {
    "text": "forms it's primarily application logs and service logs for us so here's one of",
    "start": "1177770",
    "end": "1184640"
  },
  {
    "text": "the things that we do with our service logs on an hourly basis each of our",
    "start": "1184640",
    "end": "1189650"
  },
  {
    "text": "servers is writing all of those logs to timber timber is encrypting them and storing them in s3 we have a Hadoop",
    "start": "1189650",
    "end": "1197780"
  },
  {
    "text": "cluster that is managed by Amazon EMR elastic MapReduce and for every hours",
    "start": "1197780",
    "end": "1204620"
  },
  {
    "text": "worth of logs we kick off a job on our Hadoop cluster which queries timbered",
    "start": "1204620",
    "end": "1212090"
  },
  {
    "text": "find out where that hours worth of logs are downloads them decrypts them and transforms them into a redshift load",
    "start": "1212090",
    "end": "1219410"
  },
  {
    "text": "file the load file then gets uploaded back into s3 it's encrypted again and",
    "start": "1219410",
    "end": "1226430"
  },
  {
    "text": "then we use redshifts capability to on a",
    "start": "1226430",
    "end": "1232490"
  },
  {
    "text": "massively parallel basis to suck that data into our data warehouse and then that's the basis for our regular",
    "start": "1232490",
    "end": "1238520"
  },
  {
    "text": "reporting and our ad hoc analytics I",
    "start": "1238520",
    "end": "1243220"
  },
  {
    "start": "1244000",
    "end": "1244000"
  },
  {
    "text": "mentioned that we use s3 in a way that might not immediately jump to mind which is for coordinating dynamic",
    "start": "1244300",
    "end": "1250640"
  },
  {
    "text": "configuration we make heavy use of what are sometimes called feature toggles or",
    "start": "1250640",
    "end": "1255920"
  },
  {
    "text": "feature switches ours are actually a little bit more than a switch when we roll out a new feature it's almost",
    "start": "1255920",
    "end": "1262730"
  },
  {
    "text": "always behind a feature toggle and the capabilities that we have include the",
    "start": "1262730",
    "end": "1268280"
  },
  {
    "text": "ability to enable that feature for certain tests customers so we can roll a feature out into production at",
    "start": "1268280",
    "end": "1274640"
  },
  {
    "text": "production scale and production configuration and then test it against our some test accounts that we have when",
    "start": "1274640",
    "end": "1281240"
  },
  {
    "text": "we're happy with the way that that's working we can slowly dial up the use of that feature while we watch our system",
    "start": "1281240",
    "end": "1286760"
  },
  {
    "text": "and make sure there's not any unanticipated problems operating it at scale so we'll start it off at one",
    "start": "1286760",
    "end": "1292400"
  },
  {
    "text": "percent and gradually climb up to a hundred percent we do this with configuration files that we store in s3",
    "start": "1292400",
    "end": "1297800"
  },
  {
    "text": "simply because it's a really easy way to do that so our servers poll our configuration",
    "start": "1297800",
    "end": "1303590"
  },
  {
    "text": "files on a minute-by-minute basis they use the HTTP head operations which is",
    "start": "1303590",
    "end": "1309320"
  },
  {
    "text": "what happens when you call s3s get object metod metadata operation and they",
    "start": "1309320",
    "end": "1315650"
  },
  {
    "text": "just remember the last etag that they saw and if the e-tag changes they read that file and consume the new",
    "start": "1315650",
    "end": "1322580"
  },
  {
    "text": "configuration one thing I want to mention about this approach it works really well for us but it also leaves",
    "start": "1322580",
    "end": "1329990"
  },
  {
    "text": "you once you're dialed up to a hundred percent and you're sure you're not going to be rolling that feature back with a",
    "start": "1329990",
    "end": "1335240"
  },
  {
    "text": "bunch of conditional code in your code base and you need to set aside cycles to",
    "start": "1335240",
    "end": "1342919"
  },
  {
    "text": "remove that because that just becomes technical debt so after a while we pull that conditional code out we get rid of",
    "start": "1342919",
    "end": "1349010"
  },
  {
    "text": "the config file for that particular feature and it becomes a baked in part of our system so I want to talk now",
    "start": "1349010",
    "end": "1360830"
  },
  {
    "start": "1359000",
    "end": "1359000"
  },
  {
    "text": "about a few of the design decisions that we've made along the way or chat challenges that we've run into and the",
    "start": "1360830",
    "end": "1366080"
  },
  {
    "text": "solutions that we've we've come up with for those and the first one is that we deal with a wide variety of file sizes",
    "start": "1366080",
    "end": "1372890"
  },
  {
    "text": "and one-size-fits-all strategy for uploading and processing those files doesn't work very well we've got",
    "start": "1372890",
    "end": "1380330"
  },
  {
    "text": "everything from text files to virtual machine images even within a single category like images which makes up a",
    "start": "1380330",
    "end": "1386299"
  },
  {
    "text": "big part of our content there can be three orders of magnitude size difference in in the sizes of the images",
    "start": "1386299",
    "end": "1393140"
  },
  {
    "text": "we want to maintain fairly linear performance as best we can across all of those file sizes and we want to make",
    "start": "1393140",
    "end": "1399620"
  },
  {
    "text": "sure that if a bunch of large uploads happen to hit one of our servers it doesn't cause resource starvation that",
    "start": "1399620",
    "end": "1406669"
  },
  {
    "text": "adversely affects the other things that that server is doing like managing downloads and content processing and",
    "start": "1406669",
    "end": "1412130"
  },
  {
    "text": "uploads of smaller files so in order to do this our upload logic is size aware",
    "start": "1412130",
    "end": "1418580"
  },
  {
    "text": "when we get a upload request for a file that's 15 megabytes or smaller we do the",
    "start": "1418580",
    "end": "1425150"
  },
  {
    "text": "simplest thing possible we use the s3 put object operation we upload that file",
    "start": "1425150",
    "end": "1431360"
  },
  {
    "text": "in the qwest thread and we do our content processing we do our metadata updates",
    "start": "1431360",
    "end": "1437840"
  },
  {
    "text": "and we return status to the client when we have a larger file that's being uploaded or when we don't know what the",
    "start": "1437840",
    "end": "1444530"
  },
  {
    "text": "size of the file is we switch over and use the Amazon s3 multi-part API so in",
    "start": "1444530",
    "end": "1450530"
  },
  {
    "text": "that case in order to manage resource or starvation we have a thread pool that's",
    "start": "1450530",
    "end": "1456710"
  },
  {
    "text": "dedicated to doing the multi-part uploads in front of that thread pool we have a blocking array so when a",
    "start": "1456710",
    "end": "1463790"
  },
  {
    "text": "particular machine gets overloaded with large file uploads what happens is the thread pool gets consumed the blocking",
    "start": "1463790",
    "end": "1470420"
  },
  {
    "text": "array gets filled up and we start to reject requests for new large file uploads and then that propagates back to",
    "start": "1470420",
    "end": "1477890"
  },
  {
    "text": "the client who can retry and hit another server that's not having that resource problem we are currently uploading using",
    "start": "1477890",
    "end": "1486830"
  },
  {
    "text": "the multi-part API with a fixed five megabyte part size if you know a lot",
    "start": "1486830",
    "end": "1492140"
  },
  {
    "text": "about multi-part then you'll realize that that implies a limitation on the file size that we can handle because you",
    "start": "1492140",
    "end": "1498200"
  },
  {
    "text": "can only have 10,000 parts in an s3 multi-part upload and so that means that",
    "start": "1498200",
    "end": "1503810"
  },
  {
    "text": "we currently have a 50 gigabyte file size limit that's something that has been acceptable to us up to now",
    "start": "1503810",
    "end": "1510080"
  },
  {
    "text": "obviously that's going to change in the future but it's a fairly easy change for us to make we just either have to go",
    "start": "1510080",
    "end": "1515810"
  },
  {
    "text": "with a larger fixed size fixed part size or a dynamic part sighs let me let me",
    "start": "1515810",
    "end": "1526490"
  },
  {
    "text": "hold the questions until the end where we can get you on the mic and we'll talk about that so another aspect of our",
    "start": "1526490",
    "end": "1536690"
  },
  {
    "start": "1532000",
    "end": "1532000"
  },
  {
    "text": "system is that whenever possible we'd like to make the content immediately available and by available I don't just mean the ability to download what you",
    "start": "1536690",
    "end": "1543560"
  },
  {
    "text": "just uploaded but to download thumbnails of it to search for it and find it and",
    "start": "1543560",
    "end": "1549260"
  },
  {
    "text": "whatnot so that's a challenge because some of the content processing that we",
    "start": "1549260",
    "end": "1554630"
  },
  {
    "text": "do takes quite a bit of time so our solution for that is a mix of content",
    "start": "1554630",
    "end": "1560150"
  },
  {
    "text": "processing strategies we sometimes do synchronous processing sometimes asynchronous and sometimes we all call",
    "start": "1560150",
    "end": "1565790"
  },
  {
    "text": "optimus stick synchronous processing so let me give you some examples of that when you",
    "start": "1565790",
    "end": "1571760"
  },
  {
    "text": "upload an image or a video one of the first things that we do is extract the EXIF metadata from it and we store that",
    "start": "1571760",
    "end": "1578000"
  },
  {
    "text": "off for indexing and searching and whatnot that's a pretty quick operation it by and large doesn't depend on the",
    "start": "1578000",
    "end": "1584120"
  },
  {
    "text": "file size because that metadata tends to be up at the at the front of the file so we do that synchronously so when we",
    "start": "1584120",
    "end": "1590450"
  },
  {
    "text": "return a 200 response to you as a result of an upload of a video or an image all",
    "start": "1590450",
    "end": "1596180"
  },
  {
    "text": "that metadata is available in our metadata store in our indexes video",
    "start": "1596180",
    "end": "1602000"
  },
  {
    "text": "transcoding which we do so that you can see your videos on different different devices is quite different it depends",
    "start": "1602000",
    "end": "1609320"
  },
  {
    "text": "heavily on the the size of the video and so and we also don't do that in our own",
    "start": "1609320",
    "end": "1616910"
  },
  {
    "text": "service we rely on amazon elastic transcoder service so that is an",
    "start": "1616910",
    "end": "1622010"
  },
  {
    "text": "asynchronous operation we store the file off in s3 we send ET sa SQS message and",
    "start": "1622010",
    "end": "1629830"
  },
  {
    "text": "usually within a matter of a minute or less they transcode that video into the",
    "start": "1629830",
    "end": "1635090"
  },
  {
    "text": "formats that we request and we have those available in s3 and we get notified using the simple notification",
    "start": "1635090",
    "end": "1641300"
  },
  {
    "text": "service finally we've got a type of",
    "start": "1641300",
    "end": "1647020"
  },
  {
    "text": "processing that we've added fairly recently as we have upped our game on the way we handle documents which is to",
    "start": "1647020",
    "end": "1654680"
  },
  {
    "text": "transform documents to PDF so if you upload a word document from your desktop you can at least read it on all of your devices as a PDF file and we found that",
    "start": "1654680",
    "end": "1662420"
  },
  {
    "text": "the time required to do this is pretty unpredictable it doesn't just depend on the size of the file it doesn't just",
    "start": "1662420",
    "end": "1669020"
  },
  {
    "text": "depend on the format of the file so what we do in an effort to maximize the",
    "start": "1669020",
    "end": "1674870"
  },
  {
    "text": "amount of to minimize the latency for the availability of this PDF transformation is we try it we kick off",
    "start": "1674870",
    "end": "1684290"
  },
  {
    "text": "a synchronous operation to do the transformation if it takes too long we'll kill that return a response to the",
    "start": "1684290",
    "end": "1691160"
  },
  {
    "text": "client and then kick off an asynchronous attempt to do that",
    "start": "1691160",
    "end": "1696250"
  },
  {
    "start": "1696000",
    "end": "1696000"
  },
  {
    "text": "since a lot of our clients are running on mobile devices a lot of our clients have intermittent network connections",
    "start": "1697639",
    "end": "1704039"
  },
  {
    "text": "and the initial versions of our API",
    "start": "1704039",
    "end": "1709679"
  },
  {
    "text": "required uploading files in a single HTTP request response and that's a very",
    "start": "1709679",
    "end": "1714779"
  },
  {
    "text": "difficult thing to do when you've got an intermittent connection or if you have a slow connection so one of the things",
    "start": "1714779",
    "end": "1721230"
  },
  {
    "text": "that we are rolling out is a an ability for clients to resume and upload because",
    "start": "1721230",
    "end": "1728279"
  },
  {
    "text": "well let me back up earlier versions of our system had a multi-part upload API",
    "start": "1728279",
    "end": "1734989"
  },
  {
    "text": "that's very similar in fact it was a kind of a thin layer on top of s3's",
    "start": "1734989",
    "end": "1740519"
  },
  {
    "text": "multi-part API and in speaking with our clients as we were developing new api's",
    "start": "1740519",
    "end": "1746279"
  },
  {
    "text": "one of the things that we heard was that that was a difficult complex series of",
    "start": "1746279",
    "end": "1752129"
  },
  {
    "text": "operations for them to go through they've got to maintain a lot of State on the client side about what parts have been uploaded what parts haven't been",
    "start": "1752129",
    "end": "1758519"
  },
  {
    "text": "uploaded they've got to notify us when they're done we end up with a lot of never completed multi-part uploads it",
    "start": "1758519",
    "end": "1765629"
  },
  {
    "text": "was it was not an ideal situation for for client complexity particularly for",
    "start": "1765629",
    "end": "1770730"
  },
  {
    "text": "happy path operations so in an effort to make that a simpler operation for the",
    "start": "1770730",
    "end": "1777389"
  },
  {
    "text": "happy path and still allow recoverability when we failed we are providing a resumable upload capability",
    "start": "1777389",
    "end": "1784980"
  },
  {
    "text": "and the way that this works is that we tell clients if you want to upload a file we don't care how big it is we",
    "start": "1784980",
    "end": "1791249"
  },
  {
    "text": "don't care how small your pipe is to us or how intermittent the connection is just go ahead and start to send it to us",
    "start": "1791249",
    "end": "1797489"
  },
  {
    "text": "just like you would for a small file if it fails midstream then we save off the",
    "start": "1797489",
    "end": "1803249"
  },
  {
    "text": "information about our ongoing multi-part upload and we give them a way of coming",
    "start": "1803249",
    "end": "1808320"
  },
  {
    "text": "back to us and saying how much did you save and where should I resume and then they can resume the upload just using",
    "start": "1808320",
    "end": "1814619"
  },
  {
    "text": "the standard HTTP content range header and when we go through as many cycles as",
    "start": "1814619",
    "end": "1820200"
  },
  {
    "text": "we need to they're making progress on the upload of this file and whenever we get the last bite then we",
    "start": "1820200",
    "end": "1825800"
  },
  {
    "text": "can complete the multi-part upload and make the content available we ran into a",
    "start": "1825800",
    "end": "1834020"
  },
  {
    "text": "problem implementing this that I want to call out to you because it was quite unexpected to us in fact we didn't",
    "start": "1834020",
    "end": "1840620"
  },
  {
    "text": "realize this until we first tried to exercise this capability in an",
    "start": "1840620",
    "end": "1845660"
  },
  {
    "text": "environment other than a single developers desktop and that is that the s3 multi-part API which is composed of",
    "start": "1845660",
    "end": "1855320"
  },
  {
    "text": "what I think of as a transaction you initiate the upload you upload the parts and then you complete the trip of the",
    "start": "1855320",
    "end": "1861620"
  },
  {
    "text": "upload that needs to be done with a single I am principles credentials our",
    "start": "1861620",
    "end": "1868310"
  },
  {
    "text": "service is running on ec2 instances and we use for for all of our other",
    "start": "1868310",
    "end": "1875230"
  },
  {
    "text": "interaction with AWS services we simply use the instance profile credentials",
    "start": "1875230",
    "end": "1880880"
  },
  {
    "text": "that each of those hosts receives the fact that the thing that we discovered is that that doesn't work for multi-part",
    "start": "1880880",
    "end": "1887930"
  },
  {
    "text": "uploads you can't use instance profile credentials and have a multi-part upload that spans hosts because the principal",
    "start": "1887930",
    "end": "1894350"
  },
  {
    "text": "that's associated with those credentials differs on on each of your ec2 hosts so",
    "start": "1894350",
    "end": "1900050"
  },
  {
    "text": "we talked with the s3 folks about this it's not a very well-known limitation frankly in their API the the URL that",
    "start": "1900050",
    "end": "1908150"
  },
  {
    "text": "I've got on this slide here is a to a blog post and AWS blog post that talks about this a little bit but talking with",
    "start": "1908150",
    "end": "1915440"
  },
  {
    "text": "the s3 team they made a couple of suggestions one was that we could use a security token service to provide",
    "start": "1915440",
    "end": "1922100"
  },
  {
    "text": "credentials for our multi-part uploads or we could use pre-signed URLs we just",
    "start": "1922100",
    "end": "1928160"
  },
  {
    "text": "thought we went with this STS solution so the way that this works is that instead of using our instance profile",
    "start": "1928160",
    "end": "1935570"
  },
  {
    "text": "credentials to initiate upload and complete the multi-part transaction",
    "start": "1935570",
    "end": "1940610"
  },
  {
    "text": "which can now in the resumable world be happening on multiple machines we go off",
    "start": "1940610",
    "end": "1945890"
  },
  {
    "text": "to another service that has a consistent set of I am credentials and we ask it to",
    "start": "1945890",
    "end": "1953450"
  },
  {
    "text": "generate STS tokens for us and then we use those tokens for the multi-part uploads we continue",
    "start": "1953450",
    "end": "1959399"
  },
  {
    "text": "to use instance profile credentials for all of our other AWS service",
    "start": "1959399",
    "end": "1964700"
  },
  {
    "text": "interactions so like our uploads our",
    "start": "1964700",
    "end": "1970889"
  },
  {
    "start": "1967000",
    "end": "1967000"
  },
  {
    "text": "downloads also vary quite a bit in size and we want to maintain reasonable performance we have the same kind of",
    "start": "1970889",
    "end": "1977309"
  },
  {
    "text": "resource starvation concerns when a bunch of large file downloads hit a machine we don't want it to to fall over",
    "start": "1977309",
    "end": "1984379"
  },
  {
    "text": "for the other operations that it's performing and so we also have size",
    "start": "1984379",
    "end": "1989399"
  },
  {
    "text": "aware download logic so if you ask for a download of a fairly small file and our",
    "start": "1989399",
    "end": "1996570"
  },
  {
    "text": "boundary is currently at five megabytes we go to s3 into a single get object again we do that we're the simplest way",
    "start": "1996570",
    "end": "2003829"
  },
  {
    "text": "possible we do it on the request thread we do see intermittent failures Network",
    "start": "2003829",
    "end": "2010639"
  },
  {
    "text": "oriented network related intermittent failures even sitting potentially in the",
    "start": "2010639",
    "end": "2017809"
  },
  {
    "text": "same data center with s3 so we do retry those downloads one time and this simple",
    "start": "2017809",
    "end": "2025039"
  },
  {
    "text": "approach covers about ninety percent of our customers files for larger files we",
    "start": "2025039",
    "end": "2030919"
  },
  {
    "text": "have implemented a parallel download capability now this is not something that comes out of the box today in the",
    "start": "2030919",
    "end": "2037869"
  },
  {
    "text": "s3 SDK you can you can go out to github and find third party or open source",
    "start": "2037869",
    "end": "2045519"
  },
  {
    "text": "implementations of parallel download capabilities for s3 we wrote our own using apache HTTP client so we download",
    "start": "2045519",
    "end": "2053750"
  },
  {
    "text": "these large files in order to maximize the use of our bandwidth between s3 and our ec2 instances in five megabytes",
    "start": "2053750",
    "end": "2061878"
  },
  {
    "text": "using range requests that s3 supports so we generate an s3 pre-signed URL and",
    "start": "2061879",
    "end": "2069519"
  },
  {
    "text": "hand that off to a dedicated thread pool and this works a lot like it does for our large file uploads we have a single",
    "start": "2069519",
    "end": "2076009"
  },
  {
    "text": "thread pool dedicated to this so we can limit the resources that get consumed we have a blocking queue in front of it so",
    "start": "2076009",
    "end": "2082309"
  },
  {
    "text": "if too many large downloads hit one box those resources get consumed and we can",
    "start": "2082309",
    "end": "2087888"
  },
  {
    "text": "fail back to the client and other parts of other things that the service is doing or unaffected this allows us with this",
    "start": "2087889",
    "end": "2095600"
  },
  {
    "text": "with apache HTTP client to have a connection pool and to reuse connections",
    "start": "2095600",
    "end": "2100790"
  },
  {
    "text": "so we save a lot of cycles on the setup and teardown of our connections with s3 because that all that these connections",
    "start": "2100790",
    "end": "2108860"
  },
  {
    "text": "are doing is range requests downloads so",
    "start": "2108860",
    "end": "2115930"
  },
  {
    "start": "2113000",
    "end": "2113000"
  },
  {
    "text": "we've talked a lot about thumbnails but",
    "start": "2115930",
    "end": "2120950"
  },
  {
    "text": "one of the things that I'm not sure that we've made clear is that for images most of our thumbnails are generated on the",
    "start": "2120950",
    "end": "2126830"
  },
  {
    "text": "fly so we do this to keep our service as simple as possible we don't try to save",
    "start": "2126830",
    "end": "2132710"
  },
  {
    "text": "off thumbnails in s3 or any other storage system and look them up we get",
    "start": "2132710",
    "end": "2137780"
  },
  {
    "text": "thumbnail requests in various sizes from various clients so it makes sense for us",
    "start": "2137780",
    "end": "2143150"
  },
  {
    "text": "in most cases to simply cut a thumbnail in with exactly the parameters that are requested on request and that works out",
    "start": "2143150",
    "end": "2151250"
  },
  {
    "text": "well for for most images if you take an image from your cell phone that's a few megabytes in size it really doesn't cost",
    "start": "2151250",
    "end": "2157160"
  },
  {
    "text": "us that much to download the entire image from s3 cut the thumbnail and serve it out as a 256 x 256 image it",
    "start": "2157160",
    "end": "2165050"
  },
  {
    "text": "does cost us a lot though when we start talking about larger files in that case the expense comes in in two areas we've",
    "start": "2165050",
    "end": "2171740"
  },
  {
    "text": "got a large file to download from s3 and we've got a large file to cut a thumbnail from so one of the things that",
    "start": "2171740",
    "end": "2178280"
  },
  {
    "text": "we have added to our system is the use of another s3 bucket basically as an",
    "start": "2178280",
    "end": "2184130"
  },
  {
    "text": "infinite cash for thumbnails so when we get a request for a thumbnail from a",
    "start": "2184130",
    "end": "2189950"
  },
  {
    "text": "large image the first time will download that from our content bucket and cut it",
    "start": "2189950",
    "end": "2195290"
  },
  {
    "text": "down to an intermediate size of I think it's 2k by 2k and in jpg format and then",
    "start": "2195290",
    "end": "2205360"
  },
  {
    "text": "we'll cut that we will save that off into a separate bucket that we have where we've applied an expiration policy",
    "start": "2205360",
    "end": "2212690"
  },
  {
    "text": "so we have a 48-hour expiration policy on this intermediate size thumbnail will",
    "start": "2212690",
    "end": "2217880"
  },
  {
    "text": "cut it down to the final size and deliver the response to the client the next time that they call asking for that",
    "start": "2217880",
    "end": "2223430"
  },
  {
    "text": "same thumbnail or thumbnail based actually any thumbnail based off of that same image will go off and get the the 2k by 2k image from the",
    "start": "2223430",
    "end": "2231530"
  },
  {
    "text": "thumbnail bucket and save about ninety percent of the load bandwidth from the",
    "start": "2231530",
    "end": "2237830"
  },
  {
    "text": "content bucket so we do this for any",
    "start": "2237830",
    "end": "2243350"
  },
  {
    "text": "standard image formats there over about 10 megabyte in size and we do it for",
    "start": "2243350",
    "end": "2248480"
  },
  {
    "text": "other image formats which can take quite a while to cut thumbnails from primarily raw images so we're cashing those in a",
    "start": "2248480",
    "end": "2256880"
  },
  {
    "text": "separate s3 bucket with an expiration policy and let me let me mention what our key is because as I as I mentioned",
    "start": "2256880",
    "end": "2263960"
  },
  {
    "text": "before keys particular key prefixes are important we use the keep we use a key",
    "start": "2263960",
    "end": "2269090"
  },
  {
    "text": "that's generated from the customer ID the image ID and the image version and then we hash those things if we didn't",
    "start": "2269090",
    "end": "2275960"
  },
  {
    "text": "hatch them if we just used a concatenation of those things then a bunch of requests from a single customer",
    "start": "2275960",
    "end": "2281300"
  },
  {
    "text": "could cause us a hot key problem so so we just generate a cache and the reason",
    "start": "2281300",
    "end": "2286370"
  },
  {
    "text": "that we've chose those things is that they are enough to uniquely identify an image in s3 and they all come in in",
    "start": "2286370",
    "end": "2293570"
  },
  {
    "text": "their thumbnail requests so we don't have to make a trip to DynamoDB in order to pull that metadata out and the final",
    "start": "2293570",
    "end": "2304100"
  },
  {
    "start": "2301000",
    "end": "2301000"
  },
  {
    "text": "challenge I want to want to mention is what we do with large downloads in",
    "start": "2304100",
    "end": "2309740"
  },
  {
    "text": "previous versions of our system we we did a lot of this actually where we would generate a pre-signed as three URL",
    "start": "2309740",
    "end": "2317030"
  },
  {
    "text": "and hand it off to our clients and let them interact and let s three do the heavy lifting of moving the bikes around",
    "start": "2317030",
    "end": "2323200"
  },
  {
    "text": "that was a less than ideal situation for us as a service provider because it",
    "start": "2323200",
    "end": "2328820"
  },
  {
    "text": "didn't give us a lot of visibility into how clients were using our service we knew when they asked us for an s3 URL",
    "start": "2328820",
    "end": "2335150"
  },
  {
    "text": "but we didn't know what kind of retries and what kind of failures they were running into so we made a very conscious",
    "start": "2335150",
    "end": "2341780"
  },
  {
    "text": "decision with with current generation of cloud drive to become an intermediary between the content and s3 and our",
    "start": "2341780",
    "end": "2349220"
  },
  {
    "text": "clients in most cases so we could do things like generate thumbnails on the fly however that doesn't work in all",
    "start": "2349220",
    "end": "2356030"
  },
  {
    "text": "situations and in particular we have really large files files that we're not going to perform any translations on it any transformations",
    "start": "2356030",
    "end": "2363340"
  },
  {
    "text": "on on the fly files that there's no read for us no reason for us to write to disk on our servers before sending it back to",
    "start": "2363340",
    "end": "2370390"
  },
  {
    "text": "the client we will for compatible clients generate a pre-signed short-lived s3 priests on URL and just",
    "start": "2370390",
    "end": "2378400"
  },
  {
    "text": "let them go directly to s3 for downloads",
    "start": "2378400",
    "end": "2383609"
  },
  {
    "text": "so I want to leave you with a few takeaways one of them is that s 3 is a",
    "start": "2385620",
    "end": "2392020"
  },
  {
    "start": "2386000",
    "end": "2386000"
  },
  {
    "text": "very flexible system it's it's such a simple API that it lends itself to a lot of different use cases so we use it for",
    "start": "2392020",
    "end": "2398410"
  },
  {
    "text": "the obvious things the big data things like customer content and log files but",
    "start": "2398410",
    "end": "2403480"
  },
  {
    "text": "we also use it for them some things that you might not immediately think of s3 as a candidate for we use it for basically",
    "start": "2403480",
    "end": "2409930"
  },
  {
    "text": "an infinitely sized cash where with a very simple eviction policy we use it",
    "start": "2409930",
    "end": "2416110"
  },
  {
    "text": "for coordinating configuration across across our hosts so keep your keep your mind open as you're working with the AWS",
    "start": "2416110",
    "end": "2422560"
  },
  {
    "text": "services and rely on the the simplicity of s3 to do things that might not",
    "start": "2422560",
    "end": "2427600"
  },
  {
    "text": "immediately jump to mind the selection of keys is important I've told you what we do with keys and if you're going to",
    "start": "2427600",
    "end": "2433390"
  },
  {
    "text": "have high volumes of API operations against s3 it's important that you do the same realize that your upload and",
    "start": "2433390",
    "end": "2440650"
  },
  {
    "text": "download strategies can't be one size fits all well it depends on your service but for us they can't be one size fits",
    "start": "2440650",
    "end": "2446770"
  },
  {
    "text": "all fits all so we have to be put a little more smarts in the service as far",
    "start": "2446770",
    "end": "2453460"
  },
  {
    "text": "as how it deals with uploading and downloading different kinds of content in different sizes of content something",
    "start": "2453460",
    "end": "2459430"
  },
  {
    "text": "I haven't talked a whole lot about is remember the first fallacy of distributed computing the network is",
    "start": "2459430",
    "end": "2464860"
  },
  {
    "text": "reliable the network isn't reliable and we look through our log files we see regular intermittent failures in our",
    "start": "2464860",
    "end": "2471910"
  },
  {
    "text": "interactions with s3 and with all the other services that we talk to as well and what we have arrived at doing is a",
    "start": "2471910",
    "end": "2481060"
  },
  {
    "text": "single retry which is going to work in almost all of the cases of intermittent failures and then simply failing back to",
    "start": "2481060",
    "end": "2488560"
  },
  {
    "text": "the client the problem with more retry policies is that we're just one",
    "start": "2488560",
    "end": "2493890"
  },
  {
    "text": "service in a layer of services and if every service is doing five retries with exponential back-off you get a",
    "start": "2493890",
    "end": "2500430"
  },
  {
    "text": "multiplicative effect when you have a hard failure and it takes a long time to return the final failure to the client",
    "start": "2500430",
    "end": "2507270"
  },
  {
    "text": "who's going to have the final say about retries anyway and those those long running operations can cause resource",
    "start": "2507270",
    "end": "2514410"
  },
  {
    "text": "starvation problems so before we learn this lesson we did see some problems with hard failures causing us to run out",
    "start": "2514410",
    "end": "2522180"
  },
  {
    "text": "of threads for example on servers so keep that in mind and be smart about what your retry policies are going to",
    "start": "2522180",
    "end": "2530880"
  },
  {
    "start": "2530000",
    "end": "2530000"
  },
  {
    "text": "hand it back to tarlochan for some final thoughts thank you Kevin so if if you",
    "start": "2530880",
    "end": "2543300"
  },
  {
    "text": "haven't already tried Amazon Cloud Drive try it like I mentioned there's some great subscription plants you can try it",
    "start": "2543300",
    "end": "2550980"
  },
  {
    "text": "for free for three months and if you're a amazon prime subscriber you get",
    "start": "2550980",
    "end": "2555990"
  },
  {
    "text": "unlimited photos for free if you have some cool ideas build some apps leveraging our api and sdk there's some",
    "start": "2555990",
    "end": "2563550"
  },
  {
    "text": "great revenue sharing opportunities and you can reach millions of customers well",
    "start": "2563550",
    "end": "2570600"
  },
  {
    "text": "thank you all for coming and remember to put your evals myself and Kevin will be",
    "start": "2570600",
    "end": "2577320"
  },
  {
    "text": "here if you have any questions thank you",
    "start": "2577320",
    "end": "2581570"
  }
]