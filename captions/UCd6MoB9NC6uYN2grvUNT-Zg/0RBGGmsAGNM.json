[
  {
    "start": "0",
    "end": "51000"
  },
  {
    "text": "good afternoon everybody quick voice check you guys can hear me in the back",
    "start": "110",
    "end": "5240"
  },
  {
    "text": "great awesome well thank you for coming to this presentation and good afternoon",
    "start": "5240",
    "end": "10610"
  },
  {
    "text": "my name is Nathan Kapoor I'm a product manager with the Amazon ec2 team and",
    "start": "10610",
    "end": "16850"
  },
  {
    "text": "really excited to share some of the details around one of our latest instances p3 which currently are the",
    "start": "16850",
    "end": "24210"
  },
  {
    "text": "most powerful GPU computing platform out in the industry co-presenting with me is",
    "start": "24210",
    "end": "30960"
  },
  {
    "text": "going to be Nick and Alfredo with Airbnb these gentlemen are part of the ML",
    "start": "30960",
    "end": "38850"
  },
  {
    "text": "infrastructure team and they will share with us how Airbnb is using machine",
    "start": "38850",
    "end": "44129"
  },
  {
    "text": "learning for their applications and how does p3 I've got to fit into the overall",
    "start": "44129",
    "end": "49590"
  },
  {
    "text": "picture to get started this slide I",
    "start": "49590",
    "end": "54899"
  },
  {
    "start": "51000",
    "end": "89000"
  },
  {
    "text": "created it actually before our keynotes this morning and so it needs to be",
    "start": "54899",
    "end": "60390"
  },
  {
    "text": "updated but overall you know ec2 we have a broad portfolio of instances ranging",
    "start": "60390",
    "end": "66240"
  },
  {
    "text": "from general-purpose T 2 and 4 and now and five and we have C 4 and C 5",
    "start": "66240",
    "end": "74159"
  },
  {
    "text": "instances for compute optimize and over the last few years for five years to be",
    "start": "74159",
    "end": "79619"
  },
  {
    "text": "specific we have been steadily investing in a category of instances of what we",
    "start": "79619",
    "end": "85049"
  },
  {
    "text": "call as the accelerated computing instances so so let's talk a little bit deeper about our current portfolio for",
    "start": "85049",
    "end": "93060"
  },
  {
    "start": "89000",
    "end": "194000"
  },
  {
    "text": "these for these type of instances and essentially these instances have some form of a hardware accelerator that",
    "start": "93060",
    "end": "100320"
  },
  {
    "text": "complements the V CPUs and memory that are available as a part of that instance so p2 instances we actually launched",
    "start": "100320",
    "end": "108630"
  },
  {
    "text": "them September of last year they are based on Nvidia's k80",
    "start": "108630",
    "end": "114060"
  },
  {
    "text": "accelerator and they actually are useful for a really wide set of compute GPU",
    "start": "114060",
    "end": "120960"
  },
  {
    "text": "compute applications such as machine learning and deep learning you know high",
    "start": "120960",
    "end": "126000"
  },
  {
    "text": "performance applications you know financial computing and bass processing and rendering applications our g3",
    "start": "126000",
    "end": "133440"
  },
  {
    "text": "instance we actually launched it earlier this year about four or five months back it",
    "start": "133440",
    "end": "138650"
  },
  {
    "text": "based on an m60 GPU and it's actually targeted towards graphics workloads such",
    "start": "138650",
    "end": "146030"
  },
  {
    "text": "as 3d rendering virtualization VR video encoding and decoding and one of our",
    "start": "146030",
    "end": "153710"
  },
  {
    "text": "newer editions you know in this portfolio is f1 instance which we announced reinvent last year and made",
    "start": "153710",
    "end": "162140"
  },
  {
    "text": "generally available in April this year and f1 instance provides access to FPGAs",
    "start": "162140",
    "end": "169660"
  },
  {
    "text": "with that instance and a few GS as to quickly recap give you the ability to",
    "start": "169660",
    "end": "175330"
  },
  {
    "text": "you know customize the acceleration by targeting in a custom design hardware",
    "start": "175330",
    "end": "181390"
  },
  {
    "text": "and these these instances good for applications such as financial computing",
    "start": "181390",
    "end": "187360"
  },
  {
    "text": "genomics accelerated search data analytics and image processing so one of",
    "start": "187360",
    "end": "194750"
  },
  {
    "start": "194000",
    "end": "349000"
  },
  {
    "text": "the common questions that I get you know with this portfolio from customers and",
    "start": "194750",
    "end": "201020"
  },
  {
    "text": "partners is around the different type of compute options we are we have and that",
    "start": "201020",
    "end": "206660"
  },
  {
    "text": "are available in the industry in general and how they compare against each other so we want to take a minute or so to",
    "start": "206660",
    "end": "212660"
  },
  {
    "text": "kind of talk through these compute options that are available so generally speaking at a high level we have CPUs",
    "start": "212660",
    "end": "219950"
  },
  {
    "text": "GPUs and FPGAs that can be used for various forms of computational workloads",
    "start": "219950",
    "end": "225590"
  },
  {
    "text": "right so as we all know CPUs you know they've been around for a while",
    "start": "225590",
    "end": "231830"
  },
  {
    "text": "you usually have tens in some cases hundreds of processing cores and they",
    "start": "231830",
    "end": "237290"
  },
  {
    "text": "actually come with the predefined instruction set besides that they also have predefined data path for example",
    "start": "237290",
    "end": "244130"
  },
  {
    "text": "the support you know floating-point 64 bit numeric data type or FP 32 so they",
    "start": "244130",
    "end": "250010"
  },
  {
    "text": "come to these fixed data types and fixed instruction sets and they're optimized for general-purpose computing so again",
    "start": "250010",
    "end": "256280"
  },
  {
    "text": "really wide range of applications that they're suitable for GPUs on the other",
    "start": "256280",
    "end": "262070"
  },
  {
    "text": "hand usually have thousands of processing cores and they come it's similar to CPU they come with",
    "start": "262070",
    "end": "269240"
  },
  {
    "text": "predefined instruction sets and predefined data path width right and just by the nature having thousands of",
    "start": "269240",
    "end": "276979"
  },
  {
    "text": "these processing cores they're highly effective at parallel execution and over",
    "start": "276979",
    "end": "282139"
  },
  {
    "text": "the next few slides I'll actually give an example of an specific operation type",
    "start": "282139",
    "end": "287740"
  },
  {
    "text": "that benefit greatly from GPU acceleration FPGA is on the other hand",
    "start": "287740",
    "end": "293259"
  },
  {
    "text": "are actually you know hardware devices that actually need to be programmed they",
    "start": "293259",
    "end": "298699"
  },
  {
    "text": "provide access to millions of programmable logic cells they don't come",
    "start": "298699",
    "end": "305270"
  },
  {
    "text": "with an instruction set if you have to program them and similarly they don't come with a predefined data path width",
    "start": "305270",
    "end": "311599"
  },
  {
    "text": "if you want to use 8 bits or 7 bits or 5 bits to kind of represent your number",
    "start": "311599",
    "end": "317150"
  },
  {
    "text": "you can do that you can actually program the FPGA to kind of support the data type and one of the big differences",
    "start": "317150",
    "end": "323360"
  },
  {
    "text": "between FPGAs and GPUs and CPUs FPGA is the execution is actually Hardware timed",
    "start": "323360",
    "end": "329870"
  },
  {
    "text": "so there's a concept of a clock and every clock tick translates to an",
    "start": "329870",
    "end": "335030"
  },
  {
    "text": "execution of the actual FPGA engine so again at a high level this kind of summarizes between CPUs GPUs and a few G",
    "start": "335030",
    "end": "342650"
  },
  {
    "text": "is what what what compute options are generally out there and how they how they compare against each other so",
    "start": "342650",
    "end": "350150"
  },
  {
    "start": "349000",
    "end": "417000"
  },
  {
    "text": "talking a little bit about p3 so p3 instances they're based on invidious latest Tesla V 100 GPU it's based on",
    "start": "350150",
    "end": "359240"
  },
  {
    "text": "their Volta architecture so these in instances are currently industry's most",
    "start": "359240",
    "end": "364460"
  },
  {
    "text": "powerful GPU based platforms available and they provide up to one petaflop of",
    "start": "364460",
    "end": "371169"
  },
  {
    "text": "computational performance in a single instance so that's just staggering performance right there as compared to",
    "start": "371169",
    "end": "378110"
  },
  {
    "text": "p2 and again I'll share more details around these specific performance",
    "start": "378110",
    "end": "383300"
  },
  {
    "text": "improvements machine learning applications or applications in general that can take advantage of",
    "start": "383300",
    "end": "388719"
  },
  {
    "text": "floating-point 16-bit numeric data type will actually see a 14x performance",
    "start": "388719",
    "end": "394190"
  },
  {
    "text": "improvement over P 2's high-performance applications computing applications",
    "start": "394190",
    "end": "400690"
  },
  {
    "text": "that are you know sensitive to numeric data type and precision and that use FP",
    "start": "400690",
    "end": "406270"
  },
  {
    "text": "64 will actually see about a 2.6 X performance improvement over P 2 and",
    "start": "406270",
    "end": "412930"
  },
  {
    "text": "once again I'm gonna get into details around these comparisons talking a",
    "start": "412930",
    "end": "418030"
  },
  {
    "start": "417000",
    "end": "527000"
  },
  {
    "text": "little bit about the the Tesla V 100 GPU in detail the GPU is based on a twelve",
    "start": "418030",
    "end": "424840"
  },
  {
    "text": "nanometer manufacturing process it's got over twenty billion transistors on it which is actually 40 percent",
    "start": "424840",
    "end": "430810"
  },
  {
    "text": "higher the Nvidia's previous generation GPU at the same time it is actually 50%",
    "start": "430810",
    "end": "439150"
  },
  {
    "text": "more power efficient so what that means is although NVIDIA has increased the number of transistors that are that they",
    "start": "439150",
    "end": "444880"
  },
  {
    "text": "have packed into the GPU they've also optimized the power of fishin see so essentially in the same",
    "start": "444880",
    "end": "449920"
  },
  {
    "text": "power envelope they have actually packed in more performance and this really comes through in some of the performance",
    "start": "449920",
    "end": "456040"
  },
  {
    "text": "benchmarks they have added a new functionality called as tensor course and again I'll get into details around",
    "start": "456040",
    "end": "463870"
  },
  {
    "text": "what these tensor cores are and how they help with with boosting mixed precision",
    "start": "463870",
    "end": "469540"
  },
  {
    "text": "performance there are six hundred and forty tensor cores that are available on a per GPU basis and that translates to",
    "start": "469540",
    "end": "478600"
  },
  {
    "text": "125 teraflops of mix position performance for GPU to count to round",
    "start": "478600",
    "end": "484720"
  },
  {
    "text": "out some of the details around this GPU from Nvidia it provides their second",
    "start": "484720",
    "end": "491110"
  },
  {
    "text": "generation and ulink GPGPU to GPU communication link which provides up to",
    "start": "491110",
    "end": "498910"
  },
  {
    "text": "300 gigabytes per second of total throughput for energy PU communication again I'll go get into details on what",
    "start": "498910",
    "end": "506110"
  },
  {
    "text": "that really means for applications that you guys are working on and then sixteen gigabytes of GPU memory per GPU with",
    "start": "506110",
    "end": "513640"
  },
  {
    "text": "peak memory bandwidth of 900 gigabytes per second so again a really performant GPU really high performance EP from from",
    "start": "513640",
    "end": "521349"
  },
  {
    "text": "Nvidia now to kind of walk through walk",
    "start": "521350",
    "end": "528460"
  },
  {
    "text": "through how GPUs really you know help boost performance I want",
    "start": "528460",
    "end": "535540"
  },
  {
    "text": "to use the I want to use this example here where I'm this example is",
    "start": "535540",
    "end": "541450"
  },
  {
    "text": "representing a matrix multiply of two matrices that are three by three in size",
    "start": "541450",
    "end": "547020"
  },
  {
    "text": "and and as you can see on matrix C that kind of represents how how you would",
    "start": "547020",
    "end": "553600"
  },
  {
    "text": "calculate the result matrix C so just walking through the actual calculation",
    "start": "553600",
    "end": "559980"
  },
  {
    "text": "c11 which is the first cell in the matrix C is going to be calculated as as I've",
    "start": "559980",
    "end": "566170"
  },
  {
    "text": "depicted in the slide where you're multiplying elements in the first row of matrix a with with the elements in the",
    "start": "566170",
    "end": "572320"
  },
  {
    "text": "first column of matrix B adding them up together to kind of come up with the",
    "start": "572320",
    "end": "577990"
  },
  {
    "text": "actual resulting c11 result now if you were doing this operation on a CPU",
    "start": "577990",
    "end": "585840"
  },
  {
    "text": "versus trying to farm it out of our GPU for a 3x3 matrix size you probably won't",
    "start": "585840",
    "end": "592360"
  },
  {
    "text": "see a performance difference so it's not really advantage advantageous for a matrix of this size but if you scale out",
    "start": "592360",
    "end": "601030"
  },
  {
    "text": "where your matrix are 1024 by 1024 like",
    "start": "601030",
    "end": "606670"
  },
  {
    "text": "1k by 1k matrix and you multiply that matrix with another 1k by 1k matrix then",
    "start": "606670",
    "end": "613390"
  },
  {
    "text": "these multiply and accumulate operations really add up quickly in the total number of operations you need to do and",
    "start": "613390",
    "end": "619300"
  },
  {
    "text": "this is where GPUs can come in and really help out right so I can mention previously you have thousands of these",
    "start": "619300",
    "end": "625300"
  },
  {
    "text": "cores that you're disposable at at your disposal in the GPU and you could actually farm out at these multiply and",
    "start": "625300",
    "end": "631570"
  },
  {
    "text": "Okuma accumulate operations over these thousands of cores you have available get the results back quickly and and",
    "start": "631570",
    "end": "639160"
  },
  {
    "text": "achieve pretty significant performance boosts over a CPU based implementation another important data type another",
    "start": "639160",
    "end": "645820"
  },
  {
    "start": "644000",
    "end": "676000"
  },
  {
    "text": "important factor consider specifically when using GPUs for computational",
    "start": "645820",
    "end": "650880"
  },
  {
    "text": "enhancements is around the numeric precision that is used in these kind of",
    "start": "650880",
    "end": "657010"
  },
  {
    "text": "operations right so you have you know you have FB 64 that represents",
    "start": "657010",
    "end": "663590"
  },
  {
    "text": "64 bits used to represent a floating-point number then you have single position raises FP 32 and then",
    "start": "663590",
    "end": "670550"
  },
  {
    "text": "obviously you have FP 16 which actually represents you know 16-bit floating-point numbers and the reason",
    "start": "670550",
    "end": "676850"
  },
  {
    "start": "676000",
    "end": "813000"
  },
  {
    "text": "why that is important is because of the tensor course that have been made available in this new GPU so in addition",
    "start": "676850",
    "end": "684140"
  },
  {
    "text": "to the over 5,000 cuda cores that are available in this GPU there are 640",
    "start": "684140",
    "end": "691160"
  },
  {
    "text": "dedicated tensor course like I mentioned earlier and each tensor core is actually",
    "start": "691160",
    "end": "697640"
  },
  {
    "text": "designed to to to to process the operation that I've described as D is",
    "start": "697640",
    "end": "704900"
  },
  {
    "text": "equal to a multiplied by B times added with C so again if you go back to the",
    "start": "704900",
    "end": "710330"
  },
  {
    "text": "matrix multiply example that I gave earlier this is where these these new",
    "start": "710330",
    "end": "715580"
  },
  {
    "text": "tensor cores like really step up where they're dedicated for operations that involve multiplying accumulate across",
    "start": "715580",
    "end": "723500"
  },
  {
    "text": "these smaller these data types around F V 16 so again so so that's essentially",
    "start": "723500",
    "end": "731240"
  },
  {
    "text": "that one of the core functionalities of the tensor course with the performance",
    "start": "731240",
    "end": "737510"
  },
  {
    "text": "advantage that you actually end up seeing it really comes through so in the two charts that I have the one that is",
    "start": "737510",
    "end": "743870"
  },
  {
    "text": "on the left it's actually comparing F P 32 or the single precision performance",
    "start": "743870",
    "end": "751610"
  },
  {
    "text": "of the v100 GPU as compared to invidious previous generation GPA which is the P",
    "start": "751610",
    "end": "757850"
  },
  {
    "text": "100 and the to the set of columns you're seeing at the bottom I'm not sure if",
    "start": "757850",
    "end": "763280"
  },
  {
    "text": "it's coming through on the screen but you know it's the matrix size representing so so if you look at the",
    "start": "763280",
    "end": "768590"
  },
  {
    "text": "second set of columns for a 1k by 1k matrix multiply the V 100 provides",
    "start": "768590",
    "end": "775730"
  },
  {
    "text": "almost a 2x performance improvement over the P 100 so that's that's good you know",
    "start": "775730",
    "end": "781400"
  },
  {
    "text": "2x is pretty good but on the other side if you look at the other graph that is towards the middle of the the slide that",
    "start": "781400",
    "end": "788690"
  },
  {
    "text": "is showing when if you switch down to FP 16 the kind of performance boosts you",
    "start": "788690",
    "end": "794150"
  },
  {
    "text": "actually get so these ten these tensor cores really really step up when it comes to",
    "start": "794150",
    "end": "799500"
  },
  {
    "text": "trying to perform FP 16 based numeric operations where they're staying close",
    "start": "799500",
    "end": "805470"
  },
  {
    "text": "to 10 X 9 X performance who improvement over the Pascal 100 GPU so bringing this",
    "start": "805470",
    "end": "814500"
  },
  {
    "start": "813000",
    "end": "912000"
  },
  {
    "text": "back to ec2 and the instances we have P 2 instances that we launched September",
    "start": "814500",
    "end": "820440"
  },
  {
    "text": "of last year use the kad accelerator which is based on their Kepler architecture and P 3",
    "start": "820440",
    "end": "827220"
  },
  {
    "text": "instances a latest one that we launched yet a few weeks back based on the Volta architecture and this slide kind of",
    "start": "827220",
    "end": "834240"
  },
  {
    "text": "compares the performance improvement across the different numerical precision types for these accelerators right so in",
    "start": "834240",
    "end": "842070"
  },
  {
    "text": "the first chart we are comparing mixed precision or FP 16 performance kad",
    "start": "842070",
    "end": "847170"
  },
  {
    "text": "actually does not support FP 16 data type so the max you will get is when",
    "start": "847170",
    "end": "852480"
  },
  {
    "text": "you're actually using FP 32 and if you just compare share the teraflops of what",
    "start": "852480",
    "end": "857880"
  },
  {
    "text": "you can get NP 3 versus P 2 is 14x if you look at F P 32 performance",
    "start": "857880",
    "end": "864450"
  },
  {
    "text": "specifically it is around 1.7 X and FP",
    "start": "864450",
    "end": "869640"
  },
  {
    "text": "64 the increase in teraflops is around 2.6 X right so for machine",
    "start": "869640",
    "end": "876960"
  },
  {
    "text": "learning applications we actually did an internal benchmark where we used MX net",
    "start": "876960",
    "end": "882660"
  },
  {
    "text": "as the framework rest net 50 as the model and image net as the data set to",
    "start": "882660",
    "end": "889320"
  },
  {
    "text": "actually benchmark the performance of P 3 versus P 2 and for machine learning",
    "start": "889320",
    "end": "895290"
  },
  {
    "text": "training we are seeing an over 7x performance improvement over over",
    "start": "895290",
    "end": "902580"
  },
  {
    "text": "peaches and over the KTBS platform so again really exciting performance boost you know generation over generation",
    "start": "902580",
    "end": "909560"
  },
  {
    "text": "between P 2 and P 3 so I can dive a little bit deeper on the instance sizes",
    "start": "909560",
    "end": "916020"
  },
  {
    "start": "912000",
    "end": "966000"
  },
  {
    "text": "and some of the specs are on these instances we support three sizes the",
    "start": "916020",
    "end": "921270"
  },
  {
    "text": "largest one being with 8 GPUs in it 64 V CPUs and almost half a terabyte of",
    "start": "921270",
    "end": "927900"
  },
  {
    "text": "RAM and we actually use we actually use any",
    "start": "927900",
    "end": "933630"
  },
  {
    "text": "link as the mechanism for peer-to-peer transaction between GPUs and I'll have a",
    "start": "933630",
    "end": "939330"
  },
  {
    "text": "few more slides to go over details into how does NV link compare with PCI",
    "start": "939330",
    "end": "944520"
  },
  {
    "text": "Express for GPU to GPU transfer we",
    "start": "944520",
    "end": "950160"
  },
  {
    "text": "support up to 25 gigabits of networking throughput for the largest exercise and",
    "start": "950160",
    "end": "955370"
  },
  {
    "text": "based on the increments that we were seeing in the EBS performance over the year",
    "start": "955370",
    "end": "960390"
  },
  {
    "text": "p3 has a 40 percent performance improvement over p2 instances so the",
    "start": "960390",
    "end": "967650"
  },
  {
    "start": "966000",
    "end": "1047000"
  },
  {
    "text": "other thing that we paid a lot of attention to when we designed this instance was around the data true put",
    "start": "967650",
    "end": "972930"
  },
  {
    "text": "into this GPU right so again these are described is a really powerful GPUs and",
    "start": "972930",
    "end": "978810"
  },
  {
    "text": "we want to make sure that we can have you know really high throughput data",
    "start": "978810",
    "end": "984090"
  },
  {
    "text": "pipes coming from the CPU into the GPU so then you can keep the GPU busy and",
    "start": "984090",
    "end": "989400"
  },
  {
    "text": "and have a high utilization of the GPU so besides that we have also double down",
    "start": "989400",
    "end": "994620"
  },
  {
    "text": "on making sure that we have good peer-to-peer transaction between the",
    "start": "994620",
    "end": "1000590"
  },
  {
    "text": "GPUs again with the intention of making sure we can keep a high utilization of the of the v100 GPUs so comparing P 2",
    "start": "1000590",
    "end": "1009530"
  },
  {
    "text": "and P 3 when it comes to peer-to-peer transfers and host to GPU transfers for",
    "start": "1009530",
    "end": "1018050"
  },
  {
    "text": "for peer-to-peer there's a 9x improvement between P 3 and P 2 that is",
    "start": "1018050",
    "end": "1023660"
  },
  {
    "text": "mainly provided via nd link where we can do 300 gigabytes per second for the CPU",
    "start": "1023660",
    "end": "1030170"
  },
  {
    "text": "to GPU throughput we've also increased that pretty significantly where on a per",
    "start": "1030170",
    "end": "1036199"
  },
  {
    "text": "GPU level there's an 8x improvement and a 4x and the full instance size level so",
    "start": "1036200",
    "end": "1041390"
  },
  {
    "text": "let's get into details and understand what does the actual data topology look like for a pea 3/16 Excel instance so P",
    "start": "1041390",
    "end": "1050330"
  },
  {
    "start": "1047000",
    "end": "1112000"
  },
  {
    "text": "3/16 Excel it's a it's a dual socketed or p3 is a dual socketed machine we're",
    "start": "1050330",
    "end": "1057050"
  },
  {
    "text": "in the 16 Excel configuration you have access to both the CPUs and all the V CPUs they provide",
    "start": "1057050",
    "end": "1062960"
  },
  {
    "text": "so if you start with one configuration so let's talk about the CPU zero configuration so in that we actually",
    "start": "1062960",
    "end": "1070160"
  },
  {
    "text": "have to buy 16 PCI Express gen3 links",
    "start": "1070160",
    "end": "1075280"
  },
  {
    "text": "that connect to PCI Express switches that you guys are seeing in the green blocks and one PCI Express which that",
    "start": "1075280",
    "end": "1084590"
  },
  {
    "text": "ends up connecting to to downstream GPUs and then repeat that configuration for the second set of GPUs in this behind",
    "start": "1084590",
    "end": "1091160"
  },
  {
    "text": "the same and then replicate that for the second set of CPUs and GPUs so that kind",
    "start": "1091160",
    "end": "1096919"
  },
  {
    "text": "of talks about the the architecture for data throughput from the host CPU into",
    "start": "1096919",
    "end": "1105200"
  },
  {
    "text": "the GPU and like I mentioned this is overall an 4x improvement over what what",
    "start": "1105200",
    "end": "1111169"
  },
  {
    "text": "P 2 can do now any link is actually layers on top of this where it's a",
    "start": "1111169",
    "end": "1117559"
  },
  {
    "start": "1112000",
    "end": "1210000"
  },
  {
    "text": "direct point-to-point set of links that connect GPUs to and other side of GPUs",
    "start": "1117559",
    "end": "1123740"
  },
  {
    "text": "directly so each GPU has 6 in willing ports and we end up leveraging those",
    "start": "1123740",
    "end": "1129740"
  },
  {
    "text": "connections to connect all the GPU in a hyper mesh configuration to enable peer-to-peer transactions so let me give",
    "start": "1129740",
    "end": "1135919"
  },
  {
    "text": "a animation of how this would actually translate so let's assume that we",
    "start": "1135919",
    "end": "1141200"
  },
  {
    "text": "actually want to transfer data from GPUs 0 into GPU 3 over PCI Express so let's",
    "start": "1141200",
    "end": "1149450"
  },
  {
    "text": "assume that n billing doesn't exist and we want to use PCI Express for that transfer so in that case the data will",
    "start": "1149450",
    "end": "1156650"
  },
  {
    "text": "actually take this path and will traverse over the PCI Express links down",
    "start": "1156650",
    "end": "1162380"
  },
  {
    "text": "to the other lengths and to the GPU 3 in the case of NB link it'll be a direct",
    "start": "1162380",
    "end": "1167570"
  },
  {
    "text": "transfer it will just again directly go over the NB a link and transfer data",
    "start": "1167570",
    "end": "1173120"
  },
  {
    "text": "from gp0 to GPU 3 4 the other scenario that is even more interesting is the",
    "start": "1173120",
    "end": "1179809"
  },
  {
    "text": "scenario where you would want to share data from GPU 0 into GPU 6 so in this",
    "start": "1179809",
    "end": "1185299"
  },
  {
    "text": "case you actually actually have to go through the quick pack interconnect that connects to two CPUs which typically",
    "start": "1185299",
    "end": "1191090"
  },
  {
    "text": "involves like a memory copy and that's the path that the the at reverses so in the case of in the",
    "start": "1191090",
    "end": "1196980"
  },
  {
    "text": "link it is again a point-to-point link and it's a lower latency transfer and by",
    "start": "1196980",
    "end": "1203399"
  },
  {
    "text": "just the sheer number of n billion connections we have we can enable much higher throughput over what we can do",
    "start": "1203399",
    "end": "1208889"
  },
  {
    "text": "over PCI Express so so so that kind of sums it up from like an instance",
    "start": "1208889",
    "end": "1214130"
  },
  {
    "start": "1210000",
    "end": "1248000"
  },
  {
    "text": "instance a detailed perspective I mean just to summarize you know from an",
    "start": "1214130",
    "end": "1219179"
  },
  {
    "text": "application and a use case perspective there are just two broad categories of applications that p3 is really suitable",
    "start": "1219179",
    "end": "1225720"
  },
  {
    "text": "for one is their on the machine learning ml such a you know a I space where again",
    "start": "1225720",
    "end": "1231029"
  },
  {
    "text": "the keynote this morning we highlight there are a whole bunch of applications where natural language processing you",
    "start": "1231029",
    "end": "1236519"
  },
  {
    "text": "know image and video recognition and recommendation systems and then another set of applications around high",
    "start": "1236519",
    "end": "1241679"
  },
  {
    "text": "performance computing where yes CFD financial and data analytics simulations",
    "start": "1241679",
    "end": "1246750"
  },
  {
    "text": "and computational chemistry great so now I would like to hand it over to NIC NIC",
    "start": "1246750",
    "end": "1251940"
  },
  {
    "text": "is a product manager of the air and Beadon be and he's going to share with us how Airbnb uses machine learning and",
    "start": "1251940",
    "end": "1258259"
  },
  {
    "text": "then Alfred is going to join and talk about some of the details around their architecture right good all right thank",
    "start": "1258259",
    "end": "1271350"
  },
  {
    "start": "1266000",
    "end": "1303000"
  },
  {
    "text": "you very much um all right so so I'm NIC I'm the NIC handle the product manager",
    "start": "1271350",
    "end": "1277320"
  },
  {
    "text": "of machine learning infrastructure at Airbnb and I'm gonna start off just set",
    "start": "1277320",
    "end": "1282539"
  },
  {
    "text": "the stage of how everybody uses machine learning and then I'll go into what the",
    "start": "1282539",
    "end": "1288570"
  },
  {
    "text": "machine learning infrastructure team at Airbnb is doing to help the rest of the",
    "start": "1288570",
    "end": "1293639"
  },
  {
    "text": "company use ml and then I'll hand it off to Alfredo and he'll go deeper into the",
    "start": "1293639",
    "end": "1299070"
  },
  {
    "text": "our use of the p3 all right so set the",
    "start": "1299070",
    "end": "1306659"
  },
  {
    "start": "1303000",
    "end": "1317000"
  },
  {
    "text": "stage everybody's mission is to create a world where where people can belong when they travel by being connected to local",
    "start": "1306659",
    "end": "1313169"
  },
  {
    "text": "cultures and having unique travel experiences so a bit more about Airbnb",
    "start": "1313169",
    "end": "1319909"
  },
  {
    "start": "1317000",
    "end": "1346000"
  },
  {
    "text": "just some stats I think most of you are familiar with with Airbnb but kind of",
    "start": "1319909",
    "end": "1325649"
  },
  {
    "text": "set the stage of the scale of the data that we're dealing with so there are 65,000 cities",
    "start": "1325649",
    "end": "1331530"
  },
  {
    "text": "listed with homes on Airbnb four million",
    "start": "1331530",
    "end": "1336640"
  },
  {
    "text": "Airbnb listings worldwide 191 plus countries and 200 million Airbnb guests",
    "start": "1336640",
    "end": "1344650"
  },
  {
    "text": "arrivals all time so that's a lot of data that's going to take a lot of compute so but the the really",
    "start": "1344650",
    "end": "1354580"
  },
  {
    "start": "1346000",
    "end": "1387000"
  },
  {
    "text": "complicated thing about Airbnb and the need for machine learning really arises in marketplace dynamics so we have",
    "start": "1354580",
    "end": "1362610"
  },
  {
    "text": "guests on the left and hosts on the right with their homes so in kind of a",
    "start": "1362610",
    "end": "1371710"
  },
  {
    "text": "standard marketplace you have supply and demand and you want to you know connect the supply with the demand but it's not",
    "start": "1371710",
    "end": "1379240"
  },
  {
    "text": "quite so straightforward with Airbnb just because of how unique some of the",
    "start": "1379240",
    "end": "1384370"
  },
  {
    "text": "demand is and how unique some of the supply is so there's this complicated",
    "start": "1384370",
    "end": "1389860"
  },
  {
    "text": "matching problem where we have you know unique guests preferences maybe they",
    "start": "1389860",
    "end": "1394930"
  },
  {
    "text": "want to check in early or they want a hot tub or they want you know you know a",
    "start": "1394930",
    "end": "1402700"
  },
  {
    "text": "good space for their family there's a variety of needs and then we have",
    "start": "1402700",
    "end": "1407740"
  },
  {
    "text": "listings that can meet those different demands and hosts with preferences also",
    "start": "1407740",
    "end": "1413770"
  },
  {
    "text": "you know they might they might not want you know early check-in they might have",
    "start": "1413770",
    "end": "1419890"
  },
  {
    "text": "some preferences about what kinds of trips are happening at their home maybe they really just want business travelers",
    "start": "1419890",
    "end": "1425940"
  },
  {
    "text": "and so this kind of presents this this complicated matching problem where we really need to understand both the",
    "start": "1425940",
    "end": "1432070"
  },
  {
    "text": "guests and and the hosts and the supply the listings so as you can imagine",
    "start": "1432070",
    "end": "1440530"
  },
  {
    "text": "there's a huge amount of machine learning that is required to make that",
    "start": "1440530",
    "end": "1445540"
  },
  {
    "text": "dynamic happen but some some quick examples so pricing these listings you",
    "start": "1445540",
    "end": "1452950"
  },
  {
    "text": "know every listing is unique so pricing them is pretty complicated there's you know unique amenities and sometimes the",
    "start": "1452950",
    "end": "1459100"
  },
  {
    "text": "listing will add a hot tub in we'll get more value out of that and so we have these Smart pricing",
    "start": "1459100",
    "end": "1465000"
  },
  {
    "text": "recommendations where you you know you'll you'll need to price a listing",
    "start": "1465000",
    "end": "1470610"
  },
  {
    "text": "out for every single day into the future and the you know full listing changes",
    "start": "1470610",
    "end": "1477090"
  },
  {
    "text": "like they add a hot tub you'll need to change those prices and you also need to change those prices if there's different amounts of demand for some you know",
    "start": "1477090",
    "end": "1484680"
  },
  {
    "text": "different nights and if that demand evolves like if there's a big conference in town so we use it for risk bad actors",
    "start": "1484680",
    "end": "1492360"
  },
  {
    "text": "detection you know protecting the community we use it to help grow the marketplace place by you know optimizing",
    "start": "1492360",
    "end": "1501270"
  },
  {
    "text": "our ad budget you know we have an experiences product now where there's a",
    "start": "1501270",
    "end": "1506400"
  },
  {
    "text": "lot more personalization involved there's even more preferences involved in do you want to go you know do a you",
    "start": "1506400",
    "end": "1514710"
  },
  {
    "text": "know a unique food experience or a unique music experience payments",
    "start": "1514710",
    "end": "1520140"
  },
  {
    "text": "customer service examples of routing so I can go on and on and there's all these different examples but I want to give a",
    "start": "1520140",
    "end": "1527220"
  },
  {
    "start": "1526000",
    "end": "1619000"
  },
  {
    "text": "more concrete example just so you can kind of see why deep learning is actually really important so on the Left",
    "start": "1527220",
    "end": "1534900"
  },
  {
    "text": "this is the San Francisco Bay Area and these are Airbnb listings in the different colors so the clusters of",
    "start": "1534900",
    "end": "1543720"
  },
  {
    "text": "colors are basically examples of listings that are commonly viewed",
    "start": "1543720",
    "end": "1550620"
  },
  {
    "text": "together so you can you can see these areas where you know they might want to",
    "start": "1550620",
    "end": "1556530"
  },
  {
    "text": "stay in some specific city and that's really great but it would you know all",
    "start": "1556530",
    "end": "1563070"
  },
  {
    "text": "these listings are really actually quite unique and all of those listings in those areas are really not comparable so",
    "start": "1563070",
    "end": "1570810"
  },
  {
    "text": "on the right-hand side we've added listing attributes so listing attributes",
    "start": "1570810",
    "end": "1576480"
  },
  {
    "text": "are things like structured data like is there a hot tub or not or you know is",
    "start": "1576480",
    "end": "1581900"
  },
  {
    "text": "there Wi-Fi is there washing machine those kinds of things where we can say",
    "start": "1581900",
    "end": "1588210"
  },
  {
    "text": "yes or no that exists but there's also you know listing images listing",
    "start": "1588210",
    "end": "1594740"
  },
  {
    "text": "text these things that require deep learning to really extract structure from them and so when you add that in",
    "start": "1594740",
    "end": "1601940"
  },
  {
    "text": "you really see that these pockets of demand are not really pockets but",
    "start": "1601940",
    "end": "1607429"
  },
  {
    "text": "they're kind of spread out and within some city the you know the unique listings there might be many different",
    "start": "1607429",
    "end": "1614270"
  },
  {
    "text": "types of listings that cluster together so now talk a little bit about the",
    "start": "1614270",
    "end": "1623059"
  },
  {
    "start": "1619000",
    "end": "1689000"
  },
  {
    "text": "machine learning infrastructure team so this is the team that we're a part of so machine learning infrastructures mission",
    "start": "1623059",
    "end": "1629510"
  },
  {
    "text": "is to equip Airbnb with shared technology to build production-ready ml applications with no incidental",
    "start": "1629510",
    "end": "1635779"
  },
  {
    "text": "complexity okay so a few kind of important parts of this incidental",
    "start": "1635779",
    "end": "1641029"
  },
  {
    "text": "complexity we define as basically something that if somebody does it it",
    "start": "1641029",
    "end": "1646700"
  },
  {
    "text": "shouldn't have to be done again so you want to remove as much incidental complexity as possible and and the goal",
    "start": "1646700",
    "end": "1652820"
  },
  {
    "text": "is to make it so that this doesn't happen you know you don't have to repeatedly do the same kinds of things so setting up environments you know",
    "start": "1652820",
    "end": "1661940"
  },
  {
    "text": "defining future transformations on on your data those kinds of things it's",
    "start": "1661940",
    "end": "1668779"
  },
  {
    "text": "also important to note that it's it's shared technology it's not the ml infrastructure team builds it and then",
    "start": "1668779",
    "end": "1674299"
  },
  {
    "text": "the rest of the company uses it it's kind of more like an open source project within Airbnb where we'll work on it and",
    "start": "1674299",
    "end": "1681860"
  },
  {
    "text": "we'll kind of lead the development but really the rest of the company should contribute back and work with us on it",
    "start": "1681860",
    "end": "1689230"
  },
  {
    "start": "1689000",
    "end": "1803000"
  },
  {
    "text": "so we use this diagram to kind of help us think through our infrastructure",
    "start": "1691059",
    "end": "1697070"
  },
  {
    "text": "there's the machine learning workflows this iterative process right you know",
    "start": "1697070",
    "end": "1703700"
  },
  {
    "text": "you start off with data collection you train your model there's probably a huge",
    "start": "1703700",
    "end": "1709340"
  },
  {
    "text": "amount of iteration that happens within that training then you want to do inference you want to serve that model",
    "start": "1709340",
    "end": "1715429"
  },
  {
    "text": "to some production data and then you you want to monitor that and make sure that",
    "start": "1715429",
    "end": "1721669"
  },
  {
    "text": "that model is healthy healthy so at the center of that is this orchestration layer where you really want to",
    "start": "1721669",
    "end": "1728240"
  },
  {
    "text": "to make it so that you know all of those things happen smoothly together and",
    "start": "1728240",
    "end": "1735040"
  },
  {
    "text": "without common shared infrastructure that connects all of these different",
    "start": "1735040",
    "end": "1741710"
  },
  {
    "text": "tools you end up in the outside of this circle it's very slow this this I kind",
    "start": "1741710",
    "end": "1747050"
  },
  {
    "text": "of think of this as like a flywheel it happens really slowly and what you really want is you want this tight loop",
    "start": "1747050",
    "end": "1753980"
  },
  {
    "text": "where all of these different pieces fit together really well and it's it's easy",
    "start": "1753980",
    "end": "1759230"
  },
  {
    "text": "to to you know train your models and improve them as quickly as possible so",
    "start": "1759230",
    "end": "1766240"
  },
  {
    "text": "you know one of these is big machines and and I mentioned that that training",
    "start": "1766240",
    "end": "1772340"
  },
  {
    "text": "bubble is something that takes a long time you know it's you want to you know",
    "start": "1772340",
    "end": "1777559"
  },
  {
    "text": "really iterate on models you want to experiment try new things and and that's really where the p3 comes in the faster",
    "start": "1777559",
    "end": "1784130"
  },
  {
    "text": "we can make that loop and the faster that we can test and produce better",
    "start": "1784130",
    "end": "1789380"
  },
  {
    "text": "models the faster this whole flywheel becomes so now I'm gonna hand it off to",
    "start": "1789380",
    "end": "1795200"
  },
  {
    "text": "Alfredo to talk a bit more about the ML workflow and some of the the uses of the",
    "start": "1795200",
    "end": "1801500"
  },
  {
    "text": "p3 within it",
    "start": "1801500",
    "end": "1804309"
  },
  {
    "start": "1803000",
    "end": "1958000"
  },
  {
    "text": "hey can you hear me awesome so Nick",
    "start": "1807640",
    "end": "1812710"
  },
  {
    "text": "talks about some of the key pieces and an m/l workflow you've seen them before but I'm just gonna walk through at",
    "start": "1812710",
    "end": "1819100"
  },
  {
    "text": "Airbnb when you typically want to make a model kind of what's involved right now so generally you're identifying a",
    "start": "1819100",
    "end": "1825549"
  },
  {
    "text": "problem so that involves some sort of iteration on a couple of data sets usually you might want to figure out",
    "start": "1825549",
    "end": "1832360"
  },
  {
    "text": "specifically do I have a classifier agressor that's going to be my outputs",
    "start": "1832360",
    "end": "1837450"
  },
  {
    "text": "maybe some kind of statistical model maybe it's an optimization problem these are all things we encounter at Airbnb",
    "start": "1837450",
    "end": "1843850"
  },
  {
    "text": "and we have to kind of make a good environment so it's easy to interact with that data right the output of that",
    "start": "1843850",
    "end": "1851260"
  },
  {
    "text": "should really be a data set or a superset of all the data you're going to want for actually exploring that problem",
    "start": "1851260",
    "end": "1858270"
  },
  {
    "text": "so that's going to involve selecting what we call your features and those are going to be your direct inputs into your",
    "start": "1858270",
    "end": "1865210"
  },
  {
    "text": "actual model of workflow unfortunately for many types of architectures so if",
    "start": "1865210",
    "end": "1871150"
  },
  {
    "text": "your seem like a neural network or if using a boosted model you're gonna have to pre-process that data that might",
    "start": "1871150",
    "end": "1876400"
  },
  {
    "text": "involve scaling so like images you might have to you know resize them with some",
    "start": "1876400",
    "end": "1881950"
  },
  {
    "text": "numeric data you have to transform it and with text data and freeform you know",
    "start": "1881950",
    "end": "1888309"
  },
  {
    "text": "and I'll PU you do have other more sophisticated embeddings as well and after that comes what Nick talks about",
    "start": "1888309",
    "end": "1895870"
  },
  {
    "text": "which is a very long sort of iterative cycle of figuring out what model am I going to use you know a lot of the times",
    "start": "1895870",
    "end": "1903000"
  },
  {
    "text": "you might want to throw simply like a boosted model at the problem and and see",
    "start": "1903000",
    "end": "1908350"
  },
  {
    "text": "how it performs and maybe move on to something more complicated but if you're able to make that cycle very tight if",
    "start": "1908350",
    "end": "1914500"
  },
  {
    "text": "you're able to you know train a model on a massive data set maybe get the optimal",
    "start": "1914500",
    "end": "1919600"
  },
  {
    "text": "such variation soon your hyper parameters and get results for you quickly then you're able to kind of",
    "start": "1919600",
    "end": "1925450"
  },
  {
    "text": "sweep the landscape and really get something that's that's very performance you know it's a production not really",
    "start": "1925450",
    "end": "1932260"
  },
  {
    "text": "have to settle very early finally some models do you have to be put into production and this is often not a",
    "start": "1932260",
    "end": "1939340"
  },
  {
    "text": "trivial thing so you might need real-time and friends we have streaming data and see of",
    "start": "1939340",
    "end": "1944920"
  },
  {
    "text": "latency guarantees or you might need regular batch training and you know you're talking about petabytes of data",
    "start": "1944920",
    "end": "1950650"
  },
  {
    "text": "that that needs to be scored on a daily basis so it's important to make that word flip very easy as well now at",
    "start": "1950650",
    "end": "1961240"
  },
  {
    "start": "1958000",
    "end": "2056000"
  },
  {
    "text": "Airbnb here's sort of the the state yeah we arrived at that was sort of the",
    "start": "1961240",
    "end": "1966760"
  },
  {
    "text": "status quo and that's it for identifying the problem for iterative analysis that's very much like a local workflow",
    "start": "1966760",
    "end": "1973600"
  },
  {
    "text": "right I'm generally gonna have somebody working it's preferable to work on your laptop or in an environment where you",
    "start": "1973600",
    "end": "1979840"
  },
  {
    "text": "can kind of interact with the data so we developed something I'll talk about in a little bit but we kind of have an",
    "start": "1979840",
    "end": "1986200"
  },
  {
    "text": "environment where people can can work in the ipython notebook and you know interact with these massive data sets",
    "start": "1986200",
    "end": "1992400"
  },
  {
    "text": "access if you're familiar with air flow and you know the resources to kind of",
    "start": "1992400",
    "end": "1998080"
  },
  {
    "text": "transform petabytes of data and get what you need out selecting your features out",
    "start": "1998080",
    "end": "2003360"
  },
  {
    "text": "is something that ultimately is going to be you know MapReduce job spark job some",
    "start": "2003360",
    "end": "2009300"
  },
  {
    "text": "kind of heavy sort of workflow and that's not something you want running on your laptop right so fundamentally",
    "start": "2009300",
    "end": "2017160"
  },
  {
    "text": "that's something that you have to offload and that transition needs to be kind of easy to do but you'll notice",
    "start": "2017160",
    "end": "2023640"
  },
  {
    "text": "there's three squares here that are green that are actually pretty computationally intensive traditionally",
    "start": "2023640",
    "end": "2029940"
  },
  {
    "text": "for things like encodings and embeddings for selecting your model for doing hyper",
    "start": "2029940",
    "end": "2035040"
  },
  {
    "text": "parameter tuning your options are pretty much you know either you run in a local environment or you SSH into a box with",
    "start": "2035040",
    "end": "2042810"
  },
  {
    "text": "some GPS on it and you kind of do all that there and when you want to go to",
    "start": "2042810",
    "end": "2048540"
  },
  {
    "text": "production then you have to translate that you know put it somewhere and maybe add some additional code actually",
    "start": "2048540",
    "end": "2054690"
  },
  {
    "text": "production alized it as you can see there's a lot of kind of pitfalls in that workflow and yet it's a very very",
    "start": "2054690",
    "end": "2060810"
  },
  {
    "start": "2056000",
    "end": "2182000"
  },
  {
    "text": "common workflow at many companies even around the Bay Area one of the big ones",
    "start": "2060810",
    "end": "2066000"
  },
  {
    "text": "is people want to play with a lot of different models when you encounter a problem right it's nice to be able to",
    "start": "2066000",
    "end": "2072510"
  },
  {
    "text": "just run something five minutes later no it worked it didn't work you know how can I change it",
    "start": "2072510",
    "end": "2079830"
  },
  {
    "text": "and that's not really possible because training is pretty slow and especially with newer and newer architectures they",
    "start": "2079830",
    "end": "2086520"
  },
  {
    "text": "get deeper and deeper and slower and slower to train so it's not uncommon for a day or more to be a reasonable time",
    "start": "2086520",
    "end": "2093149"
  },
  {
    "text": "line to get results back hyper parameter tuning is also something that's kind of",
    "start": "2093150",
    "end": "2098160"
  },
  {
    "text": "overlooked a lot of the times typically there are recommendations for you know",
    "start": "2098160",
    "end": "2103620"
  },
  {
    "text": "hyper parameters for different types of architectures but they're not you know locally optimal for your specific",
    "start": "2103620",
    "end": "2110490"
  },
  {
    "text": "problem so unless you're able to spawn you know hundreds of these jobs and get the results back quickly you're probably",
    "start": "2110490",
    "end": "2116310"
  },
  {
    "text": "going to stick with what was you know passable in production and then as we mentioned so p2 instances and p3",
    "start": "2116310",
    "end": "2123990"
  },
  {
    "text": "instances are great but they're expensive right if you have you know a hundred of these running like I'm sure",
    "start": "2123990",
    "end": "2130320"
  },
  {
    "text": "that the budget for for your computes gonna kind of blow it pretty quickly and really it's expensive because you're not",
    "start": "2130320",
    "end": "2136980"
  },
  {
    "text": "using them most of the time right giving users access to these so they can like develop on them means that 90% of the",
    "start": "2136980",
    "end": "2144450"
  },
  {
    "text": "time they're actually you know in vim and not actually running a job so",
    "start": "2144450",
    "end": "2149460"
  },
  {
    "text": "they're not you know being kept hot and that just doesn't scale very well and Leslie people at least the Airbnb want",
    "start": "2149460",
    "end": "2157320"
  },
  {
    "text": "to use a lot of different frameworks we don't like to impose that you must use tensorflow you must use PI torch you",
    "start": "2157320",
    "end": "2164340"
  },
  {
    "text": "know you should really be able to use kind of whatever you want in and have it run in the performant fashion so we",
    "start": "2164340",
    "end": "2171330"
  },
  {
    "text": "thought of a couple of these problems and really sorted over the last year",
    "start": "2171330",
    "end": "2176610"
  },
  {
    "text": "kind of formalizing a good end infrastructure that will let us accomplish all these things so I'm happy",
    "start": "2176610",
    "end": "2183300"
  },
  {
    "start": "2182000",
    "end": "2433000"
  },
  {
    "text": "to announce big head by Airbnb and that's our attempt at really making this",
    "start": "2183300",
    "end": "2190020"
  },
  {
    "text": "as friction-free as possible the biggest sort of as Nick mentioned thing we want",
    "start": "2190020",
    "end": "2196800"
  },
  {
    "text": "to focus on is removing incidental complexity everywhere along the the workflow so the idea is that from start",
    "start": "2196800",
    "end": "2204270"
  },
  {
    "text": "to finish you should be able to go in a Jupiter notebook you know play around with your data",
    "start": "2204270",
    "end": "2210570"
  },
  {
    "text": "explore it analyze it have access to all these data sources around the company",
    "start": "2210570",
    "end": "2215820"
  },
  {
    "text": "and if you want to use plain Python you can use that to you then you should be",
    "start": "2215820",
    "end": "2221220"
  },
  {
    "text": "able to take that data use whatever frameworks you want you can mix them and match them you can you know have some",
    "start": "2221220",
    "end": "2228240"
  },
  {
    "text": "pieces running and tensorflow some in pi torch some in plain scikit-learn chain",
    "start": "2228240",
    "end": "2233970"
  },
  {
    "text": "them together get your features by simply clicking in a shopping cart kind of like you know going on Amazon and",
    "start": "2233970",
    "end": "2240270"
  },
  {
    "text": "ordering a book and getting your data out in a few minutes being able to version your models so you",
    "start": "2240270",
    "end": "2246720"
  },
  {
    "text": "can figure out you know which versions works best and even if you're developing",
    "start": "2246720",
    "end": "2252300"
  },
  {
    "text": "locally wouldn't it be nice to just click run on your model and have a deploy to fleet of supercomputers that's",
    "start": "2252300",
    "end": "2259050"
  },
  {
    "text": "something that we really wanted and and p3 just kind of make that possible for us and a final piece that was so so so",
    "start": "2259050",
    "end": "2266700"
  },
  {
    "text": "key and a lot of companies struggle with and we're hoping it to really crack is why don't you just take your model that",
    "start": "2266700",
    "end": "2274170"
  },
  {
    "text": "you worked on in this notebook and with one or two clicks have it running in production and that was really a key",
    "start": "2274170",
    "end": "2280800"
  },
  {
    "text": "piece of of this infrastructure so to accomplish these things our environments",
    "start": "2280800",
    "end": "2287400"
  },
  {
    "text": "Red Spot is it's an in-house it's full that kind of is a jupiter notebook",
    "start": "2287400",
    "end": "2294119"
  },
  {
    "text": "environments you can spawn dedicated instances that you know will be like a",
    "start": "2294119",
    "end": "2300119"
  },
  {
    "text": "p3 instance you can use a shared instance with many other users if you",
    "start": "2300119",
    "end": "2305310"
  },
  {
    "text": "have lower compute requirements but essentially a one-stop shop where you",
    "start": "2305310",
    "end": "2310350"
  },
  {
    "text": "can access you know all of your data you can run spark jobs from your notebook and everything and kind of work quickly",
    "start": "2310350",
    "end": "2317520"
  },
  {
    "text": "and easily and iteratively there zip line is that shopping cart tool and",
    "start": "2317520",
    "end": "2322650"
  },
  {
    "text": "feature engineering tool I mentioned that essentially lets you generate training sets with very very minimal",
    "start": "2322650",
    "end": "2330420"
  },
  {
    "text": "added code you just select the data you once maybe how you a granade it and we",
    "start": "2330420",
    "end": "2337020"
  },
  {
    "text": "go ahead and run that for you in a distributed fashion for managing",
    "start": "2337020",
    "end": "2342150"
  },
  {
    "text": "models you have something called model repo this is something that we haven't really seen tackled too much before but",
    "start": "2342150",
    "end": "2349049"
  },
  {
    "text": "there's a lot of models at a company and eventually they get to a point where it's kind of hard to keep track of that",
    "start": "2349049",
    "end": "2355049"
  },
  {
    "text": "ins semantic versioning is not something that's always great for 4ml you can't",
    "start": "2355049",
    "end": "2363029"
  },
  {
    "text": "really stick like outputs of weights in that and get and expect that to work either so we developed something to",
    "start": "2363029",
    "end": "2370109"
  },
  {
    "text": "better handle all the different variations you might have on the model and all the different training runs you",
    "start": "2370109",
    "end": "2375750"
  },
  {
    "text": "might have and just have this really easily accessible just by doing getting and put to this to this service to",
    "start": "2375750",
    "end": "2384480"
  },
  {
    "text": "deploy to production we have something called deep thoughts and that really lets you do like online scoring so you",
    "start": "2384480",
    "end": "2392339"
  },
  {
    "text": "can have that same model running in a docker container and you know and receiving calls via a little lightweight",
    "start": "2392339",
    "end": "2398190"
  },
  {
    "text": "shim on top now we're the p3 actually fits in here and I'll be talking a",
    "start": "2398190",
    "end": "2404039"
  },
  {
    "text": "little bit more about this is final component called big cue and that's what",
    "start": "2404039",
    "end": "2409920"
  },
  {
    "text": "I mentioned where you're able to from a notebook dispatch your training runs to",
    "start": "2409920",
    "end": "2415559"
  },
  {
    "text": "to fleet of servers with GPUs in them and get the results back so no longer do",
    "start": "2415559",
    "end": "2421950"
  },
  {
    "text": "we need to have somebody SSH into one machine and run their model we can you",
    "start": "2421950",
    "end": "2427680"
  },
  {
    "text": "know just do that from wherever and we can even scale that out so to give you",
    "start": "2427680",
    "end": "2434490"
  },
  {
    "start": "2433000",
    "end": "2539000"
  },
  {
    "text": "an idea of why this actually works and why the p3 made this possible we did",
    "start": "2434490",
    "end": "2440339"
  },
  {
    "text": "some benchmarks very early on so you'll notice that these are considerably slower than what was presented earlier",
    "start": "2440339",
    "end": "2446910"
  },
  {
    "text": "because we were using pre-release versions of tensorflow and we're also using FP 32 but the interesting thing",
    "start": "2446910",
    "end": "2455279"
  },
  {
    "text": "for us was yes p3 instances are faster you know on a single GPU basis when you",
    "start": "2455279",
    "end": "2461700"
  },
  {
    "text": "look at something like ResNet 50 right you know it's nice to be able to scale up to eight GPUs but historically",
    "start": "2461700",
    "end": "2469049"
  },
  {
    "text": "because of PCI Express is bandwidth limitations when you scale out to like 50 GPUs it doesn't actually scale very",
    "start": "2469049",
    "end": "2476069"
  },
  {
    "text": "well but with p3s you definitely do get close to linear skilling and I don't know if",
    "start": "2476069",
    "end": "2481979"
  },
  {
    "text": "you saw like the Facebook paper where they trained resin at 50 and an hour on imagenet you can now get pretty close to",
    "start": "2481979",
    "end": "2488670"
  },
  {
    "text": "that on a fraction of the GPUs and a fraction of the cost we use something",
    "start": "2488670",
    "end": "2493829"
  },
  {
    "text": "called horror Vlad to do this it's a tool by uber it uses MPI and NC CL which",
    "start": "2493829",
    "end": "2501479"
  },
  {
    "text": "is kind of videos hardware-accelerated library for reductions and helps them",
    "start": "2501479",
    "end": "2507690"
  },
  {
    "text": "compute the gradient but essentially while we were kind of limited in initial",
    "start": "2507690",
    "end": "2512910"
  },
  {
    "text": "testing to these number of instances and you should be able to scale this up you know probably two or three acts and see",
    "start": "2512910",
    "end": "2519680"
  },
  {
    "text": "you know not a huge reduction in your speed ups what this means though is when",
    "start": "2519680",
    "end": "2526200"
  },
  {
    "text": "you see like a 71 X speed-up over you know single GPU you no longer have",
    "start": "2526200",
    "end": "2531479"
  },
  {
    "text": "people running single jobs on a single GPU so now you have people running jobs",
    "start": "2531479",
    "end": "2536849"
  },
  {
    "text": "that take a few minutes and because jobs only take a few minutes were able to do",
    "start": "2536849",
    "end": "2542460"
  },
  {
    "start": "2539000",
    "end": "2694000"
  },
  {
    "text": "something like this and this is kind of the architecture for big cube the idea is that as a client you can dispatch",
    "start": "2542460",
    "end": "2549869"
  },
  {
    "text": "these jobs these asynchronous jobs and a fashion that's very familiar to you if",
    "start": "2549869",
    "end": "2555089"
  },
  {
    "text": "you write to HPC and if like scheduled jobs to run the supercomputer the",
    "start": "2555089",
    "end": "2561089"
  },
  {
    "text": "difference is you can do this from a jupiter notebook right with the library if you have like a tensor flow model we",
    "start": "2561089",
    "end": "2566670"
  },
  {
    "text": "have you know all these little wrappers for them and you can kind of just dispatch it to big queue and when it's",
    "start": "2566670",
    "end": "2572219"
  },
  {
    "text": "fit if you get your fit model back its you realized out the actual run happens",
    "start": "2572219",
    "end": "2577589"
  },
  {
    "text": "within docker so anything that you have in your environments in Jupiter you have",
    "start": "2577589",
    "end": "2582749"
  },
  {
    "text": "on your environments on the p3 machine or if you need 50 p3 machines or you",
    "start": "2582749",
    "end": "2589049"
  },
  {
    "text": "know 10 p3 machines that'll all be there also because we allow you to mix models and mix",
    "start": "2589049",
    "end": "2595680"
  },
  {
    "text": "different types of frameworks this means you can get some pretty complicated workflows in production with maybe 10 or",
    "start": "2595680",
    "end": "2603029"
  },
  {
    "text": "11 lines of code and actually train them in a few hours where it might have taken days or in the",
    "start": "2603029",
    "end": "2609839"
  },
  {
    "text": "past really what wouldn't have even tried it additionally there's one huge saving for",
    "start": "2609839",
    "end": "2616960"
  },
  {
    "text": "us which is you're keeping notes hot so if you have 10 or 11 users you know",
    "start": "2616960",
    "end": "2622810"
  },
  {
    "text": "trying to run all these jobs at the same time the machine's going to be kept as busy as possible right each machine is",
    "start": "2622810",
    "end": "2629670"
  },
  {
    "text": "allocated to task if you don't have enough we can actually use because we're",
    "start": "2629670",
    "end": "2634960"
  },
  {
    "text": "on the cloud and on-premise scaling to Auto scale is out so if we initially",
    "start": "2634960",
    "end": "2640869"
  },
  {
    "text": "reserved three or four we can you know go up to like 10 or 11 and kind of get the the performance we need to you know",
    "start": "2640869",
    "end": "2648400"
  },
  {
    "text": "not having everything take an hour well at the same time you know not not",
    "start": "2648400",
    "end": "2654310"
  },
  {
    "text": "spending a ton of money doing it so this is something we're really excited about it's really only possible because you",
    "start": "2654310",
    "end": "2661510"
  },
  {
    "text": "have this amazing multi node scaling with p3s things to end of a link but",
    "start": "2661510",
    "end": "2668790"
  },
  {
    "text": "we'll be we're planning on open sourcing some of this stuff file into your future so this is going to be a pretty",
    "start": "2668790",
    "end": "2675369"
  },
  {
    "text": "interesting little little change in an MLM for many people yeah cool well",
    "start": "2675369",
    "end": "2681609"
  },
  {
    "text": "thanks Alfredo so yeah just to summarize p3s again one of the most powerful GP",
    "start": "2681609",
    "end": "2687609"
  },
  {
    "text": "platforms in the industry right now again pretty significant performance boost or RP two instances and with that",
    "start": "2687609",
    "end": "2695400"
  },
  {
    "start": "2694000",
    "end": "2887000"
  },
  {
    "text": "we'll open it up for questions naked and afraid if you guys want to join me you",
    "start": "2695400",
    "end": "2702310"
  },
  {
    "text": "guys gonna walk up to the mics any questions just a phenomenal presentation",
    "start": "2702310",
    "end": "2708070"
  },
  {
    "text": "you guys got it it's great",
    "start": "2708070",
    "end": "2713250"
  },
  {
    "text": "let me ask you a question to Nick and Fredo so the new service we launched a",
    "start": "2713989",
    "end": "2719130"
  },
  {
    "text": "sage maker how does that fit in what do you guys think about it yeah so so I",
    "start": "2719130",
    "end": "2727249"
  },
  {
    "text": "think that there what's really interesting is that the way that we've thought about this is that there are so",
    "start": "2727249",
    "end": "2734609"
  },
  {
    "text": "many different little pieces of this infrastructure and you know Alfredo talked about five of them but really",
    "start": "2734609",
    "end": "2740759"
  },
  {
    "text": "there are a bunch of even sub pieces within those and you know we we've",
    "start": "2740759",
    "end": "2746999"
  },
  {
    "text": "talked to the sage maker team and I think that what they're doing is they're building it in a very similar fashion",
    "start": "2746999",
    "end": "2752999"
  },
  {
    "text": "and so that means that some of the pieces that we're working on might not",
    "start": "2752999",
    "end": "2759239"
  },
  {
    "text": "you know it might actually make more sense for us to use sage maker in some places and you know in some cases it",
    "start": "2759239",
    "end": "2766380"
  },
  {
    "text": "might make sense because we have such a specific application at Airbnb to use",
    "start": "2766380",
    "end": "2773369"
  },
  {
    "text": "the kind of custom in-house infrastructure that we're building but I think that what's really cool is that it",
    "start": "2773369",
    "end": "2779849"
  },
  {
    "text": "you know anywhere else this means that that same thing can happen where you",
    "start": "2779849",
    "end": "2785069"
  },
  {
    "text": "know somebody can start off with this this big piece of infrastructure that does everything that that they need and",
    "start": "2785069",
    "end": "2790529"
  },
  {
    "text": "then they realize that it doesn't do you know everything that they need because they have something you know very",
    "start": "2790529",
    "end": "2795690"
  },
  {
    "text": "specific in-house and then work on some custom solution but that's way better",
    "start": "2795690",
    "end": "2800699"
  },
  {
    "text": "than you know Alfredo mentioned that we've been working on this for about a year that's way better than a year ago",
    "start": "2800699",
    "end": "2806069"
  },
  {
    "text": "when we were like wow we have to build a huge amount and it's going to take forever to get it to a place where that",
    "start": "2806069",
    "end": "2811410"
  },
  {
    "text": "workflows is you know really fast so great thank you got a question there yes thanks for the",
    "start": "2811410",
    "end": "2817739"
  },
  {
    "text": "presentation so you showed two types of interconnects PCIe with qpi versus",
    "start": "2817739",
    "end": "2823229"
  },
  {
    "text": "unwilling right is it configurable for the customer or how does it work or is it all under the hood",
    "start": "2823229",
    "end": "2828959"
  },
  {
    "text": "yeah so it is it is predefined like once you launch instance size like if you",
    "start": "2828959",
    "end": "2834420"
  },
  {
    "text": "launch a 16 Excel the topology is predefined and you actually get that pre-configured for you",
    "start": "2834420",
    "end": "2840269"
  },
  {
    "text": "so you don't have to go through the process of setting up those links or defining additional parameters to kind of set that up if you go forward like a",
    "start": "2840269",
    "end": "2846630"
  },
  {
    "text": "8xl config you know you basically get like one CPU socket with the for GPS in it you still",
    "start": "2846630",
    "end": "2852560"
  },
  {
    "text": "have any boolean able and that's also already pre-configured for you oh I see okay thanks yep any other questions",
    "start": "2852560",
    "end": "2865930"
  },
  {
    "text": "great well thank you for attending we're gonna hang out for a few minutes if you guys want a stop by and you know say",
    "start": "2872420",
    "end": "2879319"
  },
  {
    "text": "hello we're gonna be if we're gonna be around again thank you and hope you have a great conference [Applause]",
    "start": "2879319",
    "end": "2889429"
  }
]