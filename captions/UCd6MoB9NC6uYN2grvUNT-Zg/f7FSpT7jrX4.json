[
  {
    "text": "hi everyone I'm Andy Koopmans I'm the general manager for Amazon Neptune I",
    "start": "2659",
    "end": "7799"
  },
  {
    "text": "also in addition to Neptune were on Amazon Elastic search service and I was on ElastiCache thanks a lot for joining",
    "start": "7799",
    "end": "15089"
  },
  {
    "text": "us today to hear Nike story in the under graph journey how many of you are using",
    "start": "15089",
    "end": "20369"
  },
  {
    "text": "graph today and how many of those were using graph for using Neptune oh yeah",
    "start": "20369",
    "end": "28890"
  },
  {
    "text": "that's not bad but I see there's a lot of folks you're not using graph yet so let me just tell you a bit about graph a",
    "start": "28890",
    "end": "35760"
  },
  {
    "text": "bit about Neptune and then I'll hand it over to the Nike team so I joined the",
    "start": "35760",
    "end": "42329"
  },
  {
    "text": "Amazon about two and a half years ago and one of the things that really attracted me was this whole graph problem and was really interesting on",
    "start": "42329",
    "end": "50579"
  },
  {
    "text": "this journey is that you know there's a lot of interconnection between data and",
    "start": "50579",
    "end": "55649"
  },
  {
    "text": "existing databases really make it hard to really extract maximum value out of that graph is quite special because",
    "start": "55649",
    "end": "63379"
  },
  {
    "text": "really graph focuses on not the data itself but how the data relates to each other and the kind of use cases that you",
    "start": "63379",
    "end": "72299"
  },
  {
    "text": "can extract out of that are you know super super compelling and actually since we announced Neptune last year",
    "start": "72299",
    "end": "78090"
  },
  {
    "text": "I've really been surprised and positively surprised at how many of you",
    "start": "78090",
    "end": "84750"
  },
  {
    "text": "how many of our customers have been using graph in innovative ways to get",
    "start": "84750",
    "end": "89820"
  },
  {
    "text": "more of their business data and you know graph databases don't replace existing databases but they do allow you to",
    "start": "89820",
    "end": "97229"
  },
  {
    "text": "deliver a whole different new kind of user experience and extract more value out of your data so some really",
    "start": "97229",
    "end": "104220"
  },
  {
    "text": "interesting use cases we've been seeing we've been seeing a lot of social use cases which Nike is gonna be talking",
    "start": "104220",
    "end": "109649"
  },
  {
    "text": "about today a lot of recommendations and a lot of fraud detection and fraud",
    "start": "109649",
    "end": "116340"
  },
  {
    "text": "detection can be IT fraud there can be credit card fraud we actually have a quite a few anti-money laundering use",
    "start": "116340",
    "end": "122640"
  },
  {
    "text": "cases being built on top of Amazon Neptune we've seen drug discovery",
    "start": "122640",
    "end": "127860"
  },
  {
    "text": "examples in the Life Sciences space and we think manufacturing",
    "start": "127860",
    "end": "133690"
  },
  {
    "text": "using graph as a way to model their supply chain and and how the products",
    "start": "133690",
    "end": "140050"
  },
  {
    "text": "have to come together and what the impact could be from a certain part of the supply chain breaking down at",
    "start": "140050",
    "end": "146620"
  },
  {
    "text": "Thomson Reuters is using a Neptune for mapping global tax laws so the power of",
    "start": "146620",
    "end": "154180"
  },
  {
    "text": "graph really spans lots of different industries lots of use cases I know it's",
    "start": "154180",
    "end": "159849"
  },
  {
    "text": "not always easy to kind of figure out how to get started so one of the key focus areas for my team has been to do",
    "start": "159849",
    "end": "166390"
  },
  {
    "text": "create content and create examples and some of them already on our website we're also publishing more and more code",
    "start": "166390",
    "end": "173110"
  },
  {
    "text": "and examples on github and what you're gonna see the next few months is us actually publishing you know more and",
    "start": "173110",
    "end": "180160"
  },
  {
    "text": "more content and examples and just code samples that you can pull down and try and figure out how to get started with",
    "start": "180160",
    "end": "187180"
  },
  {
    "text": "graph so just a couple of words about Neptune itself Neptune is a fully",
    "start": "187180",
    "end": "192430"
  },
  {
    "text": "managed very fast purpose-built graph database you may have heard Andy Jesse",
    "start": "192430",
    "end": "199269"
  },
  {
    "text": "today talk about the purpose of a purpose-built database portfolio our goal is ready to deliver",
    "start": "199269",
    "end": "204959"
  },
  {
    "text": "best-in-class databases for all the different data categories and you know",
    "start": "204959",
    "end": "211299"
  },
  {
    "text": "we believe the best-in-class ultimately is gonna win because getting a graph to a graph database to execute",
    "start": "211299",
    "end": "219010"
  },
  {
    "text": "really really fast and make it easy to use it's very different from making a relational database work fast or making",
    "start": "219010",
    "end": "225280"
  },
  {
    "text": "a key-value store work fast so that's why we did a Neptune and I'm really you",
    "start": "225280",
    "end": "232720"
  },
  {
    "text": "know excited to have the Nike team here that can share how they've used Neptune to build out their social graph and",
    "start": "232720",
    "end": "239650"
  },
  {
    "text": "again I think one of the things that attracted Nike to Neptune was the fact it was very much focused at graph",
    "start": "239650",
    "end": "246489"
  },
  {
    "text": "workloads itself so with no no longer do I'd like to invite mark bogan haim and",
    "start": "246489",
    "end": "253540"
  },
  {
    "text": "Aditya Soni they're from the Nikes user Services team and they'll tell you a bit",
    "start": "253540",
    "end": "260769"
  },
  {
    "text": "more about the Nike social graph joining thank you so much Andy for that awesome",
    "start": "260769",
    "end": "266450"
  },
  {
    "text": "introduction hello reinvent how are y'all doing out there I hope you having a great conference I",
    "start": "266450",
    "end": "272870"
  },
  {
    "text": "certainly am my name is Mark Ryan Haim and I am a senior engineering manager at",
    "start": "272870",
    "end": "278300"
  },
  {
    "text": "Nike as Andy said in the user Services Department we're here today to talk to",
    "start": "278300",
    "end": "284480"
  },
  {
    "text": "you about our journey of building a social graph on top of Amazon Neptune",
    "start": "284480",
    "end": "290650"
  },
  {
    "text": "now you might be wondering why is Nike building a social graph not everyone in",
    "start": "290650",
    "end": "298370"
  },
  {
    "text": "this room might be familiar with some of our mobile experiences like the Nike run",
    "start": "298370",
    "end": "303830"
  },
  {
    "text": "Club Nike Training Club or a Nike app or our sneakers application so I wanted to",
    "start": "303830",
    "end": "311840"
  },
  {
    "text": "spend some time at the beginning of the session to set some context but how we use the social graph and power a variety",
    "start": "311840",
    "end": "319010"
  },
  {
    "text": "of features within these applications the first implementation of our graph",
    "start": "319010",
    "end": "325010"
  },
  {
    "text": "was actually on top of Oracle in the data center we then evolved and moved to",
    "start": "325010",
    "end": "331130"
  },
  {
    "text": "the cloud and there we used Cassandra as our back-end data store we had a variety",
    "start": "331130",
    "end": "337040"
  },
  {
    "text": "of challenges with Cassandra and I'll go into some detail on what those were then",
    "start": "337040",
    "end": "344210"
  },
  {
    "text": "the business came to us and said we love what you have with your graph but can",
    "start": "344210",
    "end": "349970"
  },
  {
    "text": "you do this super-sized at a much larger scale so I'll talk about that I also",
    "start": "349970",
    "end": "357230"
  },
  {
    "text": "want to give it a little bit of a plug to the AWS data lab and how that helped",
    "start": "357230",
    "end": "362960"
  },
  {
    "text": "us to get to a working proof of concept in a very short time period after that",
    "start": "362960",
    "end": "370700"
  },
  {
    "text": "Adithya will come back up here and talk to you all about our Neptune implementation as well as how we did",
    "start": "370700",
    "end": "377690"
  },
  {
    "text": "data migration on a live 24 by 7 system one thing of note we will not have open",
    "start": "377690",
    "end": "386690"
  },
  {
    "text": "mic QA at the end of this session but if you do have technical questions we're",
    "start": "386690",
    "end": "392150"
  },
  {
    "text": "more than happy to stick around for a few minutes after so just come up to the stage and we around and we'll give you your answers",
    "start": "392150",
    "end": "400480"
  },
  {
    "text": "but let's start with setting some context about our Nike experiences I",
    "start": "400480",
    "end": "407890"
  },
  {
    "text": "want to start with our Nike run clip app or any receive for short as the name",
    "start": "407890",
    "end": "414380"
  },
  {
    "text": "implies the NRC application is really all about the runners out there it is",
    "start": "414380",
    "end": "420740"
  },
  {
    "text": "designed to keep metrics that runners are interested in things like pace time",
    "start": "420740",
    "end": "426550"
  },
  {
    "text": "shortest run longest run fastest 5k all",
    "start": "426550",
    "end": "431690"
  },
  {
    "text": "of those things but in addition to basically keeping those metrics we also",
    "start": "431690",
    "end": "438620"
  },
  {
    "text": "wanted to build a runners community around this experience it is in that",
    "start": "438620",
    "end": "444230"
  },
  {
    "text": "vein that we introduced the friending feature within this application the",
    "start": "444230",
    "end": "450500"
  },
  {
    "text": "friending feature allows Nike users to friend other Nike users within this",
    "start": "450500",
    "end": "455750"
  },
  {
    "text": "ecosystem we then store these relationships inside of our graph it is",
    "start": "455750",
    "end": "463640"
  },
  {
    "text": "these relationships that power the leaderboard feature which you see app on",
    "start": "463640",
    "end": "469040"
  },
  {
    "text": "the left hand side of this slide and essentially brings in some of the",
    "start": "469040",
    "end": "475070"
  },
  {
    "text": "competitive spirit within this experience I mean who wouldn't want to be at the top spot right beating all",
    "start": "475070",
    "end": "482210"
  },
  {
    "text": "your friends it motivates the runners to keep running and also keeps the users",
    "start": "482210",
    "end": "489470"
  },
  {
    "text": "more engaged with this experience to give you some idea about scale we have",
    "start": "489470",
    "end": "497150"
  },
  {
    "text": "more than 25 million users that are part of this domain they have more than 150",
    "start": "497150",
    "end": "504170"
  },
  {
    "text": "million relationships between all of those users so aside from finding",
    "start": "504170",
    "end": "514070"
  },
  {
    "text": "friends organically within the ecosystem we have another feature which is our",
    "start": "514070",
    "end": "519349"
  },
  {
    "text": "friends recommendation engine and essentially we wanted to write to put",
    "start": "519349",
    "end": "525410"
  },
  {
    "text": "this in production for all of our user base but we found out that if we had users that had more than 50",
    "start": "525410",
    "end": "532910"
  },
  {
    "text": "friends we simply couldn't compute this at runtime because it was too slow so",
    "start": "532910",
    "end": "539720"
  },
  {
    "text": "our MVP was if he had 50 friends or less you got to see this feature we wanted to",
    "start": "539720",
    "end": "546020"
  },
  {
    "text": "enable this for all the users however so we came up with EMR and a sport process",
    "start": "546020",
    "end": "551870"
  },
  {
    "text": "to calculate all this data beforehand then write the results into a dynamo",
    "start": "551870",
    "end": "557240"
  },
  {
    "text": "table and it was served from there",
    "start": "557240",
    "end": "561820"
  },
  {
    "text": "another domain that sits on top of our graph is our interest domain it is used",
    "start": "563920",
    "end": "570290"
  },
  {
    "text": "in quite a few different applications I'm showing two of them here on the left",
    "start": "570290",
    "end": "576440"
  },
  {
    "text": "hand side you see our Nike app this is our e-commerce application you can think",
    "start": "576440",
    "end": "583910"
  },
  {
    "text": "of it as nike comm on your iPhone or your Android during the onboarding",
    "start": "583910",
    "end": "590000"
  },
  {
    "text": "process a user is asked which sports they're most interested in so in this",
    "start": "590000",
    "end": "596720"
  },
  {
    "text": "case they selected running and training we then store this data in the graph and",
    "start": "596720",
    "end": "602780"
  },
  {
    "text": "it allows us to personalize the experience for the user",
    "start": "602780",
    "end": "608410"
  },
  {
    "text": "during content creation for the feed we take into consideration which sports the",
    "start": "608410",
    "end": "614810"
  },
  {
    "text": "user is actually interested in this means this particular user would see",
    "start": "614810",
    "end": "619940"
  },
  {
    "text": "many more stories about running and training compared to let's say basketball soccer or tennis for example",
    "start": "619940",
    "end": "630070"
  },
  {
    "text": "outside of these what we call onboarding interests we also allow the user to",
    "start": "630070",
    "end": "635600"
  },
  {
    "text": "follow a variety of sports teams these are from the NHL the NBA the NFL and",
    "start": "635600",
    "end": "642650"
  },
  {
    "text": "even some soccer teams again storing this data tells us more about the user",
    "start": "642650",
    "end": "649760"
  },
  {
    "text": "and what they're interested in seeing so it allows us to personalize the experience for the user the other",
    "start": "649760",
    "end": "658280"
  },
  {
    "text": "application where interest is used is our sneakers app the sneakers app is really designed for",
    "start": "658280",
    "end": "664940"
  },
  {
    "text": "all the sneaker heads out there these are users that are interested in the latest and greatest sneakers like",
    "start": "664940",
    "end": "672980"
  },
  {
    "text": "these vapor makes plus for example and usually demand far outstrips the supply",
    "start": "672980",
    "end": "680260"
  },
  {
    "text": "so it can be somewhat challenging to purchase this product the users in this",
    "start": "680260",
    "end": "685580"
  },
  {
    "text": "space are very passionate and they would like to purchase the product so if you get to see the screen got'em that means",
    "start": "685580",
    "end": "693470"
  },
  {
    "text": "yes you made it and you were able to purchase this hot item now in the",
    "start": "693470",
    "end": "698780"
  },
  {
    "text": "background we store this data as an interest inside of our graph we do this",
    "start": "698780",
    "end": "705560"
  },
  {
    "text": "for two reasons one because we can use it for order history so when you come",
    "start": "705560",
    "end": "711980"
  },
  {
    "text": "back into the experience we see you see which purchase which product you have",
    "start": "711980",
    "end": "717590"
  },
  {
    "text": "purchased but also it informs us again what's the user actually interested in",
    "start": "717590",
    "end": "723740"
  },
  {
    "text": "and we can do things like product recommendations on prior purchases to",
    "start": "723740",
    "end": "729980"
  },
  {
    "text": "give you some idea of scale within this domain we have more than 300 thousand",
    "start": "729980",
    "end": "735410"
  },
  {
    "text": "interests in this domain and more than 15 million users are following at least",
    "start": "735410",
    "end": "741380"
  },
  {
    "text": "one interest overall we store more than 100 million user interest relationships",
    "start": "741380",
    "end": "748370"
  },
  {
    "text": "within the domain last but not least we",
    "start": "748370",
    "end": "753770"
  },
  {
    "text": "have our linking domain also powered by our graph our users have told us listen",
    "start": "753770",
    "end": "760580"
  },
  {
    "text": "we don't want to remember yet another user ID password just to log into the",
    "start": "760580",
    "end": "765680"
  },
  {
    "text": "Nike account so can you make that easier and we said sure so we allow you to link various social",
    "start": "765680",
    "end": "772970"
  },
  {
    "text": "networks with your Nike account in the US this is predominantly Facebook but we",
    "start": "772970",
    "end": "780380"
  },
  {
    "text": "also have variety of Asian networks that we support like WeChat in China for",
    "start": "780380",
    "end": "785750"
  },
  {
    "text": "example or lion in Japan more than 15",
    "start": "785750",
    "end": "791090"
  },
  {
    "text": "million users have elected to link their accounts in this fashion",
    "start": "791090",
    "end": "796120"
  },
  {
    "text": "here you see a couple of reed operations that happen on our graph I will not go",
    "start": "797019",
    "end": "804410"
  },
  {
    "text": "through all of them but I would like to point out that get list of friends is on",
    "start": "804410",
    "end": "809629"
  },
  {
    "text": "there twice this is because we need to support a variety of different client",
    "start": "809629",
    "end": "815809"
  },
  {
    "text": "access patterns there's a paginating flavor and there's a server-to-server flavor the paginating flavor is really",
    "start": "815809",
    "end": "824629"
  },
  {
    "text": "used for the mobile experience let's assume I have 200 friends I really don't",
    "start": "824629",
    "end": "829910"
  },
  {
    "text": "but let's assume we're not going to display all of them at once on the screen right so we might display 20 of",
    "start": "829910",
    "end": "837619"
  },
  {
    "text": "them and then as I scroll through the list calls get made back to the server",
    "start": "837619",
    "end": "842990"
  },
  {
    "text": "to give me more batches of 20 so that it can continue to scroll now the",
    "start": "842990",
    "end": "849529"
  },
  {
    "text": "leaderboard feature I just talked about all that data is calculated on the server side so their pattern is slightly",
    "start": "849529",
    "end": "857149"
  },
  {
    "text": "different and that they would like to get all the data as quickly as they can",
    "start": "857149",
    "end": "862509"
  },
  {
    "text": "this is true whether we have 5 friends 500 friends or like some of our users",
    "start": "862509",
    "end": "869350"
  },
  {
    "text": "60,000 I would like to have you keep that in mind that we have these",
    "start": "869350",
    "end": "875269"
  },
  {
    "text": "different client access patterns because I'll refer to it again later so here then is a high-level",
    "start": "875269",
    "end": "882410"
  },
  {
    "text": "architecture of our overall domain it's a busy slide so let me walk you through",
    "start": "882410",
    "end": "888230"
  },
  {
    "text": "it the users experience starts when requests hit our front door which is",
    "start": "888230",
    "end": "894499"
  },
  {
    "text": "implemented an API gateway from there they go to the edge router which is",
    "start": "894499",
    "end": "900079"
  },
  {
    "text": "lambda based and it basically facilitates authentication and authorization from their traffic flows",
    "start": "900079",
    "end": "908149"
  },
  {
    "text": "to our domain router from there it goes to the various domains that I just",
    "start": "908149",
    "end": "913490"
  },
  {
    "text": "talked about friending interest linking and recommendations this slide is",
    "start": "913490",
    "end": "921170"
  },
  {
    "text": "slightly outdated as it still shows Cassandra is our back-end data store",
    "start": "921170",
    "end": "926709"
  },
  {
    "text": "in addition to storing data in the data store there's a variety of teams that",
    "start": "926709",
    "end": "932990"
  },
  {
    "text": "need to know when changes in the graph are happening so for that purpose we put events onto an SNS topic and they",
    "start": "932990",
    "end": "940339"
  },
  {
    "text": "basically hung up a queue and that is how they get that information we at Nike",
    "start": "940339",
    "end": "947000"
  },
  {
    "text": "take our users privacy very very seriously so for that reason we have yet",
    "start": "947000",
    "end": "955220"
  },
  {
    "text": "one more micro service which is our user visibility service this service",
    "start": "955220",
    "end": "960639"
  },
  {
    "text": "responsibility is to check a user's privacy setting at runtime what we do",
    "start": "960639",
    "end": "967759"
  },
  {
    "text": "not want to do is hand out data for users that have explicitly told us that",
    "start": "967759",
    "end": "973880"
  },
  {
    "text": "they would like to remain private so",
    "start": "973880",
    "end": "979430"
  },
  {
    "text": "let's talk about some of those challenges that we had with Kassandra I don't want you to get the wrong",
    "start": "979430",
    "end": "986089"
  },
  {
    "text": "impression I don't want to bash on Cassandra it is an awesome key value store and for",
    "start": "986089",
    "end": "993170"
  },
  {
    "text": "certain use cases it's the best thing out there however a key value store is",
    "start": "993170",
    "end": "1000970"
  },
  {
    "text": "not the best use case when you have graft domain problems if you want to ask",
    "start": "1000970",
    "end": "1006939"
  },
  {
    "text": "questions like what's the proximity of two nodes inside the graph or what's the",
    "start": "1006939",
    "end": "1012579"
  },
  {
    "text": "shortest path where you want to do things like inferencing Cassandra simply",
    "start": "1012579",
    "end": "1018279"
  },
  {
    "text": "won't help you it will store your data but all the questions you want to answer",
    "start": "1018279",
    "end": "1023500"
  },
  {
    "text": "you have to implement in your service layer we modeled our data in a graph",
    "start": "1023500",
    "end": "1030280"
  },
  {
    "text": "like structure A or B where a is some",
    "start": "1030280",
    "end": "1035288"
  },
  {
    "text": "entity having a relationship r with another entity B and even though we",
    "start": "1035289",
    "end": "1041319"
  },
  {
    "text": "modeled it this way because Cassandra doesn't have a graph engine the database",
    "start": "1041319",
    "end": "1046928"
  },
  {
    "text": "itself couldn't help us another issue we",
    "start": "1046929",
    "end": "1051940"
  },
  {
    "text": "ran into was simply that of maintenance so doing things like updating the",
    "start": "1051940",
    "end": "1059590"
  },
  {
    "text": "underlying OS or deploying security fixes took a long time given our data volume and our",
    "start": "1059590",
    "end": "1067870"
  },
  {
    "text": "traffic we had to run quite a large cluster just updating one of those nodes",
    "start": "1067870",
    "end": "1074310"
  },
  {
    "text": "took between 90 minutes to two hours so to update the entire ring was a",
    "start": "1074310",
    "end": "1081370"
  },
  {
    "text": "multi-day operation all of this took time away from being able to focus on",
    "start": "1081370",
    "end": "1087850"
  },
  {
    "text": "creating new features and focusing on application code now we at Nike we run",
    "start": "1087850",
    "end": "1095140"
  },
  {
    "text": "the DevOps model so it was my team's responsibility to make sure that this",
    "start": "1095140",
    "end": "1100780"
  },
  {
    "text": "ecosystem is healthy we didn't rely on any sort of separate team another issue",
    "start": "1100780",
    "end": "1108340"
  },
  {
    "text": "we had was that a scalability we were on a slightly older version of the data",
    "start": "1108340",
    "end": "1114970"
  },
  {
    "text": "stacks community edition we didn't have V notes available to us so this meant if",
    "start": "1114970",
    "end": "1121960"
  },
  {
    "text": "we needed more capacity our only option was to double the ring or half the ring",
    "start": "1121960",
    "end": "1129060"
  },
  {
    "text": "while doubling the ring unfortunately also came with doubling the maintenance pain I just talked about in addition to",
    "start": "1129060",
    "end": "1137200"
  },
  {
    "text": "that it didn't give us enough elasticity to basically have the ring at a size where we went over pain all in all all",
    "start": "1137200",
    "end": "1147520"
  },
  {
    "text": "of these problems are related to the fact that we needed to manage this infrastructure and it wasn't managed for",
    "start": "1147520",
    "end": "1154750"
  },
  {
    "text": "us we knew that whatever the next database that we were going to use had",
    "start": "1154750",
    "end": "1160930"
  },
  {
    "text": "to be managed for us so we wouldn't have to worry about managing the infrastructure and could focus on coming",
    "start": "1160930",
    "end": "1169090"
  },
  {
    "text": "up with new features and writing application code instead here's a couple",
    "start": "1169090",
    "end": "1175870"
  },
  {
    "text": "of more challenges about Cassandra that are closer related to the way Cassandra works rather than the infrastructure in",
    "start": "1175870",
    "end": "1184720"
  },
  {
    "text": "Cassandra the data is accessed via partition key we build a variety of",
    "start": "1184720",
    "end": "1190690"
  },
  {
    "text": "materialized views in order to give the data to our clients",
    "start": "1190690",
    "end": "1196780"
  },
  {
    "text": "now remember I had said that we had different client access patterns well",
    "start": "1196780",
    "end": "1202310"
  },
  {
    "text": "because of that we needed to create a variety of materialized views",
    "start": "1202310",
    "end": "1207910"
  },
  {
    "text": "materialized views are great because they're very performant on the read side",
    "start": "1207910",
    "end": "1213070"
  },
  {
    "text": "however the downside is that your data model becomes fairly inflexible and as",
    "start": "1213070",
    "end": "1219920"
  },
  {
    "text": "more and more patterns client access patterns emerge you have to build more and more materialized views making the",
    "start": "1219920",
    "end": "1226940"
  },
  {
    "text": "entire ecosystem more and more complicated Cassandra will tell you that",
    "start": "1226940",
    "end": "1233150"
  },
  {
    "text": "they support two billion objects in a wide row and man does that sound impressive however in reality in order",
    "start": "1233150",
    "end": "1241280"
  },
  {
    "text": "to hit our read latency requirements we found out that we could only store roughly a hundred thousand objects",
    "start": "1241280",
    "end": "1248750"
  },
  {
    "text": "within a wide row now imagine an interest that is very popular like",
    "start": "1248750",
    "end": "1254000"
  },
  {
    "text": "running which is followed by 13 million users at 100k width that meant we needed",
    "start": "1254000",
    "end": "1261590"
  },
  {
    "text": "to store a hundred and thirty rows in the database just to store this data",
    "start": "1261590",
    "end": "1267430"
  },
  {
    "text": "again making the ecosystem more complicated than we wanted it to be we",
    "start": "1267430",
    "end": "1275000"
  },
  {
    "text": "did not have averaging within the version of Cassandra that we were running so there was no sum or group I",
    "start": "1275000",
    "end": "1281690"
  },
  {
    "text": "or any of that so we had difficulty running analytical queries answering simple questions like what is my average",
    "start": "1281690",
    "end": "1289310"
  },
  {
    "text": "friend count across the entire ecosystem again we had to rely upon EMR and spark",
    "start": "1289310",
    "end": "1296810"
  },
  {
    "text": "and writing code in order to answer those questions because the database alone was not helpful right performance",
    "start": "1296810",
    "end": "1305720"
  },
  {
    "text": "in Cassandra is usually pretty good but even that has a downside and that it",
    "start": "1305720",
    "end": "1312920"
  },
  {
    "text": "uses tombstones in order to do deletions now for those of you who don't know",
    "start": "1312920",
    "end": "1318380"
  },
  {
    "text": "Cassandra I know what tombstones are essentially it's Cassandra sway a flag in a record for deletion at a later",
    "start": "1318380",
    "end": "1325389"
  },
  {
    "text": "time using a process called compaction earlier this year one of our partner",
    "start": "1325389",
    "end": "1332859"
  },
  {
    "text": "teams at Nike needed to update a couple hundred thousand interest in our domain",
    "start": "1332859",
    "end": "1338099"
  },
  {
    "text": "so they issued all these updates but what they didn't realize was that we",
    "start": "1338099",
    "end": "1343869"
  },
  {
    "text": "were caching some of this data so when they did a repack they didn't quite get",
    "start": "1343869",
    "end": "1349089"
  },
  {
    "text": "the data that they had anticipated so what did they do well they ran the updates again and then a third time for",
    "start": "1349089",
    "end": "1357099"
  },
  {
    "text": "good measure well all of these updates caused massive amounts of tombstoning",
    "start": "1357099",
    "end": "1363159"
  },
  {
    "text": "within the Cassandra Ring it took us a couple of days to fine-tune the",
    "start": "1363159",
    "end": "1369729"
  },
  {
    "text": "compaction process combating these tombstones all while trying to remain up",
    "start": "1369729",
    "end": "1375820"
  },
  {
    "text": "and have read operations still happening those were not fun days I can tell you",
    "start": "1375820",
    "end": "1381579"
  },
  {
    "text": "that so we were running this",
    "start": "1381579",
    "end": "1386679"
  },
  {
    "text": "infrastructure for two maybe two and a half years or so when the business came",
    "start": "1386679",
    "end": "1391959"
  },
  {
    "text": "to us and said listen we love what you have with your graph but we really want",
    "start": "1391959",
    "end": "1397059"
  },
  {
    "text": "you to like when there's a much rather scale we were concerned that our current",
    "start": "1397059",
    "end": "1403079"
  },
  {
    "text": "Cassandra infrastructure wasn't up to the task so that's when we seriously looked at Neptune as a possible solution",
    "start": "1403079",
    "end": "1412499"
  },
  {
    "text": "on the cassandra side we were storing bi-directional relationships now what",
    "start": "1413099",
    "end": "1420039"
  },
  {
    "text": "does that mean it means if I invite Andy to be my friend and he accepts that",
    "start": "1420039",
    "end": "1425889"
  },
  {
    "text": "request we stored two records in the database one indicating that I'm a",
    "start": "1425889",
    "end": "1432190"
  },
  {
    "text": "friend of Andy's and then the inverse of Andy is a friend of mine the business",
    "start": "1432190",
    "end": "1438639"
  },
  {
    "text": "asked us to allow unidirectional relationships within the graph",
    "start": "1438639",
    "end": "1445739"
  },
  {
    "text": "essentially having public entities inside the graph that you can't just be followed",
    "start": "1445739",
    "end": "1451269"
  },
  {
    "text": "removing the friction of having to accept some sort of request",
    "start": "1451269",
    "end": "1457010"
  },
  {
    "text": "in the fending model the bi-directional model we did have users that had 60 or",
    "start": "1457010",
    "end": "1464910"
  },
  {
    "text": "70 thousand friends so it was 1 to 60 or basically 1 to thousands of friends type",
    "start": "1464910",
    "end": "1471390"
  },
  {
    "text": "scale but with removing the friction we really felt that that would balloon to",
    "start": "1471390",
    "end": "1477330"
  },
  {
    "text": "1/2 million type scale the last point on",
    "start": "1477330",
    "end": "1482490"
  },
  {
    "text": "this slide again is simply to say Cassandra was unmanaged we really needed",
    "start": "1482490",
    "end": "1488490"
  },
  {
    "text": "something to be managed for us on the data layer so we wouldn't have to worry about so how then do we get to 1/2",
    "start": "1488490",
    "end": "1496020"
  },
  {
    "text": "million type scale well as you might anticipate it's by putting very",
    "start": "1496020",
    "end": "1503610"
  },
  {
    "text": "well-known Nike contracted athletes into the ecosystem think people like Serena",
    "start": "1503610",
    "end": "1511140"
  },
  {
    "text": "Williams Cristiano Ronaldo or LeBron James for example and you can easily see",
    "start": "1511140",
    "end": "1517920"
  },
  {
    "text": "how many many more people might want to follow these public influencers inside",
    "start": "1517920",
    "end": "1523530"
  },
  {
    "text": "the graph we were very concerned about",
    "start": "1523530",
    "end": "1528830"
  },
  {
    "text": "performance as I said we didn't think asana was up to the task",
    "start": "1528830",
    "end": "1534540"
  },
  {
    "text": "so we started to look at Neptune but we wanted to make sure that Neptune was up",
    "start": "1534540",
    "end": "1540630"
  },
  {
    "text": "to the task so we did a lot of performance testing between the two systems this is not exactly an apples to",
    "start": "1540630",
    "end": "1548550"
  },
  {
    "text": "apples comparison as we use the 9 node to Sandra ring comparing it to just one",
    "start": "1548550",
    "end": "1554850"
  },
  {
    "text": "readwrite note on the Neptune side but as you can see for small reads of 10 5k",
    "start": "1554850",
    "end": "1562860"
  },
  {
    "text": "and 10k Neptune actually performed better than the entire Cassandra ring",
    "start": "1562860",
    "end": "1569120"
  },
  {
    "text": "but what about those super node scenarios 10 million users following",
    "start": "1569120",
    "end": "1575010"
  },
  {
    "text": "Serena Williams again this was run on just one readwrite node and while these",
    "start": "1575010",
    "end": "1582150"
  },
  {
    "text": "numbers may not be overly impressive being in the seconds when everything these days is in milliseconds or",
    "start": "1582150",
    "end": "1588360"
  },
  {
    "text": "microseconds it gave us enough confidence that Neptune was actually up to this task",
    "start": "1588360",
    "end": "1594820"
  },
  {
    "text": "it didn't topple over there weren't any errors and we certainly could speed",
    "start": "1594820",
    "end": "1599890"
  },
  {
    "text": "things up by adding more read replicas so this is like worst case scenario as",
    "start": "1599890",
    "end": "1606190"
  },
  {
    "text": "you can see anything that had a thousand edges or less was very performant so let",
    "start": "1606190",
    "end": "1617350"
  },
  {
    "text": "me plug a little bit this great AWS offering called the data lab we use this",
    "start": "1617350",
    "end": "1626260"
  },
  {
    "text": "in order to come up with a working proof of concept within just one week imagine",
    "start": "1626260",
    "end": "1635020"
  },
  {
    "text": "a world where your manager comes to you and says I need you to build this new system and you need to use this new",
    "start": "1635020",
    "end": "1642580"
  },
  {
    "text": "fandangled AWS technology that you don't really know that much about so you go",
    "start": "1642580",
    "end": "1649360"
  },
  {
    "text": "off and start building and eventually you run into some issues those might be",
    "start": "1649360",
    "end": "1655840"
  },
  {
    "text": "performance related where things just aren't working quite the way you had anticipated so what's a lonely engineer",
    "start": "1655840",
    "end": "1663820"
  },
  {
    "text": "to do well if you're like me you probably Google for an answer right you go to Stack Overflow or all",
    "start": "1663820",
    "end": "1670960"
  },
  {
    "text": "these other places to see if other people have run into this problem and see how they solved it let's say you",
    "start": "1670960",
    "end": "1677950"
  },
  {
    "text": "don't find a good answer for your problem set you might open a ticket with AWS but this isn't in production yet so",
    "start": "1677950",
    "end": "1685990"
  },
  {
    "text": "you won't open their emergency-type and get a response in 15 minutes it might take four hours it might take",
    "start": "1685990",
    "end": "1693400"
  },
  {
    "text": "eight there might be a lot of back-and-forth for them to really understand what your problem is and how",
    "start": "1693400",
    "end": "1698410"
  },
  {
    "text": "to help you if they don't know the answer they might have to reach out to",
    "start": "1698410",
    "end": "1704290"
  },
  {
    "text": "some subject matter expert inside of Amazon delaying an answer even further",
    "start": "1704290",
    "end": "1711030"
  },
  {
    "text": "now imagine this very same scenario and this subject matter expert sits across",
    "start": "1711030",
    "end": "1717610"
  },
  {
    "text": "the table from you now that is the power of the data lab it has held Admiral headquarters in",
    "start": "1717610",
    "end": "1724890"
  },
  {
    "text": "Seattle and essentially you have access to whatever domain experts that you need",
    "start": "1724890",
    "end": "1731640"
  },
  {
    "text": "and it really shortens the feedback cycle to that point I sent four of my",
    "start": "1731640",
    "end": "1738419"
  },
  {
    "text": "engineers up to Seattle to work on this proof-of-concept and they ran into not",
    "start": "1738419",
    "end": "1744090"
  },
  {
    "text": "just one but two gremlin driver bugs on the second day this did not slow them",
    "start": "1744090",
    "end": "1751169"
  },
  {
    "text": "down however because the domain experts in Amazon came up with a workaround the",
    "start": "1751169",
    "end": "1756929"
  },
  {
    "text": "very next day so I can only highly suggest to reach out to your Tam's",
    "start": "1756929",
    "end": "1762720"
  },
  {
    "text": "and when you work on things that are new technology and familiar to you to use",
    "start": "1762720",
    "end": "1768539"
  },
  {
    "text": "the data lab to get bootstrapped on new technology now it's a dufus turn and",
    "start": "1768539",
    "end": "1775740"
  },
  {
    "text": "he's going to talk to you about our actual Neptune implementation as well as",
    "start": "1775740",
    "end": "1780929"
  },
  {
    "text": "how do we do data migration on a system that's never done thank you Mark so I'm",
    "start": "1780929",
    "end": "1789090"
  },
  {
    "text": "a detail I'm going to talk about how we implemented our social graph in using",
    "start": "1789090",
    "end": "1794309"
  },
  {
    "text": "Neptune how did we do data migration and how did we do production cutover so",
    "start": "1794309",
    "end": "1800820"
  },
  {
    "text": "before I talk about Neptune implementation I would like to show you over Cassandra data model for interest",
    "start": "1800820",
    "end": "1807299"
  },
  {
    "text": "so think about the scenario we would like to get for a given interest ID give",
    "start": "1807299",
    "end": "1814350"
  },
  {
    "text": "me all the users who are following that particularly interest as Mark mentioned earlier Cassandra is basically key value",
    "start": "1814350",
    "end": "1822659"
  },
  {
    "text": "store you cannot join two tables so we had multiple tables that we needed a",
    "start": "1822659",
    "end": "1828179"
  },
  {
    "text": "query to just to get one simple answer so as you see on your screen there is",
    "start": "1828179",
    "end": "1833820"
  },
  {
    "text": "interest table where we were storing interest ID and other metadata another",
    "start": "1833820",
    "end": "1839940"
  },
  {
    "text": "table is relationship timeline we are forgiving user and relationship and",
    "start": "1839940",
    "end": "1845520"
  },
  {
    "text": "interest we were storing timeline of that data third table is partitioned table as Mark mentioned that we have to",
    "start": "1845520",
    "end": "1854669"
  },
  {
    "text": "store data in multiple power tition because of wide roam so what we had to do for given interest ID and",
    "start": "1854669",
    "end": "1861180"
  },
  {
    "text": "relationship we had to get a list of all partition IDs once we had list of",
    "start": "1861180",
    "end": "1866640"
  },
  {
    "text": "partition IDs we were creating another table called object index which was",
    "start": "1866640",
    "end": "1872100"
  },
  {
    "text": "storing for a given partition ID list of users so this is just one example think",
    "start": "1872100",
    "end": "1879090"
  },
  {
    "text": "about all of the use cases that Mark mentioned earlier we had literally more than 13 tables in interest domain",
    "start": "1879090",
    "end": "1885890"
  },
  {
    "text": "there's another table called count if you want to get count of user who are",
    "start": "1885890",
    "end": "1891180"
  },
  {
    "text": "following your interest we had to store that as well for each partition ID we",
    "start": "1891180",
    "end": "1896670"
  },
  {
    "text": "were storing count for each partition so how did we simplify this data model",
    "start": "1896670",
    "end": "1903920"
  },
  {
    "text": "because we are using social graph it fits perfectly in Grambling property",
    "start": "1903920",
    "end": "1909030"
  },
  {
    "text": "graph so as you see on your screen that we have friends domain and interest",
    "start": "1909030",
    "end": "1915630"
  },
  {
    "text": "domain represented in single data model all of our query they were simplified in addition to",
    "start": "1915630",
    "end": "1923340"
  },
  {
    "text": "simplified query we got another advantage like wearing a cross domain",
    "start": "1923340",
    "end": "1929570"
  },
  {
    "text": "think about the scenario I want to get lists of friends who are following",
    "start": "1929570",
    "end": "1935160"
  },
  {
    "text": "running in case of Cassandra I had to query two tables first I needed to get",
    "start": "1935160",
    "end": "1941010"
  },
  {
    "text": "all of my friends once I get all the result we had to query that for all of",
    "start": "1941010",
    "end": "1947160"
  },
  {
    "text": "my friend tell me who are following running but in Neptune we can write or",
    "start": "1947160",
    "end": "1953220"
  },
  {
    "text": "we are writing as one simple query which gives me this answer another benefit",
    "start": "1953220",
    "end": "1959460"
  },
  {
    "text": "that we got out of this particular data model that in Cassandra all of our",
    "start": "1959460",
    "end": "1966900"
  },
  {
    "text": "business logic was in service layer for example you want to get for a given user",
    "start": "1966900",
    "end": "1974040"
  },
  {
    "text": "who give me their friends of friends so in Cassandra from service layer it",
    "start": "1974040",
    "end": "1979110"
  },
  {
    "text": "was calling dollar to get list of friends once it get the list of friends from service layer we were making",
    "start": "1979110",
    "end": "1986010"
  },
  {
    "text": "another call to dollar that for each friends give me their friends and then returning result to",
    "start": "1986010",
    "end": "1991620"
  },
  {
    "text": "user but in action we can write one simple query on da layer which gives me",
    "start": "1991620",
    "end": "1998100"
  },
  {
    "text": "all of this data so all of the business logic is on my query layer on my da",
    "start": "1998100",
    "end": "2003500"
  },
  {
    "text": "layer there is no logic on service layer so here is one example for Neptune query",
    "start": "2003500",
    "end": "2010880"
  },
  {
    "text": "to get list of followers as you see it's straightforward for a given source",
    "start": "2010880",
    "end": "2017030"
  },
  {
    "text": "vertex which is user ID we will get all the IDS which is connected to that",
    "start": "2017030",
    "end": "2022850"
  },
  {
    "text": "source ID with edge called follows another thing that you see that instead",
    "start": "2022850",
    "end": "2029660"
  },
  {
    "text": "of getting all the data I'm doing a while loop where while that particular",
    "start": "2029660",
    "end": "2035120"
  },
  {
    "text": "graph traversal has next element give me next 10000 reason behind this particular",
    "start": "2035120",
    "end": "2042230"
  },
  {
    "text": "code when you create your own action cluster you provide a query timeout so",
    "start": "2042230",
    "end": "2048470"
  },
  {
    "text": "if you set a big query timeout depending on how big your graph is your query might timeout in our case we have users",
    "start": "2048470",
    "end": "2057139"
  },
  {
    "text": "with like millions of followers so that means if he has smaller query timeout that particular query might timeout so",
    "start": "2057140",
    "end": "2064639"
  },
  {
    "text": "that's why we are getting using the next operator to get 10000 results in each",
    "start": "2064640",
    "end": "2071210"
  },
  {
    "text": "iteration that could protect against any query timeout that you will have so how",
    "start": "2071210",
    "end": "2080300"
  },
  {
    "text": "you programmatically access your graph we use gremlin language variant which is",
    "start": "2080300",
    "end": "2086750"
  },
  {
    "text": "glv it's basically library which allows you to programmatically access your",
    "start": "2086750",
    "end": "2092090"
  },
  {
    "text": "graph traversal so we use Java and that's why we are using Java glv so",
    "start": "2092090",
    "end": "2098140"
  },
  {
    "text": "important thing is before you you can create your graph traversal you have to",
    "start": "2098140",
    "end": "2103820"
  },
  {
    "text": "construct a cluster builder so here I am providing my Neptune cluster host and",
    "start": "2103820",
    "end": "2109790"
  },
  {
    "text": "port in addition to Neptune cluster host and port there are some connection",
    "start": "2109790",
    "end": "2116090"
  },
  {
    "text": "pooling specific properties that you have to define depending on size of your",
    "start": "2116090",
    "end": "2121250"
  },
  {
    "text": "Neptune cluster that you have were using and size of size of your instances where your server is running",
    "start": "2121250",
    "end": "2128110"
  },
  {
    "text": "you have to do some performance testing to come up with this number so once we",
    "start": "2128110",
    "end": "2136020"
  },
  {
    "text": "created our data model once we had all the queries figured out and we know how",
    "start": "2136020",
    "end": "2141610"
  },
  {
    "text": "we are programmatically querying our graph next thing we wanted to know how",
    "start": "2141610",
    "end": "2146650"
  },
  {
    "text": "do we monitor over neptune cluster good news is Neptune and cloud wat Neptune",
    "start": "2146650",
    "end": "2155080"
  },
  {
    "text": "and cloud watch are integrated so that means you get lot of metrics related related to your Neptune cluster in cloud",
    "start": "2155080",
    "end": "2162520"
  },
  {
    "text": "watch so you can create your custom dashboard you can create your custom alerts using those metrics so here are",
    "start": "2162520",
    "end": "2169150"
  },
  {
    "text": "some example we are getting CPU utilization Network throughput Grambling",
    "start": "2169150",
    "end": "2176140"
  },
  {
    "text": "error count of Grambling requests and gremlin requests per second but there",
    "start": "2176140",
    "end": "2181480"
  },
  {
    "text": "are lots of other metrics that you can get out of cloud watch metrics so so far",
    "start": "2181480",
    "end": "2187900"
  },
  {
    "text": "it sounds very easy that you can implement using Neptune you can your data model is easy",
    "start": "2187900",
    "end": "2193240"
  },
  {
    "text": "monitoring is easy but while we were implementing we found some issue with",
    "start": "2193240",
    "end": "2198310"
  },
  {
    "text": "Neptune as well so first issue is maintenance window so when you set up",
    "start": "2198310",
    "end": "2204760"
  },
  {
    "text": "your Neptune cluster you have to provide 30 minutes weekly maintenance window",
    "start": "2204760",
    "end": "2211480"
  },
  {
    "text": "during which if there is any system updates and that would be applied to your napkin cluster at Nike we had",
    "start": "2211480",
    "end": "2219130"
  },
  {
    "text": "expectation of running our system 24 by 7 we have user all over the world who",
    "start": "2219130",
    "end": "2225640"
  },
  {
    "text": "are using our apps so that means we could not afford that 30-second downtime",
    "start": "2225640",
    "end": "2230650"
  },
  {
    "text": "in two as well second issue that we found was pagination as Mark mentioned",
    "start": "2230650",
    "end": "2237310"
  },
  {
    "text": "earlier that we have lots of use cases for our mobile client where we have to",
    "start": "2237310",
    "end": "2242380"
  },
  {
    "text": "return our data in pages Neptune has a function called range where you can",
    "start": "2242380",
    "end": "2248890"
  },
  {
    "text": "provide your start index and count index to get data so in our case think about",
    "start": "2248890",
    "end": "2254980"
  },
  {
    "text": "an example we want to get pages and each page should have ten followers so four first",
    "start": "2254980",
    "end": "2264069"
  },
  {
    "text": "three first three traversal you will get like really good performance but as you",
    "start": "2264069",
    "end": "2269950"
  },
  {
    "text": "go deeper into your traversal response time from this range function would start degrading so think about I want to",
    "start": "2269950",
    "end": "2277660"
  },
  {
    "text": "get page will start index is 10,000 and I want to get 10 elements the way in",
    "start": "2277660",
    "end": "2283900"
  },
  {
    "text": "action work it has to traverse through your graph to find out where is your 10,000 element is and then it turned you",
    "start": "2283900",
    "end": "2291369"
  },
  {
    "text": "10 elements so that means just to get 10 elements its traversing through 10,000",
    "start": "2291369",
    "end": "2297160"
  },
  {
    "text": "10 records third issue that we found with Neptune was count so similarly as",
    "start": "2297160",
    "end": "2303730"
  },
  {
    "text": "pagination the way count work it needs to know what the size of your graph and",
    "start": "2303730",
    "end": "2309369"
  },
  {
    "text": "based on that it return you a response which was not performant when we did",
    "start": "2309369",
    "end": "2314500"
  },
  {
    "text": "performance testing there are couple of other issue that we found with napkin",
    "start": "2314500",
    "end": "2320010"
  },
  {
    "text": "one was related to query timeout so when",
    "start": "2320010",
    "end": "2325300"
  },
  {
    "text": "you create your napkin cluster you provide a value of query timeout and that value is applied to all of your",
    "start": "2325300",
    "end": "2332800"
  },
  {
    "text": "queries running on that particular napkin cluster but in our case we had",
    "start": "2332800",
    "end": "2338740"
  },
  {
    "text": "some queries which takes less than 10 milliseconds for example if I want to",
    "start": "2338740",
    "end": "2344380"
  },
  {
    "text": "create relationship between two users or if I want to see if two users are connected or not that takes less than 10",
    "start": "2344380",
    "end": "2351190"
  },
  {
    "text": "milliseconds but there are some use cases for example getting friends of friends or getting recommendations which",
    "start": "2351190",
    "end": "2358839"
  },
  {
    "text": "takes more than 200 or 300 milliseconds as an engineer we want to fail fast so",
    "start": "2358839",
    "end": "2365170"
  },
  {
    "text": "we want to set lower value of query time out for the queries which takes less than 10 millisecond and higher value of",
    "start": "2365170",
    "end": "2372190"
  },
  {
    "text": "query time out for the queries which takes more time but in case of Neptune there's only one global level query",
    "start": "2372190",
    "end": "2379839"
  },
  {
    "text": "timeout so we could not do that another issue that we found with Neptune even",
    "start": "2379839",
    "end": "2386230"
  },
  {
    "text": "though you get lots of metrics using cloud watch there is a matrix where you can see average response time for all of",
    "start": "2386230",
    "end": "2393170"
  },
  {
    "text": "your queries but that gives you all of the queries running on your napkin",
    "start": "2393170",
    "end": "2398420"
  },
  {
    "text": "cluster it gives you aggregate of that if you want to find out that how a",
    "start": "2398420",
    "end": "2403820"
  },
  {
    "text": "particular query is performing in your production cluster you cannot get that",
    "start": "2403820",
    "end": "2409040"
  },
  {
    "text": "matrix so out of all of these issues that I talked about our biggest",
    "start": "2409040",
    "end": "2415100"
  },
  {
    "text": "challenge was maintenance and failover so we worked with AWS and try to figure",
    "start": "2415100",
    "end": "2422240"
  },
  {
    "text": "out a solution which was this is this is an initial proposal which a double is recommended so how it if it would have",
    "start": "2422240",
    "end": "2431900"
  },
  {
    "text": "been worth that instead of using single napkin cluster proposal was to create",
    "start": "2431900",
    "end": "2437690"
  },
  {
    "text": "two in action clusters primary and secondary on the right when social app",
    "start": "2437690",
    "end": "2445220"
  },
  {
    "text": "writing to neptune cluster instead of directly writing to neptune cluster",
    "start": "2445220",
    "end": "2450370"
  },
  {
    "text": "recommendation was to put AC stream so once social API put record and nici",
    "start": "2450370",
    "end": "2457070"
  },
  {
    "text": "stream it would return successful response to consumer and then there are two lambdas on this diagram where",
    "start": "2457070",
    "end": "2465040"
  },
  {
    "text": "responsibilities of these lambdas are read records out of Kinesis stream and",
    "start": "2465040",
    "end": "2470420"
  },
  {
    "text": "write it into neptune neptune cluster for read instead of reading from your",
    "start": "2470420",
    "end": "2476330"
  },
  {
    "text": "readers endpoint proposal was to put NL b in front of read read endpoints and in",
    "start": "2476330",
    "end": "2483500"
  },
  {
    "text": "that NL be put target host as primary data endpoint and second D reader endpoint and then there is another",
    "start": "2483500",
    "end": "2490520"
  },
  {
    "text": "lambda on read side responsibility of this lambda is to query DNS server and",
    "start": "2490520",
    "end": "2496460"
  },
  {
    "text": "to make sure your primary and secondary reader endpoints are up and running",
    "start": "2496460",
    "end": "2501610"
  },
  {
    "text": "so think about a scenario where during maintenance we have primary cluster down",
    "start": "2501610",
    "end": "2507280"
  },
  {
    "text": "so what would happen from social API it would put record on finish stream and",
    "start": "2507280",
    "end": "2514760"
  },
  {
    "text": "return successful response to user so user will get successful response and then lambda which is inserting record on",
    "start": "2514760",
    "end": "2522890"
  },
  {
    "text": "secondary cluster it would successfully insert record on secondary cluster but the lambda whose",
    "start": "2522890",
    "end": "2529190"
  },
  {
    "text": "responsibilities to insert record on primary cluster that would fail and that",
    "start": "2529190",
    "end": "2534950"
  },
  {
    "text": "would put record on dlq on the reed side because we have lambda which is querying",
    "start": "2534950",
    "end": "2541490"
  },
  {
    "text": "against DNS server it would update and will be and remove primary leader",
    "start": "2541490",
    "end": "2546920"
  },
  {
    "text": "endpoint as target host so that means both read and write would work during maintenance window but we did not go",
    "start": "2546920",
    "end": "2555140"
  },
  {
    "text": "with this proposal there were three reasons behind that first was we have",
    "start": "2555140",
    "end": "2561500"
  },
  {
    "text": "some events where we expect those events to be first in first out think about a",
    "start": "2561500",
    "end": "2567080"
  },
  {
    "text": "friend invitation and friend acceptance to accept a friendship we want to make",
    "start": "2567080",
    "end": "2572180"
  },
  {
    "text": "sure friend invitation exists so that means if on our Kinesis stream we have",
    "start": "2572180",
    "end": "2577430"
  },
  {
    "text": "two events one is for invitation and one is for acceptance we want to process invitation event first so that was one",
    "start": "2577430",
    "end": "2585410"
  },
  {
    "text": "reason second reason was as Mark mentioned earlier from social API we are",
    "start": "2585410",
    "end": "2591560"
  },
  {
    "text": "also publishing data on SNS topics so that other consumer or other internal",
    "start": "2591560",
    "end": "2596660"
  },
  {
    "text": "Nike user can consume that SNS topic one of the use case of SNS subscription is",
    "start": "2596660",
    "end": "2602920"
  },
  {
    "text": "once we insert data to Neptune cluster there is another team called",
    "start": "2602920",
    "end": "2608330"
  },
  {
    "text": "notification team that send a push notification to another user that some",
    "start": "2608330",
    "end": "2613760"
  },
  {
    "text": "other user invited you and accept the friendship so that means if that team",
    "start": "2613760",
    "end": "2619100"
  },
  {
    "text": "send message to another user but that's not insert into inserted into database that might create trace condition third",
    "start": "2619100",
    "end": "2627320"
  },
  {
    "text": "reason because just to write directly to Neptune cluster we added in easy stream",
    "start": "2627320",
    "end": "2633170"
  },
  {
    "text": "and lambda that adds more failure point so how did we simplify this particular",
    "start": "2633170",
    "end": "2638870"
  },
  {
    "text": "solution so",
    "start": "2638870",
    "end": "2645600"
  },
  {
    "text": "if you see red side is still same we still have NLB lambda and which is",
    "start": "2645600",
    "end": "2653370"
  },
  {
    "text": "querying against DNS server but on the right instead of having Kinesis stream",
    "start": "2653370",
    "end": "2658800"
  },
  {
    "text": "and lambda our app is directly writing to Neptune cluster so now our app has",
    "start": "2658800",
    "end": "2664680"
  },
  {
    "text": "more control over data flow so again in this case think about my primary cluster",
    "start": "2664680",
    "end": "2671670"
  },
  {
    "text": "goes down what would happen from rear side it would still serve all the",
    "start": "2671670",
    "end": "2676800"
  },
  {
    "text": "requests but from the right it would add it out and it would send error to the",
    "start": "2676800",
    "end": "2682440"
  },
  {
    "text": "caller but the way our traffic is distributed more than 95% calls or read",
    "start": "2682440",
    "end": "2689130"
  },
  {
    "text": "calls so that means during that maintenance window we are still serving all the read calls",
    "start": "2689130",
    "end": "2694350"
  },
  {
    "text": "and only less than 5% calls for the write which are failing so it basically simplified our solution",
    "start": "2694350",
    "end": "2702440"
  },
  {
    "text": "that's why we went with this particular approach so another issue that we had",
    "start": "2702440",
    "end": "2708030"
  },
  {
    "text": "with Cassandra to run analytic query the way we were doing analytic query in",
    "start": "2708030",
    "end": "2714720"
  },
  {
    "text": "Cassandra we were having an EMR cluster and then spark job which was running to",
    "start": "2714720",
    "end": "2720150"
  },
  {
    "text": "get us analytic data but in Neptune you have couple of options first option is",
    "start": "2720150",
    "end": "2727380"
  },
  {
    "text": "you can add an additional read replica to your Neptune cluster and you can send",
    "start": "2727380",
    "end": "2733500"
  },
  {
    "text": "all of your analytic query to that read replica problem with this option is as",
    "start": "2733500",
    "end": "2739440"
  },
  {
    "text": "soon as you add new read replicas to Neptune cluster your Neptune cluster",
    "start": "2739440",
    "end": "2745440"
  },
  {
    "text": "would start sending production traffic to that tree replica so that means that particular reader instance would also",
    "start": "2745440",
    "end": "2751290"
  },
  {
    "text": "get traffic from analytic query as well as production traffic depending on how",
    "start": "2751290",
    "end": "2756750"
  },
  {
    "text": "CPU intensive or analytic query is it might impact with production traffic to",
    "start": "2756750",
    "end": "2762930"
  },
  {
    "text": "solve that issue we went with second approach which was we took snapshot from",
    "start": "2762930",
    "end": "2768900"
  },
  {
    "text": "our production cluster created a separate Neptune cluster which was separately for running analytic query",
    "start": "2768900",
    "end": "2776490"
  },
  {
    "text": "ran all the analytic query on the separate and then tear down that cluster so we",
    "start": "2776490",
    "end": "2782220"
  },
  {
    "text": "could do all of these thing with cloud formation scripts so we have one single cloud formation script that we are using",
    "start": "2782220",
    "end": "2788610"
  },
  {
    "text": "which creates and takes the snapshot from production cluster create a separate cluster and then tear down that",
    "start": "2788610",
    "end": "2795060"
  },
  {
    "text": "cluster so once we are done with all of these things will be implemented we had",
    "start": "2795060",
    "end": "2801570"
  },
  {
    "text": "all of our monitoring setup next task was to do data migration so for data",
    "start": "2801570",
    "end": "2808920"
  },
  {
    "text": "migration again we had we explored multiple options first option was using",
    "start": "2808920",
    "end": "2815550"
  },
  {
    "text": "bulk data loader endpoint provided by Neptune the way bulk data loader",
    "start": "2815550",
    "end": "2821520"
  },
  {
    "text": "endpoint works you have to provide a CSV file of your nodes and edges and then",
    "start": "2821520",
    "end": "2828660"
  },
  {
    "text": "bulk data loader endpoint would read all of your nodes and edges and migrate that",
    "start": "2828660",
    "end": "2834270"
  },
  {
    "text": "data to an action cluster it does all of this work in lots of parallel threads",
    "start": "2834270",
    "end": "2839940"
  },
  {
    "text": "and that's why it uses 90 mode now up to 90% CPU from your Neptune cluster but in",
    "start": "2839940",
    "end": "2847620"
  },
  {
    "text": "our case we have multiple domains we have friends interest and links we did",
    "start": "2847620",
    "end": "2853590"
  },
  {
    "text": "not want to do a Big Bang migration so think about if we would have used this particular approach we migrated our",
    "start": "2853590",
    "end": "2860900"
  },
  {
    "text": "interest traffic or we migrated all of our interest data then we sent production traffic for interest but when",
    "start": "2860900",
    "end": "2868260"
  },
  {
    "text": "we had to do migration for friends because we already have production traffic for interest on that natural",
    "start": "2868260",
    "end": "2874620"
  },
  {
    "text": "cluster we did not want any other process to use 90% of CPU from our",
    "start": "2874620",
    "end": "2880380"
  },
  {
    "text": "production traffic another issue with bulk data loader endpoint is if you have",
    "start": "2880380",
    "end": "2886950"
  },
  {
    "text": "any property on your edge or node when it migrated that property to an option",
    "start": "2886950",
    "end": "2892830"
  },
  {
    "text": "cluster it sets the cardinality of that property as set cardinality but we",
    "start": "2892830",
    "end": "2899190"
  },
  {
    "text": "wanted to use a singular single cardinality for all of our properties so that's why we did not go with bulk data",
    "start": "2899190",
    "end": "2906510"
  },
  {
    "text": "load or endpoint approach second approach that we followed was dual right",
    "start": "2906510",
    "end": "2911820"
  },
  {
    "text": "Oh we explore must do all right this is the same this is the same approach that we",
    "start": "2911820",
    "end": "2916940"
  },
  {
    "text": "used when we migrated our data from Oracle to Cassandra the way it were from",
    "start": "2916940",
    "end": "2923900"
  },
  {
    "text": "your service layer you are inserting data into your Neptune cluster as well as Cassandra so this duel right but it",
    "start": "2923900",
    "end": "2931490"
  },
  {
    "text": "creates tight coupling between two databases and now because for each write you are inserting data into two",
    "start": "2931490",
    "end": "2939080"
  },
  {
    "text": "different databases it adds additional latency to your write request and also",
    "start": "2939080",
    "end": "2944900"
  },
  {
    "text": "third issue with this approach it was if we have any failure in inserting data",
    "start": "2944900",
    "end": "2950780"
  },
  {
    "text": "into neptune cluster it was really hard to rollback that particular transaction in Cassandra because in Cassandra we",
    "start": "2950780",
    "end": "2958160"
  },
  {
    "text": "were inserting into multiple tables so it becomes very difficult to do a rollback in Cassandra so the approach",
    "start": "2958160",
    "end": "2965570"
  },
  {
    "text": "that we took was even processor so even processor is basically two phase",
    "start": "2965570",
    "end": "2971540"
  },
  {
    "text": "approach first you sync your live traffic to your new data base and then",
    "start": "2971540",
    "end": "2977360"
  },
  {
    "text": "you extract all of your old data and do a data thing this is the repeatable",
    "start": "2977360",
    "end": "2982550"
  },
  {
    "text": "process that means irrespective of any database you are using you can use the",
    "start": "2982550",
    "end": "2988370"
  },
  {
    "text": "same approach so how this is working so",
    "start": "2988370",
    "end": "2993920"
  },
  {
    "text": "you have seen this diagram on left side where production traffic is going to our",
    "start": "2993920",
    "end": "3000910"
  },
  {
    "text": "socials API and inserting data into Cassandra and also from our social API",
    "start": "3000910",
    "end": "3006580"
  },
  {
    "text": "once we insert data into Cassandra we are publishing that event on SNS topic",
    "start": "3006580",
    "end": "3012640"
  },
  {
    "text": "so we created a queue which were which we subscribe to that SNS topic and it",
    "start": "3012640",
    "end": "3018400"
  },
  {
    "text": "was getting all the events from SNS topic then we created another service",
    "start": "3018400",
    "end": "3023620"
  },
  {
    "text": "called event processor responsibility of event processor is to read data out of",
    "start": "3023620",
    "end": "3029320"
  },
  {
    "text": "queue and write it into an action cluster so that means in this phase we",
    "start": "3029320",
    "end": "3034750"
  },
  {
    "text": "are syncing all of our live traffic to Neptune cluster because we are using SQS",
    "start": "3034750",
    "end": "3040990"
  },
  {
    "text": "it remove coupling between two databases and there is no performance impact",
    "start": "3040990",
    "end": "3046090"
  },
  {
    "text": "right cause once we start once we synched all the production live traffic",
    "start": "3046090",
    "end": "3051750"
  },
  {
    "text": "we extracted data out of Cassandra created a data file and created another",
    "start": "3051750",
    "end": "3058120"
  },
  {
    "text": "service called data ticker responsibility of data ticker is read",
    "start": "3058120",
    "end": "3063340"
  },
  {
    "text": "data out of data file send it to even processor and insert recording to",
    "start": "3063340",
    "end": "3068950"
  },
  {
    "text": "natural cluster so once we migrated all of our data next step was doing a",
    "start": "3068950",
    "end": "3077020"
  },
  {
    "text": "production cut over so to do a production cut over we already had a",
    "start": "3077020",
    "end": "3082510"
  },
  {
    "text": "social API version 1 which was integrated with Cassandra we created",
    "start": "3082510",
    "end": "3088240"
  },
  {
    "text": "another version of API called social API version 2 which was integrated Neptune",
    "start": "3088240",
    "end": "3093370"
  },
  {
    "text": "so it was reading and writing to Neptune cluster for social API version 1 in",
    "start": "3093370",
    "end": "3098950"
  },
  {
    "text": "addition to writing the data to Neptune cluster it was also syncing same data to",
    "start": "3098950",
    "end": "3105400"
  },
  {
    "text": "Neptune cluster via even processor and cue similarly social API version 2 in",
    "start": "3105400",
    "end": "3111600"
  },
  {
    "text": "addition to inserting data into Neptune cluster it was sinking same data to",
    "start": "3111600",
    "end": "3116770"
  },
  {
    "text": "Cassandra reason behind that was if we had to roll back from our Neptune",
    "start": "3116770",
    "end": "3122770"
  },
  {
    "text": "cluster if we are syncing data between two databases rollback would be easier",
    "start": "3122770",
    "end": "3128530"
  },
  {
    "text": "there is no data difference between two databases so once we had all the data in",
    "start": "3128530",
    "end": "3135700"
  },
  {
    "text": "sync between Neptune and Cassandra we did canary deployment where we route 95%",
    "start": "3135700",
    "end": "3142930"
  },
  {
    "text": "of traffic to social EP version one five percent of traffic to social API version 2 and once we monitor over social eBay",
    "start": "3142930",
    "end": "3150460"
  },
  {
    "text": "version 2 made sure data is inserted into natural cluster without any issue we slowly routed more traffic to social",
    "start": "3150460",
    "end": "3158590"
  },
  {
    "text": "API version 2 and once we had 100 percent traffic routed to social API version 2 we deprecated or clean up our",
    "start": "3158590",
    "end": "3166570"
  },
  {
    "text": "social API version 1 and deleted Cassandra database so that was all about",
    "start": "3166570",
    "end": "3172030"
  },
  {
    "text": "our implementation data migration and production cutover thank you thank you",
    "start": "3172030",
    "end": "3179670"
  },
  {
    "text": "so we hope you we have piqued your interest in all the cool things that",
    "start": "3179670",
    "end": "3184960"
  },
  {
    "text": "Nike is doing in the engineering space if we have you can go to medium.com",
    "start": "3184960",
    "end": "3191410"
  },
  {
    "text": "slash Nike engineering and find out about other cool things that we're doing in the engineering space there are a",
    "start": "3191410",
    "end": "3200380"
  },
  {
    "text": "couple more sessions around Neptune so this is a technology that you're really interested in I encourage you to go to",
    "start": "3200380",
    "end": "3207609"
  },
  {
    "text": "these sessions they're all tomorrow here at the Venetian and with that we would",
    "start": "3207609",
    "end": "3212650"
  },
  {
    "text": "really like to thank you for attending our session and please do not forget to give us feedback and fill out the survey",
    "start": "3212650",
    "end": "3219009"
  },
  {
    "text": "in the mobile app thank you so much and have a good rest of your conference [Applause]",
    "start": "3219009",
    "end": "3227819"
  }
]