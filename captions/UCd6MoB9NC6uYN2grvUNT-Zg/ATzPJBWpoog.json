[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "hello everyone we're about to start welcome to STG 329 my name is Andrew",
    "start": "30",
    "end": "8610"
  },
  {
    "text": "Zico from solution architect based out of Luxembourg I'm working for AWS and",
    "start": "8610",
    "end": "13759"
  },
  {
    "text": "today we're going to talk about help protect wise optimize performance of",
    "start": "13759",
    "end": "18960"
  },
  {
    "text": "Cassandra and kafka workloads on Amazon EBS I would like to ask you to welcome",
    "start": "18960",
    "end": "23970"
  },
  {
    "text": "my co-presenters protect wise team Jean Stevens who is CTO and co-founder of",
    "start": "23970",
    "end": "30449"
  },
  {
    "text": "protect wise and Robert RL who is director of DevOps in protect wise today",
    "start": "30449",
    "end": "38489"
  },
  {
    "start": "38000",
    "end": "66000"
  },
  {
    "text": "we can have a pretty busy agenda we're going to have an entertainer on Sycamore native layers we're going to discuss how",
    "start": "38489",
    "end": "45649"
  },
  {
    "text": "Cassandra and Kafka work we're going to cover some best practices for these",
    "start": "45649",
    "end": "51449"
  },
  {
    "text": "types of workloads on AWS and of course we're gonna go through protect YC use case and their use case for kafka",
    "start": "51449",
    "end": "59399"
  },
  {
    "text": "Cassandra as free and some specific optimizations they made on AWS and of course we're gonna have some lessons",
    "start": "59399",
    "end": "65640"
  },
  {
    "text": "learned I would like to start with this slide and I would like to start with no sequel as a technology we often refer to",
    "start": "65640",
    "end": "72150"
  },
  {
    "start": "66000",
    "end": "145000"
  },
  {
    "text": "no sequel as a technology but it's not really something like that it's more like a movement and a bunch of",
    "start": "72150",
    "end": "78990"
  },
  {
    "text": "technologies combined by their similar ideas it started quite some time ago and",
    "start": "78990",
    "end": "84330"
  },
  {
    "text": "most of the protocols were well known actually 90 90s and 80s of last century",
    "start": "84330",
    "end": "90360"
  },
  {
    "text": "like gossip protocol and faxes and so on but the big movement started in 2004",
    "start": "90360",
    "end": "97049"
  },
  {
    "text": "with a MapReduce paper which wasn't directly referring to null sequel but",
    "start": "97049",
    "end": "103560"
  },
  {
    "text": "that made a great shift and in 2007 dynamic beep dynamic sorry paper was",
    "start": "103560",
    "end": "109680"
  },
  {
    "text": "published and that was published by Amazon and that was a huge step forward",
    "start": "109680",
    "end": "115409"
  },
  {
    "text": "no sequel databases and nautical world and in 2007 a lot of different projects",
    "start": "115409",
    "end": "121799"
  },
  {
    "text": "started like MongoDB in a foj and in 2008 Apache Cassandra started they were",
    "start": "121799",
    "end": "127320"
  },
  {
    "text": "going to talk about Apache Cassandra today and since 2012 2012 we have",
    "start": "127320",
    "end": "133500"
  },
  {
    "text": "a lot of like new databases coming in every year solving different aspects of",
    "start": "133500",
    "end": "140480"
  },
  {
    "text": "of non-sequel workloads and different aspects of data processing",
    "start": "140480",
    "end": "145770"
  },
  {
    "start": "145000",
    "end": "214000"
  },
  {
    "text": "no no neighs how to pick up the database the best thing is to go for database per",
    "start": "145770",
    "end": "151890"
  },
  {
    "text": "workload approach so for example if you have something like cash and you need to know you workload for sure right but if",
    "start": "151890",
    "end": "158610"
  },
  {
    "text": "you have something like cash you know something like data streaming for data streaming for example you can go with",
    "start": "158610",
    "end": "164250"
  },
  {
    "text": "Kafka or other options right location inside already some am cash so you can",
    "start": "164250",
    "end": "170370"
  },
  {
    "text": "have key value in memory databases for fast data process and you can have of course in relational databases which",
    "start": "170370",
    "end": "177660"
  },
  {
    "text": "will be nice for the workloads which need really high consistency of data and",
    "start": "177660",
    "end": "183270"
  },
  {
    "text": "it can be like my sickle Postgres or some other choices right of course you",
    "start": "183270",
    "end": "188400"
  },
  {
    "text": "can have like search engines like elastic search soar which really nice play for like different types of",
    "start": "188400",
    "end": "195330"
  },
  {
    "text": "variable requests you have you can have graph databases and of course you can",
    "start": "195330",
    "end": "200820"
  },
  {
    "text": "have something like a big do distributed database like MongoDB you can Cassandra",
    "start": "200820",
    "end": "207180"
  },
  {
    "text": "which can be like main source of truth and main database for high-throughput both the reads and for writes if you if",
    "start": "207180",
    "end": "216959"
  },
  {
    "start": "214000",
    "end": "259000"
  },
  {
    "text": "you can see like as you can see that it's a bunch of different databases and if you can imagine the huge clusters of",
    "start": "216959",
    "end": "223320"
  },
  {
    "text": "ball or both like Cassandra and elasticsearch and solar and Redis and so",
    "start": "223320",
    "end": "229260"
  },
  {
    "text": "on it's kind of a really complicated to manage that right so we have a bunch of",
    "start": "229260",
    "end": "234269"
  },
  {
    "text": "options for databases on a SS like Amazon Elastic cache right like dynamo",
    "start": "234269",
    "end": "240570"
  },
  {
    "text": "DB like Kinesis for streaming was on redshift there's a data warehouse and",
    "start": "240570",
    "end": "245850"
  },
  {
    "text": "Amazon Elastic search has certain managed service engine so which place",
    "start": "245850",
    "end": "251100"
  },
  {
    "text": "really cool but today we're going to talk about DIY kind of setups Cassandra",
    "start": "251100",
    "end": "257070"
  },
  {
    "text": "Kafka on AWS and for these the essential part of course is storage and we have",
    "start": "257070",
    "end": "262169"
  },
  {
    "start": "259000",
    "end": "289000"
  },
  {
    "text": "like several major services for storage like Amazon EBS we're gonna talk about that",
    "start": "262169",
    "end": "267200"
  },
  {
    "text": "today Amazon Elastic file system as free of course of course glacier and so on",
    "start": "267200",
    "end": "272420"
  },
  {
    "text": "and what is nice about these services that they are completely managed by AWS",
    "start": "272420",
    "end": "277730"
  },
  {
    "text": "and they are really tightly integrated with different management tools with security tools and you don't need to",
    "start": "277730",
    "end": "284270"
  },
  {
    "text": "bother for integration or some maintain these types of integrations and so on",
    "start": "284270",
    "end": "289820"
  },
  {
    "start": "289000",
    "end": "320000"
  },
  {
    "text": "so what's cassandra why to use that apache cassandra is an open-source database based on dynamic model it's",
    "start": "289820",
    "end": "296660"
  },
  {
    "text": "massively scalable a Geo distributed high-performance key value database so it can scale up to thousand nodes really",
    "start": "296660",
    "end": "304160"
  },
  {
    "text": "quickly and quite quite fast and efficient and on most often use cases",
    "start": "304160",
    "end": "309320"
  },
  {
    "text": "for patchy cassandra like time series data data for social media platforms and",
    "start": "309320",
    "end": "314510"
  },
  {
    "text": "like e-commerce data like user sessions for commerce and so on so how Cassandra",
    "start": "314510",
    "end": "321200"
  },
  {
    "start": "320000",
    "end": "406000"
  },
  {
    "text": "works on the cluster level you can see the green box it's a cluster it's a Cassandra cluster which is the main",
    "start": "321200",
    "end": "327950"
  },
  {
    "text": "deployment unit in Cassandra inside the cluster you can have multiple like blue",
    "start": "327950",
    "end": "333020"
  },
  {
    "text": "boxes multiple data centers and data centers are there for geo distribution and inside the data center you can have",
    "start": "333020",
    "end": "339530"
  },
  {
    "text": "racks and racks are there for disaster recovery and high availability so inside",
    "start": "339530",
    "end": "346370"
  },
  {
    "text": "the racks you can have in the cluster you can have one or many data centers and inside the data center you can have",
    "start": "346370",
    "end": "352580"
  },
  {
    "text": "one or many racks inside the cluster there are nodes and Cassandra is a hash",
    "start": "352580",
    "end": "357740"
  },
  {
    "text": "ring so basically it doesn't have any single node of failure or its masterless",
    "start": "357740",
    "end": "363320"
  },
  {
    "text": "right so any node can act as a coordinator node and can coordinate the",
    "start": "363320",
    "end": "368780"
  },
  {
    "text": "request to all the other nodes and on the cluster level what you can have is",
    "start": "368780",
    "end": "374330"
  },
  {
    "text": "of course it's snitching so it's a concept of node discovery and working",
    "start": "374330",
    "end": "382280"
  },
  {
    "text": "with with different types of nodes and there are some snitches for AWS specific like easy to snitch multi-regional I see",
    "start": "382280",
    "end": "389300"
  },
  {
    "text": "two stage there is network topology there is read and write consistency on the cluster level and",
    "start": "389300",
    "end": "396920"
  },
  {
    "text": "the nodes are interacting with each other with the use of gossip protocol and they're actually gossiping it around",
    "start": "396920",
    "end": "402170"
  },
  {
    "text": "the state of the cluster and where the data resides on the nodes okay how",
    "start": "402170",
    "end": "407690"
  },
  {
    "start": "406000",
    "end": "592000"
  },
  {
    "text": "Cassandra works on the node level this diagram shows both reads and writes from",
    "start": "407690",
    "end": "413420"
  },
  {
    "text": "the node so solid lines are for reads and dashed lines are for writes so what",
    "start": "413420",
    "end": "419990"
  },
  {
    "text": "we can see here like a write is quite straightforward Cassandra node starts to write in a commit log first which is",
    "start": "419990",
    "end": "428240"
  },
  {
    "text": "therefore disaster recovery mainly and after that it writes to memo and after it is written the data is written both",
    "start": "428240",
    "end": "435470"
  },
  {
    "text": "in commit lock and in mem table the right is successful after a certain",
    "start": "435470",
    "end": "440780"
  },
  {
    "text": "amount of time mem table is flashed on the disk and it effectively becomes SS",
    "start": "440780",
    "end": "446060"
  },
  {
    "text": "table which is immutable structure with which cannot be changed so the data is immutable there it actually it can be",
    "start": "446060",
    "end": "453770"
  },
  {
    "text": "the the data as a stable can be deleted during the compaction process or it can",
    "start": "453770",
    "end": "458990"
  },
  {
    "text": "be reckoned stuff like that but now they can be changed in addition to these two",
    "start": "458990",
    "end": "464450"
  },
  {
    "text": "things during the write process you can have a hint at handoff so if no notes are available for write for the right of",
    "start": "464450",
    "end": "471230"
  },
  {
    "text": "the particular key it the coordinator or node can can store it in the hinted",
    "start": "471230",
    "end": "477320"
  },
  {
    "text": "handoff and the write will be delayed so the read sequence is a bit more",
    "start": "477320",
    "end": "483740"
  },
  {
    "text": "complicated the thing is like the data is immutable and SS tables and you have per note you can have like hundreds of",
    "start": "483740",
    "end": "490340"
  },
  {
    "text": "asses tables and they can be quite big right so they can be gigabytes of data per SS table so to manage to be able",
    "start": "490340",
    "end": "498410"
  },
  {
    "text": "actually to read this data really fast the first thing which is implemented in",
    "start": "498410",
    "end": "503900"
  },
  {
    "text": "Cassandra is a blonde filter one filter is probabilistic structure which answers quite a simple question is there any",
    "start": "503900",
    "end": "510290"
  },
  {
    "text": "chances that the certain key is within this SS table so yes if it says no so it",
    "start": "510290",
    "end": "520130"
  },
  {
    "text": "means that 99% that the data shouldn't be in this SS table it goes to the next",
    "start": "520130",
    "end": "526820"
  },
  {
    "text": "SS table so after blonde filter is checked then it goes the cosigner Nell goes to key",
    "start": "526820",
    "end": "533190"
  },
  {
    "text": "cash if it's enabled and by default it's enabled and tries to find where's the",
    "start": "533190",
    "end": "539100"
  },
  {
    "text": "ski on disk on the particular SS table then it goes to the partition summary and tries to find where is the position",
    "start": "539100",
    "end": "547140"
  },
  {
    "text": "of the particular key in the partition index to find a position of data and read it one more thing as SS tables are",
    "start": "547140",
    "end": "554340"
  },
  {
    "text": "immutable there are certain procedures called compaction procedures which are implemented on the node level so they",
    "start": "554340",
    "end": "561030"
  },
  {
    "text": "can be like four main types of compaction size thyroid compaction which is by default cup of compaction and",
    "start": "561030",
    "end": "567150"
  },
  {
    "text": "which is really suitable and well seeds form right heavy workloads then it can",
    "start": "567150",
    "end": "573330"
  },
  {
    "text": "be leveled compaction which fits more like 80 percent read partner and you can",
    "start": "573330",
    "end": "578970"
  },
  {
    "text": "have time windows compaction or data data based compaction I mean based on",
    "start": "578970",
    "end": "585840"
  },
  {
    "text": "the particular time or date so you can have as a stable from Wednesday for",
    "start": "585840",
    "end": "591120"
  },
  {
    "text": "Thursday and so on and so forth so application interacts with Cassandra application interacts Europe interacts",
    "start": "591120",
    "end": "597660"
  },
  {
    "start": "592000",
    "end": "652000"
  },
  {
    "text": "with Cassandra with the user driver normally and don't even have the bunch of settings actually one of the most",
    "start": "597660",
    "end": "604410"
  },
  {
    "text": "important thing is a load balancing you do a lot of cool features there so for example you can whitelist a set of a set",
    "start": "604410",
    "end": "611550"
  },
  {
    "text": "of nodes inside the Cassandra cluster and only work with these nodes",
    "start": "611550",
    "end": "616850"
  },
  {
    "text": "another teacher you can have like a round-robin load balancing policies and",
    "start": "616850",
    "end": "622230"
  },
  {
    "text": "so on in addition what driver does it helps you to deal with errors to handle",
    "start": "622230",
    "end": "628050"
  },
  {
    "text": "errors it has a retries policy so again to deal with errors you can just retry the request and of course you can you",
    "start": "628050",
    "end": "636420"
  },
  {
    "text": "have to think about like consistency level both for reads and writes by",
    "start": "636420",
    "end": "641520"
  },
  {
    "text": "default its local quorum so which means if you have replication factor free for Cassandra if two nodes return the same",
    "start": "641520",
    "end": "648960"
  },
  {
    "text": "result the read will be successful some best practices for Cassandra on AWS",
    "start": "648960",
    "end": "655100"
  },
  {
    "start": "652000",
    "end": "719000"
  },
  {
    "text": "there are a couple of those first thing is like from infrastructure standpoint",
    "start": "655100",
    "end": "661830"
  },
  {
    "text": "think about cost of traffic because traffic has cost and for the big clusters like thousands of nodes because",
    "start": "661830",
    "end": "669330"
  },
  {
    "text": "can be quite high so next thing from the operations side regular repairs it's",
    "start": "669330",
    "end": "676320"
  },
  {
    "text": "really important for Cassandra and you can utilize EBS features like EBS",
    "start": "676320",
    "end": "681960"
  },
  {
    "text": "snapshots - for backing up your data for bringing the node really fast online and",
    "start": "681960",
    "end": "687710"
  },
  {
    "text": "for planning scaling operations you can use abs elastic volumes feature just to",
    "start": "687710",
    "end": "693210"
  },
  {
    "text": "have really fast increase of I ops on a GP tour volumes so from application data",
    "start": "693210",
    "end": "701580"
  },
  {
    "text": "schema the most important thing is like to carefully pick up the partition key to avoid hard partitions and so on and",
    "start": "701580",
    "end": "708470"
  },
  {
    "text": "just consider some like collecting the data from errors on the application side",
    "start": "708470",
    "end": "714540"
  },
  {
    "text": "because it's big part of the picture what's going on with your Cassandra cluster so what's a batch Kafka and why",
    "start": "714540",
    "end": "722790"
  },
  {
    "start": "719000",
    "end": "761000"
  },
  {
    "text": "to use that Apache Kafka is the open source distributed streaming platform and it allows you to publish and",
    "start": "722790",
    "end": "728760"
  },
  {
    "text": "subscribe to streams of records store stores these streams in a fault-tolerant way process them and most commonly use",
    "start": "728760",
    "end": "735990"
  },
  {
    "text": "cases are like either to build the data pipelines to capture in data and transfer data between systems and",
    "start": "735990",
    "end": "741990"
  },
  {
    "text": "applications like a queue style right and all to build the real-time applications which actually react to the",
    "start": "741990",
    "end": "749490"
  },
  {
    "text": "to the streams of data so Kafka is oftenly put before the actual database",
    "start": "749490",
    "end": "755060"
  },
  {
    "text": "to to remove some of the lot and to reduce the amount of load put on the",
    "start": "755060",
    "end": "760260"
  },
  {
    "text": "database so how carica works there to measure things on Kafka the first thing is",
    "start": "760260",
    "end": "765930"
  },
  {
    "start": "761000",
    "end": "820000"
  },
  {
    "text": "logical unit which call which is called topic so basically your application",
    "start": "765930",
    "end": "771150"
  },
  {
    "text": "works with topic and topic can be divided into multiple partitions and",
    "start": "771150",
    "end": "778310"
  },
  {
    "text": "partitions are replicated across the nodes in the primary secondary mode and",
    "start": "778310",
    "end": "784380"
  },
  {
    "text": "the partitions actually consists of segments and effectively each segment is a set of two files its index file and",
    "start": "784380",
    "end": "791550"
  },
  {
    "text": "log file in addition you can have two types of applications or working with Kafka one",
    "start": "791550",
    "end": "798410"
  },
  {
    "text": "is producer which actually writes the data and another one is consumer and consumers can be combined in the groups",
    "start": "798410",
    "end": "805810"
  },
  {
    "text": "to be able to to read the data efficiently so and Kafka relies on",
    "start": "805810",
    "end": "812630"
  },
  {
    "text": "zookeeper for monitoring for for keeping the cluster state keeping in old state",
    "start": "812630",
    "end": "818540"
  },
  {
    "text": "and so on some best practices around deploying Kafka on AWS of course the",
    "start": "818540",
    "end": "827630"
  },
  {
    "start": "820000",
    "end": "868000"
  },
  {
    "text": "first thing is to isolate Kafka data on the separate disk because it takes some time for calf canal to to be online",
    "start": "827630",
    "end": "834860"
  },
  {
    "text": "after it joins the clustering so for each node it's better to increase max number of file descriptors per",
    "start": "834860",
    "end": "841160"
  },
  {
    "text": "process because it's really crucial Kafka works with an enormous amount of small files then on the B settings part",
    "start": "841160",
    "end": "850660"
  },
  {
    "text": "it's general best practice to over-provision the number of partitions and the guys from protect wise will",
    "start": "850660",
    "end": "857450"
  },
  {
    "text": "explain your why and of course you have to configure correctly zookeeper and to",
    "start": "857450",
    "end": "864230"
  },
  {
    "text": "be able to to failover and to their other nodes right so choosing proper",
    "start": "864230",
    "end": "871670"
  },
  {
    "start": "868000",
    "end": "996000"
  },
  {
    "text": "instance and storage types of course everything here it's really crucial and important topic and everything here",
    "start": "871670",
    "end": "877850"
  },
  {
    "text": "depends on database implementation and in the particular database that Cassandra Kafka and so on on your data",
    "start": "877850",
    "end": "883490"
  },
  {
    "text": "schema on access Parton's and it's something should always to consider and of course you can adopt the compute and",
    "start": "883490",
    "end": "892250"
  },
  {
    "text": "storage types on the go so if you demand change if your pattern change you can",
    "start": "892250",
    "end": "897980"
  },
  {
    "text": "adopt that so focus on your work clothes for we have the benchmark here which has",
    "start": "897980",
    "end": "906620"
  },
  {
    "text": "to access the first acts is requests per second and the second one is volume so",
    "start": "906620",
    "end": "912590"
  },
  {
    "text": "if you will see on the if you look at these diagrams if you have like a general workload with like any amount of",
    "start": "912590",
    "end": "922100"
  },
  {
    "text": "data and not really high our PS you can go with armed force and",
    "start": "922100",
    "end": "928980"
  },
  {
    "text": "gp2 s the volumes if your and after that if you're like really need a high",
    "start": "928980",
    "end": "935430"
  },
  {
    "text": "throughput on the database and your amount of data is quite relatively small what you can do you can go either with I",
    "start": "935430",
    "end": "942210"
  },
  {
    "text": "freeze ITU's both for reads and writes cases but if you have for example 50 50",
    "start": "942210",
    "end": "947850"
  },
  {
    "text": "read and write ratio or like more write heavy workloads you can go with a C",
    "start": "947850",
    "end": "953250"
  },
  {
    "text": "force because Cassandra is CPU bounded and you can use GPUs as well as a good",
    "start": "953250",
    "end": "959310"
  },
  {
    "text": "practice to use st 1e of EBS volume for",
    "start": "959310",
    "end": "964350"
  },
  {
    "text": "the commit lock it's quite cheap and it has a really high prepared for kaftan",
    "start": "964350",
    "end": "969900"
  },
  {
    "text": "general recommendation like for most of use cases is to go with our force to use",
    "start": "969900",
    "end": "976350"
  },
  {
    "text": "st 1 EBS volume which is recommendation given by confluence or depending on the",
    "start": "976350",
    "end": "984450"
  },
  {
    "text": "case if you have really a lot of small files the best thing is to go with gbts",
    "start": "984450",
    "end": "990260"
  },
  {
    "text": "so I will hand it over to to Jean and he",
    "start": "990260",
    "end": "996420"
  },
  {
    "start": "996000",
    "end": "1027000"
  },
  {
    "text": "will go through protect wise use case please welcome Jean",
    "start": "996420",
    "end": "1002310"
  },
  {
    "text": "[Applause] Thank You Andre as he mentioned I am",
    "start": "1002310",
    "end": "1008230"
  },
  {
    "text": "gene Stevens I'm the co-founder as well as the CTO of protect wise and what I'm going to do today is talk you through",
    "start": "1008230",
    "end": "1014170"
  },
  {
    "text": "our use case I'll be followed here very shortly by one of my colleagues Robert",
    "start": "1014170",
    "end": "1019300"
  },
  {
    "text": "Terrell who's the director of DevOps he's actually responsible for making sure the stuff functions well in",
    "start": "1019300",
    "end": "1024699"
  },
  {
    "text": "production but let's talk to you through our use case a little bit here's a basic",
    "start": "1024699",
    "end": "1029949"
  },
  {
    "text": "really basic architectural overview protect wise is a cyber security company we actually have a data platform running",
    "start": "1029949",
    "end": "1039280"
  },
  {
    "text": "inside of AWS and that it consumes a tremendous amount of data has a lot of data in motion a lot of data at rest and",
    "start": "1039280",
    "end": "1046329"
  },
  {
    "text": "it really uses this data in a very aggressive manner in order to detect threats and attacks these types of",
    "start": "1046329",
    "end": "1052570"
  },
  {
    "text": "things so I'm not gonna tell you a lot about the business because I'm not supposed to but also because I want to",
    "start": "1052570",
    "end": "1058780"
  },
  {
    "text": "focus on exactly some of the technology challenges we have here so I'm going to talk you through a little bit about how",
    "start": "1058780",
    "end": "1064299"
  },
  {
    "text": "data moves through our platform and how we kind of think about some of these promise spaces what I'm about to",
    "start": "1064299",
    "end": "1070210"
  },
  {
    "text": "describe to you actually it's going to sound really like overly ambitious to the point of foolishness and in fact it",
    "start": "1070210",
    "end": "1076510"
  },
  {
    "text": "was a really great experience for us we were getting this company off the ground four and a half years ago where we were",
    "start": "1076510",
    "end": "1082960"
  },
  {
    "text": "out in the valley pitch in raising money and that doing that kind of thing and we described what we wanted to do what",
    "start": "1082960",
    "end": "1088240"
  },
  {
    "text": "you're seeing drawn up here to a lot of people after never like that is nuts there is the the state of Technology is",
    "start": "1088240",
    "end": "1094120"
  },
  {
    "text": "not equal to this problem domain so let me step into this because a really big takeaway for us was not just the",
    "start": "1094120",
    "end": "1100030"
  },
  {
    "text": "motivation to say oh wow if we saw that it's gonna be awesome then right but to really kind of relay this idea that",
    "start": "1100030",
    "end": "1106450"
  },
  {
    "text": "really big problems super hyper aggressive workloads and demands there's this whole back catalog of Technology",
    "start": "1106450",
    "end": "1113890"
  },
  {
    "text": "things we know that we want to do and we kind of long to live in some future state where that stuff is easy and",
    "start": "1113890",
    "end": "1119500"
  },
  {
    "text": "normal I mean probably you want to have this case study have the effect of",
    "start": "1119500",
    "end": "1124539"
  },
  {
    "text": "saying maybe there's things in your workloads that you're looking at that look like that'd be an absurd luxury but",
    "start": "1124539",
    "end": "1130480"
  },
  {
    "text": "actually pretty manageable so let's step into this a little bit like I mention",
    "start": "1130480",
    "end": "1135900"
  },
  {
    "text": "protect wise is a cyber security company where we basically deliver our enterprise security from the cloud and",
    "start": "1135900",
    "end": "1142230"
  },
  {
    "text": "what we do is we actually memorize at the packet level the communication is happening inside of our customers",
    "start": "1142230",
    "end": "1148260"
  },
  {
    "text": "networks and we're going to ship that to the cloud and create that as the pressure and as a transaction workload",
    "start": "1148260",
    "end": "1154470"
  },
  {
    "text": "that we're going to walk you through some of the architecture of how we address it how we get that to scale up so inside of our customers network and I",
    "start": "1154470",
    "end": "1162540"
  },
  {
    "text": "think I got a laser pointer here I'm not sure if it works it does not okay on the",
    "start": "1162540",
    "end": "1168000"
  },
  {
    "text": "right hand side you can see we have a list of blocks by our customer network",
    "start": "1168000",
    "end": "1173510"
  },
  {
    "text": "we have a lightweight software sensor that we give our customers and it's typically attached like a span or a tap",
    "start": "1173510",
    "end": "1179460"
  },
  {
    "text": "you know sometimes it's inside a data center it's in the cloud it's an industrial assets we don't really care",
    "start": "1179460",
    "end": "1185730"
  },
  {
    "text": "which means we're not the boss of what kind of data we end up seeing which is a really interesting problem so they're",
    "start": "1185730",
    "end": "1191910"
  },
  {
    "text": "attached there they're doing packet capture and just for the X conversations that we're going to have today this just",
    "start": "1191910",
    "end": "1198000"
  },
  {
    "text": "assumed that this works right they actually take that data that compress optimize and stream that often over the",
    "start": "1198000",
    "end": "1203940"
  },
  {
    "text": "internet though sometimes over dark fiber to AWS and that is being ingested",
    "start": "1203940",
    "end": "1210000"
  },
  {
    "text": "at line rate so there's a near real-time system something's happening locally in the network network pressure network",
    "start": "1210000",
    "end": "1215700"
  },
  {
    "text": "transactions bits and bytes of ones and zeros that go across the wires go across the Wi-Fi etc are being captured and",
    "start": "1215700",
    "end": "1223110"
  },
  {
    "text": "being relayed in a in a really compressed an efficient binary format to",
    "start": "1223110",
    "end": "1228330"
  },
  {
    "text": "our cloud where we ingest this at line rate so we saw the service and we measure it like gigabits per second make",
    "start": "1228330",
    "end": "1234330"
  },
  {
    "text": "it bits per second depending on the size of the customer and we just kind of absorb whatever they throw at us that's in the aggregate that can supply it",
    "start": "1234330",
    "end": "1240570"
  },
  {
    "text": "spike widely 10 X 100 times you know it's a standard workloads and we're just",
    "start": "1240570",
    "end": "1245940"
  },
  {
    "text": "going to absorb it so it's going to hit our ingest service which you can see here on the left side of those vertical",
    "start": "1245940",
    "end": "1251700"
  },
  {
    "text": "dotted lines that blue box that ingest service gets that line rate data we have packet data we've got metadata that we",
    "start": "1251700",
    "end": "1259620"
  },
  {
    "text": "can extract from packets coming in we also have context and other information from like the endpoints and the",
    "start": "1259620",
    "end": "1265260"
  },
  {
    "text": "firewalls and that kind of stuff coming into our platform so in some cases the meta de about the network traffic actually a",
    "start": "1265260",
    "end": "1271470"
  },
  {
    "text": "greater amount of data than the actual traffic itself it's a really interesting problem as it gets ingested we push it",
    "start": "1271470",
    "end": "1279030"
  },
  {
    "text": "through this process of line rate these structuring the data it's kind of like a beam of light using a prism and it kind",
    "start": "1279030",
    "end": "1284309"
  },
  {
    "text": "of bands out onto many different colors right well we have many different forms of data being spread into our platform",
    "start": "1284309",
    "end": "1290280"
  },
  {
    "text": "but there's two really general tracks here we're going to talk about there is a tract of data that is about the",
    "start": "1290280",
    "end": "1296730"
  },
  {
    "text": "packets that we receive and it looks like we must build it here that is being stored inside of s3 and then we have the",
    "start": "1296730",
    "end": "1304290"
  },
  {
    "text": "metadata that we actually extract which can be really voluminous and that's being passed into cassandra and solar",
    "start": "1304290",
    "end": "1311460"
  },
  {
    "text": "I'm going to talk a bit more about that today I want to be clear for today Cassandra and solar for us is data",
    "start": "1311460",
    "end": "1317309"
  },
  {
    "text": "stacks enterprise and that is being stored in ec2 EBS but also inside of",
    "start": "1317309",
    "end": "1324600"
  },
  {
    "text": "what we call our cold store which is an s3 and we're going to talk to you a little bit about how we built a system",
    "start": "1324600",
    "end": "1329730"
  },
  {
    "text": "that can query data in s3 and get like one to three second response times even",
    "start": "1329730",
    "end": "1335549"
  },
  {
    "text": "while scanning quote-unquote hundreds of billions of rows at a time so the system is meant to scale really well does about",
    "start": "1335549",
    "end": "1343020"
  },
  {
    "text": "we peek over ten million transactions a second probably closer to fifteen just changed this since we put up the slide",
    "start": "1343020",
    "end": "1349520"
  },
  {
    "text": "we do about a hundred billion transactions a day and we have many petabytes at rest and we're gonna",
    "start": "1349520",
    "end": "1354690"
  },
  {
    "text": "subscribe a system that streams data coordinates in a fully distributed manner handles all of that and stores",
    "start": "1354690",
    "end": "1360960"
  },
  {
    "text": "and index that we have indexes in the petabyte range I mean this is a really big set of data so okay so I'm screen",
    "start": "1360960",
    "end": "1368970"
  },
  {
    "text": "'candy the short of it is that we're not just going to absorb this data in order to allow you know us to kind of use it",
    "start": "1368970",
    "end": "1377040"
  },
  {
    "text": "offline or asynchronously we're actually going to give our customers direct access to this we're gonna have a system",
    "start": "1377040",
    "end": "1382530"
  },
  {
    "text": "that's going to process and move this data with the expectation of real time something happens locally on their",
    "start": "1382530",
    "end": "1387570"
  },
  {
    "text": "network network latency later it's available for human interaction and human inquiry asking questions about it",
    "start": "1387570",
    "end": "1394620"
  },
  {
    "text": "the things our customers do will be investigate identify threats remediate",
    "start": "1394620",
    "end": "1401330"
  },
  {
    "text": "etc and so a lot of actionable intelligence that has really strict real-time requirements or near real-time",
    "start": "1401330",
    "end": "1406470"
  },
  {
    "text": "requirements and so by presenting this together we have this large multi-tenant system that we can sell to our customers",
    "start": "1406470",
    "end": "1413190"
  },
  {
    "text": "that has all this data in the aggregate and of all them multi-tenancy challenges inside a one single memory for the",
    "start": "1413190",
    "end": "1420059"
  },
  {
    "text": "network in the cloud and that transitions into something that people use to detect attacks in real time but",
    "start": "1420059",
    "end": "1427650"
  },
  {
    "text": "we're also going to store an unlimited amount of copy of this data our standard offering actually is a year of retention",
    "start": "1427650",
    "end": "1432870"
  },
  {
    "text": "which is just nuts and we're gonna let people search it and say we want those",
    "start": "1432870",
    "end": "1438030"
  },
  {
    "text": "search times to return in one to three seconds so really high-level Bowles I'm going to hand us off to Robert a",
    "start": "1438030",
    "end": "1443539"
  },
  {
    "text": "director of DevOps and he's going to talk you through some of the audaciousness of this which I hope is",
    "start": "1443539",
    "end": "1449880"
  },
  {
    "text": "super encouraging because it's super fun but also to talk about there's really practical ways to step in to a ludicrous",
    "start": "1449880",
    "end": "1455669"
  },
  {
    "text": "amount of data have it perform really well and be really cost effective we are a venture back startup we had to solve",
    "start": "1455669",
    "end": "1462330"
  },
  {
    "text": "this without bankrupting the company we're running out of money you know and so we found a way and we're gonna show it to you guys thanks gene so this is",
    "start": "1462330",
    "end": "1471630"
  },
  {
    "text": "kind of in a nutshell what we were trying to behold like this is the shape the problem we're trying to solve is to",
    "start": "1471630",
    "end": "1476700"
  },
  {
    "text": "have a data pipeline with very low latency we wanted to detect threats",
    "start": "1476700",
    "end": "1481919"
  },
  {
    "text": "quickly I need to be high availability I would don't really have any tolerance for downtime or data loss the database",
    "start": "1481919",
    "end": "1489720"
  },
  {
    "text": "needs to handle billions of writes per hour trillions of Records per year be able to handle those 100x bursts that",
    "start": "1489720",
    "end": "1496110"
  },
  {
    "text": "gene talked about and still be able to do searches with kind of sub 10 seconds and response time even for really",
    "start": "1496110",
    "end": "1502890"
  },
  {
    "text": "arbitrary queries I mean you know being able to search for just like I want to see this IP for this time range is",
    "start": "1502890",
    "end": "1508200"
  },
  {
    "text": "pretty easy but if you want to say hey for the finance department for the last week do we see any traffic that was on",
    "start": "1508200",
    "end": "1514500"
  },
  {
    "text": "port 80 but was not HTTP protocol from a threat perspective that's really interesting that could be somebody",
    "start": "1514500",
    "end": "1520380"
  },
  {
    "text": "trying to egress data or a threat coming in I you know trying to get through the firewall without being noticed you know",
    "start": "1520380",
    "end": "1527280"
  },
  {
    "text": "and that's classically a really hard problem for a typical database to solve so first I want to talk about how we",
    "start": "1527280",
    "end": "1533100"
  },
  {
    "start": "1530000",
    "end": "1619000"
  },
  {
    "text": "achieve the low end in latency and the high burst tolerance so the data processing pipeline",
    "start": "1533100",
    "end": "1538230"
  },
  {
    "text": "basically so this was our initial architecture when we started we figured all right we ingest the data and an",
    "start": "1538230",
    "end": "1544080"
  },
  {
    "text": "intake we run it through each processing engine in turn once it's gone through all the processing engines you know and",
    "start": "1544080",
    "end": "1550740"
  },
  {
    "text": "they publish their results to Kafka I will store the data into s3 once we're done processing it the major problem we",
    "start": "1550740",
    "end": "1557820"
  },
  {
    "text": "had here is that these engines some of them you know may not be burst tolerant they may have problems dealing with a",
    "start": "1557820",
    "end": "1564240"
  },
  {
    "text": "particular class of data or something like that and you know if we've got all the way over on the right if Engine 3 is",
    "start": "1564240",
    "end": "1570450"
  },
  {
    "text": "a little bit slow temporarily that backs up all the way back up to the customer sensor and the customer doesn't have",
    "start": "1570450",
    "end": "1575580"
  },
  {
    "text": "this you know ludicrously large buffer that we potentially have with s3 I so",
    "start": "1575580",
    "end": "1582390"
  },
  {
    "text": "basically what we did to solve this was instead of doing that we write the",
    "start": "1582390",
    "end": "1588060"
  },
  {
    "text": "packet data immediately on reception we write the packet 802 S 3 and then we're",
    "start": "1588060",
    "end": "1593490"
  },
  {
    "text": "publishing just references to the s3 objects in Kafka so if your programmer",
    "start": "1593490",
    "end": "1600000"
  },
  {
    "text": "this is kind of like saying it'll pass by reference instead of passed by value the data that's in Kafka you know we",
    "start": "1600000",
    "end": "1605880"
  },
  {
    "text": "considered trying to pass all of this through Kafka it's like all that packet data through Kafka and using it that way but the size of the Kafka cluster would",
    "start": "1605880",
    "end": "1612960"
  },
  {
    "text": "need to run would be pretty crazy so that wasn't really an option so this is you know the pass by reference thing really saved us here and then what",
    "start": "1612960",
    "end": "1620190"
  },
  {
    "start": "1619000",
    "end": "1638000"
  },
  {
    "text": "happens is from there the data is you know each engine runs essentially the",
    "start": "1620190",
    "end": "1626250"
  },
  {
    "text": "same consumer group consumes consumes the same topics from Kafka figures out which data eats the process downloads it",
    "start": "1626250",
    "end": "1632850"
  },
  {
    "text": "back from s3 processes it publishes the results to Kafka and then those results",
    "start": "1632850",
    "end": "1639090"
  },
  {
    "start": "1638000",
    "end": "1657000"
  },
  {
    "text": "are persisted to Cassandra so we're sort",
    "start": "1639090",
    "end": "1644400"
  },
  {
    "text": "of using s3 as a queue here it's doing the heavy lifting for us Kafka is handling the message semantics that at",
    "start": "1644400",
    "end": "1650460"
  },
  {
    "text": "least once guaranty and that sort of thing but s3 is handling the the big",
    "start": "1650460",
    "end": "1655830"
  },
  {
    "text": "data volume so to talk a little bit more detail about our Kafka clusters today we",
    "start": "1655830",
    "end": "1663750"
  },
  {
    "start": "1657000",
    "end": "1725000"
  },
  {
    "text": "have about 45 Kafka brokers you know about a thousand topics maybe",
    "start": "1663750",
    "end": "1669960"
  },
  {
    "text": "1,200 partitions per broker we used to have a lot more topics than that we used",
    "start": "1669960",
    "end": "1675450"
  },
  {
    "text": "to go with a topic per data type per sensor per customer the problem with",
    "start": "1675450",
    "end": "1681119"
  },
  {
    "text": "that is that ultimately a topic is is kind of a unit as scaling like if you have 12 partitions in a topic then you",
    "start": "1681119",
    "end": "1686700"
  },
  {
    "text": "can have 12 consumers on a topic I and so if you get one burst from one customer they're kind of saturating",
    "start": "1686700",
    "end": "1693659"
  },
  {
    "text": "their own little topic if we share a topic then we have more more economy of scale we're using GP to EBS volumes as",
    "start": "1693659",
    "end": "1701820"
  },
  {
    "text": "Andre mentioned sp1 is much more typical use case for Kafka but because we're",
    "start": "1701820",
    "end": "1707580"
  },
  {
    "text": "publishing very small message ISM because we have a lot of partitions per broker we're concerned about the amount",
    "start": "1707580",
    "end": "1713639"
  },
  {
    "text": "of random i/o we do and so we use GP to and you as you can see we we peak over",
    "start": "1713639",
    "end": "1719070"
  },
  {
    "text": "100 megabytes per second per per broker today I was really with no problem so we",
    "start": "1719070",
    "end": "1727230"
  },
  {
    "start": "1725000",
    "end": "1823000"
  },
  {
    "text": "initially went with Kafka because we needed a message bus that we could scale would ensure we didn't drop data it has",
    "start": "1727230",
    "end": "1733889"
  },
  {
    "text": "it at least once guarantee I and that's really crucial for us you know we're more concerned you know if we're e",
    "start": "1733889",
    "end": "1740399"
  },
  {
    "text": "consume the same data by mistake that's not a problem but if we fail to consume data if we fail to detect a threat that",
    "start": "1740399",
    "end": "1746100"
  },
  {
    "text": "that would be a big problem but something we didn't really think about",
    "start": "1746100",
    "end": "1751110"
  },
  {
    "text": "when we when we chose Kafka that has been a huge huge win for us has been testing in production you guys probably",
    "start": "1751110",
    "end": "1758039"
  },
  {
    "text": "seen the t-shirt you know I don't know always test but when I do it's in production i we actually do a lot of",
    "start": "1758039",
    "end": "1763769"
  },
  {
    "text": "testing in production I part of that is because of our scale it's really hard to afford to run a whole second like copy",
    "start": "1763769",
    "end": "1771570"
  },
  {
    "text": "of production that's it the kind of this kind of ludicrous scale and you know",
    "start": "1771570",
    "end": "1776820"
  },
  {
    "text": "have enough customers not paying for it but it's also a security sensitive data we don't really want to take a copy the",
    "start": "1776820",
    "end": "1782279"
  },
  {
    "text": "customers data and just copy it over into our test environment that's you know potentially less secure so what we",
    "start": "1782279",
    "end": "1788399"
  },
  {
    "text": "can do instead is if we have a new version of an engine it's got significant changes we're not sure how",
    "start": "1788399",
    "end": "1793559"
  },
  {
    "text": "it's going to perform or we just want to make sure that it's bug free we can have it consume from the same topic that the",
    "start": "1793559",
    "end": "1799980"
  },
  {
    "text": "current production version is consuming have it published profiling topic and then compare the",
    "start": "1799980",
    "end": "1805889"
  },
  {
    "text": "results and make sure that it's still working okay likewise you know we can change we can test significant performance changes",
    "start": "1805889",
    "end": "1812669"
  },
  {
    "text": "excuse me test significant changes to make sure that they'll keep up performance wise make sure that it's not",
    "start": "1812669",
    "end": "1817799"
  },
  {
    "text": "falling behind if we change GC settings or instance types or whatever so some",
    "start": "1817799",
    "end": "1825059"
  },
  {
    "start": "1823000",
    "end": "2018000"
  },
  {
    "text": "notes about Kafka a Kafka partition is your fundamental unit of scaling so use",
    "start": "1825059",
    "end": "1832169"
  },
  {
    "text": "lots of partitions I mean not millions or something but we kind of try and",
    "start": "1832169",
    "end": "1837419"
  },
  {
    "text": "target about 4x to 10x partitions per consumer so that we can add more",
    "start": "1837419",
    "end": "1842700"
  },
  {
    "text": "consumers I you know obviously you can you can add more partitions to a topic but if the data is already in there the",
    "start": "1842700",
    "end": "1848970"
  },
  {
    "text": "new partitions that you add are going to be empty you want to keep an eye on like the so I",
    "start": "1848970",
    "end": "1856349"
  },
  {
    "text": "think earlier I may have said you know 200 partitions in a topic in some of our larger topics is actually a hundred",
    "start": "1856349",
    "end": "1862619"
  },
  {
    "text": "ninety-two is the number we use right now we tend to use multiples of six or",
    "start": "1862619",
    "end": "1867720"
  },
  {
    "text": "twelve kind of thing so with 192 you can have 12 or 16 or 24 or 96 consumers and",
    "start": "1867720",
    "end": "1875779"
  },
  {
    "text": "everybody's getting an equal number of partitions assigned to them so you get even distribution some of you consider",
    "start": "1875779",
    "end": "1883019"
  },
  {
    "text": "is the the partition assignment strategy the default partition assignment",
    "start": "1883019",
    "end": "1888090"
  },
  {
    "text": "strategy in Kafka is is still the range assignment if you have consumers that",
    "start": "1888090",
    "end": "1894029"
  },
  {
    "text": "are reading multiple topics like one one engine for instance in air model that's",
    "start": "1894029",
    "end": "1899429"
  },
  {
    "text": "consuming several topics the round-robin partition assignment is what you want rather than range just for scaling",
    "start": "1899429",
    "end": "1906450"
  },
  {
    "text": "reasons the range partition assignment says for each topic that you're",
    "start": "1906450",
    "end": "1912090"
  },
  {
    "text": "consuming in this consumer group consumer number one gets partitioned number one consumer number two gets partition number two etc so if you have",
    "start": "1912090",
    "end": "1920970"
  },
  {
    "text": "several topics and each have ten partitions you can do two consumers or you do five consumers you need ten",
    "start": "1920970",
    "end": "1926999"
  },
  {
    "text": "consumers but if you add an eleventh consumer he's going to be bored if you have nine that last that first consumer",
    "start": "1926999",
    "end": "1934950"
  },
  {
    "text": "is going to get partition number one and purchase number 10 he's going to get double the workload that's obviously not ideal so",
    "start": "1934950",
    "end": "1942490"
  },
  {
    "text": "with round-robin I what it does is it takes all the partitions of all the",
    "start": "1942490",
    "end": "1947620"
  },
  {
    "text": "topics that are being consumed by the consumer group shuffles them and then assigns them kind of yeah and in",
    "start": "1947620",
    "end": "1953710"
  },
  {
    "text": "round-robin order and what that means is if yeah let's say you have ten topics and ten partitions that's a hundred",
    "start": "1953710",
    "end": "1959980"
  },
  {
    "text": "total partition topics if you got twenty consumers each one's going to get five partitions so it allows you to scale",
    "start": "1959980",
    "end": "1968470"
  },
  {
    "text": "better some caveats and warnings about Kafka now something that we we",
    "start": "1968470",
    "end": "1974050"
  },
  {
    "text": "recognized recently that we hadn't really budgeted for considered was the cross a zebra application cost in a",
    "start": "1974050",
    "end": "1980170"
  },
  {
    "text": "large Kafka cluster Kafka only has a limited Iraq awareness you can make sure that the second copy of your data is not",
    "start": "1980170",
    "end": "1987700"
  },
  {
    "text": "in the same easy as the first copy of your data so you know if we lose an AZ",
    "start": "1987700",
    "end": "1992740"
  },
  {
    "text": "the Kafka cluster can keep on truckin which is which is great but producers",
    "start": "1992740",
    "end": "1998770"
  },
  {
    "text": "and consumers only talk to the leader of the partition whatever wherever that is that's who they're talking to the upshot",
    "start": "1998770",
    "end": "2006240"
  },
  {
    "text": "of this is with replication factor two so if you have your primary copy of your data then one backup your data with",
    "start": "2006240",
    "end": "2011790"
  },
  {
    "text": "could could cross AZ's three or more times three times just with a single",
    "start": "2011790",
    "end": "2017310"
  },
  {
    "text": "consumer or single producer here's how that looks so you've got to produce or producing data he writes it to the",
    "start": "2017310",
    "end": "2023310"
  },
  {
    "start": "2018000",
    "end": "2047000"
  },
  {
    "text": "leader now the leaders got to replicate the data so the write already crossed the AZ once now the leader replicates",
    "start": "2023310",
    "end": "2029520"
  },
  {
    "text": "the data back you know the first AZ again so it's crossed the data across the AZ boundary twice you've paid",
    "start": "2029520",
    "end": "2036180"
  },
  {
    "text": "network traffic twice and now the consumer reads the data and he's",
    "start": "2036180",
    "end": "2041190"
  },
  {
    "text": "crossing the AZ boundary again if you have many consumers of the same data that that multiplies I here's how that",
    "start": "2041190",
    "end": "2049440"
  },
  {
    "start": "2047000",
    "end": "2070000"
  },
  {
    "text": "looks in perspective for for those forty five nodes $8,000 for the reserved",
    "start": "2049440",
    "end": "2055470"
  },
  {
    "text": "instance pricing on those nodes I think there's c-4 to excel and then",
    "start": "2055470",
    "end": "2060870"
  },
  {
    "text": "another 8,000 for the GP to EBS volumes that would be half that if we use this you want to be four thousand but it's",
    "start": "2060870",
    "end": "2067050"
  },
  {
    "text": "dominated by the network traffic there's over forty thousand dollars something else to keep in mind is that a",
    "start": "2067050",
    "end": "2074210"
  },
  {
    "start": "2070000",
    "end": "2181000"
  },
  {
    "text": "single broker failure impacts the whole cluster oh it's not it doesn't if you lose a single broker it's not that the",
    "start": "2074210",
    "end": "2080360"
  },
  {
    "text": "clusters down forever until the broker comes back up but what happens is when",
    "start": "2080360",
    "end": "2085520"
  },
  {
    "text": "the broker fails any partitions that he was the leader for there needs to be a leader election held to figure out who",
    "start": "2085520",
    "end": "2091520"
  },
  {
    "text": "now is going to own that partition on large clusters that can take over a minute and then if that was a broker",
    "start": "2091520",
    "end": "2099470"
  },
  {
    "text": "restart I like a crash or something like that for us it always seems to happen that the broker returns just as that",
    "start": "2099470",
    "end": "2106070"
  },
  {
    "text": "rebalancing has finished up so while that rebalance was going on the producers were pause the consumers were",
    "start": "2106070",
    "end": "2112100"
  },
  {
    "text": "paused the rebalance is done they're just starting to get ready to consume data again the broker comes back leader",
    "start": "2112100",
    "end": "2117170"
  },
  {
    "text": "election happens again so plan for occasional pauses like it's it's",
    "start": "2117170",
    "end": "2122450"
  },
  {
    "text": "normally working fine but if you can't tolerate the occasional minute or two of stall this may not be ideal for you",
    "start": "2122450",
    "end": "2130630"
  },
  {
    "text": "something else is mostly trust the default settings with Kafka we've gotten",
    "start": "2130630",
    "end": "2136250"
  },
  {
    "text": "into trouble a few times tweaking settings it's pretty much all the settings particularly the timeouts",
    "start": "2136250",
    "end": "2142070"
  },
  {
    "text": "seem to be law of unintended consequences you know you get like we increase the",
    "start": "2142070",
    "end": "2150320"
  },
  {
    "text": "time out to get fewer rebalances and we get fewer rebalances but the flipside is that the cluster takes longer to notice",
    "start": "2150320",
    "end": "2156590"
  },
  {
    "text": "the consumer died and meanwhile everybody else is still consuming and then it figures out that the consumer",
    "start": "2156590",
    "end": "2162710"
  },
  {
    "text": "died and so it says all right we got to rebalance this and everybody consumes the same data over again",
    "start": "2162710",
    "end": "2170150"
  },
  {
    "text": "so aside from partition assignment where round-robin I think is is significantly",
    "start": "2170150",
    "end": "2175670"
  },
  {
    "text": "preferable spend some quality time with the docs before you adjust the defaults",
    "start": "2175670",
    "end": "2181300"
  },
  {
    "start": "2181000",
    "end": "2272000"
  },
  {
    "text": "all right so that was how we're using s3 as a cue now I want to talk about how we're using s3 is a database and",
    "start": "2181300",
    "end": "2188590"
  },
  {
    "text": "particularly how we got there so this is a shape of what we're trying to build",
    "start": "2188590",
    "end": "2194320"
  },
  {
    "text": "needed very high throughput good burst tolerance high scalability these are all",
    "start": "2194320",
    "end": "2200900"
  },
  {
    "text": "Cassander sweet spot but we also wanted to support those",
    "start": "2200900",
    "end": "2207109"
  },
  {
    "text": "arbitrary queries and that's definitely not Cassandra sweetspot you you pretty much your only doing",
    "start": "2207109",
    "end": "2212750"
  },
  {
    "text": "either primary key lookups and Cassandra or using a secondary index for a",
    "start": "2212750",
    "end": "2219289"
  },
  {
    "text": "specific thing but if you need to do just every single column is indexed effectively that's that's not Cassandra that's leucine",
    "start": "2219289",
    "end": "2226119"
  },
  {
    "text": "so either elasticsearch or solar cloud or for us because we were already using",
    "start": "2226119",
    "end": "2231500"
  },
  {
    "text": "data stacks Enterprise they have a product that it's called a DSC search that just basically solar is tightly",
    "start": "2231500",
    "end": "2238609"
  },
  {
    "text": "coupled to Cassandra which is nice because if you're using elasticsearch or solar cloud plus a database you're you",
    "start": "2238609",
    "end": "2246829"
  },
  {
    "text": "write to the database then you write to elasticsearch and you hope that both of those rights succeeded and as you may",
    "start": "2246829",
    "end": "2254839"
  },
  {
    "text": "know well as a search has had you know consistency issues over the years so typically what you end up doing is having some batch job that later goes",
    "start": "2254839",
    "end": "2261769"
  },
  {
    "text": "back and makes sure if at all the data you wrote to your primary database really is in your index I we haven't had",
    "start": "2261769",
    "end": "2268130"
  },
  {
    "text": "that problem with with data stacks Enterprise so that's that's been fantastic normally you'd like your",
    "start": "2268130",
    "end": "2277549"
  },
  {
    "text": "lucien indexes to live in memory whether its data stacks enterprise or anything else but the the volume of data that we",
    "start": "2277549",
    "end": "2285920"
  },
  {
    "text": "were dealing with that was clearly you know that wasn't even vaguely acceptable to be buying many terabytes of ram you",
    "start": "2285920",
    "end": "2294319"
  },
  {
    "text": "know and trying to keep it all in memory so we initially tried or we initially",
    "start": "2294319",
    "end": "2300500"
  },
  {
    "text": "started on SSDs I - - XLS and that was just barely fast enough it took a ton of",
    "start": "2300500",
    "end": "2306980"
  },
  {
    "text": "tuning but we got there the problem is that a month of data roughly speaking",
    "start": "2306980",
    "end": "2313880"
  },
  {
    "text": "was about a hundred hi - two XLS and we keep here at data so the first month is",
    "start": "2313880",
    "end": "2319309"
  },
  {
    "text": "fine yeah 100 I - - Excel is pretty expensive but you know ok we've got some",
    "start": "2319309",
    "end": "2325009"
  },
  {
    "text": "big customers we could deal with this but by the end of 12 months we would have 1200 - those you know I that wasn't",
    "start": "2325009",
    "end": "2332480"
  },
  {
    "text": "so good I should also mention we started by time we did that mostly because it's very",
    "start": "2332480",
    "end": "2337900"
  },
  {
    "text": "difficult to scale DC search because you have to re indexed like the data that is",
    "start": "2337900",
    "end": "2343630"
  },
  {
    "text": "streamed to a new node if you expand the cluster has to be re indexed in solar but we got some huge wins out of",
    "start": "2343630",
    "end": "2350440"
  },
  {
    "text": "sharding by time and I recommend everybody that's looking at any knows sequel this is not a Cassandra thing or",
    "start": "2350440",
    "end": "2357220"
  },
  {
    "text": "a Kafka thing or anything if you're looking at a no sequel product and you're hearing these talks about we're running a thousand node cluster of",
    "start": "2357220",
    "end": "2364599"
  },
  {
    "text": "something I guarantee you there's an ops guy up at night like not sleeping well",
    "start": "2364599",
    "end": "2370359"
  },
  {
    "text": "about that thousand node cluster and if you can make it 10 100 node clusters it's a lot easier to deal with",
    "start": "2370359",
    "end": "2377609"
  },
  {
    "text": "so in our quest to reduce the price we",
    "start": "2377609",
    "end": "2382900"
  },
  {
    "text": "realized that our data once once we had retired a shard essentially once we",
    "start": "2382900",
    "end": "2387940"
  },
  {
    "text": "stopped once with rolled a new shard and and the old shard was no longer taking primary rights it was still taking a few",
    "start": "2387940",
    "end": "2394569"
  },
  {
    "text": "rights but the right volume was much much lower and at that point we're actually able to store the data on GP to",
    "start": "2394569",
    "end": "2400690"
  },
  {
    "text": "EBS volumes on r4 to excels and that was I think about a 50% cost savings 50 or",
    "start": "2400690",
    "end": "2408819"
  },
  {
    "text": "75 percent cost savings unfortunately migrating the data off the ITU's to the r4 2x cells is very operationally",
    "start": "2408819",
    "end": "2415779"
  },
  {
    "text": "intensive you know we could do it and it saved us some money but it was a ton of manual labor so lessons learned at this",
    "start": "2415779",
    "end": "2424930"
  },
  {
    "start": "2422000",
    "end": "2487000"
  },
  {
    "text": "point if you go this route with this kind of data set expect to spend a lot",
    "start": "2424930",
    "end": "2430599"
  },
  {
    "text": "of time on tuning there's a lot of outdated advice out there talk to these",
    "start": "2430599",
    "end": "2436059"
  },
  {
    "text": "stacks if you're using DSC they have some tuning test tuning advice that was battle tested it protect wise they came",
    "start": "2436059",
    "end": "2442779"
  },
  {
    "text": "out spent a few days with us went back with like 20 JIRA tickets that are all resolved now so you know but some of the",
    "start": "2442779",
    "end": "2451329"
  },
  {
    "text": "outdated advice out there you'll see is like you you should never have more than an aka keep size for Cassandra well for",
    "start": "2451329",
    "end": "2457210"
  },
  {
    "text": "DC search you really want more like 30 gigabytes and g1g see works very well for that I've also seen some fun about G",
    "start": "2457210",
    "end": "2464380"
  },
  {
    "text": "1 GC we haven't had any issues with it so I definitely recommend it or high heaps and if at all possible use",
    "start": "2464380",
    "end": "2474190"
  },
  {
    "text": "Amazon EBS rather than ephemeral the operational I'll get into this a little",
    "start": "2474190",
    "end": "2480910"
  },
  {
    "text": "bit more later but but the operational gains that you get if you can possibly use it are definitely make it worthwhile",
    "start": "2480910",
    "end": "2489360"
  },
  {
    "start": "2487000",
    "end": "2633000"
  },
  {
    "text": "actually hear talk right now about why we weren't use EVs so the first thing we noticed was that instances without",
    "start": "2489360",
    "end": "2497350"
  },
  {
    "text": "ephemeral attached seem to fail less frequently like noticeably less frequently I would assume I haven't",
    "start": "2497350",
    "end": "2505960"
  },
  {
    "text": "asked Amazon this but I would assume that you know disk is one of the things that fails if you don't have disk attached to it there's one less thing to",
    "start": "2505960",
    "end": "2512050"
  },
  {
    "text": "fail I assume that that's why EVs volumes do fail but it's really really",
    "start": "2512050",
    "end": "2519430"
  },
  {
    "text": "rare I mean obviously plan for it but out of in the last four years and we",
    "start": "2519430",
    "end": "2526330"
  },
  {
    "text": "currently have about 4000 EBS volumes I think in us two in the last four years I",
    "start": "2526330",
    "end": "2532750"
  },
  {
    "text": "think we've had three EBS volume failures so it happens but obviously we have I mean we've had that many as as",
    "start": "2532750",
    "end": "2538930"
  },
  {
    "text": "failures a week so it's it's much more rare and losing the data you know having",
    "start": "2538930",
    "end": "2544900"
  },
  {
    "text": "to having to restore the data from backup or having to stream it back from replicas is the much more intensive",
    "start": "2544900",
    "end": "2550360"
  },
  {
    "text": "issue if you get an instance failure with nothing but EBS involved you just stop the instance to start the instance",
    "start": "2550360",
    "end": "2556600"
  },
  {
    "text": "and you're on new hardware and your data is still there more importantly for us",
    "start": "2556600",
    "end": "2562090"
  },
  {
    "text": "were the games that we got from decoupling state from compute this is",
    "start": "2562090",
    "end": "2567130"
  },
  {
    "text": "something we didn't really we weren't aiming at when we went into it but this was a big lesson for us you know the the",
    "start": "2567130",
    "end": "2576010"
  },
  {
    "text": "whole stateless web app thing like the web guys figured this out years ago that",
    "start": "2576010",
    "end": "2581350"
  },
  {
    "text": "stateless is fantastic but we've always assumed well it's the database you can't that's where the state is you can't be",
    "start": "2581350",
    "end": "2587920"
  },
  {
    "text": "stateless there but if you have the storage decoupled from the compute one",
    "start": "2587920",
    "end": "2593140"
  },
  {
    "text": "of the things that that means is I if you've got a Cassandra cluster that needs more CPU you stopped one AZ change",
    "start": "2593140",
    "end": "2600760"
  },
  {
    "text": "the instance type back up again stop the next a-z change the instance type now you have more CPU",
    "start": "2600760",
    "end": "2606160"
  },
  {
    "text": "more memory whatever it was you needed we did this as we were streaming data over so that we can re index data more",
    "start": "2606160",
    "end": "2612369"
  },
  {
    "text": "quickly likewise if you run you know you",
    "start": "2612369",
    "end": "2617619"
  },
  {
    "text": "get a customer that just suddenly starts shipping you massive amounts of data and you're suddenly about to run out of disk space you can expand an EBS volume where",
    "start": "2617619",
    "end": "2624910"
  },
  {
    "text": "with a ephemeral store you can't just say hey I like my i2 to XL to now have three terabytes",
    "start": "2624910",
    "end": "2632219"
  },
  {
    "start": "2633000",
    "end": "2743000"
  },
  {
    "text": "alright so despite all the wins that we got from EBS we needed something less",
    "start": "2633869",
    "end": "2639189"
  },
  {
    "text": "expensive you know even that 50 percent or whatever it was cost savings was not enough for the data set size that we had",
    "start": "2639189",
    "end": "2645819"
  },
  {
    "text": "in the size company that we are so wanted to get off of ephemeral entirely and we wanted to stop running a dozen",
    "start": "2645819",
    "end": "2653169"
  },
  {
    "text": "huge Cassandra clusters even if they're on EBS so we figured hey s3 is cheap how",
    "start": "2653169",
    "end": "2658749"
  },
  {
    "text": "can we use that it's working pretty low as a Kafka queue for us so what we ended",
    "start": "2658749",
    "end": "2666880"
  },
  {
    "text": "up doing and this is basically the right path for the data arriving in our system are really the metadata arriving in our",
    "start": "2666880",
    "end": "2674229"
  },
  {
    "text": "system is we're initially writing it still at a Cassandra and indexing it",
    "start": "2674229",
    "end": "2680199"
  },
  {
    "text": "solar but after a few hours go by when most of the writes have finished by that",
    "start": "2680199",
    "end": "2687489"
  },
  {
    "text": "I mean we still have observations that come in maybe weeks or months later where we find out hey there was a threat",
    "start": "2687489",
    "end": "2693369"
  },
  {
    "text": "here that you know we just we just discovered but the vast majority that writes have completed so after a few",
    "start": "2693369",
    "end": "2700089"
  },
  {
    "text": "hours we can pull the data out of Cassandra run a spark job to create park' files write those park' files to",
    "start": "2700089",
    "end": "2707439"
  },
  {
    "text": "s3 while we're doing that we're also computing some bloom filters and writing those two solar I'll get to that on the",
    "start": "2707439",
    "end": "2713049"
  },
  {
    "text": "read path but the main thing here the the park' files are much smaller I think",
    "start": "2713049",
    "end": "2719049"
  },
  {
    "text": "we end up getting about 90% compression like it's it's 90% less data in s3 than",
    "start": "2719049",
    "end": "2725109"
  },
  {
    "text": "it was in Cassandra part of that is because s3 already has the redundancy built in so we don't need three AZ's the",
    "start": "2725109",
    "end": "2731380"
  },
  {
    "text": "way we did with Cassandra and part of it is because park' has run length encoding it's a columnar datastore you can throw lz4 or",
    "start": "2731380",
    "end": "2738880"
  },
  {
    "text": "some other compression algorithm on top of it you get really good compression so",
    "start": "2738880",
    "end": "2744010"
  },
  {
    "start": "2743000",
    "end": "2838000"
  },
  {
    "text": "now we're going to talk about the read path this is kind of where the magic",
    "start": "2744010",
    "end": "2749410"
  },
  {
    "text": "happens on those bloom filters so if it's very recent data if somebody's asking us for something for the last",
    "start": "2749410",
    "end": "2754930"
  },
  {
    "text": "hour or if they're doing a time range a very large time range that includes last hour we're still asking the Cassandra",
    "start": "2754930",
    "end": "2761170"
  },
  {
    "text": "and solar for that's that time range but",
    "start": "2761170",
    "end": "2766530"
  },
  {
    "text": "lifting a whole like tens of millions of files from s3 you could do it but you'd",
    "start": "2766530",
    "end": "2772450"
  },
  {
    "text": "rather not write you you want to only lift the files that have the answers that you're looking for so just like you",
    "start": "2772450",
    "end": "2779200"
  },
  {
    "text": "know Andre mentioned that Cassandra uses bloom filters to decide might this data",
    "start": "2779200",
    "end": "2784480"
  },
  {
    "text": "be in this SS table I and the nice property of a bloom filter is if it says",
    "start": "2784480",
    "end": "2789670"
  },
  {
    "text": "no it's definitely not if it says yes then it's like it's adjustable but let's",
    "start": "2789670",
    "end": "2795250"
  },
  {
    "text": "say it's like 99 percent probability that it's yes so this means that if we have you know if our bloom filters say",
    "start": "2795250",
    "end": "2801940"
  },
  {
    "text": "lift these thousand files out of s3 then nine hundred ninety of them include the",
    "start": "2801940",
    "end": "2807400"
  },
  {
    "text": "answers that we're looking for and we lifted whatever they can't do math on stage ten files that we didn't need to",
    "start": "2807400",
    "end": "2815020"
  },
  {
    "text": "but the other key here is is highly parallel so we're able to and again",
    "start": "2815020",
    "end": "2821740"
  },
  {
    "text": "we've we've decoupled state from compute and so we're able to scale if we want to",
    "start": "2821740",
    "end": "2827440"
  },
  {
    "text": "be able to scale sorry if we want to be able to scale air our reads we can just",
    "start": "2827440",
    "end": "2833680"
  },
  {
    "text": "add more nodes to the spark cluster they read more objects in parallel so to",
    "start": "2833680",
    "end": "2840550"
  },
  {
    "start": "2838000",
    "end": "2885000"
  },
  {
    "text": "recap the lessons we learn overall Kafka is really valuable part of our platform",
    "start": "2840550",
    "end": "2846720"
  },
  {
    "text": "works great on EBS I'm not sure if we ever ran it on there ephemeral but yeah",
    "start": "2846720",
    "end": "2851890"
  },
  {
    "text": "definitely use EBS for it all right but if you're expecting massive scale keep in mind the cross AZ replication costs",
    "start": "2851890",
    "end": "2858930"
  },
  {
    "text": "you know for example two gigabytes per second for one month adds up to five petabytes also keep in mind that a",
    "start": "2858930",
    "end": "2866950"
  },
  {
    "text": "single broker causes availability problems not data loss but temporary glitches a",
    "start": "2866950",
    "end": "2872490"
  },
  {
    "text": "small clusters are easy to operate larger clusters have more issues so",
    "start": "2872490",
    "end": "2878290"
  },
  {
    "text": "that's again just like I was saying with Cassandra if you can operate more smaller clusters rather than one huge",
    "start": "2878290",
    "end": "2884260"
  },
  {
    "text": "cluster you'd really prefer to do it for Cassandra you know the main thing we",
    "start": "2884260",
    "end": "2891340"
  },
  {
    "start": "2885000",
    "end": "2928000"
  },
  {
    "text": "learned is that Cassandra and EBS both come a long way in a short time when we started we we actually were running just",
    "start": "2891340",
    "end": "2898810"
  },
  {
    "text": "pure Cassandra on magnetic EBS and it fell over instantly this was back in you",
    "start": "2898810",
    "end": "2904210"
  },
  {
    "text": "know 2013 2014 I you know and in fact data stacks advice still still on their",
    "start": "2904210",
    "end": "2909850"
  },
  {
    "text": "blog today from 2014 you know unless you want to add more complexity for your operations team use ephemeral today the",
    "start": "2909850",
    "end": "2916360"
  },
  {
    "text": "same blog but you know more recent posts as gp2 is the the best choice for most",
    "start": "2916360",
    "end": "2922150"
  },
  {
    "text": "workloads so you know make sure you check the date on blogs that you're",
    "start": "2922150",
    "end": "2927610"
  },
  {
    "text": "reading about particular about Cassandra or EBS something else to keep in mind for Cassandra",
    "start": "2927610",
    "end": "2934150"
  },
  {
    "start": "2928000",
    "end": "3019000"
  },
  {
    "text": "on the one hand it's really good at handling bursts which is normally a good thing but when you're benchmarking a",
    "start": "2934150",
    "end": "2940030"
  },
  {
    "text": "database I you know if you're benchmarking wise people or Postgres you could just Ram data into it and you know",
    "start": "2940030",
    "end": "2947350"
  },
  {
    "text": "if it's not accepting the inserts then it's not working if it's accepting the inserts it's keeping up good enough with",
    "start": "2947350",
    "end": "2953560"
  },
  {
    "text": "Cassandra I it will accept bursts and the way that that data bursts ends up",
    "start": "2953560",
    "end": "2960370"
  },
  {
    "text": "impacting it is it puts off the compactions those SS tables that it's writing to disk it wants to come back",
    "start": "2960370",
    "end": "2966160"
  },
  {
    "text": "those into fewer larger SS tables but if it's not keeping up it can just wait to do that a little bit what this means",
    "start": "2966160",
    "end": "2972880"
  },
  {
    "text": "from a benchmarking perspective is you want to run longer benchmarks watch for",
    "start": "2972880",
    "end": "2978490"
  },
  {
    "text": "pending compactions watch for block native transport requests while you're running those and",
    "start": "2978490",
    "end": "2983560"
  },
  {
    "text": "also make sure that you match your benchmark I mean this seems obvious but people will target like oh I want to do",
    "start": "2983560",
    "end": "2990520"
  },
  {
    "text": "a million writes per second and so they do a million writes per second and bam done but if you've if you're also doing",
    "start": "2990520",
    "end": "2995620"
  },
  {
    "text": "a hundred thousand reads per second or even a thousand reads per second that can significantly significantly affect",
    "start": "2995620",
    "end": "3001800"
  },
  {
    "text": "things there's a tool Kassandra stress which amazingly enough does all of this you can you can tune",
    "start": "3001800",
    "end": "3009150"
  },
  {
    "text": "your object sizes you read write ratios your key distribution so you can use",
    "start": "3009150",
    "end": "3014759"
  },
  {
    "text": "that single tool to match what you think your production workload is going to be",
    "start": "3014759",
    "end": "3020119"
  },
  {
    "start": "3019000",
    "end": "3092000"
  },
  {
    "text": "for Amazon EBS and s3 the main thing we learned here is that higher latency",
    "start": "3020119",
    "end": "3026960"
  },
  {
    "text": "doesn't always mean lower throughput you know for some of you your your workload",
    "start": "3026960",
    "end": "3032400"
  },
  {
    "text": "may be all about time to first byte and in that case if you need that first byte in under millisecond you know today",
    "start": "3032400",
    "end": "3038789"
  },
  {
    "text": "you're not gonna get it from s3 I probably not gonna get it from EBS either you're gonna need it in memory",
    "start": "3038789",
    "end": "3044359"
  },
  {
    "text": "but for massive workloads I when you're lifting tremendous amounts of data and",
    "start": "3044359",
    "end": "3050400"
  },
  {
    "text": "you need it in say a second you can still do that with s3 it's it can be",
    "start": "3050400",
    "end": "3055680"
  },
  {
    "text": "just as fast as ephemeral I you know the key there is that just as we see in CPU",
    "start": "3055680",
    "end": "3060930"
  },
  {
    "text": "megahertz kind of topped out and we've just started throwing more cores at problems you can bypass the high cost of",
    "start": "3060930",
    "end": "3068039"
  },
  {
    "text": "really fast storage by increasing parallelism to slightly slower storage",
    "start": "3068039",
    "end": "3074690"
  },
  {
    "text": "and also being able to decouple compute from state leads to all the same wins",
    "start": "3074690",
    "end": "3080069"
  },
  {
    "text": "that stateless service is brought to web front-ends and like I it's really that's",
    "start": "3080069",
    "end": "3086819"
  },
  {
    "text": "been one that you know we didn't rent a spate going in but we've learned a lot and and that's been tremendously helpful",
    "start": "3086819",
    "end": "3092809"
  },
  {
    "start": "3092000",
    "end": "3188000"
  },
  {
    "text": "one last thing we have really high write rate to s3 and this is something that",
    "start": "3092809",
    "end": "3099479"
  },
  {
    "text": "you know we struggled with initially and there's some blog posts out there but we didn't find them super clear so I wanted",
    "start": "3099479",
    "end": "3105599"
  },
  {
    "text": "to put this out explicitly just like Cassandra has sort of a ring structure",
    "start": "3105599",
    "end": "3113130"
  },
  {
    "text": "and you want to avoid hot spotting by writing all your data in one place in the ring s3 likewise could be viewed as",
    "start": "3113130",
    "end": "3120539"
  },
  {
    "text": "a ring structure and the objects unlike a standard they're not hashed there there's you know they appear around the",
    "start": "3120539",
    "end": "3126569"
  },
  {
    "text": "ring in lexicographic order what that means is that all the object names that",
    "start": "3126569",
    "end": "3132809"
  },
  {
    "text": "you wanted to use like you know we'll start with date well let's start with custom i d-- or something like that will attend",
    "start": "3132809",
    "end": "3138690"
  },
  {
    "text": "a hot spot i adding some randomness and five bytes for us has been way more than",
    "start": "3138690",
    "end": "3145020"
  },
  {
    "text": "sufficient talking to the s3 team adding sufficient randomness at the beginning of the object name will completely",
    "start": "3145020",
    "end": "3152700"
  },
  {
    "text": "alleviate that problem we've had over a billion objects and five petabytes in a bucket that's by far not the biggest",
    "start": "3152700",
    "end": "3161059"
  },
  {
    "text": "bucket in u.s. quest 2 for instance but and that was our production bucket and",
    "start": "3161059",
    "end": "3168030"
  },
  {
    "text": "while it was taking regular production workload we had an additional workload that we needed to run against that it",
    "start": "3168030",
    "end": "3173130"
  },
  {
    "text": "was 1600 api calls for per second for two weeks straight and it didn't have any impact i you know i don't think that",
    "start": "3173130",
    "end": "3180839"
  },
  {
    "text": "that would have been the case how do we not arrange the data well to begin with i'm gonna hand the mic back to jean or",
    "start": "3180839",
    "end": "3189420"
  },
  {
    "start": "3188000",
    "end": "3593000"
  },
  {
    "text": "do we want to just come on all on stage and do questions yeah let's all come up",
    "start": "3189420",
    "end": "3195650"
  },
  {
    "text": "so basically that's kind of a walkthrough of how we handle some of this these really large data problems so",
    "start": "3196640",
    "end": "3202529"
  },
  {
    "text": "really high throughput a lot of data in motion a lot of data to rest because of our retrospective analysis data arrest",
    "start": "3202529",
    "end": "3208500"
  },
  {
    "text": "my year ago it's just as relevant as data from this afternoon so it's how do you build a system like this the first",
    "start": "3208500",
    "end": "3214380"
  },
  {
    "text": "version of our product was pretty expensive we are struggling with",
    "start": "3214380",
    "end": "3219650"
  },
  {
    "text": "datasets that could not fit in ram as we mentioned so there's all SSD backed and then we were able to find a way to get",
    "start": "3219650",
    "end": "3227369"
  },
  {
    "text": "this data organized in park' and s3 we had a 95 percent cost reduction we actually have a viable business on top",
    "start": "3227369",
    "end": "3233520"
  },
  {
    "text": "of ludicrous amount of amount of data so at this point we really want to leave",
    "start": "3233520",
    "end": "3238799"
  },
  {
    "text": "you guys with this session with its sense that really aggressive workloads",
    "start": "3238799",
    "end": "3243990"
  },
  {
    "text": "are really manageable if you just embrace the basics of AWS get passionate",
    "start": "3243990",
    "end": "3249720"
  },
  {
    "text": "about storage get passionate about like EBS and throughput I ops get passionate about s/3 s/3",
    "start": "3249720",
    "end": "3256140"
  },
  {
    "text": "is the oldest thing out there and it is in my personal opinion the coolest piece of technology in all of AWS they hate it",
    "start": "3256140",
    "end": "3262829"
  },
  {
    "text": "when I say that but it is we have gotten quarter per terabyte a second out of that thing I mean where do you get that",
    "start": "3262829",
    "end": "3268619"
  },
  {
    "text": "right so what can you do with effect becomes all sudden easy so with",
    "start": "3268619",
    "end": "3273900"
  },
  {
    "text": "that in mind questions there's two microphones we can't see you really well because he's like you can see it a hand",
    "start": "3273900",
    "end": "3279360"
  },
  {
    "text": "up they but go up to the microphone please so if you have questions and one",
    "start": "3279360",
    "end": "3285810"
  },
  {
    "text": "other Cassandra sides you mentioned to not use V nodes can you oh yeah I knew",
    "start": "3285810",
    "end": "3291720"
  },
  {
    "text": "that I was gonna miss them on my speaker notes so if you're using regular Cassandra and not DSC search I",
    "start": "3291720",
    "end": "3298230"
  },
  {
    "text": "definitely use V nodes it makes operations way easier however",
    "start": "3298230",
    "end": "3303570"
  },
  {
    "text": "if you're using DC search you don't want to use V nodes the solar query ends up",
    "start": "3303570",
    "end": "3310050"
  },
  {
    "text": "being expanded into like if you have 32 V nodes then the coordinator breaks that",
    "start": "3310050",
    "end": "3315420"
  },
  {
    "text": "query up sends it to each node that needs to answer the question and each of those queries on each of those nodes has",
    "start": "3315420",
    "end": "3320640"
  },
  {
    "text": "32 or clauses added to it by the coordinator because the V nodes so",
    "start": "3320640",
    "end": "3327270"
  },
  {
    "text": "that's not great did you so what made you yeah so in the",
    "start": "3327270",
    "end": "3337410"
  },
  {
    "text": "early days like I'm one of the things that we expected like you know three or four years ago was that we might need to",
    "start": "3337410",
    "end": "3343260"
  },
  {
    "text": "be on from with customers they had their own data centers you know customers are security sensitive we're concerned about",
    "start": "3343260",
    "end": "3349410"
  },
  {
    "text": "that that's that's less the landscapes changed really fast that's less of a concern now but at the time the goal was",
    "start": "3349410",
    "end": "3355500"
  },
  {
    "text": "to not be too married to AWS today I one",
    "start": "3355500",
    "end": "3363240"
  },
  {
    "text": "of the things I don't think that Kinesis allows fan-out I'm not sure if you can have two consumers consume the same",
    "start": "3363240",
    "end": "3368790"
  },
  {
    "text": "stream from Kinesis anybody know if it doesn't I would wait till it does because that being able to test in",
    "start": "3368790",
    "end": "3374970"
  },
  {
    "text": "production has been a huge huge win for us thank you yeah along similar lines",
    "start": "3374970",
    "end": "3381000"
  },
  {
    "text": "have you considered like SNS and SQS in sort of a as a fan out and guaranteed",
    "start": "3381000",
    "end": "3387000"
  },
  {
    "text": "delivery once yeah that's that's one again we're so the data that's in Kafka",
    "start": "3387000",
    "end": "3393030"
  },
  {
    "text": "is in Kafka even after it was consumed for however long your buffer is so we do",
    "start": "3393030",
    "end": "3398370"
  },
  {
    "text": "like 24 hours I we used SNS and SQS briefly but one thing",
    "start": "3398370",
    "end": "3404430"
  },
  {
    "text": "we couldn't do with that would say hey we just consumed this data once and we think we got it wrong we want to",
    "start": "3404430",
    "end": "3412430"
  },
  {
    "text": "reconsider this consumed and you certainly couldn't stand up a second",
    "start": "3414440",
    "end": "3419460"
  },
  {
    "text": "copy of your production engine and run it on the last two hours where the traffic without having first known that",
    "start": "3419460",
    "end": "3425550"
  },
  {
    "text": "you wanted to keep a second sqs topic around just in case you wanted to do that so how many know that you know",
    "start": "3425550",
    "end": "3433710"
  },
  {
    "text": "yo Cassandra Craster and how big each node the largest cluster that we've run",
    "start": "3433710",
    "end": "3438900"
  },
  {
    "text": "was over 250 nodes I don't remember exactly I only remember it because of a",
    "start": "3438900",
    "end": "3445050"
  },
  {
    "text": "specific issue we were in into it when we added the 256 node today we we tend",
    "start": "3445050",
    "end": "3450570"
  },
  {
    "text": "to target a smaller where it like I think the most recent hot shard was 180 nodes that was the first party question",
    "start": "3450570",
    "end": "3457650"
  },
  {
    "text": "was the second part oh so those are i2 to XLS so it's a terabyte and a half of",
    "start": "3457650",
    "end": "3464360"
  },
  {
    "text": "SSD we have about 500 gigs if Cassandra data and 500 gigs of solar indexes per",
    "start": "3464360",
    "end": "3470400"
  },
  {
    "text": "node okay on that yep and have we so",
    "start": "3470400",
    "end": "3480480"
  },
  {
    "text": "really great talk thank you for all the in-depth technical details now that you have this really impressive basis what's",
    "start": "3480480",
    "end": "3489120"
  },
  {
    "text": "next basically two questions specifically one now you have one data",
    "start": "3489120",
    "end": "3495630"
  },
  {
    "text": "source at least one of the earlier slides you showed packets from on-prem customer systems right have you thought",
    "start": "3495630",
    "end": "3501570"
  },
  {
    "text": "about getting data from the cloud now users in the clouds oh we do getting",
    "start": "3501570",
    "end": "3507270"
  },
  {
    "text": "cloud at the scene yeah we are already yeah that's that's or has the I don't",
    "start": "3507270",
    "end": "3514380"
  },
  {
    "text": "know if they I don't know if the embargo is lifted on yeah yeah so I think the",
    "start": "3514380",
    "end": "3521250"
  },
  {
    "text": "embargo officially lifted like within the last several minutes they had an email that said we can talk about this",
    "start": "3521250",
    "end": "3526260"
  },
  {
    "text": "now that the at the end to talk not on the front touch part of the talk kind of funny but yeah we are now AWS network",
    "start": "3526260",
    "end": "3532830"
  },
  {
    "text": "security competency certified so we're kind of a launch partner on that front but honestly the idea is that this is",
    "start": "3532830",
    "end": "3539770"
  },
  {
    "text": "pretty available it's pretty approachable everybody should be doing it yeah and and the cloud is definitely",
    "start": "3539770",
    "end": "3545260"
  },
  {
    "text": "like like people that are in the cloud is definitely one of the you know one of the groups that were targeting this is not this was never envisioned as being a",
    "start": "3545260",
    "end": "3552100"
  },
  {
    "text": "strictly on-prem it was you know data centers offices ICS environments the",
    "start": "3552100",
    "end": "3557380"
  },
  {
    "text": "cloud anybody's cloud to be honest with you the industrial environments there are super cool you get to see some",
    "start": "3557380",
    "end": "3564070"
  },
  {
    "text": "really crazy stuff there it's like going back to the 1990s and discover your gallery pretty fun I think wow there's",
    "start": "3564070",
    "end": "3570430"
  },
  {
    "text": "botnets that are out there they're being beacon - they've been existed in 20 years but some pieces windows 95",
    "start": "3570430",
    "end": "3576100"
  },
  {
    "text": "oscilloscopes somewhere is beginning out - a sequel this by the way there's a pipe gonna be a secondary market I'm",
    "start": "3576100",
    "end": "3581140"
  },
  {
    "text": "collecting all that stuff yeah okay I know we got a couple more questions I think we're gonna drop down here so that the next speakers and stuff can I can",
    "start": "3581140",
    "end": "3589420"
  },
  {
    "text": "come in and start sitting up thank you guys",
    "start": "3589420",
    "end": "3593760"
  }
]