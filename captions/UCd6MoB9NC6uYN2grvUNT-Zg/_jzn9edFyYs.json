[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "okay so good morning everybody my name is Colin sessile I'm the an IT director",
    "start": "1700",
    "end": "6990"
  },
  {
    "text": "at Merck with responsibility for system retirement and archival program I'm here",
    "start": "6990",
    "end": "13019"
  },
  {
    "text": "with my colleague Bubba Pat row from cognizant today to talk to you about a project we undertook in the first half",
    "start": "13019",
    "end": "18930"
  },
  {
    "text": "of this year for the retirement of some of the maining assets on an IBM z cloud mainframe how we transferred the move to",
    "start": "18930",
    "end": "25980"
  },
  {
    "text": "AWS and the capabilities of AWS that we were looking for in a solution and also",
    "start": "25980",
    "end": "32398"
  },
  {
    "text": "how the cognizant processes and methodologies were able to assist us in the program Thank You Colin",
    "start": "32399",
    "end": "40469"
  },
  {
    "text": "my name is Baba Petro I lead a practice in infamous lifecycle management in this",
    "start": "40469",
    "end": "47129"
  },
  {
    "text": "pace of data and analytics thank you and Thank You Colin for being partnering",
    "start": "47129",
    "end": "54719"
  },
  {
    "text": "with us on this program it always amazed me how much in depth understanding you",
    "start": "54719",
    "end": "61079"
  },
  {
    "text": "have on the business problem situation relevant to mark and more than anything",
    "start": "61079",
    "end": "66240"
  },
  {
    "text": "your integrity the warranty thank you we will cover",
    "start": "66240",
    "end": "72689"
  },
  {
    "text": "today the marks business problem and situation that Colin will first explain",
    "start": "72689",
    "end": "78810"
  },
  {
    "text": "then we'll go over to the cognizance adaptive data foundation a new offering",
    "start": "78810",
    "end": "84000"
  },
  {
    "text": "in the space of data and analytics then we'll go over in the solution what was adopted to solve marks problem on the",
    "start": "84000",
    "end": "91439"
  },
  {
    "text": "data modernization and following followed by the business benefits that mark realized okay so we're just quickly",
    "start": "91439",
    "end": "101579"
  },
  {
    "start": "99000",
    "end": "190000"
  },
  {
    "text": "talk about Merck for a second for anyone not familiar Newark is a global life",
    "start": "101579",
    "end": "106680"
  },
  {
    "text": "sciences company specializing in human and animal health care we operate in",
    "start": "106680",
    "end": "112259"
  },
  {
    "text": "over 140 countries we've been in business for 125 years and then globally",
    "start": "112259",
    "end": "117869"
  },
  {
    "text": "as milk sharpened dome the the driver behind this project is that we had a",
    "start": "117869",
    "end": "125149"
  },
  {
    "text": "final IBM Z cloud mainframe a very powerful computing acid very fit for",
    "start": "125149",
    "end": "130229"
  },
  {
    "text": "purpose however we had migrated to the majority of our asset or flat and so we were looking to",
    "start": "130229",
    "end": "136140"
  },
  {
    "text": "complete the process and close down ZZ cloud the challenges here clearly there",
    "start": "136140",
    "end": "142230"
  },
  {
    "text": "was a significant operating cost associated with running a mainframe for",
    "start": "142230",
    "end": "147239"
  },
  {
    "text": "the remaining assets that we had running but additionally the mainframe has it's a powerful platform however in terms of",
    "start": "147239",
    "end": "153209"
  },
  {
    "text": "the analytics and usability and customer interface is not on par with some of the",
    "start": "153209",
    "end": "158910"
  },
  {
    "text": "technologies that we have available at this point in time so we were looking to would either retire or archive our",
    "start": "158910",
    "end": "166349"
  },
  {
    "text": "remaining assets and the asset that we'll be talking about today was one that was still under our data retention",
    "start": "166349",
    "end": "172260"
  },
  {
    "text": "policy so we had an obligation they lost or obligations to attain the data for an extended period so we partnered with",
    "start": "172260",
    "end": "178829"
  },
  {
    "text": "cognizant and we'll talk briefly about how you how cognizant went about the",
    "start": "178829",
    "end": "185060"
  },
  {
    "text": "retirement process and then we'll talk about some of the benefits we gained from that",
    "start": "185060",
    "end": "190340"
  },
  {
    "start": "190000",
    "end": "633000"
  },
  {
    "text": "Thank You cognizance adaptive data foundation is a new offering in the",
    "start": "190340",
    "end": "200790"
  },
  {
    "text": "space of data and analytics all of us and some fashion or form engaged",
    "start": "200790",
    "end": "208579"
  },
  {
    "text": "modernizing our companies digitally this is not new the digital natives are",
    "start": "208579",
    "end": "216319"
  },
  {
    "text": "causing the disruption in the marketplace that we have seen in last few years hardly anybody can deny that",
    "start": "216319",
    "end": "225680"
  },
  {
    "text": "at cognizant we unravel the patterns in",
    "start": "225680",
    "end": "231419"
  },
  {
    "text": "which these digital natives are causing the disruption and threatening well",
    "start": "231419",
    "end": "237269"
  },
  {
    "text": "established forms if they are not catching the bus to get on to the",
    "start": "237269",
    "end": "243540"
  },
  {
    "text": "modernizing their landscape their business operations there they're providing how they are providing the",
    "start": "243540",
    "end": "249630"
  },
  {
    "text": "customer experience so we unravel that and that is where comes cognizance",
    "start": "249630",
    "end": "257250"
  },
  {
    "text": "adopted at a foundation we see them through three building blocks",
    "start": "257250",
    "end": "265820"
  },
  {
    "text": "Everage transformation starts with customer if you are inside two customers",
    "start": "266090",
    "end": "273400"
  },
  {
    "text": "experience that you want to generate for if that is good you can sustain in the",
    "start": "273400",
    "end": "279290"
  },
  {
    "text": "marketplace in a greater way to deliver to that customer experience you are",
    "start": "279290",
    "end": "284540"
  },
  {
    "text": "product in services got to be intelligent we cannot look at in silos",
    "start": "284540",
    "end": "292370"
  },
  {
    "text": "products and services separately because all they'll go to the same point of",
    "start": "292370",
    "end": "298370"
  },
  {
    "text": "giving to the customer that experience that you promised and to deliver that experience your business operation got",
    "start": "298370",
    "end": "305570"
  },
  {
    "text": "to be adaptive responsive and intelligent this created the the the",
    "start": "305570",
    "end": "315919"
  },
  {
    "text": "avenue for bringing a reference model an",
    "start": "315919",
    "end": "321580"
  },
  {
    "text": "architecture overarching architecture that addresses this concern that is",
    "start": "321580",
    "end": "327289"
  },
  {
    "text": "where it comes cognizance adapted at a foundation so it starts with very three",
    "start": "327289",
    "end": "340700"
  },
  {
    "text": "basic principles starts with one responsive data architecture the pace at",
    "start": "340700",
    "end": "348380"
  },
  {
    "text": "which you get the data ingested consumed and report has to cover holistic view of",
    "start": "348380",
    "end": "357410"
  },
  {
    "text": "the data the structure varying structures of data the data sets the",
    "start": "357410",
    "end": "364550"
  },
  {
    "text": "temporal temporal values at a pace at which you can consume and you can use",
    "start": "364550",
    "end": "370729"
  },
  {
    "text": "for intelligence that's the first principle that we used in the adaptive",
    "start": "370729",
    "end": "376970"
  },
  {
    "text": "data foundation second why I mean this a",
    "start": "376970",
    "end": "382460"
  },
  {
    "text": "time in this in the industry people cannot wait for weeks and months to",
    "start": "382460",
    "end": "388760"
  },
  {
    "text": "generate a new data inside a new data set that helps you to create a new data",
    "start": "388760",
    "end": "394789"
  },
  {
    "text": "insight you got to be have",
    "start": "394789",
    "end": "399800"
  },
  {
    "text": "this intelligence built into your data management to generate help produce the",
    "start": "399800",
    "end": "408349"
  },
  {
    "text": "pace at which you you want to bring that inside out and third companies like",
    "start": "408349",
    "end": "416780"
  },
  {
    "text": "Amazon producing identifying and producing a new insight at a greater",
    "start": "416780",
    "end": "424789"
  },
  {
    "text": "space you don't believe in eleven point five six second two-stage aisle in the",
    "start": "424789",
    "end": "432080"
  },
  {
    "text": "marketplace you need the pace you need scale that is where deliberate scale",
    "start": "432080",
    "end": "437690"
  },
  {
    "text": "comes into play and these are the core principles of adapted data foundation",
    "start": "437690",
    "end": "442909"
  },
  {
    "text": "you have intelligent data works built on top of these three is a transformation",
    "start": "442909",
    "end": "449270"
  },
  {
    "text": "toolkit that adopted at a foundation has it and it is built on key nineteen",
    "start": "449270",
    "end": "457250"
  },
  {
    "text": "attributes we have identified we have seen the patterns and we identified 19 attributes key to address all these",
    "start": "457250",
    "end": "465949"
  },
  {
    "text": "three different pillars let's let's",
    "start": "465949",
    "end": "474110"
  },
  {
    "text": "contextualize for Amazon Web service and AWS the ADF story the ADF foundations to",
    "start": "474110",
    "end": "485900"
  },
  {
    "text": "work we believe your environment got to",
    "start": "485900",
    "end": "491270"
  },
  {
    "text": "be cloud enable or cloud leverage that is the first thing that you have to located second you our compute got to be",
    "start": "491270",
    "end": "502430"
  },
  {
    "text": "separate from their storage it's no more monolithic third your stories that you",
    "start": "502430",
    "end": "515089"
  },
  {
    "text": "adopt for handling all the analytics workload got to be containerized the",
    "start": "515089",
    "end": "520849"
  },
  {
    "text": "compute got to be containerized the storage got to be object storage for",
    "start": "520849",
    "end": "526279"
  },
  {
    "text": "your enterprise workloads to generate the pace",
    "start": "526279",
    "end": "531380"
  },
  {
    "text": "the scale these are the principal you got to adopt at the enterprise level",
    "start": "531380",
    "end": "537130"
  },
  {
    "text": "with the aim at generating the responsive insight at a massive scale",
    "start": "537130",
    "end": "546280"
  },
  {
    "text": "it's not easy you have lot of lines of business you know enterprise it's not easy to get to generate that insight but",
    "start": "546280",
    "end": "553550"
  },
  {
    "text": "if you adopt these principles at every level of your enterprise you would be",
    "start": "553550",
    "end": "559400"
  },
  {
    "text": "able to get to this point in this time no more it is uni direction of the data",
    "start": "559400",
    "end": "566060"
  },
  {
    "text": "flow I have to consume the data through the reports is just one way it doesn't",
    "start": "566060",
    "end": "573050"
  },
  {
    "text": "work anymore you got to have multi directional flow of the data going into",
    "start": "573050",
    "end": "581150"
  },
  {
    "text": "the customer experience bringing the customer experience back into building the the modeling work immediately and",
    "start": "581150",
    "end": "588290"
  },
  {
    "text": "then generating the enhanced analytics to support the customer experience got",
    "start": "588290",
    "end": "594620"
  },
  {
    "text": "to have multi directional flow built in to your model and to fundamentally at",
    "start": "594620",
    "end": "602930"
  },
  {
    "text": "the the at the gate of this throng this thing you got to have a responsive model",
    "start": "602930",
    "end": "612560"
  },
  {
    "text": "underneath all of it a model that can self address self here it got to have",
    "start": "612560",
    "end": "619100"
  },
  {
    "text": "those things built in it is not an add-on typically we do that ardan we",
    "start": "619100",
    "end": "624920"
  },
  {
    "text": "clearly say that with clearly prescribes what not to be hard on on the modeling",
    "start": "624920",
    "end": "632560"
  },
  {
    "start": "633000",
    "end": "978000"
  },
  {
    "text": "let's spend some time and I will request Colin to talk about it we used automated",
    "start": "633220",
    "end": "640280"
  },
  {
    "text": "data lifecycle management is one of the attributes that I talked about nineteen attributes are adaptive data foundation",
    "start": "640280",
    "end": "646460"
  },
  {
    "text": "in the more context so part of the",
    "start": "646460",
    "end": "651740"
  },
  {
    "text": "challenge we had on this particular project is that the the acid have been",
    "start": "651740",
    "end": "657350"
  },
  {
    "text": "resident on the mainframe for an extended period of time we used a portion of the asset but the",
    "start": "657350",
    "end": "663230"
  },
  {
    "text": "entire asset under our data protection responsibilities so the first two phases",
    "start": "663230",
    "end": "669470"
  },
  {
    "text": "here was the lifecycle management the the discover and the classify was very important in this instance because we",
    "start": "669470",
    "end": "676160"
  },
  {
    "text": "needed before we could migrate the data and test the migration we needed to understand what the asset was how it was",
    "start": "676160",
    "end": "684050"
  },
  {
    "text": "being used and what the retention we needed to apply to it was so that was a couple of phases that became integral or",
    "start": "684050",
    "end": "690860"
  },
  {
    "text": "very critical to our program was the understanding of this asset and ensuring that as we bought it over we understood",
    "start": "690860",
    "end": "697940"
  },
  {
    "text": "it sufficiently to decide design the testing methodology that would allow us to prove that we retained the asset and",
    "start": "697940",
    "end": "704720"
  },
  {
    "text": "made our met our attention policies and",
    "start": "704720",
    "end": "709880"
  },
  {
    "text": "the foundation how did we achieve that everything we built keeping in mind the",
    "start": "709880",
    "end": "718340"
  },
  {
    "text": "delivery at scale there is a mainframe sitting not up unknowns what data is to",
    "start": "718340",
    "end": "724490"
  },
  {
    "text": "be there nobody knows so if you have a people dependency on your discovery then",
    "start": "724490",
    "end": "730130"
  },
  {
    "text": "you are lacking something you always say XYZ said this is what my requirement you",
    "start": "730130",
    "end": "736190"
  },
  {
    "text": "got to hit to the point where what makes this source of truth for your for any",
    "start": "736190",
    "end": "741710"
  },
  {
    "text": "modernization story she got to have the intelligent built in how you discover",
    "start": "741710",
    "end": "747140"
  },
  {
    "text": "the data how you classify the data we classified the data with the intelligence saying this is all the day",
    "start": "747140",
    "end": "755300"
  },
  {
    "text": "every day you are using this is category one this is potentially to be used in similar reporting fashion category two",
    "start": "755300",
    "end": "762230"
  },
  {
    "text": "and there is another set of data massive data about 6070 terabyte after doing the",
    "start": "762230",
    "end": "769160"
  },
  {
    "text": "DD we found about 70 terabyte of 60 terabyte of data that to be kept for it",
    "start": "769160",
    "end": "775670"
  },
  {
    "text": "ins and compliance only discovery only identification and classification of the",
    "start": "775670",
    "end": "781100"
  },
  {
    "text": "data without having any people dependency directly taking it from this from the source of truth from the system",
    "start": "781100",
    "end": "787820"
  },
  {
    "text": "made us the confidence that we have not left anything undiscovered that is",
    "start": "787820",
    "end": "794600"
  },
  {
    "text": "number one number two then we started migrating them and implementing them baneling the",
    "start": "794600",
    "end": "800489"
  },
  {
    "text": "monitoring process it's important than how you monitor anybody can say that",
    "start": "800489",
    "end": "806100"
  },
  {
    "text": "okay I have a job it failed and I monitor but bringing that self adjustment the job adjusts by itself and",
    "start": "806100",
    "end": "814049"
  },
  {
    "text": "tells you I fail and I fixed it here is the report when you do the testing how",
    "start": "814049",
    "end": "820859"
  },
  {
    "text": "exactly you are doing the testing at every level of bringing the data over",
    "start": "820859",
    "end": "827429"
  },
  {
    "text": "and consuming the report requires a different level of insight and that is",
    "start": "827429",
    "end": "832619"
  },
  {
    "text": "where the principles of adaptive data foundation we leveraged here next we",
    "start": "832619",
    "end": "841799"
  },
  {
    "text": "will talk about the implementation framework we talked about the data they",
    "start": "841799",
    "end": "849689"
  },
  {
    "text": "are sitting on virtual tape aids old could be 30 years hardly anybody knows",
    "start": "849689",
    "end": "855600"
  },
  {
    "text": "what data is there the virtual tape is not easily available that you can run a",
    "start": "855600",
    "end": "860939"
  },
  {
    "text": "report and get it and this is the case for most of the legacy systems and we",
    "start": "860939",
    "end": "869189"
  },
  {
    "text": "used different rex programming to go to",
    "start": "869189",
    "end": "874319"
  },
  {
    "text": "the bottom of it and then get the data and we got about 10,000 plus data data",
    "start": "874319",
    "end": "880860"
  },
  {
    "text": "volume our data sets that we can say uniquely identifiable that requires an analysis and categorization and make the",
    "start": "880860",
    "end": "889169"
  },
  {
    "text": "decision what to be kept and what not to be kept so the only analysis gave us",
    "start": "889169",
    "end": "895410"
  },
  {
    "text": "those window of opportunity second the the tape volume was more than 110",
    "start": "895410",
    "end": "902939"
  },
  {
    "text": "terabyte and it can go through the legacy you will see those are already compressed when you uncompress them it",
    "start": "902939",
    "end": "909929"
  },
  {
    "text": "could be 150 terabyte plus if you would have looked at that gigantic it would",
    "start": "909929",
    "end": "915389"
  },
  {
    "text": "have taken a different way of thinking what to be done to get those data out",
    "start": "915389",
    "end": "920899"
  },
  {
    "text": "right so to handle that the discovery and analysis and also the migration",
    "start": "920899",
    "end": "927269"
  },
  {
    "text": "required the scale when the when it comes to the testing",
    "start": "927269",
    "end": "932580"
  },
  {
    "text": "part we used well-established GXP compliant sampling methods and we will",
    "start": "932580",
    "end": "940529"
  },
  {
    "text": "talk about it shown how did we do it in the solution slide when it comes to the reporting it",
    "start": "940529",
    "end": "948180"
  },
  {
    "text": "was very important that in mainframe you can write 1015 jobs to pull a report out",
    "start": "948180",
    "end": "954240"
  },
  {
    "text": "and after 10 years 15 years these people are not there when you go and saying",
    "start": "954240",
    "end": "959880"
  },
  {
    "text": "what I have to do it's very hard to reverse engineer those things but we",
    "start": "959880",
    "end": "965160"
  },
  {
    "text": "could do that at the scale at which we can provide this report in the very fast",
    "start": "965160",
    "end": "970620"
  },
  {
    "text": "fashion we'll also talk about when when we talk about the entire solution that we implemented let me just jump in here",
    "start": "970620",
    "end": "979290"
  },
  {
    "text": "for one second so one of the more interesting challenges that we face during this project in the modern world",
    "start": "979290",
    "end": "986580"
  },
  {
    "text": "many times we're becoming very custom door and we make assumptions around the",
    "start": "986580",
    "end": "992339"
  },
  {
    "text": "availability to move data in a very large very fast manner what we discovered is the the the contracts we",
    "start": "992339",
    "end": "1001459"
  },
  {
    "text": "had with communication companies and some of the infrastructure that we had in place were not scale to move the",
    "start": "1001459",
    "end": "1006770"
  },
  {
    "text": "volume of data that we're referring to here mainframe data is typically bits and bytes very small low volumes what we",
    "start": "1006770",
    "end": "1013820"
  },
  {
    "text": "had in place have been fit for purpose for decades but now we're trying to move at Bubba said nearly a hundred terabytes",
    "start": "1013820",
    "end": "1020930"
  },
  {
    "text": "of data we need to move it fast and we need to be able to demonstrate control from end to end and we were looking at",
    "start": "1020930",
    "end": "1027290"
  },
  {
    "text": "multiple options including introducing new contracts with the telecommunication",
    "start": "1027290",
    "end": "1032480"
  },
  {
    "text": "companies introducing new hardware both on our side and at IBM and the timeframe",
    "start": "1032480",
    "end": "1038058"
  },
  {
    "text": "to actually set those up was very significant and then once you've established that you then got to move",
    "start": "1038059",
    "end": "1044000"
  },
  {
    "text": "the data so we ended up with a fairly complex solution whereby we transfer the",
    "start": "1044000",
    "end": "1049670"
  },
  {
    "text": "data off the a mainframe onto a IBM a storage device we then brought that to",
    "start": "1049670",
    "end": "1056450"
  },
  {
    "text": "the Merck environment transferred the data off onto a Merck device and then on",
    "start": "1056450",
    "end": "1061520"
  },
  {
    "text": "to the AWS snowball and then shipped it over to which sounds an incredibly convoluted and complex and unnecessary process the",
    "start": "1061520",
    "end": "1069710"
  },
  {
    "text": "challenger's and it said when you used to back to think about it understandable IBM and Amazon both run their own data",
    "start": "1069710",
    "end": "1075679"
  },
  {
    "text": "centers and they want to ensure security and control in those environments they don't want to have any unregistered",
    "start": "1075679",
    "end": "1081230"
  },
  {
    "text": "devices located within their data centers so neither company understandably wanted to allow the",
    "start": "1081230",
    "end": "1086480"
  },
  {
    "text": "others companies mass storage device into their environment so this added the complexity of us actually saying will",
    "start": "1086480",
    "end": "1091970"
  },
  {
    "text": "ship it to Merc will then transfer the data between the two devices and move it over complex cumbersome but it turned",
    "start": "1091970",
    "end": "1099679"
  },
  {
    "text": "out to be a quicker way of actually moving this large volume of data given the infrastructure we had in place than",
    "start": "1099679",
    "end": "1105710"
  },
  {
    "text": "trying to improve the infrastructure and deploy that in the timeframe we had available thank you",
    "start": "1105710",
    "end": "1113950"
  },
  {
    "text": "we used primarily three we call attributes of the adapted upon us and",
    "start": "1114020",
    "end": "1120350"
  },
  {
    "text": "primarily three attributes solving mark's problem here the automated data lifecycle management the cloud enabled",
    "start": "1120350",
    "end": "1129159"
  },
  {
    "text": "architecture at scale and unified security model and for every aspect of",
    "start": "1129159",
    "end": "1138169"
  },
  {
    "text": "the attribute there are framework there are workbench there are services toolkits that we can leverage to deliver",
    "start": "1138169",
    "end": "1146000"
  },
  {
    "text": "at scale to advance and that's be at the",
    "start": "1146000",
    "end": "1156919"
  },
  {
    "text": "wrong angle okay so what we did was we",
    "start": "1156919",
    "end": "1161950"
  },
  {
    "text": "implemented this work loads in debuffs",
    "start": "1161950",
    "end": "1167049"
  },
  {
    "text": "we didn't want to say that finalize the requirement which is typically the case",
    "start": "1167049",
    "end": "1172190"
  },
  {
    "text": "finalize the requirement then we will do it because it's never-ending every time you speak to a new person you",
    "start": "1172190",
    "end": "1178190"
  },
  {
    "text": "will get different understanding of what data to be archived what data to be modernized and we created the devops in",
    "start": "1178190",
    "end": "1185840"
  },
  {
    "text": "a way every data ingested goes through the migration process goes to the",
    "start": "1185840",
    "end": "1192049"
  },
  {
    "text": "validation goes to the functional validation of reporting that it",
    "start": "1192049",
    "end": "1198290"
  },
  {
    "text": "was reporting so we run through these things in multiple chunks and it can",
    "start": "1198290",
    "end": "1203770"
  },
  {
    "text": "almost like a self cycle self oil engine for every chunk the moment one comes in",
    "start": "1203770",
    "end": "1210380"
  },
  {
    "text": "a dataset how does it go through this life cycle until the user report being generated we will talk about the data",
    "start": "1210380",
    "end": "1220460"
  },
  {
    "start": "1218000",
    "end": "1359000"
  },
  {
    "text": "migration followed by the validation principles and how did we do validate and then we will also talk about the the",
    "start": "1220460",
    "end": "1227960"
  },
  {
    "text": "user experience and the reporting this is very complex what you see but the",
    "start": "1227960",
    "end": "1235180"
  },
  {
    "text": "essence of it is that at every integration point we looked at it how to",
    "start": "1235180",
    "end": "1242870"
  },
  {
    "text": "build in the elasticity at large what do I mean that if we have we adopt a",
    "start": "1242870",
    "end": "1250490"
  },
  {
    "text": "snowball and snowball comes out after week you realize certain amount data is",
    "start": "1250490",
    "end": "1256880"
  },
  {
    "text": "still there you have to send another snowball it's it's not going to be an easy cycle you got to find out for ma",
    "start": "1256880",
    "end": "1264230"
  },
  {
    "text": "bulk migration snowball for the other ones which is the peripheral data that",
    "start": "1264230",
    "end": "1270920"
  },
  {
    "text": "you discover until almost to the end of this program you should have an elasticity to built in how do you want",
    "start": "1270920",
    "end": "1277580"
  },
  {
    "text": "to make the data over and that was we kept it in the mind that a certain",
    "start": "1277580",
    "end": "1283580"
  },
  {
    "text": "amount of data that is being consumed through the pores we will bring it without having the mass the bulk",
    "start": "1283580",
    "end": "1291490"
  },
  {
    "text": "appliances that we brought in towards the end of the program that it wasn't in easy conversation like Mormons but like",
    "start": "1291490",
    "end": "1298670"
  },
  {
    "text": "Colin mentioned about it it wasn't easy there are certain things that you would",
    "start": "1298670",
    "end": "1304850"
  },
  {
    "text": "also come across when you do legacy modernization these this master is",
    "start": "1304850",
    "end": "1311450"
  },
  {
    "text": "migration devices are built to take the data into the cloud data center they are",
    "start": "1311450",
    "end": "1317270"
  },
  {
    "text": "not built to do they take the data of your cloud data center we experience",
    "start": "1317270",
    "end": "1322730"
  },
  {
    "text": "that issue we went through almost two months of churning process before they could reconfigure this bulk device to",
    "start": "1322730",
    "end": "1330500"
  },
  {
    "text": "take the data off of data center and those are the experiences we'll talk about shortly while those things going on we didn't",
    "start": "1330500",
    "end": "1337489"
  },
  {
    "text": "wait we got the data over we created these reports this business started getting building the confidence yes we",
    "start": "1337489",
    "end": "1344450"
  },
  {
    "text": "are able to see the report so there's the end of the day they have to give",
    "start": "1344450",
    "end": "1349489"
  },
  {
    "text": "sign off that mainframe can be shot down so we built elasticity at every",
    "start": "1349489",
    "end": "1355489"
  },
  {
    "text": "integration point to generate the Magnusson it scale so a couple of things",
    "start": "1355489",
    "end": "1360649"
  },
  {
    "text": "to add those Baba mentioned the the challenge of particularly for IBM their devices were configured to bring data",
    "start": "1360649",
    "end": "1365749"
  },
  {
    "text": "into their datacenter not take it out took some time and the fortunately even if it's the pipes we had available to us",
    "start": "1365749",
    "end": "1372259"
  },
  {
    "text": "and not large enough to move the data it allowed us to get a portion of the data over which allowed us to construct the",
    "start": "1372259",
    "end": "1379249"
  },
  {
    "text": "tests and be the reports and be working with the business I'm sure many of you",
    "start": "1379249",
    "end": "1384559"
  },
  {
    "text": "immediately saw the more interesting part of this slide buried in there one of the challenges we face is that the",
    "start": "1384559",
    "end": "1391279"
  },
  {
    "text": "data in the IBM world is in episodic format the data in the snow in the s3",
    "start": "1391279",
    "end": "1396619"
  },
  {
    "text": "world is in ASCII so one of our basic opportunities for testing is obviously",
    "start": "1396619",
    "end": "1402679"
  },
  {
    "text": "to say do you have the list of files on both sides and are they the same size and maybe run some check sums so you can",
    "start": "1402679",
    "end": "1408979"
  },
  {
    "text": "demonstrate confidence that you brought the data over in its entirety we couldn't do that because we had to",
    "start": "1408979",
    "end": "1414349"
  },
  {
    "text": "transform the data in a fundamental way during the transfer process which eliminated any byte counts check sums",
    "start": "1414349",
    "end": "1420710"
  },
  {
    "text": "and other basic testing configurations so what we were looking to do was we did",
    "start": "1420710",
    "end": "1426739"
  },
  {
    "text": "the the byte size counts where we could only do it from the IBM Z cloud until",
    "start": "1426739",
    "end": "1433190"
  },
  {
    "text": "the the cluster of three colors in the Middle where we transformed it from episodic to ASCII so we then had to",
    "start": "1433190",
    "end": "1439940"
  },
  {
    "text": "conduct testing to show integrity of the data in terms of record count field",
    "start": "1439940",
    "end": "1445070"
  },
  {
    "text": "counts as we moved from the absolute format into the ASCII format and then to",
    "start": "1445070",
    "end": "1451639"
  },
  {
    "text": "conclude as we'll talk about in the validation we went ahead and took some of our experience in our manufacturing",
    "start": "1451639",
    "end": "1458179"
  },
  {
    "text": "process and we conduct we did actually designed and implemented programs that",
    "start": "1458179",
    "end": "1463429"
  },
  {
    "text": "would extract all data on the eve Zika outside extract the same data from the same locations in",
    "start": "1463429",
    "end": "1470500"
  },
  {
    "text": "the files on the mate on the Amazon side and then we utilize testing protocols to",
    "start": "1470500",
    "end": "1476920"
  },
  {
    "text": "actually confer confirm 100% those fields and records were the same so we",
    "start": "1476920",
    "end": "1482980"
  },
  {
    "text": "had to go into more complex testing methodology than we would normally get",
    "start": "1482980",
    "end": "1488320"
  },
  {
    "text": "into because of this transformation of the data which took out some of our fundamental testing capabilities so we",
    "start": "1488320",
    "end": "1501370"
  },
  {
    "start": "1499000",
    "end": "1562000"
  },
  {
    "text": "used the ANSI standard which is very standard for the many drug manufacturing industry and and some time and very",
    "start": "1501370",
    "end": "1510430"
  },
  {
    "text": "often what we do is when the massive data to be ingested or transformed a foreign transformation program you would",
    "start": "1510430",
    "end": "1518380"
  },
  {
    "text": "say okay I will pick up thousand record at the beginning of the file but that did not that that wasn't sufficient",
    "start": "1518380",
    "end": "1525040"
  },
  {
    "text": "enough for the industry of you know GXP compliant industry so we had to pick",
    "start": "1525040",
    "end": "1531360"
  },
  {
    "text": "records in the beginning of the file end of the file middle of the file and and do the",
    "start": "1531360",
    "end": "1538030"
  },
  {
    "text": "comparison at scale that can say that you know these are the records aware off",
    "start": "1538030",
    "end": "1543730"
  },
  {
    "text": "these are the records were not off during the migration process and that is",
    "start": "1543730",
    "end": "1548890"
  },
  {
    "text": "equal applicable for any modernizing large programs if you are dealing with you know a strict regulation like gxb",
    "start": "1548890",
    "end": "1556540"
  },
  {
    "text": "compliance regulation industry",
    "start": "1556540",
    "end": "1560130"
  },
  {
    "start": "1562000",
    "end": "1671000"
  },
  {
    "text": "we used the amazon has a very net native",
    "start": "1563700",
    "end": "1572990"
  },
  {
    "text": "reporting capability at ena we use the athena first we understood we use the",
    "start": "1572990",
    "end": "1580380"
  },
  {
    "text": "glue to understand the metadata of the structure then we use the athena for reporting and so about 100 and 150 gb",
    "start": "1580380",
    "end": "1590070"
  },
  {
    "text": "data it brings to each queries and generates report in 10 12 15 seconds",
    "start": "1590070",
    "end": "1597750"
  },
  {
    "text": "that was not there earlier experience business used to have on the legacy they",
    "start": "1597750",
    "end": "1603420"
  },
  {
    "text": "have to wait for the program the job to run and and schedule and all that here is a real-time access to your large data",
    "start": "1603420",
    "end": "1610200"
  },
  {
    "text": "set and get the at the volume and they don't have been realized to what volume",
    "start": "1610200",
    "end": "1617520"
  },
  {
    "text": "they are running these reports because immediately the report comes in we have to create in a certain ways this data to",
    "start": "1617520",
    "end": "1627210"
  },
  {
    "text": "be created in the design for the reporting performance so we used something called partitioning even in s3",
    "start": "1627210",
    "end": "1633810"
  },
  {
    "text": "so there is not many people doing it but there are these are the principle that we looked at it you have to have these",
    "start": "1633810",
    "end": "1641130"
  },
  {
    "text": "performance built-in when you are designing it for large-scale dealing here so we parties to participate of",
    "start": "1641130",
    "end": "1650760"
  },
  {
    "text": "data and it still generates report in a 10-15 seconds and the user was so happy",
    "start": "1650760",
    "end": "1657060"
  },
  {
    "text": "about it also we enabled this service bell service calling approach to your",
    "start": "1657060",
    "end": "1662190"
  },
  {
    "text": "athena reports so you can always get the same report through any service calls",
    "start": "1662190",
    "end": "1668570"
  },
  {
    "start": "1671000",
    "end": "1756000"
  },
  {
    "text": "okay so concluding here the some key challenges and learnings that we had as",
    "start": "1672380",
    "end": "1679340"
  },
  {
    "text": "with many assets that have existed for a long period of time the knowledge of the",
    "start": "1679340",
    "end": "1686030"
  },
  {
    "text": "asset had diminished the knowledge of what the structures were how to use it the capabilities so the subject matter",
    "start": "1686030",
    "end": "1693410"
  },
  {
    "text": "experts availability was limited and we had to do a lot of analysis at the beginning of the process in order to",
    "start": "1693410",
    "end": "1699400"
  },
  {
    "text": "really understand what well they asked it was and what our obligations were touching on obligations in a murk",
    "start": "1699400",
    "end": "1705770"
  },
  {
    "text": "working the regulated industry we have our data retention policies so that constricted our ability to just wipe out",
    "start": "1705770",
    "end": "1712400"
  },
  {
    "text": "the data this was something that's still retained under our regulations and we kept we have to design for that life",
    "start": "1712400",
    "end": "1718880"
  },
  {
    "text": "cycle we mentioned the challenges the to cloud services and yes it's fine if you",
    "start": "1718880",
    "end": "1726470"
  },
  {
    "text": "want to move data in a small volume but when we're dealing with the many of terabytes this presented a significant",
    "start": "1726470",
    "end": "1732770"
  },
  {
    "text": "challenges we've touched on and we come back to our selves into a corner with the timeframe this was our final acid on",
    "start": "1732770",
    "end": "1739640"
  },
  {
    "text": "the mainframe if there's a large ant one that took a long time to move and obviously was the mainframe of a very",
    "start": "1739640",
    "end": "1745490"
  },
  {
    "text": "powerful platform but also very costly platform so the clock was running of the meter was running while we were doing",
    "start": "1745490",
    "end": "1751460"
  },
  {
    "text": "this work to try and shut the system down in time on the key learnings that",
    "start": "1751460",
    "end": "1757970"
  },
  {
    "start": "1756000",
    "end": "1959000"
  },
  {
    "text": "we regard from this program implementation at Merck whenever we we",
    "start": "1757970",
    "end": "1765289"
  },
  {
    "text": "deal with a compliance relevant data sets of in a massive scale you have to",
    "start": "1765289",
    "end": "1772760"
  },
  {
    "text": "have early analysis of the data so you can understand what is the raw data",
    "start": "1772760",
    "end": "1778330"
  },
  {
    "text": "redundant obsolete and temporary it's easy to say all the data and business",
    "start": "1778330",
    "end": "1784159"
  },
  {
    "text": "always will say all the data I need until you prove them what is the duplicate and it has to be the model in",
    "start": "1784159",
    "end": "1791059"
  },
  {
    "text": "which you prove them has to be defensible and that is built into your",
    "start": "1791059",
    "end": "1796130"
  },
  {
    "text": "automated data lifecycle management attribute of adaptive data foundation the second your your testing cycle has",
    "start": "1796130",
    "end": "1804169"
  },
  {
    "text": "to be air driven how you drive we were testing and validation at a different level of",
    "start": "1804169",
    "end": "1809540"
  },
  {
    "text": "testing some could be technical testing some could be functional testing some could be non-functional requirement",
    "start": "1809540",
    "end": "1816380"
  },
  {
    "text": "testing and all these things has to be AI best self adjusting to certain",
    "start": "1816380",
    "end": "1823160"
  },
  {
    "text": "benchmark that you establish from the security and compliance protocols",
    "start": "1823160",
    "end": "1828950"
  },
  {
    "text": "whatever solution that you built in how do you how do you ensure the amount of",
    "start": "1828950",
    "end": "1836030"
  },
  {
    "text": "data that you expose and what data to be first and what data to be kept for",
    "start": "1836030",
    "end": "1841970"
  },
  {
    "text": "retention compliance is very important to establish the design frame work at",
    "start": "1841970",
    "end": "1847400"
  },
  {
    "text": "the beginning of your implementation and third the network latency and the",
    "start": "1847400",
    "end": "1854150"
  },
  {
    "text": "bandwidth always will be challenged whenever we deal with a legacy modernization when the legacy",
    "start": "1854150",
    "end": "1860390"
  },
  {
    "text": "modernization LLC five years ago when the G cloud was established at Merck it",
    "start": "1860390",
    "end": "1865790"
  },
  {
    "text": "was a good selling point you don't need a ten table line established with your G",
    "start": "1865790",
    "end": "1871460"
  },
  {
    "text": "cloud you need only 15 Mbps 14 Mbps line that is good enough to get the data from",
    "start": "1871460",
    "end": "1877580"
  },
  {
    "text": "mainframe but when we when we do this kind of use case to get all these data",
    "start": "1877580",
    "end": "1883280"
  },
  {
    "text": "it could be hundred terabyte for another situation it could be in petabyte so the",
    "start": "1883280",
    "end": "1888350"
  },
  {
    "text": "network bandwidth always will be challenged and that shouldn't that shouldn't block in but the scale you can",
    "start": "1888350",
    "end": "1895670"
  },
  {
    "text": "always find how do you achieve it right now we have a lot of mass storage migration devices available and they",
    "start": "1895670",
    "end": "1903380"
  },
  {
    "text": "they are well established certified for our compliance reasons as well and the",
    "start": "1903380",
    "end": "1908600"
  },
  {
    "text": "other point which is very important is even in AWS context compression is",
    "start": "1908600",
    "end": "1916160"
  },
  {
    "text": "available when you an Athena like reports and glue-like tools they their",
    "start": "1916160",
    "end": "1923180"
  },
  {
    "text": "cost their cost is based on the uses the data scan how much amount of scan data",
    "start": "1923180",
    "end": "1929150"
  },
  {
    "text": "that you are scanning we compress the source data so the scanning cost is",
    "start": "1929150",
    "end": "1935240"
  },
  {
    "text": "based on the compressed value volume so even though we let's say we ended up keeping 60 terabyte it was well I think",
    "start": "1935240",
    "end": "1943970"
  },
  {
    "text": "it less than thirty thirty-five terabyte on this storage point of view so s3 cost was optimized Athena cost was optimized",
    "start": "1943970",
    "end": "1952190"
  },
  {
    "text": "blue cost was optimized as well and these features are available in AWS so",
    "start": "1952190",
    "end": "1960380"
  },
  {
    "start": "1959000",
    "end": "2014000"
  },
  {
    "text": "fondly the business benefits clearly we achieved a reduced operating cost we did",
    "start": "1960380",
    "end": "1965870"
  },
  {
    "text": "meet our a time line fortunately he was tight by the end but we we got there as",
    "start": "1965870",
    "end": "1971840"
  },
  {
    "text": "Bubba mentioned the business community experienced significant increases in their performance and the date of access",
    "start": "1971840",
    "end": "1977960"
  },
  {
    "text": "capabilities we retained our current security models so obviously implemented",
    "start": "1977960",
    "end": "1984650"
  },
  {
    "text": "in a different manner but the same user laws and groups who were able to replicate across the two environments",
    "start": "1984650",
    "end": "1990590"
  },
  {
    "text": "and meet with therefore our compliance obligations and the Athena came with far",
    "start": "1990590",
    "end": "1998480"
  },
  {
    "text": "superior business analytic capabilities and putting power into the hands of the",
    "start": "1998480",
    "end": "2003520"
  },
  {
    "text": "business to access the data rather than relying on the development of specific",
    "start": "2003520",
    "end": "2008590"
  },
  {
    "text": "reports from the IT community to",
    "start": "2008590",
    "end": "2015910"
  },
  {
    "start": "2014000",
    "end": "2131000"
  },
  {
    "text": "summarize to wrap up this session of course adaptive data foundation provides",
    "start": "2015910",
    "end": "2021310"
  },
  {
    "text": "you tons of workbenches toolkits frameworks but let me put these",
    "start": "2021310",
    "end": "2029410"
  },
  {
    "text": "questions to you for your modernization program before I embark on it three",
    "start": "2029410",
    "end": "2035380"
  },
  {
    "text": "fundamental key questions you should ask yourself these are must ask and must",
    "start": "2035380",
    "end": "2041530"
  },
  {
    "text": "answer questions for your transformation programs is your data foundation that",
    "start": "2041530",
    "end": "2048730"
  },
  {
    "text": "you you you're trying to establish is it a deductible is due are you clear on",
    "start": "2048730",
    "end": "2058090"
  },
  {
    "text": "what business outcome you want to deliver that's the first question that",
    "start": "2058090",
    "end": "2064360"
  },
  {
    "text": "you should ask are you clear what business outcome you want to deliver",
    "start": "2064360",
    "end": "2070000"
  },
  {
    "text": "say you are clear on that is your data intelligent enough an adaptive Data",
    "start": "2070000",
    "end": "2077740"
  },
  {
    "text": "Foundation has a has a has a tool that",
    "start": "2077740",
    "end": "2082960"
  },
  {
    "text": "tells you what is the IQ of your data and third how many business processes in",
    "start": "2082960",
    "end": "2091330"
  },
  {
    "text": "your enterprise in your organic organization are truly using embedded",
    "start": "2091330",
    "end": "2096760"
  },
  {
    "text": "inside and you many of these data like program in last let's say five years are",
    "start": "2096760",
    "end": "2104940"
  },
  {
    "text": "are falling short of the expectation the reason is missing purpose you implement",
    "start": "2104940",
    "end": "2114760"
  },
  {
    "text": "it and then so what what is next the purpose is missing so you got to ask",
    "start": "2114760",
    "end": "2121360"
  },
  {
    "text": "these three questions and answer them very clearly what do you want to achieve",
    "start": "2121360",
    "end": "2127180"
  },
  {
    "text": "it yep that concludes what we wanted to",
    "start": "2127180",
    "end": "2133840"
  },
  {
    "text": "cover for any questions we're happy to take them [Applause]",
    "start": "2133840",
    "end": "2143270"
  },
  {
    "text": "so the question I think is it did we consider anything other than snowball from ingesting the data into s3 into s3",
    "start": "2151780",
    "end": "2158780"
  },
  {
    "text": "or into another platform so we did look",
    "start": "2158780",
    "end": "2164119"
  },
  {
    "text": "at other methodologies we looked at here the ability to move the data through",
    "start": "2164119",
    "end": "2169220"
  },
  {
    "text": "different high speed pipes but ultimately in order to load the data",
    "start": "2169220",
    "end": "2174680"
  },
  {
    "text": "into snowball we need to have a piece of hardware that Amazon is going to allow",
    "start": "2174680",
    "end": "2180320"
  },
  {
    "text": "into its data centers and so the snowball was the one that was authorized so that became the de facto solution in",
    "start": "2180320",
    "end": "2186020"
  },
  {
    "text": "this case and the question became how do we move the data on as a snowball as quickly as possible",
    "start": "2186020",
    "end": "2191349"
  },
  {
    "text": "that answers a question even just to add to what Colin mentioned even we said you",
    "start": "2191349",
    "end": "2198320"
  },
  {
    "text": "have a mainframe and g-cloud that is where the data was can we have a file server connected to your mainframe so we",
    "start": "2198320",
    "end": "2205130"
  },
  {
    "text": "can dump the data and bring it over but this understanding this these things all",
    "start": "2205130",
    "end": "2211400"
  },
  {
    "text": "built in for it from economical standpoint there is no these mainframe racks does not have sockets to connect a",
    "start": "2211400",
    "end": "2219200"
  },
  {
    "text": "distributed midrange server so the answer was no so you cannot connect any",
    "start": "2219200",
    "end": "2226010"
  },
  {
    "text": "other mid-range server to a mainframe rack so the data center doesn't even allow it to do it and again it's what",
    "start": "2226010",
    "end": "2232970"
  },
  {
    "text": "would IBM not only what can be connected but what does IBM allow into its data center for the security and protection",
    "start": "2232970",
    "end": "2240440"
  },
  {
    "text": "reasons if we missed anything I'd be fascinated to know any ideas you have",
    "start": "2240440",
    "end": "2246710"
  },
  {
    "text": "but we had to deal with what we had in front of us yes Krantz oft is a third",
    "start": "2246710",
    "end": "2256640"
  },
  {
    "text": "party software back-office associates produces it we've been using Krantz soft",
    "start": "2256640",
    "end": "2262550"
  },
  {
    "text": "as a data movement and term and validation tool as part of RSA P",
    "start": "2262550",
    "end": "2269510"
  },
  {
    "text": "implementation for 10 years 10 15 years now 10 years getting ahead of myself",
    "start": "2269510",
    "end": "2275540"
  },
  {
    "text": "and so when we were looking for a piece of software that could compare files and",
    "start": "2275540",
    "end": "2282200"
  },
  {
    "text": "in this case the sample files were extracted from the tooth we talked with our testing team and they recommended the cran soft utility and",
    "start": "2282200",
    "end": "2289369"
  },
  {
    "text": "that worked very well cran soft could have compared the entire dataset side-by-side and done 100 strain",
    "start": "2289369",
    "end": "2296329"
  },
  {
    "text": "comparison of the entire files but then you get into the problem of how do you move those entire datasets to the",
    "start": "2296329",
    "end": "2301400"
  },
  {
    "text": "testing software so we just settled for moving the sample files so I'm not a",
    "start": "2301400",
    "end": "2316220"
  },
  {
    "text": "network guy by any means my understanding is that we were only",
    "start": "2316220",
    "end": "2322369"
  },
  {
    "text": "achieving 20 to 25 percent of the theoretical capability of our bandwidth",
    "start": "2322369",
    "end": "2328040"
  },
  {
    "text": "between the IBM data center and the merck data center so even though we",
    "start": "2328040",
    "end": "2333050"
  },
  {
    "text": "could have forecast perhaps we could have moved the data in two weeks to",
    "start": "2333050",
    "end": "2338210"
  },
  {
    "text": "three weeks and everything been perfect when we were encountering the actual performance we were seeing a far longer",
    "start": "2338210",
    "end": "2344390"
  },
  {
    "text": "an extended period of time to move the data I don't know Bubba whether you can add anything at that no I didn't you",
    "start": "2344390",
    "end": "2352568"
  },
  {
    "text": "well with the Iovine Network has a very large connection into a double us that",
    "start": "2362430",
    "end": "2369730"
  },
  {
    "text": "we were relatively confident that we could move the data but even with the",
    "start": "2369730",
    "end": "2375760"
  },
  {
    "text": "vote even with a large bandwidth to move a hundred terabytes of data once we had the availability of a snowball it was",
    "start": "2375760",
    "end": "2382750"
  },
  {
    "text": "still quicker to actually just load it onto the snowball and ship it plus we didn't want to get in a situation where",
    "start": "2382750",
    "end": "2388420"
  },
  {
    "text": "we were chewing up the bandwidth and causing any performance issues for other Merck services going up against AWS so",
    "start": "2388420",
    "end": "2396460"
  },
  {
    "text": "the primary concern was in the connection from the IBM mainframe into the merck data center it was fit for",
    "start": "2396460",
    "end": "2402940"
  },
  {
    "text": "purpose but we were trying to use it for a different purpose at this point",
    "start": "2402940",
    "end": "2408060"
  },
  {
    "text": "okay thank you very much thank you we have one more question I think from the",
    "start": "2413580",
    "end": "2436980"
  },
  {
    "text": "key elements for me that allowed us to accelerate this cycle was quickly",
    "start": "2436980",
    "end": "2442980"
  },
  {
    "text": "getting to a point where we understood what those legacy assets were not just",
    "start": "2442980",
    "end": "2451140"
  },
  {
    "text": "they're a collection of files for what were their purposes so that we could instill confidence in our business",
    "start": "2451140",
    "end": "2457140"
  },
  {
    "text": "counterparts that the reports that we were designing were fit for purpose",
    "start": "2457140",
    "end": "2462680"
  },
  {
    "text": "ensuring we had a robust testing protocol that again gave the business confidence that we could show that yes",
    "start": "2462680",
    "end": "2469710"
  },
  {
    "text": "we've not looked at every bit in this in every field but we've done sufficient",
    "start": "2469710",
    "end": "2475140"
  },
  {
    "text": "sample testing to ensure that we've delivered the solution and ensuring that",
    "start": "2475140",
    "end": "2485040"
  },
  {
    "text": "you've got a clear movement of data this was something that caught us by surprise time and time again as to how",
    "start": "2485040",
    "end": "2490140"
  },
  {
    "text": "complicated it became to actually ship data I'm just used to plug it in and move the data this was just in the",
    "start": "2490140",
    "end": "2496830"
  },
  {
    "text": "volume of data and the legacy connections became a challenge for us",
    "start": "2496830",
    "end": "2503090"
  },
  {
    "text": "just to add what one other item that comes my mind is whenever you have an",
    "start": "2503090",
    "end": "2509160"
  },
  {
    "text": "established network with a cloud partner lot of other third party dependency",
    "start": "2509160",
    "end": "2516030"
  },
  {
    "text": "comes into play example a TNT contract could be there with a cloud partner so you might say hey I'm done with my job",
    "start": "2516030",
    "end": "2523590"
  },
  {
    "text": "next month I'm off the contract no there is a dependency clause that that we",
    "start": "2523590",
    "end": "2531270"
  },
  {
    "text": "they're with their service provider with the cloud is built into your you know a",
    "start": "2531270",
    "end": "2536820"
  },
  {
    "text": "heads up that SLA that you have to give it to them so when you program even your time line your program you have to keep",
    "start": "2536820",
    "end": "2543570"
  },
  {
    "text": "in mind what call what are the other peripheral contract this cloud party has it accordingly that",
    "start": "2543570",
    "end": "2549700"
  },
  {
    "text": "established a time frame for you so the",
    "start": "2549700",
    "end": "2569680"
  },
  {
    "text": "question is how else are we using this data for other business intelligence",
    "start": "2569680",
    "end": "2574960"
  },
  {
    "text": "purposes at this point in time the the data was not being actively used for",
    "start": "2574960",
    "end": "2583420"
  },
  {
    "text": "other purposes and I think for the most part we have equivalent or even",
    "start": "2583420",
    "end": "2589390"
  },
  {
    "text": "potentially better data sources available to us through other protocols but we have a data retention policy we'd",
    "start": "2589390",
    "end": "2598029"
  },
  {
    "text": "made business decisions related to this that we may potentially have to explain",
    "start": "2598029",
    "end": "2603130"
  },
  {
    "text": "to a journal or authorities and so we need to ensure that we could retain this data set even if the value of it perhaps",
    "start": "2603130",
    "end": "2610210"
  },
  {
    "text": "is not what it was when we purchased it it was more merck has a retention",
    "start": "2610210",
    "end": "2615609"
  },
  {
    "text": "policies and we always meet those policies so sometimes we do things for whether there may not be a huge amount",
    "start": "2615609",
    "end": "2621489"
  },
  {
    "text": "of value but with we're obligated to do so okay well thank you everyone thank",
    "start": "2621489",
    "end": "2632200"
  },
  {
    "text": "you",
    "start": "2632200",
    "end": "2634349"
  }
]