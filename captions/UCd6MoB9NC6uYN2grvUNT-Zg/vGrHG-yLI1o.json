[
  {
    "start": "0",
    "end": "43000"
  },
  {
    "text": "so good afternoon and welcome everyone and I am young Strika I'm faculty at UC",
    "start": "89",
    "end": "7379"
  },
  {
    "text": "Berkeley and I am also a co-founder and executive chairman at data breeze and I",
    "start": "7379",
    "end": "15109"
  },
  {
    "text": "am today together with Rahul Bhatia from Amazon AWS we are going to talk about",
    "start": "15109",
    "end": "23340"
  },
  {
    "text": "two things first we are going to give you a short update on Apache spark with",
    "start": "23340",
    "end": "29189"
  },
  {
    "text": "a focus on Apache 2-0 which was released earlier this year and then we are going",
    "start": "29189",
    "end": "35070"
  },
  {
    "text": "to talk about some best practices about how to run a Apache spark in AWS so let",
    "start": "35070",
    "end": "43140"
  },
  {
    "start": "43000",
    "end": "43000"
  },
  {
    "text": "me start with a short history Apache started as a project as a research project back in 2009 and the when we",
    "start": "43140",
    "end": "53820"
  },
  {
    "text": "started Apache it's actually you are also using Amazonia we are some of the early users of AWS and not surprisingly",
    "start": "53820",
    "end": "62760"
  },
  {
    "text": "a lot of development and testing happened on ec2 from day one so you",
    "start": "62760",
    "end": "69450"
  },
  {
    "text": "could say that Apache spark was kind of built from for the cloud from a beyond",
    "start": "69450",
    "end": "76310"
  },
  {
    "text": "it becomes open-source in 2010 and then it becomes a top Apache project in 2013",
    "start": "76310",
    "end": "83820"
  },
  {
    "text": "and today is a most active big data project and it is growing very fast and",
    "start": "83820",
    "end": "95040"
  },
  {
    "start": "90000",
    "end": "90000"
  },
  {
    "text": "actually by some matrix or most of the metrics the growth is accelerating so it",
    "start": "95040",
    "end": "102150"
  },
  {
    "text": "is an hour like slowing down so here are some numbers the number of contributors",
    "start": "102150",
    "end": "107820"
  },
  {
    "text": "have increased has increased by almost 50 percent since last year the number of",
    "start": "107820",
    "end": "113280"
  },
  {
    "text": "submit attendees we have we run three spark summits one in on the East Coast",
    "start": "113280",
    "end": "120090"
  },
  {
    "text": "on the west coast and one in Europe this year was over 5,000 people and probably",
    "start": "120090",
    "end": "126750"
  },
  {
    "text": "the most impressive figure is a number of meetup members across the world",
    "start": "126750",
    "end": "132380"
  },
  {
    "text": "it from last year increased by you know",
    "start": "132380",
    "end": "137420"
  },
  {
    "text": "over four times and actually that number if you look now probably sober to 280",
    "start": "137420",
    "end": "143720"
  },
  {
    "text": "thousand is growing very fast every day all says you know and many of you here",
    "start": "143720",
    "end": "149750"
  },
  {
    "start": "148000",
    "end": "148000"
  },
  {
    "text": "Apache spark now has a tremendous moment in industry it's running in clouds of",
    "start": "149750",
    "end": "158810"
  },
  {
    "text": "course starting with Amazon EMR and many companies provided Bose as a service",
    "start": "158810",
    "end": "166460"
  },
  {
    "text": "data bricks provide an end-to-end enterprise grade a solution on top of",
    "start": "166460",
    "end": "173060"
  },
  {
    "text": "spark then every a big data band or",
    "start": "173060",
    "end": "179110"
  },
  {
    "text": "today it's actually includes apache spark in their distribution the one",
    "start": "179110",
    "end": "186770"
  },
  {
    "start": "185000",
    "end": "185000"
  },
  {
    "text": "thing you know i think that is pretty well known by this time but i just",
    "start": "186770",
    "end": "192500"
  },
  {
    "text": "wanted one to make sure is that apache spark was on released it was built from",
    "start": "192500",
    "end": "200090"
  },
  {
    "text": "the ground up to take advantage of the increasing memory on you have on the",
    "start": "200090",
    "end": "205550"
  },
  {
    "text": "servers so people understood very very you know from early on that Apache is",
    "start": "205550",
    "end": "211220"
  },
  {
    "text": "fast and is fast because it's using in memory using memory for comp or storage",
    "start": "211220",
    "end": "219380"
  },
  {
    "text": "and computation however Apache is not all as far as fast",
    "start": "219380",
    "end": "224959"
  },
  {
    "text": "when you have and you can push the data in memory it's also fast when the data",
    "start": "224959",
    "end": "230600"
  },
  {
    "text": "it's on the disk or on SSD and this is",
    "start": "230600",
    "end": "235910"
  },
  {
    "text": "from actually 2014 Apache warns grace",
    "start": "235910",
    "end": "241820"
  },
  {
    "text": "sort benchmark the best numbers and to give you a sense the past year record",
    "start": "241820",
    "end": "249920"
  },
  {
    "text": "was hauled by a cluster from Yahoo",
    "start": "249920",
    "end": "255470"
  },
  {
    "text": "Hadoop cluster and this was sort sort thing like this one terabyte data",
    "start": "255470",
    "end": "264300"
  },
  {
    "text": "using 2000 over 2000 servers and it took 72 minutes in 2014",
    "start": "264300",
    "end": "270060"
  },
  {
    "text": "spark you on Amazon on the ec2 took three times like three times less time",
    "start": "270060",
    "end": "278580"
  },
  {
    "text": "and on one tenth of the machines right of the number of machines and in this",
    "start": "278580",
    "end": "285000"
  },
  {
    "text": "case a bottleneck was the communication as a communication network it's actually",
    "start": "285000",
    "end": "290639"
  },
  {
    "text": "we could push the network to ten gigabits per second per host that's the",
    "start": "290639",
    "end": "297090"
  },
  {
    "text": "limitations so the less you know the takeaway here is that if your spark is",
    "start": "297090",
    "end": "302550"
  },
  {
    "text": "not only good for on your data if it's in memory it's also very good at any",
    "start": "302550",
    "end": "307770"
  },
  {
    "text": "scales and when the data it's stored out of memory so next I'd like to talk about",
    "start": "307770",
    "end": "317250"
  },
  {
    "text": "Apache spark 2-0 so Apache spark 2-0 it was released earlier this year and it's",
    "start": "317250",
    "end": "324389"
  },
  {
    "text": "actually incorporates the biggest changes in spark since we released it in",
    "start": "324389",
    "end": "331880"
  },
  {
    "text": "2009 and the the features for in 2-0 can",
    "start": "331880",
    "end": "340770"
  },
  {
    "text": "be grouped in three categories the first one has to has to deal with the api's",
    "start": "340770",
    "end": "346949"
  },
  {
    "start": "341000",
    "end": "341000"
  },
  {
    "text": "and when Apache 2-0 first of all we have a much more mature is sequel so actually",
    "start": "346949",
    "end": "354449"
  },
  {
    "text": "we are pretty much follows a sequel 2003",
    "start": "354449",
    "end": "361889"
  },
  {
    "text": "standard and the other major thing is that we raise it we are raising the",
    "start": "361889",
    "end": "369000"
  },
  {
    "text": "level of abstractions from rdd's to data frames so as you all of you know rdd's",
    "start": "369000",
    "end": "375690"
  },
  {
    "text": "are was a main data abstraction for in spark rd d stands for resilient",
    "start": "375690",
    "end": "384029"
  },
  {
    "text": "distributed data set and is basically a collection of tuples which is distributed across the machines and",
    "start": "384029",
    "end": "390900"
  },
  {
    "text": "estoppel has a key and a value the value has no semantics for Sparky just a block",
    "start": "390900",
    "end": "396089"
  },
  {
    "text": "okay and data frame in turn it's it's",
    "start": "396089",
    "end": "404530"
  },
  {
    "start": "404000",
    "end": "404000"
  },
  {
    "text": "actually it's think about it as a table right so it's not only a key and the",
    "start": "404530",
    "end": "410170"
  },
  {
    "text": "value is basically a bunch of columns and each column has a data type okay so",
    "start": "410170",
    "end": "415600"
  },
  {
    "text": "you have a lot of richer semantics the upper operators on data data frames are",
    "start": "415600",
    "end": "424000"
  },
  {
    "text": "mostly relational operators think about sequel kind of operators with a heavy",
    "start": "424000",
    "end": "430840"
  },
  {
    "text": "emphasis on statistical analysis like quantile standard deviation skewers the",
    "start": "430840",
    "end": "436570"
  },
  {
    "text": "distribution you can compute just with a simple comment the data frames was",
    "start": "436570",
    "end": "441970"
  },
  {
    "text": "proper or popularized by r and python with the pandas communities and their",
    "start": "441970",
    "end": "448270"
  },
  {
    "text": "large e they are today and they be used largely by data scientists okay so the",
    "start": "448270",
    "end": "455980"
  },
  {
    "text": "it's and what data frames again I tell",
    "start": "455980",
    "end": "461350"
  },
  {
    "text": "you it so it's a table has more semantics columns types for each columns",
    "start": "461350",
    "end": "467530"
  },
  {
    "text": "and so forth but what does this value so it buys you two things first it makes it much easier",
    "start": "467530",
    "end": "475000"
  },
  {
    "text": "to write programs right it's a higher level API and to demonstrate this point",
    "start": "475000",
    "end": "481060"
  },
  {
    "text": "here is an example this is using rdd's it's a snippet of code and I'm going to",
    "start": "481060",
    "end": "488740"
  },
  {
    "text": "ask you anyone knows what this code is doing okay don't feel bad you know is",
    "start": "488740",
    "end": "495460"
  },
  {
    "text": "even you know as part people real pro you know it's heavy duty programmers may",
    "start": "495460",
    "end": "502240"
  },
  {
    "text": "have difficulty getting what is this what this is doing and what is actually",
    "start": "502240",
    "end": "508300"
  },
  {
    "text": "is doing I can show you next and this is the next piece of the next line of code is the same functionality implemented",
    "start": "508300",
    "end": "516190"
  },
  {
    "text": "using data frames and here it is now I hope that almost everyone can understand",
    "start": "516190",
    "end": "523030"
  },
  {
    "text": "what this is doing right it's averaging the age by Department in an organization",
    "start": "523030",
    "end": "530100"
  },
  {
    "text": "okay so again beta frames has more semantics and because of that it makes",
    "start": "530100",
    "end": "538380"
  },
  {
    "text": "it much easier to write programs okay",
    "start": "538380",
    "end": "543500"
  },
  {
    "text": "this is a first advantage the second advantage of data frames is because you",
    "start": "543500",
    "end": "549329"
  },
  {
    "start": "546000",
    "end": "546000"
  },
  {
    "text": "can use these semantics to better optimize the algorithms in particular",
    "start": "549329",
    "end": "555810"
  },
  {
    "text": "right now if you're if you if you know let me just take a step back you know that on top of spark spark is an",
    "start": "555810",
    "end": "562500"
  },
  {
    "text": "ecosystem you have spark or and you have a bunch of libraries on top of it to",
    "start": "562500",
    "end": "568500"
  },
  {
    "text": "support different workloads one was is to support sequel workloads and this",
    "start": "568500",
    "end": "573600"
  },
  {
    "text": "Park sequel spark sequel has a lot has an optimizer called catalyst and now is",
    "start": "573600",
    "end": "582029"
  },
  {
    "text": "data frames which again is our tables like in cycles and I told you that data",
    "start": "582029",
    "end": "587160"
  },
  {
    "text": "frames operators are mostly relational operators what we do what we can do is",
    "start": "587160",
    "end": "593639"
  },
  {
    "text": "that we have all the data frames operators using the same logical plan",
    "start": "593639",
    "end": "598920"
  },
  {
    "text": "like sequel so you unify the data frames and sequel and",
    "start": "598920",
    "end": "604050"
  },
  {
    "text": "they share the same query optimizer in the same execution engine data frames are also tightly integrated with the",
    "start": "604050",
    "end": "610800"
  },
  {
    "text": "rest of SPARC machine learning libraries takes data frame frames as both inputs",
    "start": "610800",
    "end": "616439"
  },
  {
    "text": "and the outputs and still you can still if you really want to use rdd's it's",
    "start": "616439",
    "end": "621930"
  },
  {
    "text": "very easy to go between our DDS and data frames okay so why do you care about",
    "start": "621930",
    "end": "627540"
  },
  {
    "text": "this you care about this because every optimization you can do this logical",
    "start": "627540",
    "end": "633089"
  },
  {
    "text": "plan or the execution is going to be inherited and we're going to be available to everything and all the",
    "start": "633089",
    "end": "639720"
  },
  {
    "text": "programs you write on top of SPARC using data frames oh okay and to make this",
    "start": "639720",
    "end": "646319"
  },
  {
    "text": "point here is a benchmark these two",
    "start": "646319",
    "end": "651389"
  },
  {
    "start": "648000",
    "end": "648000"
  },
  {
    "text": "lines represent it's an aggregation you aggregate a bunch of numbers this is not",
    "start": "651389",
    "end": "657149"
  },
  {
    "text": "exactly important through our numbers ourselves and here it's the implementation of this",
    "start": "657149",
    "end": "663180"
  },
  {
    "text": "patience for Python and Scala using rdd's so it's before dataframes okay",
    "start": "663180",
    "end": "669630"
  },
  {
    "text": "and the time it's on the x-axis so what",
    "start": "669630",
    "end": "674970"
  },
  {
    "text": "you can see you have a difference in time so it's logically the same operation the reason is because you have",
    "start": "674970",
    "end": "681480"
  },
  {
    "text": "different implementations and python it's in this in this case you know it's",
    "start": "681480",
    "end": "688230"
  },
  {
    "text": "a little bit less efficient it shouldn't be a surprise because as you know SPARC",
    "start": "688230",
    "end": "693480"
  },
  {
    "text": "is the was developed in Scala so it's : a native right now here what happens",
    "start": "693480",
    "end": "701930"
  },
  {
    "text": "after using data frames so there are two things to notice here first whether you",
    "start": "701930",
    "end": "710250"
  },
  {
    "text": "view your do aggregation using data frame in Scala Python R or sequel it takes the same amount of time why it's",
    "start": "710250",
    "end": "718230"
  },
  {
    "text": "again because underneath now all of them they share the same logical plan and the",
    "start": "718230",
    "end": "724080"
  },
  {
    "text": "execution engine okay the other thing is that it's better right and this is",
    "start": "724080",
    "end": "730800"
  },
  {
    "text": "because we made a bunch of optimizations optimization which are enabled by the",
    "start": "730800",
    "end": "736500"
  },
  {
    "text": "data frames so I'm going to talk a little bit about that next so the second",
    "start": "736500",
    "end": "742380"
  },
  {
    "text": "set of features have to deal with improving the performance and internally the project which was doing this that we",
    "start": "742380",
    "end": "751490"
  },
  {
    "text": "push the data bridge was called tungsten and at the end we got improvements in",
    "start": "751490",
    "end": "759630"
  },
  {
    "text": "the speed between five and 20 X so what is tungsten doing on one hand is doing",
    "start": "759630",
    "end": "767160"
  },
  {
    "text": "these typical database optimizations it's again over data frames and sequel",
    "start": "767160",
    "end": "772490"
  },
  {
    "text": "like John reordering pushdown predicates and things like that it's improving on",
    "start": "772490",
    "end": "780120"
  },
  {
    "text": "the data representation you can use compact binary representation I'll talk a little more about that next",
    "start": "780120",
    "end": "786500"
  },
  {
    "text": "and it's also doing more sophisticated optimization like holes code generations",
    "start": "786500",
    "end": "793610"
  },
  {
    "text": "remove expensive iterate or call fuse across multiple operator so as a high-level what this means is that we",
    "start": "793610",
    "end": "800310"
  },
  {
    "text": "have people operators in a queries like filtering or things like that or aggregation instead of every operator",
    "start": "800310",
    "end": "807690"
  },
  {
    "text": "going through the entire table record by record try scanning the record the scanning the entire table you fools fuse",
    "start": "807690",
    "end": "815220"
  },
  {
    "text": "operators and you have multiple operators going you know through a",
    "start": "815220",
    "end": "821220"
  },
  {
    "text": "record right okay so instead of operator",
    "start": "821220",
    "end": "826380"
  },
  {
    "text": "one going through the table sends an operator to go into the table you have operating one and operating two going",
    "start": "826380",
    "end": "832050"
  },
  {
    "text": "through a record and then the next record Co path it goes through the table only once and this has significant",
    "start": "832050",
    "end": "838980"
  },
  {
    "text": "benefits like for instance of course you have fewer scans and also it has much",
    "start": "838980",
    "end": "845250"
  },
  {
    "text": "better cash flow colleague okay so this is and the other thing what what data",
    "start": "845250",
    "end": "856200"
  },
  {
    "start": "848000",
    "end": "848000"
  },
  {
    "text": "frames enables it enables us to be much smarter and more efficient about",
    "start": "856200",
    "end": "862860"
  },
  {
    "text": "managing the memory is a memory so stepping back with rdd's typically when",
    "start": "862860",
    "end": "871020"
  },
  {
    "text": "you load the RTD simha more you busier Eliza okay and you create Java objects",
    "start": "871020",
    "end": "877320"
  },
  {
    "text": "and I'll give you an example next but you are going to end with a huge number",
    "start": "877320",
    "end": "882720"
  },
  {
    "text": "of Java objects which will take a lot of RAM memory actually the memory overhead",
    "start": "882720",
    "end": "889560"
  },
  {
    "text": "is four eight times more than the serialize format so if you have one gigabyte on disk you load it in memory",
    "start": "889560",
    "end": "896520"
  },
  {
    "text": "you'll be sterilized it you are going to end up with four to eight gigabytes and if you have a lot of objects then you",
    "start": "896520",
    "end": "904860"
  },
  {
    "text": "have the garbage collection problem right because park it's using JVM right",
    "start": "904860",
    "end": "912110"
  },
  {
    "text": "and it has also a little memory memory locality and cache locality because you",
    "start": "912110",
    "end": "917160"
  },
  {
    "text": "have to jump over to get the data across all these pointers with data frames",
    "start": "917160",
    "end": "923339"
  },
  {
    "text": "because we have more sama acid as a semantics about syntax we know the",
    "start": "923339",
    "end": "929070"
  },
  {
    "text": "syntax of the data we know their format we can actually keep the binary format",
    "start": "929070",
    "end": "936330"
  },
  {
    "text": "in memory so it can keep entire table in memory it's one big object so much much",
    "start": "936330",
    "end": "943830"
  },
  {
    "text": "your object so we don't have the GC problem garbage collection problem much lower memory overhead almost no memory",
    "start": "943830",
    "end": "950610"
  },
  {
    "text": "overhead and much better cash flow Khaled so so here is to drive home the",
    "start": "950610",
    "end": "956880"
  },
  {
    "start": "956000",
    "end": "956000"
  },
  {
    "text": "point this is how it's again sharing if you like our DD it's a key value and",
    "start": "956880",
    "end": "963000"
  },
  {
    "text": "when you load it this kind of something what you get in memory this is a hash map and you see you have a key value",
    "start": "963000",
    "end": "970860"
  },
  {
    "text": "this is your entry and this is what Java is doing is keeping a pointer for the",
    "start": "970860",
    "end": "977070"
  },
  {
    "text": "back key a pointer for the value and a pointer to the next entry we have three pointer there so actually if your key",
    "start": "977070",
    "end": "984480"
  },
  {
    "text": "and values are integer or something like that you have more pointer than actually useful data and when you scan or",
    "start": "984480",
    "end": "992100"
  },
  {
    "text": "whatever you need to jump to get all this value here some values through the moment through the memory and here with",
    "start": "992100",
    "end": "1000650"
  },
  {
    "text": "what you can do is data frames and like how I can manage a memory so is data frames we just maintain all the table",
    "start": "1000650",
    "end": "1009500"
  },
  {
    "text": "right all the data frame in one big memory block ok and because we know the",
    "start": "1009500",
    "end": "1018500"
  },
  {
    "text": "value what each of them represent and what is their format we know so if we",
    "start": "1018500",
    "end": "1026449"
  },
  {
    "text": "need to grab two treats for instance the tensor out from the data frames we know",
    "start": "1026449",
    "end": "1032839"
  },
  {
    "text": "exactly how to get that right it will be an offset from the beginning of this",
    "start": "1032839",
    "end": "1039020"
  },
  {
    "text": "page memory page which contains the data frame okay",
    "start": "1039020",
    "end": "1046929"
  },
  {
    "text": "so how good and are these optimizations here it's TP CDF as you know it's a",
    "start": "1050940",
    "end": "1060910"
  },
  {
    "text": "standard benchmark for big data for sequel workloads and with yellow it's",
    "start": "1060910",
    "end": "1068730"
  },
  {
    "text": "the time for spark 1.6 it was the last version before 2-0 and with blue it's",
    "start": "1068730",
    "end": "1075460"
  },
  {
    "text": "2-0 so you can see if this is all these queries the performance it's increasing",
    "start": "1075460",
    "end": "1083289"
  },
  {
    "text": "by a factor or the latency is decreasing by a factor of 10 in some cases ok so",
    "start": "1083289",
    "end": "1090549"
  },
  {
    "text": "it's a pretty massive performance improvement the last category of",
    "start": "1090549",
    "end": "1095799"
  },
  {
    "text": "features in 2-0 its deals with",
    "start": "1095799",
    "end": "1100860"
  },
  {
    "text": "structured streaming so they are related with streaming in a nutshell structure",
    "start": "1100860",
    "end": "1110830"
  },
  {
    "start": "1107000",
    "end": "1107000"
  },
  {
    "text": "streaming it's also taking these data frames and provide you a data frame",
    "start": "1110830",
    "end": "1119650"
  },
  {
    "text": "based streaming API so it's very similar when you write a streaming application",
    "start": "1119650",
    "end": "1126580"
  },
  {
    "text": "or you write a batch application in SPARC today which 2-0 the code will look",
    "start": "1126580",
    "end": "1132460"
  },
  {
    "text": "pretty much the same why they intuition",
    "start": "1132460",
    "end": "1137500"
  },
  {
    "text": "what is the intuition behind that well because if you think about the main difference at the high level okay there",
    "start": "1137500",
    "end": "1145000"
  },
  {
    "text": "are more differences but as a high level the difference between something like a badge application and the streaming",
    "start": "1145000",
    "end": "1151750"
  },
  {
    "text": "applications like in the case of the streaming application have infinite tables because that new logs are new",
    "start": "1151750",
    "end": "1158530"
  },
  {
    "text": "rows which are continuously added to the existing tables there's a difference so",
    "start": "1158530",
    "end": "1163780"
  },
  {
    "text": "once you can export that abstractions now the operators are very similar and",
    "start": "1163780",
    "end": "1171700"
  },
  {
    "text": "this allows you to do a lot of cool things also they are enabled because all",
    "start": "1171700",
    "end": "1176950"
  },
  {
    "text": "of them again they share the same logical plan and equation plan so let's see what you can",
    "start": "1176950",
    "end": "1185770"
  },
  {
    "text": "do first let me start with what typical you do is a streaming systems so here",
    "start": "1185770",
    "end": "1191800"
  },
  {
    "text": "how a streaming pipeline looks like you get something in Kafka or kinases you do",
    "start": "1191800",
    "end": "1198190"
  },
  {
    "text": "some ETL and then you store that your logs in the database if you have an IOT",
    "start": "1198190",
    "end": "1204040"
  },
  {
    "text": "application this is kind of what you do or you do some reporting or you do some",
    "start": "1204040",
    "end": "1212080"
  },
  {
    "text": "dashboards so you look at the data or you can be consumed by some applications",
    "start": "1212080",
    "end": "1218050"
  },
  {
    "text": "maybe to trigger on some thing is greater than a threshold but what you",
    "start": "1218050",
    "end": "1223210"
  },
  {
    "text": "can do is Park which is difficult to do otherwise which is unique in SPARC it's",
    "start": "1223210",
    "end": "1229750"
  },
  {
    "text": "because because all these workloads are integrated and again the user same execution engine the same data formats",
    "start": "1229750",
    "end": "1238110"
  },
  {
    "text": "you can do for instance ad-hoc queries on the stream data so think about the",
    "start": "1239310",
    "end": "1246400"
  },
  {
    "text": "following scenario you get the data you get an alert on your data you monitor a set of logs from your clusters or IOT",
    "start": "1246400",
    "end": "1253300"
  },
  {
    "text": "devices then on that alert you can immediately go to go and do ad hoc",
    "start": "1253300",
    "end": "1259600"
  },
  {
    "text": "queries to figure out what's happening right all the data is it's streamed all this",
    "start": "1259600",
    "end": "1271630"
  },
  {
    "text": "is you hear more and more about this you can do machine learning online machine learning the data comes in as soon as",
    "start": "1271630",
    "end": "1278260"
  },
  {
    "text": "it's coming you feed it to the machine learning algorithms so you can continuously update the model so this",
    "start": "1278260",
    "end": "1285520"
  },
  {
    "text": "allows you to make decisions much faster or much fresher data okay so this is",
    "start": "1285520",
    "end": "1297370"
  },
  {
    "text": "pretty much about to zero and now I'm going to hand to Rahul to tell you some",
    "start": "1297370",
    "end": "1304660"
  },
  {
    "text": "exciting things about how to use Apache spark on his ito thank you and thank you very much so that was a great talk I",
    "start": "1304660",
    "end": "1313180"
  },
  {
    "text": "mean we learned about in terms of structure screaming we'd learn about data frames you learned about Johnson but if you take it if you think",
    "start": "1313180",
    "end": "1321370"
  },
  {
    "text": "about it this is about sparking how do you translate that on AWS as a platform",
    "start": "1321370",
    "end": "1327040"
  },
  {
    "text": "to think about that let's look at what are the options you have in terms of the first option the first choice what you",
    "start": "1327040",
    "end": "1333220"
  },
  {
    "start": "1329000",
    "end": "1329000"
  },
  {
    "text": "get any time you're thinking about doesn't matter which which what what I",
    "start": "1333220",
    "end": "1338620"
  },
  {
    "text": "using to run spark on ALW as the first option comes to your head is which type of instance I'm using right am i using a",
    "start": "1338620",
    "end": "1345700"
  },
  {
    "text": "memory optimized am i using a story optimized compute optimized automated general-purpose instances and if you",
    "start": "1345700",
    "end": "1352510"
  },
  {
    "text": "think of it each of them lends to a specific class of workload which you mentioned if you if you read deeper into",
    "start": "1352510",
    "end": "1358090"
  },
  {
    "text": "what I am talked about for instance like how tungsten has off heap storage which",
    "start": "1358090",
    "end": "1364090"
  },
  {
    "text": "is much more customized so now you could leverage an instance like x1 which",
    "start": "1364090",
    "end": "1369100"
  },
  {
    "text": "provides you 128 cores with 2 terabyte of RAM and you can think of that as a",
    "start": "1369100",
    "end": "1374640"
  },
  {
    "text": "multi many workers running sharing a same Terra by 2 terabyte of memory the",
    "start": "1374640",
    "end": "1380020"
  },
  {
    "text": "implications of that could be for example let's say you're running a very heavy sort workload which is doing lot of data transfer across nodes now since",
    "start": "1380020",
    "end": "1387160"
  },
  {
    "text": "all of the node all of the my data could be localized on the same machine it could help you in those kind of situations but again the workload has to",
    "start": "1387160",
    "end": "1394480"
  },
  {
    "text": "be the environment where there is implications in terms of how you store the memory in terms of how the memory gets transferred or how you can optimize",
    "start": "1394480",
    "end": "1401050"
  },
  {
    "text": "that and as you say the tungsten provides of the Offutt memory very don't have to they we don't have to worry",
    "start": "1401050",
    "end": "1407260"
  },
  {
    "text": "about the JVM in a traditional model to leverage this kind of system you will have to run multiple spark workers on the same",
    "start": "1407260",
    "end": "1413800"
  },
  {
    "text": "machine each of which is using a JVM and the every time a JVM is trying to talk to out the JVM you're doing some",
    "start": "1413800",
    "end": "1419350"
  },
  {
    "text": "deceleration see lies in between there for them to communicate with each other but this way you could reduce that because the workers are using a shared",
    "start": "1419350",
    "end": "1425290"
  },
  {
    "text": "memory model if you were and not only that if a JVM crashes the offic memory",
    "start": "1425290",
    "end": "1430690"
  },
  {
    "text": "still lives on which means the new when the new JVM creates the new worker comes it can just start using off its storage rather",
    "start": "1430690",
    "end": "1436420"
  },
  {
    "text": "than having to digitalize the whole thing again that's one of the example for instance where you can run a memory",
    "start": "1436420",
    "end": "1441760"
  },
  {
    "text": "optimized workload much more efficiently on spark using the x1 platform the same will apply for our force announced",
    "start": "1441760",
    "end": "1447700"
  },
  {
    "text": "yesterday for instance or if you look at it in cases it's it's a common",
    "start": "1447700",
    "end": "1453340"
  },
  {
    "start": "1450000",
    "end": "1450000"
  },
  {
    "text": "misconception that it's park everything has to fit in the memory if I ain't",
    "start": "1453340",
    "end": "1459520"
  },
  {
    "text": "talked about the sorting which we did using on hit of Lewis a terabyte sword",
    "start": "1459520",
    "end": "1464530"
  },
  {
    "text": "benchmark and if you look at if you really read the benchmark outputs of it whatever little realizes it's Park in",
    "start": "1464530",
    "end": "1472330"
  },
  {
    "text": "this case was optimized by the community in terms of using the local storage of using local SSD which is there on iTunes",
    "start": "1472330",
    "end": "1479380"
  },
  {
    "text": "to run the benchmark because or for instance you think about I threes which was announced yesterday they can give",
    "start": "1479380",
    "end": "1485020"
  },
  {
    "text": "you up to a 3.3 million I ops or 16 gigabyte per second of throughput to the",
    "start": "1485020",
    "end": "1491470"
  },
  {
    "text": "nvme based local disks and in that case is that the boundaries between how you",
    "start": "1491470",
    "end": "1497140"
  },
  {
    "text": "use a memory or how you store data on assess these actually becomes very seamless and this part gives you the ability to store the data set in there",
    "start": "1497140",
    "end": "1503770"
  },
  {
    "text": "for example sometimes you may have when you create a object from streaming you may have put a parameter which is called",
    "start": "1503770",
    "end": "1511570"
  },
  {
    "text": "memory and disk for example if you ever notice right but what is telling is Park is like trying to cop share a copy in",
    "start": "1511570",
    "end": "1517570"
  },
  {
    "text": "memory and share store a copy in disk itself not only that there are optimizations in the sense of like for",
    "start": "1517570",
    "end": "1523630"
  },
  {
    "text": "example when you run sequel queries when you're done query is how Park Hae could help in terms of columnist storage could",
    "start": "1523630",
    "end": "1529930"
  },
  {
    "text": "help you run run those queries very effectively even when it has stored on the disks rather than having been cached",
    "start": "1529930",
    "end": "1536470"
  },
  {
    "text": "in memory this is primarily if you're running some things which are very data heavy workloads these are a great",
    "start": "1536470",
    "end": "1542950"
  },
  {
    "text": "instance to for you to run that kind of data heavy workload because you have a lot of data and you can store the data in outside heat back in the local",
    "start": "1542950",
    "end": "1550270"
  },
  {
    "text": "storage itself but he will lick a bit further now as you move from a query",
    "start": "1550270",
    "end": "1558400"
  },
  {
    "start": "1553000",
    "end": "1553000"
  },
  {
    "text": "workload as you move from a data processing workload into a algorithm based workload like a numerical",
    "start": "1558400",
    "end": "1564670"
  },
  {
    "text": "computation this is where GPUs starts giving you the power giving you the benefit of that the p2 instances for",
    "start": "1564670",
    "end": "1571150"
  },
  {
    "text": "example gives you 16 nvd Nvidia k80 GPUs and how do you utilize that to run that",
    "start": "1571150",
    "end": "1578230"
  },
  {
    "text": "on Amazon you army which is available as a standard all of the libraries installed who can",
    "start": "1578230",
    "end": "1583630"
  },
  {
    "text": "start running your own machine learning workloads data breaks in the blog announced that they're gonna soon I had support for Peters with red spot but if",
    "start": "1583630",
    "end": "1590800"
  },
  {
    "text": "you really think about it what does when you run a workload when you run when you wanna utilize a GPU based computation",
    "start": "1590800",
    "end": "1597790"
  },
  {
    "text": "inspark something like tensor frames for example what happens in that case is you",
    "start": "1597790",
    "end": "1604180"
  },
  {
    "text": "have a spark memory model and then it gets translated from spark to a Java then Java baked to another C++ tensor",
    "start": "1604180",
    "end": "1611440"
  },
  {
    "text": "frame or tensorflow based object that's utilization deceleration that communication takes a lot of time in",
    "start": "1611440",
    "end": "1616570"
  },
  {
    "text": "fact you cannot learn more about it this awesome talk by Tim hunter on how he talks about how tensor frames works",
    "start": "1616570",
    "end": "1622300"
  },
  {
    "text": "underneath the scenes and with tensor frames what is happening with in terms",
    "start": "1622300",
    "end": "1627700"
  },
  {
    "start": "1625000",
    "end": "1625000"
  },
  {
    "text": "of GPU what is really happening is that you have a tungsten binary object and the object does not get C realized these",
    "start": "1627700",
    "end": "1634330"
  },
  {
    "text": "lies in between different layers of communication from CPU from back to its spot data frames from back to a tensor",
    "start": "1634330",
    "end": "1640180"
  },
  {
    "text": "frame object in there and this gives you benefit in terms of speeding running",
    "start": "1640180",
    "end": "1646410"
  },
  {
    "text": "what looks like kernel density is scoring which is basically a way to estimate the probability density",
    "start": "1646410",
    "end": "1652570"
  },
  {
    "start": "1647000",
    "end": "1647000"
  },
  {
    "text": "function of random variables and it texts and it tells you that when you're running back using a tensor frames alone",
    "start": "1652570",
    "end": "1658810"
  },
  {
    "text": "on a computer or on a computer of instances versus on GPUs itself and you",
    "start": "1658810",
    "end": "1663910"
  },
  {
    "text": "can see the 60 X throughput difference in terms of how fast it can run tensor",
    "start": "1663910",
    "end": "1669070"
  },
  {
    "text": "frames using GPU where we have removed the communication channel communication barriers between different layers of",
    "start": "1669070",
    "end": "1674740"
  },
  {
    "text": "that processing so these are few examples where you can see how you can",
    "start": "1674740",
    "end": "1680170"
  },
  {
    "text": "use memory optimize how you can choose storage optimize or how you can run a GPU instances bit SPARC on AWS and how",
    "start": "1680170",
    "end": "1686470"
  },
  {
    "text": "can leverage those for for running a workload now the one thing we all have",
    "start": "1686470",
    "end": "1692710"
  },
  {
    "text": "been stressing about on the spur in this talk was that the SPARC is being used for computing you have your data stored",
    "start": "1692710",
    "end": "1700900"
  },
  {
    "text": "somewhere else be it s3e be it read chef be it Ororo DynamoDB and you're reading that data back from one of these sources",
    "start": "1700900",
    "end": "1708160"
  },
  {
    "text": "into spark you're computing your object you're computing your processing and writing it back somewhere else that spark is not a",
    "start": "1708160",
    "end": "1714810"
  },
  {
    "text": "storage engine it can store data for computation what it needs but it's not really a computation engine a storage",
    "start": "1714810",
    "end": "1721200"
  },
  {
    "text": "engine so what that means is that it gives you advantage to use things like spot instances because it's not a",
    "start": "1721200",
    "end": "1729090"
  },
  {
    "start": "1724000",
    "end": "1724000"
  },
  {
    "text": "computation is not a storage engine if your computers loss it has the ability to recreate the compute from the",
    "start": "1729090",
    "end": "1735090"
  },
  {
    "text": "lineatus stores from the information it stores from the redundant data to copy the stores and get you back to the sales",
    "start": "1735090",
    "end": "1741000"
  },
  {
    "text": "stage and spot using spot you can run your workload much more cheaply much more efficiently on AWS sometimes you",
    "start": "1741000",
    "end": "1747450"
  },
  {
    "text": "can save have savings as much as 10x for example running where you can leverage a spot to run most of your spark or close",
    "start": "1747450",
    "end": "1753630"
  },
  {
    "text": "on AWS since your data is living outside of the spot so even if the spot goes away this part will make the",
    "start": "1753630",
    "end": "1759060"
  },
  {
    "text": "transferring for you most of the frame bus will make that transparent for you and when you run it always it's a good",
    "start": "1759060",
    "end": "1766050"
  },
  {
    "text": "idea make sure that you're not running your driver on spark now because the driver goes then you're in trouble and of course have a mix of spot +",
    "start": "1766050",
    "end": "1773730"
  },
  {
    "text": "on-demand or our eyes or whatever you have so you could even if the spot gets taken away you still could complete your work if you're on everything on spot",
    "start": "1773730",
    "end": "1780300"
  },
  {
    "text": "I then you may end up losing some work but few things again just in general",
    "start": "1780300",
    "end": "1786870"
  },
  {
    "start": "1786000",
    "end": "1786000"
  },
  {
    "text": "which is that first is enhanced networking this part needs a lot in",
    "start": "1786870",
    "end": "1795360"
  },
  {
    "text": "terms of communicating between each other and if you're running a workload which needs a lot of data transfer for",
    "start": "1795360",
    "end": "1800880"
  },
  {
    "text": "instance between two nodes for example query processing sorting over flowed in for example where a lot of reiden is to",
    "start": "1800880",
    "end": "1807570"
  },
  {
    "text": "get transferred between two nodes itself and this is where you could use things like enhance networking which gives you",
    "start": "1807570",
    "end": "1812760"
  },
  {
    "text": "flat very high packet per second or elastic network adapter where in case of",
    "start": "1812760",
    "end": "1818580"
  },
  {
    "text": "benchmarks or in case of running workloads like sorting with a lot of data is being transferred between between nodes running in a cluster you",
    "start": "1818580",
    "end": "1825510"
  },
  {
    "text": "could leverage 20 GB vs which is there with m 4m m for 10 X 16 X large the",
    "start": "1825510",
    "end": "1832290"
  },
  {
    "text": "largest of the instances like you know PTO's x ones and all of them can give",
    "start": "1832290",
    "end": "1837690"
  },
  {
    "text": "you up to 20 Gbps off connectivity when you place them within a placement group in fact the sorting benchmark was run on",
    "start": "1837690",
    "end": "1844650"
  },
  {
    "text": "the instances being in a placement group because a placement Lopes where it provides to that kind of bandwidth all the Technic cells are all",
    "start": "1844650",
    "end": "1852509"
  },
  {
    "text": "the notes which says that they have 10 Gbps network or 20 Gbps network is that",
    "start": "1852509",
    "end": "1857730"
  },
  {
    "text": "where you can leverage the placement group to keep them get the leverage the bandwidth in case of workload is where a",
    "start": "1857730",
    "end": "1863070"
  },
  {
    "text": "lot of it is getting transfer between two nodes now these are some other choices you have today in terms of",
    "start": "1863070",
    "end": "1869610"
  },
  {
    "text": "running the workload but the question still remains so we learned a lot about what SPARC is what are the benefits of",
    "start": "1869610",
    "end": "1874769"
  },
  {
    "text": "SPARC 2.0 we learn about some of the choices available to you and I AWS in terms of running your workload but which",
    "start": "1874769",
    "end": "1881429"
  },
  {
    "text": "instance you choose how do you figure out what to run we had to run it and to for that I would again in white iron to",
    "start": "1881429",
    "end": "1887129"
  },
  {
    "text": "talk about how you can choose the right instance how you choose the right size on it of Luis thank you yes so like",
    "start": "1887129",
    "end": "1900929"
  },
  {
    "text": "Rahul mentioned mentioned early on today you know Amazon is doing a fantastic job",
    "start": "1900929",
    "end": "1907139"
  },
  {
    "text": "giving you choices right and the last time and I looked there are over 100 type of instances you can choose from",
    "start": "1907139",
    "end": "1914669"
  },
  {
    "text": "and every day they add new ones right so that then you is a very natural question",
    "start": "1914669",
    "end": "1921629"
  },
  {
    "text": "on one hand you have your application on the other hand you have all these other choices so the question is basically how",
    "start": "1921629",
    "end": "1928769"
  },
  {
    "text": "many instances you need and what type of instances you should use for your applications so in this the rest of the",
    "start": "1928769",
    "end": "1936419"
  },
  {
    "text": "talk I am going to try to address these questions and but the first question before even going in which instances you",
    "start": "1936419",
    "end": "1944850"
  },
  {
    "text": "are going to use which type of instances how many is the first question it does it matter right if it doesn't matter why",
    "start": "1944850",
    "end": "1952230"
  },
  {
    "text": "you care right you pick whatever well it turns out that it matters like you'd",
    "start": "1952230",
    "end": "1957899"
  },
  {
    "text": "expect and here is just a micro benchmark it's a mark tricks multiplication and you have five",
    "start": "1957899",
    "end": "1966750"
  },
  {
    "text": "configurations all the configurations they have the same the same kind of",
    "start": "1966750",
    "end": "1972090"
  },
  {
    "text": "memory and cost the same the same resources the same cost but these I",
    "start": "1972090",
    "end": "1978509"
  },
  {
    "text": "differ by how many instances you have and you're",
    "start": "1978509",
    "end": "1984360"
  },
  {
    "text": "seeing this plot on the y axis you have the time how long it takes to multiply",
    "start": "1984360",
    "end": "1989549"
  },
  {
    "text": "these matrices and how much for each configuration how much it did take for",
    "start": "1989549",
    "end": "1995100"
  },
  {
    "text": "this configuration to multiply these matrices so maybe not surprising here if",
    "start": "1995100",
    "end": "2000620"
  },
  {
    "text": "you can fit every single non machine it you get the best results the dark blue",
    "start": "2000620",
    "end": "2007039"
  },
  {
    "start": "2007000",
    "end": "2007000"
  },
  {
    "text": "line right so okay so maybe this will imply that maybe it's a simple rule I",
    "start": "2007039",
    "end": "2015919"
  },
  {
    "text": "just try to consolidate as much as I can on the fewer instances as possible it",
    "start": "2015919",
    "end": "2022610"
  },
  {
    "text": "turns out that that's not always the case for the different our workloads and",
    "start": "2022610",
    "end": "2030200"
  },
  {
    "text": "this again this is QR factorization is a linear algebra and as a linear algebra micro benchmarks if you look at the same",
    "start": "2030200",
    "end": "2038840"
  },
  {
    "text": "configurations you get very different results in this case the one machine configuration is the worse and having",
    "start": "2038840",
    "end": "2048589"
  },
  {
    "text": "more machines in this case is a is a is a best okay again cost is a same amount",
    "start": "2048589",
    "end": "2055638"
  },
  {
    "text": "of money and intuitively here the reason",
    "start": "2055639",
    "end": "2060770"
  },
  {
    "text": "for it's a matrix multiplication is much better you if you don't want to do it on a machine it's because the communication",
    "start": "2060770",
    "end": "2066500"
  },
  {
    "text": "is about on against our case in the case of QR factorization you can paralyze it",
    "start": "2066500",
    "end": "2072560"
  },
  {
    "text": "much more effectively and now you can do all this is the bottleneck is how much income fast you can do on each machine",
    "start": "2072560",
    "end": "2078440"
  },
  {
    "text": "and if you have more machines you know the aggregate memorybot",
    "start": "2078440",
    "end": "2083450"
  },
  {
    "text": "of a bus if you do the but you know if you have two machines we have prices as",
    "start": "2083450",
    "end": "2089868"
  },
  {
    "text": "almost twice as much as capacity into the memory of the bus to the memory you",
    "start": "2089869",
    "end": "2097760"
  },
  {
    "text": "are going to do better if you have more machines right because the aggregate memory bus bandwidth is higher if you",
    "start": "2097760",
    "end": "2104480"
  },
  {
    "text": "have more machines okay so now what you are going to do",
    "start": "2104480",
    "end": "2110980"
  },
  {
    "start": "2110000",
    "end": "2110000"
  },
  {
    "text": "about it so we developed at Berkeley a tool which",
    "start": "2110980",
    "end": "2117400"
  },
  {
    "text": "is called Airness and it's available it's early release please go ahead and",
    "start": "2117400",
    "end": "2123339"
  },
  {
    "text": "you can check it out and the goal of these tools is that giving a job to tell",
    "start": "2123339",
    "end": "2130480"
  },
  {
    "text": "you how many instances and how many of",
    "start": "2130480",
    "end": "2136359"
  },
  {
    "text": "these instances you should use for this to run your job right what type of",
    "start": "2136359",
    "end": "2142570"
  },
  {
    "text": "instances and how many instances and the way is doing it is that going to run",
    "start": "2142570",
    "end": "2148060"
  },
  {
    "text": "your job or for smaller datasets much smaller data set right so you have to",
    "start": "2148060",
    "end": "2154300"
  },
  {
    "text": "process one petabyte is going to run on a few megabytes or a few tens of megabytes and it's going to somehow",
    "start": "2154300",
    "end": "2164339"
  },
  {
    "text": "triangulate and trying to here is going to figure out what is what what a piece",
    "start": "2164339",
    "end": "2169930"
  },
  {
    "text": "is that says you should use and how many of them okay and you can use it in two",
    "start": "2169930",
    "end": "2175960"
  },
  {
    "text": "ways on one hand you can say I want to spend so much time so much money so tell",
    "start": "2175960",
    "end": "2184900"
  },
  {
    "text": "me which instances I should use so that I run the job the fastest or I want to",
    "start": "2184900",
    "end": "2191349"
  },
  {
    "text": "run this job like think about the ETL job like by sometimes you need to process a data if I say from the",
    "start": "2191349",
    "end": "2197109"
  },
  {
    "text": "previous hour or the previous day so you have a deadline what is the cheapest way",
    "start": "2197109",
    "end": "2202420"
  },
  {
    "text": "I can run the job to meet my deadlines okay start the toys this is an example",
    "start": "2202420",
    "end": "2212140"
  },
  {
    "text": "showing again it's showing how that in",
    "start": "2212140",
    "end": "2219130"
  },
  {
    "text": "this case you have the same type of instance the same instance type r3x",
    "start": "2219130",
    "end": "2224890"
  },
  {
    "text": "large and on the x-axis we vary the number of machines and the time for this particular computer for this particular",
    "start": "2224890",
    "end": "2231550"
  },
  {
    "text": "applications it's on the y-axis since you can see as you add more machines as",
    "start": "2231550",
    "end": "2236650"
  },
  {
    "text": "you'd expect the time will go down but after a while",
    "start": "2236650",
    "end": "2241950"
  },
  {
    "text": "the time is actually going to slowly increase why is that because for this a",
    "start": "2241950",
    "end": "2248350"
  },
  {
    "text": "particular application as you start to use more machines in this case more than whatever 40 the communication becomes a",
    "start": "2248350",
    "end": "2256330"
  },
  {
    "text": "bottleneck you send more and more data over the network so in this case if you",
    "start": "2256330",
    "end": "2262210"
  },
  {
    "text": "have a deadline say you want to do your stuff in something like two hours is",
    "start": "2262210",
    "end": "2272890"
  },
  {
    "text": "this a orange line then you can see that the optimal number it will be 40",
    "start": "2272890",
    "end": "2279430"
  },
  {
    "text": "machines or 20 actually 20 in this case sorry right so if you go if you spend",
    "start": "2279430",
    "end": "2287050"
  },
  {
    "text": "more much more money and you go to 100 120 machines you won't get any benefits",
    "start": "2287050",
    "end": "2293230"
  },
  {
    "text": "you are just going to spend more money okay",
    "start": "2293230",
    "end": "2298440"
  },
  {
    "start": "2298000",
    "end": "2298000"
  },
  {
    "text": "so this figure just shows these orange dots just tells you how accurate in",
    "start": "2302170",
    "end": "2308499"
  },
  {
    "text": "prediction is Ernest the dark line is a real execution of",
    "start": "2308499",
    "end": "2313809"
  },
  {
    "text": "your of the job with orange or the orange dot is a prediction given by a",
    "start": "2313809",
    "end": "2320890"
  },
  {
    "text": "vest so in this case the prediction is very good you are just only between 10",
    "start": "2320890",
    "end": "2326230"
  },
  {
    "text": "and 50 percent off from the real execution time finally here it's a more",
    "start": "2326230",
    "end": "2333640"
  },
  {
    "start": "2332000",
    "end": "2332000"
  },
  {
    "text": "realistic application there's a set of two to operations in a genomics pipeline",
    "start": "2333640",
    "end": "2342880"
  },
  {
    "text": "we build at Berkeley Adam and just look at the first two top group of bars here",
    "start": "2342880",
    "end": "2351369"
  },
  {
    "text": "we have two configurations with orange 45 are 3x large instances and second",
    "start": "2351369",
    "end": "2356619"
  },
  {
    "text": "green and blue is 64 M series two large 2x large instances was the left-hand",
    "start": "2356619",
    "end": "2365170"
  },
  {
    "text": "side you see that time how long it takes to run these operations on the right",
    "start": "2365170",
    "end": "2372249"
  },
  {
    "text": "hand side this is a price for each configuration so if you look for marked",
    "start": "2372249",
    "end": "2377799"
  },
  {
    "text": "up as the at the top if you use 45 are",
    "start": "2377799",
    "end": "2382900"
  },
  {
    "text": "3x large instances you can do it one point x times faster and by the way this",
    "start": "2382900",
    "end": "2391749"
  },
  {
    "text": "will be 20% cheaper so in this case you",
    "start": "2391749",
    "end": "2396819"
  },
  {
    "text": "can get over 2x better performance of price ratio if you use the right",
    "start": "2396819",
    "end": "2404920"
  },
  {
    "text": "configuration",
    "start": "2404920",
    "end": "2407220"
  },
  {
    "start": "2410000",
    "end": "2410000"
  },
  {
    "text": "so we are going to wrap up so in summary today suppose a spark is a de facto",
    "start": "2410329",
    "end": "2416539"
  },
  {
    "text": "standard for big data processing he's",
    "start": "2416539",
    "end": "2421849"
  },
  {
    "text": "growing faster than ever across almost every metric and this year",
    "start": "2421849",
    "end": "2428839"
  },
  {
    "text": "we are very excited about release of the version 2-0 which again represents the",
    "start": "2428839",
    "end": "2436069"
  },
  {
    "text": "biggest improvement since the original release today AWS is a best cloud",
    "start": "2436069",
    "end": "2445279"
  },
  {
    "text": "platform to run Apache spark again we developed a batch Apache spark very",
    "start": "2445279",
    "end": "2451069"
  },
  {
    "text": "early on on AWS and like Rahul said",
    "start": "2451069",
    "end": "2457209"
  },
  {
    "text": "Apache spark is a computation engine right it's not a storage engine it can",
    "start": "2457209",
    "end": "2464989"
  },
  {
    "text": "process the data from any storage or have any way restore in AWS you have",
    "start": "2464989",
    "end": "2471259"
  },
  {
    "text": "your data stored you can use Apache spark to process it it can take",
    "start": "2471259",
    "end": "2478609"
  },
  {
    "text": "advantage for different workloads of all these AWS instance this you can choose",
    "start": "2478609",
    "end": "2485569"
  },
  {
    "text": "from and I just shown you this tool our nest which can help you to select the",
    "start": "2485569",
    "end": "2493729"
  },
  {
    "text": "optimal number and type of instances for your particular job so is this I think",
    "start": "2493729",
    "end": "2499339"
  },
  {
    "text": "we have time for a few questions",
    "start": "2499339",
    "end": "2502989"
  }
]