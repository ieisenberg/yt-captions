[
  {
    "text": "okay good evening everyone and thanks for being the real troopers and joining",
    "start": "60",
    "end": "6960"
  },
  {
    "text": "us this point in the evening preciate you showing up so I'm John Mallory I'm",
    "start": "6960",
    "end": "12509"
  },
  {
    "text": "on the storage business development team for AWS focus a lot on data Lakes",
    "start": "12509",
    "end": "18410"
  },
  {
    "text": "so I'm gonna kick this off and then I have Jean and Josh who will introduce themselves to talk about a use case",
    "start": "18410",
    "end": "26789"
  },
  {
    "text": "using s3 for a data like style architecture so we'll try to wrap this",
    "start": "26789",
    "end": "32340"
  },
  {
    "text": "up in about 50 minutes and give you some time for questions so hang in there if",
    "start": "32340",
    "end": "37530"
  },
  {
    "text": "you have burning questions you want to ask so to start it off when we think",
    "start": "37530",
    "end": "43829"
  },
  {
    "text": "about data lakes and one of the drivers towards building an architecture on s3 it's really that for most customers we",
    "start": "43829",
    "end": "51719"
  },
  {
    "text": "find that deriving value from data and how they use data for their businesses",
    "start": "51719",
    "end": "58530"
  },
  {
    "text": "is really a journey they're not unless they're very sophisticated and they're doing this on Prem today and want to",
    "start": "58530",
    "end": "65220"
  },
  {
    "text": "lift and shift to AWS they're probably going to start out with a few",
    "start": "65220",
    "end": "70320"
  },
  {
    "text": "well-defined you know high-value traditional use cases and from there",
    "start": "70320",
    "end": "76590"
  },
  {
    "text": "really explore how they can use data to evolve and transform their business and so",
    "start": "76590",
    "end": "83939"
  },
  {
    "text": "really one of the precepts when we build a data Lake is how can we build an architecture that's going to let them",
    "start": "83939",
    "end": "90630"
  },
  {
    "text": "take this journey non-disruptive lee you know where they can once they get the data established they can innovate",
    "start": "90630",
    "end": "97890"
  },
  {
    "text": "around the data using more and more sophisticated tools and methods as they evolve so that you know they can move",
    "start": "97890",
    "end": "105479"
  },
  {
    "text": "from basic things like maybe bi data warehouse augmentation to you know much",
    "start": "105479",
    "end": "112680"
  },
  {
    "text": "more advanced things like advanced analytics AI ml and you know tie that",
    "start": "112680",
    "end": "118740"
  },
  {
    "text": "back to business outcomes so why do people use AWS for a big data or why",
    "start": "118740",
    "end": "125759"
  },
  {
    "text": "should you consider you know what we're gonna talk about next for your business",
    "start": "125759",
    "end": "132020"
  },
  {
    "text": "and some of the fundamentals are it's about agility how can you take your data",
    "start": "132020",
    "end": "137450"
  },
  {
    "text": "and quickly find new value in that data and then quickly you know if you find a",
    "start": "137450",
    "end": "144830"
  },
  {
    "text": "use case where you've got new value established by using new analytic methods may be applying some model",
    "start": "144830",
    "end": "151700"
  },
  {
    "text": "training to a historical data set you can quickly roll that into production and essentially speed time to result so",
    "start": "151700",
    "end": "160280"
  },
  {
    "text": "that's really a key attribute of a data like that we're promoting is you know go",
    "start": "160280",
    "end": "167330"
  },
  {
    "text": "fast fail fast but when you find something that works quickly roll that into production which is all about",
    "start": "167330",
    "end": "173900"
  },
  {
    "text": "scalability and cost-effectiveness so once you find a use case you want to adopt you want to be able to scale that",
    "start": "173900",
    "end": "181360"
  },
  {
    "text": "seamlessly non-disruptive ly and cost-effectively and so those are key",
    "start": "181360",
    "end": "186770"
  },
  {
    "text": "considerations versus maybe trying to do this on Prem where you have to worry about provisioning hardware you know",
    "start": "186770",
    "end": "193670"
  },
  {
    "text": "deploying the services you want to use getting those into production that can take a long time another is services we",
    "start": "193670",
    "end": "203300"
  },
  {
    "text": "have a lot of services that can help you quickly derive value from your data and",
    "start": "203300",
    "end": "209120"
  },
  {
    "text": "that's one of the themes of this presentation is how can you use essentially serverless capabilities and",
    "start": "209120",
    "end": "217160"
  },
  {
    "text": "query and place capabilities to quickly experiment and then innovate around your",
    "start": "217160",
    "end": "223220"
  },
  {
    "text": "data without having to worry about a lot of pre provisioning of assets or",
    "start": "223220",
    "end": "228730"
  },
  {
    "text": "migrating data from one platform to another before you can actually use it",
    "start": "228730",
    "end": "233860"
  },
  {
    "text": "which place and do they get to insights much more quickly since that's really",
    "start": "233860",
    "end": "239239"
  },
  {
    "text": "the bottom line is speed I mean that is really probably as much as cost or in a",
    "start": "239239",
    "end": "247940"
  },
  {
    "text": "lot of cases even more so than cost a key business outcome is how can you get",
    "start": "247940",
    "end": "252950"
  },
  {
    "text": "to results more quickly and then finally a lot of you are probably doing this on Prem so how can we make it easier to",
    "start": "252950",
    "end": "260390"
  },
  {
    "text": "migrate your data migrate your use cases move to the cloud more quickly so you can really start to realize these",
    "start": "260390",
    "end": "266960"
  },
  {
    "text": "benefits much more in a much quicker fashion and in a more cost effective",
    "start": "266960",
    "end": "272720"
  },
  {
    "text": "fashion so how do we define data like since the terms been around pretty much",
    "start": "272720",
    "end": "279410"
  },
  {
    "text": "since the early days of Hadoop kind of kicked off this whole discussion around data Lake and defining a data Lake we",
    "start": "279410",
    "end": "287540"
  },
  {
    "text": "define it you know somewhat similarly but maybe with some additional twist so",
    "start": "287540",
    "end": "292750"
  },
  {
    "text": "essentially it's about taking your data and centralizing it on a common platform",
    "start": "292750",
    "end": "298430"
  },
  {
    "text": "so maybe you have a lot of silos that are purpose-built to do specific things today how can we kind of take all those",
    "start": "298430",
    "end": "306110"
  },
  {
    "text": "silos maybe all those forked copies of data let's start to get tough to govern",
    "start": "306110",
    "end": "311560"
  },
  {
    "text": "consolidate and then build around that so that's the foundation of the data Lake is consolidate your data separate",
    "start": "311560",
    "end": "319669"
  },
  {
    "text": "the compute the processing all the tools that you bring to the data so that you",
    "start": "319669",
    "end": "325160"
  },
  {
    "text": "can kind of innovate around this data non-disruptive lis so decoupled storage and compute and",
    "start": "325160",
    "end": "330950"
  },
  {
    "text": "process you want to be able to get the data in and transform it into useable formats quickly so that's another key",
    "start": "330950",
    "end": "338060"
  },
  {
    "text": "attribute of it ultimately if you're taking a bunch of purpose-built silos maybe four different lines of business",
    "start": "338060",
    "end": "344289"
  },
  {
    "text": "how do you bring those together into a centralized but secure and multi tenant",
    "start": "344289",
    "end": "350450"
  },
  {
    "text": "platform so secure multi-tenancy is another attribute and ultimately in the",
    "start": "350450",
    "end": "356900"
  },
  {
    "text": "spirit of time to results how can you do more in place with the data and then",
    "start": "356900",
    "end": "363020"
  },
  {
    "text": "finally for rapid innovation around your data how can you essentially define schema",
    "start": "363020",
    "end": "369940"
  },
  {
    "text": "on-the-fly so that you can experiment around your data much more quickly rather than having rigidly defined",
    "start": "369940",
    "end": "376280"
  },
  {
    "text": "schema that take a lot of time to transform and model before you can ask a",
    "start": "376280",
    "end": "381830"
  },
  {
    "text": "new question in of your data so these are some of the concepts that we apply to defining an AWS data like so what do",
    "start": "381830",
    "end": "391310"
  },
  {
    "text": "I mean by processing and querying in place since that's the title of this presentation essentially there are two",
    "start": "391310",
    "end": "397580"
  },
  {
    "text": "fundamental concepts he one is how do you process data with",
    "start": "397580",
    "end": "403600"
  },
  {
    "text": "user-defined functions so if you're doing custom processing if you're trying to build data pipelines kind of do",
    "start": "403600",
    "end": "410479"
  },
  {
    "text": "multi-step processing and want to orchestrate that all how do you do that",
    "start": "410479",
    "end": "416030"
  },
  {
    "text": "and that's really what we call user-defined functions where you're gonna bring your own code your own",
    "start": "416030",
    "end": "421490"
  },
  {
    "text": "functions your own tools and essentially execute all that without having to worry",
    "start": "421490",
    "end": "427280"
  },
  {
    "text": "about physical infrastructures so that's the first element of processing and querying in place is really using lambda",
    "start": "427280",
    "end": "434990"
  },
  {
    "text": "is the engine to do that and obviously there are a host of lambda sessions here",
    "start": "434990",
    "end": "440900"
  },
  {
    "text": "at the conference so I won't belabor that but it is a key tool in building a",
    "start": "440900",
    "end": "446900"
  },
  {
    "text": "data Lake particularly in starting to automate how you use the data like so",
    "start": "446900",
    "end": "452840"
  },
  {
    "text": "that you can really focus less on orchestration and more on querying your",
    "start": "452840",
    "end": "458900"
  },
  {
    "text": "data and deriving value which is the second function is how can you go from managed services or unmanaged services",
    "start": "458900",
    "end": "466820"
  },
  {
    "text": "where you have to spin up servers provision and stall code and manage all",
    "start": "466820",
    "end": "472759"
  },
  {
    "text": "the updates kind of you know do infrastructure much as you do on Prem today typically and start to turn that",
    "start": "472759",
    "end": "480169"
  },
  {
    "text": "into manage services where all you worry about is executing queries and catalogs",
    "start": "480169",
    "end": "486830"
  },
  {
    "text": "and transformations against your data without worrying about physical resources so that's the second part is",
    "start": "486830",
    "end": "494300"
  },
  {
    "text": "how do you catalog transform and query your data once it's in s3 without having",
    "start": "494300",
    "end": "500060"
  },
  {
    "text": "to worry about physical resources and that really comes down to services like glue Athena redshift spectrum Sage Maker",
    "start": "500060",
    "end": "507500"
  },
  {
    "text": "is some common examples so once you",
    "start": "507500",
    "end": "512899"
  },
  {
    "text": "start to use these definitions this is what a data like starts to look like conceptually where you have a lot of",
    "start": "512899",
    "end": "519169"
  },
  {
    "text": "different ingest methods because the data lake is going to have to encompass a lot of different data sources and you",
    "start": "519169",
    "end": "525170"
  },
  {
    "text": "want to match the right tool to the right data source both the optimized cost time to ingest and efficiency",
    "start": "525170",
    "end": "533690"
  },
  {
    "text": "and then how do you start to bring processing and analytics um to the data",
    "start": "533690",
    "end": "539420"
  },
  {
    "text": "where it lives in place in s3 which s3 really becomes the foundation of the",
    "start": "539420",
    "end": "545390"
  },
  {
    "text": "data like and then just as importantly how do you catalog and transform that",
    "start": "545390",
    "end": "551210"
  },
  {
    "text": "data because without a catalog a data log is a data Lake is meaningless it's",
    "start": "551210",
    "end": "556580"
  },
  {
    "text": "just a data store and then how do you control access and security around that",
    "start": "556580",
    "end": "563210"
  },
  {
    "text": "common platform to get back to that multi-tenancy so this is an example the services we apply so whole host of enjo",
    "start": "563210",
    "end": "571130"
  },
  {
    "text": "services some very specific catalog and transform services a lot of different",
    "start": "571130",
    "end": "576920"
  },
  {
    "text": "both native AWS processing and analytic tools as well as a whole host of",
    "start": "576920",
    "end": "582950"
  },
  {
    "text": "third-party vendors through our marketplace that you can bring to s3 into the data and then the foundation",
    "start": "582950",
    "end": "589400"
  },
  {
    "text": "that ties across all of these is access and security through things like I am an",
    "start": "589400",
    "end": "595190"
  },
  {
    "text": "encryption so why do we use s3 for the data like several different region",
    "start": "595190",
    "end": "603140"
  },
  {
    "text": "reasons to do this the first is durability I mean if you're",
    "start": "603140",
    "end": "608390"
  },
  {
    "text": "gonna consolidate all your data put it on a platform that's got to be rock-solid s3 has 11 nines of durability",
    "start": "608390",
    "end": "616510"
  },
  {
    "text": "spread across multiple physically separated access zones good levels of",
    "start": "616510",
    "end": "622280"
  },
  {
    "text": "availability it's really designed to be mission-critical grade in terms of how",
    "start": "622280",
    "end": "628580"
  },
  {
    "text": "protects your data against lost corruption and how it makes it available so that's a foundation of if you're",
    "start": "628580",
    "end": "636170"
  },
  {
    "text": "going to use this data to drive business value it's got to be rock-solid in those",
    "start": "636170",
    "end": "641300"
  },
  {
    "text": "elements high performance is another characteristic vest 3 which may be",
    "start": "641300",
    "end": "648110"
  },
  {
    "text": "counterintuitive if you come from the traditional on-premise world of flash arrays and Hadoop and you know a lot of",
    "start": "648110",
    "end": "656870"
  },
  {
    "text": "other you know in-memory databases and you know things like spark but",
    "start": "656870",
    "end": "664270"
  },
  {
    "text": "ultimately if you consider about a Big Data Platform what you really want to be",
    "start": "664270",
    "end": "670209"
  },
  {
    "text": "able to do is scan and process large volumes of data very quickly and have",
    "start": "670209",
    "end": "677350"
  },
  {
    "text": "that scale without limits and that's what else three excels at is is you know",
    "start": "677350",
    "end": "682870"
  },
  {
    "text": "essentially performance scales very linearly as you throw more and more",
    "start": "682870",
    "end": "688330"
  },
  {
    "text": "threads of work against our data store and you can get tremendously high amounts of throughput I mean we have",
    "start": "688330",
    "end": "695470"
  },
  {
    "text": "customers doing hundreds of gigabytes up to you know terabyte level types of",
    "start": "695470",
    "end": "701770"
  },
  {
    "text": "process against data and s3 so for a lot of big data workloads it's a very high",
    "start": "701770",
    "end": "708580"
  },
  {
    "text": "performance seamlessly scalable platform security is key I mean I am is the",
    "start": "708580",
    "end": "715540"
  },
  {
    "text": "foundation of security at s3 that integrates into s3 you can get granular",
    "start": "715540",
    "end": "721360"
  },
  {
    "text": "access down to you know essentially prefix and even individual object level",
    "start": "721360",
    "end": "726940"
  },
  {
    "text": "and tie that across multiple services so multi tenant security is baked in and",
    "start": "726940",
    "end": "732970"
  },
  {
    "text": "then really ease of use is important because you don't want to worry about",
    "start": "732970",
    "end": "738130"
  },
  {
    "text": "managing your data managing your storage platforms you want to worry about how you get value out of that data and then",
    "start": "738130",
    "end": "745390"
  },
  {
    "text": "finally integration with a whole host of third-party vendors and a lot of native",
    "start": "745390",
    "end": "751390"
  },
  {
    "text": "services where you can process the data in place is also very important so",
    "start": "751390",
    "end": "756870"
  },
  {
    "text": "another key element is cost optimization how can you collect and store cost",
    "start": "756870",
    "end": "763420"
  },
  {
    "text": "effectively more data often plays into how you derive more value from your data",
    "start": "763420",
    "end": "769410"
  },
  {
    "text": "particularly when you're talking about things like model training for machine learning of taking a lot of historical",
    "start": "769410",
    "end": "775750"
  },
  {
    "text": "data building models and then going back and testing and validating those models based on real-world historical data also",
    "start": "775750",
    "end": "784180"
  },
  {
    "text": "how can you kind of optimize that balance between performance and storage",
    "start": "784180",
    "end": "790270"
  },
  {
    "text": "and bulk capacity and so really you can start to build a tearing strategy where high-performance",
    "start": "790270",
    "end": "797290"
  },
  {
    "text": "workloads may live on local block volume storage with things like SSD volumes or",
    "start": "797290",
    "end": "804970"
  },
  {
    "text": "nvme storage but then ultimately the balance of your data can start to set up",
    "start": "804970",
    "end": "811930"
  },
  {
    "text": "a tearing strategy so that historical data can be kept much more cost-effectively and so an example here",
    "start": "811930",
    "end": "819220"
  },
  {
    "text": "is with Hadoop I mean essentially if you have SPARC you have very high performance and OOP workloads store",
    "start": "819220",
    "end": "825670"
  },
  {
    "text": "those in a local file system but then use s3 and by extension glacier for",
    "start": "825670",
    "end": "832840"
  },
  {
    "text": "colder deeper more historical data assets or data that isn't processed as",
    "start": "832840",
    "end": "838630"
  },
  {
    "text": "frequently and then you can start to set up policies to automate that using tools",
    "start": "838630",
    "end": "844090"
  },
  {
    "text": "that are baked into s3 like storage analytics to help you make intelligent decisions about where to place data we",
    "start": "844090",
    "end": "852910"
  },
  {
    "text": "talked about rapid ingest and matching the ingest of the data to how you get it",
    "start": "852910",
    "end": "859000"
  },
  {
    "text": "into your data Lake and so essentially you're gonna have a lot of different",
    "start": "859000",
    "end": "864100"
  },
  {
    "text": "data sources from streaming data where you may use the Kinesis family a product",
    "start": "864100",
    "end": "870390"
  },
  {
    "text": "traditional relational databases and data warehouses where you may use",
    "start": "870390",
    "end": "875470"
  },
  {
    "text": "database migration services to integrate into the data like a lot of different",
    "start": "875470",
    "end": "880750"
  },
  {
    "text": "traditional sensor devices and file storage on prem where you may use things",
    "start": "880750",
    "end": "888100"
  },
  {
    "text": "like storage gateway to convert files to objects natively preserve their metadata",
    "start": "888100",
    "end": "893680"
  },
  {
    "text": "and all the data attributes and then things like lift and shifting from",
    "start": "893680",
    "end": "899530"
  },
  {
    "text": "on-premise hadoop clusters maybe you collect data offline like we have a",
    "start": "899530",
    "end": "905410"
  },
  {
    "text": "cruise ship customer who essentially put snowball devices on their ships at the",
    "start": "905410",
    "end": "911800"
  },
  {
    "text": "end of the journey ship those back AWS ingest that into the cloud and then they",
    "start": "911800",
    "end": "917830"
  },
  {
    "text": "can quickly analyze that data you know do model training so that they can make",
    "start": "917830",
    "end": "923500"
  },
  {
    "text": "their routing and essentially optimize their cost and",
    "start": "923500",
    "end": "929250"
  },
  {
    "text": "fuel economy and routing based on you know journey that was collected over the",
    "start": "929250",
    "end": "935400"
  },
  {
    "text": "course of our data that was collected over the course of a journey and then finally you're probably always if you're",
    "start": "935400",
    "end": "942240"
  },
  {
    "text": "on Prem today one are going to do hybrid environments so you can start to do Direct Connect virtual private cloud so",
    "start": "942240",
    "end": "950220"
  },
  {
    "text": "you can integrate the on-premise world and the cloud world together from an",
    "start": "950220",
    "end": "955800"
  },
  {
    "text": "analytics and our data Lake perspective other considerations people think of a",
    "start": "955800",
    "end": "963180"
  },
  {
    "text": "data Lake as one big s3 bucket when we talk about consolidating data but that's",
    "start": "963180",
    "end": "969150"
  },
  {
    "text": "not a good design pattern essentially for reasons of data governance you know",
    "start": "969150",
    "end": "976380"
  },
  {
    "text": "kind of controlling how data gets checked into your data lake so that your",
    "start": "976380",
    "end": "981870"
  },
  {
    "text": "data Lake is curated high quality data things like keeping all your raw source",
    "start": "981870",
    "end": "989040"
  },
  {
    "text": "data for historical purposes but then maybe having optimized data go into the",
    "start": "989040",
    "end": "994860"
  },
  {
    "text": "data like you want to probably start to segregate ingest buckets and",
    "start": "994860",
    "end": "1001040"
  },
  {
    "text": "transformation buckets and keep those separate and distinct from the actual data Lake itself it just makes life much",
    "start": "1001040",
    "end": "1008300"
  },
  {
    "text": "easier and it's a great design pattern that you can prepare the data keep raw sets and govern the data more",
    "start": "1008300",
    "end": "1016400"
  },
  {
    "text": "effectively than just dumping it all in the data lake and then trying to use it you probably want to think about how you",
    "start": "1016400",
    "end": "1024170"
  },
  {
    "text": "can aggregate smaller files particularly if you're doing streaming workloads into",
    "start": "1024170",
    "end": "1030290"
  },
  {
    "text": "larger files because that's more cost effective it's going to be more performant and it's easier to manage and",
    "start": "1030290",
    "end": "1038060"
  },
  {
    "text": "a lot of analytic tools will work better with larger files particularly if you",
    "start": "1038060",
    "end": "1043780"
  },
  {
    "text": "optimize the format just part of that aggregation process as well you're gonna",
    "start": "1043780",
    "end": "1049940"
  },
  {
    "text": "want to use the right tool to integrate the data in for example if you've got",
    "start": "1049940",
    "end": "1055490"
  },
  {
    "text": "file data you're gonna want to use something like Storage Gateway to use that and integrate that into the",
    "start": "1055490",
    "end": "1062330"
  },
  {
    "text": "data Lake and you're going to want to start to automate all these data pipelines using functions like lambdas",
    "start": "1062330",
    "end": "1069170"
  },
  {
    "text": "so you can really focus on building the pipeline once to accommodate your use",
    "start": "1069170",
    "end": "1075590"
  },
  {
    "text": "cases and then focus on how you actually derive value from the data rather than",
    "start": "1075590",
    "end": "1080660"
  },
  {
    "text": "all the orchestration stuff since that's really just a necessity so Kinesis is",
    "start": "1080660",
    "end": "1087410"
  },
  {
    "text": "really a great tool for essentially streaming data ingest into the data like",
    "start": "1087410",
    "end": "1092630"
  },
  {
    "text": "multiple flavors of it and when we talk about data Lake a lot of people think traditional structured semi-structured",
    "start": "1092630",
    "end": "1100850"
  },
  {
    "text": "data is really what defines a data Lake but more and more we're seeing use cases",
    "start": "1100850",
    "end": "1106280"
  },
  {
    "text": "where a data Lake starts to encompass unstructured data like voice and video",
    "start": "1106280",
    "end": "1112670"
  },
  {
    "text": "as well like we have you know several people that do video in just for",
    "start": "1112670",
    "end": "1118190"
  },
  {
    "text": "security analytics for sports analytics multimedia where they want to start to",
    "start": "1118190",
    "end": "1124130"
  },
  {
    "text": "automatically using machine learning tagging categorized data query on",
    "start": "1124130",
    "end": "1130160"
  },
  {
    "text": "different types of data that's also a data like use case and things like Kinesis video streams and some of our",
    "start": "1130160",
    "end": "1136850"
  },
  {
    "text": "richer media services like recognition and Lex you know start to play into that",
    "start": "1136850",
    "end": "1143420"
  },
  {
    "text": "video and voice domain for data Lakes as well so think broad when you're thinking about how you might build a data like",
    "start": "1143420",
    "end": "1150080"
  },
  {
    "text": "architecture to encompass broader sets of data than just traditional analytics",
    "start": "1150080",
    "end": "1157630"
  },
  {
    "text": "so if you start to think about building this pipeline this is an example of what it looks like where you start to have",
    "start": "1158740",
    "end": "1165370"
  },
  {
    "text": "different steps in your workflow from ingest to catalogue and transformation",
    "start": "1165370",
    "end": "1171050"
  },
  {
    "text": "to process in place but then maybe load into more optimized tools where you're",
    "start": "1171050",
    "end": "1177920"
  },
  {
    "text": "loading a curated data set into a tool like redshift for an enterprise data",
    "start": "1177920",
    "end": "1183170"
  },
  {
    "text": "warehouse query scenario and start to automate that with lambda functions is",
    "start": "1183170",
    "end": "1190150"
  },
  {
    "text": "another good design consider but ultimately we want to try to as much",
    "start": "1190150",
    "end": "1197600"
  },
  {
    "text": "as possible heavily leveraged process data in place because this means you're",
    "start": "1197600",
    "end": "1202760"
  },
  {
    "text": "gonna focus on using the functionality you want to get value out of data",
    "start": "1202760",
    "end": "1207910"
  },
  {
    "text": "without having to move data between different tools you bring the tools to the data so once you have it in s3 you",
    "start": "1207910",
    "end": "1215360"
  },
  {
    "text": "can start to use things as I mentioned earlier like Athena for ad hoc query redshift spectrum for data warehouse",
    "start": "1215360",
    "end": "1222350"
  },
  {
    "text": "type scenarios sage maker for model training at scale and AWS glue for",
    "start": "1222350",
    "end": "1230000"
  },
  {
    "text": "things like cataloging and transforming your data in place so another key",
    "start": "1230000",
    "end": "1236780"
  },
  {
    "text": "consideration in building a data Lake is we continue to try to add capabilities",
    "start": "1236780",
    "end": "1242840"
  },
  {
    "text": "to s3 that are really going to enhance what you can do with your data so last",
    "start": "1242840",
    "end": "1248930"
  },
  {
    "text": "year the first step in this was we introduced s3 select and glacier select",
    "start": "1248930",
    "end": "1254120"
  },
  {
    "text": "where essentially you can start to access your data in a way that's much",
    "start": "1254120",
    "end": "1259550"
  },
  {
    "text": "more optimized for analytics and data Lake style environments than just the",
    "start": "1259550",
    "end": "1264770"
  },
  {
    "text": "traditional get object and so essentially the motivation but behind",
    "start": "1264770",
    "end": "1270950"
  },
  {
    "text": "introducing this was if you think about a lot of analytic tools it's about scanning large volumes of data to get",
    "start": "1270950",
    "end": "1279170"
  },
  {
    "text": "specific parts of that data to actually execute on redshift spectrum which I'll",
    "start": "1279170",
    "end": "1285560"
  },
  {
    "text": "talk about a little bit more essentially is one of those scenarios where it's a data warehouse a lot of data scans",
    "start": "1285560",
    "end": "1291590"
  },
  {
    "text": "typically involved you would traditionally load that data into redshift execute there but with trim you",
    "start": "1291590",
    "end": "1300410"
  },
  {
    "text": "can now store data in s3 as tables and scan across those so when people ran",
    "start": "1300410",
    "end": "1307900"
  },
  {
    "text": "these type of jobs they queried a bunch of data and into a lot of that data on",
    "start": "1307900",
    "end": "1314480"
  },
  {
    "text": "the floor after they retrieved it from s3 so a lot of customers could only",
    "start": "1314480",
    "end": "1320600"
  },
  {
    "text": "potentially use about 10% of the data that they would actually retrieved from s3 at the end of the day and so",
    "start": "1320600",
    "end": "1328810"
  },
  {
    "text": "essentially what s3 select and glacier looked war was what if you could start",
    "start": "1328810",
    "end": "1335350"
  },
  {
    "text": "to intelligently have the storage layer based on things like sequel expressions",
    "start": "1335350",
    "end": "1341070"
  },
  {
    "text": "do the scanning of the data and the filtering of data in the storage layer",
    "start": "1341070",
    "end": "1346540"
  },
  {
    "text": "where the data lives and only return the results that are relevant to those sequel scans so you could start to do",
    "start": "1346540",
    "end": "1353500"
  },
  {
    "text": "things like from this object rather than do a get object have a pushdown where",
    "start": "1353500",
    "end": "1360160"
  },
  {
    "text": "you could issue a sequel statement saying select this object and then issue",
    "start": "1360160",
    "end": "1365520"
  },
  {
    "text": "from this object essentially sequel conditionals where you know maybe you",
    "start": "1365520",
    "end": "1372670"
  },
  {
    "text": "would say out of this object I only want the data that is going to be if it's",
    "start": "1372670",
    "end": "1378400"
  },
  {
    "text": "demographic data you know a user that is from this IP address or a range of IP",
    "start": "1378400",
    "end": "1386380"
  },
  {
    "text": "addresses push that down via sequel statements s3 will perform that scan",
    "start": "1386380",
    "end": "1391570"
  },
  {
    "text": "only return the results that are actually needed so if you think about that that's very different the storage",
    "start": "1391570",
    "end": "1398920"
  },
  {
    "text": "is now intelligent and so you can start to do things quicker more",
    "start": "1398920",
    "end": "1404350"
  },
  {
    "text": "cost-effectively a couple of examples we had an essay that went and for fun using",
    "start": "1404350",
    "end": "1410890"
  },
  {
    "text": "lambda wrote a server less MapReduce function using s3 select and comparing",
    "start": "1410890",
    "end": "1418270"
  },
  {
    "text": "that to what they would do without select and essentially for a given",
    "start": "1418270",
    "end": "1423880"
  },
  {
    "text": "function they were able to do a 2x faster at 1/5 of the cost",
    "start": "1423880",
    "end": "1430810"
  },
  {
    "text": "another one is with EMR 5.18 which is relatively new we now have a presto",
    "start": "1430810",
    "end": "1437140"
  },
  {
    "text": "connector for EMR that leverages s3 select where it essentially will do predicate push down and use s3 select to",
    "start": "1437140",
    "end": "1446590"
  },
  {
    "text": "pre-filter data so now if you consider a job before a job after it can be as much",
    "start": "1446590",
    "end": "1453550"
  },
  {
    "text": "as 5x faster you know essentially two-and-a-half percent the compute for a given process",
    "start": "1453550",
    "end": "1461540"
  },
  {
    "text": "job which means it's much more cost-effective and much quicker",
    "start": "1461540",
    "end": "1467420"
  },
  {
    "text": "choosing the right data formats another key consideration so essentially there's",
    "start": "1467420",
    "end": "1472700"
  },
  {
    "text": "no such thing as a best data format but if you're using CSV et svj song they're",
    "start": "1472700",
    "end": "1480500"
  },
  {
    "text": "easy to use they're ubiquitous but they're not efficient so if you start to consider moving to something like a",
    "start": "1480500",
    "end": "1486470"
  },
  {
    "text": "optimize call nor compress self-describing format it's often much",
    "start": "1486470",
    "end": "1492200"
  },
  {
    "text": "quicker to scan that data query that data as well as much more cost-effective",
    "start": "1492200",
    "end": "1497480"
  },
  {
    "text": "so you want to consider getting your data in the right format maybe in one of",
    "start": "1497480",
    "end": "1502520"
  },
  {
    "text": "these staging buckets before you actually load it in the data lake itself and here's an example not that you would",
    "start": "1502520",
    "end": "1509930"
  },
  {
    "text": "ever do you know select star statement but kind of an extreme use case where if",
    "start": "1509930",
    "end": "1516290"
  },
  {
    "text": "you had you know like a standard textile format you converted that to parque it's",
    "start": "1516290",
    "end": "1523550"
  },
  {
    "text": "you know essentially about 99 percent faster to query 99 plus percent cheaper",
    "start": "1523550",
    "end": "1531170"
  },
  {
    "text": "to query and uses about 1/8 of the space to store that in s3 so you can see what",
    "start": "1531170",
    "end": "1537620"
  },
  {
    "text": "a big win this is all around both from your raw storage cost as well as your scan and query cost and really getting",
    "start": "1537620",
    "end": "1547700"
  },
  {
    "text": "all this data prep done aggregating your data getting it ready to use is the",
    "start": "1547700",
    "end": "1553250"
  },
  {
    "text": "majority of the work that you need to do so in terms of speeding time to results",
    "start": "1553250",
    "end": "1558470"
  },
  {
    "text": "if you could start to transform that data catalog that data where it lives",
    "start": "1558470",
    "end": "1564190"
  },
  {
    "text": "natively that's a big win and that's why we developed glue which essentially is",
    "start": "1564190",
    "end": "1569570"
  },
  {
    "text": "serverless ETL and data catalog for data the lives in s3 so as you ingest data in",
    "start": "1569570",
    "end": "1575660"
  },
  {
    "text": "you trigger glue it'll crawl the data discover it discover schema and the data",
    "start": "1575660",
    "end": "1581260"
  },
  {
    "text": "build the data catalog that all these query and place analytic tools can use and then if you want to do things like",
    "start": "1581260",
    "end": "1588050"
  },
  {
    "text": "transform into part okay.you author a transformation job using you know typical Python spark and",
    "start": "1588050",
    "end": "1596500"
  },
  {
    "text": "essentially you can execute those ETL jobs server lessly as well in s3 other",
    "start": "1596500",
    "end": "1605929"
  },
  {
    "text": "query and place tools that are really important Athena's I talked about if you want to do ad hoc sequel exploration of",
    "start": "1605929",
    "end": "1611870"
  },
  {
    "text": "data Athena's array tool to do that essentially you go to the console you define tables of data in s3",
    "start": "1611870",
    "end": "1618860"
  },
  {
    "text": "you write sequel queries you execute them you only pay for the amount of data",
    "start": "1618860",
    "end": "1624919"
  },
  {
    "text": "you scanned so it's quick it's easy it's cost effective standard sequel so you",
    "start": "1624919",
    "end": "1630620"
  },
  {
    "text": "can start to use third-party tools that do things like data visualization JDBC",
    "start": "1630620",
    "end": "1636110"
  },
  {
    "text": "to integrate with this and now you've got a very easy way to start to explore visualize categorize your data redshift",
    "start": "1636110",
    "end": "1645379"
  },
  {
    "text": "spectrum is a great tool if you have warehouses on Prem you want to bend the",
    "start": "1645379",
    "end": "1650750"
  },
  {
    "text": "cost equation get that data into s3 you can spin up a redshift cluster for the",
    "start": "1650750",
    "end": "1656419"
  },
  {
    "text": "highest performance most critical most complex datasets but then keep all the",
    "start": "1656419",
    "end": "1662330"
  },
  {
    "text": "extended data in table format and s3 and query across both of these domains so it",
    "start": "1662330",
    "end": "1669889"
  },
  {
    "text": "really changes the equation and the amount of data you can consume from a",
    "start": "1669889",
    "end": "1675500"
  },
  {
    "text": "data warehouse environment and do it much more cost-effectively in a much more performant manner Sage Maker is a",
    "start": "1675500",
    "end": "1684049"
  },
  {
    "text": "service we released a year ago here it's really about how do you take data large",
    "start": "1684049",
    "end": "1690200"
  },
  {
    "text": "sets of data build and train models very quickly very cost-effectively get them",
    "start": "1690200",
    "end": "1695779"
  },
  {
    "text": "into production with a whole variety of AI ml tools so you can use sage maker",
    "start": "1695779",
    "end": "1702320"
  },
  {
    "text": "against state and s3 to do this in place as well so given this what are some of",
    "start": "1702320",
    "end": "1709669"
  },
  {
    "text": "the use cases we see customers using their data lakes for essentially warehouse modernization and bi",
    "start": "1709669",
    "end": "1717070"
  },
  {
    "text": "augmentation is a very common use case for people that want to migrate from on-prem do",
    "start": "1717070",
    "end": "1722130"
  },
  {
    "text": "AWS if you're doing things like big Hadoop clusters on-prem you're worried",
    "start": "1722130",
    "end": "1727230"
  },
  {
    "text": "about protecting your data you get a copy in s3 to protect your data and then",
    "start": "1727230",
    "end": "1732420"
  },
  {
    "text": "you can start to experiment do proof of values around that data maybe roll new",
    "start": "1732420",
    "end": "1738180"
  },
  {
    "text": "use cases into the cloud real time analytics using things like Kinesis",
    "start": "1738180",
    "end": "1743250"
  },
  {
    "text": "spark stream processing is big and we'll hear more about that from protract wise",
    "start": "1743250",
    "end": "1749340"
  },
  {
    "text": "and then bi and data exploration and AI and machine learning are also very",
    "start": "1749340",
    "end": "1754950"
  },
  {
    "text": "common so with that I'm going to turn it over to a real-world case study to talk",
    "start": "1754950",
    "end": "1760260"
  },
  {
    "text": "about some of these principles thank you",
    "start": "1760260",
    "end": "1765810"
  },
  {
    "text": "John so we're gonna go through a case study about how we ingest a massive amount of",
    "start": "1765810",
    "end": "1771720"
  },
  {
    "text": "data and how we use s3 as a data Lake we're gonna get into it I'm gonna start with a little background like how the",
    "start": "1771720",
    "end": "1778290"
  },
  {
    "text": "problem domain and the kind of data we work with data volume that kind of stuff but what I would like you to do is kind",
    "start": "1778290",
    "end": "1783720"
  },
  {
    "text": "of mentally match your own problem sets that you're working on and think about this in terms of like there's probably a",
    "start": "1783720",
    "end": "1789690"
  },
  {
    "text": "lot of projects that you have that seem utterly untenable for like a data volume perspective or maybe a number of you",
    "start": "1789690",
    "end": "1795600"
  },
  {
    "text": "here today are already working with what seems like untenable volumes of data what we're gonna show you is how to take",
    "start": "1795600",
    "end": "1801990"
  },
  {
    "text": "a very progressive view into making a data Lake really work well for you",
    "start": "1801990",
    "end": "1807600"
  },
  {
    "text": "that's fully interactive fully searchable that kind of stuff so a little background protect wise we're",
    "start": "1807600",
    "end": "1813090"
  },
  {
    "text": "startup we're young company we've been on the market for about three and a half years we are what we would call and",
    "start": "1813090",
    "end": "1818430"
  },
  {
    "text": "network detection and response basically we do threat detection of network data that is observed inside of our customers",
    "start": "1818430",
    "end": "1825720"
  },
  {
    "text": "networks most of whom are very large organization so think a large enterprise fortune 2,000 number of mid-market",
    "start": "1825720",
    "end": "1831200"
  },
  {
    "text": "customers as well but we passively get a copy of that data and ship it to the",
    "start": "1831200",
    "end": "1836640"
  },
  {
    "text": "cloud we compress we optimize streaming to the cloud so think like packet capture data that's coming out of raw",
    "start": "1836640",
    "end": "1843000"
  },
  {
    "text": "network streams layer 2 data and hire think if you are familiar with the OSI model I also think about features you",
    "start": "1843000",
    "end": "1850230"
  },
  {
    "text": "extract from that data and if you've been working with data for a long time you know that data about data is off",
    "start": "1850230",
    "end": "1856050"
  },
  {
    "text": "more voluminous than the source data that you started with and you're really tempted to want to work with that but it's it's a big challenge right and so",
    "start": "1856050",
    "end": "1863520"
  },
  {
    "text": "today um inside a AWS we ingest a ton of data per day we do about half a trillion",
    "start": "1863520",
    "end": "1871230"
  },
  {
    "text": "data points per hour and we ingest that data Cole that's it down that's at the",
    "start": "1871230",
    "end": "1876990"
  },
  {
    "text": "top of the funnel and we Compaq that down into a data Lake that is then used for not just like automative kind of",
    "start": "1876990",
    "end": "1883500"
  },
  {
    "text": "threat detection but it's also used for interactive querying so our customers",
    "start": "1883500",
    "end": "1889290"
  },
  {
    "text": "can actually hit this data Lake so we're opening this up to the outside world and you've got to think of things like how",
    "start": "1889290",
    "end": "1895230"
  },
  {
    "text": "many IP addresses do you see how many network communications do you see how many files how many URLs how many certificates how many all this",
    "start": "1895230",
    "end": "1901680"
  },
  {
    "text": "stuff that's it's an network traffic right go into that same data Lake that is spans into very very large numbers",
    "start": "1901680",
    "end": "1909960"
  },
  {
    "text": "and allow our users to just ask really open-ended Lucene style questions now",
    "start": "1909960",
    "end": "1916170"
  },
  {
    "text": "we're gonna get into the meat of some of this technology but remember this is a 200 level introduction we've been asked",
    "start": "1916170",
    "end": "1922500"
  },
  {
    "text": "to keep it at a high level if you want to dig deep come talk to us afterwards this rabbit hole goes down pretty far",
    "start": "1922500",
    "end": "1928830"
  },
  {
    "text": "it's a lot of fun but you know we got a system that does you know tens of millions of transactions and messages a",
    "start": "1928830",
    "end": "1935970"
  },
  {
    "text": "second it's a lot of data and in a previous era this would have been really hard and like at a Hadoop type of world",
    "start": "1935970",
    "end": "1942480"
  },
  {
    "text": "it wouldn't worked at all however with new data technology sitting",
    "start": "1942480",
    "end": "1947580"
  },
  {
    "text": "on top of st 3 we can do things that we were never able to do before and it could be faster and it can be",
    "start": "1947580",
    "end": "1954360"
  },
  {
    "text": "cheaper and it's just a lot easier than it used to be so when we were designing",
    "start": "1954360",
    "end": "1959550"
  },
  {
    "text": "this some core strategy and I would like you to think about how you want to",
    "start": "1959550",
    "end": "1965160"
  },
  {
    "text": "problem-solve as a technologist inside of your own company core strategy are",
    "start": "1965160",
    "end": "1970410"
  },
  {
    "text": "things like you know time to market you know obviously that's important for a startup but it's important for anybody in any business group to think about",
    "start": "1970410",
    "end": "1976470"
  },
  {
    "text": "themselves is that they can't be measured really on their ambition or goals in their hopes you really only get",
    "start": "1976470",
    "end": "1982140"
  },
  {
    "text": "measured by what you actually end up accomplishing right so think in terms of being able to move quickly think of also",
    "start": "1982140",
    "end": "1988140"
  },
  {
    "text": "in terms of the cost of goods so your cogs innovating on that front we're",
    "start": "1988140",
    "end": "1993149"
  },
  {
    "text": "talking about massive cost reduction here just so just a moment ago being able to be innovative with regard to the",
    "start": "1993149",
    "end": "1999720"
  },
  {
    "text": "basic business costs of being able to exploit the data that you now are in charge of that is core to your strategy",
    "start": "1999720",
    "end": "2006230"
  },
  {
    "text": "you know that's something you've got to adopt as a first principle you also want",
    "start": "2006230",
    "end": "2011629"
  },
  {
    "text": "to take a very evolutionary architecture so you don't want to solve all these things upfront you want to take a phased",
    "start": "2011629",
    "end": "2016879"
  },
  {
    "text": "approach into building just a piece at a time which is good we could only handle so much at once we have to have a clear mind about where we want to go but just",
    "start": "2016879",
    "end": "2024139"
  },
  {
    "text": "build this a piece at a time and then provide a really fast feedback loop that allows continuous learning so you're watching your progression through",
    "start": "2024139",
    "end": "2030590"
  },
  {
    "text": "all of these stages being able to go from a traditional old-school Big Data type of system to something that is",
    "start": "2030590",
    "end": "2037330"
  },
  {
    "text": "maybe best described as post Big Data is a terrible term but it's the worst",
    "start": "2037330",
    "end": "2042440"
  },
  {
    "text": "except for all the other terms and then feed that into how you think about your own technology and how you iterate have",
    "start": "2042440",
    "end": "2049280"
  },
  {
    "text": "a sense of continuous learning and attach that to the fact that just like time to market is foundational human",
    "start": "2049280",
    "end": "2056030"
  },
  {
    "text": "resource time is the most limited resource we have so when protect wise was was was",
    "start": "2056030",
    "end": "2062569"
  },
  {
    "text": "tackling this we looked at basically how we source our data how it flows through this pipeline how we index and how we",
    "start": "2062569",
    "end": "2069770"
  },
  {
    "text": "store this in a way that can be really active so let me paint that picture for you a little bit and then we're gonna",
    "start": "2069770",
    "end": "2076158"
  },
  {
    "text": "get into the bits and the bytes of how we delivered this technology at the end of the day what we do is we give our",
    "start": "2076159",
    "end": "2081980"
  },
  {
    "text": "customers these lightweight software sensors they're free they can have as many as they want which means we can",
    "start": "2081980",
    "end": "2087169"
  },
  {
    "text": "really open the floodgates on this but what they do is they do packet capture they compress they optimize to do some",
    "start": "2087169",
    "end": "2093740"
  },
  {
    "text": "deep packet inspection so packet by packet feature extraction and they come ship that to the cloud which is AWS",
    "start": "2093740",
    "end": "2099680"
  },
  {
    "text": "obviously and then we run it through a suite of threat detection capabilities right that's basic transactional type of stuff that's good but it's still like",
    "start": "2099680",
    "end": "2106310"
  },
  {
    "text": "you know tens of millions of transactions a second and then we what's more interesting is that we store that",
    "start": "2106310",
    "end": "2114440"
  },
  {
    "text": "data for an unlimited amount of time in fact our standard offerings a year retention when you think of this this is",
    "start": "2114440",
    "end": "2119450"
  },
  {
    "text": "an obscene amount of data right what super manageable this is the cool part this is what I want you to take away",
    "start": "2119450",
    "end": "2125630"
  },
  {
    "text": "from this and the core element behind all this kind of strategy was our",
    "start": "2125630",
    "end": "2130640"
  },
  {
    "text": "willingness to bet on AWS as roadmap to say you know we are not in an infrastructure business you know we",
    "start": "2130640",
    "end": "2136490"
  },
  {
    "text": "would talk to certain investors and other business leaders and you know people in the industry they're like what that much data you need to be building",
    "start": "2136490",
    "end": "2142640"
  },
  {
    "text": "your own data center right I mean I'm probably preaching to the choir you're here at AWS for a reason that is nonsense that is a previous eras way of",
    "start": "2142640",
    "end": "2149510"
  },
  {
    "text": "thinking innovation time to market cogs innovation specifically these types of",
    "start": "2149510",
    "end": "2155300"
  },
  {
    "text": "things are the core challenge being good at them means you get to win and so we're gonna walk you through this a",
    "start": "2155300",
    "end": "2160970"
  },
  {
    "text": "little bit but I do want to talk a little bit about this cogs innovation and then we'll move on to the tech the",
    "start": "2160970",
    "end": "2167480"
  },
  {
    "text": "rate of innovation is king that is the number one thing your ability to create",
    "start": "2167480",
    "end": "2172670"
  },
  {
    "text": "new technology and easily introduce that new technology this haystack will define",
    "start": "2172670",
    "end": "2178250"
  },
  {
    "text": "your ability to succeed you cannot want your way into this you can simply bill bill bill get faster fail quickly",
    "start": "2178250",
    "end": "2184330"
  },
  {
    "text": "tighten that loop as much as possible and not pretend that you're gonna be able to forecast us all what we were",
    "start": "2184330",
    "end": "2190400"
  },
  {
    "text": "able to find though is that we had very reliable outcomes when it came to the effectiveness of the system but",
    "start": "2190400",
    "end": "2196310"
  },
  {
    "text": "specifically with regard to not needing to boil the ocean is this concept that",
    "start": "2196310",
    "end": "2201620"
  },
  {
    "text": "you can design technology that is good at surgically getting the fast paths",
    "start": "2201620",
    "end": "2207230"
  },
  {
    "text": "through the data you don't need to solve for like if you have hundreds of trillions of data points you will need",
    "start": "2207230",
    "end": "2212240"
  },
  {
    "text": "to solve for all that upfront you need to find a way to store it and then you need to find a way to get fast paths in",
    "start": "2212240",
    "end": "2218300"
  },
  {
    "text": "to ask the really basic questions pre compute the answers the most common questions and never scan don't scan you",
    "start": "2218300",
    "end": "2226490"
  },
  {
    "text": "know what you're gonna see right now is effectively a system that trades transactional costs for i/o cost because",
    "start": "2226490",
    "end": "2234440"
  },
  {
    "text": "IO is something we can get really good at and from the very beginning we were most enthusiastic about s3 as a",
    "start": "2234440",
    "end": "2241640"
  },
  {
    "text": "technology which is like from Amazon's perspective they they're like we have such cool stuff why are you focusing on",
    "start": "2241640",
    "end": "2247340"
  },
  {
    "text": "the s3 IO and the fundamentals at the end of the day are going to define your ability to succeed here and if you get",
    "start": "2247340",
    "end": "2253820"
  },
  {
    "text": "really good at getting away from transit channel costs like per transaction costs and you get good two at i/o costs you",
    "start": "2253820",
    "end": "2261469"
  },
  {
    "text": "changed the model on which you're able to deliver really large systems which means you can have massive amounts of",
    "start": "2261469",
    "end": "2267199"
  },
  {
    "text": "data it's a lot more affordable specifically we're going to cite this our costs of delivering the system",
    "start": "2267199",
    "end": "2273619"
  },
  {
    "text": "started out with using Cassandra and solar which was data stacks Enterprise",
    "start": "2273619",
    "end": "2280009"
  },
  {
    "text": "for us fantastic set of technology great partner running on like ec2 instances",
    "start": "2280009",
    "end": "2285499"
  },
  {
    "text": "think of like what you do when you have hundreds of terabytes of indexes right that kind of a system if you work with",
    "start": "2285499",
    "end": "2292279"
  },
  {
    "text": "this works really well when it fits into memory but who wants to buy a several hundred terabytes of RAM right you're",
    "start": "2292279",
    "end": "2297559"
  },
  {
    "text": "not going to do that that doesn't make any sense so what you do is you throw it onto SSDs and you get really good at scanning right so previous way of doing things",
    "start": "2297559",
    "end": "2303759"
  },
  {
    "text": "what we were able to do is get that moved to an s3 to say let's take that",
    "start": "2303759",
    "end": "2309079"
  },
  {
    "text": "compute cost and I just make it a storage and i/o cost by doing so we were able to actually lower our AWS cost",
    "start": "2309079",
    "end": "2316059"
  },
  {
    "text": "inside a AWS was already pretty approachable by 95 percent that is one",
    "start": "2316059",
    "end": "2321229"
  },
  {
    "text": "twentieth of its previous costs building all that infrastructure and owning the",
    "start": "2321229",
    "end": "2327589"
  },
  {
    "text": "backend that provides that i/o on your own is crazy building your own infrastructure puts all this strategy at",
    "start": "2327589",
    "end": "2334939"
  },
  {
    "text": "fundamental risk so what we're gonna do now I'm gonna hand this over to Josh Hollander he's a director of engineering",
    "start": "2334939",
    "end": "2340999"
  },
  {
    "text": "he's going to talk to you a little bit about the technology we built I want you to walk away with this one understanding",
    "start": "2340999",
    "end": "2349509"
  },
  {
    "text": "high performance probabilistic data lakes are the future of analytics for us",
    "start": "2349509",
    "end": "2355099"
  },
  {
    "text": "security analytics what you're going to see today is an exploration of using",
    "start": "2355099",
    "end": "2360529"
  },
  {
    "text": "basic open source technology correctly aligned in front of s3 took a problem",
    "start": "2360529",
    "end": "2366289"
  },
  {
    "text": "that was really really difficult and made it embarrassingly parallel and easy",
    "start": "2366289",
    "end": "2371479"
  },
  {
    "text": "to execute so let's get into this Josh",
    "start": "2371479",
    "end": "2376809"
  },
  {
    "text": "thanks Jean hi everyone good evening I'll just really briefly tell you a",
    "start": "2377650",
    "end": "2382699"
  },
  {
    "text": "little bit more about what Explorer is what the challenges that we're trying to solve it is a way to interactively search our",
    "start": "2382699",
    "end": "2390690"
  },
  {
    "text": "customers entire network timelines so as Jean said we're capturing massive amounts of packet data we're extracting",
    "start": "2390690",
    "end": "2396359"
  },
  {
    "text": "metadata from those and storing it and allowing the customer to essentially",
    "start": "2396359",
    "end": "2402000"
  },
  {
    "text": "query anything that they can think of so here's an example query with the which they could use for exploration another",
    "start": "2402000",
    "end": "2409230"
  },
  {
    "text": "possible use case is to hunt for threats so they may have been made aware of a threat that might have come into their",
    "start": "2409230",
    "end": "2417270"
  },
  {
    "text": "network and they can use Explorer to look for instances of that threat that have not already been detected and then",
    "start": "2417270",
    "end": "2423930"
  },
  {
    "text": "finally the the last thing is to be able to visualize these threats just hunting for them getting raw data isn't isn't",
    "start": "2423930",
    "end": "2429810"
  },
  {
    "text": "very helpful to a security analyst who isn't necessarily highly technical this gives them the ability to work with the",
    "start": "2429810",
    "end": "2436560"
  },
  {
    "text": "data through the tool and the tool looks a little something like this it has query data listing and then the",
    "start": "2436560",
    "end": "2444660"
  },
  {
    "text": "graph which is really where the meat of it is where they can explore the graph of the data that they've queried so",
    "start": "2444660",
    "end": "2451349"
  },
  {
    "text": "before Explorer we were storing data in Apache Cassandra and we were storing it",
    "start": "2451349",
    "end": "2456750"
  },
  {
    "text": "for key value lookups for when threat detection would come in we would use Cassandra to do to quickly grab more",
    "start": "2456750",
    "end": "2462990"
  },
  {
    "text": "metadata about the detection that an analyst might be working with in order",
    "start": "2462990",
    "end": "2468810"
  },
  {
    "text": "to deliver a solution like Explorer we needed to make that data searchable instead of just key value lookups we",
    "start": "2468810",
    "end": "2473849"
  },
  {
    "text": "need to make we need to add sorting so that they could say I want to order my data by say the biggest connection or",
    "start": "2473849",
    "end": "2479910"
  },
  {
    "text": "the the most bytes downloaded or the largest file in the connection we're conversely the smallest and then also we",
    "start": "2479910",
    "end": "2486720"
  },
  {
    "text": "wanted to include if you're familiar with search technology like we've seen or solar fastenings that users could do",
    "start": "2486720",
    "end": "2494520"
  },
  {
    "text": "aggregations on demand and and explore their data in a more drill down format",
    "start": "2494520",
    "end": "2500240"
  },
  {
    "text": "so we chose being good big standard users and having a decent mount of solar",
    "start": "2500240",
    "end": "2505560"
  },
  {
    "text": "experience we chose data stacks Enterprise Edition which allowed us to work on the existing data in Cassandra",
    "start": "2505560",
    "end": "2513960"
  },
  {
    "text": "and make it searchable with a fairly low effort one of the reasons that we wanted",
    "start": "2513960",
    "end": "2519740"
  },
  {
    "text": "with Cassandra at the time was that our data is not immutable it a network connection begins and has a certain time",
    "start": "2519740",
    "end": "2526100"
  },
  {
    "text": "a lifetime and data is accumulated findings are made during that lifetime and then a connection ends and at that",
    "start": "2526100",
    "end": "2532010"
  },
  {
    "text": "point you know we then it becomes immutable it becomes essentially cold",
    "start": "2532010",
    "end": "2537200"
  },
  {
    "text": "and dead but we didn't want to wait till that data happens to make it searchable for the user because sometimes connections can last minutes or so",
    "start": "2537200",
    "end": "2543920"
  },
  {
    "text": "excuse me seconds sometimes minutes sometimes hours sometimes months so we have a large variability there but most",
    "start": "2543920",
    "end": "2550610"
  },
  {
    "text": "importantly as Gina said we want to optimize time to market and data stacks let us do that by essentially adding",
    "start": "2550610",
    "end": "2556580"
  },
  {
    "text": "indexes to the data that we'd already accumulated this this solution we're great in terms of getting us to market",
    "start": "2556580",
    "end": "2562880"
  },
  {
    "text": "fast and it was a it's a it's a robust product however there was a lot of operational issues it is a JVM based",
    "start": "2562880",
    "end": "2571520"
  },
  {
    "text": "product so we had to deal with garbage collection tuning right off the bat also",
    "start": "2571520",
    "end": "2577580"
  },
  {
    "text": "if anybody familiar with solar its tuning and working with its indices and caches is kind of a black art and so",
    "start": "2577580",
    "end": "2585140"
  },
  {
    "text": "that expended it took a lot of operational overhead and data retention we have to provision ec2 instances and",
    "start": "2585140",
    "end": "2592250"
  },
  {
    "text": "run large drives to accumulate this large volume of data and retention became a big problem the other side of",
    "start": "2592250",
    "end": "2598730"
  },
  {
    "text": "the equation is it costed you know we have to pay licenses like we said we have to have large amount of instances",
    "start": "2598730",
    "end": "2604520"
  },
  {
    "text": "of storage so the cost became exorbitant quickly and then the operational",
    "start": "2604520",
    "end": "2610100"
  },
  {
    "text": "overhead adds a cost in engineering time so we started looking around for",
    "start": "2610100",
    "end": "2615350"
  },
  {
    "text": "alternatives the possibilities are what we could mimic data stacks by building",
    "start": "2615350",
    "end": "2622220"
  },
  {
    "text": "something with Cassandra and elasticsearch but the complexity of that was deemed to be even greater than using",
    "start": "2622220",
    "end": "2627350"
  },
  {
    "text": "DSC which I think knowing knowing what I know about elasticsearch and cassandra separately that I would not want to work",
    "start": "2627350",
    "end": "2632690"
  },
  {
    "text": "as two together we looked at no sequel solutions of course no sequel kind of",
    "start": "2632690",
    "end": "2638300"
  },
  {
    "text": "means no index in our industry so not really helpful when you are doing searching we started looking at data",
    "start": "2638300",
    "end": "2644060"
  },
  {
    "text": "like solutions you know they're cost-effective and they can do these SQL queries so we've looked at things like hive presto this is before Athena book",
    "start": "2644060",
    "end": "2652550"
  },
  {
    "text": "we of considered Athena at the time as well what we really want was the cost and scale of s3 based warehouse or data Lake",
    "start": "2652550",
    "end": "2660329"
  },
  {
    "text": "but with the query performance that we got out of DSC so we decided you know",
    "start": "2660329",
    "end": "2666880"
  },
  {
    "text": "using the building blocks that that AWS gives you that we could build something that was a hybrid of custom plus",
    "start": "2666880",
    "end": "2672189"
  },
  {
    "text": "off-the-shelf and we arrived at a solution that uses kind of I would say the the holy triangle of of the three it",
    "start": "2672189",
    "end": "2679779"
  },
  {
    "text": "looks a lot like presto and hive but it leverages SPARC s3 and data stacks and",
    "start": "2679779",
    "end": "2686609"
  },
  {
    "text": "the way it does this is we stream our network data off of Kafka using spark",
    "start": "2686609",
    "end": "2695049"
  },
  {
    "text": "and we write it to Cassandra to handle the the mutable nature of some of this",
    "start": "2695049",
    "end": "2700959"
  },
  {
    "text": "data as that data cools or becomes cold we will then transfer use spark to again",
    "start": "2700959",
    "end": "2707049"
  },
  {
    "text": "transfer it to s3 for what we called cold storage this lets us leverage",
    "start": "2707049",
    "end": "2712989"
  },
  {
    "text": "existing tools to handle them the mutability problem and get our data in s3 where we can really leverage the the",
    "start": "2712989",
    "end": "2720809"
  },
  {
    "text": "cost-effectiveness and the performance what we did though was we combined them",
    "start": "2720809",
    "end": "2726339"
  },
  {
    "text": "with a secret ingredient which was about trying to improve on the query performance problem and that secret",
    "start": "2726339",
    "end": "2732339"
  },
  {
    "text": "ingredient is probabilistic meta indexes which sounds like fancy term that I made up because I did they're really just",
    "start": "2732339",
    "end": "2739899"
  },
  {
    "text": "bloom filters so hive has this feature",
    "start": "2739899",
    "end": "2745479"
  },
  {
    "text": "ready bloom filters people aren't super familiar with I wasn't familiar when I first started learning about hive but",
    "start": "2745479",
    "end": "2751449"
  },
  {
    "text": "ork has support for bloom filters on a black level well black level is really targeted at HDFS when you fetch file",
    "start": "2751449",
    "end": "2759219"
  },
  {
    "text": "from HDFS you can split it into certain you can split a splittable format like work or parquet into blocks orc has",
    "start": "2759219",
    "end": "2765549"
  },
  {
    "text": "bloom filters that says that are designed to store data about the particular blocks that you're reading",
    "start": "2765549",
    "end": "2771069"
  },
  {
    "text": "and this let's this lets a Hadoop query",
    "start": "2771069",
    "end": "2776289"
  },
  {
    "text": "engine like hive skip certain blocks that it doesn't need to read well that doesn't really work for s3 where you're",
    "start": "2776289",
    "end": "2781449"
  },
  {
    "text": "doing gets on large files so what we realized is our customization was well",
    "start": "2781449",
    "end": "2786870"
  },
  {
    "text": "just push the bloom filters further up in the system and say can allow our",
    "start": "2786870",
    "end": "2792060"
  },
  {
    "text": "system to convict or tell us which files are interesting for a query before we",
    "start": "2792060",
    "end": "2797160"
  },
  {
    "text": "even go fetch them so we came up with the idea of what we call a searchable",
    "start": "2797160",
    "end": "2803040"
  },
  {
    "text": "bloom filter or an indexed bloom filter I'm very familiar with search",
    "start": "2803040",
    "end": "2809340"
  },
  {
    "text": "technologies like solar and leucine an elastic search so if you're familiar with them this might look kind of this",
    "start": "2809340",
    "end": "2816210"
  },
  {
    "text": "might be familiar to you basically what we realized is we could take a bloom filter and treat it like terms in a",
    "start": "2816210",
    "end": "2822570"
  },
  {
    "text": "document an inverted index you store terms or words for searching and then",
    "start": "2822570",
    "end": "2827880"
  },
  {
    "text": "the documents that those terms correspond to but we realize we could we could treat the bits in a bloom filter",
    "start": "2827880",
    "end": "2833850"
  },
  {
    "text": "that were set as indexes into an array as terms that say what documents",
    "start": "2833850",
    "end": "2840090"
  },
  {
    "text": "correspond to them so we have an index that looks something like this where you could take an IP address and say these are the values the bits that need to be",
    "start": "2840090",
    "end": "2846480"
  },
  {
    "text": "indexed in the filter that will go to a particular doc ID and this lets us ask of our query system what files may have",
    "start": "2846480",
    "end": "2854010"
  },
  {
    "text": "contained this IP address so in order to",
    "start": "2854010",
    "end": "2859770"
  },
  {
    "text": "build a system like this we didn't want to have to build everything from scratch right we want to avoid inventing or",
    "start": "2859770",
    "end": "2865140"
  },
  {
    "text": "reinventing wheels that had already been invented spark has a really great rich set of",
    "start": "2865140",
    "end": "2871080"
  },
  {
    "text": "api's that allow you to interact with the spark sequel data frame and data set",
    "start": "2871080",
    "end": "2877130"
  },
  {
    "text": "api so what we decided to do is we would use sparks pruned filtered scan API to",
    "start": "2877130",
    "end": "2883410"
  },
  {
    "text": "let spark handle the query parsing the query optimization then hand off the the",
    "start": "2883410",
    "end": "2890160"
  },
  {
    "text": "optimized query to us via ninh it's push down filters API which says here's the",
    "start": "2890160",
    "end": "2895650"
  },
  {
    "text": "filters that I would like to push down to your storage layer that you you can run so this is an API that's usually",
    "start": "2895650",
    "end": "2901410"
  },
  {
    "text": "used for implementing additional storage solutions for spark that's there it's",
    "start": "2901410",
    "end": "2907740"
  },
  {
    "text": "their plug-in API for storage solutions so we use that to intercept those queries go talk to our meta store and",
    "start": "2907740",
    "end": "2914070"
  },
  {
    "text": "say hey what files in s3 might match this and what files would and so that we could eliminate them",
    "start": "2914070",
    "end": "2920010"
  },
  {
    "text": "and we hand off the resulting files back to what bar calls its file scanner idd which is what does the meat of the work",
    "start": "2920010",
    "end": "2926520"
  },
  {
    "text": "it does the fetching from s3 the filtering the sorting and any other",
    "start": "2926520",
    "end": "2931830"
  },
  {
    "text": "particular pieces that are involved in that query UDF's and things like that",
    "start": "2931830",
    "end": "2937700"
  },
  {
    "text": "this way we could basically in this sandwich between step and one and three we could get a lot of a lot of free tech",
    "start": "2937700",
    "end": "2945810"
  },
  {
    "text": "basically within an allow us to insert our little little bit of secret sauce without doing a huge amount of work the",
    "start": "2945810",
    "end": "2952950"
  },
  {
    "text": "result as I said as a kind of hive like query system but with those problematic meta indexes the outcome is that we got",
    "start": "2952950",
    "end": "2961590"
  },
  {
    "text": "a 95 percent cost reduction over the previous system but without sacrificing any real amount of performance what the",
    "start": "2961590",
    "end": "2971580"
  },
  {
    "text": "reason we were able to do this is cuz we minimize the amount of data that these queries need to pull from s3 we're no longer scanning for common queries we're",
    "start": "2971580",
    "end": "2978600"
  },
  {
    "text": "not scanning the entire file system well object store and this led to a p95",
    "start": "2978600",
    "end": "2984270"
  },
  {
    "text": "query time that was in seconds which is competitive with what data stacks had and a key value lookup time that was",
    "start": "2984270",
    "end": "2990540"
  },
  {
    "text": "sub-second that's right we do key value lookups against a data like it seems nuts but when you have a huge amount of",
    "start": "2990540",
    "end": "2996930"
  },
  {
    "text": "data that you're ingesting and writing and in a very tiny fraction that the customer is actually going to have time",
    "start": "2996930",
    "end": "3002690"
  },
  {
    "text": "to look at there's no way they could ever look at all of our data we didn't want to expend a huge amount of effort",
    "start": "3002690",
    "end": "3008270"
  },
  {
    "text": "keeping a key value store like Cassandra around for this data that that has a",
    "start": "3008270",
    "end": "3013640"
  },
  {
    "text": "high cost so we figured we could deliver that same performance with the data like at a much lower cost",
    "start": "3013640",
    "end": "3019520"
  },
  {
    "text": "the other great thing is John's talked about is separating compute and storage means that our storage scale out is now",
    "start": "3019520",
    "end": "3025340"
  },
  {
    "text": "is zero nearly zero effort it pretty much is zero effort right has three handles it for you and then on top of",
    "start": "3025340",
    "end": "3031820"
  },
  {
    "text": "that our query layer can be easily expanded to handle customer query volume without having to also expand storage or",
    "start": "3031820",
    "end": "3038750"
  },
  {
    "text": "vice versa so how do AWS services specifically enable",
    "start": "3038750",
    "end": "3044000"
  },
  {
    "text": "Explorer I talked about it a little bit it's basically s3 it's all about s3 it's",
    "start": "3044000",
    "end": "3049370"
  },
  {
    "text": "what makes Explorer work almost don't even need to say more it's cheap it's reliable scale it's fast it's",
    "start": "3049370",
    "end": "3056780"
  },
  {
    "text": "it's wonderful another technology we use is EMR when we first decided to build",
    "start": "3056780",
    "end": "3063770"
  },
  {
    "text": "this system we mentioned that we wanted to use spark to DevOps and DevOps said well we don't know how to operate spark so this is not not a good idea we just",
    "start": "3063770",
    "end": "3071360"
  },
  {
    "text": "leave that up test to AWS EMR does a huge portion that worked for us it gives",
    "start": "3071360",
    "end": "3077030"
  },
  {
    "text": "us essentially kind of spark on demand it enables easy auto scaling of both our",
    "start": "3077030",
    "end": "3082190"
  },
  {
    "text": "ingest and query clusters and it makes it simple for us the future so we talked",
    "start": "3082190",
    "end": "3088850"
  },
  {
    "text": "about AWS roadmap being foundational to our business one feature that John",
    "start": "3088850",
    "end": "3094280"
  },
  {
    "text": "talked about is that really coincides with what we're doing is s3 select combining our probabilistic meta store",
    "start": "3094280",
    "end": "3100940"
  },
  {
    "text": "with our with s3 and then using s3 select to further eliminate rows that we",
    "start": "3100940",
    "end": "3106370"
  },
  {
    "text": "don't need back from s3 is going to give us huge performance increase and cost",
    "start": "3106370",
    "end": "3111710"
  },
  {
    "text": "decrease and we're currently working on integrating that into our system now that s3 has park' support or s3",
    "start": "3111710",
    "end": "3118400"
  },
  {
    "text": "select has Parque sport rather leveraging key nieces firehose this is a",
    "start": "3118400",
    "end": "3123650"
  },
  {
    "text": "product we've really taken an interest in lately to simplify our ingest pipeline get the raw data into s3 right",
    "start": "3123650",
    "end": "3130460"
  },
  {
    "text": "away and then we can work on ingest down from that simplifying the complexity of",
    "start": "3130460",
    "end": "3136010"
  },
  {
    "text": "running a Kafka and SPARC pipeline is something that we're really excited about and then going more serverless",
    "start": "3136010",
    "end": "3142100"
  },
  {
    "text": "so once you get your data in s3 you can rely on lambdas to do a lot more work also we were thinking about relying on",
    "start": "3142100",
    "end": "3149540"
  },
  {
    "text": "moving our meta store off of DSC and onto RDS as well as trying to reduce",
    "start": "3149540",
    "end": "3156110"
  },
  {
    "text": "query compute and ingest server maintenance so EMR is is SPARC on demand",
    "start": "3156110",
    "end": "3162140"
  },
  {
    "text": "or presto on demand or high dupe on demand but it is not serverless you still have servers that you need to keep",
    "start": "3162140",
    "end": "3167450"
  },
  {
    "text": "an eye on and manage so lessons learned we took a features first approach and it",
    "start": "3167450",
    "end": "3175040"
  },
  {
    "text": "worked really well for us we got to market really quickly and it enabled us time while customers enjoyed the feat work on making the feature cheaper and",
    "start": "3175040",
    "end": "3181430"
  },
  {
    "text": "more efficient a hybrid of build and buy is effective you don't always need the",
    "start": "3181430",
    "end": "3186589"
  },
  {
    "text": "solution to you sometimes you just need the building blocks like s3 like EMR",
    "start": "3186589",
    "end": "3192009"
  },
  {
    "text": "excuse me open source ecosystem is rich how do I go back the open source ecosystem is rich you know spark has all",
    "start": "3192009",
    "end": "3198829"
  },
  {
    "text": "the features we needed we just needed adder a little bit of special sauce to it AWS tools are at flexible building",
    "start": "3198829",
    "end": "3204859"
  },
  {
    "text": "blocks so s3 EMR firehose Kinesis they really give you all the the basics that",
    "start": "3204859",
    "end": "3211249"
  },
  {
    "text": "you need to build these systems and that the AWS roadmap leads an innovation we're very excited about the roadmap for",
    "start": "3211249",
    "end": "3218119"
  },
  {
    "text": "things like s3 select that we're going to be using very soon in the end a high-speed probabilistic data Lake is",
    "start": "3218119",
    "end": "3224869"
  },
  {
    "text": "fundamental for our business and I think it's that this kind of thing is the future for everyone so thank you",
    "start": "3224869",
    "end": "3232900"
  },
  {
    "text": "okay great and thanks for joining us and you know we'll open it up for her",
    "start": "3239319",
    "end": "3246140"
  },
  {
    "text": "questions if you have any anybody I think we've got some mics set up or you",
    "start": "3246140",
    "end": "3252680"
  },
  {
    "text": "can uh where's the mics come up to the stage whatever suits your fancy you know",
    "start": "3252680",
    "end": "3262459"
  },
  {
    "text": "in the data streaming Oh in a data streaming scenario if we wanted to use",
    "start": "3262459",
    "end": "3268549"
  },
  {
    "text": "s3 as a store obviously if you're doing tons and tons of puts it's probably an anti-pattern right so I've been using dynamodb and s3",
    "start": "3268549",
    "end": "3276920"
  },
  {
    "text": "so DiMeo DB for the fast stuff that's just you know constant puts and that's three from work batches is there is",
    "start": "3276920",
    "end": "3284329"
  },
  {
    "text": "there a better way to do that so I can just use s3 is that like a fire hose scenario or somewhere we can buffer up",
    "start": "3284329",
    "end": "3290150"
  },
  {
    "text": "and then you know shoot some optimal file size and s3 yeah and I guess also",
    "start": "3290150",
    "end": "3295519"
  },
  {
    "text": "second question like what what would be the optimal file size for s3 like in your scenarios okay do you want to take",
    "start": "3295519",
    "end": "3301969"
  },
  {
    "text": "the sure I mean I think I would say large fire hose definitely it's actually",
    "start": "3301969",
    "end": "3307190"
  },
  {
    "text": "what were you know like I said we were looking at using it to be a replacement for for Kafka a Kafka spark solution",
    "start": "3307190",
    "end": "3313509"
  },
  {
    "text": "fire hose definitely helps you just fire your data at s3 and it takes care of the",
    "start": "3313509",
    "end": "3319789"
  },
  {
    "text": "aggregation it has the ability to convert it to park' which is something we're really you know keen on using and",
    "start": "3319789",
    "end": "3325910"
  },
  {
    "text": "then as far as optimal file size that is an interesting one I think for park' the",
    "start": "3325910",
    "end": "3331160"
  },
  {
    "text": "HDFS kind of era recommendation was a gigabyte per file that way too big for",
    "start": "3331160",
    "end": "3337009"
  },
  {
    "text": "s3 for our use case right we would much rather have kind of the 128 to 256 Meg",
    "start": "3337009",
    "end": "3344809"
  },
  {
    "text": "files so it says not super scientific but we know that the the fetching of those the the latency of gigabyte files",
    "start": "3344809",
    "end": "3351789"
  },
  {
    "text": "introduces too much latency to our queries so we tend to go on the the medium to smaller side yeah yeah no I",
    "start": "3351789",
    "end": "3358910"
  },
  {
    "text": "would concur with that I mean we do see people with s3 select you know maybe",
    "start": "3358910",
    "end": "3365319"
  },
  {
    "text": "tend towards going to a bit bigger file size just because they can men you know start",
    "start": "3365319",
    "end": "3372219"
  },
  {
    "text": "to use if they've integrated select you know start to pull specific subsets out",
    "start": "3372219",
    "end": "3378489"
  },
  {
    "text": "of those files but in general if you're not using select I'd say that you know traditional 128 to 256 Meg this is",
    "start": "3378489",
    "end": "3386919"
  },
  {
    "text": "probably about the sweet spot of cost performance trade-off yeah you know I",
    "start": "3386919",
    "end": "3395649"
  },
  {
    "text": "would like to comment on that - the reality is you'll have to experiment obviously you already know this and test",
    "start": "3395649",
    "end": "3402099"
  },
  {
    "text": "for your specific use case but one of the things that we were most delighted about s3 in particular it's like the most counterintuitive system from a",
    "start": "3402099",
    "end": "3408639"
  },
  {
    "text": "performance perspective you would get you encounter it actually gets faster the harder you push it and we were able",
    "start": "3408639",
    "end": "3414549"
  },
  {
    "text": "to by throwing more resources at it actually spins up resources in the background and it gets ahead so as more",
    "start": "3414549",
    "end": "3421539"
  },
  {
    "text": "as we get crushed by demand queries get shorter yeah crazy ya know the key to",
    "start": "3421539",
    "end": "3427329"
  },
  {
    "text": "performance from s3 is definitely more and more parallel threads of activity so",
    "start": "3427329",
    "end": "3433059"
  },
  {
    "text": "you know starting to figure out how to partition your data accordingly so that",
    "start": "3433059",
    "end": "3438699"
  },
  {
    "text": "you can really parallel eyes access is really the way you optimize performance",
    "start": "3438699",
    "end": "3445169"
  },
  {
    "text": "yeah wondering what you guys we're using for the front-end interface so what's",
    "start": "3445169",
    "end": "3453069"
  },
  {
    "text": "the tech we're using for the front-end interface yeah that's just a JavaScript HTML CSS just",
    "start": "3453069",
    "end": "3458079"
  },
  {
    "text": "all homegrown kind of thing I'm a little bit we modify some you know if you're familiar with d3 yeah so we modified",
    "start": "3458079",
    "end": "3465159"
  },
  {
    "text": "that a little bit make it a little faster it's it's pretty good right and then it's react on the backend yeah so",
    "start": "3465159",
    "end": "3470769"
  },
  {
    "text": "you have you build something that takes yeah and their face choices and turns it",
    "start": "3470769",
    "end": "3476949"
  },
  {
    "text": "into query yes yeah actually that we have our own custom query language that",
    "start": "3476949",
    "end": "3483249"
  },
  {
    "text": "we kind of dubbed pql which actually looks a lot like leucine because it turns it into leucine queries because we",
    "start": "3483249",
    "end": "3489819"
  },
  {
    "text": "used to run it against solar and then we take the leucine queries actually translate them and to spark sequel queries as a way to kind of migrate the",
    "start": "3489819",
    "end": "3495939"
  },
  {
    "text": "system you know I'm you know leucine being a little more friendly to URLs basically yeah thanks",
    "start": "3495939",
    "end": "3503729"
  },
  {
    "text": "hello question there so earlier you said he's good to pre-process data for the",
    "start": "3505020",
    "end": "3512319"
  },
  {
    "text": "data lake so so are some we had a problem with is we purposely avoided",
    "start": "3512319",
    "end": "3518650"
  },
  {
    "text": "doing pre-processing because we did not want to limit our users view of the data",
    "start": "3518650",
    "end": "3524490"
  },
  {
    "text": "so as it as any recommended sort of rule of thumb about would be too much processing pre-processing okay yeah and",
    "start": "3524490",
    "end": "3534460"
  },
  {
    "text": "obviously you know one size doesn't fit all in terms of recommendations I mean that's a general best practice is kind",
    "start": "3534460",
    "end": "3541450"
  },
  {
    "text": "of separate ingest from the actual data like itself and you know maybe start to",
    "start": "3541450",
    "end": "3547779"
  },
  {
    "text": "do some transformation from raw and a more optimized for ingest but you know",
    "start": "3547779",
    "end": "3553390"
  },
  {
    "text": "certainly if you wanted to you know kind of give a fully materialized view in the",
    "start": "3553390",
    "end": "3560319"
  },
  {
    "text": "data lake itself that's a viable option so you know it really comes down to what",
    "start": "3560319",
    "end": "3565420"
  },
  {
    "text": "you're trying to do but we really want to try to get some segregation between you know for data quality purposes for",
    "start": "3565420",
    "end": "3573700"
  },
  {
    "text": "catalog purposes for you know maybe trying to optimize things you know start",
    "start": "3573700",
    "end": "3580270"
  },
  {
    "text": "to segregate out different steps of workflow but you know it's certainly",
    "start": "3580270",
    "end": "3586990"
  },
  {
    "text": "your call what works best for you",
    "start": "3586990",
    "end": "3591630"
  },
  {
    "text": "just really quickly you guys talked about bloom filters and basically pre",
    "start": "3595529",
    "end": "3601900"
  },
  {
    "text": "selecting data before before you did a push down to s3 I was wondering what's",
    "start": "3601900",
    "end": "3607059"
  },
  {
    "text": "your s3 metadata philosophy in terms of knowing exactly which s3 buckets to go",
    "start": "3607059",
    "end": "3614349"
  },
  {
    "text": "after once you have once you have selected your datasets is it off the",
    "start": "3614349",
    "end": "3620710"
  },
  {
    "text": "naming conventions or you have implemented something more unique to",
    "start": "3620710",
    "end": "3626200"
  },
  {
    "text": "define the metadata for a story itself it's too fun to find the minutes oh",
    "start": "3626200",
    "end": "3633119"
  },
  {
    "text": "sorry it's a little echoey in here that the the the index itself is telling us",
    "start": "3633119",
    "end": "3639250"
  },
  {
    "text": "which files to pick off right so the the metadata index is you know we create say like tell me about this IP address and",
    "start": "3639250",
    "end": "3645730"
  },
  {
    "text": "it's literally giving us a list of the files in s3 and those files are park' and they are the metadata that we've",
    "start": "3645730",
    "end": "3651490"
  },
  {
    "text": "extracted from the from the network transactions let's add that makes sense it sort of getting on the right",
    "start": "3651490",
    "end": "3656769"
  },
  {
    "text": "direction yeah so I mean if you if you're familiar with hive or press though they're they're metadata catalog",
    "start": "3656769",
    "end": "3662319"
  },
  {
    "text": "or glue it's just is based on partitions right so it's how you partition your data and s3 with with the basically the",
    "start": "3662319",
    "end": "3669700"
  },
  {
    "text": "directory structure we've taken that beyond that we actually instead of",
    "start": "3669700",
    "end": "3675549"
  },
  {
    "text": "storing the partition information we store each individual file so we can associate the bloom filters with each file okay if that helps",
    "start": "3675549",
    "end": "3682299"
  },
  {
    "text": "very good sounds thank you good question around the glue so especially when",
    "start": "3682299",
    "end": "3688960"
  },
  {
    "text": "you're using a glue as their data processing mechanism I noticed that like it has limitations in terms of change data capture",
    "start": "3688960",
    "end": "3695339"
  },
  {
    "text": "especially when there is a change data instead of reprocessing the data if you weren't really inject one of the change",
    "start": "3695339",
    "end": "3700990"
  },
  {
    "text": "data so is there any like better mechanism like oh anything in the roadmap for glue or any alternatives",
    "start": "3700990",
    "end": "3707259"
  },
  {
    "text": "that you recommend obviously I can't comment about Gulu roadmap particularly",
    "start": "3707259",
    "end": "3714069"
  },
  {
    "text": "since I represent more the storage services but you know we'd certainly be",
    "start": "3714069",
    "end": "3719140"
  },
  {
    "text": "happy you know to facilitate or glue roadmap discussion but in",
    "start": "3719140",
    "end": "3724650"
  },
  {
    "text": "general if glue you know particularly for the transformation side doesn't meet",
    "start": "3724650",
    "end": "3732600"
  },
  {
    "text": "your purposes then you know it's really at the end of the day built on managed",
    "start": "3732600",
    "end": "3738600"
  },
  {
    "text": "spark and so you know you may want to consider you know essentially using",
    "start": "3738600",
    "end": "3744180"
  },
  {
    "text": "spark for your transformation engine today but really want to encourage using",
    "start": "3744180",
    "end": "3750600"
  },
  {
    "text": "glue is the catalog if you can at a minimum because not only is it about",
    "start": "3750600",
    "end": "3756500"
  },
  {
    "text": "cataloging your data starting to associate richer metadata with the",
    "start": "3756500",
    "end": "3761700"
  },
  {
    "text": "catalog but it really becomes if you think about it and think about data like security the catalog is the crux of",
    "start": "3761700",
    "end": "3769550"
  },
  {
    "text": "granular security for your data like and glue has introduced some really interesting new features that start to",
    "start": "3769550",
    "end": "3776070"
  },
  {
    "text": "make fine grained access and presenting different materialized views of what's",
    "start": "3776070",
    "end": "3782520"
  },
  {
    "text": "in that data like for different users you know kind of a reality for the data",
    "start": "3782520",
    "end": "3788130"
  },
  {
    "text": "like so I definitely encourage catalog and if ETL doesn't meet your purposes",
    "start": "3788130",
    "end": "3793800"
  },
  {
    "text": "you know use something out-of-band to do transformation makes it alright for your",
    "start": "3793800",
    "end": "3806040"
  },
  {
    "text": "when you move to the AWS ecosystem what was your tool for ETL scheduling",
    "start": "3806040",
    "end": "3813870"
  },
  {
    "text": "and tasks did you if you're using glue ETL did you explore alternatives like",
    "start": "3813870",
    "end": "3820290"
  },
  {
    "text": "airflow and can you yeah Tony we",
    "start": "3820290",
    "end": "3825870"
  },
  {
    "text": "actually do use airflow for our other we we actually have like data where data leaks do it basically purpose-built for",
    "start": "3825870",
    "end": "3833850"
  },
  {
    "text": "different things and for this one actually since it's kind of more of a stream based even even the the immutable",
    "start": "3833850",
    "end": "3841410"
  },
  {
    "text": "or the mutable part is sort of stream based in a sense we stream that it into Sandra and then we have a special kind",
    "start": "3841410",
    "end": "3847680"
  },
  {
    "text": "of sidecar decks that we write that allows us to kind of stream it back out as it as it goes cold so the ETL is more",
    "start": "3847680",
    "end": "3853440"
  },
  {
    "text": "of a continuous process and it's actually arkhan EMR that just runs in a loop",
    "start": "3853440",
    "end": "3858830"
  },
  {
    "text": "which is really Allspark streaming is actually it's just loops of spat of batch jobs that are just running smaller",
    "start": "3858830",
    "end": "3864510"
  },
  {
    "text": "intervals so we generally use spark spark streaming as it were so you don't really quite a schedule because it's",
    "start": "3864510",
    "end": "3870750"
  },
  {
    "text": "always running yes yes that's that's exactly correct now for our other warehouse stuff we we do use glue and we",
    "start": "3870750",
    "end": "3877500"
  },
  {
    "text": "also use air flow for for certain things did you find any advantages with glue I",
    "start": "3877500",
    "end": "3885360"
  },
  {
    "text": "mean when we first started using glue it was a little newer so and I'm not actually really interacting with it so",
    "start": "3885360",
    "end": "3891120"
  },
  {
    "text": "much I can't say too much but air flow was our first kind of choice because it",
    "start": "3891120",
    "end": "3897090"
  },
  {
    "text": "was really on glue and and you know it's great because we it's Python and some of our developer our data science developers our Python people so yeah III",
    "start": "3897090",
    "end": "3905280"
  },
  {
    "text": "don't know I don't use it much myself daily so I can't comment too much all right so Kay thank you well what was",
    "start": "3905280",
    "end": "3914820"
  },
  {
    "text": "your strategy for updating mutable Records in the s3 daylight ah so we",
    "start": "3914820",
    "end": "3921930"
  },
  {
    "text": "don't because it's you know and actually I talked a little about some of the",
    "start": "3921930",
    "end": "3927420"
  },
  {
    "text": "things that hive does I've actually does have updates which I think it's just it's crazy if you're using the orc",
    "start": "3927420",
    "end": "3932910"
  },
  {
    "text": "format what we do instead is like I kind of pointed out we never really got rid of DSC entirely we actually use DSC for",
    "start": "3932910",
    "end": "3939270"
  },
  {
    "text": "the hot data no no I lost my mic there",
    "start": "3939270",
    "end": "3945390"
  },
  {
    "text": "as the data gets cold it actually becomes completely frozen at some point when the network connection ends that's",
    "start": "3945390",
    "end": "3950580"
  },
  {
    "text": "when we move it into s3 we do occasionally because we are a company that looks back over our data we find",
    "start": "3950580",
    "end": "3956100"
  },
  {
    "text": "new findings about data from a security standpoint we do occasionally make new",
    "start": "3956100",
    "end": "3961170"
  },
  {
    "text": "hot records that actually overlap the existing records and the query system is designed in such a way that it will",
    "start": "3961170",
    "end": "3966660"
  },
  {
    "text": "query both systems and then merge the results together which is actually what a solar like a solar cluster does it",
    "start": "3966660",
    "end": "3972750"
  },
  {
    "text": "queries all the systems merge the results together we just have to merge things from kind of two disparate systems so you cheated so for very large",
    "start": "3972750",
    "end": "3979020"
  },
  {
    "text": "systems you want to take a process at least once approach which means you're willing to take the outliers of",
    "start": "3979020",
    "end": "3984330"
  },
  {
    "text": "over-processing yeah or they didn't you need to and you just get good at duping it after this after the fact so that gives you",
    "start": "3984330",
    "end": "3990819"
  },
  {
    "text": "robustness against late-arriving data as well as out of order data yeah okay the",
    "start": "3990819",
    "end": "3995829"
  },
  {
    "text": "amount of updates are much smaller then okay I think we're being yeah I think we're being cut off your mic signal but",
    "start": "3995829",
    "end": "4002460"
  },
  {
    "text": "if you have questions come up here and we'll be happy to take your questions",
    "start": "4002460",
    "end": "4007980"
  },
  {
    "text": "you know off to the city here so thank you again for attending",
    "start": "4007980",
    "end": "4013910"
  }
]