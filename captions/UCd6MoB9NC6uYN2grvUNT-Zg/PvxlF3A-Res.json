[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "well good afternoon everybody thanks for thanks for coming to listen to us talk",
    "start": "60",
    "end": "5490"
  },
  {
    "text": "about analyzing data streams in real time with Amazon Kinesis so I'm Alan McGinnis I am a Solutions",
    "start": "5490",
    "end": "12300"
  },
  {
    "text": "Architect here at AWS and I work primarily with customers in the",
    "start": "12300",
    "end": "18180"
  },
  {
    "text": "streaming data space so I work with customers that are doing eventing",
    "start": "18180",
    "end": "23630"
  },
  {
    "text": "logging logging they're streaming their data logs so a lot of what we'll discuss",
    "start": "23630",
    "end": "29099"
  },
  {
    "text": "today is quite often practiced by many of the customers that I work with and hopefully hopefully you guys can learn a",
    "start": "29099",
    "end": "35399"
  },
  {
    "text": "little bit about it as well and with me today later on who will speak specifically about what they're doing at",
    "start": "35399",
    "end": "41010"
  },
  {
    "text": "Zynga in this space is millon bran bat so hook him up and kind of dive deep",
    "start": "41010",
    "end": "46770"
  },
  {
    "text": "into what they're doing in a specific use case at Zynga so before we get into",
    "start": "46770",
    "end": "52110"
  },
  {
    "start": "50000",
    "end": "131000"
  },
  {
    "text": "some of the details let's just quickly talk about why stream your data and it",
    "start": "52110",
    "end": "58379"
  },
  {
    "text": "really comes down to timely decisions so data is being generated continuously",
    "start": "58379",
    "end": "64710"
  },
  {
    "text": "right I think we all we all know that think about even the simplest kind of multi tier website I've got a website",
    "start": "64710",
    "end": "70560"
  },
  {
    "text": "out there with a database behind it and I'm just serving up some data maybe my users are doing something with",
    "start": "70560",
    "end": "76530"
  },
  {
    "text": "that data in a traditional world if you think back maybe just just a few years",
    "start": "76530",
    "end": "81600"
  },
  {
    "text": "ago we really didn't do much with the data that we got in that in that",
    "start": "81600",
    "end": "86610"
  },
  {
    "text": "interaction until maybe tonight right we went reran a batch process against that",
    "start": "86610",
    "end": "91950"
  },
  {
    "text": "data to get some insights into what's going on well if you think about that",
    "start": "91950",
    "end": "97590"
  },
  {
    "text": "data and when it was generated it was generated as soon as perhaps that user clicked on the website but if I don't do",
    "start": "97590",
    "end": "104850"
  },
  {
    "text": "anything with that data until tonight or tomorrow or next week I lose a lot of value there's something I can learn from",
    "start": "104850",
    "end": "111030"
  },
  {
    "text": "the fact that a user did something in my app or on my website you know immediately after it happened so we like",
    "start": "111030",
    "end": "117119"
  },
  {
    "text": "to talk about sort of the the value of data and how it diminishes over time so it's kind of an important concept to",
    "start": "117119",
    "end": "123659"
  },
  {
    "text": "think about when you're thinking about streaming data because today it's fairly straightforward to capture the data and",
    "start": "123659",
    "end": "129119"
  },
  {
    "text": "do something with it and that's what will that's what we'll discuss so stream new data in second so that you",
    "start": "129119",
    "end": "135810"
  },
  {
    "text": "can get actionable insights quickly so ingest video so I'm not going to talk a",
    "start": "135810",
    "end": "141870"
  },
  {
    "text": "lot about video today but I'll touch on a very briefly video right is just data it's data that's streaming that you can",
    "start": "141870",
    "end": "148139"
  },
  {
    "text": "capture and do some insights make some insights out of the out of that data primarily we're going to talk about the",
    "start": "148139",
    "end": "153209"
  },
  {
    "text": "sort of classic data today things like click stream or game events but as soon",
    "start": "153209",
    "end": "158819"
  },
  {
    "text": "as it's generated do something with it process it on the fly and in real time get some insights out of it by applying",
    "start": "158819",
    "end": "165169"
  },
  {
    "text": "calculating some analytics doing some machine learning so it's very important helps you get insights into what your",
    "start": "165169",
    "end": "172319"
  },
  {
    "text": "customers are doing in real time with the data so some of the most common",
    "start": "172319",
    "end": "177799"
  },
  {
    "start": "175000",
    "end": "228000"
  },
  {
    "text": "streaming use cases that we see again the camera on the left is a good example",
    "start": "177799",
    "end": "183030"
  },
  {
    "text": "of a video a video example but the rest of the the four here are more common which is the typical data streaming so",
    "start": "183030",
    "end": "190079"
  },
  {
    "text": "things like industrial automation of Getty I've got devices in a factory they're always generating data log",
    "start": "190079",
    "end": "195239"
  },
  {
    "text": "analytics this by far and away is probably the most common use case that I see working with customers in this space",
    "start": "195239",
    "end": "201090"
  },
  {
    "text": "is applications are continuously creating logs doing something with those",
    "start": "201090",
    "end": "206250"
  },
  {
    "text": "logs as soon as they're created very important using streaming data to feed data lakes and I'll get into a specific",
    "start": "206250",
    "end": "213150"
  },
  {
    "text": "example about that here in a moment that IOT device monitoring so this is sort of similar to the industrial automation example but everything today is becoming",
    "start": "213150",
    "end": "221609"
  },
  {
    "text": "connected and all of those devices are sending data somewhere so let's do something with that as soon as it gets",
    "start": "221609",
    "end": "227819"
  },
  {
    "text": "created so in the AWS world what are some of the services that help you",
    "start": "227819",
    "end": "234590"
  },
  {
    "start": "228000",
    "end": "265000"
  },
  {
    "text": "collect and analyze this streaming data we have a collection of services in the Amazon Kinesis space from left to right",
    "start": "234590",
    "end": "242189"
  },
  {
    "text": "we have data streams data firehose data analytics and video streams now I'm not",
    "start": "242189",
    "end": "248129"
  },
  {
    "text": "going to talk anymore about video streams but it was introduced at our conference our reinvent conference in",
    "start": "248129",
    "end": "253819"
  },
  {
    "text": "November of last year so that's why there's a few slides in here that",
    "start": "253819",
    "end": "259859"
  },
  {
    "text": "discuss a little bit about video but the bulk of the day we'll talk about data so",
    "start": "259859",
    "end": "265740"
  },
  {
    "start": "265000",
    "end": "406000"
  },
  {
    "text": "when you think about Kinesis data streams or data analytics or data",
    "start": "265740",
    "end": "271620"
  },
  {
    "text": "firehose we're talking about streaming data processing it with some tool and",
    "start": "271620",
    "end": "276990"
  },
  {
    "text": "then writing the results of that process somewhere else so that I can get some insights into it so if you look at this",
    "start": "276990",
    "end": "284130"
  },
  {
    "text": "slide don't worry too much about the details but we look at some data from the left kinases data streams as the",
    "start": "284130",
    "end": "292590"
  },
  {
    "text": "primary ingestion tool and then with cases data streams you can choose to use",
    "start": "292590",
    "end": "298320"
  },
  {
    "text": "any number of stream processing solutions to get the insights into the",
    "start": "298320",
    "end": "303330"
  },
  {
    "text": "data so here we show spark running on EMR your own code that you can write on",
    "start": "303330",
    "end": "308729"
  },
  {
    "text": "ec2 using some libraries and tools that we've provided you can use lambda to",
    "start": "308729",
    "end": "315570"
  },
  {
    "text": "process that data in real time or you can use Kinesis data analytics to get",
    "start": "315570",
    "end": "321120"
  },
  {
    "text": "some real-time insights into that by writing some standard SQL and then from the output of analytics you can send it",
    "start": "321120",
    "end": "327120"
  },
  {
    "text": "to any number of downstream sources or destinations such as s3 redshift elasticsearch and Splunk via Kinesis",
    "start": "327120",
    "end": "334740"
  },
  {
    "text": "data firehose this is one example of kind of a typical data flow that you might see and all of these services",
    "start": "334740",
    "end": "340979"
  },
  {
    "text": "Kinesis streams firehose and analytics give you the ability to do that in real time they're fully managed so there's no",
    "start": "340979",
    "end": "348270"
  },
  {
    "text": "ec2 instances that you need to manage under the covers they're fully managed services they're scalable so firehose",
    "start": "348270",
    "end": "356520"
  },
  {
    "text": "and analytics do some out of scaling automatically and with Kinesis streams you can scale very simply by adding",
    "start": "356520",
    "end": "362820"
  },
  {
    "text": "additional capacity we call them shards to the stream and the very cost-effective so the pricing model in",
    "start": "362820",
    "end": "370469"
  },
  {
    "text": "these services is generally based on how much data you put into the actual",
    "start": "370469",
    "end": "375560"
  },
  {
    "text": "service so I'm not going to get into much detail on the cost component but",
    "start": "375560",
    "end": "381389"
  },
  {
    "text": "they're basically the more you use the services that more they cost much like many of the AWS services a quick show of",
    "start": "381389",
    "end": "388979"
  },
  {
    "text": "hands maybe how many folks are already streaming data in your in your environment today doing some amount of",
    "start": "388979",
    "end": "395310"
  },
  {
    "text": "streaming and processing hard to see ok it's about the third so cool so some of this is",
    "start": "395310",
    "end": "401740"
  },
  {
    "text": "probably old hat to you and some of this for some of you is is new let's talk a",
    "start": "401740",
    "end": "407350"
  },
  {
    "start": "406000",
    "end": "474000"
  },
  {
    "text": "little bit about a couple of use cases so clickstream analysis this is",
    "start": "407350",
    "end": "412720"
  },
  {
    "text": "something that typically in years past and not even that far ago and I'm sure many organizations are still doing this",
    "start": "412720",
    "end": "418630"
  },
  {
    "text": "in a batch process today clickstream click events are being created being",
    "start": "418630",
    "end": "424570"
  },
  {
    "text": "stored in the local log filing was an Apache access log or something and we run a job on a daily basis that takes",
    "start": "424570",
    "end": "430600"
  },
  {
    "text": "the data off of those servers puts them into a data warehouse and then we run a batch process on a nightly basis to get",
    "start": "430600",
    "end": "437050"
  },
  {
    "text": "some insights out of it that's all well and good but you're like I discussed a moment ago you lose some of the value in",
    "start": "437050",
    "end": "444580"
  },
  {
    "text": "that data if you don't process it until much later so by streaming those clickstream events in real time through",
    "start": "444580",
    "end": "452320"
  },
  {
    "text": "like a fire hose on the front end so collect this data do some analysis with it do some aggregations in real time so",
    "start": "452320",
    "end": "459250"
  },
  {
    "text": "how many times has this page been viewed in the past five minutes right if I can",
    "start": "459250",
    "end": "465190"
  },
  {
    "text": "get that as soon as that five-minute window closed I learned a lot about what's going on in my application so",
    "start": "465190",
    "end": "471729"
  },
  {
    "text": "that's one example of I'm doing real-time analysis calculating real-time",
    "start": "471729",
    "end": "477160"
  },
  {
    "start": "474000",
    "end": "552000"
  },
  {
    "text": "analytics so this example talks about analyzing streaming social media data it",
    "start": "477160",
    "end": "482590"
  },
  {
    "text": "can be any kind of data that's relevant to your applications so calculating KPIs",
    "start": "482590",
    "end": "487930"
  },
  {
    "text": "whatever whatever metrics are important to your business why wait until a batch",
    "start": "487930",
    "end": "494620"
  },
  {
    "text": "process to figure out what that means so I've worked with one customer there in the e-commerce space and they sell",
    "start": "494620",
    "end": "501160"
  },
  {
    "text": "products on their website and they want to know every hour what is moving what",
    "start": "501160",
    "end": "506380"
  },
  {
    "text": "are people looking at and this is not Amazon by the way this is a different ecommerce company but what what are people looking at on the website what's",
    "start": "506380",
    "end": "512050"
  },
  {
    "text": "the popular product they used to do this on a daily basis and they wanted to apply some marketing to their system",
    "start": "512050",
    "end": "517930"
  },
  {
    "text": "they want to say well things aren't this particular product I just promoted I really like to know how well that promotion is working and I don't want to",
    "start": "517930",
    "end": "524620"
  },
  {
    "text": "wait until tomorrow to do that so they've implemented a solution similar to this where they're collecting the",
    "start": "524620",
    "end": "530920"
  },
  {
    "text": "metrics off of their e-commerce website and they are building hourly a dashboard that's based on the real-time metrics",
    "start": "530920",
    "end": "537670"
  },
  {
    "text": "that are created on the fly every hour so that's their KPI is looking at the",
    "start": "537670",
    "end": "543310"
  },
  {
    "text": "metrics related to the products that are selling well or not well on their website and they're building dashboards",
    "start": "543310",
    "end": "548950"
  },
  {
    "text": "around that in real time as opposed to building something on the backend IRT stream processing so this is a very",
    "start": "548950",
    "end": "556360"
  },
  {
    "text": "common pattern that we're seeing - there's like I mentioned a moment ago there's so many devices these days that",
    "start": "556360",
    "end": "561459"
  },
  {
    "text": "are generating data continuously let's collect that data and build some alarms",
    "start": "561459",
    "end": "567550"
  },
  {
    "text": "around it so on this one right we've got a tractor that's it's sending metrics",
    "start": "567550",
    "end": "573520"
  },
  {
    "text": "about what it's doing in the field and then it can recognize when parts and some of this of course machine learning",
    "start": "573520",
    "end": "579790"
  },
  {
    "text": "is built into the tractor but it's learning a little bit about what's going on maintenance periods and it's sending",
    "start": "579790",
    "end": "586029"
  },
  {
    "text": "data to the backend and it can alert technicians in real-time when a part is",
    "start": "586029",
    "end": "592300"
  },
  {
    "text": "about to fail and they can go and reorder those parts so all of those three examples if we look at the go back",
    "start": "592300",
    "end": "599860"
  },
  {
    "text": "go back right they all have Kinesis analytics in the middle so getting some of those insights into the data is",
    "start": "599860",
    "end": "608200"
  },
  {
    "text": "pretty straightforward when you're using a service like Kinesis analytics so maybe quick show of hands how many folks",
    "start": "608200",
    "end": "614080"
  },
  {
    "text": "here have looked at specifically the Kinesis data analytics tool and have used some sequel to get some insights",
    "start": "614080",
    "end": "619900"
  },
  {
    "text": "out of your streaming data okay so it's pretty smaller set then then a moment ago and that's that's good so I think",
    "start": "619900",
    "end": "626770"
  },
  {
    "text": "today we're going to focus a little bit about Kinesis data analytics and millon",
    "start": "626770",
    "end": "631839"
  },
  {
    "text": "we'll talk a little about specifically how they have used this tool to get some insights into their their data so I'm",
    "start": "631839",
    "end": "637930"
  },
  {
    "text": "going to dive into a few of the key concepts in Kinesis analytics and and",
    "start": "637930",
    "end": "643959"
  },
  {
    "text": "then like I said millon will we'll take over from there so as I mentioned it's",
    "start": "643959",
    "end": "649600"
  },
  {
    "start": "647000",
    "end": "698000"
  },
  {
    "text": "all about the pace of data things like hourly server logs weekly or monthly bills these are all kind of batch things",
    "start": "649600",
    "end": "655959"
  },
  {
    "text": "think about your credit card write a batch process once a month you get a statement all the transactions you made",
    "start": "655959",
    "end": "661110"
  },
  {
    "text": "our batch top had a monthly basis and then you pay for it think about the stream",
    "start": "661110",
    "end": "667269"
  },
  {
    "text": "processing imagine if you didn't hear from your credit card company about a potential fraud transaction until the",
    "start": "667269",
    "end": "672519"
  },
  {
    "text": "end of the month right that would be pretty bad so you know what a lot of credit card companies do is they'll",
    "start": "672519",
    "end": "678040"
  },
  {
    "text": "stream that data and then they have a fraud detection algorithm that looks at every transaction being made on your",
    "start": "678040",
    "end": "684069"
  },
  {
    "text": "card and they can determine if there's fraud in real time and that's why you might get a text or a phone call from your your credit card company so a good",
    "start": "684069",
    "end": "690129"
  },
  {
    "text": "example there of like a batch process and a real-time process and how we can",
    "start": "690129",
    "end": "695139"
  },
  {
    "text": "really benefit and how your customers can benefit from from real-time so Kinesis analytics today the sources for",
    "start": "695139",
    "end": "703600"
  },
  {
    "start": "698000",
    "end": "718000"
  },
  {
    "text": "your Kinesis data analytics application can be a Kinesis data stream or a Kinesis data firehose and then you write",
    "start": "703600",
    "end": "710319"
  },
  {
    "text": "your sequel to process the streaming data that's coming from one of those two streaming services and then you output",
    "start": "710319",
    "end": "716110"
  },
  {
    "text": "what you learn based on your sequel so typical pattern data producer a",
    "start": "716110",
    "end": "721920"
  },
  {
    "text": "streaming service like Amazon Kinesis and then a data consumer and the Kinesis",
    "start": "721920",
    "end": "727360"
  },
  {
    "text": "analytics application that is your consumer right you have a data producer",
    "start": "727360",
    "end": "732850"
  },
  {
    "text": "in this case it's showing a mobile client sending data to a Kinesis data stream and then your Kinesis analytics",
    "start": "732850",
    "end": "738629"
  },
  {
    "text": "application will listen to that streaming data and then depending on the code that you write in your sequel",
    "start": "738629",
    "end": "743860"
  },
  {
    "text": "you'll get some insights out of the data so it's typically a three step process basically you connect to your streaming",
    "start": "743860",
    "end": "750610"
  },
  {
    "start": "746000",
    "end": "758000"
  },
  {
    "text": "source you write your sequel and then you continuously deliver those sequel results and I'll get into a bit more",
    "start": "750610",
    "end": "756189"
  },
  {
    "text": "detail about the the sequel here in a moment one of the nice features to that you can also do with casus analytics is",
    "start": "756189",
    "end": "762689"
  },
  {
    "start": "758000",
    "end": "902000"
  },
  {
    "text": "pre-process the data with a lambda function before it even hits your sequel",
    "start": "762689",
    "end": "768069"
  },
  {
    "text": "that you've written in the app so why would you want to do that well think",
    "start": "768069",
    "end": "773230"
  },
  {
    "text": "about Kinesis data analytics uses sequel like I said so sequel requires a schema right select something from this table",
    "start": "773230",
    "end": "781480"
  },
  {
    "text": "where these you know these criteria are met in order to have a nice schema what",
    "start": "781480",
    "end": "788860"
  },
  {
    "text": "if your data is not structured in a way that allows it to be kind of mapped to a schema imagine that it's unstructured",
    "start": "788860",
    "end": "794589"
  },
  {
    "text": "input data or it's a very complex maybe it's like a very nested JSON",
    "start": "794589",
    "end": "800290"
  },
  {
    "text": "record how do you map a very complex nested JSON to a to a sequel",
    "start": "800290",
    "end": "807910"
  },
  {
    "text": "table for an example it's not that straightforward so what you can do is use a lambda function and you can",
    "start": "807910",
    "end": "815290"
  },
  {
    "text": "flatten out the data to do some amount of transformation from your original data source into a nice flat structured",
    "start": "815290",
    "end": "821920"
  },
  {
    "text": "so it makes it very easy to query against using sequel so that's one reason why you might do some",
    "start": "821920",
    "end": "827140"
  },
  {
    "text": "pre-processing you might want to enrich the data for example perhaps some of the source data includes the IP address of",
    "start": "827140",
    "end": "836340"
  },
  {
    "text": "whatever the data producer was maybe if it's a mobile phone like earlier you want to send in some information like an",
    "start": "836340",
    "end": "842440"
  },
  {
    "text": "IP address from the mobile but you want to do some geo geo location well you can enrich the data before it even reaches",
    "start": "842440",
    "end": "849400"
  },
  {
    "text": "the sequel engine you can rich it and rich it with perhaps the city state",
    "start": "849400",
    "end": "854590"
  },
  {
    "text": "country by creating a lambda function that takes the IP address and does a geoip look up and then enriches the data",
    "start": "854590",
    "end": "861220"
  },
  {
    "text": "and then now in your sequel you can do things like group by state group by city group by country you can get a lot of",
    "start": "861220",
    "end": "867880"
  },
  {
    "text": "insights into your data because you've created an enriched data set before it",
    "start": "867880",
    "end": "873700"
  },
  {
    "text": "even arrived in your processing engine so that's pre-processing it's not required Kinesis data analytics will",
    "start": "873700",
    "end": "880060"
  },
  {
    "text": "look at your source data and it will create a schema based on what it sees and it'll sample your data we always",
    "start": "880060",
    "end": "887200"
  },
  {
    "text": "recommend that if you go that path that you go and you look at the schema that we created to make sure that accurately",
    "start": "887200",
    "end": "893140"
  },
  {
    "text": "reflects your data because we're just taking a sample of the source data however if you do have complex or you",
    "start": "893140",
    "end": "899110"
  },
  {
    "text": "want to enrich then you can then you can pre-process it with lambda so how do I write streaming sequel it's pretty",
    "start": "899110",
    "end": "904960"
  },
  {
    "start": "902000",
    "end": "944000"
  },
  {
    "text": "straightforward if you've written sequel against a relational database it's going",
    "start": "904960",
    "end": "910600"
  },
  {
    "text": "to feel very similar with a few additional constructs so this is streaming so you see this statement here",
    "start": "910600",
    "end": "916180"
  },
  {
    "text": "creates stream calls per IP stream think of like a stream in this context as a",
    "start": "916180",
    "end": "922030"
  },
  {
    "text": "temporary table for your streaming data so imagine if I replace that word stream",
    "start": "922030",
    "end": "927370"
  },
  {
    "text": "with table it's going to feel very much like a create table statement that you against a relational database so now",
    "start": "927370",
    "end": "933620"
  },
  {
    "text": "what this represents is kind of a temporary buffer in the Kinesis analytics application that represents",
    "start": "933620",
    "end": "939680"
  },
  {
    "text": "kind of a snapshot in time of the data that's streaming through my data source and then how do I populate it it's",
    "start": "939680",
    "end": "946850"
  },
  {
    "start": "944000",
    "end": "973000"
  },
  {
    "text": "called a pump so a pump is a continuously running insert statement",
    "start": "946850",
    "end": "952190"
  },
  {
    "text": "that populates that stream that I just mentioned on the previous slide so this pump as you can see here there's an",
    "start": "952190",
    "end": "957410"
  },
  {
    "text": "insert statement that's inserting data into the stream called event timestamp",
    "start": "957410",
    "end": "962480"
  },
  {
    "text": "I wish I had named the two streams the same so it was because it would correlate but I think you get the point so this insert statement is running",
    "start": "962480",
    "end": "968860"
  },
  {
    "text": "continuously and populating the in application stream then how do we",
    "start": "968860",
    "end": "975320"
  },
  {
    "start": "973000",
    "end": "1018000"
  },
  {
    "text": "aggregate that data so remember this stream right it's kind of unbounded a",
    "start": "975320",
    "end": "981500"
  },
  {
    "text": "stream of data if I just had said select count star from my stream of data where",
    "start": "981500",
    "end": "987170"
  },
  {
    "text": "these criteria are met kinesio etics will fail on you and why because it's a continuous stream there's",
    "start": "987170",
    "end": "992840"
  },
  {
    "text": "no time down you need to tell the system for streaming data I need to know something about like a period of time",
    "start": "992840",
    "end": "999260"
  },
  {
    "text": "for which you want me to do that aggregation and that's where Windows come in so it's not you can use Windows",
    "start": "999260",
    "end": "1006370"
  },
  {
    "text": "if you want Microsoft Windows but it's now windows there are there are two actually multiple types but the two",
    "start": "1006370",
    "end": "1013240"
  },
  {
    "text": "primary types of windows that you would use in a streaming data application are",
    "start": "1013240",
    "end": "1018660"
  },
  {
    "start": "1018000",
    "end": "1092000"
  },
  {
    "text": "sliding and tumbling so a sliding window is basically the metric will be or the",
    "start": "1018660",
    "end": "1026500"
  },
  {
    "text": "aggregate will be calculated at the end of that window period so imagine I said",
    "start": "1026500",
    "end": "1031930"
  },
  {
    "text": "I wanted to have a 10-minute window every 10 minutes the aggregation is run and there's an output so if I want to",
    "start": "1031930",
    "end": "1038500"
  },
  {
    "text": "count how many times an event has occurred over this 10 minute window it'll the Kinesis analytics application",
    "start": "1038500",
    "end": "1045670"
  },
  {
    "text": "and we'll look at the events run its count aggregation and now at the end of that window I get a result a sliding",
    "start": "1045670",
    "end": "1053050"
  },
  {
    "text": "windows a little different sliding window is a record comes in using a 10-minute window again every time a",
    "start": "1053050",
    "end": "1058480"
  },
  {
    "text": "record comes in I look at the previous 10 minutes from when I received that specific record",
    "start": "1058480",
    "end": "1064840"
  },
  {
    "text": "calculate the aggregate and then have an output so the difference is for every input that I receive in a sliding window",
    "start": "1064840",
    "end": "1071080"
  },
  {
    "text": "there is an output so if a hundred records come in there's a hundred records that go out each aggregation is",
    "start": "1071080",
    "end": "1078370"
  },
  {
    "text": "built on that sliding window where with the tumbling window it's just on that period every 10 minutes there's an",
    "start": "1078370",
    "end": "1083710"
  },
  {
    "text": "output and you can have custom windows based on the number of rows so you can say I want to build a window based on the last hundred records I saw that's",
    "start": "1083710",
    "end": "1090820"
  },
  {
    "text": "possible as well and then there is the ability to do some timestamp",
    "start": "1090820",
    "end": "1099179"
  },
  {
    "start": "1092000",
    "end": "1132000"
  },
  {
    "text": "calculations so things like row time so when did Kinesis analytics first insert",
    "start": "1099179",
    "end": "1104500"
  },
  {
    "text": "the record into the application when did the record when was it received by Kinesis analytics and then the event",
    "start": "1104500",
    "end": "1111220"
  },
  {
    "text": "time is typically a timestamp that you would include in your message itself and",
    "start": "1111220",
    "end": "1118510"
  },
  {
    "text": "so you can build your aggregations based on those timestamps to do aggregations",
    "start": "1118510",
    "end": "1124299"
  },
  {
    "text": "based on the event time when did it occur when did it arrive when was it first processed so these timestamps can",
    "start": "1124299",
    "end": "1129490"
  },
  {
    "text": "be used in in your queries and then you can combine them to help with late",
    "start": "1129490",
    "end": "1135220"
  },
  {
    "start": "1132000",
    "end": "1182000"
  },
  {
    "text": "events so in a distributed system think about the fact that maybe hundreds of different devices or servers sending",
    "start": "1135220",
    "end": "1143140"
  },
  {
    "text": "this streaming data it's entirely possible that if I'm doing a tumbling window which means every five minutes I",
    "start": "1143140",
    "end": "1148779"
  },
  {
    "text": "want to do a I want to do an aggregation if a record arrived slightly late maybe",
    "start": "1148779",
    "end": "1155649"
  },
  {
    "text": "the data producer had a you know it created an event at four fifty nine fifty-nine and then it had a network",
    "start": "1155649",
    "end": "1163299"
  },
  {
    "text": "blip and it didn't actually get until our back end until you know 501 or something it missed the window that it",
    "start": "1163299",
    "end": "1169960"
  },
  {
    "text": "was supposed to be in so you can combine these windows by using a combination of the event time in the row time two to",
    "start": "1169960",
    "end": "1177940"
  },
  {
    "text": "basically handle late arriving events so so some of the customers that are using",
    "start": "1177940",
    "end": "1184600"
  },
  {
    "start": "1182000",
    "end": "1220000"
  },
  {
    "text": "Kinesis today is a lot of them these are just a couple a few names you probably",
    "start": "1184600",
    "end": "1190539"
  },
  {
    "text": "notice here things like companies like Netflix and lyft process a lot of data through Kinesis and then the one that",
    "start": "1190539",
    "end": "1197409"
  },
  {
    "text": "we're going to jump in - in more detail is Zynga so I'll turn it over to milind now and he's going to",
    "start": "1197409",
    "end": "1203559"
  },
  {
    "text": "come up here and talk about their use case and it's all yours",
    "start": "1203559",
    "end": "1209700"
  },
  {
    "text": "Thank You Alan hey guys my name is millon I'm a",
    "start": "1209700",
    "end": "1215260"
  },
  {
    "text": "principal a software engineer head Singha this works okay and what I do",
    "start": "1215260",
    "end": "1222520"
  },
  {
    "start": "1220000",
    "end": "1229000"
  },
  {
    "text": "there is anything to do with the analytics infrastructure it's a lot of fun that said let me jump right into our",
    "start": "1222520",
    "end": "1230230"
  },
  {
    "text": "motivation so Zynga's motivation is to connect the world through games but that comes with",
    "start": "1230230",
    "end": "1236470"
  },
  {
    "text": "the challenge and the challenge being how do we exactly know if games are fun",
    "start": "1236470",
    "end": "1241659"
  },
  {
    "text": "and social once people start playing these games how do we know we're if we're actually connecting the world through games and that's where analytic",
    "start": "1241659",
    "end": "1248740"
  },
  {
    "text": "side Zynga comes into play it's a system that allows or enables games to submit",
    "start": "1248740",
    "end": "1254559"
  },
  {
    "text": "game events which can be used later on for processing and doing analysis on it",
    "start": "1254559",
    "end": "1260200"
  },
  {
    "text": "to generate insights and that keep the key part there being we want to generate",
    "start": "1260200",
    "end": "1265929"
  },
  {
    "text": "data-driven insights so in in my part of the presentation I'm going to talk about",
    "start": "1265929",
    "end": "1271900"
  },
  {
    "start": "1271000",
    "end": "1322000"
  },
  {
    "text": "these things first I'm gonna talk about what game events are ed Zynga specifically for",
    "start": "1271900",
    "end": "1277840"
  },
  {
    "text": "Zynga and we talked about what when does Zynga need a streaming solution that",
    "start": "1277840",
    "end": "1283240"
  },
  {
    "text": "Allen just talked about I'm also going to talk about when the Zynga not need such a solution and then I'll dive into",
    "start": "1283240",
    "end": "1290020"
  },
  {
    "text": "the implementation details of how we went about to implement a data analytics",
    "start": "1290020",
    "end": "1295950"
  },
  {
    "text": "system within our use case at Zynga along with how even we implemented the",
    "start": "1295950",
    "end": "1302799"
  },
  {
    "text": "monitoring of this system we could still largely manage system how do we make sure that it's doing what we think it",
    "start": "1302799",
    "end": "1308320"
  },
  {
    "text": "should be doing and then later on we'll give ourselves a score how do we do in our implementation did we do well did we",
    "start": "1308320",
    "end": "1313720"
  },
  {
    "text": "meet our design goals and then lastly I'm going to talk about some best practices that we learned along the way",
    "start": "1313720",
    "end": "1319169"
  },
  {
    "text": "while implementing this system so what are game events at Zynga imagine you're",
    "start": "1319169",
    "end": "1324909"
  },
  {
    "start": "1322000",
    "end": "1420000"
  },
  {
    "text": "playing Words With Friends the new verse with friends you just start at that and downloaded it when you launch it",
    "start": "1324909",
    "end": "1330220"
  },
  {
    "text": "hypothetically you could generate these kind of game events you could generate a visit game event which could tell us",
    "start": "1330220",
    "end": "1335980"
  },
  {
    "text": "later on for analysis of daily active users you can generate an install day",
    "start": "1335980",
    "end": "1341159"
  },
  {
    "text": "and install game event which I'll talk about later on as a use case of how that can be useful we can also start tracking",
    "start": "1341159",
    "end": "1347230"
  },
  {
    "text": "your session duration in the game so that we can know whether or not you're engaged in the game and if you're liking",
    "start": "1347230",
    "end": "1353409"
  },
  {
    "text": "it or not let's just say you start a game with a player and you that that",
    "start": "1353409",
    "end": "1359500"
  },
  {
    "text": "could generate some game events as well you can generate a social game event there that you started a game and let's",
    "start": "1359500",
    "end": "1365590"
  },
  {
    "text": "just say you powered up and bought some power-ups in the game and you want to",
    "start": "1365590",
    "end": "1370870"
  },
  {
    "text": "track that those Goods and so that considered Goods game event or you can switch over to the messaging chat in the",
    "start": "1370870",
    "end": "1376570"
  },
  {
    "text": "game and start messaging your your player and that could generate messages game event and then another use case is",
    "start": "1376570",
    "end": "1383019"
  },
  {
    "text": "message click so let's just say you can click onto the word that you played and you looked up the definition of the word",
    "start": "1383019",
    "end": "1389350"
  },
  {
    "text": "in a dictionary in the game and that could generate a message clicks game event so these are all hypothetical by",
    "start": "1389350",
    "end": "1395769"
  },
  {
    "text": "the way I'm not saying this is what we're doing but this is just a ways to see the different ways that we could generate game events so given that use",
    "start": "1395769",
    "end": "1404470"
  },
  {
    "text": "case those game events there you can send them to a traditional warehouse as well as you could send them to a",
    "start": "1404470",
    "end": "1411100"
  },
  {
    "text": "streaming system for more real-time analysis of the of the of the events",
    "start": "1411100",
    "end": "1416230"
  },
  {
    "text": "that are being generated so one use case is this so let's just say me and my",
    "start": "1416230",
    "end": "1422950"
  },
  {
    "start": "1420000",
    "end": "1433000"
  },
  {
    "text": "coworker broth are playing this new words with friends on the iMessage platform and what we want to know though",
    "start": "1422950",
    "end": "1430299"
  },
  {
    "text": "the p.m. on the words team wants to know how hot is the game doing after went",
    "start": "1430299",
    "end": "1436120"
  },
  {
    "start": "1433000",
    "end": "1554000"
  },
  {
    "text": "live after went worldwide how well this doing so the question that they want to answer is how many installs did the new",
    "start": "1436120",
    "end": "1443769"
  },
  {
    "text": "words with friends game have on the iMessage platform in the last 10 minutes how do you do that to make that happen",
    "start": "1443769",
    "end": "1451990"
  },
  {
    "text": "we have to do a certain few things for example the game itself needs to",
    "start": "1451990",
    "end": "1457389"
  },
  {
    "text": "generate an install game event so in our case what we do is the install game",
    "start": "1457389",
    "end": "1462429"
  },
  {
    "text": "events are tracked through like Allen said the Kinesis data stream system which which helps a lot because",
    "start": "1462429",
    "end": "1469239"
  },
  {
    "text": "that solution is integrated with other services that Amazon provides and that",
    "start": "1469239",
    "end": "1474999"
  },
  {
    "text": "made the integration with Amazon Kinesis data and analytics product a lot simpler",
    "start": "1474999",
    "end": "1481539"
  },
  {
    "text": "for us and that data stream is also then sent to our data store in the backend so",
    "start": "1481539",
    "end": "1488080"
  },
  {
    "text": "I'll talk about that use case in a second but here's the case when we don't actually need a streaming service so to",
    "start": "1488080",
    "end": "1495399"
  },
  {
    "text": "answer a question like this what is the words with friends d77 install date retention and what that's",
    "start": "1495399",
    "end": "1502899"
  },
  {
    "text": "saying is that the from the moment that the user downloaded the game and started playing which percentages of those users",
    "start": "1502899",
    "end": "1510339"
  },
  {
    "text": "came back and started playing the game again on starting day 1 day 2 day 3 all",
    "start": "1510339",
    "end": "1515889"
  },
  {
    "text": "the way to day 7 the only way we can know that is if we track a user level metric so we have to track each day that",
    "start": "1515889",
    "end": "1522820"
  },
  {
    "text": "the user came and their presence was in our team in our in our system it was",
    "start": "1522820",
    "end": "1527859"
  },
  {
    "text": "present that and then we could do analysis on that particular user what for a streaming system that's really",
    "start": "1527859",
    "end": "1533829"
  },
  {
    "text": "difficult not difficult but really cost me to do because you don't want to do use a level metric tracking for everyone",
    "start": "1533829",
    "end": "1540549"
  },
  {
    "text": "in your in your system and this is going to blow up your system right and so that's not an ideal use case for",
    "start": "1540549",
    "end": "1545559"
  },
  {
    "text": "streaming processing service instead you would use a traditional warehouse once the data is collected you run some",
    "start": "1545559",
    "end": "1550809"
  },
  {
    "text": "queries on a nightly basis and figure that out so going back to our use case of",
    "start": "1550809",
    "end": "1558749"
  },
  {
    "start": "1554000",
    "end": "1708000"
  },
  {
    "text": "installs before we built that use case and the system the solution for it we",
    "start": "1558749",
    "end": "1565149"
  },
  {
    "text": "had a core set of design principles in mind that we wanted to implement in our",
    "start": "1565149",
    "end": "1570879"
  },
  {
    "text": "system so what are those principles there's any things that we considered first and foremost we wanted to use a",
    "start": "1570879",
    "end": "1577570"
  },
  {
    "text": "managed service and this is the main reason to do this is so because we didn't want to take on the operational",
    "start": "1577570",
    "end": "1583359"
  },
  {
    "text": "burden of running services systems in-house we wanted to let Amazon worry about those things and then the other",
    "start": "1583359",
    "end": "1589629"
  },
  {
    "text": "thing that we wanted to do is build a stateless design and the reason for this",
    "start": "1589629",
    "end": "1594820"
  },
  {
    "text": "is anything that's valuable so in our case our data we don't want to manage that we want",
    "start": "1594820",
    "end": "1599950"
  },
  {
    "text": "to store all that managed all that store all that user data that we generate we wanted to store that in a managed",
    "start": "1599950",
    "end": "1606100"
  },
  {
    "text": "service because it's a more reliable and they have the resource to do it and so we use the stateless design in that case",
    "start": "1606100",
    "end": "1612090"
  },
  {
    "text": "we also wanted to make sure that our pipeline was loosely coupled decoupled",
    "start": "1612090",
    "end": "1617799"
  },
  {
    "text": "right so what that meant is that every subsystem in the pipeline did one thing",
    "start": "1617799",
    "end": "1623110"
  },
  {
    "text": "really well and that what enables us to do later on is we could put in more sort",
    "start": "1623110",
    "end": "1628330"
  },
  {
    "text": "of finer grain or managing of monitoring around each so each of the subsystems and we can scale horizontally vertically",
    "start": "1628330",
    "end": "1634539"
  },
  {
    "text": "very really easy when we do that the other thing we want to do is make sure that this new system that we come",
    "start": "1634539",
    "end": "1640120"
  },
  {
    "text": "up with was extensible and what I mean by that is that if a new use case came for streaming for that we wanted to do",
    "start": "1640120",
    "end": "1647019"
  },
  {
    "text": "it we didn't want to go back to the drawing board on board and we engineers the whole pipeline we wanted to make it",
    "start": "1647019",
    "end": "1652630"
  },
  {
    "text": "extensible and so I'll talk about how we did that the other thing we wanted to do is we wanted to empower our customers",
    "start": "1652630",
    "end": "1658090"
  },
  {
    "text": "and so our game teams whenever they wanted to use our service we didn't want them to be you know blocked by us that",
    "start": "1658090",
    "end": "1664929"
  },
  {
    "text": "we do they should be self-managed and so serviced and so we wanted to make sure that they could easily use this system without worrying about a lot of",
    "start": "1664929",
    "end": "1670720"
  },
  {
    "text": "different technical details and then obviously we want to make sure the system scaled well to our traffic",
    "start": "1670720",
    "end": "1676210"
  },
  {
    "text": "patterns so that was an important consideration and then the last two fault tolerant performance system want",
    "start": "1676210",
    "end": "1682600"
  },
  {
    "text": "to make sure that especially in a streaming system if there is a disruption or if there is some sort of",
    "start": "1682600",
    "end": "1688870"
  },
  {
    "text": "issue with the system that it gracefully handles that and that becomes more important for us because if we're",
    "start": "1688870",
    "end": "1695470"
  },
  {
    "text": "looking at things like install data and if there's a bleep on the system we don't want to know we don't want to",
    "start": "1695470",
    "end": "1700600"
  },
  {
    "text": "think that people stop playing our games but but there's actually a graceful way to handle that and so I'll talk about",
    "start": "1700600",
    "end": "1706240"
  },
  {
    "text": "how we did that so that said this is the high level implementation overview of",
    "start": "1706240",
    "end": "1713139"
  },
  {
    "start": "1708000",
    "end": "1782000"
  },
  {
    "text": "that system and that's a lot of boxes but it's it's really most of it is all managed right and so on the left there",
    "start": "1713139",
    "end": "1719500"
  },
  {
    "text": "you see the data producers that would be our game teams they send data to Kinesis",
    "start": "1719500",
    "end": "1725380"
  },
  {
    "text": "data streams using the kinesin service and then that data and this is a box",
    "start": "1725380",
    "end": "1731049"
  },
  {
    "text": "around Kinesis data analytics which is the Landor processor gets and that data is then",
    "start": "1731049",
    "end": "1736590"
  },
  {
    "text": "massaged into a aggregation friend the output which I'll show you later on how I do that",
    "start": "1736590",
    "end": "1742049"
  },
  {
    "text": "then Kinesis data analytics gets that data and it actually does the aggregation and generates the",
    "start": "1742049",
    "end": "1749059"
  },
  {
    "text": "aggregations that we care about the count average Max and min and so on and",
    "start": "1749059",
    "end": "1754289"
  },
  {
    "text": "then the output is then written out to another Kinesis data stream and by doing that it makes the system of really",
    "start": "1754289",
    "end": "1760639"
  },
  {
    "text": "extensible because now we can use lambda readers or any-any consumers of Kinesis",
    "start": "1760639",
    "end": "1765690"
  },
  {
    "text": "to read that I related data to do whatever we want in our case what we did was we wrote lambda readers that read",
    "start": "1765690",
    "end": "1772350"
  },
  {
    "text": "from these Kinesis streams and glued that data back to the consumers that",
    "start": "1772350",
    "end": "1777720"
  },
  {
    "text": "cared about that data by calling their api's so here's an example imagine",
    "start": "1777720",
    "end": "1784080"
  },
  {
    "start": "1782000",
    "end": "1867000"
  },
  {
    "text": "there's a game out there called game X and they have they generate data like this right so they have a counter called",
    "start": "1784080",
    "end": "1790470"
  },
  {
    "text": "gimmicks counter and they give me a game and we call the MX ID and and they want the aggregation of this data be sent to",
    "start": "1790470",
    "end": "1798029"
  },
  {
    "text": "a metric called game X output metric in our case we use a convention of adding the type of aggregation we want so",
    "start": "1798029",
    "end": "1804809"
  },
  {
    "text": "that's the vertical CNT count at the end so with this particular game wants to",
    "start": "1804809",
    "end": "1810629"
  },
  {
    "text": "generate aggregation of counts of how many gimmicks counters it gets in a minute so how do we do that here's a",
    "start": "1810629",
    "end": "1817080"
  },
  {
    "text": "example again so imagine we played broth and I played this game and it generated",
    "start": "1817080",
    "end": "1823230"
  },
  {
    "text": "this raw data stream on the top there you see and what it has is has the game",
    "start": "1823230",
    "end": "1828480"
  },
  {
    "text": "ID it has the the time step of the record and then it has the particular",
    "start": "1828480",
    "end": "1834210"
  },
  {
    "text": "type of event that it generated so in this case is game x counter it also generated a game X message click event",
    "start": "1834210",
    "end": "1840989"
  },
  {
    "text": "which in our case we don't care about because we just want to I so we'll discard that row I'll show you how we do",
    "start": "1840989",
    "end": "1847139"
  },
  {
    "text": "that and and we just want to aggregate the data of game X counters so after going through the Kinesis data stream",
    "start": "1847139",
    "end": "1853109"
  },
  {
    "text": "system data analytics system we want the output to be what you see at the bottom",
    "start": "1853109",
    "end": "1858269"
  },
  {
    "text": "there that at the 39th minute there was the count was 3 and at the 55th minute",
    "start": "1858269",
    "end": "1863940"
  },
  {
    "text": "the count was 1 so how do we do that the key component to make that happen is",
    "start": "1863940",
    "end": "1870000"
  },
  {
    "start": "1867000",
    "end": "1875000"
  },
  {
    "text": "the preprocessor the Landrieu pre-processors so here's the diagram of",
    "start": "1870000",
    "end": "1875280"
  },
  {
    "start": "1875000",
    "end": "1946000"
  },
  {
    "text": "what our preprocessor lamda does so we could do whatever we want in this lambda in our case what we do is we take that",
    "start": "1875280",
    "end": "1881340"
  },
  {
    "text": "raw input stream and we transform it to be an aggregation friendly input for the Kinesis data analytics and then that is",
    "start": "1881340",
    "end": "1889650"
  },
  {
    "text": "done through three functions right so first because we retrieve data set from the Kinesis streams we're getting the",
    "start": "1889650",
    "end": "1896610"
  },
  {
    "text": "data in the Kinesis producer library formatted data right which the lambda or the key Kinesis data analytics at that",
    "start": "1896610",
    "end": "1903390"
  },
  {
    "text": "time didn't understand so what we had to do is we had to parse that and kind of explode that in to be not not to be a",
    "start": "1903390",
    "end": "1910260"
  },
  {
    "text": "Kinesis producer library formatted batch and then then what we do is we would match records data records to relevant",
    "start": "1910260",
    "end": "1917220"
  },
  {
    "text": "metrics so the by relevance I mean metrics that game teams care about not that not all the data records but the",
    "start": "1917220",
    "end": "1923520"
  },
  {
    "text": "data records that they want to subscribe to right so we will we throw away stuff that we filter out the data that the",
    "start": "1923520",
    "end": "1928560"
  },
  {
    "text": "game teams don't want and only keep the ones that they want and then the third thing that our lambda does is it Maps",
    "start": "1928560",
    "end": "1934200"
  },
  {
    "text": "the input data to an output metric taxonomy or the schema that that that I",
    "start": "1934200",
    "end": "1939300"
  },
  {
    "text": "showed you earlier so then sometimes that could be multiple metrics that we do that too so going",
    "start": "1939300",
    "end": "1946710"
  },
  {
    "text": "back to our example of the raw entries that you saw the CSV entries now after going through the preprocessor",
    "start": "1946710",
    "end": "1952830"
  },
  {
    "text": "in our case the those raw those are all entries are turned into these JSON blobs and that's the schema that you see",
    "start": "1952830",
    "end": "1959250"
  },
  {
    "text": "that's a that we wrote our Kinesis data analytic sequel around to make it aggregation friendly so what you see",
    "start": "1959250",
    "end": "1965250"
  },
  {
    "text": "there is the time is its own its own key value then the source the metric which",
    "start": "1965250",
    "end": "1970290"
  },
  {
    "text": "is the output that we care about the detail in this case doesn't matter but if people wanted to add other details to",
    "start": "1970290",
    "end": "1975420"
  },
  {
    "text": "aggregate by they could do that and then the actual value for that particular roast in this case the value was one and",
    "start": "1975420",
    "end": "1983370"
  },
  {
    "text": "then the next thing that we do is the sequel so what is this equal look like",
    "start": "1983370",
    "end": "1988530"
  },
  {
    "text": "right so at a high level the sequel does this right so there's the top yellow box is the preprocessor output and so that's",
    "start": "1988530",
    "end": "1994950"
  },
  {
    "text": "what we transform the raw data to the taxonomy output texana me then the the next set of gray boxes is the sequel",
    "start": "1994950",
    "end": "2000650"
  },
  {
    "text": "part so our sequel is actually designed around three windows and I'll talk to",
    "start": "2000650",
    "end": "2006730"
  },
  {
    "text": "you talk to you guys about why we use three windows so the first window is where we group by the row type which is",
    "start": "2006730",
    "end": "2012850"
  },
  {
    "text": "the timestamp that the Kinesis data analytics gets on arrival of the raw",
    "start": "2012850",
    "end": "2017980"
  },
  {
    "text": "data the record time which is the event timestamp that Allen talked about earlier and and the source and the",
    "start": "2017980",
    "end": "2023830"
  },
  {
    "text": "metric and the detail which which you saw in the previous slide and then we use the floor sequel function which the",
    "start": "2023830",
    "end": "2029710"
  },
  {
    "text": "Kinesis data analytic system provides to to kind of normalize the times to the",
    "start": "2029710",
    "end": "2034750"
  },
  {
    "text": "closest minute and that's how we get rid of the seconds in in the in the time step because we want to do a minute",
    "start": "2034750",
    "end": "2039820"
  },
  {
    "text": "level aggregation so the output of that is then sent to another tumbling window",
    "start": "2039820",
    "end": "2045000"
  },
  {
    "text": "in there we merge multiple input streams so imagine if you have one particular",
    "start": "2045000",
    "end": "2050970"
  },
  {
    "text": "event game event that's of high volume and and you want to be able to keep up",
    "start": "2050970",
    "end": "2056560"
  },
  {
    "text": "with that high volume so Kinesis data analytic streams what they provide is the ability to increase the input",
    "start": "2056560",
    "end": "2061750"
  },
  {
    "text": "parallelism and so in our and so when that happens your input stream is then split into different streams then you",
    "start": "2061750",
    "end": "2068500"
  },
  {
    "text": "need to basically merge them back in in order to make some analysis around it but if you because if you treated them",
    "start": "2068500",
    "end": "2074620"
  },
  {
    "text": "separately then your final output could be incorrect right because one stream",
    "start": "2074620",
    "end": "2079898"
  },
  {
    "text": "could have more counts than the other stream so the second tumbling window we use to merge those multiple inputs again",
    "start": "2079899",
    "end": "2086139"
  },
  {
    "text": "using by group PI and then the third window which is where we use a sliding window that's actually where we",
    "start": "2086140",
    "end": "2092830"
  },
  {
    "text": "calculate the count the sum the average the minimum max over ten minute sliding",
    "start": "2092830",
    "end": "2097990"
  },
  {
    "text": "window so why don't we go with 10 minutes because we know that in mind in our case our data usually on worst case",
    "start": "2097990",
    "end": "2103450"
  },
  {
    "text": "scenario takes 10 minutes to get through the pipeline so we wanted to kind of amend that later arriving data by giving",
    "start": "2103450",
    "end": "2109360"
  },
  {
    "text": "enough time but usually on average it takes about three minutes so 10 minutes is more than enough in our case and then",
    "start": "2109360",
    "end": "2116080"
  },
  {
    "text": "finally the data of that aggregated values are written out to another Kinesis tree which the consumers consume by using a lambda so going back to our",
    "start": "2116080",
    "end": "2123460"
  },
  {
    "start": "2122000",
    "end": "2161000"
  },
  {
    "text": "example what does that look like so the data that after the first window of that",
    "start": "2123460",
    "end": "2130000"
  },
  {
    "text": "tumbling window this is what the data ends up looking like right so there you can see in green the the seconds have been normalized right",
    "start": "2130000",
    "end": "2136200"
  },
  {
    "text": "so that's close it's rounded off to the nearest minute but here what we really want is that the 39th minute and the",
    "start": "2136200",
    "end": "2142800"
  },
  {
    "text": "40th minute output we want them to we want to merge them together because it's",
    "start": "2142800",
    "end": "2147809"
  },
  {
    "text": "really when it within the minute boundary and then the 55th minute can be its own output which is fine so how do",
    "start": "2147809",
    "end": "2153839"
  },
  {
    "text": "we do that and that's where the second tumbling window comes into the third sliding window comes into play so let me",
    "start": "2153839",
    "end": "2159780"
  },
  {
    "text": "walk you through a scenario I think this might be more telling of the different output that you can get using a tumbling",
    "start": "2159780",
    "end": "2165750"
  },
  {
    "start": "2161000",
    "end": "2185000"
  },
  {
    "text": "window so imagine that you have a system that produces a thousand records and it's for within a minute so from twelve",
    "start": "2165750",
    "end": "2172319"
  },
  {
    "text": "to twelve oh one so when it goes through processing in the first minute it gets 800 records in the second minute it gets",
    "start": "2172319",
    "end": "2179220"
  },
  {
    "text": "190 and the third minute it gets ten so what does this data look like when it goes through a tumbling window so the",
    "start": "2179220",
    "end": "2186030"
  },
  {
    "start": "2185000",
    "end": "2230000"
  },
  {
    "text": "output would look something like this where in the at 1201 you're going to get a hundred so sorry 800 at 1202 you're",
    "start": "2186030",
    "end": "2192569"
  },
  {
    "text": "gonna get hundred and ninety and then at 12:03 you'll get ten right and and this isn't so if we had this solution then",
    "start": "2192569",
    "end": "2199770"
  },
  {
    "text": "our consumers will need to be smarter and be more stateful because they'll need to aggregate all the outputs across",
    "start": "2199770",
    "end": "2205200"
  },
  {
    "text": "all those minutes into coming up with a final thousand answer so that wasn't desirable the other issue with this is",
    "start": "2205200",
    "end": "2211079"
  },
  {
    "text": "that if there's a disruption or if our your Canisius data analytics app needs to be restarted what will happen is then",
    "start": "2211079",
    "end": "2218280"
  },
  {
    "text": "the the system might reproduce duplicate data it will repeat the output and so",
    "start": "2218280",
    "end": "2223589"
  },
  {
    "text": "even if we had a stateful consumer it's gonna get a false aggregation value so that wasn't quite what we were looking",
    "start": "2223589",
    "end": "2229319"
  },
  {
    "text": "for now what if you what if you go through a sliding window right so with",
    "start": "2229319",
    "end": "2234720"
  },
  {
    "start": "2230000",
    "end": "2291000"
  },
  {
    "text": "the sliding window the data is kind of sliding of the windows kind of sliding so every the the cardinality of this",
    "start": "2234720",
    "end": "2241829"
  },
  {
    "text": "output is going to be really high so for a thousand input records there's give me a thousand output records and each time",
    "start": "2241829",
    "end": "2247470"
  },
  {
    "text": "the aggregation is going to increase right and what we really want is the last one which is the 1200 332 of a",
    "start": "2247470",
    "end": "2253440"
  },
  {
    "text": "thousand records right but what this means is not that our consumers needs to consume a lot of data right which which",
    "start": "2253440",
    "end": "2259230"
  },
  {
    "text": "again wasn't ideal and so what we really wanted to do is use a multi window chain",
    "start": "2259230",
    "end": "2264690"
  },
  {
    "text": "approach right and we wanted to combine the the cardinality cardinality reduction feature of a tumbling window with the",
    "start": "2264690",
    "end": "2271699"
  },
  {
    "text": "summation feature of a sliding window and and produce something like this so at 12:01 it produces 8 800 records at",
    "start": "2271699",
    "end": "2279199"
  },
  {
    "text": "12:02 we it produces 990 and then at 12:03 it produces a thousand records and",
    "start": "2279199",
    "end": "2284449"
  },
  {
    "text": "so that's manageable it's small enough data stream and it's the last value is the correct value and it's not a lot of data so that's what we want so now in",
    "start": "2284449",
    "end": "2293329"
  },
  {
    "start": "2291000",
    "end": "2322000"
  },
  {
    "text": "our in our solution like I showed earlier we're using three windows and the tumbling windows is a 1-minute",
    "start": "2293329",
    "end": "2301689"
  },
  {
    "text": "summation the group by is what by done by 1-minute windows and then that all of",
    "start": "2301689",
    "end": "2306769"
  },
  {
    "text": "that data is fed into a sliding window which is the sequel of window at the bottom that you see and I'm so there's",
    "start": "2306769",
    "end": "2313309"
  },
  {
    "text": "improvements on how we could do this better but that at the time when we did this we use the floor function to do",
    "start": "2313309",
    "end": "2318529"
  },
  {
    "text": "that in today you could use a step function so now the final data output",
    "start": "2318529",
    "end": "2323599"
  },
  {
    "start": "2322000",
    "end": "2345000"
  },
  {
    "text": "the Kinesis data output is what we want so at the 39th minute you get the count 3 which is what we want it the sum is also 3 and then the",
    "start": "2323599",
    "end": "2331309"
  },
  {
    "text": "55th minute gets the count 1 this is exactly what we want it so this is just with one one particular metric that we",
    "start": "2331309",
    "end": "2338660"
  },
  {
    "text": "want it to have your gate but what if a game team goes and adds another metric they want to aggregate by we don't want",
    "start": "2338660",
    "end": "2344239"
  },
  {
    "text": "to exactly go and we start or our applications because that would disrupt the existing streaming aggregations that",
    "start": "2344239",
    "end": "2352039"
  },
  {
    "start": "2345000",
    "end": "2359000"
  },
  {
    "text": "are occurring and we didn't want to do that so how do we achieve this how do we achieve a non-disruptive updates",
    "start": "2352039",
    "end": "2357279"
  },
  {
    "text": "implementation into our system so what we did was then we created this little",
    "start": "2357279",
    "end": "2362619"
  },
  {
    "start": "2359000",
    "end": "2418000"
  },
  {
    "text": "subsystem so on the left you see that bubble is of the custom customers or the game teams what they do is they write",
    "start": "2362619",
    "end": "2369199"
  },
  {
    "text": "into a metric into an s3 file with the custom of the aggregation metric that they care about then our preprocessor periodically will",
    "start": "2369199",
    "end": "2376429"
  },
  {
    "text": "read from that s3 bucket file and then the Kinesis data data analytics invokes",
    "start": "2376429",
    "end": "2382459"
  },
  {
    "text": "the preprocessor whenever it invokes it starts noticing that there's a new set of metrics that we need to start",
    "start": "2382459",
    "end": "2387920"
  },
  {
    "text": "aggregating by and everything just works after that right and and so how do we get the lab the metric from s3 into the",
    "start": "2387920",
    "end": "2395809"
  },
  {
    "text": "preprocessor well so the only pieces of managed hardware for this system that we have is",
    "start": "2395809",
    "end": "2401010"
  },
  {
    "text": "our Jenkins build server and so what our lambda does is it actually makes a cost of our Jenkins build box which then",
    "start": "2401010",
    "end": "2406710"
  },
  {
    "text": "generates the preprocessor metrics and pushes it into s3 which then the",
    "start": "2406710",
    "end": "2411810"
  },
  {
    "text": "preprocessor reads from so this is a pretty nice and simple solution that worked out pretty well for us so with",
    "start": "2411810",
    "end": "2419340"
  },
  {
    "text": "that said this is a system that works pretty well but it's a largely managed system mostly by Amazon's",
    "start": "2419340",
    "end": "2425340"
  },
  {
    "text": "and so we wanted first make sure that our system is correct so we wanted to measure correctness we wanted to measure",
    "start": "2425340",
    "end": "2432420"
  },
  {
    "text": "the completeness of our system like is it getting all the data that it cares about that we care about and lastly that",
    "start": "2432420",
    "end": "2437880"
  },
  {
    "text": "the data is fresh enough right that there's no really lag real lag in the system so with the Kinesis data",
    "start": "2437880",
    "end": "2443580"
  },
  {
    "text": "analytics system you get a metric called Millie's behind latest right that's a called watch metric that you get it what",
    "start": "2443580",
    "end": "2450210"
  },
  {
    "start": "2445000",
    "end": "2468000"
  },
  {
    "text": "with this you can tell if you particularly if one of your Kinesis data analytics application is falling behind",
    "start": "2450210",
    "end": "2455430"
  },
  {
    "text": "in reading the stream input that it's getting that's good and it's helpful but",
    "start": "2455430",
    "end": "2460710"
  },
  {
    "text": "it doesn't tell you if the data is complete correct or fresh so this is where we implemented another monitoring",
    "start": "2460710",
    "end": "2466530"
  },
  {
    "text": "system called the heartbeat monitoring so the heartbeat monitoring is going",
    "start": "2466530",
    "end": "2471930"
  },
  {
    "text": "back to our high-level overview of our system is a system that artificially generates game events every minute a",
    "start": "2471930",
    "end": "2480900"
  },
  {
    "start": "2473000",
    "end": "2514000"
  },
  {
    "text": "hundred artificial game events every minute had to injection points in the system so where you see there is that at",
    "start": "2480900",
    "end": "2488100"
  },
  {
    "text": "the start of the pipeline through the HTTP layer we inject 100 records using a",
    "start": "2488100",
    "end": "2494100"
  },
  {
    "text": "lambda and that we call tier 0 injector and then another injector we put in that",
    "start": "2494100",
    "end": "2499260"
  },
  {
    "text": "injects directly into the Kinesis data stream bypassing the HTTP endpoints and",
    "start": "2499260",
    "end": "2504510"
  },
  {
    "text": "which we call tier 1 injector and I'll talk about why that was important when I give you the metrics that actually",
    "start": "2504510",
    "end": "2510360"
  },
  {
    "text": "collected that but the taxonomy that each of these injectors generate is slightly different because we needed to",
    "start": "2510360",
    "end": "2516660"
  },
  {
    "text": "be able to compare the output between those two injectors later on so what you see there's the kingdom is set to tier 0",
    "start": "2516660",
    "end": "2523620"
  },
  {
    "text": "for pure zero injector and tier one for tier 1 injector so here's an actual clot",
    "start": "2523620",
    "end": "2529140"
  },
  {
    "start": "2528000",
    "end": "2590000"
  },
  {
    "text": "watch metric graph for these injectors so what you see there is the tier 0 injector metrics",
    "start": "2529140",
    "end": "2534750"
  },
  {
    "text": "and there where you see is if we get a hundred consistently for every minute we're doing good we're not losing data",
    "start": "2534750",
    "end": "2541440"
  },
  {
    "text": "and we're not duplicating the data that's doing this doing well but what you see here is there's a lot of spikes",
    "start": "2541440",
    "end": "2546600"
  },
  {
    "text": "right and I'll talk about why we have a lot of spikes but sometimes like you see on March 12th that there's a dip of data",
    "start": "2546600",
    "end": "2554430"
  },
  {
    "text": "and then there's immediately after that there's a big spike right after right so what do we make of this right so what",
    "start": "2554430",
    "end": "2560100"
  },
  {
    "text": "this tells us is two things it could be one of two things one is that are the",
    "start": "2560100",
    "end": "2565470"
  },
  {
    "text": "HTTP layer that we inject our data through is generating duplicate data through retries and this is a common use",
    "start": "2565470",
    "end": "2571860"
  },
  {
    "text": "case especially if you have a mobile device generating data into a pipeline giving connectivity issues and whatnot",
    "start": "2571860",
    "end": "2577290"
  },
  {
    "text": "it'll do a lot of retries or the second thing is that our Kinesis and the Kinesis data analytic system is actually",
    "start": "2577290",
    "end": "2583470"
  },
  {
    "text": "dropping data and duplicating data right so how do we know which one is it right and that's where the second heartbeat",
    "start": "2583470",
    "end": "2589350"
  },
  {
    "text": "metric comes into play this is the Tier one heartbeat metric and here as you can see on march 12 there was no dip or",
    "start": "2589350",
    "end": "2596040"
  },
  {
    "start": "2590000",
    "end": "2630000"
  },
  {
    "text": "spike before the same data set so what that tells us is that our this was our HTTP layer that was generating duplicate",
    "start": "2596040",
    "end": "2602460"
  },
  {
    "text": "data and so the other spikes that you see throughout the other days that's more than a hundred that shows that that",
    "start": "2602460",
    "end": "2609570"
  },
  {
    "text": "usually happens when we retry sending data to our Kinesis streams and sometimes when there's a to put issue or",
    "start": "2609570",
    "end": "2616380"
  },
  {
    "text": "whatnot we'll do a lot of retries and sometimes EDI will get through sometimes they won't so we have to retry and that's what those spikes are indicative",
    "start": "2616380",
    "end": "2622380"
  },
  {
    "text": "of so things are good so this system monitoring system that we built we felt really confident with our solution",
    "start": "2622380",
    "end": "2628860"
  },
  {
    "text": "afterwards okay so now what's our scorecard how do we do in our implementation going back to our design",
    "start": "2628860",
    "end": "2634830"
  },
  {
    "start": "2630000",
    "end": "2636000"
  },
  {
    "text": "principles that we started out with did we meet all of them right so our heart means that yes we met so let me walk",
    "start": "2634830",
    "end": "2641610"
  },
  {
    "start": "2636000",
    "end": "2747000"
  },
  {
    "text": "through them really quick so for managed service yeah so we used lambdas we use Kinesis data analytics and we uses",
    "start": "2641610",
    "end": "2647190"
  },
  {
    "text": "Kinesis data streams to build this the only manner service that we have is an in-house Jenkins job which also by the",
    "start": "2647190",
    "end": "2652920"
  },
  {
    "text": "way runs on EC yes so that made it really simple that that we met that goal state let the stateless stateless design",
    "start": "2652920",
    "end": "2660690"
  },
  {
    "text": "we store all of our game events and Kinesis data streams so we didn't have to worry about storing that data loosely",
    "start": "2660690",
    "end": "2666300"
  },
  {
    "text": "loosely coupled as saw our input of our data is decoupled from the filtering of our data to the",
    "start": "2666300",
    "end": "2672660"
  },
  {
    "text": "aggregation of our data to the output of our data so that worked really well the extensible system a just writing it to",
    "start": "2672660",
    "end": "2678630"
  },
  {
    "text": "Kinesis streams Canisius data streams made that our system was quite extensible by using s3 as a way to",
    "start": "2678630",
    "end": "2684420"
  },
  {
    "text": "collect the metrics that game teams want to aggregate on they it's all controlled by them",
    "start": "2684420",
    "end": "2690420"
  },
  {
    "text": "we do have smarts built in if there's a typo or something like that we'll generate a nice email for our game event",
    "start": "2690420",
    "end": "2697200"
  },
  {
    "text": "game team and let them know that hey there's a type on your aggregation metrics but now it's all automated and it empowers the customers so we weren't",
    "start": "2697200",
    "end": "2703770"
  },
  {
    "text": "the blockers for that and then the Scout scalable system again just by subscribing and aggregating data that",
    "start": "2703770",
    "end": "2709350"
  },
  {
    "text": "game teams care about we made the system quite scalable instead of instead of aggregating every single type of game",
    "start": "2709350",
    "end": "2715230"
  },
  {
    "text": "event we just aggregate the data the game teams care about so that that made the system scalable for us and reduced",
    "start": "2715230",
    "end": "2720570"
  },
  {
    "text": "cost quite a bit and then the last two fault tolerant and performant we completely rely on Amazon to help us do",
    "start": "2720570",
    "end": "2725670"
  },
  {
    "text": "that right and so we expect that the Amazon system services are you know graceful",
    "start": "2725670",
    "end": "2731520"
  },
  {
    "text": "when it comes to disruptions off of their systems when things need to be rebooted and they do it within our SLA s",
    "start": "2731520",
    "end": "2736860"
  },
  {
    "text": "which makes it or which makes our system quite fault-tolerant and performant was just to be able to handle the amount of",
    "start": "2736860",
    "end": "2742440"
  },
  {
    "text": "data that comes through the system so we did what pretty well we were quite satisfied with the implementation so",
    "start": "2742440",
    "end": "2748620"
  },
  {
    "start": "2747000",
    "end": "2753000"
  },
  {
    "text": "what are some of the best practices in this solution that we learned throughout through our building this system five",
    "start": "2748620",
    "end": "2755490"
  },
  {
    "start": "2753000",
    "end": "2857000"
  },
  {
    "text": "things right so first we learned quickly that we don't need to over engineer keep it simple keep keep only apply the use",
    "start": "2755490",
    "end": "2762000"
  },
  {
    "text": "cases that we need to implement the use cases that we need to apply for right now and then make it extensible later on",
    "start": "2762000",
    "end": "2768870"
  },
  {
    "text": "like which we did the second thing was take advantage of the exactly once semantics of the preprocessor lambda so",
    "start": "2768870",
    "end": "2776970"
  },
  {
    "text": "lambda is by design can be involved multiple times they give you at least one semantics which doesn't work for a",
    "start": "2776970",
    "end": "2782550"
  },
  {
    "text": "streaming system you don't want a bleep or a spike in your installed data stream",
    "start": "2782550",
    "end": "2787560"
  },
  {
    "text": "when it wasn't actually your installed so what this with Kinesis data analytics promises is that even though the lambda",
    "start": "2787560",
    "end": "2794430"
  },
  {
    "text": "preprocessor could be invoked multiple times the input of that data that it gets is going to be exactly what so it",
    "start": "2794430",
    "end": "2799830"
  },
  {
    "text": "deduplicate s-- the data for us which pretty well for us to take advantage of that and the other thing you want to do is use the preprocessor to transform the",
    "start": "2799830",
    "end": "2806700"
  },
  {
    "text": "data to be an aggregation friendly format so by doing that it removes the need to restart your Kinesis data",
    "start": "2806700",
    "end": "2812400"
  },
  {
    "text": "analytics or update the sequel every time there's a new metric you need to track you just you just need to add configuration and everything just works",
    "start": "2812400",
    "end": "2819420"
  },
  {
    "text": "just fine you don't need to update the sequel every time so take taking anon take advantage of the preprocessor in that way and then also design with the",
    "start": "2819420",
    "end": "2826140"
  },
  {
    "text": "consumers in mind right so we wanted to simplify the lives of our consumers we don't want to have them implement",
    "start": "2826140",
    "end": "2831329"
  },
  {
    "text": "stateful designs or we implement the Kinesis client libraries again on their",
    "start": "2831329",
    "end": "2837000"
  },
  {
    "text": "sides to reduce to retrieve the data so in our case we used lambdas to just call their API and did all the heavy lifting",
    "start": "2837000",
    "end": "2843359"
  },
  {
    "text": "for them and last but not least use an OP team based aggregation system so",
    "start": "2843359",
    "end": "2849299"
  },
  {
    "text": "again this is referring to the case that don't aggregate everything only aggregate the data that your customers",
    "start": "2849299",
    "end": "2854490"
  },
  {
    "text": "care about in our case our game teams okay great so that's everything thank",
    "start": "2854490",
    "end": "2860220"
  },
  {
    "start": "2857000",
    "end": "2876000"
  },
  {
    "text": "you for that and I guess now time for some questions Allen okay so it looks like we have 11",
    "start": "2860220",
    "end": "2868170"
  },
  {
    "text": "minutes",
    "start": "2868170",
    "end": "2870440"
  },
  {
    "text": "so yeah any questions you have for probably milind more than me but any questions shout him out yep so the",
    "start": "2873240",
    "end": "2888670"
  },
  {
    "start": "2876000",
    "end": "3073000"
  },
  {
    "text": "question was what's the advantage of using cases data streams versus Kinesis data firehose",
    "start": "2888670",
    "end": "2894490"
  },
  {
    "text": "so Kinesis data firehose is primarily a although it does have a streaming component in is primarily a managed",
    "start": "2894490",
    "end": "2900580"
  },
  {
    "text": "consumer for your stream so it manages the the buffering of the data and the",
    "start": "2900580",
    "end": "2906100"
  },
  {
    "text": "delivery of your data to one of four destinations can be Splunk redshift",
    "start": "2906100",
    "end": "2911410"
  },
  {
    "text": "elasticsearch or s3 so if ultimately all you want to do is stream data in and",
    "start": "2911410",
    "end": "2916750"
  },
  {
    "text": "deliver it to one of those four places firehose is a better solution however with data streams gives you the",
    "start": "2916750",
    "end": "2923950"
  },
  {
    "text": "ability to have a custom streaming processor using AWS lambda using spark",
    "start": "2923950",
    "end": "2930190"
  },
  {
    "text": "streaming using flink any number of kind of open source or third-party tools or",
    "start": "2930190",
    "end": "2935260"
  },
  {
    "text": "our own library which is the Kinesis client library which you would run on ec2 so streams is like a I want to write",
    "start": "2935260",
    "end": "2942640"
  },
  {
    "text": "code to do custom processing of my streaming data and fire hoses I have streaming data and I largely want to",
    "start": "2942640",
    "end": "2948880"
  },
  {
    "text": "just put it into one of these four destinations and in our case we were already in the Kinesis stream ecosystem",
    "start": "2948880",
    "end": "2955780"
  },
  {
    "text": "and we didn't want to bring another system so it made sense for us to continue using that ecosystem anything",
    "start": "2955780",
    "end": "2964780"
  },
  {
    "text": "else way in the back",
    "start": "2964780",
    "end": "2968220"
  },
  {
    "text": "yeah so most of our lag comes in that batching so we use fluent to write to",
    "start": "2983760",
    "end": "2989740"
  },
  {
    "text": "Kinesis which does micro batches for us and then also when we read data from Kinesis to loading to our consumers or",
    "start": "2989740",
    "end": "2997060"
  },
  {
    "text": "to send to Vertica sorry not vertical but the knesset data streams to get",
    "start": "2997060",
    "end": "3002850"
  },
  {
    "text": "Kinesis data analytic stream again there's batching going on there as well so that batching is what induces the delay sometimes it is the network",
    "start": "3002850",
    "end": "3010830"
  },
  {
    "text": "connectivity from game teams that when they send that data that that it's an it's up in its in the transmission phase",
    "start": "3010830",
    "end": "3017280"
  },
  {
    "text": "and it hasn't got to us quite yet so typically on worst case scenarios we see 10 minutes but usually it's 3 minutes",
    "start": "3017280",
    "end": "3024350"
  },
  {
    "text": "yeah so we do 2 types of micro batches one is size based and the other is time",
    "start": "3026930",
    "end": "3032430"
  },
  {
    "text": "based so I think our micro batches are 6 bags or I forget so 6 Meg's big and then the",
    "start": "3032430",
    "end": "3038790"
  },
  {
    "text": "time is I think 10 minutes because there are some stream data streams that we get a lot of events which I hit that size",
    "start": "3038790",
    "end": "3044370"
  },
  {
    "text": "really quickly and some they're really slow which will hit the time quickly",
    "start": "3044370",
    "end": "3049970"
  },
  {
    "text": "anymore way over there on the right",
    "start": "3052310",
    "end": "3057890"
  },
  {
    "text": "I don't know I couldn't hear it did you hear my speaking up a little bit sorry we don't have microphones it does Inga",
    "start": "3070530",
    "end": "3082560"
  },
  {
    "start": "3073000",
    "end": "3101000"
  },
  {
    "text": "did they have cases of log streaming",
    "start": "3082560",
    "end": "3086060"
  },
  {
    "text": "yeah the question is what's our story around log streaming basically is that",
    "start": "3087860",
    "end": "3094890"
  },
  {
    "text": "the question like how how would you stream existing vlogs so there there's a",
    "start": "3094890",
    "end": "3101970"
  },
  {
    "start": "3101000",
    "end": "3191000"
  },
  {
    "text": "number ways to do it there's a lot of products like fluent II that have agents that you could use to stream to a",
    "start": "3101970",
    "end": "3107940"
  },
  {
    "text": "Kinesis stream or a Kinesis data firehose we have an open source library called the Kinesis agent which",
    "start": "3107940",
    "end": "3114090"
  },
  {
    "text": "essentially tails existing log files so if your system is already just logging",
    "start": "3114090",
    "end": "3119760"
  },
  {
    "text": "to disk you can stream those log records as streaming data by installing the",
    "start": "3119760",
    "end": "3127110"
  },
  {
    "text": "Kinesis agent and configuring it to tail those log files and then every",
    "start": "3127110",
    "end": "3132540"
  },
  {
    "text": "individual row that gets added to your log file will be a record that can get streamed to either Kinesis data streams",
    "start": "3132540",
    "end": "3138360"
  },
  {
    "text": "or Kinesis data firehose so it's a nice solution if you've got an existing system you don't want to have to introduce code changes to produce the",
    "start": "3138360",
    "end": "3145740"
  },
  {
    "text": "data you can just install an agent that'll run in the background and stream those records",
    "start": "3145740",
    "end": "3151040"
  },
  {
    "text": "yeah",
    "start": "3154490",
    "end": "3157490"
  },
  {
    "text": "those for you I think these are good can you repeat you mind speaking up but that",
    "start": "3169600",
    "end": "3174850"
  },
  {
    "text": "was for a millon right on their their use case yeah the query latency right so",
    "start": "3174850",
    "end": "3196900"
  },
  {
    "start": "3191000",
    "end": "3197000"
  },
  {
    "text": "the question was how long does it take for us to query the the data that comes",
    "start": "3196900",
    "end": "3202990"
  },
  {
    "text": "in through the streaming system and then do we expose this to game teams in our",
    "start": "3202990",
    "end": "3208030"
  },
  {
    "text": "case well we do to let them know what SLA is are so we tell them that within their four game adds a new metric within",
    "start": "3208030",
    "end": "3215380"
  },
  {
    "text": "ten minutes they should start seeing their data and if that doesn't happen then we have internal cloud launch",
    "start": "3215380",
    "end": "3220810"
  },
  {
    "text": "alarms that tell us like hey there's something wrong way before they find out so we try to address that as quickly as",
    "start": "3220810",
    "end": "3226750"
  },
  {
    "text": "possible but within within the company we let people with all the game teams know that this is the SLA that are",
    "start": "3226750",
    "end": "3232360"
  },
  {
    "text": "currently advertised and then usually they're pretty happy about that just letting them know giving them giving",
    "start": "3232360",
    "end": "3238120"
  },
  {
    "text": "keeping that transparent with them keeps them happy so they can build their East cases or reports around that as opposed",
    "start": "3238120",
    "end": "3244180"
  },
  {
    "text": "to letting them guess so it is being crisp paired with the game teams yeah",
    "start": "3244180",
    "end": "3252570"
  },
  {
    "text": "could you speak up it's hard to hear there's some noise up here",
    "start": "3257180",
    "end": "3262088"
  },
  {
    "text": "yeah I tell you what yeah sure so you",
    "start": "3271880",
    "end": "3285830"
  },
  {
    "start": "3285000",
    "end": "3352000"
  },
  {
    "text": "would have to deliver that so the question is how would you how does this integrate with Athena is that the the",
    "start": "3285830",
    "end": "3291170"
  },
  {
    "text": "basic question where are you there you go yes sir yeah so essentially Athena needs the",
    "start": "3291170",
    "end": "3298100"
  },
  {
    "text": "data to live in in s3 right so what you need to do is deliver this data your streaming data to s3 one popular way to",
    "start": "3298100",
    "end": "3305720"
  },
  {
    "text": "do that would be to go through Kinesis data fire hose and have it deliver the data to s3 and then of course configure",
    "start": "3305720",
    "end": "3313010"
  },
  {
    "text": "athena to use the the data that got delivered there you can build your own custom processor as well either from",
    "start": "3313010",
    "end": "3319100"
  },
  {
    "text": "data streams or data fire data firehose with lambda if you want to do some transformation but ultimately you have",
    "start": "3319100",
    "end": "3324470"
  },
  {
    "text": "to get the data into a 3-2 that Athena can leverage it yeah",
    "start": "3324470",
    "end": "3333040"
  },
  {
    "text": "let me take that one sure so the question is something like an sqs versus",
    "start": "3348910",
    "end": "3355070"
  },
  {
    "start": "3352000",
    "end": "3499000"
  },
  {
    "text": "a Kinesis datastreams when would you use a streaming system versus a queueing system alright so",
    "start": "3355070",
    "end": "3361790"
  },
  {
    "text": "there's a few a few differences the biggest difference is with a Kinesis",
    "start": "3361790",
    "end": "3367520"
  },
  {
    "text": "stream with the data stream you can have multiple consumers processing the same records concurrently in parallel at",
    "start": "3367520",
    "end": "3374810"
  },
  {
    "text": "their own rate right so if that's your requirement then a stream is really what",
    "start": "3374810",
    "end": "3379880"
  },
  {
    "text": "you're after with a queue typically you're talking about I put a message into a queue and I want to have one",
    "start": "3379880",
    "end": "3385630"
  },
  {
    "text": "consuming process pop the message off the queue and do something with it and then the message is not available to",
    "start": "3385630",
    "end": "3391580"
  },
  {
    "text": "other consumers right so that's one of the primary differences between like a",
    "start": "3391580",
    "end": "3398000"
  },
  {
    "text": "queue and a stream some others but that's the big one to add to that I'd say what we do is uh the the batches",
    "start": "3398000",
    "end": "3404450"
  },
  {
    "text": "once their post Kinesis we actually queue them at using sqs and then we have consumers that running ECS",
    "start": "3404450",
    "end": "3410780"
  },
  {
    "text": "that retrieve data from Leki unloaded to our warehouse which is in Vertica so that for that we use a queueing system",
    "start": "3410780",
    "end": "3416330"
  },
  {
    "text": "but for the streaming use case we use kinesin streams because that that was an ecosystem that already existed on the",
    "start": "3416330",
    "end": "3424400"
  },
  {
    "text": "flip side of that we have time yes so let's say you had a number of messages but you wanted to scale your consumers",
    "start": "3424400",
    "end": "3430400"
  },
  {
    "text": "based on the number of messages so I've got you know one piece of work to do but I want to do it in parallel so I want to",
    "start": "3430400",
    "end": "3436580"
  },
  {
    "text": "have dozens of servers all doing this same thing then a queue would be a good solution because you can scale your",
    "start": "3436580",
    "end": "3441920"
  },
  {
    "text": "solution based on the number of messages in the queue right you can use the queue depth to determine okay now I should add",
    "start": "3441920",
    "end": "3448610"
  },
  {
    "text": "more more workers more consumers to do some work but those would be doing the same thing as opposed to a stream where",
    "start": "3448610",
    "end": "3455120"
  },
  {
    "text": "you have multiple consumers who might be doing different things in parallel",
    "start": "3455120",
    "end": "3459790"
  },
  {
    "text": "anything else yeah",
    "start": "3462330",
    "end": "3466940"
  },
  {
    "text": "is there a limit to the number of sequels that you can run in analytics so there is a limit which is what we call",
    "start": "3470780",
    "end": "3477569"
  },
  {
    "text": "in application streams so if you remember when I was talking earlier I was saying there's the the statement",
    "start": "3477569",
    "end": "3483660"
  },
  {
    "text": "that was create stream and it looked a lot like a table you can have 15 I believe I think Ryan",
    "start": "3483660",
    "end": "3490440"
  },
  {
    "text": "is just left 15 in application streams is the maximum that you can have in a single Kinesis analytics application so",
    "start": "3490440",
    "end": "3497549"
  },
  {
    "text": "depending on how you construct your sequel you can have basically think",
    "start": "3497549",
    "end": "3502589"
  },
  {
    "start": "3499000",
    "end": "3599000"
  },
  {
    "text": "about your input source which is your data coming off of a Kinesis stream you can sort of fan that out internally in",
    "start": "3502589",
    "end": "3508799"
  },
  {
    "text": "your cases analytics application into potentially 15 different paths through your app but that's that would be the",
    "start": "3508799",
    "end": "3514710"
  },
  {
    "text": "limit but your sequel can be there is a limit but it's kilobytes I forget",
    "start": "3514710",
    "end": "3519900"
  },
  {
    "text": "exactly what it is but that would be a very large application if you met that limit the number of K of your actual",
    "start": "3519900",
    "end": "3526410"
  },
  {
    "text": "sequel code itself",
    "start": "3526410",
    "end": "3529160"
  },
  {
    "text": "yeah his question is about data retention in a Kinesis data stream so in",
    "start": "3552980",
    "end": "3558330"
  },
  {
    "text": "cases data stream there's no it doesn't matter how many times it's been consumed it's purely",
    "start": "3558330",
    "end": "3563340"
  },
  {
    "text": "based on the expiration of the stream so by default that's 24 hours so a message will sit in the stream for 24 hours",
    "start": "3563340",
    "end": "3570810"
  },
  {
    "text": "whether it's consumed once or dozens of times it doesn't matter at once that 24 hour period has expired then the data is",
    "start": "3570810",
    "end": "3577590"
  },
  {
    "text": "gone you can extend it to be up to 7 days that's the maximum retention period you can set on a stream yeah that Zynga",
    "start": "3577590",
    "end": "3584880"
  },
  {
    "text": "this was quite useful for us because if we ever need to replay the data stream the data is already there in Kinesis and",
    "start": "3584880",
    "end": "3591600"
  },
  {
    "text": "then we don't need to go to our archives to get it again and reload it we just rerun our consumers with different times",
    "start": "3591600",
    "end": "3599600"
  },
  {
    "text": "right the question was have we considered Kafka versus using Kinesis so yeah a couple of years ago we when we",
    "start": "3605230",
    "end": "3613369"
  },
  {
    "text": "started to look into Kinesis we did look at Kafka previous to that we were using",
    "start": "3613369",
    "end": "3618440"
  },
  {
    "text": "scribe as our data transmission system and scribe as you know it's a it's a",
    "start": "3618440",
    "end": "3625220"
  },
  {
    "text": "dying Tech and it was something that Facebook wrote there is no longer maintained and a lot of issues so we need to get out of that so the biggest",
    "start": "3625220",
    "end": "3632119"
  },
  {
    "text": "bear the biggest thing that why we went weaken Isis is the fact that it's managed for us with Kafka we had to",
    "start": "3632119",
    "end": "3638390"
  },
  {
    "text": "manage our own clusters which we didn't want to really get into with with what we wanted to focus on we wanted to focus",
    "start": "3638390",
    "end": "3643999"
  },
  {
    "text": "more on adding business value not managing the service so that's all you want with Kinesis and which is which",
    "start": "3643999",
    "end": "3650150"
  },
  {
    "text": "worked out pretty well because now we can take advantage of these other Kinesis compatible services like Mises",
    "start": "3650150",
    "end": "3656569"
  },
  {
    "text": "data analytics to do our streaming use keys",
    "start": "3656569",
    "end": "3660489"
  },
  {
    "text": "right so the question was are recalculating the same metrics for historical data as well or are you just",
    "start": "3683029",
    "end": "3689819"
  },
  {
    "text": "doing that in real time for reals and stuff so it really depends on our consumer so this is really just an aggregation engine that we use the",
    "start": "3689819",
    "end": "3695879"
  },
  {
    "text": "Kinesis data analytics for once we send the data off to the consumers we have a",
    "start": "3695879",
    "end": "3701999"
  },
  {
    "text": "consumer that records that data in its own data store for for graphing purposes and to show viewing purposes internally",
    "start": "3701999",
    "end": "3708929"
  },
  {
    "text": "so they'll keep track of historical data from the point we started sending them data but any other analysis that like",
    "start": "3708929",
    "end": "3715379"
  },
  {
    "text": "analysts or developers or game product managers want to do they have two",
    "start": "3715379",
    "end": "3721499"
  },
  {
    "text": "options they can either set up a metric or they can go look at our warehouse which has the the raw data I got sitting",
    "start": "3721499",
    "end": "3727289"
  },
  {
    "text": "there in Vertica right so the question",
    "start": "3727289",
    "end": "3736619"
  },
  {
    "text": "is like if you create a new metric and you wanted to validate that metric yeah you have to if we did the validation for",
    "start": "3736619",
    "end": "3743039"
  },
  {
    "text": "them already so they could try it but if they wanted to do they can run the same query against our vertical ver house and",
    "start": "3743039",
    "end": "3748979"
  },
  {
    "text": "get the same output to look at the output results right you couldn't you",
    "start": "3748979",
    "end": "3756599"
  },
  {
    "text": "couldn't use the Kinesis data analytics query you need to understand the schema of your warehouse and writing a query",
    "start": "3756599",
    "end": "3763049"
  },
  {
    "text": "that translates we're gonna look do you",
    "start": "3763049",
    "end": "3770369"
  },
  {
    "text": "mind if we we're getting getting kicked off there's another another session about to come in we'll come back and answer your question",
    "start": "3770369",
    "end": "3777469"
  }
]