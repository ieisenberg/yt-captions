[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "hello everyone thank you for coming and for joining us for a session about amazon redshift my",
    "start": "1920",
    "end": "9360"
  },
  {
    "text": "name is rahul pathak i'm the senior product manager for the service and you'll also be hearing today from",
    "start": "9360",
    "end": "14639"
  },
  {
    "text": "anurag gupta who's in the front who's the general manager and my boss so i'll make",
    "start": "14639",
    "end": "19680"
  },
  {
    "text": "sure i say the right things here but what you'll learn about today will be an overview of redshift how we set",
    "start": "19680",
    "end": "26560"
  },
  {
    "text": "out to achieve performance ease of use and deliver it to you at a very low price and then",
    "start": "26560",
    "end": "31840"
  },
  {
    "text": "underag will come and talk to us about some of the things that we've been working on over the past year and some",
    "start": "31840",
    "end": "37120"
  },
  {
    "text": "of the upcoming features uh that we hope will be useful to you as you think about redshift in your own architectures",
    "start": "37120",
    "end": "45640"
  },
  {
    "start": "45000",
    "end": "78000"
  },
  {
    "text": "so amazon redshift it's a fast easy to use petabyte scale data",
    "start": "48559",
    "end": "54719"
  },
  {
    "text": "warehousing service that costs under a thousand dollars per terabyte per year",
    "start": "54719",
    "end": "60399"
  },
  {
    "text": "and when we were thinking about redshift our design points were we wanted to deliver a system that was",
    "start": "60399",
    "end": "66479"
  },
  {
    "text": "10 times faster than row-based stores for analytic queries at a tenth the cost",
    "start": "66479",
    "end": "72640"
  },
  {
    "text": "of traditional systems and i'll talk to you about how we set out to achieve that",
    "start": "72640",
    "end": "79039"
  },
  {
    "start": "78000",
    "end": "145000"
  },
  {
    "text": "so one of the things to keep in mind when you're doing data warehousing and large scale analytic queries at scale is",
    "start": "79520",
    "end": "85360"
  },
  {
    "text": "that i o and being efficient about how you do it and doing it fast is critical to achieving performance",
    "start": "85360",
    "end": "92320"
  },
  {
    "text": "and so redshift is a columnar data warehousing system and what this means is",
    "start": "92320",
    "end": "97520"
  },
  {
    "text": "unlike a traditional row-based database where if you wanted to sum say the amount column you would have to",
    "start": "97520",
    "end": "103920"
  },
  {
    "text": "read every single row in your table and then throw away three out of the four rows",
    "start": "103920",
    "end": "110159"
  },
  {
    "text": "which is inefficient because we don't care about three-quarters of that data for this query in a columnar system you can only focus",
    "start": "110159",
    "end": "116960"
  },
  {
    "text": "on reading the data that you care about and so you do one-fourth the amount of io you get the result you need",
    "start": "116960",
    "end": "123680"
  },
  {
    "text": "and the reason this is beneficial is if you think about it most analytic queries are not get me everything i know about",
    "start": "123680",
    "end": "129520"
  },
  {
    "text": "this person most analytic queries are i want to know all the orders that happened or i want to know all the totals i want to know",
    "start": "129520",
    "end": "135840"
  },
  {
    "text": "which items were purchased so they operate on a subset of the rows in the table and you're able to get much better",
    "start": "135840",
    "end": "141840"
  },
  {
    "text": "performance for these kinds of queries another big benefit of columnar storage",
    "start": "141840",
    "end": "149680"
  },
  {
    "start": "145000",
    "end": "255000"
  },
  {
    "text": "is compression and so because you have uh light data stored sequentially what",
    "start": "149680",
    "end": "155120"
  },
  {
    "text": "you're able to do is then apply the the compression algorithm that's most appropriate for each column",
    "start": "155120",
    "end": "161519"
  },
  {
    "text": "each column is stored independently and when you actually wrote load ref shift using the copy command which is a",
    "start": "161519",
    "end": "168239"
  },
  {
    "text": "bulk sql load command that automatically loads every node in your cluster in parallel",
    "start": "168239",
    "end": "173760"
  },
  {
    "text": "redshift will sample your data and then automatically pick the compression algorithm that's best for the data that",
    "start": "173760",
    "end": "180239"
  },
  {
    "text": "you have at hand you can always analyze this the output you see on the right there is the result",
    "start": "180239",
    "end": "186319"
  },
  {
    "text": "of running an analyze sql command on a table you'll see that each column has a different compression algorithm or has",
    "start": "186319",
    "end": "192800"
  },
  {
    "text": "one that's most appropriate for its data and you can always override this when you're defining your tables which you do",
    "start": "192800",
    "end": "198319"
  },
  {
    "text": "using standard sql statements and dml statements and the benefits of compression for you",
    "start": "198319",
    "end": "205440"
  },
  {
    "text": "are really two-fold and our customers typically see anywhere from four to eight x compression on real",
    "start": "205440",
    "end": "211440"
  },
  {
    "text": "world data sets you know this is an average what you might see that some columns compress",
    "start": "211440",
    "end": "216959"
  },
  {
    "text": "much better than others for example if you had something which was storing just boolean values it would get much better",
    "start": "216959",
    "end": "222480"
  },
  {
    "text": "compression and with compression what you get is more performance and the reason is that every",
    "start": "222480",
    "end": "228640"
  },
  {
    "text": "single read that you do contains more relevant data so if you think about 4 to 8x",
    "start": "228640",
    "end": "233840"
  },
  {
    "text": "compression if we read a single block we'll get that much more relevant data for the queries",
    "start": "233840",
    "end": "239439"
  },
  {
    "text": "the other benefit is cost so if you have a four terabyte database that collapses down to a one terabyte",
    "start": "239439",
    "end": "246000"
  },
  {
    "text": "data warehouse in redshift you've just saved yourself a ton of money on storage and on computing capacity",
    "start": "246000",
    "end": "252080"
  },
  {
    "text": "and again this is done automatically for you the third technique that redshift uses",
    "start": "252080",
    "end": "258880"
  },
  {
    "start": "255000",
    "end": "307000"
  },
  {
    "text": "to deliver performance is an idea called zone maps and what zone maps do and they apply",
    "start": "258880",
    "end": "264800"
  },
  {
    "text": "best for sorted columns you're able to keep track of the minimum and maximum values in every block",
    "start": "264800",
    "end": "271280"
  },
  {
    "text": "so if you think of a table that might contain event data say hits to your website and you're logging",
    "start": "271280",
    "end": "276720"
  },
  {
    "text": "that it unfolds in time and you have your table sorted by time",
    "start": "276720",
    "end": "281759"
  },
  {
    "text": "if you write a query that has a range in it in your where clause say show me all the data for the past month",
    "start": "281759",
    "end": "287759"
  },
  {
    "text": "redshift can use these zone maps to skip over blocks that it knows won't contain the data that's relevant for the query",
    "start": "287759",
    "end": "294320"
  },
  {
    "text": "because it knows the minimum and maximum values per block and again you'll see this theme of being efficient about doing i o avoiding it",
    "start": "294320",
    "end": "301440"
  },
  {
    "text": "when we don't have to and being intelligent about our knowledge of the data to give you better performance",
    "start": "301440",
    "end": "308720"
  },
  {
    "start": "307000",
    "end": "383000"
  },
  {
    "text": "at some point though uh we're going to have to go to disc sorry",
    "start": "309120",
    "end": "315440"
  },
  {
    "text": "uh we will have to go to disc to do our reads and redshift runs on a hardware platform that's really custom designed",
    "start": "315440",
    "end": "322880"
  },
  {
    "text": "for high throughput data processing and so it's uh it's based on the ec2",
    "start": "322880",
    "end": "328320"
  },
  {
    "text": "high storage instance it comes in two sizes the extra large has 16 gigabytes of ram",
    "start": "328320",
    "end": "335840"
  },
  {
    "text": "three discs on it with two terabytes of compressed storage so if you're getting 4x compression you can store",
    "start": "335840",
    "end": "343199"
  },
  {
    "text": "eight t on that machine and uh there's a creatively named eight extra",
    "start": "343199",
    "end": "349520"
  },
  {
    "text": "large which is eight times as big on every dimension so 128 gigs of ram",
    "start": "349520",
    "end": "355520"
  },
  {
    "text": "24 spindles 16 cores and 16 terabytes of compressed storage",
    "start": "355520",
    "end": "361759"
  },
  {
    "text": "and with these you get over two gigabytes per second scan rate and the idea is that we want to give you local",
    "start": "361759",
    "end": "368319"
  },
  {
    "text": "disk so that you can get maximum performance but then as part of the management of the service we take care of replication",
    "start": "368319",
    "end": "374800"
  },
  {
    "text": "and backups and durability so you get the benefits of local storage for performance but manage storage for ease",
    "start": "374800",
    "end": "381280"
  },
  {
    "text": "of use from an architecture perspective",
    "start": "381280",
    "end": "387520"
  },
  {
    "start": "383000",
    "end": "441000"
  },
  {
    "text": "redshift is designed to be run as a clustered system and so you have a leader node which is",
    "start": "387520",
    "end": "392800"
  },
  {
    "text": "your sql endpoint you can connect to this any of your bi tools sql clients or",
    "start": "392800",
    "end": "398800"
  },
  {
    "text": "existing applications using jdbc and odbc and postgres drivers",
    "start": "398800",
    "end": "404560"
  },
  {
    "text": "and the leader node receives your queries and coordinates execution of those queries across all of the compute",
    "start": "404560",
    "end": "410479"
  },
  {
    "text": "nodes in your cluster and the compute nodes are where your data is stored and where data processing",
    "start": "410479",
    "end": "416160"
  },
  {
    "text": "takes place that's where the query execution engine is operating on your data",
    "start": "416160",
    "end": "421599"
  },
  {
    "text": "it's columnar storage and we try and do as much in parallel as we can these nodes are loaded and backed up in",
    "start": "421599",
    "end": "428800"
  },
  {
    "text": "parallel from amazon s3 that's where the backups happen and you can also parallel load from",
    "start": "428800",
    "end": "435120"
  },
  {
    "text": "amazon dynamodb and unrug we'll be talking about some other enhancements we've made to loading",
    "start": "435120",
    "end": "440800"
  },
  {
    "text": "as we go through and then we also make available a single node variant where all of these are",
    "start": "440800",
    "end": "447039"
  },
  {
    "start": "441000",
    "end": "460000"
  },
  {
    "text": "co-located uh this costs less than 80 uh it costs 85 cents an hour and it's",
    "start": "447039",
    "end": "452160"
  },
  {
    "text": "available and great for dev and test and for standing up environments quickly and then as you scale up into production",
    "start": "452160",
    "end": "458160"
  },
  {
    "text": "uses you can grow from there",
    "start": "458160",
    "end": "461599"
  },
  {
    "start": "460000",
    "end": "510000"
  },
  {
    "text": "so just to talk a little bit about how we do things in parallel to really drive performance uh for you",
    "start": "463440",
    "end": "470160"
  },
  {
    "text": "so when you when you're running queries you want them to run as fast as possible and so when you execute a query",
    "start": "470160",
    "end": "475919"
  },
  {
    "text": "what the leader node does is it goes through any logical transformations or optimizations that it can do",
    "start": "475919",
    "end": "481440"
  },
  {
    "text": "and it then generates a physical execution plan uh which is generated c-plus plus code",
    "start": "481440",
    "end": "487599"
  },
  {
    "text": "which is compiled and then executed in parallel on all of the compute nodes",
    "start": "487599",
    "end": "492879"
  },
  {
    "text": "and then redshift actually caches this com compiled code and persists it so that it can reuse it",
    "start": "492879",
    "end": "499280"
  },
  {
    "text": "if you run the same query again even if your query parameters would change and so all of this is designed really to",
    "start": "499280",
    "end": "506000"
  },
  {
    "text": "achieve as much performance as we can on every single query",
    "start": "506000",
    "end": "511599"
  },
  {
    "start": "510000",
    "end": "614000"
  },
  {
    "text": "data loading is also something that we do in parallel and our goal with all of these parallel operations was really to",
    "start": "512479",
    "end": "518159"
  },
  {
    "text": "have cluster size not be a factor so if you wanted to load a terabyte per node into",
    "start": "518159",
    "end": "523360"
  },
  {
    "text": "a three node cluster it would take the same amount of time as if you were loading a terabyte per node into a 10",
    "start": "523360",
    "end": "528880"
  },
  {
    "text": "node cluster and loading is done in parallel redshift has a modified sql copy command",
    "start": "528880",
    "end": "537120"
  },
  {
    "text": "s3 has been was our initial loading point and it's great because s3 is effectively",
    "start": "537120",
    "end": "542880"
  },
  {
    "text": "the chicago o'hare for aws if you put your data on s3 it can really be consumed by any other service that",
    "start": "542880",
    "end": "549200"
  },
  {
    "text": "speaks to it which is virtually all of them and so when you issue this copy command redshift will load every compute node in",
    "start": "549200",
    "end": "556000"
  },
  {
    "text": "parallel simultaneously and the goal here is to give you linear load throughput as you",
    "start": "556000",
    "end": "561440"
  },
  {
    "text": "increase the number of nodes in your cluster and redshift takes care of automatically distributing the data as",
    "start": "561440",
    "end": "567040"
  },
  {
    "text": "appropriate as defined by you across all of these nodes you don't have to make any of those decisions",
    "start": "567040",
    "end": "573120"
  },
  {
    "text": "regarding the physical layout except for defining how your data is how your tables are created at create time",
    "start": "573120",
    "end": "579839"
  },
  {
    "text": "there's a concept called a distribution key where you can when you define that control how",
    "start": "579839",
    "end": "585440"
  },
  {
    "text": "data is spread across the nodes and we also support parallel loading",
    "start": "585440",
    "end": "591680"
  },
  {
    "text": "from dynamodb using the same copy command and there you can specify the amount of",
    "start": "591680",
    "end": "596720"
  },
  {
    "text": "read throughput from dynamo that redshift uses and so we see customers that have dynamodb in their stacks for",
    "start": "596720",
    "end": "603760"
  },
  {
    "text": "low latency data capture and then bring that data into redshift using the copy command to run analytics on it",
    "start": "603760",
    "end": "611040"
  },
  {
    "text": "in the cluster itself",
    "start": "611040",
    "end": "614320"
  },
  {
    "start": "614000",
    "end": "703000"
  },
  {
    "text": "one another operation that we manage for you is backup and so backup",
    "start": "617360",
    "end": "622399"
  },
  {
    "text": "to s3 is continuous it's automatic and it's incremental so anytime you load",
    "start": "622399",
    "end": "627760"
  },
  {
    "text": "data into a redshift cluster will automatically begin backing that data up into s3",
    "start": "627760",
    "end": "633200"
  },
  {
    "text": "and we only back up the data that's changed and so that means that we try and be efficient with the usage of s3 storage",
    "start": "633200",
    "end": "639760"
  },
  {
    "text": "for backup and you actually get free backup up to the amount of data warehousing storage that you've",
    "start": "639760",
    "end": "644839"
  },
  {
    "text": "provisioned so if you have a four terabyte cluster you'll get four terabytes free backup utilization",
    "start": "644839",
    "end": "650880"
  },
  {
    "text": "on s3 now these automatic backups called system snapshots you can choose how long",
    "start": "650880",
    "end": "657440"
  },
  {
    "text": "they're retained anywhere from 1 to 35 days that's configurable by you and at that point they automatically age",
    "start": "657440",
    "end": "664160"
  },
  {
    "text": "out you can also take your own user snapshots at any point and those are",
    "start": "664160",
    "end": "669279"
  },
  {
    "text": "kept until you explicitly delete them and what this allows you to do is clone your cluster or your environment",
    "start": "669279",
    "end": "676000"
  },
  {
    "text": "anytime that you need to so for example we have customers who will take a snapshot of their production clusters",
    "start": "676000",
    "end": "682480"
  },
  {
    "text": "stand them up in a dev environment to do some testing or development and then once they have a model they can reuse",
    "start": "682480",
    "end": "688560"
  },
  {
    "text": "they'll push that back into production or you could share clutch share snapshots with other accounts perhaps a",
    "start": "688560",
    "end": "694640"
  },
  {
    "text": "different group within your company or if you're a service provider share them with your client allowing you to",
    "start": "694640",
    "end": "700000"
  },
  {
    "text": "essentially migrate your configurations around as you need so backing up is just one part of the",
    "start": "700000",
    "end": "706160"
  },
  {
    "start": "703000",
    "end": "757000"
  },
  {
    "text": "story you also need to be able to restore and restore is a push button operation",
    "start": "706160",
    "end": "711680"
  },
  {
    "text": "you simply select a snapshot and click the restore button in the console or use an api call",
    "start": "711680",
    "end": "717839"
  },
  {
    "text": "and once that's done redshift will stand up a new cluster and then start streaming data back onto",
    "start": "717839",
    "end": "724000"
  },
  {
    "text": "it and so as soon as the system metadata is available you can begin running queries and",
    "start": "724000",
    "end": "729040"
  },
  {
    "text": "redshift will stream in the data needed to respond to those queries from s3 and what this means for you is that",
    "start": "729040",
    "end": "735760"
  },
  {
    "text": "as soon as your cluster's up and running and typically as long as one to five percent of the data's back most of your",
    "start": "735760",
    "end": "741760"
  },
  {
    "text": "normal workloads will run normally and so for example we may have three years of data in our data warehouse but most",
    "start": "741760",
    "end": "748079"
  },
  {
    "text": "of our queries focus on what happened yesterday or last week or last month and streaming restore allows you to get to",
    "start": "748079",
    "end": "754800"
  },
  {
    "text": "that really quickly resize is another operation that we had",
    "start": "754800",
    "end": "761519"
  },
  {
    "start": "757000",
    "end": "826000"
  },
  {
    "text": "spent a lot of time working on and made a parallel operation and so what this does is it allows you",
    "start": "761519",
    "end": "767120"
  },
  {
    "text": "to scale your clusters as your data needs and analytic needs grow and when",
    "start": "767120",
    "end": "772240"
  },
  {
    "text": "you select resize you remain online for reads we actually provision a new cluster in",
    "start": "772240",
    "end": "777760"
  },
  {
    "text": "the background and do a parallel copy of data from node to node and so if you think about it say we had",
    "start": "777760",
    "end": "784959"
  },
  {
    "text": "four terabytes of data per node on the source cluster when you're moving to the target cluster",
    "start": "784959",
    "end": "790560"
  },
  {
    "text": "which is four nodes in this case you'd only have three terabytes of data on the per node on the target",
    "start": "790560",
    "end": "796000"
  },
  {
    "text": "and so the time taken to resize is proportional to the time it takes to move that 4t from one node essentially",
    "start": "796000",
    "end": "802959"
  },
  {
    "text": "since it's done in parallel it's really about the amount of data on each node on the source",
    "start": "802959",
    "end": "808160"
  },
  {
    "text": "you can resize up or down we only charge you for the source cluster",
    "start": "808160",
    "end": "813279"
  },
  {
    "text": "and when the resize is complete we automatically use dns to move your sql endpoint to point to your new resize cluster and",
    "start": "813279",
    "end": "820079"
  },
  {
    "text": "so you can scale without having to change your client applications or any code that you've written",
    "start": "820079",
    "end": "827200"
  },
  {
    "start": "826000",
    "end": "847000"
  },
  {
    "text": "um and in addition if you wanted to scale while remaining online for reads and writes you could simply clone from a",
    "start": "827519",
    "end": "834560"
  },
  {
    "text": "snapshot resize your clone and then cut over when you're ready so a lot of flexibility in these options",
    "start": "834560",
    "end": "840880"
  },
  {
    "text": "things like this are much harder to do on premise where you have to think about racks and wheeling them around",
    "start": "840880",
    "end": "848000"
  },
  {
    "start": "847000",
    "end": "891000"
  },
  {
    "text": "and from a resize perspective redshift lets you start with a single two terabyte node",
    "start": "849680",
    "end": "855199"
  },
  {
    "text": "and scale all the way up to a hundred of the eight extra large nodes for 1.6",
    "start": "855199",
    "end": "860480"
  },
  {
    "text": "petabytes of compressed data in your data warehouse and clusters have to be homogeneous so",
    "start": "860480",
    "end": "866880"
  },
  {
    "text": "if you're running the the excel size which is 2t you can have up to 32 of those for 64",
    "start": "866880",
    "end": "873680"
  },
  {
    "text": "terabytes and at any point you can begin using two or more eight extra larges",
    "start": "873680",
    "end": "879680"
  },
  {
    "text": "and the performance of eight single extra larges is roughly equivalent to the performance of an eight extra large",
    "start": "879680",
    "end": "885519"
  },
  {
    "text": "so it's really about thinking about your scaling increment and how your needs are expanding",
    "start": "885519",
    "end": "891839"
  },
  {
    "start": "891000",
    "end": "990000"
  },
  {
    "text": "from a pricing perspective we've really designed redshift to be very very cost effective and our",
    "start": "893440",
    "end": "899680"
  },
  {
    "text": "goal is to allow you to analyze all of the data that you care about not to throw away data just to fit it into",
    "start": "899680",
    "end": "906399"
  },
  {
    "text": "a data warehouse that you've licensed and don't want to spend extra money for or have to deal with",
    "start": "906399",
    "end": "912639"
  },
  {
    "text": "and so we have really three pricing options with redshift what you're looking at is the price for the two terabyte node",
    "start": "912639",
    "end": "919199"
  },
  {
    "text": "it's under a dollar an hour 85 cents for our on demand hourly pricing and we have",
    "start": "919199",
    "end": "924480"
  },
  {
    "text": "customers that during proof of concept will run clusters nine to five monday to friday delete them save a snapshot",
    "start": "924480",
    "end": "932480"
  },
  {
    "text": "keep them deleted over the weekend and then spin them back up again on monday to resume testing",
    "start": "932480",
    "end": "937760"
  },
  {
    "text": "and so that hourly flexibility lets you really control your costs and then what we find is that uh if you",
    "start": "937760",
    "end": "945040"
  },
  {
    "text": "take the two terabyte plus two terabyte node at an hourly rate and look at the cost per terabyte per year that's about",
    "start": "945040",
    "end": "952320"
  },
  {
    "text": "thirty seven hundred dollars we then offer a one year reservation where you commit to a one year term",
    "start": "952320",
    "end": "959440"
  },
  {
    "text": "and for that you get a forty percent discount so effectively your cost comes down to just over two thousand dollars",
    "start": "959440",
    "end": "964800"
  },
  {
    "text": "per terabyte per year and the three three-year term gets you over a 70 discount bringing the cost per",
    "start": "964800",
    "end": "971040"
  },
  {
    "text": "terabyte to under a thousand dollars per terabyte per year and for top-of-the-market columnar technology",
    "start": "971040",
    "end": "977279"
  },
  {
    "text": "this is a a radically new price point and it's opened up redshift to a ton of different applications not just",
    "start": "977279",
    "end": "983600"
  },
  {
    "text": "traditional data warehousing but also big data and sas applications",
    "start": "983600",
    "end": "990480"
  },
  {
    "start": "990000",
    "end": "1058000"
  },
  {
    "text": "now we've spent a lot of time on redshift thinking about security and so with redshift you're able to use",
    "start": "992240",
    "end": "998800"
  },
  {
    "text": "ssl to secure data over the wire you can enable transparent data",
    "start": "998800",
    "end": "1003920"
  },
  {
    "text": "encryption which is for data at rest and that's aes256 hardware accelerated",
    "start": "1003920",
    "end": "1010399"
  },
  {
    "text": "and every single block that's written to disk whether it's a temporary spill or data that you load is encrypted and your",
    "start": "1010399",
    "end": "1016480"
  },
  {
    "text": "backups are encrypted and there's no direct access to the compute node so they're walled off inside their",
    "start": "1016480",
    "end": "1023040"
  },
  {
    "text": "own virtual private network and so cannot be accessed and all of our communication with things like s3",
    "start": "1023040",
    "end": "1030319"
  },
  {
    "text": "is also encrypted using ssl and then you can choose to run this inside of a vpc which is our virtual",
    "start": "1030319",
    "end": "1036640"
  },
  {
    "text": "private cloud which is a logically isolated portion of the aws cloud where you control networking routing and you",
    "start": "1036640",
    "end": "1043438"
  },
  {
    "text": "can connect it via vpn to your existing networks and redshift has been through",
    "start": "1043439",
    "end": "1049520"
  },
  {
    "text": "pci compliance as well as sock 1 and sock 2 so it's an area of focus for us and a",
    "start": "1049520",
    "end": "1055840"
  },
  {
    "text": "priority for us we also",
    "start": "1055840",
    "end": "1060960"
  },
  {
    "start": "1058000",
    "end": "1143000"
  },
  {
    "text": "really take our duty of safeguarding your data very seriously and so we automatically",
    "start": "1060960",
    "end": "1066080"
  },
  {
    "text": "replicate data within a cluster anytime you load data it's replicated onto different drives",
    "start": "1066080",
    "end": "1072400"
  },
  {
    "text": "on other nodes in the cluster to help you deal with drive failures transparently while queries remain in",
    "start": "1072400",
    "end": "1077840"
  },
  {
    "text": "flight and we also continuously back up that data to s3 so you always have multiple",
    "start": "1077840",
    "end": "1083200"
  },
  {
    "text": "copies of data in your cluster and these backups which are continuous and automatic and incremental",
    "start": "1083200",
    "end": "1089919"
  },
  {
    "text": "go to s3 and s3 is really designed for 119's of durability and so what you have is all of the performance benefits of",
    "start": "1089919",
    "end": "1096480"
  },
  {
    "text": "local storage and all of the benefits of durability that you get from the managed replication and automatic",
    "start": "1096480",
    "end": "1103360"
  },
  {
    "text": "backups we're also monitoring everything in your system continuously so we can deal with component failures node failures we can",
    "start": "1103360",
    "end": "1110880"
  },
  {
    "text": "replace nodes and when those happen due to streaming restore your cluster is",
    "start": "1110880",
    "end": "1116240"
  },
  {
    "text": "available as soon as that node's provisioned and connected a matter of minutes",
    "start": "1116240",
    "end": "1121679"
  },
  {
    "text": "and these snapshots can be restored into any availability zone within an aws region",
    "start": "1121679",
    "end": "1127600"
  },
  {
    "text": "and so if for example there was some issue you could restore from snapshot into another",
    "start": "1127600",
    "end": "1133360"
  },
  {
    "text": "availability zone as long as redshift is offered there and we're offered in at least two generally three availability",
    "start": "1133360",
    "end": "1139440"
  },
  {
    "text": "zones in all the regions that we're in",
    "start": "1139440",
    "end": "1142879"
  },
  {
    "start": "1143000",
    "end": "1196000"
  },
  {
    "text": "another thing that we realize is that data warehousing typically exists in the context of",
    "start": "1146080",
    "end": "1151520"
  },
  {
    "text": "another set of tools around business intelligence data integration and software and services and expertise",
    "start": "1151520",
    "end": "1158640"
  },
  {
    "text": "and so we've been working hard to support the ecosystem around redshift and we're excited to see it growing",
    "start": "1158640",
    "end": "1165039"
  },
  {
    "text": "and really our goal with redshift was to allow you to use the tools that you use today so from a application in bi",
    "start": "1165039",
    "end": "1171679"
  },
  {
    "text": "perspective you can connect to redshift using jdbc and odbc and standard postgres drivers that you can download",
    "start": "1171679",
    "end": "1178799"
  },
  {
    "text": "from the postgres website so really anything that you have should work with the system",
    "start": "1178799",
    "end": "1183919"
  },
  {
    "text": "and the names that you see here are folks that have put redshift through their internal testing and certified it",
    "start": "1183919",
    "end": "1189919"
  },
  {
    "text": "as supported platforms but customers are using a wide range of products with redshift",
    "start": "1189919",
    "end": "1197120"
  },
  {
    "start": "1196000",
    "end": "1224000"
  },
  {
    "text": "there's also the aws marketplace think of this as the app store for aws and here you can go to find software to use",
    "start": "1198880",
    "end": "1205520"
  },
  {
    "text": "with redshift they're one-click deployments a range of pricing options one of our",
    "start": "1205520",
    "end": "1211440"
  },
  {
    "text": "partners jaspersoft has done a lot of work around an hourly model and the ability to auto-discover your redshift",
    "start": "1211440",
    "end": "1217440"
  },
  {
    "text": "clusters so a range of options for getting up and running quickly and something i definitely encourage you",
    "start": "1217440",
    "end": "1223360"
  },
  {
    "text": "to check out and from a feature perspective",
    "start": "1223360",
    "end": "1228960"
  },
  {
    "start": "1224000",
    "end": "1279000"
  },
  {
    "text": "we've been launching a ton our real our goal with with how we do products at aws",
    "start": "1228960",
    "end": "1234480"
  },
  {
    "text": "is to listen to our customers and iterate quickly we've expanded globally we recently went",
    "start": "1234480",
    "end": "1240799"
  },
  {
    "text": "into singapore and sydney so we're in the states europe and asia pacific",
    "start": "1240799",
    "end": "1246000"
  },
  {
    "text": "we've been working on our security certifications we've added features to support full",
    "start": "1246000",
    "end": "1251200"
  },
  {
    "text": "encryption from client-side encrypted files on s3 to encrypted clusters to",
    "start": "1251200",
    "end": "1256799"
  },
  {
    "text": "unloading encrypted extracts if you need that and have focused on really improving the",
    "start": "1256799",
    "end": "1262080"
  },
  {
    "text": "manageability query capability and data loading flexibility of redshift",
    "start": "1262080",
    "end": "1267520"
  },
  {
    "text": "and all of these have been driven by customer feedback so i encourage you to to let us know about what you need if",
    "start": "1267520",
    "end": "1272799"
  },
  {
    "text": "you try the system and if you're already using it i'd love to hear about the features that you'd like to see from us",
    "start": "1272799",
    "end": "1279919"
  },
  {
    "start": "1279000",
    "end": "1293000"
  },
  {
    "text": "and with that i'll turn it over to android thank you",
    "start": "1281120",
    "end": "1286280"
  },
  {
    "start": "1293000",
    "end": "1322000"
  },
  {
    "text": "i was having a flashback during that presentation because it's pretty much the same content i did this time last",
    "start": "1294159",
    "end": "1299679"
  },
  {
    "text": "year so let's talk about what's next so normally in aws we don't really",
    "start": "1299679",
    "end": "1305600"
  },
  {
    "text": "project too far forward so this is mostly stuff that's going to be delivered in the next couple of weeks",
    "start": "1305600",
    "end": "1311360"
  },
  {
    "text": "that's about as far forward as we're willing to commit to you guys yeah it's a pretty",
    "start": "1311360",
    "end": "1318880"
  },
  {
    "text": "high change environment um so a handful of the stuff was announced by",
    "start": "1318880",
    "end": "1325840"
  },
  {
    "start": "1322000",
    "end": "1410000"
  },
  {
    "text": "jeff barr earlier today some of it will be new but i'll talk to it in a little bit more detail",
    "start": "1325840",
    "end": "1331280"
  },
  {
    "text": "so rahul talked quite a bit about our security story which i think you",
    "start": "1331280",
    "end": "1337520"
  },
  {
    "text": "know we've really tried to harden the service we take our custodial responsibility for your data very",
    "start": "1337520",
    "end": "1342559"
  },
  {
    "text": "seriously one of the things we've added recently is hardware security module support either",
    "start": "1342559",
    "end": "1349600"
  },
  {
    "text": "in the cloud managed by us or on premise managed by you and what that does is that if you're",
    "start": "1349600",
    "end": "1355760"
  },
  {
    "text": "running an hsn device on premise then you hold the keys to",
    "start": "1355760",
    "end": "1361360"
  },
  {
    "text": "decrypt your cluster and we only access those when we start up and so if at any time you choose to",
    "start": "1361360",
    "end": "1367760"
  },
  {
    "text": "uh for whatever reason turn off our ability to access the cluster all of the data is unavailable",
    "start": "1367760",
    "end": "1372799"
  },
  {
    "text": "whether it's in backup whether it's on the cluster etc so that basically helps",
    "start": "1372799",
    "end": "1377840"
  },
  {
    "text": "people who are thinking about moving from on-premise but i have a degree of uncertainty about the cloud that they",
    "start": "1377840",
    "end": "1382960"
  },
  {
    "text": "continue to own uh you know the ability to curtail access to their data whenever",
    "start": "1382960",
    "end": "1389280"
  },
  {
    "text": "they choose the other thing that's not on the slide that we've added recently is the ability for you to manage your",
    "start": "1389280",
    "end": "1396080"
  },
  {
    "text": "rotation of encryption keys just through the console or through an api and so that",
    "start": "1396080",
    "end": "1402000"
  },
  {
    "text": "basically lets you through an api model run whatever policies you want to rather than are trying to manage the policies",
    "start": "1402000",
    "end": "1408559"
  },
  {
    "text": "for you we've also added audit logging so",
    "start": "1408559",
    "end": "1414080"
  },
  {
    "start": "1410000",
    "end": "1447000"
  },
  {
    "text": "earlier today andy talked about cloudtrail which gives you the ability",
    "start": "1414080",
    "end": "1419360"
  },
  {
    "text": "to get all system activity up to s3 so we're a participant in cloudtrail and so all",
    "start": "1419360",
    "end": "1425600"
  },
  {
    "text": "of your redshift api events will go there in addition we also are taking all the database",
    "start": "1425600",
    "end": "1431760"
  },
  {
    "text": "activities so the successful logins login failures queries loads everything like that also pushing that to s3 and",
    "start": "1431760",
    "end": "1438240"
  },
  {
    "text": "then it's available through the same variety of tools for you to analyze and understand what people are",
    "start": "1438240",
    "end": "1444480"
  },
  {
    "text": "doing with your data and",
    "start": "1444480",
    "end": "1450080"
  },
  {
    "start": "1447000",
    "end": "1514000"
  },
  {
    "text": "finally we've added sns alerts so that you can basically configure an ssns topic",
    "start": "1450080",
    "end": "1456000"
  },
  {
    "text": "decide which things that you're interested in monitoring whether it's security maintenance",
    "start": "1456000",
    "end": "1462080"
  },
  {
    "text": "you know progress of things like a load and do that also",
    "start": "1462080",
    "end": "1467760"
  },
  {
    "text": "for a particular cluster as opposed to all clusters and then you know based on",
    "start": "1467760",
    "end": "1473360"
  },
  {
    "text": "which thing whether it's an error warning informational message and then have that communicated either by email",
    "start": "1473360",
    "end": "1479840"
  },
  {
    "text": "or text and so you might say that i want all warnings or errors for security for",
    "start": "1479840",
    "end": "1487360"
  },
  {
    "text": "my production cluster sent to me on a text message but for development clusters i will maybe take a lower bar",
    "start": "1487360",
    "end": "1494240"
  },
  {
    "text": "and only do that over email so with these features we hope that in",
    "start": "1494240",
    "end": "1500159"
  },
  {
    "text": "addition to providing the manageability of the service on your behalf we also start to give you visibility into what's",
    "start": "1500159",
    "end": "1506320"
  },
  {
    "text": "going on control over what's going on so that you retain the same sorts of capabilities you have on premise today",
    "start": "1506320",
    "end": "1515559"
  },
  {
    "start": "1514000",
    "end": "1598000"
  },
  {
    "text": "batch operations so rahul talked about our ability to add",
    "start": "1515840",
    "end": "1522400"
  },
  {
    "text": "copy from s3 that's you know it runs in parallel and that is",
    "start": "1522400",
    "end": "1528400"
  },
  {
    "text": "basically chicago o'hare and i don't know about you i prefer direct flights and so",
    "start": "1528400",
    "end": "1534400"
  },
  {
    "text": "one of the things that we've added here is that you know people have a multitude of different uh source data so you might",
    "start": "1534400",
    "end": "1542080"
  },
  {
    "text": "have something sitting in a hadoop cluster or emr as a managed hadoop environment or",
    "start": "1542080",
    "end": "1548000"
  },
  {
    "text": "cassandra or what have you and so what we're trying to provide here is the ability for you to run a copy command",
    "start": "1548000",
    "end": "1554240"
  },
  {
    "text": "where you specify a shell script that from which you're going to get information then you do a little bit of",
    "start": "1554240",
    "end": "1559760"
  },
  {
    "text": "setup to be able to let us run ssh tunneling over to your",
    "start": "1559760",
    "end": "1565039"
  },
  {
    "text": "ec2 node or for that matter your node on premise that's a part of your vpc",
    "start": "1565039",
    "end": "1570400"
  },
  {
    "text": "and let us extract data and so we think that this is an ability to not just make things easy",
    "start": "1570400",
    "end": "1576320"
  },
  {
    "text": "through managing all the parallelization and so forth but also make things possible so",
    "start": "1576320",
    "end": "1582559"
  },
  {
    "text": "that with a little bit of uh work you we can make you know",
    "start": "1582559",
    "end": "1587679"
  },
  {
    "text": "these things available to you for over excuse me a wide variety of sources",
    "start": "1587679",
    "end": "1592799"
  },
  {
    "text": "without necessarily going up to s3",
    "start": "1592799",
    "end": "1598200"
  },
  {
    "start": "1598000",
    "end": "1655000"
  },
  {
    "text": "so cluster creation up till recently has been taking about 15 to 20 minutes",
    "start": "1601039",
    "end": "1607039"
  },
  {
    "text": "we're moving that down with warm pools to around three minutes and so you might",
    "start": "1607039",
    "end": "1612400"
  },
  {
    "text": "say well you know it used to take me six months to get a cluster why do i care about the difference between those two",
    "start": "1612400",
    "end": "1618080"
  },
  {
    "text": "and the fact is is that you really want node replacements to be fast because node replacements aren't just about initial",
    "start": "1618080",
    "end": "1624400"
  },
  {
    "text": "cluster creation they're also about handling failures so if a node drops and if you're running a large enough cluster",
    "start": "1624400",
    "end": "1630720"
  },
  {
    "text": "you know inevitably sooner or later a node will drop a three minutes of downtime is a very different story than",
    "start": "1630720",
    "end": "1637440"
  },
  {
    "text": "15 or 20 minutes so what we're basically doing under the scenes there is that we pre-provision notes out of",
    "start": "1637440",
    "end": "1643840"
  },
  {
    "text": "ec2 we set up most of the environment for it and then as you ask for that node you",
    "start": "1643840",
    "end": "1649679"
  },
  {
    "text": "have to basically do the vpc and final config to make it your node",
    "start": "1649679",
    "end": "1655840"
  },
  {
    "start": "1655000",
    "end": "1695000"
  },
  {
    "text": "rahul talked about resize and how we've made that a simple operation for you we're we've also been working",
    "start": "1656720",
    "end": "1663840"
  },
  {
    "text": "really hard on making it a faster operation for you so in the past if you're moving say",
    "start": "1663840",
    "end": "1670799"
  },
  {
    "text": "i guess in this case a couple of uh eight terabytes of data off of an eight node cluster to a 16 node cluster",
    "start": "1670799",
    "end": "1677679"
  },
  {
    "text": "um it was it took about more than a day and we've reduced that by a factor of four that's still",
    "start": "1677679",
    "end": "1683520"
  },
  {
    "text": "something that we hope to improve over time but we're now getting into a range where it's really the network transfer",
    "start": "1683520",
    "end": "1689360"
  },
  {
    "text": "rates that are dominating which is exactly where you want it to be",
    "start": "1689360",
    "end": "1695640"
  },
  {
    "start": "1695000",
    "end": "1776000"
  },
  {
    "text": "so let's talk about performance and concurrency so in our current workload management",
    "start": "1697279",
    "end": "1702480"
  },
  {
    "text": "what you can control is how many slots you want to make available by default",
    "start": "1702480",
    "end": "1708159"
  },
  {
    "text": "redshift comes with five slots that run queries you can expand that up to 15 slots you can expand the number of",
    "start": "1708159",
    "end": "1715679"
  },
  {
    "text": "queues and distribute you know how many uh who goes to what queue and how many",
    "start": "1715679",
    "end": "1721440"
  },
  {
    "text": "resources that that goes to so one of the things we've changed there is is that in the past",
    "start": "1721440",
    "end": "1727600"
  },
  {
    "text": "the amount of resources available to each slot were the same so if you had 15",
    "start": "1727600",
    "end": "1733760"
  },
  {
    "text": "slots then they were basically all got 1 15 of the available memory on this",
    "start": "1733760",
    "end": "1739039"
  },
  {
    "text": "resource but if you think about it you've got a environment where you might have a large number of concurrent point",
    "start": "1739039",
    "end": "1745520"
  },
  {
    "text": "queries running and maybe some smaller number of batch queries and",
    "start": "1745520",
    "end": "1750559"
  },
  {
    "text": "you'd like to associate the number of resources accordingly so in this case if i had five queries of which",
    "start": "1750559",
    "end": "1757039"
  },
  {
    "text": "four were small one was big in the past i was basically allocating far too many",
    "start": "1757039",
    "end": "1762240"
  },
  {
    "text": "resources to that first number and now i can set it up so that the second queue",
    "start": "1762240",
    "end": "1767679"
  },
  {
    "text": "gets far more resources and presumably runs faster as you know the hash joins",
    "start": "1767679",
    "end": "1772960"
  },
  {
    "text": "may be run in memory and so on so this is a feature that approximate",
    "start": "1772960",
    "end": "1780240"
  },
  {
    "start": "1776000",
    "end": "1853000"
  },
  {
    "text": "count distinct where count distinct is something that we see a lot from our customers particularly our ad tech",
    "start": "1780240",
    "end": "1786799"
  },
  {
    "text": "environment where we you know people want to know how many people had an impression how many",
    "start": "1786799",
    "end": "1793039"
  },
  {
    "text": "distinct customers did i see with this campaign etc and account distinctive ends up being an",
    "start": "1793039",
    "end": "1798320"
  },
  {
    "text": "expensive operation because you basically need to sort and remove duplicates so we've added",
    "start": "1798320",
    "end": "1803840"
  },
  {
    "text": "an approximate version of that so rather than saying select count distinct customers from",
    "start": "1803840",
    "end": "1810480"
  },
  {
    "text": "campaign you would say select approximate account distinct customers from campaign",
    "start": "1810480",
    "end": "1815679"
  },
  {
    "text": "so not much of a change and we get on average about a 20x improvement uh",
    "start": "1815679",
    "end": "1822080"
  },
  {
    "text": "anywhere from 10 to 30. and we've just started implementing this feature so we're really interested in your feedback",
    "start": "1822080",
    "end": "1827919"
  },
  {
    "text": "on it and we're seeing about certainly less than a two percent error bar we're aiming for about a one percent error bar",
    "start": "1827919",
    "end": "1834559"
  },
  {
    "text": "on that so for you know it's maybe not what you would use for financial data where you",
    "start": "1834559",
    "end": "1839760"
  },
  {
    "text": "need to be right to the penny but if you're running things like campaigns impressions and things like that it",
    "start": "1839760",
    "end": "1845679"
  },
  {
    "text": "makes a lot of sense and you know the difference between an hour and a few seconds is significant",
    "start": "1845679",
    "end": "1854799"
  },
  {
    "start": "1853000",
    "end": "1959000"
  },
  {
    "text": "distributed tables so rahul talked a little bit earlier about how we have the model to specify how data is distributed",
    "start": "1854960",
    "end": "1861600"
  },
  {
    "text": "in the cluster so you can basically in the past you were able to specify data is round robin so basically every",
    "start": "1861600",
    "end": "1868559"
  },
  {
    "text": "new row goes onto a different node in the cluster so you get a nice even distribution and then we also provided",
    "start": "1868559",
    "end": "1874960"
  },
  {
    "text": "uh data is hashed and so the the reason you might pick a hash version of data is that that way the",
    "start": "1874960",
    "end": "1881919"
  },
  {
    "text": "joins are co-located and so i don't have to go across a network in order to satisfy everything",
    "start": "1881919",
    "end": "1887440"
  },
  {
    "text": "so what we've added now is also a third option which is all and so what we do in this case is that",
    "start": "1887440",
    "end": "1893600"
  },
  {
    "text": "the data for a particular query for a particular table would be replicated on each of the storage nodes",
    "start": "1893600",
    "end": "1900000"
  },
  {
    "text": "and if you resize it's going to be you know copied to the extra nodes either up or down etc and",
    "start": "1900000",
    "end": "1907360"
  },
  {
    "text": "the reason you want to do that is that you might have if you're running something like a star schema",
    "start": "1907360",
    "end": "1913919"
  },
  {
    "text": "a big fact table you're certainly going to hash that out and a bunch of dimension tables which might be",
    "start": "1913919",
    "end": "1919600"
  },
  {
    "text": "comparatively small let's say the months in the year or something like that and so you really",
    "start": "1919600",
    "end": "1925279"
  },
  {
    "text": "don't want to go across the network for that and you'd much rather have that come locally and so there is a standard",
    "start": "1925279",
    "end": "1932000"
  },
  {
    "text": "benchmark called a star schema benchmark which is basically a",
    "start": "1932000",
    "end": "1937039"
  },
  {
    "text": "star schema version of tpch which some of you may be familiar with and so running that on redshift before",
    "start": "1937039",
    "end": "1944240"
  },
  {
    "text": "we get a little of you know somewhere between two and three times improvement basically just by moving some of these",
    "start": "1944240",
    "end": "1951039"
  },
  {
    "text": "uh dimension tables to be replicated across all notes you know which is a simple alter table command",
    "start": "1951039",
    "end": "1959760"
  },
  {
    "start": "1959000",
    "end": "2036000"
  },
  {
    "text": "concurrency so a lot of uh basically all of the columnar data warehousing vendors",
    "start": "1961519",
    "end": "1967200"
  },
  {
    "text": "are optimized to be able to run big queries fast and so the notion there is is that i",
    "start": "1967200",
    "end": "1973120"
  },
  {
    "text": "want to basically throw a lot of hardware at an individual query and so we've all been in the",
    "start": "1973120",
    "end": "1979519"
  },
  {
    "text": "some level of teams or i shouldn't say all almost all i don't know all vendors",
    "start": "1979519",
    "end": "1985360"
  },
  {
    "text": "you know in the teens of concurrency as a result so you can run much higher concurrency workloads on an oltp system",
    "start": "1985360",
    "end": "1992640"
  },
  {
    "text": "of course but you can't then you know use all of your resources for that so",
    "start": "1992640",
    "end": "1998320"
  },
  {
    "text": "we're now moving towards allowing you to increase that number to 50 and",
    "start": "1998320",
    "end": "2003600"
  },
  {
    "text": "a lot of that has to do with workload resource management as i expressed earlier and so you'll have to be",
    "start": "2003600",
    "end": "2009360"
  },
  {
    "text": "thoughtful about what you know how and why you use this so you know you'll have to experiment but",
    "start": "2009360",
    "end": "2015039"
  },
  {
    "text": "what the basic thought here is is that if you have a mix of big queries and small queries then you can",
    "start": "2015039",
    "end": "2021360"
  },
  {
    "text": "run the small queries at much higher concurrency in one slide one queue with a much higher",
    "start": "2021360",
    "end": "2027919"
  },
  {
    "text": "workload and then run the other ones with much more resource against them again an area that we're going to",
    "start": "2027919",
    "end": "2033519"
  },
  {
    "text": "continue to evolve over time so that's basically the",
    "start": "2033519",
    "end": "2041200"
  },
  {
    "text": "the set of things that i'm prepared to talk about right now so if this is basically a i chart which shows the",
    "start": "2041200",
    "end": "2048398"
  },
  {
    "text": "various features that we've delivered over the course of this year since service launch and you can kind of",
    "start": "2048399",
    "end": "2054398"
  },
  {
    "text": "see from it both the notion of you know more things coming at a faster pace and",
    "start": "2054399",
    "end": "2061280"
  },
  {
    "text": "you know we're very much looking to that happen continue to be the case over time and uh you know there's six weeks left",
    "start": "2061280",
    "end": "2067760"
  },
  {
    "text": "in 2013 and we have uh significant plans for the rest of the year as well",
    "start": "2067760",
    "end": "2075720"
  },
  {
    "start": "2074000",
    "end": "2125000"
  },
  {
    "text": "with that i just wanted to give you a sense of some of the other people that you can talk to about",
    "start": "2076399",
    "end": "2082480"
  },
  {
    "text": "redshift here in some of the other sessions i mean we're a vendor you know vendors always",
    "start": "2082480",
    "end": "2087599"
  },
  {
    "text": "carry suspicion so you know i think on when a little later today there's a",
    "start": "2087599",
    "end": "2093280"
  },
  {
    "text": "session from has offers aggregate knowledge and desk.com which basically are talking about how they're trying to",
    "start": "2093280",
    "end": "2099359"
  },
  {
    "text": "get maximal performance from amazon redshift has offers talking is talking about how",
    "start": "2099359",
    "end": "2105119"
  },
  {
    "text": "to run loads very very quickly and like a minute at a",
    "start": "2105119",
    "end": "2110880"
  },
  {
    "text": "time aggregate knowledge is talking about how to run very complex workloads and desk.com is",
    "start": "2110880",
    "end": "2117040"
  },
  {
    "text": "actually puts redshift underneath their customer facing uh website so they can run that high",
    "start": "2117040",
    "end": "2122880"
  },
  {
    "text": "concurrency um then tomorrow amazon",
    "start": "2122880",
    "end": "2129599"
  },
  {
    "start": "2125000",
    "end": "2174000"
  },
  {
    "text": "the data warehouse is going to talk about how they've been using amazon redshift alongside their existing",
    "start": "2129599",
    "end": "2136480"
  },
  {
    "text": "technology stack and amazon runs one of the larger data warehouses in the world and so those",
    "start": "2136480",
    "end": "2142960"
  },
  {
    "text": "guys have a ton of knowledge and for those of you who are moving from enterprises or who have",
    "start": "2142960",
    "end": "2148160"
  },
  {
    "text": "large amounts of data in situ i think that that'll be an interesting talk to understand uh you know how",
    "start": "2148160",
    "end": "2154400"
  },
  {
    "text": "we think about things and then the third talk here is again um",
    "start": "2154400",
    "end": "2161760"
  },
  {
    "text": "one where we have a nasdaq talking about their experience outlook which is a",
    "start": "2161760",
    "end": "2169839"
  },
  {
    "text": "nordstrom's company and around isobar",
    "start": "2170480",
    "end": "2175960"
  }
]