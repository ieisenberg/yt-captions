[
  {
    "text": "In this video, you’ll see how to process \ndata from Amazon Kinesis Data Streams",
    "start": "0",
    "end": "3943"
  },
  {
    "text": "with Amazon Kinesis Data Analytics, \na managed service for Apache Flink.",
    "start": "3943",
    "end": "8157"
  },
  {
    "text": "With this solution, you can read streaming \ndata from a Kinesis data stream,",
    "start": "8922",
    "end": "12318"
  },
  {
    "text": "transform the data and output the \nresults to another Kinesis data stream,",
    "start": "12318",
    "end": "15924"
  },
  {
    "text": "and run real-time queries on the \ntransformed data from a Studio notebook.",
    "start": "15924",
    "end": "19678"
  },
  {
    "text": "Before we begin, let’s review the \nAmazon services we’ll be using.",
    "start": "22205",
    "end": "25444"
  },
  {
    "text": "Amazon Kinesis Data Streams is a fully\n managed service for ingesting high",
    "start": "26652",
    "end": "30414"
  },
  {
    "text": "velocity streaming data at any scale \nfrom a range of AWS services as well",
    "start": "30414",
    "end": "34665"
  },
  {
    "text": "as external devices like \nmobile apps and sensors.",
    "start": "34665",
    "end": "37309"
  },
  {
    "text": "Once data has been ingested, it can \nbe consumed by a number of different",
    "start": "38074",
    "end": "41289"
  },
  {
    "text": "services and applications.",
    "start": "41289",
    "end": "42829"
  },
  {
    "text": "In this video, our consumer will \nbe Amazon Kinesis Data Analytics,",
    "start": "43239",
    "end": "47264"
  },
  {
    "text": "which is a managed \nservice for Apache Flink.",
    "start": "47264",
    "end": "49327"
  },
  {
    "text": "We’ll use Amazon Kinesis Data Analytics\nto read data from Amazon Kinesis Data",
    "start": "51488",
    "end": "55543"
  },
  {
    "text": "Streams, perform some transformations \nupon this data, and then output the",
    "start": "55543",
    "end": "58849"
  },
  {
    "text": "results to another Kinesis data stream.",
    "start": "58849",
    "end": "60904"
  },
  {
    "text": "To get started, let's walk through \nthe steps for provisioning an",
    "start": "63120",
    "end": "65598"
  },
  {
    "text": "Amazon Kinesis data stream.",
    "start": "65598",
    "end": "67147"
  },
  {
    "text": "First, we’ll give our data stream a name.",
    "start": "68511",
    "end": "70160"
  },
  {
    "text": "Next, we’ll choose our data stream capacity.",
    "start": "72720",
    "end": "74802"
  },
  {
    "text": "On-demand mode provides a stream that \nadapts to variable input and output traffic.",
    "start": "75478",
    "end": "79655"
  },
  {
    "text": "As shown here, the maximum write \ncapacity for ingesting data in on-",
    "start": "80797",
    "end": "84069"
  },
  {
    "text": "demand mode is 200 megabytes per \nsecond and 200,000 records per second.",
    "start": "84069",
    "end": "88335"
  },
  {
    "text": "Read capacity can scale up \nto 400 megabytes per second.",
    "start": "88911",
    "end": "92000"
  },
  {
    "text": "With provisioned mode, we need \nto specify the stream's capacity.",
    "start": "93142",
    "end": "96328"
  },
  {
    "text": "For Kinesis Data Streams, \nthe unit of scale is a shard.",
    "start": "96606",
    "end": "99840"
  },
  {
    "text": "The write capacity per shard is one megabyte \nper second and one thousand records.",
    "start": "100760",
    "end": "104792"
  },
  {
    "text": "Read capacity is two megabytes per second.",
    "start": "105202",
    "end": "107506"
  },
  {
    "text": "For automatic scaling, we’ll \nselect on-demand mode.",
    "start": "108603",
    "end": "111273"
  },
  {
    "text": "The default data stream settings can be edited \nafter the data stream is created and active.",
    "start": "113591",
    "end": "117760"
  },
  {
    "text": "For demonstration purposes, we \nhave already created two data streams,",
    "start": "118968",
    "end": "122007"
  },
  {
    "text": "so let’s cancel out of this creation process.",
    "start": "122007",
    "end": "124269"
  },
  {
    "text": "We have one input and one output data stream.",
    "start": "125366",
    "end": "127631"
  },
  {
    "text": "Let’s switch to our Python environment to explore \nhow we’ll produce data into our input stream.",
    "start": "128696",
    "end": "133026"
  },
  {
    "text": "Kinesis Data Streams can capture data \nfrom many sources, including application",
    "start": "135276",
    "end": "139019"
  },
  {
    "text": "and service logs, clickstream data, \nsensor data, and in-app user events.",
    "start": "139019",
    "end": "143211"
  },
  {
    "text": "For this example, we’ll generate data by \nrunning an Apache Flink Table API locally.",
    "start": "143732",
    "end": "148000"
  },
  {
    "text": "We have defined our \ntable as the destination sink.",
    "start": "149219",
    "end": "151700"
  },
  {
    "text": "We also specified a table schema for\n sales orders that will be generated,",
    "start": "152797",
    "end": "156203"
  },
  {
    "text": "a Kinesis connector, the name of our input \nstream, our AWS Region, and the output format.",
    "start": "156203",
    "end": "161074"
  },
  {
    "text": "We’ll be using a built-in Apache Flink \nDataGen source object to generate",
    "start": "162215",
    "end": "165591"
  },
  {
    "text": "mock data for our sink table.",
    "start": "165591",
    "end": "167184"
  },
  {
    "text": "Let’s run the program.",
    "start": "168314",
    "end": "169400"
  },
  {
    "text": "The program has started producing data.",
    "start": "174676",
    "end": "176430"
  },
  {
    "text": "Let’s return to Amazon Kinesis \nto view our input data stream.",
    "start": "177150",
    "end": "180263"
  },
  {
    "text": "We’ll open the Data viewer tab.",
    "start": "181205",
    "end": "182888"
  },
  {
    "text": "Next, we'll select one of the shards \nfrom the stream and get its records.",
    "start": "184000",
    "end": "187170"
  },
  {
    "text": "Now that we’ve confirmed we’re generating \ndata into our Kinesis Data Stream,",
    "start": "190905",
    "end": "194396"
  },
  {
    "text": "let’s return to our Python environment.",
    "start": "194396",
    "end": "196327"
  },
  {
    "text": "Here, we have another \nApache Flink application.",
    "start": "199020",
    "end": "201308"
  },
  {
    "text": "In this case, the application reads the \ndata from our input stream, transforms",
    "start": "201763",
    "end": "205366"
  },
  {
    "text": "it by performing some aggregations, \nand then writes the transformed data",
    "start": "205366",
    "end": "208645"
  },
  {
    "text": "to our destination stream.",
    "start": "208645",
    "end": "210095"
  },
  {
    "text": "This destination sink schema is \nformatted for aggregated data.",
    "start": "211314",
    "end": "214609"
  },
  {
    "text": "We’ll use a tumbling window to aggregate \nthe total count and sum amount of each",
    "start": "215783",
    "end": "219114"
  },
  {
    "text": "product in the various orders \nwe're getting in 10 second intervals.",
    "start": "219114",
    "end": "222228"
  },
  {
    "text": "The transformed aggregated data is \nthen written to our destination data stream.",
    "start": "223414",
    "end": "226948"
  },
  {
    "text": "Now that we’ve viewed the code, \nlet’s see how to create a Kinesis",
    "start": "229729",
    "end": "232461"
  },
  {
    "text": "Data Analytics streaming application.",
    "start": "232461",
    "end": "234421"
  },
  {
    "text": "The Apache Flink configuration \ndefaults to the most recent version.",
    "start": "235474",
    "end": "238514"
  },
  {
    "text": "We’ll give the application a name.",
    "start": "240553",
    "end": "242047"
  },
  {
    "text": "For access permissions, we can either \ncreate or choose an AWS Identity and",
    "start": "244741",
    "end": "248525"
  },
  {
    "text": "Access Management (IAM) role.",
    "start": "248525",
    "end": "250562"
  },
  {
    "text": "Then we can review the settings and \ncreate our streaming application.",
    "start": "251789",
    "end": "254719"
  },
  {
    "text": "For our purposes, we’ve already created an \napplication that is actively processing data.",
    "start": "255372",
    "end": "259558"
  },
  {
    "text": "Let’s switch to that one.",
    "start": "259736",
    "end": "260889"
  },
  {
    "text": "First, we’ll review the \napplication’s configuration.",
    "start": "263216",
    "end": "265681"
  },
  {
    "text": "Here, we specified the \napplication code location.",
    "start": "266567",
    "end": "269031"
  },
  {
    "text": "The path includes the Amazon Simple \nStorage Service (Amazon S3) bucket",
    "start": "270240",
    "end": "273849"
  },
  {
    "text": "where the code is stored and the \nname of the compressed code file.",
    "start": "273850",
    "end": "276665"
  },
  {
    "text": "Additional configuration options include\n Snapshots, Checkpoint configuration,",
    "start": "277807",
    "end": "282278"
  },
  {
    "text": "Scaling, Logging and \nmonitoring, and Networking.",
    "start": "282278",
    "end": "286084"
  },
  {
    "text": "Now let’s open the Apache Flink dashboard \nto learn about the running application.",
    "start": "289351",
    "end": "293108"
  },
  {
    "text": "This graph provides details about \nthe running job, which is useful for",
    "start": "294260",
    "end": "297420"
  },
  {
    "text": "troubleshooting and fine-tuning \napplication performance.",
    "start": "297420",
    "end": "300232"
  },
  {
    "text": "Now let’s go to Amazon Kinesis Data \nAnalytics Studio, where we can create",
    "start": "302396",
    "end": "306181"
  },
  {
    "text": "a Studio notebook that will allow us \nto interact with the streaming data.",
    "start": "306181",
    "end": "309364"
  },
  {
    "text": "We can use a Quick create method or \ncreate a notebook with custom settings.",
    "start": "310495",
    "end": "314160"
  },
  {
    "text": "We then give the notebook a name, and\nspecify an IAM service role and an AWS",
    "start": "315214",
    "end": "319343"
  },
  {
    "text": "Glue database to define the metadata \nfor our source and destination.",
    "start": "319343",
    "end": "322836"
  },
  {
    "text": "We’ll be working with a previously \ncreated Studio notebook,",
    "start": "324011",
    "end": "326735"
  },
  {
    "text": "so we’ll switch over to that now.",
    "start": "326735",
    "end": "328518"
  },
  {
    "text": "Let's open this notebook in Apache Zeppelin.",
    "start": "331057",
    "end": "333138"
  },
  {
    "text": "In Apache Zeppelin we'll open a code \nfile that contains our order viewer.",
    "start": "334324",
    "end": "337697"
  },
  {
    "text": "This will allow us to view the transformed \norders that are being written to our output stream.",
    "start": "338251",
    "end": "342000"
  },
  {
    "text": "First, we define a table that specifies our\nschema and the details about where we",
    "start": "343208",
    "end": "346809"
  },
  {
    "text": "are reading our order data from.",
    "start": "346809",
    "end": "348466"
  },
  {
    "text": "Again, we’ll specify that we \nare using the Kinesis connector.",
    "start": "349596",
    "end": "352399"
  },
  {
    "text": "We'll be reading from our \norder output data stream.",
    "start": "353530",
    "end": "355871"
  },
  {
    "text": "Let’s run the code to create the table.",
    "start": "357700",
    "end": "359396"
  },
  {
    "text": "With our table created, we’ll run a real-\ntime query to view incoming orders",
    "start": "360571",
    "end": "364032"
  },
  {
    "text": "where the number of products \nin the order is greater than one.",
    "start": "364032",
    "end": "366564"
  },
  {
    "text": "Behind the scenes an Apache Flink job \nis started, the output of which is",
    "start": "367750",
    "end": "371130"
  },
  {
    "text": "presented to us in this Zeppelin notebook.",
    "start": "371130",
    "end": "373128"
  },
  {
    "text": "Our real-time query refreshes \nthe results every 10 seconds",
    "start": "374236",
    "end": "377382"
  },
  {
    "text": "We can click the FLINK JOB link to view \nthe job in the Apache Flink dashboard.",
    "start": "378690",
    "end": "382445"
  },
  {
    "text": "In the dashboard, we can see our job \nis running and explore its job graph.",
    "start": "383609",
    "end": "387192"
  },
  {
    "text": "You've just seen how to process data \nfrom Amazon Kinesis Data Streams",
    "start": "390193",
    "end": "393126"
  },
  {
    "text": "with Amazon Kinesis Data Analytics.",
    "start": "393126",
    "end": "395246"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "396587",
    "end": "399584"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "399945",
    "end": "401975"
  }
]