[
  {
    "start": "0",
    "end": "56000"
  },
  {
    "text": "all right good morning my name is Alan",
    "start": "410",
    "end": "6509"
  },
  {
    "text": "McGinnis I'm a Solutions Architect here at AWS work in our Big Data space",
    "start": "6509",
    "end": "11910"
  },
  {
    "text": "primarily focusing on streaming data helping a lot of customers do what we're",
    "start": "11910",
    "end": "18449"
  },
  {
    "text": "about to talk about today which is moving from batch to streaming so in my",
    "start": "18449",
    "end": "23730"
  },
  {
    "text": "role I just traditional II talk to customers about streaming data things like Kinesis in our space which we're",
    "start": "23730",
    "end": "28980"
  },
  {
    "text": "going to talk about a bit talked a lot too about things like Kafka as well to our customers today we're gonna focus on",
    "start": "28980",
    "end": "34200"
  },
  {
    "text": "though kind of just in some general terms to some extent how to move from",
    "start": "34200",
    "end": "39239"
  },
  {
    "text": "batch to streaming we have with me sitting off the side but he'll join us",
    "start": "39239",
    "end": "44399"
  },
  {
    "text": "about halfway through Brandon Smith and he's going to talk to you specifically about how his team in Amazon flex did",
    "start": "44399",
    "end": "53070"
  },
  {
    "text": "this exact thing so kind of the first half I'll cover I'll cover a number of things like the real-time streaming data",
    "start": "53070",
    "end": "59550"
  },
  {
    "start": "56000",
    "end": "78000"
  },
  {
    "text": "overview just so we're all on the same page get some information to you about just what is batch what is streaming and",
    "start": "59550",
    "end": "67049"
  },
  {
    "text": "then we'll jump over to Brandon who will cover a lot more detail about what they did so hopefully you can learn something",
    "start": "67049",
    "end": "73470"
  },
  {
    "text": "from what you know other teams within Amazon have done to make this transition",
    "start": "73470",
    "end": "78830"
  },
  {
    "start": "78000",
    "end": "198000"
  },
  {
    "text": "so we'll start by sort of setting a little bit of context this probably",
    "start": "78830",
    "end": "83939"
  },
  {
    "text": "isn't very new to most of you but you know what is batch processing maybe a show of hands who here because either",
    "start": "83939",
    "end": "89549"
  },
  {
    "text": "worked on or is you know already familiar with a batch process in your organization right been around for years",
    "start": "89549",
    "end": "95150"
  },
  {
    "text": "so what does the community say it is you know Wikipedia execution of a series of jobs in a program on a computer without",
    "start": "95150",
    "end": "102450"
  },
  {
    "text": "manual intervention that's a lot of words what's funny is the definition of",
    "start": "102450",
    "end": "107670"
  },
  {
    "text": "batch actually goes way back when we used to use punch cards to program computers so we had these cards right",
    "start": "107670",
    "end": "114030"
  },
  {
    "text": "with all the little cutouts that indicated the code and they were fed to the machines in batch right they were",
    "start": "114030",
    "end": "119250"
  },
  {
    "text": "provided in small batches to the machines to to program them so that's kind of where the term batch started but",
    "start": "119250",
    "end": "126659"
  },
  {
    "text": "what does it mean in the Big Data space or even even in the just the data space it's typically you know data collected",
    "start": "126659",
    "end": "131849"
  },
  {
    "text": "over a period of time processed and analyzed on a relatively regular schedule could be every hour",
    "start": "131849",
    "end": "138900"
  },
  {
    "text": "could be every day it could be weekly could be monthly but it's a well-known schedule doing something with that data",
    "start": "138900",
    "end": "146180"
  },
  {
    "text": "and then typically you combine several of these together to obtain your final",
    "start": "146180",
    "end": "152340"
  },
  {
    "text": "result so like I said you might have an hourly job and then at night you might roll up some of these hourly",
    "start": "152340",
    "end": "157849"
  },
  {
    "text": "aggregations into another value put that into another table so typically batch",
    "start": "157849",
    "end": "163109"
  },
  {
    "text": "processing is doing this continuously and it's using multiple batch jobs to",
    "start": "163109",
    "end": "168299"
  },
  {
    "text": "get your final result a great example of batch which I think you'll all be familiar with would be something like",
    "start": "168299",
    "end": "174239"
  },
  {
    "text": "your credit card statement right they obviously send you a bill once a month and how did I do that",
    "start": "174239",
    "end": "180690"
  },
  {
    "text": "it's a batch job most likely they they probably have some real-time components in there which we'll discuss but in general this this statement that you",
    "start": "180690",
    "end": "187709"
  },
  {
    "text": "receive once a month it's a batch job right it's running on it running on a regular schedule collecting a bunch of information doing some aggregation and",
    "start": "187709",
    "end": "194280"
  },
  {
    "text": "giving you the result so that's a high level view of batch so when we get to",
    "start": "194280",
    "end": "199530"
  },
  {
    "text": "talk about streaming we think about data that's produced continuously now what's",
    "start": "199530",
    "end": "206549"
  },
  {
    "text": "interesting here is that even in a batch world the data that's being batch is in most cases still being produced",
    "start": "206549",
    "end": "213510"
  },
  {
    "text": "continuously right whether the data is coming off of a mobile app or clickstream data from a website",
    "start": "213510",
    "end": "218819"
  },
  {
    "text": "application logs things like metering IRT sensors all of these things are",
    "start": "218819",
    "end": "224819"
  },
  {
    "text": "being created continuously so that we can kind of learn more about what's",
    "start": "224819",
    "end": "230280"
  },
  {
    "text": "going on in our business right now so that's sort of how we go from we start talking about going from batch where we're taking data and doing things with",
    "start": "230280",
    "end": "237629"
  },
  {
    "text": "it on an hour or daily basis but that data was created continuously so now we",
    "start": "237629",
    "end": "243150"
  },
  {
    "text": "look at the data and we think well this is being created continuously how can I take advantage of that going back to",
    "start": "243150",
    "end": "249930"
  },
  {
    "text": "that credit card example again imagine you're using your card you're all in Las Vegas you're probably not from Las Vegas",
    "start": "249930",
    "end": "257060"
  },
  {
    "text": "maybe some of you swiped your card here at the casino to take some cash out maybe you swiped it at a restaurant and",
    "start": "257060",
    "end": "262710"
  },
  {
    "text": "you got an alert from your credit card companies I noticed weird transaction you know you",
    "start": "262710",
    "end": "267830"
  },
  {
    "text": "were in New York yesterday now you're in Las Vegas you know reply one to this text if that's if that's you",
    "start": "267830",
    "end": "274130"
  },
  {
    "text": "so fraud detection in the credit card space right that's a great example of a real-time real-time use case of how",
    "start": "274130",
    "end": "280759"
  },
  {
    "text": "they're analyzing real-time data to make some intelligent decisions that help you as their customer so that starts to talk",
    "start": "280759",
    "end": "287900"
  },
  {
    "start": "286000",
    "end": "381000"
  },
  {
    "text": "us about it helps us to think about kind of the value of our data and the diminishing value of data over time so",
    "start": "287900",
    "end": "293780"
  },
  {
    "text": "we like to think about data being recent data being very valuable if you can do",
    "start": "293780",
    "end": "299810"
  },
  {
    "text": "something with it so that fraud detection is a great example there's a great paper from Forrester Research",
    "start": "299810",
    "end": "306110"
  },
  {
    "text": "called perishable insights that's where this chart comes from but it talks about you can make time critical decisions",
    "start": "306110",
    "end": "311599"
  },
  {
    "text": "with data that's being created right now and then as time goes on the older data",
    "start": "311599",
    "end": "318500"
  },
  {
    "text": "becomes less valuable but its importance can be increased if you can combine that",
    "start": "318500",
    "end": "325789"
  },
  {
    "text": "old data with the new data that's being created right now so imagine I'm generating real-time data from let's",
    "start": "325789",
    "end": "332690"
  },
  {
    "text": "just talk Alisa sees a clickstream example I've got a website I've got people clicking on it",
    "start": "332690",
    "end": "338240"
  },
  {
    "text": "viewing web pages on the website if I can combine an aggregate value of how",
    "start": "338240",
    "end": "343880"
  },
  {
    "text": "many users are looking at this particular item on my website in the last hour and then compare that with the",
    "start": "343880",
    "end": "350330"
  },
  {
    "text": "same item that the number of customers viewed last week last month last year I",
    "start": "350330",
    "end": "355759"
  },
  {
    "text": "get some insights into how my business is doing maybe with a promotion that I",
    "start": "355759",
    "end": "361159"
  },
  {
    "text": "ran on that particular item right if I can tell that I've got a lot more visits",
    "start": "361159",
    "end": "366470"
  },
  {
    "text": "to this particular webpage in the last hour than I did in the same hour yesterday and I ran a promotion last night or an email campaign great I'm",
    "start": "366470",
    "end": "373280"
  },
  {
    "text": "getting some success I'm seeing that my in real time I'm seeing that my campaign has been successful so recent data is",
    "start": "373280",
    "end": "379550"
  },
  {
    "text": "very valuable if you can act upon it so how do i process this real-time streaming data kind of a five phase",
    "start": "379550",
    "end": "386599"
  },
  {
    "start": "381000",
    "end": "531000"
  },
  {
    "text": "process now most customers that I work they don't have to do all five of these things when you're talking about processing streams but in general you of",
    "start": "386599",
    "end": "395180"
  },
  {
    "text": "course you collect the data in most cases you transform it in some I'm gonna get into more detail about",
    "start": "395180",
    "end": "400610"
  },
  {
    "text": "this shortly some amount of analysis and then you react and persist so these are",
    "start": "400610",
    "end": "406669"
  },
  {
    "text": "kind of the five steps that is very common but a lot of times some people just collect transform and persist maybe",
    "start": "406669",
    "end": "412759"
  },
  {
    "text": "you don't want to analyze it right there on the spot maybe you don't have to react to it maybe you just want to persist it so that you can do some",
    "start": "412759",
    "end": "418249"
  },
  {
    "text": "analysis later that's all fine it's still part of the streaming data kind of workflow key requirements to building a",
    "start": "418249",
    "end": "425119"
  },
  {
    "text": "streaming data system and these are pretty typical kind of a these aren't",
    "start": "425119",
    "end": "430339"
  },
  {
    "text": "necessarily specific to like the Amazon Kinesis suite of products but I'll talk briefly about what Kinesis can provide",
    "start": "430339",
    "end": "436699"
  },
  {
    "text": "in these six areas so streaming data system needs to be durable right when I",
    "start": "436699",
    "end": "442099"
  },
  {
    "text": "write data to it the data needs to be persistent 90b need to be confident that my message is that I'm streaming to this",
    "start": "442099",
    "end": "447589"
  },
  {
    "text": "system are stored in a durable manner needs to be continuous so as data is flowing through the system I need to be",
    "start": "447589",
    "end": "453949"
  },
  {
    "text": "able to process it you know in close to real-time if that's my requirement so a",
    "start": "453949",
    "end": "459529"
  },
  {
    "text": "continuously processing system is very important in the real-time space needs to be fast now the typical definition",
    "start": "459529",
    "end": "466129"
  },
  {
    "text": "that people when I talk to customers like what's real-time to you a lot of times they're like well real-time doesn't have to mean sub second to me",
    "start": "466129",
    "end": "472789"
  },
  {
    "text": "like if I get an answer in a minute that's great that's real-time enough for my business so fast is a little bit",
    "start": "472789",
    "end": "478569"
  },
  {
    "text": "contextual right it depends on what is your definition of fast but in most cases you know sub second in our space",
    "start": "478569",
    "end": "484759"
  },
  {
    "text": "we consider that pretty fast and so the system needs to be able to support your requirements in that regard needs to be",
    "start": "484759",
    "end": "491029"
  },
  {
    "text": "correct so there are semantics in real-time processing like at least once processing only once processing so these",
    "start": "491029",
    "end": "498169"
  },
  {
    "text": "are important to make sure that you maybe don't introduce duplicates into a downstream system so your system needs",
    "start": "498169",
    "end": "504050"
  },
  {
    "text": "to be able to support that needs to be reactive so if you're trying to do something with that real-time data",
    "start": "504050",
    "end": "510519"
  },
  {
    "text": "perhaps you want to alert an operator in your organization that there's an anomaly on the stream right you need to",
    "start": "510519",
    "end": "516110"
  },
  {
    "text": "have the ability to do that and needs to be reliable so if there's issues in this system it needs to fail quickly fail",
    "start": "516110",
    "end": "523419"
  },
  {
    "text": "fail fast in other words so that it comes back without having an impact on your system so it needs to be very",
    "start": "523420",
    "end": "529399"
  },
  {
    "text": "reliable platform and so with that we'll talk about a few of the systems that we have at AWS that enable",
    "start": "529399",
    "end": "536600"
  },
  {
    "start": "531000",
    "end": "648000"
  },
  {
    "text": "there's bullet points that I just discussed we have three products in the",
    "start": "536600",
    "end": "542299"
  },
  {
    "text": "Kinesis space and these are all part of our streaming streaming data products",
    "start": "542299",
    "end": "547749"
  },
  {
    "text": "konista streams Kinesis fire hose and Kinesis analytics now quick show hands how many folks are already using one or",
    "start": "547749",
    "end": "555139"
  },
  {
    "text": "some of these today in your environment awesome that was a pretty good pretty good show of hands so then I won't spend",
    "start": "555139",
    "end": "561350"
  },
  {
    "text": "a ton of time I know most interesting is kind of Brandon's talk on how they've done this so I won't spend a whole lot of time here but we will quickly cover",
    "start": "561350",
    "end": "568279"
  },
  {
    "text": "what they do so that if you're not familiar with it you can kind of get a good idea of how they might help you so Kinesis streams this was deployed",
    "start": "568279",
    "end": "576499"
  },
  {
    "text": "about five years ago it is we call it here if you this one of these bullet points says for technical developers the",
    "start": "576499",
    "end": "582619"
  },
  {
    "text": "reason we say that generally is because when you are producing data to a stream you typically have to write code to",
    "start": "582619",
    "end": "588470"
  },
  {
    "text": "produce the data and you also have to write code to consume the data so the streaming component the buffer in the",
    "start": "588470",
    "end": "594169"
  },
  {
    "text": "middle is a managed service but you're generally writing some code on both ends to put the data in there and to get the",
    "start": "594169",
    "end": "600019"
  },
  {
    "text": "data off now Kinesis firehose is one level higher in the you know it's",
    "start": "600019",
    "end": "606139"
  },
  {
    "text": "abstract some of the consumer side away from you it's like a managed consumer on top of a stream so while you still may",
    "start": "606139",
    "end": "612649"
  },
  {
    "text": "have to write code to produce data to firehose firehose will have a managed consumer that will take data off of the",
    "start": "612649",
    "end": "619369"
  },
  {
    "text": "stream and persist it somewhere that you configure without you having to touch any code so I'll get into more details",
    "start": "619369",
    "end": "624799"
  },
  {
    "text": "on firehose in a moment and then analytics so now that we have our data streaming through either Kinesis streams",
    "start": "624799",
    "end": "630079"
  },
  {
    "text": "or Kinesis firehose how can I get some insights into that data and that's what Kinesis analytics provides gives you the",
    "start": "630079",
    "end": "637160"
  },
  {
    "text": "ability to see into your data using standard SQL and do things like",
    "start": "637160",
    "end": "642289"
  },
  {
    "text": "filtering or aggregation on your streaming data and it can use data that's either in streams or firehose to",
    "start": "642289",
    "end": "647689"
  },
  {
    "text": "do that so streams gives you the ability to lie be reliably ingest and store",
    "start": "647689",
    "end": "654410"
  },
  {
    "start": "648000",
    "end": "756000"
  },
  {
    "text": "streaming data at low cost you build your own custom real-time applications so in this picture you see there's kind",
    "start": "654410",
    "end": "661009"
  },
  {
    "text": "of four consumers now there's a few different ways that you can",
    "start": "661009",
    "end": "666139"
  },
  {
    "text": "consume data off of a Kinesis stream we have Kinesis analytics on the top which I'll talk about and as I just mentioned",
    "start": "666139",
    "end": "672290"
  },
  {
    "text": "very common is that's something like spark running on EMR so if you are a spark shop you already have spark",
    "start": "672290",
    "end": "679209"
  },
  {
    "text": "developers then if you aren't industry mning yet then if you implement",
    "start": "679209",
    "end": "684319"
  },
  {
    "text": "something like a Kinesis streams you can implement spark as your consumer on top",
    "start": "684319",
    "end": "689540"
  },
  {
    "text": "of Kinesis stream probably the most common thing that we see we have a library ourselves KCl the Kinesis client",
    "start": "689540",
    "end": "697490"
  },
  {
    "text": "library which is a consumer library for Kinesis streams and that would be the third point here the custom code on ec2",
    "start": "697490",
    "end": "704199"
  },
  {
    "text": "it's probably the most popular stream processing library that runs on AWS",
    "start": "704199",
    "end": "710420"
  },
  {
    "text": "today it's a very simple library it abstracts a lot of the complexities to stream processing a very popular choice",
    "start": "710420",
    "end": "717410"
  },
  {
    "text": "for doing stream consumption and then the fourth one is the simplest and it's gaining a lot of traction is to use",
    "start": "717410",
    "end": "722569"
  },
  {
    "text": "lambda so quick show of hands AWS lambda how many folks here are pretty familiar with that good show hands awesome",
    "start": "722569",
    "end": "729050"
  },
  {
    "text": "yeah lambda is great is our serverless compute service you just write your function you write like the business",
    "start": "729050",
    "end": "736189"
  },
  {
    "text": "logic of what should happen to the data that's on the stream and then you make the relationship just through a",
    "start": "736189",
    "end": "741709"
  },
  {
    "text": "configuration you say this function is going to be the subscriber to this stream and then the function just",
    "start": "741709",
    "end": "746839"
  },
  {
    "text": "automatically gets invoked in your business logic is applied so it's a very simple way to do stream processing and",
    "start": "746839",
    "end": "753050"
  },
  {
    "text": "that's becoming very very popular because of its simplicity so fire hose",
    "start": "753050",
    "end": "758860"
  },
  {
    "start": "756000",
    "end": "780000"
  },
  {
    "text": "like I said a moment ago the nice thing about fire hose is the managed consumer so if I go back one all of these these",
    "start": "758860",
    "end": "765980"
  },
  {
    "text": "four things that you see here analytics EMR custom color and ec2 or lambda you",
    "start": "765980",
    "end": "773660"
  },
  {
    "text": "have to do some amount of coding right whether it's sequel or the code and lambda or your own custom code in SPARC or the KCl application it's your coding",
    "start": "773660",
    "end": "780519"
  },
  {
    "start": "780000",
    "end": "835000"
  },
  {
    "text": "here you're just configuring where does my day to go you say I'm streaming my",
    "start": "780519",
    "end": "785809"
  },
  {
    "text": "data into firehose I just want to land it in s3 that's all I want to do I want",
    "start": "785809",
    "end": "790879"
  },
  {
    "text": "to take my streaming data and I want to dump it into s3 so that I can run some other processes on it later or I want to",
    "start": "790879",
    "end": "796009"
  },
  {
    "text": "put it into redshift if you're already a redshift customer our data warehouse platform and you just want to put your data in there very simple just",
    "start": "796009",
    "end": "801920"
  },
  {
    "text": "configure fire hose fire hose will automatically manage how that data gets delivered into these destinations and",
    "start": "801920",
    "end": "808220"
  },
  {
    "text": "the destinations rs3 redshift and elasticsearch service it's just configuration on your end no",
    "start": "808220",
    "end": "814790"
  },
  {
    "text": "custom code required there is some coding you can do as a part of fire hose",
    "start": "814790",
    "end": "819889"
  },
  {
    "text": "to do data transformation kind of on the fly and I'm going to talk about that in more detail which is really cool but in",
    "start": "819889",
    "end": "827000"
  },
  {
    "text": "general fire hose just gives you that one layer of the abstraction so you don't have to write in a custom code to",
    "start": "827000",
    "end": "832819"
  },
  {
    "text": "get your data streaming data into these destinations and then analytics so I think I discussed it probably in as much",
    "start": "832819",
    "end": "839720"
  },
  {
    "start": "835000",
    "end": "891000"
  },
  {
    "text": "detail as we need to but basically you have your data in either fire hose or streams you want to do something with it",
    "start": "839720",
    "end": "845870"
  },
  {
    "text": "you want to say how many times has this event occurred in the past five minutes",
    "start": "845870",
    "end": "851920"
  },
  {
    "text": "right you write a very simple sequel statement that does a select count from and there's some some lower-level stuff",
    "start": "851920",
    "end": "858829"
  },
  {
    "text": "that we're not going to get into in this talk about how your data gets mapped into a schema because yes it's sequel therefore it needs a schema Kinesis",
    "start": "858829",
    "end": "866509"
  },
  {
    "text": "analytics will infer a schema automatically based on what it sees in the data that gives you the capability now to write sequel against that schema",
    "start": "866509",
    "end": "874329"
  },
  {
    "text": "but things like select count from select from where like applying filters these",
    "start": "874329",
    "end": "880699"
  },
  {
    "text": "are all very standard sequel statements that I'm sure you're all very familiar with you can write those have sequel",
    "start": "880699",
    "end": "886310"
  },
  {
    "text": "analytics apply those in real time against your stream and give you real-time results so doing streaming",
    "start": "886310",
    "end": "893839"
  },
  {
    "start": "891000",
    "end": "1184000"
  },
  {
    "text": "analysis what are some of the benefits of it the first one is and this is by",
    "start": "893839",
    "end": "899269"
  },
  {
    "text": "far and away kind of the biggest benefit that we see is getting the immediate results so if we think about when I was",
    "start": "899269",
    "end": "907579"
  },
  {
    "text": "talking about casus analytics and doing something like select count star from my stream that's a pretty common batch job",
    "start": "907579",
    "end": "916069"
  },
  {
    "text": "right a lot of times you're doing hourly roll-ups so let's say in your data",
    "start": "916069",
    "end": "921470"
  },
  {
    "text": "warehouse or in your batch back-end you're saying every hour I run this job to do a rollup so this is some batch job",
    "start": "921470",
    "end": "926480"
  },
  {
    "text": "that's running on the back end giving you account moving that data into a different in your data warehouse wouldn't it be",
    "start": "926480",
    "end": "932500"
  },
  {
    "text": "great if you didn't have to run that batch job just have those aggregations being done on the fly so my data is",
    "start": "932500",
    "end": "938530"
  },
  {
    "text": "being streamed I'm running a job that's looking at the streaming data in real time doing the count in real time to say",
    "start": "938530",
    "end": "945820"
  },
  {
    "text": "how many times has this thing happened over this past hour and then the output is that one value right the aggregate",
    "start": "945820",
    "end": "952270"
  },
  {
    "text": "value that you then save into your database so now you don't need that batch job anymore to do those hourly",
    "start": "952270",
    "end": "959110"
  },
  {
    "text": "aggregations you have perhaps 24 rows that you create one for out one for",
    "start": "959110",
    "end": "964510"
  },
  {
    "text": "every hour in real time in your database and each row just contains the the",
    "start": "964510",
    "end": "969910"
  },
  {
    "text": "hourly roll-up right there on the fly so it's pretty powerful you don't have to wait until tonight to get the hourly",
    "start": "969910",
    "end": "975460"
  },
  {
    "text": "results right there done right there as soon as that hour window closes so getting those immediate results is one",
    "start": "975460",
    "end": "980680"
  },
  {
    "text": "of the most powerful things you can do with kind of a streaming data analysis system doing streaming filtering is also",
    "start": "980680",
    "end": "987910"
  },
  {
    "text": "very powerful so a lot of times somewhere in your ETL process on the back end you do have amount a certain",
    "start": "987910",
    "end": "993460"
  },
  {
    "text": "amount of filtering you probably say oh this data right here I don't really need it in this job yes it's in my source data it's in this database but I'm just",
    "start": "993460",
    "end": "1000510"
  },
  {
    "text": "going to run part of my sequel statement is in my batch job and get rid of this",
    "start": "1000510",
    "end": "1005700"
  },
  {
    "text": "data so just get rid of it in real time right as the data comes in if it's a data type that I'm not interested in for",
    "start": "1005700",
    "end": "1011550"
  },
  {
    "text": "my analysis just filter it out well as part of your query right your real-time query just says where this is you know",
    "start": "1011550",
    "end": "1018690"
  },
  {
    "text": "where these values are within this set that you care about therefore all the other data that's not useful through",
    "start": "1018690",
    "end": "1024360"
  },
  {
    "text": "that particular job it just goes away don't need it for that things like anomaly detection so I talked about the",
    "start": "1024360",
    "end": "1030959"
  },
  {
    "text": "credit card use case that is one of the you know great benefits of getting immediate results from your streaming",
    "start": "1030960",
    "end": "1037560"
  },
  {
    "text": "data is how many it was my credit card used in a place where it shouldn't have been kind of in a more realistic sense",
    "start": "1037560",
    "end": "1043490"
  },
  {
    "text": "we have a lot of customers who do things like collecting all of their application",
    "start": "1043490",
    "end": "1048720"
  },
  {
    "text": "logs across all of their servers in their fleet that could be hundreds dozens or hundreds of servers they can",
    "start": "1048720",
    "end": "1054660"
  },
  {
    "text": "stream all those logs through a Kinesis stream or Kinesis firehose and they analyze it with something like Kinesis",
    "start": "1054660",
    "end": "1061650"
  },
  {
    "text": "analytics and they look for anomalies so in an Apache access I've got a customer who looks at Apache",
    "start": "1061650",
    "end": "1067499"
  },
  {
    "text": "access logs and they do a count of how many 500 response codes were returned by",
    "start": "1067499",
    "end": "1072590"
  },
  {
    "text": "by all of their systems in aggregate and when it goes beyond a threshold that they've set they're like something's",
    "start": "1072590",
    "end": "1078480"
  },
  {
    "text": "messed up here and they alert their operators to go take a look at it so very great way to do anomaly detection",
    "start": "1078480",
    "end": "1084240"
  },
  {
    "text": "is streaming the data in real-time and building some analysis on it right there as soon as it's generated in many cases",
    "start": "1084240",
    "end": "1090870"
  },
  {
    "text": "it can reduce your complexity depends on the complexity of course of your batch jobs but if you have a lot of different",
    "start": "1090870",
    "end": "1095940"
  },
  {
    "text": "jobs doing things kind of I mean in either in order or out of order on an hourly or daily basis like I was",
    "start": "1095940",
    "end": "1102179"
  },
  {
    "text": "mentioning earlier if you can do some of that in real time you can remove a lot of those batch jobs off the back end you",
    "start": "1102179",
    "end": "1107820"
  },
  {
    "text": "know in most cases I would say we're not suggesting that your batch jobs go away entirely this is just likely going to",
    "start": "1107820",
    "end": "1113850"
  },
  {
    "text": "simplify your batch processes because you're doing a lot of the roll-ups in real time and it's fully managed so all",
    "start": "1113850",
    "end": "1122190"
  },
  {
    "text": "the Kinesis services that we just discussed are fully managed there's no servers that you need to manage to make",
    "start": "1122190",
    "end": "1128009"
  },
  {
    "text": "this work very very very simple just a lot of configuration and that's it no no",
    "start": "1128009",
    "end": "1133559"
  },
  {
    "text": "servers and potentially very little code if you're using something like firehose and it's very scalable so it enables",
    "start": "1133559",
    "end": "1141119"
  },
  {
    "text": "parallel processing we didn't get into the details of streams and how it scales but things like Kinesis streams and",
    "start": "1141119",
    "end": "1146190"
  },
  {
    "text": "Kinesis firehose as your data ingestion rate increases there's really very",
    "start": "1146190",
    "end": "1151440"
  },
  {
    "text": "little you have to do to support that in the Kinesis streams case you might just have to go add a few shards in the",
    "start": "1151440",
    "end": "1157679"
  },
  {
    "text": "firehose case there's nothing it automatically will recognize that the rate at which you're adding records to",
    "start": "1157679",
    "end": "1165059"
  },
  {
    "text": "the stream maybe has reached a threshold and under the covers firehose will increase capacity so there's very little",
    "start": "1165059",
    "end": "1171840"
  },
  {
    "text": "you have to do to manage at a kind of ever-increasing ingestion rate and a",
    "start": "1171840",
    "end": "1178350"
  },
  {
    "text": "horizontally scale so that's the kind of the point I just made you can easily scale wide to support more data",
    "start": "1178350",
    "end": "1184700"
  },
  {
    "start": "1184000",
    "end": "1229000"
  },
  {
    "text": "so before brandon comes up we'll talk about a few best practices I think this",
    "start": "1184700",
    "end": "1190559"
  },
  {
    "text": "will help put a lot of these things together the first thing and this is a doesn't always apply to just you know",
    "start": "1190559",
    "end": "1196289"
  },
  {
    "text": "this doesn't just apply to streaming but if you're thinking of moving from a batch to a streaming solution don't boil",
    "start": "1196289",
    "end": "1203730"
  },
  {
    "text": "the ocean don't try and do it all at once right there's some techniques that you can take that will make it much simpler so let's we take a look at this",
    "start": "1203730",
    "end": "1210810"
  },
  {
    "text": "picture here pretty exam pretty typical example we've got something producing data it's an application running through",
    "start": "1210810",
    "end": "1216660"
  },
  {
    "text": "some app databases I have an example of just a simple ETL job here that's taking some data maybe dumping it into a data warehouse",
    "start": "1216660",
    "end": "1222780"
  },
  {
    "text": "and then maybe there's some other ETL jobs that are running against the data in the data warehouse pretty really",
    "start": "1222780",
    "end": "1227850"
  },
  {
    "text": "simple example right so begin by streaming your data in parallel to your",
    "start": "1227850",
    "end": "1234450"
  },
  {
    "start": "1229000",
    "end": "1306000"
  },
  {
    "text": "existing batch process there's very low risk and very little cost associated with just streaming the data in parallel",
    "start": "1234450",
    "end": "1242160"
  },
  {
    "text": "to your batch so let your batch run your business probably depends on it so your",
    "start": "1242160",
    "end": "1247350"
  },
  {
    "text": "first step could be okay I'm going to put a producer on my on my application",
    "start": "1247350",
    "end": "1254280"
  },
  {
    "text": "that's going to take their log records and just forward them off to Kinesis and then I'll just drop those data into s3",
    "start": "1254280",
    "end": "1260610"
  },
  {
    "text": "and I can do something with it later right very simple first step and then",
    "start": "1260610",
    "end": "1266340"
  },
  {
    "text": "add in your streaming analysis over time right so as I do that using Amazon",
    "start": "1266340",
    "end": "1272850"
  },
  {
    "text": "Kinesis to do things like doing the aggregations in real time it can forward",
    "start": "1272850",
    "end": "1278010"
  },
  {
    "text": "the aggregate values straight to the database and I can over time I think my",
    "start": "1278010",
    "end": "1283200"
  },
  {
    "text": "slide built out there yeah so you can see right over time certain components",
    "start": "1283200",
    "end": "1288720"
  },
  {
    "text": "of my system will go away as I slowly add more capabilities to my streaming data system so the point of this one is",
    "start": "1288720",
    "end": "1296250"
  },
  {
    "text": "you know do it incrementally don't worry about going from batch to streaming all in one shot pick the pieces that make",
    "start": "1296250",
    "end": "1302250"
  },
  {
    "text": "the most sense for your business and do it in parallel another best practice is",
    "start": "1302250",
    "end": "1308460"
  },
  {
    "start": "1306000",
    "end": "1344000"
  },
  {
    "text": "performing ITL rather than ETL so what do we mean by ITL ingest transform and",
    "start": "1308460",
    "end": "1314490"
  },
  {
    "text": "load as opposed as opposed to extract transform and load so ETL of course means typically we extract data from",
    "start": "1314490",
    "end": "1321330"
  },
  {
    "text": "another database another data source somewhere else right we have probably relational databases scattered throughout my architecture and ETL jobs",
    "start": "1321330",
    "end": "1329160"
  },
  {
    "text": "are running to take data out of one do some combination and put it into another well if I talk about ingest transform and",
    "start": "1329160",
    "end": "1335300"
  },
  {
    "text": "load that means I'm taking the data as its created I'm ingesting it and then I'm transforming it and loading it into",
    "start": "1335300",
    "end": "1341750"
  },
  {
    "text": "my downstream systems so how can we do that so doing the transformations in",
    "start": "1341750",
    "end": "1346970"
  },
  {
    "start": "1344000",
    "end": "1487000"
  },
  {
    "text": "near-real-time rather than a scheduled job so a lot of times your data that's coming in as input might be in one",
    "start": "1346970",
    "end": "1353990"
  },
  {
    "text": "format which might not be quite the same format that you need in your downstream system so a very simple example perhaps",
    "start": "1353990",
    "end": "1359630"
  },
  {
    "text": "your downstream system is elasticsearch as a very simple example elasticsearch wants you to upload json documents into",
    "start": "1359630",
    "end": "1366950"
  },
  {
    "text": "the indices what if your data that's being generated is CSV or some",
    "start": "1366950",
    "end": "1372740"
  },
  {
    "text": "unstructured text or some other delimited type of text that is your",
    "start": "1372740",
    "end": "1377810"
  },
  {
    "text": "application produces well if you can transform that in real time you don't need a back-end job to do it right so",
    "start": "1377810",
    "end": "1383630"
  },
  {
    "text": "transform it in real time using something like firehose and lambda so fire house the fire hose has direct",
    "start": "1383630",
    "end": "1389570"
  },
  {
    "text": "integration with lambda that allows you to write a very simple lambda function that will take the raw data in firehose",
    "start": "1389570",
    "end": "1395420"
  },
  {
    "text": "and apply your business logic and in this case it might be converting from something unstructured to JSON and then",
    "start": "1395420",
    "end": "1401870"
  },
  {
    "text": "it'll return that data back to the fire hose system which will then persist the",
    "start": "1401870",
    "end": "1406880"
  },
  {
    "text": "JSON into something like elasticsearch you can also enrich the data on the fly a lot a very common back-end batch",
    "start": "1406880",
    "end": "1413210"
  },
  {
    "text": "process is enriching this data I have data over here which might be keyed by something like a customer ID and perhaps",
    "start": "1413210",
    "end": "1419930"
  },
  {
    "text": "in my destination table I want to include the customer name as a part of the record that I dump into this other",
    "start": "1419930",
    "end": "1425390"
  },
  {
    "text": "table well if I can and you might have a back-end job that does that as a part of",
    "start": "1425390",
    "end": "1430700"
  },
  {
    "text": "the ETL job it enriches the data to include the name as well as the customer ID well I'll do that on the fly",
    "start": "1430700",
    "end": "1437420"
  },
  {
    "text": "have your enrichment data source called from this lambda function that's looking",
    "start": "1437420",
    "end": "1443300"
  },
  {
    "text": "at these so now we've got our raw data coming in which represents something like a customer and it's unstructured so",
    "start": "1443300",
    "end": "1451370"
  },
  {
    "text": "my data transformation function takes the unstructured data converts it to JSON takes the customer ID calls out to",
    "start": "1451370",
    "end": "1459050"
  },
  {
    "text": "my enrichment datasource gets the customer name and now creates that in red structured JSON record and then passes",
    "start": "1459050",
    "end": "1467170"
  },
  {
    "text": "it back to fire hose and fire hose will now persist that final record into the",
    "start": "1467170",
    "end": "1473470"
  },
  {
    "text": "destination and in this case I'm showing s3 but it could be something like elasticsearch as well so now you don't",
    "start": "1473470",
    "end": "1479530"
  },
  {
    "text": "have a back-end enrichment or transformation job it's all being done in real time as the data is produced a",
    "start": "1479530",
    "end": "1488070"
  },
  {
    "start": "1487000",
    "end": "1600000"
  },
  {
    "text": "couple of other ones so aggregate upon arrival so continuously write your raw",
    "start": "1488070",
    "end": "1493330"
  },
  {
    "text": "data to persistent store so basically one of the nice things",
    "start": "1493330",
    "end": "1500290"
  },
  {
    "text": "about using something like Kinesis analytics is that it it's just a parallel path to something like an ISA",
    "start": "1500290",
    "end": "1506170"
  },
  {
    "text": "Kinesis dreams or Kinesis firehose so if you're writing your raw streaming data into firehose you can also save your raw",
    "start": "1506170",
    "end": "1513880"
  },
  {
    "text": "data into s3 because a lot of cases you want to collect all that data and you just want to store the value the",
    "start": "1513880",
    "end": "1519100"
  },
  {
    "text": "original values that were created but you still want to do some amount of aggregation on it in real time so the",
    "start": "1519100",
    "end": "1526000"
  },
  {
    "text": "top path here shows our data being passed a firehose firehose is going to store the raw data and s3 uh Nagre gated",
    "start": "1526000",
    "end": "1532960"
  },
  {
    "text": "just the raw text but then in parallel I'm going to have Kinesis analytics do",
    "start": "1532960",
    "end": "1538120"
  },
  {
    "text": "an aggregate and aggregation on that data using the business logic that you would write in your sequel code and then",
    "start": "1538120",
    "end": "1544900"
  },
  {
    "text": "it's going to dump that aggregated data into a data store and depends what your end goal here is but very typical",
    "start": "1544900",
    "end": "1553480"
  },
  {
    "text": "pattern that we see is a lot of customers stream data into firehose persist the raw data into s3 but they",
    "start": "1553480",
    "end": "1559510"
  },
  {
    "text": "want to drive a real-time dashboard off of the data that's being created so",
    "start": "1559510",
    "end": "1564580"
  },
  {
    "text": "they'll create a Kinesis analytics application to do like one-hour roll-ups and then dump the one-hour aggregate",
    "start": "1564580",
    "end": "1571720"
  },
  {
    "text": "values into something like dynamo DB and then build a dashboard on top of the dynamo table as opposed to building a",
    "start": "1571720",
    "end": "1578050"
  },
  {
    "text": "dashboard off of s3 are off of the data warehouse where they have to run these roll-up jobs later so with the",
    "start": "1578050",
    "end": "1583900"
  },
  {
    "text": "aggregations being done in real-time that dashboard is updated as soon as you know the one hour is up so if my",
    "start": "1583900",
    "end": "1590170"
  },
  {
    "text": "business person is looking for some information about the noon to one hour period they will get that information",
    "start": "1590170",
    "end": "1597070"
  },
  {
    "text": "you know 101 p.m. which is pretty powerful so with that I'll hand it over",
    "start": "1597070",
    "end": "1602470"
  },
  {
    "text": "to Brandon who's going to talk about how they took a lot of these concepts and implemented it in a actual example I",
    "start": "1602470",
    "end": "1611730"
  },
  {
    "text": "don't hear any there there it is alright thanks Alan hey everyone so I'm Brandon Smith I am a",
    "start": "1616710",
    "end": "1624220"
  },
  {
    "text": "software engineer and I've been at Amazon over 12 years now I started out",
    "start": "1624220",
    "end": "1629290"
  },
  {
    "text": "in Seattle worked in the Kindle group for about eight years I worked on the original Kindle and on you know several",
    "start": "1629290",
    "end": "1635950"
  },
  {
    "text": "different projects in the Kindle group then I had the opportunity to go work in Amsterdam and so I transferred out there",
    "start": "1635950",
    "end": "1641650"
  },
  {
    "text": "for a couple years and worked in our office out there in the AWS work mail team so I helped build and launch that",
    "start": "1641650",
    "end": "1646840"
  },
  {
    "text": "service and then I moved to Austin Texas where I'm based now and I work in the last mile delivery department of Amazon",
    "start": "1646840",
    "end": "1653710"
  },
  {
    "text": "we're basically responsible for getting all the packages that you guys order from our warehouses to your front",
    "start": "1653710",
    "end": "1659530"
  },
  {
    "text": "doorsteps and so for the last couple of years I've been working on a project called Amazon flex and so what does",
    "start": "1659530",
    "end": "1666850"
  },
  {
    "start": "1665000",
    "end": "1729000"
  },
  {
    "text": "Amazon flex so from the screenshot up here on the right you can see that it's",
    "start": "1666850",
    "end": "1672100"
  },
  {
    "text": "a mobile app for Android and iOS that that drivers use to deliver packages to",
    "start": "1672100",
    "end": "1677170"
  },
  {
    "text": "people's houses how many of you have heard of this thing Amazon flex before okay a few people yeah so you know we",
    "start": "1677170",
    "end": "1683290"
  },
  {
    "text": "have this crowdsource model that's launched in over 30 cities around the u.s. and you know think of it as one of",
    "start": "1683290",
    "end": "1690820"
  },
  {
    "text": "the similar to one of the other gig economy apps but for Amazon packages right so Amazon flex this app allows",
    "start": "1690820",
    "end": "1698110"
  },
  {
    "text": "delivery partners who are independent contractors to you know work as little",
    "start": "1698110",
    "end": "1703210"
  },
  {
    "text": "or as much as they want and control their own schedule delivery partners sign up through the",
    "start": "1703210",
    "end": "1709360"
  },
  {
    "text": "app they onboard themselves they go through a background check when the background check clears",
    "start": "1709360",
    "end": "1714760"
  },
  {
    "text": "then they're ready to deliver then the app kind of walks them through all the Amazon delivery processes that they need to follow I lets them set up their",
    "start": "1714760",
    "end": "1722530"
  },
  {
    "text": "schedule at suggest turn-by-turn navigation and shows them their earnings but they have in the app",
    "start": "1722530",
    "end": "1729559"
  },
  {
    "start": "1729000",
    "end": "1791000"
  },
  {
    "text": "we also have a sort of second set of users a second set of users which our",
    "start": "1729559",
    "end": "1735059"
  },
  {
    "text": "frame is on logistics and these are delivery service providers that are companies that deliver packages",
    "start": "1735059",
    "end": "1742320"
  },
  {
    "text": "worldwide for Amazon so the app is used to make deliveries around the world for",
    "start": "1742320",
    "end": "1750269"
  },
  {
    "text": "several Amazon businesses so it delivers standard amazon.com orders delivers",
    "start": "1750269",
    "end": "1755489"
  },
  {
    "text": "prime now orders delivers Amazon fresh restaurants and grocery store orders",
    "start": "1755489",
    "end": "1761659"
  },
  {
    "text": "it's currently used to deliver millions of packages per year all over the world so as you can imagine the app has a",
    "start": "1761659",
    "end": "1768239"
  },
  {
    "text": "pretty complex set of delivery workflows to support all the various pickup and",
    "start": "1768239",
    "end": "1773340"
  },
  {
    "text": "routing scanning packages all the delivery actions as well as the offline features like the ability to fertility",
    "start": "1773340",
    "end": "1780629"
  },
  {
    "text": "partners to find their scheduled blocks of work which are essentially correspond",
    "start": "1780629",
    "end": "1785639"
  },
  {
    "text": "to different route links so anywhere from two to four hours typically so",
    "start": "1785639",
    "end": "1791999"
  },
  {
    "start": "1791000",
    "end": "1890000"
  },
  {
    "text": "let's talk about the the problem at a high level of what we were trying to solve here what we want to do similar to",
    "start": "1791999",
    "end": "1799289"
  },
  {
    "text": "what Alan was talking about right is to ingest transform and load right so we wanted to collect process and store",
    "start": "1799289",
    "end": "1804809"
  },
  {
    "text": "telemetry data from the mat from the Amazon flex app while it's out there",
    "start": "1804809",
    "end": "1810659"
  },
  {
    "text": "being used in the field to make deliveries and so what is telemetry data telemetry data the word time little",
    "start": "1810659",
    "end": "1818220"
  },
  {
    "text": "em'ly tree comes from the Greek roots meaning remote and measurement so think",
    "start": "1818220",
    "end": "1823379"
  },
  {
    "text": "of it as any data that you want to remotely measure and collect out from your from your app out in the field so",
    "start": "1823379",
    "end": "1830039"
  },
  {
    "text": "this includes things like metrics that we want to see that are happening in the app crashes when those happen log data",
    "start": "1830039",
    "end": "1836399"
  },
  {
    "text": "sensor data click stream data as you can kind of click or see what the user is doing as their as they're using the app",
    "start": "1836399",
    "end": "1842539"
  },
  {
    "text": "that kind of stuff so for the Amazon",
    "start": "1842539",
    "end": "1848009"
  },
  {
    "text": "flex app what that means as we collect things like GPS data coordinates and metrics on how the app is being used we",
    "start": "1848009",
    "end": "1857460"
  },
  {
    "text": "collect events for when the user does things like when they log in for example to use the app when they",
    "start": "1857460",
    "end": "1864160"
  },
  {
    "text": "scan a package and when a package is delivered so to give you a sense of this",
    "start": "1864160",
    "end": "1869200"
  },
  {
    "text": "scale we kind of talked about the stream zero the year we currently collect and",
    "start": "1869200",
    "end": "1874480"
  },
  {
    "text": "process millions and millions of events per day and in our Kinesis stream that",
    "start": "1874480",
    "end": "1879550"
  },
  {
    "text": "alan mentioned before we currently process anywhere from five to twenty megabytes of data per second coming into",
    "start": "1879550",
    "end": "1885430"
  },
  {
    "text": "those streams and it kind of depends on the time of day as it Peaks something down and so what is the goal what's the",
    "start": "1885430",
    "end": "1892450"
  },
  {
    "start": "1890000",
    "end": "1955000"
  },
  {
    "text": "goal that we have here you know basically it's to understand what's happening out in the field we want to",
    "start": "1892450",
    "end": "1899710"
  },
  {
    "text": "make sure that our app isn't performing at its full potential and very efficiently so that if it is right it",
    "start": "1899710",
    "end": "1906220"
  },
  {
    "text": "can save save millions of dollars over the course of a year and more importantly improve and make the delivery experience better for the for",
    "start": "1906220",
    "end": "1913540"
  },
  {
    "text": "the for both the delivery providers that are using the app and for the end customers that are getting packages delivered to their houses and so what we",
    "start": "1913540",
    "end": "1920230"
  },
  {
    "text": "really want to do is to collect and analyze all this telemetry data so we can optimize the apps performance and",
    "start": "1920230",
    "end": "1926530"
  },
  {
    "text": "the delivery workflows so we can focus our time on improving how the app is",
    "start": "1926530",
    "end": "1932020"
  },
  {
    "text": "working how how performance the app is and how intuitive and easy those delivery workflows are for the users so",
    "start": "1932020",
    "end": "1939580"
  },
  {
    "text": "for example some of the ways that we've been able to improve the app performance include optimizing the login time",
    "start": "1939580",
    "end": "1945750"
  },
  {
    "text": "optimizing the different network holes that the app makes and improving our routing algorithms based on the route",
    "start": "1945750",
    "end": "1952210"
  },
  {
    "text": "actually the actual route execution that has happened okay so next I'm gonna walk",
    "start": "1952210",
    "end": "1958150"
  },
  {
    "start": "1955000",
    "end": "1982000"
  },
  {
    "text": "through some use cases these are just some use cases that we had that we wanted to solve they're probably similar",
    "start": "1958150",
    "end": "1964060"
  },
  {
    "text": "to some of the use cases that you know you may want to solve with your applications you don't have to remember",
    "start": "1964060",
    "end": "1969340"
  },
  {
    "text": "these use cases and just kind of give them an examples to walk through as examples and then later in the talk I'll",
    "start": "1969340",
    "end": "1976810"
  },
  {
    "text": "go over the actual architecture and the solution that we came up with and how we solved some of these use cases so the",
    "start": "1976810",
    "end": "1982840"
  },
  {
    "start": "1982000",
    "end": "2032000"
  },
  {
    "text": "first one is to collect real-time metrics so that we can build alarms right this is probably the most important thing when you're building",
    "start": "1982840",
    "end": "1989260"
  },
  {
    "text": "application is to alarms so you know when something is not working right you know ideally want to know within",
    "start": "1989260",
    "end": "1995350"
  },
  {
    "text": "minutes if there are any delivery problems either worldwide or isolate it",
    "start": "1995350",
    "end": "2001350"
  },
  {
    "text": "into a city or neighborhood that we're in so for example if the delivery count drops below our expected thresholds we",
    "start": "2001350",
    "end": "2009270"
  },
  {
    "text": "want to trigger an alarm so we know about that and we can look into it perhaps we'll schedule more drivers to",
    "start": "2009270",
    "end": "2015360"
  },
  {
    "text": "pick up the slack and if there's a surge in demand or maybe there's a bug that",
    "start": "2015360",
    "end": "2020640"
  },
  {
    "text": "we've discovered that we know we need immediate patching and so alarming as you can see is very important really",
    "start": "2020640",
    "end": "2028110"
  },
  {
    "text": "it's it's this case is all about monitoring and arming when something goes wrong the second use",
    "start": "2028110",
    "end": "2034110"
  },
  {
    "start": "2032000",
    "end": "2088000"
  },
  {
    "text": "case is for troubleshooting and so having alarms and being notified of the problem is the first step the",
    "start": "2034110",
    "end": "2041640"
  },
  {
    "text": "next step is to be able to troubleshoot what's happening out in the field when there is a problem that's identified so",
    "start": "2041640",
    "end": "2048419"
  },
  {
    "text": "in this case we wanted to be able to do real-time log and crash analysis we",
    "start": "2048419",
    "end": "2054780"
  },
  {
    "text": "consider logs and crashes telemetry data because as I mentioned before it's it's all remote data that's that's being",
    "start": "2054780",
    "end": "2060090"
  },
  {
    "text": "collected or that's that's happening out in the field so it falls under this kind of telemetry bucket so what we do is we",
    "start": "2060090",
    "end": "2067408"
  },
  {
    "text": "probably want to do is publish all these logs to cloud watch logs in AWS where you can filter and search and look",
    "start": "2067409",
    "end": "2074190"
  },
  {
    "text": "through all that data in real time and you know as a developer that's in the on-call rotation this is really kind of",
    "start": "2074190",
    "end": "2080970"
  },
  {
    "text": "an essential tool right as you guys probably know for being able to remotely figure out and diagnose what's happening",
    "start": "2080970",
    "end": "2086368"
  },
  {
    "text": "out there in the field the third use case is another pretty common one it's",
    "start": "2086369",
    "end": "2091800"
  },
  {
    "start": "2088000",
    "end": "2147000"
  },
  {
    "text": "to have you know reporting dashboards so normally in the batch processing that",
    "start": "2091800",
    "end": "2098580"
  },
  {
    "text": "that alan mentioned before you know you can write sequel you can generate reports you can create visualizations from that data but what we really want",
    "start": "2098580",
    "end": "2106440"
  },
  {
    "text": "is real-time dashboards instead of sort of daily or hourly batch reports so",
    "start": "2106440",
    "end": "2112410"
  },
  {
    "text": "clearly to achieve this we need to go from daily batch processing to to",
    "start": "2112410",
    "end": "2117900"
  },
  {
    "text": "real-time metrics dashboards we wanted have dashboards to monitor various things",
    "start": "2117900",
    "end": "2123090"
  },
  {
    "text": "oops including having health dashboards for operations manager that are seeing what's happening in a",
    "start": "2123090",
    "end": "2129320"
  },
  {
    "text": "warehouse or delivery station having crash report dashboards and having",
    "start": "2129320",
    "end": "2136070"
  },
  {
    "text": "deployment status dashboards this data is all you know aggregated and anonymized of course and so we're kind",
    "start": "2136070",
    "end": "2143840"
  },
  {
    "text": "of looking for the general trends in these reports and these dashboards the next use case is for releases for doing",
    "start": "2143840",
    "end": "2151340"
  },
  {
    "start": "2147000",
    "end": "2207000"
  },
  {
    "text": "deploying software updates to the app deploying mobile software app updates can be tricky as some of you may know",
    "start": "2151340",
    "end": "2157480"
  },
  {
    "text": "when you have to consider that it's worldwide there's various configuration options the backend services could be",
    "start": "2157480",
    "end": "2163700"
  },
  {
    "text": "changing at the same time underneath and so it's really important to be able to monitor and see the status of your",
    "start": "2163700",
    "end": "2169040"
  },
  {
    "text": "deployments as they're getting rolled out around the world and so when we deploy new map versions we want to",
    "start": "2169040",
    "end": "2174530"
  },
  {
    "text": "monitor these the status in real time we wanted to be able to release code you",
    "start": "2174530",
    "end": "2180290"
  },
  {
    "text": "know smoothly and confidently so for example it's valuable to see real-time",
    "start": "2180290",
    "end": "2186440"
  },
  {
    "text": "metrics and visualizations in dashboards for the different release candidates that are going out and to sort of",
    "start": "2186440",
    "end": "2192980"
  },
  {
    "text": "smaller subsets of users and smaller groups that were that we're deploying the changes to before we deploy it out",
    "start": "2192980",
    "end": "2199480"
  },
  {
    "text": "to everyone around the world because obviously our app is you know being used to make millions and millions of",
    "start": "2199480",
    "end": "2205220"
  },
  {
    "text": "deliveries right so the the next use case is for data sharing right so we",
    "start": "2205220",
    "end": "2213860"
  },
  {
    "start": "2207000",
    "end": "2300000"
  },
  {
    "text": "want to be able to share our data with sister teams in the in the Amazon last",
    "start": "2213860",
    "end": "2219320"
  },
  {
    "text": "mile delivery group you know an important important key here is that you",
    "start": "2219320",
    "end": "2224540"
  },
  {
    "text": "know at Amazon particularly assert a lot of other companies as well information",
    "start": "2224540",
    "end": "2229580"
  },
  {
    "text": "security is a top priority right because our customer trust is our top priority so you know one thing we do from the",
    "start": "2229580",
    "end": "2236510"
  },
  {
    "text": "beginning is we want to follow the standard security and encryption best practices and you know only grant the",
    "start": "2236510",
    "end": "2243110"
  },
  {
    "text": "encryption key access and and permissions to that on a need-to-know basis so what we want to do here is we",
    "start": "2243110",
    "end": "2251870"
  },
  {
    "text": "want our new telemetry data to go into s3 so that sister teams can subscribe to",
    "start": "2251870",
    "end": "2257510"
  },
  {
    "text": "that data and get the and s notifications and consume that data in in near real-time so our",
    "start": "2257510",
    "end": "2263910"
  },
  {
    "text": "customers then want to take that data and join it with their own data sets so they can they may have their own",
    "start": "2263910",
    "end": "2269400"
  },
  {
    "text": "back-end services or their own back-end data sets right where they might want to combine the data with the telemetry data from there from the client from the",
    "start": "2269400",
    "end": "2275520"
  },
  {
    "text": "applications and then kind of combine that and analyze it all in one place and so we really kind of want to create",
    "start": "2275520",
    "end": "2281340"
  },
  {
    "text": "these data like right these data lakes of data so that you know other teams can can catalog and find and discover data",
    "start": "2281340",
    "end": "2288690"
  },
  {
    "text": "that's useful for them and interesting to them for their business use case I'm sure there's I think I saw a few",
    "start": "2288690",
    "end": "2294750"
  },
  {
    "text": "other talks on this concept of data catalog in data leaks that you can check out later this week for more on that and",
    "start": "2294750",
    "end": "2302070"
  },
  {
    "start": "2300000",
    "end": "2401000"
  },
  {
    "text": "then the final use case extends a little bit up on the previous one and so what",
    "start": "2302070",
    "end": "2307080"
  },
  {
    "text": "this one is to be able to do deeper analytics and use some of the machine learning type of tools so you know both",
    "start": "2307080",
    "end": "2314910"
  },
  {
    "text": "looking at both the the stream of new data and the historical data we're going",
    "start": "2314910",
    "end": "2320370"
  },
  {
    "text": "to be able to build machine learning models on the data create predictions and detect anomalies we also want to be",
    "start": "2320370",
    "end": "2326370"
  },
  {
    "text": "able to support a variety of frameworks right that for processing and analyzing the data so we want to be able to",
    "start": "2326370",
    "end": "2331590"
  },
  {
    "text": "support Hadoop - be able supports SPARC and storm these type of machine learning frameworks and different frameworks that",
    "start": "2331590",
    "end": "2339090"
  },
  {
    "text": "the machine learning scientists and and the business intelligence engineers that",
    "start": "2339090",
    "end": "2344730"
  },
  {
    "text": "they typically use and that they're familiar with and the tools that they like so they can easily access the data and they can easily analyze it on their",
    "start": "2344730",
    "end": "2351390"
  },
  {
    "text": "own and so all this data again with the security in mind should be anonymized and aggregated at the appropriate level",
    "start": "2351390",
    "end": "2357180"
  },
  {
    "text": "so you need to be able to solve for that you know to meet both the business use case so the data is valuable for analysis but also has the right level of",
    "start": "2357180",
    "end": "2365340"
  },
  {
    "text": "information security and so some of the questions that you can imagine that our business wants to answer includes you",
    "start": "2365340",
    "end": "2372780"
  },
  {
    "text": "know modeling how increased surge prices might affect driver demand predicting",
    "start": "2372780",
    "end": "2378570"
  },
  {
    "text": "how the the supplier of drivers might be needed to work on a given day and doing",
    "start": "2378570",
    "end": "2384810"
  },
  {
    "text": "analysis on a be testing results for different features that were rolling out",
    "start": "2384810",
    "end": "2390080"
  },
  {
    "text": "okay cool so now we're going to kind of trance into from the problem and some of the",
    "start": "2390080",
    "end": "2395700"
  },
  {
    "text": "use cases though that we wanted to solve into actually how we built this for the Amazon flex team so as Alan mentioned",
    "start": "2395700",
    "end": "2403620"
  },
  {
    "start": "2401000",
    "end": "2453000"
  },
  {
    "text": "one of the best practice is the the first thing to mention is that we tried to follow you know an agile mindset we",
    "start": "2403620",
    "end": "2410100"
  },
  {
    "text": "wanted to build things iteratively so we started with an initial batch processing system as a kind of initial version",
    "start": "2410100",
    "end": "2417120"
  },
  {
    "text": "proof-of-concept version and then we evolved it over several iterations to get into a stream based system and so",
    "start": "2417120",
    "end": "2422370"
  },
  {
    "text": "we'll walk through that we'll walk through that next you know this can definitely be tricky when you have a",
    "start": "2422370",
    "end": "2427590"
  },
  {
    "text": "live production system I'm not going to get into the details of how to do that or how to change the you know change the",
    "start": "2427590",
    "end": "2433380"
  },
  {
    "text": "car tire while you're driving so if you're interested in this we've learned some stuff in this area so you know feel",
    "start": "2433380",
    "end": "2438900"
  },
  {
    "text": "free to find me afterwards we can talk a little bit more so if for the rest of the talk you know we'll go over the",
    "start": "2438900",
    "end": "2444330"
  },
  {
    "text": "high-level system design how we implemented this and how we kind of progressed from incrementally from our",
    "start": "2444330",
    "end": "2449670"
  },
  {
    "text": "batch based system into a real-time streaming system okay so the first",
    "start": "2449670",
    "end": "2454830"
  },
  {
    "start": "2453000",
    "end": "2522000"
  },
  {
    "text": "iteration was to use existing systems that existed within Amazon and so this",
    "start": "2454830",
    "end": "2461160"
  },
  {
    "text": "was the fastest way for us to get some data and and to see what's happening in the app right so we instrumented the app",
    "start": "2461160",
    "end": "2467300"
  },
  {
    "text": "in the code for both iOS and Android to collect metrics send those metrics to existing back-end",
    "start": "2467300",
    "end": "2473370"
  },
  {
    "text": "systems I just do a cloud here to give that concept and then you know we built ETL jobs to load the data into this big",
    "start": "2473370",
    "end": "2481350"
  },
  {
    "text": "you know Oracle data warehouse and from there we could query it we could build reports we can a very very like batch",
    "start": "2481350",
    "end": "2487470"
  },
  {
    "text": "based batch based system right and to analyze what's happening and so you know this initial version was fine it was",
    "start": "2487470",
    "end": "2493500"
  },
  {
    "text": "suboptimal but it kind of got the job done initially but there were some problems with it right so some of the",
    "start": "2493500",
    "end": "2499770"
  },
  {
    "text": "main problems where one is is a batch process right and so doing reports had a",
    "start": "2499770",
    "end": "2505470"
  },
  {
    "text": "24 hour delay second it had a very fixed and inflexible database schema couldn't",
    "start": "2505470",
    "end": "2511980"
  },
  {
    "text": "really change that very easily and the third analysis was both slow and difficult because you had to write",
    "start": "2511980",
    "end": "2517320"
  },
  {
    "text": "sequel um and that was really the only way to access the data all right the",
    "start": "2517320",
    "end": "2523320"
  },
  {
    "start": "2522000",
    "end": "2613000"
  },
  {
    "text": "second iteration was to use AWS and so what we decided to do was to use",
    "start": "2523320",
    "end": "2528520"
  },
  {
    "text": "the Amazon mobile analytics or am a service that ADB was had and to do this",
    "start": "2528520",
    "end": "2536170"
  },
  {
    "text": "we just had to use the ADB s SDK to instrument the app to collect events and associated with those events when things",
    "start": "2536170",
    "end": "2542440"
  },
  {
    "text": "happen in the app when you know the driver delivered a package or when they or when they scanned the package or things like that associated with each of",
    "start": "2542440",
    "end": "2549100"
  },
  {
    "text": "those events was different attributes you had context around what happened with that event and and metrics the",
    "start": "2549100",
    "end": "2555040"
  },
  {
    "text": "different actual timing data or account data that kind of stuff",
    "start": "2555040",
    "end": "2560050"
  },
  {
    "text": "and so yeah as I said before like we instrumented when they logged in when they scan a package that kind of stuff",
    "start": "2560050",
    "end": "2565780"
  },
  {
    "text": "and so the data would be sent in through the app 3 Amazon mobile analytics and",
    "start": "2565780",
    "end": "2570970"
  },
  {
    "text": "then part of Amazon mobile analytics was they gave you a cloud formation template that ran and you own a business account",
    "start": "2570970",
    "end": "2577180"
  },
  {
    "text": "and had a Python script associated with it that with load data from from a ma",
    "start": "2577180",
    "end": "2584110"
  },
  {
    "text": "into your redshift cluster and so for those that don't know what cloud formation is real quick it's it's a way",
    "start": "2584110",
    "end": "2590860"
  },
  {
    "text": "to define it it's a template language a way to define your ats resources that gets a point into a divot account and so",
    "start": "2590860",
    "end": "2598470"
  },
  {
    "text": "yeah that set up this this whole confirmation ETL system that loaded data",
    "start": "2598470",
    "end": "2603880"
  },
  {
    "text": "into redshift in the future we used AWS pinpoint which kind of got",
    "start": "2603880",
    "end": "2609490"
  },
  {
    "text": "merged in with AMA and we'll kind of talk about that a little bit later and so now this this ETL system was now run",
    "start": "2609490",
    "end": "2617830"
  },
  {
    "start": "2613000",
    "end": "2627000"
  },
  {
    "text": "hourly so in worst case it was a two-hour delay but it was still very fixed inflexible database schema and",
    "start": "2617830",
    "end": "2623590"
  },
  {
    "text": "analysis was still flow in difficult with sequel so with the third iteration",
    "start": "2623590",
    "end": "2630270"
  },
  {
    "start": "2627000",
    "end": "2788000"
  },
  {
    "text": "we implemented an automated system to update the database schema and redshift",
    "start": "2630270",
    "end": "2635790"
  },
  {
    "text": "this is important because it makes adding new events easier and so what we",
    "start": "2635790",
    "end": "2641320"
  },
  {
    "text": "did is we define the schema configuration file it was in JSON and it did two things one is it automatically",
    "start": "2641320",
    "end": "2647620"
  },
  {
    "text": "updated the schema in redshift so when we added the new event or we added a new column of metrics data that we wanted to",
    "start": "2647620",
    "end": "2653350"
  },
  {
    "text": "collect or how much to do that we wanted to collect there was an automated job that would automatically update the redshift schema and then the second",
    "start": "2653350",
    "end": "2659830"
  },
  {
    "text": "thing it did is we generated both Java code for Android and objective-c code for iOS to generate enumerations of",
    "start": "2659830",
    "end": "2666490"
  },
  {
    "text": "possible values that we could collect in the app so that when your instrumenting your data in the app for collection",
    "start": "2666490",
    "end": "2672970"
  },
  {
    "text": "those values would be sort of defined in code for you so you could just kind of reuse those and they would match what",
    "start": "2672970",
    "end": "2679090"
  },
  {
    "text": "shows up in the in the database and redshift and so if you used AWS glue",
    "start": "2679090",
    "end": "2685030"
  },
  {
    "text": "before I think this is one of the fairly new atps services and this is a little bit different than what they do for",
    "start": "2685030",
    "end": "2690670"
  },
  {
    "text": "keeping their data schemas up to date but what we've done is we've essentially",
    "start": "2690670",
    "end": "2697900"
  },
  {
    "text": "built a configuration driven system wherever you define this config in JSON and it generates the code and",
    "start": "2697900",
    "end": "2703720"
  },
  {
    "text": "automatically updates the schema in redshift we only ever do additive changes of",
    "start": "2703720",
    "end": "2709720"
  },
  {
    "text": "course so this is this you know conforms to the open-closed principle in design",
    "start": "2709720",
    "end": "2714850"
  },
  {
    "text": "right and so we never modify or delete any existing data which which makes",
    "start": "2714850",
    "end": "2719950"
  },
  {
    "text": "sense because if it's telemetry data its historical data and so really it's immutable right so you don't really you",
    "start": "2719950",
    "end": "2725080"
  },
  {
    "text": "don't really expect it to change and so this system would work well with AWS to clue because glue uses what's called a",
    "start": "2725080",
    "end": "2731770"
  },
  {
    "text": "crawler to discover your data and so it's it crawls over your schema and redshift",
    "start": "2731770",
    "end": "2736930"
  },
  {
    "text": "or whatever other data store you're using and it updates its schema in the glue data catalog and so you know once",
    "start": "2736930",
    "end": "2744250"
  },
  {
    "text": "our automated system here for doing the schema config updates once it updates our redshift schema then glues crawler",
    "start": "2744250",
    "end": "2750340"
  },
  {
    "text": "would see that change and update the data catalog um so this would this would play well with that but so a long story",
    "start": "2750340",
    "end": "2756190"
  },
  {
    "text": "short we automated the database schema updates and redshift so that we're matching we're always matching what data",
    "start": "2756190",
    "end": "2761710"
  },
  {
    "text": "is being collected from the app and so you know we still have this to our batch",
    "start": "2761710",
    "end": "2767290"
  },
  {
    "text": "process but now our schema automatically updates so that makes a little bit easier for adding new telemetry data",
    "start": "2767290",
    "end": "2773050"
  },
  {
    "text": "types you just have to as a developer you just have to edit this schema configuration file and now it defines for you the",
    "start": "2773050",
    "end": "2780670"
  },
  {
    "text": "schema that's in redshift automatically and the schema of available its how much data that you want to collect from your app so that makes it simple okay so now",
    "start": "2780670",
    "end": "2788500"
  },
  {
    "start": "2788000",
    "end": "2877000"
  },
  {
    "text": "iteration for the moment you've all been waiting for is to use streams and so we introduced streams here we",
    "start": "2788500",
    "end": "2796850"
  },
  {
    "text": "still use Amazon mobile analytics to collect the data but as I mentioned it's now part of pin point got rolled into",
    "start": "2796850",
    "end": "2802190"
  },
  {
    "text": "that service and pin point supports loading your data into Kinesis streams",
    "start": "2802190",
    "end": "2807740"
  },
  {
    "text": "or Kinesis firehose so instead of the using this cloud formation stack that we had before cloud formation pinpoint just",
    "start": "2807740",
    "end": "2817340"
  },
  {
    "text": "publishes the data into a konista stream and and then from there we can load it into a Kinesis firehose which loads it",
    "start": "2817340",
    "end": "2825859"
  },
  {
    "text": "into redshift and so what we did is we wrote a lambda that filters out the events that don't conform to our schema",
    "start": "2825859",
    "end": "2833359"
  },
  {
    "text": "so that that json schema config that defines the possible values you know it",
    "start": "2833359",
    "end": "2839890"
  },
  {
    "text": "if you don't follow that if one of the developers does not use that and they send an event that's sort of unknown",
    "start": "2839890",
    "end": "2845330"
  },
  {
    "text": "then we filter it out in in the lambda there that's between the konista stream and the Kinesis firehose as sort of an",
    "start": "2845330",
    "end": "2851720"
  },
  {
    "text": "unknown unknown datatype or unknown event right and so and so so that's that's one nice thing to make sure your",
    "start": "2851720",
    "end": "2857840"
  },
  {
    "text": "data is always clean but then you know the data from there gets loaded into Canisius fire hose and fire hose has",
    "start": "2857840",
    "end": "2864950"
  },
  {
    "text": "what are called delivery streams and from there you can configure those delivery streams to publish data into",
    "start": "2864950",
    "end": "2870770"
  },
  {
    "text": "either a redshift an s3 or redshift in elastic search or s3 an elastic search sorry so now you know rather then rather",
    "start": "2870770",
    "end": "2880490"
  },
  {
    "start": "2877000",
    "end": "2947000"
  },
  {
    "text": "than an hourly batch job it comes in through the stream goes to the firehose into s3 and you know it took four",
    "start": "2880490",
    "end": "2888350"
  },
  {
    "text": "iterations but now we got into a a streaming system the other improvement",
    "start": "2888350",
    "end": "2894109"
  },
  {
    "text": "that we made as part of this was we also implemented data retention logic and so",
    "start": "2894109",
    "end": "2899270"
  },
  {
    "text": "what we did is we created in our redshift we paid monthly monthly tables of data and so we could configure easily",
    "start": "2899270",
    "end": "2905750"
  },
  {
    "text": "how many months worth of data we wanted to keep and and drop the tables that",
    "start": "2905750",
    "end": "2911090"
  },
  {
    "text": "were older than X months and all the data would always be kept in s3 forever but then that keeps our our Arend shift",
    "start": "2911090",
    "end": "2917540"
  },
  {
    "text": "costs lower because we can control control exactly how much data we want to keep in there so now we have a streaming",
    "start": "2917540",
    "end": "2924260"
  },
  {
    "text": "process with a delay a couple minutes to get through from the app three MA through the Kinesis stream",
    "start": "2924260",
    "end": "2929750"
  },
  {
    "text": "through the fire hose and into redshift and s3 and yeah so we did it so we're",
    "start": "2929750",
    "end": "2936410"
  },
  {
    "text": "not done yet right we still have this this data that only goes to redshift and",
    "start": "2936410",
    "end": "2943220"
  },
  {
    "text": "s3 and it's kind of difficult to analyze right with sequel and so here we go in",
    "start": "2943220",
    "end": "2948260"
  },
  {
    "start": "2947000",
    "end": "2989000"
  },
  {
    "text": "the final iteration we in our current one we introduced generic message types and so what what we wanted to do here is",
    "start": "2948260",
    "end": "2956420"
  },
  {
    "text": "we wanted to publish data according to you may have seen this pattern recording",
    "start": "2956420",
    "end": "2961640"
  },
  {
    "text": "data temperature right so you have like warm cold and hot data and so all the cold data for all time goes to s3",
    "start": "2961640",
    "end": "2968440"
  },
  {
    "text": "redshift keeps the warm data which may be the last few months and then elasticsearch just keeps the hot data",
    "start": "2968440",
    "end": "2973970"
  },
  {
    "text": "which is maybe the last month or the last couple weeks worth of data and so what a high level it looks like this the",
    "start": "2973970",
    "end": "2979070"
  },
  {
    "text": "data comes in we collect it we ingest it from the app and the King uses firehose you know gets it transforms it and then",
    "start": "2979070",
    "end": "2985790"
  },
  {
    "text": "it it loads it into the various data stores and so let's go into little bit",
    "start": "2985790",
    "end": "2991160"
  },
  {
    "text": "details of how we have how we did this now we simplified this and optimize the system so first you probably noticed on",
    "start": "2991160",
    "end": "2996470"
  },
  {
    "text": "the previous slide that there were two lambda functions there was one that that filtered out events before it went into",
    "start": "2996470",
    "end": "3002530"
  },
  {
    "text": "off of the off of the konista stream then there was the the transform lambda and so we combine those into one to",
    "start": "3002530",
    "end": "3010210"
  },
  {
    "text": "simplify that and so that that allowed us to get rid of the Kinesis stream",
    "start": "3010210",
    "end": "3016330"
  },
  {
    "text": "entirely right so we we utilize the Kinesis firehoses transformation lambda here and and cut out the previous lambda",
    "start": "3016330",
    "end": "3023740"
  },
  {
    "text": "with the previous can use this stream we also cut out a May right so at this point there's there was really no we",
    "start": "3023740",
    "end": "3029710"
  },
  {
    "text": "didn't need to use that and so what we did is we went directly from the app straight to the Kinesis firehose and so",
    "start": "3029710",
    "end": "3038080"
  },
  {
    "text": "so now we control that code we control that code rather than using the Avs SDK to send data through AMA we can now use",
    "start": "3038080",
    "end": "3045790"
  },
  {
    "text": "the ABS SDK to send data to Kinesis firehose and so we can control the schema of data and so what we were able",
    "start": "3045790",
    "end": "3053290"
  },
  {
    "text": "to do here is introduce generic message types and so we can define those message",
    "start": "3053290",
    "end": "3059410"
  },
  {
    "text": "type for the various types of data that we want to collect how much EJB I want to",
    "start": "3059410",
    "end": "3064790"
  },
  {
    "text": "collect and so what we did is we used protocol buffers to do that and so we have these protocol buffers to define",
    "start": "3064790",
    "end": "3070220"
  },
  {
    "text": "the structure of the data and if you haven't use protocol buffers before is think of it as you know an interface",
    "start": "3070220",
    "end": "3076430"
  },
  {
    "text": "description language kind of like an optimized version of XML and so what it does is it generates source code in",
    "start": "3076430",
    "end": "3082520"
  },
  {
    "text": "various languages so in Java and objective-c for us and and then we use",
    "start": "3082520",
    "end": "3088160"
  },
  {
    "text": "it in multiple places so showed up out here right so we use it in the app from the generated code we use it in the",
    "start": "3088160",
    "end": "3094880"
  },
  {
    "text": "transformation lambda when processing the data we use those protocol buffers",
    "start": "3094880",
    "end": "3100070"
  },
  {
    "text": "for updating the redshifts schema when you don't read data types show up and then any consumers that want to consume",
    "start": "3100070",
    "end": "3106130"
  },
  {
    "text": "the data can use those protocol buffers to deserialize and consume out the data right and so now this system allows us",
    "start": "3106130",
    "end": "3113870"
  },
  {
    "text": "to collect process and store any arbitrary type of telemetry data including metrics logs crashes",
    "start": "3113870",
    "end": "3120590"
  },
  {
    "text": "sensor data clickstream data you know anything like that anything that we talked about before with telemetry data and now we're no longer tied to just",
    "start": "3120590",
    "end": "3127580"
  },
  {
    "text": "redshift either you know we configured our Kinesis firehose delivery stream to",
    "start": "3127580",
    "end": "3133280"
  },
  {
    "text": "publish to redshift and s3 here and then we also we also had our transformation",
    "start": "3133280",
    "end": "3140450"
  },
  {
    "text": "lambda published data into a second Kinesis firehose to load it into lastik search there's also the we also you know",
    "start": "3140450",
    "end": "3150680"
  },
  {
    "text": "once the data is in s3 there's also the ability to subscribe to that data through SNS topics right so other teams",
    "start": "3150680",
    "end": "3158030"
  },
  {
    "text": "can write their own consumer lambdas and subscribe to that data and process it in real time or they can extract it out of",
    "start": "3158030",
    "end": "3163760"
  },
  {
    "text": "s3 load it into their own redshifts join it with their data and so this kind of opens up multiple ways to to consume the",
    "start": "3163760",
    "end": "3170060"
  },
  {
    "text": "data once it's an elastic search it's easy to do dashboards you can use something like Kabana or other",
    "start": "3170060",
    "end": "3175280"
  },
  {
    "text": "visualization tools to visualize that data in in real time okay so now to",
    "start": "3175280",
    "end": "3181160"
  },
  {
    "start": "3179000",
    "end": "3213000"
  },
  {
    "text": "summarize for this final iteration right we have now a streaming process with",
    "start": "3181160",
    "end": "3186170"
  },
  {
    "text": "just a few seconds delay from the app into the data stores in the backend we have an auto updating database schema",
    "start": "3186170",
    "end": "3192200"
  },
  {
    "text": "and we generic message types that we can support any types of data that we want very easily and now analysis we can",
    "start": "3192200",
    "end": "3199720"
  },
  {
    "text": "still do sequel but it's it's now a little bit more flexible because now we can process these dimensions payloads and you can kind of process them as you",
    "start": "3199720",
    "end": "3206560"
  },
  {
    "text": "like as you want to consume them if you want to write custom code in your own lambda you can so that that's an option",
    "start": "3206560",
    "end": "3211869"
  },
  {
    "text": "okay so now just to get into get a sense I know there's a little bit hard to follow with these diagrams I'm going to",
    "start": "3211869",
    "end": "3218020"
  },
  {
    "start": "3213000",
    "end": "3411000"
  },
  {
    "text": "walk through the data flow really quickly to give you an idea of how data flows through the system so so first you",
    "start": "3218020",
    "end": "3224800"
  },
  {
    "text": "know let's start in the app the app is collected we use this protocol buffer to generate source code and then you",
    "start": "3224800",
    "end": "3230680"
  },
  {
    "text": "instrument the app for Android and iOS to send the telemetry data that you want to send the data is sterilized by the",
    "start": "3230680",
    "end": "3237070"
  },
  {
    "text": "protocol buffer and loaded into a Kinesis firehose sent to Kinesis firehose we batch and we kind of batch the data",
    "start": "3237070",
    "end": "3243849"
  },
  {
    "text": "into five kilobytes chunks to kind of take advantage of the five kilobyte",
    "start": "3243849",
    "end": "3248890"
  },
  {
    "text": "pricing that Kinesis firehose has and kind of optimize our our costs their",
    "start": "3248890",
    "end": "3255150"
  },
  {
    "text": "firehose buffers up the data in five megabytes unk's so receipt we receive five megabytes pretty quickly and it's",
    "start": "3255150",
    "end": "3263380"
  },
  {
    "text": "typically under a second and so now the data is there and Canisius firehose then the transformation lambda gets called",
    "start": "3263380",
    "end": "3269230"
  },
  {
    "text": "and it converts those five megabytes own files into json and puts them into s3",
    "start": "3269230",
    "end": "3275040"
  },
  {
    "text": "and so now they're in s3 then then firehose issues a redshift",
    "start": "3275040",
    "end": "3280060"
  },
  {
    "text": "copy command to load the data in from that transform file into our redshift",
    "start": "3280060",
    "end": "3286000"
  },
  {
    "text": "instance and then the data transformation lambda also does one thing it copies the data into a second",
    "start": "3286000",
    "end": "3291490"
  },
  {
    "text": "Kinesis firehose which is configured to publish the data into elasticsearch so now we're at this point where some entry",
    "start": "3291490",
    "end": "3297910"
  },
  {
    "text": "data is coming in is getting replicated within seconds to the three different data sources s3 redshift and",
    "start": "3297910",
    "end": "3303960"
  },
  {
    "text": "elasticsearch and now from here we have multiple consumers of the data we have",
    "start": "3303960",
    "end": "3309369"
  },
  {
    "text": "the first we have the the data joining news case in the data Lake use case right where other teams want to join the",
    "start": "3309369",
    "end": "3316420"
  },
  {
    "text": "telemetry client-side application data with their back-end service data and so they can do that they can take the data",
    "start": "3316420",
    "end": "3322630"
  },
  {
    "text": "out of s3 and load it combine it with their data loading to redshifts and do joins there we also",
    "start": "3322630",
    "end": "3330050"
  },
  {
    "text": "have other teams who want to consume the data in in near real time so they can subscribe to the SNS events when stuff",
    "start": "3330050",
    "end": "3336319"
  },
  {
    "text": "gets loaded into s3 and they can write their own consumer lambdas to use the protocol buffers to serialize the data and now they just have the data they can",
    "start": "3336319",
    "end": "3342560"
  },
  {
    "text": "write code process that data we still have redshift so we can still write sequel and generate reports that way and",
    "start": "3342560",
    "end": "3349819"
  },
  {
    "text": "now since it's an elastic search we can use Cabana and other dashboards right and view things in real time so so",
    "start": "3349819",
    "end": "3357470"
  },
  {
    "text": "that's that's kind of how data flows through the system hopefully that who that's clear enough and easy enough to follow one common",
    "start": "3357470",
    "end": "3363740"
  },
  {
    "text": "question that we get is you know why use Kinesis firehose as opposed to a Kinesis stream right so the biggest benefit I",
    "start": "3363740",
    "end": "3371329"
  },
  {
    "text": "think Ellen mentioned it as well earlier is that Kinesis firehose is a fully fully managed this is sort of one level",
    "start": "3371329",
    "end": "3377150"
  },
  {
    "text": "higher up than a-- than just a Kinesis stream and so this data transformation",
    "start": "3377150",
    "end": "3383960"
  },
  {
    "text": "Landa gives you flexibility right we had sort of that duplicated to two lambdas that were doing similar things and we",
    "start": "3383960",
    "end": "3390619"
  },
  {
    "text": "kind of combined that all in one place since you have this convenient transformation lambda in the firehose that lets you process and transform your",
    "start": "3390619",
    "end": "3396920"
  },
  {
    "text": "data so that it's easy to load into different data source you can certainly write that code in your own lambda",
    "start": "3396920",
    "end": "3403190"
  },
  {
    "text": "function but firehose handles that for you and just makes that that kind of data binding between things a lot easier",
    "start": "3403190",
    "end": "3410560"
  },
  {
    "text": "so yeah future improvements you know some ideas that we've talked about that",
    "start": "3410560",
    "end": "3416390"
  },
  {
    "start": "3411000",
    "end": "3471000"
  },
  {
    "text": "we can do in the future to make things better Allen mentioned Kinesis analytics we're not using that yet we can we want",
    "start": "3416390",
    "end": "3422030"
  },
  {
    "text": "to use that next that will allow us to you know query and extract some of the data in real time the other idea is to",
    "start": "3422030",
    "end": "3429890"
  },
  {
    "text": "use something like a TBS Athena or redshift spectrum to query data that's an s3 oops and then finally you know we",
    "start": "3429890",
    "end": "3438829"
  },
  {
    "text": "can use something like that some of the AWS AI services to do to do you know",
    "start": "3438829",
    "end": "3444020"
  },
  {
    "text": "deeper analysis so for example you could use some of the frameworks like tensor flow or NX net or spark or Amazon EMR",
    "start": "3444020",
    "end": "3450200"
  },
  {
    "text": "and process the data and try to extract more more use more deeper analysis out of it and more deeper data",
    "start": "3450200",
    "end": "3456710"
  },
  {
    "text": "analytics but you know really the nice thing is since we used AWS for all this we have a system that's flexible you",
    "start": "3456710",
    "end": "3463250"
  },
  {
    "text": "know it enables us to use any number of other frameworks that we can plug into the system and and we can use to process",
    "start": "3463250",
    "end": "3469220"
  },
  {
    "text": "our data okay so to summarize did we solve all our use cases yes we did so",
    "start": "3469220",
    "end": "3476660"
  },
  {
    "start": "3471000",
    "end": "3600000"
  },
  {
    "text": "you know first we have real-time metrics and alarming x' alarming we have dashboards we have the",
    "start": "3476660",
    "end": "3482990"
  },
  {
    "text": "real-time logs and and crash data that we can that we can use to troubleshoot issues when they happen you know we can",
    "start": "3482990",
    "end": "3490400"
  },
  {
    "text": "monitor new releases with our app dashboards and you know now we publish",
    "start": "3490400",
    "end": "3495619"
  },
  {
    "text": "to date at s3 so that we can share data with our teams and sister teams and enable deeper analytics use cases and so",
    "start": "3495619",
    "end": "3502400"
  },
  {
    "text": "you know to kind of wrap up and give you a sense of the business impact that's possible that we had with the Amazon",
    "start": "3502400",
    "end": "3507650"
  },
  {
    "text": "Amazon flex delivery business you know one of the pieces of telemetry data that we measured is the negative impact of",
    "start": "3507650",
    "end": "3515570"
  },
  {
    "text": "that it made on delivery partners when the app crashes while making a delivery and so when the app crashes while making",
    "start": "3515570",
    "end": "3522080"
  },
  {
    "text": "delivery what we did is we record a timestamp when the crash happens and then we calculate the difference between",
    "start": "3522080",
    "end": "3527240"
  },
  {
    "text": "when that crash happened and when the user got back to the screen that they were on before the crash so we're",
    "start": "3527240",
    "end": "3533240"
  },
  {
    "text": "basically measuring the time lost due to crashes right and so over the past three months we looked at the metrics we",
    "start": "3533240",
    "end": "3539330"
  },
  {
    "text": "looked at the real-time logs we looked at the crash data and we're able to reduce the number of app crashes and to",
    "start": "3539330",
    "end": "3546109"
  },
  {
    "text": "the point where we were able to save about half a second per delivery right so theoretically for a million",
    "start": "3546109",
    "end": "3551960"
  },
  {
    "text": "deliveries that would be 500,000 seconds which is roughly 160 hours which is roughly four 40-hour work weeks which is",
    "start": "3551960",
    "end": "3559339"
  },
  {
    "text": "roughly a month a month work months of work of time right that we're saving in",
    "start": "3559339",
    "end": "3564800"
  },
  {
    "text": "frustration for our for our drivers that's a that's a pretty significant thing over the course of a year right if you add all that up were speeding up",
    "start": "3564800",
    "end": "3570950"
  },
  {
    "text": "these deliveries and we're speeding up to the B time for our customers that is measured in months or years right which",
    "start": "3570950",
    "end": "3579140"
  },
  {
    "text": "is a significant amount of time savings and making things better for our customers right and so without doing this without collecting all this",
    "start": "3579140",
    "end": "3584869"
  },
  {
    "text": "telemetry data you know measuring these improvements we not really been possible right okay so",
    "start": "3584869",
    "end": "3591200"
  },
  {
    "text": "what are your takeaways from this talk just to wrap it up so first is agility",
    "start": "3591200",
    "end": "3597230"
  },
  {
    "text": "right moving to real-time data use your business can react quicker increase flexibility means that you know you can",
    "start": "3597230",
    "end": "3603829"
  },
  {
    "text": "support and having generic message types that you can put whatever data you want in your streams allows you to handle",
    "start": "3603829",
    "end": "3609500"
  },
  {
    "text": "future use cases easier streams allow you to multiplex and share your data so you have increased share ability with",
    "start": "3609500",
    "end": "3615140"
  },
  {
    "text": "other teams right you can put your data s3 and people can consume it through SNS and then finally extensibility you know",
    "start": "3615140",
    "end": "3621200"
  },
  {
    "text": "processing streams allows you to write to multiple systems and enables other other tools and other frameworks to be",
    "start": "3621200",
    "end": "3627170"
  },
  {
    "text": "able to to work on top of that data right and so really you know by moving from batch sequel based streaming sequel",
    "start": "3627170",
    "end": "3634309"
  },
  {
    "text": "based processing to real-time streams you know we can extract more business value from our data and we can improve",
    "start": "3634309",
    "end": "3641000"
  },
  {
    "text": "our customer experience for our customers so that's it thank you",
    "start": "3641000",
    "end": "3646930"
  },
  {
    "text": "[Applause]",
    "start": "3646930",
    "end": "3650819"
  }
]