[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "okay guys we're gonna go ahead and get started welcome to arc 401 serverless",
    "start": "60",
    "end": "5640"
  },
  {
    "text": "architectural patterns and best practices thank you my name is drew",
    "start": "5640",
    "end": "13469"
  },
  {
    "text": "Dennis and with me today you guys are lucky my trio Ranganath is gonna be here to help me out",
    "start": "13469",
    "end": "19170"
  },
  {
    "text": "we're both AWS solution architects based in Dallas Texas our agenda today is to",
    "start": "19170",
    "end": "27449"
  },
  {
    "text": "focus on four primary macro serverless patterns we'll start off with a web",
    "start": "27449",
    "end": "32610"
  },
  {
    "text": "application pattern which is also applicable to mobile backends micro-services or api deployments",
    "start": "32610",
    "end": "39120"
  },
  {
    "text": "then I'll talk about a data leg pattern which is new for this year talk about some specific sub patterns related to",
    "start": "39120",
    "end": "45809"
  },
  {
    "text": "cataloging and analytical processing of your data in the data Lake and then my",
    "start": "45809",
    "end": "51930"
  },
  {
    "text": "trio will come up and talk about stream processing and operations automation but",
    "start": "51930",
    "end": "57210"
  },
  {
    "text": "before we get into those items I want to cover some foundational concepts related",
    "start": "57210",
    "end": "62309"
  },
  {
    "text": "to service applications that are applicable to all of the patterns we'll be discussing today all of these",
    "start": "62309",
    "end": "68159"
  },
  {
    "text": "patterns are well adopted and well used by AWS customers today so there's no smoke and mirrors at all going on these",
    "start": "68159",
    "end": "74850"
  },
  {
    "text": "are all referenceable and we'll talk about some references as we go so let's",
    "start": "74850",
    "end": "80909"
  },
  {
    "start": "80000",
    "end": "80000"
  },
  {
    "text": "start off with a little bit about what a service application is as you all know on AWS there are many choices or",
    "start": "80909",
    "end": "88619"
  },
  {
    "text": "platforms you can choose from to deploy your applications certainly you can ploy deploy them in virtual machines on ec2",
    "start": "88619",
    "end": "94890"
  },
  {
    "text": "you can deploy them inside of docker containers on ec2 with the service like",
    "start": "94890",
    "end": "99930"
  },
  {
    "text": "ECS and then there's this class of services that's kind of in the middle called manage services and these are",
    "start": "99930",
    "end": "107250"
  },
  {
    "text": "services where you may not really be responsible for servers but servers still exist servers are still important",
    "start": "107250",
    "end": "114720"
  },
  {
    "text": "when you consume that service because you need to be able to right-size that service potentially scale that service",
    "start": "114720",
    "end": "121560"
  },
  {
    "text": "and you do that by defining the number of servers you're going to be using and then on the right you see the class of",
    "start": "121560",
    "end": "128580"
  },
  {
    "text": "services that we're going to be focusing on today which are serverless services where servers are",
    "start": "128580",
    "end": "133990"
  },
  {
    "text": "present at all in the consumption of those services so you're reducing your operational overhead because of that so",
    "start": "133990",
    "end": "141070"
  },
  {
    "text": "no operating systems to manage so there are four common tenants that anyone from",
    "start": "141070",
    "end": "146110"
  },
  {
    "text": "AWS will adhere to when they talk about serverless applications again no servers to provision or manage but also you",
    "start": "146110",
    "end": "153040"
  },
  {
    "text": "never pay for idle with serverless applications so if your application is cyclical in nature maybe you do a lot of",
    "start": "153040",
    "end": "160120"
  },
  {
    "text": "in two month processing you don't pay for those times when it's not in use also service applications have built-in",
    "start": "160120",
    "end": "167560"
  },
  {
    "text": "high availability and disaster recovery all of the service services we'll be talking about today sit at a regional",
    "start": "167560",
    "end": "174370"
  },
  {
    "text": "level within AWS so they automatically span all availability zones to provide",
    "start": "174370",
    "end": "180010"
  },
  {
    "text": "you that built-in high availability and disaster recovery these are really really big things you had to wrestle",
    "start": "180010",
    "end": "185800"
  },
  {
    "text": "with prior to service applications and then perhaps the most important aspect of a service application is it scales",
    "start": "185800",
    "end": "193210"
  },
  {
    "text": "with usage so as user requests that are coming in grow or maybe as data is",
    "start": "193210",
    "end": "198820"
  },
  {
    "text": "entering your application and that amount of data grows the service application will scale as well and we",
    "start": "198820",
    "end": "205240"
  },
  {
    "text": "really shift from having servers be the core or the focus and the unit of scale",
    "start": "205240",
    "end": "210550"
  },
  {
    "text": "to now lambda functions being the core and the unit of scale for these applications so because of that it's",
    "start": "210550",
    "end": "217000"
  },
  {
    "start": "217000",
    "end": "217000"
  },
  {
    "text": "really important to speak a little bit about how lambda functions behave when",
    "start": "217000",
    "end": "222100"
  },
  {
    "text": "you first invoke a lambda function it goes through a process that we'll call a cold start process that I'm sure many of",
    "start": "222100",
    "end": "228940"
  },
  {
    "text": "you have heard of before this involves downloading code to lambda starting it",
    "start": "228940",
    "end": "234580"
  },
  {
    "text": "up in a container on a lambda host and then running any initialization code that you've defined inside of your",
    "start": "234580",
    "end": "241480"
  },
  {
    "text": "lambda function so you can see in this example there's I'm importing some external libraries I'm actually making a",
    "start": "241480",
    "end": "248440"
  },
  {
    "text": "connection to an RDS database in a relational database in this case RDS and",
    "start": "248440",
    "end": "253510"
  },
  {
    "text": "I'm doing all of that outside of the handler so that happens only during the",
    "start": "253510",
    "end": "259359"
  },
  {
    "text": "cold start process and then lambda will keep the container with within which all",
    "start": "259359",
    "end": "264400"
  },
  {
    "text": "that code runs available for a period of time so that as subsequent invitations come into that lambda",
    "start": "264400",
    "end": "271310"
  },
  {
    "text": "function it will be warm and all of that stuff will have already executed so you don't have to go through that again",
    "start": "271310",
    "end": "277130"
  },
  {
    "text": "so it's really important to take advantage of container reuse as much as you can in your service applications it",
    "start": "277130",
    "end": "283670"
  },
  {
    "text": "makes them more performant each subsequent invocation as long as it is warm will just happen at the handler",
    "start": "283670",
    "end": "290000"
  },
  {
    "text": "that you can see highlighted here in blue if cold start times are an issue for",
    "start": "290000",
    "end": "295250"
  },
  {
    "text": "your application you can always keep them warm by scheduling them with cloud watch events that's a very common",
    "start": "295250",
    "end": "301070"
  },
  {
    "text": "practice and also keep in mind you can attach elastic network interfaces Orion eyes to your lambda functions to allow",
    "start": "301070",
    "end": "307970"
  },
  {
    "text": "them to communicate with private resources inside of your V PC and this is a very common practice as well but",
    "start": "307970",
    "end": "314240"
  },
  {
    "text": "should only be used if it's needed to communicate with those private VPC resources as it will actually add to the",
    "start": "314240",
    "end": "321470"
  },
  {
    "text": "cold start times a few other lambda best practices you definitely want to",
    "start": "321470",
    "end": "327500"
  },
  {
    "text": "minimize your package sizes to only include necessities some AWS SDKs like",
    "start": "327500",
    "end": "332900"
  },
  {
    "text": "Java and.net are kind of modular so you can just bring over the components of that that you need and the same is true",
    "start": "332900",
    "end": "338450"
  },
  {
    "text": "with third-party libraries were applicable it's always the best practice to separate the lambda handler that we",
    "start": "338450",
    "end": "344900"
  },
  {
    "text": "just talked about from your core logic of your function typically a lambda handler is going to process an incoming",
    "start": "344900",
    "end": "351170"
  },
  {
    "text": "event and it's your code is going to be separate from that so put it in an outside function from the handler so",
    "start": "351170",
    "end": "357530"
  },
  {
    "text": "that you can get better instrumentation and unit testing of your code leverage environment variables as much as",
    "start": "357530",
    "end": "363830"
  },
  {
    "text": "possible last time we were up here this had not been released yet or at least we weren't available to talk about it at",
    "start": "363830",
    "end": "368900"
  },
  {
    "text": "that point but environment variables is a fantastic way to change the configuration of your lambda functions",
    "start": "368900",
    "end": "374870"
  },
  {
    "text": "and change the way your code behaves without directly changing the code inside the function it's always",
    "start": "374870",
    "end": "381140"
  },
  {
    "text": "recommended to self-contained dependencies you know we include some libraries with lambda with the service",
    "start": "381140",
    "end": "386180"
  },
  {
    "text": "itself like AWS SDKs and sometimes you might have a version mismatch so for",
    "start": "386180",
    "end": "391190"
  },
  {
    "text": "that reason we always recommend self containing your dependencies in your package as much as possible keep in mind",
    "start": "391190",
    "end": "397370"
  },
  {
    "text": "that lambda has a single knob for sizing the Reese sources that are available to that lambda function it's a memory knob but",
    "start": "397370",
    "end": "404389"
  },
  {
    "text": "by dialing that up or down you're also affecting the amount of CPU that's associated to that lambda function so by",
    "start": "404389",
    "end": "411110"
  },
  {
    "text": "dialing it up you can really improve your cold start times as an example and the performance of your lambda function",
    "start": "411110",
    "end": "416300"
  },
  {
    "text": "and remember there's a max memory use statistic that's reported in cloud watch logs for every invocation so you can",
    "start": "416300",
    "end": "423379"
  },
  {
    "text": "leverage that to make sure your lambda functions are right sized another way to",
    "start": "423379",
    "end": "429559"
  },
  {
    "start": "428000",
    "end": "428000"
  },
  {
    "text": "get great insight into your lambda functions is to leverage AWS x-ray it's very easy to implement there's some",
    "start": "429559",
    "end": "435710"
  },
  {
    "text": "tight integration with lambda there's just a checkbox with lambda configurations to enable active tracing",
    "start": "435710",
    "end": "441439"
  },
  {
    "text": "and this will immediately be able to give you insight into how your lambda functions in the lambda service itself",
    "start": "441439",
    "end": "446599"
  },
  {
    "text": "is performing by default x-ray will sample one invocation per second and",
    "start": "446599",
    "end": "452809"
  },
  {
    "text": "then 5% thereafter and x-ray will also provide kind of what I call an airport",
    "start": "452809",
    "end": "459020"
  },
  {
    "text": "delay map if any of you have ever seen the u.s. map with airports spread across",
    "start": "459020",
    "end": "464959"
  },
  {
    "text": "it and it will show you kind of in a graph of the amount of delays the amount of cancellations that each Airport is",
    "start": "464959",
    "end": "471020"
  },
  {
    "text": "experiencing you get kind of that same kind of map with x-ray for service and interactions so as upstream and",
    "start": "471020",
    "end": "478610"
  },
  {
    "text": "downstream services with AWS x-ray upstream and downstream of your lambda function you can get a nice map to kind",
    "start": "478610",
    "end": "484430"
  },
  {
    "text": "of see average times and execution latencies there's also a way to kind of",
    "start": "484430",
    "end": "489800"
  },
  {
    "text": "customize the sampling rate and enable this downstream communication in your x-ray traces so I've included a quick",
    "start": "489800",
    "end": "496550"
  },
  {
    "text": "code sample to show you how to do that I realize this may be hard to see from up at the top but there the second line",
    "start": "496550",
    "end": "502789"
  },
  {
    "text": "here in this code sample is actually including a sample JSON configuration file to configure a custom sampling",
    "start": "502789",
    "end": "509569"
  },
  {
    "text": "interval control for your x-ray traces and then the third line you'll see we actually wrap the require statement for",
    "start": "509569",
    "end": "516709"
  },
  {
    "text": "your AWS SDK with x-ray so that as you instantiate AWS clients like in this",
    "start": "516709",
    "end": "523219"
  },
  {
    "text": "case s 3 it will be instrumented and downstream calls will be instrumented with x-ray here's a quick example of",
    "start": "523219",
    "end": "531649"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "what's produced an x-ray for a lambda function you the top half of this graph is actually pertinent to the lambda",
    "start": "531649",
    "end": "538610"
  },
  {
    "text": "service itself and you can see dwell time as an example or how long a function waited for the lambda service",
    "start": "538610",
    "end": "545720"
  },
  {
    "text": "to begin executing and then you can see the entire duration of your lambda function executing from start to finish",
    "start": "545720",
    "end": "551690"
  },
  {
    "text": "with respect to the service below that is specific to your function so here you",
    "start": "551690",
    "end": "557990"
  },
  {
    "text": "start with the initialization phase the code that we talked about outside of the handler and how long that executes and",
    "start": "557990",
    "end": "563990"
  },
  {
    "text": "then the the handler and the rest any downstream calls in this case s3 that you see and you see a bit of a gap from",
    "start": "563990",
    "end": "571160"
  },
  {
    "text": "the initial from when the lambda service started invoking your function to when the initialization code starts executing",
    "start": "571160",
    "end": "578630"
  },
  {
    "text": "in your function and that can be attributed to those cold start issues that we talked about before in this example I also want to talk a little bit",
    "start": "578630",
    "end": "587990"
  },
  {
    "start": "586000",
    "end": "586000"
  },
  {
    "text": "foundationally about deployments and modeling your service applications you",
    "start": "587990",
    "end": "594079"
  },
  {
    "text": "know it used to be before service applications got really really popular that you could kind of leverage",
    "start": "594079",
    "end": "599329"
  },
  {
    "text": "CloudFormation to define lambda and API gateway methods but as serverless applications became more complex you",
    "start": "599329",
    "end": "606050"
  },
  {
    "text": "know with hundreds or potentially thousands of lambda functions then it we recognized a need to provide a more",
    "start": "606050",
    "end": "612290"
  },
  {
    "text": "effective way with less verbosity to model your applications and that's why we came out with Sam Sam is built on top",
    "start": "612290",
    "end": "620149"
  },
  {
    "text": "of CloudFormation which is really good news because any third-party tool that supports cloud formation will inherently",
    "start": "620149",
    "end": "626420"
  },
  {
    "text": "work with Sam Sam also supports a lot of the great cloud formation features like parameters and intrinsic functions and",
    "start": "626420",
    "end": "633589"
  },
  {
    "text": "you can combine server lists or Sam resources and notations",
    "start": "633589",
    "end": "639079"
  },
  {
    "text": "with traditional cloud formation resources and notations so very flexible Sam also includes some ways to deploy",
    "start": "639079",
    "end": "646100"
  },
  {
    "text": "your application it will provide a unique artifact for deployment load that up into s3 and then it will leverage",
    "start": "646100",
    "end": "652670"
  },
  {
    "text": "cloud formation to take that and deploy that into the cloud formation service as a change set you also may have heard",
    "start": "652670",
    "end": "660740"
  },
  {
    "text": "this summer we announced the public beta of Sam local and we're really excited about this because we received a lot of",
    "start": "660740",
    "end": "666709"
  },
  {
    "text": "your feedback about providing an effective way to do local testing of your lambda functions before",
    "start": "666709",
    "end": "672980"
  },
  {
    "text": "you deploy them into the service and that's exactly the use case that Sam local aims to address so you can develop",
    "start": "672980",
    "end": "679190"
  },
  {
    "text": "and test mock events test your lambda functions and we even give you a local",
    "start": "679190",
    "end": "684950"
  },
  {
    "text": "copy of API gateway that you can spin up it supports hot reloading so you can change your code kind of on the fly and",
    "start": "684950",
    "end": "691220"
  },
  {
    "text": "test them and once your unit tests are kind of complete locally Sam local has a CLI that actually includes all of the",
    "start": "691220",
    "end": "697580"
  },
  {
    "text": "Sam commands to package and deploy your application to the service and of course",
    "start": "697580",
    "end": "705110"
  },
  {
    "start": "704000",
    "end": "704000"
  },
  {
    "text": "probably the best way within a ws to deploy CloudFormation templates in a custom pipeline is code pipeline this is",
    "start": "705110",
    "end": "711620"
  },
  {
    "text": "our continuous integration service so it has built-in support now for Sam templates and code CloudFormation",
    "start": "711620",
    "end": "719750"
  },
  {
    "text": "templates you can also integrate additional stages and custom stages in your pipeline for example code build",
    "start": "719750",
    "end": "726350"
  },
  {
    "text": "stages to do building and potentially testing stages as well and you can leverage code pipeline to deploy to",
    "start": "726350",
    "end": "732320"
  },
  {
    "text": "multiple environments so if for example you want to do some testing in a in a development environment and then if",
    "start": "732320",
    "end": "737630"
  },
  {
    "text": "tests pass automatically deploying to a staging environment and then potentially have a manual approval if tests passed",
    "start": "737630",
    "end": "744410"
  },
  {
    "text": "there before it goes in production all of those types of scenarios are supported with code pipeline and also I",
    "start": "744410",
    "end": "752120"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "want to mention a new capability within lambda that provides the ability to have",
    "start": "752120",
    "end": "757460"
  },
  {
    "text": "a lambda alias mapped to multiple lambda function versions and this is really",
    "start": "757460",
    "end": "763340"
  },
  {
    "text": "important for canary deployments that you can do now with lambda so now you can define in your Sam templates",
    "start": "763340",
    "end": "769820"
  },
  {
    "text": "multiple function versions and specify an amount of traffic and an interval at which traffic will move over from one",
    "start": "769820",
    "end": "777080"
  },
  {
    "text": "version to another for your canary based deployments and this is fully supported by a code deploy code deploy will",
    "start": "777080",
    "end": "783440"
  },
  {
    "text": "actually monitor cloud watch alerts for failures and your newer lambda function and the failures occur it will also",
    "start": "783440",
    "end": "789860"
  },
  {
    "text": "initiate a rollback for you so we're really excited to have this available to your deployments as well ok let's go",
    "start": "789860",
    "end": "798650"
  },
  {
    "text": "ahead and start talking about our first pattern which is the web application pattern this is the",
    "start": "798650",
    "end": "803840"
  },
  {
    "start": "803000",
    "end": "803000"
  },
  {
    "text": "common pattern where you have static content above the top in this diagram maybe some resources in s3 that's",
    "start": "803840",
    "end": "810850"
  },
  {
    "text": "provided and delivered through our content delivery network cloud front at",
    "start": "810850",
    "end": "816950"
  },
  {
    "text": "the bottom you might have your more dynamic calls and content going through API gateway API is backed by lambda",
    "start": "816950",
    "end": "823940"
  },
  {
    "text": "functions potentially although they could be backed by something else and then have lambda itself call other",
    "start": "823940",
    "end": "829730"
  },
  {
    "text": "downstream services like in this case DynamoDB and it's really important not to leave out Cognito in this pattern",
    "start": "829730",
    "end": "835880"
  },
  {
    "text": "because kognito is what can provide signup and sign in to your applications",
    "start": "835880",
    "end": "841910"
  },
  {
    "text": "as well as identity federation across many internal as well as web identity providers so this pattern is well used",
    "start": "841910",
    "end": "849830"
  },
  {
    "text": "and well adopted across a lot of different web sites today one that I want to specifically call out as bustled",
    "start": "849830",
    "end": "856430"
  },
  {
    "text": "I don't know if any of you guys have checked out bustled comma I encourage you to do so it's a great news",
    "start": "856430",
    "end": "861530"
  },
  {
    "text": "entertainment lifestyle website targeted at women and bustled ran on a traditional infrastructure and moved",
    "start": "861530",
    "end": "868040"
  },
  {
    "text": "over to a server list environment and realized 84% savings in their operational costs in fact the CTO here",
    "start": "868040",
    "end": "875120"
  },
  {
    "text": "is quoted as saying they really don't need to worry too much about operational overhead anymore and that speaks",
    "start": "875120",
    "end": "880130"
  },
  {
    "text": "directly to the lack of servers that you have to be concerned with the built in H a and D are capabilities that we talked",
    "start": "880130",
    "end": "886730"
  },
  {
    "text": "about before now security with serverless applications is also worth",
    "start": "886730",
    "end": "892640"
  },
  {
    "text": "discussing because it's a little bit different from traditional applications each of these services that we've talked",
    "start": "892640",
    "end": "897980"
  },
  {
    "text": "about in this pattern has a lot of security features so we won't be able to go over all of them but I'll call a few",
    "start": "897980",
    "end": "903860"
  },
  {
    "text": "out there's an origin access identity capability with cloud front to ensure that only cloud front can access the",
    "start": "903860",
    "end": "910370"
  },
  {
    "text": "resources that are stored inside your s3 bucket both cloud front and api gateway",
    "start": "910370",
    "end": "915650"
  },
  {
    "text": "now support AWS certificate manager or a CM TLS certificates so that makes life a",
    "start": "915650",
    "end": "921920"
  },
  {
    "text": "lot easier now to create custom domain names and manage the TLS certificates",
    "start": "921920",
    "end": "927500"
  },
  {
    "text": "for those domain names for your api gateway and cloud front end points and then of course we heavily leveraged IAM",
    "start": "927500",
    "end": "933890"
  },
  {
    "text": "to make sure that api Gateway has privileges to call lambda functions and that lambda functions have",
    "start": "933890",
    "end": "939470"
  },
  {
    "text": "an execution role that they can leverage the clerk to call downstream services now one of the most important topics",
    "start": "939470",
    "end": "946730"
  },
  {
    "text": "when we talk about security is authorization of your API gateway methods and there's many different ways",
    "start": "946730",
    "end": "952760"
  },
  {
    "text": "to do that with API gateway probably the most common is as I mentioned before I am authorization so the caller of that",
    "start": "952760",
    "end": "959930"
  },
  {
    "text": "API needs to have I am credentials in order to you know make that call",
    "start": "959930",
    "end": "965360"
  },
  {
    "text": "successful and to get access to that particular method and there's a few different ways to leverage Cognito to",
    "start": "965360",
    "end": "972950"
  },
  {
    "text": "provide those iam credentials and we'll talk about a few scenarios related to that here in just a minute in addition",
    "start": "972950",
    "end": "979070"
  },
  {
    "text": "to iam authorization there's also custom authorization capabilities with API",
    "start": "979070",
    "end": "984560"
  },
  {
    "text": "gateway and this involves a lambda function that you're responsible for and the sole purpose of that lambda function",
    "start": "984560",
    "end": "991160"
  },
  {
    "text": "is to return an I am policy that it can validate with the I am service to",
    "start": "991160",
    "end": "996710"
  },
  {
    "text": "provide access to that particular method to go ahead and execute the lambda function in this case behind that API",
    "start": "996710",
    "end": "1002260"
  },
  {
    "text": "method now there's two types of custom authorizers with api gateway a token",
    "start": "1002260",
    "end": "1008110"
  },
  {
    "text": "request type where a token is included in an authorization header for that request and this enables you to",
    "start": "1008110",
    "end": "1014770"
  },
  {
    "text": "implement strategies like JWT validation or olaf provider call-outs so you can",
    "start": "1014770",
    "end": "1020770"
  },
  {
    "text": "contact open ID or Cognito providers to validate the tokens in those requests",
    "start": "1020770",
    "end": "1025990"
  },
  {
    "text": "there's also a request type that gives you a little more customized ability and flexibility so beyond just using that",
    "start": "1025990",
    "end": "1032380"
  },
  {
    "text": "authorization header you can use all headers in the request as well as query strings stage variables and context",
    "start": "1032380",
    "end": "1039250"
  },
  {
    "text": "variables so if you wanted to for example have different authentication schemes or strategies across different",
    "start": "1039250",
    "end": "1045880"
  },
  {
    "text": "stages of your application maybe dev would authorize a certain way but in production we doth Erised a different",
    "start": "1045880",
    "end": "1051970"
  },
  {
    "text": "way you can achieve those types of scenarios with a request authorizer",
    "start": "1051970",
    "end": "1057600"
  },
  {
    "start": "1057000",
    "end": "1057000"
  },
  {
    "text": "let's talk specifically about Cognito and how it enables some of the authorization scenarios we just talked",
    "start": "1057600",
    "end": "1064210"
  },
  {
    "text": "about in this particular slide we have three users over on the left side and user a authenticates to a web",
    "start": "1064210",
    "end": "1071320"
  },
  {
    "text": "identity provider this could be Google Facebook amazon.com even and that web",
    "start": "1071320",
    "end": "1076360"
  },
  {
    "text": "identity provider provides a token back to the user that it uses to send to",
    "start": "1076360",
    "end": "1081970"
  },
  {
    "text": "kognito and exchange that for an iam credential essentially through a role so",
    "start": "1081970",
    "end": "1088060"
  },
  {
    "text": "this is a very common pattern to get IEM authorization based on a web identity provider and get access to in this case",
    "start": "1088060",
    "end": "1094690"
  },
  {
    "text": "a slash web resource that's defined inside of your API another scenario user",
    "start": "1094690",
    "end": "1101200"
  },
  {
    "text": "B is actually defined inside a Cognito user pools this is a directory of users",
    "start": "1101200",
    "end": "1106300"
  },
  {
    "text": "and groups that you can maintain inside of AWS and the flow here is very similar to the previous where I exchanged a",
    "start": "1106300",
    "end": "1112360"
  },
  {
    "text": "token from authentication for a I am credentials and I leverage kognito",
    "start": "1112360",
    "end": "1117460"
  },
  {
    "text": "identity providers to do that and this also gives me a way to implement I am authorization but what's really nice",
    "start": "1117460",
    "end": "1124240"
  },
  {
    "text": "about this flow is I can leverage group memberships and attributes to get the route to get a role for my m to give me",
    "start": "1124240",
    "end": "1131050"
  },
  {
    "text": "access to make those calls so I if I'm a member of a certain group or maybe I'm in the engineering department then I can",
    "start": "1131050",
    "end": "1137500"
  },
  {
    "text": "get access to those iam roles and then lastly there's built-in support an API",
    "start": "1137500",
    "end": "1142780"
  },
  {
    "text": "gateway for a Cognito user pools authorizers so this is separate from the custom authorizers and the IM",
    "start": "1142780",
    "end": "1149350"
  },
  {
    "text": "authorizers that we talked about previously a specific type designed for Cognito user pools where the JSON web",
    "start": "1149350",
    "end": "1155650"
  },
  {
    "text": "token that you receive from authentication can be validated directly to give you access to your resource also",
    "start": "1155650",
    "end": "1163840"
  },
  {
    "start": "1163000",
    "end": "1163000"
  },
  {
    "text": "about a month ago we had a really big announcement around regional endpoints with API gateway and this is decoupling",
    "start": "1163840",
    "end": "1170500"
  },
  {
    "text": "API gateway from cloud front it's a really big release because it enables multi regions serverless applications",
    "start": "1170500",
    "end": "1178030"
  },
  {
    "text": "now with this pattern you can actually create regional endpoints for API",
    "start": "1178030",
    "end": "1183040"
  },
  {
    "text": "gateway and as you can see here separate regions and each of these can be associated with the same custom domain",
    "start": "1183040",
    "end": "1189670"
  },
  {
    "text": "name and then you can use weighted routing for example with C names in a DNS service like route 53 to direct",
    "start": "1189670",
    "end": "1197200"
  },
  {
    "text": "traffic from this from a request for the same name in this case API",
    "start": "1197200",
    "end": "1202850"
  },
  {
    "text": "or calm and you could use weighted routing to direct all of the requests potentially to one region and if there's",
    "start": "1202850",
    "end": "1208850"
  },
  {
    "text": "a failover you could automatically send those requests down to the other region or you can implement an active-active",
    "start": "1208850",
    "end": "1215030"
  },
  {
    "text": "strategy where both regions are experiencing a portion of the waiting and then lastly I just want to mention a",
    "start": "1215030",
    "end": "1222380"
  },
  {
    "start": "1221000",
    "end": "1221000"
  },
  {
    "text": "few frameworks that are available to help you adopt serverless web applications the first for the Python",
    "start": "1222380",
    "end": "1228500"
  },
  {
    "text": "developers in the audience that are familiar with a decorator based API is chalice chalice allows you to model",
    "start": "1228500",
    "end": "1235100"
  },
  {
    "text": "api's and API paths with lambda functions and deploying application from",
    "start": "1235100",
    "end": "1240650"
  },
  {
    "text": "Python code into a service environment and then if you have existing nodejs Express applications or java",
    "start": "1240650",
    "end": "1248060"
  },
  {
    "text": "applications written in a number of different web frameworks we have built-in libraries and frameworks inside",
    "start": "1248060",
    "end": "1254870"
  },
  {
    "text": "of github to convert those over into the AWS service platform okay now let's talk",
    "start": "1254870",
    "end": "1262940"
  },
  {
    "text": "about pattern 2 which is our data Lake pattern now one thing that's for sure",
    "start": "1262940",
    "end": "1268070"
  },
  {
    "text": "when you when you think about data analytics and processing data and that is your needs around that are probably",
    "start": "1268070",
    "end": "1275240"
  },
  {
    "text": "going to change at some point in the future they don't usually stay consistent and that's really why",
    "start": "1275240",
    "end": "1280490"
  },
  {
    "text": "adopting a data Lake is so important because it sets the foundational components for you to be more agile one",
    "start": "1280490",
    "end": "1287360"
  },
  {
    "text": "of the common tenants of a data Lake is to get all of your organizational data into that data Lake you may not know",
    "start": "1287360",
    "end": "1293090"
  },
  {
    "text": "exactly what you're going to do with that but very low barriers to entry to get that data in and then a data Lake",
    "start": "1293090",
    "end": "1300110"
  },
  {
    "text": "should implement a schema on read strategy so that as you request that data and interact with that data and ask",
    "start": "1300110",
    "end": "1306050"
  },
  {
    "text": "questions of it you can specify the schema to do so so in the future when new questions arise that you need to",
    "start": "1306050",
    "end": "1312380"
  },
  {
    "text": "gain from your data you'll be able to do that with a schema on read approach so a",
    "start": "1312380",
    "end": "1317720"
  },
  {
    "text": "data Lake should be a home for all types of data whether structured semi-structured or completely unstructured it's it should support bi",
    "start": "1317720",
    "end": "1324710"
  },
  {
    "text": "and analytical use cases that are very traditional as well as some of the more modern predictive analytics based on",
    "start": "1324710",
    "end": "1330290"
  },
  {
    "text": "machine learning and it's very complimentary to an enterprise data warehouse so data lakes are very",
    "start": "1330290",
    "end": "1336750"
  },
  {
    "text": "commonly used as staging areas into those data warehouses for analytical purposes so we definitely don't see",
    "start": "1336750",
    "end": "1342570"
  },
  {
    "text": "these as competitive things and then lastly a serverless data lake should implement decoupled compute and storage",
    "start": "1342570",
    "end": "1349440"
  },
  {
    "text": "so you can have multiple layers of a compute environment multiple compute environments working across the same",
    "start": "1349440",
    "end": "1355920"
  },
  {
    "text": "data in each of those laterz layers can scale independently from each other and they can be transient right so you can",
    "start": "1355920",
    "end": "1362310"
  },
  {
    "text": "only pay for them during executions at the center here or here's a look at the",
    "start": "1362310",
    "end": "1368880"
  },
  {
    "text": "server list data Lake Architecture at the center of the architecture is s3 which has a lot of great features",
    "start": "1368880",
    "end": "1374060"
  },
  {
    "text": "relative to a data Lake pattern that we'll discuss obviously a data Lake",
    "start": "1374060",
    "end": "1380370"
  },
  {
    "text": "needs to be able to accept data in a lot of different forms so ingest is very important my tree is gonna talk in very",
    "start": "1380370",
    "end": "1386910"
  },
  {
    "text": "good detail about stream processing and ingesting data in great volumes so I'm",
    "start": "1386910",
    "end": "1392580"
  },
  {
    "text": "gonna leave that portion of that pattern to him cataloging and searching is a",
    "start": "1392580",
    "end": "1398130"
  },
  {
    "text": "very important aspect of a data Lake so we'll talk about a couple of patterns that leverage DynamoDB and elastic",
    "start": "1398130",
    "end": "1405120"
  },
  {
    "text": "search and a pattern that leverages glue to provide this cataloguing and search ability of the data this is all about",
    "start": "1405120",
    "end": "1410850"
  },
  {
    "text": "discovering your data that you're delivering into your data Lake and then at its core analytics and processing is",
    "start": "1410850",
    "end": "1418440"
  },
  {
    "text": "very important obviously to a data Lake there are many different server lists services that you can use to do",
    "start": "1418440",
    "end": "1424200"
  },
  {
    "text": "analytics and processing like Athena and lambda that we'll discuss in pretty good detail there's also services that aren't server",
    "start": "1424200",
    "end": "1431850"
  },
  {
    "text": "lists that have a very important role here so I want to acknowledge tools like EMR in this in this place as well",
    "start": "1431850",
    "end": "1439250"
  },
  {
    "text": "there's certainly a security security component to a data Lake so you want to be able to manage entitlements and",
    "start": "1439250",
    "end": "1445530"
  },
  {
    "text": "provide access to data to differing users or groups or constituents of your",
    "start": "1445530",
    "end": "1450960"
  },
  {
    "text": "data you want to be able to encrypt your data and we have many different ways to do that whether you're leveraging s3",
    "start": "1450960",
    "end": "1456930"
  },
  {
    "text": "natively or kms you want to be able to audit access to your data so Cloud trail",
    "start": "1456930",
    "end": "1462870"
  },
  {
    "text": "and some of its new support for s3 come into hand there and we'll talk about those and don't forget about Amazon",
    "start": "1462870",
    "end": "1468870"
  },
  {
    "text": "Maisie it's a data classification patience service for s3 and it will automatically infer the types of data",
    "start": "1468870",
    "end": "1475260"
  },
  {
    "text": "based on the contents of the files that you store in s3 with machine learning algorithms so you can actually see you",
    "start": "1475260",
    "end": "1482070"
  },
  {
    "text": "know if you've got really sensitive information that might contain personally identifiable information as",
    "start": "1482070",
    "end": "1487290"
  },
  {
    "text": "an example inside the files stored in s3 and then lastly you need a user",
    "start": "1487290",
    "end": "1492450"
  },
  {
    "text": "interface for your data Lake so you need a way for users to maybe log into a portal and begin to discover and search",
    "start": "1492450",
    "end": "1498780"
  },
  {
    "text": "the information in your data Lake or possibly a programmatic API that they can use to do those same tasks and this",
    "start": "1498780",
    "end": "1505710"
  },
  {
    "text": "API interface should obviously adhere to the security that you implemented with iam so let's talk a little bit about s3",
    "start": "1505710",
    "end": "1513990"
  },
  {
    "start": "1512000",
    "end": "1512000"
  },
  {
    "text": "as a foundation for the data Lake unlimited volume so in limited amounts",
    "start": "1513990",
    "end": "1519900"
  },
  {
    "text": "of storage with s3 that's pretty nice for a data like also basically no aggregate throughput limits with s3",
    "start": "1519900",
    "end": "1526380"
  },
  {
    "text": "basically the amount of parallel jobs and workers you can have downloading and uploading files from s3 you can grow",
    "start": "1526380",
    "end": "1532320"
  },
  {
    "text": "your bandwidth accordingly multiple storage classes so there's the standard s3 storage class but also keep in mind",
    "start": "1532320",
    "end": "1538970"
  },
  {
    "text": "infrequent access and even glacier so older data that may be stored in your data Lake can be handled and moved",
    "start": "1538970",
    "end": "1545040"
  },
  {
    "text": "differently across those layers and versioning and encryption is fully supported with us 3 there's also maybe",
    "start": "1545040",
    "end": "1551370"
  },
  {
    "text": "some lesser-known features of s3 that you can take advantage with respect to a data Lake like Cloud trail data of",
    "start": "1551370",
    "end": "1558360"
  },
  {
    "text": "events so not only getting audits of when files are deleted or added to your s3 buckets but also when files are read",
    "start": "1558360",
    "end": "1565020"
  },
  {
    "text": "that can be really useful for a data like s3 also has built-in analytics and inventory capabilities so you can see",
    "start": "1565020",
    "end": "1571530"
  },
  {
    "text": "what usage profiles are like across your groups automatically and you can always get an inventory of the data stored",
    "start": "1571530",
    "end": "1577620"
  },
  {
    "text": "inside of us 3 maybe how big that's growing how many files there are those types of things",
    "start": "1577620",
    "end": "1583040"
  },
  {
    "text": "AWS config also has built-in checks for s3 so that if you're ever concerned that",
    "start": "1583040",
    "end": "1588570"
  },
  {
    "text": "someday somebody might enable public read access on one of your s3 buckets AWS config can monitor that and can",
    "start": "1588570",
    "end": "1595860"
  },
  {
    "text": "notify you when that happens and then lastly s3 supports object tagging which is great for chargeback capabilities in",
    "start": "1595860",
    "end": "1601830"
  },
  {
    "text": "a data like you know in storing additional net or information about your objects now",
    "start": "1601830",
    "end": "1607740"
  },
  {
    "text": "let's talk specifically about cataloging and searching there's a couple of patterns here to showcase the first",
    "start": "1607740",
    "end": "1613830"
  },
  {
    "start": "1608000",
    "end": "1608000"
  },
  {
    "text": "pattern is more of an event-based pattern so as data arrives in s3 regardless of how it arrives there",
    "start": "1613830",
    "end": "1620070"
  },
  {
    "text": "I run a lambda function to store metadata about that that file inside a dynamo so that could be information",
    "start": "1620070",
    "end": "1626700"
  },
  {
    "text": "about maybe the data set that it's a member of if this is an IOT use case it could be information about the device",
    "start": "1626700",
    "end": "1632070"
  },
  {
    "text": "maybe that's sending that data it could be a history about the versions perhaps of that particular object and then I",
    "start": "1632070",
    "end": "1639240"
  },
  {
    "text": "have an update stream with DynamoDB streams and a lambda function that acts on that and sends that information as",
    "start": "1639240",
    "end": "1645270"
  },
  {
    "text": "well as additional information from the source objects into elastic search for search ability to build a searchable",
    "start": "1645270",
    "end": "1651810"
  },
  {
    "text": "index so a really great way in a well published way and documented way to provide data cataloging capabilities for",
    "start": "1651810",
    "end": "1658950"
  },
  {
    "text": "a data lake I've included a link here to a whole answers article about doing just that",
    "start": "1658950",
    "end": "1664700"
  },
  {
    "text": "also AWS Glu plays a really vital role in this space AWS Glu can dispatch",
    "start": "1664700",
    "end": "1670860"
  },
  {
    "text": "crawlers either on a scheduled basis or on demand that can automatically look",
    "start": "1670860",
    "end": "1676620"
  },
  {
    "text": "inside your files and infer the schema around those files and build a catalog",
    "start": "1676620",
    "end": "1682590"
  },
  {
    "text": "from that information so this data catalog that Glu builds automatically",
    "start": "1682590",
    "end": "1688110"
  },
  {
    "text": "with these crawlers is actually hive meta store compliant and we've built in support for that with other analytical",
    "start": "1688110",
    "end": "1694560"
  },
  {
    "text": "services like Athena and redshift spectrum so you can immediately create defined tables and those services and",
    "start": "1694560",
    "end": "1700920"
  },
  {
    "text": "begin to query them in your bi tool of your choice like quick site from an",
    "start": "1700920",
    "end": "1707940"
  },
  {
    "text": "analytics and processing perspective related to the data like there are a lot of tools that are out there to provide",
    "start": "1707940",
    "end": "1713550"
  },
  {
    "text": "access to s3 information and data and do your analytical and processing workloads",
    "start": "1713550",
    "end": "1718970"
  },
  {
    "text": "quick side is certainly one that comes to mind there have been a lot of great recent quick site releases like",
    "start": "1718970",
    "end": "1724340"
  },
  {
    "text": "geospatial visualizations HIPAA compliance and even now the ability to",
    "start": "1724340",
    "end": "1729420"
  },
  {
    "text": "attach Ian's eyes to quick site so that you can gain access into private VPC resources",
    "start": "1729420",
    "end": "1735340"
  },
  {
    "text": "predictive analytics tools with machine learning EMR as we mentioned before AWS",
    "start": "1735340",
    "end": "1741560"
  },
  {
    "text": "glue also has an ETL component and that's a great way to process data that's in your data like before it's",
    "start": "1741560",
    "end": "1748400"
  },
  {
    "text": "imported into analytical stores like redshift but I want to focus on a couple",
    "start": "1748400",
    "end": "1753530"
  },
  {
    "text": "of sub patterns around this piece of the data like related to Athena and lambda specifically first of all Athena Athena",
    "start": "1753530",
    "end": "1760430"
  },
  {
    "start": "1758000",
    "end": "1758000"
  },
  {
    "text": "is our server lists interactive query service and it provides you a way to submit sequel queries to the data that",
    "start": "1760430",
    "end": "1767630"
  },
  {
    "text": "you have stored in s3 and what's fantastic about Athena is that it is super fast in this example we processed",
    "start": "1767630",
    "end": "1774710"
  },
  {
    "text": "a hundred and seventy gigabytes of data in under 45 seconds executing this query",
    "start": "1774710",
    "end": "1780350"
  },
  {
    "text": "we even have a couple of group by parameters specified that's almost four gigabytes a second of data that we're",
    "start": "1780350",
    "end": "1786680"
  },
  {
    "text": "processing it uses presto as an engine for its data manipulation as a as a matter of fact and there's some",
    "start": "1786680",
    "end": "1792770"
  },
  {
    "text": "efficiencies and best practices that you can gain with Athena I've got a link here to include kind of a top 10 of",
    "start": "1792770",
    "end": "1798920"
  },
  {
    "start": "1794000",
    "end": "1794000"
  },
  {
    "text": "those but I want to call out a few of them that are fairly important first of",
    "start": "1798920",
    "end": "1803930"
  },
  {
    "text": "all you can leverage Athena to partition your data Athena is priced based on the amount of data that you scan so if you",
    "start": "1803930",
    "end": "1810860"
  },
  {
    "text": "can reduce the amount of data that you scan you're going to save money and you can do that very effectively by partitioning your data if your data is",
    "start": "1810860",
    "end": "1818990"
  },
  {
    "text": "stored in an s3 bucket in a path like you see here Athena will automatically",
    "start": "1818990",
    "end": "1824240"
  },
  {
    "text": "partition your data so built-in synergies but you can specify manual partitions partitioning schemes as well",
    "start": "1824240",
    "end": "1831430"
  },
  {
    "text": "strongly recommend leveraging columnar data formats for the data that's stored in your s3 data Lake so formats like",
    "start": "1831430",
    "end": "1838160"
  },
  {
    "text": "Parque Avro and ork also optimization of file sizes becomes",
    "start": "1838160",
    "end": "1843620"
  },
  {
    "text": "very important for the data stored in s3 is it one big file or is it a lot of small files if it's a lot of small files",
    "start": "1843620",
    "end": "1850190"
  },
  {
    "text": "you will have a lot of overhead interacting with s3 as a service to open and get those files as an example but if",
    "start": "1850190",
    "end": "1858350"
  },
  {
    "text": "it's just one big file that may not be enough to give you that parallelism across your compute environments that",
    "start": "1858350",
    "end": "1863900"
  },
  {
    "text": "you need so that's why it's also really important to use compression formats that support",
    "start": "1863900",
    "end": "1869799"
  },
  {
    "text": "splittable compression parquet and Orick also supports splittable compression so",
    "start": "1869799",
    "end": "1875360"
  },
  {
    "text": "those are going to be the best file formats for Athena without a doubt but if those are unavailable to you",
    "start": "1875360",
    "end": "1880730"
  },
  {
    "text": "compression algorithms like gzip or B zip 2 are also very good from that perspective there's also a batch a",
    "start": "1880730",
    "end": "1889429"
  },
  {
    "start": "1888000",
    "end": "1888000"
  },
  {
    "text": "serverless batch processing pattern that I want to showcase for your analytical processing needs this is more of a DIY",
    "start": "1889429",
    "end": "1896270"
  },
  {
    "text": "approach where you have a lambda function that takes your source data and splits it up somehow maybe by lines or",
    "start": "1896270",
    "end": "1902539"
  },
  {
    "text": "by by size and hands it off to a bunch of mapper functions that run in parallel",
    "start": "1902539",
    "end": "1907970"
  },
  {
    "text": "to process your data and then write results to a persistent store and then we have a reducer function in this",
    "start": "1907970",
    "end": "1913850"
  },
  {
    "text": "example collecting that data this is a pattern that's also well live well leveraged by customers Fannie Mae",
    "start": "1913850",
    "end": "1920419"
  },
  {
    "text": "actually leverages a pattern very similar to this to process Monte Carlo",
    "start": "1920419",
    "end": "1925429"
  },
  {
    "text": "simulations and cash flow projections for their mortgages they actually used",
    "start": "1925429",
    "end": "1930590"
  },
  {
    "text": "to do this on more of a server based approach and by adopting this approach they were able to reduce the time it",
    "start": "1930590",
    "end": "1936710"
  },
  {
    "text": "takes to run simulations by a factor of four Fannie Mae has a specific session",
    "start": "1936710",
    "end": "1942940"
  },
  {
    "text": "addressing this particularly use case so I won't steal their Thunder but it's SR v317 if in case anybody is interested",
    "start": "1942940",
    "end": "1949490"
  },
  {
    "text": "and if especially if you're in the financial industry I strongly recommend you taking a look at that one",
    "start": "1949490",
    "end": "1954860"
  },
  {
    "text": "and then lastly I just want to point out that if you have existing Python code",
    "start": "1954860",
    "end": "1960500"
  },
  {
    "text": "and maybe you don't want to write the mapping and reducing capabilities like I just showed you in the pattern yourself",
    "start": "1960500",
    "end": "1966350"
  },
  {
    "text": "you can leverage a library like pyrin to do that for you it's actually democratizing the horizontal scale of",
    "start": "1966350",
    "end": "1973460"
  },
  {
    "text": "lambda so with the default account limit of a thousand concurrent lambda functions you can actually achieve with",
    "start": "1973460",
    "end": "1979640"
  },
  {
    "text": "pyrin about ten teraflops of peak compute power that's extremely powerful to do just by taking your existing",
    "start": "1979640",
    "end": "1987320"
  },
  {
    "text": "Python code and this is also a great library to leverage for interactions with s3 so you can bring data in to s3",
    "start": "1987320",
    "end": "1995450"
  },
  {
    "text": "or read data from s3 in very very massively parallel areas across many different lambda",
    "start": "1995450",
    "end": "2002500"
  },
  {
    "text": "functions the pyrin team actually ran some tests where they achieved 60 gigabytes per second of read and 50",
    "start": "2002500",
    "end": "2009520"
  },
  {
    "text": "gigabytes per second of write performance so I've included the link to the pyrin for further reading okay no",
    "start": "2009520",
    "end": "2017640"
  },
  {
    "text": "Maitreya is gonna come over and talk about pattern 3 thank you thank you drew",
    "start": "2017640",
    "end": "2028809"
  },
  {
    "text": "so stream processing pattern 3 imagine that you are responsible for designing and operating a highly successful",
    "start": "2028809",
    "end": "2034870"
  },
  {
    "text": "e-commerce website we just came out of cyber monday you had a great cyber monday you've been collecting",
    "start": "2034870",
    "end": "2040870"
  },
  {
    "text": "clickstream data from all the actions that happen on your website but you've been processing them on a daily basis",
    "start": "2040870",
    "end": "2046690"
  },
  {
    "text": "right so 24 hours after the fact your management gives you a challenge we'd",
    "start": "2046690",
    "end": "2052090"
  },
  {
    "text": "like to bring that time down from a day to a few minutes so that's a classic example of having to deal with data",
    "start": "2052090",
    "end": "2059260"
  },
  {
    "text": "that's streaming in and that's a classic example of a stream processing application that I'd like you to keep in",
    "start": "2059260",
    "end": "2064750"
  },
  {
    "text": "mind so as we look at the characteristics these characteristics are shared by many different",
    "start": "2064750",
    "end": "2069790"
  },
  {
    "text": "applications such as clickstream analysis you have high ingest rates imagine the late rate of clicks",
    "start": "2069790",
    "end": "2076450"
  },
  {
    "text": "happening on cyber monday you want to do near real-time processing this is that challenge of trying to bring the latency",
    "start": "2076450",
    "end": "2083770"
  },
  {
    "text": "down from the time an event happens to the time you get the insight you also",
    "start": "2083770",
    "end": "2088780"
  },
  {
    "text": "have spiky traffic obviously the traffic that you experience on cyber monday is likely to be many times higher than your",
    "start": "2088780",
    "end": "2095260"
  },
  {
    "text": "average day in the rest of the year once you get a message in it's very important",
    "start": "2095260",
    "end": "2100270"
  },
  {
    "text": "that you handle that it's so message durability is important and in many cases since you're trying to recreate",
    "start": "2100270",
    "end": "2107410"
  },
  {
    "text": "the actions a customer took the order of the messaging is also important so these are some of the common characteristics",
    "start": "2107410",
    "end": "2112900"
  },
  {
    "text": "of a stream processing application so let's take a look at the first example of how you would go about meeting your",
    "start": "2112900",
    "end": "2119350"
  },
  {
    "start": "2118000",
    "end": "2118000"
  },
  {
    "text": "management's needs this is an example of a stream data ingestion and analytics on",
    "start": "2119350",
    "end": "2124900"
  },
  {
    "text": "the right in terms of an architecture so the core of this architecture here is",
    "start": "2124900",
    "end": "2130020"
  },
  {
    "text": "Amazon Kinesis firehose in the middle so Canisius fire a service that lets you ingest large",
    "start": "2130020",
    "end": "2137349"
  },
  {
    "text": "amounts of data buffers them up in micro batches and delivers them to destination",
    "start": "2137349",
    "end": "2142539"
  },
  {
    "text": "services so this lets you essentially decouple a high rate of messages coming in to a destination service where you",
    "start": "2142539",
    "end": "2149470"
  },
  {
    "text": "can better deal with batches of data rather than having to deal them in with them in streams so what you do here is",
    "start": "2149470",
    "end": "2155920"
  },
  {
    "text": "you can send data in raw record form directly to the Kinesis firehose api's or you can install a Kinesis agent which",
    "start": "2155920",
    "end": "2164079"
  },
  {
    "text": "will look at your log files and as new messages come in to the log files it will post those messages to the Kinesis",
    "start": "2164079",
    "end": "2170769"
  },
  {
    "text": "firehose delivery stream either way once a record is in the delivery stream you",
    "start": "2170769",
    "end": "2176769"
  },
  {
    "text": "can actually do transformations on those records so you can invoke a lambda function as seen here you can send the",
    "start": "2176769",
    "end": "2183279"
  },
  {
    "text": "raw records to the lambda function and in the function you can have custom logic the logic can be to transform",
    "start": "2183279",
    "end": "2189700"
  },
  {
    "text": "their messages take a record and transform it from say a numeric timestamp to a human readable timestamp",
    "start": "2189700",
    "end": "2195700"
  },
  {
    "text": "for example or you might be able to look up data in a database like dynamodb so if you want to pull out information",
    "start": "2195700",
    "end": "2201400"
  },
  {
    "text": "about the user and that you've stored in DynamoDB you can make a call and enrich",
    "start": "2201400",
    "end": "2206410"
  },
  {
    "text": "the record as you do the transformation once the transformation is done those",
    "start": "2206410",
    "end": "2211930"
  },
  {
    "text": "records are sent back to Kinesis fire hose and fire hose now makes micro batches of those and delivers them to",
    "start": "2211930",
    "end": "2218650"
  },
  {
    "text": "destination services so on the right you see examples of destination services s3",
    "start": "2218650",
    "end": "2223749"
  },
  {
    "text": "is an example you can store your transformed records in batches in s3 or you can send those records directly to",
    "start": "2223749",
    "end": "2230650"
  },
  {
    "text": "redshift where they can be loaded into tables and be available for analytics you can also send those records to",
    "start": "2230650",
    "end": "2236859"
  },
  {
    "text": "elasticsearch service where they can be indexed and available for searches as well as visualization on tools like",
    "start": "2236859",
    "end": "2243099"
  },
  {
    "text": "gabbana the pattern at the bottom here is also a common thing that we see customers do this is a feature called",
    "start": "2243099",
    "end": "2248859"
  },
  {
    "text": "source record backup so when you are doing transformations you might want to keep a copy of the untransformed",
    "start": "2248859",
    "end": "2255219"
  },
  {
    "text": "raw records and enabling this feature gets firehose to send a copy of that to",
    "start": "2255219",
    "end": "2260499"
  },
  {
    "text": "an s3 bucket that you define once you set up this type of pipeline you can also monitor its performance and you do",
    "start": "2260499",
    "end": "2267460"
  },
  {
    "text": "that by looking at CloudWatch metrics that firehose publishes so you can look at metrics that tell you how far behind are you",
    "start": "2267460",
    "end": "2274570"
  },
  {
    "text": "from the head of the queue in terms of delivering delivering messages to s3 so you can tell how much you are falling",
    "start": "2274570",
    "end": "2281440"
  },
  {
    "text": "behind in terms of the messages that are coming in as you implement this pattern some best practices so firehose has two",
    "start": "2281440",
    "end": "2289119"
  },
  {
    "text": "key parameters you need to tune one is the buffer size and the other is the buffer interval so these two parameters",
    "start": "2289119",
    "end": "2295660"
  },
  {
    "text": "interact together to decide how frequently messages are delivered to the destinations and how big each of those",
    "start": "2295660",
    "end": "2301540"
  },
  {
    "text": "batch records are so if you bias towards larger objects obviously you get fewer",
    "start": "2301540",
    "end": "2307690"
  },
  {
    "text": "invocations to things like the transformation lambda function you also get fewer invocations to s3 in",
    "start": "2307690",
    "end": "2313150"
  },
  {
    "text": "terms of s3 puts so your costs can be lower firehose also lets you enable compression so this lets you compress",
    "start": "2313150",
    "end": "2320109"
  },
  {
    "text": "your data as it delivers to the destination a great idea to reduce your storage cost as well as to keep your",
    "start": "2320109",
    "end": "2325839"
  },
  {
    "text": "data transfer costs low we talked about stores record backup it's a great way to troubleshoot issues with your",
    "start": "2325839",
    "end": "2332080"
  },
  {
    "text": "transformation if you have a copy of the raw records and specifically if you are loading your data in to redshift it's a",
    "start": "2332080",
    "end": "2339160"
  },
  {
    "text": "good idea to follow the redshift data loading best practices the link is out here this talks about how you can format",
    "start": "2339160",
    "end": "2345550"
  },
  {
    "text": "your data to make it better and easily suited to load it into redshift for example how to deal with time series",
    "start": "2345550",
    "end": "2351880"
  },
  {
    "text": "data how to deal with the data that is sorted so do keep that in mind let's",
    "start": "2351880",
    "end": "2358570"
  },
  {
    "text": "imagine another scenario it's again the holiday seasons and we have sold a million connected thermostats so people",
    "start": "2358570",
    "end": "2365560"
  },
  {
    "start": "2359000",
    "end": "2359000"
  },
  {
    "text": "have taken these thermal thermostats delivered at home they've turned them on and now you want to provide a cloud-based service that takes the data",
    "start": "2365560",
    "end": "2373150"
  },
  {
    "text": "from these thermostats and provides some intelligence to those devices so obviously these thermostats are",
    "start": "2373150",
    "end": "2378640"
  },
  {
    "text": "low-power devices so they don't have much memory much cpu capacity they may not even have connectivity all the time",
    "start": "2378640",
    "end": "2385390"
  },
  {
    "text": "so you want to use a protocol that is optimized for such internet of thing type devices so we choose to use mqtt",
    "start": "2385390",
    "end": "2392740"
  },
  {
    "text": "here in this example as that protocol that is well-suited for low-power devices the server side of that is AWS",
    "start": "2392740",
    "end": "2400390"
  },
  {
    "text": "IOT so AWS IOT acts as a server-side and receives the messages and measurements from the",
    "start": "2400390",
    "end": "2406450"
  },
  {
    "text": "devices over MQTT once the message messages are in IOT we can write IOT",
    "start": "2406450",
    "end": "2413380"
  },
  {
    "text": "rules on that so these rules in sequel determine filtering on this messages",
    "start": "2413380",
    "end": "2418600"
  },
  {
    "text": "that come in so if you have different message types such as temperature humidity you can use a rule to filter",
    "start": "2418600",
    "end": "2424360"
  },
  {
    "text": "out just the temperature readings and react to those at the other side of a rule you can set up what's called an IOT",
    "start": "2424360",
    "end": "2430510"
  },
  {
    "text": "action which will deliver those messages that come out of the rule to a destination so in this example you see",
    "start": "2430510",
    "end": "2436930"
  },
  {
    "text": "destinations such as s3 where you can store the raw images or the raw sensor data that comes in you can also use the",
    "start": "2436930",
    "end": "2444070"
  },
  {
    "text": "pattern we saw before where you can send those records to firehose and have them",
    "start": "2444070",
    "end": "2449140"
  },
  {
    "text": "batch and send to s3 or in the middle you can decide that you want to process them in near-real-time",
    "start": "2449140",
    "end": "2454690"
  },
  {
    "text": "and you can deliver those to a Kinesis stream and hook up or create a real-time",
    "start": "2454690",
    "end": "2461140"
  },
  {
    "text": "application to deal with that so let's look at an example of a real-time application that deals with that imagine",
    "start": "2461140",
    "end": "2467650"
  },
  {
    "start": "2465000",
    "end": "2465000"
  },
  {
    "text": "that we have these sensors these are temperature sensors we know temperature tends to be a little noisy so we want to",
    "start": "2467650",
    "end": "2473560"
  },
  {
    "text": "take those measurements we want to smooth them out over a 60 minute 60-second period so every minute we want",
    "start": "2473560",
    "end": "2479350"
  },
  {
    "text": "to smooth that out and then compare that against the threshold that the owner sets so you might have a threshold of 90",
    "start": "2479350",
    "end": "2485800"
  },
  {
    "text": "degrees I might light like things cooler and I might have set my threshold as 80 degrees but that's a threshold that the",
    "start": "2485800",
    "end": "2491740"
  },
  {
    "text": "user sets and I want to send an alert when the temperature the average temperature reading exceeds the",
    "start": "2491740",
    "end": "2497980"
  },
  {
    "text": "threshold so this is a pipeline that achieves that starting from the left we have the data that arrived in the",
    "start": "2497980",
    "end": "2504430"
  },
  {
    "text": "Kinesis stream this could have arrived from the IOT pipeline that we saw in the previous slide and then we attach that",
    "start": "2504430",
    "end": "2510880"
  },
  {
    "text": "to a Kinesis analytics application so Kinesis analytics is a service that lets",
    "start": "2510880",
    "end": "2516670"
  },
  {
    "text": "you perform real-time near real-time analytics on data coming into a stream",
    "start": "2516670",
    "end": "2522040"
  },
  {
    "start": "2519000",
    "end": "2519000"
  },
  {
    "text": "and you represent your logic using sequel so this is streaming sequel I'll walk you through some of the parts of",
    "start": "2522040",
    "end": "2528130"
  },
  {
    "text": "this the parts in blue basically set up the source as well as the destination definitions the parts in yellow",
    "start": "2528130",
    "end": "2535830"
  },
  {
    "text": "that one minute window over which we want to smooth our data or average the data and the parts in green represents",
    "start": "2535830",
    "end": "2543330"
  },
  {
    "text": "the aggregation right so this is where we do the some of the measurements that the sensor centers we also count the",
    "start": "2543330",
    "end": "2549600"
  },
  {
    "text": "number of measurements so this gives us the ability to do an average and then we are grouping the whole measurement set",
    "start": "2549600",
    "end": "2556110"
  },
  {
    "text": "by the device ID so the end result of all of this processing is that we get one averaged measurement set every",
    "start": "2556110",
    "end": "2563670"
  },
  {
    "text": "minute per unique device that is in our fleet which is what we really wanted so you can see how simple it is with just a",
    "start": "2563670",
    "end": "2569880"
  },
  {
    "text": "few lines of sequel to represent a pretty complex processing requirement once we have those aggregated",
    "start": "2569880",
    "end": "2576660"
  },
  {
    "text": "measurements we can deliver those to a destination Kinesis stream again and to",
    "start": "2576660",
    "end": "2581850"
  },
  {
    "text": "perform a custom thresholding logic right so this is where we take those average measurements and then we pull",
    "start": "2581850",
    "end": "2588510"
  },
  {
    "text": "out the user's defined thresholds from DynamoDB compare the two and decide if",
    "start": "2588510",
    "end": "2593880"
  },
  {
    "text": "you need to send an alert using SNS the other thing you see in this diagram is the error stream so every time you",
    "start": "2593880",
    "end": "2600600"
  },
  {
    "text": "process a stream of messages through Canisius analytics there could be some miss formatted records and what Canisius",
    "start": "2600600",
    "end": "2608160"
  },
  {
    "text": "analytics will do is deliver those miss formatted or error records to a destination in this case we send it to a",
    "start": "2608160",
    "end": "2615420"
  },
  {
    "text": "firehose delivery stream and store a copy of those records in s3 so we can go back and look at those and try to fix",
    "start": "2615420",
    "end": "2622170"
  },
  {
    "text": "any errors later so this is a good practice let's double click on the",
    "start": "2622170",
    "end": "2627300"
  },
  {
    "text": "middle part here where Kinesis streams have triggered lambda and sees the few best practices around that one of the",
    "start": "2627300",
    "end": "2634710"
  },
  {
    "start": "2634000",
    "end": "2634000"
  },
  {
    "text": "things you have to remember when you set this up is that Kinesis streams are scaled in terms of shards so each shard",
    "start": "2634710",
    "end": "2641250"
  },
  {
    "text": "in Kinesis stream has a certain capacity one megabytes per second in and two",
    "start": "2641250",
    "end": "2646530"
  },
  {
    "text": "megabytes per second out so this is your unit of scale when you configure a Kinesis stream the important thing to",
    "start": "2646530",
    "end": "2652800"
  },
  {
    "text": "remember is that for each shard that you have in your Kinesis stream you get one parallel concurrent invocation of lambda",
    "start": "2652800",
    "end": "2659490"
  },
  {
    "text": "so it's a one to one relationship so if you have five shards you get five parallel lambda invocations so this",
    "start": "2659490",
    "end": "2665250"
  },
  {
    "text": "means that you need to code your lambda function in order to be able to handle the capacity of a shard within that one",
    "start": "2665250",
    "end": "2672030"
  },
  {
    "text": "parallel invocation the other thing is you have a tunable parameter called a batch size this configures how many",
    "start": "2672030",
    "end": "2679170"
  },
  {
    "text": "messages maximum that Kinesis streams will send to a one lambda function invocation so this sets you that batch",
    "start": "2679170",
    "end": "2686220"
  },
  {
    "text": "size the default is a hundred but often we find that when you have very high throughput applications you might want",
    "start": "2686220",
    "end": "2692970"
  },
  {
    "text": "to tune that higher the maximum is 10,000 so the effect of that is you get more messages per invocation and you can",
    "start": "2692970",
    "end": "2700350"
  },
  {
    "text": "process them all in one invocation sometimes you may find that the amount",
    "start": "2700350",
    "end": "2707130"
  },
  {
    "start": "2705000",
    "end": "2705000"
  },
  {
    "text": "of processing that you want to do in your lambda function is so much that you can't keep up with the rate of messages",
    "start": "2707130",
    "end": "2712530"
  },
  {
    "text": "coming into one shard right in that case you may want to consider creating a fan-out pattern in this pattern you take",
    "start": "2712530",
    "end": "2719220"
  },
  {
    "text": "your lambda function logic and split it into two parts the first part is the dispatcher this is responsible for",
    "start": "2719220",
    "end": "2725910"
  },
  {
    "text": "getting that big batch of messages from Kinesis streams making smaller batches out of that and invoking a lambda",
    "start": "2725910",
    "end": "2732330"
  },
  {
    "text": "processor function for each of those batches in parallel so that's on the right side so as you can imagine you",
    "start": "2732330",
    "end": "2738390"
  },
  {
    "text": "parallelized more by using this pattern so you can get a higher throughput and",
    "start": "2738390",
    "end": "2743730"
  },
  {
    "text": "you can also get potentially lower latency because you are processing an entire large batch in parallel on the",
    "start": "2743730",
    "end": "2749820"
  },
  {
    "text": "right but if you look at that when you're processing things in parallel what you give up is message ordering",
    "start": "2749820",
    "end": "2755790"
  },
  {
    "text": "right whereas before you would get a set of messages in order in one invocation since you're now processing them all in",
    "start": "2755790",
    "end": "2763050"
  },
  {
    "text": "parallel you can no longer guarantee that you're going to process them in order so depending upon your requirements",
    "start": "2763050",
    "end": "2768390"
  },
  {
    "text": "this fan-out pattern might be a good fit or may not be a good fit an example of a",
    "start": "2768390",
    "end": "2775440"
  },
  {
    "start": "2775000",
    "end": "2775000"
  },
  {
    "text": "customer that's using the stream processing pattern is thumbs and Reuters they provide news and data feeds to",
    "start": "2775440",
    "end": "2780990"
  },
  {
    "text": "businesses and professionals worldwide in all these products they have product teams who wanted to understand how those",
    "start": "2780990",
    "end": "2786930"
  },
  {
    "text": "products were being used they wanted to basically do usage analysis tracking and so they want they built a product called",
    "start": "2786930",
    "end": "2793320"
  },
  {
    "text": "product inside and this collects all that information click stream and analytics from all their properties and",
    "start": "2793320",
    "end": "2799140"
  },
  {
    "text": "feed that through a pipeline similar to what we saw and generates inside that the product teams can look at and",
    "start": "2799140",
    "end": "2805440"
  },
  {
    "text": "use that insights to improve the products in near real-time so some statistics they shared with us their",
    "start": "2805440",
    "end": "2811530"
  },
  {
    "text": "currently tested up to four thousand requests per second peak they're also looking at growing that to over ten",
    "start": "2811530",
    "end": "2818310"
  },
  {
    "text": "thousand requests per second peak that works out to almost twenty five billion requests per month the end-to-end",
    "start": "2818310",
    "end": "2825630"
  },
  {
    "text": "latency they are observing between the time a request comes in to the time they get the insight is less than ten seconds",
    "start": "2825630",
    "end": "2831000"
  },
  {
    "text": "and they're very happy to tell us that since they launched this service or product they haven't lost any records so",
    "start": "2831000",
    "end": "2837690"
  },
  {
    "text": "far since they launched that so that's an example of a successful stream processing pipeline some best practices",
    "start": "2837690",
    "end": "2845220"
  },
  {
    "text": "so remember you have to tune the batch size this lets you invoke less lambda functions and you pay less because we",
    "start": "2845220",
    "end": "2851760"
  },
  {
    "text": "charge by the number of invocations in lambda it's also important to tune that memory dial if you tune the memory dial",
    "start": "2851760",
    "end": "2857910"
  },
  {
    "text": "higher you get more CPU and that might help you process their messages in one invocation faster and help you keep up",
    "start": "2857910",
    "end": "2864510"
  },
  {
    "text": "with the rate of messages coming from one shard if you have control over the client side of your processing pipeline",
    "start": "2864510",
    "end": "2870990"
  },
  {
    "text": "we highly encourage looking at kpl which is the Kinesis producer library this is",
    "start": "2870990",
    "end": "2876270"
  },
  {
    "text": "a client side library what it does is it locally batches up multiple messages into one Genesis record so what that",
    "start": "2876270",
    "end": "2883860"
  },
  {
    "text": "lets you do is fully saturate your Kinesis stream capacity as well as make much fewer api invocations to the",
    "start": "2883860",
    "end": "2890160"
  },
  {
    "text": "Kinesis stream so a great idea for you to be able to batch things on the client side before sending it to the stream",
    "start": "2890160",
    "end": "2896250"
  },
  {
    "text": "processing pipeline since we're talking about stream processing there are",
    "start": "2896250",
    "end": "2901410"
  },
  {
    "start": "2899000",
    "end": "2899000"
  },
  {
    "text": "several related services we looked at Canisius streams in much detail there are also sqs simple queuing service and",
    "start": "2901410",
    "end": "2907740"
  },
  {
    "text": "SNS simple notification service that can play a role in server less architectures for stream processing this is charge",
    "start": "2907740",
    "end": "2915570"
  },
  {
    "text": "here aims to compare these services across various dimensions I just wanted to highlight two of those message",
    "start": "2915570",
    "end": "2921270"
  },
  {
    "text": "ordering for example so Kinesis stream orders messages strictly within a shard when you're looking at sqs the standard",
    "start": "2921270",
    "end": "2928770"
  },
  {
    "text": "queue is best effort so it doesn't guarantee ordering but you can also have a FIFO queue which guarantees ordering",
    "start": "2928770",
    "end": "2934830"
  },
  {
    "text": "within a concept called a message while S&S doesn't guarantee ordering at all looking at other things like can you",
    "start": "2934830",
    "end": "2941920"
  },
  {
    "text": "go forward and backward in time Kinesis stream lets you do that through what's called a shard iterator so you can",
    "start": "2941920",
    "end": "2947229"
  },
  {
    "text": "actually reprocess messages up to the retention period while sqs an SNS once",
    "start": "2947229",
    "end": "2953289"
  },
  {
    "text": "you get a notification it won't be sent back to you again once you acknowledge it successfully right so keep that in",
    "start": "2953289",
    "end": "2959289"
  },
  {
    "text": "mind and depending upon your business requirements one or more of these use cases will be better fit by one of these",
    "start": "2959289",
    "end": "2965979"
  },
  {
    "text": "services all right let's look at pattern four which is operations automation so",
    "start": "2965979",
    "end": "2971829"
  },
  {
    "text": "I'm going to talk to you in more detail about three of those patterns number one is periodic jobs we all have",
    "start": "2971829",
    "end": "2977170"
  },
  {
    "start": "2973000",
    "end": "2973000"
  },
  {
    "text": "requirements to do things on a regular basis we look at some examples of that we have event event triggered workflows",
    "start": "2977170",
    "end": "2983799"
  },
  {
    "text": "so we want to trigger a complex workflow when an event happens or we want to enforce security policies and we feel",
    "start": "2983799",
    "end": "2990489"
  },
  {
    "text": "that there are other use cases also like audit and notification responding to alarms as well as trying to extend AWS",
    "start": "2990489",
    "end": "2996699"
  },
  {
    "text": "functionality for example creating custom order scaling and the best part about using serverless patterns for this",
    "start": "2996699",
    "end": "3002969"
  },
  {
    "text": "is that you can be highly available scaleable and auditable just by using inherent properties of the services that",
    "start": "3002969",
    "end": "3009269"
  },
  {
    "text": "you are using let's look at the first example which is periodic tasks so we've",
    "start": "3009269",
    "end": "3014339"
  },
  {
    "start": "3013000",
    "end": "3013000"
  },
  {
    "text": "all had a case where we need to actually share you things like EBS snapshots and configure things like how long we want",
    "start": "3014339",
    "end": "3020279"
  },
  {
    "text": "to retain these EBS snapshots if you have a very large fleet of AWS accounts and your operating multiple AWS regions",
    "start": "3020279",
    "end": "3027299"
  },
  {
    "text": "it can be a challenge to coordinate all of that and keep those EBS snapshots going and track all of those over time",
    "start": "3027299",
    "end": "3033569"
  },
  {
    "text": "so we created a solution called the ops Automator the link below actually takes",
    "start": "3033569",
    "end": "3038729"
  },
  {
    "text": "you to a QuickStart where you can deploy this it takes a form of two templates the first one you deploy in a master",
    "start": "3038729",
    "end": "3045329"
  },
  {
    "text": "account which will be where all that logic and tracking happens and the second set of templates are what you",
    "start": "3045329",
    "end": "3051059"
  },
  {
    "text": "deploy in the slave accounts where you want to actually trigger the action so those are the accounts where you have",
    "start": "3051059",
    "end": "3056640"
  },
  {
    "text": "the EBS volumes that you want to snapshot and the master account is where you define things like properties of the",
    "start": "3056640",
    "end": "3063089"
  },
  {
    "text": "tasks the duration the frequency and things like that all of those properties are configured in a dynamo DB table",
    "start": "3063089",
    "end": "3070500"
  },
  {
    "text": "so the way the process starts is that a cloud watch event which is a periodic event triggers the event handler lambda",
    "start": "3070500",
    "end": "3077220"
  },
  {
    "text": "function this function pulls class task definition from dynamodb looks at the",
    "start": "3077220",
    "end": "3082830"
  },
  {
    "text": "tasks and decides what it needs to do by invoking one or more event handler or task executor lambda functions those",
    "start": "3082830",
    "end": "3090869"
  },
  {
    "text": "lambda functions will assume a role in the slave accounts and perform the action that you want so in order to take",
    "start": "3090869",
    "end": "3097859"
  },
  {
    "text": "EBS snapshots all you have to do is tag your ec2 instances with a special tag called the ops automated task list set",
    "start": "3097859",
    "end": "3105210"
  },
  {
    "text": "the task well the tag value as create snapshot and the lambda function will create a snapshot for that ec2 instance",
    "start": "3105210",
    "end": "3111450"
  },
  {
    "text": "it'll do that across multiple accounts and across multiple regions the best part about this is it lots of track how",
    "start": "3111450",
    "end": "3118320"
  },
  {
    "text": "those tasks went by writing information into dynamo dB as well as into cloud watch logs if the",
    "start": "3118320",
    "end": "3124440"
  },
  {
    "text": "process encounters any errors it will notify you automatically through SNS so you can receive an email or an SMS so",
    "start": "3124440",
    "end": "3131190"
  },
  {
    "text": "it's a great way for you to automate commonly occurring periodic tasks the entire framework can also be extended to",
    "start": "3131190",
    "end": "3137099"
  },
  {
    "text": "do other tasks it has built-in support for redshift clusters but you can also do other things like invoke system",
    "start": "3137099",
    "end": "3143460"
  },
  {
    "text": "manager tasks to perform actions on servers you could also use it to trigger security actions like triggering as an",
    "start": "3143460",
    "end": "3150599"
  },
  {
    "text": "inspector assessment any automation of that nature what if you have a different",
    "start": "3150599",
    "end": "3157080"
  },
  {
    "start": "3156000",
    "end": "3156000"
  },
  {
    "text": "case where you have a requirement to orchestrate a more complex workflow imagine you have an image website or",
    "start": "3157080",
    "end": "3164250"
  },
  {
    "text": "place where people are uploading images and as Susan as an image is uploaded you want an image to go through a set of",
    "start": "3164250",
    "end": "3170720"
  },
  {
    "text": "workflow actions so for example let's say that a user uploads an image the",
    "start": "3170720",
    "end": "3176220"
  },
  {
    "text": "image arrives in s3 we have an event notification on that which triggers a lambda function that lambda functions",
    "start": "3176220",
    "end": "3182700"
  },
  {
    "text": "job is to start a step function state machine invocation so in the step",
    "start": "3182700",
    "end": "3188520"
  },
  {
    "text": "function state machine we have a complex workflow that orchestrates the job of multiple lambda functions the first",
    "start": "3188520",
    "end": "3195000"
  },
  {
    "text": "thing this does is it invokes a function to extract metadata from the image this could be things like the image type",
    "start": "3195000",
    "end": "3201530"
  },
  {
    "text": "information from the exif header for example it also then goes in in works in parallel to other lambda functions the",
    "start": "3201530",
    "end": "3209130"
  },
  {
    "text": "first one invokes recognition to try to find out the objects in the scene",
    "start": "3209130",
    "end": "3214200"
  },
  {
    "text": "the second one does an image thumbnail on the object and finally all that information is written into dynamo DB",
    "start": "3214200",
    "end": "3221430"
  },
  {
    "text": "using a fourth lambda function so the end result of that is when the user",
    "start": "3221430",
    "end": "3226560"
  },
  {
    "text": "refreshes their page after uploading an image they see all the metadata they see a thumbnail and they see the object tags",
    "start": "3226560",
    "end": "3232860"
  },
  {
    "text": "that they were have seen in the image so this is an example it's actually part of our reference architectures for image",
    "start": "3232860",
    "end": "3239310"
  },
  {
    "text": "processing and you can use this pattern to extend it to do other things that you want to do upon events a quick look at",
    "start": "3239310",
    "end": "3246510"
  },
  {
    "text": "the state machine behind this it looks very much like a flowchart it has the ability to orchestrate multiple steps",
    "start": "3246510",
    "end": "3252990"
  },
  {
    "start": "3247000",
    "end": "3247000"
  },
  {
    "text": "you can have parallel steps and you can even have error handling and you can break out if something goes wrong at one",
    "start": "3252990",
    "end": "3259440"
  },
  {
    "text": "of the steps the other advantage of this is it makes it really easy to add at an additional step later right so if you",
    "start": "3259440",
    "end": "3266070"
  },
  {
    "text": "decide that you need more processing you can add that into the pipeline let's",
    "start": "3266070",
    "end": "3271590"
  },
  {
    "text": "take a final example here this is to enforce security policies so let's say that in your organization you have a",
    "start": "3271590",
    "end": "3277740"
  },
  {
    "start": "3272000",
    "end": "3272000"
  },
  {
    "text": "rule that says we should never allow a security group rule to allow RDP access",
    "start": "3277740",
    "end": "3283860"
  },
  {
    "text": "from the world at large so you can enforce that policy in multiple ways but here is an example of a way to enforce",
    "start": "3283860",
    "end": "3290370"
  },
  {
    "text": "detect as well as mitigate that policy in one step so the way this starts is",
    "start": "3290370",
    "end": "3295380"
  },
  {
    "text": "once somebody creates such a rule a cloud watch event is triggered on any security group changes that event is fed",
    "start": "3295380",
    "end": "3302910"
  },
  {
    "text": "to a lambda function which has custom logic the custom logic checks against the allowed types of rules and denied",
    "start": "3302910",
    "end": "3308910"
  },
  {
    "text": "types of rules if a rule is supposed to be denied it immediately sends an email alert and then it goes back and makes an",
    "start": "3308910",
    "end": "3315600"
  },
  {
    "text": "API call to delete that rule so the end to end time in which you are exposed is in the order of seconds right so it's",
    "start": "3315600",
    "end": "3321750"
  },
  {
    "text": "almost nothing the other thing you can do in this pattern is you can send those events to an event bus so you can route",
    "start": "3321750",
    "end": "3328050"
  },
  {
    "text": "them to a different AWS account if you want to an example of a customer that",
    "start": "3328050",
    "end": "3333210"
  },
  {
    "start": "3333000",
    "end": "3333000"
  },
  {
    "text": "uses operations automation pattern is Autodesk they provide software for people who buildings they have many engineering",
    "start": "3333210",
    "end": "3339779"
  },
  {
    "text": "teams who want to launch experimental projects but they felt that they couldn't innovate fast enough because",
    "start": "3339779",
    "end": "3344819"
  },
  {
    "text": "the amount of time it took to launch a new AWS account and condition it to their security requirements took very",
    "start": "3344819",
    "end": "3351630"
  },
  {
    "text": "very long and a lot of effort so what it has created a tool called Taylor as the name suggests the job of Taylor is to",
    "start": "3351630",
    "end": "3358470"
  },
  {
    "text": "tailor an AWS account to meet the corporate standards so it configures I am it configures config and",
    "start": "3358470",
    "end": "3364460"
  },
  {
    "text": "direct-connect according to the corporate standards Autodesk has brought the time down from 10 hours of manual",
    "start": "3364460",
    "end": "3370440"
  },
  {
    "text": "work to just 10 minutes they're doing a lot more projects in parallel now and the entire framework Taylor is available",
    "start": "3370440",
    "end": "3377039"
  },
  {
    "text": "to you open-source so if you have this kind of problem you can adopt this and extend it by using the source available",
    "start": "3377039",
    "end": "3382769"
  },
  {
    "text": "to you when you're looking at automation as automation so a couple of best practices when you're calling AWS api's",
    "start": "3382769",
    "end": "3390150"
  },
  {
    "text": "you may get throttled if you make those API calls at a higher rate so handle those using exponential retry back of",
    "start": "3390150",
    "end": "3396299"
  },
  {
    "text": "algorithms it's a good idea to publish custom metrics such as the number of EBS volume snapshotted so you can keep track",
    "start": "3396299",
    "end": "3402869"
  },
  {
    "text": "of how that automation is doing over time enable x-ray as we talked about and with automation try to document how to",
    "start": "3402869",
    "end": "3410219"
  },
  {
    "text": "disable the events and triggering to your operations workflow so if something goes wrong you can disable that to",
    "start": "3410219",
    "end": "3416700"
  },
  {
    "text": "troubleshoot it so in summary we look today at how you can use dev op tools to",
    "start": "3416700",
    "end": "3422910"
  },
  {
    "start": "3419000",
    "end": "3419000"
  },
  {
    "text": "automate your server less deployments we've done a deep dive into four main patterns web applications data leaks",
    "start": "3422910",
    "end": "3429210"
  },
  {
    "text": "stream processing and operations automation so we highly encourage you to take these patterns home with you to",
    "start": "3429210",
    "end": "3434700"
  },
  {
    "text": "your organization's and see what fits in terms of solving your business problems and let us know what you can build with",
    "start": "3434700",
    "end": "3440339"
  },
  {
    "text": "server less and join the service revolution",
    "start": "3440339",
    "end": "3444588"
  },
  {
    "start": "3444000",
    "end": "3444000"
  },
  {
    "text": "we wanted to leave you with some related sessions the highlight here is SRV 3:1 seven which is Fannie Mae talking about",
    "start": "3450369",
    "end": "3456559"
  },
  {
    "text": "the HPC workload and further reading so these are white papers which will tell you more about serverless patterns and",
    "start": "3456559",
    "end": "3462589"
  },
  {
    "start": "3459000",
    "end": "3459000"
  },
  {
    "text": "some of the considerations so thank you enjoy the rest of the show [Applause]",
    "start": "3462589",
    "end": "3469219"
  }
]