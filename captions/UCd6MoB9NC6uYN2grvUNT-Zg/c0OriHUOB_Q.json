[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "hello all right i think we're gonna get started here so first off uh a big welcome to everybody and thank",
    "start": "799",
    "end": "7040"
  },
  {
    "text": "you for coming i think it takes some commitment to be here at 4 30 on thursday uh",
    "start": "7040",
    "end": "12480"
  },
  {
    "text": "so thanks for that i'm vidya srinivasan i run engineering and operations for amazon redshift",
    "start": "12480",
    "end": "18800"
  },
  {
    "text": "and i'm going to be co-presenting this session with timon from nustar who's director of infrastructure there i'm",
    "start": "18800",
    "end": "25119"
  },
  {
    "text": "just going to be covering a very brief overview of redshift what the platform was all about",
    "start": "25119",
    "end": "30640"
  },
  {
    "text": "and why it is relevant for adtech and then had it after timon who's going to talk to us about",
    "start": "30640",
    "end": "35680"
  },
  {
    "text": "the journey that nustar has had with this platform for analytics and especially talk to you about some",
    "start": "35680",
    "end": "40800"
  },
  {
    "text": "best practices around using it so hope you find it interesting so let's get started uh so we released",
    "start": "40800",
    "end": "48079"
  },
  {
    "text": "uh retrotrice's service two reinvents ago uh the first the very first one on valentine's day actually vengeance on",
    "start": "48079",
    "end": "54719"
  },
  {
    "text": "valentine's day last year and this is a petabyte scale sql data warehouse",
    "start": "54719",
    "end": "60000"
  },
  {
    "text": "for aws and we really conceived of this with three basic principles we said we wanted to build something that was",
    "start": "60000",
    "end": "66880"
  },
  {
    "text": "simple fast and cheap so let's just take that apart what do we mean by simple we wanted a",
    "start": "66880",
    "end": "72720"
  },
  {
    "text": "data warehouse that that would be super easy for you to understand to deal with and use as a customer",
    "start": "72720",
    "end": "78799"
  },
  {
    "text": "and we spend a lot of time internally to figure out how we can remove all the friction that you have to",
    "start": "78799",
    "end": "84560"
  },
  {
    "text": "deal with whether it's with creation whether it's with querying whether it's getting your data in or managing and really we wanted to build",
    "start": "84560",
    "end": "92079"
  },
  {
    "text": "this not just for the dbas we wanted to build this for the developers the engineers",
    "start": "92079",
    "end": "97600"
  },
  {
    "text": "the data scientists because we just want to unleash that population and give them access to data in an easy",
    "start": "97600",
    "end": "103200"
  },
  {
    "text": "way so simplicity is a big part of what drives our design and architecture the second thing is fast so i'm here to",
    "start": "103200",
    "end": "110880"
  },
  {
    "text": "meet a customer who tells me please make my query slower you know that just doesn't happen right",
    "start": "110880",
    "end": "116000"
  },
  {
    "text": "everybody wants things faster you put more data in and you have less time to compute retro being an mpp system and a columnar",
    "start": "116000",
    "end": "123439"
  },
  {
    "text": "system helps quite a bit with ensuring that whatever you're running gets done quicker faster because you have so much",
    "start": "123439",
    "end": "130160"
  },
  {
    "text": "resource dedicated for every query that you run so that's fast the third aspect is cheap",
    "start": "130160",
    "end": "136400"
  },
  {
    "text": "well that's just everybody's on a budget and what we wanted to do was give an offering that would be disruptively",
    "start": "136400",
    "end": "142160"
  },
  {
    "text": "priced when we launched last year so for retro for example you could create a cluster and pay as low as 1000",
    "start": "142160",
    "end": "149680"
  },
  {
    "text": "per terabyte per year and this is compressed storage and it's a managed database service so",
    "start": "149680",
    "end": "155200"
  },
  {
    "text": "it was a fairly uh fairly competitively priced and we also",
    "start": "155200",
    "end": "160480"
  },
  {
    "text": "give a bunch of different options for you to adjust prices that i can talk about that in a little bit",
    "start": "160480",
    "end": "166319"
  },
  {
    "start": "165000",
    "end": "382000"
  },
  {
    "text": "so let me just quickly dive into the architecture the redshift is a cluster architecture and when you create a",
    "start": "166319",
    "end": "171519"
  },
  {
    "text": "retrofit cluster you're going to create one leader node and any number of compute nodes the leader node is",
    "start": "171519",
    "end": "176879"
  },
  {
    "text": "responsible for three things it's responsible to be a sql endpoint for all the external clients so this is",
    "start": "176879",
    "end": "182640"
  },
  {
    "text": "what your bi tools will connect to or any of your postgresql clients and it's responsible",
    "start": "182640",
    "end": "189760"
  },
  {
    "text": "for communicating with them on one end and with all the internal nodes on the other end and we would never allow an external",
    "start": "189760",
    "end": "195920"
  },
  {
    "text": "client to directly talk to internal compute nodes for security reasons so the leader node also stores all the",
    "start": "195920",
    "end": "201840"
  },
  {
    "text": "metadata for the cluster so this is where we store the catalog system tables etc that gives the store state about what",
    "start": "201840",
    "end": "208720"
  },
  {
    "text": "data you have in the cluster and furthermore the leader node also coordinates query execution meaning it",
    "start": "208720",
    "end": "214319"
  },
  {
    "text": "takes the incoming sql uh it takes it apart parses it uh figures out what's the most optimized",
    "start": "214319",
    "end": "220480"
  },
  {
    "text": "query plan for it and then generates the code that needs to get executed this code then gets shipped off to the",
    "start": "220480",
    "end": "225920"
  },
  {
    "text": "compute nodes when it now the compute node is where all your data is actually stored",
    "start": "225920",
    "end": "231200"
  },
  {
    "text": "and it's stored in a local storage in columnar format local storage because we want direct attach disks uh meaning the bandwidth that you",
    "start": "231200",
    "end": "238239"
  },
  {
    "text": "have to move data from the computer to the cpu is is pretty high and in general with data warehousing the goal is to",
    "start": "238239",
    "end": "244560"
  },
  {
    "text": "minimize i o so columnist for the same reason local storage is for the same reason and",
    "start": "244560",
    "end": "251360"
  },
  {
    "text": "the interesting feature or the characteristic of this architecture is all the major",
    "start": "251360",
    "end": "257359"
  },
  {
    "text": "operations that you do with your cluster happens in parallel across the compute nodes and that's really what",
    "start": "257359",
    "end": "263440"
  },
  {
    "text": "gives us the scale out nature of uh of redshift so what does that mean so when you're doing inserts or when you're doing loads",
    "start": "263440",
    "end": "270800"
  },
  {
    "text": "either from s3 dynamodb emr or by doing ssh",
    "start": "270800",
    "end": "276000"
  },
  {
    "text": "behind the scenes the compute nodes will all work in parallel to go in just the data same thing happens when we do a backup",
    "start": "276000",
    "end": "282960"
  },
  {
    "text": "or a restore or a resize anytime you're moving a lot of data around or crunching data we actually do",
    "start": "282960",
    "end": "288400"
  },
  {
    "text": "everything in parallel same thing happens for query execution the query is get a part of the query gets executed in",
    "start": "288400",
    "end": "295280"
  },
  {
    "text": "each of the nodes and then gets aggregated to the leader node and the results get sent out and this entire thing is is on an hpc",
    "start": "295280",
    "end": "302960"
  },
  {
    "text": "network we use a 10 gig minimally over subscribe network internally to connect all these nodes",
    "start": "302960",
    "end": "309360"
  },
  {
    "text": "the compute nodes themselves comes in two types of instances there's the hdd or the dw1 which is magnetic",
    "start": "309360",
    "end": "317360"
  },
  {
    "text": "spinning disk and in this case you can start with a two terabyte node and",
    "start": "317360",
    "end": "322400"
  },
  {
    "text": "go all the way up to a two petabyte cluster a two petabyte cluster comes when you use 128 nodes or you can go with the dw2",
    "start": "322400",
    "end": "330800"
  },
  {
    "text": "instance type which we launched earlier this year which is basically gives you more memory and cpu and it is",
    "start": "330800",
    "end": "337360"
  },
  {
    "text": "it is ssd storage um and you start with a smaller data set size it's 160 gigs all the way up to two",
    "start": "337360",
    "end": "344000"
  },
  {
    "text": "okay that's wrong it's 328 terabytes i should change that um so the",
    "start": "344000",
    "end": "350880"
  },
  {
    "text": "okay so how do you decide between the two uh it's going to depend on the data set size if you have really a couple of hundred gigs of data ssd may",
    "start": "350880",
    "end": "357440"
  },
  {
    "text": "be a better bet the other thing that uh affects the choice is with ssds you get about five and a half",
    "start": "357440",
    "end": "364560"
  },
  {
    "text": "times better performance and sorry you get five and a half times it's five and a half times more",
    "start": "364560",
    "end": "370319"
  },
  {
    "text": "expensive and it's about 10 to 15 times better performance so it really boils down to the price",
    "start": "370319",
    "end": "375680"
  },
  {
    "text": "performance trade-off that you're willing to make you know which one you want to pick",
    "start": "375680",
    "end": "381840"
  },
  {
    "start": "382000",
    "end": "422000"
  },
  {
    "text": "all right so let's talk about some of the attic use cases that we see um in use with redshift today so you're",
    "start": "383360",
    "end": "390240"
  },
  {
    "text": "all i'm sure you're all familiar with all the all of these terms but the fundamental thing here is in across all these use cases",
    "start": "390240",
    "end": "396479"
  },
  {
    "text": "essentially what you're trying to do is to figure out what ad to serve who saw the ads that were served how",
    "start": "396479",
    "end": "403360"
  },
  {
    "text": "much was how much did that cost and then what action did the user take when they did see that ad",
    "start": "403360",
    "end": "409520"
  },
  {
    "text": "and you want to bring all this information back into your analytics system and then figure out what the next step",
    "start": "409520",
    "end": "414560"
  },
  {
    "text": "should be and how you're going to optimize your system and just be more effective overall and",
    "start": "414560",
    "end": "419840"
  },
  {
    "text": "we've had a number of customers use it in different for different use cases so for example",
    "start": "419840",
    "end": "425520"
  },
  {
    "text": "accordant media uses redshift in the context of real-time bidding so the bidding itself happens using",
    "start": "425520",
    "end": "430720"
  },
  {
    "text": "cloudfront and dynamodb but redshift comes into play when they're trying to train their models based on what already happened at",
    "start": "430720",
    "end": "437280"
  },
  {
    "text": "theorent and fixo they use retro for mobile ad optimization",
    "start": "437280",
    "end": "442479"
  },
  {
    "text": "aggregate knowledge or new star we're going to hear about quite a bit from timon mixpo uses redship for video advertising",
    "start": "442479",
    "end": "449360"
  },
  {
    "text": "strongly uses registered for email marketing campaign but anyway i didn't want to go through everything the point i'm just trying to",
    "start": "449360",
    "end": "455440"
  },
  {
    "text": "make is we've had we've seen a lot of adoption in this space and we've seen usage in a lot of different",
    "start": "455440",
    "end": "461039"
  },
  {
    "text": "use cases as well and we'll be happy to get into details of any one of them",
    "start": "461039",
    "end": "468800"
  },
  {
    "start": "467000",
    "end": "649000"
  },
  {
    "text": "so if you think about why why is that the case and why does redshift why is it why is it an interesting",
    "start": "468960",
    "end": "474960"
  },
  {
    "text": "platform for ad tech it really boils down to what the needs for the attic",
    "start": "474960",
    "end": "480080"
  },
  {
    "text": "segment is and how uh retrieve can actually satisfy them so what we're dealing with here is",
    "start": "480080",
    "end": "485360"
  },
  {
    "text": "typically a lot of data you need to get this crunch in a fairly short amount of time because you want to",
    "start": "485360",
    "end": "491440"
  },
  {
    "text": "use the analysis for doing taking action before the next day comes or the before",
    "start": "491440",
    "end": "497039"
  },
  {
    "text": "the afternoon or on an hourly basis and you're on a budget to figure this out budget in the sense you have to",
    "start": "497039",
    "end": "502720"
  },
  {
    "text": "justify the need to store and analyze all of this data from a business point of view",
    "start": "502720",
    "end": "509599"
  },
  {
    "text": "and if you think about what how redshift operates and how we've designed the service with a lot of data",
    "start": "509599",
    "end": "514640"
  },
  {
    "text": "you can actually go from for a 160 gigs all the way to two petabytes within a single cluster",
    "start": "514640",
    "end": "520800"
  },
  {
    "text": "or alternately you can actually create multiple different clusters and you know it's pretty unlimited how how much data",
    "start": "520800",
    "end": "526320"
  },
  {
    "text": "you want to store in use the other point here is we use s3 as sort of the data hub where",
    "start": "526320",
    "end": "532800"
  },
  {
    "text": "most of the loads happen from s3 and given this setup with s3 as the data",
    "start": "532800",
    "end": "538080"
  },
  {
    "text": "and registered as a computation engine you can pick and choose what data sets go into which clusters",
    "start": "538080",
    "end": "543519"
  },
  {
    "text": "and mix and match how you want to analyze your workloads and i'll talk a little bit more about this",
    "start": "543519",
    "end": "549519"
  },
  {
    "text": "if you look at the budget side of it uh one it's it's priced at a it's it's",
    "start": "549519",
    "end": "555519"
  },
  {
    "text": "priced um you know it you can go as low as thousand thousand dollars per terabyte per year",
    "start": "555519",
    "end": "561200"
  },
  {
    "text": "and hopefully it makes decisions around what data you want to keep versus what data you have to throw",
    "start": "561200",
    "end": "566480"
  },
  {
    "text": "a lot easier and hopefully you don't have to sample data but rather run your queries across the entire data set",
    "start": "566480",
    "end": "573360"
  },
  {
    "text": "furthermore the other thing that we've seen our customers do because we allow you to store your backups and resurrect",
    "start": "573360",
    "end": "578399"
  },
  {
    "text": "them into clusters very easily one of the things that people do to keep",
    "start": "578399",
    "end": "583600"
  },
  {
    "text": "costs under control is they'll spin up a cluster when they want to do some analysis and spin it down so for example in the weekends",
    "start": "583600",
    "end": "590240"
  },
  {
    "text": "customers several customers spin down their clusters because they're not actually crunching anything and just the ease with which you can",
    "start": "590240",
    "end": "596640"
  },
  {
    "text": "bring it up and down makes this very easy to do and the third component is time again we do",
    "start": "596640",
    "end": "603120"
  },
  {
    "text": "crunch queries quite fast we're about 10 times faster compared to row based data stores and",
    "start": "603120",
    "end": "610320"
  },
  {
    "text": "and we're in mpp system meaning you can add more nodes to give more resources for your queries and potentially speed",
    "start": "610320",
    "end": "616720"
  },
  {
    "text": "that up the other thing you could do is you could use ssds which again speed things up by",
    "start": "616720",
    "end": "622399"
  },
  {
    "text": "10 to 20 times uh furthermore one of the requirements we got specifically from from our tech",
    "start": "622399",
    "end": "628320"
  },
  {
    "text": "customers is there was a willingness to trade speed for accuracy so we started introducing approximate",
    "start": "628320",
    "end": "634640"
  },
  {
    "text": "functions i'll talk a little bit more about that but essentially the goal there is to provide you with",
    "start": "634640",
    "end": "640160"
  },
  {
    "text": "different tools and options so that you it's easy for you to make those trade-offs particularly depending on your workload",
    "start": "640160",
    "end": "645920"
  },
  {
    "text": "and what you're trying to do i'm just going to quickly cover a few",
    "start": "645920",
    "end": "651839"
  },
  {
    "start": "649000",
    "end": "689000"
  },
  {
    "text": "functions that are relevant that we see as very as being in use by by our attack",
    "start": "651839",
    "end": "657200"
  },
  {
    "text": "customers one of them is the approximate count distinct function this is based on the hyper log log algorithm and essentially it gives you",
    "start": "657200",
    "end": "664000"
  },
  {
    "text": "about 10 to 20 times speed up and we say we we guarantee a maximum error bound of two percent",
    "start": "664000",
    "end": "670399"
  },
  {
    "text": "and a lot of our internal workloads at least when we've done this test typically it ends up being less than one",
    "start": "670399",
    "end": "675519"
  },
  {
    "text": "percent but theoretically it's a two percent limit and it can help you very quickly figure out a basic stuff like",
    "start": "675519",
    "end": "681920"
  },
  {
    "text": "number of unique visitors uh unique impressions etc so pretty pretty handy handy function to",
    "start": "681920",
    "end": "688399"
  },
  {
    "text": "have the other feature that's a lot that's used quite a bit is copy from json",
    "start": "688399",
    "end": "695200"
  },
  {
    "start": "689000",
    "end": "719000"
  },
  {
    "text": "so we have customers who have a lot of json data coming in um and what what we allow with this",
    "start": "695200",
    "end": "701120"
  },
  {
    "text": "feature is to take your json stream and then decompose that into columns within the retro table",
    "start": "701120",
    "end": "706800"
  },
  {
    "text": "uh if it's a direct mapping you can use auto or you can actually specify a mapping in a json paths file that",
    "start": "706800",
    "end": "712800"
  },
  {
    "text": "we'll use to to map your json data to reach of columns",
    "start": "712800",
    "end": "718720"
  },
  {
    "start": "719000",
    "end": "864000"
  },
  {
    "text": "the third feature that this is the requirement we've been hearing and we just announced this feature yesterday",
    "start": "720399",
    "end": "726639"
  },
  {
    "text": "it's currently not fully available it's going to be available pretty soon essentially this feature is this is",
    "start": "726639",
    "end": "732959"
  },
  {
    "text": "a multi-dimensional index using space filling curves and the problem that we were going after",
    "start": "732959",
    "end": "738720"
  },
  {
    "text": "here is what we currently support with respect to sorting is a multi uh it's a compound sort key so in that",
    "start": "738720",
    "end": "745440"
  },
  {
    "text": "case what you end up doing is you specify a number of columns and we will sort it based on the order",
    "start": "745440",
    "end": "750480"
  },
  {
    "text": "in which you specify so it's one compound sort key uh the disadvantage of this sort of a key is when your queries come in without",
    "start": "750480",
    "end": "757600"
  },
  {
    "text": "the leading thought column the performance of the sorting is not all that good because you don't have the leading key",
    "start": "757600",
    "end": "764639"
  },
  {
    "text": "typically data warehouses have solved this problem with the use of projections so basically you will store your set of",
    "start": "764639",
    "end": "770959"
  },
  {
    "text": "columns in different sorted orders depending on what you think your queries are going to come in",
    "start": "770959",
    "end": "776160"
  },
  {
    "text": "and you will use the appropriate one at runtime the problem with projections are one",
    "start": "776160",
    "end": "781760"
  },
  {
    "text": "there's a space overhead because you're actually storing the same columns in multiple different formats or different orders",
    "start": "781760",
    "end": "787200"
  },
  {
    "text": "and number two which is a bigger deal for us is that we now have to maintain the same data",
    "start": "787200",
    "end": "792560"
  },
  {
    "text": "and there's a huge maintenance overhead when you do updates and search deletes you just have so many locations you have",
    "start": "792560",
    "end": "797600"
  },
  {
    "text": "to go and make sure everything gets corrected um so instead what we did was we came up",
    "start": "797600",
    "end": "802800"
  },
  {
    "text": "with sort of an intermediate solution where we said we're going to create an index uh whereby we actually interleave all the",
    "start": "802800",
    "end": "809519"
  },
  {
    "text": "bits that you provide as part of your sort columns and we are going to create a",
    "start": "809519",
    "end": "814639"
  },
  {
    "text": "minimum and maximum bucket for that for that key and maintain that essentially what this",
    "start": "814639",
    "end": "820720"
  },
  {
    "text": "means for you is you can specify uh from one to eight columns as indexing columns and we will",
    "start": "820720",
    "end": "827120"
  },
  {
    "text": "internally create these indices for you but as far as querying goes",
    "start": "827120",
    "end": "832480"
  },
  {
    "text": "all each one of these columns is treated as a first class citizen meaning you can specify any subset of columns and still see a",
    "start": "832480",
    "end": "838880"
  },
  {
    "text": "performance gain you're not committed to always specifying the leading column",
    "start": "838880",
    "end": "844240"
  },
  {
    "text": "so the performance gain here varies quite a bit when you have a larger data set you will see a lot more performance benefit",
    "start": "844240",
    "end": "850880"
  },
  {
    "text": "over the compound sort key if you if you do specify the leading column the compound sort key still gives slightly",
    "start": "850880",
    "end": "857040"
  },
  {
    "text": "better performance so there's some nuances around how to get the most out of this feature",
    "start": "857040",
    "end": "863920"
  },
  {
    "start": "864000",
    "end": "1011000"
  },
  {
    "text": "and the last one that i wanted to highlight more as the way that you can use the platform more so than a feature",
    "start": "865760",
    "end": "871440"
  },
  {
    "text": "as such as around how you can use it in the context of managing a large number of clusters",
    "start": "871440",
    "end": "877120"
  },
  {
    "text": "so one of the things that i've pointed out earlier is one it's easy to create clusters and it's",
    "start": "877120",
    "end": "882560"
  },
  {
    "text": "easy to so and it's easy to um create not just it's easy to scale out",
    "start": "882560",
    "end": "888639"
  },
  {
    "text": "meaning you can add more notes to a single cluster or create a lot of new clusters the other thing that's available to you",
    "start": "888639",
    "end": "894800"
  },
  {
    "text": "in terms of raw data is all your backups are stored in s3 and available to you and all your load data is also in",
    "start": "894800",
    "end": "901600"
  },
  {
    "text": "various s3 buckets typically now what this affords you to do is just various different use cases so for",
    "start": "901600",
    "end": "907760"
  },
  {
    "text": "example let's say you wanted to create a qa cluster you don't have it doesn't have to take or a pre-prod cluster it doesn't take",
    "start": "907760",
    "end": "913519"
  },
  {
    "text": "weeks to do that all you have to do is maybe take your production cluster a backup of that",
    "start": "913519",
    "end": "918800"
  },
  {
    "text": "and just do a create on that and that takes about three minutes on average our cluster creates at about three minutes",
    "start": "918800",
    "end": "924800"
  },
  {
    "text": "so it's a very quick way for you to get that done another thing this affords you is",
    "start": "924800",
    "end": "929839"
  },
  {
    "text": "you can now create separate clusters for each one of your customers uh or each one of your workloads and",
    "start": "929839",
    "end": "936079"
  },
  {
    "text": "because of the way we price you get an exact dollar amount for how much you have to spend to maintain that so it",
    "start": "936079",
    "end": "942079"
  },
  {
    "text": "could be you know 65 dollars a day and then now you can make some smart decisions around",
    "start": "942079",
    "end": "947120"
  },
  {
    "text": "well is it useful for you to now keep that up do you get enough value out of it or do you want to shut that down so",
    "start": "947120",
    "end": "954480"
  },
  {
    "text": "that's the other benefit you get from a setup like this a third benefit could just be around well you might have one big cluster with",
    "start": "954480",
    "end": "960959"
  },
  {
    "text": "a lot of workloads and maybe there is one day where you have to get all your results in half the time",
    "start": "960959",
    "end": "966720"
  },
  {
    "text": "for some reason one other thing you can do is quickly take part of the workloads and run in a different cluster",
    "start": "966720",
    "end": "972240"
  },
  {
    "text": "and speed up the time it takes to produce results um anyway the i mean there are multiple",
    "start": "972240",
    "end": "977839"
  },
  {
    "text": "different possibilities i think all i wanted to point out is you have some building blocks here with respect to backups your data",
    "start": "977839",
    "end": "983920"
  },
  {
    "text": "and just the ease with which you can manage a single cluster or multiple clusters uh that affords use cases and that",
    "start": "983920",
    "end": "990320"
  },
  {
    "text": "affords a behavior that might be different than what you are typically used to something something to think about so with that i'm going to hand off",
    "start": "990320",
    "end": "997120"
  },
  {
    "text": "to timon",
    "start": "997120",
    "end": "999680"
  },
  {
    "text": "thank you am i all set great um so",
    "start": "1002399",
    "end": "1008720"
  },
  {
    "text": "over the course of the next half hour uh i'm going to talk to you a little bit",
    "start": "1008720",
    "end": "1014079"
  },
  {
    "start": "1011000",
    "end": "1048000"
  },
  {
    "text": "about some of the products and platforms we built for online ad analytics at nustar",
    "start": "1014079",
    "end": "1020079"
  },
  {
    "text": "how redshift has helped us make it made it possible and opened up new horizons for things that weren't previously",
    "start": "1020079",
    "end": "1025918"
  },
  {
    "text": "possible for us so first i'll take you through four textbook ad tech problems",
    "start": "1025919",
    "end": "1031438"
  },
  {
    "text": "that our customers really wanted answers to then i'll show you how we solve those problems with redshift and share with you the",
    "start": "1031439",
    "end": "1037199"
  },
  {
    "text": "design lessons we learned along the way and finally i'll talk to you about some of the operational advantages redshift affords us",
    "start": "1037199",
    "end": "1042558"
  },
  {
    "text": "and how it's allowed us to execute on new ideas that weren't possible in the past",
    "start": "1042559",
    "end": "1048558"
  },
  {
    "start": "1048000",
    "end": "1083000"
  },
  {
    "text": "so let's go through our four problems the first is how many ads should i show you um understanding the correct number of",
    "start": "1048559",
    "end": "1054480"
  },
  {
    "text": "ads to show is critical because obviously you don't want to upset people you don't want to waste money by showing them too many ads but on the flip side you also want to",
    "start": "1054480",
    "end": "1060480"
  },
  {
    "text": "reach your audience you want to make yourself known and this problem is often called the frequency problem or the optimal frequency problem",
    "start": "1060480",
    "end": "1067039"
  },
  {
    "text": "so the second question is how much i pay for ads and some ads are more effective than",
    "start": "1067039",
    "end": "1072799"
  },
  {
    "text": "others if they're working better than others you should probably spend money on those and not the poorly performing ones",
    "start": "1072799",
    "end": "1078080"
  },
  {
    "text": "obviously so this class of problem is called attribution or attribution analysis third question",
    "start": "1078080",
    "end": "1084400"
  },
  {
    "start": "1083000",
    "end": "1102000"
  },
  {
    "text": "is how do i reach this group of people i'm interested in seeing for less money than i pay to reach them",
    "start": "1084400",
    "end": "1089600"
  },
  {
    "text": "right now if you can reach the same people for less money on some other site and some other place on the web",
    "start": "1089600",
    "end": "1095280"
  },
  {
    "text": "you should probably reorganize your spend to reach them there and not on the more expensive places and this is called computing overlap",
    "start": "1095280",
    "end": "1101840"
  },
  {
    "text": "right and so our final problem probably the most vexing the most problematic one that no engineer or product manager",
    "start": "1101840",
    "end": "1108400"
  },
  {
    "start": "1102000",
    "end": "1117000"
  },
  {
    "text": "wants to hear is could you just run us a custom query and obviously this goes by many names but",
    "start": "1108400",
    "end": "1113600"
  },
  {
    "text": "we'll call it ad hoc here and i'll tell you a little bit about how we solved that so",
    "start": "1113600",
    "end": "1118880"
  },
  {
    "start": "1117000",
    "end": "1149000"
  },
  {
    "text": "for each of these four problems i'll go into the to the question and dig into the core of",
    "start": "1118880",
    "end": "1123919"
  },
  {
    "text": "it a little bit i'll give you some scale numbers and production to set your expectations and then i'll cover a simplified version",
    "start": "1123919",
    "end": "1130400"
  },
  {
    "text": "of each solution so that we get our footing then we'll talk about the prod challenges and how we solve them and",
    "start": "1130400",
    "end": "1136080"
  },
  {
    "text": "finally i'll give you rough estimates of execution times on certain clusters and hopefully i'm going to draw some",
    "start": "1136080",
    "end": "1141280"
  },
  {
    "text": "parallels to canonical data warehousing problems the non-adtech people can",
    "start": "1141280",
    "end": "1146880"
  },
  {
    "text": "get an idea of what we're talking about so let's go to our first problem which was frequency you know how many ads should i show you",
    "start": "1146880",
    "end": "1152799"
  },
  {
    "start": "1149000",
    "end": "1168000"
  },
  {
    "text": "and deciding the right number of ads to show um varies by advertiser obviously because you have different criteria right some",
    "start": "1152799",
    "end": "1159039"
  },
  {
    "text": "people are interested in maximizing the number of purchases while staying under budget yet others want to drive sign ups third",
    "start": "1159039",
    "end": "1165360"
  },
  {
    "text": "group might want to maximize profit but regardless of what your criteria are this is fundamentally a cohort analysis",
    "start": "1165360",
    "end": "1171679"
  },
  {
    "start": "1168000",
    "end": "1206000"
  },
  {
    "text": "problem right we're asking the question did the behavior of the people who fall into the cohort of people that",
    "start": "1171679",
    "end": "1177520"
  },
  {
    "text": "saw n ads differ from those that saw n plus one ads and cohort analysis basically boils down to a multi-stage",
    "start": "1177520",
    "end": "1184559"
  },
  {
    "text": "aggregation first you aggregate to decide which cohort each person falls into",
    "start": "1184559",
    "end": "1189760"
  },
  {
    "text": "and then you take those cohorts and you aggregate once more to put together their activity into some",
    "start": "1189760",
    "end": "1194960"
  },
  {
    "text": "type of roi metric so in our case we need to compute the number of ads each person saw in order to define their cohort and then",
    "start": "1194960",
    "end": "1202400"
  },
  {
    "text": "we want to aggregate once again to produce our roi metric now the wrench that's thrown into the works here is",
    "start": "1202400",
    "end": "1208880"
  },
  {
    "start": "1206000",
    "end": "1241000"
  },
  {
    "text": "that when i say a person or people i actually mean unique cookies right it's a random number we put in",
    "start": "1208880",
    "end": "1214559"
  },
  {
    "text": "in the in the cookie of a user in their browser so when you account for multiple devices multiple browsers cookie churn you know",
    "start": "1214559",
    "end": "1221600"
  },
  {
    "text": "firefox automatically deleting all the cookies after seven days you actually end up with a pretty significant number of people so for a",
    "start": "1221600",
    "end": "1227679"
  },
  {
    "text": "quarter that's about 20 billion people that's that's a lot right so with that kind of limitation in mind",
    "start": "1227679",
    "end": "1234000"
  },
  {
    "text": "let's go through uh the skeleton sql of this and um and kind of understand why that's",
    "start": "1234000",
    "end": "1239280"
  },
  {
    "text": "ostensibly a hard problem so in this example let's say we're trying to optimize for profit",
    "start": "1239280",
    "end": "1244880"
  },
  {
    "text": "and uh you know i've magically assigned each impression a cost and a uh and a revenue value based on",
    "start": "1244880",
    "end": "1251120"
  },
  {
    "text": "last touch attribution don't worry if you don't know what last touch attribution is just imagine i know what the cost and the revenue driven by each impression",
    "start": "1251120",
    "end": "1257440"
  },
  {
    "text": "already is an impression being a view of an ad for those people that aren't in the ad tech world the first",
    "start": "1257440",
    "end": "1263600"
  },
  {
    "text": "stage of this query computes the number of ads that each person saw and also sums up the cost and",
    "start": "1263600",
    "end": "1269600"
  },
  {
    "text": "revenue right so this is straightforward we're just saying hey for this date range go group by user id and accumulate the number of",
    "start": "1269600",
    "end": "1275679"
  },
  {
    "text": "impressions every single user saw then the second stage is take all the people",
    "start": "1275679",
    "end": "1282159"
  },
  {
    "text": "who saw n ads and accumulate their statistics together right so we're just taking the common table",
    "start": "1282159",
    "end": "1288960"
  },
  {
    "text": "expression frequency intermediate and we're pivoting it and re-aggregating by their cohort",
    "start": "1288960",
    "end": "1294559"
  },
  {
    "text": "pretty straightforward now you may be thinking yourself well uh that's a pretty big aggregate because",
    "start": "1294559",
    "end": "1300240"
  },
  {
    "start": "1296000",
    "end": "1360000"
  },
  {
    "text": "it's uh computed uh it's a group it's a group by on the user id and like i just said for a",
    "start": "1300240",
    "end": "1305280"
  },
  {
    "text": "quarter that may be 20 billion people so that frequency intermediate cte is going to have 20 billion rows uh but my answer to you",
    "start": "1305280",
    "end": "1313039"
  },
  {
    "text": "will be is hey that's not actually big enough because that was just the skeleton query in reality if you've run an ad tech you",
    "start": "1313039",
    "end": "1319760"
  },
  {
    "text": "know or an analysis report you're going to have different date ranges you're going to have to segment by campaign and by site",
    "start": "1319760",
    "end": "1325600"
  },
  {
    "text": "and by all these other things so it's actually significantly more than uh",
    "start": "1325600",
    "end": "1330960"
  },
  {
    "text": "than 20 billion rows that are going to have to go in there so in reality what we're doing is saying",
    "start": "1330960",
    "end": "1337360"
  },
  {
    "text": "hey that's already a big number but if we ran it off of the raw data the raw impression rose there's",
    "start": "1337360",
    "end": "1343520"
  },
  {
    "text": "about 200 billion of those in a quarter for us so it's not practical to run those every time for say six different date",
    "start": "1343520",
    "end": "1350400"
  },
  {
    "text": "ranges for all kinds of different segmentations on the raw data so in production in order to speed things up",
    "start": "1350400",
    "end": "1355919"
  },
  {
    "text": "we materialized an intermediate aggregation in a way that could support those needs so let's go look at what that",
    "start": "1355919",
    "end": "1361039"
  },
  {
    "start": "1360000",
    "end": "1461000"
  },
  {
    "text": "intermediate aggregation looks like so effectively this table speeds up our",
    "start": "1361039",
    "end": "1366400"
  },
  {
    "text": "computation of our cohorts by partially aggregating each user's activity by day by campaign and by site so what",
    "start": "1366400",
    "end": "1373600"
  },
  {
    "text": "that allows to do allows us to do is to efficiently scan certain date ranges courtesy of the first part of the sort key",
    "start": "1373600",
    "end": "1380480"
  },
  {
    "text": "so we've ordered the table in such a way that we can if we're only looking at the last 30 days of data for one of our date",
    "start": "1380480",
    "end": "1385600"
  },
  {
    "text": "ranges then we can quickly eliminate the rest of the quarter but if we want to run the whole quarter we can just scan the whole",
    "start": "1385600",
    "end": "1391039"
  },
  {
    "text": "table and then our segmentation and aggregation to get the desired cohort criteria comes thanks",
    "start": "1391039",
    "end": "1397120"
  },
  {
    "text": "to the last three columns in the sort key right now i've added these extra facets to this",
    "start": "1397120",
    "end": "1403039"
  },
  {
    "text": "intermediate to support these business use cases of segmentation and different date ranges but the downside is that by adding these",
    "start": "1403039",
    "end": "1409120"
  },
  {
    "text": "extra facets we end up with three to ten times more rows um so for a quarter i quoted 20 billion users",
    "start": "1409120",
    "end": "1415520"
  },
  {
    "text": "that translates to about 70 billion rows in this table but you know as i mentioned earlier it",
    "start": "1415520",
    "end": "1421200"
  },
  {
    "text": "beats the pants off of aggregating 200 billion rows of raw data each time so",
    "start": "1421200",
    "end": "1426960"
  },
  {
    "text": "this is great for speeding up the query but it has another added benefit that i like to talk about so this type of aggregated intermediate",
    "start": "1426960",
    "end": "1433360"
  },
  {
    "text": "can be built incrementally day by day so i can basically pull in a new day's worth of data squish",
    "start": "1433360",
    "end": "1439919"
  },
  {
    "text": "it down and insert it into this table every single day and i won't have to do a vacuum because this preserves the sort order",
    "start": "1439919",
    "end": "1446080"
  },
  {
    "text": "of the sort key right because by definition the new day's worth of data has a greater record date than all the",
    "start": "1446080",
    "end": "1451440"
  },
  {
    "text": "previous days so that's a really big added benefit not only do i get to build this up incrementally over time",
    "start": "1451440",
    "end": "1457440"
  },
  {
    "text": "but i don't have to run a vacuum so i find that tremendously useful so what does that that intermediate",
    "start": "1457440",
    "end": "1464080"
  },
  {
    "start": "1461000",
    "end": "1495000"
  },
  {
    "text": "translate to in terms of our final query well it should come pretty naturally first we've widened the grouping",
    "start": "1464080",
    "end": "1470080"
  },
  {
    "text": "right so we're now grouping by user id and campaign id and site id so we can understand per",
    "start": "1470080",
    "end": "1475360"
  },
  {
    "text": "campaign per site what that user's cohort is and then uh we're basically using the other table so",
    "start": "1475360",
    "end": "1482720"
  },
  {
    "text": "this is this is very straightforward i mean this is basically the stuff we run in prod so you could take that and put it on",
    "start": "1482720",
    "end": "1488000"
  },
  {
    "text": "your cluster and get real results out of it so without being too abstract here or rather not to be too abstract here and",
    "start": "1488000",
    "end": "1494080"
  },
  {
    "text": "to get to something concrete that you can take back home i'm going to give you a taste of how long this takes to run for uh you know a",
    "start": "1494080",
    "end": "1500559"
  },
  {
    "start": "1495000",
    "end": "1554000"
  },
  {
    "text": "small part of our workload daily so updating the intermediate takes about a minute a day and that's taking",
    "start": "1500559",
    "end": "1506799"
  },
  {
    "text": "several billion rows of raw data and aggregating them down to that frequency intermediate table so it takes about 65 seconds so we go",
    "start": "1506799",
    "end": "1513600"
  },
  {
    "text": "from that couple of billion to just under a billion rows that get added to this table every day",
    "start": "1513600",
    "end": "1519440"
  },
  {
    "text": "then we compute the cohorts for 90 days so we scan that table of 70 billion rows and we compute",
    "start": "1519440",
    "end": "1526080"
  },
  {
    "text": "people's cohorts and that takes just about 10 minutes so we're going from 70 billion rows",
    "start": "1526080",
    "end": "1532400"
  },
  {
    "text": "down to uh you know maybe 20 billion rows right because i said for a quarter we have 20 billion unique user ids so i",
    "start": "1532400",
    "end": "1538880"
  },
  {
    "text": "gotta roughly get it down to that first and then finally doing that final aggregation to go from",
    "start": "1538880",
    "end": "1544960"
  },
  {
    "text": "20 billion rows to a couple of million output rows for our analysis takes about 11 seconds so",
    "start": "1544960",
    "end": "1551919"
  },
  {
    "text": "all in all takes 12 minutes not bad and in the context of our full workload where we run it for six date ranges and",
    "start": "1551919",
    "end": "1557600"
  },
  {
    "start": "1554000",
    "end": "1583000"
  },
  {
    "text": "two different types of groupings and all of our clients at once um you know hundreds of advertisers at",
    "start": "1557600",
    "end": "1563039"
  },
  {
    "text": "once takes about two and a half hours on uh an eight a big eight big node ssd",
    "start": "1563039",
    "end": "1569679"
  },
  {
    "text": "cluster um and you know just for perspective on demand that's gonna cost you a cool 96 dollars",
    "start": "1569679",
    "end": "1574799"
  },
  {
    "text": "and yeah it's cool to attach a uh a dollar amount to this but we learned more than the fact that this",
    "start": "1574799",
    "end": "1581440"
  },
  {
    "text": "is actually cheap to do we actually learned that it's cheap and really really really easy to do",
    "start": "1581440",
    "end": "1586559"
  },
  {
    "text": "so these massive multi-stage aggregations are actually our bread and butter they're so easy and so fast to implement",
    "start": "1586559",
    "end": "1592640"
  },
  {
    "text": "correctly that we use them everywhere they're the gimmes in all of our reports this is the simplest possible thing we",
    "start": "1592640",
    "end": "1597919"
  },
  {
    "text": "can do and it's quick it's easy it's straightforward and i barely think about it when i'm computing the complexity",
    "start": "1597919",
    "end": "1604080"
  },
  {
    "text": "or the cost of a marginal report these are the things i tack on to make reports possible this is",
    "start": "1604080",
    "end": "1609279"
  },
  {
    "text": "this is the simple stuff so um we learned that hey don't be afraid of big aggregations it's really quite straightforward to do",
    "start": "1609279",
    "end": "1616000"
  },
  {
    "text": "them and they run pretty quickly so that was our first problem frequency let's move on to our next one which was attribution which asked",
    "start": "1616000",
    "end": "1622000"
  },
  {
    "start": "1620000",
    "end": "1655000"
  },
  {
    "text": "how much should i pay for ads or rather what is the value of a particular ad",
    "start": "1622000",
    "end": "1627360"
  },
  {
    "text": "and at the end of the day uh most of these problems uh are are the closest that any any",
    "start": "1627360",
    "end": "1634000"
  },
  {
    "text": "problem in this class boils down to the closest thing ad tech has to a religious war everybody has their",
    "start": "1634000",
    "end": "1639600"
  },
  {
    "text": "opinions everybody has their models i really try to stay out of it i try to",
    "start": "1639600",
    "end": "1645039"
  },
  {
    "text": "build the intermediates and the building blocks for analyzing all kinds of different models so that my data scientists can go fight that",
    "start": "1645039",
    "end": "1651279"
  },
  {
    "text": "religious war elsewhere right so if we think about most of these",
    "start": "1651279",
    "end": "1657120"
  },
  {
    "start": "1655000",
    "end": "1698000"
  },
  {
    "text": "models they actually boil down to something straightforward which is sessionization which is a problem as old as the internet right",
    "start": "1657120",
    "end": "1663440"
  },
  {
    "text": "basically you're trying to look at each user's activity as a timeline and uh and and kind of go backwards in",
    "start": "1663440",
    "end": "1670799"
  },
  {
    "text": "time and say i'm going to start at my conversion and a conversion can be anything it could be a purchase it can be a sign up it can be requesting a quote",
    "start": "1670799",
    "end": "1676799"
  },
  {
    "text": "i've represented it with a dollar sign here visually and you're going back in time and looking at all the ads and all",
    "start": "1676799",
    "end": "1682159"
  },
  {
    "text": "the activities people took before they bought something to better understand what was the influencer for that purchase",
    "start": "1682159",
    "end": "1688240"
  },
  {
    "text": "so like i said on the far right you have the dollar sign and then as you go to the left you have you know for instance three ads that",
    "start": "1688240",
    "end": "1694559"
  },
  {
    "text": "that person saw before purchasing so what does this look like in terms of sql",
    "start": "1694559",
    "end": "1700320"
  },
  {
    "start": "1698000",
    "end": "1732000"
  },
  {
    "text": "it's really straightforward you join the views onto the conversions such that a conversion happens after an",
    "start": "1700320",
    "end": "1706159"
  },
  {
    "text": "impression and you make sure it's for the same user and you move along with your life right this basically builds that timeline for",
    "start": "1706159",
    "end": "1712159"
  },
  {
    "text": "all of our users at once the real challenge is not this it's what you do on top of this",
    "start": "1712159",
    "end": "1717679"
  },
  {
    "text": "right and for us we basically settled on a class of models that we wanted to analyze called counting models and",
    "start": "1717679",
    "end": "1724080"
  },
  {
    "text": "and these these counting models are basically any model that takes into account the proximity of an impression",
    "start": "1724080",
    "end": "1730000"
  },
  {
    "text": "to the final conversion and we can talk about proximity in in two different ways concretely one is",
    "start": "1730000",
    "end": "1736159"
  },
  {
    "text": "the position of an ad relative to the conversion so we can say that the rank in the timeline is an",
    "start": "1736159",
    "end": "1742000"
  },
  {
    "text": "important thing to analyze or we could talk about proximity in terms of time",
    "start": "1742000",
    "end": "1747279"
  },
  {
    "start": "1744000",
    "end": "1767000"
  },
  {
    "text": "and usually these two things line up but they're slightly different you can say i want to know what the hour offset you",
    "start": "1747279",
    "end": "1752480"
  },
  {
    "text": "know between the conversion the impression is because ads that got shown nearer to the conversion are more valuable you know in my mental model",
    "start": "1752480",
    "end": "1759600"
  },
  {
    "text": "than others right so these are two simple enough ideas let's see how we implement them on top of the sessionization that i",
    "start": "1759600",
    "end": "1765440"
  },
  {
    "text": "introduced just a moment ago so there's a there's a lot on this slide um and oh good it's not cut off uh",
    "start": "1765440",
    "end": "1774320"
  },
  {
    "start": "1767000",
    "end": "1839000"
  },
  {
    "text": "most of this query is the boilerplate i showed you earlier right there's only one part that really matters and it's that window function at the",
    "start": "1774320",
    "end": "1781200"
  },
  {
    "text": "bottom of the select those two lines of the code are basically the meat of the whole thing if you've never messed around with window",
    "start": "1781200",
    "end": "1786720"
  },
  {
    "text": "functions by the way please go do that they're one of the most powerful parts of sql redshift supports them very well go out and use them so",
    "start": "1786720",
    "end": "1794159"
  },
  {
    "text": "basically those two lines are the ones that compute the position argument and then the date diff before",
    "start": "1794159",
    "end": "1800000"
  },
  {
    "text": "it is what produces the hour offset so now you know assuming i run this in production it's going to take a couple",
    "start": "1800000",
    "end": "1806559"
  },
  {
    "text": "of hundred billion rows for a quarter of raw impressions and join them to a couple of hundred million rows of",
    "start": "1806559",
    "end": "1812159"
  },
  {
    "text": "conversions and then annotate them with the hour difference between the conversion and",
    "start": "1812159",
    "end": "1817279"
  },
  {
    "text": "the impression as well as their position in the timeline and this outputs single digit billions",
    "start": "1817279",
    "end": "1823120"
  },
  {
    "text": "of output rows and we do this in in not much time at all tens of minutes it really kind of depends on the day",
    "start": "1823120",
    "end": "1829440"
  },
  {
    "text": "um but you know know that you can do this in about half an hour if you wanted to on data scales",
    "start": "1829440",
    "end": "1834480"
  },
  {
    "text": "of that size so now we have our sessions table right that's been populated by this query and we would like to know what",
    "start": "1834480",
    "end": "1841840"
  },
  {
    "start": "1839000",
    "end": "1945000"
  },
  {
    "text": "statistics we can compute on top of it so you could do something extremely straightforward and naive and",
    "start": "1841840",
    "end": "1846880"
  },
  {
    "text": "say oh sorry that's right um you could basically say hey i want to compute what uh that",
    "start": "1846880",
    "end": "1853840"
  },
  {
    "text": "this certain site appears on average in the nth position of users timelines right and that's just average of",
    "start": "1853840",
    "end": "1859760"
  },
  {
    "text": "position very straightforward to understand right or i could do something a little more subtle",
    "start": "1859760",
    "end": "1864799"
  },
  {
    "text": "and and replicate the last touch attribution model which gives all of the credit for an ad to the last ad that was shown",
    "start": "1864799",
    "end": "1871679"
  },
  {
    "text": "or last all the credit for a conversion to the last ad that was shown and so what happens when i when i evaluate the position equals one",
    "start": "1871679",
    "end": "1878000"
  },
  {
    "text": "expression it gives me a zero or one boolean flag and so if you were the last thing in the timeline you get all the revenue great",
    "start": "1878000",
    "end": "1884000"
  },
  {
    "text": "cool or you could do something even more sophisticated and say that on average",
    "start": "1884000",
    "end": "1891200"
  },
  {
    "text": "before this site a user sees n distinct sites right so if if you have",
    "start": "1891200",
    "end": "1896960"
  },
  {
    "text": "some notion of fatigue right somebody sees an ad or just the opposite that it's it's building up a trust in them that they",
    "start": "1896960",
    "end": "1902720"
  },
  {
    "text": "see this ad across the internet you could weight things appropriately based on that statistic",
    "start": "1902720",
    "end": "1908080"
  },
  {
    "text": "so basically you know these are some straightforward examples but pretty much anything the data scientists want i can implement in here",
    "start": "1908080",
    "end": "1914799"
  },
  {
    "text": "and pretty quickly so since i'm only running this portion of the query over those last few",
    "start": "1914799",
    "end": "1920080"
  },
  {
    "text": "single-digit billions of rows um takes like 10 seconds to run this on a couple of billion rows and that's",
    "start": "1920080",
    "end": "1926480"
  },
  {
    "text": "really incredible i don't like i can't overstate that we we use dozens of different things",
    "start": "1926480",
    "end": "1932159"
  },
  {
    "text": "in this in this way um we compute dozens of different statistics and it runs in 10 seconds we spend all this time",
    "start": "1932159",
    "end": "1937440"
  },
  {
    "text": "building the sessions table and then it takes 10 seconds 10 seconds it's really anticlimactic in a way",
    "start": "1937440",
    "end": "1942640"
  },
  {
    "text": "so um for our production workloads we basically have 45 days of conversions",
    "start": "1942640",
    "end": "1949600"
  },
  {
    "start": "1945000",
    "end": "1966000"
  },
  {
    "text": "we have 90 days worth of impressions because you have to look 45 more days back we compute about 11 different statistics",
    "start": "1949600",
    "end": "1955840"
  },
  {
    "text": "for all of our clients and we do that in two hours right and that's 76 bucks on demand like that's really",
    "start": "1955840",
    "end": "1960960"
  },
  {
    "text": "cool that you can go back and analyze 90 days worth of data in two hours on a cluster of that size so what did we",
    "start": "1960960",
    "end": "1967760"
  },
  {
    "start": "1966000",
    "end": "2033000"
  },
  {
    "text": "learn from building our attribution query well we found out that window functions are really the bee's",
    "start": "1967760",
    "end": "1973360"
  },
  {
    "text": "knees uh we love them you can sessionize the data with them you can compute really interesting statistics on top of the sessionized",
    "start": "1973360",
    "end": "1979440"
  },
  {
    "text": "data with them and the best part of it is it's two lines of code i don't have to worry about the implementation of that i just",
    "start": "1979440",
    "end": "1984799"
  },
  {
    "text": "tell it what i want and it does it and that's pretty magic i don't know if you've managed software engineering teams but",
    "start": "1984799",
    "end": "1990720"
  },
  {
    "text": "the the devil is in the details implementation is always where things get caught up and and this takes a lot of",
    "start": "1990720",
    "end": "1996480"
  },
  {
    "text": "that out so it's really powerful another thing we learned was that if your sort key contains all of the",
    "start": "1996480",
    "end": "2002720"
  },
  {
    "text": "columns that are going to be in your window function there is very minimal overhead to running a window function function on",
    "start": "2002720",
    "end": "2009120"
  },
  {
    "text": "top of enormous tables in fact our data science team writes and designs and proposes brand new window",
    "start": "2009120",
    "end": "2014880"
  },
  {
    "text": "functions for use in this query all without me they just do it themselves they send it forward do we do a code",
    "start": "2014880",
    "end": "2021200"
  },
  {
    "text": "review we run a few tests but by and large they're they're self-serve and that's really amazing too it's",
    "start": "2021200",
    "end": "2026880"
  },
  {
    "text": "increased our productivity drastically so we talked about attribution that was our second question",
    "start": "2026880",
    "end": "2032080"
  },
  {
    "text": "i'm going to move on to our third question which was overlap which was the question of how do i reach these people i'm interested",
    "start": "2032080",
    "end": "2038159"
  },
  {
    "start": "2033000",
    "end": "2057000"
  },
  {
    "text": "in for less money somewhere else and the question amounts to counting the co-occurrences of people on different",
    "start": "2038159",
    "end": "2045279"
  },
  {
    "text": "sites specifically we'd love to present a table that basically showed you where you saw people on site a versus",
    "start": "2045279",
    "end": "2052240"
  },
  {
    "text": "site or what people you saw on both side a and site b and told you which one is cheaper to buy on so let's walk through",
    "start": "2052240",
    "end": "2058878"
  },
  {
    "start": "2057000",
    "end": "2105000"
  },
  {
    "text": "this table concretely you'll notice that's supposed to be lined up on the 90",
    "start": "2058879",
    "end": "2064079"
  },
  {
    "text": "but in any case 90 of the people you see on c right that's why it's red 90",
    "start": "2064079",
    "end": "2070079"
  },
  {
    "text": "of those people you also see on b right so and that's supposed to be also",
    "start": "2070079",
    "end": "2076398"
  },
  {
    "text": "lined up on the bottom so i'm actually going to go back a few slides so you can see it clearly i'm saying that 90 of people you see on",
    "start": "2076399",
    "end": "2082720"
  },
  {
    "text": "c also appear on b in addition to that it costs 9.50 to see",
    "start": "2082720",
    "end": "2087760"
  },
  {
    "text": "those people on c but it only costs a dollar and five cents to see those people on b",
    "start": "2087760",
    "end": "2092878"
  },
  {
    "text": "so really the actionable advice we can take away from this table is that hey stop spending money on c",
    "start": "2092879",
    "end": "2098560"
  },
  {
    "text": "spend money on b you're gonna see the same people anyways so how do we compute this sorry i'm going to jump ahead here we",
    "start": "2098560",
    "end": "2105280"
  },
  {
    "start": "2105000",
    "end": "2226000"
  },
  {
    "text": "compute it with a self join on the impressions table but to clarify things",
    "start": "2105280",
    "end": "2111280"
  },
  {
    "text": "mentally for you i'm going to present this in pieces so first we need to know where every person",
    "start": "2111280",
    "end": "2117200"
  },
  {
    "text": "was seen right which site they were seen on and what's their user id right and then we're going to join this thing",
    "start": "2117200",
    "end": "2123040"
  },
  {
    "text": "onto itself right and you'll notice a weird condition here",
    "start": "2123040",
    "end": "2128240"
  },
  {
    "text": "for the for the join key um it's greater than instead of being an ecwid join",
    "start": "2128240",
    "end": "2133359"
  },
  {
    "text": "and what that does is actually prevents us from computing the the diagonal row and the bottom half of",
    "start": "2133359",
    "end": "2139440"
  },
  {
    "text": "the table i'm not going to get into what you could put in that bottom half of the table just know that you don't want to double count so you",
    "start": "2139440",
    "end": "2144720"
  },
  {
    "text": "need to have this condition there but you do this self-join and again you may be thinking hey",
    "start": "2144720",
    "end": "2150560"
  },
  {
    "text": "that intermediate table that you presented in the previous slide is also keyed on user id so for a",
    "start": "2150560",
    "end": "2156400"
  },
  {
    "text": "quarter again that's 20 billion rows but again i'm going to have the same conversation with you about hey in real life it's way more than 20",
    "start": "2156400",
    "end": "2163520"
  },
  {
    "text": "billion rows because you have to segment by date ranges and campaign and site and so on and so",
    "start": "2163520",
    "end": "2168880"
  },
  {
    "text": "just like the frequency intermediate you end up with a table that has 70 billion rows for a quarter that you're going to try to self-join on",
    "start": "2168880",
    "end": "2175200"
  },
  {
    "text": "and that's a ton of work right so what we do is in the same vein",
    "start": "2175200",
    "end": "2180320"
  },
  {
    "text": "we materialize this intermediate and speeds things up and you can incrementally build it and it has all the right sorkies i won't bore you with",
    "start": "2180320",
    "end": "2185760"
  },
  {
    "text": "that the final query amounts to um basically taking that intermediate and",
    "start": "2185760",
    "end": "2191760"
  },
  {
    "text": "crushing it down to user id site and campaign and then self-joining it onto onto there and then we widen the",
    "start": "2191760",
    "end": "2198320"
  },
  {
    "text": "aggregation at the bottom there so for an advertiser for a particular campaign can see what the overlap between the",
    "start": "2198320",
    "end": "2204079"
  },
  {
    "text": "different sites they purchased on was and that's that's a big deal right this is the the fundamental question of",
    "start": "2204079",
    "end": "2209839"
  },
  {
    "text": "where can i go spend less money and get the same reach and uh sorry so that was that",
    "start": "2209839",
    "end": "2217440"
  },
  {
    "text": "that was to point out the fact that this is now segmented by campaign in this query rather than just being",
    "start": "2217440",
    "end": "2222480"
  },
  {
    "text": "across all of your campaigns at once so um in production the workload",
    "start": "2222480",
    "end": "2228160"
  },
  {
    "text": "looks much the same as frequency right we spend a minute updating that intermediate we spend about 15",
    "start": "2228160",
    "end": "2233280"
  },
  {
    "text": "minutes computing the co occurrences right that that self joins about 15 minutes and then taking the output of that self join",
    "start": "2233280",
    "end": "2239280"
  },
  {
    "text": "and aggregating it takes another 50 seconds so all in all let's say it's 16 minutes and that's a that's a pretty good deal for us",
    "start": "2239280",
    "end": "2245040"
  },
  {
    "text": "17 minutes and uh for our entire workload of again six date ranges and",
    "start": "2245040",
    "end": "2250400"
  },
  {
    "text": "three groupings and blah blah blah again 96 dollars really cool and what we learned from doing",
    "start": "2250400",
    "end": "2256000"
  },
  {
    "text": "this was one if you correctly sort yourself joins there's actually very little overhead to",
    "start": "2256000",
    "end": "2261920"
  },
  {
    "text": "them right so if we think back to the frequency calculation that was a simple multi-stage aggregation there",
    "start": "2261920",
    "end": "2267119"
  },
  {
    "text": "were no joins there it's conceptually a simpler query but what ends up happening is that it runs in about 12 minutes and this ran in",
    "start": "2267119",
    "end": "2273359"
  },
  {
    "text": "17 minutes so it's kind of amazing it blows my mind that a well-sorted multi-stage",
    "start": "2273359",
    "end": "2278400"
  },
  {
    "text": "aggregation is actually roughly the same cost as a well-sorted self-joint and if you think about it for",
    "start": "2278400",
    "end": "2284480"
  },
  {
    "text": "a while you'll realize that because you've sorted it and distributed it properly there's no shuffling of data",
    "start": "2284480",
    "end": "2290079"
  },
  {
    "text": "so you just do a bunch of sequential i o and you take little blocks of the data and you basically do a cartesian product",
    "start": "2290079",
    "end": "2296000"
  },
  {
    "text": "on a small block at a time so there's no big hash tables in memory you don't carry a bunch of state around you can just compute this whole thing",
    "start": "2296000",
    "end": "2302079"
  },
  {
    "text": "incrementally as you scan a row so conceptually again you're scanning this table with some small overhead a couple of minutes on",
    "start": "2302079",
    "end": "2308880"
  },
  {
    "text": "tens of billions of rows of input so that's what we learned from computing overlap and it's particularly useful",
    "start": "2308880",
    "end": "2315920"
  },
  {
    "text": "for us so we'll go to our fourth in most vexing problem which was ad-hoc queries and",
    "start": "2315920",
    "end": "2323359"
  },
  {
    "start": "2317000",
    "end": "2355000"
  },
  {
    "text": "for us we simply could not afford the distraction from our core product that servicing ad hoc queries would",
    "start": "2323359",
    "end": "2329119"
  },
  {
    "text": "require you need people you need time um and and then there's always the back and forth with the client that takes up",
    "start": "2329119",
    "end": "2334640"
  },
  {
    "text": "more of their time so we did as we kind of turned the tables around and gave them back their data and said",
    "start": "2334640",
    "end": "2339839"
  },
  {
    "text": "here why don't you run the query um and so we we created this product called the data science developers kit",
    "start": "2339839",
    "end": "2346160"
  },
  {
    "text": "which is basically etl as a service we unify all their data streams we clean it up we pack it up and ship it out at",
    "start": "2346160",
    "end": "2351920"
  },
  {
    "text": "the event level to them so that they can run whatever analyses they want and because our etl systems were already",
    "start": "2351920",
    "end": "2357839"
  },
  {
    "text": "doing most of this work it was it was half solved the next question was uh to find find",
    "start": "2357839",
    "end": "2363280"
  },
  {
    "text": "out what they needed and how to get it to them and so we identified these eight fact tables um you know these are billion row",
    "start": "2363280",
    "end": "2368880"
  },
  {
    "text": "tables and uh 33 metadata tables which you can think of as little dimension tables you know tens of",
    "start": "2368880",
    "end": "2375359"
  },
  {
    "text": "thousands of rows and between those eight fact tables and 33 dimension tables",
    "start": "2375359",
    "end": "2380960"
  },
  {
    "text": "you could answer pretty much any question you had on your advertising data so what we did is we packed these all up",
    "start": "2380960",
    "end": "2386400"
  },
  {
    "text": "and dropped them in an s3 bucket every day that they could load neat right well not so neat because you",
    "start": "2386400",
    "end": "2392000"
  },
  {
    "text": "actually have to figure out how to give them exactly their data you can't ship them somebody else's data so this is actually quite tricky if",
    "start": "2392000",
    "end": "2398800"
  },
  {
    "text": "you've if you've run a business you know that most of your business rules are encoded in a sql database somewhere",
    "start": "2398800",
    "end": "2404160"
  },
  {
    "text": "so deciding whether an entity or an event is owned by one client or another is actually a ton of work",
    "start": "2404160",
    "end": "2410319"
  },
  {
    "text": "and so we started walking back and understanding what what all that business logic did and we found out that",
    "start": "2410319",
    "end": "2415599"
  },
  {
    "text": "it's there were 42 views in a postgres database that did 121 joins over 1100",
    "start": "2415599",
    "end": "2420880"
  },
  {
    "text": "lines of sql to figure out what all of our entities and all of our events",
    "start": "2420880",
    "end": "2426079"
  },
  {
    "text": "and who they are excuse me the mapping between who owned them and what they were so um the prospect of identically",
    "start": "2426079",
    "end": "2432400"
  },
  {
    "text": "replicating this logic in another system let alone another system that didn't support sql was like",
    "start": "2432400",
    "end": "2438240"
  },
  {
    "text": "okay you're in for a real good time um and so we decided obviously since i'm giving a talk about redshift that hey sql supports",
    "start": "2438240",
    "end": "2445359"
  },
  {
    "text": "redshift so let's find a way to get that logic into redshift and then we'll use redshift to perform those joins",
    "start": "2445359",
    "end": "2451200"
  },
  {
    "text": "and get those entities and events out to the user um and and so anytime somebody says i'm",
    "start": "2451200",
    "end": "2457839"
  },
  {
    "text": "going to replicate logic from one database system to another you should be running for the hills ad hoc replication is no joke it will",
    "start": "2457839",
    "end": "2463839"
  },
  {
    "text": "get you fired sooner than later but i'm going to show you how you can use a lot of reliable open source tools",
    "start": "2463839",
    "end": "2469520"
  },
  {
    "text": "to actually synchronize say something like a postgres database on premises with redshift and get that logic in",
    "start": "2469520",
    "end": "2475280"
  },
  {
    "text": "there safely so we basically took a bunch of open source tools that ship with postgres like pg dump and pg",
    "start": "2475280",
    "end": "2481119"
  },
  {
    "text": "restore and what we do is we dump a transactionally safe",
    "start": "2481119",
    "end": "2486960"
  },
  {
    "text": "version of the entire database with pg dump and then using pg restore and it's two different modes schema only and data",
    "start": "2486960",
    "end": "2493200"
  },
  {
    "text": "only we export the schema the ddl that you need to build up all the tables and then we export all",
    "start": "2493200",
    "end": "2498880"
  },
  {
    "text": "the tables as csvs then we put those in s3 and then we load them into our edge of clusters right and",
    "start": "2498880",
    "end": "2504960"
  },
  {
    "text": "now i have a full transactional copy of all of my metadata and all my business rules and all my views",
    "start": "2504960",
    "end": "2510319"
  },
  {
    "text": "in redshift in the exact same state that it was in my postgres database and so obviously we do this on a daily",
    "start": "2510319",
    "end": "2517200"
  },
  {
    "text": "basis right before we run all of our analytics and you could do it more frequently and you",
    "start": "2517200",
    "end": "2523520"
  },
  {
    "text": "could do it via s3 or you could do something neat like copy from ssh and have your redshift cluster ssh to",
    "start": "2523520",
    "end": "2530000"
  },
  {
    "text": "your postgres cluster and then have it dump these things out we actually found that having it in s3",
    "start": "2530000",
    "end": "2536640"
  },
  {
    "text": "is really valuable because you can effectively go back and time travel you can go in and say hey i want an exact snapshot of what",
    "start": "2536640",
    "end": "2542319"
  },
  {
    "text": "happened yesterday so i can debug this problem or so i can go fix a problem that we had yesterday right so you can use all these open",
    "start": "2542319",
    "end": "2549040"
  },
  {
    "text": "source tools uh it's a couple hundred lines of code it's been running for 200-ish days of",
    "start": "2549040",
    "end": "2554880"
  },
  {
    "text": "uptime for us it's bulletproof because pg restore and pg dump and s3 are all",
    "start": "2554880",
    "end": "2560079"
  },
  {
    "text": "bulletproof so i encourage you to use this technique it's battle hardened i know several other companies that do the same thing",
    "start": "2560079",
    "end": "2566000"
  },
  {
    "text": "so anyways to round back we now have all of our data and our metadata replicated into",
    "start": "2566000",
    "end": "2572000"
  },
  {
    "text": "redshift we'd like to get some data out for a particular client well that's as simple as a join so",
    "start": "2572000",
    "end": "2578560"
  },
  {
    "text": "you basically wrap an unload statement around a join on one of your fact tables it drops it right into their s3 buckets",
    "start": "2578560",
    "end": "2584560"
  },
  {
    "text": "and that's pretty magical and the two assurances that your clients get from this is one it's well-formed",
    "start": "2584560",
    "end": "2589760"
  },
  {
    "text": "data right it came out of redshift which has type safety on the columns",
    "start": "2589760",
    "end": "2595200"
  },
  {
    "text": "has null safety you can assert certain things about this in the data phase the database will uh will enforce those for you it's going",
    "start": "2595200",
    "end": "2601200"
  },
  {
    "text": "to be well structured i mean you could load it into redshift which is a good idea or you could load it into any other system i mean csvs are",
    "start": "2601200",
    "end": "2607440"
  },
  {
    "text": "the bread and butter of analytics right everybody's gone through and you know done csv analytics even on their laptop",
    "start": "2607440",
    "end": "2612560"
  },
  {
    "text": "on the command line and also since we as a company are doing",
    "start": "2612560",
    "end": "2617599"
  },
  {
    "text": "all work on all of our clients data at once and we're doing it far more you know far more aggressive things than they are usually we've actually vetted",
    "start": "2617599",
    "end": "2625040"
  },
  {
    "text": "out the scalability of redshift as a system so they don't need to go through a whole big process of figuring out which data warehouse they want to use",
    "start": "2625040",
    "end": "2631760"
  },
  {
    "text": "they just come in the door and they say oh you guys are using redshift and you do it on way more data so we can use redshift too",
    "start": "2631760",
    "end": "2637200"
  },
  {
    "text": "great and that puts a lot of people at ease so unfortunately i can't tell you how many clients use this but just know",
    "start": "2637200",
    "end": "2642240"
  },
  {
    "text": "that the workload for the number of clients adds up to about an hour and a half on that same 8x dw 28x large cluster",
    "start": "2642240",
    "end": "2650000"
  },
  {
    "text": "you know it's not that expensive running a product like this for 55 a day is a pretty good deal if you ask me and the other thing we learned",
    "start": "2650000",
    "end": "2657119"
  },
  {
    "text": "obviously uh is that hey if your business logic is in redshift is in sql keep it in sql you can get",
    "start": "2657119",
    "end": "2663280"
  },
  {
    "text": "into redshift easily and quickly and there are open source tools that are really easy to compose to build these",
    "start": "2663280",
    "end": "2668560"
  },
  {
    "text": "types of replication systems so those were our four questions those were our four solutions",
    "start": "2668560",
    "end": "2674400"
  },
  {
    "text": "let's talk about how redshift made this possible and talk from an operational perspective",
    "start": "2674400",
    "end": "2680400"
  },
  {
    "text": "so if naively we went back through all the slides and looked at the how long do these workloads take and summed them up",
    "start": "2680400",
    "end": "2686319"
  },
  {
    "text": "we would end up with eight and a half hours of execution time right just naively if we put it together and that was on that eight x",
    "start": "2686319",
    "end": "2692400"
  },
  {
    "text": "uh eight times uh eight x large ssd nodes right so whatever you may say that's",
    "start": "2692400",
    "end": "2698160"
  },
  {
    "text": "fast slow i don't really care right we need to deliver our reports in a certain amount of time but it's not eight and a half hours we",
    "start": "2698160",
    "end": "2704079"
  },
  {
    "text": "don't have eight and a half hours right because we're working on you know a day by 8 a.m the next day we",
    "start": "2704079",
    "end": "2709119"
  },
  {
    "text": "have to have reports for the previous day and so at best we have eight hours but that ignores the reality of other etl",
    "start": "2709119",
    "end": "2714880"
  },
  {
    "text": "systems and whatever's on the other end of the redshift pipeline so for us it meant we had five hours to do this all so you say",
    "start": "2714880",
    "end": "2721359"
  },
  {
    "text": "eight and a half hours of execution time um make a bigger cluster right run these workloads on one",
    "start": "2721359",
    "end": "2727680"
  },
  {
    "text": "gigantic cluster and so i doubled the node count and ran a benchmark and so i've broken",
    "start": "2727680",
    "end": "2734720"
  },
  {
    "text": "it out by restore time which means hey you have a snapshot you need to restore it and boot a new cluster every morning there's an hour of",
    "start": "2734720",
    "end": "2741200"
  },
  {
    "text": "maintenance which is effectively hey load the new day's worth of data and do the updates on the intermediates and finally you have the execution time",
    "start": "2741200",
    "end": "2747599"
  },
  {
    "text": "so what do we see from this well for 691 dollars a day i can take down my execution time from eight",
    "start": "2747599",
    "end": "2754079"
  },
  {
    "text": "and a half hours to six hours but that's still not my five hour deadline so i've spent a ton of money",
    "start": "2754079",
    "end": "2759839"
  },
  {
    "text": "right and i could potentially go double this cluster again and see if it goes faster but that wasn't the right solution what",
    "start": "2759839",
    "end": "2765599"
  },
  {
    "text": "we did instead was actually split it out into four clusters right and so the advantage of this were clear",
    "start": "2765599",
    "end": "2773119"
  },
  {
    "text": "i mean obviously we're now done in under our five hour timeline so that's great and you'll notice that we actually saved",
    "start": "2773119",
    "end": "2780160"
  },
  {
    "text": "20 on our total price and how is that possible right because we're we're doing double work on a lot of this we're loading the new day's",
    "start": "2780160",
    "end": "2786400"
  },
  {
    "text": "worth of data on four clusters instead of one cluster we're restoring three of those clusters instead of one",
    "start": "2786400",
    "end": "2792000"
  },
  {
    "text": "cluster so we're doubling up on a lot of time but in reality because we're tailoring",
    "start": "2792000",
    "end": "2797119"
  },
  {
    "text": "certain workloads to certain clusters and we could scale them independently we end up saving money",
    "start": "2797119",
    "end": "2802400"
  },
  {
    "text": "so that's one huge operational advantage the other operational advantage of splitting out workloads this way is that",
    "start": "2802400",
    "end": "2809200"
  },
  {
    "text": "if your monolithic cluster goes down for any reason hey now all four of your reporting workloads are late",
    "start": "2809200",
    "end": "2815200"
  },
  {
    "text": "right instead of hey we had one cluster go down so just that one workload is late it's really valuable to us to know that",
    "start": "2815200",
    "end": "2821359"
  },
  {
    "text": "we have this kind of independence of workloads and then second of all let's say you're migrating from one region to another or",
    "start": "2821359",
    "end": "2828000"
  },
  {
    "text": "you're failing over from one region to another to be able to fail over workloads one at a time is extremely valuable",
    "start": "2828000",
    "end": "2834240"
  },
  {
    "text": "and really opens up a lot of windows operationally right so the lessons we learned uh from",
    "start": "2834240",
    "end": "2840800"
  },
  {
    "start": "2838000",
    "end": "2891000"
  },
  {
    "text": "from operating amazon redshift actually have nothing to do with the workloads they they actually the impressive thing",
    "start": "2840800",
    "end": "2847040"
  },
  {
    "text": "about redshift is that they've taken this thing that was typically impossible to operate or really unwieldy to operate",
    "start": "2847040",
    "end": "2852640"
  },
  {
    "text": "to provision to deploy and so on to orchestrate and they've made it easy and so that made us rethink holistically",
    "start": "2852640",
    "end": "2858000"
  },
  {
    "text": "how we design these systems right i know now that no matter how many workloads or platforms or products i",
    "start": "2858000",
    "end": "2863599"
  },
  {
    "text": "come up with that are different from each other i can run them in parallel and i'm probably going to save money",
    "start": "2863599",
    "end": "2868720"
  },
  {
    "text": "so my parting piece of advice to all of you is to go out there and experiment aggressively with breaking out your",
    "start": "2868720",
    "end": "2874160"
  },
  {
    "text": "workloads in different wretch of clusters run the experiments they don't cost that much and they certainly opened our eyes",
    "start": "2874160",
    "end": "2880880"
  },
  {
    "text": "to an incredible set of of uh of product opportunities that weren't",
    "start": "2880880",
    "end": "2885920"
  },
  {
    "text": "possible in the past so thank you",
    "start": "2885920",
    "end": "2893040"
  }
]