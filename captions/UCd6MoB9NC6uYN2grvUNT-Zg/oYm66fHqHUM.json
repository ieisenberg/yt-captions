[
  {
    "text": "- Hi, welcome to Generative\nAI Foundations on AWS.",
    "start": "1530",
    "end": "6127"
  },
  {
    "text": "My name is Emily Webber. I'm a principal machine\nlearning specialist at Solutions Architect and at AWS.",
    "start": "7050",
    "end": "12795"
  },
  {
    "text": "And today you are gonna\nlearn about generative AI. You've heard about generative AI, you've heard about all sorts of things.",
    "start": "12795",
    "end": "19590"
  },
  {
    "text": "The purpose of this class\nis to dive super deep. So we have, in fact, no\nless than seven topics",
    "start": "19590",
    "end": "27450"
  },
  {
    "text": "that you're gonna get to\nlearn about here today. So they're broken up\ninto different classes.",
    "start": "27450",
    "end": "32490"
  },
  {
    "text": "Each class is about 45 minutes of slides. So you're gonna get to learn\nabout lots of concepts.",
    "start": "32490",
    "end": "39300"
  },
  {
    "text": "You get to dive really deep and\nexplore these complex topics and interests that you may have.",
    "start": "39300",
    "end": "45960"
  },
  {
    "text": "And then we're gonna have a hands-on demo. So each of the 60-minute\nsessions basically,",
    "start": "45960",
    "end": "52620"
  },
  {
    "text": "that you'll be able to\njust watch on YouTube. So you can fast forward, you\ncan slow down, you can pause,",
    "start": "52620",
    "end": "58559"
  },
  {
    "text": "take screenshots, do\nwhatever it is you wanna do, and then you're gonna get\nall the resources, basically.",
    "start": "58560",
    "end": "64260"
  },
  {
    "text": "So all the content you're gonna\nbe able to view and watch, and then step through on your own.",
    "start": "64260",
    "end": "70680"
  },
  {
    "text": "And so with that, let's get started. So today, the session we're\ngonna learn about right now,",
    "start": "70680",
    "end": "78390"
  },
  {
    "text": "this session is what\nare foundation models? What on earth is a foundation model? Where do they come from?",
    "start": "78390",
    "end": "84719"
  },
  {
    "text": "How do they impact generative AI and the end-to-end life cycle for interacting with maintaining,\nupdating, troubleshooting,",
    "start": "84720",
    "end": "93299"
  },
  {
    "text": "foundation models. And in particular, in\nthe hands-on walkthrough, we're gonna take a look at\nfoundation models on AWS",
    "start": "93300",
    "end": "100650"
  },
  {
    "text": "and especially on Amazon SageMaker. You'll notice that there\nis in fact a Llama here on this slide that's 'cause we're all fond",
    "start": "100650",
    "end": "108180"
  },
  {
    "text": "of the Llama models. And you'll see them come up quite a bit. And so, hypothetically\nspeaking, just for fun,",
    "start": "108180",
    "end": "115440"
  },
  {
    "text": "let's say I asked you to learn\neverything on the internet, which right away, you know,\nis basically impossible.",
    "start": "115440",
    "end": "122250"
  },
  {
    "text": "Like, obviously, how could\nsomeone learn everything on the internet? But for the sake of argument,\nlet's say you tried to do it,",
    "start": "122250",
    "end": "128970"
  },
  {
    "text": "so you'd probably look at the structure of the most popular sites on the internet.",
    "start": "128970",
    "end": "134730"
  },
  {
    "text": "Maybe you'd map out some\ntype of decision tree of all of the different\nareas and topics and domains.",
    "start": "134730",
    "end": "140940"
  },
  {
    "text": "And then you try to store your knowledge of these things, right? You might store your notes,",
    "start": "140940",
    "end": "146580"
  },
  {
    "text": "you maybe, you'd wanna store the files. Obviously they're nicely\nstored for you already, but in any case, you'd\nwanna be storing your notes",
    "start": "146580",
    "end": "153000"
  },
  {
    "text": "or something, but it's gonna\ntake a long time, right? The largest bottleneck here is really",
    "start": "153000",
    "end": "159660"
  },
  {
    "text": "just the human time that it would take to, to literally read everything online. And so just to put some\nnumbers here, last I checked,",
    "start": "159660",
    "end": "169260"
  },
  {
    "text": "there were just under 6\nbillion pages on the internet. And actually the average time\nthat a person spends looking",
    "start": "169260",
    "end": "177690"
  },
  {
    "text": "on a website is 52 seconds. So that's pretty short page viewing.",
    "start": "177690",
    "end": "184829"
  },
  {
    "text": "But nonetheless, multiply those together, you get right around 5 billion\nhours that it would take",
    "start": "184830",
    "end": "190980"
  },
  {
    "text": "just to view all of the\npages online and skim them. And now realistically, if you\nwere actually reading this,",
    "start": "190980",
    "end": "197760"
  },
  {
    "text": "obviously you'd spend\nsome time on more pages, some time on less pages. But in any case, if you look\nat how many human hours we work",
    "start": "197760",
    "end": "206210"
  },
  {
    "text": "in a given year, assuming\nI'm working about eight hours a day, maybe I'm working\nfive days in a week,",
    "start": "207570",
    "end": "214440"
  },
  {
    "text": "maybe I'm working 50 weeks in a year, it's gonna take a human\nabout 255,000 years",
    "start": "214440",
    "end": "223040"
  },
  {
    "text": "to read everything online,\nwhich is insane, right? Obviously that's multiple human lifetimes.",
    "start": "224070",
    "end": "231480"
  },
  {
    "text": "And then on top of that, the\ninternet is evolving, right? There's so much information online,",
    "start": "231480",
    "end": "237750"
  },
  {
    "text": "there's so many new creatives,\nnew content, new you know, things that are popping up constantly.",
    "start": "237750",
    "end": "243840"
  },
  {
    "text": "And so that number of\nfiles that you have to read just continues growing.",
    "start": "243840",
    "end": "248553"
  },
  {
    "text": "A foundation model can\ndo this in a few months. So this is why foundation\nmodels are so exciting.",
    "start": "249840",
    "end": "256170"
  },
  {
    "text": "This is why foundation\nmodels are so interesting. They're able to, through\nlarge scale neural networks,",
    "start": "256170",
    "end": "262169"
  },
  {
    "text": "through distributed training, through PyTorch and scripts and what not, they're able to quote unquote\nlearn or read or understand",
    "start": "262170",
    "end": "270780"
  },
  {
    "text": "or parse massive, massive\namounts of information,",
    "start": "270780",
    "end": "275780"
  },
  {
    "text": "massive amounts of data. So this is why foundation\nmodels are so powerful. You can do a lot with foundation models.",
    "start": "276810",
    "end": "283500"
  },
  {
    "text": "Essentially, a foundation model\nis a machine learning model that's designed to cover\nmany different tasks.",
    "start": "283500",
    "end": "290669"
  },
  {
    "text": "And so in traditional\nmachine learning models, you would use say, a classification model",
    "start": "290670",
    "end": "295740"
  },
  {
    "text": "or a regressive model to\nsolve just one or two tasks. And foundation models are\npowerful because first off,",
    "start": "295740",
    "end": "302850"
  },
  {
    "text": "they're trained on so many\ndifferent sets of data and these massive data sets.",
    "start": "302850",
    "end": "308100"
  },
  {
    "text": "And then essentially they\nlearn naturally occurring learning styles from those files online.",
    "start": "308100",
    "end": "317069"
  },
  {
    "text": "So they'll naturally learn classification, they'll naturally learn\nquestion-answering, they'll learn summarization\nand things like this.",
    "start": "317070",
    "end": "324750"
  },
  {
    "text": "And so today you can build applications using foundation models\nfor almost anything.",
    "start": "324750",
    "end": "331470"
  },
  {
    "text": "There's a huge amount of\ncreativity that folks are bringing to design and develop\nnet new foundation models",
    "start": "331470",
    "end": "339840"
  },
  {
    "text": "to incorporate foundation models\ninto existing applications. Design new applications\nfrom NLP to computer vision,",
    "start": "339840",
    "end": "347700"
  },
  {
    "text": "to code generation, audio generation, video generation, search\nsummarization, the gamut, right?",
    "start": "347700",
    "end": "354690"
  },
  {
    "text": "It's a really exciting space. And interestingly enough, many machine learning tasks where again,",
    "start": "354690",
    "end": "363240"
  },
  {
    "text": "years ago we would've looked at this as just a classification task. So you would take a tax, for\nexample, this person says,",
    "start": "363240",
    "end": "371190"
  },
  {
    "text": "I'm not that into this house. It's too expensive and it's too far from the train line, right?",
    "start": "371190",
    "end": "376470"
  },
  {
    "text": "And we put this into a\nmachine learning model. And then this model\nahead of time was trained",
    "start": "376470",
    "end": "383160"
  },
  {
    "text": "to perform binary classification. So it's just labeling\nthat text as good or bad,",
    "start": "383160",
    "end": "390150"
  },
  {
    "text": "positive or negative. And so here the model would return with a negative sentiment.",
    "start": "390150",
    "end": "395490"
  },
  {
    "text": "So it's gonna output sentiment negative, and that's traditional classification.",
    "start": "395490",
    "end": "400980"
  },
  {
    "text": "Today, you can recast this\ntask as a generative task.",
    "start": "400980",
    "end": "405980"
  },
  {
    "text": "So you can take this same\ntext, the same phrase, and then give it an instruction.",
    "start": "406980",
    "end": "413220"
  },
  {
    "text": "So in the prompt for your\nlarge language model, you can just add a prompt that says, classify the sentence into\npositive or negative sentiment.",
    "start": "413220",
    "end": "421770"
  },
  {
    "text": "You can have a more\nfun way of saying this. You can say like the\ninstructions specifically.",
    "start": "421770",
    "end": "428370"
  },
  {
    "text": "So this instruction, you can\nword this however you want to. You can word it as, is\nthis person happy or sad?",
    "start": "428370",
    "end": "435720"
  },
  {
    "text": "Are they likely to buy\nor likely to not buy? You can use really any type\nof prompt that you want",
    "start": "435720",
    "end": "442530"
  },
  {
    "text": "and that you find works\nwell for your use case. And so in any case, you'll\nput this whole prompt",
    "start": "442530",
    "end": "447960"
  },
  {
    "text": "as it's called into the model, and then your agent will\nrespond, classifying it.",
    "start": "447960",
    "end": "454230"
  },
  {
    "text": "And so again, whereas\npreviously we used to use these very discriminative\nsort of static models",
    "start": "454230",
    "end": "460889"
  },
  {
    "text": "that were explicitly trained\non a small number of tasks ahead of time. Now most people are\nusing foundation models",
    "start": "460890",
    "end": "468750"
  },
  {
    "text": "because they're trained\non these massive data sets and are very powerful. And also because they're so\nflexible for different tasks.",
    "start": "468750",
    "end": "478139"
  },
  {
    "text": "So you can take the same\nmodel and ask it to summarize this text or ask it to translate this text",
    "start": "478140",
    "end": "484740"
  },
  {
    "text": "or ask it to stylize this text. And so it's very computationally\nand resource efficient",
    "start": "484740",
    "end": "490770"
  },
  {
    "text": "to have everything bundled\nin this one foundation model, which you can then use for n\nnumber of downstream use cases.",
    "start": "490770",
    "end": "497313"
  },
  {
    "text": "And so there are many ways to\ncustomize a foundation model. Once you pick a foundation\nmodel and start working with it,",
    "start": "498150",
    "end": "506819"
  },
  {
    "text": "most people will want to\ncustomize this in a certain way. There are core trade-offs in\ncustomizing foundation models,",
    "start": "506820",
    "end": "514174"
  },
  {
    "text": "and we're gonna understand those today. So on this X-axis here, you see\nwe have complexity and cost.",
    "start": "514174",
    "end": "520820"
  },
  {
    "text": "So complexity and cost\nare sort of associated with each other in the\nsense that when something",
    "start": "520860",
    "end": "526379"
  },
  {
    "text": "is a lot harder to do, generally it's gonna\ntake more time to do it.",
    "start": "526380",
    "end": "531510"
  },
  {
    "text": "You're gonna need more experts to do it and you're gonna need\nmore compute to do that.",
    "start": "531510",
    "end": "536581"
  },
  {
    "text": "And so that pushes up your cost. At the same time, in many cases\nthis also improves accuracy.",
    "start": "536581",
    "end": "543080"
  },
  {
    "text": "So we're gonna look at a number of different customization techniques.",
    "start": "544140",
    "end": "549420"
  },
  {
    "text": "We're updating and\nmaintaining foundation models that while they can be more\ncomplex and more expensive,",
    "start": "549420",
    "end": "556140"
  },
  {
    "text": "they usually will increase accuracy. And so the first one is\ncalled prompt engineering.",
    "start": "556140",
    "end": "561270"
  },
  {
    "text": "So let's say you pick\nyour Llama or your Falcon or your Titan or your any\ntype of foundation model",
    "start": "561270",
    "end": "570330"
  },
  {
    "text": "you're trying to work with, you pick this and then\nyou send it some prompts, just like we saw on the previous slide.",
    "start": "570330",
    "end": "576360"
  },
  {
    "text": "You send it some\ninstructions, some questions, you send it some prompts,",
    "start": "576360",
    "end": "581430"
  },
  {
    "text": "and you'll quickly start to realize that when you update your\nprompts, you get a better model.",
    "start": "581430",
    "end": "587250"
  },
  {
    "text": "So there's, I'm sorry, you get a better response\nmore than anything. So when you change and\nwhen you hack the prompt",
    "start": "587250",
    "end": "593940"
  },
  {
    "text": "and you stylize the prompt, you can actually develop a\nprompt template eventually.",
    "start": "593940",
    "end": "600090"
  },
  {
    "text": "That is a way to boost the accuracy and the performance of your model. For most customers and\nfor most developers,",
    "start": "600090",
    "end": "607080"
  },
  {
    "text": "that isn't enough. That's the starting points. The next really common phase to move into",
    "start": "607080",
    "end": "613110"
  },
  {
    "text": "is called retrieval augmented generation. We're gonna dive into this\nthroughout the course. So fear not, but retrieval\naugmented generation",
    "start": "613110",
    "end": "621150"
  },
  {
    "text": "or RAG basically refers\nto a pattern where you, your user will ask a question,",
    "start": "621150",
    "end": "627870"
  },
  {
    "text": "they'll type in some type of query, and then we'll take an LLM.",
    "start": "627870",
    "end": "632910"
  },
  {
    "text": "And so we'll take the\nquery and we'll use an LLM to generate embeddings from that query.",
    "start": "632910",
    "end": "639300"
  },
  {
    "text": "And then we're gonna search in\nthe embedding space actually. So we'll look in a corpus of documents,",
    "start": "639300",
    "end": "644730"
  },
  {
    "text": "we'll find the most similar document, retrieve that document, and\nthen generate the response.",
    "start": "644730",
    "end": "650970"
  },
  {
    "text": "Actually we can stylize the\nresponse to the consumer and to the customer based on\nthis document that we have.",
    "start": "650970",
    "end": "658980"
  },
  {
    "text": "And so that's retrieval\naugmented generation, we're gonna have a whole\nclass on it later on. So lots of, lots of ways you\ncan learn more about this.",
    "start": "658980",
    "end": "666510"
  },
  {
    "text": "So retrieval augmented\ngeneration is a great next step from prompt engineering. There is another way, however,",
    "start": "666510",
    "end": "673860"
  },
  {
    "text": "to improve the performance\nof your foundation model specifically for a downstream task.",
    "start": "673860",
    "end": "679200"
  },
  {
    "text": "So like, let's say I'm in healthcare, let's say I'm in financial services, or let's say I'm in\nmedia and entertainment",
    "start": "679200",
    "end": "686580"
  },
  {
    "text": "and I like working with Llama, you know, I enjoy working with Vicuna and Falcon",
    "start": "686580",
    "end": "693720"
  },
  {
    "text": "and all of these open source\nor you know, third party LLMs. But what I really wanna do\nis I wanna take my data sets",
    "start": "693720",
    "end": "701460"
  },
  {
    "text": "and I wanna customize\nthose foundation models so the performance is exactly in line",
    "start": "701460",
    "end": "707340"
  },
  {
    "text": "with what my organization wants to see. So again, prompt engineering\nis a way to do that. Retrieval augmented generation\ncan help you do that.",
    "start": "707340",
    "end": "714600"
  },
  {
    "text": "Fine tuning is another step, it's another way to take\nsamples from your data,",
    "start": "714600",
    "end": "720720"
  },
  {
    "text": "be those large samples in the case of unsupervised pre-trainings,",
    "start": "720720",
    "end": "725970"
  },
  {
    "text": "sort of like a continued\npre-training or domain adaptation. So you can do that or you\ncan take just small records,",
    "start": "725970",
    "end": "732420"
  },
  {
    "text": "you can put them in your\nprompt, which we'll learn about. That's called few shot prompting. You can also actually\nupdate the learnable weights",
    "start": "732420",
    "end": "739649"
  },
  {
    "text": "of the parameters producing a new model that's then fine tuned on your domain",
    "start": "739650",
    "end": "744960"
  },
  {
    "text": "and on your downstream task. Don't worry, we'll learn about all that throughout the class. And then my personal favorite technique,",
    "start": "744960",
    "end": "752610"
  },
  {
    "text": "it's all the way out here. You'll notice this jump. So there's this sort of\nexponential jump here",
    "start": "752610",
    "end": "758760"
  },
  {
    "text": "in complexity and cost,\nbut likewise in accuracy. And so pre-training really it refers",
    "start": "758760",
    "end": "765660"
  },
  {
    "text": "to creating a new foundation model. So it means taking your\nmultiple terabytes of data sets",
    "start": "765660",
    "end": "772140"
  },
  {
    "text": "that you're interested in working, be that in language or be that in vision or some new niche modality that\nyou're developing right now.",
    "start": "772140",
    "end": "779940"
  },
  {
    "text": "Which by the way, let me know if you are, 'cause that's awesome. But in any case, you're creating\nthis new foundation model.",
    "start": "779940",
    "end": "786270"
  },
  {
    "text": "You've got your neural\nnetwork code, your, you know, custom data sets, your\ndistributed training environment,",
    "start": "786270",
    "end": "792390"
  },
  {
    "text": "and you're hacking, this\namazing, brilliant thing. Pre-training is unambiguously the best way",
    "start": "792390",
    "end": "799350"
  },
  {
    "text": "to get a much more accurate model, now that is contingent on you\nbeing able to meet performance",
    "start": "799350",
    "end": "806100"
  },
  {
    "text": "on certain key steps. And we'll learn how to do\nthat throughout this class. So there's a whole lecture\njust on pre-training",
    "start": "806100",
    "end": "814260"
  },
  {
    "text": "on each of these topics actually, so you can learn how to do\nthis really, really well.",
    "start": "814260",
    "end": "819843"
  },
  {
    "text": "And then we come to find out\nthat the best generative models are actually built on human feedback.",
    "start": "821730",
    "end": "828330"
  },
  {
    "text": "So this is something I\nam super passionate about in a past life before moving\ninto computer science,",
    "start": "828330",
    "end": "835350"
  },
  {
    "text": "I was a creative writer actually. And so I spent so many\nwonderful classes learning about",
    "start": "835350",
    "end": "842180"
  },
  {
    "text": "literature and having these\namazing discussions where, you know, 10 people can\nread a book, for example",
    "start": "842370",
    "end": "849420"
  },
  {
    "text": "and we all interpret the\nbook very differently. Or all of us watch a movie",
    "start": "849420",
    "end": "854670"
  },
  {
    "text": "and we interpret the movie differently. Like we see different themes in it,",
    "start": "854670",
    "end": "860790"
  },
  {
    "text": "we see different characters\nthat are interesting to us, we respond to it differently. And so generative AI and generative models",
    "start": "860790",
    "end": "868529"
  },
  {
    "text": "are really powerful when they\naggregate this human feedback at scale.",
    "start": "868530",
    "end": "873690"
  },
  {
    "text": "And so we're actually\ngonna have a whole lecture that's just on this. So the technical term here\nis reinforcement learning",
    "start": "873690",
    "end": "880500"
  },
  {
    "text": "with human feedback\nand it works like this. You start with a generative model.",
    "start": "880500",
    "end": "886320"
  },
  {
    "text": "So this can be a large language model, this can be a computer vision model, this can be a modality star,\nsort of any generative model.",
    "start": "886320",
    "end": "896060"
  },
  {
    "text": "And then you'll, so you'll start\nwith this generative model, and then you'll send in\nyour prompts to this model.",
    "start": "896250",
    "end": "901710"
  },
  {
    "text": "You'll send in, maybe you have 10 prompts, maybe you have a couple thousand prompts.",
    "start": "901710",
    "end": "907529"
  },
  {
    "text": "And these are like directly\nfrom your business. These are directly from your application, directly from your domain.",
    "start": "907530",
    "end": "914070"
  },
  {
    "text": "They really matter. So they can be about\nsummarizing the call transcripts",
    "start": "914070",
    "end": "919770"
  },
  {
    "text": "in your call center. They can be about answering\nquestions in customer service.",
    "start": "919770",
    "end": "925620"
  },
  {
    "text": "They can be about generating\nnew ads or new domains.",
    "start": "925620",
    "end": "930620"
  },
  {
    "text": "They can be about generating blog posts. They can be literally any type of content",
    "start": "932280",
    "end": "937410"
  },
  {
    "text": "that you're trying to create. You can do this. And so you'll catalog a\nnumber of these prompts.",
    "start": "937410",
    "end": "944310"
  },
  {
    "text": "You take the prompts, you send the prompts to\nthis generative model, and you'll find out that the model gives,",
    "start": "944310",
    "end": "950700"
  },
  {
    "text": "can give you many\ndifferent responses, right? So maybe you'll get four\nor five different prompts,",
    "start": "950700",
    "end": "956880"
  },
  {
    "text": "I'm sorry, four or five different\nresponses for each prompt. So you have one prompt, you\nget like five responses.",
    "start": "956880",
    "end": "963480"
  },
  {
    "text": "You're actually gonna send\nall of those responses to the, to your users or to data labelers.",
    "start": "963480",
    "end": "970950"
  },
  {
    "text": "So humans that you'll hire for\nthe task of organizing these",
    "start": "970950",
    "end": "975950"
  },
  {
    "text": "and ranking these. So the humans will pick their favorites, they'll rank these sort of best or worst,",
    "start": "977820",
    "end": "985260"
  },
  {
    "text": "you'll update your training data sets, and then you're actually\ngonna train a reward model.",
    "start": "985260",
    "end": "991850"
  },
  {
    "text": "So again, whole lecture\njust on this topic, but we're gonna learn how\nto train a reward model",
    "start": "991850",
    "end": "997440"
  },
  {
    "text": "that aggregates this\nhuman feedback at scale. And so this is how generative AI models",
    "start": "997440",
    "end": "1005089"
  },
  {
    "text": "are able to sort of get around\nthis really sticky problem of subjective human preferences,",
    "start": "1005090",
    "end": "1011570"
  },
  {
    "text": "like particularly in\nliterature and vision, right? There are so many ways to\ninterpret natural language",
    "start": "1011570",
    "end": "1017660"
  },
  {
    "text": "and interpret images. And so using this sort of\naggregated human feedback",
    "start": "1017660",
    "end": "1023180"
  },
  {
    "text": "at scale, we're then able\nto use this reward model to update and to improve the\noriginal generative model.",
    "start": "1023180",
    "end": "1030799"
  },
  {
    "text": "And so a couple of the\ntechniques we just looked at, we looked at instruction fine tuning,",
    "start": "1030800",
    "end": "1036860"
  },
  {
    "text": "where you take key\ninstructions you care about, and then do sort of a basic\nsupervised fine tuning. And then we looked at this\nreinforcement learning",
    "start": "1036860",
    "end": "1044030"
  },
  {
    "text": "with human feedback lifecycle. And both of these are critical\nways that you can implement",
    "start": "1044030",
    "end": "1051940"
  },
  {
    "text": "and improve your own foundation\nmodels, generally speaking.",
    "start": "1051980",
    "end": "1056453"
  },
  {
    "text": "And so a couple model spotlights for you. So this model spotlight is\nobviously stable diffusion.",
    "start": "1057410",
    "end": "1064279"
  },
  {
    "text": "So in this case, we're sending in a prompt\nto stable diffusion. So we're sending a prompt\nlandscape of the beautiful city",
    "start": "1064280",
    "end": "1071840"
  },
  {
    "text": "of Paris, rebuilt near the\nPacific Ocean in sunny California because why not?",
    "start": "1071840",
    "end": "1077840"
  },
  {
    "text": "With great weather, Sandy\nbeach, palm trees, architecture,",
    "start": "1077840",
    "end": "1082840"
  },
  {
    "text": "et cetera, et cetera. And then we get this sort\nof amazing, amazing output.",
    "start": "1082880",
    "end": "1089380"
  },
  {
    "text": "I did this myself. I went into the SageMaker\nconsole after looking online",
    "start": "1089690",
    "end": "1095810"
  },
  {
    "text": "to find good prompts. So I go look up good\nprompts, copy the prompt, paste it in to my SageMaker\nFoundation model hub,",
    "start": "1095810",
    "end": "1105110"
  },
  {
    "text": "and then I generate this\namazing image, download it, and we're ready to go.",
    "start": "1105110",
    "end": "1110690"
  },
  {
    "text": "You can also, in stable diffusion, you can add negative prompts. So negative prompts are handy",
    "start": "1110690",
    "end": "1117020"
  },
  {
    "text": "because it's a way to\ntell the model like, look, I really don't want it to be\nblurry or I don't want it to be",
    "start": "1117020",
    "end": "1123940"
  },
  {
    "text": "in a certain category. And so here it's funny\nbecause we said no trees and no green, and yet\nclearly we have both trees",
    "start": "1124880",
    "end": "1132830"
  },
  {
    "text": "and both green, but this way it sort of\nminimizes those things. So if we hadn't included\nthese negative prompts,",
    "start": "1132830",
    "end": "1141380"
  },
  {
    "text": "most likely there would\nbe a lot more green. And so this way we were\ngoing for some of that,",
    "start": "1141380",
    "end": "1146660"
  },
  {
    "text": "kind of that sunny motif. And then when you're interacting\nwith foundation models,",
    "start": "1146660",
    "end": "1151940"
  },
  {
    "text": "you're sending in\ndifferent hyper parameters. So you can set the sort of dimensions",
    "start": "1151940",
    "end": "1159050"
  },
  {
    "text": "of the output image that you want. So here I'm giving it a much larger width, so width of 720 because I want\nsort of that landscape view.",
    "start": "1159050",
    "end": "1168170"
  },
  {
    "text": "And then a standard height of 512. If you want a portrait or a square, you can do 512 by 512.",
    "start": "1168170",
    "end": "1174560"
  },
  {
    "text": "And if you want the other\ndirection, then you just, you just rotate it out.",
    "start": "1174560",
    "end": "1178763"
  },
  {
    "text": "Great and then a couple of\nother hyper parameters as well. The guidance scale is interesting",
    "start": "1179600",
    "end": "1183750"
  },
  {
    "text": "because this is a way to\nsort of tell your model",
    "start": "1184940",
    "end": "1189940"
  },
  {
    "text": "how intensely you want it\nto care about the prompt. Like if you want the model\nto really just obsess",
    "start": "1189950",
    "end": "1196460"
  },
  {
    "text": "about the prompt and do nothing else, then you set a higher guidance scale. You max out that guidance scale.",
    "start": "1196460",
    "end": "1202730"
  },
  {
    "text": "If you want the model to be\na little bit more creative, to have some sort of you know, liberties",
    "start": "1202730",
    "end": "1209060"
  },
  {
    "text": "in how it interprets the prompt, then you reduce the guidance scale.",
    "start": "1209060",
    "end": "1214100"
  },
  {
    "text": "It's more common actually to\nsee a very detailed prompt like this with a higher guidance scale.",
    "start": "1214100",
    "end": "1221030"
  },
  {
    "text": "So in this case, it's interesting that I use such a complex prompt",
    "start": "1221030",
    "end": "1226370"
  },
  {
    "text": "with actually a pretty\nliberal guidance scale because this is a lower number. So I'm giving the model more\nfreedom to do what it wants",
    "start": "1226370",
    "end": "1234200"
  },
  {
    "text": "and it still comes back beautiful. The seed is another interesting\nhyper parameter you can set,",
    "start": "1234200",
    "end": "1242059"
  },
  {
    "text": "because setting the seed is sort of a way of giving the model like\na completely new modality",
    "start": "1242060",
    "end": "1248660"
  },
  {
    "text": "to explore. So if you set the seed to\nreally any other number,",
    "start": "1248660",
    "end": "1255020"
  },
  {
    "text": "it will encourage the model to like pick a completely different style\nor a completely different mode,",
    "start": "1255020",
    "end": "1260630"
  },
  {
    "text": "different colors, different\nshapes, different backgrounds. It will still be following your prompts,",
    "start": "1260630",
    "end": "1266543"
  },
  {
    "text": "particularly depending\non the guidance scale. But when you just change the seed, that's an easy way to just\nsort of get a different model",
    "start": "1267380",
    "end": "1275809"
  },
  {
    "text": "or a different response\nrather that you prefer. And so let's take a look at\nlanguage foundation models",
    "start": "1275810",
    "end": "1283430"
  },
  {
    "text": "because these LLMs are\ncertainly the most popular thing to talk about in technology today.",
    "start": "1283430",
    "end": "1290810"
  },
  {
    "text": "But as we'll come to find out, they're not that new actually. Foundation models and\nlanguage foundation models",
    "start": "1290810",
    "end": "1296960"
  },
  {
    "text": "have been around for a long time. So back in 2017, right\nbefore I joined Amazon,",
    "start": "1296960",
    "end": "1303260"
  },
  {
    "text": "actually the transformer\nemerged from the planet",
    "start": "1303260",
    "end": "1308260"
  },
  {
    "text": "whose name I don't remember\nto save the humans, but no, I'm kidding. So the transformer is a\nmachine learning model",
    "start": "1308870",
    "end": "1315770"
  },
  {
    "text": "that is designed to operate\nreally well on sequences. So the core transformer was actually built",
    "start": "1315770",
    "end": "1321950"
  },
  {
    "text": "to handle translation. So it has two parts, an\nencoder and a decoder. So it takes in a string of\ntext and it outputs a string",
    "start": "1321950",
    "end": "1330140"
  },
  {
    "text": "of text originally again done to enhance machine translation. And transformers were interesting",
    "start": "1330140",
    "end": "1337520"
  },
  {
    "text": "because they operated\nreally, really well at scale. So they set a new state of the\nart for machine translation,",
    "start": "1337520",
    "end": "1344660"
  },
  {
    "text": "but more than anything, there were a net new way of thinking about how to learn sequences.",
    "start": "1344660",
    "end": "1349700"
  },
  {
    "text": "So rather than recurrent neural\nnetworks rather than LSTMs, rather than CNNs actually.",
    "start": "1349700",
    "end": "1358460"
  },
  {
    "text": "So yeah, so transformers became\nthis really interesting way of approaching knowledge using",
    "start": "1358460",
    "end": "1364549"
  },
  {
    "text": "this core self attention mechanism. That's a lot of matrix multiplication. And so in any case,",
    "start": "1364550",
    "end": "1371360"
  },
  {
    "text": "the year after that gave us two models that sort of supercharged NLP.",
    "start": "1371360",
    "end": "1376966"
  },
  {
    "text": "And so one of them was of course BERT, the bidirectional encoding\nrepresentation transformers,",
    "start": "1376967",
    "end": "1383030"
  },
  {
    "text": "the BERT model and BERT\nmodels are really useful for classification.",
    "start": "1383030",
    "end": "1388340"
  },
  {
    "text": "BERT models are encoder only, which means it's going to a larger output",
    "start": "1388340",
    "end": "1396260"
  },
  {
    "text": "and produce a smaller, as\nlarger input, excuse me, produces smaller outputs. And BERT models are handy\nagain for classification",
    "start": "1396260",
    "end": "1403880"
  },
  {
    "text": "and for smaller tasks. BERT models tend to fit on single GPUs or single accelerators,\nand they're quite handy.",
    "start": "1403880",
    "end": "1411770"
  },
  {
    "text": "We also saw GPT-1 in 2018, which back then was sort of\ninteresting but not a big deal.",
    "start": "1411770",
    "end": "1417950"
  },
  {
    "text": "And so in any case,\nthere've been many, many, many years of NLP fascination\nwith language models.",
    "start": "1417950",
    "end": "1425830"
  },
  {
    "text": "Year over year we saw\nthese interesting scales of language models where\nresearchers propose",
    "start": "1426680",
    "end": "1434540"
  },
  {
    "text": "these scaling laws to help us just be bold and throw even more data\nand even more accelerators",
    "start": "1434540",
    "end": "1442250"
  },
  {
    "text": "at these models and really\nproduce these amazing results. And so you see that now in 2023,",
    "start": "1442250",
    "end": "1448670"
  },
  {
    "text": "there are a lot of foundation\nmodels and language, whereas previously there were just a few. And so what this timeline tells\nus is that foundation models",
    "start": "1448670",
    "end": "1456980"
  },
  {
    "text": "and large language models\nhave been around for years. There's a very active,\nvery interesting, robust,",
    "start": "1456980",
    "end": "1463070"
  },
  {
    "text": "mature research community\nthat's exploring these and you too can benefit\nfrom that, from them.",
    "start": "1463070",
    "end": "1468559"
  },
  {
    "text": "That's all we're saying here. So one other foundation\nmodel spotlight for you,",
    "start": "1468560",
    "end": "1473570"
  },
  {
    "text": "AI21 is an AWS partner. Their models are available on the SageMaker Foundation model hub.",
    "start": "1473570",
    "end": "1480289"
  },
  {
    "text": "They're also a customer\nof AWS as is Stability for the record. And so in any case, this\nAI21 Jurassic-2 model,",
    "start": "1480290",
    "end": "1490000"
  },
  {
    "text": "jumbo means it's quite large. So jumbo means that it's north\nof 100 billion parameters.",
    "start": "1490580",
    "end": "1497059"
  },
  {
    "text": "And we'll give it a prompt. And so here the prompt I'm\ngiving it is tell me a story",
    "start": "1497060",
    "end": "1503210"
  },
  {
    "text": "about a dog, running down the street and then I click generate text. And this is in the SageMaker\nFoundation model hub,",
    "start": "1503210",
    "end": "1510470"
  },
  {
    "text": "by the way, where we\nhave this nice playground that we'll take a look\nat in the hands-on demo. And so we generate this text\nand then we get this cute story",
    "start": "1510470",
    "end": "1519380"
  },
  {
    "text": "about a dog named Max. He's a very happy dog, he loved to run. Once he was out for a walk, Mr. Jones,",
    "start": "1519380",
    "end": "1527360"
  },
  {
    "text": "Mr. Jones was holding his leash, Max pulled him down the street, Max was excited, he just wanted to run.",
    "start": "1527360",
    "end": "1533539"
  },
  {
    "text": "Mr. Jones was having a\nhard time, et cetera. So it's funny because\nthis sounds like a story,",
    "start": "1533540",
    "end": "1541100"
  },
  {
    "text": "but we don't really\nhave a narrative, right? There's not really a conclusion,\nthere's not a climax. So anyway, there are further\nways to evaluate this,",
    "start": "1541100",
    "end": "1548510"
  },
  {
    "text": "but it's still pretty close\nto being a good story. And then on this side,",
    "start": "1548510",
    "end": "1553610"
  },
  {
    "text": "I'm sort of giving the\nmodel a red herring. So I'm seeing like how complex it can go.",
    "start": "1553610",
    "end": "1560630"
  },
  {
    "text": "So if you found two shoes, one for the right foot\nand one for the left foot, how many shoes would you have?",
    "start": "1560630",
    "end": "1566270"
  },
  {
    "text": "Now obviously we're pretty sure\nthis is gonna be two shoes, but we just wanna make\nsure that the model has",
    "start": "1566270",
    "end": "1572907"
  },
  {
    "text": "this sort of basic common sense. And so if you found two shoes, one for the right foot,\none for the left foot,",
    "start": "1572907",
    "end": "1578149"
  },
  {
    "text": "you would have two shoes, great. So the model is able to sort of read this,",
    "start": "1578150",
    "end": "1582592"
  },
  {
    "text": "somewhat more complex prompt and give us a reasonable answer. So that's good.",
    "start": "1583430",
    "end": "1587923"
  },
  {
    "text": "All right and so the typical\nfoundation model lifecycle starts with picking a\nbase foundation model.",
    "start": "1589400",
    "end": "1596270"
  },
  {
    "text": "And again, the second\nlecture in our series is gonna be just about how to\npick a good foundation model.",
    "start": "1596270",
    "end": "1602140"
  },
  {
    "text": "So we're gonna dive pretty deep into that. So we'll pick a base foundation model according to the domain,\nthe modality, the size,",
    "start": "1602140",
    "end": "1610130"
  },
  {
    "text": "the performance, et cetera\nof that foundation model. Then we're gonna use prompt\nengineering on that model.",
    "start": "1610130",
    "end": "1616640"
  },
  {
    "text": "We'll develop prompt templates, we'll hack the syntax to\nget really good performance",
    "start": "1616640",
    "end": "1622040"
  },
  {
    "text": "and then we'll evaluate the\nperformance with our users. We'll actually send the\nresponses of that model",
    "start": "1622040",
    "end": "1629660"
  },
  {
    "text": "to our users, see if they like it, see what their responses are, store those human preferences,\nfine tune the model.",
    "start": "1629660",
    "end": "1637940"
  },
  {
    "text": "So actually improve those\nbase trainable weights to make the model even more\nperformant in our domain.",
    "start": "1637940",
    "end": "1644510"
  },
  {
    "text": "Then we're gonna update that\noriginal foundation model and put it back into our application. And so in the lecture\nand in the whole class,",
    "start": "1644510",
    "end": "1651679"
  },
  {
    "text": "we'll learn about each of these\nsteps in much more detail. And so with that, let's\ntake a look at the demo.",
    "start": "1651680",
    "end": "1658133"
  },
  {
    "text": "So in this demo we are\ngoing to explore a notebook, as it were.",
    "start": "1659630",
    "end": "1664993"
  },
  {
    "text": "This is gonna be a notebook\nof the Falcon model, which is running on SageMaker Jumpstart.",
    "start": "1664993",
    "end": "1670970"
  },
  {
    "text": "And we're gonna interact with this to learn about text generation. So feel free to follow\nalong with me if you like.",
    "start": "1670970",
    "end": "1678169"
  },
  {
    "text": "The short URL is right\nhere, bit.ly/sm-nb-1.",
    "start": "1678170",
    "end": "1683170"
  },
  {
    "text": "This is already a public\nnotebook we're gonna step through or you are welcome to\njust scan that QR code",
    "start": "1685070",
    "end": "1691820"
  },
  {
    "text": "and have the notebook sent to you in your manner of choosing.",
    "start": "1691820",
    "end": "1697940"
  },
  {
    "text": "So now that you have the\nnotebook, let's get to it.",
    "start": "1697940",
    "end": "1701842"
  },
  {
    "text": "All right, so here we are. As you can see, of course,",
    "start": "1704300",
    "end": "1709340"
  },
  {
    "text": "we're in AWS, sitting\nhere in North Virginia. And this is SageMaker, right?",
    "start": "1709340",
    "end": "1715583"
  },
  {
    "text": "So in SageMaker we have\nthese foundation models, these are models that\nyou can interact with",
    "start": "1717174",
    "end": "1723920"
  },
  {
    "text": "to do all sorts of things,\ndo all sorts of tasks. So some of them are open source models.",
    "start": "1723920",
    "end": "1730340"
  },
  {
    "text": "As you can see we have Falcon\nin a variety of options, some using BF16, actually\nquite a few Falcon using BF16.",
    "start": "1730340",
    "end": "1739610"
  },
  {
    "text": "But in any case, instruct\nmodels and then generic models. By the way, if you're gonna fine tune,",
    "start": "1739610",
    "end": "1746030"
  },
  {
    "text": "feel free to start with models that haven't been instruction fine tuned. If they've already been\ninstruction fine tuned,",
    "start": "1746030",
    "end": "1753110"
  },
  {
    "text": "then you're not gonna wanna\nfine tune that further. But if not, then that's\ngood for fine tuning. But in any case, what's\nhandy about the models here",
    "start": "1753110",
    "end": "1762380"
  },
  {
    "text": "and especially the playground,\nis that we can poke at them.",
    "start": "1762380",
    "end": "1767380"
  },
  {
    "text": "So let's say we wanna work\nwith AI21 as an example",
    "start": "1767630",
    "end": "1772630"
  },
  {
    "text": "of a proprietary model that's available in SageMaker Jumpstart. We'll click view model.",
    "start": "1773330",
    "end": "1779123"
  },
  {
    "text": "And then after we've clicked view model, it's gonna take us to\nthe model details page",
    "start": "1780920",
    "end": "1788000"
  },
  {
    "text": "in just a minute here.",
    "start": "1788000",
    "end": "1789983"
  },
  {
    "text": "Great. All right, so the model details page, and we see this is indeed AI21 Jurassic,",
    "start": "1795380",
    "end": "1801830"
  },
  {
    "text": "and what do you know? There is a playground available. And so the playground is really handy.",
    "start": "1801830",
    "end": "1807230"
  },
  {
    "text": "It's a way that you can\nprompt the model directly. And so essentially this means",
    "start": "1807230",
    "end": "1813050"
  },
  {
    "text": "you're not of course hosting the model, you're just sending it the prompt and getting the response back. And so we can choose a few examples,",
    "start": "1813050",
    "end": "1821180"
  },
  {
    "text": "actually we can choose,\nlet's see, outline creator,",
    "start": "1821180",
    "end": "1825833"
  },
  {
    "text": "and then let's see if we can make this bar a little bit larger. Here we go, great.",
    "start": "1826880",
    "end": "1831973"
  },
  {
    "text": "So we're gonna write sections, we're gonna write sections\nto a great blog post for the following title, how\nto start a personal blog,",
    "start": "1831973",
    "end": "1839570"
  },
  {
    "text": "blog sections. Okay so clearly we have\na few examples here.",
    "start": "1839570",
    "end": "1845360"
  },
  {
    "text": "So this is your few shot prompting, and then now we're asking\nthe model to write sections",
    "start": "1845360",
    "end": "1854740"
  },
  {
    "text": "for this new blog. So let's see what we've got. So we're gonna generate the text.",
    "start": "1855020",
    "end": "1859100"
  },
  {
    "text": "All right and here we have new sections.",
    "start": "1861530",
    "end": "1864233"
  },
  {
    "text": "Great. Okay, so clearly it works. We get content out that seems reasonable.",
    "start": "1867350",
    "end": "1874310"
  },
  {
    "text": "And now let's say that\nwe've interacted with it through the playground, we're ready to move into\nthe notebook experience.",
    "start": "1874310",
    "end": "1881150"
  },
  {
    "text": "That is over here. So in this view, you can see I'm running\non SageMaker studio.",
    "start": "1881150",
    "end": "1889430"
  },
  {
    "text": "What is SageMaker studio you ask? So it's a IDE for machine learning.",
    "start": "1889430",
    "end": "1896060"
  },
  {
    "text": "But beyond that, it actually\nruns lots of compute.",
    "start": "1896060",
    "end": "1901060"
  },
  {
    "text": "So every notebook that\nyou're running on SageMaker is actually, we call it a\nkernel gateway application.",
    "start": "1901370",
    "end": "1909770"
  },
  {
    "text": "What that means is it's a\ndifferent instance actually, where it has the ability to\nrun on a different instance.",
    "start": "1909770",
    "end": "1916160"
  },
  {
    "text": "And so I can change this out. For example, just in your\nnotebook, you can click here,",
    "start": "1916160",
    "end": "1922850"
  },
  {
    "text": "the stop, right? You can give this a click and then actually select\nany of these instances.",
    "start": "1922850",
    "end": "1929930"
  },
  {
    "text": "And now remember, this isn't\nchanging your entire IDE,",
    "start": "1929930",
    "end": "1934313"
  },
  {
    "text": "like the visual that you see here that is provided by a Jupyter server, which is actually built\nand managed by Amazon.",
    "start": "1935540",
    "end": "1943460"
  },
  {
    "text": "And so just to see that\nin the console here. So let's say we go out to the AWS console,",
    "start": "1943460",
    "end": "1950240"
  },
  {
    "text": "and let's check out SageMaker studio. So right up here.",
    "start": "1950240",
    "end": "1955250"
  },
  {
    "text": "And then let's say we\nwant to manage studio",
    "start": "1955250",
    "end": "1958980"
  },
  {
    "text": "that's under domains. And I'm using this diffuse domain",
    "start": "1960260",
    "end": "1966139"
  },
  {
    "text": "and I'm running on this Falcon. And you'll notice that there are",
    "start": "1966140",
    "end": "1971630"
  },
  {
    "text": "a couple different parts here. One part is this Jupyter\nServer application,",
    "start": "1971630",
    "end": "1977900"
  },
  {
    "text": "again, built to managed by Amazon. And this is running your visual here.",
    "start": "1977900",
    "end": "1984200"
  },
  {
    "text": "So the Jupyter Lab experience\nand this whole visual browsing",
    "start": "1984200",
    "end": "1989200"
  },
  {
    "text": "experience that is literally\nrunning on this guy,",
    "start": "1989870",
    "end": "1994870"
  },
  {
    "text": "on this Jupyter server. And so every time we run a notebook,",
    "start": "1996740",
    "end": "2003160"
  },
  {
    "text": "what we're gonna do is\nactually connect that into the Jupyter server. So let's say I wanna\ncreate a new notebook.",
    "start": "2003160",
    "end": "2010123"
  },
  {
    "text": "So let's say I create a new notebook and just for fun, maybe I need a GPU,",
    "start": "2011020",
    "end": "2017860"
  },
  {
    "text": "or maybe I wanna run on\na custom accelerator. So I have all of these instances,",
    "start": "2017860",
    "end": "2024130"
  },
  {
    "text": "I can choose from many\ndifferent options of M-series, C-series, accelerated\ncompute, memory optimized,",
    "start": "2024130",
    "end": "2033673"
  },
  {
    "text": "all sorts of things. And so yeah, so what I'm\nsaying is you can pick from any of these instances\nfor each notebook,",
    "start": "2034660",
    "end": "2041530"
  },
  {
    "text": "actually lemme just pick one to show you. So for each notebook, again,",
    "start": "2041530",
    "end": "2046990"
  },
  {
    "text": "this is gonna be running\non a different machine. And then over here on this left hand side,",
    "start": "2046990",
    "end": "2053050"
  },
  {
    "text": "as the instances come online, you'll start to see\nthem actually show here.",
    "start": "2053050",
    "end": "2058570"
  },
  {
    "text": "And so this will give us an\nindication of the instances",
    "start": "2058570",
    "end": "2063163"
  },
  {
    "text": "that are available in our IDE, I digress. Let's get back to Falcon.",
    "start": "2064480",
    "end": "2069703"
  },
  {
    "text": "So now that we know where we are, you'll know that I downloaded the notebook",
    "start": "2071500",
    "end": "2076659"
  },
  {
    "text": "from the SageMaker examples. So for those of you who\nare following with me, this is the notebook you should see.",
    "start": "2076660",
    "end": "2083936"
  },
  {
    "text": "So SageMaker Jumpstart, text\ngeneration with Falcon models. We're gonna go over here and\nlet's see how far we can get.",
    "start": "2083936",
    "end": "2092139"
  },
  {
    "text": "So first off, you're gonna be\ninstalling the SageMaker SDK, and then we're gonna point to a model.",
    "start": "2092140",
    "end": "2099760"
  },
  {
    "text": "What's interesting about this notebook is that it actually gives\nyou a handy dropdown.",
    "start": "2099760",
    "end": "2104859"
  },
  {
    "text": "So you can see there are\nmany different model IDs. You've got Falcon 40b and then instruct,",
    "start": "2104860",
    "end": "2112750"
  },
  {
    "text": "and then the same for\n7b and then instruct. So again, obviously the instruction ones",
    "start": "2112750",
    "end": "2118390"
  },
  {
    "text": "have already been instruction fine tuned, and the base ones have not.",
    "start": "2118390",
    "end": "2122413"
  },
  {
    "text": "And then we have this\ncute little dropdown here that lets us pick the model\nwe'd like to interact with.",
    "start": "2123640",
    "end": "2131080"
  },
  {
    "text": "So I'm gonna interact\nwith the 7b instruct, and then we can just\nconfirm that yes, indeed",
    "start": "2131080",
    "end": "2137980"
  },
  {
    "text": "that is the right one. And let's just show you here. So the model ID that I'm\ninteracting with is this one.",
    "start": "2137980",
    "end": "2145870"
  },
  {
    "text": "Yeah, so we've got the Falcon 7b instruct, and then we're gonna do our SageMaker.",
    "start": "2145870",
    "end": "2152230"
  },
  {
    "text": "One line model.deploys. So this is scarily easy because\nit's already in jumpstart.",
    "start": "2152230",
    "end": "2159935"
  },
  {
    "text": "So because this model\nalready is packaged nicely",
    "start": "2160090",
    "end": "2165090"
  },
  {
    "text": "to be hosted on SageMaker, we can just hit this\none line model.deploy,",
    "start": "2165130",
    "end": "2170680"
  },
  {
    "text": "and then the predictor comes up. Now I've already done this, so I will avoid the wait time here.",
    "start": "2170680",
    "end": "2177400"
  },
  {
    "text": "You see this, this took a good\n16 minutes to turn online. So do be patient if you're\nrunning this at home.",
    "start": "2177400",
    "end": "2185383"
  },
  {
    "text": "And then the notebook\nauthors were very helpful and indicated different instances that have been tested\nwith the Falcon model.",
    "start": "2186310",
    "end": "2196290"
  },
  {
    "text": "And so we see 7b is on the\ng5 across a couple options.",
    "start": "2196480",
    "end": "2201480"
  },
  {
    "text": "And then the p4d, so\ndifferent varieties for you.",
    "start": "2201850",
    "end": "2206850"
  },
  {
    "text": "And then the 40b as well,\nsome of the larger g5s. And then of course the p4d.",
    "start": "2207040",
    "end": "2213369"
  },
  {
    "text": "Pro tip, make sure you pick\nthe smallest instance you can. If you're new to AWS,",
    "start": "2213370",
    "end": "2218800"
  },
  {
    "text": "that means going with\nthe smaller number here. So a smaller T-shirt size\nif you will, the small size,",
    "start": "2218800",
    "end": "2227070"
  },
  {
    "text": "that means the instance\nis literally smaller, it's gonna have fewer CPUs,",
    "start": "2227530",
    "end": "2232990"
  },
  {
    "text": "it's gonna have fewer accelerators. And everything about\nit is gonna be smaller.",
    "start": "2232990",
    "end": "2238240"
  },
  {
    "text": "The amount of bandwidth that sees, if there's any instant storage,\nthat's gonna be smaller.",
    "start": "2238240",
    "end": "2243820"
  },
  {
    "text": "And so as a corollary,\nwhen you pick a larger one, so that 48 that's there,\nthere's gonna be more CPU.",
    "start": "2243820",
    "end": "2253240"
  },
  {
    "text": "It also means the pricing is heftier with the larger instances and the pricing is smaller\nwith the smaller instances.",
    "start": "2253240",
    "end": "2259840"
  },
  {
    "text": "And so you always wanna pick\nthe smallest instance you can,",
    "start": "2259840",
    "end": "2264133"
  },
  {
    "text": "generally speaking as a\nway to keep costs low. And that's what we're gonna do here.",
    "start": "2265090",
    "end": "2270550"
  },
  {
    "text": "And so they have a couple notes for you about changing the number of\nGPUs, which is very handy.",
    "start": "2270550",
    "end": "2277390"
  },
  {
    "text": "And then here, actually I like this. So if you are using a larger instance,",
    "start": "2277390",
    "end": "2282789"
  },
  {
    "text": "which sometimes you wanna do because maybe you're\nmaxing out throughputs",
    "start": "2282790",
    "end": "2288970"
  },
  {
    "text": "or you're testing\ndifferent hyper parameters that actually need more infrastructure. And so you'll need a larger instance.",
    "start": "2288970",
    "end": "2295390"
  },
  {
    "text": "So if you're gonna do that, just make sure you set this parameter. So my model.environment,",
    "start": "2295390",
    "end": "2302109"
  },
  {
    "text": "and then just increase the\nnumber of GPUs right there. All right, and so now theoretically,",
    "start": "2302110",
    "end": "2308230"
  },
  {
    "text": "this model should be deployed. And actually we don't have\nto be theoretical about this. We can just check.",
    "start": "2308230",
    "end": "2314050"
  },
  {
    "text": "So I'm gonna go up to\nthis little home folder and let's go down to deployments.",
    "start": "2314050",
    "end": "2320050"
  },
  {
    "text": "Let's see what endpoints we have. I'm gonna close that out. And lo and behold, we do\nindeed have endpoints.",
    "start": "2320050",
    "end": "2327970"
  },
  {
    "text": "This is great. So this is the SageMaker\nexample, hugging face. And then what do you know,\nthe hugging face LLM,",
    "start": "2327970",
    "end": "2335350"
  },
  {
    "text": "Falcon 7b instruct handy. Let's do it.",
    "start": "2335350",
    "end": "2340833"
  },
  {
    "text": "One, two, three, take a breath. Let's roll.",
    "start": "2343480",
    "end": "2345723"
  },
  {
    "text": "Great. Okay, so here's our prompt. This is the prompt we send to the model.",
    "start": "2348520",
    "end": "2353770"
  },
  {
    "text": "Tell me about Amazon SageMaker. You'll see we're putting\nthat in this payload object.",
    "start": "2353770",
    "end": "2358960"
  },
  {
    "text": "So our inputs are indeed this texturing and then the parameters.",
    "start": "2358960",
    "end": "2364060"
  },
  {
    "text": "So how the model should\ninteract with this prompt.",
    "start": "2364060",
    "end": "2369060"
  },
  {
    "text": "Those of you who are\nmore familiar with say, playground experiences, you\nmight not be comfortable",
    "start": "2369430",
    "end": "2375940"
  },
  {
    "text": "with these parameters and that's okay. Don't stress out about it. But you data scientists out there,",
    "start": "2375940",
    "end": "2382000"
  },
  {
    "text": "obviously you wanna consider\nthese in more detail, but using the default values\nis always a good choice",
    "start": "2382000",
    "end": "2387910"
  },
  {
    "text": "for starting. So we sent in our prompt, tell\nme about Amazon SageMaker,",
    "start": "2387910",
    "end": "2394000"
  },
  {
    "text": "and we get back a paragraph\nfrom the model again.",
    "start": "2394000",
    "end": "2399000"
  },
  {
    "text": "So this model is coming\ndirectly from the predictor. This is coming straight\nout of the Falcon 7b model.",
    "start": "2399040",
    "end": "2408450"
  },
  {
    "text": "I did not write this, nor did I put this in the model myself.",
    "start": "2409660",
    "end": "2414700"
  },
  {
    "text": "This came out from the model Amazon, let's just read it here. Amazon SageMaker, lost developers, create,",
    "start": "2414700",
    "end": "2422740"
  },
  {
    "text": "train and deployment\nmachine learning models from the infrastructure. Hey, looks good to me.",
    "start": "2422740",
    "end": "2429280"
  },
  {
    "text": "I would check the box on that. Great. Okay and then the notebook\ngives us some more information",
    "start": "2429280",
    "end": "2436150"
  },
  {
    "text": "about the Falcon model built by TII, and is currently the best\nopen source model available",
    "start": "2436150",
    "end": "2443680"
  },
  {
    "text": "via the open LLM\nleaderboard, which is great. And then you'll see a\ncouple more functions here.",
    "start": "2443680",
    "end": "2450100"
  },
  {
    "text": "Let's step through this. So we have this nice query endpoint, which basically is a lightweight wrapper",
    "start": "2450100",
    "end": "2458020"
  },
  {
    "text": "around this predictor.predict, and then is gonna give us some\ninputs and responses nicely.",
    "start": "2458020",
    "end": "2464890"
  },
  {
    "text": "So, so now we'll query the endpoint, and this time we're gonna\nask it to write a program",
    "start": "2464890",
    "end": "2470920"
  },
  {
    "text": "to compute the factorial in Python. See if it does this.",
    "start": "2470920",
    "end": "2475843"
  },
  {
    "text": "All right, here is a Python\nprogram to compute factorial.",
    "start": "2478720",
    "end": "2482922"
  },
  {
    "text": "I am pretty sure this is the factorial. I actually don't remember\nthe factorial equation.",
    "start": "2484870",
    "end": "2490900"
  },
  {
    "text": "If you're interested in\nlooking this up at home and then letting us know\nif it is indeed accurate,",
    "start": "2490900",
    "end": "2496540"
  },
  {
    "text": "please let me know. I am pretty sure it does\nhave to do with certainly n-1, so hmm.",
    "start": "2496540",
    "end": "2503619"
  },
  {
    "text": "Yep and then it's this recursive function 'cause it takes itself. Okay, great.",
    "start": "2503620",
    "end": "2508153"
  },
  {
    "text": "Onward ho. Next we're gonna ask\nit to build a website.",
    "start": "2509800",
    "end": "2514723"
  },
  {
    "text": "Hmm, it's funny 'cause I was\nthinking like in my mind, to me this means, hey, give\nme the code to do this,",
    "start": "2521260",
    "end": "2529060"
  },
  {
    "text": "but yeah okay, here\nobviously they're saying in 10 simple steps. So what we want are the\n10 steps that you need",
    "start": "2529060",
    "end": "2536349"
  },
  {
    "text": "to create a website. So choose your domain name, register, web posting provider, which frankly,",
    "start": "2536350",
    "end": "2545710"
  },
  {
    "text": "arguably you would choose\nthat before the domain",
    "start": "2545710",
    "end": "2550710"
  },
  {
    "text": "because you can't really\nregister the domain before you have the web hosting\nprovider, but that's fine.",
    "start": "2551200",
    "end": "2556660"
  },
  {
    "text": "And then you create your\nwebsite design, add content.",
    "start": "2556660",
    "end": "2559692"
  },
  {
    "text": "Yeah, looks pretty good. I mean, notably they don't\nmake any technical suggestions",
    "start": "2561760",
    "end": "2567040"
  },
  {
    "text": "about how you would do\nany of these things, but at least it's a good\nlist of 10 things to do.",
    "start": "2567040",
    "end": "2572893"
  },
  {
    "text": "And then translation. So translate English to\nFrench, sea otter is to,",
    "start": "2573940",
    "end": "2580450"
  },
  {
    "text": "I will not butcher this for you, but I'm gonna take their word for it,",
    "start": "2580450",
    "end": "2585460"
  },
  {
    "text": "that this is indeed the French\nway of saying peppermint",
    "start": "2585460",
    "end": "2589490"
  },
  {
    "text": "or no, I'm, I won't say that one for you. And then cheese, here we go.",
    "start": "2591783",
    "end": "2596920"
  },
  {
    "text": "Okay, so let's run this. Ah yes, fromage, of course.",
    "start": "2596920",
    "end": "2602922"
  },
  {
    "text": "And then we'll do some sentiment analysis.",
    "start": "2605230",
    "end": "2607663"
  },
  {
    "text": "Great, so in this case, here\nwe're doing a little bit of few shot prompting actually,",
    "start": "2610660",
    "end": "2615970"
  },
  {
    "text": "because we wanna tell the model\nto provide this sentiment, be it negative or positive,",
    "start": "2615970",
    "end": "2622720"
  },
  {
    "text": "and then we give it this last tweet. New music video is incredible\nand the sentiment comes back",
    "start": "2622720",
    "end": "2629140"
  },
  {
    "text": "and it's obviously positive. Couple more examples here.",
    "start": "2629140",
    "end": "2633133"
  },
  {
    "text": "When was the C programming\nlanguage invented? Okay, this one we should see.",
    "start": "2634660",
    "end": "2640839"
  },
  {
    "text": "So folks, it's always good\nto check these things. So let's see if we can check this.",
    "start": "2640840",
    "end": "2646660"
  },
  {
    "text": "When was the C programming\nlanguage invented?",
    "start": "2646660",
    "end": "2651660"
  },
  {
    "text": "Okay. The most creative period\noccurring during 1972.",
    "start": "2655900",
    "end": "2664180"
  },
  {
    "text": "Okay, Bell Labs. All right, looks pretty accurate to me.",
    "start": "2664180",
    "end": "2670543"
  },
  {
    "text": "And then the recipe for a\ndelicious lemon cheesecake,",
    "start": "2672850",
    "end": "2676603"
  },
  {
    "text": "graham crackers, butter, cream cheese, and then the instructions.",
    "start": "2685750",
    "end": "2692293"
  },
  {
    "text": "Okay, great. Into the springform pan, springform pan.",
    "start": "2694600",
    "end": "2699640"
  },
  {
    "text": "Then beat the cream\ncheese and sugar together. Add these things, bake,\ncool for 10 minutes",
    "start": "2699640",
    "end": "2707310"
  },
  {
    "text": "and then sprinkle on top. I mean, that sounds pretty fair. We just generated a recipe for cheesecake.",
    "start": "2708370",
    "end": "2716110"
  },
  {
    "text": "And then the last one here\nis gonna be summarization. So the summarization example,",
    "start": "2716110",
    "end": "2721033"
  },
  {
    "text": "they are providing this extensive input. So the input is this, right?",
    "start": "2722050",
    "end": "2727780"
  },
  {
    "text": "It's basically three paragraphs of content describing the Falcon model.",
    "start": "2727780",
    "end": "2733690"
  },
  {
    "text": "And then we get information\nabout all the places Falcon is available, the\ndifferent use cases it solves.",
    "start": "2733690",
    "end": "2740920"
  },
  {
    "text": "And then this all comes back, right? So here is the input because of that little wrapper function.",
    "start": "2740920",
    "end": "2746680"
  },
  {
    "text": "So here's the input and then\nyou see it has the instruction right down here. So summarize the article above,",
    "start": "2746680",
    "end": "2752349"
  },
  {
    "text": "and then the output is right here. TII made state of the Art Falcon model,",
    "start": "2752350",
    "end": "2758980"
  },
  {
    "text": "available on SageMaker Jumpstart, pre-trained models, et cetera. Great.",
    "start": "2758980",
    "end": "2764740"
  },
  {
    "text": "Okay, so accurate summarization. And then a handy guide for\nsome of the parameters as well.",
    "start": "2764740",
    "end": "2773640"
  },
  {
    "text": "And then a couple limits on\ninputs, medium and large,",
    "start": "2774430",
    "end": "2779430"
  },
  {
    "text": "which is specifically the number\nof input and output tokens. And if you're new to NLP, remember as a token is a part of a word,",
    "start": "2782650",
    "end": "2790060"
  },
  {
    "text": "basically tokens are how\nwe decompose language to feed them to machines. And then there's a little bit more,",
    "start": "2790060",
    "end": "2796930"
  },
  {
    "text": "this is a, hmm, generating\nfew tokens at a time.",
    "start": "2796930",
    "end": "2801930"
  },
  {
    "text": "Okay, so this is sort of\niteratively, that's right. Walking through this range\nand then feeding them",
    "start": "2802000",
    "end": "2810000"
  },
  {
    "text": "to the model sequentially. And then at the end we'll do a cleanup.",
    "start": "2811120",
    "end": "2815353"
  },
  {
    "text": "Alright. Yeah, great. We've got the list.",
    "start": "2817420",
    "end": "2822940"
  },
  {
    "text": "Multiple iterations, this\nis fun, iteration three.",
    "start": "2822940",
    "end": "2827533"
  },
  {
    "text": "Hmm. So basically what this is\nsaying is that when the document that you wanna send to a\nfoundation model is too long,",
    "start": "2828970",
    "end": "2836563"
  },
  {
    "text": "if it's too long to fit\nin the token length, then just send it through\npiece by piece basically.",
    "start": "2837400",
    "end": "2844570"
  },
  {
    "text": "So you can loop through the document or loop through your range",
    "start": "2844570",
    "end": "2850660"
  },
  {
    "text": "and then send parts of the document up and it will generate responses\nto different pieces of that.",
    "start": "2850660",
    "end": "2858840"
  },
  {
    "text": "And so here we went through 10 iterations listing a variety of services.",
    "start": "2858950",
    "end": "2865930"
  },
  {
    "text": "This is not uncommon actually to see this kind of degradation. So here we just, I feel like\nthere's probably some looping",
    "start": "2865930",
    "end": "2874680"
  },
  {
    "text": "in the model going on\nhere in not a good way. But in any case, that was an\nexample of using tokenization",
    "start": "2874870",
    "end": "2881089"
  },
  {
    "text": "in order to perform text generation with Falcon on SageMaker Jumpstart.",
    "start": "2881980",
    "end": "2887020"
  },
  {
    "text": "And so that is the end\nof this first video. I hope you enjoyed it. And in the next one we are going to learn",
    "start": "2887020",
    "end": "2895150"
  },
  {
    "text": "about how to pick the\nright foundation model. So I'll see you there.",
    "start": "2895150",
    "end": "2898603"
  }
]