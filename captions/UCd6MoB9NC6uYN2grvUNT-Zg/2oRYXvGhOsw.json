[
  {
    "text": "well hi everybody thank you for joining",
    "start": "280",
    "end": "2720"
  },
  {
    "text": "me today we're going to be talking about",
    "start": "2720",
    "end": "5440"
  },
  {
    "text": "responsible AI with llama guard on Sage",
    "start": "5440",
    "end": "8599"
  },
  {
    "text": "maker and we're going to use a small new",
    "start": "8599",
    "end": "11000"
  },
  {
    "text": "feature in Sage maker called inference",
    "start": "11000",
    "end": "13440"
  },
  {
    "text": "components so that you're able to deploy",
    "start": "13440",
    "end": "16039"
  },
  {
    "text": "two models two Foundation models on the",
    "start": "16039",
    "end": "19240"
  },
  {
    "text": "same inference",
    "start": "19240",
    "end": "21080"
  },
  {
    "text": "endpoint um in a very cost-effective way",
    "start": "21080",
    "end": "24160"
  },
  {
    "text": "so let's get into it today we're going",
    "start": "24160",
    "end": "26240"
  },
  {
    "text": "to look at a single llama 27b model as a",
    "start": "26240",
    "end": "31239"
  },
  {
    "text": "reference um there's a lot of",
    "start": "31239",
    "end": "33520"
  },
  {
    "text": "responsible Ai and AI safety features",
    "start": "33520",
    "end": "35840"
  },
  {
    "text": "that's already gone into the",
    "start": "35840",
    "end": "37040"
  },
  {
    "text": "pre-training phase of this model so if",
    "start": "37040",
    "end": "39800"
  },
  {
    "text": "you end up spend sending your input",
    "start": "39800",
    "end": "42480"
  },
  {
    "text": "payloads as a request directly to llama",
    "start": "42480",
    "end": "44760"
  },
  {
    "text": "2 you're likely to get a really good",
    "start": "44760",
    "end": "47280"
  },
  {
    "text": "good response as well from a safety and",
    "start": "47280",
    "end": "49160"
  },
  {
    "text": "a responsible AI",
    "start": "49160",
    "end": "50960"
  },
  {
    "text": "perspective um but what we can do is we",
    "start": "50960",
    "end": "53960"
  },
  {
    "text": "can add a few an additional layer of",
    "start": "53960",
    "end": "56600"
  },
  {
    "text": "safety with another model called llama",
    "start": "56600",
    "end": "59640"
  },
  {
    "text": "guard and keep in mind that llama guard",
    "start": "59640",
    "end": "62280"
  },
  {
    "text": "is actually the same llama 27b model",
    "start": "62280",
    "end": "65840"
  },
  {
    "text": "instruction fine-tuned for specific",
    "start": "65840",
    "end": "69520"
  },
  {
    "text": "cases or specific categories that you",
    "start": "69520",
    "end": "71920"
  },
  {
    "text": "can even change for your organization's",
    "start": "71920",
    "end": "75000"
  },
  {
    "text": "uh requirements so what'll happen now is",
    "start": "75000",
    "end": "77920"
  },
  {
    "text": "instead of sending it directly to llama",
    "start": "77920",
    "end": "79479"
  },
  {
    "text": "2 you'll first send your request to",
    "start": "79479",
    "end": "81600"
  },
  {
    "text": "llama guard llama guard will classify",
    "start": "81600",
    "end": "85360"
  },
  {
    "text": "whether that incoming request is safe or",
    "start": "85360",
    "end": "88360"
  },
  {
    "text": "unsafe and then depending on how you",
    "start": "88360",
    "end": "90759"
  },
  {
    "text": "decide based on your organization's",
    "start": "90759",
    "end": "92759"
  },
  {
    "text": "requirements we'll send it across to the",
    "start": "92759",
    "end": "95600"
  },
  {
    "text": "to the to the real uh large language",
    "start": "95600",
    "end": "97960"
  },
  {
    "text": "model the Llama 21 and you can get your",
    "start": "97960",
    "end": "100159"
  },
  {
    "text": "response and send it back to your",
    "start": "100159",
    "end": "102119"
  },
  {
    "text": "calling",
    "start": "102119",
    "end": "103159"
  },
  {
    "text": "service I urge you to take a look at the",
    "start": "103159",
    "end": "106119"
  },
  {
    "text": "model card and prompt formats for llama",
    "start": "106119",
    "end": "109360"
  },
  {
    "text": "guard and here's a link for you to",
    "start": "109360",
    "end": "111960"
  },
  {
    "text": "review at your own time for now let's",
    "start": "111960",
    "end": "114560"
  },
  {
    "text": "jump into the code and see how we can",
    "start": "114560",
    "end": "116399"
  },
  {
    "text": "implement this on a sagemaker endpoint",
    "start": "116399",
    "end": "118920"
  },
  {
    "text": "with inference comp on all right let's",
    "start": "118920",
    "end": "121840"
  },
  {
    "text": "get into the code um in this sample",
    "start": "121840",
    "end": "124960"
  },
  {
    "text": "notebook uh the link for which will be",
    "start": "124960",
    "end": "127320"
  },
  {
    "text": "available for you in the description",
    "start": "127320",
    "end": "129800"
  },
  {
    "text": "below uh you will see that we're about",
    "start": "129800",
    "end": "131959"
  },
  {
    "text": "to deploy both the Llama guard model and",
    "start": "131959",
    "end": "134959"
  },
  {
    "text": "then the Llama 27b model on the same",
    "start": "134959",
    "end": "137760"
  },
  {
    "text": "Sage maker inference component",
    "start": "137760",
    "end": "140040"
  },
  {
    "text": "endpoint uh we'll skip through the few",
    "start": "140040",
    "end": "142280"
  },
  {
    "text": "resources though I would urge you to",
    "start": "142280",
    "end": "143920"
  },
  {
    "text": "take a look at it uh on your own time",
    "start": "143920",
    "end": "146080"
  },
  {
    "text": "specifically this 4minute YouTube video",
    "start": "146080",
    "end": "148239"
  },
  {
    "text": "of what uh Sage maker inference",
    "start": "148239",
    "end": "150680"
  },
  {
    "text": "components are for now let's get into",
    "start": "150680",
    "end": "152920"
  },
  {
    "text": "the code I'm going to install botos 3",
    "start": "152920",
    "end": "156560"
  },
  {
    "text": "and sagemaker",
    "start": "156560",
    "end": "157959"
  },
  {
    "text": "sdks uh this one is already installed on",
    "start": "157959",
    "end": "161040"
  },
  {
    "text": "my uh environment so I won't run it",
    "start": "161040",
    "end": "163280"
  },
  {
    "text": "again but I'll just import them into my",
    "start": "163280",
    "end": "166159"
  },
  {
    "text": "notebook",
    "start": "166159",
    "end": "167200"
  },
  {
    "text": "envirment these are a few setups that uh",
    "start": "167200",
    "end": "170120"
  },
  {
    "text": "helps sagemaker understand what is the I",
    "start": "170120",
    "end": "172280"
  },
  {
    "text": "am permissions and roles and the buckets",
    "start": "172280",
    "end": "174760"
  },
  {
    "text": "to be used for deploying this sagemaker",
    "start": "174760",
    "end": "177400"
  },
  {
    "text": "endpoint this code is uh written in a",
    "start": "177400",
    "end": "180400"
  },
  {
    "text": "manner that you don't need to run this",
    "start": "180400",
    "end": "182280"
  },
  {
    "text": "notebook on sagemaker notebooks only you",
    "start": "182280",
    "end": "184680"
  },
  {
    "text": "can run it wherever you want and uh",
    "start": "184680",
    "end": "187280"
  },
  {
    "text": "it'll figure out the right uh roles and",
    "start": "187280",
    "end": "190080"
  },
  {
    "text": "and regions and and buckets on its own",
    "start": "190080",
    "end": "193239"
  },
  {
    "text": "um let's go ahead and create the model",
    "start": "193239",
    "end": "195920"
  },
  {
    "text": "and endpoint configuration endpoint and",
    "start": "195920",
    "end": "198000"
  },
  {
    "text": "you've already seen this in the previous",
    "start": "198000",
    "end": "199120"
  },
  {
    "text": "video so we'll Bree Breeze past this",
    "start": "199120",
    "end": "201599"
  },
  {
    "text": "particular",
    "start": "201599",
    "end": "202519"
  },
  {
    "text": "one we'll pull in the latest version of",
    "start": "202519",
    "end": "205080"
  },
  {
    "text": "the large model inference container or",
    "start": "205080",
    "end": "207080"
  },
  {
    "text": "the LMI container here we go uh we'll",
    "start": "207080",
    "end": "210040"
  },
  {
    "text": "create a small name of the main of the",
    "start": "210040",
    "end": "213879"
  },
  {
    "text": "the guard llm which is llama guard in",
    "start": "213879",
    "end": "215680"
  },
  {
    "text": "this case we're going to be using the",
    "start": "215680",
    "end": "217560"
  },
  {
    "text": "awq version the smaller quantise version",
    "start": "217560",
    "end": "220519"
  },
  {
    "text": "of the Lama guard model so let me go",
    "start": "220519",
    "end": "222040"
  },
  {
    "text": "ahead and create the name this is a",
    "start": "222040",
    "end": "223400"
  },
  {
    "text": "small utility function that creates a",
    "start": "223400",
    "end": "225519"
  },
  {
    "text": "unique name based on the time stamp as",
    "start": "225519",
    "end": "228040"
  },
  {
    "text": "well as the string that I provide here",
    "start": "228040",
    "end": "230080"
  },
  {
    "text": "so the name that we're going to uh name",
    "start": "230080",
    "end": "232439"
  },
  {
    "text": "it is is this one ending at",
    "start": "232439",
    "end": "234959"
  },
  {
    "text": "456 this is the Crux of creating the awq",
    "start": "234959",
    "end": "238680"
  },
  {
    "text": "Llama guard model",
    "start": "238680",
    "end": "240480"
  },
  {
    "text": "uh we're going to specify the uh the",
    "start": "240480",
    "end": "243920"
  },
  {
    "text": "model ID from the hugging face uh repo",
    "start": "243920",
    "end": "247439"
  },
  {
    "text": "and in this case I'm using the blos lard",
    "start": "247439",
    "end": "250720"
  },
  {
    "text": "7B",
    "start": "250720",
    "end": "252159"
  },
  {
    "text": "awq you'll notice that we're using the",
    "start": "252159",
    "end": "254720"
  },
  {
    "text": "awq quantise option here and this is",
    "start": "254720",
    "end": "257280"
  },
  {
    "text": "just as simple as creating uh an",
    "start": "257280",
    "end": "259880"
  },
  {
    "text": "environment variable uh and adding them",
    "start": "259880",
    "end": "261799"
  },
  {
    "text": "to this this dict over here uh let's go",
    "start": "261799",
    "end": "265120"
  },
  {
    "text": "ahead and create this model and here we",
    "start": "265120",
    "end": "268120"
  },
  {
    "text": "go there we go so now we have this new",
    "start": "268120",
    "end": "271199"
  },
  {
    "text": "model for the safety uh for the safety",
    "start": "271199",
    "end": "274320"
  },
  {
    "text": "llm uh We've specified this before now",
    "start": "274320",
    "end": "278240"
  },
  {
    "text": "let's go ahead and create the main model",
    "start": "278240",
    "end": "279680"
  },
  {
    "text": "which is the the second model here llama",
    "start": "279680",
    "end": "282400"
  },
  {
    "text": "27b this is what it's going to be",
    "start": "282400",
    "end": "285000"
  },
  {
    "text": "called and we do something similar here",
    "start": "285000",
    "end": "287440"
  },
  {
    "text": "we specify a few more environment",
    "start": "287440",
    "end": "289320"
  },
  {
    "text": "variables this time for the main",
    "start": "289320",
    "end": "291600"
  },
  {
    "text": "llm the biggest difference that you'll",
    "start": "291600",
    "end": "293680"
  },
  {
    "text": "notice is of course that the model ID is",
    "start": "293680",
    "end": "295759"
  },
  {
    "text": "different this one is the larger 7B",
    "start": "295759",
    "end": "298400"
  },
  {
    "text": "model which is not aw two quanti and the",
    "start": "298400",
    "end": "300759"
  },
  {
    "text": "reason we've done that is to make sure",
    "start": "300759",
    "end": "302479"
  },
  {
    "text": "that you see the difference between the",
    "start": "302479",
    "end": "304360"
  },
  {
    "text": "two um in this case we're going to use",
    "start": "304360",
    "end": "307680"
  },
  {
    "text": "the VM uh framework as well as specified",
    "start": "307680",
    "end": "311080"
  },
  {
    "text": "tensor parallel degree Max which means",
    "start": "311080",
    "end": "313880"
  },
  {
    "text": "it tells the LMI container to Shard the",
    "start": "313880",
    "end": "316720"
  },
  {
    "text": "model across all of the available gpus",
    "start": "316720",
    "end": "319000"
  },
  {
    "text": "on your G4 g512 XL uh",
    "start": "319000",
    "end": "323479"
  },
  {
    "text": "instance uh so let's go ahead and create",
    "start": "323479",
    "end": "325440"
  },
  {
    "text": "this model",
    "start": "325440",
    "end": "326880"
  },
  {
    "text": "too there we go so the main llm model",
    "start": "326880",
    "end": "329759"
  },
  {
    "text": "has been created and this is what it's",
    "start": "329759",
    "end": "332160"
  },
  {
    "text": "called now we're going to actually",
    "start": "332160",
    "end": "334319"
  },
  {
    "text": "deploy the endpoint and to do that",
    "start": "334319",
    "end": "336840"
  },
  {
    "text": "endpoint as you've seen earlier we",
    "start": "336840",
    "end": "338520"
  },
  {
    "text": "create an endpoint configuration which",
    "start": "338520",
    "end": "340919"
  },
  {
    "text": "basically specifies the instance type um",
    "start": "340919",
    "end": "344840"
  },
  {
    "text": "and the initial instance count so we're",
    "start": "344840",
    "end": "346560"
  },
  {
    "text": "going to go ahead and do that right away",
    "start": "346560",
    "end": "348520"
  },
  {
    "text": "create the names and create the initial",
    "start": "348520",
    "end": "351880"
  },
  {
    "text": "instance counts maximum instance counts",
    "start": "351880",
    "end": "354240"
  },
  {
    "text": "in this case uh I'm actually going to",
    "start": "354240",
    "end": "356360"
  },
  {
    "text": "change this to uh maximum instances of",
    "start": "356360",
    "end": "359120"
  },
  {
    "text": "two so that that we'll we'll actually",
    "start": "359120",
    "end": "360759"
  },
  {
    "text": "take a look at the scaling uh scaling up",
    "start": "360759",
    "end": "363759"
  },
  {
    "text": "of this instance type uh when the",
    "start": "363759",
    "end": "366080"
  },
  {
    "text": "traffic",
    "start": "366080",
    "end": "367759"
  },
  {
    "text": "increases uh I'm going to go ahead and",
    "start": "367759",
    "end": "369599"
  },
  {
    "text": "create the endpoint configuration here",
    "start": "369599",
    "end": "371960"
  },
  {
    "text": "and the specifics of this is we've",
    "start": "371960",
    "end": "373880"
  },
  {
    "text": "specified the initial instance count uh",
    "start": "373880",
    "end": "376440"
  },
  {
    "text": "the model data download time in seconds",
    "start": "376440",
    "end": "380080"
  },
  {
    "text": "um I've specified this as 1,200 or 20",
    "start": "380080",
    "end": "383039"
  },
  {
    "text": "minutes and sometimes this is useful you",
    "start": "383039",
    "end": "385000"
  },
  {
    "text": "can go up to 3,600 uh seconds or 1 hour",
    "start": "385000",
    "end": "387960"
  },
  {
    "text": "and this is useful for some of these",
    "start": "387960",
    "end": "389120"
  },
  {
    "text": "larger models these are large language",
    "start": "389120",
    "end": "390800"
  },
  {
    "text": "models after all so it may take some",
    "start": "390800",
    "end": "392120"
  },
  {
    "text": "time for the initial download uh and",
    "start": "392120",
    "end": "394800"
  },
  {
    "text": "then the container startup so sagemaker",
    "start": "394800",
    "end": "396560"
  },
  {
    "text": "goes ahead and does a few health checks",
    "start": "396560",
    "end": "399919"
  },
  {
    "text": "uh at the container level itself on your",
    "start": "399919",
    "end": "401960"
  },
  {
    "text": "behalf so that we're making sure",
    "start": "401960",
    "end": "403199"
  },
  {
    "text": "everything is good to go before we",
    "start": "403199",
    "end": "405160"
  },
  {
    "text": "Market as in service are ready to",
    "start": "405160",
    "end": "407199"
  },
  {
    "text": "consume traffic from your uh from your",
    "start": "407199",
    "end": "409440"
  },
  {
    "text": "production",
    "start": "409440",
    "end": "410840"
  },
  {
    "text": "environments uh you'll also notice that",
    "start": "410840",
    "end": "412880"
  },
  {
    "text": "we're specifying the least outstanding",
    "start": "412880",
    "end": "415440"
  },
  {
    "text": "requests routing strategy and this is",
    "start": "415440",
    "end": "418120"
  },
  {
    "text": "because these models are are rather",
    "start": "418120",
    "end": "420840"
  },
  {
    "text": "large and uh with this particular least",
    "start": "420840",
    "end": "423840"
  },
  {
    "text": "outstanding request or L routing",
    "start": "423840",
    "end": "426360"
  },
  {
    "text": "strategy we're able to pick out those",
    "start": "426360",
    "end": "429319"
  },
  {
    "text": "instances which have the least",
    "start": "429319",
    "end": "431360"
  },
  {
    "text": "outstanding requests so that we're more",
    "start": "431360",
    "end": "433680"
  },
  {
    "text": "effectively uh uh load balancing our",
    "start": "433680",
    "end": "436240"
  },
  {
    "text": "traffic behind our stagemaker endpoint",
    "start": "436240",
    "end": "439280"
  },
  {
    "text": "so let's go ahead and create that",
    "start": "439280",
    "end": "440319"
  },
  {
    "text": "endpoint configuration as well all right",
    "start": "440319",
    "end": "444000"
  },
  {
    "text": "now here's the Endo that we specified",
    "start": "444000",
    "end": "447240"
  },
  {
    "text": "earlier I'm going to go ahead and",
    "start": "447240",
    "end": "450199"
  },
  {
    "text": "create the endpoint by providing the",
    "start": "450199",
    "end": "453440"
  },
  {
    "text": "endpoint configuration that we just",
    "start": "453440",
    "end": "454919"
  },
  {
    "text": "specified up here and this step will",
    "start": "454919",
    "end": "457520"
  },
  {
    "text": "take a little bit of time because we're",
    "start": "457520",
    "end": "459120"
  },
  {
    "text": "actually pulling uh the the G5 12xl",
    "start": "459120",
    "end": "463160"
  },
  {
    "text": "instance from the pool of instances that",
    "start": "463160",
    "end": "465960"
  },
  {
    "text": "stage maker provides behind the scenes",
    "start": "465960",
    "end": "468199"
  },
  {
    "text": "and this sometimes can take a little bit",
    "start": "468199",
    "end": "469720"
  },
  {
    "text": "of time uh what's happening now is we're",
    "start": "469720",
    "end": "473000"
  },
  {
    "text": "setting things up uh Sage maker will",
    "start": "473000",
    "end": "475759"
  },
  {
    "text": "take a few minutes to do the container",
    "start": "475759",
    "end": "478199"
  },
  {
    "text": "down to do the container download it'll",
    "start": "478199",
    "end": "480800"
  },
  {
    "text": "go ahead and download the actual models",
    "start": "480800",
    "end": "484479"
  },
  {
    "text": "uh for you and then go ahead and uh",
    "start": "484479",
    "end": "487360"
  },
  {
    "text": "specify uh the the instance scaling",
    "start": "487360",
    "end": "490120"
  },
  {
    "text": "aspects as",
    "start": "490120",
    "end": "492720"
  },
  {
    "text": "well uh there's a small utility function",
    "start": "493520",
    "end": "496120"
  },
  {
    "text": "here that is uh going to wait for the",
    "start": "496120",
    "end": "499479"
  },
  {
    "text": "endpoint to come up if I go ahead and",
    "start": "499479",
    "end": "501479"
  },
  {
    "text": "and switch to the models I can see that",
    "start": "501479",
    "end": "504639"
  },
  {
    "text": "my safe endpoint which I just started",
    "start": "504639",
    "end": "506400"
  },
  {
    "text": "creating right now uh is in creating",
    "start": "506400",
    "end": "509080"
  },
  {
    "text": "status and in a few moments this will be",
    "start": "509080",
    "end": "511039"
  },
  {
    "text": "fully created awesome so we now have our",
    "start": "511039",
    "end": "514159"
  },
  {
    "text": "endpoint in service so we've created our",
    "start": "514159",
    "end": "518279"
  },
  {
    "text": "uh sagemaker inference endpoint now is",
    "start": "518279",
    "end": "520839"
  },
  {
    "text": "the time to deploy the actual models",
    "start": "520839",
    "end": "523279"
  },
  {
    "text": "onto this endpoint that we just created",
    "start": "523279",
    "end": "525320"
  },
  {
    "text": "that's in service now using something",
    "start": "525320",
    "end": "527360"
  },
  {
    "text": "known as inference",
    "start": "527360",
    "end": "529000"
  },
  {
    "text": "components so let's see what this is",
    "start": "529000",
    "end": "531040"
  },
  {
    "text": "like these are the names of the two",
    "start": "531040",
    "end": "533000"
  },
  {
    "text": "models that we created previously we're",
    "start": "533000",
    "end": "535920"
  },
  {
    "text": "going to create the name of the",
    "start": "535920",
    "end": "537440"
  },
  {
    "text": "inference component based on the model",
    "start": "537440",
    "end": "540000"
  },
  {
    "text": "itself and this takes a couple of",
    "start": "540000",
    "end": "542760"
  },
  {
    "text": "configurations so for example how many",
    "start": "542760",
    "end": "544760"
  },
  {
    "text": "copies do we want of the Guard model uh",
    "start": "544760",
    "end": "548920"
  },
  {
    "text": "initially which for me I'll specify as",
    "start": "548920",
    "end": "551760"
  },
  {
    "text": "one we'll also specify the maximum",
    "start": "551760",
    "end": "554920"
  },
  {
    "text": "copies of the this particular guard",
    "start": "554920",
    "end": "557519"
  },
  {
    "text": "guard model per instance and this will",
    "start": "557519",
    "end": "559839"
  },
  {
    "text": "come in handy when we do autoscaling",
    "start": "559839",
    "end": "561800"
  },
  {
    "text": "later",
    "start": "561800",
    "end": "562600"
  },
  {
    "text": "on we also specify a couple more",
    "start": "562600",
    "end": "565040"
  },
  {
    "text": "configuration parameters here the first",
    "start": "565040",
    "end": "566920"
  },
  {
    "text": "one being the minimum memory required uh",
    "start": "566920",
    "end": "569519"
  },
  {
    "text": "in this case I've specified as 1024 or 1",
    "start": "569519",
    "end": "571959"
  },
  {
    "text": "Gigabyte the reason for this is um the",
    "start": "571959",
    "end": "575320"
  },
  {
    "text": "LL guard model is actually the 7B model",
    "start": "575320",
    "end": "577959"
  },
  {
    "text": "uh which requires at least 1 GB of",
    "start": "577959",
    "end": "580279"
  },
  {
    "text": "memory as well as one minimum GPU",
    "start": "580279",
    "end": "583480"
  },
  {
    "text": "accelerator device which I've specified",
    "start": "583480",
    "end": "586120"
  },
  {
    "text": "here so the inference component name is",
    "start": "586120",
    "end": "589680"
  },
  {
    "text": "now uh this one 46- I let's go ahead and",
    "start": "589680",
    "end": "594040"
  },
  {
    "text": "create that inference",
    "start": "594040",
    "end": "595959"
  },
  {
    "text": "component there we go so this is now set",
    "start": "595959",
    "end": "598959"
  },
  {
    "text": "up the inference component behind the",
    "start": "598959",
    "end": "600920"
  },
  {
    "text": "scenes while this is happening let's",
    "start": "600920",
    "end": "603000"
  },
  {
    "text": "deploy the second inference component",
    "start": "603000",
    "end": "604720"
  },
  {
    "text": "which is the main llm as well um we'll",
    "start": "604720",
    "end": "608279"
  },
  {
    "text": "do the same thing here we'll specify an",
    "start": "608279",
    "end": "610160"
  },
  {
    "text": "initial copy of one uh we'll also",
    "start": "610160",
    "end": "613040"
  },
  {
    "text": "specify the maximum per instance as four",
    "start": "613040",
    "end": "616640"
  },
  {
    "text": "and the and because both llama guard and",
    "start": "616640",
    "end": "619640"
  },
  {
    "text": "llama 27b are actually the same models",
    "start": "619640",
    "end": "622200"
  },
  {
    "text": "from a graph perspective it's just that",
    "start": "622200",
    "end": "624279"
  },
  {
    "text": "L guard is instruction fine-tuned for",
    "start": "624279",
    "end": "626600"
  },
  {
    "text": "safety aspects uh we can specify the",
    "start": "626600",
    "end": "628920"
  },
  {
    "text": "same parameters for",
    "start": "628920",
    "end": "630800"
  },
  {
    "text": "both uh let's go ahead and set that up",
    "start": "630800",
    "end": "634120"
  },
  {
    "text": "there we go and in this case we're going",
    "start": "634120",
    "end": "637000"
  },
  {
    "text": "ahead and deploying the inference",
    "start": "637000",
    "end": "640200"
  },
  {
    "text": "component now while this is happening",
    "start": "640200",
    "end": "642519"
  },
  {
    "text": "this is also going to take a little bit",
    "start": "642519",
    "end": "643720"
  },
  {
    "text": "of time because now what we're doing is",
    "start": "643720",
    "end": "645399"
  },
  {
    "text": "we're downloading the uh model weights",
    "start": "645399",
    "end": "648160"
  },
  {
    "text": "from hugging face what we specified",
    "start": "648160",
    "end": "650120"
  },
  {
    "text": "earlier um it's going to go ahead and",
    "start": "650120",
    "end": "652760"
  },
  {
    "text": "set up the LMI container on the",
    "start": "652760",
    "end": "654839"
  },
  {
    "text": "inference component onto the same stage",
    "start": "654839",
    "end": "657360"
  },
  {
    "text": "maker endpoint behind the scenes",
    "start": "657360",
    "end": "660399"
  },
  {
    "text": "couple of parameters that are",
    "start": "660399",
    "end": "661560"
  },
  {
    "text": "interesting that uh that you might have",
    "start": "661560",
    "end": "663440"
  },
  {
    "text": "noticed is the model data download",
    "start": "663440",
    "end": "665600"
  },
  {
    "text": "timeout because these models are",
    "start": "665600",
    "end": "667639"
  },
  {
    "text": "slightly larger uh they are large",
    "start": "667639",
    "end": "669440"
  },
  {
    "text": "language models we've specified um 1024",
    "start": "669440",
    "end": "673959"
  },
  {
    "text": "uh or rather 1200 seconds for this",
    "start": "673959",
    "end": "676360"
  },
  {
    "text": "one and uh in this case we're also",
    "start": "676360",
    "end": "679760"
  },
  {
    "text": "specifying the initial copy count uh the",
    "start": "679760",
    "end": "682680"
  },
  {
    "text": "minimum memory Etc that we've specified",
    "start": "682680",
    "end": "685440"
  },
  {
    "text": "up ahead so now that we've we've",
    "start": "685440",
    "end": "688720"
  },
  {
    "text": "deployed the inference components for",
    "start": "688720",
    "end": "690880"
  },
  {
    "text": "both these models let's take a look at",
    "start": "690880",
    "end": "693000"
  },
  {
    "text": "some of the configuration parameters",
    "start": "693000",
    "end": "694560"
  },
  {
    "text": "that we",
    "start": "694560",
    "end": "695920"
  },
  {
    "text": "specified uh we already talked about the",
    "start": "695920",
    "end": "698959"
  },
  {
    "text": "minimum memory required um and the",
    "start": "698959",
    "end": "701639"
  },
  {
    "text": "number of acceler accelerator devices",
    "start": "701639",
    "end": "704560"
  },
  {
    "text": "required uh we also talked about the",
    "start": "704560",
    "end": "706600"
  },
  {
    "text": "initial copy count let's talk about the",
    "start": "706600",
    "end": "709440"
  },
  {
    "text": "model data download timeout the reason",
    "start": "709440",
    "end": "712000"
  },
  {
    "text": "for this is you these models are large",
    "start": "712000",
    "end": "714320"
  },
  {
    "text": "so sometimes it takes time to download",
    "start": "714320",
    "end": "716639"
  },
  {
    "text": "them from uh from hugging face you will",
    "start": "716639",
    "end": "719880"
  },
  {
    "text": "be able to use um uh your own models as",
    "start": "719880",
    "end": "723760"
  },
  {
    "text": "well if you host them on S3 uh this will",
    "start": "723760",
    "end": "726399"
  },
  {
    "text": "be much faster because the LMI container",
    "start": "726399",
    "end": "729040"
  },
  {
    "text": "downloads using a tool called S5 CMD and",
    "start": "729040",
    "end": "732639"
  },
  {
    "text": "you would have seen that in our previous",
    "start": "732639",
    "end": "734440"
  },
  {
    "text": "previous uh videos as well let's go",
    "start": "734440",
    "end": "737160"
  },
  {
    "text": "ahead and wait for a little bit to uh",
    "start": "737160",
    "end": "741000"
  },
  {
    "text": "see if the inference components have",
    "start": "741000",
    "end": "742680"
  },
  {
    "text": "been",
    "start": "742680",
    "end": "744320"
  },
  {
    "text": "deployed and I can actually run the CLI",
    "start": "744320",
    "end": "746880"
  },
  {
    "text": "command as well to see what happening",
    "start": "746880",
    "end": "749959"
  },
  {
    "text": "and as you can see the two inference",
    "start": "749959",
    "end": "751600"
  },
  {
    "text": "components the first one for the main",
    "start": "751600",
    "end": "753160"
  },
  {
    "text": "llm is in creating stage and then the",
    "start": "753160",
    "end": "756360"
  },
  {
    "text": "second one which is the uh llama guard",
    "start": "756360",
    "end": "758920"
  },
  {
    "text": "llm is also in creating",
    "start": "758920",
    "end": "763240"
  },
  {
    "text": "stage okay you see that the Llama guard",
    "start": "763399",
    "end": "769480"
  },
  {
    "text": "uh the smaller version the awq quantize",
    "start": "769480",
    "end": "772079"
  },
  {
    "text": "version took a lot less time to go into",
    "start": "772079",
    "end": "774800"
  },
  {
    "text": "service um we're still waiting for the",
    "start": "774800",
    "end": "777199"
  },
  {
    "text": "Llama 7B model which is the non",
    "start": "777199",
    "end": "779160"
  },
  {
    "text": "quantized version because this is a",
    "start": "779160",
    "end": "780959"
  },
  {
    "text": "slightly larger version uh it's still in",
    "start": "780959",
    "end": "783199"
  },
  {
    "text": "creating stage okay so now you see that",
    "start": "783199",
    "end": "786279"
  },
  {
    "text": "both the inference components are in",
    "start": "786279",
    "end": "789279"
  },
  {
    "text": "service and uh we see",
    "start": "789279",
    "end": "792240"
  },
  {
    "text": "that in in our space as well we can see",
    "start": "792240",
    "end": "796040"
  },
  {
    "text": "the name of the inference components",
    "start": "796040",
    "end": "798440"
  },
  {
    "text": "let's do a small s sanity check to list",
    "start": "798440",
    "end": "801079"
  },
  {
    "text": "all of the inference components that are",
    "start": "801079",
    "end": "803079"
  },
  {
    "text": "currently um showing up and we see that",
    "start": "803079",
    "end": "807079"
  },
  {
    "text": "the Llama 27b is deployed onto the same",
    "start": "807079",
    "end": "810600"
  },
  {
    "text": "inference endpoint the mys safe endpoint",
    "start": "810600",
    "end": "813519"
  },
  {
    "text": "ending with",
    "start": "813519",
    "end": "815160"
  },
  {
    "text": "643 um it's accepting all traffic and",
    "start": "815160",
    "end": "817600"
  },
  {
    "text": "it's in service and then the second",
    "start": "817600",
    "end": "819639"
  },
  {
    "text": "component which is um llama 2 or llama",
    "start": "819639",
    "end": "824120"
  },
  {
    "text": "guard 7B the awq quantized version is",
    "start": "824120",
    "end": "827560"
  },
  {
    "text": "also deployed onto the same endpoint",
    "start": "827560",
    "end": "829720"
  },
  {
    "text": "ending with",
    "start": "829720",
    "end": "830720"
  },
  {
    "text": "643 and it's also in service so there",
    "start": "830720",
    "end": "834440"
  },
  {
    "text": "you have it now we've already deployed",
    "start": "834440",
    "end": "836759"
  },
  {
    "text": "both of these components let's now go",
    "start": "836759",
    "end": "838839"
  },
  {
    "text": "ahead and do a small uh sanity check and",
    "start": "838839",
    "end": "842440"
  },
  {
    "text": "and send some sample payloads to make",
    "start": "842440",
    "end": "844680"
  },
  {
    "text": "sure that these these inference",
    "start": "844680",
    "end": "845839"
  },
  {
    "text": "components are working so what we're",
    "start": "845839",
    "end": "847800"
  },
  {
    "text": "going to do now is set up a few utility",
    "start": "847800",
    "end": "850040"
  },
  {
    "text": "tools that are llama guard specific and",
    "start": "850040",
    "end": "853240"
  },
  {
    "text": "what you see here is I can specify the",
    "start": "853240",
    "end": "856399"
  },
  {
    "text": "instruction and the task and a few",
    "start": "856399",
    "end": "858839"
  },
  {
    "text": "unsafe content categories and this is",
    "start": "858839",
    "end": "861800"
  },
  {
    "text": "something that you can modify that suits",
    "start": "861800",
    "end": "864120"
  },
  {
    "text": "for your that suits your use case or for",
    "start": "864120",
    "end": "866120"
  },
  {
    "text": "your um Enterprise scenario",
    "start": "866120",
    "end": "869600"
  },
  {
    "text": "in this case I've taken some of the",
    "start": "869600",
    "end": "870920"
  },
  {
    "text": "defaults that are provided by by The",
    "start": "870920",
    "end": "873240"
  },
  {
    "text": "Meta team and I'm just going to go ahead",
    "start": "873240",
    "end": "875480"
  },
  {
    "text": "and create these these utility",
    "start": "875480",
    "end": "878839"
  },
  {
    "text": "functions um here's a small uh example",
    "start": "878839",
    "end": "883839"
  },
  {
    "text": "and this is a typical example that works",
    "start": "883839",
    "end": "885399"
  },
  {
    "text": "well because what I'm about to send in",
    "start": "885399",
    "end": "887399"
  },
  {
    "text": "is I forgot how to kill a process in",
    "start": "887399",
    "end": "889920"
  },
  {
    "text": "Linux can you help um and typically in",
    "start": "889920",
    "end": "893519"
  },
  {
    "text": "traditional scenarios you might notice",
    "start": "893519",
    "end": "895399"
  },
  {
    "text": "that the word kill is a little bit",
    "start": "895399",
    "end": "897079"
  },
  {
    "text": "scandalous and and some some some models",
    "start": "897079",
    "end": "899560"
  },
  {
    "text": "might um reject this as unsafe out the",
    "start": "899560",
    "end": "902959"
  },
  {
    "text": "out the gate but that's actually not",
    "start": "902959",
    "end": "904600"
  },
  {
    "text": "true because I'm asking a perfectly",
    "start": "904600",
    "end": "906680"
  },
  {
    "text": "legitimate question which is how to kill",
    "start": "906680",
    "end": "908360"
  },
  {
    "text": "a process in Linux let's first do a",
    "start": "908360",
    "end": "910560"
  },
  {
    "text": "quick tinity check and uh here's what",
    "start": "910560",
    "end": "912920"
  },
  {
    "text": "we're actually sending to the Lama guard",
    "start": "912920",
    "end": "915839"
  },
  {
    "text": "payload and as you can see this is the",
    "start": "915839",
    "end": "917920"
  },
  {
    "text": "request that we're about to send and if",
    "start": "917920",
    "end": "920880"
  },
  {
    "text": "as you can see we're we've specified our",
    "start": "920880",
    "end": "923120"
  },
  {
    "text": "unsafe content",
    "start": "923120",
    "end": "924880"
  },
  {
    "text": "categories um and we've also added the",
    "start": "924880",
    "end": "929839"
  },
  {
    "text": "uh the actual conversation this is what",
    "start": "929839",
    "end": "932680"
  },
  {
    "text": "we've we've specified I forgot how to",
    "start": "932680",
    "end": "934240"
  },
  {
    "text": "kill it process in",
    "start": "934240",
    "end": "935639"
  },
  {
    "text": "Linux and let's see how the Llama guard",
    "start": "935639",
    "end": "939480"
  },
  {
    "text": "component uh sends back as a as a option",
    "start": "939480",
    "end": "943440"
  },
  {
    "text": "Yep this looks like it's been",
    "start": "943440",
    "end": "945040"
  },
  {
    "text": "categorized as safe so it sounds like L",
    "start": "945040",
    "end": "947480"
  },
  {
    "text": "guard has given us the thumbs up and we",
    "start": "947480",
    "end": "949079"
  },
  {
    "text": "can go ahead and send this request to",
    "start": "949079",
    "end": "951360"
  },
  {
    "text": "the second uh uh llama llama 2 model",
    "start": "951360",
    "end": "955560"
  },
  {
    "text": "behind the",
    "start": "955560",
    "end": "957480"
  },
  {
    "text": "scenes uh in this case let me go ahead",
    "start": "957480",
    "end": "960600"
  },
  {
    "text": "and uh do a little bit of formatting and",
    "start": "960600",
    "end": "963319"
  },
  {
    "text": "do a sanity check as to what we're",
    "start": "963319",
    "end": "965240"
  },
  {
    "text": "actually about to send I forgot how to",
    "start": "965240",
    "end": "967319"
  },
  {
    "text": "kill a process in Linux can you help and",
    "start": "967319",
    "end": "970399"
  },
  {
    "text": "here we go now we're going to send this",
    "start": "970399",
    "end": "972040"
  },
  {
    "text": "the second option to the main larger llm",
    "start": "972040",
    "end": "974839"
  },
  {
    "text": "behind the scenes and as you can notice",
    "start": "974839",
    "end": "977319"
  },
  {
    "text": "this is going to take a little bit of",
    "start": "977319",
    "end": "978360"
  },
  {
    "text": "time because we're not using the awq",
    "start": "978360",
    "end": "980839"
  },
  {
    "text": "Conti version of the model and this will",
    "start": "980839",
    "end": "984319"
  },
  {
    "text": "take about says it's about 14 seconds",
    "start": "984319",
    "end": "986519"
  },
  {
    "text": "for the output and let's see what the",
    "start": "986519",
    "end": "988360"
  },
  {
    "text": "model sent back back yep of course in L",
    "start": "988360",
    "end": "991000"
  },
  {
    "text": "Linux you can kill using the kill",
    "start": "991000",
    "end": "992399"
  },
  {
    "text": "command",
    "start": "992399",
    "end": "993920"
  },
  {
    "text": "absolutely so this is the right answer",
    "start": "993920",
    "end": "996639"
  },
  {
    "text": "and just for completeness sake let's go",
    "start": "996639",
    "end": "998480"
  },
  {
    "text": "back and send uh an example which is uh",
    "start": "998480",
    "end": "1002600"
  },
  {
    "text": "clearly something that is not not safe",
    "start": "1002600",
    "end": "1004480"
  },
  {
    "text": "so I'm going to uncomment this one it's",
    "start": "1004480",
    "end": "1006399"
  },
  {
    "text": "similar to what we just sent and I'm",
    "start": "1006399",
    "end": "1008279"
  },
  {
    "text": "going to say I forgot how to kill can",
    "start": "1008279",
    "end": "1010079"
  },
  {
    "text": "you can you help I remove the the Linux",
    "start": "1010079",
    "end": "1013160"
  },
  {
    "text": "component and now this is this is",
    "start": "1013160",
    "end": "1014560"
  },
  {
    "text": "something that's scandalous this is",
    "start": "1014560",
    "end": "1015600"
  },
  {
    "text": "something that I don't think a lot of",
    "start": "1015600",
    "end": "1016800"
  },
  {
    "text": "Enterprises will allow their users to",
    "start": "1016800",
    "end": "1018920"
  },
  {
    "text": "send in so let me go ahead and see what",
    "start": "1018920",
    "end": "1021839"
  },
  {
    "text": "uh what happens now we're going to wrap",
    "start": "1021839",
    "end": "1024880"
  },
  {
    "text": "it up in the same unsafe categories for",
    "start": "1024880",
    "end": "1029038"
  },
  {
    "text": "llama guard to understand what we can",
    "start": "1029039",
    "end": "1030720"
  },
  {
    "text": "allow and not allow and this is the",
    "start": "1030720",
    "end": "1032839"
  },
  {
    "text": "conversation that's actually going in as",
    "start": "1032839",
    "end": "1034480"
  },
  {
    "text": "part of the payload and now we're going",
    "start": "1034480",
    "end": "1036480"
  },
  {
    "text": "to send this payload into uh the Llama",
    "start": "1036480",
    "end": "1039678"
  },
  {
    "text": "guard component and here we go this is",
    "start": "1039679",
    "end": "1042720"
  },
  {
    "text": "the one that we",
    "start": "1042720",
    "end": "1045160"
  },
  {
    "text": "specified and there there we go it's",
    "start": "1045160",
    "end": "1048199"
  },
  {
    "text": "specified this as unsafe and therefore",
    "start": "1048199",
    "end": "1050640"
  },
  {
    "text": "we should not continue and probably",
    "start": "1050640",
    "end": "1052480"
  },
  {
    "text": "throw an error to the to the user um",
    "start": "1052480",
    "end": "1055160"
  },
  {
    "text": "that this is something that we do not",
    "start": "1055160",
    "end": "1056520"
  },
  {
    "text": "allow for this uh for this endpoint now",
    "start": "1056520",
    "end": "1059840"
  },
  {
    "text": "let's go ahead and skip to the the fifth",
    "start": "1059840",
    "end": "1062720"
  },
  {
    "text": "optional part of this demo which is",
    "start": "1062720",
    "end": "1065160"
  },
  {
    "text": "autoscaling and we're going to use a",
    "start": "1065160",
    "end": "1067039"
  },
  {
    "text": "component of the botov 3 SDK called Auto",
    "start": "1067039",
    "end": "1070919"
  },
  {
    "text": "application autoscaling and I'd urge you",
    "start": "1070919",
    "end": "1072679"
  },
  {
    "text": "to go through this section yourself as",
    "start": "1072679",
    "end": "1075240"
  },
  {
    "text": "well uh we're going to specify uh the",
    "start": "1075240",
    "end": "1078280"
  },
  {
    "text": "inference component as the autoscaling",
    "start": "1078280",
    "end": "1080720"
  },
  {
    "text": "parameter and these are uh the scaling",
    "start": "1080720",
    "end": "1083679"
  },
  {
    "text": "Dimensions that we're going to be",
    "start": "1083679",
    "end": "1085280"
  },
  {
    "text": "requesting sagemaker to monitor so we're",
    "start": "1085280",
    "end": "1087120"
  },
  {
    "text": "going to monitor the desired copy count",
    "start": "1087120",
    "end": "1090880"
  },
  {
    "text": "um as part of our autoscaling parameter",
    "start": "1090880",
    "end": "1093679"
  },
  {
    "text": "we will register this target so that",
    "start": "1093679",
    "end": "1095440"
  },
  {
    "text": "it's now something that's uh deployed",
    "start": "1095440",
    "end": "1099039"
  },
  {
    "text": "and just for a sanity check let's go",
    "start": "1099039",
    "end": "1100520"
  },
  {
    "text": "ahead and take a look at what we just",
    "start": "1100520",
    "end": "1101720"
  },
  {
    "text": "did we said we specified the inference",
    "start": "1101720",
    "end": "1104480"
  },
  {
    "text": "component of llama 2 7B the main llm",
    "start": "1104480",
    "end": "1108280"
  },
  {
    "text": "that we we had previously created we",
    "start": "1108280",
    "end": "1110600"
  },
  {
    "text": "specifi the dimension of desired copy",
    "start": "1110600",
    "end": "1113559"
  },
  {
    "text": "count for that inference",
    "start": "1113559",
    "end": "1115720"
  },
  {
    "text": "component uh we have specified the RO",
    "start": "1115720",
    "end": "1119640"
  },
  {
    "text": "the maximum capacity and the minimum",
    "start": "1119640",
    "end": "1121559"
  },
  {
    "text": "capacity and it's also providing a few",
    "start": "1121559",
    "end": "1124080"
  },
  {
    "text": "information uh a few information like",
    "start": "1124080",
    "end": "1126120"
  },
  {
    "text": "when did we actually create it what is",
    "start": "1126120",
    "end": "1128080"
  },
  {
    "text": "the actually what is the the the",
    "start": "1128080",
    "end": "1131200"
  },
  {
    "text": "scalable Target Arn and and a few HTTP",
    "start": "1131200",
    "end": "1134000"
  },
  {
    "text": "errors uh or HTTP status",
    "start": "1134000",
    "end": "1137720"
  },
  {
    "text": "codes let's put the scaling policy in",
    "start": "1137720",
    "end": "1140840"
  },
  {
    "text": "motion this should be uh putting the",
    "start": "1140840",
    "end": "1142960"
  },
  {
    "text": "scaling policy in uh in",
    "start": "1142960",
    "end": "1145919"
  },
  {
    "text": "motion now because I don't have a way to",
    "start": "1145919",
    "end": "1148679"
  },
  {
    "text": "generate the traffic uh alive I'm just",
    "start": "1148679",
    "end": "1151679"
  },
  {
    "text": "going to go ahead and show you what are",
    "start": "1151679",
    "end": "1153559"
  },
  {
    "text": "the current and desired instance counts",
    "start": "1153559",
    "end": "1155880"
  },
  {
    "text": "for both of our inference components so",
    "start": "1155880",
    "end": "1158440"
  },
  {
    "text": "here's the end point we have one G5 12x",
    "start": "1158440",
    "end": "1161600"
  },
  {
    "text": "large in behind our",
    "start": "1161600",
    "end": "1163720"
  },
  {
    "text": "endpoint uh we have one llama 27b the",
    "start": "1163720",
    "end": "1168240"
  },
  {
    "text": "main LM component and the desired copy",
    "start": "1168240",
    "end": "1170880"
  },
  {
    "text": "right now is one and then we have the",
    "start": "1170880",
    "end": "1172960"
  },
  {
    "text": "Llama guard as well on the same endpoint",
    "start": "1172960",
    "end": "1174919"
  },
  {
    "text": "and both both the desired and current",
    "start": "1174919",
    "end": "1176960"
  },
  {
    "text": "copy is one I'm going to manually change",
    "start": "1176960",
    "end": "1179880"
  },
  {
    "text": "and update the inference component to",
    "start": "1179880",
    "end": "1181760"
  },
  {
    "text": "three for the main llm you can actually",
    "start": "1181760",
    "end": "1184840"
  },
  {
    "text": "do this for the Llama guard component as",
    "start": "1184840",
    "end": "1186760"
  },
  {
    "text": "well but for now let's stick to",
    "start": "1186760",
    "end": "1188400"
  },
  {
    "text": "increasing or scaling out just the the",
    "start": "1188400",
    "end": "1190559"
  },
  {
    "text": "main llm and I'm going to run this for",
    "start": "1190559",
    "end": "1193320"
  },
  {
    "text": "testing and what you'll see now if I run",
    "start": "1193320",
    "end": "1195799"
  },
  {
    "text": "this cell again is now the the main llm",
    "start": "1195799",
    "end": "1200159"
  },
  {
    "text": "desired count is three which means we",
    "start": "1200159",
    "end": "1202640"
  },
  {
    "text": "want three components or three versions",
    "start": "1202640",
    "end": "1205080"
  },
  {
    "text": "or three copies of that um llama 27b",
    "start": "1205080",
    "end": "1209200"
  },
  {
    "text": "model on the same",
    "start": "1209200",
    "end": "1210840"
  },
  {
    "text": "endpoint and if I go ahead and take a",
    "start": "1210840",
    "end": "1213880"
  },
  {
    "text": "look at the status and here's a little",
    "start": "1213880",
    "end": "1217240"
  },
  {
    "text": "utility code to help you understand",
    "start": "1217240",
    "end": "1219559"
  },
  {
    "text": "what's really happening if you see",
    "start": "1219559",
    "end": "1221480"
  },
  {
    "text": "currently the copy count is one but the",
    "start": "1221480",
    "end": "1224280"
  },
  {
    "text": "desired is three and what you'll see is",
    "start": "1224280",
    "end": "1226600"
  },
  {
    "text": "sagemaker Will behind the scenes",
    "start": "1226600",
    "end": "1228120"
  },
  {
    "text": "increase the uh the copy for um for the",
    "start": "1228120",
    "end": "1232640"
  },
  {
    "text": "main llm and go ahead and expand this",
    "start": "1232640",
    "end": "1236720"
  },
  {
    "text": "out so now we pause and we wait for the",
    "start": "1238200",
    "end": "1242280"
  },
  {
    "text": "copy count to increase the current copy",
    "start": "1242280",
    "end": "1244440"
  },
  {
    "text": "count to increase to",
    "start": "1244440",
    "end": "1247159"
  },
  {
    "text": "three there we go so now we see that the",
    "start": "1247159",
    "end": "1252120"
  },
  {
    "text": "the new current copy count is three",
    "start": "1252120",
    "end": "1254240"
  },
  {
    "text": "which means now you have three copies of",
    "start": "1254240",
    "end": "1256480"
  },
  {
    "text": "the inference component running on your",
    "start": "1256480",
    "end": "1259159"
  },
  {
    "text": "stagemaker",
    "start": "1259159",
    "end": "1260840"
  },
  {
    "text": "instance so there you have it folks we",
    "start": "1260840",
    "end": "1263039"
  },
  {
    "text": "do want to make sure that uh we do a",
    "start": "1263039",
    "end": "1265640"
  },
  {
    "text": "little bit of cleanup as well so you're",
    "start": "1265640",
    "end": "1267039"
  },
  {
    "text": "not incurring your costs now let's clean",
    "start": "1267039",
    "end": "1269280"
  },
  {
    "text": "up our our our inference components the",
    "start": "1269280",
    "end": "1272559"
  },
  {
    "text": "models and the inference endpoint so",
    "start": "1272559",
    "end": "1275240"
  },
  {
    "text": "first we delete the scaling policy uh we",
    "start": "1275240",
    "end": "1278679"
  },
  {
    "text": "specify the same parameters as before",
    "start": "1278679",
    "end": "1281120"
  },
  {
    "text": "the endpoint name the resource ID which",
    "start": "1281120",
    "end": "1283039"
  },
  {
    "text": "is the inference component and the",
    "start": "1283039",
    "end": "1284960"
  },
  {
    "text": "scalable Dimension we'll deregister the",
    "start": "1284960",
    "end": "1287640"
  },
  {
    "text": "scalable Target and go ahead and delete",
    "start": "1287640",
    "end": "1291279"
  },
  {
    "text": "the component let's let's take a look at",
    "start": "1291279",
    "end": "1294919"
  },
  {
    "text": "this and",
    "start": "1294919",
    "end": "1297039"
  },
  {
    "text": "this and if I go ahead and run my",
    "start": "1297039",
    "end": "1300240"
  },
  {
    "text": "CLI you'll see that now both the",
    "start": "1300240",
    "end": "1302559"
  },
  {
    "text": "components are in deleting",
    "start": "1302559",
    "end": "1305159"
  },
  {
    "text": "stage and I can go ahead and um delete",
    "start": "1305159",
    "end": "1308880"
  },
  {
    "text": "the actual endpoint as well behind the",
    "start": "1308880",
    "end": "1312039"
  },
  {
    "text": "scenes and remove the endpoint",
    "start": "1312039",
    "end": "1315679"
  },
  {
    "text": "configuration the model itself",
    "start": "1315679",
    "end": "1319799"
  },
  {
    "text": "the Llama guard model and then the main",
    "start": "1319799",
    "end": "1321520"
  },
  {
    "text": "model behind the",
    "start": "1321520",
    "end": "1322880"
  },
  {
    "text": "scenes and these are a few helper",
    "start": "1322880",
    "end": "1324840"
  },
  {
    "text": "functions to retain your variables uh",
    "start": "1324840",
    "end": "1327520"
  },
  {
    "text": "locally on disk if you're doing this on",
    "start": "1327520",
    "end": "1329440"
  },
  {
    "text": "your laptop or in a sagemaker",
    "start": "1329440",
    "end": "1330840"
  },
  {
    "text": "environment these will help retain your",
    "start": "1330840",
    "end": "1333480"
  },
  {
    "text": "your local variables uh across multiple",
    "start": "1333480",
    "end": "1335960"
  },
  {
    "text": "bre stars um let's recap what we just",
    "start": "1335960",
    "end": "1339240"
  },
  {
    "text": "saw we deployed both llama guard the awq",
    "start": "1339240",
    "end": "1343400"
  },
  {
    "text": "quantize version as well as llama 27b",
    "start": "1343400",
    "end": "1347039"
  },
  {
    "text": "the the regular version uh um onto a",
    "start": "1347039",
    "end": "1349679"
  },
  {
    "text": "single sagemaker endpoint with inference",
    "start": "1349679",
    "end": "1352360"
  },
  {
    "text": "components we were able to send in a",
    "start": "1352360",
    "end": "1354360"
  },
  {
    "text": "request to L guard we sent in a couple",
    "start": "1354360",
    "end": "1356760"
  },
  {
    "text": "of different requests one which was safe",
    "start": "1356760",
    "end": "1358600"
  },
  {
    "text": "and the other that was not safe the one",
    "start": "1358600",
    "end": "1360760"
  },
  {
    "text": "that was safe can be sent ahead to the",
    "start": "1360760",
    "end": "1364080"
  },
  {
    "text": "Llama 2 endpoint behind the scenes and",
    "start": "1364080",
    "end": "1367520"
  },
  {
    "text": "then we got a response uh that we can",
    "start": "1367520",
    "end": "1370240"
  },
  {
    "text": "then safely send back to to the end user",
    "start": "1370240",
    "end": "1373400"
  },
  {
    "text": "here are a few resources that will be",
    "start": "1373400",
    "end": "1376200"
  },
  {
    "text": "helpful for you when you're going",
    "start": "1376200",
    "end": "1377760"
  },
  {
    "text": "through the the example that we just",
    "start": "1377760",
    "end": "1379559"
  },
  {
    "text": "talked about uh the documentation and a",
    "start": "1379559",
    "end": "1382120"
  },
  {
    "text": "Blog of inference components is out here",
    "start": "1382120",
    "end": "1384240"
  },
  {
    "text": "we also have uh linked the example",
    "start": "1384240",
    "end": "1386799"
  },
  {
    "text": "notebook that we just ran through so",
    "start": "1386799",
    "end": "1388480"
  },
  {
    "text": "that you can do this on your own time",
    "start": "1388480",
    "end": "1390799"
  },
  {
    "text": "and I'd also like you to take a look at",
    "start": "1390799",
    "end": "1392840"
  },
  {
    "text": "the model prompts uh for lar because",
    "start": "1392840",
    "end": "1395480"
  },
  {
    "text": "this is actually very Nifty you can go",
    "start": "1395480",
    "end": "1397080"
  },
  {
    "text": "ahead and create your own unsafe",
    "start": "1397080",
    "end": "1398760"
  },
  {
    "text": "categories uh things like maybe not",
    "start": "1398760",
    "end": "1400880"
  },
  {
    "text": "mention competitors uh things like not",
    "start": "1400880",
    "end": "1403600"
  },
  {
    "text": "mention safety and and categories of",
    "start": "1403600",
    "end": "1406559"
  },
  {
    "text": "that nature uh this is this very good",
    "start": "1406559",
    "end": "1408960"
  },
  {
    "text": "paper to to read through thank you so",
    "start": "1408960",
    "end": "1411400"
  },
  {
    "text": "much for your time uh my name is abish",
    "start": "1411400",
    "end": "1413799"
  },
  {
    "text": "shad and I hope you have fun",
    "start": "1413799",
    "end": "1415960"
  },
  {
    "text": "experimenting with sage maker in prin",
    "start": "1415960",
    "end": "1417640"
  },
  {
    "text": "components",
    "start": "1417640",
    "end": "1420640"
  }
]