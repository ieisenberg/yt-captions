[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "all right hello everyone thanks for joining us my name is Roy Hassan I'm the",
    "start": "530",
    "end": "6839"
  },
  {
    "text": "business development for Amazon Athena with AWS I'm joined with Shannon jaudy",
    "start": "6839",
    "end": "13740"
  },
  {
    "text": "he is a principal engineer from st. grid all right so today a few things we",
    "start": "13740",
    "end": "21630"
  },
  {
    "text": "wanted to cover the first thing I wanted to talk about it's kind of walk you through a few common customer use cases",
    "start": "21630",
    "end": "28619"
  },
  {
    "text": "or trends that we're seeing with all of our customers using Amazon Athena the next thing is I want to talk about",
    "start": "28619",
    "end": "34980"
  },
  {
    "text": "something new that we're announcing at reinvent today we call this workload",
    "start": "34980",
    "end": "40500"
  },
  {
    "text": "isolation in cost controls this is something that a lot of our customers have been asking for and it's here so",
    "start": "40500",
    "end": "46260"
  },
  {
    "text": "we'll talk about that and then lastly I'll pass it on to Shane and he'll talk to you about how SendGrid uses Amazon",
    "start": "46260",
    "end": "52590"
  },
  {
    "text": "Athena to sort of reinvent and we think a popular feature email activity okay so",
    "start": "52590",
    "end": "61770"
  },
  {
    "start": "60000",
    "end": "60000"
  },
  {
    "text": "Amazon Athena was first introduced around this time in 2016",
    "start": "61770",
    "end": "67229"
  },
  {
    "text": "since then we saw a lot of customers you know yourself in the room here using",
    "start": "67229",
    "end": "72450"
  },
  {
    "text": "Athena and a number of different ways really exciting use cases really interesting use cases we see Amazon",
    "start": "72450",
    "end": "78570"
  },
  {
    "text": "Athena being used to dive into service logs whether it's AWS service logs your",
    "start": "78570",
    "end": "84299"
  },
  {
    "text": "own server infrastructure logs and analyzing that data and really understanding what's going on we see",
    "start": "84299",
    "end": "89490"
  },
  {
    "text": "customers are looking at intrusion detection right can we understand who is accessing our infrastructure and how",
    "start": "89490",
    "end": "95100"
  },
  {
    "text": "they're accessing the infrastructure and can we can we block them right we're",
    "start": "95100",
    "end": "100140"
  },
  {
    "text": "seeing customers using Amazon Athena to take off ad hoc usage or ad hoc queries",
    "start": "100140",
    "end": "105930"
  },
  {
    "text": "from existing data warehouse is kind of reducing the overall pressure on these mission critical systems and moving them",
    "start": "105930",
    "end": "113790"
  },
  {
    "text": "to - Athena also looking at using Athena with the AP is that we have to build",
    "start": "113790",
    "end": "119790"
  },
  {
    "text": "some new and exciting applications and services and Shane will walk us through an example of what sangre did with these",
    "start": "119790",
    "end": "126180"
  },
  {
    "text": "applicator with the api's so customer trends so before we get started",
    "start": "126180",
    "end": "133930"
  },
  {
    "text": "what I want to do is kind of work backwards from from the problem and let's identify sort of who our target",
    "start": "133930",
    "end": "139930"
  },
  {
    "text": "audiences who's the target user so there's two users here that you see on the screen the first one is the data consumer the data consumer is is really",
    "start": "139930",
    "end": "148209"
  },
  {
    "text": "interested with being able to discover available data sets and be able to",
    "start": "148209",
    "end": "153489"
  },
  {
    "text": "easily query the data without really having to worry about too much do I need to move the data how do I access the",
    "start": "153489",
    "end": "159639"
  },
  {
    "text": "data but also being able to bring their own tool and say I'm very familiar with sequel workbench or some other tools and",
    "start": "159639",
    "end": "165519"
  },
  {
    "text": "I want to use those tools to create my data the second user that we have here is a data engineer right the data",
    "start": "165519",
    "end": "171549"
  },
  {
    "text": "engineer is concerned with properly securing protecting and delivering this data in a manner and a maintainable and",
    "start": "171549",
    "end": "178959"
  },
  {
    "text": "scalable way right so we still need these these data engineers to help us bring the data from its raw form through",
    "start": "178959",
    "end": "186250"
  },
  {
    "text": "some kind of data pipeline process that secure it encrypt it and then make it available to the data consumer in a way",
    "start": "186250",
    "end": "193120"
  },
  {
    "text": "that's easily consumable for them so the",
    "start": "193120",
    "end": "198879"
  },
  {
    "text": "first years care so the first trend that we see with our customers is data leaks and what data leaks are in one sentence",
    "start": "198879",
    "end": "205419"
  },
  {
    "text": "and you know there's a lot of pre mutations to this and you'll see much of this throughout reinvent but basically",
    "start": "205419",
    "end": "211629"
  },
  {
    "text": "it's it's taking data taking all of your data and be able to store it in a common",
    "start": "211629",
    "end": "217720"
  },
  {
    "text": "single repository inside of the cloud in AWS so then you can take that data and",
    "start": "217720",
    "end": "223120"
  },
  {
    "text": "you can use it in a number of different ways now the concept or another idea in a data like is having the separation of",
    "start": "223120",
    "end": "229000"
  },
  {
    "text": "storage metadata and compute so you can pick the best tool for the job for each",
    "start": "229000",
    "end": "234340"
  },
  {
    "text": "layer of the architecture and be able to scale it independently the animation",
    "start": "234340",
    "end": "242199"
  },
  {
    "text": "plane so to build the data Lake the first thing the data engineer really needs to do is they need to understand",
    "start": "242199",
    "end": "247750"
  },
  {
    "text": "the the source systems right understanding what the data where data",
    "start": "247750",
    "end": "253030"
  },
  {
    "text": "is coming from what format it's in what do I need to get out of that data and there's a number of different things and",
    "start": "253030",
    "end": "258549"
  },
  {
    "text": "you see on the left side you know mostly streaming type of applications but on the bottom EDW enterprise data warehouse",
    "start": "258549",
    "end": "265380"
  },
  {
    "text": "that's more Tredici maybe bigger larger type of operational data but one example of",
    "start": "265380",
    "end": "272660"
  },
  {
    "text": "taking that data in is using Amazon Kinesis data firehose so this particular example is about streaming data how can",
    "start": "272660",
    "end": "280010"
  },
  {
    "start": "277000",
    "end": "277000"
  },
  {
    "text": "we stream the data in to our data Lake and be able to process very easily and",
    "start": "280010",
    "end": "285410"
  },
  {
    "text": "I'll show I'll show you in a minute another way to do this but there's a number of different ways to get data into your data Lake so one of the things",
    "start": "285410",
    "end": "294110"
  },
  {
    "text": "that Amazon Kinesis data firehose announced was having the ability to take",
    "start": "294110",
    "end": "299540"
  },
  {
    "text": "that raw data and actually convert it right take it from its raw JSON form or",
    "start": "299540",
    "end": "304880"
  },
  {
    "text": "CSV form and convert it to a query optimized for format and when we say query optimized it means something like",
    "start": "304880",
    "end": "311420"
  },
  {
    "text": "Apache park' or see that our columnar file formats that are better more efficient for for querying with Athena",
    "start": "311420",
    "end": "317810"
  },
  {
    "text": "the other thing it also does is it it structures the data on s3 in a folder",
    "start": "317810",
    "end": "323060"
  },
  {
    "text": "structure that's easy to manage and also help us improve performance and cost",
    "start": "323060",
    "end": "328520"
  },
  {
    "text": "when we create our data all right so",
    "start": "328520",
    "end": "334670"
  },
  {
    "start": "332000",
    "end": "332000"
  },
  {
    "text": "interesting streaming data is one common example and you'll see this with a lot of our customers but another requirement",
    "start": "334670",
    "end": "341570"
  },
  {
    "text": "that we sees be able to extract operation on also business data from",
    "start": "341570",
    "end": "346670"
  },
  {
    "text": "existing databases in data warehouse and this could be you know from redshift it can be from any other kind of data",
    "start": "346670",
    "end": "352550"
  },
  {
    "text": "warehouse from Aurora from dynamo dB right how do we get that data out of those systems bring it into our data",
    "start": "352550",
    "end": "358520"
  },
  {
    "text": "Lake right and start doing analytics and machine learning on top of that so this",
    "start": "358520",
    "end": "365900"
  },
  {
    "text": "process can be done in a number of different ways but the common ones are a one-way copy and also incrementally",
    "start": "365900",
    "end": "373610"
  },
  {
    "text": "using something called change sorry change data capture right CDC AWS",
    "start": "373610",
    "end": "379310"
  },
  {
    "text": "database migration service is a service that allows you to do just that to take data from one source system and move it",
    "start": "379310",
    "end": "387020"
  },
  {
    "text": "over to another source system one neat feature of database migration services actually lets you save that data not",
    "start": "387020",
    "end": "393320"
  },
  {
    "text": "into a target database but into a three in a CSV file format the other neat",
    "start": "393320",
    "end": "398990"
  },
  {
    "text": "thing last week we announced a new feature for DMS that lets you write the data events right as",
    "start": "398990",
    "end": "405180"
  },
  {
    "text": "you get these events in real time from your source database right those events",
    "start": "405180",
    "end": "410370"
  },
  {
    "text": "to Kinesis data streams so now we're turning this process into a streaming process and be able to push it through",
    "start": "410370",
    "end": "416820"
  },
  {
    "text": "the pipeline I showed you earlier where we take the streaming events send them to Kinesis data firehose and then write",
    "start": "416820",
    "end": "423120"
  },
  {
    "text": "them out into a3 in columnar optimized file formats and proper partitioning so it's another way of tying this this into",
    "start": "423120",
    "end": "431250"
  },
  {
    "text": "our data like ingestion process all right next our data engineer really has",
    "start": "431250",
    "end": "438780"
  },
  {
    "start": "435000",
    "end": "435000"
  },
  {
    "text": "a couple decisions to make so a typical data processing pipeline there are",
    "start": "438780",
    "end": "444300"
  },
  {
    "text": "common tasks that need to be done things like deduplication removing Knolls",
    "start": "444300",
    "end": "449750"
  },
  {
    "text": "reformatting data and time fields those are all part of data processing jobs",
    "start": "449750",
    "end": "456120"
  },
  {
    "text": "that anybody has to do these tasks you know can easily be automated using AWS glue ETL and configured to be able to",
    "start": "456120",
    "end": "465330"
  },
  {
    "text": "process the data on regular intervals right as data flows in from Kinesis or from other sources we want to be able to",
    "start": "465330",
    "end": "472020"
  },
  {
    "text": "sort of say let's deduplicate right let's clean this up let's reformat and restructure some of the data and we can",
    "start": "472020",
    "end": "477840"
  },
  {
    "text": "automate it using a Debates glue ETL so what we're doing here is we're really",
    "start": "477840",
    "end": "484229"
  },
  {
    "text": "removing the need for the end consumer the data consumer to do these kind of mundane tasks that are typical for their",
    "start": "484229",
    "end": "491430"
  },
  {
    "text": "workload right they know they need to deduplicate they need to to restructure things we can do that ahead of time so",
    "start": "491430",
    "end": "496830"
  },
  {
    "text": "when they get the data they can actually do their job and not worry about it other times there's some unique data",
    "start": "496830",
    "end": "504780"
  },
  {
    "text": "transformations that need to be performed there could be more complex they could be more specific maybe you",
    "start": "504780",
    "end": "510090"
  },
  {
    "text": "got some specific custom functions that you need to run on the data and process the data maybe there's a machine learning model",
    "start": "510090",
    "end": "517070"
  },
  {
    "text": "retraining that you need to do the data engineer that has the option of spinning",
    "start": "517070",
    "end": "522330"
  },
  {
    "text": "Amazon EMR manage clusters to be able to take advantage of the power and the flexibility and the tools available to",
    "start": "522330",
    "end": "528450"
  },
  {
    "text": "process that data in a unique way and then bring it back into the pipeline so the next piece",
    "start": "528450",
    "end": "536680"
  },
  {
    "text": "that you know that we have to go through is once all the data is processed and",
    "start": "536680",
    "end": "542060"
  },
  {
    "text": "stored in s3 we have the option now to store the data encrypted in s3 and we",
    "start": "542060",
    "end": "547850"
  },
  {
    "text": "can use s3 standard encryption or we can let AWS KMS manage the keys for us so",
    "start": "547850",
    "end": "554660"
  },
  {
    "text": "there you have that flexibility the other thing that our data engineer now needs to do is they'll need to configure",
    "start": "554660",
    "end": "560170"
  },
  {
    "text": "AWS Glueck rollers and these crawlers whether what they do is they actually look at the data that you stored in s3",
    "start": "560170",
    "end": "567010"
  },
  {
    "text": "they understand the schema and a technical metadata of that and then they create the appropriate tables inside of",
    "start": "567010",
    "end": "573980"
  },
  {
    "text": "the glue data catalog to represent those tables so later when we're querying it we can simply reference those tables and",
    "start": "573980",
    "end": "581750"
  },
  {
    "text": "be able to understand which columns and where does the data reside after the",
    "start": "581750",
    "end": "588770"
  },
  {
    "text": "data has been cataloged and we have our our tables the next thing we want to do is you want to configure permissions",
    "start": "588770",
    "end": "593960"
  },
  {
    "text": "right typically in a normal work environment maybe you want to open some of the data to your your users but as",
    "start": "593960",
    "end": "600050"
  },
  {
    "text": "you creating data leaks you want to be able to protect it you want to be able to restrict which user is access to",
    "start": "600050",
    "end": "605150"
  },
  {
    "text": "which data whether it has you know personal identifying information we want to restrict that even more and make sure",
    "start": "605150",
    "end": "611600"
  },
  {
    "text": "the right users have access to the right tables so something new in 2018 in",
    "start": "611600",
    "end": "618200"
  },
  {
    "start": "615000",
    "end": "615000"
  },
  {
    "text": "October AWS Glu data catalog released this feature that we call fine-grained",
    "start": "618200",
    "end": "623570"
  },
  {
    "text": "access control and what this does it allows you to attach policies to an IM",
    "start": "623570",
    "end": "629060"
  },
  {
    "text": "identity such as a user a group or role as well as AWS Glu data catalog",
    "start": "629060",
    "end": "635240"
  },
  {
    "text": "resources like databases and tables and you can see in this example that we're creating a read-only policy to that all",
    "start": "635240",
    "end": "645410"
  },
  {
    "text": "the tables in the the example on the score DB database so this is a feature",
    "start": "645410",
    "end": "651020"
  },
  {
    "text": "that you can go ahead and use today and be able to restrict users to access the",
    "start": "651020",
    "end": "656060"
  },
  {
    "text": "data using these these policies",
    "start": "656060",
    "end": "661000"
  },
  {
    "text": "okay all right not clicking okay here we",
    "start": "663250",
    "end": "670660"
  },
  {
    "start": "670000",
    "end": "670000"
  },
  {
    "text": "go so the next part once we sort of did that once we just at all the data we",
    "start": "670660",
    "end": "677200"
  },
  {
    "text": "prepared everything for querying now our data engineer see if this works again no",
    "start": "677200",
    "end": "683380"
  },
  {
    "text": "it's weird alright now our data engineer exposes Amazon Athena to the to the data",
    "start": "683380",
    "end": "690070"
  },
  {
    "text": "consumer right so once we created our data Lake now we give the data consumer",
    "start": "690070",
    "end": "695170"
  },
  {
    "text": "an interface right a way to query the data inside of our data link what the data engineer also needs to do is they",
    "start": "695170",
    "end": "700990"
  },
  {
    "text": "need to grant that data consumer access to the Athena service so they have to define the route policies that will give",
    "start": "700990",
    "end": "706990"
  },
  {
    "text": "the user ability to execute queries on top of the Athena service they also have",
    "start": "706990",
    "end": "712390"
  },
  {
    "text": "to define permissions for databases and tables like we saw in a previous slide that tells them which users which tables",
    "start": "712390",
    "end": "718960"
  },
  {
    "text": "they can access in which one they cannot and then lastly also give them access to the s3 buckets where the physical data",
    "start": "718960",
    "end": "724600"
  },
  {
    "text": "actually resides so user can execute",
    "start": "724600",
    "end": "730480"
  },
  {
    "text": "these queries using the Athena console so if you log in we have a sequel",
    "start": "730480",
    "end": "735700"
  },
  {
    "text": "interface you can go in and type your queries you can use BI tools Amazon",
    "start": "735700",
    "end": "740710"
  },
  {
    "text": "quick site is a great choice for that there's a lot of other bi tools that you can use as well you can use the AWS CLI",
    "start": "740710",
    "end": "747580"
  },
  {
    "text": "so if you're using CLI tools for automation or you're building kind of script you can use that as well the SDK",
    "start": "747580",
    "end": "754420"
  },
  {
    "text": "as well so you'll see from an in change talk how they're using the SDK the api's to be able to run queries on athena and",
    "start": "754420",
    "end": "761530"
  },
  {
    "text": "then also the JDBC and ODBC drivers they're available for you today to be able to connect popular BI tools and",
    "start": "761530",
    "end": "769450"
  },
  {
    "text": "I'll talk about that in a minute as well just to kind of show you how it's it's common to execute ad-hoc queries and we",
    "start": "769450",
    "end": "775810"
  },
  {
    "text": "see that a lot but also using the CL is and the API is to build automation right",
    "start": "775810",
    "end": "781150"
  },
  {
    "text": "to build processes to automate your queries run them in a background less interactive but it's--it's fits better",
    "start": "781150",
    "end": "787810"
  },
  {
    "text": "into a data processing pipeline so this",
    "start": "787810",
    "end": "793660"
  },
  {
    "text": "is just another example where on the big box we have sort of data Lake so what we've defined so far",
    "start": "793660",
    "end": "799000"
  },
  {
    "text": "and on the right side we can bring these different tools we can bring different processes to do different things so",
    "start": "799000",
    "end": "804940"
  },
  {
    "text": "reporting and analytics can bring using JDBC and ODBC drivers any number of BI tools machine learning you'll see in a",
    "start": "804940",
    "end": "811750"
  },
  {
    "text": "minute when I talk about Amazon Sage maker and how can we integrate Amazon sage maker for machine learning and and",
    "start": "811750",
    "end": "817420"
  },
  {
    "text": "model preparation and exploration how can we integrate that into our data link also custom applications right how can I",
    "start": "817420",
    "end": "824320"
  },
  {
    "text": "build custom applications to integrate into my data Lake and use that critical",
    "start": "824320",
    "end": "829360"
  },
  {
    "text": "data to start making some data-driven decisions and and build applications or driven based on my data here's an",
    "start": "829360",
    "end": "838390"
  },
  {
    "text": "example of just a few partners that support DD the Amazon Athena JDBC no DC",
    "start": "838390",
    "end": "845590"
  },
  {
    "text": "drivers this is just a subset of them is a whole lot of other ones that you can use definitely feel free to to go out",
    "start": "845590",
    "end": "853090"
  },
  {
    "text": "there and use it don't think that if your preferred BI vendors on the list it's not supported go into our website",
    "start": "853090",
    "end": "859600"
  },
  {
    "text": "we have a longer list and then the folks integrated with our drivers alright so",
    "start": "859600",
    "end": "866350"
  },
  {
    "start": "864000",
    "end": "864000"
  },
  {
    "text": "we talked about consuming the data so one one user or one consumer of the data is a data analyst right so when we look",
    "start": "866350",
    "end": "874330"
  },
  {
    "text": "at this you know there's a number of things that we did over the this you know 20 18 some improvements and",
    "start": "874330",
    "end": "880660"
  },
  {
    "text": "optimization that we've done to really help this din analyst be more efficient and be able to get access to the data",
    "start": "880660",
    "end": "887530"
  },
  {
    "text": "much quicker so the first thing we did was we released a newer version of our",
    "start": "887530",
    "end": "893740"
  },
  {
    "text": "JDBC know DBC drivers with two to five X improvement in performance so these are",
    "start": "893740",
    "end": "899920"
  },
  {
    "text": "some improvements that we've done to the driver itself so it gives your users that are using the JDBC drivers or the",
    "start": "899920",
    "end": "905620"
  },
  {
    "text": "ODBC drivers with BI tools it gives them much better experience much more snappier and and something",
    "start": "905620",
    "end": "911950"
  },
  {
    "text": "that they're more used to create table a select so this is a feature that we also",
    "start": "911950",
    "end": "918040"
  },
  {
    "text": "introduced it's a really neat feature it basically takes I creates a brand new table from the results of another query",
    "start": "918040",
    "end": "925540"
  },
  {
    "text": "so if you run a query and you want to take that result and we're save as a table and then we used a table for",
    "start": "925540",
    "end": "932190"
  },
  {
    "text": "for future queries or let your other users create a table create table a select let you do that the other thing",
    "start": "932190",
    "end": "938640"
  },
  {
    "text": "that create a blast select allows you to do is actually reformat the data so if you're starting with a table that's",
    "start": "938640",
    "end": "944970"
  },
  {
    "text": "based on text formats like CSV or JSON you can create a new table from the result set but actually save it in park'",
    "start": "944970",
    "end": "952800"
  },
  {
    "text": "or C file formats column agree optimize for file formats and the next time the",
    "start": "952800",
    "end": "958530"
  },
  {
    "text": "users query the data it's optimal much faster much more cost-effective yeah",
    "start": "958530",
    "end": "964920"
  },
  {
    "text": "it's working the last one is thin of views so this",
    "start": "964920",
    "end": "970950"
  },
  {
    "text": "really you know allows the analyst to really abstract some of the complex queries and transformations that they do",
    "start": "970950",
    "end": "978330"
  },
  {
    "text": "inside their sequel but and also really expose the the columns and the rows that the end-user is interested in so you can",
    "start": "978330",
    "end": "985680"
  },
  {
    "text": "come up with a complex query that has a lot of business logic while other transformations and then save it as a",
    "start": "985680",
    "end": "991680"
  },
  {
    "text": "view and then let the user that is consuming it only see that view so they're not really bothered by the",
    "start": "991680",
    "end": "997620"
  },
  {
    "text": "complexity of D of the actual query so these are really good enhancements and they're great features that we released",
    "start": "997620",
    "end": "1003080"
  },
  {
    "text": "over the year we see a lot of great feedback from customers around how this improved their their workflow helped",
    "start": "1003080",
    "end": "1009440"
  },
  {
    "text": "them be more efficient and really get access to the data quicker so here's",
    "start": "1009440",
    "end": "1015800"
  },
  {
    "text": "another new feature this is something that we recently announced or the I",
    "start": "1015800",
    "end": "1021740"
  },
  {
    "text": "believe last week so this is the change to an enhancement to the JDBC know DBC",
    "start": "1021740",
    "end": "1027589"
  },
  {
    "text": "drivers and it gives you the ability to federate access using Active Directory",
    "start": "1027590",
    "end": "1033250"
  },
  {
    "text": "so previously you'd have to use your AWS access key and secret key to be able to",
    "start": "1033250",
    "end": "1038600"
  },
  {
    "text": "authenticate the JDBC driver and use the service now what you can do is you can actually connect the driver to Microsoft",
    "start": "1038600",
    "end": "1045740"
  },
  {
    "text": "Active Directory so your users are logging in with their Active Directory credentials and are not bothered with",
    "start": "1045740",
    "end": "1051920"
  },
  {
    "text": "managing access key seeker key this is a really good feature also for the data engineer because now they don't have to",
    "start": "1051920",
    "end": "1058430"
  },
  {
    "text": "manage AWS access keys and distribute their users and with you know refreshing them and then",
    "start": "1058430",
    "end": "1065510"
  },
  {
    "text": "potential leaks and things like that so this makes it very very easy to integrate with new users coming on board",
    "start": "1065510",
    "end": "1071630"
  },
  {
    "text": "give them access to let them get start quickly all right automated reporting so",
    "start": "1071630",
    "end": "1078170"
  },
  {
    "start": "1075000",
    "end": "1075000"
  },
  {
    "text": "a common use case that we see for Athena is automating the execution of queries to generate operational and business",
    "start": "1078170",
    "end": "1084950"
  },
  {
    "text": "reports so you know here's a reference architecture this kind of helps you get",
    "start": "1084950",
    "end": "1090020"
  },
  {
    "text": "started it's not the full thing there's obviously changes that we can make to this to improve it but this is just a",
    "start": "1090020",
    "end": "1095720"
  },
  {
    "text": "way to get started so the first thing we do is we want to schedule or create a schedule and the function the calls the",
    "start": "1095720",
    "end": "1102890"
  },
  {
    "text": "Athena api's and kick-starts queries write this give me one query this could",
    "start": "1102890",
    "end": "1108080"
  },
  {
    "text": "be a whole bunch of different queries it's really up to you how you want to set this up but you can run your query",
    "start": "1108080",
    "end": "1113830"
  },
  {
    "text": "the next thing we want to do is we want to actually take the query ID that we got back from from those queries and we",
    "start": "1113830",
    "end": "1120590"
  },
  {
    "text": "just want to save them in Amazon DynamoDB and the reason we do that is this later we want to go back and look",
    "start": "1120590",
    "end": "1126560"
  },
  {
    "text": "at those at the status of those queries or we want to see if a query failed why did it fail I need to know the query ID",
    "start": "1126560",
    "end": "1133160"
  },
  {
    "text": "so I can go look them up so we store them in and DynamoDB you can go back and look at them later once the query",
    "start": "1133160",
    "end": "1140330"
  },
  {
    "text": "completes or the query is complete the result sets are actually written automatically for you into Amazon s3 the",
    "start": "1140330",
    "end": "1147200"
  },
  {
    "text": "next thing that happens once the data gets owned is three is you can actually set an s3 trigger on that particular",
    "start": "1147200",
    "end": "1153740"
  },
  {
    "text": "bucket that triggers a lambda function every time a new file is saved into that",
    "start": "1153740",
    "end": "1159230"
  },
  {
    "text": "bucket so what that does is it lets us actually take that take that notification and sort of say okay the",
    "start": "1159230",
    "end": "1165800"
  },
  {
    "text": "job is not complete I'm going to notify maybe an SNS topic and say the job",
    "start": "1165800",
    "end": "1172700"
  },
  {
    "text": "complete let me send an email notification to a system maybe I'll send it to pager Duty maybe I'll send it some",
    "start": "1172700",
    "end": "1179840"
  },
  {
    "text": "other notification into an orchestration system that now tracks the progress of these reports but this is a very simple",
    "start": "1179840",
    "end": "1186500"
  },
  {
    "text": "kind of reference architecture several a solution that lets you run automated queries automated reports very easily",
    "start": "1186500",
    "end": "1195279"
  },
  {
    "text": "all right so I said I'll come back to the the data science use case and here is just one example so you can see again",
    "start": "1195950",
    "end": "1202720"
  },
  {
    "text": "on the left side we have our data like this this is sort of represents our data lake but we want to do is we want to",
    "start": "1202720",
    "end": "1209539"
  },
  {
    "text": "enable the data scientists the ability to access the data very easily of course",
    "start": "1209539",
    "end": "1214760"
  },
  {
    "text": "they can go to as three directly and pull the data in and play around with them and do what they need to do with it",
    "start": "1214760",
    "end": "1220279"
  },
  {
    "text": "but it's easier and actually more secure if we give them access through Athena to",
    "start": "1220279",
    "end": "1226399"
  },
  {
    "text": "be able to query that data that they want because what happens then is we already pre process the data we removed",
    "start": "1226399",
    "end": "1232100"
  },
  {
    "text": "all the duplicates and and reformatted things we applied security and could and",
    "start": "1232100",
    "end": "1237380"
  },
  {
    "text": "permissioning to that data so now all of that is already contained in our data like if we give our data scientist user",
    "start": "1237380",
    "end": "1244519"
  },
  {
    "text": "access via Athena to the data lake now we're controlling that the whole flow",
    "start": "1244519",
    "end": "1250600"
  },
  {
    "text": "pajas Tina is a really neat Python open source library that you can go in and",
    "start": "1250600",
    "end": "1257120"
  },
  {
    "text": "download and use and basically what that gives you is gives you a database type driver in a Python environment that",
    "start": "1257120",
    "end": "1264139"
  },
  {
    "text": "gives you access to Athena so it makes the workflow for that data scientist working inside a Python notebook very",
    "start": "1264139",
    "end": "1270919"
  },
  {
    "text": "very easy and and just get started right away without really having to learn any new tools or any new patterns of access",
    "start": "1270919",
    "end": "1279760"
  },
  {
    "start": "1279000",
    "end": "1279000"
  },
  {
    "text": "so applications so custom applications so you'll hear from Shane in a few",
    "start": "1279760",
    "end": "1285740"
  },
  {
    "text": "minutes sort of what they did but this is a really in my opinion a really really neat feature of Athena it really",
    "start": "1285740",
    "end": "1293029"
  },
  {
    "text": "gives you the ability to use the API to build new applications on top of the data so running sequel queries on top of",
    "start": "1293029",
    "end": "1299269"
  },
  {
    "text": "data almost like a database but the data resides in s3 so the storage cost remained very minimal the access pattern",
    "start": "1299269",
    "end": "1306500"
  },
  {
    "text": "is is very flexible and you have the ability to sort of control who access",
    "start": "1306500",
    "end": "1311750"
  },
  {
    "text": "the data and how you can see in this case AWS app sync if you're not familiar",
    "start": "1311750",
    "end": "1318710"
  },
  {
    "text": "with this they have a graph QL backend easily integrated into into Athena so",
    "start": "1318710",
    "end": "1325490"
  },
  {
    "text": "then you can run you can build applications whether they're mobile application or web-based applications that maintain",
    "start": "1325490",
    "end": "1332330"
  },
  {
    "text": "data for online access but also for offline access as the ability to cache some of that data which is really really",
    "start": "1332330",
    "end": "1338149"
  },
  {
    "text": "cool like I said before anybody's tools and",
    "start": "1338149",
    "end": "1343460"
  },
  {
    "text": "SDKs the CLI a really great way to use the service again you're not managing",
    "start": "1343460",
    "end": "1348679"
  },
  {
    "text": "databases you're not spinning up databases so if you do need access to that data and you don't want to deal",
    "start": "1348679",
    "end": "1355549"
  },
  {
    "text": "with database connections like JDBC know to BC you can do it via the API is in a CLI alright so up till now the common",
    "start": "1355549",
    "end": "1364639"
  },
  {
    "text": "use case was a data leak the next use case or trend I'm going to look at is inspecting AWS service logs so many of",
    "start": "1364639",
    "end": "1374359"
  },
  {
    "text": "the there we go okay so many of the AWS",
    "start": "1374359",
    "end": "1382070"
  },
  {
    "text": "services already store all of the log data into s3 so services like cloud",
    "start": "1382070",
    "end": "1387559"
  },
  {
    "text": "trail CloudFront classic and application load balancers they all store the data",
    "start": "1387559",
    "end": "1393499"
  },
  {
    "text": "inside of is three in in normally in in text formats like CSV and JSON and the",
    "start": "1393499",
    "end": "1400009"
  },
  {
    "text": "reason I do that is to really make that those logs really portable and accessible to any modern tool including",
    "start": "1400009",
    "end": "1407479"
  },
  {
    "text": "Athena so one key aspect of improving",
    "start": "1407479",
    "end": "1414259"
  },
  {
    "text": "query performance and reducing the cost per user is managing the amount of data",
    "start": "1414259",
    "end": "1419690"
  },
  {
    "text": "scan so you heard me talk about columnar file formats and partitioning at the end of the day when we talk about Athena the",
    "start": "1419690",
    "end": "1426529"
  },
  {
    "text": "the one point about performance and costs is data scans so if we can reduce",
    "start": "1426529",
    "end": "1433159"
  },
  {
    "text": "the amount of data that we scan we speed up the queries and we also reduce the amount of money that you pay at the end",
    "start": "1433159",
    "end": "1439339"
  },
  {
    "text": "of the day so that's something that we want to also take into consideration here so as you can see here we're using",
    "start": "1439339",
    "end": "1445489"
  },
  {
    "text": "AWS glue again the idea here is to take the data in convert it to two columnar",
    "start": "1445489",
    "end": "1452719"
  },
  {
    "text": "file formats optimize the data clean it and then push it back in to as three",
    "start": "1452719",
    "end": "1458349"
  },
  {
    "text": "actually back into a three in partitioned file formats previously you've seen Kinesis did a",
    "start": "1458349",
    "end": "1464549"
  },
  {
    "text": "firehose doing that in real time in a streaming way in this case we're actually taking bulk sets of logs and",
    "start": "1464549",
    "end": "1471449"
  },
  {
    "text": "we're running them through AWS blue ETL processing them doing kind of the same thing converting them to Parque",
    "start": "1471449",
    "end": "1478549"
  },
  {
    "text": "reformatting the sari formatting the the output is three location and then",
    "start": "1478549",
    "end": "1484499"
  },
  {
    "text": "writing the data back out into that",
    "start": "1484499",
    "end": "1487969"
  },
  {
    "text": "so when ingesting and storing log data many times it results in a lot of small files some of you already use Athena",
    "start": "1489929",
    "end": "1497039"
  },
  {
    "text": "today probably notice this where you have a lot of you know kilobyte worth files not megabytes not gigabytes a lot",
    "start": "1497039",
    "end": "1503940"
  },
  {
    "text": "of very small files so this is not really ideal when trying to query data it's definitely recommended to sort of",
    "start": "1503940",
    "end": "1510299"
  },
  {
    "text": "periodically merge these files into large ones so they're much more efficient for for query",
    "start": "1510299",
    "end": "1516419"
  },
  {
    "text": "however we're all working on an enhancement that would really improve query performance when accessing tables",
    "start": "1516419",
    "end": "1522989"
  },
  {
    "text": "that contain a lot of little small files so we understand it is a problem that customers have there are some best",
    "start": "1522989",
    "end": "1529319"
  },
  {
    "text": "practices that you can follow today to kind of get around it and we recommend you keep following them but we're also",
    "start": "1529319",
    "end": "1534839"
  },
  {
    "text": "working to try to improve this behind the scenes so there's there's less of that heavy lifting for you guys to do",
    "start": "1534839",
    "end": "1541940"
  },
  {
    "text": "all right so at this point deal has been converted to columnar format it's partition on s3 similar to our data",
    "start": "1542299",
    "end": "1549869"
  },
  {
    "text": "Lake use case now our data engineer is going to configure AWS glue to be able",
    "start": "1549869",
    "end": "1556409"
  },
  {
    "text": "to query that sorry to catalogue that data and then we can move on so we",
    "start": "1556409",
    "end": "1561449"
  },
  {
    "text": "catalog it permissioning of course the same thing as before and then we move on",
    "start": "1561449",
    "end": "1568739"
  },
  {
    "text": "so again our data engineer now exposes Amazon Athena to the end user then user",
    "start": "1568739",
    "end": "1575459"
  },
  {
    "text": "can start querying the data using the different tools that we have to offer all right so I'll go through this so",
    "start": "1575459",
    "end": "1582389"
  },
  {
    "text": "this is Athena workgroups this is a new feature that we are announcing today in",
    "start": "1582389",
    "end": "1587879"
  },
  {
    "start": "1585000",
    "end": "1585000"
  },
  {
    "text": "beta so Athena workgroups and you can you can see here I'll kind of read it to you again because I think it's a good",
    "start": "1587879",
    "end": "1593279"
  },
  {
    "text": "description so Athena workgroups are used to isolate queries between different teams workloads or",
    "start": "1593279",
    "end": "1599000"
  },
  {
    "text": "applications and set limits on amount of data each query and the entire war group can process so think about this it gives",
    "start": "1599000",
    "end": "1606230"
  },
  {
    "text": "you workload isolation be able to separate different type of workloads ad hoc reporting automation it gives you",
    "start": "1606230",
    "end": "1613010"
  },
  {
    "text": "query metrics so now you can actually get the metrics the information about how long a query ran how much data",
    "start": "1613010",
    "end": "1618710"
  },
  {
    "text": "scanned whether the query succeeded or not available for you in car watch and",
    "start": "1618710",
    "end": "1624680"
  },
  {
    "text": "also give you the ability to set cost controls to say if a query exceeds a particular threshold of amount of data",
    "start": "1624680",
    "end": "1632120"
  },
  {
    "text": "scanned I'm gonna cancel that query right I'm gonna send some kind of notification to let somebody know that",
    "start": "1632120",
    "end": "1637700"
  },
  {
    "text": "hey something is going on here so creating war groups this is just a",
    "start": "1637700",
    "end": "1644570"
  },
  {
    "text": "screenshot of what that looks like so the engineers will have to go in to",
    "start": "1644570",
    "end": "1650510"
  },
  {
    "text": "find a work group so whether they're you know the finding work groups for different business units different teams inside of the organization they can",
    "start": "1650510",
    "end": "1657890"
  },
  {
    "text": "actually define the output location of the work group so if I have an ad-hoc team and I have a dev up team and have",
    "start": "1657890",
    "end": "1664940"
  },
  {
    "text": "an engineering team they're all querying and doing different things I want the output of their queries to be located in",
    "start": "1664940",
    "end": "1671150"
  },
  {
    "text": "an s3 bucket that's owned and dedicated to them so I have an ability through",
    "start": "1671150",
    "end": "1676160"
  },
  {
    "text": "work groups to define that the other one I can also set km as keys so if I have",
    "start": "1676160",
    "end": "1682130"
  },
  {
    "text": "different users of of work groups each one of them has different kms key requirements we can define it",
    "start": "1682130",
    "end": "1689080"
  },
  {
    "text": "independently here per work group we can publish all of those metrics into cloud watch like I said before and I'll show",
    "start": "1689080",
    "end": "1695240"
  },
  {
    "text": "you a screenshot of that and then the last one is the data engineer now has the ability to sort of override all user",
    "start": "1695240",
    "end": "1702070"
  },
  {
    "text": "configuration with the work group which what that means is that when new users are on-boarded to your data links to",
    "start": "1702070",
    "end": "1709310"
  },
  {
    "text": "query using Athena they can be put into work groups and by putting this check",
    "start": "1709310",
    "end": "1714770"
  },
  {
    "text": "box basically they don't have to configure anything on their end right they bring a bi tool they use the CLI to",
    "start": "1714770",
    "end": "1721400"
  },
  {
    "text": "use the API they log in and that's it all the configuration that we typically",
    "start": "1721400",
    "end": "1726410"
  },
  {
    "text": "give you with the API and JDBC now are automatically applied by the workgroup so it makes bringing",
    "start": "1726410",
    "end": "1732789"
  },
  {
    "text": "new users on board very easy and very seamless so here is just a metrics",
    "start": "1732789",
    "end": "1739019"
  },
  {
    "start": "1737000",
    "end": "1737000"
  },
  {
    "text": "report that you get from the from the workgroups screen I give you total bytes",
    "start": "1739019",
    "end": "1744070"
  },
  {
    "text": "cans for the workgroup successful queries failed queries you know query",
    "start": "1744070",
    "end": "1749529"
  },
  {
    "text": "executions per and then all this metrics actually goes into cloud watch so you get a per query metric you get a Perl or",
    "start": "1749529",
    "end": "1756850"
  },
  {
    "text": "group metric so it's an aggregation of all the queries inside the workgroup and then all of that goes into cloud watch",
    "start": "1756850",
    "end": "1762700"
  },
  {
    "text": "so if you have more advanced use cases around alarming and reporting you can do it in cloud watch all right so when we",
    "start": "1762700",
    "end": "1772509"
  },
  {
    "start": "1769000",
    "end": "1769000"
  },
  {
    "text": "speak about customers about having the ability to really control cost right for their Athena usage you know we commonly hear two main",
    "start": "1772509",
    "end": "1780159"
  },
  {
    "text": "scenarios so the first one is you know having users who can execute you know",
    "start": "1780159",
    "end": "1786730"
  },
  {
    "text": "whether they're bad queries or queries that just scan a lot more data than really expected being able to stop them",
    "start": "1786730",
    "end": "1792610"
  },
  {
    "text": "I'd not just let them run in rack up cost but let's give you the ability to stop them so we can in workgroup so you",
    "start": "1792610",
    "end": "1798399"
  },
  {
    "text": "can actually define that and say a per query limit if the query exceeds a certain amount of data scanned we can",
    "start": "1798399",
    "end": "1805570"
  },
  {
    "text": "cancel that query right away the other use case that we're seeing is giving",
    "start": "1805570",
    "end": "1812740"
  },
  {
    "text": "users sort of building automations and and more automated processes rather than ad-hoc and how do you separate the two",
    "start": "1812740",
    "end": "1820000"
  },
  {
    "text": "together right how do you say you know the ad-hoc users are running X amount of queries this is what their usage looks",
    "start": "1820000",
    "end": "1826269"
  },
  {
    "text": "like and the reporting systems are doing something different and this is what their experience looks like one of the",
    "start": "1826269",
    "end": "1832059"
  },
  {
    "text": "common challenges that we're seeing is that if you have automated scripts running and running queries when the ad",
    "start": "1832059",
    "end": "1838720"
  },
  {
    "text": "hoc users come in and look at their history right they're not able to be able to see what they ran because the",
    "start": "1838720",
    "end": "1845289"
  },
  {
    "text": "automated query is completely overwhelmed the query history right so now they don't know where they were you",
    "start": "1845289",
    "end": "1850809"
  },
  {
    "text": "know what they're running so by using this we can separate into work groups now the other thing we can do is within",
    "start": "1850809",
    "end": "1857889"
  },
  {
    "text": "the work group because we have different users different tools running and running queries we",
    "start": "1857889",
    "end": "1863960"
  },
  {
    "text": "want to be able to set a tiered or hierarchical thresholds so we can",
    "start": "1863960",
    "end": "1869690"
  },
  {
    "text": "actually notify the data engineer management as usage starts to increase",
    "start": "1869690",
    "end": "1875029"
  },
  {
    "text": "so in this case you can see here we're sending a limit of 10 gigabytes for the workgroup per hour so every hour we look",
    "start": "1875029",
    "end": "1882499"
  },
  {
    "text": "at the usage and we say are we hitting that threshold right and if we're past that threshold per hour we will send a cloud watch",
    "start": "1882499",
    "end": "1888950"
  },
  {
    "text": "notification and then an email can go out you know Lima can go out to management or whoever to take a look at",
    "start": "1888950",
    "end": "1895519"
  },
  {
    "text": "it the other we can then set and another threshold that said at a 24-hour level",
    "start": "1895519",
    "end": "1902059"
  },
  {
    "text": "if the workgroup exceeded a terabytes of data at this point every query that runs we're just gonna cancel them right we're",
    "start": "1902059",
    "end": "1908869"
  },
  {
    "text": "gonna basically disable the work group altogether and we're not gonna let any more queries execute and the idea here",
    "start": "1908869",
    "end": "1914749"
  },
  {
    "text": "is that something went wrong or either you know the users exceeded the amount",
    "start": "1914749",
    "end": "1920330"
  },
  {
    "text": "of data scan that you really wanted to allow we're just going to cancel we're just going to disable the work group and",
    "start": "1920330",
    "end": "1926049"
  },
  {
    "text": "basically stop all the queries and then you can go back and say okay is this a problem with the automation do I need to",
    "start": "1926049",
    "end": "1932299"
  },
  {
    "text": "up the limit what do I need to do but at least you're protecting yourself from incurring a lot of cost without having",
    "start": "1932299",
    "end": "1938509"
  },
  {
    "text": "any control over it and in last thing I",
    "start": "1938509",
    "end": "1945350"
  },
  {
    "text": "said is before all the different metrics go back into the cloud watch it's not just the cost control type of metrics",
    "start": "1945350",
    "end": "1950749"
  },
  {
    "text": "that you the usage so so you know query succeeded query failed which is really",
    "start": "1950749",
    "end": "1956210"
  },
  {
    "text": "helpful if you're trying to understand what is the overall experience of a work group okay and then the last one with",
    "start": "1956210",
    "end": "1964759"
  },
  {
    "text": "regards to war groups is around usage notifications so as I said before all these metrics go into cloud watch so you",
    "start": "1964759",
    "end": "1972109"
  },
  {
    "text": "can actually go into cloud watch and start build some more complex or more intricate sort of notifications and",
    "start": "1972109",
    "end": "1978859"
  },
  {
    "text": "reporting on top of that data without typically just relying on the graphs that we provide to you in the Athena",
    "start": "1978859",
    "end": "1984830"
  },
  {
    "text": "console we have a lot more flexibility in here and as cloud watch adds more features and functionality you can start",
    "start": "1984830",
    "end": "1990649"
  },
  {
    "text": "taking advantage of them all right when I head over to Shane now",
    "start": "1990649",
    "end": "1999230"
  },
  {
    "text": "all right thanks right all right so my name is",
    "start": "1999230",
    "end": "2004580"
  },
  {
    "text": "Shane Andrade and I work at SendGrid I'm a principal engineer there and I've been there about five and a half years if you",
    "start": "2004580",
    "end": "2010279"
  },
  {
    "text": "haven't heard of Sun grid this is not working for me Sun grid is an email SAS company and we send about 45 billion",
    "start": "2010279",
    "end": "2017570"
  },
  {
    "text": "emails a month to our from our 78,000",
    "start": "2017570",
    "end": "2022850"
  },
  {
    "text": "customers that reside in over 100 countries around the world and we ourselves",
    "start": "2022850",
    "end": "2028010"
  },
  {
    "text": "we're distributed amongst four offices mainly located in the US and one in London these are some of the top web",
    "start": "2028010",
    "end": "2036019"
  },
  {
    "text": "brands that trust us to send their emails for them you can see some pretty",
    "start": "2036019",
    "end": "2041120"
  },
  {
    "text": "popular brands like uber eBay and they trust us to send their email for them so",
    "start": "2041120",
    "end": "2046880"
  },
  {
    "text": "this is a product line we started in the transactional email space and if you're",
    "start": "2046880",
    "end": "2051888"
  },
  {
    "text": "not familiar with what a transactional email is it's basically an email that was triggered off with some user action",
    "start": "2051889",
    "end": "2057858"
  },
  {
    "text": "so like maybe an order confirmation or a password reset and these are just kind of one-off emails that are based off of",
    "start": "2057859",
    "end": "2064669"
  },
  {
    "text": "some one-time user action once we tackled that we jumped into the email",
    "start": "2064669",
    "end": "2070069"
  },
  {
    "text": "marketing space with our marketing campaigns application and this is where email marketers send their promotional",
    "start": "2070069",
    "end": "2076970"
  },
  {
    "text": "emails newsletters offers and such that they send in mass to their recipients we",
    "start": "2076970",
    "end": "2083720"
  },
  {
    "text": "also have something called the parse API which is a little bit different it receives an inbound email on your behalf",
    "start": "2083720",
    "end": "2089569"
  },
  {
    "text": "and we parse it for you and send that to your HTTP server so you can automate against that so you can handle things",
    "start": "2089569",
    "end": "2095990"
  },
  {
    "text": "like support ticket updates which is a really common use case we see for this feature and lastly which is what I'm",
    "start": "2095990",
    "end": "2102500"
  },
  {
    "text": "going to be talking about today is our email activity email activity is a diagnosis tool for our customers to be",
    "start": "2102500",
    "end": "2108950"
  },
  {
    "text": "able to help self-service their issues with SendGrid so maybe they're having some sort of deliverability issue or",
    "start": "2108950",
    "end": "2115250"
  },
  {
    "text": "they're not really sure if a particular email made it to a particular recipient they can log into SendGrid go to email",
    "start": "2115250",
    "end": "2121490"
  },
  {
    "text": "activity and they can view all of the events associated with that email message as it went through SendGrid",
    "start": "2121490",
    "end": "2128530"
  },
  {
    "text": "it's a little bit more about email activity we built it originally on Prem and the early days of SendGrid",
    "start": "2128530",
    "end": "2134720"
  },
  {
    "text": "and this is a high level of what it looks like so an email is sent through our mail pipeline from one of our",
    "start": "2134720",
    "end": "2141170"
  },
  {
    "text": "customers and it's sent to us that goes through our system and as it goes through our mail pipeline it the",
    "start": "2141170",
    "end": "2146809"
  },
  {
    "text": "pipeline generates various events as it makes its way through those events get collected and stored into elasticsearch",
    "start": "2146809",
    "end": "2153880"
  },
  {
    "text": "hopefully that email gets delivered on the right end there and to email provider like maybe Gmail or Yahoo and",
    "start": "2153880",
    "end": "2160790"
  },
  {
    "text": "if a recipient chooses to engage with that email meaning they open it or maybe click on a",
    "start": "2160790",
    "end": "2166579"
  },
  {
    "text": "link in that email then we collect those events as well and store them into elasticsearch so for any given message",
    "start": "2166579",
    "end": "2172670"
  },
  {
    "text": "that we receive from our customers and send for them we can see the full history of what happened to that email",
    "start": "2172670",
    "end": "2178910"
  },
  {
    "text": "hasn't made its way through Sun grid and into the users inbox and it really gives a holistic view of what the journey of",
    "start": "2178910",
    "end": "2186319"
  },
  {
    "text": "that email was this architecture had some limitations though one of the",
    "start": "2186319",
    "end": "2191720"
  },
  {
    "text": "limitations was that we could only store seven days of history for for a given user and the reason for this is there's",
    "start": "2191720",
    "end": "2200210"
  },
  {
    "text": "a couple of reasons but the main reason is this was a free offering as part of your SendGrid package and our elastic",
    "start": "2200210",
    "end": "2207349"
  },
  {
    "text": "search cluster was already pretty large and we didn't really want to invest any more hardware into it as it would really",
    "start": "2207349",
    "end": "2212480"
  },
  {
    "text": "eat into our our bottom line because it was a free service and so we kind of",
    "start": "2212480",
    "end": "2218270"
  },
  {
    "text": "just capped it at seven days and despite the fact that we had a lot of customers requesting more storage furthermore we",
    "start": "2218270",
    "end": "2227420"
  },
  {
    "text": "had some high volume customers high volume customers our customers that send more than you know maybe a hundred",
    "start": "2227420",
    "end": "2232490"
  },
  {
    "text": "million emails a month and they were actually limited to just two days",
    "start": "2232490",
    "end": "2237559"
  },
  {
    "text": "because of the amount of data that they were generating and some of the technical limitations on the elastic search side prevented us from paging",
    "start": "2237559",
    "end": "2244549"
  },
  {
    "text": "deeply into their data set and we could only really give them two days of their history so this was another limitation that we had on our previous system and",
    "start": "2244549",
    "end": "2253220"
  },
  {
    "text": "lastly as we went from that transactional space which is really focused on",
    "start": "2253220",
    "end": "2258430"
  },
  {
    "text": "developers as our customer we started going into that marketing space where we had a lot of non-technical users start",
    "start": "2258430",
    "end": "2264400"
  },
  {
    "text": "to join SendGrid we had email marketers small business owners and they didn't really understand",
    "start": "2264400",
    "end": "2269470"
  },
  {
    "text": "how to use this existing system of you know dealing with events rather than",
    "start": "2269470",
    "end": "2275380"
  },
  {
    "text": "something like you know emails which is what they're familiar with and they didn't really know how to translate the",
    "start": "2275380",
    "end": "2280390"
  },
  {
    "text": "concept of an event to a particular email and that was really burdensome for",
    "start": "2280390",
    "end": "2285430"
  },
  {
    "text": "them so it was at this point we decided okay we are going to reaaargh attack this but we need to think of a better",
    "start": "2285430",
    "end": "2292839"
  },
  {
    "text": "way to do this we had a few issues of how we were gonna get started and one of",
    "start": "2292839",
    "end": "2298960"
  },
  {
    "start": "2297000",
    "end": "2297000"
  },
  {
    "text": "those issues was around how we were gonna handle provisioning we knew that because some users wanted to have a",
    "start": "2298960",
    "end": "2308130"
  },
  {
    "text": "longer search history we could we could charge for this and the users that wanted more history we could provide",
    "start": "2308130",
    "end": "2313599"
  },
  {
    "text": "maybe a 30 date history for them and we had people who maybe didn't want that we could offer them maybe two or three days",
    "start": "2313599",
    "end": "2318760"
  },
  {
    "text": "of history and so we weren't really sure what the demand was gonna be and so we",
    "start": "2318760",
    "end": "2324790"
  },
  {
    "text": "had some risks around how we were gonna provision our hardware again this was all on prime at the time and so we",
    "start": "2324790",
    "end": "2330700"
  },
  {
    "text": "weren't really sure if the demand was going to be really high or really low so",
    "start": "2330700",
    "end": "2335920"
  },
  {
    "text": "if we under provisioned meaning that we had more users than our Hardware cousin could support that would affect other",
    "start": "2335920",
    "end": "2341589"
  },
  {
    "text": "people on the cluster query times would be slow we would probably have delays in",
    "start": "2341589",
    "end": "2346900"
  },
  {
    "text": "ingesting data so that was a risk for us and on the flip side of course if we over provisioned well then we're footing",
    "start": "2346900",
    "end": "2353680"
  },
  {
    "text": "the bill for all this hardware that's not being used and it's just sitting there idle so that was a risk for us as well so we knew that we wanted to build this",
    "start": "2353680",
    "end": "2359920"
  },
  {
    "text": "in the cloud and this was about two years ago when we started on this project and we",
    "start": "2359920",
    "end": "2366250"
  },
  {
    "text": "built a prototype in in AWS on cog search and it worked and it did",
    "start": "2366250",
    "end": "2374890"
  },
  {
    "text": "everything we needed it to do and as what I said about two years ago that's when Athena dropped so right after we",
    "start": "2374890",
    "end": "2380170"
  },
  {
    "text": "finished this prototype we start hearing things about Athena and we're like oh that actually sounds really interesting and our solution architect even sent us",
    "start": "2380170",
    "end": "2386440"
  },
  {
    "text": "over and you know a sample architecture of what it could look like using Athena and really started to intrigue us that's how",
    "start": "2386440",
    "end": "2392660"
  },
  {
    "text": "we got to Athena so replace that with Athena all right so some of the things",
    "start": "2392660",
    "end": "2401030"
  },
  {
    "text": "that actually drove us to use Athena one is that it was serverless this was a",
    "start": "2401030",
    "end": "2407750"
  },
  {
    "text": "huge plus for us this was our first step into the cloud as a company we really had no prior cloud experience and so",
    "start": "2407750",
    "end": "2414560"
  },
  {
    "text": "this really took one big risk away from us moving on to a completely new platform we didn't have to worry about",
    "start": "2414560",
    "end": "2420260"
  },
  {
    "text": "managing servers or provisioning anything and it made it really easy for us to get started",
    "start": "2420260",
    "end": "2425530"
  },
  {
    "text": "second was its elasticity the elasticity of of Athena and and and s3 were perfect",
    "start": "2425530",
    "end": "2435080"
  },
  {
    "text": "for our pricing model that we wanted to have where we had you know this free tier and we had this paid tier we didn't know what the demand was gonna be and",
    "start": "2435080",
    "end": "2441080"
  },
  {
    "text": "Athena charges you know basically per use and that was really that really fit our costing model very well and lastly",
    "start": "2441080",
    "end": "2449900"
  },
  {
    "text": "was the API integration it was very flexible we could integrate in a number of different ways we can use",
    "start": "2449900",
    "end": "2455240"
  },
  {
    "text": "the AWS SDKs wikis the CLI we can use JDBC I don't think that ODBC at the time",
    "start": "2455240",
    "end": "2462230"
  },
  {
    "text": "but because we were running this on go we can obviously do JDBC integrations so",
    "start": "2462230",
    "end": "2467619"
  },
  {
    "text": "the API integration was really simple for us to get started all right so this was our first",
    "start": "2467619",
    "end": "2473960"
  },
  {
    "text": "architecture during our beta and the way it worked was at this point we had all",
    "start": "2473960",
    "end": "2480740"
  },
  {
    "text": "the events that were getting emitted from the mail pipeline going to Kafka that's the little thing at the top and",
    "start": "2480740",
    "end": "2486050"
  },
  {
    "text": "from there we had a consumer Kafka called Sakura Sakura an open source",
    "start": "2486050",
    "end": "2491660"
  },
  {
    "text": "project from Pinterest that we just used with a few modifications out of the box",
    "start": "2491660",
    "end": "2497630"
  },
  {
    "text": "and basically its job what it does is it consumes off of a Kafka stream and it writes up to s3 and the format that",
    "start": "2497630",
    "end": "2503750"
  },
  {
    "text": "Athena wants so it pretty much did all the heavy lifting for us and it helped us get to market faster so basically",
    "start": "2503750",
    "end": "2509750"
  },
  {
    "text": "it's job via it consumes off of Kafka and writes up to s3 every couple minutes once it makes it into s3 we have",
    "start": "2509750",
    "end": "2516050"
  },
  {
    "text": "partitions set up per user I've seen a lot of a really common partitioning",
    "start": "2516050",
    "end": "2521480"
  },
  {
    "text": "schemas by date we chose user ID because all of our queries coming we're gonna be based on a single user",
    "start": "2521480",
    "end": "2527740"
  },
  {
    "text": "there's never any sort of cross user queries going on and of course as a SendGrid user someone shouldn't have",
    "start": "2527740",
    "end": "2534340"
  },
  {
    "text": "access to your data and you shouldn't have access to somebody else's data so this made sense for us and so that's the",
    "start": "2534340",
    "end": "2539770"
  },
  {
    "text": "the ingest portion and then on the right side we have our API which which exposes",
    "start": "2539770",
    "end": "2545380"
  },
  {
    "text": "all the api's that we offer to our customers and it takes the incoming request from our customer and then",
    "start": "2545380",
    "end": "2551140"
  },
  {
    "text": "basically translates that incoming query into a query that Athena wants and then sends that to s3 and then sends the",
    "start": "2551140",
    "end": "2559570"
  },
  {
    "text": "results back up through the API so a little bit more in depth on that on the Left we have the user and they'll",
    "start": "2559570",
    "end": "2565390"
  },
  {
    "text": "send in something that looks like this so they'll say show me all the emails that were sent to John Doe and Gmail",
    "start": "2565390",
    "end": "2572500"
  },
  {
    "text": "comm and the status is delivered so the API takes that it parses it creates a parse tree reed-like translates it into",
    "start": "2572500",
    "end": "2580270"
  },
  {
    "text": "the sequel format that Athena wants sends it something that looks like this Athena of course returns a CSV result",
    "start": "2580270",
    "end": "2587860"
  },
  {
    "text": "and since all of our api's deal with json we take that CSV result and converted it back into JSON and return",
    "start": "2587860",
    "end": "2594370"
  },
  {
    "text": "that back to the user so that's kind of a high level of what the API does so this architecture worked pretty well but",
    "start": "2594370",
    "end": "2600460"
  },
  {
    "text": "as we got deeper and deeper into the beta as time went on we noticed that our queries were beginning to slow down and",
    "start": "2600460",
    "end": "2605890"
  },
  {
    "text": "we were kind of wondering you know what's going on we started scratching our heads over this we noticed that the",
    "start": "2605890",
    "end": "2612400"
  },
  {
    "text": "only thing that really changed was the number of files and is very kind of mentioned this is if you've worked with Athena this might have been a few for",
    "start": "2612400",
    "end": "2617560"
  },
  {
    "text": "you as well we noticed that the number of files correlates to the query times so we ran a little experiment just to",
    "start": "2617560",
    "end": "2623350"
  },
  {
    "text": "test our hypothesis and this is what we saw so we took a million records and put",
    "start": "2623350",
    "end": "2628570"
  },
  {
    "text": "them all in a single work file uploaded it to s3 had Athena query it and it",
    "start": "2628570",
    "end": "2633880"
  },
  {
    "text": "results came back pretty quickly we then took those million records put them across 10 files uploaded them to s3",
    "start": "2633880",
    "end": "2640360"
  },
  {
    "text": "created again took a little longer do the same with a hundred files a thousand files and then 10,000 files we saw every",
    "start": "2640360",
    "end": "2646570"
  },
  {
    "text": "time we increase the number of files while the even though it was the same dataset it was just split across different files we saw the query times increase and so",
    "start": "2646570",
    "end": "2654900"
  },
  {
    "text": "we knew that was going to be an issue for us so at this point we knew that we needed to hand the older data differently in so this is",
    "start": "2654900",
    "end": "2663280"
  },
  {
    "text": "our second architecture and what we ended up doing was we ended up making",
    "start": "2663280",
    "end": "2668430"
  },
  {
    "text": "basically forking the big Kafka streams and so on the very left side we have Sakura still Sakura is still pulling off",
    "start": "2668430",
    "end": "2675250"
  },
  {
    "text": "a Kafka and it's writing up to s3 and to what we're calling the bachelor or the",
    "start": "2675250",
    "end": "2680380"
  },
  {
    "text": "Bachelorette now and it's still partitioned by user ID the only difference here is the court instead of writing out once every couple minutes",
    "start": "2680380",
    "end": "2686950"
  },
  {
    "text": "it's now writing out once an hour so over the course of an hour it's just consuming off of Kafka and then once an",
    "start": "2686950",
    "end": "2692319"
  },
  {
    "text": "hour it uploads the history and in the middle we introduced this concept of a speed layer which this succour it's",
    "start": "2692319",
    "end": "2699730"
  },
  {
    "text": "pulling off a Kafka and it's writing up more frequently and once a minute and it makes it into the speed layer in s3 we",
    "start": "2699730",
    "end": "2707380"
  },
  {
    "text": "changed the partitioning scheme here on this bucket we did a mod 100 of the user ID instead of individual user IDs the",
    "start": "2707380",
    "end": "2714640"
  },
  {
    "text": "reason for that being that basically we got some of the work benefits out of that we were able to group data together",
    "start": "2714640",
    "end": "2720579"
  },
  {
    "text": "more and we didn't have just you know one event profile which is pretty much pointless footwork so we ended up",
    "start": "2720579",
    "end": "2726040"
  },
  {
    "text": "batching those together and getting a little more use out of that so basically over the course of an hour what happens",
    "start": "2726040",
    "end": "2731920"
  },
  {
    "text": "is the speed layer succour is writing out you know a bunch of files you know",
    "start": "2731920",
    "end": "2737109"
  },
  {
    "text": "what's a file per minute per partition and then on the left side at the end of",
    "start": "2737109",
    "end": "2742960"
  },
  {
    "text": "that hour the Batchelor succour uploads its file and then the data and the speed Luckett the speed bucket gets deleted",
    "start": "2742960",
    "end": "2749410"
  },
  {
    "text": "and so that's how we kept the number of files down the way we handle that on the athena side it was we basically set up",
    "start": "2749410",
    "end": "2754930"
  },
  {
    "text": "each of those as a separate table and athena and when we issue a query to athena we issue a single query and just",
    "start": "2754930",
    "end": "2761140"
  },
  {
    "text": "Union distinct the results together and that D duplicates any of the data and",
    "start": "2761140",
    "end": "2767319"
  },
  {
    "text": "also queries both places and also read just returns a single result set so it kept our API more or less the same so",
    "start": "2767319",
    "end": "2776049"
  },
  {
    "start": "2776000",
    "end": "2776000"
  },
  {
    "text": "that worked great until one day it didn't what we noticed was as we got very close to the end of our beta we",
    "start": "2776049",
    "end": "2783309"
  },
  {
    "text": "started ramping up the number of users and as we added more users we were seeing memory issues on the batch layer",
    "start": "2783309",
    "end": "2789549"
  },
  {
    "text": "succour the reason being is that over the course of an hour had to store more and more data in memory before I",
    "start": "2789549",
    "end": "2796329"
  },
  {
    "text": "could upload to s3 and so we were getting memory pressure and it would eventually the process would eventually",
    "start": "2796329",
    "end": "2802390"
  },
  {
    "text": "die and we had to find another way around this so this was our third and final architecture we realized that we",
    "start": "2802390",
    "end": "2808150"
  },
  {
    "text": "didn't want to handle the speed and bachelors separately from the on-prem",
    "start": "2808150",
    "end": "2813789"
  },
  {
    "text": "side we needed to do that with an AWS and this was our solution for that so again we start with the core of",
    "start": "2813789",
    "end": "2820059"
  },
  {
    "text": "consuming aqua Kafka and it still writes up to the speed layer once a minute we enabled s3 event notifications on this",
    "start": "2820059",
    "end": "2827440"
  },
  {
    "text": "bucket so kind of as Roy mentioned you can get triggers based off of when file",
    "start": "2827440",
    "end": "2832509"
  },
  {
    "text": "changes occur to s3 so whenever we upload a file or delete a file those events get sent to SNS and then from",
    "start": "2832509",
    "end": "2838779"
  },
  {
    "text": "there they get forwarded to sqs and then we have a lambda consuming off of that",
    "start": "2838779",
    "end": "2844509"
  },
  {
    "text": "queue on sqs and those get rented dynamo so the reason why we do this is the",
    "start": "2844509",
    "end": "2852309"
  },
  {
    "text": "following reason this is what store ends up being stored in dynamo so for a given partition we know the number of bytes",
    "start": "2852309",
    "end": "2858880"
  },
  {
    "text": "and the number of files that are in that partition on s3 and we use that information with another lambda so this",
    "start": "2858880",
    "end": "2866380"
  },
  {
    "text": "lambda wakes up every so often and says hey dynamo which partitions are too large which have too many files in them",
    "start": "2866380",
    "end": "2872499"
  },
  {
    "text": "it gets that response from dynamo and then hands that off to Glu Glu at that",
    "start": "2872499",
    "end": "2878709"
  },
  {
    "text": "point takes those files that are partitioned by a mod 100 of the user ID repartition z-- them by individual user",
    "start": "2878709",
    "end": "2885130"
  },
  {
    "text": "ID and then sends them over to the batch bucket on s3 on the batch batch bucket",
    "start": "2885130",
    "end": "2893289"
  },
  {
    "text": "we also have event notifications enabled and they go through that same cycle of SMS and eventually make it into dynamo",
    "start": "2893289",
    "end": "2898989"
  },
  {
    "text": "now the reason why we go through the whole trouble of putting stuff into dynamo rather than just querying this",
    "start": "2898989",
    "end": "2904900"
  },
  {
    "text": "stuff off as three directly is because s3 is actually really good at what it does in terms of you know storing and",
    "start": "2904900",
    "end": "2910719"
  },
  {
    "text": "retrieving these these objects but it's not really good at querying the the sort of like the metadata of the filesystem",
    "start": "2910719",
    "end": "2916539"
  },
  {
    "text": "or the files themselves so we have to kind of cache that separately and that's where you use dynamo 4 and then as far",
    "start": "2916539",
    "end": "2923499"
  },
  {
    "text": "as querying the data it's exactly as the pre guess architecture we still have Athena with two separate tables one for the",
    "start": "2923499",
    "end": "2930340"
  },
  {
    "text": "batchly one for the bachelor one for this bead layer and then at unions the results together and we turns them up to",
    "start": "2930340",
    "end": "2936609"
  },
  {
    "text": "the API so this was our final architecture and it's been in production for about eight or nine months now since we launched and it's been working great",
    "start": "2936609",
    "end": "2944520"
  },
  {
    "text": "so some of the benefits we've seen from using Athena's since we switched over from our on-prem solution one is just",
    "start": "2944520",
    "end": "2951970"
  },
  {
    "start": "2945000",
    "end": "2945000"
  },
  {
    "text": "the scalability it's it has scaled tremendously so as we add new users to",
    "start": "2951970",
    "end": "2959770"
  },
  {
    "text": "the system as we get new SendGrid customers we haven't had to make any architectural changes the system just",
    "start": "2959770",
    "end": "2965770"
  },
  {
    "text": "scales and it's built on a very you know proven technology s3 and Athena has now",
    "start": "2965770",
    "end": "2971950"
  },
  {
    "text": "to us proven itself as well in terms of scalability so we're really excited and we're actually using it in future projects as well another thing we've",
    "start": "2971950",
    "end": "2979869"
  },
  {
    "text": "seen is the reduced sort of hidden costs that we see with a lot of fun from solutions we have with our with our",
    "start": "2979869",
    "end": "2987190"
  },
  {
    "text": "on-prem elasticsearch cluster we had a lot of DevOps support we had to you know",
    "start": "2987190",
    "end": "2992950"
  },
  {
    "text": "maintain the server's update the hardware update the OS of the elastic surge man into the cluster all these",
    "start": "2992950",
    "end": "2998740"
  },
  {
    "text": "things that take up time from our precious DevOps resources and we don't",
    "start": "2998740",
    "end": "3004200"
  },
  {
    "text": "have that with Athena we've had zero DevOps tickets in regards to this project so we're really excited about that as well and lastly the most",
    "start": "3004200",
    "end": "3011460"
  },
  {
    "text": "importantly is that our customer satisfaction has gone through the roof with this product we've had tremendous",
    "start": "3011460",
    "end": "3017490"
  },
  {
    "text": "feedback on customers using this how much easier it is to use and the",
    "start": "3017490",
    "end": "3023730"
  },
  {
    "text": "increase storage history has been really great so we're really thrilled about that as well and just to close it out",
    "start": "3023730",
    "end": "3030150"
  },
  {
    "text": "I want to reiterate some of the things that I had mentioned that initially brought us to Athena and just confirm",
    "start": "3030150",
    "end": "3038520"
  },
  {
    "text": "that after having using it the these are the things that drew us to it and after working with it these are actually true",
    "start": "3038520",
    "end": "3044520"
  },
  {
    "text": "so yes it is serverless we don't have to manage anything we don't have to worry about for provisioning VMs we don't have to worry",
    "start": "3044520",
    "end": "3051150"
  },
  {
    "text": "about scaling or even auto scaling it just magically scales behind the scenes for us so that's been super super",
    "start": "3051150",
    "end": "3058050"
  },
  {
    "text": "helpful in terms of development time and just engineering time in general second was the",
    "start": "3058050",
    "end": "3064050"
  },
  {
    "text": "elasticity our costs are very much accurate because we can we can base them",
    "start": "3064050",
    "end": "3071040"
  },
  {
    "text": "off of you know what the expected usage is going to be rather than guessing like",
    "start": "3071040",
    "end": "3078630"
  },
  {
    "text": "oh we might have a 10% adoption of this for paid people versus free people so this has been really great for us as",
    "start": "3078630",
    "end": "3084870"
  },
  {
    "text": "well and then lastly the API integration after using the the AWS SDKs the API",
    "start": "3084870",
    "end": "3092040"
  },
  {
    "text": "integration was super easy to use and they're all consistent across all their products line as well so that's whenever",
    "start": "3092040",
    "end": "3099690"
  },
  {
    "text": "we integrate with another Amazon product we can expect the same level of quality and just the consistency across them as",
    "start": "3099690",
    "end": "3106440"
  },
  {
    "text": "well as that's been fantastic also and that's all I have good yeah all right",
    "start": "3106440",
    "end": "3114480"
  },
  {
    "text": "yeah thank you very much thanks oh yeah thanks everyone for coming please complete the session survey and the",
    "start": "3114480",
    "end": "3120510"
  },
  {
    "text": "mobile app and did you want to mention about that yeah so I mean I think we have a time for a couple questions but",
    "start": "3120510",
    "end": "3126120"
  },
  {
    "text": "we also have sort of like a meet and greet a little bit after this actually right after the session up in Widow",
    "start": "3126120",
    "end": "3133110"
  },
  {
    "text": "lounge I think one level up so if some folks have more specific questions you want to chat with us please meet us",
    "start": "3133110",
    "end": "3140220"
  },
  {
    "text": "there but I think we have a couple minutes still thank you very much appreciate it",
    "start": "3140220",
    "end": "3146070"
  },
  {
    "text": "[Applause]",
    "start": "3146070",
    "end": "3149589"
  }
]