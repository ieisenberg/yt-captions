[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "[Music]",
    "start": "4110",
    "end": "8240"
  },
  {
    "text": "hello I'm sheryn a technical associate",
    "start": "8240",
    "end": "11000"
  },
  {
    "text": "here at the AWS office in Cape Town",
    "start": "11000",
    "end": "13719"
  },
  {
    "text": "today I'm going to show you what to do",
    "start": "13719",
    "end": "15879"
  },
  {
    "text": "if AWS resources are used to crawl your",
    "start": "15879",
    "end": "18160"
  },
  {
    "text": "website let's get",
    "start": "18160",
    "end": "20119"
  },
  {
    "text": "started if AWS resources are crawling",
    "start": "20119",
    "end": "23000"
  },
  {
    "text": "your website and not following your",
    "start": "23000",
    "end": "24720"
  },
  {
    "text": "rules you can take a few steps to",
    "start": "24720",
    "end": "26679"
  },
  {
    "text": "mitigate this first create or modify",
    "start": "26679",
    "end": "30080"
  },
  {
    "text": "your robots text file to control which",
    "start": "30080",
    "end": "32200"
  },
  {
    "text": "crawlers can access your site which",
    "start": "32200",
    "end": "34280"
  },
  {
    "text": "Pages they can crawl and at what",
    "start": "34280",
    "end": "37120"
  },
  {
    "text": "rate open a text editor and create a new",
    "start": "37120",
    "end": "40280"
  },
  {
    "text": "file called robots text to block a",
    "start": "40280",
    "end": "43920"
  },
  {
    "text": "specific crawler add a line like this",
    "start": "43920",
    "end": "47480"
  },
  {
    "start": "45000",
    "end": "99000"
  },
  {
    "text": "replace crawler name with the actual",
    "start": "47480",
    "end": "49800"
  },
  {
    "text": "user agent name of the crawler you want",
    "start": "49800",
    "end": "52000"
  },
  {
    "text": "to",
    "start": "52000",
    "end": "52960"
  },
  {
    "text": "block to allow a crawler but limited",
    "start": "52960",
    "end": "55840"
  },
  {
    "text": "craw rate add lines like this",
    "start": "55840",
    "end": "60760"
  },
  {
    "text": "this allows the crawler but makes it",
    "start": "60840",
    "end": "62840"
  },
  {
    "text": "pause for 60 seconds between requests to",
    "start": "62840",
    "end": "66280"
  },
  {
    "text": "block all crawlers use a wild card note",
    "start": "66280",
    "end": "70000"
  },
  {
    "text": "that blocking or crawlers may make your",
    "start": "70000",
    "end": "71960"
  },
  {
    "text": "site harder for users to find in search",
    "start": "71960",
    "end": "74560"
  },
  {
    "text": "engines you can also use allow rules to",
    "start": "74560",
    "end": "77759"
  },
  {
    "text": "specify which directories or Pages",
    "start": "77759",
    "end": "79920"
  },
  {
    "text": "crawlers can access for example once",
    "start": "79920",
    "end": "83280"
  },
  {
    "text": "your robots text files configured add",
    "start": "83280",
    "end": "85840"
  },
  {
    "text": "the Robux text file to your root domain",
    "start": "85840",
    "end": "88759"
  },
  {
    "text": "for example if your domain is",
    "start": "88759",
    "end": "90840"
  },
  {
    "text": "example.com then",
    "start": "90840",
    "end": "93840"
  },
  {
    "text": "www.example.com",
    "start": "93840",
    "end": "96840"
  },
  {
    "text": "robotext if an AWS resource is still",
    "start": "97600",
    "end": "100479"
  },
  {
    "start": "99000",
    "end": "152000"
  },
  {
    "text": "calling your site without adhering to",
    "start": "100479",
    "end": "102560"
  },
  {
    "text": "the robot text rules you can submit an",
    "start": "102560",
    "end": "105159"
  },
  {
    "text": "abuse report to the AWS trust and safety",
    "start": "105159",
    "end": "108079"
  },
  {
    "text": "team gather complete logs showing the",
    "start": "108079",
    "end": "111320"
  },
  {
    "text": "date time stamp with time zone and",
    "start": "111320",
    "end": "114799"
  },
  {
    "text": "Source IP of the crawling activity on",
    "start": "114799",
    "end": "118240"
  },
  {
    "text": "occasion the trust and safety team may",
    "start": "118240",
    "end": "120479"
  },
  {
    "text": "request additional information from you",
    "start": "120479",
    "end": "122719"
  },
  {
    "text": "in order to complete the",
    "start": "122719",
    "end": "124680"
  },
  {
    "text": "investigation including the targeted",
    "start": "124680",
    "end": "126840"
  },
  {
    "text": "domain name and its hosting IPS can",
    "start": "126840",
    "end": "129440"
  },
  {
    "text": "result in a quicker resolution then",
    "start": "129440",
    "end": "131920"
  },
  {
    "text": "submit an abuse report attaching those",
    "start": "131920",
    "end": "134080"
  },
  {
    "text": "logs as evidence the AWS trust and",
    "start": "134080",
    "end": "137239"
  },
  {
    "text": "safety team will review your robot's",
    "start": "137239",
    "end": "139280"
  },
  {
    "text": "text and the logs to determine if the",
    "start": "139280",
    "end": "141519"
  },
  {
    "text": "AWS customer is non-compliant so now you",
    "start": "141519",
    "end": "144720"
  },
  {
    "text": "know how to control Crawlers on your",
    "start": "144720",
    "end": "146640"
  },
  {
    "text": "website using robots text and how to",
    "start": "146640",
    "end": "148840"
  },
  {
    "text": "report abuse if",
    "start": "148840",
    "end": "151080"
  },
  {
    "text": "needed thanks for watching and happy",
    "start": "151080",
    "end": "153480"
  },
  {
    "start": "152000",
    "end": "167000"
  },
  {
    "text": "cloud computing from all of us here at",
    "start": "153480",
    "end": "155599"
  },
  {
    "text": "AWS",
    "start": "155599",
    "end": "158599"
  },
  {
    "text": "[Music]",
    "start": "160240",
    "end": "165710"
  }
]