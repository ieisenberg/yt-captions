[
  {
    "start": "0",
    "end": "81000"
  },
  {
    "text": "all righty it is 2:15 we are back on schedule I think it's my great pleasure",
    "start": "79",
    "end": "10320"
  },
  {
    "text": "to introduce dr. Peter white who heads the bio medical genetics Corps at the",
    "start": "10320",
    "end": "17279"
  },
  {
    "text": "Research Institute for Nationwide Children's Hospital and today he'll be talking about population scale genomics",
    "start": "17279",
    "end": "24330"
  },
  {
    "text": "algorithms that he's done following him will be ah be in the lore from Johns Hopkins University talking about RNA",
    "start": "24330",
    "end": "31439"
  },
  {
    "text": "seek analysis so two great talks about going to scale in genomics as we know",
    "start": "31439",
    "end": "36930"
  },
  {
    "text": "genomics is becoming a problem a big data problem without further ado okay",
    "start": "36930",
    "end": "48050"
  },
  {
    "text": "okay hi yes and good afternoon everyone",
    "start": "48050",
    "end": "54629"
  },
  {
    "text": "as I said my name's Peter white I'm from Nationwide Children's Hospital which is in Columbus Ohio and I'm going to be",
    "start": "54629",
    "end": "61199"
  },
  {
    "text": "talking to you about some work that we did earlier this year as part of the Intel head in the clouds challenge",
    "start": "61199",
    "end": "68390"
  },
  {
    "text": "David Everards mentioned in his keynote speech this morning and so that mean the second half of the tool code I'll talk",
    "start": "68390",
    "end": "75270"
  },
  {
    "text": "about that before I get into that let me give you some background on genome sequencing and what's involved so the",
    "start": "75270",
    "end": "82619"
  },
  {
    "start": "81000",
    "end": "111000"
  },
  {
    "text": "human genome project began in 1991 and it was an international effort to",
    "start": "82619",
    "end": "87750"
  },
  {
    "text": "sequence and assemble the structure of the first human genome it took this these papers came out 10 years later in",
    "start": "87750",
    "end": "94979"
  },
  {
    "text": "2001 this was the release to the first or after the human genome but it wasn't actually completed until the final draft",
    "start": "94979",
    "end": "101759"
  },
  {
    "text": "was in 2000 2006 you're looking about 15 years and a cost of 3 billion dollars to",
    "start": "101759",
    "end": "108840"
  },
  {
    "text": "sequence a single human genome now this is what it would look like if you were",
    "start": "108840",
    "end": "114390"
  },
  {
    "start": "111000",
    "end": "135000"
  },
  {
    "text": "to print out that Gina this is housed in the Wellcome collection in London it's a",
    "start": "114390",
    "end": "119790"
  },
  {
    "text": "collection of over a hundred and ten volumes each volume is containing just pages and pages of a C's GS and T's and",
    "start": "119790",
    "end": "126990"
  },
  {
    "text": "I'm actually headed to England with my family tomorrow and my kids think they're going to get a trip to the Tower of London but we're gonna go see this",
    "start": "126990",
    "end": "133290"
  },
  {
    "text": "instead so today we can sequence genomes a lot",
    "start": "133290",
    "end": "140090"
  },
  {
    "start": "135000",
    "end": "202000"
  },
  {
    "text": "faster and so this is sort of giving birth to a new era where we can actually sequence thousands of genomes and this",
    "start": "140090",
    "end": "147109"
  },
  {
    "text": "is because of a new type of sequencing technology that's called next-generation sequencing technology now in my lab of",
    "start": "147109",
    "end": "153680"
  },
  {
    "text": "children we have two of these next-generation sequences this little guy here is called a my seek instrument",
    "start": "153680",
    "end": "159980"
  },
  {
    "text": "is great for sequencing bacterial or viral genomes or for doing smaller studies but our main workhorses this",
    "start": "159980",
    "end": "166040"
  },
  {
    "text": "machine the Illumina HiSeq 4000 this is capable of generating five billion",
    "start": "166040",
    "end": "171650"
  },
  {
    "text": "sequencing reactions in parallel that's about 1.2 terabytes of raw data",
    "start": "171650",
    "end": "176659"
  },
  {
    "text": "coming off this machine every three and a half days but that amount of",
    "start": "176659",
    "end": "181790"
  },
  {
    "text": "sequencing output allows us to sequence up to twelve human genomes every three and a half days so focus of my research",
    "start": "181790",
    "end": "189019"
  },
  {
    "text": "has been in an area of translational bioinformatics how do we turn all of this data coming off of these machines",
    "start": "189019",
    "end": "195439"
  },
  {
    "text": "into something meaningful in terms of molecular diagnostics and clinical",
    "start": "195439",
    "end": "200509"
  },
  {
    "text": "treatments so it would be nice if the data coming off these sequences actually",
    "start": "200509",
    "end": "207409"
  },
  {
    "start": "202000",
    "end": "255000"
  },
  {
    "text": "looked anything like this but it doesn't because the way next-generation sequencing works is the genome has to be",
    "start": "207409",
    "end": "213379"
  },
  {
    "text": "chopped into very small pieces and then sequence in parallel so there's a bit of",
    "start": "213379",
    "end": "218629"
  },
  {
    "text": "kin to taking this entire collection I'm running it through a paper shredder this is what the output actually looks like",
    "start": "218629",
    "end": "224900"
  },
  {
    "text": "so the analysis process has to reassemble all of those volumes and get them looking back to something readable",
    "start": "224900",
    "end": "232549"
  },
  {
    "text": "and then what we're looking for is that one page in one of those volumes where a",
    "start": "232549",
    "end": "238159"
  },
  {
    "text": "letter has changed in this case we have a C that's changed to a T and often this",
    "start": "238159",
    "end": "243560"
  },
  {
    "text": "is what could be responsible for a genetic disease so that's what we're doing in the analysis going from that",
    "start": "243560",
    "end": "249049"
  },
  {
    "text": "shredded output to finding a pathogenic what we call a pathogenic genetic variant so when we first started doing",
    "start": "249049",
    "end": "258440"
  },
  {
    "start": "255000",
    "end": "308000"
  },
  {
    "text": "this the problem that we ran into very quickly was the this instrument in a high speed mode can sequence a human",
    "start": "258440",
    "end": "265849"
  },
  {
    "text": "genome in as little as 24 hours or around two days and it was taking us up",
    "start": "265849",
    "end": "272180"
  },
  {
    "text": "to two weeks to do the data analysis and so our solution to this problem was to come up with a new approach to paralyze",
    "start": "272180",
    "end": "279740"
  },
  {
    "text": "the analysis and we called that solution Churchill and what it's done what it",
    "start": "279740",
    "end": "285050"
  },
  {
    "text": "essentially does is it goes through each of the steps that are required in the analysis process and very efficiently",
    "start": "285050",
    "end": "291199"
  },
  {
    "text": "performs the parallelization and so I'm not going to go into any of the details of specifically how that works but we",
    "start": "291199",
    "end": "297889"
  },
  {
    "text": "published this earlier this year in genome biology and so if you search for church or maybe my name genome biology",
    "start": "297889",
    "end": "304460"
  },
  {
    "text": "it should be a top here in Google but",
    "start": "304460",
    "end": "309589"
  },
  {
    "start": "308000",
    "end": "381000"
  },
  {
    "text": "when you look at strategies for doing this with doing this workflow there's a couple of ways that you can paralyze it",
    "start": "309589",
    "end": "315469"
  },
  {
    "text": "what others have done is they've used a natural way to do the parallelization and that's by paralyzing the analysis by",
    "start": "315469",
    "end": "321409"
  },
  {
    "text": "chromosome so the genome is comprised of the 22 autosomes and the two sex chromosomes so here you could see you",
    "start": "321409",
    "end": "327589"
  },
  {
    "text": "could paralyze this analysis over the 20 over the 24 chromosomes there's two problems with this approach first of all",
    "start": "327589",
    "end": "334370"
  },
  {
    "text": "it's imbalanced that chromosomes differ in size chromosome 21 is the smallest and around 50 mega bases the chromosome",
    "start": "334370",
    "end": "341900"
  },
  {
    "text": "one is the largest at 250 mega bases so any analysis that's paralyzed by chromosome will be dependent on the",
    "start": "341900",
    "end": "348560"
  },
  {
    "text": "speed it takes to complete the largest chromosome the second thing is you're limited to 24 parallel processes there's",
    "start": "348560",
    "end": "355099"
  },
  {
    "text": "only 24 chromosomes that you can paralyze the analysis over and so what we did with Churchill is came up with",
    "start": "355099",
    "end": "360889"
  },
  {
    "text": "something that we call a balanced sub chromosomal region paralyzation strategy and that's why we called it Churchill",
    "start": "360889",
    "end": "367339"
  },
  {
    "text": "that was just too big of a mouthful to say that but essentially what we're able to do is to take each chromosome and",
    "start": "367339",
    "end": "373009"
  },
  {
    "text": "break it into smaller sub reads smaller sub regions and so you can see the result is that we can then balance the",
    "start": "373009",
    "end": "378979"
  },
  {
    "text": "analysis process some of the benefits of this is the the church we algorithms",
    "start": "378979",
    "end": "385279"
  },
  {
    "start": "381000",
    "end": "431000"
  },
  {
    "text": "very highly efficient in terms of the computer resources that it utilizes",
    "start": "385279",
    "end": "390360"
  },
  {
    "text": "and so what I'm showing you here is CPUs CPU utilization and then the time that",
    "start": "390360",
    "end": "395759"
  },
  {
    "text": "it takes to complete analysis of a whole human genome and when we compared this to two alternative an hour two or",
    "start": "395759",
    "end": "402090"
  },
  {
    "text": "Turner's hips to Churchill a point line called huge seek from Stanford and a pipeline called gatk queue from the",
    "start": "402090",
    "end": "408240"
  },
  {
    "text": "Broad Institute you can see that Churchill was a lot more efficient and was able to complete the analysis in the",
    "start": "408240",
    "end": "413550"
  },
  {
    "text": "shortest amount of time and this is on a single this is on a single machine and",
    "start": "413550",
    "end": "418580"
  },
  {
    "text": "what you can see here is this drop in utilization closely matches the",
    "start": "418580",
    "end": "424259"
  },
  {
    "text": "chromosome or size and this piece Hugh cheek is using a chromosomal paralyzation strategy so we've Churchill",
    "start": "424259",
    "end": "432960"
  },
  {
    "start": "431000",
    "end": "492000"
  },
  {
    "text": "we're able to scale beyond a single computer or a single instance and to use",
    "start": "432960",
    "end": "438930"
  },
  {
    "text": "multiple instances and increase and divide the genome into smaller and smaller regions allowing us to speed up",
    "start": "438930",
    "end": "444870"
  },
  {
    "text": "the analysis process so what I'm showing you here is this is speed up and this is the number of CPUs so the yellow and the",
    "start": "444870",
    "end": "451979"
  },
  {
    "text": "green are gatk q and huge seek and they're really limited to that 24 parallel processes but with church or we",
    "start": "451979",
    "end": "458849"
  },
  {
    "text": "closely track with a computational prediction of how much you can speed this process up that was predicted by",
    "start": "458849",
    "end": "464190"
  },
  {
    "text": "handles loss the church was the red line here and then there's a result of this",
    "start": "464190",
    "end": "469469"
  },
  {
    "text": "were able to reduce the analysis time substantially in this case but it somewhere out this was sixteen instances",
    "start": "469469",
    "end": "476580"
  },
  {
    "text": "on ec2 and since I couldn't 768 CPUs that allowed us to do the",
    "start": "476580",
    "end": "482550"
  },
  {
    "text": "analysis in about 99 minutes and right now we're running at around 77 minutes to do the analysis so we're trying to",
    "start": "482550",
    "end": "488610"
  },
  {
    "text": "get that down to under an hour but I work in a Children's Hospital",
    "start": "488610",
    "end": "494490"
  },
  {
    "start": "492000",
    "end": "543000"
  },
  {
    "text": "where speed can be critical with newborns that have suspected genetic",
    "start": "494490",
    "end": "500580"
  },
  {
    "text": "disease but with certain accuracy is very important for us we don't want to sacrifice quality in for speed I mean",
    "start": "500580",
    "end": "508259"
  },
  {
    "text": "what we were able to do is using a standard that came out from NIST last year something that's called the genomes in a bottle standard this is a standard",
    "start": "508259",
    "end": "516029"
  },
  {
    "text": "genome that has validated marrying calls were able to compare the output Churchill to the other pipelines",
    "start": "516029",
    "end": "522400"
  },
  {
    "text": "and what we found is that across the board church will have the highest number of validated snips the lowest",
    "start": "522400",
    "end": "528430"
  },
  {
    "text": "error rates and highest accuracy and then in terms of diagnostic effectiveness we use a measure that",
    "start": "528430",
    "end": "533800"
  },
  {
    "text": "called the udin index which is the sum of sensitivity and specificity minus one we saw that we have the highest overall",
    "start": "533800",
    "end": "541140"
  },
  {
    "text": "diagnostic effectiveness the other part of this is the the Churchill algorithm",
    "start": "541140",
    "end": "547540"
  },
  {
    "start": "543000",
    "end": "568000"
  },
  {
    "text": "is a hundred percent reproducible and deterministic and what I mean by deterministic is the regardless of the",
    "start": "547540",
    "end": "554310"
  },
  {
    "text": "scale of paralyzation regardless if I run it on my cluster or you run it on your cluster or you run it in the cloud",
    "start": "554310",
    "end": "560740"
  },
  {
    "text": "you'll always get the same results back and it's the only pipeline out there that has those features so how does this",
    "start": "560740",
    "end": "569589"
  },
  {
    "start": "568000",
    "end": "634000"
  },
  {
    "text": "tie into population scale genomics well in June last year childrens launched in",
    "start": "569589",
    "end": "578140"
  },
  {
    "text": "partnership with JC technology Nationwide Children's launched a new company called genome next which is a",
    "start": "578140",
    "end": "585270"
  },
  {
    "text": "genomic analysis software as a service company that provides genomic is provides a cloud platform for running",
    "start": "585270",
    "end": "592390"
  },
  {
    "text": "genomic analysis and Church was one of their flagship pipelines and I was at",
    "start": "592390",
    "end": "597760"
  },
  {
    "text": "this meeting with James hermus the CEO of genome next when we heard Intel announced the head in the clouds challenge and I said that sat with each",
    "start": "597760",
    "end": "606100"
  },
  {
    "text": "having both had the idea what would happen we could we take Churchill run this on a really large data set and so",
    "start": "606100",
    "end": "614140"
  },
  {
    "text": "what we proposed was that we would analyze the thousand genomes data set which is one of the largest publicly",
    "start": "614140",
    "end": "619690"
  },
  {
    "text": "available genomic data sets that are available and so we submitted an application and we said that we'd like",
    "start": "619690",
    "end": "626080"
  },
  {
    "text": "to be able to scale Churchill using AWS and see if we can perform the analysis",
    "start": "626080",
    "end": "631300"
  },
  {
    "text": "of the entire dataset in under a week so the thousand genomes was a project that",
    "start": "631300",
    "end": "637600"
  },
  {
    "start": "634000",
    "end": "667000"
  },
  {
    "text": "began in 2008 and was conducted by an international consortium with over 400",
    "start": "637600",
    "end": "643630"
  },
  {
    "text": "scientists and was designed to capture really and create a catalog of human",
    "start": "643630",
    "end": "649480"
  },
  {
    "text": "genetic variation from around the globe and it's been done in a number of phases and",
    "start": "649480",
    "end": "655209"
  },
  {
    "text": "the cost of sequencing has dropped they were able to actually increase for a thousand genomes to a final sample set",
    "start": "655209",
    "end": "661300"
  },
  {
    "text": "of 2504 genomes and this was from 26 populations around the globe now they",
    "start": "661300",
    "end": "667929"
  },
  {
    "start": "667000",
    "end": "701000"
  },
  {
    "text": "completed actually need to go back on sorry do we go back yep",
    "start": "667929",
    "end": "676899"
  },
  {
    "text": "they actually completed phase free of the analysis in April 2013 or phase 3 of",
    "start": "676899",
    "end": "682779"
  },
  {
    "text": "the sequencing this all of the genomes have been completed in terms of sequencing and it wasn't until 18 months",
    "start": "682779",
    "end": "688540"
  },
  {
    "text": "later that the analysis of this was completed and in September last year they released the final results final",
    "start": "688540",
    "end": "695499"
  },
  {
    "text": "results and I think a publication for this work is coming out there later this year so in terms of being able to",
    "start": "695499",
    "end": "703899"
  },
  {
    "start": "701000",
    "end": "755000"
  },
  {
    "text": "analyze a population scale data set one of the challenges that we had to overcome was data upload the final 1kg",
    "start": "703899",
    "end": "712540"
  },
  {
    "text": "dataset consists of two thousand five hundred for exome analysis and whole genome analysis for each individual so",
    "start": "712540",
    "end": "719379"
  },
  {
    "text": "there's just two different strategies to do sequencing the total size of that dataset is 70 terabytes so the challenge",
    "start": "719379",
    "end": "726459"
  },
  {
    "text": "is how do you transfer that amount of data quickly in order to be out of a formal analysis announcer's you can't",
    "start": "726459",
    "end": "732850"
  },
  {
    "text": "transfer it easily so you take your analysis to the data and so that's what",
    "start": "732850",
    "end": "737889"
  },
  {
    "text": "we did with genome next we were able to use because all of the data for this project has been stored and made",
    "start": "737889",
    "end": "744429"
  },
  {
    "text": "available in the s3 bucket we're able to co-locate our analysis platform in the",
    "start": "744429",
    "end": "750699"
  },
  {
    "text": "same region where the data was stored and that's in the u.s. East region so",
    "start": "750699",
    "end": "756100"
  },
  {
    "text": "this is an example of how the platform that was developed to run the analysis",
    "start": "756100",
    "end": "761410"
  },
  {
    "text": "this is a screenshot and what we wanted to do is to be able to try to fully",
    "start": "761410",
    "end": "766569"
  },
  {
    "text": "automate as much of the process as possible and so it started here with us",
    "start": "766569",
    "end": "771879"
  },
  {
    "text": "importing a sample index and then essentially what we have to do is give",
    "start": "771879",
    "end": "777369"
  },
  {
    "start": "775000",
    "end": "793000"
  },
  {
    "text": "provide an excel sheet that contained each of the sample names and then the location in the s3 bucket where that",
    "start": "777369",
    "end": "784059"
  },
  {
    "text": "fast queue data was stored and then when this was that this excel sheet was uploaded all the analysis was",
    "start": "784059",
    "end": "790420"
  },
  {
    "text": "kicked off automatically so the second challenge we had to have come was in",
    "start": "790420",
    "end": "796180"
  },
  {
    "text": "terms of scale analyzing what I talked about earlier I was showing you analysis of a single genome now we're talking",
    "start": "796180",
    "end": "801700"
  },
  {
    "text": "about analyzing over 5,000 samples and so this was the overview of the",
    "start": "801700",
    "end": "806880"
  },
  {
    "text": "infrastructure that we put together what we're starting with here this is the",
    "start": "806880",
    "end": "812410"
  },
  {
    "text": "genome next portal that would import the data from the s3 bucket and then perform",
    "start": "812410",
    "end": "818530"
  },
  {
    "text": "performing the analysis the port form in the analysis and then delivering the results back into s3 and to go into the",
    "start": "818530",
    "end": "825550"
  },
  {
    "text": "details that are a bit more detail you're starting here with so this was us",
    "start": "825550",
    "end": "831820"
  },
  {
    "start": "826000",
    "end": "907000"
  },
  {
    "text": "in our lab in in Columbus submitting the request all of the data so we submitted",
    "start": "831820",
    "end": "838000"
  },
  {
    "text": "our excel sheet that contained the location of all the samples in s3 if then viral load balancer that request",
    "start": "838000",
    "end": "845230"
  },
  {
    "text": "goes to the genome next portal well the portal uses is three different queue services first of all the analysis",
    "start": "845230",
    "end": "851800"
  },
  {
    "text": "request service up here sends a message to what we call the church or listener and this actually then deploys the",
    "start": "851800",
    "end": "858790"
  },
  {
    "text": "infrastructure that's needed to perform the analysis for a given sample so in this case depending on the size of the",
    "start": "858790",
    "end": "865000"
  },
  {
    "text": "input is an exome or whole genome you could anywhere be from a single instance up to sixteen instances person harmful",
    "start": "865000",
    "end": "872160"
  },
  {
    "text": "the analysis then begins all the infrastructure is launched the analysis begins and messages are sent back that",
    "start": "872160",
    "end": "878110"
  },
  {
    "text": "report on the status of the analysis and then once it's completed the final",
    "start": "878110",
    "end": "884170"
  },
  {
    "text": "results are delivered the files are copied back to s3 and then the users able to download download the results",
    "start": "884170",
    "end": "890200"
  },
  {
    "text": "back through the web web browser at the same time all that infrastructure that you deployed is take is automatically",
    "start": "890200",
    "end": "896800"
  },
  {
    "text": "taken down now we did this using anywhere up to a thousand instances over",
    "start": "896800",
    "end": "903700"
  },
  {
    "text": "the course of the seven over the course of the seven days and this was a schematic that we put together to kind",
    "start": "903700",
    "end": "910480"
  },
  {
    "start": "907000",
    "end": "992000"
  },
  {
    "text": "of summarize the results essentially we were able to take all 2,500 individuals",
    "start": "910480",
    "end": "916770"
  },
  {
    "text": "5000 samples 85 trillion base pairs of data totaling 70 terabytes in input",
    "start": "916770",
    "end": "924339"
  },
  {
    "text": "files it was about 148 thousand different int input files and then anywhere up to a thousand instances",
    "start": "924339",
    "end": "930540"
  },
  {
    "text": "completing analysis in the seven days now in terms of what the results look",
    "start": "930540",
    "end": "937450"
  },
  {
    "text": "like because this dataset had been analyzed by the thousand genomes consortium we're then able to compare our results that we go back back to that",
    "start": "937450",
    "end": "944620"
  },
  {
    "text": "original data set and what we found in terms of different types of genetic variants that we identified we looked",
    "start": "944620",
    "end": "950589"
  },
  {
    "text": "for single nucleotide polymorphisms and some of they're called indels these are small deletions and insertions in the",
    "start": "950589",
    "end": "956740"
  },
  {
    "text": "genome for this for the snip cause we had a high degree of overlap with the cause that Churchill called and those",
    "start": "956740",
    "end": "964120"
  },
  {
    "text": "that were identified by the thousand genomes for the in down calls we actually discovered just over five",
    "start": "964120",
    "end": "969880"
  },
  {
    "text": "million in dales that hadn't been reported by the thousand genomes and we think this was because we used a more",
    "start": "969880",
    "end": "977080"
  },
  {
    "text": "sensitive algorithm for doing the the indent Court a more sensitive algorithm",
    "start": "977080",
    "end": "982360"
  },
  {
    "text": "for doing the in Dale calling and when we look at some of the validation rates of this data we are actually seeing a",
    "start": "982360",
    "end": "988240"
  },
  {
    "text": "high level of validation of these in-depth calls but in terms of the",
    "start": "988240",
    "end": "993880"
  },
  {
    "start": "992000",
    "end": "1016000"
  },
  {
    "text": "overall variant frequencies when you study a population genetics you want to say how common is this variant in my",
    "start": "993880",
    "end": "999820"
  },
  {
    "text": "given population if we observed a frequency of 20% in our analysis the",
    "start": "999820",
    "end": "1005790"
  },
  {
    "text": "thousand genomes also reported a 20% frequency so we had an extremely tight correlation for the overall population",
    "start": "1005790",
    "end": "1011600"
  },
  {
    "text": "frequencies for the variants that we caught in common so with that I'll finish up I want to",
    "start": "1011600",
    "end": "1019350"
  },
  {
    "text": "thank the support we had from genome next in performing this and particularly thank Amazon and Intel for funding the",
    "start": "1019350",
    "end": "1026040"
  },
  {
    "text": "project and for allowing us to perform the thousand genomes analysis Nationwide",
    "start": "1026040",
    "end": "1031110"
  },
  {
    "text": "Insurance has been very supportive or large in genomics program at MCH and",
    "start": "1031110",
    "end": "1036449"
  },
  {
    "text": "then this is my team most of the work I've described here has been done by Ben Kelly who was the lead developer on",
    "start": "1036449",
    "end": "1042360"
  },
  {
    "text": "Churchill and James Fitch who did most of the thousand analysis and with that I'll stop and I'm",
    "start": "1042360",
    "end": "1048240"
  },
  {
    "text": "going to take questions after Abby's talk it's not hand over to Abby okay so",
    "start": "1048240",
    "end": "1064770"
  },
  {
    "start": "1051000",
    "end": "1126000"
  },
  {
    "text": "hi I'm Deena Laura I'm a postdoc at Johns Hopkins University in computational genomics working between",
    "start": "1064770",
    "end": "1071430"
  },
  {
    "text": "the labs of Ben Langmead who's in the audience here and Jeff leak we focus on",
    "start": "1071430",
    "end": "1076770"
  },
  {
    "text": "analyzing RNA sequencing data and we try to do it at scale with Amla Amazon",
    "start": "1076770",
    "end": "1083010"
  },
  {
    "text": "Elastic MapReduce and I I first started Amazon Web Services when I started this",
    "start": "1083010",
    "end": "1089100"
  },
  {
    "text": "postdoc almost two years ago now and now I'm I'm on Amazon Web Services junkie we",
    "start": "1089100",
    "end": "1094710"
  },
  {
    "text": "actually have a couple of clusters running right now analyzing sequence",
    "start": "1094710",
    "end": "1099750"
  },
  {
    "text": "read archive data that there are two such clusters each span about 2,000",
    "start": "1099750",
    "end": "1105180"
  },
  {
    "text": "processing cores and that's two that's the size of two clusters we would use at",
    "start": "1105180",
    "end": "1111060"
  },
  {
    "text": "Hopkins so we don't do it though because we're drunk with computational power we",
    "start": "1111060",
    "end": "1118440"
  },
  {
    "text": "do it for the science I'm not sure what I'm supposed to press here play the",
    "start": "1118440",
    "end": "1124320"
  },
  {
    "text": "green one okay okay we do it for the science and when we do computational",
    "start": "1124320",
    "end": "1130140"
  },
  {
    "start": "1126000",
    "end": "1253000"
  },
  {
    "text": "genomics we are we're interested in the aspect of molecular biology that is the",
    "start": "1130140",
    "end": "1136590"
  },
  {
    "text": "flow of genetic information within living things like you and me and others so this is captured a good first",
    "start": "1136590",
    "end": "1144300"
  },
  {
    "text": "approximation to this is captured by the statement made by Francis Crick this craig central dogma that is roughly DNA",
    "start": "1144300",
    "end": "1153720"
  },
  {
    "text": "is transcribed into RNA which is then translated into protein I said we focus",
    "start": "1153720",
    "end": "1160140"
  },
  {
    "text": "on RNA but you might then say well why should you why should you study RNA when",
    "start": "1160140",
    "end": "1165810"
  },
  {
    "text": "is it all the information encoded in the DNA and no that's not true at all because DNA I mean it's kind of like",
    "start": "1165810",
    "end": "1173460"
  },
  {
    "text": "saying you've listened to a Beatles record by looking at it Gru's you gotta play the record so what",
    "start": "1173460",
    "end": "1181270"
  },
  {
    "text": "we do is we allow the cellular machinery to unpack and interpret the compressed",
    "start": "1181270",
    "end": "1187690"
  },
  {
    "text": "information stored in DNA and gain some insight and to what the sequences of a's t's c's and g's mean okay okay so RNA is",
    "start": "1187690",
    "end": "1198100"
  },
  {
    "text": "important how do we find out about it well you sequence it and roughly what",
    "start": "1198100",
    "end": "1203980"
  },
  {
    "text": "happens in the sequencing experiment in broad strokes is you extract RNA you",
    "start": "1203980",
    "end": "1209470"
  },
  {
    "text": "isolate and purify it you fragment it so it's about you know maybe a few hundred base pairs long",
    "start": "1209470",
    "end": "1216280"
  },
  {
    "text": "segments a few hundred base pairs long then you convert it to a complimentary",
    "start": "1216280",
    "end": "1222130"
  },
  {
    "text": "DNA blogaid on some adapters you you",
    "start": "1222130",
    "end": "1228520"
  },
  {
    "text": "obtain short sequencing reads and we deal with sequencing reads that are maybe between 50 and 300 base pairs long",
    "start": "1228520",
    "end": "1235200"
  },
  {
    "text": "software do you develop to analyze this data does that and that's what we do so",
    "start": "1235200",
    "end": "1243670"
  },
  {
    "text": "we created the software tool called rail RNA to analyze lots and lots of these",
    "start": "1243670",
    "end": "1248680"
  },
  {
    "text": "sequencing samples and why might you be interested in lots and lots of sequencing samples well it just turns",
    "start": "1248680",
    "end": "1254440"
  },
  {
    "start": "1253000",
    "end": "1348000"
  },
  {
    "text": "out that a lot of investigators are now analyzing a lot of sequencing samples",
    "start": "1254440",
    "end": "1260650"
  },
  {
    "text": "and you can see this if you look at these public archives of sequencing data",
    "start": "1260650",
    "end": "1266430"
  },
  {
    "text": "let's see so I mean if you do any kind of publicly funded research if you're",
    "start": "1266430",
    "end": "1272050"
  },
  {
    "text": "publishing papers you're going to release the data you used and you generally do this by putting it on a",
    "start": "1272050",
    "end": "1277990"
  },
  {
    "text": "public archive one of them is the sequence read archive another is this European nucleotide archive and they",
    "start": "1277990",
    "end": "1284080"
  },
  {
    "text": "have these great plots on their site you can see just there about 1 times 10 to",
    "start": "1284080",
    "end": "1289750"
  },
  {
    "text": "the 13 reads on on the European nucleotide archive now I think that",
    "start": "1289750",
    "end": "1295960"
  },
  {
    "text": "spans between 1 million and 10 million samples and another interesting figure",
    "start": "1295960",
    "end": "1301330"
  },
  {
    "text": "is the one on the right which is the redoubling time and that figure is essentially telling us it's telling us",
    "start": "1301330",
    "end": "1307420"
  },
  {
    "text": "well you know whenever you're doing something at scale it's great to say that the data you're working with is growing in size exponentially and",
    "start": "1307420",
    "end": "1313840"
  },
  {
    "text": "that would be true if the plot on the right were a flat line or approached a flat line as the number of years or the",
    "start": "1313840",
    "end": "1322030"
  },
  {
    "text": "year increase but what's what it's actually saying is that in about two years",
    "start": "1322030",
    "end": "1327550"
  },
  {
    "text": "we will have doubled the number the amount of data on this archive that's",
    "start": "1327550",
    "end": "1333400"
  },
  {
    "text": "been accumulated up to that point and that spans about seems like seven years",
    "start": "1333400",
    "end": "1338590"
  },
  {
    "text": "which is pretty impressive so why why are we getting more data on these",
    "start": "1338590",
    "end": "1344140"
  },
  {
    "text": "archives and it's because the cost of sequencing is dropping and the experiments are getting larger and",
    "start": "1344140",
    "end": "1349930"
  },
  {
    "text": "larger genomics initiatives are now they're accumulating these large RNA seek data sets and probably one of the",
    "start": "1349930",
    "end": "1357160"
  },
  {
    "text": "pioneers in this was the encode study that now you know it now seems like a paltry number but they did a hundred RNA",
    "start": "1357160",
    "end": "1363640"
  },
  {
    "text": "sequencing samples but they actually did a study that spend a lot of next-generation sequencing technologies",
    "start": "1363640",
    "end": "1368970"
  },
  {
    "text": "there's the jihadist study which spanned 465 lymphoblastoid cell lines samples",
    "start": "1368970",
    "end": "1376660"
  },
  {
    "text": "depression genes and networks 965 this is divided into two groups half of them",
    "start": "1376660",
    "end": "1382420"
  },
  {
    "text": "have major depressive disorder the other half didn't TCGA over 2000 cancer samples and recently the gtex consortium",
    "start": "1382420",
    "end": "1391060"
  },
  {
    "text": "released about 10,000 RNA sequencing samples and they spanned there are 175 individuals these are difficult to",
    "start": "1391060",
    "end": "1397720"
  },
  {
    "text": "obtain tissues maybe 43 I think it's 43 difficult to obtain tissues across these",
    "start": "1397720",
    "end": "1403210"
  },
  {
    "text": "175 individuals and they still haven't been analyzed completely so you might",
    "start": "1403210",
    "end": "1409450"
  },
  {
    "text": "ask yourself okay so I so all these other people are caring about a lot of samples why should I care about their",
    "start": "1409450",
    "end": "1416050"
  },
  {
    "start": "1416000",
    "end": "1654000"
  },
  {
    "text": "data why might I want to reanalyze their data because if you're making a tool to analyze a lot of samples well why not",
    "start": "1416050",
    "end": "1423490"
  },
  {
    "text": "analyze samples that already exist you're repurposing rich hard to obtain",
    "start": "1423490",
    "end": "1428590"
  },
  {
    "text": "data as I said you know gtex these are recently well these are deceased individuals these are tissues that come",
    "start": "1428590",
    "end": "1435190"
  },
  {
    "text": "from them where are you going to get this data are you an investigator you",
    "start": "1435190",
    "end": "1440200"
  },
  {
    "text": "know with limited resources going to be able to get this yourself probably but someone else a group several groups",
    "start": "1440200",
    "end": "1446730"
  },
  {
    "text": "a consortiums scale of groups has has done this already",
    "start": "1446730",
    "end": "1452490"
  },
  {
    "text": "another reason is just no two analyses are commensurate okay you might do an",
    "start": "1452490",
    "end": "1457590"
  },
  {
    "text": "analysis one way another guy might do an analysis another way and you might disagree with existing analyses something as simple as how you normalize",
    "start": "1457590",
    "end": "1464610"
  },
  {
    "text": "the coverage coverage of your sample across a genome can significantly affect",
    "start": "1464610",
    "end": "1470640"
  },
  {
    "text": "your results and you might have to reduce someone else's analysis especially if you're trying to if you're trying to analyze a lot of data from",
    "start": "1470640",
    "end": "1477510"
  },
  {
    "text": "different experiments to make them commensurate you'll have to rerun your pipeline on that data another great",
    "start": "1477510",
    "end": "1483600"
  },
  {
    "text": "reason is to ensure the reproducibility of someone else's results and science need to verify that someone else's",
    "start": "1483600",
    "end": "1490110"
  },
  {
    "text": "results are you know someone else obtained results in perhaps the right way or it verify the the effectiveness",
    "start": "1490110",
    "end": "1498540"
  },
  {
    "text": "of the statistical methods used by other investigators or you might add power to your experiment you might have done a",
    "start": "1498540",
    "end": "1505440"
  },
  {
    "text": "study with lymphoblastoid cell lines and you might oughta mint the power of that experiment with with experiments from",
    "start": "1505440",
    "end": "1512010"
  },
  {
    "text": "the G of audist study so there's some incentive to reanalyze publicly available data but why should you do it",
    "start": "1512010",
    "end": "1519000"
  },
  {
    "text": "now in the cloud there are two one-word answers to this question that I see it one of them is elasticity",
    "start": "1519000",
    "end": "1525480"
  },
  {
    "text": "and the other is reproducibility the elasticity elasticity works in two",
    "start": "1525480",
    "end": "1530580"
  },
  {
    "text": "directions in one direction its elasticity and compute so if you're using your institutional cluster you're",
    "start": "1530580",
    "end": "1536309"
  },
  {
    "text": "gonna submit a job to some queue and it's gonna sit there in the queue and it",
    "start": "1536309",
    "end": "1541470"
  },
  {
    "text": "could be could be sitting there in the queue for a night right and I mean",
    "start": "1541470",
    "end": "1547650"
  },
  {
    "text": "you're hoping that there'll be compute available for you to do your job you're you're just praying that it'll start",
    "start": "1547650",
    "end": "1553230"
  },
  {
    "text": "well I mean if you use a service like Amazon Web Services you'll submit your job and if we use elastic MapReduce and",
    "start": "1553230",
    "end": "1559650"
  },
  {
    "text": "we typically find that within 10 to 20 minutes our job starts and it'll start with as many processing nodes as as we",
    "start": "1559650",
    "end": "1568170"
  },
  {
    "text": "request so it's not like we have to wait patiently for enough nodes to come",
    "start": "1568170",
    "end": "1573570"
  },
  {
    "text": "become free on an institutional cluster so there is elasticity in compute but there's also elasticity in storage with",
    "start": "1573570",
    "end": "1581100"
  },
  {
    "text": "s3 pretty much means you have infinite storage if you're analyzing a lot of sequencing samples for example that you",
    "start": "1581100",
    "end": "1587760"
  },
  {
    "text": "have on a study on a per sample basis it's about 2 gigabytes per sample but the intermediate data our pipeline acute",
    "start": "1587760",
    "end": "1594290"
  },
  {
    "text": "sorry our pipeline accumulates takes up about 10 times that amount so let's say",
    "start": "1594290",
    "end": "1599820"
  },
  {
    "text": "we're analyzing 10,000 samples that's going to be about 200 terabytes which you know your institutional cluster",
    "start": "1599820",
    "end": "1605760"
  },
  {
    "text": "might not have available so your I mean and using using a cloud computing",
    "start": "1605760",
    "end": "1612090"
  },
  {
    "text": "service like Amazon Web Services frees you from concerns about you know having enough having as much compute as you",
    "start": "1612090",
    "end": "1618780"
  },
  {
    "text": "want and running out of space another great reason to use a cloud computing",
    "start": "1618780",
    "end": "1624480"
  },
  {
    "text": "service to do your analyses is there you're not going to find a more reproducible system unless you use",
    "start": "1624480",
    "end": "1631170"
  },
  {
    "text": "another investigators hardware and software in exactly the same configuration the next best thing is for",
    "start": "1631170",
    "end": "1638820"
  },
  {
    "text": "to use for you to use the same hardware type the same instance types the same",
    "start": "1638820",
    "end": "1644280"
  },
  {
    "text": "size the cluster and the same software version the software configuration which",
    "start": "1644280",
    "end": "1650190"
  },
  {
    "text": "the software we developed helps you do huh all right maybe I messed up the",
    "start": "1650190",
    "end": "1658020"
  },
  {
    "start": "1654000",
    "end": "1748000"
  },
  {
    "text": "title of that slide but the point of this slide is also to say that you might",
    "start": "1658020",
    "end": "1664770"
  },
  {
    "text": "use any other any cloud service provider but a problem you run into when you're",
    "start": "1664770",
    "end": "1670380"
  },
  {
    "text": "working with distributed systems is consistency and consistency is when you're working with cloud storage like",
    "start": "1670380",
    "end": "1676980"
  },
  {
    "text": "s3 you might there's like a transaction you might say I write a file to this",
    "start": "1676980",
    "end": "1684300"
  },
  {
    "text": "cloud service to this cloud storage system and then I try to read after it",
    "start": "1684300",
    "end": "1689760"
  },
  {
    "text": "and those two operations might not be consistent I mean ah no I might not get back the exactly what I wrote and so",
    "start": "1689760",
    "end": "1699990"
  },
  {
    "text": "there's some delay this is an eventual consistency there so now all amazon",
    "start": "1699990",
    "end": "1705690"
  },
  {
    "text": "regions have so-called read after write consistency but you might actually have deleted a file and then",
    "start": "1705690",
    "end": "1711870"
  },
  {
    "text": "my listed directory on s3 and after you've deleted the file that file might still be present you might update a file and then try to",
    "start": "1711870",
    "end": "1719580"
  },
  {
    "text": "retrieve it and you might get what the original version of the file and one way",
    "start": "1719580",
    "end": "1725820"
  },
  {
    "text": "Amazon addresses this is through EMR FS which we use in our software to ensure",
    "start": "1725820",
    "end": "1732690"
  },
  {
    "text": "that when we write 2 s 3 we get fully consistent results and I don't think any",
    "start": "1732690",
    "end": "1739500"
  },
  {
    "text": "other cloud service adviser has this feature so that's advantage that's",
    "start": "1739500",
    "end": "1747630"
  },
  {
    "text": "advantaging mr all right so back to the problem we're trying to solve we're actually when you have these RNA",
    "start": "1747630",
    "end": "1753840"
  },
  {
    "start": "1748000",
    "end": "1769000"
  },
  {
    "text": "sequencing reads you want to find out where they originated in the genome and so you align it to the genome and",
    "start": "1753840",
    "end": "1759150"
  },
  {
    "text": "sometimes you're lucky and a read cluck correctly aligns the reference genome end-to-end and this is an example of",
    "start": "1759150",
    "end": "1765660"
  },
  {
    "text": "such an alignment but sometimes you're not because RNA has segments of DNA",
    "start": "1765660",
    "end": "1772620"
  },
  {
    "text": "missing from it called introns and these are segments from the genome that aren't expressed so when these introns are",
    "start": "1772620",
    "end": "1779309"
  },
  {
    "text": "overlapped ok there's going to be a piece of the genome missing from the sequence read and you'll have to align",
    "start": "1779309",
    "end": "1785490"
  },
  {
    "text": "that read in pieces you'll divide the read up into short overlapping segments",
    "start": "1785490",
    "end": "1790800"
  },
  {
    "text": "this is what we do called read let's okay and then we align the read listed",
    "start": "1790800",
    "end": "1796230"
  },
  {
    "text": "the genome to infer the presence of introns so you'll see on one side here",
    "start": "1796230",
    "end": "1802020"
  },
  {
    "text": "of I don't know ok you'll see on one side of this intron you'll have this",
    "start": "1802020",
    "end": "1808110"
  },
  {
    "text": "short segment of the read a short segment of the read it's 25 base pairs long and aligns to one side of this",
    "start": "1808110",
    "end": "1814290"
  },
  {
    "text": "intron this side of the read the right side of the read aligns to the other side of the intron ok and we can easily",
    "start": "1814290",
    "end": "1821100"
  },
  {
    "text": "infer the presence of this intron but sometimes reads won't overlap the intron",
    "start": "1821100",
    "end": "1826440"
  },
  {
    "text": "right in the middle of a read right in the middle of that read right here could",
    "start": "1826440",
    "end": "1831540"
  },
  {
    "text": "overlap the intron towards the end of a read so what we'll have to do is accumulate a list of introns okay that",
    "start": "1831540",
    "end": "1839610"
  },
  {
    "text": "from that sample or across samples okay and then knowing the presence of that Inchon we create we create segments",
    "start": "1839610",
    "end": "1850370"
  },
  {
    "text": "we take segments of the genome around the intron and concatenate them and then realign reads to infer the presence of",
    "start": "1850370",
    "end": "1858320"
  },
  {
    "start": "1857000",
    "end": "1913000"
  },
  {
    "text": "the intron so the software you created is called real RNA and with just one to",
    "start": "1858320",
    "end": "1864740"
  },
  {
    "text": "two commands you get alignments just like the one illustrated on the previous slide introns and their coverage in",
    "start": "1864740",
    "end": "1872510"
  },
  {
    "text": "Dells insertions and deletions in these reads and just coverage information which parts of the genome are covered by",
    "start": "1872510",
    "end": "1879290"
  },
  {
    "text": "how many reads and all we do is you have one file and you list all your facts",
    "start": "1879290",
    "end": "1884480"
  },
  {
    "text": "cues they could be at they could be on the web they could be on an FTP server",
    "start": "1884480",
    "end": "1890720"
  },
  {
    "text": "they could be local you specify the assembly and this one's hg19 human",
    "start": "1890720",
    "end": "1896810"
  },
  {
    "text": "reference it's it's not then we specify the number of instances we want to use",
    "start": "1896810",
    "end": "1902750"
  },
  {
    "text": "the number of computers you want to use in the cloud and an output directory which is an s3 it is a directory in an",
    "start": "1902750",
    "end": "1908510"
  },
  {
    "text": "s3 bucket and you can download it at",
    "start": "1908510",
    "end": "1914540"
  },
  {
    "start": "1913000",
    "end": "2055000"
  },
  {
    "text": "railay oh great so how does this software work how do we",
    "start": "1914540",
    "end": "1921170"
  },
  {
    "text": "actually achieve our ends you want to scale you want software that gets faster",
    "start": "1921170",
    "end": "1926750"
  },
  {
    "text": "when you have more workers in a cluster and you also want it to work on a lot of data you want you don't want it to break",
    "start": "1926750",
    "end": "1932330"
  },
  {
    "text": "when you suddenly reach a thousand samples or 5,000 samples so the approach we took was the program what was the",
    "start": "1932330",
    "end": "1939350"
  },
  {
    "text": "MapReduce programming model and you can understand MapReduce as you take a computer cluster and you divide it into",
    "start": "1939350",
    "end": "1945650"
  },
  {
    "text": "a set of workers and you take a computing task or set a computing tasks",
    "start": "1945650",
    "end": "1951500"
  },
  {
    "text": "and you arrange them so they're a sequence of alternating aggregation and computation steps in these aggregation",
    "start": "1951500",
    "end": "1958310"
  },
  {
    "text": "steps the data or group is grouped and sorted and the computation steps each worker operates on some subset of the",
    "start": "1958310",
    "end": "1964790"
  },
  {
    "text": "data so to illustrate how this works in",
    "start": "1964790",
    "end": "1972050"
  },
  {
    "text": "rail I'll actually describe how Braille exploits this MapReduce",
    "start": "1972050",
    "end": "1977910"
  },
  {
    "text": "programming model to reduce redundancy analysis and the idea is roughly that you're going to have lots of reads",
    "start": "1977910",
    "end": "1984299"
  },
  {
    "text": "across these samples and there's going to be a lot of duplication across these reads a lot of duplicate sequences you",
    "start": "1984299",
    "end": "1990150"
  },
  {
    "text": "don't necessarily need to align these sequences more than once when you're",
    "start": "1990150",
    "end": "1995760"
  },
  {
    "text": "just looking for introns similarly when you divide the read up into these short",
    "start": "1995760",
    "end": "2001160"
  },
  {
    "text": "wavelet sequences there's a lot of duplication across these read lists and you don't have to align identical real",
    "start": "2001160",
    "end": "2007940"
  },
  {
    "text": "it's more than once so what we do is this is just a part of the pipeline we aggregate in this aggregates that we",
    "start": "2007940",
    "end": "2014210"
  },
  {
    "text": "group reads by their unique identical unique sequence so and then we segment",
    "start": "2014210",
    "end": "2021080"
  },
  {
    "text": "these unique sequences group the unique read lit sequences and align the unique",
    "start": "2021080",
    "end": "2026510"
  },
  {
    "text": "read mode sequences so this reduces the line and burden so significantly then we",
    "start": "2026510",
    "end": "2033890"
  },
  {
    "text": "aggregate against after aligning these unique read live sequences so we can put we can associate",
    "start": "2033890",
    "end": "2039620"
  },
  {
    "text": "the read let's back with unique read sequences the read the read sequences to",
    "start": "2039620",
    "end": "2045049"
  },
  {
    "text": "which they belong this is another aggregation step and then we search for introns within each unique read sequence",
    "start": "2045049",
    "end": "2051260"
  },
  {
    "text": "all right this goes so so this is an",
    "start": "2051260",
    "end": "2057138"
  },
  {
    "start": "2055000",
    "end": "2112000"
  },
  {
    "text": "interesting story because you might here's part of the story is how we get",
    "start": "2057139",
    "end": "2063919"
  },
  {
    "text": "improved throughput with rail RNA when we increase the number of samples we",
    "start": "2063919",
    "end": "2069590"
  },
  {
    "text": "analyze so naively you might say okay I analyzed 10 samples on a fixed cluster",
    "start": "2069590",
    "end": "2077330"
  },
  {
    "text": "computer cluster of fixed size say a hundred computers and I analyzed 20 samples on a fixed on a cluster of a",
    "start": "2077330",
    "end": "2084020"
  },
  {
    "text": "fixed size I wouldn't expect that I analyze more samples per hour when I",
    "start": "2084020",
    "end": "2091040"
  },
  {
    "text": "analyze more samples right so they easily I'd expect that it's constant my",
    "start": "2091040",
    "end": "2097730"
  },
  {
    "text": "throughput is constant as I increase the number of samples I analyzed but rail RNA has improved throughput the more",
    "start": "2097730",
    "end": "2106100"
  },
  {
    "text": "samples you analyze simply because it's reduced redundancy in this analysis this is illustrated here on this plot where we",
    "start": "2106100",
    "end": "2115940"
  },
  {
    "text": "analyzed increasing numbers of G of oddest samples on the same cluster of",
    "start": "2115940",
    "end": "2121760"
  },
  {
    "text": "40c3 2 X large computers or instances that's 8 process that's each processing",
    "start": "2121760",
    "end": "2128660"
  },
  {
    "text": "core so it's 320 processing cores in total and you can see that as we increase the number of samples here to 3",
    "start": "2128660",
    "end": "2136970"
  },
  {
    "text": "we get improved throughput and improve throughput actually means that how much it costs to analyze a sample goes down",
    "start": "2136970",
    "end": "2145310"
  },
  {
    "text": "the more samples you analyze we use spot instances and that's why these prices",
    "start": "2145310",
    "end": "2151070"
  },
  {
    "text": "might seem lower and it cost us we we bid I think 11 cents so we paid up to",
    "start": "2151070",
    "end": "2158840"
  },
  {
    "text": "because there's a an Amazon that there's a surcharge we paid it around 25 cents",
    "start": "2158840",
    "end": "2165550"
  },
  {
    "text": "per instance per hour so can we go back a few slides back back back yes okay",
    "start": "2165550",
    "end": "2176330"
  },
  {
    "text": "that's perfect okay so that's the scalability story with respect to number",
    "start": "2176330",
    "end": "2182510"
  },
  {
    "text": "of samples but of course there's also skin on there's also speed up in practice so ideally it's you might say",
    "start": "2182510",
    "end": "2189680"
  },
  {
    "text": "okay I use twice as many computers I do things twice as fast but in practice there their network bottlenecks their",
    "start": "2189680",
    "end": "2195350"
  },
  {
    "text": "iOS use so we we actually we scale okay",
    "start": "2195350",
    "end": "2202460"
  },
  {
    "start": "2198000",
    "end": "2229000"
  },
  {
    "text": "but it costs more the more computers you use to analyze your Sam your samples so",
    "start": "2202460",
    "end": "2208310"
  },
  {
    "text": "there so what we did was we compensated for this effect with this reduced redundancy analysis analyze more samples",
    "start": "2208310",
    "end": "2215180"
  },
  {
    "text": "on more computers and and these two effects cancel each other out in some",
    "start": "2215180",
    "end": "2222050"
  },
  {
    "text": "sense and you get pretty good scale so",
    "start": "2222050",
    "end": "2227570"
  },
  {
    "text": "that's what we did we one of the milestones so we analyzed all of",
    "start": "2227570",
    "end": "2233960"
  },
  {
    "start": "2229000",
    "end": "2276000"
  },
  {
    "text": "jihadists in just 12 hours on 1920 processing cores it cost us 69 cents per sample and spy",
    "start": "2233960",
    "end": "2239810"
  },
  {
    "text": "still Ayman is a pretty hard task so it I mean to get all the all those",
    "start": "2239810",
    "end": "2247190"
  },
  {
    "text": "results coverage information in Dells alignments in 12 hours we feel as an achievement and next up is for us to",
    "start": "2247190",
    "end": "2253520"
  },
  {
    "text": "analyze all of human Illumina RNA seek on SRA and as I was just describing we're doing this we're in the process of",
    "start": "2253520",
    "end": "2259490"
  },
  {
    "text": "doing this right now we have clusters doing this right now on EMR were very excited about about how we're gonna use",
    "start": "2259490",
    "end": "2265520"
  },
  {
    "text": "this tool in the future and we hope you try to use it too if you're interested in doing this kind of work so rail dot",
    "start": "2265520",
    "end": "2272450"
  },
  {
    "text": "bio thanks a lot",
    "start": "2272450",
    "end": "2275589"
  }
]