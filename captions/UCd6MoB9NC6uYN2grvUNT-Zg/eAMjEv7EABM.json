[
  {
    "start": "0",
    "end": "215000"
  },
  {
    "text": "hello and welcome to today's webinar on machine learning I'm gonna wait two more",
    "start": "5210",
    "end": "12150"
  },
  {
    "text": "minutes to give participants a chance to login so please stand by we'll be",
    "start": "12150",
    "end": "17520"
  },
  {
    "text": "starting in about two minutes",
    "start": "17520",
    "end": "20900"
  },
  {
    "text": "you",
    "start": "27060",
    "end": "29119"
  },
  {
    "text": "once again we are just waiting one or two more minutes to allow people the",
    "start": "63120",
    "end": "69690"
  },
  {
    "text": "opportunity to login to today's webinar will begin in approximately one minute",
    "start": "69690",
    "end": "76880"
  },
  {
    "text": "you",
    "start": "83520",
    "end": "85578"
  },
  {
    "text": "hello to all good morning my name is Chris Green ACK I am a partner Solutions",
    "start": "123729",
    "end": "130599"
  },
  {
    "text": "Architect and the global segment lead for machine learning thank you for",
    "start": "130599",
    "end": "135670"
  },
  {
    "text": "joining us today for our continuing series on Amazon sage makers built-in",
    "start": "135670",
    "end": "142030"
  },
  {
    "text": "algorithms today we're going to take a look at neural topic modeling with the",
    "start": "142030",
    "end": "148630"
  },
  {
    "text": "senior Solutions Architect Chris burns before we begin though I'd just like to point out that you can ask questions at",
    "start": "148630",
    "end": "156430"
  },
  {
    "text": "any time during this webinar by using the question panel on the screen on your",
    "start": "156430",
    "end": "162040"
  },
  {
    "text": "right so please do take that opportunity to do that for you no short questions",
    "start": "162040",
    "end": "168340"
  },
  {
    "text": "things that can be handled immediately I'll be answering those interactively",
    "start": "168340",
    "end": "173380"
  },
  {
    "text": "however there will be a longer Q&A session hosted by our speaker Chris",
    "start": "173380",
    "end": "179680"
  },
  {
    "text": "burns at the end of today's webinar so that said I'd like to hand it over to",
    "start": "179680",
    "end": "185230"
  },
  {
    "text": "our senior Solutions Architect in AI and machine learning Chris burns",
    "start": "185230",
    "end": "192000"
  },
  {
    "text": "Chris thank you for the great intro and hello everyone once again this is my",
    "start": "193930",
    "end": "199459"
  },
  {
    "text": "third or fourth time behind the mic on this series so I certainly appreciate the the continued attendance and I have",
    "start": "199459",
    "end": "205730"
  },
  {
    "text": "not been voted off the air yet so I guess that's always a bonus so before I",
    "start": "205730",
    "end": "210980"
  },
  {
    "text": "get started today I wanted the very very first thing I wanted to do was remind",
    "start": "210980",
    "end": "216079"
  },
  {
    "start": "215000",
    "end": "296000"
  },
  {
    "text": "you that these deep dive webinars are a jumping-off point they're not an end unto themselves and there is a team here",
    "start": "216079",
    "end": "223159"
  },
  {
    "text": "at AWS specifically built and staffed to help you our partners so this is a",
    "start": "223159",
    "end": "229970"
  },
  {
    "text": "partner only webinar I'm sorry partly partner invite only webinar and we're",
    "start": "229970",
    "end": "235069"
  },
  {
    "text": "here to help you guys out so what we have is we have obviously myself based in Texas we also now have finally happy",
    "start": "235069",
    "end": "243469"
  },
  {
    "text": "to to announce that we have presents globally I'm seeing some familiar names in the attendee list I'm seeing some",
    "start": "243469",
    "end": "250000"
  },
  {
    "text": "some domain names from from the UK and from EMEA and such so we have Alejandro",
    "start": "250000",
    "end": "257060"
  },
  {
    "text": "now in Berlin we have Bogdan in London get myself and Chris here on in the US",
    "start": "257060",
    "end": "263030"
  },
  {
    "text": "and we are also have open three open Rick Rex that we're going to try to fill we're working diligently to get those",
    "start": "263030",
    "end": "268220"
  },
  {
    "text": "filled to provide even more service to our partner so if you have any questions after today's webinar I know when I'm",
    "start": "268220",
    "end": "274789"
  },
  {
    "text": "taking in information that's relatively new or I'm not you know I'm not adept at",
    "start": "274789",
    "end": "279860"
  },
  {
    "text": "that my questions come later they don't come right there on the spot so please feel free to email any one of",
    "start": "279860",
    "end": "285229"
  },
  {
    "text": "us and we'll get those questions answered as soon as possible I only ask that you send the email via your partner",
    "start": "285229",
    "end": "291320"
  },
  {
    "text": "domain name otherwise you'll probably go to you're probably a filter so in the",
    "start": "291320",
    "end": "296930"
  },
  {
    "start": "296000",
    "end": "478000"
  },
  {
    "text": "next hour we're gonna talk all about NTM I'm gonna break down the agenda here",
    "start": "296930",
    "end": "302690"
  },
  {
    "text": "what is narrow topic modeling what it can do how does it differ from Lda",
    "start": "302690",
    "end": "308139"
  },
  {
    "text": "you've been listening to the series we did do a webinar ld8 not too long ago",
    "start": "308139",
    "end": "313639"
  },
  {
    "text": "before the holidays the MTM algorithm is Bill hundred percent by Amazon data scientist so we're gonna get into the",
    "start": "313639",
    "end": "320750"
  },
  {
    "text": "details about what the differentiators are by using the Amazon and TM algorithm",
    "start": "320750",
    "end": "325909"
  },
  {
    "text": "we're talk about when did she MTM / comprehend or maybe even she wouldn't choose it / lDA's what good's a",
    "start": "325909",
    "end": "331950"
  },
  {
    "text": "little bit of differences there weren't talk about preparing your data for NTM training and then we're going to talk",
    "start": "331950",
    "end": "337200"
  },
  {
    "text": "about training now this is actually my first webinar where I'm not going to live code anything in a notebook I don't",
    "start": "337200",
    "end": "344040"
  },
  {
    "text": "think you're gonna lose any value from that there's two great example notebooks",
    "start": "344040",
    "end": "349110"
  },
  {
    "text": "that are built into sage maker now and there are some more in github that you can use and those do an excellent job of",
    "start": "349110",
    "end": "354720"
  },
  {
    "text": "walking you through and re-read summer are summarizing and capturing what I'm",
    "start": "354720",
    "end": "359940"
  },
  {
    "text": "going to talk about today in in practice so with that let's just let's jump in",
    "start": "359940",
    "end": "365610"
  },
  {
    "text": "neural topic modeling we're probably all familiar with topic modeling so I'm not",
    "start": "365610",
    "end": "370770"
  },
  {
    "text": "gonna spend too much time on the concept of topic modeling but obviously because it is the the topic of the webinar it's",
    "start": "370770",
    "end": "378690"
  },
  {
    "text": "going to come up frequently an tiem it's an unsupervised learning algorithm that's used to organize a corpus of",
    "start": "378690",
    "end": "384060"
  },
  {
    "text": "documents in the topics that contain word groupings based on their statistical distribution so if you have",
    "start": "384060",
    "end": "389970"
  },
  {
    "text": "any familiarity of all with topic modeling that's pretty cookie cutter definition of topic modeling and what it",
    "start": "389970",
    "end": "396270"
  },
  {
    "text": "really does is it provides a means for you to visualize the contents and large collections of text in terms of learned",
    "start": "396270",
    "end": "401490"
  },
  {
    "text": "topics we're going to talk a lot about latent representations today in just a",
    "start": "401490",
    "end": "406860"
  },
  {
    "text": "bit I'm gonna throw some some vocabulary definitions so that we're all on the same page another thing I wanted to mention before I dive in is that these",
    "start": "406860",
    "end": "413250"
  },
  {
    "text": "webinars we have a very very broad audience and it can sometimes be difficult to prepare a one-hour webinar",
    "start": "413250",
    "end": "419490"
  },
  {
    "text": "and balance depth with breadth so if we don't go deep enough for you or if you",
    "start": "419490",
    "end": "425040"
  },
  {
    "text": "think we've left something out please feel free to email us and we'll try to get that incorporated into the next into",
    "start": "425040",
    "end": "430380"
  },
  {
    "text": "the next webinar I'm back like that still I believe I owe I owe responses to a couple of folks from the last webinar",
    "start": "430380",
    "end": "436590"
  },
  {
    "text": "I did I did not forget about you I'm just a little bit behind okay now late",
    "start": "436590",
    "end": "443550"
  },
  {
    "text": "representations sorry for that tangent there this this is a concept that we",
    "start": "443550",
    "end": "450510"
  },
  {
    "text": "need to be cognizant of when we're talking about topic modeling and the limitations of topic modeling because there's no guarantee that our topics are",
    "start": "450510",
    "end": "457500"
  },
  {
    "text": "gonna align with how humans might naturally think and this is this is what like modeling meets sentiment analysis",
    "start": "457500",
    "end": "462570"
  },
  {
    "text": "many times you're going to see Lda or other NLP algorithms say topic modeling",
    "start": "462570",
    "end": "468600"
  },
  {
    "text": "/ sentiment analysis and I want to draw a little bit of a delineation point between those two because one is easier",
    "start": "468600",
    "end": "474960"
  },
  {
    "text": "than the other one is more reliable than the other now to dive into this a little",
    "start": "474960",
    "end": "480840"
  },
  {
    "start": "478000",
    "end": "528000"
  },
  {
    "text": "bit one of my favorite examples of ambiguity I'm gonna be that guy so we",
    "start": "480840",
    "end": "487320"
  },
  {
    "text": "know what MTM is topic modeling when you use it when should you not use it if you have a scenario where there's sarcasm or",
    "start": "487320",
    "end": "496020"
  },
  {
    "text": "innuendo or double entendres in the text you need to think carefully about how you're going to apply topic modeling to",
    "start": "496020",
    "end": "501690"
  },
  {
    "text": "that and this one my favorite examples of all time of ambiguity we have a sentence here I saw a man on a hill with",
    "start": "501690",
    "end": "506730"
  },
  {
    "text": "a telescope and there are many different ways I've just listed before here of many different ways of interpreting that",
    "start": "506730",
    "end": "512729"
  },
  {
    "text": "now for topic modeling we kinda get a pass because the topics in this sentence are man Hill telescope almost announced",
    "start": "512729",
    "end": "520080"
  },
  {
    "text": "but when we start to get into sentiment and deeper meaning you can see how it can get confusing very very quick that's",
    "start": "520080",
    "end": "527220"
  },
  {
    "text": "that's really sort of the bad news so I didn't mean to open this up on a on a down note but there are many use cases",
    "start": "527220",
    "end": "534000"
  },
  {
    "start": "528000",
    "end": "752000"
  },
  {
    "text": "today that that are in practice they are working very very well they're very very",
    "start": "534000",
    "end": "539880"
  },
  {
    "text": "efficient content personalization is one matter of fact we have a worker on a",
    "start": "539880",
    "end": "545100"
  },
  {
    "text": "specialized workshops that sort of encapsulates this forecasting and",
    "start": "545100",
    "end": "550140"
  },
  {
    "text": "personalized personalization but this will allow you to utilize MTM to understand related documents based on",
    "start": "550140",
    "end": "557040"
  },
  {
    "text": "phrases or topic similarities so pretty much the standard definition of topic modeling again semantic search social",
    "start": "557040",
    "end": "564060"
  },
  {
    "text": "and social analytics and information management information management is a really popular we'll call it a",
    "start": "564060",
    "end": "570960"
  },
  {
    "text": "low-hanging fruit use case if you will of North topic modeling you're gonna see you know maybe some metadata generation",
    "start": "570960",
    "end": "577020"
  },
  {
    "text": "you're going to see support tickets classified by priority by using NTM you're going to see a lot of very",
    "start": "577020",
    "end": "582930"
  },
  {
    "text": "practical very useful use cases there and they're they're excellent jumping-off points if you're new to NLP",
    "start": "582930",
    "end": "589860"
  },
  {
    "text": "or if you're just new to neural topic modeling",
    "start": "589860",
    "end": "595010"
  },
  {
    "text": "Paul says I thought I skip the slide so apologize for that so now that I've laid",
    "start": "601190",
    "end": "606920"
  },
  {
    "text": "out what no topic modeling is you might be asking yourself the question should I use NCM or should I use Amazon",
    "start": "606920",
    "end": "612170"
  },
  {
    "text": "comprehend and a theme you're gonna I'm gonna come back to over and over again in this webinar is just really depends",
    "start": "612170",
    "end": "618110"
  },
  {
    "text": "on your scenario it depends on your dataset machine learning it's very very difficult to classify use cases as one",
    "start": "618110",
    "end": "625430"
  },
  {
    "text": "size fits all data is always is it's got that's got the differences there's a",
    "start": "625430",
    "end": "630950"
  },
  {
    "text": "different scenarios you have different requirements so look to MTM if you need fine-grained control over the layers of",
    "start": "630950",
    "end": "637880"
  },
  {
    "text": "your model if you need to customize the vocabulary for instance the comprehend room medical",
    "start": "637880",
    "end": "643420"
  },
  {
    "text": "came from this from this idea that we needed to customize vocabulary and I've",
    "start": "643420",
    "end": "648710"
  },
  {
    "text": "seen this before I've seen this on stock exchange floors in areas where there's a",
    "start": "648710",
    "end": "654410"
  },
  {
    "text": "certain vernacular or there's a lot of terminology used over and over again that normal normal people would not",
    "start": "654410",
    "end": "660380"
  },
  {
    "text": "understand or recognize so that's a customized vocabulary also if you have regulatory hurdles and you just simply",
    "start": "660380",
    "end": "666500"
  },
  {
    "text": "can't use a service you have to maybe train this train this on Sage maker in",
    "start": "666500",
    "end": "671870"
  },
  {
    "text": "Amazon and maybe take that model put it on an edge site or something like that NTM can fill that gap when you want to",
    "start": "671870",
    "end": "680510"
  },
  {
    "text": "comprehend maybe you're new to machine learning maybe have tight deadlines maybe you're just over subscribed and",
    "start": "680510",
    "end": "686000"
  },
  {
    "text": "you simply don't have that the time to dedicate to something as detailed as completing an NPM process the the",
    "start": "686000",
    "end": "693220"
  },
  {
    "text": "requirement here is if it can be done with an existing service if your",
    "start": "693220",
    "end": "698390"
  },
  {
    "text": "requirements can be met with an existing service it's not logical to add complexity to the process even if you",
    "start": "698390",
    "end": "703940"
  },
  {
    "text": "want to learn about it I would not use that as an excuse to use NPM over comprehend because what's going to",
    "start": "703940",
    "end": "709010"
  },
  {
    "text": "happen is the scenario is probably gonna present itself again in the future and if you have to use NPM then then you",
    "start": "709010",
    "end": "715250"
  },
  {
    "text": "could take the opportunity to learn more about it so by rule my my advice to everybody is if it can be done with an",
    "start": "715250",
    "end": "720620"
  },
  {
    "text": "existing service choose the existing service also comprehend it's got some built-in features that are very very",
    "start": "720620",
    "end": "726170"
  },
  {
    "text": "handy here you see this image here we have keyword topic groups we have document",
    "start": "726170",
    "end": "731270"
  },
  {
    "text": "relationships topic so it's not it's not exactly a bag of words but it does break it out very very nicely for you it's available and",
    "start": "731270",
    "end": "737210"
  },
  {
    "text": "super clean JSON that's callable from from any any language or service that",
    "start": "737210",
    "end": "742700"
  },
  {
    "text": "can call an api whereas with NTM you have to build all that yourself on the backend so I had mentioned I was going",
    "start": "742700",
    "end": "755750"
  },
  {
    "start": "752000",
    "end": "1003000"
  },
  {
    "text": "to get into a bit of vocabulary so let me do that now again just a quick reminder we have a large array of skill",
    "start": "755750",
    "end": "762200"
  },
  {
    "text": "levels on these webinars so if this is all you know very very old-school to you",
    "start": "762200",
    "end": "767270"
  },
  {
    "text": "or old hat to you I do apologize but the content I'm sorry the concept of latent",
    "start": "767270",
    "end": "774800"
  },
  {
    "text": "representation this means if variables are not directly observed but they're",
    "start": "774800",
    "end": "779870"
  },
  {
    "text": "inferred by other variables so they'll give you an example of that is if I see the words bike car train mileage speed I",
    "start": "779870",
    "end": "788030"
  },
  {
    "text": "can infer that that's a topic of or about mobility or transportation or some",
    "start": "788030",
    "end": "793670"
  },
  {
    "text": "some type of other word that you can you can infer that's what latent representation is and because the",
    "start": "793670",
    "end": "799940"
  },
  {
    "text": "languages are so nuanced it's going to be very very difficult to actually spit out a topic that is not a late",
    "start": "799940",
    "end": "805880"
  },
  {
    "text": "representation now perplexity protects complexity and the topic coherence the second one there or the",
    "start": "805880",
    "end": "811880"
  },
  {
    "text": "third one are probably the cornerstones of this webinar the cornerstones of NTM perplexity is the measure measures a",
    "start": "811880",
    "end": "819680"
  },
  {
    "text": "models capability to describe documents according to a generative process based on the learns to the topics now I want",
    "start": "819680",
    "end": "826820"
  },
  {
    "text": "to draw a couple of distinctions here generative process this is this is versus a discriminative process so if",
    "start": "826820",
    "end": "832400"
  },
  {
    "text": "you have a classification problem it's discriminative generative almost it's it's I like to think of it as as you as",
    "start": "832400",
    "end": "838700"
  },
  {
    "text": "the algorithm learns as it goes and now this is an optimization target so",
    "start": "838700",
    "end": "844010"
  },
  {
    "text": "perplexity is is a what I mean by optimization target means we influence the convergence of this model based on",
    "start": "844010",
    "end": "850040"
  },
  {
    "text": "parameters has optimizing we want to optimize for perplexity now coherence",
    "start": "850040",
    "end": "855320"
  },
  {
    "text": "also known as topic coherence also known as topic interpretability this measures the models",
    "start": "855320",
    "end": "860910"
  },
  {
    "text": "by the semantic similarity of the words that you see each topic and a good model will have words that are semantically",
    "start": "860910",
    "end": "867000"
  },
  {
    "text": "similar now this is you're normally up to you know two dozen 18 in the past",
    "start": "867000",
    "end": "873720"
  },
  {
    "text": "that was measured by a process called NPM I normalized point-wise mutual information the downside to that method",
    "start": "873720",
    "end": "880230"
  },
  {
    "text": "was that it requires a very very large corpus of documents and also the the key",
    "start": "880230",
    "end": "885720"
  },
  {
    "text": "point here is topic coherence is not optimized in that learning process it is",
    "start": "885720",
    "end": "890820"
  },
  {
    "text": "evaluated after training so one of the goals of MTM was to be able to optimize",
    "start": "890820",
    "end": "896550"
  },
  {
    "text": "not only on perplexity but optimize on copy coherence now I've",
    "start": "896550",
    "end": "905370"
  },
  {
    "text": "brought up Lda a few times the latent fear slay allocation this is not a webinar on lda",
    "start": "905370",
    "end": "911100"
  },
  {
    "text": "we have an excellent webinar already archived on Lda but obviously I have to bring it up just to tell you about the",
    "start": "911100",
    "end": "916350"
  },
  {
    "text": "differences between Lda and NPM the objectives of the two are identical topic modeling those are the purpose of",
    "start": "916350",
    "end": "923160"
  },
  {
    "text": "these algorithms the means are very different in Lda you can have a Gibbs",
    "start": "923160",
    "end": "928260"
  },
  {
    "text": "sampling you're gonna have a variational Bayes inference give sampling as though it's a Monte Carlo is a Markov chain for",
    "start": "928260",
    "end": "934650"
  },
  {
    "text": "generating samples of joint distributions and the variational Bayes of course is a Bayes inference for",
    "start": "934650",
    "end": "941070"
  },
  {
    "text": "discovering the maximum likelihood of estimates that's not neural networks",
    "start": "941070",
    "end": "946080"
  },
  {
    "text": "there so what we have now with MTM is we've introduced neural variation inference and I have a link at the end",
    "start": "946080",
    "end": "952050"
  },
  {
    "text": "of this to the an excellent paper on what neural variational inference is for the purposes of this webinar and for the",
    "start": "952050",
    "end": "958350"
  },
  {
    "text": "next half an hour we'd see though it's a it's a flexible framework to offer more expressive results it's an attempt by",
    "start": "958350",
    "end": "963960"
  },
  {
    "text": "Amazon to solve for the intractability of Bayesian inference when dimensionality increases to the point",
    "start": "963960",
    "end": "970380"
  },
  {
    "text": "that the base breaks down so what we've done here is we have the same concept of LD a topic modeling but we've inserted a",
    "start": "970380",
    "end": "977610"
  },
  {
    "text": "neural variational inference rather than the traditional means for for",
    "start": "977610",
    "end": "983460"
  },
  {
    "text": "convergence and what's the result as I already mentioned in previous slide the result is we get optimizable coherence",
    "start": "983460",
    "end": "988830"
  },
  {
    "text": "awareness that means with our hyper parameters with our file with our tuning of layers with our data inputs we can",
    "start": "988830",
    "end": "994680"
  },
  {
    "text": "optimize coherence in addition to perplexity now I talked about this",
    "start": "994680",
    "end": "1005990"
  },
  {
    "start": "1003000",
    "end": "1232000"
  },
  {
    "text": "obviously this series is sage maker built-in so what we would say built-in it doesn't just mean that we've",
    "start": "1005990",
    "end": "1011870"
  },
  {
    "text": "prepackaged existing industry standard algorithms for you to use there's more to the to the sage maker built-ins then",
    "start": "1011870",
    "end": "1018920"
  },
  {
    "text": "might might meet the I and these algorithms that come with value-adds is I guess the best way to determine I'll",
    "start": "1018920",
    "end": "1025040"
  },
  {
    "text": "determine that as you there's added value as bonuses there's increased convenience or some type of benefits to",
    "start": "1025040",
    "end": "1032000"
  },
  {
    "text": "using these built-ins so when it comes to the value adds of NTM how much been a couple minutes talking about that we",
    "start": "1032000",
    "end": "1037640"
  },
  {
    "text": "already mentioned that the MPM I that's that's old and busted now so the new hotness is W et Cie word embedding topic",
    "start": "1037640",
    "end": "1045470"
  },
  {
    "text": "coherence it's a it is computationally efficient the difference here is MPM I calculated after training and required a",
    "start": "1045470",
    "end": "1052940"
  },
  {
    "text": "very very large sampling of text whereas word embedded W et Cie it is",
    "start": "1052940",
    "end": "1058700"
  },
  {
    "text": "computationally efficient so if you have a small corpus of documents W et Cie is gonna try to match that and without",
    "start": "1058700",
    "end": "1065120"
  },
  {
    "text": "losing without losing I'll use the word accuracy here so this image here is a quick comparison to LD a and the new",
    "start": "1065120",
    "end": "1072290"
  },
  {
    "text": "neural variational models and we use the twenty newsgroup dataset the best numbers are in bold I do apologize I",
    "start": "1072290",
    "end": "1078350"
  },
  {
    "text": "blew it up a little bit might be a little bit blurry but we see that the LD a with the collapse cubes performed well",
    "start": "1078350",
    "end": "1083540"
  },
  {
    "text": "on complexity but the NPM ice cores is relatively low and we see with the neural models MTM R we get great scores",
    "start": "1083540",
    "end": "1091040"
  },
  {
    "text": "with perplexity as well as MP mi you could almost say that through perplexity and the topic coherence is analogous to",
    "start": "1091040",
    "end": "1098330"
  },
  {
    "text": "the variance and the bias trade-off when you're talking about a traditional image",
    "start": "1098330",
    "end": "1104000"
  },
  {
    "text": "classification or maybe a random forest so we want to we want to optimize on both perplexity as well stop the",
    "start": "1104000",
    "end": "1110150"
  },
  {
    "text": "coherence and we see that with this new word embedding topic coherence we can achieve an excellent optimization",
    "start": "1110150",
    "end": "1116210"
  },
  {
    "text": "between those two I highly recommend this paper Dane randon he is a Amazon",
    "start": "1116210",
    "end": "1123620"
  },
  {
    "text": "machine learning data scientist excuse me out of New York I believe and it a great job with this algorithm it's",
    "start": "1123620",
    "end": "1129670"
  },
  {
    "text": "very performant and there's a couple more benefits that we'll talk about so I don't just sound like a like a rah-rah",
    "start": "1129670",
    "end": "1135310"
  },
  {
    "text": "salesman the auxilary vocabulary channel this was to me this was a big this was a",
    "start": "1135310",
    "end": "1140740"
  },
  {
    "text": "big ad to me I was really happy when I saw this I don't call myself a data scientist I call myself a machine",
    "start": "1140740",
    "end": "1146230"
  },
  {
    "text": "learning practitioner as such I have very little patience for certain aspects of the data science world and one of",
    "start": "1146230",
    "end": "1153430"
  },
  {
    "text": "them was the embedding and the mapping of topics to a vocabulary list that can",
    "start": "1153430",
    "end": "1159340"
  },
  {
    "text": "be cumbersome and what we have now with the auxilary vocabulary channel think of it as an add-on we have training",
    "start": "1159340",
    "end": "1164740"
  },
  {
    "text": "channels we have test channels the of validation channels if you use stage maker hopefully you're familiar with the term channel it's synonymous with an s3",
    "start": "1164740",
    "end": "1171240"
  },
  {
    "text": "subdirectory dedicated to that subject of a training test or in this case",
    "start": "1171240",
    "end": "1177070"
  },
  {
    "text": "vocabulary the first image on the top there we took from synthetic data so",
    "start": "1177070",
    "end": "1183040"
  },
  {
    "text": "those numbers are just synthetically generated already the embedding was already done and what we have here is",
    "start": "1183040",
    "end": "1190680"
  },
  {
    "text": "well it's just a bunch of numbers in the bottom picture of course we've used the",
    "start": "1190680",
    "end": "1196390"
  },
  {
    "text": "auxiliary Bo Cabaret channel and now we can see that the words are right here easy to learn now so this is this is",
    "start": "1196390",
    "end": "1202270"
  },
  {
    "text": "excellent time-saver if you're doing some hyper premier optimization and you're doing many many permutations of",
    "start": "1202270",
    "end": "1207670"
  },
  {
    "text": "this algorithm and you want to just scan through the cloud watch logs rather than happen to to create some kind of mapping mechanism that's it's a",
    "start": "1207670",
    "end": "1214570"
  },
  {
    "text": "big value add and also it's great feature for those new to NLP I think it's a it was did a great job of",
    "start": "1214570",
    "end": "1220090"
  },
  {
    "text": "helping kick-start the learning process rather than having to worry about the",
    "start": "1220090",
    "end": "1225340"
  },
  {
    "text": "nuances of mapping word embeddings and",
    "start": "1225340",
    "end": "1232810"
  },
  {
    "start": "1232000",
    "end": "1382000"
  },
  {
    "text": "now to drill down a little bit further on w et Cie this is a measures of the",
    "start": "1232810",
    "end": "1237850"
  },
  {
    "text": "similarity of words so it's a topic coherence metric I'll just go back to that and we see here is calculated with",
    "start": "1237850",
    "end": "1244030"
  },
  {
    "text": "each topic that's the the large rectangular I'm sorry the vertical box and then we also give you the average",
    "start": "1244030",
    "end": "1249790"
  },
  {
    "text": "score typical values now mathematically values range from zero to one but",
    "start": "1249790",
    "end": "1256450"
  },
  {
    "text": "normally you're going to see values ranging from point 2 to point 8 and now point 8 would be almost",
    "start": "1256450",
    "end": "1262630"
  },
  {
    "text": "unheard of whereas a point two would probably just be a baseline for for",
    "start": "1262630",
    "end": "1268270"
  },
  {
    "text": "noise and now you can see some of these topics in this particular there are 20 topics in this particular training some",
    "start": "1268270",
    "end": "1274060"
  },
  {
    "text": "of them range all we have I see 41 here so so very very visitors are excellent",
    "start": "1274060",
    "end": "1280330"
  },
  {
    "text": "levels I don't want to demean the mid amid metric level of 0.4 and now we also",
    "start": "1280330",
    "end": "1292420"
  },
  {
    "text": "have topic uniqueness so one interesting thing I want to draw your attention to here again on the Left we're showing our",
    "start": "1292420",
    "end": "1298030"
  },
  {
    "text": "synthetic data and we see that the W etc' score is not present you have to use the auxilary vocabulary channel if",
    "start": "1298030",
    "end": "1304240"
  },
  {
    "text": "you want to you also use W et Cie that's dependent on that on that mapping and also we see 40 you just to describe this",
    "start": "1304240",
    "end": "1312010"
  },
  {
    "text": "really quick the topic uniqueness we want we want as much uniqueness as",
    "start": "1312010",
    "end": "1317140"
  },
  {
    "text": "possible and so the measurement of this is between 1 1 K and 1 so if we have K",
    "start": "1317140",
    "end": "1324280"
  },
  {
    "text": "topics so we have capital K there and we're gonna take each word in that topic",
    "start": "1324280",
    "end": "1332700"
  },
  {
    "text": "trying to do the math in my head I apologize but essentially the the calculation is such that it's between 1",
    "start": "1333570",
    "end": "1339880"
  },
  {
    "text": "K and 1 so if we see this the topic uniqueness of the synthetic data 5",
    "start": "1339880",
    "end": "1345340"
  },
  {
    "text": "topics so between 1 1 v & 1 or 0.2 and 1.0 we see that it's the synthetic data",
    "start": "1345340",
    "end": "1352660"
  },
  {
    "text": "did very very poorly in the topic uniqueness and that was simply because it was again simply generated synthetic",
    "start": "1352660",
    "end": "1360010"
  },
  {
    "text": "data but let this be a you know a sort of a lesson if you're taking a shortcut on synthetic data the topic uniqueness",
    "start": "1360010",
    "end": "1367870"
  },
  {
    "text": "score may suffer well we see on actual data the topic uniquely score 0.75 is",
    "start": "1367870",
    "end": "1373720"
  },
  {
    "text": "actually is actually quite good alright",
    "start": "1373720",
    "end": "1383890"
  },
  {
    "start": "1382000",
    "end": "1427000"
  },
  {
    "text": "so I'm gonna pause here I've spent almost 10 minutes a good portion of the entire webinar on the differentiators of",
    "start": "1383890",
    "end": "1389800"
  },
  {
    "text": "NTM and so just to summarize w ET see that's word embedding topic attention",
    "start": "1389800",
    "end": "1395920"
  },
  {
    "text": "comprehension is in that that's optimizable it is computationally efficient and whereas M PMI is out so",
    "start": "1395920",
    "end": "1403000"
  },
  {
    "text": "what that means is its savings on training essentially auxilary vocabulary channel that's going to map the word",
    "start": "1403000",
    "end": "1410440"
  },
  {
    "text": "embeddings to your vocabulary for you no more no more needing to map those manually well obviously we're saying",
    "start": "1410440",
    "end": "1416680"
  },
  {
    "text": "manually but obviously you have some kind of scripting or a code process in place and then topic uniqueness good",
    "start": "1416680",
    "end": "1422290"
  },
  {
    "text": "topic model she's entering topics that are unique so we want a higher a higher score move on to preparing data for an",
    "start": "1422290",
    "end": "1432160"
  },
  {
    "start": "1427000",
    "end": "1589000"
  },
  {
    "text": "tiem in training now this is again not a",
    "start": "1432160",
    "end": "1438190"
  },
  {
    "text": "one-size-fits-all scenario so so takes for the grain of salt some of these steps may be done in different orders",
    "start": "1438190",
    "end": "1443740"
  },
  {
    "text": "this is a general guideline preparing data and the first one is the data sis is a data source known to be good it's",
    "start": "1443740",
    "end": "1450100"
  },
  {
    "text": "easy for us to use known good data sources in our examples you partners you have to go out to the real world where",
    "start": "1450100",
    "end": "1455560"
  },
  {
    "text": "it's very messy and you have to try to you know clean these data sets prior to",
    "start": "1455560",
    "end": "1461070"
  },
  {
    "text": "training but the data source needs to be free of corruption I mean you gotta be",
    "start": "1461070",
    "end": "1466120"
  },
  {
    "text": "careful of white if the white space has been stripped out if there are markup symbols so it really depends what kind of document you're dealing with here so",
    "start": "1466120",
    "end": "1472270"
  },
  {
    "text": "HTML there's a lot of noise and those type of documents you can move on and tokenizing this is also this is also covered in",
    "start": "1472270",
    "end": "1478720"
  },
  {
    "text": "much greater detail in the LDA webinar i'll just throw that out there so you have to determine what delimiter is going to determine a discrete block of",
    "start": "1478720",
    "end": "1485050"
  },
  {
    "text": "text so you want to go by paragraph you want to go by saying it's gonna go by document that's gonna happen in your tokenizing phase there's also an example",
    "start": "1485050",
    "end": "1491380"
  },
  {
    "text": "of tokenizing in the NTM notebook so well I won't dig into the details of that here on the call or on the webinar",
    "start": "1491380",
    "end": "1498490"
  },
  {
    "text": "it is in the it is in the notebook and then you have your stemming in your in your lemma ties if you've done any NLP",
    "start": "1498490",
    "end": "1504010"
  },
  {
    "text": "at all those gonna be very familiar to you it looks like I copied and pasted it accidentally so that is not the",
    "start": "1504010",
    "end": "1511090"
  },
  {
    "text": "definition of stemming and lemma thighs exists established to find the root word of a particular piece of vocabulary and",
    "start": "1511090",
    "end": "1518530"
  },
  {
    "text": "limit izing is you want to group if I have run running runs I want to group",
    "start": "1518530",
    "end": "1523930"
  },
  {
    "text": "those all that's what the the practice of limit izing is so I'll get that fixed",
    "start": "1523930",
    "end": "1529480"
  },
  {
    "text": "before this slide deck is is distributed now our NTM parameters are of type float",
    "start": "1529480",
    "end": "1536020"
  },
  {
    "text": "a folk type of float 32 so your input data has to be converted to mp4 32 as",
    "start": "1536020",
    "end": "1541780"
  },
  {
    "text": "well that was a that caught me the first time I was going through this to practice on a dates that I picked out",
    "start": "1541780",
    "end": "1547360"
  },
  {
    "text": "and it took me a minute to read through the docs and make sure that I had my my input data in the form of float 32 and",
    "start": "1547360",
    "end": "1553720"
  },
  {
    "text": "then NTM likes record IO protobuf format so plenty of utilities on in stage maker",
    "start": "1553720",
    "end": "1560200"
  },
  {
    "text": "documentation and in our past webinars to show you how to convert into that format for training and then finally the",
    "start": "1560200",
    "end": "1568150"
  },
  {
    "text": "if you use the ML TK limit Iser that we use in the notebook that's going to create an array for you of unique",
    "start": "1568150",
    "end": "1574660"
  },
  {
    "text": "vocabulary and it's super easy to just type that in to a file and send off to",
    "start": "1574660",
    "end": "1580090"
  },
  {
    "text": "your vocab auxilary channel so that you have that vocabulary ready to go all",
    "start": "1580090",
    "end": "1591460"
  },
  {
    "start": "1589000",
    "end": "1673000"
  },
  {
    "text": "right we get to the training portion we also have introduced something called subsampling this is an excellent feature",
    "start": "1591460",
    "end": "1597760"
  },
  {
    "text": "to f5 should put this in a differentiator section rather than the training section but what are you gonna do so typical training you pass the",
    "start": "1597760",
    "end": "1604660"
  },
  {
    "text": "entire data set to the algorithm you feed it the entire data sets the algorithm with each epoch with",
    "start": "1604660",
    "end": "1610090"
  },
  {
    "text": "subsampling we can choose to send only a portion it is randomly randomly selected",
    "start": "1610090",
    "end": "1615910"
  },
  {
    "text": "and sent off and so what we get if you look at this graphic here is you can get",
    "start": "1615910",
    "end": "1621850"
  },
  {
    "text": "time savings don't pay attention to the e box because the April were the second",
    "start": "1621850",
    "end": "1627700"
  },
  {
    "text": "instance here where we had 49 box the data set is 80% smaller so the D box were gonna go much faster and we see",
    "start": "1627700",
    "end": "1634120"
  },
  {
    "text": "that we had a significant on more than 20% time and savings with very little",
    "start": "1634120",
    "end": "1639520"
  },
  {
    "text": "loss very little loss at all in our our balance between our T u RW e t see now",
    "start": "1639520",
    "end": "1646540"
  },
  {
    "text": "some simple use controlled by hyper parameter which I thought was a excellent way of doing that because what you can do now is you can just add that",
    "start": "1646540",
    "end": "1652540"
  },
  {
    "text": "to your high program reprimands when you want to test different permutations so if you want to test it very very low",
    "start": "1652540",
    "end": "1658690"
  },
  {
    "text": "subsampling very very high subsampling you can get that range automated and and see if you can still optimize on",
    "start": "1658690",
    "end": "1665810"
  },
  {
    "text": "these metrics and in decreased training time when it comes to the actual",
    "start": "1665810",
    "end": "1675920"
  },
  {
    "start": "1673000",
    "end": "1794000"
  },
  {
    "text": "training there's only two hyper parameters required the feature dimensions which is you know pretty",
    "start": "1675920",
    "end": "1681890"
  },
  {
    "text": "standard and we have a minimum of one I'm not sure what you would do with one feature in a maximum of a 1 million now",
    "start": "1681890",
    "end": "1688820"
  },
  {
    "text": "I want to put that into just a little bit of perspective here 1 million unique words or vocabulary size is very big",
    "start": "1688820",
    "end": "1696590"
  },
  {
    "text": "that's the scientific phrase for that very big if I take an example of the Tolstoy's war and peace there are twenty",
    "start": "1696590",
    "end": "1703460"
  },
  {
    "text": "thousand four hundred and some change were unique words in that in that",
    "start": "1703460",
    "end": "1708770"
  },
  {
    "text": "writing so to have a vocabulary size of 1 million is very very large and then",
    "start": "1708770",
    "end": "1714140"
  },
  {
    "text": "the number of topics this is something you're gonna play with depending on your needs depending on your requirements and",
    "start": "1714140",
    "end": "1719210"
  },
  {
    "text": "what what you hope to achieve the minimum is two of course but in the maximum was one thousand one of the",
    "start": "1719210",
    "end": "1724640"
  },
  {
    "text": "little trick that you can do here is if your input tokens they so you have so many documents if you set your number of",
    "start": "1724640",
    "end": "1730820"
  },
  {
    "text": "topics to less than the documents what you've done is actually created a means of reducing your dimensionality and",
    "start": "1730820",
    "end": "1737480"
  },
  {
    "text": "you've eliminated as documents that are essentially you know they've scored so",
    "start": "1737480",
    "end": "1742730"
  },
  {
    "text": "low that they're probably don't have a good topic information in them and you've reduced that dimensionality now",
    "start": "1742730",
    "end": "1748910"
  },
  {
    "text": "the rest of the training all adjustable here you're going to see the the usual suspects when it comes to neural",
    "start": "1748910",
    "end": "1754370"
  },
  {
    "text": "networks this this one hopefully you can see my mouse here but this number of patients epochs the hyper stage makers",
    "start": "1754370",
    "end": "1762350"
  },
  {
    "text": "want to do your favor and it's going to train it's going to look for the most efficient you know loss function and the",
    "start": "1762350",
    "end": "1770420"
  },
  {
    "text": "epochs and stop early if you don't increase efficiency over so many epochs it so you can you can set that here by",
    "start": "1770420",
    "end": "1776900"
  },
  {
    "text": "the number of patients and box if you say if I go three epochs without any improvement go ahead and kill it but you",
    "start": "1776900",
    "end": "1782300"
  },
  {
    "text": "can fine-tune that everything else here should be somewhat familiar to you when in terms of in terms of neural",
    "start": "1782300",
    "end": "1788390"
  },
  {
    "text": "network parameters",
    "start": "1788390",
    "end": "1793030"
  },
  {
    "start": "1794000",
    "end": "1890000"
  },
  {
    "text": "and Wow I won't do that much faster than I thought so it looks like we still have lots and lots of time for four questions",
    "start": "1795460",
    "end": "1805420"
  },
  {
    "text": "let's see here so the only question posted here oh here we go Chris you can",
    "start": "1806590",
    "end": "1814310"
  },
  {
    "text": "just handle those go ahead one second",
    "start": "1814310",
    "end": "1821390"
  },
  {
    "text": "yeah just my window sighs it's not cooperating so the session will be",
    "start": "1821390",
    "end": "1827960"
  },
  {
    "text": "online shortly afterwards this session will be online afterwards I can't speak to when if you've been to any webinars",
    "start": "1827960",
    "end": "1835040"
  },
  {
    "text": "in the past we have a great asset and Sookie wig that manages this for us",
    "start": "1835040",
    "end": "1840470"
  },
  {
    "text": "matter of fact you might even heard Chris and I talking before web webinar started because we weren't sure if our mic was hot or not but she she will",
    "start": "1840470",
    "end": "1847400"
  },
  {
    "text": "manage that there's a couple of permutations it needs to go through before it's distributed in a public venue but it will be it will be",
    "start": "1847400",
    "end": "1854240"
  },
  {
    "text": "socialized afterwards and so can the next question is can NTM be applied to",
    "start": "1854240",
    "end": "1860060"
  },
  {
    "text": "language learning right now it only supports English out of the box and I don't know what the roadmap has in store",
    "start": "1860060",
    "end": "1867680"
  },
  {
    "text": "for other languages but I'll definitely get back to you on that that's the question that I'm sure the service team",
    "start": "1867680",
    "end": "1872780"
  },
  {
    "text": "gets quite a bit and they should have some idea of what the what the road map looks like for that just wait here see",
    "start": "1872780",
    "end": "1885470"
  },
  {
    "text": "if any other questions come in Chris anything you think I lost over two quick that you think I should talk about a little bit more well I I think I'd say",
    "start": "1885470",
    "end": "1894980"
  },
  {
    "start": "1890000",
    "end": "1907000"
  },
  {
    "text": "generally that 2018 was the year of natural language processing there were",
    "start": "1894980",
    "end": "1902060"
  },
  {
    "text": "quite of innovations in data science last year so I think these are highly",
    "start": "1902060",
    "end": "1909140"
  },
  {
    "start": "1907000",
    "end": "1969000"
  },
  {
    "text": "useful algorithms one thing that I'll just add also is that with all the sage",
    "start": "1909140",
    "end": "1915620"
  },
  {
    "text": "maker built-in algorithms you get automated memory management and scaling",
    "start": "1915620",
    "end": "1922540"
  },
  {
    "text": "one of the most frequent questions that I get when I'm working with folks in eh maker is you know they're using chaos",
    "start": "1922540",
    "end": "1930520"
  },
  {
    "text": "or maybe just straight-up tensorflow and and they're running out of memory when",
    "start": "1930520",
    "end": "1935870"
  },
  {
    "text": "they're running their models and I upon more questions I determine hate you can",
    "start": "1935870",
    "end": "1943730"
  },
  {
    "text": "use a linear learner or you can use NTM you know try using some of these",
    "start": "1943730",
    "end": "1949010"
  },
  {
    "text": "built-in algorithms because the memory management is built-in not only can you grow out of a single GPU but you can",
    "start": "1949010",
    "end": "1956360"
  },
  {
    "text": "also grow out of a out of us of a machine in other words a more than eight",
    "start": "1956360",
    "end": "1962570"
  },
  {
    "text": "GPUs and into a cluster of systems of course I see some other questions coming in so I'll just I'll let you do that",
    "start": "1962570",
    "end": "1969170"
  },
  {
    "text": "yeah so one question here in recommendation of a primer for this for this topic for this course I really",
    "start": "1969170",
    "end": "1976580"
  },
  {
    "text": "genuinely recommend you hit all of these links looks like I copied and pasted",
    "start": "1976580",
    "end": "1982600"
  },
  {
    "text": "again incorrectly but visit the unique links I apologize for that I'm gonna",
    "start": "1982600",
    "end": "1989990"
  },
  {
    "text": "laugh it off I'm gonna laugh my own my own ineptitude off there but it said especially the MTM paper at archive.org",
    "start": "1989990",
    "end": "1997160"
  },
  {
    "text": "and then also there are two to say to make your notebooks that are in built into every notebook instance that you",
    "start": "1997160",
    "end": "2003670"
  },
  {
    "text": "spin up one is very very basic it's going to use synthetic data it's going to give you a good idea of the flow of",
    "start": "2003670",
    "end": "2009390"
  },
  {
    "text": "NTM training and the second one is more advanced you're going to see the tokenization you're going to see the",
    "start": "2009390",
    "end": "2015070"
  },
  {
    "text": "limitation you're going to see the conversion to MP float32 and so you're also also you're going to see a great",
    "start": "2015070",
    "end": "2021400"
  },
  {
    "text": "example of the vocabulary auxilary channel used which i think is just really a no-brainer to use that so I",
    "start": "2021400",
    "end": "2027880"
  },
  {
    "text": "recommend all these sources you go through all those sources and I think you'll have a great great foundation to",
    "start": "2027880",
    "end": "2033250"
  },
  {
    "text": "go ahead and tackle your own data set or maybe just you know if you're up for the challenge of grab your data set and and",
    "start": "2033250",
    "end": "2038710"
  },
  {
    "text": "and switch it in for the the second notebook the one that's under the the",
    "start": "2038710",
    "end": "2043930"
  },
  {
    "text": "scientific theory section of stage maker and then so here's a question how can we",
    "start": "2043930",
    "end": "2049600"
  },
  {
    "start": "2048000",
    "end": "2074000"
  },
  {
    "text": "get the list of these links so this this this very presentation will be distributed after the we're live right",
    "start": "2049600",
    "end": "2056710"
  },
  {
    "text": "now but this will be sent to you you're going to get a link at your email address that you used to register",
    "start": "2056710",
    "end": "2062210"
  },
  {
    "text": "and so you get to hit listen to this all over again if you so choose and you can scrub these links out of that and I'm",
    "start": "2062210",
    "end": "2075110"
  },
  {
    "start": "2074000",
    "end": "2174000"
  },
  {
    "text": "not sure if this is a question or a statement it's a good map a good map for determining which algorithm to use out",
    "start": "2075110",
    "end": "2080929"
  },
  {
    "text": "of the numerous ones again machine learning starts with answering a",
    "start": "2080930",
    "end": "2087169"
  },
  {
    "text": "question how do i improve this process how do i solve this problem and from that you you derive your requirements to",
    "start": "2087170",
    "end": "2094550"
  },
  {
    "text": "solve it what pieces of information do you need and then from that point it should point you to the algorithm very",
    "start": "2094550",
    "end": "2100220"
  },
  {
    "text": "rarely do you start with an algorithm and say okay here's a great algorithm what problems do I want to solve with it it's a it's a bottoms-up",
    "start": "2100220",
    "end": "2106010"
  },
  {
    "text": "a solutioning process but NTM you're gonna you again comprehend if you if",
    "start": "2106010",
    "end": "2112190"
  },
  {
    "text": "you're very new to NLP I recommend comprehend get familiar with that get familiar with how comprehend gives you",
    "start": "2112190",
    "end": "2118700"
  },
  {
    "text": "results back so that you can understand what latent representation means so you understand an idea of what topics per",
    "start": "2118700",
    "end": "2125750"
  },
  {
    "text": "document and the similarities mean and then step up to an n TM or Lda if you",
    "start": "2125750",
    "end": "2131960"
  },
  {
    "text": "said if you so choose we have both of these options available to you and the the only analogy I could think of I was",
    "start": "2131960",
    "end": "2138110"
  },
  {
    "text": "trying to think of a great analogy for when to use n TM Wendy's Lda it's very circumstantial you know",
    "start": "2138110",
    "end": "2143660"
  },
  {
    "text": "sometimes you need a five-star socket and sometimes you need a 12 star socket but they both you know turn turn a bolt",
    "start": "2143660",
    "end": "2151280"
  },
  {
    "text": "the same way so you could try both sage maker I think you know it's cheap enough",
    "start": "2151280",
    "end": "2157220"
  },
  {
    "text": "if your document corpus is not too large try both it may be averaged out the scores may beat it may be to use both an",
    "start": "2157220",
    "end": "2164030"
  },
  {
    "text": "average out the results for your topics and then you're gonna get a good feel for which one works in which of your",
    "start": "2164030",
    "end": "2170600"
  },
  {
    "text": "scenarios and then we have question here any plans to integrate with things like",
    "start": "2170600",
    "end": "2177350"
  },
  {
    "start": "2174000",
    "end": "2225000"
  },
  {
    "text": "tensor flow Jas I don't know so that's a roadmap question that I can't speculate",
    "start": "2177350",
    "end": "2183560"
  },
  {
    "text": "on I do apologize but we do have we do have neo and I'm not an expert on tensor",
    "start": "2183560",
    "end": "2189530"
  },
  {
    "text": "flow Jas I do apologize for that but we have the ability to train a model",
    "start": "2189530",
    "end": "2194750"
  },
  {
    "text": "and with neo deploy that to many different end points so if you can work to apply a pre-trained model to",
    "start": "2194750",
    "end": "2201350"
  },
  {
    "text": "tensorflow j/s maybe try that out otherwise I can't I can't speak the",
    "start": "2201350",
    "end": "2206660"
  },
  {
    "text": "roadmap I will definitely take the question back to the team and see what answer that they give me but out of the box I we don't have support for",
    "start": "2206660",
    "end": "2213320"
  },
  {
    "text": "tensorflow gjs today and it looks like",
    "start": "2213320",
    "end": "2225470"
  },
  {
    "text": "those are all the questions we have I'll give it one more minute here for any other questions to come in and it looks",
    "start": "2225470",
    "end": "2231680"
  },
  {
    "text": "like once again I I finished early I do apologize I feel a little guilty I could I could have filled the next 15 minutes",
    "start": "2231680",
    "end": "2238880"
  },
  {
    "text": "with more content I got a habit of talking too fast and speaking through things so it was",
    "start": "2238880",
    "end": "2244910"
  },
  {
    "text": "anybody who wants to me go back to any of the slides that that were in the in the slide deck or in webinar or revisit",
    "start": "2244910",
    "end": "2250670"
  },
  {
    "text": "any topics if I glossed over something too quickly please let me know we can we can use this time that we have together",
    "start": "2250670",
    "end": "2255980"
  },
  {
    "text": "and go back over those Chris if you don't mind I'll just throw something out",
    "start": "2255980",
    "end": "2262240"
  },
  {
    "text": "the topic of continuous integration and deployment and the cadence for",
    "start": "2262240",
    "end": "2267890"
  },
  {
    "text": "retraining models is certainly much more on people's minds these days I mean I",
    "start": "2267890",
    "end": "2275270"
  },
  {
    "text": "think we've gone from a point where people are just asking what is machine learning and how do I do it to now folks",
    "start": "2275270",
    "end": "2281750"
  },
  {
    "text": "are using it and putting it in production so obviously see ICD",
    "start": "2281750",
    "end": "2287230"
  },
  {
    "text": "determining a cadence and a workflow for C ICD is is much more of a topics is",
    "start": "2287230",
    "end": "2293330"
  },
  {
    "text": "there anything that occurred to you in developing this webinar that might be special for doing neural topic modeling",
    "start": "2293330",
    "end": "2301880"
  },
  {
    "text": "and creating a cadence for refreshing your models yeah I think the need for",
    "start": "2301880",
    "end": "2307160"
  },
  {
    "start": "2305000",
    "end": "2382000"
  },
  {
    "text": "retraining when it comes to topic modeling is paramount it's actually I",
    "start": "2307160",
    "end": "2312410"
  },
  {
    "text": "don't want to say it's more important I don't want to use that phrase but if you have an image classification I like to pick on image classification for some",
    "start": "2312410",
    "end": "2318290"
  },
  {
    "text": "reason you know when you start to get model decay or data drift because you're going to start to see many many more",
    "start": "2318290",
    "end": "2324290"
  },
  {
    "text": "misses than hits but when it comes to topic modeling because we've introduced Leyton representation it's very",
    "start": "2324290",
    "end": "2330260"
  },
  {
    "text": "difficult to to validate the accuracy in an automated manner this is a great opportunity for human-in-the-loop",
    "start": "2330260",
    "end": "2336190"
  },
  {
    "text": "retraining and what you're going to see here is you're going to see a needs to constantly retrain refine and as your",
    "start": "2336190",
    "end": "2342260"
  },
  {
    "text": "documentation grows you're going to see which documents are more important to your training process and which ones you",
    "start": "2342260",
    "end": "2348049"
  },
  {
    "text": "can omit and then what that gives you is it gives you a better quality data set which should translate to greater",
    "start": "2348049",
    "end": "2354619"
  },
  {
    "text": "accuracy when it comes to metrics for perplexity as well as topic comprehension so I guess to summarize you know Chris's",
    "start": "2354619",
    "end": "2362779"
  },
  {
    "text": "questionnaire retraining is likely far likely more important when it comes to",
    "start": "2362779",
    "end": "2368299"
  },
  {
    "text": "NLP that when it come because this is a call we talked about generative processes versus discriminative",
    "start": "2368299",
    "end": "2374359"
  },
  {
    "text": "processes so as a generative process the retraining is going to be paramount so",
    "start": "2374359",
    "end": "2382880"
  },
  {
    "start": "2382000",
    "end": "2445000"
  },
  {
    "text": "we did get a new question in is there a good way for filtering timestamps or erroneous info and data when doing NLP",
    "start": "2382880",
    "end": "2390910"
  },
  {
    "text": "that is a classic data prep problem time",
    "start": "2390910",
    "end": "2396200"
  },
  {
    "text": "stamps are going to not play a very",
    "start": "2396200",
    "end": "2402020"
  },
  {
    "text": "productive role in NLP as a matter of fact that you're going to want to probably create a mapping mechanism",
    "start": "2402020",
    "end": "2409359"
  },
  {
    "text": "yourself for that remember this is this were talking more vocabulary talk about text talk about topics and topic",
    "start": "2409359",
    "end": "2416210"
  },
  {
    "text": "comprehension so introducing the idea of a time stamp I think would be incredibly difficult and if you have a specific use",
    "start": "2416210",
    "end": "2421970"
  },
  {
    "text": "case that you're working on with that and you want to share with us we can you know send that over to us and we'll see what kind of advice we can get there but",
    "start": "2421970",
    "end": "2428510"
  },
  {
    "text": "when I hear the phrase time stamps I get I get nervous when it comes to training",
    "start": "2428510",
    "end": "2434960"
  },
  {
    "text": "because I'm thinking is really gonna it's going to be very very difficult to to understand that as a topic and so",
    "start": "2434960",
    "end": "2445880"
  },
  {
    "start": "2445000",
    "end": "2532000"
  },
  {
    "text": "another question can NTM do well in corpus of text where we already know the topic and it's somewhat narrow say",
    "start": "2445880",
    "end": "2452089"
  },
  {
    "text": "medical text on cancer treatment and the answer there is yes matter of fact we have comprehension for medical",
    "start": "2452089",
    "end": "2458029"
  },
  {
    "text": "it is a sub what we'll call it as a sub module we'll have comprehend where the",
    "start": "2458029",
    "end": "2463720"
  },
  {
    "text": "vocabulary is specific to health care now if you want to take that even further into a granular use case where",
    "start": "2463720",
    "end": "2471550"
  },
  {
    "text": "it's specific to cancer terminology and what's also interesting bout healthcare is you you almost introduce you in",
    "start": "2471550",
    "end": "2477340"
  },
  {
    "text": "Teresa if you have a we have topic comprehension we talked about so latent representation walk bike car equals",
    "start": "2477340",
    "end": "2485290"
  },
  {
    "text": "transportation but when it comes to medical you can infer the state of a person so if you see a condition X and",
    "start": "2485290",
    "end": "2492340"
  },
  {
    "text": "symptom Y not all not only can you infer with late representation that these are",
    "start": "2492340",
    "end": "2497560"
  },
  {
    "text": "it's a disease present but you can then infer the state of the patient in terms of sick or not sick",
    "start": "2497560",
    "end": "2503590"
  },
  {
    "text": "so drilling down into more specific vocabulary is uh oops sorry about that",
    "start": "2503590",
    "end": "2514440"
  },
  {
    "text": "so yeah I absolutely answer is yes patek yes when it comes to can use NTM on",
    "start": "2514440",
    "end": "2520330"
  },
  {
    "text": "specific vocabulary so Chris I",
    "start": "2520330",
    "end": "2533520"
  },
  {
    "start": "2532000",
    "end": "2658000"
  },
  {
    "text": "I think we're buying Danielle a phenomenal job excellent why don't do you have any other pointers obviously",
    "start": "2533520",
    "end": "2540450"
  },
  {
    "text": "this screen here if we could get this to the audience as quickly as possible I'm",
    "start": "2540450",
    "end": "2548220"
  },
  {
    "text": "sure some have done a screen capture on it but that I will I will correct this I",
    "start": "2548220",
    "end": "2553590"
  },
  {
    "text": "will remove these duplicates my topic comprehension on this slide is very low so we get that we're gonna send out and",
    "start": "2553590",
    "end": "2561330"
  },
  {
    "text": "again this webinar was really meant to just give you an overview of the why on NTM and so for that for the for the how",
    "start": "2561330",
    "end": "2569610"
  },
  {
    "text": "and to see the value in person I recommend you you know try it out grab yourself some some documentation that",
    "start": "2569610",
    "end": "2575280"
  },
  {
    "text": "you have for a use case and run it through there and again questions please please email us we we get very few",
    "start": "2575280",
    "end": "2581100"
  },
  {
    "text": "questions after these webinars when you actually dive into the work so only two only two things could be true one is",
    "start": "2581100",
    "end": "2587640"
  },
  {
    "text": "that you we do such a good job with these webinars that you become instant experts on the topic and you no longer",
    "start": "2587640",
    "end": "2592920"
  },
  {
    "text": "need your help or the other one is that for some reason you're afraid to email us so please don't don't be afraid to",
    "start": "2592920",
    "end": "2599070"
  },
  {
    "text": "email fantastic so with that said I'll",
    "start": "2599070",
    "end": "2604890"
  },
  {
    "text": "thank you again Chris this is an ongoing series where we're covering all of sage",
    "start": "2604890",
    "end": "2610950"
  },
  {
    "text": "makers growing built-in functionality not just algorithms but of course we",
    "start": "2610950",
    "end": "2616770"
  },
  {
    "text": "have ground truth new feature which we'll be covering in the future the very",
    "start": "2616770",
    "end": "2622290"
  },
  {
    "text": "next webinar is for Valentine's Day February 14th I'll be giving a",
    "start": "2622290",
    "end": "2628850"
  },
  {
    "text": "presentation on factorization machines and recommendation engines and I'll be",
    "start": "2628850",
    "end": "2634350"
  },
  {
    "text": "covering the very important new utility that we have which is called object to",
    "start": "2634350",
    "end": "2639390"
  },
  {
    "text": "vac so definitely you set your calendar for that so thank you again Chris thank",
    "start": "2639390",
    "end": "2645210"
  },
  {
    "text": "you all for joining and we'll see you at the next webinar yep thank you everybody",
    "start": "2645210",
    "end": "2653570"
  }
]