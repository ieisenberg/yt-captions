[
  {
    "start": "0",
    "end": "43000"
  },
  {
    "text": "- Welcome to 'Back To Basics'.",
    "start": "6180",
    "end": "7830"
  },
  {
    "text": "In this episode, we will discuss selecting",
    "start": "7830",
    "end": "10590"
  },
  {
    "text": "the right inferencing option",
    "start": "10590",
    "end": "12000"
  },
  {
    "text": "for deploying machine learning\nmodels on Amazon SageMaker.",
    "start": "12000",
    "end": "15540"
  },
  {
    "text": "You may be wondering what is inferencing?",
    "start": "15540",
    "end": "18390"
  },
  {
    "text": "When you deploy your trained\nmachine learning models",
    "start": "18390",
    "end": "20849"
  },
  {
    "text": "to make predictions in\nreal world scenarios",
    "start": "20850",
    "end": "23550"
  },
  {
    "text": "or production environments,\nit is known as inferencing.",
    "start": "23550",
    "end": "26939"
  },
  {
    "text": "Customers often have a hard time",
    "start": "26940",
    "end": "29100"
  },
  {
    "text": "figuring out the best deployment strategy",
    "start": "29100",
    "end": "31440"
  },
  {
    "text": "because every use case\nhas different requirements",
    "start": "31440",
    "end": "33990"
  },
  {
    "text": "like latency, throughput, payload size.",
    "start": "33990",
    "end": "37440"
  },
  {
    "text": "Let's look at a real world scenario",
    "start": "37440",
    "end": "39270"
  },
  {
    "text": "and decide which option is\nthe best in this use case.",
    "start": "39270",
    "end": "43350"
  },
  {
    "start": "43000",
    "end": "97000"
  },
  {
    "text": "Say you have a banking customer",
    "start": "43350",
    "end": "45300"
  },
  {
    "text": "working on a fraud detection project,",
    "start": "45300",
    "end": "47190"
  },
  {
    "text": "where they wanted to deploy\na binary classification",
    "start": "47190",
    "end": "50280"
  },
  {
    "text": "XGBoost model that has\nalready been trained",
    "start": "50280",
    "end": "53489"
  },
  {
    "text": "on their label data set,",
    "start": "53490",
    "end": "55380"
  },
  {
    "text": "to quickly predict whether a transaction",
    "start": "55380",
    "end": "57780"
  },
  {
    "text": "is fraudulent or not.",
    "start": "57780",
    "end": "59579"
  },
  {
    "text": "This scenario experiences\na constant traffic",
    "start": "59580",
    "end": "62430"
  },
  {
    "text": "and a payload size of under four MB.",
    "start": "62430",
    "end": "65460"
  },
  {
    "text": "Their current deployed models\nhave high response time",
    "start": "65460",
    "end": "69000"
  },
  {
    "text": "and they're struggling to find\nthe right inferencing option,",
    "start": "69000",
    "end": "72180"
  },
  {
    "text": "which can process millions\nof transactions per second.",
    "start": "72180",
    "end": "75660"
  },
  {
    "text": "Your customer has already spent months",
    "start": "75660",
    "end": "77940"
  },
  {
    "text": "improving the model's accuracy,",
    "start": "77940",
    "end": "79740"
  },
  {
    "text": "and they do not want to\nspend additional time",
    "start": "79740",
    "end": "82619"
  },
  {
    "text": "experimenting with the\nright deployment option.",
    "start": "82620",
    "end": "85980"
  },
  {
    "text": "With that, let's dive deep",
    "start": "85980",
    "end": "88080"
  },
  {
    "text": "into various Amazon SageMaker\ndeployment options available",
    "start": "88080",
    "end": "92040"
  },
  {
    "text": "and understand how to pick the best option",
    "start": "92040",
    "end": "95130"
  },
  {
    "text": "based on your requirements.",
    "start": "95130",
    "end": "97380"
  },
  {
    "start": "97000",
    "end": "266000"
  },
  {
    "text": "Your first option is\nSageMaker Real-Time Inference.",
    "start": "97380",
    "end": "101189"
  },
  {
    "text": "This option is ideal for use cases",
    "start": "101190",
    "end": "103650"
  },
  {
    "text": "that requires low latency\nand high throughput.",
    "start": "103650",
    "end": "106800"
  },
  {
    "text": "In this option, you get a persistent",
    "start": "106800",
    "end": "108960"
  },
  {
    "text": "and a fully managed\nendpoint that can handle",
    "start": "108960",
    "end": "111810"
  },
  {
    "text": "sustained traffic.",
    "start": "111810",
    "end": "113310"
  },
  {
    "text": "This endpoint is running\non the instance type",
    "start": "113310",
    "end": "116070"
  },
  {
    "text": "of your choice.",
    "start": "116070",
    "end": "117600"
  },
  {
    "text": "Your application makes\nan inferencing request",
    "start": "117600",
    "end": "120330"
  },
  {
    "text": "and gets an immediate response.",
    "start": "120330",
    "end": "122940"
  },
  {
    "text": "Real-Time Inference can\nsupport payload size",
    "start": "122940",
    "end": "125460"
  },
  {
    "text": "up to six megabytes and a\nprocessing time of 60 seconds.",
    "start": "125460",
    "end": "129929"
  },
  {
    "text": "Some of the use cases are ad serving,",
    "start": "129930",
    "end": "132750"
  },
  {
    "text": "personalized recommendations\nand fraud detection,",
    "start": "132750",
    "end": "135660"
  },
  {
    "text": "all of which need a fast response.",
    "start": "135660",
    "end": "138570"
  },
  {
    "text": "Your second option is\nSageMaker Serverless Inference.",
    "start": "138570",
    "end": "142200"
  },
  {
    "text": "It's best suitable for workloads",
    "start": "142200",
    "end": "144030"
  },
  {
    "text": "with intermittent or\nunpredictable traffic patterns.",
    "start": "144030",
    "end": "147630"
  },
  {
    "text": "Unlike the Real-Time\nscenario, in this option,",
    "start": "147630",
    "end": "150570"
  },
  {
    "text": "you have ideal periods\nbetween traffic bursts,",
    "start": "150570",
    "end": "153720"
  },
  {
    "text": "and your workload should be\nable to tolerate cold starts.",
    "start": "153720",
    "end": "157470"
  },
  {
    "text": "Serverless Inference automatically\nlaunchs compute resources",
    "start": "157470",
    "end": "161190"
  },
  {
    "text": "and scales depending upon traffic,",
    "start": "161190",
    "end": "163500"
  },
  {
    "text": "eliminating the need for\nyou to choose instance types",
    "start": "163500",
    "end": "166590"
  },
  {
    "text": "or manage scaling policies,",
    "start": "166590",
    "end": "168480"
  },
  {
    "text": "and allows you to only\npay for what you use.",
    "start": "168480",
    "end": "171360"
  },
  {
    "text": "It can support a payload\nsize up to four megabytes",
    "start": "171360",
    "end": "174000"
  },
  {
    "text": "and a processing time up to 60 seconds.",
    "start": "174000",
    "end": "176970"
  },
  {
    "text": "Some of the examples are test workloads,",
    "start": "176970",
    "end": "179700"
  },
  {
    "text": "extracting and analyzing\ndata from documents,",
    "start": "179700",
    "end": "182790"
  },
  {
    "text": "form processing, and chat bots",
    "start": "182790",
    "end": "185219"
  },
  {
    "text": "with unpredictable usage times.",
    "start": "185220",
    "end": "188130"
  },
  {
    "text": "Your third option is SageMaker\nAsynchronous Inference.",
    "start": "188130",
    "end": "192240"
  },
  {
    "text": "It's perfect for inferences",
    "start": "192240",
    "end": "194430"
  },
  {
    "text": "when you want to queue requests",
    "start": "194430",
    "end": "196409"
  },
  {
    "text": "and have a large payload\nsize, up to one gigabyte,",
    "start": "196410",
    "end": "199980"
  },
  {
    "text": "or long processing\ntimes, up to 15 minutes.",
    "start": "199980",
    "end": "202800"
  },
  {
    "text": "You can scale down your endpoints to zero",
    "start": "202800",
    "end": "205590"
  },
  {
    "text": "when there are no requests to process,",
    "start": "205590",
    "end": "207780"
  },
  {
    "text": "which makes it a very\ncost-effective option.",
    "start": "207780",
    "end": "210270"
  },
  {
    "text": "A few examples are computer\nvision and object detection.",
    "start": "210270",
    "end": "214890"
  },
  {
    "text": "And a common theme with this choice",
    "start": "214890",
    "end": "216960"
  },
  {
    "text": "is the ability to wait on\ninferencing or responses.",
    "start": "216960",
    "end": "220290"
  },
  {
    "text": "Your final option is\nSageMaker Batch Transform.",
    "start": "220290",
    "end": "223860"
  },
  {
    "text": "It's available for offline processing,",
    "start": "223860",
    "end": "226230"
  },
  {
    "text": "when you have large amounts of data",
    "start": "226230",
    "end": "228180"
  },
  {
    "text": "and you do not need a persistent endpoint.",
    "start": "228180",
    "end": "230849"
  },
  {
    "text": "Typically, it is used for\npre-processing data sets.",
    "start": "230850",
    "end": "233940"
  },
  {
    "text": "Compared to all the\noptions discussed before,",
    "start": "233940",
    "end": "236970"
  },
  {
    "text": "this has the highest\nthroughput option available,",
    "start": "236970",
    "end": "239520"
  },
  {
    "text": "it can support large datasets\nthat are gigabytes in size",
    "start": "239520",
    "end": "243450"
  },
  {
    "text": "and processing time up to days.",
    "start": "243450",
    "end": "245879"
  },
  {
    "text": "Some of the examples\nare data pre-processing,",
    "start": "245880",
    "end": "248820"
  },
  {
    "text": "churn prediction, and\npredictive maintenance.",
    "start": "248820",
    "end": "252120"
  },
  {
    "text": "Coming back to the fraud\ndetection use case.",
    "start": "252120",
    "end": "254730"
  },
  {
    "text": "Based on the requirements of\nlow latency, high throughput,",
    "start": "254730",
    "end": "258389"
  },
  {
    "text": "payload size being less\nthan four megabytes,",
    "start": "258390",
    "end": "261060"
  },
  {
    "text": "real-time prediction,",
    "start": "261060",
    "end": "262440"
  },
  {
    "text": "you would use a SageMaker\nReal-Time Inference.",
    "start": "262440",
    "end": "266250"
  },
  {
    "start": "266000",
    "end": "335000"
  },
  {
    "text": "To do so, you create a SageMaker model",
    "start": "266250",
    "end": "268860"
  },
  {
    "text": "from the trained model artifact.",
    "start": "268860",
    "end": "271020"
  },
  {
    "text": "Then you configure a\nReal-Time Inference endpoint",
    "start": "271020",
    "end": "273750"
  },
  {
    "text": "by specifying a few properties,",
    "start": "273750",
    "end": "275730"
  },
  {
    "text": "like EC2 instance types and counts.",
    "start": "275730",
    "end": "278130"
  },
  {
    "text": "Then you deploy your model to\nthe SageMaker hosting services",
    "start": "278130",
    "end": "281460"
  },
  {
    "text": "and get an endpoint that\ncan be used for inferencing,",
    "start": "281460",
    "end": "284759"
  },
  {
    "text": "in order to detect fraudulent\ntransactions in real-time.",
    "start": "284760",
    "end": "288420"
  },
  {
    "text": "This endpoint is fully managed\nand supports auto-scaling.",
    "start": "288420",
    "end": "292020"
  },
  {
    "text": "As a best practice,",
    "start": "292020",
    "end": "293370"
  },
  {
    "text": "make sure the instance size can handle",
    "start": "293370",
    "end": "295110"
  },
  {
    "text": "the expected heavy traffic\nand latency requirements.",
    "start": "295110",
    "end": "298620"
  },
  {
    "text": "End users will send an inference request",
    "start": "298620",
    "end": "301199"
  },
  {
    "text": "and invoke the model for prediction.",
    "start": "301200",
    "end": "303630"
  },
  {
    "text": "In response to the request,",
    "start": "303630",
    "end": "305370"
  },
  {
    "text": "the model will generate a prediction",
    "start": "305370",
    "end": "307320"
  },
  {
    "text": "and send it as a response in\nreal-time to the end user.",
    "start": "307320",
    "end": "311340"
  },
  {
    "text": "In summary, it's important to\ngather relevant requirements",
    "start": "311340",
    "end": "314850"
  },
  {
    "text": "like payload size,\nlatency, request timeout,",
    "start": "314850",
    "end": "318540"
  },
  {
    "text": "and traffic patterns,",
    "start": "318540",
    "end": "320280"
  },
  {
    "text": "so that you can decide which\nmodel deployment option",
    "start": "320280",
    "end": "323700"
  },
  {
    "text": "is best for your specific use case.",
    "start": "323700",
    "end": "326640"
  },
  {
    "text": "Check out the links in\nthe description below",
    "start": "326640",
    "end": "328680"
  },
  {
    "text": "for more details.",
    "start": "328680",
    "end": "330060"
  },
  {
    "text": "See you next time.",
    "start": "330060",
    "end": "331560"
  }
]