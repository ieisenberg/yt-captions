[
  {
    "start": "0",
    "end": "92000"
  },
  {
    "text": "Amazon Managed Workflows\nfor Apache Airflow",
    "start": "0",
    "end": "3000"
  },
  {
    "text": "is a managed environment \nin the AWS cloud",
    "start": "3000",
    "end": "6000"
  },
  {
    "text": "for the open source \nApache Airflow project.",
    "start": "6000",
    "end": "8000"
  },
  {
    "text": "Apache Airflow is a popular\nworkflow management platform",
    "start": "8001",
    "end": "11000"
  },
  {
    "text": "The project was open sourced from the",
    "start": "11000",
    "end": "14000"
  },
  {
    "text": "very first commit, and the Airflow community",
    "start": "14000",
    "end": "17000"
  },
  {
    "text": "has hundreds of active members who\nsupport each other in solving their problems.",
    "start": "17000",
    "end": "20000"
  },
  {
    "text": "Airflow is commonly used for tasks like",
    "start": "20000",
    "end": "23000"
  },
  {
    "text": "running ETL jobs",
    "start": "23000",
    "end": "26000"
  },
  {
    "text": "managing Machine Learning pipelines,\nand automating DevOps tasks.",
    "start": "26000",
    "end": "29000"
  },
  {
    "text": "Customers love Airflow's supportive community",
    "start": "29000",
    "end": "32000"
  },
  {
    "text": "and robust ecosystem. But many have asked us:",
    "start": "32000",
    "end": "35000"
  },
  {
    "text": "\"Can AWS help us to deploy and rapidly scale",
    "start": "35000",
    "end": "38000"
  },
  {
    "text": "Apache Airflow environments without having\nto set up and maintain",
    "start": "38000",
    "end": "41000"
  },
  {
    "text": "the infrastructure ourselves?\"",
    "start": "41000",
    "end": "44000"
  },
  {
    "text": "Today, using Amazon Managed Workflows\nfor Apache Airflow,",
    "start": "44000",
    "end": "47000"
  },
  {
    "text": "you can author, schedule, and monitor",
    "start": "47000",
    "end": "50000"
  },
  {
    "text": "workflows using Airflow inside AWS",
    "start": "50000",
    "end": "53000"
  },
  {
    "text": "without facing the common challenges involved\nin running your own",
    "start": "53000",
    "end": "56000"
  },
  {
    "text": "Airflow environments.",
    "start": "56000",
    "end": "59000"
  },
  {
    "text": "Managed Workflows take care of your\nAirflow environment setup, scaling, ",
    "start": "59000",
    "end": "62000"
  },
  {
    "text": "and security, as well as handing",
    "start": "62000",
    "end": "64266"
  },
  {
    "text": "upgrades and monitoring.",
    "start": "64500",
    "end": "67500"
  },
  {
    "text": "This means you can spend a lot less time ",
    "start": "67500",
    "end": "70500"
  },
  {
    "text": "managing Airflow environments, and a lot more\ntime using them",
    "start": "70500",
    "end": "73500"
  },
  {
    "text": "to execute your data processing workflows\nin the cloud.",
    "start": "73500",
    "end": "76500"
  },
  {
    "text": "Getting started with Managed Workflows is easy.",
    "start": "76500",
    "end": "79500"
  },
  {
    "text": "You can create an Airflow environment\nand run your first",
    "start": "79500",
    "end": "82500"
  },
  {
    "text": "workflow in minutes. And, Managed Workflows",
    "start": "82500",
    "end": "85500"
  },
  {
    "text": "environments come pre-configured for \nhigh availability",
    "start": "85500",
    "end": "88500"
  },
  {
    "text": "and automatic scaling.\nHere's how to get started.",
    "start": "88500",
    "end": "91500"
  },
  {
    "text": "First, in the AWS Management",
    "start": "91500",
    "end": "92866"
  },
  {
    "start": "92000",
    "end": "129000"
  },
  {
    "text": "console",
    "start": "92866",
    "end": "95866"
  },
  {
    "text": "navigate to Managed Apache Airflow",
    "start": "95866",
    "end": "98866"
  },
  {
    "text": "click on create environment, and start by\nentering a name for your environment.",
    "start": "98866",
    "end": "101866"
  },
  {
    "text": "Here, you can also see the version of Airflow",
    "start": "103500",
    "end": "106500"
  },
  {
    "text": "that will be installed.\nManaged Workflows run",
    "start": "106500",
    "end": "109500"
  },
  {
    "text": "the actual open source Apache Airflow \nsoftware in the",
    "start": "109500",
    "end": "112500"
  },
  {
    "text": "environments it manages for you.",
    "start": "112500",
    "end": "115500"
  },
  {
    "text": "Minor version patches for environment\nwill be performed",
    "start": "115500",
    "end": "118500"
  },
  {
    "text": "automatically during a maintenance window\nyou can define. And,",
    "start": "118500",
    "end": "121500"
  },
  {
    "text": "when major version upgrades are available,",
    "start": "121500",
    "end": "124500"
  },
  {
    "text": "you can apply them on-demand, with the ability to\nroll back to the",
    "start": "124500",
    "end": "127500"
  },
  {
    "text": "previous version if required.",
    "start": "127500",
    "end": "128500"
  },
  {
    "text": "Airflow workflows are expressed",
    "start": "128500",
    "end": "131500"
  },
  {
    "start": "129000",
    "end": "291000"
  },
  {
    "text": "as a collection of all the tasks you want to run,\norganized in a way",
    "start": "131500",
    "end": "134500"
  },
  {
    "text": "that reflect their relationships and dependencies.",
    "start": "134500",
    "end": "137500"
  },
  {
    "text": "In technical terms, this collection of tasks",
    "start": "137500",
    "end": "140500"
  },
  {
    "text": "is called a Directed Acyclic Graph, or DAG.",
    "start": "140500",
    "end": "143500"
  },
  {
    "text": "To define workflows in Airflow,",
    "start": "143500",
    "end": "146500"
  },
  {
    "text": "you write DAGs using the Python programming language.",
    "start": "146500",
    "end": "149500"
  },
  {
    "text": "Managed Workflows uses Amazon S3,",
    "start": "149500",
    "end": "152500"
  },
  {
    "text": "our Simple Storage Service, to access the\nAirflow DAG files.",
    "start": "152500",
    "end": "155500"
  },
  {
    "text": "So, the next step is to specify an existing",
    "start": "155500",
    "end": "158500"
  },
  {
    "text": "S3 bucket and path where your Airflow",
    "start": "158500",
    "end": "161500"
  },
  {
    "text": "DAG files will be located.\nYou can also optionally",
    "start": "161500",
    "end": "164500"
  },
  {
    "text": "install Airflow plugins and Python modules",
    "start": "164500",
    "end": "167500"
  },
  {
    "text": "required by your DAGs by providing a plugins zip file",
    "start": "167500",
    "end": "170500"
  },
  {
    "text": "and a requirements text file.\nWe'll click next to continue.",
    "start": "170500",
    "end": "173500"
  },
  {
    "text": "Managed Workflows create",
    "start": "173500",
    "end": "176500"
  },
  {
    "text": "the resources for each Airflow environment",
    "start": "176500",
    "end": "179500"
  },
  {
    "text": "inside an Amazon Virtual Private Cloud\nor VPC.",
    "start": "179500",
    "end": "182500"
  },
  {
    "text": "So, you'll need to select the VPC you want to use.",
    "start": "182500",
    "end": "185500"
  },
  {
    "text": "Each environment automically spans",
    "start": "185500",
    "end": "188500"
  },
  {
    "text": "multiple Availability Zones, or AZs, so",
    "start": "188500",
    "end": "191500"
  },
  {
    "text": "you need to select private subnets in\ntwo different AZs",
    "start": "191500",
    "end": "194500"
  },
  {
    "text": "from your VPC as well.\nNext, you need to",
    "start": "194500",
    "end": "197500"
  },
  {
    "text": "choose how you want to access the Airflow UI.",
    "start": "197500",
    "end": "200500"
  },
  {
    "text": "You can choose the Private Network option,\nwhich will create a fully-private",
    "start": "200500",
    "end": "203500"
  },
  {
    "text": "endpoint accessible only from inside your VPC.",
    "start": "203500",
    "end": "206500"
  },
  {
    "text": "Or, you can choose to create a publicly accessible endpoint.",
    "start": "206500",
    "end": "209500"
  },
  {
    "text": "Both options are authenticated and authorized",
    "start": "209500",
    "end": "212500"
  },
  {
    "text": "via AWS's Identity and Access Management System",
    "start": "212500",
    "end": "215500"
  },
  {
    "text": "or IAM. \nFor this demo,",
    "start": "215500",
    "end": "218500"
  },
  {
    "text": "we'll choose the public network option.",
    "start": "218500",
    "end": "220500"
  },
  {
    "text": "Managed workflows provide automatic",
    "start": "220500",
    "end": "223433"
  },
  {
    "text": "worker scaling for you. When setting up\na new environment, you",
    "start": "223433",
    "end": "226433"
  },
  {
    "text": "select an initial environment class, and a maximum",
    "start": "226433",
    "end": "229433"
  },
  {
    "text": "number of workers. The environment class\ncontrols the amount",
    "start": "229433",
    "end": "232433"
  },
  {
    "text": "of vCPUs and RAM available for the worker nodes",
    "start": "232433",
    "end": "235433"
  },
  {
    "text": "and can be monitored and changed at any time.",
    "start": "235433",
    "end": "238433"
  },
  {
    "text": "Managed Workflows will scale the worker nodes\nup to your maximum",
    "start": "238433",
    "end": "241433"
  },
  {
    "text": "to accomodate your workloads, and when there are",
    "start": "241433",
    "end": "244433"
  },
  {
    "text": "no further queued or running tasks,\nit will remove those additional workers.",
    "start": "244433",
    "end": "247433"
  },
  {
    "text": "The service will also automatically",
    "start": "247433",
    "end": "250433"
  },
  {
    "text": "monitor the health of the worker and scheduler nodes,",
    "start": "250433",
    "end": "253433"
  },
  {
    "text": "restarting them on fresh containers when required.",
    "start": "253433",
    "end": "256432"
  },
  {
    "text": "Managed Workflows make it easy to run Airflow",
    "start": "256433",
    "end": "259433"
  },
  {
    "text": "with in-built security. \nAll of your data is encrypted",
    "start": "259433",
    "end": "262433"
  },
  {
    "text": "by default. You can choose to use an encryption key",
    "start": "262433",
    "end": "265433"
  },
  {
    "text": "managed by AWS or select a key that\nyou've configured",
    "start": "265433",
    "end": "268433"
  },
  {
    "text": "in the AWS Key Management Service.",
    "start": "268433",
    "end": "271433"
  },
  {
    "text": "To provide environment observability,\nyou can configure",
    "start": "271433",
    "end": "274433"
  },
  {
    "text": "the level of logging to persist for your tasks,\nas well as for the",
    "start": "274433",
    "end": "277433"
  },
  {
    "text": "web server, scheduler, worker nodes, and\nDAG processing logs.",
    "start": "277433",
    "end": "280433"
  },
  {
    "text": "These logs, as well as your environment",
    "start": "280433",
    "end": "283433"
  },
  {
    "text": "metrics, are stored in our observability service,",
    "start": "283433",
    "end": "286433"
  },
  {
    "text": "Amazon CloudWatch. You can also provide tags",
    "start": "286433",
    "end": "289433"
  },
  {
    "text": "for your environment if you wish.",
    "start": "289433",
    "end": "292433"
  },
  {
    "start": "291000",
    "end": "408000"
  },
  {
    "text": "To execute tasks, the Airflow workers assume\nan execution role.",
    "start": "292433",
    "end": "295433"
  },
  {
    "text": "This gives you a simple and secure way\nto access",
    "start": "295433",
    "end": "298433"
  },
  {
    "text": "other AWS services from your tasks.",
    "start": "298433",
    "end": "301433"
  },
  {
    "text": "Here, you can either select an existing role,\nor have the console help you",
    "start": "301433",
    "end": "304433"
  },
  {
    "text": "to create a new role for your workers.\nAfter selecting a role,",
    "start": "304433",
    "end": "306400"
  },
  {
    "text": "click create environment.",
    "start": "306400",
    "end": "308000"
  },
  {
    "text": "Once the environment becomes available",
    "start": "308000",
    "end": "311000"
  },
  {
    "text": "click the Open Airlflow UI link",
    "start": "311000",
    "end": "314000"
  },
  {
    "text": "to access the environment.\nAs you can see,",
    "start": "314000",
    "end": "317000"
  },
  {
    "text": "this is the same AIrflow U that you would see\nif you downloaded",
    "start": "317000",
    "end": "320000"
  },
  {
    "text": "and installed Airflow on your own.",
    "start": "320000",
    "end": "323000"
  },
  {
    "text": "To finish this demo, let's upload a DAG from our workstation\nand run it in the cloud.",
    "start": "323000",
    "end": "326000"
  },
  {
    "text": "We'll view our Airflow environment details and click the link",
    "start": "326000",
    "end": "329000"
  },
  {
    "text": "in the DAG code in Amazon S3 section.",
    "start": "329000",
    "end": "332000"
  },
  {
    "text": "Then we'll click into the DAG",
    "start": "332000",
    "end": "333000"
  },
  {
    "text": "folder and upload a new DAG file.",
    "start": "333000",
    "end": "336000"
  },
  {
    "text": "After a short wait, the new DAG shows up",
    "start": "336000",
    "end": "339000"
  },
  {
    "text": "inside the Airflow UI, and clicking in to it,",
    "start": "339000",
    "end": "342000"
  },
  {
    "text": "we can load the Graph View to examine what it does.",
    "start": "342000",
    "end": "345000"
  },
  {
    "text": "This example DAG performs an ETL style workflow,",
    "start": "345000",
    "end": "348000"
  },
  {
    "text": "copying CSV files from a public dataset into\nan Amazon S3 bucket,",
    "start": "348000",
    "end": "351000"
  },
  {
    "text": "perfoming a join and aggregation across those CSV files,",
    "start": "351000",
    "end": "354000"
  },
  {
    "text": "and storing the result in S3 as well.",
    "start": "354000",
    "end": "357000"
  },
  {
    "text": "This DAG uses a built-in Airflow sensor to check",
    "start": "357000",
    "end": "360000"
  },
  {
    "text": "S3 and a built-in operator to execute",
    "start": "360000",
    "end": "363000"
  },
  {
    "text": "serverless SQL queries against CSV files",
    "start": "363000",
    "end": "366000"
  },
  {
    "text": "using Amazon Athena. But, because Managed Workflows",
    "start": "366000",
    "end": "369000"
  },
  {
    "text": "are running the same open source Apache Airflow",
    "start": "369000",
    "end": "372000"
  },
  {
    "text": "that you can download yourself, your workflows",
    "start": "372000",
    "end": "375000"
  },
  {
    "text": "can use any of the built-in sensors and operators\nthat are part of the Airflow project,",
    "start": "375000",
    "end": "378000"
  },
  {
    "text": "as well as the features from any plugins",
    "start": "378000",
    "end": "381000"
  },
  {
    "text": "or Python libraries that you choose to install\ninto your environment.",
    "start": "381000",
    "end": "384000"
  },
  {
    "text": "Finally, as we mentioned before,",
    "start": "384000",
    "end": "387000"
  },
  {
    "text": "Managed Workflows integrates with Amazon CloudWatch\nfor logging and monitoring.",
    "start": "387000",
    "end": "390000"
  },
  {
    "text": "No matter how many Airflow environments you create,",
    "start": "390000",
    "end": "393000"
  },
  {
    "text": "you can view their logs and monitor all their essential metrics",
    "start": "393000",
    "end": "396000"
  },
  {
    "text": "like Task Delays, DAG Load Failures, and more,",
    "start": "396000",
    "end": "399000"
  },
  {
    "text": "from one central location.",
    "start": "399000",
    "end": "401500"
  },
  {
    "text": "To learn more, please visit our website",
    "start": "401500",
    "end": "404866"
  },
  {
    "text": "and thanks for joining me for this brief overview.",
    "start": "404866",
    "end": "407866"
  }
]