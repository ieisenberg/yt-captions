[
  {
    "start": "0",
    "end": "107000"
  },
  {
    "text": "all right welcome everyone my name is Jeremy Wallace I'm a Solutions Architect here at Amazon Web Services and I'm very",
    "start": "0",
    "end": "7140"
  },
  {
    "text": "excited today to introduce a company who provides millions of Time series metrics per second to customers all around the",
    "start": "7140",
    "end": "13290"
  },
  {
    "text": "world and so here to discuss how they've used EBS to lower costs while improving",
    "start": "13290",
    "end": "18510"
  },
  {
    "text": "performance and availability of Cassandra on AWS it's my pleasure to introduce Mike Hefner from data",
    "start": "18510",
    "end": "25800"
  },
  {
    "text": "engineering at liberado and Peter Norton who's the operations lead here at",
    "start": "25800",
    "end": "30840"
  },
  {
    "text": "liberado so I'll pass it on to them and",
    "start": "30840",
    "end": "35000"
  },
  {
    "text": "welcome yeah so just a little background for those who might not know liberado we",
    "start": "39710",
    "end": "49200"
  },
  {
    "text": "are part of the SolarWinds monitoring cloud that is a combination of products",
    "start": "49200",
    "end": "54750"
  },
  {
    "text": "you see here we're both with LeBron o today and you know specifically liberado",
    "start": "54750",
    "end": "61920"
  },
  {
    "text": "is a metrics and monitoring solution provided as a service in the cloud we're",
    "start": "61920",
    "end": "69240"
  },
  {
    "text": "consuming electric mentioned hundreds of thousands millions of time series every second we store that for you do data",
    "start": "69240",
    "end": "77100"
  },
  {
    "text": "processing on that provide real-time dashboards and push all that data",
    "start": "77100",
    "end": "82409"
  },
  {
    "text": "through our powerful learning platform we have hundreds of integrations so",
    "start": "82409",
    "end": "89189"
  },
  {
    "text": "based on you know what you're running in your stack today there's a high chance that your artery running with something",
    "start": "89189",
    "end": "96030"
  },
  {
    "text": "that we support today and we also have you know third-party contributions and",
    "start": "96030",
    "end": "101130"
  },
  {
    "text": "other integrations that will get you going and get data into the system",
    "start": "101130",
    "end": "106340"
  },
  {
    "text": "because we are at AWS conference I want to mention and we also have native cloud watch support so you just give us your I",
    "start": "106340",
    "end": "112920"
  },
  {
    "start": "107000",
    "end": "107000"
  },
  {
    "text": "am credentials we'll pull that data into the system natively into your account",
    "start": "112920",
    "end": "118340"
  },
  {
    "text": "real-time dashboards so these work as well if you are firefighting on your laptop or you want to take this put it",
    "start": "118340",
    "end": "125670"
  },
  {
    "text": "on the wall and see how real-time updating view of your infrastructure and",
    "start": "125670",
    "end": "131190"
  },
  {
    "text": "all that data that we are collecting all that goes through our learning framework so",
    "start": "131190",
    "end": "136350"
  },
  {
    "text": "everything that you can put on a chart you can create an alert and we'll send those notifications out to this popular",
    "start": "136350",
    "end": "141930"
  },
  {
    "text": "services that you're using today slack ops Giannini pager duty cetera so like I",
    "start": "141930",
    "end": "150960"
  },
  {
    "text": "mentioned we consume a large number of Time series metrics hundreds of thousands every second into the service",
    "start": "150960",
    "end": "157940"
  },
  {
    "text": "we have chosen and we've been on for many years now Apache Cassandra as the",
    "start": "157940",
    "end": "163620"
  },
  {
    "text": "sort of data store for all that processing and new Apache Cassandra you",
    "start": "163620",
    "end": "169710"
  },
  {
    "text": "know one of the major use cases for that is time-series metrics it works great for sort of that scalable high right",
    "start": "169710",
    "end": "177030"
  },
  {
    "text": "load and you know the ability to sort of horizontally scale that out and yeah all",
    "start": "177030",
    "end": "183870"
  },
  {
    "text": "the all of our data is committed directly into that and we serve all of our requires from that as our primary",
    "start": "183870",
    "end": "189930"
  },
  {
    "text": "data store for metrics so we do run multiple Cassandra rings our",
    "start": "189930",
    "end": "195210"
  },
  {
    "start": "192000",
    "end": "192000"
  },
  {
    "text": "infrastructure and we've sort of split them into I would say two distinct",
    "start": "195210",
    "end": "200400"
  },
  {
    "text": "categories we have our real-time Cassandra rings that and these are consuming the traffic coming directly",
    "start": "200400",
    "end": "207150"
  },
  {
    "text": "into our API this is the real-time one-second resolution data we're storing that for you know on about average of",
    "start": "207150",
    "end": "214860"
  },
  {
    "text": "about a week retention sort of that window where you want that fire fighting high resolution high granularity data",
    "start": "214860",
    "end": "221220"
  },
  {
    "text": "and the challenges there is that because that workload is so heavy we're",
    "start": "221220",
    "end": "226620"
  },
  {
    "text": "expanding those rings simply to consume that right load so a lot of CPU scaling",
    "start": "226620",
    "end": "232290"
  },
  {
    "text": "of those rings much larger ring sizes in general to support that workload on the",
    "start": "232290",
    "end": "238500"
  },
  {
    "text": "other side you know we take that data we down sample that two 15-minute samples",
    "start": "238500",
    "end": "243570"
  },
  {
    "text": "to hourly samples we store that for longer so give that sort of historical trend analysis in those rings that we",
    "start": "243570",
    "end": "250440"
  },
  {
    "text": "push that data into the right loads much much smaller because we are down sampling but the challenge there is that",
    "start": "250440",
    "end": "256769"
  },
  {
    "text": "because we are storing that data for longer we actually use more of the disks on those nodes so much higher disk",
    "start": "256769",
    "end": "263940"
  },
  {
    "text": "volumes on those rings but smaller rings in general they're you",
    "start": "263940",
    "end": "271260"
  },
  {
    "start": "269000",
    "end": "269000"
  },
  {
    "text": "know sort of the sort where we were last",
    "start": "271260",
    "end": "278250"
  },
  {
    "text": "year we do run we were running Cassandra 2-0 11 as our primary release across all of our rings",
    "start": "278250",
    "end": "284940"
  },
  {
    "text": "some patches on top of that we're using the I to 2x large instant site if you've",
    "start": "284940",
    "end": "291240"
  },
  {
    "text": "read the data stacks post or the Netflix post as sort of the preferred instance type gives you the best balance of SSD",
    "start": "291240",
    "end": "297840"
  },
  {
    "text": "performance with you know larger CPU and memory torque bounce there so the sort",
    "start": "297840",
    "end": "304050"
  },
  {
    "text": "of the default go-to for Cassandra workloads we had I think at last count about 160 of those running total across",
    "start": "304050",
    "end": "312150"
  },
  {
    "text": "both our real-time rings and our longer retention rings liberado has been a",
    "start": "312150",
    "end": "318000"
  },
  {
    "text": "longtime Amazon customer so we were in that camp that you know we never used DBS you know",
    "start": "318000",
    "end": "325800"
  },
  {
    "text": "hearing the horror stories of past nothing in our infrastructure Cassandra or anything else actually ran on EBS at",
    "start": "325800",
    "end": "333720"
  },
  {
    "text": "this point in time so that meant that these nodes were limited to so that one and a half terabyte you can get from an",
    "start": "333720",
    "end": "339810"
  },
  {
    "text": "eye to 2x large if you raid 0 over those drives so some of the challenges from",
    "start": "339810",
    "end": "347100"
  },
  {
    "start": "344000",
    "end": "344000"
  },
  {
    "text": "that you know because we were locked in at the I to 2x large we didn't have flexibility in terms of capacity in",
    "start": "347100",
    "end": "354090"
  },
  {
    "text": "terms of being able to run on larger CPU",
    "start": "354090",
    "end": "359400"
  },
  {
    "text": "core accounts for those real-time rings where we actually had that high workload coming in so we ended up actually",
    "start": "359400",
    "end": "366000"
  },
  {
    "text": "running these rings pretty hot well past sort of that are even a general comfort",
    "start": "366000",
    "end": "371400"
  },
  {
    "text": "level that you should be running cassandra at and like I said because we",
    "start": "371400",
    "end": "377100"
  },
  {
    "text": "were not on EBS that persistent data was tied to that instance so you know when",
    "start": "377100",
    "end": "382139"
  },
  {
    "text": "we lost instances which you know we lost a lot of instances over time it meant",
    "start": "382139",
    "end": "387539"
  },
  {
    "text": "that we were spending hours two days just streaming data back into these rings to replace those nodes and you",
    "start": "387539",
    "end": "394349"
  },
  {
    "text": "never want to run it like a degraded ring state for that long of a time and because we were then",
    "start": "394349",
    "end": "401000"
  },
  {
    "text": "limited at that 1/2 terabytes we had to actually scale those rings out on the longer retention side simply to add disk",
    "start": "401000",
    "end": "408620"
  },
  {
    "text": "capacity we didn't need any more core count performance was great it's it's simply we're running out of disk space",
    "start": "408620",
    "end": "414699"
  },
  {
    "text": "so store just to illustrate you know when you're running a ring a ring at a",
    "start": "414699",
    "end": "420229"
  },
  {
    "start": "416000",
    "end": "416000"
  },
  {
    "text": "60 75 percent CPU level and you actually hit the point where you actually do need",
    "start": "420229",
    "end": "426289"
  },
  {
    "text": "to scale that up if you know Apache Cassandra you know that you know when",
    "start": "426289",
    "end": "431569"
  },
  {
    "text": "you bring a new note in it starts the bootstrapping process the existing nodes split up their tokens stream their data",
    "start": "431569",
    "end": "438740"
  },
  {
    "text": "into these new nodes but they're doing this while they're consuming the existing write load you're pushing into",
    "start": "438740",
    "end": "444919"
  },
  {
    "text": "this ring so they're now doing this double duty to support streaming into the new node but also handing that",
    "start": "444919",
    "end": "450620"
  },
  {
    "text": "existing handling that existing write load so you get to this scenario where you then actually have nodes that then",
    "start": "450620",
    "end": "456680"
  },
  {
    "text": "end up just collapsing they just cannot do this much workload starting at that high base CPU level so we ended up",
    "start": "456680",
    "end": "463279"
  },
  {
    "text": "having to do a lot of scenarios where you know we would try this try some",
    "start": "463279",
    "end": "468650"
  },
  {
    "text": "hacks to get a node ring to scale up or we just stand up an entirely new ring of",
    "start": "468650",
    "end": "474169"
  },
  {
    "text": "a larger size move data over but what this generally meant is that you know we were spending a lot of a lot of money on",
    "start": "474169",
    "end": "481430"
  },
  {
    "start": "480000",
    "end": "480000"
  },
  {
    "text": "the Cassandra part of our infrastructure specifically at last count it was approximately 60% of our total AWS",
    "start": "481430",
    "end": "490370"
  },
  {
    "text": "infrastructure costs coming just directly from these I to 2x larges so",
    "start": "490370",
    "end": "497060"
  },
  {
    "start": "497000",
    "end": "497000"
  },
  {
    "text": "that brings us up to sort of late last year you know one of the things we had recently earlier in the year scaled up",
    "start": "497060",
    "end": "505250"
  },
  {
    "text": "our ops team and saw one of the major directives that we had for the team was",
    "start": "505250",
    "end": "511310"
  },
  {
    "text": "we needed to move off of ec2 classic and on to the Amazon B PC environment it was",
    "start": "511310",
    "end": "517490"
  },
  {
    "text": "clear from all the new services the new instance types that the future was on the V PC environment and the team took",
    "start": "517490",
    "end": "524510"
  },
  {
    "text": "this as an opportunity to sort of do all the infrastructure work that we had put in place there really go to sort",
    "start": "524510",
    "end": "532450"
  },
  {
    "text": "of all the best debris tools of salts that terraform flasks and also we wanted",
    "start": "532450",
    "end": "538180"
  },
  {
    "text": "to use this then as an opportunity to overhaul all the Cassandra work that",
    "start": "538180",
    "end": "543550"
  },
  {
    "text": "we'd done go to the latest releases move to Newark Hardware and I think one of",
    "start": "543550",
    "end": "549519"
  },
  {
    "text": "the things that really kicked this off was last year at this conference we attended the CrowdStrike talk on how",
    "start": "549519",
    "end": "556630"
  },
  {
    "text": "they had moved there Cassandra workload to Amazon EBS saw this talk really you",
    "start": "556630",
    "end": "563529"
  },
  {
    "text": "know they were able to do this successfully got some big wins on that so that was really sort of the spark",
    "start": "563529",
    "end": "570100"
  },
  {
    "text": "that ignited this if you haven't seen that talk highly recommend going to see",
    "start": "570100",
    "end": "575170"
  },
  {
    "text": "that talk coming back watching that I would say large portions of this talk sort of build on that you know",
    "start": "575170",
    "end": "582430"
  },
  {
    "start": "582000",
    "end": "582000"
  },
  {
    "text": "specifically actually when we started off our sort of initial spec that we wanted to hit came from a lot of the",
    "start": "582430",
    "end": "588760"
  },
  {
    "text": "things that CrowdStrike found for us we wanted to go - Cassandra - to latest",
    "start": "588760",
    "end": "594100"
  },
  {
    "text": "port latest release in that line not too comfortable going to Cassandra 3 Oh at",
    "start": "594100",
    "end": "600459"
  },
  {
    "text": "the time because you know we had been successful in Cassandra - so I wanted to",
    "start": "600459",
    "end": "605860"
  },
  {
    "text": "just go to the latest sort of release and that sort of future line moved to the sea for instances though you know",
    "start": "605860",
    "end": "612579"
  },
  {
    "text": "get that additional cpu capacity the GP - obviously we wanted to go to sort of",
    "start": "612579",
    "end": "618910"
  },
  {
    "text": "split configuration data and convince separate partitions and then just the",
    "start": "618910",
    "end": "624339"
  },
  {
    "text": "best to sort of breed OS and system tunings for Cassandra the big one here",
    "start": "624339",
    "end": "630699"
  },
  {
    "text": "for us was also the enhanced networking driver because we were moving to the V PC environment so with that in place you",
    "start": "630699",
    "end": "638920"
  },
  {
    "text": "know we started right around about February of this year on this path on",
    "start": "638920",
    "end": "644680"
  },
  {
    "text": "and one of the things you know we had done some level of performance tuning",
    "start": "644680",
    "end": "650079"
  },
  {
    "start": "646000",
    "end": "646000"
  },
  {
    "text": "and stress testing but what we found in the past is that we get really the best",
    "start": "650079",
    "end": "656529"
  },
  {
    "text": "sort of test capability running our own production workload into a Kassandra ring and actually",
    "start": "656529",
    "end": "662560"
  },
  {
    "text": "seeing how that performs over time as you're doing additional compactions and just other periodic behavior of the the",
    "start": "662560",
    "end": "668980"
  },
  {
    "text": "Cassandra rings so one of the things we've done early in our process we've moved all of our Cassandra read and",
    "start": "668980",
    "end": "675790"
  },
  {
    "text": "write paths through a single tear in our back-end service now so any portion of",
    "start": "675790",
    "end": "681250"
  },
  {
    "text": "our site that needs access Cassandra measurements will actually go through this service to get that data so the",
    "start": "681250",
    "end": "687009"
  },
  {
    "text": "nice thing about that is that we can with a flip of a configuration switch is",
    "start": "687009",
    "end": "692529"
  },
  {
    "text": "if we stand up a new ring that we want to test the new configuration on put that configuration flag in place and",
    "start": "692529",
    "end": "698139"
  },
  {
    "text": "push our entire production workload into this new ring this doesn't impact the downstream clients and we're seeing sort",
    "start": "698139",
    "end": "706600"
  },
  {
    "text": "of exactly what that mirrored workload looks like going into that new that new ring and similarly on the reach side",
    "start": "706600",
    "end": "715240"
  },
  {
    "start": "713000",
    "end": "713000"
  },
  {
    "text": "Tad's different in that you know we only pull data from a single a single ring at a time but we can actually on a per user",
    "start": "715240",
    "end": "722860"
  },
  {
    "text": "basis do a similar process and that we actually flip a configuration switch that pulls data from our new test ring",
    "start": "722860",
    "end": "729730"
  },
  {
    "text": "we can check for correctness performance characteristics etc obviously dog food",
    "start": "729730",
    "end": "735819"
  },
  {
    "text": "that for our own user first scale that up to maybe some friendlies onto some",
    "start": "735819",
    "end": "742089"
  },
  {
    "text": "larger accounts if anything doesn't work right you know just quickly go back and test",
    "start": "742089",
    "end": "748540"
  },
  {
    "text": "there again so so with this in place we sort of started this process off started",
    "start": "748540",
    "end": "754569"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "our duel writing pushing our higher production workload into a new test ring that we had set up and pretty quickly",
    "start": "754569",
    "end": "761740"
  },
  {
    "text": "quickly we noticed this behavior that while the Rings themselves were",
    "start": "761740",
    "end": "767130"
  },
  {
    "text": "performing quite well in terms of you know CPU volumes were low EBS metrics",
    "start": "767130",
    "end": "773680"
  },
  {
    "text": "looked alright we are still seeing these timeouts where individual rights would",
    "start": "773680",
    "end": "780009"
  },
  {
    "text": "just be completely lost after 10 seconds where the sort of p99 for us was under",
    "start": "780009",
    "end": "786220"
  },
  {
    "text": "200 milliseconds and it was just yeah periodic individual rights that would just get lost in this system this right",
    "start": "786220",
    "end": "792910"
  },
  {
    "text": "timeout our right time out in our Cassandra config match this value of ten seconds",
    "start": "792910",
    "end": "799810"
  },
  {
    "text": "so something was just getting lost in the system but spent a lot of time so",
    "start": "799810",
    "end": "806020"
  },
  {
    "text": "we're figuring out what had happened here you know the downside is we sort of",
    "start": "806020",
    "end": "811660"
  },
  {
    "text": "introduced a whole set of variables to sort of the equation here we had upgraded Cassandra we'd gone to new",
    "start": "811660",
    "end": "818380"
  },
  {
    "text": "instance types we had switched the EBS so we sort of did a lot of back-and-forth trying to eliminate some",
    "start": "818380",
    "end": "824500"
  },
  {
    "text": "of those variables but the one thing we kept coming back to was when we took the",
    "start": "824500",
    "end": "829840"
  },
  {
    "text": "Cassandra workload and went back to our original hardware configuration we still",
    "start": "829840",
    "end": "835810"
  },
  {
    "text": "found that we were seeing these these timeouts on the version we had been using so continued a little bit more",
    "start": "835810",
    "end": "843040"
  },
  {
    "text": "went to the latest release and the 2-0 line that we had been on before and",
    "start": "843040",
    "end": "849010"
  },
  {
    "text": "didn't actually see those so at this point we were fairly confident that whatever was going on was something to",
    "start": "849010",
    "end": "855550"
  },
  {
    "text": "do with the Cassandra versions and like I mentioned we had been doing a lot of work with our tooling so we could",
    "start": "855550",
    "end": "860980"
  },
  {
    "text": "actually stand up and tear down these rings pretty quickly so we sort of did this process so we just went along and",
    "start": "860980",
    "end": "868020"
  },
  {
    "text": "bisected the releases between these two points a time till we ended up with to",
    "start": "868020",
    "end": "875290"
  },
  {
    "text": "the point where we ended up where between two hundred four and two and five we were actually noticing this bog come",
    "start": "875290",
    "end": "881200"
  },
  {
    "text": "in the timeouts would appear in two and five and as we back down to two on four",
    "start": "881200",
    "end": "887080"
  },
  {
    "text": "they were they would drop off and we sort of second-guessed sort of a lot of",
    "start": "887080",
    "end": "892540"
  },
  {
    "text": "our configuration at the time but it was pretty clear as we went back and forth that you know when we went down to two",
    "start": "892540",
    "end": "898780"
  },
  {
    "text": "on four you know performance returned to exactly where we would expect it to be so with that in place and understanding",
    "start": "898780",
    "end": "906160"
  },
  {
    "text": "that's something in two and five had broken this we continued that process get bisecting for the merge commits into",
    "start": "906160",
    "end": "913150"
  },
  {
    "text": "that two one five release finally came down to a single merge commit that",
    "start": "913150",
    "end": "918690"
  },
  {
    "text": "introduced these timeouts we went through sort of line by line did a couple PR reviews on that",
    "start": "918690",
    "end": "924889"
  },
  {
    "text": "we found a suspicious time conversion there that didn't use the right factors",
    "start": "924889",
    "end": "931850"
  },
  {
    "text": "for converting times on the timeout values for metrics so patch that filed",
    "start": "931850",
    "end": "938269"
  },
  {
    "text": "this year we're able to sort of move on from that point to sort of this next level one of the one of the things we",
    "start": "938269",
    "end": "946850"
  },
  {
    "start": "941000",
    "end": "941000"
  },
  {
    "text": "did find when we're actually going through this process of that because we had introduced so many new variables one",
    "start": "946850",
    "end": "952879"
  },
  {
    "text": "of the biggest being EBS we spent a lot of time sort of second-guessing the performance that we're getting with EBS",
    "start": "952879",
    "end": "958549"
  },
  {
    "text": "and there were sort of two major factors that came in here if the GP to the db2",
    "start": "958549",
    "end": "967009"
  },
  {
    "text": "volumes were bursting we weren't aware as to if that was actually depleting that balance it was sort of a mystery",
    "start": "967009",
    "end": "974509"
  },
  {
    "text": "quota that we weren't sure if we're depleting that somehow blocking then on the GP to device because those metrics",
    "start": "974509",
    "end": "982489"
  },
  {
    "text": "has been exist for the GP two devices similarly the resolution on EBS kallana",
    "start": "982489",
    "end": "988819"
  },
  {
    "text": "watch metrics 5-minute currently we ended up installing our own agent",
    "start": "988819",
    "end": "995869"
  },
  {
    "text": "setting a interval of 10 seconds on that and actually getting the EBS right data",
    "start": "995869",
    "end": "1001839"
  },
  {
    "text": "from the instances themselves at a 10 second resolution just ensure that you",
    "start": "1001839",
    "end": "1006999"
  },
  {
    "text": "know because we're seeing ten-second timeouts that there wasn't something hidden in those 5-minute averages that",
    "start": "1006999",
    "end": "1012819"
  },
  {
    "text": "potentially we were hitting and causing that blocking so I would recommend that as well as if you are running on an EBS",
    "start": "1012819",
    "end": "1020889"
  },
  {
    "text": "workload that you might potentially want sort of a higher resolution metric there pull those from the instance themselves",
    "start": "1020889",
    "end": "1028438"
  },
  {
    "text": "on the second point here so earlier last",
    "start": "1028439",
    "end": "1035980"
  },
  {
    "start": "1032000",
    "end": "1032000"
  },
  {
    "text": "month that was actually released so the GP to burst balance metric is now available so if you are you know going",
    "start": "1035980",
    "end": "1043808"
  },
  {
    "text": "on to this if you're testing abs you should be able to see that and see if you are depleting that burst balance for",
    "start": "1043809",
    "end": "1049899"
  },
  {
    "text": "your devices so the next sort of",
    "start": "1049899",
    "end": "1055120"
  },
  {
    "text": "struggle that we hit as we were standing up and tearing down these rings",
    "start": "1055120",
    "end": "1060160"
  },
  {
    "text": "not always tearing them down as we've wanted we didn't actually shut down",
    "start": "1060160",
    "end": "1068140"
  },
  {
    "text": "Cassandra cleanly which meant that when we actually was sanded back up it would have to replay a large portion of the",
    "start": "1068140",
    "end": "1074080"
  },
  {
    "text": "commit log and we actually hit bottlenecks on the 200 gigabyte gb2 volume that we had assigned for that we",
    "start": "1074080",
    "end": "1081010"
  },
  {
    "text": "had the 600 set you get match for that you are able to burst over that but we would actually see as these long 15 30",
    "start": "1081010",
    "end": "1088450"
  },
  {
    "text": "even longer startup times from that it was pretty clear that you know the",
    "start": "1088450",
    "end": "1093490"
  },
  {
    "text": "commit log for us was sort of a bottleneck we want to sort continue testing we had moved to a one terabyte",
    "start": "1093490",
    "end": "1099940"
  },
  {
    "text": "volume obviously additional cost there we'd also tried just putting onto the data disk not what we would like you",
    "start": "1099940",
    "end": "1108070"
  },
  {
    "text": "know we prefer to have separated those but it sort of kept us going in this",
    "start": "1108070",
    "end": "1113590"
  },
  {
    "text": "sort of general testing process and one of the things you know it just sort of",
    "start": "1113590",
    "end": "1119080"
  },
  {
    "text": "happened that you know around the same time this Twitter conversation between Jeff and Eric was ongoing and Eric had",
    "start": "1119080",
    "end": "1127150"
  },
  {
    "text": "mentioned that they had had a lot of success with the new throughput optimize",
    "start": "1127150",
    "end": "1132240"
  },
  {
    "text": "EBS volumes and specifically for the commit log volumes and that they were",
    "start": "1132240",
    "end": "1138190"
  },
  {
    "text": "obviously a cheaper performance-wise for the same amount of storage so as we were",
    "start": "1138190",
    "end": "1146500"
  },
  {
    "text": "still on sort of early days of this configuration we actually gave this a shot run out of the gate and it it",
    "start": "1146500",
    "end": "1153250"
  },
  {
    "text": "worked well for us we were actually able to set up move it from a 200 gigabyte GB to volume to a 600 gigabyte St one",
    "start": "1153250",
    "end": "1160540"
  },
  {
    "text": "partition and get that additional capacity to get those additional high",
    "start": "1160540",
    "end": "1165669"
  },
  {
    "text": "ops however still be able to not completely sort of route the cost there",
    "start": "1165669",
    "end": "1171640"
  },
  {
    "text": "because the SD one is just a little under you know 50% of the gp2 costs",
    "start": "1171640",
    "end": "1177730"
  },
  {
    "text": "there and for us kept that commit log separate from the data and so that",
    "start": "1177730",
    "end": "1184870"
  },
  {
    "start": "1183000",
    "end": "1183000"
  },
  {
    "text": "brought us up to our last sort of struggle or we hit on this journey which was as we started to scale up our read",
    "start": "1184870",
    "end": "1192639"
  },
  {
    "text": "we started to see what was in the lungs as small message drops across our",
    "start": "1192639",
    "end": "1198909"
  },
  {
    "text": "Cassander rings it would start slow maybe over a few hours but over the",
    "start": "1198909",
    "end": "1203950"
  },
  {
    "text": "period of a day or two would actually grow to essentially encompass the entire ring all the nodes were timing out",
    "start": "1203950",
    "end": "1209919"
  },
  {
    "text": "dropping messages in between them even though the Rings themselves were completely fine in terms of the capacity",
    "start": "1209919",
    "end": "1216419"
  },
  {
    "text": "CPU is low network volumes were low even even the EBS performance was was fine so",
    "start": "1216419",
    "end": "1223799"
  },
  {
    "text": "we were able to roll the Rings periodically which would sort of fix things for a day but it then slowly",
    "start": "1223799",
    "end": "1230350"
  },
  {
    "text": "start creeping back up again which was obviously not OK for us I think this was",
    "start": "1230350",
    "end": "1237580"
  },
  {
    "text": "now like four to five months into our testing framework into our testings for the period the cost for this was growing",
    "start": "1237580",
    "end": "1245470"
  },
  {
    "text": "and we've been running a lot of dual hardware some of our our eyes were starting to expire on ITU's so it got to",
    "start": "1245470",
    "end": "1253000"
  },
  {
    "text": "the point we were sort of stuck here we needed to really sort of reach out for additional help see if we can sort of",
    "start": "1253000",
    "end": "1260379"
  },
  {
    "text": "get past this stage in our testing framework so I wanted to give we",
    "start": "1260379",
    "end": "1266500"
  },
  {
    "text": "actually called up the last pickle so there are a Cassandra consulting firm they're a really great team if you've if",
    "start": "1266500",
    "end": "1273970"
  },
  {
    "text": "you haven't worked with them they were very understanding of sort of where we're at and our position the sort of",
    "start": "1273970",
    "end": "1279070"
  },
  {
    "text": "urgency to move forward as we got on the phone with them did a sort of quick",
    "start": "1279070",
    "end": "1284110"
  },
  {
    "text": "walkthrough of the code they actually took some of the messages in the logs we were able to identify where sort of that",
    "start": "1284110",
    "end": "1291179"
  },
  {
    "text": "were those drops were coming from and it turned out it was in something called",
    "start": "1291179",
    "end": "1296200"
  },
  {
    "text": "the network coalescing module that had been added to Cassandra previously and",
    "start": "1296200",
    "end": "1301259"
  },
  {
    "text": "what that does is it combines small packets sent over the wire into larger",
    "start": "1301259",
    "end": "1306370"
  },
  {
    "text": "packets typically to sort of reduce Network overhead particularly very high",
    "start": "1306370",
    "end": "1311379"
  },
  {
    "text": "packet volumes so we turn this off there is a ability just to disable this",
    "start": "1311379",
    "end": "1318159"
  },
  {
    "text": "entirely so they suggested we actually try this out across our rings we turn",
    "start": "1318159",
    "end": "1323769"
  },
  {
    "text": "this on and yeah after about two weeks we hadn't seen any of these additional timeouts so great you know we had sort",
    "start": "1323769",
    "end": "1330400"
  },
  {
    "text": "of solved all the remaining problems for us obviously downside of turning off",
    "start": "1330400",
    "end": "1336010"
  },
  {
    "text": "Network Cola saying is that you are spending more on the network throughput but for us we weren't at a volume yet",
    "start": "1336010",
    "end": "1343210"
  },
  {
    "text": "that we were seeing any real change in the performance you know of our read",
    "start": "1343210",
    "end": "1348940"
  },
  {
    "text": "and/or write volumes so for now you know it was now July we had actually then",
    "start": "1348940",
    "end": "1355320"
  },
  {
    "text": "finally gotten our first production ring up shut down the old I to rings that",
    "start": "1355320",
    "end": "1360790"
  },
  {
    "text": "we've been running there and you know just decided that it will live with that sort of network overhead for now",
    "start": "1360790",
    "end": "1367510"
  },
  {
    "text": "you know it's a future item just for come back and take a look at so flash",
    "start": "1367510",
    "end": "1373720"
  },
  {
    "text": "forward to today so where we are actually sitting now we have sort of our",
    "start": "1373720",
    "end": "1379660"
  },
  {
    "text": "two ring types the real-time ring configuration and a longer retention rings not a lot of major differences we",
    "start": "1379660",
    "end": "1389230"
  },
  {
    "text": "do use the C 4 4 s larges on our real-time range where we need that additional CPU capacity for that heavy",
    "start": "1389230",
    "end": "1396160"
  },
  {
    "text": "workload and then on the longer retention rings you know the CPU workload it's not as great but we do",
    "start": "1396160",
    "end": "1403150"
  },
  {
    "text": "want it's somewhat additional memory capacity so we have been using the m4 class machines there and obviously the",
    "start": "1403150",
    "end": "1409750"
  },
  {
    "text": "longer retention rings we've doubled the amount of disk storage that we can run there with so I think one of the biggest",
    "start": "1409750",
    "end": "1418210"
  },
  {
    "start": "1416000",
    "end": "1416000"
  },
  {
    "text": "improvements we saw when we were and we went from running at 60 to 75% CPU",
    "start": "1418210",
    "end": "1424630"
  },
  {
    "text": "volumes down to less than 25% CPU across most of our real-time rings this",
    "start": "1424630",
    "end": "1431260"
  },
  {
    "text": "essentially quitted and also other you know cos standard performance improvements equated to almost half a",
    "start": "1431260",
    "end": "1438550"
  },
  {
    "text": "second latency drop in our p99 right side so significantly better performance",
    "start": "1438550",
    "end": "1444390"
  },
  {
    "text": "right side root side as well this was you know a much better improvement from",
    "start": "1444390",
    "end": "1451450"
  },
  {
    "text": "where were before and from a cost perspective this is actually our if you looked at",
    "start": "1451450",
    "end": "1459690"
  },
  {
    "text": "our real-time ring configuration we're at about a hundred and twenty nodes that are running across our real-time rings",
    "start": "1459690",
    "end": "1465960"
  },
  {
    "text": "we drop that down to just 66 of the C's for 4x larges if you add it in the total",
    "start": "1465960",
    "end": "1473850"
  },
  {
    "text": "cost of the additional ABS EBS volumes there came out to about a 35% savings",
    "start": "1473850",
    "end": "1480590"
  },
  {
    "text": "but this is honestly I said we have a lot of additional capacity we can",
    "start": "1480590",
    "end": "1485789"
  },
  {
    "text": "actually grow into sort of the savings that we have there and hopefully",
    "start": "1485789",
    "end": "1491100"
  },
  {
    "text": "increase or that saving factor over time on the longer retention side not as much",
    "start": "1491100",
    "end": "1500309"
  },
  {
    "text": "of a savings initially out of the gate but we now actually have twice the amount of disk capacity there so once",
    "start": "1500309",
    "end": "1506760"
  },
  {
    "text": "again we totally expect you to grow into these savings and and see a lot more",
    "start": "1506760",
    "end": "1512100"
  },
  {
    "text": "value there as well now one thing also obviously on the real-time configuration",
    "start": "1512100",
    "end": "1518029"
  },
  {
    "text": "excited to take a look at what the sort of C fives will do for this cost factor for us so yeah so that's sort of the",
    "start": "1518029",
    "end": "1527640"
  },
  {
    "text": "journey that we took to get to where we are today and we hand this over to Peter",
    "start": "1527640",
    "end": "1533309"
  },
  {
    "text": "to sort of talk about the new operational story that we have all right okay so yeah so this is the new world",
    "start": "1533309",
    "end": "1541260"
  },
  {
    "text": "right and usually in operations new means bad or at least scary and we had a",
    "start": "1541260",
    "end": "1546899"
  },
  {
    "text": "lot of time to get here but you know while we were doing all of this work we were think about how are we gonna make",
    "start": "1546899",
    "end": "1552149"
  },
  {
    "text": "this so that we can make our lives better and what's interesting is if",
    "start": "1552149",
    "end": "1557399"
  },
  {
    "start": "1555000",
    "end": "1555000"
  },
  {
    "text": "you've ever I hope most of you are here because you use Cassandra you're looking at making your life better and the hard part is when you're normally doing a",
    "start": "1557399",
    "end": "1563669"
  },
  {
    "text": "recovery or you have a failure Cassandra is very slow in the same way that Mike described the scale-up behavior you have",
    "start": "1563669",
    "end": "1570539"
  },
  {
    "text": "the same issues when you're recovering the ring is down it has less capacity and you have to reflow all your data back in it's a lot of work and you can",
    "start": "1570539",
    "end": "1577110"
  },
  {
    "text": "cause a collapse when you thought you were redundant enough what EBS lets you do in B PC get in I and V PC lets you do",
    "start": "1577110",
    "end": "1584280"
  },
  {
    "text": "is it actually lets you pretend that the node didn't go down you can reattach those resources to a new node and we",
    "start": "1584280",
    "end": "1591600"
  },
  {
    "text": "were a little dubious about that but we talked to some people who had done that and we found that it works really really well now it just it acts like it didn't",
    "start": "1591600",
    "end": "1598440"
  },
  {
    "text": "die and so we kind of live this cloud ops dream of like we have no extraordinary application or Cassandra",
    "start": "1598440",
    "end": "1604680"
  },
  {
    "text": "level work to do when we bring a node back up after a failure and so we we are",
    "start": "1604680",
    "end": "1610290"
  },
  {
    "text": "actually reducing our mean time for recovery because we still can't control when a node goes down but our recovery",
    "start": "1610290",
    "end": "1615900"
  },
  {
    "text": "time is so much better that it doesn't cause as much panic or as much worry during the recovery process so the other",
    "start": "1615900",
    "end": "1625080"
  },
  {
    "text": "thing we can take advantage of here is that we you can see we brought up a lot of rings to experiment with when we were doing the git bisect when we had those",
    "start": "1625080",
    "end": "1631830"
  },
  {
    "text": "ring timeouts and failures we kept bringing up software we kept bringing up different rings we took them down to",
    "start": "1631830",
    "end": "1637530"
  },
  {
    "text": "something would delay us for a few days and we bring them back up and reconfigure them and try things out over and over so it's really important to us",
    "start": "1637530",
    "end": "1644040"
  },
  {
    "text": "to be able to do this whenever we need to the more we can automate it the faster and easier it is and the easier",
    "start": "1644040",
    "end": "1649260"
  },
  {
    "text": "it is to store the data without paying for the instance the better it is so we helped Mike's team out with that often",
    "start": "1649260",
    "end": "1655020"
  },
  {
    "text": "and we can bring these up on our own if we need to and it just is much better than having to disrupt an active ring to",
    "start": "1655020",
    "end": "1661350"
  },
  {
    "text": "run an experiment to be able to bring up a new ring try something out and then bring it down whether it's configuration disk instance type etc it's just become",
    "start": "1661350",
    "end": "1669720"
  },
  {
    "text": "a lot better for us now the way we do this the tools we use for it is terraform and i think of what terraform",
    "start": "1669720",
    "end": "1676530"
  },
  {
    "text": "does is being the substrate the substrate is all of the infrastructure that has a lifespan longer than a single",
    "start": "1676530",
    "end": "1682470"
  },
  {
    "text": "instance so for us the primary resources we have here are our EBS volumes the en",
    "start": "1682470",
    "end": "1687810"
  },
  {
    "text": "is and the security groups that go along with a new ring each one gets its own set of those resources and that we're",
    "start": "1687810",
    "end": "1693720"
  },
  {
    "text": "bringing these up with terraform they endure past any failures and we don't have to worry about the state machine of",
    "start": "1693720",
    "end": "1699030"
  },
  {
    "text": "a particular system trying to bring up new ones or not by making decisions they just only do attachments of these",
    "start": "1699030",
    "end": "1704160"
  },
  {
    "text": "existing resources one of the other things that's nice about that is that when we bring these resources down",
    "start": "1704160",
    "end": "1710190"
  },
  {
    "text": "because they've all been created you know purpose created for this one particular ring when we're done with the ring we know that all the resources can",
    "start": "1710190",
    "end": "1716310"
  },
  {
    "text": "be removed again I say that because clearly that would be what you would expect but if",
    "start": "1716310",
    "end": "1722280"
  },
  {
    "text": "anyone here is started a lip you know doing AWS from scratch we have a tendency to attach it's like security",
    "start": "1722280",
    "end": "1727560"
  },
  {
    "text": "groups to multiple things do they seem similar and after a while you're not sure what the purpose is and I think of that as cloud hoarding and this helps us",
    "start": "1727560",
    "end": "1733320"
  },
  {
    "text": "avoid that particular pitfall you can get over in many ways but for us this was a really great clean start when we",
    "start": "1733320",
    "end": "1738450"
  },
  {
    "text": "moved to VPC I'd highly recommend this or something like it to make that work out so I mentioned that terraform deals",
    "start": "1738450",
    "end": "1746580"
  },
  {
    "start": "1742000",
    "end": "1742000"
  },
  {
    "text": "with things that last longer than an instance for all of our instances we launched rings with saltstack saltstack lets us do a lot of automation around",
    "start": "1746580",
    "end": "1752970"
  },
  {
    "text": "that and lets us launch an instance it automates attaching the EBS and Eni because it has all of the things we need",
    "start": "1752970",
    "end": "1758160"
  },
  {
    "text": "to do discovery and configuration to find those resources and bring them together the Salt API also lets us put",
    "start": "1758160",
    "end": "1766050"
  },
  {
    "text": "guard rails into the process and procedures that are very straightforward so even though we do a lot of tricky",
    "start": "1766050",
    "end": "1771720"
  },
  {
    "text": "stuff inside of it we have a really straight path from start to end we launch things the same way we create",
    "start": "1771720",
    "end": "1778860"
  },
  {
    "text": "rings that would be the same way we scale rings the commands we actually run when were act when were in the trenches",
    "start": "1778860",
    "end": "1783990"
  },
  {
    "text": "are the same things we did when we stood it up which is great now how this works",
    "start": "1783990",
    "end": "1790230"
  },
  {
    "start": "1789000",
    "end": "1789000"
  },
  {
    "text": "out for us and it looks simple but what happens for us is that we have an EBS and we have in E and I in there they're",
    "start": "1790230",
    "end": "1796650"
  },
  {
    "text": "durable they don't die when the instance dies and that's part of the configuration that we do a terraform and so when we actually have an instance",
    "start": "1796650",
    "end": "1802830"
  },
  {
    "text": "failure right I mean sometimes you can control when the instance fails right you want to change the node your suspect",
    "start": "1802830",
    "end": "1808980"
  },
  {
    "text": "maybe it's not performing up the party you can control it and you still want to be able to do your shutting down of",
    "start": "1808980",
    "end": "1814320"
  },
  {
    "text": "listeners you want to be able to do a flush and a drain and a stop to make sure you have a controlled shutdown with your data but sometimes it just fails in",
    "start": "1814320",
    "end": "1820710"
  },
  {
    "text": "either case once you get to that point instead of having to do recovery instead",
    "start": "1820710",
    "end": "1825960"
  },
  {
    "text": "of having to do a replace address and wait for this whole thing to go through the very laborious process of recreating",
    "start": "1825960",
    "end": "1832740"
  },
  {
    "text": "that note in the ring what happens is the instances termination becomes the",
    "start": "1832740",
    "end": "1839100"
  },
  {
    "text": "initiation for the EBS and the NI to get disconnected from it and at that point you can just replace the instance the",
    "start": "1839100",
    "end": "1844770"
  },
  {
    "text": "source all configuration really you know only works on the attachments launching you note these resources get",
    "start": "1844770",
    "end": "1850310"
  },
  {
    "text": "attached we run our configuration we tell it okay you're ready we join the ring and it's as though 10 to 15 minutes",
    "start": "1850310",
    "end": "1858380"
  },
  {
    "text": "it's as though the node had just taken a nap and rejoined the ring it had never crashed per se there's no recovery it",
    "start": "1858380",
    "end": "1865640"
  },
  {
    "text": "just starts hints or replayed we do extend our hint time a little bit to a few hours just in case something goes",
    "start": "1865640",
    "end": "1870890"
  },
  {
    "text": "wrong but that gives us plenty of room for recovery and so once it starts it's back to normal now the other aspect of",
    "start": "1870890",
    "end": "1878900"
  },
  {
    "start": "1877000",
    "end": "1877000"
  },
  {
    "text": "dealing with issues is that disaster recovery usually means with Cassandra or the generic way to do with Cassandra is",
    "start": "1878900",
    "end": "1884830"
  },
  {
    "text": "you do SS table snapshots and then you take those to s3 and the biggest problems there are one that's very",
    "start": "1884830",
    "end": "1891410"
  },
  {
    "text": "intensive on the same interface you're using to give customers their data and so you have the lower bar of data that",
    "start": "1891410",
    "end": "1899780"
  },
  {
    "text": "you have for customers and this really high bar with the cpu sending as much data as I can to ask three it's a lot of",
    "start": "1899780",
    "end": "1905210"
  },
  {
    "text": "data and it's a lot of resources that we would rather not have to interfere with our business and also we have a lot of",
    "start": "1905210",
    "end": "1912080"
  },
  {
    "text": "redundant data the larger the SS tables get the less likely they are to get compacted out and the more likely it is your re uploading the same data over and",
    "start": "1912080",
    "end": "1918470"
  },
  {
    "text": "over to a3 when it gets to a3 you then have to run a pruning process that will eliminate the redundancy even that way",
    "start": "1918470",
    "end": "1925340"
  },
  {
    "text": "when you do that you end up with a relatively high s3 bill with EBS snapshots make this a lot better we have",
    "start": "1925340",
    "end": "1931790"
  },
  {
    "text": "our own ops API that ops API lets us initiate snapshots from cron it lets us",
    "start": "1931790",
    "end": "1937130"
  },
  {
    "text": "make sure we know they're done and then it lets us clean them out after a period and the way EBS snapshots work is that does block level differences and so a",
    "start": "1937130",
    "end": "1943520"
  },
  {
    "text": "lot of that manual pruning gets taken care of automatically in a smarter fashion then what we would be doing manually on s3 manually but we will be",
    "start": "1943520",
    "end": "1951230"
  },
  {
    "text": "doing ourselves so all right there we go",
    "start": "1951230",
    "end": "1957860"
  },
  {
    "text": "so didn't yeah that were right sorry so yeah so the other thing that we really",
    "start": "1957860",
    "end": "1962870"
  },
  {
    "text": "like and Mike alluded to this before is that we liberated ourselves from having",
    "start": "1962870",
    "end": "1968240"
  },
  {
    "text": "our storage our memory and our CPU tied to the instance type the i2 instances",
    "start": "1968240",
    "end": "1973850"
  },
  {
    "text": "are really performant in some respects but they don't really they didn't well you have at I 3 is now which are great",
    "start": "1973850",
    "end": "1979520"
  },
  {
    "text": "but they didn't offer a high ceiling to what we wanted to do now what we can do is we can take",
    "start": "1979520",
    "end": "1985440"
  },
  {
    "text": "our storage and we can say okay we anticipate there might be something where we need more CPU and we can roll",
    "start": "1985440",
    "end": "1991080"
  },
  {
    "text": "through and we can change the CPU type we can change the instance type and we can say we need to we need more memory or any more CPU or we just need big and",
    "start": "1991080",
    "end": "1998700"
  },
  {
    "text": "we can do that across an entire ring without having to destroy it or our own choice we can bring up a new ring and",
    "start": "1998700",
    "end": "2005030"
  },
  {
    "text": "flow the data to that and retire the old one it gives us a lot more flexibility but we can do it with the same procedure",
    "start": "2005030",
    "end": "2010280"
  },
  {
    "text": "over and over again we do work that procedure and that procedure has been because of that it's",
    "start": "2010280",
    "end": "2016340"
  },
  {
    "text": "been the same for again for bringing up the instance for bringing up an entire ring for expanding the ring and we",
    "start": "2016340",
    "end": "2022010"
  },
  {
    "text": "practice it over and over and that's super important and after all that we still have some additional challenge and",
    "start": "2022010",
    "end": "2028640"
  },
  {
    "text": "some interesting insights into how cassandra has been doing for us so I'm gonna hand this back to Mike so I just",
    "start": "2028640",
    "end": "2039320"
  },
  {
    "text": "wanted to sort of finish up with some sort of thoughts and ongoing itself",
    "start": "2039320",
    "end": "2045320"
  },
  {
    "text": "we're looking at specifically sort of how big data systems access discs and",
    "start": "2045320",
    "end": "2051679"
  },
  {
    "text": "how that particularly on if you are running on EBS sort of impacts performance there so take a look at kis",
    "start": "2051679",
    "end": "2060710"
  },
  {
    "text": "standard specifically and I think this pattern holds true for similar other systems because Chandra has two",
    "start": "2060710",
    "end": "2068120"
  },
  {
    "text": "different disk access modes the default is the memory map mode so what happens",
    "start": "2068120",
    "end": "2073220"
  },
  {
    "text": "there is Cassandra will take all of your s astable files your indices etc map",
    "start": "2073220",
    "end": "2079310"
  },
  {
    "text": "those into memory and then as the as reads access you know addresses within",
    "start": "2079310",
    "end": "2086270"
  },
  {
    "text": "those files the operating system will fault those pages in performing to read",
    "start": "2086270",
    "end": "2092570"
  },
  {
    "text": "query underneath and does that at a 4k block period at a time the other mode",
    "start": "2092570",
    "end": "2100310"
  },
  {
    "text": "which is standard does the sort of typical boring readwrite system calls to",
    "start": "2100310",
    "end": "2106070"
  },
  {
    "text": "access those files but that's more there for systems where memory map",
    "start": "2106070",
    "end": "2111609"
  },
  {
    "text": "not available so particular windows distributions and other systems so the",
    "start": "2111609",
    "end": "2117970"
  },
  {
    "text": "memory map system works great particularly when you're performing these sort of small random access row",
    "start": "2117970",
    "end": "2124960"
  },
  {
    "text": "reads so if your user query is actually just needs to pull back 3k of data the",
    "start": "2124960",
    "end": "2132640"
  },
  {
    "text": "4k read means you're only doing initial 1k of overhead there you're not pulling in lots of additional data just to serve",
    "start": "2132640",
    "end": "2139480"
  },
  {
    "text": "that very small access similarly performance tuning guides for Cassandra",
    "start": "2139480",
    "end": "2145630"
  },
  {
    "text": "suggests keeping the read ahead small so that you're not also reading in a lot of additional data in front of what you're",
    "start": "2145630",
    "end": "2152079"
  },
  {
    "text": "doing do those sort of small performance row reads now the thing that doesn't fit",
    "start": "2152079",
    "end": "2159190"
  },
  {
    "text": "into this pattern is compaction operations so if you're compacting a",
    "start": "2159190",
    "end": "2164980"
  },
  {
    "text": "hundred gigabyte SS table essentially you are moving through that file in a sequential pattern from beginning to end",
    "start": "2164980",
    "end": "2172619"
  },
  {
    "text": "so what what we sort of see is if you",
    "start": "2172619",
    "end": "2178930"
  },
  {
    "text": "take a look at a ring and what we did here is we set up a three node ring this",
    "start": "2178930",
    "end": "2185529"
  },
  {
    "text": "is consuming a portion of our production right traffic but it's not actually",
    "start": "2185529",
    "end": "2190839"
  },
  {
    "text": "serving an eery traffic so any reads that you're seeing from the EBS volumes",
    "start": "2190839",
    "end": "2196119"
  },
  {
    "text": "are from those compaction operations over time reading in SS tables",
    "start": "2196119",
    "end": "2201519"
  },
  {
    "text": "compacting those down to fewer tables on disk so if you take a look at the metrics here in the bottom you can",
    "start": "2201519",
    "end": "2207999"
  },
  {
    "text": "actually see that the read and write through puts that we're seeing are about the same there's pi speaking spiking to",
    "start": "2207999",
    "end": "2214450"
  },
  {
    "text": "about 10 megabytes per second but it's sort of mirrored between both the read and write side if you take a look at the",
    "start": "2214450",
    "end": "2221499"
  },
  {
    "text": "write ops in the top left there we're spiking to about a hundred I ops",
    "start": "2221499",
    "end": "2226749"
  },
  {
    "text": "per second you know writing out this data but if you then take a look at the",
    "start": "2226749",
    "end": "2233019"
  },
  {
    "text": "read ions they're actually spiking to about 20 X the write offs so we're",
    "start": "2233019",
    "end": "2239380"
  },
  {
    "text": "actually having to do about 2,000 ions on the read side simply to do these large compaction",
    "start": "2239380",
    "end": "2245190"
  },
  {
    "text": "operations overtime visualize another way if you take a look at your average",
    "start": "2245190",
    "end": "2250940"
  },
  {
    "text": "read and write out size on the right side we're doing about averaging about a",
    "start": "2250940",
    "end": "2256710"
  },
  {
    "text": "hundred and twenty K per IAP whereas in the resided we're only hitting about four or five K this maps directly to",
    "start": "2256710",
    "end": "2263880"
  },
  {
    "text": "that four K block size that we're faulting in individually page by page so",
    "start": "2263880",
    "end": "2270060"
  },
  {
    "text": "what does this actually mean well for us this impacts our cost directly on many",
    "start": "2270060",
    "end": "2275160"
  },
  {
    "start": "2271000",
    "end": "2271000"
  },
  {
    "text": "of our rings we've had to size our ABS disks much larger than the actual storage capacity that we need on them",
    "start": "2275160",
    "end": "2281819"
  },
  {
    "text": "because we just need to support this sort of base I upload that you get with",
    "start": "2281819",
    "end": "2286859"
  },
  {
    "text": "the compactions and be able to get that IAP capacity from the higher capacity",
    "start": "2286859",
    "end": "2293369"
  },
  {
    "text": "that you get on GP to volumes and this is because the iopscience from GP to is",
    "start": "2293369",
    "end": "2299579"
  },
  {
    "text": "optimized up to 256 K so if you're operating at a 4k level you're much",
    "start": "2299579",
    "end": "2305400"
  },
  {
    "text": "lower than your not really efficiently using the GP to volume for this so you",
    "start": "2305400",
    "end": "2312480"
  },
  {
    "text": "know what could potentially be done about this um you know we sort of want to propose this idea of sort of a hybrid",
    "start": "2312480",
    "end": "2320040"
  },
  {
    "start": "2316000",
    "end": "2316000"
  },
  {
    "text": "disk access mode so sticking with memory map reads for your row queries so those",
    "start": "2320040",
    "end": "2326250"
  },
  {
    "text": "small random access reads continue to do that mode with memory map that works well for that but when cassandra in this",
    "start": "2326250",
    "end": "2335280"
  },
  {
    "text": "case needs to do a compaction or a scrub operation or any of those sort of large sequential operations over a large file",
    "start": "2335280",
    "end": "2343260"
  },
  {
    "text": "size there's actually then falling back to a standard mode we're just doing this boring readwrite system calls o which",
    "start": "2343260",
    "end": "2350220"
  },
  {
    "text": "would allow us to chunk the read size based on the underlying block block size",
    "start": "2350220",
    "end": "2356700"
  },
  {
    "text": "of the filesystem obviously in this case for a GP 2 would be setting that somewhere up to 256 K so I'd love to say",
    "start": "2356700",
    "end": "2367950"
  },
  {
    "text": "that we have a PR ready to go and ready to merge into the system but what we",
    "start": "2367950",
    "end": "2375420"
  },
  {
    "text": "actually did is we took that same three node sandir ring and we took one note in that",
    "start": "2375420",
    "end": "2380700"
  },
  {
    "text": "ring and set the read ahead to that 256 K so once again it's only doing a",
    "start": "2380700",
    "end": "2385920"
  },
  {
    "text": "compaction operations over time we actually saw was on the top graph here",
    "start": "2385920",
    "end": "2391410"
  },
  {
    "text": "you can see the green line which is our node with the 256 K read ahead is",
    "start": "2391410",
    "end": "2397230"
  },
  {
    "text": "actually only I mean pretty stable somewhere under about 500 ions per second whereas in other nodes are",
    "start": "2397230",
    "end": "2403980"
  },
  {
    "text": "continuing to spike all the way up to 2 K there similarly the bottom graph there the",
    "start": "2403980",
    "end": "2410580"
  },
  {
    "text": "blue line is showing the the new readout size for that new node a bit more spiky",
    "start": "2410580",
    "end": "2416610"
  },
  {
    "text": "there but actually hitting all the way up to about 60 K or so so a lot more a",
    "start": "2416610",
    "end": "2422850"
  },
  {
    "text": "lot more performance there if we aren't using a lot of that disk capacity because our data retentions are much",
    "start": "2422850",
    "end": "2429720"
  },
  {
    "text": "shorter we could actually shrink the size of that GP to volume and cut the cost that were allocating to that a lot",
    "start": "2429720",
    "end": "2438680"
  },
  {
    "text": "so yeah in short in closing here walk through sort of where we started last",
    "start": "2438680",
    "end": "2446130"
  },
  {
    "text": "year with our Cassandra workload on instance store walked through sort of the journey and the struggle sort of we",
    "start": "2446130",
    "end": "2452280"
  },
  {
    "text": "had moving to EBS with Cassandra but now that we're there we sort of Shoji's",
    "start": "2452280",
    "end": "2459060"
  },
  {
    "text": "really like all the benefits sort of operationally big you know big benefits on the operational sort of ease factors",
    "start": "2459060",
    "end": "2465570"
  },
  {
    "text": "and then sort of finished up on some of the concepts improvements that could be",
    "start": "2465570",
    "end": "2471870"
  },
  {
    "text": "seen now that we are actually seeing new metrics running on EBS with Cassandra so",
    "start": "2471870",
    "end": "2477360"
  },
  {
    "text": "I think some takeaways you know we find in this test and other performance",
    "start": "2477360",
    "end": "2483390"
  },
  {
    "text": "issues that we've had with Cassandra is make it very easy to test your identical workload on Cassandra as early as",
    "start": "2483390",
    "end": "2490860"
  },
  {
    "text": "possible you know stress tests are great as well but you really don't see what it will",
    "start": "2490860",
    "end": "2499020"
  },
  {
    "text": "look like until you get that workload on to the rings I think the best you know",
    "start": "2499020",
    "end": "2506310"
  },
  {
    "text": "the nice thing with DBS is it gives you that instance flexibility so if you need more CPU or maybe you need",
    "start": "2506310",
    "end": "2514509"
  },
  {
    "text": "more memory for higher heap sizes you have that flexibility of instances and",
    "start": "2514509",
    "end": "2520240"
  },
  {
    "text": "can move around if needed actually even if that initial decision of instance",
    "start": "2520240",
    "end": "2525490"
  },
  {
    "text": "type wasn't correct for you or you know they announce see flies and you want to",
    "start": "2525490",
    "end": "2530950"
  },
  {
    "text": "move over to those the operational side obviously completely different story",
    "start": "2530950",
    "end": "2537509"
  },
  {
    "text": "MTTR is you know much shorter for us you know we're not staying up all night sort",
    "start": "2537509",
    "end": "2542740"
  },
  {
    "text": "of babysitting a bootstrap operation anymore and you know that's the",
    "start": "2542740",
    "end": "2547869"
  },
  {
    "text": "flexibility or cut the cost for us we have a lot more Headroom to grow into on",
    "start": "2547869",
    "end": "2553150"
  },
  {
    "text": "a lot of these Rea these rings and we'll sort of continue to see the sort of cost improvements for us and a few things we",
    "start": "2553150",
    "end": "2560740"
  },
  {
    "text": "want to look into sort of short-term future definitely sort of start kicking the tires and the Cassander three house",
    "start": "2560740",
    "end": "2566259"
  },
  {
    "text": "stuff they've done some different work with the ESS table modules they're so",
    "start": "2566259",
    "end": "2572109"
  },
  {
    "text": "curious to see how that potentially impacts sort of the disk performance metrics that we see with EBS and clearly",
    "start": "2572109",
    "end": "2580119"
  },
  {
    "text": "we need to figure out why the network of network coalescing for us doesn't work as well cut down on those ec2 transfer",
    "start": "2580119",
    "end": "2587079"
  },
  {
    "text": "costs a bit so yeah I like to thank",
    "start": "2587079",
    "end": "2592299"
  },
  {
    "text": "everybody and you can catch us some Twitter's or if you're on the showroom",
    "start": "2592299",
    "end": "2597579"
  },
  {
    "text": "floor stop by on the SolarWinds myth",
    "start": "2597579",
    "end": "2601950"
  },
  {
    "text": "Thanks",
    "start": "2609300",
    "end": "2612300"
  }
]