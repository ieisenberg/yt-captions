[
  {
    "text": "all right hey folks Emily here uh in",
    "start": "480",
    "end": "2560"
  },
  {
    "text": "this video you are going to learn how to",
    "start": "2560",
    "end": "4319"
  },
  {
    "text": "pre-train your own Foundation models on",
    "start": "4319",
    "end": "6520"
  },
  {
    "text": "stag maker so let's jump in all right so",
    "start": "6520",
    "end": "9800"
  },
  {
    "text": "so what's what's going on here so the",
    "start": "9800",
    "end": "11400"
  },
  {
    "text": "case is You Want To pre-train Your Own",
    "start": "11400",
    "end": "14320"
  },
  {
    "text": "Foundation model now why would you want",
    "start": "14320",
    "end": "16160"
  },
  {
    "text": "to do this now of course uh you might be",
    "start": "16160",
    "end": "18680"
  },
  {
    "text": "an AI startup who's out to build the",
    "start": "18680",
    "end": "21240"
  },
  {
    "text": "next Foundation model uh and so",
    "start": "21240",
    "end": "23119"
  },
  {
    "text": "obviously in that case you're going to",
    "start": "23119",
    "end": "24240"
  },
  {
    "text": "want to build your own um but also when",
    "start": "24240",
    "end": "27160"
  },
  {
    "text": "you pre-train Foundation models um it",
    "start": "27160",
    "end": "29679"
  },
  {
    "text": "gives you the ability to unlock",
    "start": "29679",
    "end": "31640"
  },
  {
    "text": "performance in new areas um so if it's",
    "start": "31640",
    "end": "34360"
  },
  {
    "text": "on new data sets new domains new parts",
    "start": "34360",
    "end": "38160"
  },
  {
    "text": "of knowledge that aren't already",
    "start": "38160",
    "end": "40120"
  },
  {
    "text": "contained in pre-trained Foundation",
    "start": "40120",
    "end": "42280"
  },
  {
    "text": "models uh then in that case uh",
    "start": "42280",
    "end": "44719"
  },
  {
    "text": "pre-training Your Own Foundation model",
    "start": "44719",
    "end": "46960"
  },
  {
    "text": "is a way that you can apply neural Nets",
    "start": "46960",
    "end": "49640"
  },
  {
    "text": "to terabytes of data to train the neural",
    "start": "49640",
    "end": "53120"
  },
  {
    "text": "net on that terabyte of data and then",
    "start": "53120",
    "end": "55440"
  },
  {
    "text": "apply that in uh use cases that are",
    "start": "55440",
    "end": "57879"
  },
  {
    "text": "relevant to you and so that's the",
    "start": "57879",
    "end": "59960"
  },
  {
    "text": "workflow that we're going to understand",
    "start": "59960",
    "end": "61280"
  },
  {
    "text": "in the video here",
    "start": "61280",
    "end": "63119"
  },
  {
    "text": "today uh and so first off on sag maker",
    "start": "63119",
    "end": "66479"
  },
  {
    "text": "uh we provide ephemeral training",
    "start": "66479",
    "end": "68200"
  },
  {
    "text": "clusters that you can use to spin up",
    "start": "68200",
    "end": "70799"
  },
  {
    "text": "these machines and to interact with",
    "start": "70799",
    "end": "73000"
  },
  {
    "text": "these accelerators um when you're",
    "start": "73000",
    "end": "75320"
  },
  {
    "text": "pre-training Your Own Foundation model",
    "start": "75320",
    "end": "77479"
  },
  {
    "text": "this can be anywhere from hundreds to",
    "start": "77479",
    "end": "80400"
  },
  {
    "text": "thousands of accelerators and no one",
    "start": "80400",
    "end": "82720"
  },
  {
    "text": "wants to uh provision and rack and stack",
    "start": "82720",
    "end": "85560"
  },
  {
    "text": "and maintain all of those uh",
    "start": "85560",
    "end": "87479"
  },
  {
    "text": "accelerators and so Cloud uh is a",
    "start": "87479",
    "end": "89600"
  },
  {
    "text": "natural choice for uh training",
    "start": "89600",
    "end": "92159"
  },
  {
    "text": "Foundation models um and then in stag",
    "start": "92159",
    "end": "94600"
  },
  {
    "text": "maker uh we've been investing in",
    "start": "94600",
    "end": "97119"
  },
  {
    "text": "distributed training libraries um GPU",
    "start": "97119",
    "end": "99920"
  },
  {
    "text": "performance and overall uh orchestration",
    "start": "99920",
    "end": "102640"
  },
  {
    "text": "for your cluster for many many years and",
    "start": "102640",
    "end": "105320"
  },
  {
    "text": "so you get to use that uh through an API",
    "start": "105320",
    "end": "108280"
  },
  {
    "text": "call and that's what we're going to",
    "start": "108280",
    "end": "109600"
  },
  {
    "text": "understand here today and so when you're",
    "start": "109600",
    "end": "112119"
  },
  {
    "text": "creating a training cluster in stemer uh",
    "start": "112119",
    "end": "115040"
  },
  {
    "text": "what's happening is essentially you're",
    "start": "115040",
    "end": "116880"
  },
  {
    "text": "creating this training job API call so",
    "start": "116880",
    "end": "120079"
  },
  {
    "text": "you're invoking an API to create a",
    "start": "120079",
    "end": "122920"
  },
  {
    "text": "training job when you happen of course",
    "start": "122920",
    "end": "125119"
  },
  {
    "text": "it's going to hit the sagemaker control",
    "start": "125119",
    "end": "127239"
  },
  {
    "text": "plane and then sag maker is going to",
    "start": "127239",
    "end": "129640"
  },
  {
    "text": "spin up and provision these resources",
    "start": "129640",
    "end": "132360"
  },
  {
    "text": "for you uh so that can be uh CPU based",
    "start": "132360",
    "end": "136239"
  },
  {
    "text": "machines that can be GPU either large or",
    "start": "136239",
    "end": "138800"
  },
  {
    "text": "small um in many different instance",
    "start": "138800",
    "end": "141360"
  },
  {
    "text": "types and sizes you can use custom",
    "start": "141360",
    "end": "143480"
  },
  {
    "text": "accelerators like our tranium instances",
    "start": "143480",
    "end": "145720"
  },
  {
    "text": "which is what we're going to look at uh",
    "start": "145720",
    "end": "147239"
  },
  {
    "text": "here today and then in your stagemaker",
    "start": "147239",
    "end": "149959"
  },
  {
    "text": "training job um essentially we're",
    "start": "149959",
    "end": "152959"
  },
  {
    "text": "tracking all of that cluster for you and",
    "start": "152959",
    "end": "155720"
  },
  {
    "text": "keeping it healthy and align and then",
    "start": "155720",
    "end": "157519"
  },
  {
    "text": "we're going to invoke your script um on",
    "start": "157519",
    "end": "160080"
  },
  {
    "text": "that uh set so your data sets um of",
    "start": "160080",
    "end": "163720"
  },
  {
    "text": "course you can provide those in your S3",
    "start": "163720",
    "end": "165599"
  },
  {
    "text": "buckets um you can also set up an EFS",
    "start": "165599",
    "end": "168519"
  },
  {
    "text": "volume or FSX for luster uh which is an",
    "start": "168519",
    "end": "171319"
  },
  {
    "text": "optimized distributed file system that",
    "start": "171319",
    "end": "174080"
  },
  {
    "text": "actually scales to the number of mounts",
    "start": "174080",
    "end": "176640"
  },
  {
    "text": "so as you increase the number of nodes",
    "start": "176640",
    "end": "179280"
  },
  {
    "text": "that are trying to to read data um FSX",
    "start": "179280",
    "end": "182040"
  },
  {
    "text": "for luster will increase the overall",
    "start": "182040",
    "end": "184360"
  },
  {
    "text": "bandwidth um for that uh volume um in",
    "start": "184360",
    "end": "188239"
  },
  {
    "text": "order to keep the performance high so",
    "start": "188239",
    "end": "190360"
  },
  {
    "text": "you can mount a luster file Vol file",
    "start": "190360",
    "end": "192680"
  },
  {
    "text": "store and then you're going to create",
    "start": "192680",
    "end": "194760"
  },
  {
    "text": "your training image in ECR and then you",
    "start": "194760",
    "end": "197720"
  },
  {
    "text": "can use a base AWS deep learning",
    "start": "197720",
    "end": "200480"
  },
  {
    "text": "container and then just add your",
    "start": "200480",
    "end": "202080"
  },
  {
    "text": "training code on top of that and so",
    "start": "202080",
    "end": "203959"
  },
  {
    "text": "sagemaker downloads uh that training",
    "start": "203959",
    "end": "206480"
  },
  {
    "text": "image for you and then uh we also set up",
    "start": "206480",
    "end": "210519"
  },
  {
    "text": "a uh log stream out to cloudwatch so",
    "start": "210519",
    "end": "213760"
  },
  {
    "text": "essentially once the instances and the",
    "start": "213760",
    "end": "215680"
  },
  {
    "text": "image um have been initialized uh then",
    "start": "215680",
    "end": "218959"
  },
  {
    "text": "we'll start streaming the logs to",
    "start": "218959",
    "end": "220680"
  },
  {
    "text": "cloudwatch and you can view all of the",
    "start": "220680",
    "end": "223120"
  },
  {
    "text": "action as it's happening uh we're store",
    "start": "223120",
    "end": "226480"
  },
  {
    "text": "the metadata and we'll store the",
    "start": "226480",
    "end": "228840"
  },
  {
    "text": "hyperparameters in the sagemaker control",
    "start": "228840",
    "end": "231239"
  },
  {
    "text": "plane both in the console and in studio",
    "start": "231239",
    "end": "234680"
  },
  {
    "text": "uh so that you can view all of the",
    "start": "234680",
    "end": "236400"
  },
  {
    "text": "activity um and then again hold on to",
    "start": "236400",
    "end": "238840"
  },
  {
    "text": "the assets and the artifacts as you're",
    "start": "238840",
    "end": "242319"
  },
  {
    "text": "experimenting once the job is finished",
    "start": "242319",
    "end": "244720"
  },
  {
    "text": "uh we'll send the output model um such",
    "start": "244720",
    "end": "247439"
  },
  {
    "text": "as the model checkpoints um to your",
    "start": "247439",
    "end": "249720"
  },
  {
    "text": "storage volume and then the core",
    "start": "249720",
    "end": "251799"
  },
  {
    "text": "training Loop itself again um really is",
    "start": "251799",
    "end": "255120"
  },
  {
    "text": "is the the job of distributed training",
    "start": "255120",
    "end": "257359"
  },
  {
    "text": "so including U model data parallel um",
    "start": "257359",
    "end": "260759"
  },
  {
    "text": "model parallel and data parallel",
    "start": "260759",
    "end": "262280"
  },
  {
    "text": "techniques uh in order to Shard uh your",
    "start": "262280",
    "end": "264960"
  },
  {
    "text": "model over all of the accelerators as",
    "start": "264960",
    "end": "266960"
  },
  {
    "text": "necessary and then keep the distributed",
    "start": "266960",
    "end": "269360"
  },
  {
    "text": "uh GR gradient Ascent running",
    "start": "269360",
    "end": "272479"
  },
  {
    "text": "online and of course you can do this",
    "start": "272479",
    "end": "274639"
  },
  {
    "text": "with your own deep learning models and",
    "start": "274639",
    "end": "276560"
  },
  {
    "text": "so in this view um we're using the sage",
    "start": "276560",
    "end": "279880"
  },
  {
    "text": "maker uh python SDK and then this pych",
    "start": "279880",
    "end": "283160"
  },
  {
    "text": "object here again is a object that's",
    "start": "283160",
    "end": "286199"
  },
  {
    "text": "pointing to the it so it's a wrap around",
    "start": "286199",
    "end": "289759"
  },
  {
    "text": "the training job API so you're calling",
    "start": "289759",
    "end": "292600"
  },
  {
    "text": "pytorch and then estimator fit and",
    "start": "292600",
    "end": "295160"
  },
  {
    "text": "that's invoking that training job API",
    "start": "295160",
    "end": "298360"
  },
  {
    "text": "it's also pointing to the Deep learning",
    "start": "298360",
    "end": "300759"
  },
  {
    "text": "container so this is pointing to the",
    "start": "300759",
    "end": "302880"
  },
  {
    "text": "base pie torch container for sag maker",
    "start": "302880",
    "end": "306479"
  },
  {
    "text": "that AWS keeps alive for you and and",
    "start": "306479",
    "end": "309680"
  },
  {
    "text": "updates um so as there are major changes",
    "start": "309680",
    "end": "312800"
  },
  {
    "text": "to P torch the framework uh we'll update",
    "start": "312800",
    "end": "315199"
  },
  {
    "text": "our base containers you can use those",
    "start": "315199",
    "end": "318240"
  },
  {
    "text": "and then uh you'll point to the",
    "start": "318240",
    "end": "320280"
  },
  {
    "text": "framework version you want to use the py",
    "start": "320280",
    "end": "322919"
  },
  {
    "text": "the python version uh and then you'll",
    "start": "322919",
    "end": "325520"
  },
  {
    "text": "Define the instance right here so the",
    "start": "325520",
    "end": "327440"
  },
  {
    "text": "number of machines you want to use the",
    "start": "327440",
    "end": "328960"
  },
  {
    "text": "type of those bring all of your own",
    "start": "328960",
    "end": "331000"
  },
  {
    "text": "custom hyperparameters right here bring",
    "start": "331000",
    "end": "333000"
  },
  {
    "text": "your metric",
    "start": "333000",
    "end": "334080"
  },
  {
    "text": "definitions um and then again you're",
    "start": "334080",
    "end": "336479"
  },
  {
    "text": "going to call model.fit and then uh we",
    "start": "336479",
    "end": "339800"
  },
  {
    "text": "have many different modes for passing",
    "start": "339800",
    "end": "341759"
  },
  {
    "text": "data to that training cluster um one",
    "start": "341759",
    "end": "344759"
  },
  {
    "text": "easy way to get started is an S3 path uh",
    "start": "344759",
    "end": "347520"
  },
  {
    "text": "so if you have a smaller data set um",
    "start": "347520",
    "end": "350319"
  },
  {
    "text": "then you can just load that from S3",
    "start": "350319",
    "end": "352680"
  },
  {
    "text": "directly and so this mode um the default",
    "start": "352680",
    "end": "355960"
  },
  {
    "text": "is just going to copy that file or that",
    "start": "355960",
    "end": "358880"
  },
  {
    "text": "you know directory onto your training",
    "start": "358880",
    "end": "360880"
  },
  {
    "text": "job and then start to copy after that",
    "start": "360880",
    "end": "362720"
  },
  {
    "text": "start to train uh once you've copied it",
    "start": "362720",
    "end": "365080"
  },
  {
    "text": "so really not not too much data um and",
    "start": "365080",
    "end": "368599"
  },
  {
    "text": "then once you do larger sets um then you",
    "start": "368599",
    "end": "371440"
  },
  {
    "text": "can start to stream using the fast file",
    "start": "371440",
    "end": "373960"
  },
  {
    "text": "mode uh so that's an option and then",
    "start": "373960",
    "end": "376680"
  },
  {
    "text": "again if you're working with terabytes",
    "start": "376680",
    "end": "378080"
  },
  {
    "text": "then just FSX for luster is is a really",
    "start": "378080",
    "end": "380520"
  },
  {
    "text": "stable",
    "start": "380520",
    "end": "381919"
  },
  {
    "text": "option so now let's take a look at a",
    "start": "381919",
    "end": "384199"
  },
  {
    "text": "demo so in this demo you are going to",
    "start": "384199",
    "end": "386520"
  },
  {
    "text": "pre-train your own 70 billion parameter",
    "start": "386520",
    "end": "389199"
  },
  {
    "text": "l to model on sag maker we're going to",
    "start": "389199",
    "end": "391720"
  },
  {
    "text": "use trinium machines uh and so we'll",
    "start": "391720",
    "end": "394560"
  },
  {
    "text": "we'll open up a notebook in sagemaker",
    "start": "394560",
    "end": "396440"
  },
  {
    "text": "studio uh we're going to use warm pools",
    "start": "396440",
    "end": "398880"
  },
  {
    "text": "for sag maker training so you can reuse",
    "start": "398880",
    "end": "401759"
  },
  {
    "text": "um those machines and that helps us",
    "start": "401759",
    "end": "403960"
  },
  {
    "text": "write our training scripts then we'll",
    "start": "403960",
    "end": "405960"
  },
  {
    "text": "interact with the distributed training",
    "start": "405960",
    "end": "407520"
  },
  {
    "text": "libraries to execute the training Loop",
    "start": "407520",
    "end": "410520"
  },
  {
    "text": "uh and then ultimately profile and",
    "start": "410520",
    "end": "412520"
  },
  {
    "text": "troubleshoot the job so let's take a",
    "start": "412520",
    "end": "415360"
  },
  {
    "text": "look okay so now we're going to take a",
    "start": "415360",
    "end": "417680"
  },
  {
    "text": "look at a demo of pre-training our own",
    "start": "417680",
    "end": "420199"
  },
  {
    "text": "llama 270b and so here we are in Sag",
    "start": "420199",
    "end": "423599"
  },
  {
    "text": "maker studio and so again within",
    "start": "423599",
    "end": "426000"
  },
  {
    "text": "sagemaker studio we're going to open up",
    "start": "426000",
    "end": "427919"
  },
  {
    "text": "Jupiter lab um one thing to note you can",
    "start": "427919",
    "end": "431000"
  },
  {
    "text": "add of course multiple spaces here in",
    "start": "431000",
    "end": "434080"
  },
  {
    "text": "Jupiter lab and then with each of these",
    "start": "434080",
    "end": "436520"
  },
  {
    "text": "spaces um this is where you change the",
    "start": "436520",
    "end": "439080"
  },
  {
    "text": "instance type so you just click on the",
    "start": "439080",
    "end": "441000"
  },
  {
    "text": "name of the space and then you can stop",
    "start": "441000",
    "end": "444160"
  },
  {
    "text": "it uh you can open Jupiter lab here and",
    "start": "444160",
    "end": "447039"
  },
  {
    "text": "then once it's stopped um you can set",
    "start": "447039",
    "end": "449840"
  },
  {
    "text": "different instances so if you want to",
    "start": "449840",
    "end": "452160"
  },
  {
    "text": "just run on a GPU or run on an",
    "start": "452160",
    "end": "455199"
  },
  {
    "text": "accelerator or maybe even a multi-gpu",
    "start": "455199",
    "end": "457360"
  },
  {
    "text": "machine uh you can do that right here",
    "start": "457360",
    "end": "460240"
  },
  {
    "text": "fortunately uh I already have it open so",
    "start": "460240",
    "end": "462360"
  },
  {
    "text": "we'll we'll open that uh and then the",
    "start": "462360",
    "end": "465159"
  },
  {
    "text": "script we're going to use is available",
    "start": "465159",
    "end": "468039"
  },
  {
    "text": "publicly so this is the example notebook",
    "start": "468039",
    "end": "470879"
  },
  {
    "text": "that we're using so again you're",
    "start": "470879",
    "end": "472720"
  },
  {
    "text": "training llama 2 70b um this is in the",
    "start": "472720",
    "end": "476120"
  },
  {
    "text": "SAG maker trinium examples so feel free",
    "start": "476120",
    "end": "478879"
  },
  {
    "text": "to to follow along with",
    "start": "478879",
    "end": "482639"
  },
  {
    "text": "me all right uh so we're going to pip",
    "start": "482639",
    "end": "485159"
  },
  {
    "text": "install boto 3 so that happens here and",
    "start": "485159",
    "end": "488000"
  },
  {
    "text": "then we're going to install the",
    "start": "488000",
    "end": "489599"
  },
  {
    "text": "Transformers uh SDK from hugging face",
    "start": "489599",
    "end": "492639"
  },
  {
    "text": "the data sets uh and then the S3",
    "start": "492639",
    "end": "495479"
  },
  {
    "text": "integration path for that so uh this",
    "start": "495479",
    "end": "499039"
  },
  {
    "text": "notebook uh is going to download the",
    "start": "499039",
    "end": "501000"
  },
  {
    "text": "wiki Corpus and the model artifact is",
    "start": "501000",
    "end": "504440"
  },
  {
    "text": "llama 2 and when you download that from",
    "start": "504440",
    "end": "506879"
  },
  {
    "text": "hugging face directly you need to pass",
    "start": "506879",
    "end": "508639"
  },
  {
    "text": "in your",
    "start": "508639",
    "end": "510039"
  },
  {
    "text": "um token and so I um obviously hid that",
    "start": "510039",
    "end": "513560"
  },
  {
    "text": "here um but uh note that you'll you'll",
    "start": "513560",
    "end": "516039"
  },
  {
    "text": "need your Huggy face token here so we're",
    "start": "516039",
    "end": "519039"
  },
  {
    "text": "going to import the data sets library",
    "start": "519039",
    "end": "520839"
  },
  {
    "text": "from hugging face and then uh we're",
    "start": "520839",
    "end": "524200"
  },
  {
    "text": "pointing to this Wiki Corpus data",
    "start": "524200",
    "end": "528160"
  },
  {
    "text": "set and so we have the wiki Corpus data",
    "start": "528160",
    "end": "530920"
  },
  {
    "text": "set and then uh remember the tokenizer",
    "start": "530920",
    "end": "534279"
  },
  {
    "text": "comes with each model so every model has",
    "start": "534279",
    "end": "537800"
  },
  {
    "text": "its own tokenizer and so we need need",
    "start": "537800",
    "end": "539600"
  },
  {
    "text": "the tokenizer uh for llama 270b and",
    "start": "539600",
    "end": "542920"
  },
  {
    "text": "that's right",
    "start": "542920",
    "end": "544360"
  },
  {
    "text": "here and then uh we load the data",
    "start": "544360",
    "end": "547720"
  },
  {
    "text": "sets and we load the",
    "start": "547720",
    "end": "551279"
  },
  {
    "text": "tokenizer and then that happens here so",
    "start": "551279",
    "end": "554360"
  },
  {
    "text": "we've got those loaded and then next um",
    "start": "554360",
    "end": "558600"
  },
  {
    "text": "again this notebook actually is going to",
    "start": "558600",
    "end": "560480"
  },
  {
    "text": "tokenize this data set locally um so we",
    "start": "560480",
    "end": "564279"
  },
  {
    "text": "we're going to download this data set",
    "start": "564279",
    "end": "566399"
  },
  {
    "text": "and then tokenize it on this machine um",
    "start": "566399",
    "end": "569399"
  },
  {
    "text": "which is why actually I picked a",
    "start": "569399",
    "end": "571640"
  },
  {
    "text": "C5 um I think I picked a",
    "start": "571640",
    "end": "575160"
  },
  {
    "text": "c518 c512 yeah so a c512 u which gives",
    "start": "575160",
    "end": "579839"
  },
  {
    "text": "you a little bit more compute power um",
    "start": "579839",
    "end": "582160"
  },
  {
    "text": "so you can tokenize this uh rapidly when",
    "start": "582160",
    "end": "585519"
  },
  {
    "text": "you're working with a larger data set um",
    "start": "585519",
    "end": "588279"
  },
  {
    "text": "then you could add um actually you could",
    "start": "588279",
    "end": "590839"
  },
  {
    "text": "add the sagemaker remote decorator to",
    "start": "590839",
    "end": "593760"
  },
  {
    "text": "take this block of code and actually run",
    "start": "593760",
    "end": "596760"
  },
  {
    "text": "that on a dedicated machine so the",
    "start": "596760",
    "end": "599360"
  },
  {
    "text": "remote decorator um is a way to wrap a",
    "start": "599360",
    "end": "601920"
  },
  {
    "text": "single function and then actually spin",
    "start": "601920",
    "end": "604279"
  },
  {
    "text": "that up on its own host uh which is a",
    "start": "604279",
    "end": "607160"
  },
  {
    "text": "nice way for tokenizing data sets so",
    "start": "607160",
    "end": "610000"
  },
  {
    "text": "that's that's something you could do",
    "start": "610000",
    "end": "611079"
  },
  {
    "text": "right",
    "start": "611079",
    "end": "611839"
  },
  {
    "text": "there uh all right and so this is",
    "start": "611839",
    "end": "614279"
  },
  {
    "text": "grouping the texts and then tokenizing",
    "start": "614279",
    "end": "617279"
  },
  {
    "text": "them all right then again I did this",
    "start": "617279",
    "end": "619839"
  },
  {
    "text": "ahead of",
    "start": "619839",
    "end": "621160"
  },
  {
    "text": "time all right and then uh I'm going to",
    "start": "621160",
    "end": "623839"
  },
  {
    "text": "point to my uh session bucket so this",
    "start": "623839",
    "end": "626279"
  },
  {
    "text": "one is of course llama on sag maker and",
    "start": "626279",
    "end": "629399"
  },
  {
    "text": "and",
    "start": "629399",
    "end": "631240"
  },
  {
    "text": "then so in the notebook again we'll",
    "start": "631240",
    "end": "634040"
  },
  {
    "text": "we'll do the tokenizing locally and then",
    "start": "634040",
    "end": "636320"
  },
  {
    "text": "we just pass it up to the S3 bucket and",
    "start": "636320",
    "end": "639560"
  },
  {
    "text": "so that is here uh it says save to dis",
    "start": "639560",
    "end": "643440"
  },
  {
    "text": "but it's actually saving directly uh to",
    "start": "643440",
    "end": "645639"
  },
  {
    "text": "the S3 bucket which is Handy all right",
    "start": "645639",
    "end": "648880"
  },
  {
    "text": "so that is now in my S3",
    "start": "648880",
    "end": "651560"
  },
  {
    "text": "bucket after this uh I'm going to run",
    "start": "651560",
    "end": "654560"
  },
  {
    "text": "this training job using again train 1 n",
    "start": "654560",
    "end": "658160"
  },
  {
    "text": "uh so this is custom accelerators from",
    "start": "658160",
    "end": "660120"
  },
  {
    "text": "Amazon um and we are going to use eight",
    "start": "660120",
    "end": "663480"
  },
  {
    "text": "of these machines so eight overall nodes",
    "start": "663480",
    "end": "666680"
  },
  {
    "text": "and then each node um has 32 processes",
    "start": "666680",
    "end": "671839"
  },
  {
    "text": "per processes per node couple of the",
    "start": "671839",
    "end": "675800"
  },
  {
    "text": "other hyperparameters here so the global",
    "start": "675800",
    "end": "678040"
  },
  {
    "text": "batch size that is the total batch um so",
    "start": "678040",
    "end": "682320"
  },
  {
    "text": "the total number of Records we're going",
    "start": "682320",
    "end": "683720"
  },
  {
    "text": "to hold in memory for this entire eight",
    "start": "683720",
    "end": "686120"
  },
  {
    "text": "node cluster and then the sequence",
    "start": "686120",
    "end": "688720"
  },
  {
    "text": "length um which we essentially take from",
    "start": "688720",
    "end": "690920"
  },
  {
    "text": "the model so that's",
    "start": "690920",
    "end": "692480"
  },
  {
    "text": "4096 and then your pipeline parallel",
    "start": "692480",
    "end": "695200"
  },
  {
    "text": "degree so the pipeline parallel degree",
    "start": "695200",
    "end": "697839"
  },
  {
    "text": "um essentially is the way that uh we're",
    "start": "697839",
    "end": "702279"
  },
  {
    "text": "going to split the different layers in",
    "start": "702279",
    "end": "704440"
  },
  {
    "text": "the neural network so your Lama 2 model",
    "start": "704440",
    "end": "706800"
  },
  {
    "text": "of course is a decoder based Transformer",
    "start": "706800",
    "end": "709240"
  },
  {
    "text": "model each Transformer head can be quite",
    "start": "709240",
    "end": "712160"
  },
  {
    "text": "large and so you need to place the",
    "start": "712160",
    "end": "714440"
  },
  {
    "text": "different blocks in your neural network",
    "start": "714440",
    "end": "716680"
  },
  {
    "text": "on different accelerators and so that's",
    "start": "716680",
    "end": "719040"
  },
  {
    "text": "the pipeline parallel degree indicates",
    "start": "719040",
    "end": "721760"
  },
  {
    "text": "is essentially how we're uh taking the",
    "start": "721760",
    "end": "725720"
  },
  {
    "text": "heads in the Llama model and then",
    "start": "725720",
    "end": "729120"
  },
  {
    "text": "placing different layers on different",
    "start": "729120",
    "end": "732079"
  },
  {
    "text": "accelerators the tensor parallel degree",
    "start": "732079",
    "end": "734839"
  },
  {
    "text": "is is another piece here so tensor",
    "start": "734839",
    "end": "736839"
  },
  {
    "text": "parallelism is where you're going to",
    "start": "736839",
    "end": "738800"
  },
  {
    "text": "take one block or one uh attention head",
    "start": "738800",
    "end": "742399"
  },
  {
    "text": "and that um head might be just so large",
    "start": "742399",
    "end": "746000"
  },
  {
    "text": "that it doesn't even fit on a single",
    "start": "746000",
    "end": "748240"
  },
  {
    "text": "accelerator by itself self and so the",
    "start": "748240",
    "end": "750199"
  },
  {
    "text": "tensor parallel degree is still um",
    "start": "750199",
    "end": "753600"
  },
  {
    "text": "placing that object on accelerators but",
    "start": "753600",
    "end": "756440"
  },
  {
    "text": "here we're placing one head on the eight",
    "start": "756440",
    "end": "759120"
  },
  {
    "text": "accelerators so just one attention block",
    "start": "759120",
    "end": "761720"
  },
  {
    "text": "over all of those eight uh cores and",
    "start": "761720",
    "end": "765360"
  },
  {
    "text": "then the data parallel size is the total",
    "start": "765360",
    "end": "767680"
  },
  {
    "text": "number of um accelerators that you need",
    "start": "767680",
    "end": "771639"
  },
  {
    "text": "to host uh essentially a version of the",
    "start": "771639",
    "end": "774399"
  },
  {
    "text": "model and so that's that's how large",
    "start": "774399",
    "end": "777160"
  },
  {
    "text": "your data parallel size that's that's",
    "start": "777160",
    "end": "779320"
  },
  {
    "text": "your data parallel size okay and then we",
    "start": "779320",
    "end": "781240"
  },
  {
    "text": "have the batch size uh per model replica",
    "start": "781240",
    "end": "784320"
  },
  {
    "text": "and so that is uh the uh so we have the",
    "start": "784320",
    "end": "788680"
  },
  {
    "text": "global batch size which again we saw at",
    "start": "788680",
    "end": "791120"
  },
  {
    "text": "the top and then um the batch size per",
    "start": "791120",
    "end": "794440"
  },
  {
    "text": "model replica essentially that is for",
    "start": "794440",
    "end": "797360"
  },
  {
    "text": "each of those uh model partitions um",
    "start": "797360",
    "end": "801399"
  },
  {
    "text": "that's how many like objects from the",
    "start": "801399",
    "end": "803680"
  },
  {
    "text": "data set they are going to have so batch",
    "start": "803680",
    "end": "805680"
  },
  {
    "text": "size per model",
    "start": "805680",
    "end": "807560"
  },
  {
    "text": "replica and that is is your your micro",
    "start": "807560",
    "end": "811240"
  },
  {
    "text": "bouches Aron thank you for the notebook",
    "start": "811240",
    "end": "813480"
  },
  {
    "text": "love it let's keep going okay uh so",
    "start": "813480",
    "end": "816839"
  },
  {
    "text": "we're going to set more hyper parameters",
    "start": "816839",
    "end": "818639"
  },
  {
    "text": "here so your train batch size training",
    "start": "818639",
    "end": "821680"
  },
  {
    "text": "directory maximum steps sequence length",
    "start": "821680",
    "end": "825079"
  },
  {
    "text": "and then all of these were just passing",
    "start": "825079",
    "end": "826600"
  },
  {
    "text": "to this hyper parameters object you can",
    "start": "826600",
    "end": "829120"
  },
  {
    "text": "pass anything you want to the hyper",
    "start": "829120",
    "end": "831480"
  },
  {
    "text": "parameters object What's Happening Here",
    "start": "831480",
    "end": "833800"
  },
  {
    "text": "is that in the training script that",
    "start": "833800",
    "end": "835560"
  },
  {
    "text": "you're going to use to invoke this um",
    "start": "835560",
    "end": "838440"
  },
  {
    "text": "you want to build build in arguments",
    "start": "838440",
    "end": "840800"
  },
  {
    "text": "basically that capture this so when you",
    "start": "840800",
    "end": "842680"
  },
  {
    "text": "add a new hyper parameter here um just",
    "start": "842680",
    "end": "845959"
  },
  {
    "text": "add it as an argument to that training",
    "start": "845959",
    "end": "848519"
  },
  {
    "text": "script that will use to invoke your your",
    "start": "848519",
    "end": "851680"
  },
  {
    "text": "job um and then uh you'll be able to use",
    "start": "851680",
    "end": "855199"
  },
  {
    "text": "those from inside the script and we're",
    "start": "855199",
    "end": "856320"
  },
  {
    "text": "going to take a look at that in this",
    "start": "856320",
    "end": "857680"
  },
  {
    "text": "demo",
    "start": "857680",
    "end": "858959"
  },
  {
    "text": "here all right and then um so opml",
    "start": "858959",
    "end": "863639"
  },
  {
    "text": "refers to a piece inside of the training",
    "start": "863639",
    "end": "866959"
  },
  {
    "text": "container so that's uh within the",
    "start": "866959",
    "end": "869440"
  },
  {
    "text": "training image um most of the program uh",
    "start": "869440",
    "end": "873399"
  },
  {
    "text": "will run inside of opml so that's",
    "start": "873399",
    "end": "876279"
  },
  {
    "text": "here and then uh this is the again uh",
    "start": "876279",
    "end": "879759"
  },
  {
    "text": "fully managed deep learning container so",
    "start": "879759",
    "end": "881839"
  },
  {
    "text": "it's the AWS",
    "start": "881839",
    "end": "883279"
  },
  {
    "text": "DLC and this is a global DLC so you're",
    "start": "883279",
    "end": "887079"
  },
  {
    "text": "going to point to that account um",
    "start": "887079",
    "end": "890320"
  },
  {
    "text": "everywhere in the world and then just",
    "start": "890320",
    "end": "891959"
  },
  {
    "text": "change out your different regions and",
    "start": "891959",
    "end": "894639"
  },
  {
    "text": "change out the um image that you want to",
    "start": "894639",
    "end": "898279"
  },
  {
    "text": "use so let's just take a look at",
    "start": "898279",
    "end": "902519"
  },
  {
    "text": "this deep learning",
    "start": "903680",
    "end": "907079"
  },
  {
    "text": "container yeah there we",
    "start": "907079",
    "end": "909160"
  },
  {
    "text": "go so these are the Deep learning",
    "start": "909160",
    "end": "913560"
  },
  {
    "text": "containers and so again they're updated",
    "start": "913560",
    "end": "916680"
  },
  {
    "text": "very",
    "start": "916680",
    "end": "917880"
  },
  {
    "text": "regularly and you can view some of the",
    "start": "917880",
    "end": "920560"
  },
  {
    "text": "available images and the sagemaker",
    "start": "920560",
    "end": "922880"
  },
  {
    "text": "images right here oops clicked on the",
    "start": "922880",
    "end": "926240"
  },
  {
    "text": "wrong button there",
    "start": "926240",
    "end": "929839"
  },
  {
    "text": "there we",
    "start": "929880",
    "end": "932240"
  },
  {
    "text": "go great so these are all the available",
    "start": "933399",
    "end": "936199"
  },
  {
    "text": "deep learning containers uh and then the",
    "start": "936199",
    "end": "938639"
  },
  {
    "text": "sage maker ones are down below uh",
    "start": "938639",
    "end": "941279"
  },
  {
    "text": "alternatively you have the ones for um",
    "start": "941279",
    "end": "943360"
  },
  {
    "text": "sort of ec2 eks ECS and then essentially",
    "start": "943360",
    "end": "947560"
  },
  {
    "text": "you need to pick the right image based",
    "start": "947560",
    "end": "950480"
  },
  {
    "text": "on what framework you're using so pie",
    "start": "950480",
    "end": "952519"
  },
  {
    "text": "torch tensor flow hugging face and then",
    "start": "952519",
    "end": "956720"
  },
  {
    "text": "um if you're doing training or inference",
    "start": "956720",
    "end": "959759"
  },
  {
    "text": "and then if you're on a CPU GPU or a",
    "start": "959759",
    "end": "962480"
  },
  {
    "text": "trinium um or an inferentia neuron core",
    "start": "962480",
    "end": "966199"
  },
  {
    "text": "and then uh this is the example URL so",
    "start": "966199",
    "end": "969240"
  },
  {
    "text": "that's your um image and then you just",
    "start": "969240",
    "end": "973399"
  },
  {
    "text": "update this based on whichever region uh",
    "start": "973399",
    "end": "976519"
  },
  {
    "text": "you are running this in and so that is",
    "start": "976519",
    "end": "979240"
  },
  {
    "text": "exactly this basically it's the pytorch",
    "start": "979240",
    "end": "982959"
  },
  {
    "text": "training image for neuron and then it's",
    "start": "982959",
    "end": "987519"
  },
  {
    "text": "13 all right and so we'll pass in a job",
    "start": "987519",
    "end": "990680"
  },
  {
    "text": "name",
    "start": "990680",
    "end": "991800"
  },
  {
    "text": "here and so the checkpoints um when you",
    "start": "991800",
    "end": "995800"
  },
  {
    "text": "are training on an on an S3 bucket with",
    "start": "995800",
    "end": "998319"
  },
  {
    "text": "data coming out of S3 then you can write",
    "start": "998319",
    "end": "1000440"
  },
  {
    "text": "your checkpoints to S3 directly if",
    "start": "1000440",
    "end": "1003160"
  },
  {
    "text": "you're using FSX for luster then um",
    "start": "1003160",
    "end": "1006800"
  },
  {
    "text": "actually you're going to write the",
    "start": "1006800",
    "end": "1007839"
  },
  {
    "text": "checkpoints to Lester and then Lester",
    "start": "1007839",
    "end": "1010199"
  },
  {
    "text": "will replicate that out to S3 after",
    "start": "1010199",
    "end": "1012680"
  },
  {
    "text": "you're",
    "start": "1012680",
    "end": "1014199"
  },
  {
    "text": "done okay so we'll point to the",
    "start": "1014199",
    "end": "1017360"
  },
  {
    "text": "checkpoints in S3 so again we're going",
    "start": "1017360",
    "end": "1020240"
  },
  {
    "text": "to write the checkpoints uh during the",
    "start": "1020240",
    "end": "1022720"
  },
  {
    "text": "training job and then those will show in",
    "start": "1022720",
    "end": "1024918"
  },
  {
    "text": "S3 on",
    "start": "1024919",
    "end": "1026400"
  },
  {
    "text": "completion many environment variables to",
    "start": "1026400",
    "end": "1029319"
  },
  {
    "text": "set",
    "start": "1029319",
    "end": "1030240"
  },
  {
    "text": "here so",
    "start": "1030240",
    "end": "1032240"
  },
  {
    "text": "EFA uh nickel neuron Malik",
    "start": "1032240",
    "end": "1038280"
  },
  {
    "text": "xla all sorts of environment variables",
    "start": "1038280",
    "end": "1040839"
  },
  {
    "text": "here uh and then here are quite a few",
    "start": "1040839",
    "end": "1042959"
  },
  {
    "text": "Flags to look at so this this is telling",
    "start": "1042959",
    "end": "1045400"
  },
  {
    "text": "neuron the type of model you're using",
    "start": "1045400",
    "end": "1047760"
  },
  {
    "text": "which is a Transformer your distribution",
    "start": "1047760",
    "end": "1050200"
  },
  {
    "text": "strategy this is llm training um and",
    "start": "1050200",
    "end": "1053760"
  },
  {
    "text": "then um enabling this Infinity",
    "start": "1053760",
    "end": "1056520"
  },
  {
    "text": "saturation and then setting a cache",
    "start": "1056520",
    "end": "1059080"
  },
  {
    "text": "directory all right so then we set the P",
    "start": "1059080",
    "end": "1061880"
  },
  {
    "text": "torch estimator so I'm going to show you",
    "start": "1061880",
    "end": "1064320"
  },
  {
    "text": "the script so we have this this local",
    "start": "1064320",
    "end": "1066320"
  },
  {
    "text": "run llama uh with the neuron uh",
    "start": "1066320",
    "end": "1069120"
  },
  {
    "text": "distributed script right here this is",
    "start": "1069120",
    "end": "1071559"
  },
  {
    "text": "the instance type so 32 XL and then",
    "start": "1071559",
    "end": "1075280"
  },
  {
    "text": "we're passing that Docker image and we",
    "start": "1075280",
    "end": "1077280"
  },
  {
    "text": "have eight instances we're passing all",
    "start": "1077280",
    "end": "1079720"
  },
  {
    "text": "of the hyperparameters rooll job name",
    "start": "1079720",
    "end": "1083320"
  },
  {
    "text": "environment variable fast file mode",
    "start": "1083320",
    "end": "1085240"
  },
  {
    "text": "because we're uh pointing to",
    "start": "1085240",
    "end": "1088039"
  },
  {
    "text": "S3 and then this is where we uh do the",
    "start": "1088039",
    "end": "1091480"
  },
  {
    "text": "warm pooling basically so um you need to",
    "start": "1091480",
    "end": "1094880"
  },
  {
    "text": "add permissions for uh warm pool",
    "start": "1094880",
    "end": "1097640"
  },
  {
    "text": "resources so it's another service limit",
    "start": "1097640",
    "end": "1099880"
  },
  {
    "text": "increase um it should be uh automatic if",
    "start": "1099880",
    "end": "1103679"
  },
  {
    "text": "you already have um access to those",
    "start": "1103679",
    "end": "1105799"
  },
  {
    "text": "instances it should go through very very",
    "start": "1105799",
    "end": "1107559"
  },
  {
    "text": "quickly and so so essentially once",
    "start": "1107559",
    "end": "1110360"
  },
  {
    "text": "you've enabled this um then when you run",
    "start": "1110360",
    "end": "1113640"
  },
  {
    "text": "this job um that cluster will stay warm",
    "start": "1113640",
    "end": "1116919"
  },
  {
    "text": "actually the resources will stay",
    "start": "1116919",
    "end": "1118559"
  },
  {
    "text": "available to you uh the image will stay",
    "start": "1118559",
    "end": "1121200"
  },
  {
    "text": "there as well and then you can rapidly",
    "start": "1121200",
    "end": "1124360"
  },
  {
    "text": "um you can rapidly iterate and so you",
    "start": "1124360",
    "end": "1126640"
  },
  {
    "text": "can run another job using that same base",
    "start": "1126640",
    "end": "1130039"
  },
  {
    "text": "image but just modifying the script um",
    "start": "1130039",
    "end": "1133280"
  },
  {
    "text": "and then uh essentially you can test um",
    "start": "1133280",
    "end": "1136280"
  },
  {
    "text": "and develop in seconds and so and when",
    "start": "1136280",
    "end": "1138880"
  },
  {
    "text": "you're developing for the sagemaker",
    "start": "1138880",
    "end": "1140520"
  },
  {
    "text": "training API definitely uh strongly",
    "start": "1140520",
    "end": "1143159"
  },
  {
    "text": "recommend using this worm pool and then",
    "start": "1143159",
    "end": "1145960"
  },
  {
    "text": "S3",
    "start": "1145960",
    "end": "1147320"
  },
  {
    "text": "checkpoints and then the distribution",
    "start": "1147320",
    "end": "1149760"
  },
  {
    "text": "mode uh so this is enabling torch run",
    "start": "1149760",
    "end": "1152880"
  },
  {
    "text": "all right so let's check out that",
    "start": "1152880",
    "end": "1153960"
  },
  {
    "text": "training script so that's over here in",
    "start": "1153960",
    "end": "1157960"
  },
  {
    "text": "scripts all right so quite a few objects",
    "start": "1157960",
    "end": "1161640"
  },
  {
    "text": "here obviously you have a",
    "start": "1161640",
    "end": "1163520"
  },
  {
    "text": "requirements.txt when you pass this um",
    "start": "1163520",
    "end": "1168760"
  },
  {
    "text": "sagemaker training will look in your",
    "start": "1168760",
    "end": "1170960"
  },
  {
    "text": "Source",
    "start": "1170960",
    "end": "1172200"
  },
  {
    "text": "directory which we pass here so we're",
    "start": "1172200",
    "end": "1174760"
  },
  {
    "text": "we're telling it to look in this",
    "start": "1174760",
    "end": "1177200"
  },
  {
    "text": "directory and then within this directory",
    "start": "1177200",
    "end": "1179679"
  },
  {
    "text": "it will default to looking for a",
    "start": "1179679",
    "end": "1182640"
  },
  {
    "text": "requirements.txt if it finds one if it",
    "start": "1182640",
    "end": "1185320"
  },
  {
    "text": "finds one it will just do a pip install",
    "start": "1185320",
    "end": "1187960"
  },
  {
    "text": "if you have packages that aren't pip",
    "start": "1187960",
    "end": "1190400"
  },
  {
    "text": "installable then you want to extend that",
    "start": "1190400",
    "end": "1192960"
  },
  {
    "text": "deep learning container so this image",
    "start": "1192960",
    "end": "1195520"
  },
  {
    "text": "that we looked at above you can",
    "start": "1195520",
    "end": "1198240"
  },
  {
    "text": "basically extend this Docker image so",
    "start": "1198240",
    "end": "1200720"
  },
  {
    "text": "you can build a new Docker file extend",
    "start": "1200720",
    "end": "1204080"
  },
  {
    "text": "this image add any other packages you",
    "start": "1204080",
    "end": "1206720"
  },
  {
    "text": "need deploy that to ECR and then you can",
    "start": "1206720",
    "end": "1209440"
  },
  {
    "text": "use it for your training jobs uh or you",
    "start": "1209440",
    "end": "1211600"
  },
  {
    "text": "can just run it uh via pip install if",
    "start": "1211600",
    "end": "1214000"
  },
  {
    "text": "it's a python",
    "start": "1214000",
    "end": "1216360"
  },
  {
    "text": "package all right so here is our run",
    "start": "1216360",
    "end": "1221559"
  },
  {
    "text": "llama all right so again we're using the",
    "start": "1227159",
    "end": "1229600"
  },
  {
    "text": "neuron distributed",
    "start": "1229600",
    "end": "1231600"
  },
  {
    "text": "SDK which is right here and that's",
    "start": "1231600",
    "end": "1234480"
  },
  {
    "text": "helping us uh work with the neuron",
    "start": "1234480",
    "end": "1236840"
  },
  {
    "text": "machines we're getting the data parallel",
    "start": "1236840",
    "end": "1238880"
  },
  {
    "text": "ranks um the data parallel size tensor",
    "start": "1238880",
    "end": "1242320"
  },
  {
    "text": "parallel Rank and then initializing",
    "start": "1242320",
    "end": "1244480"
  },
  {
    "text": "model parallelism so quite a few um",
    "start": "1244480",
    "end": "1248159"
  },
  {
    "text": "objects here again from neuron",
    "start": "1248159",
    "end": "1249520"
  },
  {
    "text": "distributed so parallel layers",
    "start": "1249520",
    "end": "1252919"
  },
  {
    "text": "optimizers utils and then the Llama",
    "start": "1252919",
    "end": "1256080"
  },
  {
    "text": "config is coming from hugging face so",
    "start": "1256080",
    "end": "1258080"
  },
  {
    "text": "that's the trans",
    "start": "1258080",
    "end": "1260600"
  },
  {
    "text": "Transformers and then uh we're importing",
    "start": "1261240",
    "end": "1264440"
  },
  {
    "text": "quite a few objects from the rest of",
    "start": "1264440",
    "end": "1266320"
  },
  {
    "text": "this repository and so that's all here",
    "start": "1266320",
    "end": "1269080"
  },
  {
    "text": "all right uh so then I'm not going to go",
    "start": "1269080",
    "end": "1271640"
  },
  {
    "text": "through this in too much detail but",
    "start": "1271640",
    "end": "1273440"
  },
  {
    "text": "please feel free to study this on your",
    "start": "1273440",
    "end": "1275039"
  },
  {
    "text": "own um so here's this all reduce which I",
    "start": "1275039",
    "end": "1279320"
  },
  {
    "text": "love and then the save on the",
    "start": "1279320",
    "end": "1282520"
  },
  {
    "text": "checkpoints saving checkpoints um is",
    "start": "1282520",
    "end": "1285720"
  },
  {
    "text": "non-trivial in distributed training uh",
    "start": "1285720",
    "end": "1288640"
  },
  {
    "text": "particularly for model and data",
    "start": "1288640",
    "end": "1290480"
  },
  {
    "text": "parallelism um because you need to",
    "start": "1290480",
    "end": "1292279"
  },
  {
    "text": "ensure that you're reconstructing the",
    "start": "1292279",
    "end": "1295440"
  },
  {
    "text": "model um such that you only have one",
    "start": "1295440",
    "end": "1298360"
  },
  {
    "text": "version of the model and you have a",
    "start": "1298360",
    "end": "1299880"
  },
  {
    "text": "coherent number of um components to",
    "start": "1299880",
    "end": "1305559"
  },
  {
    "text": "that all right and then this is the core",
    "start": "1306559",
    "end": "1309480"
  },
  {
    "text": "train uh and so like most distributed",
    "start": "1309480",
    "end": "1312120"
  },
  {
    "text": "training libraries uh it's going to look",
    "start": "1312120",
    "end": "1313960"
  },
  {
    "text": "to see which rank the um which rank the",
    "start": "1313960",
    "end": "1318880"
  },
  {
    "text": "script is operating on at that point in",
    "start": "1318880",
    "end": "1320520"
  },
  {
    "text": "time and basically when it's the leader",
    "start": "1320520",
    "end": "1323480"
  },
  {
    "text": "rank or or zero um then in this case",
    "start": "1323480",
    "end": "1326120"
  },
  {
    "text": "just printing the arguments we're",
    "start": "1326120",
    "end": "1328279"
  },
  {
    "text": "setting the seeds and then initializing",
    "start": "1328279",
    "end": "1330720"
  },
  {
    "text": "the model parallelism degree uh getting",
    "start": "1330720",
    "end": "1333679"
  },
  {
    "text": "the data parallel rank tensor parallel",
    "start": "1333679",
    "end": "1336240"
  },
  {
    "text": "rank then we load the Llama",
    "start": "1336240",
    "end": "1340559"
  },
  {
    "text": "config initialize the",
    "start": "1341679",
    "end": "1345039"
  },
  {
    "text": "weights and then create the model Direct",
    "start": "1345039",
    "end": "1348520"
  },
  {
    "text": "directly on the host",
    "start": "1348520",
    "end": "1351279"
  },
  {
    "text": "device and then here is the neuron",
    "start": "1352880",
    "end": "1358080"
  },
  {
    "text": "distributed which takes that llama model",
    "start": "1358240",
    "end": "1361600"
  },
  {
    "text": "and then reconstructs it essentially for",
    "start": "1361600",
    "end": "1366640"
  },
  {
    "text": "neuron then we load the model weights",
    "start": "1366640",
    "end": "1369480"
  },
  {
    "text": "set up the zero",
    "start": "1369480",
    "end": "1372200"
  },
  {
    "text": "Optimizer and so on so I'll I'll leave",
    "start": "1372200",
    "end": "1374720"
  },
  {
    "text": "the the rest of this um for you to study",
    "start": "1374720",
    "end": "1376799"
  },
  {
    "text": "at home but definitely really",
    "start": "1376799",
    "end": "1378760"
  },
  {
    "text": "interesting um and so when you call",
    "start": "1378760",
    "end": "1380799"
  },
  {
    "text": "model. fits that does indeed train a",
    "start": "1380799",
    "end": "1383600"
  },
  {
    "text": "model um so it launches these instances",
    "start": "1383600",
    "end": "1386559"
  },
  {
    "text": "launches this cluster and then uh so",
    "start": "1386559",
    "end": "1390000"
  },
  {
    "text": "here's an example uh when I trained this",
    "start": "1390000",
    "end": "1392159"
  },
  {
    "text": "with again uh eight uh TRN um machines",
    "start": "1392159",
    "end": "1396159"
  },
  {
    "text": "with 32 Excel and then all of the the",
    "start": "1396159",
    "end": "1399360"
  },
  {
    "text": "metadata the content um the logs are all",
    "start": "1399360",
    "end": "1402840"
  },
  {
    "text": "stored and searchable and versioned by",
    "start": "1402840",
    "end": "1405159"
  },
  {
    "text": "default so it's really easy um to take a",
    "start": "1405159",
    "end": "1407919"
  },
  {
    "text": "look at these",
    "start": "1407919",
    "end": "1409039"
  },
  {
    "text": "modify previous experiments and then use",
    "start": "1409039",
    "end": "1411919"
  },
  {
    "text": "that to uh to indicate the rest of your",
    "start": "1411919",
    "end": "1415640"
  },
  {
    "text": "work so with that uh let me show you",
    "start": "1415640",
    "end": "1418320"
  },
  {
    "text": "some",
    "start": "1418320",
    "end": "1419039"
  },
  {
    "text": "resources okay so let's let's check this",
    "start": "1419039",
    "end": "1421440"
  },
  {
    "text": "out so the first resource for you uh is",
    "start": "1421440",
    "end": "1424039"
  },
  {
    "text": "of course the the distributed training",
    "start": "1424039",
    "end": "1426279"
  },
  {
    "text": "documentation um so this does a good job",
    "start": "1426279",
    "end": "1428360"
  },
  {
    "text": "of walking through some of the key",
    "start": "1428360",
    "end": "1430360"
  },
  {
    "text": "Concepts vocabulary um the uh apis um",
    "start": "1430360",
    "end": "1435360"
  },
  {
    "text": "some of the top uh questions and all of",
    "start": "1435360",
    "end": "1437960"
  },
  {
    "text": "the other top topics that you need to",
    "start": "1437960",
    "end": "1439320"
  },
  {
    "text": "know uh to get distributed training",
    "start": "1439320",
    "end": "1442039"
  },
  {
    "text": "running and then uh of course here's",
    "start": "1442039",
    "end": "1444520"
  },
  {
    "text": "your example notebook so using the uh",
    "start": "1444520",
    "end": "1447240"
  },
  {
    "text": "llama 2 70b pre-training with sagemaker",
    "start": "1447240",
    "end": "1450279"
  },
  {
    "text": "trinium and then uh if you'd like to",
    "start": "1450279",
    "end": "1452760"
  },
  {
    "text": "watch my reinvent session on the topic",
    "start": "1452760",
    "end": "1455400"
  },
  {
    "text": "uh feel free to check that out from uh",
    "start": "1455400",
    "end": "1458720"
  },
  {
    "text": "2023 so with that thank you uh that's a",
    "start": "1458720",
    "end": "1461840"
  },
  {
    "text": "wrap I'll see you next time",
    "start": "1461840",
    "end": "1465600"
  }
]